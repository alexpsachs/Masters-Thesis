{
    "vishh": "kubernetes/pkg/client depends on an auto-generated version info file. Once\nthey become true go packages, I can get rid of kube client code in heapster.\nOn Tue Aug 26 2014 at 1:16:48 PM Daniel Smith notifications@github.com\nwrote:\n\nWhat can we do in k8s to make it easier for you to import just\nkubernetes/pkg/client & /pkg/api instead of rolling your own client which\nis likely to break in the future?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/1.\n. Awesome. I will update the code soon.\n\nOn Tue, Aug 26, 2014 at 1:44 PM, Daniel Smith notifications@github.com\nwrote:\n\nWish granted, I think we removed that yesterday or maybe it was last week.\n:)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/1#issuecomment-53488350\n.\n. Fixed.\n. Thanks for pointing it out. Fixed.\n. The influxdb docker container is setup to work on GCE only as of now. We might be able to solve this issue by moving grafana into a separate service, thereby enabling discovery of InfluxDB container natively. \n. As of now the grafana container is setup to assume that influxdb is\navailable at localhost:8086. The reason for querying the metadata servers\nis to get the public IP of the host machine.\n\nOn Sat, Aug 30, 2014 at 5:02 PM, Akram Hussein notifications@github.com\nwrote:\n\nthanks. Is there anything special about the docker container in GCE? or it\nis just the scripts that query this metadata server?\nOn Sat, Aug 30, 2014 at 4:55 PM, Vish Kannan notifications@github.com\nwrote:\n\nThe influxdb docker container is setup to work on GCE only as of now. We\nmight be able to solve this issue by moving grafana into a separate\nservice, thereby enabling discovery of InfluxDB container natively.\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/GoogleCloudPlatform/heapster/issues/3#issuecomment-53974022>\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/3#issuecomment-53974142\n.\n. Kubernetes does not provide a public IP for a service on GCE yet. This makes it hard to break down the Influx service into separate Pods. We will have to wait for some new Kubernetes features.\n. This issue has been fixed on the latest version of heapster.\n. This has been fixed in the latest version of heapster images.\n. If you run go build under cluster/coreos a binary named 'coreos' will be generated. A build file is missing which might have caused the confusion.\n. cc @vmarmol @rjnagal \n. Ping @vmarmol @rjnagal \n. Thanks for the quick review @rjnagal \n. Ping @rjnagal @vmarmol \n. @vmarmol: The master service is a recent addition. I changed a few more files. PTAL before merging.\n. Merging this now.. \n. Ping @vmarmol @rjnagal \n. It seems to work without the 'chmod'.\n\nOn Thu, Nov 13, 2014 at 8:30 AM, Victor Marmol notifications@github.com\nwrote:\n\nLGTM\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/10#issuecomment-62921097\n.\n. @sebzimmermann: This is still an open issue. I intend to fix it when I get some spare cycles. Meanwhile PRs are welcome :)\n. Coreos support has been integrated into the main binary.\n. Resolved this issue via IRC. To summarize @viklas was looking at the heapster host instead of the grafana host.\n. Ping @rjnagal \n. @jeinwag: It takes some time for the stats to be collected by heapster. Did you let it run for a minute or more? \nCan you try running against kubernetes HEAD? We currently don't qualify heapster against all kubernetes releases. We need to do a better job there. \n. Do you see any errors from heapster logs? It looks like heapster is able to\nretrieve machine information, but not the pod information.\nSince cadvisor is a static pod and is not scheduled by the master, it will\nnot show up as part of kubectl.sh or kubecfg.sh.\n\nOn Fri, Dec 12, 2014 at 6:27 PM, Adrien Lemaire notifications@github.com\nwrote:\n\nHi, I've just done this: my cloud-config drops the cadvisor.manifest in\n/etc/kubernetes/manifests/, and I run my kubelet with\n--config=/etc/kubernetes/manifests, so now all nodes have k8s\ncontainers for cadvisor.\nBut I have a few issues:\n- cadvisor isn't listed by kubecfg. How can I get kubecfg to list a\n## pod for the running cadvisor k8s?\nI can only get a couple of graphs working:\n  [image: graphs]\n  https://camo.githubusercontent.com/e96e130739c99bdabe00ac116ff1d1e9aaebf61c/687474703a2f2f692e696d6775722e636f6d2f65646b7055314d2e706e67\nHere is the head of the docker logs: http://sprunge.us/eLeB\n  I'm running kubernetes v0.5.3 with the default cadvisor.manifest\n  https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/v0.5.3/cluster/saltbase/salt/cadvisor/cadvisor.manifest\n  And I can curl localhost:10250/stats/ from a minion:\n  http://sprunge.us/QiJL\n  Any idea what could be wrong ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/15#issuecomment-66861634\n.\n. Fandekasp: Can you try running with the latest version of kubernetes? This seems like an incompatibility between the components. \nWe have an e2e test now for heapster and incompatibility should be less of an issue in the future. \n. @Fandekasp: Ok. Lets try to isolate the issue. Can you check the following:\n1. Cadvisor UI works and container's cpu and memory usage shows up in the UI. It is expected to run on port 4194.\n2. Kubelet stats API works. Kubelet is expected to run on port 10250. You can hit http://<minionIP:10250>/stats/ for machine stats (which I think works for you) and http://<minionIP:10250>/stats/heapster/heapster.\n. @Fandekasp: Are you running on a systemd machine? If so can you check that the \"Docker containers\" link in the cAdvisor UI works?\n. @Fandekasp: As @vmarmol mentioned its a cadvisor issue and a new version with a fix will be released soon. The rest of the heapster setup seems to be working for you :)\n. @Fandekasp: cadvisor 0.7.0 has been released. Can you give it a try and see if it solves your current issues?\n. I don't think kubernetes has been upgraded to run cadvisor 0.7.0. cadvisor\nand kubernetes do not follow the same version numbers. @vmarmol, when will\n0.7.0 be integrated into kubernetes?\n\nOn Fri, Dec 19, 2014 at 3:59 PM, Adrien Lemaire notifications@github.com\nwrote:\n\nIn order to write a script that will upgrade cadvisor on a running\ncluster, I manually tried the following:\n- ssh in each minions\n  $ cd /etc/kubernetes/manifests && sudo rm cadvisor.manifest && sudo wget https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/v0.7.0/cluster/saltbase/salt/cadvisor/cadvisor.manifest\n  $ sudo pkill kubelet\n  $ docker rm -f \n  $ docker rm -f \n  $ sudo /opt/bin/kubelet --address=0.0.0.0 --port=10250 --config=/etc/kubernetes/manifests --hostname_override= --etcd_servers=http://127.0.0.1:4001 --logtostderr=true\nBut this doesn't seem to fix the issue:\ncore@ip-10-165-74-145 ~$ curl http://localhost:4194/docker/\nFailed to get container \"\" with error: unable to find data for container /system.slice/docker-e9561984c4282af68df8fd228bb45b8a6c48794e89e2e0669b7abc1edc1d44a8.scope\ncore@ip-10-165-74-145 ~ $ docker ps | grep cadvisor\n100a930ef5bc        google/cadvisor:0.7.0     \"/usr/bin/cadvisor\"   8 minutes ago       Up 8 minutes                                 k8s_cadvisor.b0dae998_cadvisormanifes12uqn2ohido76855gdecd9roadm7l0.default.file_cadvisormanifes12uqn2ohido76855gdecd9roadm7l0_cb16d227\n73c9809d414a        kubernetes/pause:go       \"/pause\"              17 hours ago        Up 17 hours         0.0.0.0:4194->8080/tcp   k8s_net.a0f18f6e_cadvisormanifes12uqn2ohido76855gdecd9roadm7l0.default.file_cadvisormanifes12uqn2ohido76855gdecd9roadm7l0_1fb596ea\ncore@ip-10-165-74-145 ~ $\nThe docker logs of those containers are empty.\nAlso note that the cadvisor.manifest uses the docker image\ngoogle/cadvisor:latest, when I would expect it to use\ngoogle/cadvisor:v0.7.0 (I believe cadvisor is following the same\nversions as kubernetes).\nSince I only upgraded cadvisor in the operation, should I also upgrade\nkubernetes from v0.6.2 to v0.7.0 in order to fix that issue? Please let me\nknow if I missed any step, thank you\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/15#issuecomment-67714626\n.\n. @Fandekasp: Kubernetes is using the latest version of cadvisor. Can you try upgrading cadvisor and see if that fixes your issues? \n. We will make a binary release moving forward.\n. v0.4 has been released, along with a binary. \n. Ping @vmarmol @rjnagal \n. @nlamirault: Can you look up the logs from heapster container? We made a new heapster release yesterday. You might want to try to use the latest release.\n. From the output of kubectl.sh get pods heapster, you can figure out which\nminion is currently running heapster pod. On the minion, you can figure out\nheapster container by running docker ps and then look at the logs of the\nheapster container by running docker logs <heapster_container_id>\n\nOn Wed, Dec 3, 2014 at 4:41 PM, Nicolas Lamirault notifications@github.com\nwrote:\n\nSame errors. I use the kubernetes configuration in the deploy\ndirectory. How can i see heapster logs ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/22#issuecomment-65392166\n.\n. From the logs, it looks like heapster is not able to talk to kubernetes\nmaster. If you let heapster run for a while, do you continue to see errors\nin heapster log? Can you check if the master is reachable from the host\nwhere heapster is running? From the logs you posted, it looks like the\nmaster is running at '10.0.0.148:80 http://10.0.0.148/'.\n\nOn Mon, Dec 8, 2014 at 1:18 AM, Nicolas Lamirault notifications@github.com\nwrote:\n\nThis is output logs of some container\n$ docker ps\nCONTAINER ID        IMAGE                                 COMMAND                CREATED             STATUS              PORTS                                                                                                                                        NAMES\ne7033172a1fc        dockerfile/elasticsearch:latest       \"/elasticsearch/bin/   4 days ago          Up 4 days                                                                                                                                                        k8s_elasticsearch.7b8bd84_influx-grafana.default.etcd_89c5fea9-7ad9-11e4-858c-0800275fbc80_c72879f1   17825c3b6332        kubernetes/heapster_grafana:canary    \"/run.sh\"              4 days ago          Up 4 days                                                                                                                                                        k8s_grafana.e02bbfea_influx-grafana.default.etcd_89c5fea9-7ad9-11e4-858c-0800275fbc80_cc7cc951        9187b792e127        kubernetes/heapster_influxdb:latest   \"/run.sh\"              4 days ago          Up 4 days                                                                                                                                                        k8s_influxdb.c319f14d_influx-grafana.default.etcd_89c5fea9-7ad9-11e4-858c-0800275fbc80_05d606c2       755dcef95d77        kubernetes/pause:go                   \"/pause\"               4 days ago          Up 4 days           0.0.0.0:80->80/tcp, 0.0.0.0:8083->8083/tcp, 0.0.0.0:8086->8086/tcp, 0.0.0.0:8090->8090/tcp, 0.0.0.0:8099->8099/tcp, 0.0.0.0:9200->9200/tcp   k8s_net.380d25a7_influx-grafana.default.etcd_89c5fea9-7ad9-11e4-858c-0800275fbc80_e71f6313            ad571e1dadd0        google/cadvisor:0.6.2                 \"/usr/bin/cadvisor -   4 days ago          Up 4 days           8080/tcp, 0.0.0.0:4194->4194/tcp                                                                                                             cadvisor\n$ docker logs 9187b792e127\n=> About to create the following database: k8s\n=> Starting InfluxDB ...\n=> Waiting for confirmation of InfluxDB service startup ...\n+---------------------------------------------+|  _  _            _    || | |      / _| |          |   |  _ \\  ||   | |  _  | || |  | |  | | |) | ||   | | | ' |  | | | | \\ \\/ / |  | |  _ <  ||  | || | | | | | | || |>  <| || | |) | || |_|| ||| ||,/_/__/|/  |+---------------------------------------------+[12/03/14 10:47:13] [INFO] Loading configuration file /config/config.toml=> Waiting for confirmation of InfluxDB service startup ...{\"status\":\"ok\"}=> Creating database: k8sexec /usr/bin/influxdb -config=${CONFIG_FILE}$ docker logs 17825c3b6332=> Creating basic auth for \" admin\" user with preset passwordAdding password for user admin=> Done!========================================================================You can now connect to Grafana with the following credential:    admin:admin=========================================================================> Configuring InfluxDB=> InfluxDB has been configured as follows:   InfluxDB ADDRESS:  \"+window.location.hostname+\"   InfluxDB PORT:     8086   InfluxDB DB NAME:  k8s   InfluxDB USERNAME: root   InfluxDB PASSWORD: root   * Please check your environment variables if you find something is misconfigured. *=> Done!=> Found Elasticsearch settings.=> Set Elasticsearch url to \"http://\"+window.location.hostname+\":9200\".=> Done!=>Setting default dashboard to \\/dashboard\\/file\\/kubernetes.json=>Done=> Starting and running Nginx...$ docker logs e7033172a1fc[2014-12-03 10:53:41,200][INFO ][node                     ] [Dragonwing] version[1.4.0], pid[1], build[bc94bd8/2014-11-05T14:26:12Z][2014-12-03 10:53:41,205][INFO ][node                     ] [Dragonwing] initializing ...[2014-12-03 10:53:41,228][INFO ][plugins                  ] [Dragonwing] loaded [], sites [][2014-12-03 10:53:48,572][INFO ][node                     ] [Dragonwing] initialized[2014-12-03 10:53:48,576][INFO ][node                     ] [Dragonwing] starting ...[2014-12-03 10:53:48,750][INFO ][transport                ] [Dragonwing] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.100.8.3:9300]}[2014-12-03 10:53:48,821][INFO ][discovery                ] [Dragonwing] elasticsearch/Ec_2wTKkQ56MNS0DhTPvgw[2014-12-03 10:53:52,935][INFO ][cluster.service          ] [Dragonwing] new_master [Dragonwing][Ec_2wTKkQ56MNS0DhTPvgw][influx-grafana][inet[/10.100.8.3:9300]], reason: zen-disco-join (elected_as_master)[2014-12-03 10:53:53,001][INFO ][http                     ] [Dragonwing] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.100.8.3:9200]}[2014-12-03 10:53:53,003][INFO ][node                     ] [Dragonwing] started[2014-12-03 10:53:53,047][INFO ][gateway                  ] [Dragonwing] recovered [0] indices into cluster_state$\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/22#issuecomment-66041603\n.\n. Can you check if the master is reachable from the host which is running the\nheapster pod? From the logs the master IP seems to be \"10.0.0.227:80\nhttp://10.0.0.227/\".\n\nOn Tue, Dec 9, 2014 at 1:09 AM, Nicolas Lamirault notifications@github.com\nwrote:\n\nI had to recreate the cluster.\ncore@minion-2 $ docker ps\nCONTAINER ID        IMAGE                     COMMAND                CREATED             STATUS              PORTS                              NAMES\n03bfecae3fc0        kubernetes/heapster:0.4   \"/run.sh\"              9 seconds ago       Up 8 seconds                                           k8s_portefaix-heapster.4701ac2_portefaix-heapster-pod.default.etcd_41bb2e71-7f80-11e4-9c0c-0800275fbc80_6445f2c0\n28d5198dcd86        kubernetes/pause:go       \"/pause\"               15 minutes ago      Up 15 minutes                                          k8s_net.dbcb7509_portefaix-heapster-pod.default.etcd_41bb2e71-7f80-11e4-9c0c-0800275fbc80_5d569f21\n7a629ddf5576        google/cadvisor:0.6.2     \"/usr/bin/cadvisor -   31 minutes ago      Up 31 minutes       8080/tcp, 0.0.0.0:4194->4194/tcp   cadvisor                                                                                                           core@localhost ~ $ docker logs 03bfecae3fc0Detected Kube specific args. Starting in Kube mode.I1209 09:05:08.200574 00006 heapster.go:18] /usr/bin/heapster --kubernetes_master 10.0.0.227:80 --sink influxdb --sink_influxdb_host 10.0.0.5:8085I1209 09:05:08.201711 00006 heapster.go:19] Heapster version 0.4I1209 09:05:08.308501 00006 influxdb.go:180] Database creation failed - Post http://10.0.0.5:8085/db?u=root&p=root: net/http: transport closed before response was receivedE1209 09:05:18.417711 00006 heapster.go:22] Get http://10.0.0.227:80/api/v1beta1/pods?labels=&namespace=default: net/http: transport closed before response was receivedcore@minion-2 $ ping 10.0.0.5PING 10.0.0.5 (10.0.0.5) 56(84) bytes of data.From 10.133.0.253: icmp_seq=1 Destination Host UnreachableFrom 10.133.0.253: icmp_seq=3 Destination Host Unreachable^C--- 10.0.0.5 ping statistics ---3 packets transmitted, 0 received, +2 errors, 100% packet loss, time 2001ms\nan on the other Pod :\ncore@minion-1 $ docker ps\nCONTAINER ID        IMAGE                                 COMMAND                CREATED             STATUS              PORTS                                                                                                                                                                NAMES\ne500b2bb14df        dockerfile/elasticsearch:latest       \"/elasticsearch/bin/   22 minutes ago      Up 22 minutes                                                                                                                                                                            k8s_elasticsearch.d62bbe30_influx-grafana.default.etcd_550b08b6-7f7e-11e4-9c0c-0800275fbc80_6596fee2   6f0c7d4db0c8        kubernetes/heapster_grafana:canary    \"/run.sh\"              25 minutes ago      Up 25 minutes                                                                                                                                                                            k8s_grafana.eba1bffa_influx-grafana.default.etcd_550b08b6-7f7e-11e4-9c0c-0800275fbc80_cbe7a617         bbba1b54f60a        kubernetes/heapster_influxdb:latest   \"/run.sh\"              28 minutes ago      Up 28 minutes                                                                                                                                                                            k8s_influxdb.ce8ff15d_influx-grafana.default.etcd_550b08b6-7f7e-11e4-9c0c-0800275fbc80_e7dd9dee        49c1c7ce010a        kubernetes/pause:go                   \"/pause\"               30 minutes ago      Up 30 minutes       0.0.0.0:80->80/tcp, 0.0.0.0:8083->8083/tcp, 0.0.0.0:8086->8086/tcp, 0.0.0.0:8090->8090/tcp, 0.0.0.0:8099->8099/tcp, 0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   k8s_net.3b912643_influx-grafana.default.etcd_550b08b6-7f7e-11e4-9c0c-0800275fbc80_518a8709             deb94159bb8a        google/cadvisor:0.6.2                 \"/usr/bin/cadvisor -   34 minutes ago      Up 33 minutes       8080/tcp, 0.0.0.0:4194->4194/tcp                                                                                                                                     cadvisor\nthe ES container logs :\ncore@minion-1 $ docker logs e500b2bb14df\n[2014-12-09 08:43:43,666][INFO ][node                     ] [Random] version[1.4.0], pid[1], build[bc94bd8/2014-11-05T14:26:12Z]\n[2014-12-09 08:43:43,669][INFO ][node                     ] [Random] initializing ...\n[2014-12-09 08:43:43,683][INFO ][plugins                  ] [Random] loaded [], sites []\n[2014-12-09 08:43:51,080][INFO ][node                     ] [Random] initialized\n[2014-12-09 08:43:51,081][INFO ][node                     ] [Random] starting ...\n[2014-12-09 08:43:51,244][INFO ][transport                ] [Random] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.100.14.3:9300]}\n[2014-12-09 08:43:51,308][INFO ][discovery                ] [Random] elasticsearch/T5EkabxpT_G-GcMDtJLOmQ\n[2014-12-09 08:43:55,453][INFO ][cluster.service          ] [Random] new_master [Random][T5EkabxpT_G-GcMDtJLOmQ][influx-grafana][inet[/10.100.14.3:9300]], reason: zen-disco-join (elected_as_master)\n[2014-12-09 08:43:55,511][INFO ][http                     ] [Random] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.100.14.3:9200]}\n[2014-12-09 08:43:55,515][INFO ][node                     ] [Random] started\n[2014-12-09 08:43:55,543][INFO ][gateway                  ] [Random] recovered [0] indices into cluster_state\nThe Grafana logs :\ncore@minion-1 $ docker logs 6f0c7d4db0c8\n=> Creating basic auth for \" admin\" user with preset password\nAdding password for user admin\n=> Done!\nYou can now connect to Grafana with the following credential:\nadmin:admin\n\n=> Configuring InfluxDB\n=> InfluxDB has been configured as follows:\n   InfluxDB ADDRESS:  \"+window.location.hostname+\"\n   InfluxDB PORT:     8086\n   InfluxDB DB NAME:  k8s\n   InfluxDB USERNAME: root\n   InfluxDB PASSWORD: root\n   * Please check your environment variables if you find something is misconfigured. *\n=> Done!\n=> Found Elasticsearch settings.\n=> Set Elasticsearch url to \"http://\"+window.location.hostname+\":9200\".\n=> Done!\n=>Setting default dashboard to \\/dashboard\\/file\\/kubernetes.json\n=>Done\n=> Starting and running Nginx...\nThe InfluxDB logs :\ncore@minion-1 $ docker logs bbba1b54f60a\n=> About to create the following database: k8s\n=> Starting InfluxDB ...\n=> Waiting for confirmation of InfluxDB service startup ...\n+---------------------------------------------+|  _  _            _    || | |      / _| |          |   |  _ \\  ||   | |  _  | || |  | |  | | |) | ||   | | | ' |  | | | | \\ \\/ / |  | |  _ <  ||  | || | | | | | | || |>  <| || | |) | || |_|| ||| ||,/_/__/|/  |+---------------------------------------------+[12/09/14 08:37:07] [INFO] Loading configuration file /config/config.toml=> Waiting for confirmation of InfluxDB service startup ...{\"status\":\"ok\"}=> Creating database: k8sexec /usr/bin/influxdb -config=${CONFIG_FILE}\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/22#issuecomment-66253221\n.\n. Your kubernetes cluster isn't working properly. Where are you running\nkubernetes? Make sure that the networking requirements of kubernetes is\nsatisfied in your cluster.\n\nOn Tue, Dec 9, 2014 at 8:09 AM, Nicolas Lamirault notifications@github.com\nwrote:\n\nNo.\ncore@minion-1 ~ $ ping 10.0.0.227\nPING 10.0.0.227 (10.0.0.227) 56(84) bytes of data.\nFrom 10.133.0.253: icmp_seq=1 Destination Host Unreachable\nFrom 10.133.0.253: icmp_seq=2 Destination Host Unreachable\ncore@minion-2 ~ $ ping 10.0.0.227\nPING 10.0.0.227 (10.0.0.227) 56(84) bytes of data.\nFrom 10.133.0.253: icmp_seq=1 Destination Host Unreachable\nFrom 10.133.0.253: icmp_seq=2 Destination Host Unreachable\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/22#issuecomment-66307176\n.\n. Have you looked at this page\nhttps://github.com/errordeveloper/weave-demos/blob/master/poseidon/README.md?\nI haven't tried setting up a CoreOS cluster using virtual box. Perhaps\nsomeone on IRC (#google-containers on freenode) might be able to assist you.\n\nOn Tue, Dec 9, 2014 at 8:58 AM, Nicolas Lamirault notifications@github.com\nwrote:\n\nI use Virtualbox/CoreOS/Docker/Flannel. Which tests can i perform to check\nKubernetes network configuration ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/22#issuecomment-66316476\n.\n. The master and minions communicate via etcd. I am not familiar with the\nspecifics.\n\nOn Wed, Dec 10, 2014 at 4:12 AM, Nicolas Lamirault <notifications@github.com\n\nwrote:\ndo you know how minions retrieve the master IP (10.0.0.227) ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/22#issuecomment-66443159\n.\n. @Fandekasp: Can you post the first few lines of heapster logs? From the logs it looks like kubelet might be the cause of the issue. Can you run kubernetes from head and make sure you run the latest version of heapster?\n. @nlamirault: Were you able to make progress debugging your networking issues?\n. @Fandekasp: Looked at the logs again and it looks like 'cadvisor' container is not running on the minion. Did you setup cadvisor to run on every host?\n. Ping @vmarmol @rjnagal \n. The CPU spike could be from InfluxDB. It is expected. You can place cpu and\nmemory limits on the InfluxDB container. I am curious as to why heapster\nfails to come up. Can you post some logs from one of the failed runs of the\nheapster container?\n\nOn Sun, Dec 14, 2014 at 3:47 PM, Mitchel Kelonye notifications@github.com\nwrote:\n\nIs this normal?\nhttps://camo.githubusercontent.com/a1103d84c42f641713ba659e406d544e19f22e69/68747470733a2f2f646c2e64726f70626f7875736572636f6e74656e742e636f6d2f752f33303136323237382f53637265656e25323053686f74253230323031342d31322d31352532306174253230322e34312e3339253230414d2e706e67\nAlmost 60% cpu usage on a 2gb ram vm .. fedora 20 in digitalocean, yet\nit's not running in the first place:\n[root@k8-master heapster]# k8 get pods\nNAME                                   IMAGE(S)                       HOST                LABELS                             STATUS\n548e1e240e7bc2780d000003               fedora/apache:latest                   name=548e1e240e7bc2780d000003      Pending\n27a27940-83d9-11e4-bea6-040135044f01   kubernetes/heapster_influxdb   188.226.142.107/    name=influxGrafana                 Failed\n                                       kubernetes/heapster_grafana\n                                       dockerfile/elasticsearch\nffed25fd-83e4-11e4-bea6-040135044f01   kubernetes/heapster            188.226.142.107/    name=heapster,uses=influx-master   Failed\n55de4dd2-83e8-11e4-bea6-040135044f01   kubernetes/heapster_influxdb           name=influxGrafana                 Pending\n                                       kubernetes/heapster_grafana\n                                       dockerfile/elasticsearch\n66471f57-83e3-11e4-bea6-040135044f01   kubernetes/heapster            188.226.142.107/    name=heapster,uses=influx-master   Failed\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/24.\n. LGTM.\n. @cedbossneo: Thanks for the PR! Before merging, can you sign the CLA if it is not too much trouble:\nhttp://code.google.com/legal/individual-cla-v1.0.html\n. Ping @vmarmol @rjnagal \n. Thanks for the fix @jimmidyson \n. If we run the system daemons in separate cgroups, it is useful to track each of the raw cgroups separately. \n. Agreed!\n\nOn Tue, Jan 6, 2015 at 10:31 AM, Rohit Jnagal notifications@github.com\nwrote:\n\nTrue. These should be exported by Kubelet as a standard API. Otherwise, we\nwill end up picking lot of unrelated stuff that'll need to throw out. That\nis lot of wasted bandwidth at cluster level.\nOn Tue, Jan 6, 2015 at 10:24 AM, Vish Kannan notifications@github.com\nwrote:\n\nIf we run the system daemons in separate cgroups, it is useful to track\neach of the raw cgroups separately.\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/GoogleCloudPlatform/heapster/issues/30#issuecomment-68907976>\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/30#issuecomment-68909125\n.\n. Thanks for reporting the issue @josselin-c. This is a bug in heapster. I will fix it soon.\n. Keeping this open until a new heapster release is completed.\n. This required a change to kubelet. Waiting for a kubernetes release with that change to rollout.\n. Can you try building with the new deps @rjnagal ?\n. Ok. LGTM. Thanks for the fix @rjnagal :+1: \n. cc @rjnagal \n. Travis is not activated yet because I don't have admin privileges to this project yet. I have requested @jbeda to update the permissions. \n. LGTM\n. cc @rjnagal \n. PTAL @rjnagal \n. Good catch. LGTM\n. cc @rjnagal \n. I have sent another PR to update go vet.\n\nOn Tue, Jan 6, 2015 at 4:15 PM, Rohit Jnagal notifications@github.com\nwrote:\n\ngo vet has moved. You nedd to update travis. We already did that for\ncadvisor yesterday.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/37#issuecomment-68958773\n.\n. Ping @rjnagal. Travis is happy now :)\n. Fixed the go vet errors now.\n. cc @rjnagal @vmarmol. If possible try running heapster with this PR.\n. @rjnagal: Rebased and squashed a few commits. \n. Thanks for the update @vmarmol.\n\nOn Wed, Jan 7, 2015 at 5:31 PM, Victor Marmol notifications@github.com\nwrote:\n\nFinally got around to testing this. I works as expected. The only change\nwould be to update the README with the new instructions :)\nFeel free to do that in another PR. Merge away!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/39#issuecomment-69122999\n.\n. LGTM\n. Kubernetes v0.7.0 has an API issue. Kindly upgrade to the latest release\nv0.7.2.\n\nOn Wed, Jan 7, 2015 at 3:39 AM, Craig Wickesser notifications@github.com\nwrote:\n\nI'm running Kubernetes 0.7.0 and Heapster 0.4.0 and I noticed the Heapster\ncontainer exits after ~10 seconds. I see in its logs:\nheapster.go:22] data of kind 'NodeList', obj of type 'MinionList'\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/41.\n. Ping @rjnagal @vmarmol \n. Wonder how it sneaked in. Removed.\n. That function doesn't serve any purpose now. Removed.\n. @vmarmol addressed your comments. Don't merge it just yet. I want to test it once again.\n. This is ready to be merged.\n. Self merging this. \n. Ping @vmarmol @rjnagal \n. @vmarmol: Addressed your comments.\n. I have restarted the build with an increased timeout.\n\nOn Mon, Jan 12, 2015 at 7:26 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nThis time it timed out ... took 10m. I think its close to be fixable.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/48#issuecomment-69690274\n.\n. Increased the timeout once again and restarted the build. I hope jenkins will update github upon success.\n. @rjnagal: Jenkins is green now :)\n. In the kube world, passwords are updated. Once this change goes in, I will\nget rid of auth for grafana even in the kube world.\n\nOn Tue, Jan 13, 2015 at 11:11 AM, Victor Marmol notifications@github.com\nwrote:\n\nLGTM, although aren't there some scripts that automatically add the\nuser/password? Those may need changing.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/49#issuecomment-69800747\n.\n. This is ready to be merged now.\n. This is the final piece of integration testing. Yayyy!\n. Unless we have a separate registry, how can we ship docker images from the test machine over to the nodes? \n. Awesome. Yeah this sounds like a good idea. How about making this an\nincremental change on top of this PR?\n\nOn Tue, Jan 13, 2015 at 7:14 PM, Victor Marmol notifications@github.com\nwrote:\n\nI've been working on something similar for cAdvisor (target is Jenkins),\nbut I haven't quite finished. Take a look here:\nvmarmol/cadvisor@20046d6\nhttps://github.com/vmarmol/cadvisor/commit/20046d6d629052d809c88b234dfbcf2d063c2265\nSpecifically, docker save and docker load. That code saves a specific\ncontainer into a tar file, ships it over to another machine and loads it\nthere.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/50#issuecomment-69862989\n.\n. Thanks for the PR @rjnagal. Will merge once Travis is happy.\n. Working on it.\n\nOn Wed, Jan 14, 2015 at 4:41 PM, Victor Marmol notifications@github.com\nwrote:\n\nTests failing\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/53#issuecomment-70021348\n.\n. Fixed the issues. It should pass this time around.\n. Just needs a rebase.\n. run integration tests\n. don't merge this PR. I am testing the e2e test system with this PR.\n. test this please\n. Verified that e2e testing works.\n. ok to test\n. LGTM\n. LGTM\n. This will be very useful @rjnagal. Awesome. \nIgnore the integration tests until I fix them.\n. LGTM\n. I have fixed the integration tests and started one for this build. LGTM.\n. cc @rjnagal \n. Integration tests are now working :)\n. @rjnagal: PTAL\n. +1\n. Rebased. Safe to merge now.\n. Closing this for now. Once #71 and #69 is merged I will update the release notes.\n. @rjnagal: I simplified the pod errors code a bit. \n@vmarmol: Rebased\n. @vmarmol: I have rearranged the commits a bit to provide better context. PTAL\n. Ping.\n. Rebased\n. Wonder why travis is slow.\n. Ping @rjnagal \n. Can you check if cadvisor is running on all your nodes?\n\nOn Thu, Jan 29, 2015 at 3:15 AM, sourav82 notifications@github.com wrote:\n\nThese minions are on local intranet (running ubuntu). Not using any cloud\ninfrastructure. It is local Kubernetes cluster having two minions.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/76#issuecomment-72008281\n.\n. Thanks for reporting the issue. I will probe further on the CPU usage part.\nMeanwhile, it is possible to cap the total amount of CPU and memory that\nInfluxDB consumes. That might reduce the responsiveness of InfluxDB. Is\nthat an acceptable solution?\n\nOn Thu, Jan 29, 2015 at 7:10 AM, BinZhao notifications@github.com wrote:\n\nI updated to latest version of heapster(kubernetes/heapster:v0.6) and\ninfluxdb(kubernetes/heapster_influxdb:v0.3) and found high CPU usage of\ninfluxdb.\nI downgrade to heapster_influxdb:v0.2, still got high load, then tried\nheapster:v0.5, the cpu loads seemed normal\nheapter v0.5 + heapster_influxdb:v0.3:\n[image: v0 5]\nhttps://cloud.githubusercontent.com/assets/354668/5959700/4d975b08-a80b-11e4-84d0-b2d588efad2b.png\nheapter v0.6 + heapster_influxdb:v0.3:\n[image: v0 6]\nhttps://cloud.githubusercontent.com/assets/354668/5959702/50b53634-a80b-11e4-980c-a519e3456c78.png\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/77.\n. I agree with your assessment. I have seen InfluxDB use up all the CPU it\ncan at times. I will send out a PR to limit resource usage for the\nmonitoring infrastructure in general.\n\nOn Thu, Jan 29, 2015 at 10:17 AM, BinZhao notifications@github.com wrote:\n\nI think it's better to make trade off, since a monitoring service should\nnot cost much computing resources in cluster.\nAnd I've no idea why sending data to influxdb will cause so much CPU\nusage, I checked the network payload in influxdb's container and there is\nnot a lot of data incomes every 10 sec. I wonder maybe this issue is\nrelated to influxdb itself or its golang client?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/77#issuecomment-72075385\n.\n. @antmanler: We are working on separating heapster from InfluxDB and Grafana. Heapster by itself should use minimal CPU and we will place appropriate resource restrictions to enforce that.\n\nFor InfluxDB though, its performance becomes abysmal when we restrict it to less than 1 CPU. Since the current default cluster config in kubernetes uses 1 cpu GCE VMs, setting InfluxDBs CPU limit to '1' will consume an entire node. So I think the idea would be for users to place appropriate resource restrictions for InfluxDB. \n. Let me know if you need any help placing CPU and Memory restrictions on InfluxDB.\n. Closing this issue. \n. @rjnagal, @vmarmol: Kindly test this PR if possible before merging.\nYou can use deploy/kube.sh restart command to test this PR.\n. I have already rebased. Sorry failed to update the PR. Can you test it once\nbefore merging?\nOn Thu, Feb 5, 2015 at 9:36 AM, Victor Marmol notifications@github.com\nwrote:\n\nDoes this need to be rebased with the PR you sent a couple of days ago?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/79#issuecomment-73089870\n.\n. I need to fix the integration test to access influxdb via the proxy on api-server. \n. @vmarmol: This PR is ready to be merged as well. The integration test has been updated to work without firewall rules.\n. LGTM\n. LGTM. We will have to add some tests eventually since this will be production code.\n. LGTM. @vmarmol feel free to self merge once Travis is happy :)\n. @vmarmol This PR is finally ready to be merged. I apologize for posting such a big PR. I started small but eventually ended up doing a lot of work :(\n. Ping @vmarmol @rjnagal \n. LGTM. Merging this since this should not affect existing integration tests.\n. LGTM\n. Don't merge this PR until integration test on jenkins passes.\n. Ping @vmarmol @rjnagal. Integration tests are finally working. Time to get this merged. I want this in before other PRs.\n. @rjnagal @vmarmol Addressed comments. PTAL\n. @rjnagal: Jenkins is flaky when it comes to multiple commits per PR. Ignore the jenkins run for this PR and go ahead with the merge. I tested it manually.\n. This is done on HEAD.\n. This has been fixed on head. The sinks themselves do not run in parallel yet.\n. cc @a-robinson\n. Thanks for reporting the issue. \nI wonder if we can eliminate lastQuery completely and instead just use poll_duration?\nWe have just started working towards improving the core, and eventually scalability as well. I can imagine polling all cadvisor's in parallel at the least, in the short term. We are aiming for sharding heapster itself from ground up, but that might not happen soon.\n. This has been fixed.\n. @arkadijs: Why is group by hostname not possible? I thought it can be combined with a where container_name=<blah> clause in Grafana. \n\nWith diskio and filesystems I was thinking of separating them into separate series. Does that approach sound good?\n. Thanks for explaining! I see the problem now. We personally don't have\nexperience using InfluxDB in production and so feedback from users is much\nappreciated.\nI was thinking of going for a series per metric, but a series per container\nsounds to be better.\nI will try and work on this soon. Until then, feel free to post PRs to\nenhance the InfluxDB backend.\nOn Wed, Feb 11, 2015 at 12:03 AM, arkadijs notifications@github.com wrote:\n\nAn example.\nThere are two kinds of containers: user apps and infra. I want to show a\nstacked graph of infra containers accross the cluster. What do I do?\nselect container_name, derivative(cpu_cumulative_usage) as cpu_usage\n  from stats where container_name =~ '^infra-'\n  group by time(10s), container_name\nDoesn't work.\nderivative() if operating on non-related values that comes from different\nmachines.\nselect container_name, derivative(cpu_cumulative_usage) as cpu_usage\n  from stats where container_name =~ '^infra-'\n  group by time(10s), container_name, hostname\nIf Gafana would have a support for that - ie. grouping by more than 1\nnon-time column - which it doesn't suport with InfluxDB, that would give me\na line per machine.\nThus I need a two-stage process. A continuous query:\nselect container_name, derivative(cpu_cumulative_usage) as cpu_usage\n  from stats where container_name =~ '^infra-'\n  group by time(10s), container_name, hostname into cpu_stats_infra\nthen:\nselect container_name, sum(cpu_usage)\n  from cpu_stats_infra where $timeFilter\n  group by time($interval), container_name order asc\nin Grafana.\nSimilar pattern repeats over and over again, hitting the limitations of\nInfluxDB query language and Grafana.\nAnyway, graphing from stats is really slow. Testing is done on m3.large\nAWS instances, timeseries data fits in InfluxDB memory cache. InfluxDB\n0.8.8.\nWith diskio and filesystems I was thinking of separating them into\nseparate series. Does that approach sound good?\nYes. Please think about separating as much as possible into different\nseries. Maybe even a series table per container (name) / pod, eliminating a\nrepetitive scans over stats. I don't know if InfluxDB would be able to\ncleanup that effectively, which is probably could, but InfluxDB can join\nseries into single data set - select * from /.*/, thus providing\nperformance and/or aggregated data when required.\nIIRC, InfluxDB 0.9 will be better by storing columns as integers to refer\nto unique values, but that won't solve the performance problem in general.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/98#issuecomment-73845242\n.\n. @arkadijs I am curious to know the deployment issues with using the buddy container? \nIn general, we intend to move towards a side-car based model for discovery, sharding and storage instead of a single monolithic binary.\n. On Wed, Feb 11, 2015 at 12:26 AM, arkadijs notifications@github.com wrote:\n\n+1 container to deploy and explain to newcomer. I prefer to code a\n   solution that just works. Please avoid microservices envy.\n\nAgreed. Although cluster management tools like Kubernetes or (maybe) Fig,\nshould solve this problem.\n1. Exchanging live data via JSON formatted data on the filesystem?\n   Really?\nThe data being exchanged was expected to change very in-frequently and in\nsome cases not change at all.\n1. The buddy is overwriting the hosts file in-place, thus a race\n   condition with the reader (heapster). Use rename().\nValid point. Needs to be fixed.\n1. There is code in Heapster that queries Kubernetes nodes API.\n   following the logic it should be removed and placed into separate\n   container.\n   https://github.com/GoogleCloudPlatform/heapster/blob/master/sources/kube.go#L110\nThat is the intent in the long run.\nWe want to limit the number of platforms that we support natively. Why?\nEvery new platform being added needs to be tested and maintained as part of\nthe core binary. Separating out node discovery helps to shard and scale\nheapster easily. But limiting to CoreOS in addition to Kubernetes is fine.\n. No. Restarted the build.\n. @vmarmol: Addressed comments and fixed some more bugs. PTAL\n. @vmarmol: made changes as per your comment. Will merge once the build is green.\n. Just a few comments. Overall LGTM.\n. LGTM. @vmarmol can you rebase this PR?\n. Yayy!! Now its time to add an integration test. \n. LGTM. Jenkins failure looks like etcd issue. I have restarted the build. Will merge once the build passes.\n. A subsequent run of CI passed. But jenkins failed to update github - http://104.154.52.74/job/heapster-e2e-gce/398/\n\nMerging this PR. \n. I will take a look at this issue today.\n. I fixed some heapster side bugs that should address this issue. There is still the dynamic housekeeping issue in Cadvisor. Now that cadvisor exposes a time range for stats, I will update heapster to use that time range.\n. PTAL @vmarmol \n. Fixed the build issue. Waiting on e2e to see if there are any additional issues.\n. I have spent a lot of time now attempting to restrict godep changes to just the kubernetes package. I hope it works this time around. \n. Ping @vmarmol. This can be merged now.\n. LGTM.\n. Thanks for the PR @mbforbes :) . LGTM!\n. LGTM.\n. Rebased to HEAD.\n. @vmarmol @rjnagal: I want to include this in the v0.7 release. I tested manually and it works. I have updated the readme and release notes.\n. @saad-ali is working on this. \n. LGTM. \n. Thanks for the PR @vmarmol. Merging.\n. LGTM\n. Thanks of reporting the issue @keyz182. We will fix it in the next release.\n. This has been fixed in the recent v0.9 release.\n. Are you able to manually get stats using the URL mentioned in the logs?\nwget\nhttp://10.240.173.100:10250/stats/default/kube-dns-0133o/70de926d-bb1a-11e4-b34e-42010af06d77/etcd.\nI have seen such errors whenever cadvisor is not available.\nOn Mon, Feb 23, 2015 at 11:23 AM, Alex Robinson notifications@github.com\nwrote:\n\nPotentially related to the push of heapster 0.7. I've been seeing all\nheapster requests to my kubelets' /stats/ endpoints failing. I initially\nreported this as a Kubelet issue\nhttps://github.com/GoogleCloudPlatform/kubernetes/issues/4717, but it\nlooks possible that heapster is the one doing something wrong.\nThe errors all look like this, and I expect it's very reproducible, as\nboth clusters I've created since Friday have had these errors:\nE0223 18:32:13.471559       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.173.100:10250/stats/default/kube-dns-0133o/70de926d-bb1a-11e4-b34e-42010af06d77/etcd - Got 'Internal Error: failed to retrieve cadvisor stats\n': invalid character 'I' looking for beginning of value\nE0223 18:32:33.378167       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.147.240:10250/stats/ - Got 'Internal Error: received empty response from \"container info for \\\"/\\\"\"\n': invalid character 'I' looking for beginning of value\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/127.\n. The kubelet also restarts often, so you might want to check kubelet health\nas well.\n\nOn Mon, Feb 23, 2015 at 11:34 AM, Alex Robinson notifications@github.com\nwrote:\n\nYeah, could definitely be due to cadvisor being missing.\nroot@kubernetes-minion-8hyo:/var/log# wget http://10.240.173.100:10250/stats/default/kube-dns-0133o/70de926d-bb1a-11e4-b34e-42010af06d77/etcd\n--2015-02-23 http://10.240.173.100:10250/stats/default/kube-dns-0133o/70de926d-bb1a-11e4-b34e-42010af06d77/etcd--2015-02-23 19:33:23--  http://10.240.173.100:10250/stats/default/kube-dns-0133o/70de926d-bb1a-11e4-b34e-42010af06d77/etcd\nConnecting to 10.240.173.100:10250... connected.\nHTTP request sent, awaiting response... 500 Internal Server Error\n2015-02-23 19:33:23 ERROR 500: Internal Server Error.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/127#issuecomment-75614639\n.\n. The '10 seconds' matches heapster's default polling duration. Can you look\nfor cadvisor container restarts? Also can you post the entire heapster logs?\n\nOn Mon, Feb 23, 2015 at 11:36 AM, Alex Robinson notifications@github.com\nwrote:\n\nIt's happening every 10 seconds on every node, with no signs of kubelet\nrestarts.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/127#issuecomment-75615058\n.\n. This must have been fixed on head by #4784. Can you recreate a cluster from HEAD and see if the issue is fixed?\n. The heapster POST issue has been fixed on heapster HEAD. I will integrate\nthat into kube soon.\n\nCan we stop dumping stack traces in kubelet? Does it really help debug any\nissues?\nOn Thu, Feb 26, 2015 at 12:13 PM, Alex Robinson notifications@github.com\nwrote:\n\nSorry for the delay - I'd love to test this, but the wi-fi here isn't good\nenough for kube-up to work for me :/ I probably won't be able to until\ntomorrow\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/127#issuecomment-76260759\n.\n. deploy/kube.sh does not rebuild heapster docker images. I have pushed a\ntest image 'kubernetes/heapster:test'. Can you try again using that image?\n\nOn Thu, Feb 26, 2015 at 1:18 PM, Shawn Falkner-Horine \nnotifications@github.com wrote:\n\nFor what it's worth, I'm currently seeing the same issue, using the latest\nheapster github code.\nJust to make sure, here's the latest pull and restart that I performed:\n20:58 kube_devstack highland@kube-master1:~/heapster$ git pull origin master\nremote: Counting objects: 6, done.\nremote: Compressing objects: 100% (6/6), done.\nremote: Total 6 (delta 1), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (6/6), done.\nFrom https://github.com/GoogleCloudPlatform/heapster\n- branch            master     -> FETCH_HEAD\n  Updating d5098a9..ec1c526\n  Fast-forward\n  sources/cadvisor_test.go |    8 ++++++--\n  1 file changed, 6 insertions(+), 2 deletions(-)\n21:06 kube_devstack highland@kube-master1:~/heapster$ deploy/kube.sh restart\nheapster pods have been removed.\nheapster pods have been setup\nI see that the pods were indeed recreated, however I'm still seeing the\nPOST to /stats/ every 10 seconds:\n[...]\n/logs/kube-minion3/INFO-174305.28825:I0226 21:08:50.271049   28825 server.go:362] POST /stats/: (323.296\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825:I0226 21:09:00.278409   28825 server.go:362] POST /stats/: (411.688\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825:I0226 21:09:10.271291   28825 server.go:362] POST /stats/: (521.926\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825:I0226 21:09:20.272693   28825 server.go:362] POST /stats/: (505.051\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825:I0226 21:09:30.265936   28825 server.go:362] POST /stats/default/monitoring-heapster-controller-2gdny/6b8cfbc5-bdfb-11e4-848e-00505629c58f/heapster: (2.417704ms) 500\n/logs/kube-minion3/INFO-174305.28825:I0226 21:09:30.267541   28825 server.go:362] POST /stats/default/monitoring-heapster-controller-2gdny/6b8cfbc5-bdfb-11e4-848e-00505629c58f/heapster: (3.741048ms) 500\n/logs/kube-minion3/INFO-174305.28825:I0226 21:09:30.268122   28825 server.go:362] POST /stats/: (170.12\u00b5s) 500\nAnd from the kubectl logs:\n21:16 kube_devstack highland@kube-minion1:~$ kubectl.sh log monitoring-heapster-controller-2gdny | tail\n2015-02-26T21:17:20.268190019Z E0226 21:17:20.268150       5 kube.go:158] failed to get stats for container \"influxdb\"/\"default\" in pod \"monitoring-influx-grafana-controller-xxvi7\"\n2015-02-26T21:17:20.272222969Z E0226 21:17:20.271986       5 kubelet.go:80] failed to get stats from kubelet url: http://10.0.0.4:10250/stats/default/monitoring-influx-grafana-controller-xxvi7/6bfa5435-bdfb-11e4-848e-00505629c58f/influxdb - Got 'Internal Error: received empty response from \"Docker container info for \\\"35c432aef492333df08db6c44eb42c4977392c4bdef6c0b33182730a82480a89\\\"\"\n2015-02-26T21:17:20.272222969Z ': invalid character 'I' looking for beginning of value\n2015-02-26T21:17:20.272222969Z E0226 21:17:20.271998       5 kube.go:158] failed to get stats for container \"influxdb\"/\"default\" in pod \"monitoring-influx-grafana-controller-xxvi7\"\n2015-02-26T21:17:20.273595492Z E0226 21:17:20.272685       5 kubelet.go:80] failed to get stats from kubelet url: http://10.0.0.7:10250/stats/ - Got 'Internal Error: received empty response from \"container info for \\\"/\\\"\"\n2015-02-26T21:17:20.273595492Z ': invalid character 'I' looking for beginning of value\n2015-02-26T21:17:20.273626242Z E0226 21:17:20.273016       5 kubelet.go:80] failed to get stats from kubelet url: http://10.0.0.4:10250/stats/ - Got 'Internal Error: received empty response from \"container info for \\\"/\\\"\"\n2015-02-26T21:17:20.273626242Z ': invalid character 'I' looking for beginning of value\n2015-02-26T21:17:20.273626242Z E0226 21:17:20.273319       5 kubelet.go:80] failed to get stats from kubelet url: http://10.0.0.3:10250/stats/ - Got 'Internal Error: received empty response from \"container info for \\\"/\\\"\"\n2015-02-26T21:17:20.273626242Z ': invalid character 'I' looking for beginning of value\nIs there more that I could check? (Still finding my way around the logs.)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/127#issuecomment-76275534\n.\n. There was an issue in kubelet which affected access to cadvisor container.\nThat got fixed very recently (yesterday maybe). Running a cluster from head\nmight solve that problem.\n\nOn Thu, Feb 26, 2015 at 2:57 PM, Shawn Falkner-Horine \nnotifications@github.com wrote:\n\nAh! Thanks, I didn't realize where the image was specified -- I see what\nyou mean, now. Even better, I tried your test image, and I do see the POST\nactions change to GET:\n[...]\n/logs/kube-minion3/INFO-174305.28825 22:51:02.629496   16718 server.go:362] POST /stats/: (172.947\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825 22:51:12.624493   16718 server.go:362] POST /stats/: (433.335\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825 22:51:22.623663   16718 server.go:362] POST /stats/default/monitoring-influx-grafana-controller-182tv/db5fa8f2-be03-11e4-848e-00505629c58f/influxdb: (8.611277ms) 500\n/logs/kube-minion3/INFO-174305.28825 22:51:22.627992   16718 server.go:362] POST /stats/default/monitoring-influx-grafana-controller-182tv/db5fa8f2-be03-11e4-848e-00505629c58f/influxdb: (12.700853ms) 500\n/logs/kube-minion3/INFO-174305.28825 22:51:22.628858   16718 server.go:362] POST /stats/: (237.14\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825 22:51:32.615554   16718 server.go:362] POST /stats/: (449.188\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825 22:52:19.700102   16718 server.go:362] GET /stats/default/monitoring-heapster-controller-zjpez/06983ddc-be0a-11e4-848e-00505629c58f/heapster: (6.570953ms) 500\n/logs/kube-minion3/INFO-174305.28825 22:52:19.707005   16718 server.go:362] GET /stats/default/monitoring-heapster-controller-zjpez/06983ddc-be0a-11e4-848e-00505629c58f/heapster: (13.167025ms) 500\n/logs/kube-minion3/INFO-174305.28825 22:52:19.708450   16718 server.go:362] GET /stats/: (696.24\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825 22:52:29.702688   16718 server.go:362] GET /stats/default/monitoring-influx-grafana-controller-i4hx9/06e12bd6-be0a-11e4-848e-00505629c58f/influxdb: (9.018229ms) 500\n/logs/kube-minion3/INFO-174305.28825 22:52:29.709717   16718 server.go:362] GET /stats/default/monitoring-influx-grafana-controller-i4hx9/06e12bd6-be0a-11e4-848e-00505629c58f/influxdb: (16.410284ms) 500\n/logs/kube-minion3/INFO-174305.28825 22:52:29.711006   16718 server.go:362] GET /stats/: (340.775\u00b5s) 500\nI'm pretty sure now that my other issue is cAdvisor, since I'm still\nseeing the \"invalid character 'I' looking for beginning of value\" errors,\nbut taking this GET/POST aspect off the table is definitely helpful.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/127#issuecomment-76293563\n.\n. What version of kubernetes are you running? I am guessing v0.15.0? If so,\ncan you try using the configs from here?\n\nOn Sat, Apr 18, 2015 at 12:55 PM, Ido Shamun notifications@github.com\nwrote:\n\nI keep getting:\nE0418 16:46:17.721174       7 kubelet.go:85] failed to get stats from kubelet url: http://172.18.0.13:10250/stats/default/monitoring-influx-grafana-controller-gkukt/6889f02a-e5e8-11e4-b87a-000d3a30177a/grafana - Get http://172.18.0.13:10250/stats/default/monitoring-influx-grafana-controller-gkukt/6889f02a-e5e8-11e4-b87a-000d3a30177a/grafana: malformed HTTP response \"\\x15\\x03\\x01\\x00\\x02\\x02\"\nWhen I wget this link it downloads a binary file.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/127#issuecomment-94180860\n.\n. Can you describe how you think this metric can be used? When a container dies, heapster will stop exporting all metrics including 'uptime'. \n. LGTM. This will be useful until we have events collection and processing implemented.\n. Restarted the integration tests.\n. @vmarmol: Tests passed.\n. Excepting local disk none of the existing Storage solutions in kubernetes works for all the users. I am thinking the best way to address this issue is to provide a cookbook of storage recipes in the repo and make the users choose one based on their deployment. \nWe can simplify this by having a deployment binary with simple flags.\nAny thoughts @pires ?\n. It could also be extended to add and remove pluggable backends.\nFor example:\nDeploy InfluxDB + Grafana backend if required or setup a GCM backend, etc.\n\nOn Thu, Feb 26, 2015 at 10:25 AM, Rohit Jnagal notifications@github.com\nwrote:\n\n+1 there are too many steps in current monitoring setup for user/admin to\nrun through themselves.\nThe tool should also support cleanly shutting down monitoring for people\nwho don't use it. The subcommands I'd like to see:\n- start monitoring\n- stop monitoring\n- validate (get validate output from heapster and cadvisors, also\n  healthchecks sinks)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/133#issuecomment-76235217\n.\n. ok to test\n. Thanks for the PR @jonlangemak! Can you squash your commits into one?\n. LGTM. Thanks for the fix @jonlangemak!\n. +1 to making this change. As long as it works on most kubernetes cluster\nsetups, I am fine with that change.\n\nOn Thu, Feb 26, 2015 at 6:19 AM, Paulo Pires notifications@github.com\nwrote:\n\nSo, having something like\nyaml\n- name: \"INFLUXDB_HOST\"\n  value: '\"+window.location.hostname+\"/api/v1beta1/proxy/services/monitoring-influxdb'\nis very limitative when it comes to running\ninfluxdb\n-  grafana , especially inside Kubernetes.\nSee my suggested solution\nhttps://github.com/GoogleCloudPlatform/kubernetes/issues/4841.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/135.\n. +1 to what @rjnagal mentioned. It should be straight forward to add a new 'nodes' source. We can help you out if necessary. \n. @samek: Were you able to make any progress?\n. Heapster will have to discover the nodes that are part of a mesos cluster.\n\nOn Mon, Apr 6, 2015 at 8:10 AM, samek notifications@github.com wrote:\n\nYes and no\nWe've change the source of cadvisor so that it sends enviroment vars to\ninfluxdb.\nEnv vars are what you need.\nSent from my iPhone\n\nOn 21 Mar 2015, at 19:44, Vish Kannan notifications@github.com wrote:\n@samek: Were you able to make any progress?\n\u2014\nReply to this email directly or view it on GitHub.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/136#issuecomment-90096513\n.\n. That's correct @MikeMichel \n. @exnerd: You are missing [] for items.\n Here is an example:\n\nstring\n{\"items\":[ {\"name\":\"host1\",\"ip\":\"1.2.3.4\"},{\"name\":\"host2\",\"ip\":\"1.2.3.5\"} ] }\nHere is some test code that generates a list of external hosts on the fly.\nLet me know if you have any issues. Ideally, having a native node discovery agent in heapster for mesos might be better for a production setup. \nWe are happy to provide any help necessary to get that in. You will have to implement a new NodesApi for mesos and examples are under sources/node/kube.go and source/node/coreos.go.\n. Here is what I tried.\nshell\n$ cat /var/run/heapster/hosts \n{\n\"items\": [ {\"name\":\"host1\",\"ip\":\"10.240.214.82\"},{\"name\":\"host2\",\"ip\":\"10.240.163.208\"} ]\n}\n$ ./heapster --source=\"cadvisor:external?cadvisorPort=4194\"\nLet me know if this works for you. \nAs I mentioned earlier, it might be much easier to add a node plugin that can programmatically list all mesos nodes.\n. @naxhh: If DNS were to be enabled, we can try to resolve the hostname to an IP. If the convention with mesos is to use the node IP as the hostname, then we can re-use the hostname as IP. But I am not sure if it will work everywhere.\n+1 for integrating with zookeeper if that is the most convenient way to move forward. \n. publicIP and internalIP can both be set to the same IP. They are\ndifferent in public clouds which have a dedicated internal network. You can\nignore the cpu and memory limits for now.\nIs zookeeper used in most mesos clusters? If so, going with zookeeper makes\nsense. If not, let's use the Mesos APIs since it should work on all mesos\ndeployments.\nOn Tue, Sep 15, 2015 at 6:39 AM, Ignacio Tolstoy notifications@github.com\nwrote:\n\nI've checked the setup at work. And hostname is not always an ip.\nResolving the hostname to ip's is an option. I just wanted to make it as\nsimple as possible.\nChecking information from Zk.\nslave01:\n20150903-134310-1560936458-5050-16793\ufffd\ufffd\ufffd\ufffd\ufffd'\"master@10.0.10.1:5050*hostname0120.23.0\nslave02:\n$20150903-134316-4127195146-5050-1887\ufffd\ufffd\ufffd\ufffd\ufffd'\"master@10.0.0.2:5050*hostname0220.23.0\nThis is the information from /mesos/info_0000000001,\n/mesos/info_0000000002 This id's are auto-incremented so if you delete 01\nand add a new slave01 you'll have 02.\nThe information I've found there is pretty much the same I can found in\nthe API (state.json)\nMaybe doing it from ZK is more reliable than doing it from the API? In the\nAPI we could resolve hostnames or parse pid (pid: \"slave(1)@10.0.0.1:5051)\nFrom ZK, I should learn how unserialize that data and parse the master@ip\n:port thing....\nFrom my POV, resolving hostname to IP's is the simplest way. I don't know\nif requiring DNS will be a big problem for some users.\nFinal question: nodes.Info has a few fields. What is really required from\nthere? Public and private ip? or just private ip?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/136#issuecomment-140395941\n.\n. Most likely a refactoring bug.\n\nOn Thu, Feb 26, 2015 at 10:59 AM, Rohit Jnagal notifications@github.com\nwrote:\n\nLGTM\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/137#issuecomment-76243830\n.\n. The failure is a flake. The test passed this time around. PTAL @rjnagal \n. Done. Sent #139\n\nOn Thu, Feb 26, 2015 at 11:21 AM, Rohit Jnagal notifications@github.com\nwrote:\n\nThat's a weird flake. Might be a bad test. We should look it up.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/137#issuecomment-76247924\n.\n. ok to test\n. LGTM. Will merge once integration tests pass. Thanks for the PR @sub-mod!!!!\n. Fixed that issue and restarted the integration tests. Sorry about the delay\n@sub-mod!\n\nOn Thu, Feb 26, 2015 at 12:06 PM, sub-mod notifications@github.com wrote:\n\nwrite error: No space left on device\ngodep: go exit status 2\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/138#issuecomment-76259056\n.\n. @sub-mod: You have to rebase to HEAD\n. LGTM.\n. This is a test only change. I think we can ignore the integration tests.\n. Yeah. That makes sense. Done.\n. @vmarmol: Tests are passing!\n. ok to test\n. @pires: This PR needs a rebase.\n. @pires: I am not able to access the Grafana bashboard via the proxy on the master - https://<masterIP>/api/v1beta1/proxy/services/monitoring-grafana/.\nDoes this work for you? \n. @agios: Happy to know that the basic PR works for you. The reason for not merging this PR yet is due to the fact that setting up a client facing service in kubernetes is neither simple nor cross-platform compatible. I intend to pick this up and get Grafana to work with both a public ip and via the proxy.\nIf you have some time feel free to pick this up.\n. Closing this PR in favor of #183. The expected functionality is already on HEAD. The next release of the grafana image will have all the changes.\n. Sadly the fix was wrong :(. The test failed in reality!\n\nOn Fri, Feb 27, 2015 at 11:14 AM, Victor Marmol notifications@github.com\nwrote:\n\nMerged #143 https://github.com/GoogleCloudPlatform/heapster/pull/143.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/143#event-243487287\n.\n. That is what I am trying to fix through this PR. It is incorrectly\nreporting green. Nvm. It is still broken. Nothing has changed.\n\nOn Fri, Feb 27, 2015 at 11:17 AM, Victor Marmol notifications@github.com\nwrote:\n\n:( why did Jenkins report green?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/143#issuecomment-76454062\n.\n. This is completed on HEAD.\n. Merging this in-order to fix e2e tests.\n. LGTM. Thanks for the PR @vmarmol. This is super helpful!\n. This PR should hopefully make the tests more robust.\n. I think its time to rewrite run.sh in golang. May be after this PR.\n. LGTM except for the one nit. If we are moving to a dynamic configuration model, we can get rid of this logic completely.\n. @vmarmol: Feel free to self merge after fixing the nit.\n. LGTM! Let me know if you need help fixing the e2e test machine.\n. Do we have any docs for running in standalone mode?\n. LGTM\n. @vmarmol: This PR should get rid of the disk full issue.\n. @vmarmol @rjnagal. Take a look at this PR when you get a chance. This should make adding new sinks very easy! I can split up the PR into three parts - API, GCM transition, Influxdb + tests if required.\n. Ping @vmarmol @rjnagal \n. Required for #154.\n. PTAL @vmarmol \n. Removed the interfaces. \n. Not sure why thats happening. I added gofuzz in a previous PR and it is\nthere on HEAD.\n\nOn Thu, Mar 5, 2015 at 9:40 AM, Victor Marmol notifications@github.com\nwrote:\n\nLooks like we're missing the gofuzz dep?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/156#issuecomment-77410975\n.\n. Ping @vmarmol @rjnagal \n. @vmarmol: How can I test GCM?\n. Thanks for the review @vmarmol. I ended up combining to reduce testing time. Apologies :)\n. The test failure is real. Fixed the issue.\n. I am not sure why jenkins did not update the PR, but the test did pass: http://104.154.52.74/job/heapster-e2e-gce/573/\n. Heapster should work on CoreOS clusters.\n\nCan you post the output from '/validate' endpoint on Heapster?\nCan you pass in \"-e DEBUG=true\" while starting heapster to make it spew\nmore logs?\nOn Wed, Mar 11, 2015 at 6:27 AM, okapusta notifications@github.com wrote:\n\nI have a Deis http://deis.io cluster set up on AWS and I've tried using\nheapster to monitor it, but it pushes data into influx db only for a while\nafter start / restart and then stops http://cl.ly/image/0m0H3U2P1U1X. I\nset up everything as explained in this\nhttps://github.com/GoogleCloudPlatform/heapster/tree/master/clusters/coreos\nguide. Here are my configs. Am I doing something wrong or heapster simply\nwont work with deis?\ncore@ip-10-21-1-34 ~/monitoring $ cat cadvisor.service\n[Unit]\nDescription=cAdvisor Service\nAfter=docker.socket\nRequires=docker.socket\n[Service]\nTimeoutStartSec=10m\nRestart=always\nExecStartPre=-/usr/bin/docker kill cadvisor\nExecStartPre=-/usr/bin/docker rm -f cadvisor\nExecStartPre=/usr/bin/docker pull google/cadvisor\nExecStart=/usr/bin/docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=4194:4194 --publish=8080:8080 --name=cadvisor --net=host google/cadvisor:latest --logtostderr\nExecStop=/usr/bin/docker stop -t 2 cadvisor\n[X-Fleet]\nGlobal=true\ncore@ip-10-21-1-34 ~/monitoring $ cat influxdb.service\n[Unit]\nDescription=InfluxDB Service\nAfter=docker.socket\nRequires=docker.socket\n[Service]\nTimeoutStartSec=10m\nRestart=always\nExecStartPre=-/usr/bin/docker kill influxdb\nExecStartPre=-/usr/bin/docker rm -f influxdb\nExecStartPre=/usr/bin/docker pull kubernetes/heapster_influxdb\nExecStart=/usr/bin/docker run --name influxdb -p 8083:8083 -p 8086:8086 -p 8090:8090 -p 8099:8099 kubernetes/heapster_influxdb\nExecStop=/usr/bin/docker stop -t 2 influxdb\n[X-Fleet]\nMachineID=60023ec21a6643ebb0a71a54454d1bcc\ncore@ip-10-21-1-34 ~/monitoring $ cat heapster.service\n[Unit]\nDescription=Heapster Service\nAfter=docker.socket\nRequires=docker.socket\n[Service]\nExecStartPre=/bin/sh -c \"docker history kubernetes/heapster:latest >/dev/null || docker pull kubernetes/heapster:latest\"\nExecStart=/usr/bin/docker run -d -e INFLUXDB_HOST=10.21._._:8086 -e COREOS=true --name heapster --net=host --restart=on-failure:5 --publish=8082:8082 kubernetes/heapster:latest\n[X-Fleet]\nMachineOf=influxdb.service\ncore@ip-10-21-1-34 ~/monitoring $ cat grafana.service\n[Unit]\nDescription=Grafana service\nRequires=docker.socket\nAfter=docker.socket\n[Service]\nExecStartPre=/bin/sh -c \"docker history kubernetes/heapster_grafana:latest >/dev/null || docker pull kubernetes/heapster_grafana:latest\"\nExecStart=/usr/bin/docker run -d -p 8081:80 -e INFLUXDB_HOST=52.11._._ -e INFLUXDB_NAME=k8s -e HTTP_USER=admin -e HTTP_PASS=** kubernetes/heapster_grafana:v0.4\n[X-Fleet]\nMachineOf=heapster.service\nHeapster logs:\ncore@ip-10-21-1-34 ~/monitoring $ docker logs -f heapster\n- EXTRA_ARGS=\n- '[' '!' -z true ']'\n- EXTRA_ARGS=' --coreos'\n- '[' '!' -z ']'\n- '[' '!' -x ']'\n- HEAPSTER='/usr/bin/heapster  --coreos '\n- '[' '!' -z ']'\n- '[' '!' -z 10.21._.:8086 ']'\n- /usr/bin/heapster --coreos --sink influxdb --sink_influxdb_host 10.21._.:8086\n  I0311 10:28:25.571350       7 heapster.go:44] /usr/bin/heapster --coreos --sink influxdb --sink_influxdb_host 10.21.1.34:8086\n  I0311 10:28:25.571446       7 heapster.go:45] Heapster version 0.8\n  I0311 10:28:25.571555       7 heapster.go:46] Flags: alsologtostderr='false' bq_account='' bq_credentials_file='' bq_id='' bq_project_id='' bq_secret='notasecret' cadvisor_port='8080' coreos='true' external_hosts_file='/var/run/heapster/hosts' fleet_endpoints='http://127.0.0.1:4001' kubelet_port='10250' kubernetes_insecure='true' kubernetes_master='' listen_ip='' log_backtrace_at=':0' log_dir='' logtostderr='true' max_procs='0' poll_duration='10s' port='8082' sink='influxdb' sink_influxdb_buffer_duration='10s' sink_influxdb_host='10.21.1.34:8086' sink_influxdb_name='k8s' sink_influxdb_password='root' sink_influxdb_username='root' sink_memory_ttl='1h0m0s' stderrthreshold='2' v='0' vmodule=''\n  I0311 10:28:25.571585       7 influxdb.go:255] Using influxdb on host \"10.21.1.34:8086\" with database \"k8s\"\n  I0311 10:28:25.573334       7 heapster.go:57] Starting heapster on port 8082\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/160.\n. @okapusta: Can you let heapster run for 30 minutes and then post the logs? Heapster polls the cadvisors every 10s by default. \n. By stopped do you mean that heapster container stopped or you don't see any\nlogs from heapster?\n\nOn Wed, Mar 11, 2015 at 10:30 AM, okapusta notifications@github.com wrote:\n\nHeapster flushed data to influxdb sink one more time at 16:25:58 and then\nstopped\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/160#issuecomment-78313014\n.\n. I am trying to reproduce this issue locally. Will update the issue once I\nmake some progress.\n\nOn Wed, Mar 11, 2015 at 10:35 AM, okapusta notifications@github.com wrote:\n\nContainer is running, it just stopped querying (I guess) cadvisor\ncontainers and pushing data to InfluxDb\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/160#issuecomment-78313852\n.\n. @etcinit: Thanks for the update!\n. Can you post heapster logs? You can enable detailed logging by adding the\nfollowing flag - --vmodule=*=4.\n\nOn Wed, May 20, 2015 at 6:27 AM, biwwy notifications@github.com wrote:\n\nI have the same issue as described above on CoreOS cluster, using\nheapster:latest.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/160#issuecomment-103888589\n.\n. @rjnagal: I accumulated all these patches while trying to make a new heapster release. The test failure is due to file re-org. Updated the test now.\n. @rjnagal: Fixed the integration tests. This PR is ready for review. \n. The actual patch has the reason. \n. and updated the PR description @vmarmol \n. Ping @vmarmol @rjnagal \n. LGTM. Thank a lot for the fix @sub-mod. I completely overlooked the v1beta3 configs. Can't wait for v1beta3 to be the default :)\n. @vmarmol: The integration test under integration/kube_test.go needs to be updated with the new docker file location. \n. Error:      No error is expected but got failed to build docker binary (\"exit status 1\") - \"time=\\\"2015-03-19T17:12:55Z\\\" level=\\\"fatal\\\" msg=\\\"no Dockerfile found in .\\\" \\n\"\n. Thanks for the cleanup :) \nLGTM. Will merge once the integration test passes.\n. Lol. Thanks for the PR!\n. ok to test\n. Thanks for the fix @simon3z! \n. cc @vmarmol @rjnagal \n. Correction: Heapster fails to collect only for the pod and not entirely. Still undesirable.\n. Good idea. This seems better than having heapster expose the data directly.\n. The URL part is optional. For the kafka case, you can take in a list of IPs\nas a query param.\nAnother option is to use a DNS name and resolve the name to all its IPs.\n\nOn Mon, Sep 21, 2015 at 10:03 PM, Hewei Liu notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh I have a question here about the\n\"--sink\" flag:\nThe kafka-client usually need be configured multiple tcp endpoints for a\nbroker cluster, typically:\nvar kafkaAddrs = []string{\"192.168.1.2:9092\", \"192.168.1.2:9093\"}\nIt's different with other existing sinks such as influxdb/hawkular which\nusually has only one http_url as the endpoint.\nAnd I checked the sink extension's interface\nhttps://github.com/kubernetes/heapster/blob/masterextpoints/interfaces.go#L32,\nthen found it supports only one and http-only endpoint (_url.URL):\ntype SinkFactory func(_url.URL, HeapsterConf) ([]sinksApi.ExternalSink,\nerror)\nIt's a problem now, and what's your suggestion? Thanks.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/172#issuecomment-142182283\n.\n. Awesome :)\nDo post a WIP. We can provide some feedback, if applicable, before you\nspend a lot of much time on the PR.\n\nOn Thu, Sep 24, 2015 at 9:20 PM, yuqi huang notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh , I am developing the kafka sink. Shall\nI commit a wip PR to accept some guys' comments? Thanks : )\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/172#issuecomment-143120749\n.\n. Application metrics is something we have been wanting to collect in\nheapster for a while. But most of the existing monitoring solutions use\neither collectd or statsd and so we were looking at leveraging them for\napplication metrics. Native support in heapster sounds even better. My\nconcern is that of diverging from the open source community around\nmonitoring.\n\nOn Fri, Mar 20, 2015 at 3:21 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nCurrently Heapster collects metrics from cAdvisor to gather container\nlevel metrics. Collecting application level metrics would provide an even\nricher set of data. Applications would be required to implement an http\nendpoint to expose the metrics that it wants to publish.\nAs a PoC, we have a heapster fork, jadvisor (\nhttps://github.com/fabric8io/jadvisor), that collects JVM stats via JMX\nexposed via Jolokia (http://jolokia.org/) for http access to JMX.\nCurrently the stats that are collected are hard-coded, or rather discovered\nby what is available in the JMX tree. I would like to make this more\ngeneric so it can handle any http endpoint & hence be stack agnostic.\nThe difference that this would introduce to heapster is the need to\nconnect to individual pods rather than just to the kubelet/cadvisor so\nwould increase the number of connections made significantly. This would\nlikely require work on clustering & sharding to implement properly.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/173.\n. Application type discovery is an open issue in kubernetes. Filed an issue to discuss\nthat.\n\nFiled #177 for dynamic backend configuration.\n. I agree. Labels should get us going for now.\n. Support for peer to peer services is still being designed in kubernetes. Once that is finalized, adding support for clustering should be straight forward.\n. cc @piosz.\n. Ping @vmarmol. \n. Ping @vmarmol \n. LGTM. \n. Good point. No auth exists today. We can leverage cadvisor auth. In\nkubernetes, heapster will be accessible via the apiserver and we can rely\non the api-server auth policy for heapster.\nOn Fri, Mar 20, 2015 at 10:43 AM, Rohit Jnagal notifications@github.com\nwrote:\n\nDoes heapster handle auth today? State modifying APIs would require some\nkind of Auth setup option - maybe just like the basic one we have in\ncAdvisor.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/177#issuecomment-84085730\n.\n. I appreciate the quick weekend review @rjnagal :)\n\nOn Sat, Mar 21, 2015 at 1:35 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nMerged #179 https://github.com/GoogleCloudPlatform/heapster/pull/179.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/179#event-261317227\n.\n. ok to test\n. @agios Thanks for the PR! LGTM. Will merge once the tests pass.\n. Ping @vmarmol @rjnagal. This PR is now ready for review. \n. The cadvisor changes required for this PR are not part of any kube release. So we will have to hold off merging this PR until then.\n. We still need a release with all the kube changes.\n\nOn Wed, Mar 25, 2015 at 6:00 PM, Victor Marmol notifications@github.com\nwrote:\n\nAll the Kubernetes changes are in. Is this ready?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/181#issuecomment-86271190\n.\n. @vmarmol: As discussed offline, this PR is ready to be merged. Once a new kube release is cut, I will update the current code to not request 60 stats as max.\n. Ping @vmarmol \n. @vmarmol: Considering that the current hack works with pre 0.14 releases, we can merge this PR.\n. @vmarmol: Tests are passing now.\n. @simon3z: Thanks for this PR. The shell scripts are painful to maintain. If I get some time I would like to rewrite all of them in go. That said, any changes to make the scripts easier to manage are welcome :+1: \n. @jimmidyson: The reason for switching to the proxy is to provide an off-the-box experience with Kubernetes. As you stated, it is clearly not a production solution. #142 was an attempt to address that. It would be great if you can get the reverse proxy setup to work along with the apiserver proxy access pattern.\nMonitoring is being considered as a core Kubernetes service and that is the reason for re-using master auth. But since the current state of auth in the master is very primitive, it makes sense to not rely on that.\nYour go based setup looks much simpler that the nginx setup. I have no issues dropping the nginx setup in favor of your go based implementation.\n. @simon3z: The reason for having a separate copy in kubernetes was for bundling reasons. A specific version of kubernetes will always have a monitoring setup that works for that version and is tested continuously using e2e tests.\nI can add some documentation in kube repo to request users to post their updates here and then periodically sync those changes to the kube repo. WDYT?\n. @jimmidyson: The apiserver proxy is being used mainly for external access to Grafana. By reverse proxy are you referring to the intra-cluster link between influxdb and grafana?\n. @simon3z: Should we close this PR in favor of #187 ?\n. @pires: I think there has been some miscommunication. My only issue with #142 was that, it made it not possible to access Grafana from outside the cluster using the apiserver proxy. The RP changes are needed for sure. I believe I had the very same discussion with @jimmidyson and we agreed to not break that.\n. @jimmidyson: Overall it looks good. I will not merge this PR until you are done with your re-factoring.\nThanks for the PR!\n. @jimmidyson: I tried testing your PR and it does not work for me. My request is to have the default config work via the apiserver proxy, that way it works for everyone. Users can then make some changes to either use a public IP for the grafana service or create an ELB in place of the proxy. Specifically, if the default setup can be accessible via <masterip>/api/v1beta1/proxy/services/monitoring-grafana/, that will be awesome. If that is not possible, I am open to discussing how to provide an alternative off-the-box experience.\n\nThanks for working on this PR!\n. @jimmidyson: I tried testing this PR by adding a hostPort to Grafana container. I get a connection reset error curl: (56) Recv failure: Connection reset by peer. \nI agree with you on the fact that the proxy is not an ideal solution. I am open to suggestions on making Grafana accessible from outside the cluster without using the apiserver proxy.\n. @jimmidyson: I din't notice that the server is bound to '8080' instead of port '80'. Trying again.\n@pires: Agreed. PublicIP is definitely the best option. But it requires manual configuration. The only reason for switching over the proxy was to give a simple ux to users. Power users like yourself, know to switch over to Public IPs or ELBs. \n@jimmidyson: My hope was for the default setup to work via the proxy and have alternative (better) deployment configs, with documentation, that will use ELBs or Public IPs. Is that too much to ask for or is it difficult to achieve with your new model?\n. @jimmidyson: Ack. It works perfectly without the proxy. I am trying to debug why access via the proxy fails.\n. I figured out the issue with the proxy. I am merging this PR as-is. I will fix the proxy stuff in a subsequent PR.\nThanks for the contributions @jimmidyson and thanks for your input @pires. \n. ok to test\n. LGTM. Thanks for the cleanup @simon3z. Will merge once the tests pass.\n@simon3z: Lets discuss about apache vs native go server in #183.\n. @simon3z: #183 acheives the same goals as this PR. It includes the reverse proxy changes as well. Can you take a look at that and see if that works for you?\n. @simon3z: I merged #183 yesterday which adds a RP to access Influxdb. Can you see if that works for you? \n. LGTM. Will merge once tests pass.\n. Build is failing on Travis.\n. LGTM. Adding a unit/e2e test will be helpful in the long run. Again not required for this PR.\nI will merge as soon as the unit tests are fixed.\n. @jimmidyson: Agreed. Thats been a want for quite a while.\nThanks for the review!\n. Ping @vmarmol @rjnagal \n. ok to test.\n. @jimmidyson: Will auth be necessary even if heapster were to use the readonly master service kubernetes-ro?\n. @jimmidyson: Suggestion. Can we use the kube clientauth library instead? @erictune is working on introducing secrets into containers and using the clientauth library will be necessary soon.\n. @etune mentioned that he was going to start working on it today. So I\nexpect to see something in a week on Kubernetes HEAD.\nOn Fri, Mar 27, 2015 at 10:21 AM, Jimmi Dyson notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh - yes this makes a lot more sense. I\nwas unaware of this coming but I like it :) Are there any timescales for\nwhen this will be supported in a kubernetes release?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/195#issuecomment-87017605\n.\n. SGTM\n\nOn Fri, Mar 27, 2015 at 10:25 AM, Jimmi Dyson notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh - ok i'll update this PR to use\nclientauth & we can wait to merge it until it's usable in Kubernetes?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/195#issuecomment-87020759\n.\n. LGTM. \n@jimmidyson: Agreed. Merging this PR now. Thanks for the PR!\n. That sounds like a good idea.\nI have also been doing some work on adding in-memory cache for metrics and\nevents. I was imagining having sources write data to the cache and sinks\npulling data off of the cache in a sampling rate of their choice. WDYT?\n\nOn Wed, Apr 1, 2015 at 5:33 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nOf course this approach could also be used for monitoring multiple\nKubernetes clusters, or even for federation.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/197#issuecomment-88462161\n.\n. There is a need to expose some of the metrics via a REST endpoint. So the\ncache will be necessary then.\n\nWe can also set it up in a such a way that the in-process sinks will\nimmediately receive data and can themselves decide on when to publish the\ndata. I guess I am moving towards a hybrid solution that combines an\nin-memory cache and a channel model.\nAnother thing to consider is that if sinks are performing their own\ncaching, then we might end up duplicating data in multiple sinks until the\ndata is flushed. Having a central cache helps keep the sink logic simple -\nthe sinks can retry on failure or fallback to their default policy on\nsuccess.\nThis is just a brain dump. I am happy to be convinced otherwise.\nOn Thu, Apr 2, 2015 at 2:00 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nI think that approach would work well for exposing data to be\npulled/scraped by external services, e.g. prometheus. I see this in-memory\ncache as one example of a sink, just one that doesn't push the data\nanywhere, just exposes it for external reaping.\nHaving it used in process for push only sinks seems like an unnecessary\noverhead. If data is being pushed then why not push it as it's collected?\nPush sinks could do their own throttling of data, but keeping an in-memory\ncache of metrics when all you're trying to do is push to external sinks\nseems like a use of resources. Normally when pushing metrics you would\nconfigure the sampling rate at the source & push the data.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/197#issuecomment-88834940\n.\n. @jimmidyson: Any thoughts here? I did some minor updates to the sources code (#220) over the weekend. This should make adding multiple sinks straight forward with the current architecture, until we have a better solution.\n. I like the basic idea.  Wouldn't combining all the options for a given\nsource or sink be tedious when run manually though? Another option is to\nadd structure to flag names. Or we can ditch flags altogether and take in a\nconfig file.\n\nOn Tue, Apr 7, 2015 at 4:30 AM, Jimmi Dyson notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh - saw your changes. Happy to go with\nthe in-memory representation for now & see how it works out.\nAs for configuration of backends, how would you feel about dropping the\nindividual -kubernetes_master, -coreos, etc flags & move to a more\ngeneric --source=: flag definition. We could\nthen use the new factory methods you've added in & side-effect imports to\nadd in source & sink extensions/plugins? e.g. if I wanted to use kubernetes\nsource I would use something like:\n--source=kubernetes:?insecure=true&kubelet-port=10250&auth=/etc/heapster/clientauth.json\nThe --source flags & complementary --sinkflags could both be specified\nmultiple times.\nThis approach would make adding sources/sinks much easier IMO.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/197#issuecomment-90514196\n.\n. Most of the manual runs are for testing. Automated testing can address that issue to some extent. Your argument is sound!\n\nAs for flags, the format could be something like sources.<source_type>.<option> - e.x.: --sources.kube.master=http://kubernetes-ro/ --sources.kube.auth-file=\"/path/to/auth/file\" --sources.coreos.fleet-endpoints=\"127.0.0.1\"\nI like config file. Its simple. The same format could be used to enable dynamic configuration. I find static configuration to be a major issue with many existing monitoring and logging solutions. So I intend to evolve heapster to support config updates.\nI am fine with merging this PR and replace this with a config file once either of us has some spare cycles.\n. Yessss!! Ideally I want to get rid of all the scripts in this repo.\n. I like this idea. Most of heapster can act as a library with this change. viper sounds pretty interesting. It should satisfy most of the deployment scenarios.\n. Just to clarify, the current assumption is that older kubernetes versions should not be using the latest heapster. We bundle a specific version of heapster with a kubernetes release. We can re-think that strategy in the future once the kube API and some of the internal code stabilizes. \n. ok to test\n. LGTM. Will merge once the tests pass.\n. @cadvisorJenkinsBot: ok to test\n. @Andre-Freitas: A new release for heapster and Grafana will be out today or tomorrow. I would suggest trying with that.\n. @Andre-Freitas: Can you try out the latest release (v0.10.0) and see if it works for you?\n. From the logs you provided everything seems to be fine. Can you get seom\nclient side logs from the browser while using Grafana? Is the kube\napiserver accessible from outside the cluster? Is it running on non-default\nport?\nOn Mon, Apr 6, 2015 at 2:15 AM, Andre-Freitas notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh I have deployed the latest version from\ngithub, but now my grafana web page is giving 404 page not found regarding\nat each graph http://postimg.org/image/7ahbhrzj3/\nRegarding the logs of each container: http://ur1.ca/k4916\nRegarding heapster webpage: http://ur1.ca/k491g\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/203#issuecomment-89990157\n.\n. Discussed with @saad-ali offline about the schema. He is working on updating the PR to use a more user friendly schema.\n. @saad-ali: I was thinking of just saving the event message as value. But it looks like you are storing a lot more data that that. If we think we need to store the entire event object, then I agree with @jimmidyson on marshalling the entire object into json. Sorry about the confusion.\n. @jimmidyson: Having columns derived from Event object is confusing. I feel that the event object wasn't conceived to be human parsable.\n. @saad-ali: The PR looks good. Just a few comments. Some of the comments are related to sink functionalities and unfortunately they are not documented properly :(\n. @rjnagal: Sorry, I was too hasty. Fixed the issues.\n. Ping @rjnagal. This is ready to be merged.\n. Overall LGTM. Just a few comments. Should we wait until 0.10.0 is released?\n. Needs a rebase. Feel free to self merge once you have updated the release to v0.10.0 in the guide.\n. @vmarmol: This is safe to be merged now.\n. Ping @vmarmol \n\ncc @jimmidyson \n. @jimmidyson: From the API it is not clear if NodeLegacyHostIP refers to the internal or the external IP. We have been punting on using versioned API inside heapster for quite a while. If this is critical, then it might be worth converting internal API objects from API server over to v1beta3 before consuming inside heapster. On the flipside v1beta3 is still not complete AFAIK.\n. I don't know why jenkins is not updating this PR, but, it did pass on jenkins (successful run)\n. @vmarmol: This PR is safe to be merged now.\n. cc @vmarmol \n. @vmarmol: Instead of this PR, how about updating GCM kube-config to downsample more?\n. Closing this PR. Happy to re-open if a config based strategy will not work.\n. Ping @rjnagal \n. This is useful for 0.10.0 release.\n. Ping @saad-ali \n. 0.14.0 has a bug. Wasted a lot of time on that. 0.14.1 shoudl be fine. Zach\nis fixing the release so that a tarball will be available.\nOn Wed, Apr 1, 2015 at 5:48 PM, Victor Marmol notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh is that build live? It's having trouble\npulling it. Maybe lets use 0.14.0 for now?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/216#issuecomment-88676208\n.\n. Ping @rjnagal \n. Thanks for the super quick review @rjnagal!!!\n. @rjnagal: Distributing node certs to heapster will be difficult. So for the short term the idea is to use the read-only 'http' endpoint exposed by the kubelet. We will have to update the default kubelet port.\n. cc @roberthbailey who is working on the new kube security model.\n. cc @saad-ali \n. @rjnagal: Thanks for the quick review. Addressed your comments.\n. @rjnagal: Build is green\n. @vmarmol: Fixed your comments. PTAL\n. ok to test\n\nOn Tue, Apr 7, 2015 at 12:41 PM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/223#issuecomment-90709650\n.\n. @jimmidyson: I like the simplicity of this PR! My only concern is that it is not user friendly. I remember you suggesting viper. May I know why you chose the url approach in this PR over the viper model?\n. I don't mind merging this as long as the existing mechanism continues to work. Once we add viper/config file support, we can get rid of the existing flags altogether.\n. @jimmidyson: integration/.jenkins.sh needs to be updated in addition to the travis yml file. Can you add a short doc that describes the new config scheme and may be add a link to it from the main README.md? \n\nOverall LGTM! Thanks for working on this!\n. @cadvisorJenkinsBot: ok to test\n. @cadvisorJenkinsBot: retest\n. @cadvisorJenkinsBot: test this please\n. @jimmidyson: I figured out the reason for integration test failures. You will have to either include #226 or rebase once it gets merged. Also having a short doc that describes the new config model will help.\n. Awesome! Ping me once the docs are complete. \n. Great. LGTM except for that one nit. \n. Thanks. Can you squash the commits?\n. Great! Will merge once the tests pass!\n. Thanks for the reminder!\n. @cadvisorJenkinsBot: ok to test\n. Thanks for the PR! I can set up coveralls for this project!\n. Merging this PR since it does not touch the main code.\n. @cadvisorJenkinsBot: test this please\n. @jimmidyson: Good idea. Done.\n. @vmarmol: I made some more changes to the makefile and travis config. PTAL\n. This happens when the influxdb pod is not online. Heapster and InfluxDB are\ncreated in parallel, and heapster waits for InfluxDB to be online before\nproceeding with data collection.\nBy default, you can access Grafana at 'https:///api/\nv1beta1/proxy/services/monitoring-grafana/' . If you add a 'hostport'\noption to InfluxDB pod, it will be accessible via the node IPs.\nOn Fri, Apr 10, 2015 at 5:37 PM, Josh Reichardt notifications@github.com\nwrote:\n\nI am running Kubernetes v0.14.2, following the instructions to turn up\nHeapster using the kubectl create -f deploy/kube-config/influxdb/ command.\nPods, RCs and services seem to get created okay but I noticed the\nfollowing in the log grafana pod logs (via kubectl og\nmonitoring-heapster-controller-26873).\n2015-04-11T00:24:53.414429145Z + EXTRA_ARGS=\n2015-04-11T00:24:53.414485300Z + '[' '!' -z '\"\"' ']'\n2015-04-11T00:24:53.414500150Z + EXTRA_ARGS='\"\"'\n2015-04-11T00:24:53.414597602Z + '[' '!' -z 10.244.0.1 ']'\n2015-04-11T00:24:53.414597602Z + EXTRA_ARGS='--kubernetes_master 10.244.0.1:80 \"\"'\n2015-04-11T00:24:53.414620734Z + HEAPSTER=/usr/bin/heapster\n2015-04-11T00:24:53.414738668Z + case $SINK in\n2015-04-11T00:24:53.414762507Z + HEAPSTER='/usr/bin/heapster --sink influxdb'\n2015-04-11T00:24:53.414771068Z + '[' '!' -z 10.244.0.1 ']'\n2015-04-11T00:24:53.414778741Z + INFLUXDB_ADDRESS=\n2015-04-11T00:24:53.414797022Z + '[' '!' -z 10.244.36.227 ']'\n2015-04-11T00:24:53.414826064Z + INFLUXDB_ADDRESS=10.244.36.227:80\n2015-04-11T00:24:53.414854342Z + /usr/bin/heapster --sink influxdb --sink_influxdb_host 10.244.36.227:80 --kubernetes_master 10.244.0.1:80 '\"\"'\n2015-04-11T00:24:53.422331365Z I0411 00:24:53.421526       7 heapster.go:45] /usr/bin/heapster --sink influxdb --sink_influxdb_host 10.244.36.227:80 --kubernetes_master 10.244.0.1:80 \"\"\n2015-04-11T00:24:53.422331365Z I0411 00:24:53.422095       7 heapster.go:46] Heapster version 0.10.0\n2015-04-11T00:24:53.422331365Z I0411 00:24:53.422161       7 kube.go:237] Using Kubernetes client with master \"http://10.244.0.1:80\" and version v1beta1\n2015-04-11T00:24:53.422331365Z I0411 00:24:53.422174       7 kube.go:238] Using kubelet port \"10250\"\n2015-04-11T00:24:53.422331365Z I0411 00:24:53.422198       7 driver.go:164] Using influxdb on host \"10.244.36.227:80\" with database \"k8s\"\n2015-04-11T00:24:53.525288101Z E0411 00:24:53.524627       7 driver.go:175] Database creation failed: Post http://10.244.36.227:80/db?u=root&p=root: net/http: transport closed before response was received. Retrying after 30 seconds\n2015-04-11T00:25:23.525907630Z E0411 00:25:23.525863       7 driver.go:175] Database creation failed: Post http://10.244.36.227:80/db?u=root&p=root: read tcp 10.244.36.227:80: connection reset by peer. Retrying after 30 seconds\n2015-04-11T00:26:17.574399625Z E0411 00:26:17.574356       7 driver.go:175] Database creation failed: Post http://10.244.36.227:80/db?u=root&p=root: read tcp 10.244.36.227:80: connection reset by peer. Retrying after 30 seconds\n2015-04-11T00:27:11.624016034Z E0411 00:27:11.623943       7 driver.go:175] Database creation failed: Post http://10.244.36.227:80/db?u=root&p=root: read tcp 10.244.36.227:80: connection reset by peer. Retrying after 30 seconds\n2015-04-11T00:28:05.673956790Z E0411 00:28:05.673903       7 driver.go:175] Database creation failed: Post http://10.244.36.227:80/db?u=root&p=root: read tcp 10.244.36.227:80: connection reset by peer. Retrying after 30 seconds\n2015-04-11T00:28:59.721990128Z E0411 00:28:59.721918       7 driver.go:175] Database creation failed: Post http://10.244.36.227:80/db?u=root&p=root: read tcp 10.244.36.227:80: connection reset by peer. Retrying after 30 seconds\nI am also unsure from the readme what address I should be visiting to\naccess the Grafana GUI? I see the reference to https://\n/api/v1beta1/proxy/services/monitoring-grafana is that\ncorrect? Or will it be one of the node IP's?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/231.\n. What issues are you facing? Did you replace the '' portion in\nthe url with your kubernetes master IP? Are you running the kubernetes\napi-server in a custom port?\n\nOn Mon, Apr 13, 2015 at 7:19 PM, Josh Reichardt notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh is there an example of that somewhere?\nI was having issues browsing to the url you pointed out.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/231#issuecomment-92563294\n.\n. Don't you have to provide master auth?\n\nOn Tue, Apr 14, 2015 at 8:02 AM, Josh Reichardt notifications@github.com\nwrote:\n\nWhen I run a curl to the above master address I get the following Error:\n'dial tcp 10.244.103.11:8080: no route to host'.\nI am running the Kubernetes master on port 8080.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/231#issuecomment-92893430\n.\n. It looks like the apiserver doesn't have any auth configured. Are you able\nto access the api-server? By that I mean access http://\n<master-IP>:<Port>/.\n. Are there any firewall rules that you need to configure to access the\nmaster?\n\nOn Tue, Apr 14, 2015 at 12:54 PM, Vishnu Kannan vishnuk@google.com wrote:\n\nIt looks like the apiserver doesn't have any auth configured. Are you able\nto access the api-server? By that I mean access http://\n<master-IP>:<Port>/.\n. This sounds like a kubernetes issue. Can you raise an issue against\nkubernetes or may be reach out to some of the CoreOS users on IRC\n(google-containers on freenode)?\n\nOn Tue, Apr 14, 2015 at 1:05 PM, Josh Reichardt notifications@github.com\nwrote:\n\nNo firewall rules, right now internally it is pretty wide open. I can curl\nhttp://: okay.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/231#issuecomment-93043483\n.\n. Ah! May be you can connect with @jonlangemak who maintains a kubernetes on\nCoreOS setup with really awesome guides!\n\nOn Tue, Apr 14, 2015 at 2:05 PM, Josh Reichardt notifications@github.com\nwrote:\n\nMaybe I will just hold off until Kubernetes stabilizes a little bit :/\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/231#issuecomment-93060954\n.\n. ok to test\n. I have restarted the integration tests. Not sure why it failed. @simon3z this PR needs a rebase. Thank you!\n. @simon3z: This PR does not seem to work with kube version v0.14.1. When was the secure service added to Kube? \nP.S.: You can run the integration test manually by following this guide\n. @simon3z: I haven't tried running it manually yet. From the logs, it looks like heapster is not able to fetch any data, which happens when there are issues reaching out to the apiserver.\n. This might become unnecessary once heapster starts using secrets to talk to the apiserver securely. FYI!\n. @jimmidyson :+1: \n. #279 should have achieved the same goal. Kindly re-open if #279 doesn't satisfy your use case @simon3z \n. Kindly use vebeta1 as the runtime version for heapster. That is the default as of now. \n. This has been fixed in v0.13.0 release\n. @cadvisorJenkinsBot: ok to test\n. LGTM.\n. Self merging since tests don't use v1beta3 confis yet.\n. Self merging since tests do not capture grafana dashboards.\n. Ping @rjnagal. This PR fixes heapster head\n. Good idea! Another possible approach will be to add another label called\n'type' that will refer to the kind of object a given metrics belongs to.\nThe 'type' can be 'Container', 'Pod', 'Node', 'Replication Controller',\netc. WDYT?\n\nOn Thu, Apr 16, 2015 at 10:24 AM, Eran Gabber notifications@github.com\nwrote:\n\nI prefer that node metrics will be easily distinguishable from container\nmetrics.\nDo you plan to have the node label as the \"hostname\" or somewhere else?\nI suggest that node metrics will have an empty container name (instead of\n\"/\").\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/240#issuecomment-93790270\n.\n. Awesome.\n\nOn Fri, Apr 17, 2015 at 8:34 AM, Eran Gabber notifications@github.com\nwrote:\n\nThe 'type' label is a much better idea. In this way you can have different\nlabel names for different resource types, and you could simply omit labels\nthat do not make sense for the particular resource.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/240#issuecomment-94010391\n.\n. Ping @rjnagal \n. Ping @rjnagal \n. @rjnagal: I have rebased this PR! PTAL.\n. PIng @vmarmol @EranGabber\n. @EranGabber: Responded to your comments. I will cc in a some upcoming PRs that should address your comments.\n. @EranGabber: This PR will ensure the existing cumulative metrics are preserved.\n. @vmarmol: Give this branch a spin if possible. I am not able to figure out why the DELETE call fails :(\n. PTAL @vmarmol. I updated the docs as well. I tested manually and it works. An integration test for GCM would reduce that anxiety :)\n. Addressed comments\n. Thanks for the review @vmarmol !\n. Thanks for reporting the issue!\n\nThe 'COREOS=true' part should be fixed in the upcoming release, where we\nprovide a different configuration pattern.\nHeapster assumes it is run with net=host docker option to access fleet\nserver. You can use the 'FLAGS' environment variable to pass a list of\nfleet endpoints -  FLAGS=--fleet_endpoints=<endpoints>. Again this\nconfiguration will be simplified in the next release.\nOn Sun, Apr 19, 2015 at 11:00 AM, George Angel notifications@github.com\nwrote:\n\nTwo issues, one is minor, in the example CoreOS README, the flag is -e\nCOREOS which doesn't work:\n10 heapster.go:46] Flags: alsologtostderr='false' bq_account='' bq_credentials_file='' bq_id='' bq_project_id='' bq_secret='notasecret' cadvisor_port='8080' coreos='false' external_hosts_file='/var/run/heapster/hosts' fleet_endpoints='http://127.0.0.1:4001' kubelet_port='10250' kubernetes_insecure='true' kubernetes_master='' listen_ip='' log_backtrace_at=':0' log_dir='' logtostderr='true' max_procs='0' poll_duration='10s' port='8082' sink='influxdb' sink_influxdb_buffer_duration='10s' sink_influxdb_host='172.17.42.1:8086' sink_influxdb_name='k8s' sink_influxdb_password='root' sink_influxdb_username='root' sink_memory_ttl='1h0m0s' stderrthreshold='2' v='0' vmodule='*=3'\nShould probably be changed to: -e COREOS=true:\n9 heapster.go:46] Flags: alsologtostderr='false' bq_account='' bq_credentials_file='' bq_id='' bq_project_id='' bq_secret='notasecret' cadvisor_port='8080' coreos='true' external_hosts_file='/var/run/heapster/hosts' fleet_endpoints='http://127.0.0.1:4001' kubelet_port='10250' kubernetes_insecure='true' kubernetes_master='' listen_ip='' log_backtrace_at=':0' log_dir='' logtostderr='true' max_procs='0' poll_duration='10s' port='8082' sink='influxdb' sink_influxdb_buffer_duration='10s' sink_influxdb_host='172.17.42.1:8086' sink_influxdb_name='k8s' sink_influxdb_password='root' sink_influxdb_username='root' sink_memory_ttl='1h0m0s' stderrthreshold='2' v='0' vmodule='*=3'\nSecond, default fleet endpoint http://127.0.0.1:4001 doesn't work, as\ncall is being made inside the container, not running fleet.\nI0419 14:54:33.460410       9 heapster.go:57] Starting heapster on port 8082\nINFO client.go:291: Failed getting response from http://127.0.0.1:4001/: dial tcp 127.0.0.1:4001: connection refused\nERROR client.go:213: Unable to get result for {Get /_coreos.com/fleet/machines}, retrying in 100ms\nINFO client.go:291: Failed getting response from http://127.0.0.1:4001/: dial tcp 127.0.0.1:4001: connection refused\nERROR client.go:213: Unable to get result for {Get /_coreos.com/fleet/machines}, retrying in 200ms\nINFO client.go:291: Failed getting response from http://127.0.0.1:4001/: dial tcp 127.0.0.1:4001: connection refused\nERROR client.go:213: Unable to get result for {Get /_coreos.com/fleet/machines}, retrying in 400ms\nINFO client.go:291: Failed getting response from http://127.0.0.1:4001/: dial tcp 127.0.0.1:4001: connection refused\nERROR client.go:213: Unable to get result for {Get /_coreos.com/fleet/machines}, retrying in 800ms\nINFO client.go:291: Failed getting response from http://127.0.0.1:4001/: dial tcp 127.0.0.1:4001: connection refused\nERROR client.go:213: Unable to get result for {Get /_coreos.com/fleet/machines}, retrying in 1s\nINFO client.go:291: Failed getting response from http://127.0.0.1:4001/: dial tcp 127.0.0.1:4001: connection refused\nERROR client.go:213: Unable to get result for {Get /_coreos.com/fleet/machines}, retrying in 1s\nI0419 14:54:46.461194       9 coreos.go:48] failed to get list of machines from fleet - \"timeout reached\"\nF0419 14:54:46.461223       9 heapster.go:110] failed to get information from source\nI'm assuming you want to set it to the IP of the HOST on docker interface.\nBut I couldn't find a flag that would override that.\nThanks.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/248.\n. cc @saad-ali \n. The proxy on the apiserver has been causing quite some trouble. Thanks for digging in. Will you be able to post a patch to fix this?\n. Can we have some generic one to one mapping from a uid to a float?\n\nOn Wed, Apr 22, 2015 at 5:16 PM, Saad Ali notifications@github.com wrote:\n\nThis would be super sweet, but it it looks like sequence_number can not\nbe an arbitrary string.\nfailed to write events to influxDB - Server returned (400): sequence_number field must be float but is string (43e8c11b-e94c-11e4-a7c2-42010af0f6db)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/250#issuecomment-95373321\n.\n. LGTM excepting the few comments. I will let @vmarmol do a final review and merge this PR!\n. Nice job @saad-ali! \n. Ok merging this PR! @vmarmol if you have any comments, we can address them in a subsequent PR.\n. LGTM except for gofmt!\n. LGTM\n. #253 should solve this. The next release should have namespace label.\n. Thanks for the fix @luksa! \n@cadvisorJenkinsBot: ok to test\n. Good suggestion. I will add a more detailed guide once I get some spare\ncycles.\n\nOn Fri, Apr 24, 2015 at 1:59 PM, krancour notifications@github.com wrote:\n\nAny chance of more complete documentation and unit files for the \"various\ncomponents\" mentioned here?\nhttps://github.com/GoogleCloudPlatform/heapster/blob/master/docs/coreos.md#unit-files-for-the-various-components\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/259.\n. @saad-ali: Needs a rebase. Otherwise LGTM\n. LGTM. Thanks @saad-ali \n. Ping @vmarmol @rjnagal \n. Self merging since it has been reviewed already!\n. I tested this manually with just write scope enabled for logging and it works.\n. LGTM. This is very useful @saad-ali \n. I am considering switching over to some other CI service. Travis is too slow.\n. I ran the test manually and they pass. I think we can go ahead and merge this PR since the only code change is that of updating the version number.\n. @vmarmol: Ah! I wasn't aware of the outage. I am thinking of adding support for CircleCI just as a backup. WDYT?\n. The jenkins failure was a real issue. Fixed it. But it took a lot of time to debug the kube cluster to figure our the root cause. It should pass this time around!\n. Self merging this PR with the LGTM from @vmarmol \n. cc @saad-ali @vmarmol \n. I apologize for the confusion. The support is still intact. The\nconfiguration has changed. Take a look here\nhttps://github.com/GoogleCloudPlatform/heapster/blob/master/docs/source-configuration.md\n.\nWe had to update the configuration semantics to make it possible to support\nmore sources in the future.\n\nOn Mon, May 4, 2015 at 7:36 AM, Vadim notifications@github.com wrote:\n\nCurrently i can't find any up to date docs about how to running heapster\non coreos.\nIf i try to put --coreos parameter or --fleet_endpoints heapster print\nhelp with:\nflag provided but not defined: -coreos\nIs coreos support dropped or I need to use specific environment variables?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/273.\n. Fixed all nits. Thanks for the review @vmarmol \n. ok to test\n. Thanks for the PR @luksa! I wonder if adding access to kubeConfig is a prerequisite for this PR?\n. Ping @luksa \n. LGTM.\n. Can you post heapster logs and some information about your kubernetes setup? Are you running on GCE or bare-metal? How have you set up networking?\n. The issue is this flag --kubernetes_version=v1beta3. I removed it on\nHEAD. Can you retry with configs from latest HEAD? Sorry for the trouble!\n\nOn Wed, May 6, 2015 at 12:28 AM, biswars notifications@github.com wrote:\n\nHere is the Heapster log:\n/opt/bin/kubectl log monitoring-heapster-controller-9ihq3\n2015-05-06T07:09:23.181452490Z + EXTRA_ARGS=\n2015-05-06T07:09:23.181452490Z + '[' '!' -z --kubernetes_version=v1beta3\n']'\n2015-05-06T07:09:23.181452490Z + EXTRA_ARGS=--kubernetes_version=v1beta3\n2015-05-06T07:09:23.181452490Z + '[' '!' -z 10.100.0.1 ']'\n2015-05-06T07:09:23.181452490Z + EXTRA_ARGS='--kubernetes_master\n10.100.0.1:80 --kubernetes_version=v1beta3'\n2015-05-06T07:09:23.181452490Z + HEAPSTER=/usr/bin/heapster\n2015-05-06T07:09:23.181452490Z + case $SINK in\n2015-05-06T07:09:23.181452490Z + HEAPSTER='/usr/bin/heapster --sink\ninfluxdb'\n2015-05-06T07:09:23.181452490Z + '[' '!' -z 10.100.0.1 ']'\n2015-05-06T07:09:23.181452490Z + INFLUXDB_ADDRESS=\n2015-05-06T07:09:23.181452490Z + '[' '!' -z 10.100.44.240 ']'\n2015-05-06T07:09:23.181452490Z + INFLUXDB_ADDRESS=10.100.44.240:80\n2015-05-06T07:09:23.181452490Z + /usr/bin/heapster --sink influxdb\n--sink_influxdb_host 10.100.44.240:80 --kubernetes_master 10.100.0.1:80\n--kubernetes_version=v1beta3\n2015-05-06T07:09:24.244084682Z I0506 07:09:24.213734 8 heapster.go:45]\n/usr/bin/heapster --sink influxdb --sink_influxdb_host 10.100.44.240:80\n--kubernetes_master 10.100.0.1:80 --kubernetes_version=v1beta3\n2015-05-06T07:09:24.244084682Z I0506 07:09:24.242699 8 heapster.go:46]\nHeapster version 0.10.0\n2015-05-06T07:09:24.252171510Z I0506 07:09:24.249727 8 kube.go:237] Using\nKubernetes client with master \"http://10.100.0.1:80\" and version v1beta3\n2015-05-06T07:09:24.252171510Z I0506 07:09:24.249780 8 kube.go:238] Using\nkubelet port \"10250\"\n2015-05-06T07:09:24.252171510Z I0506 07:09:24.250028 8 driver.go:164]\nUsing influxdb on host \"10.100.44.240:80\" with database \"k8s\"\n2015-05-06T07:09:24.551762140Z E0506 07:09:24.532776 8 reflector.go:115]\nFailed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:24.551867724Z E0506 07:09:24.544043 8 driver.go:175]\nDatabase creation failed: Post http://10.100.44.240:80/db?u=root&p=root:\nnet/http: transport closed before response was received. Retrying after 30\nseconds\n2015-05-06T07:09:25.541940354Z E0506 07:09:25.541854 8 reflector.go:115]\nFailed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:26.568185275Z E0506 07:09:26.550427 8 reflector.go:115]\nFailed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:27.562186149Z E0506 07:09:27.561886 8 reflector.go:115]\nFailed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:28.570102593Z E0506 07:09:28.569595 8 reflector.go:115]\nFailed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:29.576157889Z E0506 07:09:29.575924 8 reflector.go:115]\nFailed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:30.582672970Z E0506 07:09:30.582291 8 reflector.go:115]\nFailed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:31.589976589Z E0506 07:09:31.589723 8 reflector.go:115]\nFailed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:32.596224621Z E0506 07:09:32.595927 8 reflector.go:115]\nFailed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:33.606609261Z E0506 07:09:33.606137 8 reflector.go:115]\nFailed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:34.616152119Z E0506 07:09:34.615970 8 reflector.go:115]\nFailed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:35.648468652Z E0506 07:09:35.647990 8 reflector.go:115]\nFailed to list api.Node: the server could not find the requested resource\nKubernetes Setup:\nVersion - v0.16.0-1-gfc76242f4afd2c\nNetwork Set up - Test set up with virtual box's NAT (not GCE). Kubernetes\nset up is done with private etcd discovery and post set up able to list the\nminions and also able to run pods. Able to access the minions from the host\nsystem and able to access cadvisor at port 4194\nyaml files used: https://github.com/GoogleCloudPlatform/heapster.git\ncommit -e3b3b0584de41e1da1d58c9b8b99313ee0812f4d\nLet me know if any further info needed on this.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/277#issuecomment-99353539\n.\n. I would recommend picking up a kubernetes release from kubernetes repo and\nusing the heapster version built into that. Every kubernetes release runs a\nversion of heapster by default and I would suggest sticking to that.\n\nOn Wed, May 6, 2015 at 11:04 AM, biswars notifications@github.com wrote:\n\nThanks for all your responses so far; much appreciated. In fact, the flag\nissue of --kubernetes_version=v1beta3 was observed and that was the reason\nto settle to a working version for this set up. Also, had done some testing\nby copying the heapster binary from the docker container and running\nwithout the flag that resulted in some other error. But I can confirm this\nback exactly on the error once I am back to my working network after few\nhours from now.\nBefore that (sorry but since it seems I am into a different time zone, so\ntaking some feedback early), could you let me know which version of\nkubernetes, coreos, etcd (private discovery or clustered) and heapster (you\nhave already mentioned to try from the latest) you have run the set up\nsuccessfully ? So that I can straight away work on that and confirm you\nback.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/277#issuecomment-99554497\n.\n. If you run with the latest kubernetes version (0.16.0+), secrets should work. I will try running heapster HEAD against 0.16.2 today and update this thread.\n\nDid you setup a cluster with kubernetes version 0.16.0 from scratch or did you upgrade the different components manually?\nIf you use the heapster configs from Kubernetes release 0.16.0, what errors do you observe? The etcd watch errors are benign. You can ignore them. \nCan you also try adding --vmodule=*=5 flag to FLAGS env variable in heapster-controller.yaml file? This will make heapster spit out more debugging logs.  \nWe have continuous integration tests in the kubernetes repo and so heapster bundles with every kube release is supposed to just work. \n. @biswars: How are you deploying heapster? Can you get rid of your existing heapster setup and run the following commands?\nkubectl.sh create -f <kubernetes_repo>/cluster/addons/dns/\nkubectl.sh create -f <kubernetes_repo>/cluster/addons/cluster-monitoring/influxdb/\nThe first command helps setup cluster local dns for service discovery. The second command sets up heapster, influxdb and grafana.\nCan you figure out if InfluxDB has the metrics data after running these commands? Once we get that part to work, we can focus on why Grafana dashboard is not working for you.\n. For Grafana, take a look here. You can add a public IP, which in your case can be your node's IP, to the Grafana service. This change requires re-loading your influxdb-grafana controller.\n. You can also look at the browser console to find out what queries are\nfailing and potentially why.\nOn Fri, May 8, 2015 at 12:51 PM, biswars notifications@github.com wrote:\n\nI am taking this point and will be exercising this as well along with dns\nset up. But the set up I am running is completely on my local machine with\nVMs running by virtual box. So, it is not a production set up and might not\nneed to add public IP to the node as I can access it directly.\nAny other inputs you would like to provide here for the 404 error in\ngrafana dashboard graphs ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/277#issuecomment-100339395\n.\n. The apiserver now runs with v1beta3 version by default. I have updated the\nsetup guide to use v1beta3 instead of v1beta1.\n1. Grafana does not auto generate graphs for new pods. You can add scripts\n   I believe to achieve that. If you are interested in this feature to work,\n   feel free to post a PR.\n2. Tokens are required to access the apiserver. This is to make sure that\n   the apiserver is secure. Take a look at docs/authentication.md in\n   Kubernetes repo.\n   To work around the token issue for now, use the latest heapster configs\n   from heapster repo and update \"--source=kubernetes:http://kubernetes-ro\"\n   flag to be \"--source=kubernetes:http://kubernetes-ro?auth=\". That should\n   get you past the token issue. Do keep in mind that this wont work for long\n   in upstream kubernetes.\n\nOn Sun, May 10, 2015 at 9:59 AM, biswars notifications@github.com wrote:\n\nDid some more analysis and able to get the grafana ui. Here are my\nobservations:\nAs mentioned above, I was trying to access grafana by hitting the url\nhttps://172.17.8.102:6443/api/v1beta1/proxy/services/monitoring-grafana/.\nObserved that the internal urls getting hit in the browser is\nhttps://172.17.8.102:6443/api/v1beta1/proxy/namespaces/default/services/monitoring-grafana/.\n(this always gives HTTP 404)\nWhen I access the url\nhttps://172.17.8.102:6443/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/\nthen the ui comes up.\nSo, a question here - who does this url redirection ? Is it due to the\npods used here are with v1beta3 ?\nQueries:\n1. The \"All containers CPU usage - stacked shows \"No datapoints\" initially\nwith only grafana, influxdb and heapster running. The query run is \"select\ncontainer_name, derivative(value) from \"cpu/usage_ns_cumulative\" where time\n\nnow() - 5m group by time(5s), container_name \" which gives empty result\nwhen I tried in influxdb ui. Is it the normal behavior ? The memory usage\ngraph shows the gauge value properly.\n\n1.\nThe grafana ui shows two rows distinctively for the pods heapster and\n   influx-grafana. When I run more pods onto the cluster it does not add\n   similar rows for each pod. Is this the normal behavior ?\n   2.\nI used the 0.16.2 kubernetes binaries so that I can use the secrets as\n   volume as you had mentioned earlier. But I still get error due to missing\n   \"token-system-monitoring\" and pod does not get deployed. Just out of\n   curiosity, where is this secret defined ? In heapster-controller.json, it\n   is just referenced but I think it should be defined somewhere isn't it ?\n   (Note, as mentioned earlier I am using the kubernetes binaries manually and\n   not on GCE. So, not sure if I need to define the secrete separately)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/277#issuecomment-100668464\n.\n. The IP in that URL is that of the kubernetes api-server.\n\nOn Fri, Jun 12, 2015 at 6:48 AM, shwetalakhimpur notifications@github.com\nwrote:\n\n@biswars https://github.com/biswars - I am unable to access the Grafana\nUI\nWhen I access the url\nhttps://172.17.8.102:6443/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/\nthen the ui comes up.\nPlease help me understand the IP:port here.\nIs this is IP which is running monitoring-influx-grafana-controller-y3qn3\n? This is the IP listed under IP or HOST when we do \"kubectl get pods\"\n172.17.8.102:6443\nThe port 6443, is this the port listed in \"grafana-service.json\" ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/277#issuecomment-111497696\n.\n. Take a look at kubernetes cluster setup guides. You missed the section on \"service accounts\" most likely. It is supposed to be setup by default, but it requires enabling a plugin. Sorry for the confusion!\n. Have you looked at the source config docs in this repo?\n. Heapster is having trouble accessing the API server. Can you check\nkube-proxy logs on the heapster node to ensure there are no proxy issues?\n\ncc @mikedanese\nOn Fri, Aug 21, 2015 at 4:00 AM, biswars notifications@github.com wrote:\n\nThanks @vishh https://github.com/vishh . With the serviceAccounts set\nproperly, could get the heapster up and running properly with master and\nsingle node. However, when I add one more node to the cluster (running only\nkubelet and proxy services) and heapster happens to run on that node, then\nI get the following error. (Note: I don't get any error and heapster runs\nfine with single node running on the master itself.).\ncore@core-02 ~ $ kubectl logs -f monitoring-heapster-v8-5fatp\n--namespace=kube-system\nI0821 10:35:12.208070 1 heapster.go:55] /heapster --source=kubernetes:''\n--sink=influxdb:http://monitoring-influxdb:8086\nI0821 10:35:12.208244 1 heapster.go:56] Heapster version 0.17.0\nI0821 10:35:12.214370 1 kube_factory.go:168] Using Kubernetes client with\nmaster \"https://10.100.0.1:443\" and version \"v1\"\nI0821 10:35:12.214575 1 kube_factory.go:169] Using kubelet port 10255\nE0821 10:35:12.257887 1 reflector.go:136] Failed to list api.Pod: Get\nhttps://10.100.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D: EOF\nE0821 10:35:12.259134 1 reflector.go:136] Failed to list api.Node: Get\nhttps://10.100.0.1:443/api/v1/nodes: EOF\nE0821 10:35:12.259621 1 reflector.go:136] Failed to list api.Namespace:\nGet https://10.100.0.1:443/api/v1/namespaces: EOF\nE0821 10:35:12.261380 1 kube_events.go:96] Failed to load events: Get\nhttps://10.100.0.1:443/api/v1/events: EOF\nI0821 10:35:12.265586 1 driver.go:376] created influxdb sink with options:\n{root root monitoring-influxdb:8086 k8s false}\nI0821 10:35:12.273340 1 heapster.go:66] Starting heapster on port 8082\nE0821 10:35:12.279199 1 kube_events.go:96] Failed to load events: Get\nhttps://10.100.0.1:443/api/v1/events: EOF\nE0821 10:35:13.262188 1 reflector.go:136] Failed to list api.Pod: Get\nhttps://10.100.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D: EOF\nE0821 10:35:13.269873 1 reflector.go:136] Failed to list api.Namespace:\nGet https://10.100.0.1:443/api/v1/namespaces: EOF\nE0821 10:35:13.271305 1 reflector.go:136] Failed to list api.Node: Get\nhttps://10.100.0.1:443/api/v1/nodes: EOF\nE0821 10:35:14.266397 1 reflector.go:136] Failed to list api.Pod: Get\nhttps://10.100.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D: EOF\nE0821 10:35:14.279837 1 reflector.go:136] Failed to list api.Node: Get\nhttps://10.100.0.1:443/api/v1/nodes: EOF\nE0821 10:35:14.282135 1 reflector.go:136] Failed to list api.Namespace:\nGet https://10.100.0.1:443/api/v1/namespaces: EOF\nE0821 10:35:15.270247 1 reflector.go:136] Failed to list api.Pod: Get\nhttps://10.100.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D:\nread tcp 10.100.0.1:443: connection reset by peer\nE0821 10:35:15.288348 1 reflector.go:136] Failed to list api.Namespace:\nGet https://10.100.0.1:443/api/v1/namespaces: EOF\nE0821 10:35:15.290524 1 reflector.go:136] Failed to list api.Node: Get\nhttps://10.100.0.1:443/api/v1/nodes: read tcp 10.100.0.1:443: connection\nreset by peer\nE0821 10:35:16.274316 1 reflector.go:136] Failed to list api.Pod: Get\nhttps://10.100.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D:\nread tcp 10.100.0.1:443: connection reset by peer\nE0821 10:35:16.297905 1 reflector.go:136] Failed to list api.Node: Get\nhttps://10.100.0.1:443/api/v1/nodes: EOF\nE0821 10:35:16.299074 1 reflector.go:136] Failed to list *api.Namespace:\nGet https://10.100.0.1:443/api/v1/namespaces: EOF\nWhen I log in to the heapster container running on the separate node, I\ncan see the ca.crt and token (inside /var/run/secrets/\nkubernetes.io/serviceaccount/) same as that of containers running in the\nfirst node. Not sure what might be the issue still. Don't get any related\nerror in kube-apiserver log though. Anything I am missing ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/277#issuecomment-133372938\n.\n. Ping @vmarmol \n. @lgfausak: Tokens are required for access to the kube api-server. Kubernetes exposes two default services to access the apiserver - one requires no auth and is read-only (kubernetes-ro); the other requires auth (kubernetes) and is read-write. Kubernetes is in the process of dropping support for kubernetes-ro service. Heapster used to use the kubernetes-ro service until recently.\nFor the short-term, until you figure out how to setup tokens for cluster services, I am making a change to heapster to make it possible to run heapster using kubernetes-ro service. I will get back to you with specific instructions once the required changes are in.\n. @vmarmol: E2e passes now.\n. LGTM\n. LGTM\n. Thanks for the PR @burmanm. Can you add all new go dependencies using godep save?\n. Overall LGTM. Can you add a section in docs/storage-schema.md for hawkular that describes the differences between base heapster storage schema and the one you are currently adding for hawkular?\n\nDo Squash your commits at the end. \n. @vmarmol: I am fixing e2e. Kindly bear with me for some time.\n. LGTM. Ignoring travis since jenkins runs unit tests.\n. Ping @vmarmol. e2e is green. I think we can ignore travis.\n. SSL files have to be plumbed here. Will you be able to post a PR? \nIf not, I will get to it soon.\n. Manual testing seems to work. I have made some more refactoring and updated the code. \n. Most likely yes. I will look into the failing cluster some time soon.\n. @vmarmol: Jenkins is green. I didn't change anything since the last failing run. \n. ping @vmarmol @rjnagal \n. @vmarmol: This is safe to merge now.\n. +1. I have been experimenting with Bosun as well.\nOn Mon, May 18, 2015 at 4:34 PM, Jack Foy notifications@github.com wrote:\n\nWe'd like to stream our metrics to a Riemann instance http://riemann.io/\nbefore sending them to influxdb. I'm working on a sink to support this.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/293.\n. Support for Riemann is now available on HEAD. \n. +1 for the validator. There is a pending PR to switch back to yaml format. So we will need a yaml validator.\n\nThanks for fixing this @vmarmol. Wonder how I missed this while testing manually.\n. heapster currently does not store limits. +1 for storing limits which\nshould let you calculate/graph percentage usage.\nOn Tue, May 19, 2015 at 4:25 AM, biswars notifications@github.com wrote:\n\nThe requirement is to take action based on the percentage of usage of the\ncontainers running on kubernetes cluster. Using heapster and influxDB, I\ncan get the current and max usage of cpu/memory using the series\n\"cpu/usage_ns_cumulative\" and \"memory/usage_bytes_gauge\" but not the\npercentage of usage based on the limit set at the container.\nThe first impression was this behavior might be because of not setting the\ncpu/memory limit inside the pod yaml file. When I set the limit using the\nfollowing, I observed the same behavior.\ncpu: 100\nmemory: 50000000\nWhen observed in the cAdvisor ui, it shows \"Limit unlimited\" in the Memory\nsection and \"Shares 1024 shares\" in the CPU section.\nCan I get the percentage of usage based on the limit set for the\ncontainers so that useful action can be taken ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/296.\n. Yes. Cadvisor provides Spec and Status. Spec has the limits.\n\nOn Tue, May 19, 2015 at 9:12 PM, biswars notifications@github.com wrote:\n\nJust a query here - even cAdvisor does not show the limit set to the\ncontainer (at least in the web interface). So, are there any means to get\nthe container limits using cAdvisor api directly ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/296#issuecomment-103749741\n.\n. There are two parts to your question:\n1. How to calculate instantaneous usage:\nI would recommend running continuous queries and creating a new\ninstantaneous series based on cpu/usage_ns_cumulative series. You can\ncalculate a derivative of this series with an appropriate time interval(1m\nfor example) and store in a separate series.\n2. How to partition a node's CPU?\nCPU shares are ratios and it is difficult to correlate usage with cpu\nshares. I would recommend choosing a multiplier for each CPU, (1000 per CPU\nfor example) and coming up with a max shares per node. if your node has 4\ncores, then it has a max of 4000 shares. Now each share translates to 1\nmillisecond of CPU usage per second. If you provide a container 1000\nshares, then its limit is 1 second of CPU usage per second.\nNote: CPU shares are proportional - What that means is that when the node\nis fully loaded, each container is guaranteed by the kernel to get at least\ntheir share's worth of CPU time.\n\nHope this helps!\nOn Wed, May 20, 2015 at 10:04 PM, biswars notifications@github.com wrote:\n\nThanks. Yeah, able to get the limits directly now and also able to\nco-relate with the data as sampled by heapster. For the memory usage, I am\nusing the series \"memory/usage_bytes_gauge\" and works perfect.\nHowever, having some issue with the cpu usage using the series\n\"cpu/usage_ns_cumulative\". Post some analysis, I guessed the \"value\" field\nin series \"cpu/usage_ns_cumulative\" gives the time in nano secs spent in\nthe form of cpu cycles on specific container. Any suggestion on how to\ntransform this value to the %age of cpu usage for a container ?\nNote: I am able to get the cpu limit (as specified in the pod/rc\nyaml/json) also now which gives the cpu share per container (viz. 1024, 512\netc). But getting the %age using the limit value and\n\"cpu/usage_ns_cumulative\" is where am not able to get any co-relation.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/296#issuecomment-104136446\n.\n. That sounds about right.\n\nOn Thu, May 21, 2015 at 10:58 PM, biswars notifications@github.com wrote:\n\nThanks. Very useful comments. One query before closing this. If I want to\nrelate the above two parts to get the cpu usage per container can I take\nlike this way;\na: Result of the new series from part 1 above gives the nano secs of cpu\nusage per 1m container-wise. From this get the number of seconds cpu usage\nper second. Assign this value as \"m\".\nb: Based on the cpu shares per node (taking 1000 as per part 2 above) and\nlimit set by the container (ex: 500 which is half of the cpu), if m == 0.5\nthen the container's cpu usage is 100%.\nWould this be the right way ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/296#issuecomment-104525981\n.\n. poll_duration determines how often heapster queries kubelets.\nstats_resolution determines the downsampling that heapster performes. If\nthey both are equal heapster will not perform any downsampling.\n\nOn Wed, Jun 3, 2015 at 3:07 AM, biswars notifications@github.com wrote:\n\nTo answer above query, heapster polling interval can be changed in the pod\ndefinition file by cofiguring \"--poll_duration\" and with this the data\npoints are populated. Does \"--stats_resolution\" also need to be set. By\nsetting these values as \"10s\" and \"5s\" respectively, the data points can be\nretrieved from influxDB if externally polled at certain interval (using 60\nsec). The only observation here is sometimes the data points are empty for\ntime series \"cpu/usage_ns_cumulative\". Any configuration changes need to be\nmade so that the data points can be retrieved always without fail ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/296#issuecomment-108285208\n.\n. ok to test\n. Thanks for the fix @huliz! \n. @cadvisorJenkinsBot: ok to test\n. @krousey: I can make a new release tomorrow.\n. Thanks for the fix @krousey. LGTM\n. What are the use cases for standalone mode? Is it for dev testing or having\na scaled out setup?\n\nOn Mon, Aug 3, 2015 at 12:43 AM, Roshan N notifications@github.com wrote:\n\nIs there a half-written documentation somewhere?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/299#issuecomment-127150079\n.\n. Unit tests for the cache is on its way. Otherwise this PR is good to go.\n. Fixed the new set of comments as well. Thanks for the through review @vmarmol. I apologize for the size of this PR\n. @mvdan: Did you make any progress on this?\n. LGTM\n. LGTM\n. Thanks @vmarmol! LGTM\n. @hyperbolic2346: Thanks for the PR! The coreos documentation does require some updates. I have opened #305 to fix the docs. Can you check if the steps in #305 work for you?\n. @cadvisorJenkinsBot: ok to test.\n. @hyperbolic2346: LGTM. Needs a rebase though.\n. LGTM\n. Thanks for reporting the issue @swordphilic!\n. LGTM.\n. Thanks for the PR! LGTM except for the one nit.\n. Unit tests are failing. Can you run make test-unit?\n. Coverage is not enforced as of now.\n. @cadvisorJenkinsBot: ok to test\n. LGTM.\n. LGTM. Thanks for the fix @gosharplite!\n. Support for billing has been in the roadmap for a while. Heapster does not\nsupport billing yet.\n. Ping @vmarmol \n. PTAL.\n. @cadvisorJenkinsBot: ok to test\n. We can ignore the travis failure. It will be fixed in #313. LGTM\n. @rjnagal: Travis seems to be happy with not having a vet dependency. The current failure is due to a flaky test.\n. LGTM\n. @afein: Still a WIP. But managed to rebase.\n. @afein: Can I get a quick review? I would like for this to go in before hitting any more merge conflicts.\n. Wonder why the integration test is failing.\n. I don't see the docs @vmarmol \n. LGTM\n. Heapster does not use etcd. To monitor a coreos cluster, heapster needs\naccess to fleet server(s), which can run locally or remotely.\n\nOn Mon, Jun 8, 2015 at 12:33 PM, krancour notifications@github.com wrote:\n\nVarious CoreOS cluster architectures are documented here:\nhttps://coreos.com/docs/cluster-management/setup/cluster-architectures/\nBut this one in particular is most applicable to large and/or production\nclusters:\nhttps://coreos.com/docs/cluster-management/setup/cluster-architectures/#production-cluster-with-central-services\nThe hallmark of that architecture is that common service like etcd (for\nexample) are isolated to a small, fixed number of nodes, which a larger\nnumber of less remarkable \"worker nodes\" is permitted to autoscale. Such\nworker nodes do not run etcd.\nCurrently, it does not seem possible to run heapster on such \"worker\nnodes\" that are dependent on remote etcd peers.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/318.\n. Heapster should be able to use a remote fleet server. Look here.\n. cc @vmarmol \n. Can you post logs from heapster pod? (kubectl logs <heapster pod name>)\n\nOn Mon, Jun 8, 2015 at 1:16 PM, shilpapadgaonkar notifications@github.com\nwrote:\n\nHi,\nI am trying to run heapster on openshift3. I followed the documentation\nlink\nhttps://github.com/GoogleCloudPlatform/heapster/blob/master/docs/influxdb.md\nI also made changes to remove the environment variable\nINFLUXDB_EXTERNAL_URL from InfluxDB Grafana RC config and added a route so\nthat Grafana service become reachable as an external service.\nAll services were created. InfluxDB and grafana started without issues. I\nam able to see the Grafana UI.\nThe monitoring-heapster-controller does not start though. It keeps showing\nme a pending state. I am not sure why..\nI have read in the docu somewhere that I could provide heapster a\nkubernetes source explicitly. Do I need to do this while trying to run it\non openshift3 ? Could you please let me know how I could do this?\nThanks in advance.\nRegards,\nShilpa\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/320.\n. cc @jimmydyson\n\nOn Tue, Jun 9, 2015 at 3:43 AM, shilpapadgaonkar notifications@github.com\nwrote:\n\nThanks @detiber https://github.com/detiber for your help.\nI modified a couple of things in the heapster-controller such as source to\nkkubernetes as --source=kubernetes:http://kubernetes-ro.default.local.\nNow the pod atleast gets into a running state.\nBut now I face the same issue as you where the logs say that it cant find\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/320#issuecomment-110315822\n.\n. Heapster uses a monitoring-token that is auto-generated on Kubernetes\nclusters as part of the cluster setup process. I assume that is not the\ncase in Openshift?\n\nOn Tue, Jun 9, 2015 at 7:23 AM, Jimmi Dyson notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh Are there any plans for heapster to use\nservice accounts?\n@shilpapadgaonkar https://github.com/shilpapadgaonkar You need to\ncreate a secret containing the contents of a kubeconfig file & mount it at\nthe appropriate place in your pod. Should work, but not tried it recently.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/320#issuecomment-110374627\n.\n. Exporting only when it changes is an optimization. I am fine with this PR as-is for now.\n. Closing this PR in favor of #326 \n. @cadvisorJenkinsBot: ok to test\n. @jimmidyson: Can you add some documentation to go along with this PR? This will help support new users on OpenShift.\n. LGTM. Thanks @jimmidyson \n. AFAIK the current api-server proxy isn't designed to handle metrics kind of\nworkloads. The plan was to expose a read-only 'http' endpoints on the\nkubelet which exports metrics. Is this not an option for OpenShift?\n\nOn Tue, Jun 9, 2015 at 12:33 PM, Jimmi Dyson notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh I think this should be the default once\nimplemented. What do you think?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/325#issuecomment-110478543\n.\n. Ping @rjnagal. \n. Fixed the commit issues. PTAL @rjnagal \n. @rjnagal: I fixed the e2e. This PR is good to go now. Jenkins would have covered unit testing as well.\n. LGTM\n. @vmarmol: This PR is good to go now. e2e is green.\n. Self merging since @vmarmol signed off with an LGTM already\n. cc @rjnagal \n. In Kubernetes, heapster is expected to just work. You don't have to run\nadditional cadvisor containers.\nIs the heapster pod up and running?\nIf its running, can you get logs from the pod? 'kubectl logs\n'\nI suspect its a deployment issue.\n\nOn Wed, Jun 10, 2015 at 11:46 PM, yash notifications@github.com wrote:\n\nhmm, thanks again! I am running kubernetes in CoreOS , below are the\noutputs I get, I think some issue with cgroups, any advice would be really\nhelpful:\n1) cluster-info output\nKubernetes master is running at https://localhost:8443\nmonitoring-grafana is running at\nhttps://localhost:8443/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana\nis running at\nhttps://localhost:8443/api/v1beta3/proxy/namespaces/default/services/monitoring-heapster\n2) get pods output\nNAME READY REASON RESTARTS AGE\nmonitoring-influx-grafana-controller-bafhg 2/2 Running 0 19h\n3) curl -L localhost:4194/validate\ncAdvisor version: 0.13.0\nOS version: CentOS Linux 7 (Core)\nKernel version: [Supported and recommended]\nKernel version is 4.0.4-303.fc22.x86_64. Versions >= 2.6 are supported.\n3.0+ are recommended.\nCgroup setup: [Supported and recommended]\nAvailable cgroups: map[memory:1 freezer:1 net_cls:1 perf_event:1 cpuset:1\ncpu:1 cpuacct:1 blkio:1 net_prio:1 hugetlb:1 devices:1]\nFollowing cgroups are required: [cpu cpuacct]\nFollowing other cgroups are recommended: [memory blkio cpuset devices\nfreezer]\nHierarchical memory accounting status unknown: memory cgroup not mounted.\nCgroup mount setup: [Unknown]\nCould not locate cgroup mount point.\nAny cgroup mount point that is detectible and accessible is supported.\n/sys/fs/cgroup is recommended as a standard location.\nDocker version: [Supported and recommended]\nDocker version is 1.6.0. Versions >= 1.0 are supported. 1.2+ are\nrecommended.\nDocker driver setup: [Unsupported]\nDocker exec driver is native-0.2. Storage driver is devicemapper.\nCgroups are being created through cgroup filesystem.\nDocker container state directory \"/var/lib/docker/containers\" is not\naccessible.\nBlock device setup: [Supported and recommended]\nAt least one device supports 'cfq' I/O scheduler. Some disk stats can be\nreported.\nDisk \"dm-0\" Scheduler type \"none\".\nDisk \"dm-1\" Scheduler type \"none\".\nDisk \"dm-2\" Scheduler type \"none\".\nDisk \"xvda\" Scheduler type \"cfq\".\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/333#issuecomment-111014895\n.\n. 1. You don't have to run cadvisor on every node in kubernetes. Cadvisor is\n   integrated into the kubelet\n2. you can turn on additional logging in heapster by passing a flag -\n   '--v=4'. You can specify this in heapster-controller file\n3. Heapster uses DNS by default to discover InfluxDB service. Is kube-dns\n   running in your cluster. If not I would recommend setting it up.\n4. By default, the API server is locked down and cluster admins need to\n   setup tokens which have the necessary auth information to provide access to\n   the API server. Heapster expects one such token to be available by default.\n   You can change the heapster config to not require a token.\n5. For DNS issues, I would recommend filing an issue against kubernetes\n   repo.\n\nOn Tue, Jun 16, 2015 at 2:29 AM, yash notifications@github.com wrote:\n\nin logs it gives \" skydns: incomplete CNAME chain: rcode is not equal to\nsuccess\" would be really very helpful if u can give pointers on this.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/333#issuecomment-112362474\n.\n. Can you post the output of following commands?\nkubectl describe rc --namespace=kube-system monitoring-heapster\nkubectl describe pod --namespace=kube-system monitoring-heapster-v6-hld43\n\nOn Fri, Aug 21, 2015 at 11:19 AM, Daniel Wang notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh I encountered the same issue today. I\nam new to Kubernetes and followed the getting started\nhttps://github.com/kubernetes/kubernetes/blob/master/docs/getting-started-guides/gce.md\ndoc to start my first cluster on GCE. It started up correctly. However the\npod(s) for heapster failed to get to the READY state in 50 minutes. I\ndidn't have any customization to the configs.\nWhat could go wrong?\n$ kubectl cluster-info\nKubernetes master is running at https://104.197.105.88\nKubeDNS is running at https://104.197.105.88/api/v1/proxy/namespaces/kube-system/services/kube-dns\nKubeUI is running at https://104.197.105.88/api/v1/proxy/namespaces/kube-system/services/kube-ui\nGrafana is running at https://104.197.105.88/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana\nHeapster is running at https://104.197.105.88/api/v1/proxy/namespaces/kube-system/services/monitoring-heapster\nInfluxDB is running at https://104.197.105.88/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb\n$ kubectl get --all-namespaces pods\nNAMESPACE     NAME                                           READY     STATUS             RESTARTS   AGE\nkube-system   etcd-server-kubernetes-master                  1/1       Running            2          51m\nkube-system   fluentd-cloud-logging-kubernetes-master        1/1       Running            1          51m\nkube-system   fluentd-cloud-logging-kubernetes-minion-oleh   1/1       Running            0          50m\nkube-system   fluentd-cloud-logging-kubernetes-minion-pqgz   1/1       Running            0          50m\nkube-system   fluentd-cloud-logging-kubernetes-minion-rqv7   1/1       Running            0          50m\nkube-system   fluentd-cloud-logging-kubernetes-minion-tqvc   1/1       Running            0          50m\nkube-system   kube-apiserver-kubernetes-master               1/1       Running            1          51m\nkube-system   kube-controller-manager-kubernetes-master      1/1       Running            1          51m\nkube-system   kube-dns-v8-knx96                              4/4       Running            0          51m\nkube-system   kube-scheduler-kubernetes-master               1/1       Running            1          51m\nkube-system   kube-ui-v1-0y9p9                               0/1       CapacityExceeded   0          51m\nkube-system   kube-ui-v1-8t7up                               0/1       CapacityExceeded   0          51m\nkube-system   kube-ui-v1-c4clc                               0/1       CapacityExceeded   0          51m\nkube-system   kube-ui-v1-eeqe3                               0/1       CapacityExceeded   0          51m\nkube-system   kube-ui-v1-gmqvu                               1/1       Running            0          51m\nkube-system   kube-ui-v1-gocxx                               0/1       CapacityExceeded   0          51m\nkube-system   kube-ui-v1-n885p                               0/1       CapacityExceeded   0          51m\nkube-system   monitoring-heapster-v6-hld43                   0/1       Running            8          51m\nkube-system   monitoring-heapster-v6-kbc32                   0/1       CapacityExceeded   0          51m\nkube-system   monitoring-influx-grafana-v1-4ubsn             2/2       Running            0          51m\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/333#issuecomment-133521170\n.\n. The pod seems to be restarting often. Can you post the output of kubectl logs -p --namespace=kube-system monitoring-heapster-v6-hld43?\nI suspect either InfluxDB not up or memory issues. What version of heapster is this?\n. If you are not running InfluxDB, can you remove the InfluxDB sink from\nheapster configs? The configs here\nare an example of heapster not requiring any sinks.\n\nOn Thu, Nov 12, 2015 at 12:01 PM, Nick Leli notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh I'm having a similar problem as\n@wonderfly https://github.com/wonderfly. My K8S cluster was deployed\nwith the kube-up script, on AWS. The cluster seems to be working fine,\nand I can access the Grafana UI, but do not get any heapster data. I am\nalso seeing a lot of restarts with the Pod. Looking at the logs gives me:\n$ kubectl logs -p --namespace=kube-system monitoring-heapster-v5-919mw\nI1112 19:47:10.367791       1 heapster.go:52] /heapster --source=kubernetes:'' --sink=influxdb:http://monitoring-influxdb:8086\nI1112 19:47:10.367874       1 heapster.go:53] Heapster version 0.16.0\nI1112 19:47:10.371759       1 kube_factory.go:169] Using Kubernetes client with master \"https://10.0.0.1:443\" and version \"v1\"\nI1112 19:47:10.371787       1 kube_factory.go:170] Using kubelet port 10255\nI1112 19:47:10.398029       1 driver.go:376] created influxdb sink with options: {root root monitoring-influxdb:8086 k8s false}\nI1112 19:47:10.398903       1 heapster.go:64] Starting heapster on port 8082\nE1112 19:49:23.127563       1 driver.go:231] failed to write stats to influxDB - Post http://monitoring-influxdb:8086/db/k8s/series?u=root&p=root&time_precision=s: dial tcp: lookup monitoring-influxdb: no such host\nE1112 19:50:01.464521       1 driver.go:231] failed to write stats to influxDB - Post http://monitoring-influxdb:8086/db/k8s/series?u=root&p=root&time_precision=s: dial tcp: lookup monitoring-influxdb: no such host\nIt looks like @wonderfly https://github.com/wonderfly and I do not have\nan influxDB pod running. Is that the problem?\n$ kubectl get pods --all-namespaces\nNAMESPACE     NAME                                                              READY     STATUS    RESTARTS   AGE\ndefault       mysql                                                             1/1       Running   0          15h\ndefault       wordpress                                                         1/1       Running   0          15h\nkube-system   elasticsearch-logging-v1-bz2yp                                    1/1       Running   0          16h\nkube-system   elasticsearch-logging-v1-r5jio                                    1/1       Running   0          16h\nkube-system   fluentd-elasticsearch-ip-172-20-0-67.us-west-2.compute.internal   1/1       Running   0          16h\nkube-system   fluentd-elasticsearch-ip-172-20-0-68.us-west-2.compute.internal   1/1       Running   0          16h\nkube-system   kibana-logging-v1-elr8u                                           1/1       Running   0          16h\nkube-system   kube-dns-v8-h0uk2                                                 4/4       Running   0          16h\nkube-system   kube-ui-v1-y57xg                                                  1/1       Running   0          16h\nkube-system   monitoring-heapster-v5-919mw                                      1/1       Running   2          16h\nkube-system   monitoring-influx-grafana-v1-e0xvk                                2/2       Running   0          16h\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/333#issuecomment-156218424\n.\n. @smugcloud: If you want to use Grafana, then you need InfluxDB. The fact that InfluxDB pod is not running is an issue. I do see monitoring-influx-grafana-v1-e0xvk pod running. Can you list services kubectl get svc --namespace=kube-system? \nHeapster for some reason is not able to resolve monitoring-influxdb service DNS name.\n. Hmm. The service is running. The errors might have been transient. Can we\ntry enabling detailed logging? Detailed steps here.\nI wonder if heapster is not able to fetch data from the nodes.\nOne more thing to try would be to log in to InfluxDB UI and run list\nseries to see if data is flowing to InfluxDB.\n\nOn Thu, Nov 12, 2015 at 12:43 PM, Nick Leli notifications@github.com\nwrote:\n\nSure, here you go:\n$ kubectl get svc --namespace=kube-system\nNAME                    LABELS                                                                                              SELECTOR                        IP(S)          PORT(S)\nelasticsearch-logging   k8s-app=elasticsearch-logging,kubernetes.io/cluster-service=true,kubernetes.io/name=Elasticsearch   k8s-app=elasticsearch-logging   10.0.253.115   9200/TCP\nkibana-logging          k8s-app=kibana-logging,kubernetes.io/cluster-service=true,kubernetes.io/name=Kibana                 k8s-app=kibana-logging          10.0.110.106   5601/TCP\nkube-dns                k8s-app=kube-dns,kubernetes.io/cluster-service=true,kubernetes.io/name=KubeDNS                      k8s-app=kube-dns                10.0.0.10      53/UDP\n                                                                                                                                                                           53/TCP\nkube-ui                 k8s-app=kube-ui,kubernetes.io/cluster-service=true,kubernetes.io/name=KubeUI                        k8s-app=kube-ui                 10.0.32.40     80/TCP\nmonitoring-grafana      kubernetes.io/cluster-service=true,kubernetes.io/name=Grafana                                       k8s-app=influxGrafana           10.0.160.123   80/TCP\nmonitoring-heapster     kubernetes.io/cluster-service=true,kubernetes.io/name=Heapster                                      k8s-app=heapster                10.0.189.126   80/TCP\nmonitoring-influxdb     kubernetes.io/cluster-service=true,kubernetes.io/name=InfluxDB                                      k8s-app=influxGrafana           10.0.242.132   8083/TCP\n                                                                                                                                                                           8086/TCP\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/333#issuecomment-156228212\n.\n. How are you trying to access the InfluxDB UI?\nA simpler option might be to try upgrading to Heapster v0.18.2. We made\nseveral improvements to that version.\n\nOn Thu, Nov 12, 2015 at 1:06 PM, Nick Leli notifications@github.com wrote:\n\n@vishh https://github.com/vishh I don't think they're transient as I\nhave not been able to hit the Heapster or InfluxDB URL's since the cluster\nwas created. If I try to hit the Influx DB UI I get:\n\"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"no endpoints available for \\\"monitoring-influxdb\\\"\",\n  \"code\": 500\nI stopped the service, enabled the logging, and logs on the pod are the same as before:\n$ k logs monitoring-heapster-v5-919mw --namespace=kube-system\nI1112 21:01:36.952166 1 heapster.go:52] /heapster --source=kubernetes:''\n--sink=influxdb:http://monitoring-influxdb:8086\nI1112 21:01:36.952300 1 heapster.go:53] Heapster version 0.16.0\nI1112 21:01:36.953833 1 kube_factory.go:169] Using Kubernetes client with\nmaster \"https://10.0.0.1:443\" and version \"v1\"\nI1112 21:01:36.954911 1 kube_factory.go:170] Using kubelet port 10255\nI1112 21:01:36.977794 1 driver.go:376] created influxdb sink with options:\n{root root monitoring-influxdb:8086 k8s false}\nI1112 21:01:36.978631 1 heapster.go:64] Starting heapster on port 8082\nE1112 21:03:32.617864 1 driver.go:231] failed to write stats to influxDB -\nPost\nhttp://monitoring-influxdb:8086/db/k8s/series?u=root&p=root&time_precision=s:\ndial tcp: lookup monitoring-influxdb: no such host\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/333#issuecomment-156233689\n.\n. Run kubectl proxy & and then run the following commands:\n\ncurl -G '\nhttp://localhost:8001/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb:8086/db/k8s/series?u=root&p=root&pretty=true'\n--data-urlencode \"q=list series\".\ncurl -G '\nhttp://localhost:8001/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb:8086/db/k8s/series?u=root&p=root&pretty=true'\n--data-urlencode \"q=select distinct(pod_namespace) from /cpu*/\"\nIf we see all the series and namespaces, I would suspect the Grafana to\nInfluxDB pipeline.\nOn Thu, Nov 12, 2015 at 2:01 PM, Nick Leli notifications@github.com wrote:\n\nI'm hitting the URL provided in cluster-info:\nhttps://x.x.x.x/api/v1/proxy/namespaces/kube-system/services/monitoring-heapster/.\nShould I be hitting something else?\nI'll try upgrading the heapster image and report back.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/333#issuecomment-156249683\n.\n. Yeah. I don't see any write failures for the InfluxDB sink. So I would\nsuspect the Grafana -> InfluxDB bridge.\nI wonder why the service URL isn't working for you though. The service is\nexpected to be running.\nCan you post the output of kubectl describe pods -l k8s-app=influxGrafana\nand kubectl get endpoints --all-namespaces?\nWhat version of kubernetes are you using?\n\nAs a side note, I'd suggest upgrading heapster and InfluxDB+Grafana. Recent\nreleases have a lot of improvements.\nOn Thu, Nov 12, 2015 at 3:06 PM, Nick Leli notifications@github.com wrote:\n\nWhat's also strange is that if I hit the /validate endpoint for Heapster,\nI get a (unformatted) response back. This was generated by hitting:\nhttps://master_IP/api/v1/proxy/namespaces/kube-system/services/monitoring-heapster/validate.\nEverything appears healthy from this perspective.\nHeapster Version: 0.16.0\nSource type: kube-pod-metrics\n    No pod errors\nSource type: Kube Node Metrics\nKubernetes Nodes plugin:\n    Healthy Nodes:\n        ip-172-20-0-67.us-west-2.compute.internal\n        ip-172-20-0-68.us-west-2.compute.internal\n    No node errors\nSource type: kube-events\nExternal Sinks\n    Exported metrics:\n        uptime: Number of milliseconds since the container was started      cpu/usage: Cumulative CPU usage on all cores        cpu/limit: CPU limit in millicores      memory/usage: Total memory usage        memory/working_set: Total working set usage. Working set is the memory being used and not easily dropped by the kernel      memory/limit: Memory limit      memory/page_faults: Number of page faults       memory/major_page_faults: Number of major page faults       network/rx: Cumulative number of bytes received over the network        network/rx_errors: Cumulative number of errors while receiving over the network     network/tx: Cumulative number of bytes sent over the network        network/tx_errors: Cumulative number of errors while sending over the network       filesystem/usage: Total number of bytes consumed on a filesystem        filesystem/limit: The total size of filesystem in bytes\n    Exported labels:\n        hostname: Hostname where the container ran      host_id: Identifier specific to a host. Set by cloud provider or user       container_name: User-provided name of the container or full container name for system containers        pod_name: The name of the pod       pod_id: The unique ID of the pod        pod_namespace: The namespace of the pod     namespace_id: The UID of namespace of the pod       labels: Comma-separated list of user-provided labels        resource_id: Identifier(s) specific to a metric\n    External Sinks:\nSink Type: InfluxDB\nclient: Host \"monitoring-influxdb:8086\", Database \"k8s\"\nNumber of write failures: 0\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/333#issuecomment-156265390\n.\n. Everything seems to be right. I don't see why your queries to the\nmonitoring-influxdb service is failing. Can you post the URl you used?\nCould there be a typo?\nAlso v1.0.3 is old (yeah the project is moving at light speed). Upgrade to\nv1.06 at-least. There are known issues in lower versions, that can affect\nheapster functionality.\n\nOn Thu, Nov 12, 2015 at 4:47 PM, Nick Leli notifications@github.com wrote:\n\nSure thing @vishh https://github.com/vishh, here are the outputs. I'll\ntry upgrading heapster as well.\n$ kubectl describe pod monitoring-influx-grafana-v1-e0xvk --namespace=kube-system\nName:               monitoring-influx-grafana-v1-e0xvk\nNamespace:          kube-system\nImage(s):           gcr.io/google_containers/heapster_influxdb:v0.3,gcr.io/google_containers/heapster_grafana:v0.7\nNode:               ip-172-20-0-67.us-west-2.compute.internal/172.20.0.67\nLabels:             k8s-app=influxGrafana,kubernetes.io/cluster-service=true,version=v1\nStatus:             Running\nReason:\nMessage:\nIP:             10.244.1.6\nReplication Controllers:    monitoring-influx-grafana-v1 (1/1 replicas created)\nContainers:\n  influxdb:\n    Image:  gcr.io/google_containers/heapster_influxdb:v0.3\n    Limits:\n      cpu:      100m\n      memory:       200Mi\n    State:      Running\n      Started:      Wed, 11 Nov 2015 19:58:56 -0800\n    Ready:      True\n    Restart Count:  0\n  grafana:\n    Image:  gcr.io/google_containers/heapster_grafana:v0.7\n    Limits:\n      cpu:      100m\n      memory:       100Mi\n    State:      Running\n      Started:      Wed, 11 Nov 2015 19:59:17 -0800\n    Ready:      True\n    Restart Count:  0\nConditions:\n  Type      Status\n  Ready     True\nNo events.\n$ kubectl get endpoints --all-namespaces\nNAMESPACE     NAME                    ENDPOINTS\ndefault       kubernetes              172.20.0.9:443\ndefault       wordpress-frontend      10.244.1.10:80\ndefault       wordpress-mysql         10.244.1.7:3306\nkube-system   elasticsearch-logging   10.244.0.3:9200,10.244.1.4:9200\nkube-system   kibana-logging          10.244.1.3:5601\nkube-system   kube-dns                10.244.0.4:53,10.244.0.4:53\nkube-system   kube-ui                 10.244.0.5:8080\nkube-system   monitoring-grafana      10.244.1.6:8080\nkube-system   monitoring-heapster     10.244.1.5:8082\nkube-system   monitoring-influxdb     10.244.1.6:8083,10.244.1.6:8086\n$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"0\", GitVersion:\"v1.0.3\", GitCommit:\"61c6ac5f350253a4dc002aee97b7db7ff01ee4ca\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"0+\", GitVersion:\"v1.0.0-259-gf3b752d831708c\", GitCommit:\"f3b752d831708ccdad564bd188abb11822c51595\", GitTreeState:\"clean\"}\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/333#issuecomment-156282624\n.\n. @cadvisorJenkinsBot: ok to test\n. @jszczepkowski: Can we sync up offline on the specific goals of this PR? Depending on the goals, there could be alternate pathways to getting metrics plumbed into Google Cloud Monitoring.\n. @jszczepkowski: Are you continuing to work on this PR? If so can you rebase to HEAD?\n. Closing this PR in favor of #375 \n. Ping @rjnagal \n. Rebased @rjnagal \n. @rjnagal: Tests are passing now.\n. Done\n. Done\n. Can you describe your cluster setup - Kube version, cloud provider, etc?\nCan you verify if all the pods are running?\nCan you drop 'proxy' keyword from URL and see if the api-server responds\nwith a grafana service object?\n\nOn Fri, Jun 12, 2015 at 2:54 AM, shwetalakhimpur notifications@github.com\nwrote:\n\nI seem to be on the same issues as this thread - stuck at 404 Not found,\n203 https://github.com/GoogleCloudPlatform/heapster/issues/203\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/338#issuecomment-111433357\n.\n. The IP is that of your master. It must be the IP that kubectl uses. You can\nrun 'kubectl config view` and figure out the master IP. Use that IP to\naccess Grafana.\nAnother option is to make Grafana kubernetes service externally accessible.\nTake a look at this guide:\nhttps://github.com/GoogleCloudPlatform/heapster/blob/master/docs/influxdb.md#production-setup-for-influxdb-and-grafana\n\nOn Fri, Jun 12, 2015 at 11:28 AM, shwetalakhimpur notifications@github.com\nwrote:\n\n[root@minion01 2015Jun11]# kubectl log\nmonitoring-influx-grafana-controller-t3m8h grafana\n2015-06-12T18:21:11.817897874Z Creating config file:\n/opt/grafana/config.js.tmpl => /opt/grafana/config.js\n2015-06-12T18:21:11.818159522Z\n2015-06-12T18:21:11.818173274Z Creating service proxy: /db/ =>\nhttp://monitoring-influxdb:80/db/\n2015-06-12T18:21:11.818179873Z\n2015-06-12T18:21:11.818185007Z Listening on :8080\n2015-06-12T18:21:11.818190369Z\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/338#issuecomment-111581689\n.\n. Can you try accessing Grafana using the Public IP you assigned to it?\nThe concept behind Public IPs is that the service is accessible via that IP. So you don't have to go through the API server proxy anymore.\n. I suspect #249 should resolve all the proxy woes that you currently have. \n. cc @rjnagal \n. cc @rjnagal\n. Closing this PR since this PR is being split up into multiple ones.\n. cc @rjnagal \n. @saad-ali can you review this PR?\nRohit might be busy.\n. Self merging since a release needs to be cut soon.\n. cc @rjnagal: \n. Depends on #343 \n. cc @rjnagal \n. Thanks for the quick review. Updated the label name. PTAL\n. @rjnagal: E2e is green\n. cc @rjnagal \n. @rjnagal: Updated docs\n. @rjnagal: e2e is green :dancers: \n. cc @rjnagal \n. @rjnagal: Tested manually. PTAL\n. :( Its a one line fix. I just sent out the PR. I can make a patch release quickly if that will help. I started working on better test coverage, which should have caught this issue.\n. Just a few comments. Overall LGTM\n. LGTM. Will merge once CI passes.\n. FYI: You can an IDE hook for running gofmt on save. \n. The monitoring sinks like InfluxDB should have it.\n. Here is a running instance of heapster with this change: http://104.197.27.4:8082/api/v1/metric-export\n. @a-robinson: Try again.\n. #348 \n. Fixed on hub. Thanks for reporting the issue @njuicsgz :)\n. I updated the hub to have an image with v0.14.2 tag. Thanks for this PR!\n. Can you try again with setting --cache_duration=1m? What is the size of your node where heapster is running? memory allocation from within golang libraries are failing.\n. We can reduce the amount of data we query from kubelet, which might\nalleviate this issue. But from a deployment standpoint, I would recommend\nplacing limits on all the pods you run. That way, pods don't step on each\nother.\nI would suggest 500MB of memory for heapster (to be on the safe side, you\ncan reduce this later) and ~1GB for InfluxDB. Inorder for this to work, you\nneed to apply limits to all your pods though.\n\nOn Thu, Jun 18, 2015 at 9:18 AM, jekyang notifications@github.com wrote:\n\nHi, vishh\nI've tried again with --cache_duration=1m, it would still crash with same\nmessage.There are running 15 nodes in our k8s cluster, with more than 500\npods. Today, I've also noticed that the influxdb would crashed with \"fatal\nerror: runtime: cannot allocate memory\".\nhere's my setting parameters for heapster and influxdb:\n/opt/bin/heapster --source=kubernetes:http://xx.xx.xx.xx:8080\n--sink=\"influxdb:http://127.0.0.1:19186?u=root&p=root&db=k8s\"\n--cache_duration=1m\ndocker run -i -t -p 19183:8083 -p 19186:8086 --name influxdbtest -v\n/data/:/data/ kubernetes/heapster_influxdb:v0.3\nThere are both running in one machine together. And my machine has 2 cpu\nand 4G memory. I'm not sure whether there's other config about memory that\nmightbe cause the crash, Or whether should I increase our machine memory?\nCrash log from influxdb is below, crash log for heapster is same with 1st\ncomments.\n=> About to create the following database: k8s;grafana\n=> Starting InfluxDB ...\n[06/18/15 09:46:16] [INFO] Loading configuration file /config/config.toml\n+---------------------------------------------+\n| _  _ _  |\n| | | / _| | |  | _ \\ |\n| | | _  | || |  | | | | |) | |\n| | | | ' |\n| | | | \\ \\/ / | | | _ < | | | || | | | | | | || |> <| || | |\n_) | | | |_|| ||| ||**,//**/|____/ |\n+---------------------------------------------+\n=> Waiting for confirmation of InfluxDB service startup ...\n=> Waiting for confirmation of InfluxDB service startup ...\n{\"status\":\"ok\"}\n=> Creating database: k8s\n=> Creating database: grafana\nexec /usr/bin/influxdb -config=${CONFIG_FILE}\nfatal error: runtime: cannot allocate memory\nruntime stack:\nruntime.gothrow(0xd4f790, 0x1f)\n/root/.gvm/gos/go1.4/src/runtime/panic.go:503 +0x8e fp=0x7fff7f126780\nsp=0x7fff7f126768\nruntime.persistentalloc(0x1000, 0x40, 0x14e1f38, 0x0)\n/root/.gvm/gos/go1.4/src/runtime/malloc.go:824 +0x18b fp=0x7fff7f1267a8\nsp=0x7fff7f126780\ngetempty(0x7f2f10630000, 0x7f2f10630000)\n/root/.gvm/gos/go1.4/src/runtime/mgc0.c:578 +0xc6 fp=0x7fff7f1267d8\nsp=0x7fff7f1267a8\nscanblock(0xc26b508000, 0x1338000, 0x0)\n/root/.gvm/gos/go1.4/src/runtime/mgc0.c:454 +0x837 fp=0x7fff7f126918\nsp=0x7fff7f1267d8\nscanframe(0x7fff7f126a20, 0x0, 0x301)\n/root/.gvm/gos/go1.4/src/runtime/mgc0.c:719 +0x164 fp=0x7fff7f126988\nsp=0x7fff7f126918\nruntime.gentraceback(0x4f36e0, 0xc258642bc0, 0x0, 0xc262b93440, 0x0, 0x0,\n0x7fffffff, 0x7fff7f126ad0, 0x0, 0x0, ...)\n/root/.gvm/gos/go1.4/src/runtime/traceback.go:311 +0x7a8 fp=0x7fff7f126a78\nsp=0x7fff7f126988\nscanstack(0xc262b93440)\n/root/.gvm/gos/go1.4/src/runtime/mgc0.c:777 +0x21c fp=0x7fff7f126ae8\nsp=0x7fff7f126a78\nmarkroot(0xc208010000, 0x32)\n/root/.gvm/gos/go1.4/src/runtime/mgc0.c:553 +0xe7 fp=0x7fff7f126b48\nsp=0x7fff7f126ae8\nruntime.parfordo(0xc208010000)\n/root/.gvm/gos/go1.4/src/runtime/parfor.c:91 +0x13b fp=0x7fff7f126bc8\nsp=0x7fff7f126b48\nruntime.gchelper()\n/root/.gvm/gos/go1.4/src/runtime/mgc0.c:1185 +0x4a fp=0x7fff7f126bf0\nsp=0x7fff7f126bc8\nstopm()\n/root/.gvm/gos/go1.4/src/runtime/proc.c:1181 +0x158 fp=0x7fff7f126c10\nsp=0x7fff7f126bf0\ngcstopm()\n/root/.gvm/gos/go1.4/src/runtime/proc.c:1351 +0xed fp=0x7fff7f126c38\nsp=0x7fff7f126c10\nschedule()\n/root/.gvm/gos/go1.4/src/runtime/proc.c:1551 +0x9b fp=0x7fff7f126c68\nsp=0x7fff7f126c38\nruntime.gosched_m(0xc24e60a480)\n/root/.gvm/gos/go1.4/src/runtime/proc.c:1674 +0xac fp=0x7fff7f126c88\nsp=0x7fff7f126c68\nruntime.newstack()\n/root/.gvm/gos/go1.4/src/runtime/stack.c:776 +0x4fa fp=0x7fff7f126d38\nsp=0x7fff7f126c88\nruntime.morestack()\n/root/.gvm/gos/go1.4/src/runtime/asm_amd64.s:324 +0x7e fp=0x7fff7f126d40\nsp=0x7fff7f126d38\ngoroutine 1 [runnable]:\nruntime.makeslice(0xb05c80, 0x1000, 0x1000, 0x0, 0xffffffffffffff00,\n0x710bdb)\n/root/.gvm/gos/go1.4/src/runtime/slice.go:18 fp=0xc20815d618\nsp=0xc20815d610\nnet/http.newBufioWriterSize(0x7f2f2485db60, 0xc209ed6b40, 0x1000,\n0xc20815d710)\n/root/.gvm/gos/go1.4/src/net/http/server.go:488 +0x1a2 fp=0xc20815d6b0\nsp=0xc20815d618\nnet/http.(\n_Server).newConn(0xc208058420, 0x7f2f2485d870, 0xc21520b3e8, 0xc209ed6b40,\n0x0, 0x0) /root/.gvm/gos/go1.4/src/net/http/server.go:444 +0x433\nfp=0xc20815d790 sp=0xc20815d6b0 net/http.(_Server).Serve(0xc208058420,\n0x7f2f2485be30, 0xc20817a038, 0x0, 0x0)\n/root/.gvm/gos/go1.4/src/net/http/server.go:1746 +0x2f3 fp=0xc20815d868\nsp=0xc20815d790\ngithub.com/influxdb/influxdb/api/http.(\n_HttpServer).serveListener(0xc20803e420, 0x7f2f2485be30, 0xc20817a038,\n0xc20802c9a0) /root/go/src/github.com/influxdb/influxdb/api/http/api.go:202\nhttp://github.com/influxdb/influxdb/api/http/api.go:202 +0xd8\nfp=0xc20815d8d8 sp=0xc20815d868 github.com/influxdb/influxdb/api/http.(\nhttp://github.com/influxdb/influxdb/api/http.(_HttpServer).Serve(0xc20803e420,\n0x7f2f2485be30, 0xc20817a038)\n/root/go/src/github.com/influxdb/influxdb/api/http/api.go:172 +0x1219\nfp=0xc20815d928 sp=0xc20815d8d8\ngithub.com/influxdb/influxdb/api/http.(\n_HttpServer).ListenAndServe(0xc20803e420)\n/root/go/src/github.com/influxdb/influxdb/api/http/api.go:91\nhttp://github.com/influxdb/influxdb/api/http/api.go:91 +0x207\nfp=0xc20815d9d8 sp=0xc20815d928 github.com/influxdb/influxdb/server.(\nhttp://github.com/influxdb/influxdb/server.(_Server).ListenAndServe(0xc208094090,\n0x0, 0x0)\n/root/go/src/github.com/influxdb/influxdb/server/server.go:217 +0xbac\nfp=0xc20815dcb8 sp=0xc20815d9d8\nmain.start(0x0, 0x0)\n/root/go/src/github.com/influxdb/influxdb/daemon/influxd.go:202 +0x127a\nfp=0xc20815df80 sp=0xc20815dcb8\nmain.main()\n/root/go/src/github.com/influxdb/influxdb/daemon/influxd.go:73 +0x1f\nfp=0xc20815df98 sp=0xc20815df80\nruntime.main()\n/root/.gvm/gos/go1.4/src/runtime/proc.go:63 +0xf3 fp=0xc20815dfe0\nsp=0xc20815df98\nruntime.goexit()\n/root/.gvm/gos/go1.4/src/runtime/asm_amd64.s:2232 +0x1 fp=0xc20815dfe8\nsp=0xc20815dfe0\nThanks\n-Jack\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/355#issuecomment-113206778\n.\n. @jekyang: InfluxDB does consume quite a bit of memory. Heapster's memory usage is proportional to the number of containers. Can you post the output of heap profile from a running heapster?\nhttp://localhost:8082/debug/pprof/heap If the memory usage is growing over time, I would like to investigate right away. Thanks for reporting this issue.\ncc @saad-ali @dchen1107 \n. Output here: http://104.197.27.4:8082/api/v1/metric-export\n. @rjnagal: Can you try again?\n. ok to test\n. LGTM\n. Added you to the jenkins server. PRs should be built automatically henceforth.\n. Overall LGTM. Just a couple of comments.\n. @rjnagal: Assigned this PR to you!\n. cc @rjnagal: This the test change I was referring to offline, that will prevent API issues in the future.\n. @rjnagal: PTAL. Rebased to HEAD!\n. 1. Refactor sinks internal API to have a static schema and versioning.\n2. Expose the metrics API schema.\n3. Modify the integration test to validate the API output against the\n   schema.\n\nOn Thu, Jun 18, 2015 at 2:30 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nCan you describe all the pieces in this PR a bit more for my reference.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/359#issuecomment-113295486\n.\n. Just a few comments. Overall LGTM\n. LGTM. \n. Just a couple of comments. LGTM otherwise\n. LGTM. Thanks for the PR @afein \n. Just one comment. Otherwise LGTM\n. ok to test\n\nOn Tue, Jun 23, 2015 at 9:27 AM, Piotr Szczesniak notifications@github.com\nwrote:\n\ncc @vishh https://github.com/vishh\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/366#issuecomment-114563168\n.\n. Can we drop this in favor of a autoscaling specific sink?\n. Swarm support doesn't exist as of now. Swarm doesn't seem to provide a\nremote API to expose nodes. We can exec 'swarm list' and parse its output\nto get a list of nodes. To begin with, we will need cadvisor running on all\nnodes though.\n\nIs this something you are interested in working on?\nOn Tue, Jun 23, 2015 at 8:26 PM, Yoanis Gil notifications@github.com\nwrote:\n\nHi,\nI was wondering if heapster could collect metrics from a swarm cluster? If\nyes, how would that work? Maybe we can put together a quick tutorial\nexplaining how heapster up and running with Swarm.\nBests,\nYoanis.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/367.\n. Great. Let me know if you run into any issues.\n\nOn Wed, Jun 24, 2015 at 11:22 AM, Yoanis Gil notifications@github.com\nwrote:\n\nYes I am totally in for it. I will look into swarm API to see if there is\nsomething better than doing a swarm list and parsing the ouput ;)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/367#issuecomment-114970536\n.\n. A doc PR will benefit everyone! Please do send it over. A compose file also\nsounds very useful :)\nA single hosts file is what heapster expects. In Influxdb, the host is just\na column as of now. There is one series per metric.\n\nOn Wed, Jun 24, 2015 at 8:43 PM, Yoanis Gil notifications@github.com\nwrote:\n\nHi @vishh https://github.com/vishh\nI think it might be a good idea to update the sources documentation with\nan example of what the hosts files should look like for cAdvisor. Initially\nI though it was a host per file but I was so wrong ;). After reading a bit\nI cam with this:\n{\n\"Items\": [\n{\n\"Name\": \"N0\",\n\"IP\": \"192.168.99.105\"\n},\n{\n\"Name\": \"N0\",\n\"IP\": \"192.168.99.105\"\n}\n]\n}\nLet me know if you want me to create a PR for this ;)\nCan you please let me know how heapster will work for those two sources\nabove? When I go to InfluxDB and run list series it seems like metrics\nare global to the cluster and not specific to each individual host. Is that\nabout right? Finally I think we should put together a docker-compose file\nwhich can help people up and running in no time, which is specially useful\nif you're a dev ;)\nBests,\nYoanis.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/367#issuecomment-115091855\n.\n. under 'deploy/'\n\nOn Thu, Jun 25, 2015 at 8:33 AM, Yoanis Gil notifications@github.com\nwrote:\n\nOk. I will try to send the PR today. In which directory should the\ndocker-composer file should live?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/367#issuecomment-115294785\n.\n. @yoanisgil: Any progress on this issue?\n. This is a known issue. Kubelet does not expose network stats. We will fix this as soon as Kubernetes v1.0 is rolled out. Network stats should be available for the node though.\n\ncc @dchen1107\n. Thats not expected. I will investigate! Thanks for reporting the issue.\nOn Thu, Jun 25, 2015 at 4:17 AM, Akash Agarwal notifications@github.com\nwrote:\n\nThanks for the reply !\nAssuming node means 'worker nodes where kubelet and proxy runs', I am not\ngetting any node specific network stats. Though I am getting cpu and memory\nbased stats.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/368#issuecomment-115213071\n.\n. Waiting for kubelet to expose pod level metrics. @timstclair is working on the kubelet APIs now.\n. @jimmidyson: A workaround for now SGTM. Let's get this working. The\ncadvisor stats we query directly should let us figure out the network usage\nof a pod's infra container. The infra container is tagged with special\ndocker labels.\n\nOn Mon, Nov 23, 2015 at 12:23 PM, Tim St. Clair notifications@github.com\nwrote:\n\nWhen do you aim for a stable stats v2 endpoint in kubernetes?\nI'd like to deprecate the /stats endpoint, and put any data we need going\nforward into the metrics API. In order to move heapster over to the new API\nfor v1.2, we need to have the new API in beta in a couple weeks. To this\nend, I'm only planning on including the minimum stats requirements for\nheapster.\nWhat do you think about just adding in network stats to existing heapster\nAPI while we wait for @timstclair https://github.com/timstclair's PR &\nany subsequent stabilisation of the new metrics APIs?\nWhich network stats are you looking for? It looks like heapster already\nhas\nhttps://github.com/kubernetes/heapster/blob/master/sinks/api/supported_metrics.go\nnetwork/{tx,rx}{,_errors}. I just had a meeting this morning where we\nagreed to remove a lot of the more detailed networking stats from the\nmetrics API. cc/ @mwielgus https://github.com/mwielgus\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/368#issuecomment-159053284\n.\n. SO we pull all the containers from cadvisor already. We filter out the\nsystem containers using special labels. We can similarly filter out pod\ncontainers.\n\nInfact, we can get avoid accessing kubelet APIs completely, if we had a map\nbetween kube pod container name to docker container ID.\nLook here:\nhttps://github.com/kubernetes/heapster/blob/master/sources/datasource/kubelet.go#L180\nOn Fri, Dec 4, 2015 at 8:42 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nSo this is a bit more involved than I thought it would be. AFAIK there is\nno way to query the infra container in a pod with the current kubelet API &\nas the network metrics are associated only to the infra container that\nmeans there is no way to get the network metrics.\nI guess this will have to wait for the new metrics APIs.\n@vishh https://github.com/vishh Does that sound right to you or can you\nthink of a simple way to work around this without waiting for new metrics\nAPIs?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/368#issuecomment-162015830\n.\n. LGTM. Can you squash the commits?\n. 1 through 3 is completed. 4 is still pending.\n. ok to test\n. cc @vmarmol @a-robinson\n. ok to test\n\nOn Thu, Jun 25, 2015 at 8:00 PM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/374#issuecomment-115481018\n.\n. Just one comment. Otherwise LGTM.\n. Ack. Just ping the PR once cAdvisor service is added!\n. Thanks @yoanisgil! Yes, we should place a link to the compose README from the main page.\n. Will merge once the e2e passes.\n. @cadvisorJenkinsBot: test this please\n. LGTM. Merging. Sorry for the delay @yoanisgil. I was gone for a few weeks.\n. ok to test\n. Overall LGTM. @vmarmol should be able to help you out while I am gone @piosz\n. @piosz: Can you post a snapshot of the data in GCM? Since we don't have any e2e for this sink yet, manual verification will help.\n. Thanks. Lgtm\n. @rjnagal @vmarmol \n. @mikedanese: Can you take a look at the PR once?\n. Thanks for the review @mikedanese!\n. cc @vmarmol \n. Overall LGTM. Sorry for the delay in reviewing this PR. This needs a rebase as well. \n. And do squash your commits, once you have gone through the comments.\n. @jfoy did you get a chance to work on this PR?\n. @jfoy: Ping.\n. @jfoy: This is amazing! Can't wait to try it out. Can you post some documentation on how users can use Reimann with heapster? Maybe some sample alert configs.\n\nOther than documentation, everything else LGTM.\n. @cadvisorJenkinsBot: test this please\n. @jfoy: Any updates on this PR? I can help address the godeps issue if you are busy. \n. @jfoy: godeps can be a pain at times. I went ahead and fixed godeps on HEAD and also included your patches in the process. I will open a separate PR that will include your patches in this PR. Sorry for the trouble with godeps.\n. Reverting the config changes SGTM.\nOn Wed, Jul 1, 2015 at 5:13 AM, Piotr Szczesniak notifications@github.com\nwrote:\n\nWhen I start Heapster in version 0.15.0 (or from HEAD) and GCM sink\nenabled I get an error:\nE0701 11:44:50.095197       1 heapster.go:59] Get https://www.googleapis.com/cloudmonitoring/v2beta2/projects/marine-bison-90112/metricDescriptors: x509: failed to load system roots and no roots provided\nand Heapster is crashlooping.\nThere is a problem with SSL, probably after #376\nhttps://github.com/GoogleCloudPlatform/heapster/pull/376 is merged.\nThere are few possible solutions:\n- use other image as a base for Heapster one like\n  justicefries/busybox-ssl\n- add the same commands to Heapster Dockerfile\n- revert part of the mentioned PR which mount ssl certs\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/381.\n. retest this please\n. Yes.\n\nOn Fri, Jul 31, 2015 at 2:57 PM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nThe last jenkins job succeeded. Should we merge?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/389#issuecomment-126822683\n.\n. Kubelet exposes a read only port that exposes the '/stats' endpoint. Can you try accessing via that port?\n. Upgrade heapster or kubernetes? Can you clarify your question?\n. Can you provide the output of list series from InfluxDB?\n\nOn Thu, Jul 16, 2015 at 8:17 AM, Chenglin Huang notifications@github.com\nwrote:\n\nI meet some problems when run heapster on coreos cluster, the cluster\nconsist of three nodes(192.168.56.100, 192.168.56.101, 192.168.56.102),\neach nodes:\nCoreOS 681.2.0\nfleet 0.10.2\netcd 2.0.10\ncAdvisor pulls from google/cadvisor:0.16.0\ncAdvisor runs on all nodes, each instance works fine, and can open this\nurl http://192.168.56.100:8080/containers/ to view current status.\n/usr/bin/docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:0.16.0\nInfluxdb pulls from kubernetes/heapster_influxdb:latest and runs on\n192.168.1.103.\ndocker run -p 8083:8083 -p 8086:8086 --name influxdb kubernetes/heapster_influxdb:latest\nHeapster pulls from kubernetes/heapster:v0.16.0 and runs on 192.168.1.103.\n/usr/bin/docker run -p 8082:8082 -v /home/xsyr/workspace/coreos/hosts:/var/run/heapster/hosts --name heapster kubernetes/heapster:v0.16.0 --source='cadvisor:external?cadvisorPort=8080' --sink='influxdb:http://192.168.1.103:8086' --vmodule==4\nheapster\nI0716 15:02:47.627800       1 heapster.go:52] /heapster --source=cadvisor:external?cadvisorPort=8080 --sink=influxdb:http://192.168.1.103:8086 --vmodule==4\nI0716 15:02:47.628881       1 heapster.go:53] Heapster version 0.16.0\nI0716 15:02:47.634828       1 driver.go:376] created influxdb sink with options: {root root 192.168.1.103:8086 k8s false}\nI0716 15:02:47.635100       1 manager.go:84] starting to scrape data from sources\nI0716 15:02:47.636089       1 manager.go:66] attempting to get data from source \"Cadvisor Source\"\nI0716 15:02:47.663530       1 heapster.go:64] Starting heapster on port 8082\nI0716 15:02:47.723434       1 manager.go:95] completed scraping data from sources. Errors: []\nI0716 15:02:47.724133       1 external.go:80] Storing Events to \"InfluxDB Sink\"\nI0716 15:02:47.724140       1 external.go:76] Storing Timeseries to \"InfluxDB Sink\"\nI0716 15:02:47.725534       1 driver.go:234] flushed stats to influxDB\n...\nI0716 15:02:57.724394       1 manager.go:84] starting to scrape data from sources\nI0716 15:02:57.724507       1 manager.go:66] attempting to get data from source \"Cadvisor Source\"\nI0716 15:02:57.821446       1 manager.go:95] completed scraping data from sources. Errors: []\nI0716 15:02:57.822052       1 external.go:76] Storing Timeseries to \"InfluxDB Sink\"\nI0716 15:02:57.822064       1 external.go:80] Storing Events to \"InfluxDB Sink\"\nI0716 15:02:57.823481       1 driver.go:234] flushed stats to influxDB\n...\nI use wireshark to capture the network traffic, and see heapster scrape\ndata from each cadvisor instance every 10s, by call this url\nhttp://192.168.56.102:8080/api/v1.3/subcontainers.\nThis is the validation informantion of heapster:\n$ curl http://localhost:8082/validate\nHeapster Version: 0.16.0\nSource type: Cadvisor\n    NodeList: {Items:map[host1:{PublicIP:192.168.56.100 InternalIP:192.168.56.100 ExternalID:} host2:{PublicIP:192.168.56.101 InternalIP:192.168.56.101 ExternalID:} host3:{PublicIP:192.168.56.102 InternalIP:192.168.56.102 ExternalID:}]}\n    External Nodes plugin: hosts are\n map[host1:{192.168.56.100 192.168.56.100 } host2:{192.168.56.101 192.168.56.101 } host3:{192.168.56.102 192.168.56.102 }]\nExternal Sinks\n    Exported metrics:\n        uptime: Number of milliseconds since the container was started      cpu/usage: Cumulative CPU usage on all cores        cpu/limit: CPU limit in millicores      memory/usage: Total memory usage        memory/working_set: Total working set usage. Working set is the memory being used and not easily dropped by the kernel      memory/limit: Memory limit      memory/page_faults: Number of page faults       memory/major_page_faults: Number of major page faults       network/rx: Cumulative number of bytes received over the network        network/rx_errors: Cumulative number of errors while receiving over the network     network/tx: Cumulative number of bytes sent over the network        network/tx_errors: Cumulative number of errors while sending over the network       filesystem/usage: Total number of bytes consumed on a filesystem        filesystem/limit: The total size of filesystem in bytes\n    Exported labels:\n        hostname: Hostname where the container ran      host_id: Identifier specific to a host. Set by cloud provider or user       container_name: User-provided name of the container or full container name for system containers        pod_name: The name of the pod       pod_id: The unique ID of the pod        pod_namespace: The namespace of the pod     namespace_id: The UID of namespace of the pod       labels: Comma-separated list of user-provided labels        resource_id: Identifier(s) specific to a metric\n    External Sinks:\nSink Type: InfluxDB\nclient: Host \"192.168.1.103:8086\", Database \"k8s\"\nNumber of write failures: 0\nThe probrem is heapster don't write the data into influxdb.\nDid I do something wrong?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/419.\n. ok to test\n. LGTM cc @afein \n. retest this please\n. Cluster bring up is failing :( probing deeper.\n\nOn Tue, Jul 21, 2015 at 11:46 AM, Victor Marmol notifications@github.com\nwrote:\n\nJenkins is failing :(\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/426#issuecomment-123436422\n.\n. LGTM\n. retest this please\n. LGTM\n. cc @mikedanese \n. A new version will be released tomorrow. @mikedanese: Will running heapster on namespaces other than \"default\" cause cert issues?\n. You don't have to mention \"useServiceAccount\" with latest kubernetes releases. As long as you setup ServiceAccounts in the cluster, kubernetes will auto mount the certs and let heapster communicate with api-server. cc'ing @mikedanese who is the in-house expert on service accounts.\n. Can you try with the latest release v0.17.0?\n\nOn Tue, Jul 28, 2015 at 6:29 PM, Alexey Kupershtokh \nnotifications@github.com wrote:\n\nI've just followed instructions from GoogleCloudPlatform/kubernetes#11000\n(comment)\nhttps://github.com/GoogleCloudPlatform/kubernetes/issues/11000#issuecomment-120396771\nand updated apiserver options this way:\n--- a/cluster/ubuntu/util.sh\n+++ b/cluster/ubuntu/util.sh\n@@ -212,6 +212,9 @@ KUBE_APISERVER_OPTS=\"--address=0.0.0.0 \\\n --logtostderr=true \\\n --service-cluster-ip-range=${1} \\\n --admission_control=${2} \\\n+--client-ca-file=/srv/kubernetes/ca.crt \\\n+--tls-cert-file=/srv/kubernetes/server.cert \\\n+--tls-private-key-file=/srv/kubernetes/server.key \\\n --service_account_key_file=/tmp/kube-serviceaccount.key \\\n --service_account_lookup=false \"\n EOF\nNow the certificate seems valid to the heapster. Phew.\nBut I got another problem:\nE0729 01:28:14.393329       1 reflector.go:136] Failed to list api.Node:\nthe server has asked for the client to provide credentials (get nodes)\nE0729 01:28:14.395981       1 reflector.go:136] Failed to list api.Namespace:\nthe server has asked for the client to provide credentials (get namespaces)\nE0729 01:28:14.397820       1 reflector.go:136] Failed to list *api.Pod:\nthe server has asked for the client to provide credentials (get pods)\nIs this something related to service accounts?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/429#issuecomment-125798911\n.\n. cc @mikedanese \n. Thanks for the fix @mvdan. Much appreciated!\n. LGTM\n. LGTM\n. @afein: Can you rebase this PR?\n. LGTM. I will let @mvdan do a final review before merging this.\n. @cadvisorJenkinsBot: retest this please\n. @afein: Can you fix this PR soon? I intend to make a new release of heapster today.\n. Self merging. \n. There is a different integration path for GKE and Google Cloud Monitoring via Stackdriver. We are in the process of opening this up to users. Meanwhile, if you are interested in previewing it, @a-robinson can provide you access. \n. Its happening every 3 hours. We should check the API server logs to see why\nthe watch fails periodically. This should not affect heapster's\nfunctionality though.\n\nOn Thu, Jul 30, 2015 at 7:17 AM, Lucas Botelho Coelho \nnotifications@github.com wrote:\n\n@a-robinson https://github.com/a-robinson Yes, its when i started. Now\nare logging this:\nE0730 02:03:58.099822       1 reflector.go:183] watch of api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [3780/3712]) [4779]\nE0730 05:05:50.899511       1 reflector.go:183] watch of api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [4989/4781]) [5988]\nE0730 08:04:55.667831       1 reflector.go:183] watch of api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [6182/5989]) [7181]\nE0730 11:06:36.157589       1 reflector.go:183] watch of api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [7391/7182]) [8390]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/439#issuecomment-126344118\n.\n. @afein: Did an initial pass. This PR could have been split into several smaller ones for easier/better code review. \n. @cadvisorJenkinsBot: retest this please\n. cc @dchen1107\n. What version of heapster are you using and what setup?\n\nOn Fri, Jul 31, 2015 at 2:39 AM, fzu-huang notifications@github.com wrote:\n\nmonitoring-heapster-controller-1g681 0/1 API error (404): Cannot start\ncontainer 401801b72ef28b28f01bda500784d080b5dacfa6077c660e7b978b8ae7a5279a:\n[8] System error: exec: \"/heapster\": stat /heapster: no such file or\ndirectory\nwhat's wrong??\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/443.\n. You need to setup service accounts while setting up the cluster. Refer to\nkubernetes admin guides.\n\nOn Mon, Aug 3, 2015 at 1:42 AM, fzu-huang notifications@github.com wrote:\n\nversion 0.17\ni switch the version to 0.16 \uff0cthen when i create a rc, it is wrong and log\nis :\nI0803 08:35:28.200069 1 heapster.go:52] /heapster --source=kubernetes:\nhttps://kubernetes --sink=influxdb:http://monitoring-influxdb:80\nI0803 08:35:28.200157 1 heapster.go:53] Heapster version 0.16.0\nE0803 08:35:28.200587 1 heapster.go:59] open /var/run/secrets/\nkubernetes.io/serviceaccount/token: no such file or directory\nshould i do something in kubernetes???\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/443#issuecomment-127165064\n.\n. @bh016088: https://github.com/kubernetes/heapster/blob/master/docs/source-configuration.md#kubernetes\n. @mikedanese: Can you take a look at this issue? Will heapster docker images have to be built on a GCE machine in-order to avoid this issue?\n. ok to test\n\nOn Fri, Jul 31, 2015 at 11:49 AM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/445#issuecomment-126782328\n.\n. cc @mikedanese \n. This is likely an issue. I don't think heapster purges pods whenever it\nnotices a delete event from Kube API server currently.\n\nOn Fri, Jul 31, 2015 at 12:04 PM, Alex Robinson notifications@github.com\nwrote:\n\nI'll look into the root cause, but I'm seeing this behavior on 0.15.0. Any\nidea of whether it's been fixed since would be helpful.\n@vishh https://github.com/vishh @vmarmol https://github.com/vmarmol\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/446.\n. This can cause memory leaks since the internal metrics store does not garbage collect once we stop writing metrics. I am working on fixing this now.\n. Thanks for the cleanup @mvdan. LGTM\n. retest this please\n. LGTM\n. LGTM\n. #445\n. @mvdan: That's correct. Can you update this PR by cloning #445 into the standalone config as well? \n. Did you try running this test manually in your cluster? That will give you an opportunity to see why its failing.\n. LGTM. Waiting for jenkins.\n. Thanks @mvdan \n. Unit tests are yet to be updated.\n. Ping @afein @saad-ali \n. Thanks for the review @afein \n. Can you post the output of kubectl get pods,rc,svc --namespace-all?\n. Oh my bad. It's kubectl get pods,svc,rc --all-namespaces\n. This is an issue with your kubernetes cluster. Latest clusters are expected to have a kube-system namespace created by default. I would suggest going through kubernetes admin guides and figuring out why the kube-system namespace is missing. Alternatively, you can also create kube-system namespace, or update heapster config to run it in the default namespace.\n. Just replace 'kube-system' with 'default' in all the config files. \n. It will continue to talk to the api server and recreate pods and containers once the node is online. cache is merely a routing entity.\n. Thanks for clarifying @afein. Will address it in a separate PR.\n. That's correct. I wonder why kubernetes is not populating the node internal\nIP for your cluster.\n\nOn Wed, Aug 5, 2015 at 3:19 AM, Tim Wu notifications@github.com wrote:\n\nI run the heapster on host directly, and it get the right NodeInternalIP\nI0805 18:17:02.318569   23839 kube.go:117] kube nodes found: &{Items:map[node2:{PublicIP:192.168.3.148 InternalIP:192.168.3.148 ExternalID:node2}\nIs this code[1] gets some problem inside the conatiner?\naddrs, err := net.LookupIP(hostname)\n[1]\nhttps://github.com/GoogleCloudPlatform/heapster/blob/v0.17.0/sources/nodes/kube.go#L82\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/455#issuecomment-127948973\n.\n. I think yes. Can you post a PR to fix that?\n\nOn Wed, Aug 5, 2015 at 6:13 PM, Tim Wu notifications@github.com wrote:\n\nBecause my Kubernetes's cloud providers is nil, the code[1] only set\nNodeLegacyHostIP.\nIs that ok to add NodeInternalIP?\n[1]\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/kubelet/kubelet.go#L1974\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/455#issuecomment-128199488\n.\n. @wulonghui: Closing this issue, since its fixed in Kubernetes\n. Just a few comments. Otherwise LGTM\n. ok to test\n. LGTM. Thanks for the PR @mwielgus \n. ok to test\n. Thanks for the cleanup @mwielgus :+1: \n. LGTM\n. Excluding generated files SGTM. Why not delete and regenerate extpoints? With this PR will extpoints.go be regerated whenever that package is modified?\n. I meant chnages to heapster extpoints/interface.go which should result in\nregenerating extpoints/ expoints.go. Vendoring go-extpoints makes sense.\n\nOn Mon, Aug 10, 2015 at 10:25 AM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nSee my commit messages - go generate is not supposed to be run by the\nuser, but by the developer(s).\nWhen what package is modified, go-extpoints or heapster? At the moment,\nneither. It is up to whomever changes the go-extpoints input to re-run\nthe tool. And if the tool ever changes, which it shouldn't do often,\nsomeone would eventually run the updated tool and notice.\nI thought about making travis run go generate and check if it is up to\ndate. I could bundle that with a go-extpoints in Godep/ so that it never\nbreaks in the future. What do you think?\nSide note: master currently has this problem. If go-extpoints breaks,\nheapster breaks, because we just go get it via master.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/459#issuecomment-129537406\n.\n. retest this please\n. ok to test\n. Thanks for the update @mikedanese \n. LGTM. \n. Merging this to fix tests.\n. retest this please\n. @mvdan #462 should fix jenkins. \n. retest this please\n. Heapster is having trouble reaching the GCE metadata server. Can you try\nwith the config here:\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/master/cluster/addons/cluster-monitoring/google/heapster-controller.yaml\n?\n\nOn Wed, Aug 12, 2015 at 6:34 AM, Dmitriy Lyalyuev notifications@github.com\nwrote:\n\nHi.\nI try to start Heapster pod on GCE node, but it fails with error \"the GCM\nsink is currently only supported on GCE\".\nFull docker log:\nI0812 13:13:07.687345       1 heapster.go:52] /heapster --source=kubernetes:'https://kubernetes?useServiceAccount=true&auth=' --sink=gcm --sink=gcl --poll_duration=2m --stats_resolution=1m\nI0812 13:13:07.687396       1 heapster.go:53] Heapster version 0.16.0\nI0812 13:13:07.687832       1 kube_factory.go:169] Using Kubernetes client with master \"https://10.179.240.1:443\" and version \"v1\"\nI0812 13:13:07.687847       1 kube_factory.go:170] Using kubelet port 10255\nI0812 13:13:11.439177       1 driver.go:93] created GCM sink\nE0812 13:13:11.439210       1 heapster.go:59] the GCM sink is currently only supported on GCE\nMy controller config:\n{\n    \"kind\": \"ReplicationController\",\n    \"apiVersion\": \"v1\",\n    \"metadata\": {\n    \"name\": \"monitoring-heapster-controller\",\n    \"labels\": {\n        \"name\": \"heapster\"\n    }\n    },\n    \"spec\": {\n    \"replicas\": 1,\n    \"selector\": {\n        \"name\": \"heapster\"\n    },\n    \"template\": {\n        \"metadata\": {\n        \"labels\": {\n            \"name\": \"heapster\"\n        }\n        },\n        \"spec\": {\n    \"volumes\": [\n        {\n        \"name\": \"monitoring-token\",\n        \"secret\": {\n            \"secretName\": \"heapster-token-z4pni\"\n        }\n        }\n    ],\n        \"containers\": [\n            {\n            \"name\": \"heapster\",\n            \"image\": \"kubernetes/heapster:0.17.0\",\n            \"command\": [\n                \"/heapster\",\n                \"--source=kubernetes:'https://kubernetes?useServiceAccount=true&auth='\",\n                \"--sink=gcm\",\n                \"--sink=gcl\",\n                \"--poll_duration=2m\",\n                \"--stats_resolution=1m\"\n            ],\n        \"volumeMounts\": [\n            {\n        \"name\": \"monitoring-token\",\n        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n        \"readOnly\": true\n            } ]\n            }\n        ]\n        }\n    }\n    }\n}\nIt works before I reinstall node in cluster. I tried to start 0.16.0 and\n0.17.0, but no success with same errors. Other pods working fine.\nCan you help me with start it?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/466.\n. Thanks for the PR @mwielgus! \n. ok to test\n. LGTM. Merging this for now, until a better the new APIs are added. \n. Super useful PR! Thanks @afein! Much appreciated!\n. cc @piosz \n. Thanks for the tip @jimmidyson! Updated the PR. PTAL.\n. Merging this. @jimmidyson If there are any other issues, I will address them in a separate PR.\n. @afein: Just a few comments. Overall LGTM! Good work!\n. LGTM\n. @afein: Wanna give it a shot? ;)\n. @mwielgus: Has this issue been resolved by your recent PRs?\n. @piosz: This is a proposal for a more long term API. Kindly take a look.\n. Why add the new cadvisor deps?\n. Thanks for the update Alex! Looking forward to the PRs!\n\nOn Tue, Aug 25, 2015 at 4:42 PM, Alexandros Mavrogiannis \nnotifications@github.com wrote:\n\nQuick Update:\nThis PR is almost fully operational. I've had to change the behavior of a\nfew components to comply with the new stores.\nAs soon as this PR is mergeable, I will start submitting separate smaller\nPRs that link to this one to facilitate the code review process.\nThe changes that will be included in this set of PRs are:\n- usage of the new StatStore and DayStore\n- static epsilon configuration\n- Uptime calculation bugfix\n- Unit test refactoring for compliance\n- renaming of the top-level model interface to \"Model\" instead of\n  \"Cluster\" to avoid confusion with the ClusterInfo struct\nFurthermore, based on preliminary testing, the memory usage of the\nHeapster Pod is now consistently between 30-40MB for a not-too-noisy\ncluster. Using the previous stores, the usage was 200MB instead, leading to\nan improvement of 5x (rough estimate).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/478#issuecomment-134771599.\n. Thanks for the PR @mikedanese! \n. LGTM. Thanks @piosz \n. The intention is to abstract storage backends. The backends can be InfluxDB, or hawkular or GCM. \nThe maintenance overhead is a valid concern. As of now the intention is to only support a few required backends (as mentioned above). The scope is limited to kubernetes metrics data required for various components like horizontal auto scaler, initial limits controller, scheduler, etc.\nHaving small microservices makes sense on the outset, but it also adds deployment and maintenance overhead. Since heapster is the one that controls the storage schema, having heapster read back the data makes it easy to guarantee API semantics. To avoid the query resource consumption issues, we can consider adding resource constraints per-query. Thanks for bringing this up. The design for the Sink read interface should try to minimize memory overhead. \n. One more points: The initial use cases demand access to usage histograms only. We can expand the scope to raw historical metrics if required in the future.\n@a-robinson: This should reduce the memory overhead per query.\n. cc @piosz @mwielgus\n. #478 \n. #478 \n. #478 \n. Can you open a separate issue for handling limits?\n\nOn Fri, Aug 21, 2015 at 2:08 PM, Alexandros Mavrogiannis \nnotifications@github.com wrote:\n\nAs of now, limits are being treated as metrics and they are, therefore,\nstored in a DayStore\nI propose that, initially, we should utilize the same epsilon for usages\nand limits.\nThen, after the current set of P0 issues is completed, we can edit the\nmodel to handle limits as constants, rather than timeseries.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/486#issuecomment-133564170\n.\n. @afein: Has this issue been resolved?\n. Thanks @afein. Feel free to file an issue if you have some thoughts on\ndynamic epsilon config.\n\nOn Tue, Sep 8, 2015 at 10:17 PM, Alexandros Mavrogiannis \nnotifications@github.com wrote:\n\nYes, static epsilon configuration was resolved as part of #508\nhttps://github.com/kubernetes/heapster/pull/508.\nMore dynamic options could be explored for future milestones, if that is\ndeemed useful.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/486#issuecomment-138787663\n.\n. @afein @piosz @mwielgus Is anyone planning on working on this?\n. Thanks for raising this issue! The answer is yes. Heapster already calculates instantaneous values. It should be straight forward to push instantaneous metrics. I would suggest sticking to the best available resolution.\nThe reason for storing cumulative is to give users the ability to shape the graph however they want. \nWill you be able to post a PR @dotNetDR ?\n. Yes. The implementation is currently local to GCM sink. We can move the\ninstantaneous metrics to be top level metrics instead.\n\nOn Sun, Aug 23, 2015 at 8:20 PM, dotNetDR_ notifications@github.com wrote:\n\nThank you for your reply.\nI don't know how to find instantaneous values in that file.\nis /sinks/gcm/core.go\nhttps://github.com/kubernetes/heapster/blob/release-0.16.0/sinks/gcm/core.go#L286\n?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/488#issuecomment-134002959\n.\n. Thanks for reporting the issue @mwielgus. What version of heapster is this?\n. cc @mwielgus \n. @kubernetes/heapster-maintainers \n. Update docs @mvdan \n. @cadvisorJenkinsBot: add to whitelist\n. Thanks for the PR @mwielgus! LGTM. \n. @mvdan: Itchy to get rid of Go 1.4 huh :)\n\nOn Thu, Sep 17, 2015 at 3:08 PM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nUpdate - kubernetes is not ready yet. See the races still present that\nwere found in kubernetes/kubernetes#13838\nhttps://github.com/kubernetes/kubernetes/pull/13838\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/495#issuecomment-141248007\n.\n. @kubernetes/heapster-maintainers \n. Done\n. ok to test\n\nOn Tue, Aug 25, 2015 at 3:36 PM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/498#issuecomment-134761589.\n. @cadvisorJenkinsBot: ok to test\n. cc @afein \n. We are waiting on the issues that are part of this\nmilestone to be resolved before making a new release.\n\nOn Tue, Sep 8, 2015 at 8:01 PM, Alexandros Mavrogiannis \nnotifications@github.com wrote:\n\nThe /stats endpoints were merged today in master, so you should be able to\naccess the full feature set by building the current HEAD.\nI am not aware about the exact date of the next release (\\cc @vishh\nhttps://github.com/vishh), but any further changes until that release\nwould most probably involve internal quality\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/501#issuecomment-138762644\n.\n. I would suggest providing 300MB of memory and 0.2 cores for Heapster to\nbegin with. If you see heapster getting closer to these limits, let me\nknow. Disk space should not be of concern since heapster is currently\nstateless.\n\nOn Thu, Sep 17, 2015 at 10:45 AM, Sanjana J Bhat notifications@github.com\nwrote:\n\n@afein https://github.com/afein and @vishh https://github.com/vishh,\nwhat are the recommended resource specifications for heapster container?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/501#issuecomment-141163798\n.\n. I'm closing this issue. @sanjana-bhat feel free to re-open if I failed to address any concerns.\n. #481 talks about providing access to data stored in different storage backends via a common API. I would like to integrate hawkular support for auto-scaling through this mechanism. \n. LGTM. Thanks for the update @mvdan!\n. @mvdan: Thanks for fixing the race. Which GC bug are you referring to?\n\nOn Fri, Aug 28, 2015 at 1:13 PM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh the GC bug is a serious regression in\nthe latest stable, perhaps a release is due?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/504#issuecomment-135877368.\n. Have you posted a PR?\n\nOn Fri, Aug 28, 2015 at 2:02 PM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nThere was no race, it was a bug in the garbage collection that I just\nfixed. That is what I meant with \"GC bug\". It's a serious regression\nbecause it removes all nodes at every runGC().\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/504#issuecomment-135887009.\n. @mvdan: Can you include just the makefile changes in this PR?\n. We can fix the flake. I can take a look at the failing test.\n\nOn Mon, Aug 31, 2015 at 10:20 AM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nMentioning the flake because it now happens almost always thanks to -race.\nIn the long run we want -race enabled and the flake fixed. In the short\nrun, if we can't fix the flake, we can hold off -race for now.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/504#issuecomment-136433754.\n. @afein: Replace uint16 to uint if possible. Otherwise just a few nits. Overall LGTM! Thanks!\n. LGTM. Will merge once jenkins is happy!\n. The label format for heapster and influxDB has changed. Kindly take a\nlook at the respective pod labels and edit the Grafana graph config\nappropriately. Sorry for the confusion. This was fixed upstream, but the\nchanges haven't been pushed to k8s v1.0.\n\nOn Sun, Aug 30, 2015 at 1:50 AM, Plamen Kalchev notifications@github.com\nwrote:\n\nHello,\nI've managed to run Heapster with InfluxDB and Grafana on my testing\nkubernetes cluster but I'm still missing something - I get the cluster\nstatistics, but not the pod's. Here is a screenshot from my dashboard:\nhttp://drive.cmailpro.net/~p.kalchev@icn.bg/protected/pwd/1fd655ce302b2e1b5427dd8faff985/Screen%20Shot%202015-08-30%20at%2011.37.56%20AM.png\nhttp://drive.cmailpro.net/%7Ep.kalchev@icn.bg/protected/pwd/1fd655ce302b2e1b5427dd8faff985/Screen%20Shot%202015-08-30%20at%2011.37.56%20AM.png\nAny idea what am I missing? Before switching to k8s 1.0 I was running\nheapster/influxdb/grafana without problems and all stats were displaying\ncorrectly.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/507.\n. Does this issue happen for cpu or memory?\n. Do you have access to the Influxdb UI? The UI is available on port 8083 on\nthe influxdb service. You might have to make that service public to use the\nUI.\nOnce you get to the UI, can you list all the distinct labels?\n\nOn Wed, Sep 2, 2015 at 7:25 AM, Plamen Kalchev notifications@github.com\nwrote:\n\nFor both, the error is the same: label doesn't exist. This is the pod:\n[root@k8s-master ~]# kubectl --namespace=kube-system describe pod monitoring-heapster-v8-c4ms9\nName:               monitoring-heapster-v8-c4ms9\nNamespace:          kube-system\nImage(s):           gcr.io/google_containers/heapster:v0.17.0\nNode:               .../...\nLabels:             k8s-app=heapster,kubernetes.io/cluster-service=true,version=v8\nStatus:             Running\nReason:\nMessage:\nIP:             10.16.2.5\nReplication Controllers:    monitoring-heapster-v8 (1/1 replicas created)\nContainers:\n  heapster:\n    Image:  gcr.io/google_containers/heapster:v0.17.0\n    Limits:\n      cpu:      100m\n      memory:       300Mi\n    State:      Running\n      Started:      Sat, 29 Aug 2015 16:56:14 +0300\n    Ready:      True\n    Restart Count:  0\nConditions:\n  Type      Status\n  Ready     True\nNo events.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/507#issuecomment-137100198\n.\n. Just a few questions / comments. Overall LGTM! Thanks @afein!\n. Just a couple of comments to be addressed @afein \n. Good catch @piosz! The race you mention does exist today.\n\nOn Thu, Sep 3, 2015 at 1:23 AM, Piotr Szczesniak notifications@github.com\nwrote:\n\nI still can see a problem here. Let's call scraping goroutine S and\npopulating goroutine P. Let's assume that poll_duration=1m and\nsink_frequency=2m.\nS scrapped already data for time inteval [10:00, 10:01] and started\nscraping for [10:01, 10:02]. Right after P started populating data for\n[10:00,10:02]. It found only the first part of the data, populated them and\nmarked [10:00, 10:02] interval as done. Right after that S provided data\nfor [10:01, 10:02] interval which will be never exported.\nIt can happen also in situation when poll_duration=sink_frequency\nSo there is no more races in terms @mvdan https://github.com/mvdan\nmentioned but some data may be lost.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/509#issuecomment-137376117\n.\n. LGTM.\n. ok to test\n. Feel free to self-merge @piosz!\n. LGTM. @piosz kindly test both on kubernetes and using a local cadvisor instance to validate this PR. The integration tests don't capture the cadvisor use case. Thanks!\n. Run a cadvisor locally and make heapster use an external source\n\nOn Wed, Sep 9, 2015 at 10:00 AM, Piotr Szczesniak notifications@github.com\nwrote:\n\nI've tested it on Kubernetes already. How can I test the cadvisor case?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/512#issuecomment-138975948.\n. Are the pods running on the other node? Is the other node in a Ready\nstate?\n\nOn Thu, Sep 3, 2015 at 6:59 AM, yuqi huang notifications@github.com wrote:\n\nI have a k8s cluster with one master and two nodes. I found that the heapster\ncan only get metrics from the node where its pod resides on, but cannot get\nmetrics from other nodes.\nThe err log says:\nE0903 21:38:21.235223       1 model_handlers.go:530] unable to get node metric: the requested entity does not have any metrics yet\n[{default/kubedash-mi08u 0 0} {default/jenkins5-84q70 0 0} {default/guestbook2-u6ryy 0 0} {default/jenkins2-h5xh7 0 0} {default/jenkins1-1d47t 0 0} {default/guestbook3-gp92t 0 0}]\nDoes anyone else encounter the same problem? Any help or suggestions will\nbe appreciated, thanks.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/516.\n. If the pod and containers exist and heapster is able to reach the kubelets,\nI don't expect any issues. As of now, heapster does not take into account\npod state from the Kubernetes API server. This might result in failures\nuntil the pod is running or while it is restarting.\n\nOn Fri, Sep 4, 2015 at 5:48 AM, yuqi huang notifications@github.com wrote:\n\nYes,all nodes works normally, some pods works on these nodes. I have\nentered the container, heapster can visit the kubelet's rest api of all\nnodes and get the metrics information.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/516#issuecomment-137727534\n.\n. Can you post heapster logs?\n\nOn Sat, Sep 5, 2015 at 3:24 AM, Alexander notifications@github.com wrote:\n\nI have the same issue. I have three nodes, but it shows metrics only from\ntwo of those.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/516#issuecomment-137939843\n.\n. You can run kubectl describe on the influxdb pod to figure out if it is\nrestarting. I notice a few issues.\n1. Service accounts have not been setup. You can take a look at the source\n   configuration docs in heapster repo to get more info about this.\n2. Watch for pods fails continuously. If heapster is not able to retrieve\n   pods, it will not collect any container stats belonging to pods.\n\nEnsuring time synchronization is also required.\nOn Tue, Sep 8, 2015 at 8:57 PM, yuqi huang notifications@github.com wrote:\n\n@NOX73 https://github.com/NOX73 Would you check the date of all the\nnodes? If the node's date is not synchronous with the node where the\nheapster resided, the heapster cannot collect the metrics normally.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/516#issuecomment-138772124\n.\n. LGTM. The long-term plan also sounds good. We can have a mux on top of the node and pod API objects, Thanks @mwielgus!\n. Can you post heapster logs? Grafana should be accessible via the service IP\n(192.168.3.90 in your case).\n\nOn Fri, Sep 4, 2015 at 8:15 AM, Naresht-vedams notifications@github.com\nwrote:\n\nI have a kubernetes single node cluster\nNode details\nkubectl get nodes\nNAME LABELS STATUS\n172.16.2.179 kubernetes.io/hostname=172.16.2.179 Ready\nI run this kubectl create -f deploy/kube-config/influxdb/\nkubectl -o wide get pods\nNAME READY STATUS RESTARTS AGE NODE\nheapster-j81xk 1/1 Running 0 4m 172.16.2.179\ninfludb-grafana-ltna7 2/2 Running 0 4m 172.16.2.179\nkubectl -o wide get services\nNAME LABELS SELECTOR IP(S) PORT(S)\nheapster kubernetes.io/cluster-service=true,kubernetes.io/name=Heapster\nk8s-app=heapster 192.168.3.142 80/TCP\nkubernetes component=apiserver,provider=kubernetes  192.168.3.1\n443/TCP\nmonitoring-grafana\nkubernetes.io/cluster-service=true,kubernetes.io/name=monitoring-grafana\nname=influxGrafana 192.168.3.90 80/TCP\nmonitoring-influxdb  name=influxGrafana 192.168.3.229 8083/TCP\n8086/TCP\nProblem:-\nI tried to find Grafana on localhost:3000 but no luck.\nI tried on localhost:8086 it displaying 404 page not found\nInformation:-\nInfluxdb is running on localhost:8083.\nOn 4194 port cAdvisor is running\nHow can I get grafana UI and apart from this, can I configure any thing\nelse.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/519.\n. Have you set up kube-dns?\n\nOn Sun, Sep 6, 2015 at 11:01 PM, Naresht-vedams notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh\ncurl http://172.16.2.179:8080/api/v1/namespaces\nfor above URL I am getting the response like this\n{\n  \"kind\": \"NamespaceList\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"selfLink\": \"/api/v1/namespaces\",\n    \"resourceVersion\": \"20840\"\n  },\n  \"items\": [\n    {\n      \"metadata\": {\n        \"name\": \"default\",\n        \"selfLink\": \"/api/v1/namespaces/default\",\n        \"uid\": \"9246d446-5389-11e5-a511-0800272179e9\",\n        \"resourceVersion\": \"5\",\n        \"creationTimestamp\": \"2015-09-05T04:50:03Z\"\n      },\n      \"spec\": {\n        \"finalizers\": [\n          \"kubernetes\"\n        ]\n      },\n      \"status\": {\n        \"phase\": \"Active\"\n      }\n    }\n  ]\n}\nBut in heapster logs , it is trying to connect like\nhttps://kubernetes/api/v1/namespaces this so it is giving error. what I\nhave to do to fix this.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/519#issuecomment-138196932\n.\n. Closing this issue.\n. +1. In general we need a way for handling the lifecycle of entities like\npods, nodes and containers and efficiently communicating that across all\nthe internal modules.\nThe cache is meant to just be a temporary storage of data. It is not\nexpected to be the source for lifecycle events.\n\nOn Fri, Sep 4, 2015 at 1:08 PM, Marcin Wielgus notifications@github.com\nwrote:\n\nRight now there is cache, model storage and store inside podApi's\nreflector.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/520.\n. Yeah. What do you think about separating out lifecycle events from the\ncache? The cache can be a consumer of the lifecycle events.\n\nOn Fri, Sep 4, 2015 at 2:24 PM, Marcin Wielgus notifications@github.com\nwrote:\n\nUnfortunately, the cache is used to populate the model storage so right\nnow it is in fact a source/transmitter of pod lifecycle events.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/520#issuecomment-137856710\n.\n. cc @afein \n. LGTM. Thanks @afein for handling all these features! Much appreciated!\n. Are you running using the config from heapster repo? If so, can you try\nusing admin:admin as described here?\n\nOn Tue, Sep 8, 2015 at 1:46 AM, Naresht-vedams notifications@github.com\nwrote:\n\nhttp://172.16.2.92:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana\nWhen we browse through the above URL it is asking login credentials as I\nprovided username and password from ~/.kube/config,no response when we try\nto login\ncat ~/.kube/config\napiVersion: v1\nclusters:\n- cluster:\n  insecure-skip-tls-verify: true\n  server: http://172.16.2.92:8080\n  name: ubuntu\n  contexts:\n- context:\n  cluster: ubuntu\n  user: ubuntu\n  name: ubuntu\n  current-context: ubuntu\n  kind: Config\n  preferences: {}\n  users:\n- name: ubuntu\n  user:\n    password: bmm4uNESv2rOAOYy\n    username: admin\nDoes anyone else encounter the same problem? Any help or suggestions will\nbe appreciated, thanks.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/526.\n. Are you able to view the Grafana UI? The login credentials I mentioned is\nfor Grafana itself. If you are having trouble accessing the API server\nproxy, you can try making the Grafana kubernetes service public.\n\nOn Wed, Sep 9, 2015 at 7:10 AM, Naresht-vedams notifications@github.com\nwrote:\n\nYes I am running the config from heapster repo....I tried admin:admin, but\nno luck......\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/526#issuecomment-138921666\n.\n. LGTM. Thanks @mwielgus \n. LGTM. Thanks for the fix!!\nOn Sep 9, 2015 6:17 AM, \"Marcin Wielgus\" notifications@github.com wrote:\ncc: @vishh https://github.com/vishh\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/531#issuecomment-138906218.\n. Thanks for the fix!!!\n\nOn Wed, Sep 9, 2015 at 7:15 AM, Piotr Szczesniak notifications@github.com\nwrote:\n\nMerged #532 https://github.com/kubernetes/heapster/pull/532.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/532#event-404865192.\n. LGTM\n. Yes please!\n\nOn Thu, Sep 10, 2015 at 4:21 PM, yuqi huang notifications@github.com\nwrote:\n\nRecently,I tested to run heapster standalone on a machine. It can work\nnormally. Shall I take a PR to consummate the doc about this?\nhttps://github.com/kubernetes/heapster/blob/master/docs/standalone.md\nThanks.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/535.\n. LGTM. Thanks for the PR @huangyuqi!\n. Graphite support SGTM. I can help you with the integration!\n\nOn Fri, Sep 11, 2015 at 10:18 AM, Yuvi Panda notifications@github.com\nwrote:\n\nAlso unsure if it should be a graphite sink or a statsd sink. Will\ninvestigate heapster more and update issue.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/538#issuecomment-139605203\n.\n. add to whitelist\n. @piosz: SGTM. \n. LGTM. Thanks @piosz \n. LGTM. Thanks @afein \n. LGTM. Thanks for the cleanup @mwielgus \n. This is a very useful refactoring. In addition to k8s, it will help\nintegrating with docker and possibly other data sources.\n\nOn Mon, Sep 14, 2015 at 3:11 AM, Piotr Szczesniak notifications@github.com\nwrote:\n\nI should do it during Kubernetes \"code freeze\" for 1.1.\ncc @mwielgus https://github.com/mwielgus @vishh\nhttps://github.com/vishh\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/542#issuecomment-140027545\n.\n. Any suggestions for improving error propagation? We can encode the line\nnumber or possibly log all errors as soon as they are noticed.\n\nOn Mon, Sep 14, 2015 at 4:29 AM, Marcin Wielgus notifications@github.com\nwrote:\n\nIt is not clear what is going on here:\nI0914 12:20:11.431571    2208 heapster_api_test.go:111] built and pushed heapster image\n--- FAIL: TestHeapster (70.96s)\n        Location:       heapster_api_test.go:469\n    Error:      No error is expected but got unable to modify replica count for rc heapster: replicationControllers \"heapster\" not found\nFAIL\nexit status 1\nI0914 09:46:09.983698   26455 framework.go:280] Successfully setup new kubernetes cluster version 1.0.1\n--- FAIL: TestHeapster (498.92s)\n    assertions.go:154:\n```\nLocation:   heapster_api_test.go:469\nError:      No error is expected but got expected non zero timeseries\n```\nFAIL\nexit status 1\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/543.\n. Looks like events do not have UID set. Switching to using names which are unique.\n. Here\nis the relevant code. Kubernetes uses event Name to identify events.\nArguably this can be fixed. But until it is fixed, heapster needs to work\naround that. Timestamps are not an issue since if it is not present, it\ndoes not affect heapster. It might confuse users though. So it has to be\nfixed in Kubernetes side as well.\n\nGenerating UIDs in heapster is not possible since heapster is stateless. If\nheapster were to retrieve the same event after a restart, it will assign\ndifferent UIDs. We can use a hash as a UID possibly.\nThe main use case of UIDs is to avoid duplicates while storing in sinks and\nin core heapster logic. Since these requirements are met by event.Name for\nnow, I chose to go with that. WDYT?\nOn Thu, Sep 17, 2015 at 8:05 AM, Marcin Wielgus notifications@github.com\nwrote:\n\nActually events do have UID set. I generated the similar event and it\nlooks OK. The above is not also lacking UID but also creationTimestamp - I\nwould suggest dropping broken events or giving them heapster generated uid\n- if uid & creation time is broken why should we assume that the name is\n  fine?\n{\n      \"kind\": \"Event\",\n      \"apiVersion\": \"v1\",\n      \"metadata\": {\n          \"name\": \"cassandra.1404cab927c4af5f\",\n          \"namespace\": \"default\",\n          \"selfLink\": \"/api/v1/namespaces/default/events/cassandra.1404cab927c4af5f\",\n          \"uid\": \"b014a366-5d49-11e5-bdca-42010af07fef\",\n          \"resourceVersion\": \"120398\",\n          \"creationTimestamp\": \"2015-09-17T14:37:57Z\",\n          \"deletionTimestamp\": \"2015-09-17T15:37:57Z\"\n      },\n      \"involvedObject\": {\n          \"kind\": \"Pod\",\n          \"namespace\": \"default\",\n          \"name\": \"cassandra\",\n          \"uid\": \"b00af05b-5d49-11e5-bdca-42010af07fef\",\n          \"apiVersion\": \"v1\",\n          \"resourceVersion\": \"120395\",\n          \"fieldPath\": \"implicitly required container POD\"\n      },\n      \"reason\": \"pulled\",\n      \"message\": \"Container image \\\"gcr.io/google_containers/pause:0.8.0\\\" already present on machine\",\n      \"source\": {\n          \"component\": \"kubelet\",\n          \"host\": \"kubernetes-minion-knk4\"\n      },\n      \"firstTimestamp\": \"2015-09-17T14:37:57Z\",\n      \"lastTimestamp\": \"2015-09-17T14:37:57Z\",\n      \"count\": 1\n  },\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/544#issuecomment-141115553\n.\n. More details on this issue:\nhttps://github.com/kubernetes/kubernetes/issues/14126\n\nOn Thu, Sep 17, 2015 at 9:38 AM, Vishnu Kannan vishnuk@google.com wrote:\n\nHere\nis the relevant code. Kubernetes uses event Name to identify events.\nArguably this can be fixed. But until it is fixed, heapster needs to work\naround that. Timestamps are not an issue since if it is not present, it\ndoes not affect heapster. It might confuse users though. So it has to be\nfixed in Kubernetes side as well.\nGenerating UIDs in heapster is not possible since heapster is stateless.\nIf heapster were to retrieve the same event after a restart, it will assign\ndifferent UIDs. We can use a hash as a UID possibly.\nThe main use case of UIDs is to avoid duplicates while storing in sinks\nand in core heapster logic. Since these requirements are met by event.Name\nfor now, I chose to go with that. WDYT?\nOn Thu, Sep 17, 2015 at 8:05 AM, Marcin Wielgus notifications@github.com\nwrote:\n\nActually events do have UID set. I generated the similar event and it\nlooks OK. The above is not also lacking UID but also creationTimestamp - I\nwould suggest dropping broken events or giving them heapster generated uid\n- if uid & creation time is broken why should we assume that the name is\n  fine?\n{\n      \"kind\": \"Event\",\n      \"apiVersion\": \"v1\",\n      \"metadata\": {\n          \"name\": \"cassandra.1404cab927c4af5f\",\n          \"namespace\": \"default\",\n          \"selfLink\": \"/api/v1/namespaces/default/events/cassandra.1404cab927c4af5f\",\n          \"uid\": \"b014a366-5d49-11e5-bdca-42010af07fef\",\n          \"resourceVersion\": \"120398\",\n          \"creationTimestamp\": \"2015-09-17T14:37:57Z\",\n          \"deletionTimestamp\": \"2015-09-17T15:37:57Z\"\n      },\n      \"involvedObject\": {\n          \"kind\": \"Pod\",\n          \"namespace\": \"default\",\n          \"name\": \"cassandra\",\n          \"uid\": \"b00af05b-5d49-11e5-bdca-42010af07fef\",\n          \"apiVersion\": \"v1\",\n          \"resourceVersion\": \"120395\",\n          \"fieldPath\": \"implicitly required container POD\"\n      },\n      \"reason\": \"pulled\",\n      \"message\": \"Container image \\\"gcr.io/google_containers/pause:0.8.0\\\" already present on machine\",\n      \"source\": {\n          \"component\": \"kubelet\",\n          \"host\": \"kubernetes-minion-knk4\"\n      },\n      \"firstTimestamp\": \"2015-09-17T14:37:57Z\",\n      \"lastTimestamp\": \"2015-09-17T14:37:57Z\",\n      \"count\": 1\n  },\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/544#issuecomment-141115553\n.\n. Noticed this again. \n. @mvdan: Can you make @googlebot happy?\n. @mvdan: Removed godeps for now. Sorry about losing comments. I don't see them in github history as well. Sigh :(\n. @mvdan: All your previous comments have been addressed.\n. Updated to v0.9.4. @piosz @mwielgus PTAL when you get a chance. If there are no comments, I'd like to get this merged soon.\n. rebased and fixed all comments.\n. Waiting for @piosz and @mwielgus to take a look before adding godeps commit.\n. I'm adding the godeps back in, to get this merged. cc @piosz @mwielgus\n. @piosz: Upgrading to v0.9 requires an intermediate upgrade to v0.8.9. The\nnext release of heapster (v0.19.0) will support InfluxDB v0.9.\n\n\nOn Fri, Sep 18, 2015 at 7:47 AM, Piotr Szczesniak notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh I took a look briefly and I have no\nobvious concerns. Please let me know what is a timeline with bumping the\nversion of Heapster.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/548#issuecomment-141473448.\n. @mvdan: Godeps have been added back. PTAL.\nThanks for reviewing @piosz \n. Oops. Fixed the build.\n. @mvdan: It is possible:\n1. If one of the PRs is changing the cluster version to test against, then that test run has to be exclusive.\n2. The runs should synchronize via a file lock (or something else) to avoid managing cluster lifetimes.\n3. Jenkins has to be reconfigured.\n\nGot some spare cycles ? :)\n. Merging this change.\n. Just some nits. I will make another pass once the PR is ready for review. Overall LGTM!\n. @mvdan: is this still a WIP?\n. LGTM\n. Awesome. Can you update integration/.jenkins.sh as well?\n. Ah! Never mind. Thanks for reminding the existence of Makefiles. \n. LGTM\n. cc @afein\nOn Tue, Sep 15, 2015 at 3:06 AM, Marcin Wielgus notifications@github.com\nwrote:\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/551.\n. LGTM\n. Yay! Finally! Thanks for all the amazing work @afein @mwielgus @piosz and @mvdan! LGTM! \n. ok to test\n. LGTM. Can you rebase once to latest HEAD?\n. ok to test\n\nOn Thu, Sep 17, 2015 at 12:23 PM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/555#issuecomment-141195106.\n. Great. Merging now!\n. Thanks for the fix @piosz! LGTM\n. @mwielgus: Just one more comment. Otherwise LGTM. This is super useful! \n. I wonder if running the tests in a separate namespace would resolve the\nfirst failure.\nThe second failure may be because some nodes are not Ready yet?\n\nOn Tue, Sep 15, 2015 at 3:14 PM, Marcin Wielgus notifications@github.com\nwrote:\n\nSometimes integration tests fail because of:\nI0915 23:37:51.645033   31536 heapster_api_test.go:57] Failed to delete rc: unable to modify replica count for rc heapster: replicationControllers \"heapster\" not found\n--- FAIL: TestHeapster (70.27s)\n        Location:       heapster_api_test.go:571\n    Error:      No error is expected but got unable to modify replica count for rc heapster: replicationControllers \"heapster\" not found\nor\nSystem container \"docker-daemon\" not found on host: \"kubernetes-minion-9q5q\" - map[kube-proxy:map[kubernetes-minion-lntk:{} kubernetes-minion-nfb8:{} kubernetes-minion-46ao:{}] docker-daemon:map[kubernetes-minion-lntk:{} kubernetes-minion-nfb8:{} kubernetes-minion-46ao:{}] kubelet:map[kubernetes-minion-lntk:{} kubernetes-minion-nfb8:{} kubernetes-minion-46ao:{}] machine:map[kubernetes-minion-lntk:{} kubernetes-minion-nfb8:{} kubernetes-minion-46ao:{}] system:map[kubernetes-minion-46ao:{} kubernetes-minion-nfb8:{} kubernetes-minion-lntk:{}]]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/559.\n. Docker images have been pushed. Merging this!\n. LGTM. Thanks @mwielgus !\n. LGTM.\n. That's weird. Shall we try bumping up the Kubernetes release ? It is currently set to v1.0.1.\n. To get similar behavior with InfluxDB and Grafana, we can create a DB per\nnamespace and let users manage access to those DBs via InfluxDB directly.\nInfluxDB documentation here. In\nGrafana, you'd then need to pass in DB credentials in-order to gain access\nto your namespace metrics.\n\nOn Wed, Sep 16, 2015 at 12:54 PM, mwringe notifications@github.com wrote:\n\nWhat we are doing here for Hawkular will be specific to the Hawkular sink\nin Heapster, so its not a generic solution for other sinks.\nWe separate out the metrics from each individual project and store them\ninto separate tenants in Hawkular (#555\nhttps://github.com/kubernetes/heapster/pull/555). In Hawkular will then\nrequire a token to be sent with any request and it will be used to\nauthenticate against the OpenShift server.\nWe need to run some more tests on it though before the Hawkular container\nwill support this security setup.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/564#issuecomment-140868646\n.\n. #800 is out to support InfluxDB v0.9. To create multiple DBs, the InfluxDB\ndriver under sinks/influxdb/driver.go will have to create a new database\nwhenever it encounters a new namespace and use that database for all\nmetrics that belong to that namespace.\nFrom the InfluxDB side you can create users and access policies.\n\nOn Thu, Sep 17, 2015 at 12:05 AM, shilpapadgaonkar <notifications@github.com\n\nwrote:\n@mwringe https://github.com/mwringe : I would like to try out the\nheapster-hawkular sink for my openshift env. Could you kindly point me to\nsome documentation link for the same?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/564#issuecomment-140988227\n.\n. There are quite a few issues around disk usage of InfluxDB. I'm hoping 5 days will work for most deployments.\n. A configurable value SGTM.\n\nOn Thu, Jan 14, 2016 at 1:37 PM, Kevin Moser notifications@github.com\nwrote:\n\nSeems like the retention policy name, like db name, should be configurable\nthrough the sink. At sink startup should be able to pass in a retention\npolicy. Then users can create retention policies in influx themselves for\ntheir own cases.\nAlternatively could expose the duration value and let the sink build the\npolicy on the fly configured to that duration. This to me seems more\nflexible. The default value of the duration can be 5 days, but users can\nset the duration to a different number if they so deem.\nThoughts?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/566#issuecomment-171787622\n.\n. > by retention policy does it mean keeping the data x days after the container is deleted or x-days after the data is created ? From the influxdb doc it sounds like it is the later.\n\nIt is the latter. \n\nin the case of short-lived container, it would be nice to have an aggressive retention policy. What would be a way to specify different retention policies based on the container/label ?\n\nWe should let users configure the retention policies because one solution might not work for everyone.\n. Ping @piosz @mwielgus. \n. CAdvisor is currently being updated to expose instantaneous usage.\n. cc @mvdan\nOn Fri, Sep 18, 2015 at 2:00 PM, Alexandros Mavrogiannis \nnotifications@github.com wrote:\n\nFinally! What is the time interval that is used in the calculation of the\ninstantaneous CPU metric?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/568#issuecomment-141567964\n.\n. cc @piosz @mwielgus \n. Those tests are relevant to that release. It will help with any future\npatches. WDYT?\n\nOn Thu, Sep 17, 2015 at 11:26 AM, Piotr Szczesniak <notifications@github.com\n\nwrote:\nDo you want to cherry-pick integration tests as well?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/570#issuecomment-141176525.\n. Just curious: How is DNS handled in the case of open shift? Could there be\nan issue with DNS here?\n\nOn Mon, Oct 26, 2015 at 4:58 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nThere should be no need to manually start influxdb though...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/571#issuecomment-151111320\n.\n. cc @piosz @dchen1107 \n. Self merging this change to make a new release soon.\n. cc @dchen1107 \n. Instead of returning an error, I am suggesting not exposing the\nKubernetes-specific endpoints at all - basically not registering those\nendpoints. WDYT?\n\nOn Fri, Sep 18, 2015 at 2:22 PM, Alexandros Mavrogiannis \nnotifications@github.com wrote:\n\nThat is certainly possible. We would have to create a \"Kubernetes mode\" on\nthe model which would cause the Kubernetes-specific API endpoints to return\nan appropriate message instead of invoking their corresponding Getter\nmethods.\nOther than that, no further changes should be needed, as no PodElements\nshould be received by the cache in non-Kubernetes clusters.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/575#issuecomment-141571882\n.\n. +1 for flag based control.\n\nOn Fri, Sep 18, 2015 at 2:31 PM, Marcin Wielgus notifications@github.com\nwrote:\n\n+1, I can do it on Monday. Kube-dns is not (or at least was not)\nobligatory so I would rather stick to the flag.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/575#issuecomment-141574008\n.\n. LGTM\n. LGTM.\n. @googlebot: Ignore CLAs\n. cc @mwielgus @piosz Are labels attached to images of interest to auto-scaling?\n. Is this in Heapster?\n\nOn Fri, Sep 18, 2015 at 2:24 PM, Alexandros Mavrogiannis \nnotifications@github.com wrote:\n\nFor reference, the memory limits are currently being set to -1 and the cpu\nlimits are being set to 99 millicores.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/581#issuecomment-141572184\n.\n. @mwielgus: If limits are not set on containers, the node's capacity is the limit. So instead of defaulting memory limit to some arbitrary high value, we can set it to node capacity. Similarly for cpu, we can set it to cpu capacity. This defaulting behavior is only required if one of the sources doesn't do it automatically. For example, Cadvisor could perform this defaulting.\n. Has this been fixed for real? This will affect monitoring of best effort\npods...\n\nOn Wed, Mar 2, 2016 at 7:45 AM, Marcin Wielgus notifications@github.com\nwrote:\n\nClosed #581 https://github.com/kubernetes/heapster/issues/581.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/581#event-574843464.\n. cc @dchen1107\n. cc @piosz @mwielgus\n. @dchen1107: Ping\n. @fgrzadkowski: That is TBD. I intend to handle some of the community work that are not specific to k8s. \n. @liuhewei: Is there a specific feature that you are interested in? Some of the deliverables in the roadmap are yet to be scoped out.\n. I have updated the roadmap based on your offline sync up.\nPTAL @jszczepkowski @wojtek-t @piosz @dchen1107 \n. @liuhewei: Thanks for explaining.\nFrom the short-term feature list, we're very interested in the \"Support for application metrics\". Actually we're also thinking about how to collect more application-level metrics inside containers. I assume this feature is related to cAdvisor's design here?\n\nYes. It will be based on cAdvisor's app metrics design. AFAIK, there are no better alternatives to cAdvisor when it comes to container workloads for this purpose.\n\nAnd, the scalability and performance are also key points. We're preparing to do some stress testing on our deployment later. \n\nThat's great. @piosz is also looking into scalability testing of heapster. You can possibly collaborate with him. \n\nBy the way, we'll submit our kafka-sink as a PR around this weekend for review.\n\nThat's great. Looking forward to it!\n\nAs for the long-term and future features, these policy-related features such as \"Resource usage predictions, events generating and outlier detection\" are also interesting. Actually we're doing some researches on these directions. We have our own policy-engine to analyze the data collected from heapster. If heapster can do those things, we're happy to follow the progress.\n\nWe'd love to collaborate in this area. Contributions in the form of design or PRs are welcome :)\n\nAnd I see an ecosystem related feature: Support for Docker Swarm and Mesos. I wonder if this feature need cAdvisor also be installed on Mesos or Swarm? It will be great if heapster's source-configuration is flexible enough to connect to other container management systems.\n\nThis has been something that I intend to work on soon. I believe heapster is generic enough to support any container cluster. It can already support CoreOS clusters. I plan on adding a consul discovery plugin to support swarm clusters. #136 has been open for a while as well.\n\nWe are also thinking about some other features not in the roadmap, for example, multiple heapster instances for HA, one heapster instance for multiple K8S clusters or partial nodes of one K8S cluster (for multi-tenant scenarios).\n\nWe have talked briefly about improving reliability by replicating heapster. It should be possible today to run multiple instances of heapster for HA purposes. If we shard heapster and make it self-healing, replication might not be necessary. \nRunning a single heapster for multiple cluster's is an interesting idea. ,Heapster will be used for resource metrics purposes in Kubernetes. I'm not sure how we will handle cluster identity and auth if we were to run a single heapster for multiple clusters.\n. cc @mwielgus @piosz @mvdan \n. This is not a duplicate. #584 merged the release changes into the v0.18 release branch.\n. That's interesting. AFAIK DNS is not restricted to namespaces yet in\nKubernetes. In kubernetes clusters, you should be able to re-use existing\ndeployment files under deploy/kube-config/.\nThanks for reporting!\nOn Fri, Sep 18, 2015 at 6:56 PM, Hewei Liu notifications@github.com wrote:\n\nAs a beginner, I tried deploying heapster with influxdb/grafana following\nguide\nhttps://github.com/kubernetes/heapster/blob/master/docs/influxdb.md and\nmanifests\nhttps://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb.\nI think here might be a mistake:\n\"--source=kubernetes:https://kubernetes\"\nhttps://github.com/kubernetes/heapster/blob/master/deploy/kube-config/influxdb/heapster-controller.json#L33\nIf not change, there will be errors from heapster pod:\nkubectl logs \n...\nE0919 09:17:33.649401       1 reflector.go:136] Failed to list *api.Node: Get https://kubernetes/api/v1/nodes: dial tcp: lookup kubernetes: no such host\nI wonder the reason might be the \"kubernetes\" service only exists under\n\"default\" namespace, so the \"heapster\" under \"kube-system\" namespace can't\nreach to \"kubernetes\" through DNS/Kube-proxy because there is no route\nunder \"kube-system\". Refer to heapster standalone deployment, I changed the\n--source to :\n\"--source=kubernetes\"\nFollowing:\nhttps://github.com/kubernetes/heapster/blob/master/deploy/kube-config/standalone/heapster-controller.json#L41\nThen everything is fine till now.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/586.\n. That's right. Heapster got moved into kube-system namespace and thats the\nreason for this breakage. I will fix this right away.\n\nOn Thu, Sep 24, 2015 at 7:40 AM, jonaz notifications@github.com wrote:\n\nAlso ran in to this problem source should be\n--source=kubernetes:https://kubernetes.default\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/586#issuecomment-142949009\n.\n. The search paths (resolv.conf) include svc.cluster.local and so\nkubernetes.default should work.\n\nOn Thu, Sep 24, 2015 at 11:31 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nShouldn't it be kubernetes.default.svc?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/586#issuecomment-143012963\n.\n. @huangyuqi: Just a few nits. Otherwise LGTM\n. ok to test\n. LGTM\n. ok to test\n. LGTM\n. @mwielgus: Thank you for sending out this PR :)\nJust one comment. Otherwise LGTM. \n. Awesome! LGTM!\n. ok to test\n. @huangyuqi: Thanks for the cleanup! LGTM\n. ok to test\n. Thanks for the PR @huangyuqi! LGTM\n. LGTM. Just one comment. Feel free to merge once that is resolved.\n. LGTM.\n. @huangyuqi: You seemed to closed this issue. Was that intentional?\n. LGTM. \n. cc @mvdan \n. Done!\n\nOn Tue, Sep 22, 2015 at 2:41 PM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nJust noticed that we have a file with a space, docs/Grafana Dashboard.json.\nThat should be docs/grafana-dashboard.json.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/597#issuecomment-142430758.\n. Updated the filename\n\nOn Tue, Sep 22, 2015 at 2:49 PM, Vishnu Kannan vishnuk@google.com wrote:\n\nDone!\nOn Tue, Sep 22, 2015 at 2:41 PM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nJust noticed that we have a file with a space, docs/Grafana\nDashboard.json. That should be docs/grafana-dashboard.json.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/597#issuecomment-142430758.\n. LGTM. Thanks for the cleanup @mvdan. I can imagine how the existing docs should have annoyed you :)\n. @thucatebay: I checked out the dashboards and this is great! Thanks for posting this PR!\n. @thucatebay: Your rebase ended up masking some of my comments. Can you run through the conversation history and take a look at all the comments? \n\n\nThanks!\n. Just a few nits @thucatebay. This PR LGTM. Once you remove the custom docker image, I can merge this PR and push an official grafana image.\n. @thucatebay: Here is the link to Google's CLA page. You can check if you CLA status there. \n. If you find your CLA on the link I provided, I can merge this PR right away. We can handle the nits in subsequent PRs.\n. @cadvisorJenkinsBot: ok to test\n. LGTM.\n. add to whitelist\n. @jonaz: Thanks for reporting this issue. This was an error in the release process where the binary version wasn't updated. The images have been updated now. Kindly try again!\n. @gerritbinnenmars: Can you post a WIP that will demonstrate the changes required? As we discussed over email, I'd be fine with an additional API that exposes data in CIM format.\n. @gerritbinnenmars: Thanks for filing the issue. Is this something that you can work on?\n. The recommendation is 8GB memory limit for InfluxDB. I usually see InfluxDB\nconsuming all the CPU and memory allocated to it. When left un-limited, it\nfrequently used >4 cores and more than 3GB of memory in small sized\nclusters.\nI think the underlying problem is not a leak. If we want good performance\nout of InfluxDB, we need to allocate enough resources. It might be worth\nrunning some of the benchmarks published by InfluxDB, to come up with a\nresource usage profile.\nOne more thing to consider is the amount of data being written to InfluxDB.\nOnce we cross 20k points, we will have to run InfluxDB clusters.\nI feel it is beyond the scope of this project, to help setup and manage\nInfluxDB clusters for various deployment scenarios.\nOn Thu, Sep 24, 2015 at 9:57 AM, Marcin Wielgus notifications@github.com\nwrote:\n\ncc: @piosz https://github.com/piosz @fgrzadkowski\nhttps://github.com/fgrzadkowski @jszczepkowski\nhttps://github.com/jszczepkowski\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/605#issuecomment-142988318\n.\n. We can increase the sink duration. It is currently set to 10 seconds by\ndefault.\nA thorough analysis of batch write performance is pending though.\nWith InfluxDB v0.9, we have switched to using tags, which is expected to\nsignificantly reduce storage and lookup times. In prior versions, we used\nto use columns, which resulted in multiple lookups within the DB in the\nread path.\n\nInfluxDB v0.8 is now being deprecated. So I don't see value in\ninvestigating storage backend performance.\nA meta comment that I want to re-state is that, I don't see value in making\nInfluxDB work in a scale scenario for this project. InfluxDB offers a\nhosted product, and it is pretty simple to use that instead.\nAs of now, we do not endorse InfluxDB as the recommended backend. Are you\nsuggesting we should instead endorse it?\nOn Thu, Sep 24, 2015 at 4:50 PM, Marcin Wielgus notifications@github.com\nwrote:\n\nPersonally I would double check whether we are not writing too much data\nto it or in the suboptimal way. From a brief & tired look at how the\ninfluxdb sink is implemented I would suspect that we are are writing\nsamples with the default 5 sec resolution. What about changing it to 30 sec\n(not perfect but better than switching of infuxdb)?\nOther idea is to play with how batches are constructed and sink frequency\n- if the underlying storage is key-value like it might be beneficial to\n  write data less often than every 10s so that more data is written under a\n  single key in one batch and the key lookup/disk seek happen less often.\nIn influxdb 0.8 there seem to be pluggable backends which deeply differ in\nterms of write performance\nhttps://influxdb.com/blog/2014/06/20/leveldb_vs_rocksdb_vs_hyperleveldb_vs_lmdb_performance.html\nwith HyperLevelDB appearing to be the best choice for write-intensive\nworkloads. Did we try it?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/605#issuecomment-143083142\n.\n. @mwielgus: We store all 16 of those metrics today in InfluxDB, and possibly more soon (disk io, load stats, tcp stats, etc). \nSo the actual points per second is (100 nodes * 30 pods/node * 2 containers per pod * 16 metrics per container) = ~96000 points per batch write or ~3200 points per second. This is not taking into account the system containers and the node itself.\n\nIn any case, I think we are on the same page when it comes to the level of support for InfluxDB. Ensuring that the default setup doesn't overwhelm InfluxDB makes total sense :+1: \n@piosz: Thanks for posting the PR!\n. cc @mvdan \n. cc @mvdan \n. Thanks for the fix @jimmidyson \n. ok to test\n. LGTM. Thanks @burmanm! Is Open shift managing heapster releases separately or do you want me to make a new release sometime soon?\n. Thanks for the cleanup @mvdan! LGTM.\n. Once again much appreciated changed :+1: Should we add the linter to our pre-commit hooks?\n. LGTM!\n. Ah! Got it :) \n. Feel free to self merge once tests pass.\n. LGTM\n. A huge +1. This change helps keep configs across k8s and heapster repo consistent.\n. The order of the fields does not matter IMO. From a brief glance, the PR LGTM.\n. LGTM\n. Didn't you clean up .jenkins.sh?\n. This PR LGTM\n. Oh  yeah. Jenkins github plugins handle that. We can update the plugin to\nuse the vanity domain.\nOn Mon, Sep 28, 2015 at 3:31 PM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nMerged #615 https://github.com/kubernetes/heapster/pull/615.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/615#event-421298124.\n. LGTM\n. LGTM. Thanks @mvdan!\n. At times, we upgrade the test cluster version. That is now tied to the\nintegration tests themselves for the sake of ease. If we separate cluster\nmaintenance from integration testing, we can lower the timeout and also\npossibly run multiple tests in parallel (each tests needs its own\nnamespace).\n\nOn Mon, Sep 28, 2015 at 3:58 PM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nRight now we have an overall timeout of 30m on the integration tests. The\ntimeout is sane because sometimes we have to bring up a cluster, which may\ntake up to 5-10m, plus the tests themselves which may take up to 5m.\nThe problem is that if the integration tests are broken, they take 30m to\nfail. Since Jenkins currently only does one job at a time, this means that\nif a broken PR triggers a few builds, Jenkins can be down for hours. This\nshouldn't happen.\nThe timeout should start counting from the point where the cluster is\nready. Then the timeout can be lowered to something like 10m, since the\ntests should never take more than 5m. They usually take around 2m as the\nmodel takes roughly 1m to gather up stats.\nThis way, Jenkins would be stuck for a third of the time it usually would.\nCC @vishh https://github.com/vishh\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/618.\n. Sorry for the confusion. #634 should fix this issue.\n. I suspect heapster is being killed by the kernel because of memory limits. Can you try removing the memory limits from heapster cofiguration file?\n. What version are you running? Can you post heapster logs? Also post the\noutput of dmesg from your host.\n\nOn Tue, Oct 13, 2015 at 8:18 AM, prateekrastogi notifications@github.com\nwrote:\n\nHi, I don't have any memory limit set in my config file. Also, I tried\nrunning only heapster-rc alone and it is unable to start. docker ps -a\nshows no trace of heapster container whatsoever. My docker version in\n1.7.1, build 2c2c52b-dirty. And the cluster is running on coreos.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/622#issuecomment-147747662\n.\n. We recommend to use the heapster configs bundled with specific kubernetes releases. We would ideally like heapster HEAD to support all kubernetes releases, but with the current velocity of changes, that is proving to be difficult. \n\nIf you can provide me your kube and heapster versions along with pointers to deployment files you are using for heapster, I can suggest modifications.\n. @lachlanbull: I have tried setting up an influxdb data source in proxy mode with http://localhost:8086 as the URL. Is that what you tried setting up? \nThe title of this issue says that v0.18.1 is not compatible with InfluxDB v0.9. I believe this issue is because the deployment scripts on HEAD are tied to past releases. I'm working on nightly builds that should resolve this issue.\n. I'm updating the scripts on HEAD to use a canary tagged image that will include support for InfluxDB v0.9.\nGrafana will no longer be accessible using apiserver proxy. The default service file sets up a LoadBalancer. If that doesn't work for you, I will be happy to help.\nMy interpretation of this issue is essentially HEAD configs being incompatible with each other.\n. InfluxDB v0.9 is supported from v0.19+\nOn Thu, Feb 25, 2016 at 4:05 PM, Andrew Smith notifications@github.com\nwrote:\n\nI've just loaded cluster/addons/cluster-monitoring/influxdb. When I go\nto:\n:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/datasources\nGrafana loads but I get the\nInfluxDB` Error: 404 page not found error.\nI read what you said about LoadBalancers - I just set a nodePort on my\ngrafana-service.yaml but when I go to :nodePort Grafana doesn't even load\nbecause all the HTML is referencing\n/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/css/.\nDid I miss something here?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/625#issuecomment-189045696\n.\n. That configuration is expected to work\n\nOn Fri, Feb 26, 2016 at 1:31 AM, Andrew Smith notifications@github.com\nwrote:\n\nSo is the problem that the yml files define heapster v0.20\nhttps://github.com/kubernetes/kubernetes/blob/master/cluster/addons/cluster-monitoring/influxdb/heapster-controller.yaml#L29\nand influxdb 0.5\nhttps://github.com/kubernetes/kubernetes/blob/master/cluster/addons/cluster-monitoring/influxdb/influxdb-grafana-controller.yaml#L23\n?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/625#issuecomment-189188468\n.\n. Have you updated the grafana configs to work with NodePorts or\nLoadBalancer? Look in the configs, there will be comments on how to make it\nwork outside of the proxy.\n\nOn Fri, Feb 26, 2016 at 11:30 AM, Andrew Smith notifications@github.com\nwrote:\n\nWith that configuration I get the 404 error above.\nFairly sure I'm on the right images:\n$ kubectl --namespace=kube-system get rc | grep -e heap -e minto\nheapster-v14                     heapster                      gcr.io/google_containers/heapster:v0.20.0-alpha7             k8s-app=heapster,version=v14                     1          19h\n                                 eventer                       gcr.io/google_containers/heapster:v0.20.0-alpha7\nmonitoring-influxdb-grafana-v3   influxdb                      gcr.io/google_containers/heapster_influxdb:v0.5              k8s-app=influxGrafana,version=v3                 1         19h\n                                 grafana                       gcr.io/google_containers/heapster_grafana:v2.1.1\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/625#issuecomment-189444716\n.\n. ok to test\n. Thanks @thucatebay!\n. LGTM\n. @cadvisorJenkinsBot: retest this please\n. @cadvisorJenkinsBot: retest this please\n. LGTM. Thanks @thucatebay.\n. Now, this PR needs to be ported over to Kubernetes v1.1 release branch.\n. @mvdan: I fixed it.\n. I'm adding another jenkins project to push a docker image with canary tag after every merge to master.\n. You can specify a custom checkout path in the git plugin.\n. @mvdan: addressed comments.\n. Merging this for now. We can fix the image later if required. I want to test the jenkins project.\n. @mvdan: I posted a patch with this PR to fix jenkins.\n. The default memory limits are not set to work on large clusters/workloads.\nWe can update the limits to requests for now.\n. What version are you running?\nWe are working on optimizing memory usage.\nUntil then, can you update the limits in the heapster spec to be\nrequests?\n\njson\n resources:\n            limits:\n              cpu: 100m\n              memory: 300Mi\nbecomes\nresources:\n            requests:\n              cpu: 100m\n              memory: 300Mi\nOn Tue, Oct 13, 2015 at 8:52 AM, Marek Grabowski notifications@github.com\nwrote:\n\ncc @fgrzadkowski https://github.com/fgrzadkowski @mwielgus\nhttps://github.com/mwielgus\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/632#issuecomment-147757485\n.\n. I'd suggest upgrading to heapster v0.18.2. We have optimized memory usage\nquite a bit.\n\nOn Tue, Oct 13, 2015 at 10:10 AM, Jim McCabe notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh in our production environment we are\nstill on v0.21.4 (but we will migrate to 1.x soon). In our dev & qa\nenvironments we are on v1.0.6. We see this Heapster issue in all\nenvironments.\nInterestingly we see the same version of Heapster running in our\nenvironments even though they are using different versions of Kubernetes.\n- The image is gcr.io/google_containers/heapster:v0.16.1.\n- The selector is k8s-app=heapster,version=v6\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/632#issuecomment-147780546\n.\n. > I have a completely naive and silly question - what would happen if\nHeapster was completely turned off?\n\nWith the version of Kubernetes you are running, the default monitoring\nsetup will stop working.\nStarting from Kubernetes v1.1 though, we require running heapster. But v1.1\nwill ship with a more optimized version of heapster.\nOn Tue, Oct 13, 2015 at 10:12 AM, Jim McCabe notifications@github.com\nwrote:\n\nWe'll try your suggestion (using \"requests\" in place of \"limits\") to see\nhow that fares today, and upgrading the version.\nI have a completely naive and silly question - what would happen if\nHeapster was completely turned off?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/632#issuecomment-147781124\n.\n. @tylerd-porch: That might not be possible. cc @a-robinson \n. There is a controller running on the master that ensures that the current\nstate of cluster addons matches the desired state.\nI'm not sure if that controller looks at image version. So, a patch might\nbe worth a try.\n\nOn Tue, Oct 13, 2015 at 12:53 PM, Jim McCabe notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh / @tylerd-porch\nhttps://github.com/tylerd-porch would it be possible to change the\nimage via kubectl patch? For example under items.spec.containers.image:\n\"containers\": [\n                {\n                    \"name\": \"heapster\",\n                    \"image\": \"gcr.io/google_containers/heapster:v0.16.1\",\n                    \"command\": [\n                        \"/heapster\",\n                        \"--source=kubernetes:''\"\n                    ],\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/632#issuecomment-147831428\n.\n. @cadvisorJenkinsBot: test this please\n. @mvdan Thanks for the review. PTAL.\n. Should we validate this be flag? @afein mentioned that the updates to model are dependent on model resolution.\n. add to whitelist\n. @jszczepkowski: I added you to the whitelist. Jenkins should run e2e for your PRs henceforth. \n. LGTM.\n. https://github.com/kubernetes/kubernetes/issues/12483 should solve this.\n. We can request for just one sample instead.\n. Jenkins has been painful lately. I'm hoping a long pending update will fix it.\n. I have seen this issue with InfluxDB v0.8.x. One option is to setup continuous queries in InfluxDB to sample the cpu_usage periodically into a new series, and avoid derivatives(..).\n\nAnother option is to try InfluxDB v0.9. The configs on Heapster HEAD support InfluxDB v0.9. I have not seen issues with derivative with this version.\ncc @piosz who is working on adding instantaneous cpu metrics.\n. The cache is used for the API and the model, in addition to the sinks.\nSo switching over to timeseries might not be possible.\nOne option is to avoid storing the entire cadvisor object and instead choose to store only the metrics heapster can handle.\n. Kubelet will return pod and node metrics in the future. Except for this part I agree with the rest of your comments. \nThe current state has been reached due to organic growth. \n. SGTM. cc @afein \n. Here is some relevant conversation on this with @afein.\n. We need to watch the API server for metadata. This approach might not work with other runtimes like rocket which can understand pods only. https://github.com/kubernetes/kubernetes/issues/12483 attempts to solve this issue. The labels added by kubelet are an implementation detail and ideally, the rest of the system should not depend on that implementation detail. \n. @piosz: Thanks for pointing out. I will remove numstats. This change also reduces the time window. As of now, processing two stat samples is better than 30 of them. WDYT?\n. The dynamic housekeeping feature was added to reduce cpu usage of\ncAdvisor. If we have empty cgroups, stats will never change for those\ncgroups and cAdvisor spent cpu cycles on them.\nFrom that perspective, if we do not receive a stat sample in a time\nduration, we can safely assume that the previous metric values still hold\ngood. On the flipside, it sounds like a hack :)\nOn Tue, Oct 13, 2015 at 1:08 AM, Piotr Szczesniak notifications@github.com\nwrote:\n\nIn this case you should decrease maxHousekeepingInterval\nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cadvisor/cadvisor_linux.go#L47\nto assure we don't omit any stats. I'm not sure what would be the outcome\nof this change.\n@vmarmol https://github.com/vmarmol @rjnagal\nhttps://github.com/rjnagal any thoughts?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/644#issuecomment-147639492.\n. > The same metrics that Heapster retrieves via the /stats endpoint on the kubelet are also exposed for Prometheus to scrape via /metrics. How do you feel about Heapster becoming a query & abstraction layer on top of Prometheus to provide Kubernetes semantics to Prometheus timeseries?\n\nAs of now, I think prometheus can be one of the backends for heapster. \n\nPrometheus is also used in a number of other components: etcd & skydns being the two main core Kubernetes ones I can think of straight off.\n\nI assume you are referring to metrics being in prometheus format. AFAIK, that was done because prometheus client libs were the most expressive ones as of now.\n\nI fairly recently added Kubernetes discovery to Prometheus so we have all container metrics ingested, as well as application level metrics (future work for Heapster) thanks to the wide range of language plugins & external metric exporters in the Prometheus ecosystem.\n\nThis is great. This will definitely be helpful. \n\nAlerting, sharding & federation is already built in to Prometheus.\n\nDo we have any scalability numbers on prometheus?\nPrometheus will be a good solution for monitoring.\nBut I'm not convinced that we should require all Kubernetes clusters to run prometheus.\nWe need input from users here. We can send out a survey or discuss this in the weekly hangout. \nAFAIK, users love and care about their own monitoring systems and requiring them to run prometheus might not be ideal.\nOne more concern I have is that in the future I want to have heapster optionally store metrics in a crowd-shared database, and use that data for resource prediction purposes. I haven't gotten the time to flesh out this idea completely. \n. We want kubernetes to have minimal dependencies for bootstrapping purposes. \nWe will need reliable, low latency access to node resource usage metrics and that is the reason why heapster collects and serves these metrics directly.\nInternally, we have never depended on timeseries DB for critical cluster functionalities like scheduling. \nIn this regard, we can have prometheus serve as the source of non-critical data, which are not time-sensitive. \n. I agree that Prometheus is a good candidate for better out of the box\nmonitoring experience.\nAFAIK there is at-least one scenario where we will not run Prometheus -\nGoogle Container Engine.\nI'd like to split core-cluster functionalities from monitoring. Disabling\nmonitoring using heapster is totally fine.\nBut I don't think we can disable collection and processing of core-cluster\nmetrics that are required to bootstrap the cluster functionalities like\nscheduling.\nEven in the case of auto-scaling, I'd imagine us wanting to use curated\nmetrics.\nOn Tue, Oct 13, 2015 at 6:39 AM, Jimmi Dyson notifications@github.com\nwrote:\n\n@thucatebay https://github.com/thucatebay Thanks for the feedback! I\nwas thinking of Prometheus in this scenario as a day store, similar to how\nHeapster operates now, with rules for aggregating metrics as they come in\nto keep storage & memory requirements low. This would keep the management\nof it simple but bring with it extra benefits of:\n- application level metrics (future work for heapster)\n- persistence (afaik if heapster pod dies you lose all stats which\n  could affect autoscaling)\n- sharding & federation (future work for heapster)\nThe only difference for you would be to write an external storage plugin\nfor Prometheus as opposed to a Heapster sink.\nIt would also mean that those places that don't have their own monitoring\n& alerting system as you do have would be able to expand the environment by\nadding in Prometheus alert manager if they wanted, but certainly not a\nrequirement.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/645#issuecomment-147716927\n.\n. @spiffxp: Moving forward heapster (standalone) will be run by default on all kubernetes clusters. It will be serving the metrics APIs which will be consumed by the scheduler, auto-scalers, etc. \nThe term addon is a misnomer because other addons like dns are also required for default kubernetes functionalities.\n. It is not required for v1.0.x.\nIt is required for beta features in v1.1.x.\n\nOn Tue, Oct 13, 2015 at 4:03 PM, Aaron Crickenberger \nnotifications@github.com wrote:\n\n@vishh https://github.com/vishh yeah but how about today? is this\nrequired for proper functioning of v 1.0.x or 1.1.x?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/645#issuecomment-147878257\n.\n. @burmanm: Godeps seems to be broken. \n\nGodeps/_workspace/src/k8s.io/kubernetes/pkg/util/uuid.go:23:2: cannot find package \"github.com/pborman/uuid\" in any of:\n    /var/lib/jenkins/tools/org.jenkinsci.plugins.golang.GolangInstallation/go1.4.2/src/github.com/pborman/uuid (from $GOROOT)\n    /var/lib/jenkins/workspace/project/src/k8s.io/heapster/Godeps/_workspace/src/github.com/pborman/uuid (from $GOPATH)\n    /var/lib/jenkins/workspace/project/src/github.com/pborman/uuid\ngodep: go exit status 1\n. @burmanm: Thanks for pointing that out. I fixed it. I'm trying to find out why that happened.\n. @cadvisorJenkinsBot: test this please\n. I fixed the integration tests. Looking at jenkins now.\n. ok to test\n. ok to test\nOn Wed, Oct 14, 2015 at 5:00 PM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/648#issuecomment-148236884.\n. Sorry for the delay @huangyuqi! Thanks for the PR! I have posted a couple of comments. Other than that, this PR LGTM. Lets get this in quickly.\n. @huangyuqi: Is this PR ready for review? Have you addressed the pending comments?\n. > @vishh , Would you please review the PR of kafka sink for some modification about data structure and kafka's client? : )\n\nThe part about sink setup isn't clear to me yet. Once you clarify the behavior there, this PR is good to go.\n\nBTW, Our OPS subsystem uses the ES to collect the metrics and events, so i realize a simple ES's sink. May I take another new PR about this ?\n\nSGTM! Just curious - does ES support retention policies?\n. $ ./heapster --logtostderr --source=cadvisor:external?standalone=true --sink=\"kafka:?brokers=localhost:2181&timeseriestopic=test&eventstopic=test\"\nI1103 10:00:04.764864    2337 heapster.go:55] ./heapster --logtostderr --source=cadvisor:external?standalone=true --sink=kafka:?brokers=localhost:2181&timeseriestopic=test&eventstopic=test\nI1103 10:00:04.764964    2337 heapster.go:56] Heapster version 0.18.0\nI1103 10:00:04.765003    2337 external.go:88] Running in standalone mode, external nodes source will only use localhost\nF1103 10:00:04.766070    2337 heapster.go:62] failed to connect to kafka cluster: cannot connect\nThis is the error that I was suggesting to avoid. I will post a patch against this PR to fix this. \n. @huangyuqi: I opened #683 based on this PR. I tested that against a local kafka cluster and it seems to be resilient against cluster outages. If you are OK with that, we can merge that in.\nWe need a simple integration test for this client.\n. I'm closing this PR. \n. Enclosing the entire sink config inside \" should do the trick. --sink=\"kafka:?brokers=localhost:9092\" should work even if you have multiple parameters. Can you try that?\n. @huangyuqi: Why are you not able to test sink in standalone mode?\n. @cadvisorJenkinsBot: ok to test\n. If a process were to run on one cpu continuously for a second, its usage\nwill be 1e+9 nanoseconds. If it ran on n cores continuously its usage\nwill n * 1e+9 nanoseconds. Does this make sense?\nPercentage will be usage_in_nanoseconds / (capacity_in_absolute_cores *\n1e+9).\nMeanwhile we are also working on adding an utilization metric.\nOn Mon, Oct 12, 2015 at 7:03 PM, ttllyy notifications@github.com wrote:\n\n/api/v1/model/namespaces/{namespace-name}/pods/{pod-name}/metrics/cpu-usage\nReturns a set of (Timestamp, Value) pairs for the requested pod-level\nmetric\ncpu usage unit is ns\uff0cHow to correspond to percent?\nEvery timestamp, How much is the total time?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/650.\n. cc @burmanm \n. Yeah. I had to re-build the old jenkins server and as part of that I shifted to go 1.5.1. The go version update has surfaced these issues.\n. Can one of the admins verify this patch?\n. ok to test\n. @jimmidyson: Sorry. Still fixing the jenkins server. \n. @cadvisorJenkinsBot: ok to test\n. ok to test\n\nOn Wed, Oct 14, 2015 at 1:09 PM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/654#issuecomment-148181118.\n. add to whitelist\n\nOn Wed, Oct 14, 2015 at 2:54 PM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/654#issuecomment-148213289.\n. This PR passed on jenkins: http://104.197.39.211/job/heapster-e2e-gce/2884/\n\nIt still refuses to update github properly :(\n. Thanks for the quick turnaround @burmanm \n. Thanks for the cleanup @bluebreezecf! Much appreciated!\n. cc @mwielgus @piosz \n. Thanks for debugging @ravilr! \n1. heapster should handle this situation gracefully and not panic.\n2. We should investigate why kubelet is not exporting stats. This can happen is cAdvisor initialization fails. Can you check your kubelet logs @ravilr?\n. ok to test\n. LGTM. Thanks for the PR @danmcp!\n. I suspect that heapster is not able to collect metrics from the nodes. Can\nyou run heapster with an extra flag? Add the following line to the\ncommand section in heapster controller\n - --vmodule=*=3\nWith this option, heapster logs should provide more useful information.\nOn Thu, Oct 22, 2015 at 2:00 AM, R\u00e9my Phelipot notifications@github.com\nwrote:\n\nI have the same problem with the kubernetes endpoint configured in\nunsecure mode. Here is the configuration for the replication controllers:\nGrafana/InfluxDB\napiVersion: v1kind: ReplicationControllermetadata:\n  creationTimestamp: 2015-10-22T08:42:33Z\n  generation: 1\n  labels:\n    name: influxGrafana\n  name: infludb-grafana\n  namespace: kube-system\n  resourceVersion: \"2249\"\n  selfLink: /api/v1/namespaces/kube-system/replicationcontrollers/infludb-grafana\n  uid: d6ab1f6e-7898-11e5-8753-0016c2000115spec:\n  replicas: 1\n  selector:\n    name: influxGrafana\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        name: influxGrafana\n    spec:\n      containers:\n      - image: kubernetes/heapster_influxdb:v0.4\n        imagePullPolicy: IfNotPresent\n        name: influxdb\n        ports:\n        - containerPort: 8083\n          hostPort: 8083\n          protocol: TCP\n        - containerPort: 8086\n          hostPort: 8086\n          protocol: TCP\n        resources: {}\n        terminationMessagePath: /dev/termination-log\n      - image: grafana/grafana:2.1.0\n        imagePullPolicy: IfNotPresent\n        name: grafana\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n        resources: {}\n        terminationMessagePath: /dev/termination-log\n        volumeMounts:\n        - mountPath: /var/lib/grafana\n          name: grafana-store\n      dnsPolicy: ClusterFirst\n      restartPolicy: Always\n      volumes:\n      - emptyDir: {}\n        name: grafana-storestatus:\n  observedGeneration: 1\n  replicas: 1\nHeapster\napiVersion: v1kind: ReplicationControllermetadata:\n  creationTimestamp: 2015-10-22T08:42:33Z\n  generation: 1\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\n  namespace: kube-system\n  resourceVersion: \"2250\"\n  selfLink: /api/v1/namespaces/kube-system/replicationcontrollers/heapster\n  uid: d6a5d2ee-7898-11e5-8753-0016c2000115spec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - command:\n        - /heapster\n        - --source=kubernetes:http://212.xxx.xx.21:8888?insecure=true&auth=&inClusterConfig=false\n        - --sink=influxdb:http://monitoring-influxdb.kube-system.xxx.xxxx.net:8086\n        image: kubernetes/heapster:v0.16.0\n        imagePullPolicy: IfNotPresent\n        name: heapster\n        resources: {}\n        terminationMessagePath: /dev/termination-log\n      dnsPolicy: ClusterFirst\n      restartPolicy: Alwaysstatus:\n  observedGeneration: 1\n  replicas: 1\nHere are the logs:\nI1022 08:42:39.879044       1 heapster.go:52] /heapster --source=kubernetes:http://212.xxx.xx.21:8888?insecure=true&auth=&inClusterConfig=false --sink=influxdb:http://monitoring-influxdb.kube-system.xxx.xx.net:8086\nI1022 08:42:39.879092       1 heapster.go:53] Heapster version 0.16.0\nI1022 08:42:39.879187       1 kube_factory.go:169] Using Kubernetes client with master \"http://212.xxx.xx.21:8888\" and version \"v1\"\nI1022 08:42:39.879199       1 kube_factory.go:170] Using kubelet port 10255\nE1022 08:42:40.062408       1 driver.go:322] Database creation failed: Post http://monitoring-influxdb.kube-system.xxxx.net:8086/db?u=root&p=root: read tcp 10.10.10.213:8086: connection reset by peer. Retrying after 30 seconds\nI1022 08:43:10.288550       1 driver.go:297] Created database \"k8s\" on influxdb\nI1022 08:43:10.288603       1 driver.go:376] created influxdb sink with options: {root root monitoring-influxdb.kube-system.xxxxxx.net:8086 k8s false}\nI1022 08:43:10.289937       1 heapster.go:64] Starting heapster on port 8082\n[10/22/15 08:42:41] [INFO] Loading configuration file /config/config.toml\n+---------------------------------------------+\n|  _  _            _    |\n| | |      / _| |          |   |  _ \\  |\n|   | |  _  | || |  | |  | | |) | |\n|   | | | ' |  | | | | \\ \\/ / |  | |  _ <  |\n|  | || | | | | | | || |>  <| || | |) | |\n| |_|| ||| ||,/_/__/|/  |\n+---------------------------------------------+\nThe InfluxDB contains an empty 'k8s' table.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/661#issuecomment-150154684\n.\n. cc @piosz @mwielgus \n. ok to test\n. What version of kubernetes are you running? A similar issue was reported in\nolder versions and v1.0.6 seemed to have resolved that.\n\nOn Fri, Oct 23, 2015 at 4:47 AM, R\u00e9my Phelipot notifications@github.com\nwrote:\n\nHello!\nI'm using the last version of heapster (the canary version of DockerHub)\nand I've encountered an issue when I want to monitor containers deployed in\na namespace other than \"default\". It seems that there is no data in\nInfluxDb about there containers, and there are obviously no appearing in\nthe grafana dashboard.\nHere are the configuration files:\nGrafana/InfluxDB\n{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"ReplicationController\",\n    \"metadata\": {\n        \"labels\": {\n            \"name\": \"influxGrafana\"\n        },\n        \"name\": \"infludb-grafana\",\n        \"namespace\": \"sid-system\"\n    },\n    \"spec\": {\n        \"replicas\": 1,\n        \"selector\": {\n            \"name\": \"influxGrafana\"\n        },\n        \"template\": {\n            \"metadata\": {\n                \"labels\": {\n                    \"name\": \"influxGrafana\"\n                }\n            },\n            \"spec\": {\n                \"volumes\": [\n                    {\n                        \"name\": \"influxdb-persistent-storage\",\n                        \"hostPath\": {\n                            \"path\": \"/data/influxdb\"\n                        }\n                    },\n                    {\n                        \"name\": \"grafana-store\",\n                        \"hostPath\": {\n                            \"path\": \"/data/grafana\"\n                        }\n                    }\n                ],\n                \"containers\": [{\n                    \"image\": \"kubernetes/heapster_influxdb:v0.5\",\n                    \"name\": \"influxdb\",\n                    \"ports\": [{\n                        \"containerPort\": 8083,\n                        \"hostPort\": 8083\n                    },\n                    {\n                        \"containerPort\": 8086,\n                        \"hostPort\": 8086\n                    }],\n                    \"volumeMounts\": [{\n                        \"name\": \"influxdb-persistent-storage\",\n                        \"mountPath\": \"/data\"\n                    }]\n                },\n                {\n                    \"image\": \"kubernetes/heapster_grafana:v2.1.0\",\n                    \"name\": \"grafana\",\n                    \"volumeMounts\": [{\n                        \"name\": \"grafana-store\",\n                        \"mountPath\": \"/var/lib/grafana\"\n                    }],\n                    \"ports\": [{\n                        \"containerPort\": 3000\n                    }],\n                    \"env\": [{\n                        \"name\": \"INFLUXDB_EXTERNAL_URL\",\n                        \"value\": \"/api/v1/proxy/namespaces/xxxxxxxx/services/monitoring-influxdb:api/db/\"\n                    },\n                    {\n                        \"name\": \"INFLUXDB_HOST\",\n                        \"value\": \"localhost\"\n                    },\n                    {\n                        \"name\": \"INFLUXDB_PORT\",\n                        \"value\": \"8086\"\n                    }]\n                }]\n            }\n        }\n    }\n}\nHeapster RC\n{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"ReplicationController\",\n    \"metadata\": {\n        \"labels\": {\n            \"k8s-app\": \"heapster\",\n            \"name\": \"heapster\",\n            \"version\": \"v6\"\n        },\n        \"name\": \"heapster\",\n        \"namespace\": \"xxxxxx-system\"\n    },\n    \"spec\": {\n        \"replicas\": 1,\n        \"selector\": {\n            \"k8s-app\": \"heapster\",\n            \"version\": \"v6\"\n        },\n        \"template\": {\n            \"metadata\": {\n                \"labels\": {\n                    \"k8s-app\": \"heapster\",\n                    \"version\": \"v6\"\n                }\n            },\n            \"spec\": {\n                \"containers\": [{\n                    \"image\": \"kubernetes/heapster:canary\",\n                    \"name\": \"heapster\",\n                    \"command\": [\"/heapster\",\n                    \"--source=kubernetes:http://{{ 'kubernetes_master' }}?insecure=true&auth=&inClusterConfig=false\",\n                    \"--sink=influxdb:http://monitoring-influxdb.xxxxxxxxxxx.net:8086\"]\n                }]\n            }\n        }\n    }\n}\nHere are the logs:\nW1023 13:43:40.089611   17948 cmd.go:161] log is DEPRECATED and will be removed in a future version. Use logs instead.\nI1023 11:43:15.622532       1 heapster.go:60] /heapster --source=kubernetes:http://xxx.xxx.xxx.110:8888?insecure=true&auth=&inClusterConfig=false --sink=influxdb:http://monitoring-influxdb.xxxx-system.xxx.xxx.xxx:8086 --vmodule=_=3\nI1023 11:43:15.622752       1 heapster.go:61] Heapster version 0.18.0\nI1023 11:43:15.623018       1 kube_factory.go:168] Using Kubernetes client with master \"http://xxx.xxx.xxx.110:8888\" and version \"v1\"\nI1023 11:43:15.623031       1 kube_factory.go:169] Using kubelet port 10255\nI1023 11:43:15.623389       1 external.go:116] no timeseries data between 0001-01-01 00:00:00 +0000 UTC and 0001-01-01 00:00:00 +0000 UTC\nI1023 11:43:15.706815       1 driver.go:316] created influxdb sink with options: {root root monitoring-influxdb.xxxx-system.xxx.xxx.xxx:8086 k8s false}\nI1023 11:43:15.805561       1 driver.go:245] Created database \"k8s\" on influxDB server at \"monitoring-influxdb.xxxx-system.xxx.xxx.xxx:8086\"\nI1023 11:43:15.805615       1 external.go:214] Updated sinks: [0xc2081b2080]\nI1023 11:43:15.816765       1 heapster.go:71] Starting heapster on port 8082\nI1023 11:43:20.000393       1 manager.go:162] starting to scrape data from sources start: 2015-10-23 11:43:15 +0000 UTC end: 2015-10-23 11:43:20 +0000 UTC\nI1023 11:43:20.000464       1 manager.go:103] attempting to get data from source \"Kube Pods Source\"\nI1023 11:43:20.001604       1 manager.go:103] attempting to get data from source \"Kube Node Metrics Source\"\nI1023 11:43:20.001745       1 manager.go:103] attempting to get data from source \"Kube Events Source\"\nI1023 11:43:20.001802       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"nginx-1hsi6.140fccb266e7e70c\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"/api/v1/namespaces/default/events/nginx-1hsi6.140fccb266e7e70c\", UID:\"b3413f34-7977-11e5-8507-0016c200016e\", ResourceVersion:\"2320\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581195872, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc20829b240), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"default\", Name:\"nginx-1hsi6\", UID:\"b306a93a-7977-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2316\", FieldPath:\"\"}, Reason:\"scheduled\", Message:\"Successfully assigned nginx-1hsi6 to slave-2.xxx.xxx.xxx\", Source:api.EventSource{Component:\"scheduler\", Host:\"\"}, F\n irstTimestamp:unversioned.Time{Time:time.Time{sec:63581195872, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581195872, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.002178       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"nginx-1hsi6.140fccb2713d5510\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"/api/v1/namespaces/default/events/nginx-1hsi6.140fccb2713d5510\", UID:\"b35b8486-7977-11e5-8507-0016c200016e\", ResourceVersion:\"2323\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581195872, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc20829b3e0), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"default\", Name:\"nginx-1hsi6\", UID:\"b306a93a-7977-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2316\", FieldPath:\"implicitly required container POD\"}, Reason:\"pulled\", Message:\"Pod container image \\\"gcr.io/google_containers/pause:0.8.0\\ http://gcr.io/google_containers/pause:0.8.0%5C\" already present on machin\n e\", Source:api.EventSource{Component:\"kubelet\", Host:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581195872, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581195872, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.002274       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"nginx-1hsi6.140fccb287b7768a\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"/api/v1/namespaces/default/events/nginx-1hsi6.140fccb287b7768a\", UID:\"b3951334-7977-11e5-8507-0016c200016e\", ResourceVersion:\"2324\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581195872, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc20829b5e0), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"default\", Name:\"nginx-1hsi6\", UID:\"b306a93a-7977-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2316\", FieldPath:\"implicitly required container POD\"}, Reason:\"created\", Message:\"Created with docker id fa80062475ae\", Source:api.EventSource{Component:\"kubelet\", Hos\n t:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581195872, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581195872, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.002388       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"nginx-1hsi6.140fccb293271158\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"/api/v1/namespaces/default/events/nginx-1hsi6.140fccb293271158\", UID:\"b3b25df5-7977-11e5-8507-0016c200016e\", ResourceVersion:\"2325\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581195872, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc20829b800), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"default\", Name:\"nginx-1hsi6\", UID:\"b306a93a-7977-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2316\", FieldPath:\"implicitly required container POD\"}, Reason:\"started\", Message:\"Started with docker id fa80062475ae\", Source:api.EventSource{Component:\"kubelet\", Hos\n t:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581195872, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581195872, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.002459       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"nginx-1hsi6.140fccb2a698c66e\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"/api/v1/namespaces/default/events/nginx-1hsi6.140fccb2a698c66e\", UID:\"b3e4132c-7977-11e5-8507-0016c200016e\", ResourceVersion:\"2326\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581195873, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc20829b9a0), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"default\", Name:\"nginx-1hsi6\", UID:\"b306a93a-7977-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2316\", FieldPath:\"spec.containers{nginx}\"}, Reason:\"created\", Message:\"Created with docker id 292bff95c5f0\", Source:api.EventSource{Component:\"kubelet\", Host:\"slave-2.\n xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581195873, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581195873, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.002654       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"nginx-1hsi6.140fccb2af3c8bfc\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"/api/v1/namespaces/default/events/nginx-1hsi6.140fccb2af3c8bfc\", UID:\"b3fa47b5-7977-11e5-8507-0016c200016e\", ResourceVersion:\"2327\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581195873, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc20829bac0), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"default\", Name:\"nginx-1hsi6\", UID:\"b306a93a-7977-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2316\", FieldPath:\"spec.containers{nginx}\"}, Reason:\"started\", Message:\"Started with docker id 292bff95c5f0\", Source:api.EventSource{Component:\"kubelet\", Host:\"slave-2.\n xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581195873, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581195873, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.002724       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"nginx.140fccb2503e5072\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"/api/v1/namespaces/default/events/nginx.140fccb2503e5072\", UID:\"b3074f6a-7977-11e5-8507-0016c200016e\", ResourceVersion:\"2317\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581195871, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc20829bbe0), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"ReplicationController\", Namespace:\"default\", Name:\"nginx\", UID:\"b30623e9-7977-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2315\", FieldPath:\"\"}, Reason:\"successfulCreate\", Message:\"Created pod: nginx-1hsi6\", Source:api.EventSource{Component:\"replication-controller\", Host:\"\"}, FirstTimestam\n p:unversioned.Time{Time:time.Time{sec:63581195871, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581195871, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.002888       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"nginx-hcr3s.140fcbfbb5141307\", GenerateName:\"\", Namespace:\"ntest\", SelfLink:\"/api/v1/namespaces/ntest/events/nginx-hcr3s.140fcbfbb5141307\", UID:\"df8e6c0a-7975-11e5-8507-0016c200016e\", ResourceVersion:\"2110\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581195087, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc20829bd00), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"ntest\", Name:\"nginx-hcr3s\", UID:\"df5944ab-7975-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2106\", FieldPath:\"\"}, Reason:\"scheduled\", Message:\"Successfully assigned nginx-hcr3s to slave-2.xxx.xxx.xxx\", Source:api.EventSource{Component:\"scheduler\", Host:\"\"}, FirstTi\n mestamp:unversioned.Time{Time:time.Time{sec:63581195087, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581195087, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.003002       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"nginx-hcr3s.140fcbfbbf245609\", GenerateName:\"\", Namespace:\"ntest\", SelfLink:\"/api/v1/namespaces/ntest/events/nginx-hcr3s.140fcbfbbf245609\", UID:\"dfa84b91-7975-11e5-8507-0016c200016e\", ResourceVersion:\"2113\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581195087, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc20829be20), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"ntest\", Name:\"nginx-hcr3s\", UID:\"df5944ab-7975-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2106\", FieldPath:\"implicitly required container POD\"}, Reason:\"pulled\", Message:\"Pod container image \\\"gcr.io/google_containers/pause:0.8.0\\ http://gcr.io/google_containers/pause:0.8.0%5C\" already present on machine\", So\n urce:api.EventSource{Component:\"kubelet\", Host:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581195087, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581195087, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.003172       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"nginx-hcr3s.140fcbfbe8d279af\", GenerateName:\"\", Namespace:\"ntest\", SelfLink:\"/api/v1/namespaces/ntest/events/nginx-hcr3s.140fcbfbe8d279af\", UID:\"e0132c79-7975-11e5-8507-0016c200016e\", ResourceVersion:\"2114\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581195088, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc20829bf40), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"ntest\", Name:\"nginx-hcr3s\", UID:\"df5944ab-7975-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2106\", FieldPath:\"implicitly required container POD\"}, Reason:\"created\", Message:\"Created with docker id d9de1f415a03\", Source:api.EventSource{Component:\"kubelet\", Host:\"sla\n ve-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581195088, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581195088, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.003239       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"nginx-hcr3s.140fcbfbf331dce7\", GenerateName:\"\", Namespace:\"ntest\", SelfLink:\"/api/v1/namespaces/ntest/events/nginx-hcr3s.140fcbfbf331dce7\", UID:\"e02e4d4e-7975-11e5-8507-0016c200016e\", ResourceVersion:\"2115\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581195088, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208266060), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"ntest\", Name:\"nginx-hcr3s\", UID:\"df5944ab-7975-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2106\", FieldPath:\"implicitly required container POD\"}, Reason:\"started\", Message:\"Started with docker id d9de1f415a03\", Source:api.EventSource{Component:\"kubelet\", Host:\"sla\n ve-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581195088, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581195088, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.003398       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"nginx-hcr3s.140fcc03e07fa1d2\", GenerateName:\"\", Namespace:\"ntest\", SelfLink:\"/api/v1/namespaces/ntest/events/nginx-hcr3s.140fcc03e07fa1d2\", UID:\"f479db12-7975-11e5-8507-0016c200016e\", ResourceVersion:\"2125\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581195122, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208266200), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"ntest\", Name:\"nginx-hcr3s\", UID:\"df5944ab-7975-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2106\", FieldPath:\"spec.containers{nginx}\"}, Reason:\"pulled\", Message:\"Successfully pulled image \\\"nginx\\\"\", Source:api.EventSource{Component:\"kubelet\", Host:\"slave-2.xxx.xxx\n .xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581195122, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581195122, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.003463       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"nginx-hcr3s.140fcc03f4c09ed1\", GenerateName:\"\", Namespace:\"ntest\", SelfLink:\"/api/v1/namespaces/ntest/events/nginx-hcr3s.140fcc03f4c09ed1\", UID:\"f4adbf61-7975-11e5-8507-0016c200016e\", ResourceVersion:\"2126\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581195122, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208266360), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"ntest\", Name:\"nginx-hcr3s\", UID:\"df5944ab-7975-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2106\", FieldPath:\"spec.containers{nginx}\"}, Reason:\"created\", Message:\"Created with docker id d57f432860b0\", Source:api.EventSource{Component:\"kubelet\", Host:\"slave-2.xxx.xx\n x.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581195122, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581195122, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.004014       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"nginx-hcr3s.140fcc03fbd031fb\", GenerateName:\"\", Namespace:\"ntest\", SelfLink:\"/api/v1/namespaces/ntest/events/nginx-hcr3s.140fcc03fbd031fb\", UID:\"f4bfd86d-7975-11e5-8507-0016c200016e\", ResourceVersion:\"2127\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581195122, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208266540), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"ntest\", Name:\"nginx-hcr3s\", UID:\"df5944ab-7975-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2106\", FieldPath:\"spec.containers{nginx}\"}, Reason:\"started\", Message:\"Started with docker id d57f432860b0\", Source:api.EventSource{Component:\"kubelet\", Host:\"slave-2.xxx.xx\n x.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581195122, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581195122, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.004115       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"nginx.140fcbfba0bf987d\", GenerateName:\"\", Namespace:\"ntest\", SelfLink:\"/api/v1/namespaces/ntest/events/nginx.140fcbfba0bf987d\", UID:\"df5a7778-7975-11e5-8507-0016c200016e\", ResourceVersion:\"2107\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581195087, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208266660), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"ReplicationController\", Namespace:\"ntest\", Name:\"nginx\", UID:\"df58c468-7975-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2105\", FieldPath:\"\"}, Reason:\"successfulCreate\", Message:\"Created pod: nginx-hcr3s\", Source:api.EventSource{Component:\"replication-controller\", Host:\"\"}, FirstTimestamp:unve\n rsioned.Time{Time:time.Time{sec:63581195087, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581195087, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.004187       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster-evvzv.140fcdd946624fde\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster-evvzv.140fcdd946624fde\", UID:\"a6214638-797a-11e5-8507-0016c200016e\", ResourceVersion:\"2643\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197138, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208266780), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"xxxx-system\", Name:\"heapster-evvzv\", UID:\"649e1881-7964-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"46\", FieldPath:\"spec.containers{heapster}\"}, Reason:\"killing\", Message:\"Killing with docker id b04c2816d2a3\", Source:api.EventSource{Component:\"ku\n belet\", Host:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197138, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197138, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.004270       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster-evvzv.140fcdd956432916\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster-evvzv.140fcdd956432916\", UID:\"a649c61f-797a-11e5-8507-0016c200016e\", ResourceVersion:\"2646\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197138, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc2082668a0), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"xxxx-system\", Name:\"heapster-evvzv\", UID:\"649e1881-7964-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"46\", FieldPath:\"implicitly required container POD\"}, Reason:\"killing\", Message:\"Killing with docker id c693e95706fd\", Source:api.EventSource{Compo\n nent:\"kubelet\", Host:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197138, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197138, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.004412       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster-jcqpp.140fcdd9470bfea9\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster-jcqpp.140fcdd9470bfea9\", UID:\"a659462d-797a-11e5-8507-0016c200016e\", ResourceVersion:\"2647\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197138, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc2082669c0), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"xxxx-system\", Name:\"heapster-jcqpp\", UID:\"a6034b30-797a-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2639\", FieldPath:\"\"}, Reason:\"failedScheduling\", Message:\"Error scheduling: No suitable offers for pod/task\", Source:api.EventSource{Component:\"sc\n heduler\", Host:\"\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197138, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197138, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:3}\nI1023 11:43:20.004515       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster-jcqpp.140fcdd9a9de12d8\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster-jcqpp.140fcdd9a9de12d8\", UID:\"a71fcb9f-797a-11e5-8507-0016c200016e\", ResourceVersion:\"2648\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197140, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208266ae0), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"xxxx-system\", Name:\"heapster-jcqpp\", UID:\"a6034b30-797a-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2639\", FieldPath:\"\"}, Reason:\"scheduled\", Message:\"Successfully assigned heapster-jcqpp to slave-2.xxx.xxx.xxx\", Source:api.EventSource{Component:\n \"scheduler\", Host:\"\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197140, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197140, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.004584       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster-jcqpp.140fcdd9b5d14137\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster-jcqpp.140fcdd9b5d14137\", UID:\"a73f0b37-797a-11e5-8507-0016c200016e\", ResourceVersion:\"2651\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197140, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208266c00), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"xxxx-system\", Name:\"heapster-jcqpp\", UID:\"a6034b30-797a-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2639\", FieldPath:\"implicitly required container POD\"}, Reason:\"pulled\", Message:\"Pod container image \\\"gcr.io/google_containers/pause:0.8.0\\ http://gcr.io/google_containers/pause:0.8.0%5C\" alre\n ady present on machine\", Source:api.EventSource{Component:\"kubelet\", Host:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197140, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197140, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.004850       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster-jcqpp.140fcdd9d97c86a9\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster-jcqpp.140fcdd9d97c86a9\", UID:\"a799beef-797a-11e5-8507-0016c200016e\", ResourceVersion:\"2652\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197140, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208266d20), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"xxxx-system\", Name:\"heapster-jcqpp\", UID:\"a6034b30-797a-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2639\", FieldPath:\"implicitly required container POD\"}, Reason:\"created\", Message:\"Created with docker id 5c49e11cb86c\", Source:api.EventSource{Com\n ponent:\"kubelet\", Host:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197140, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197140, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.005646       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster-jcqpp.140fcdd9ee6724dd\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster-jcqpp.140fcdd9ee6724dd\", UID:\"a7cf47d2-797a-11e5-8507-0016c200016e\", ResourceVersion:\"2653\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197141, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208266e40), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"xxxx-system\", Name:\"heapster-jcqpp\", UID:\"a6034b30-797a-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2639\", FieldPath:\"implicitly required container POD\"}, Reason:\"started\", Message:\"Started with docker id 5c49e11cb86c\", Source:api.EventSource{Com\n ponent:\"kubelet\", Host:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197141, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197141, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.007830       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster-jcqpp.140fcdda01c88445\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster-jcqpp.140fcdda01c88445\", UID:\"a801013a-797a-11e5-8507-0016c200016e\", ResourceVersion:\"2654\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197141, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208266f60), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"xxxx-system\", Name:\"heapster-jcqpp\", UID:\"a6034b30-797a-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2639\", FieldPath:\"spec.containers{heapster}\"}, Reason:\"created\", Message:\"Created with docker id 80accce04661\", Source:api.EventSource{Component:\"\n kubelet\", Host:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197141, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197141, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.007911       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster-jcqpp.140fcdda0a45673e\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster-jcqpp.140fcdda0a45673e\", UID:\"a816b4bb-797a-11e5-8507-0016c200016e\", ResourceVersion:\"2656\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197141, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208267080), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"xxxx-system\", Name:\"heapster-jcqpp\", UID:\"a6034b30-797a-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2639\", FieldPath:\"spec.containers{heapster}\"}, Reason:\"started\", Message:\"Started with docker id 80accce04661\", Source:api.EventSource{Component:\"\n kubelet\", Host:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197141, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197141, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.007995       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster-jcqpp.140fce12c46a06ba\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster-jcqpp.140fce12c46a06ba\", UID:\"394f5e18-797b-11e5-8507-0016c200016e\", ResourceVersion:\"2722\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197385, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc2082671a0), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"xxxx-system\", Name:\"heapster-jcqpp\", UID:\"a6034b30-797a-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2639\", FieldPath:\"spec.containers{heapster}\"}, Reason:\"killing\", Message:\"Killing with docker id 80accce04661\", Source:api.EventSource{Component:\"\n kubelet\", Host:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197385, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197385, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.008007       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster-jcqpp.140fce12cd017ac7\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster-jcqpp.140fce12cd017ac7\", UID:\"39653e74-797b-11e5-8507-0016c200016e\", ResourceVersion:\"2723\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197385, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc2082672c0), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"xxxx-system\", Name:\"heapster-jcqpp\", UID:\"a6034b30-797a-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2639\", FieldPath:\"implicitly required container POD\"}, Reason:\"killing\", Message:\"Killing with docker id 5c49e11cb86c\", Source:api.EventSource{Com\n ponent:\"kubelet\", Host:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197385, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197385, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.008065       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster-jgxi4.140fce14e297cc9e\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster-jgxi4.140fce14e297cc9e\", UID:\"3ebb47a1-797b-11e5-8507-0016c200016e\", ResourceVersion:\"2733\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197394, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc2082673e0), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"xxxx-system\", Name:\"heapster-jgxi4\", UID:\"3e9b5e11-797b-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2729\", FieldPath:\"\"}, Reason:\"scheduled\", Message:\"Successfully assigned heapster-jgxi4 to slave-2.xxx.xxx.xxx\", Source:api.EventSource{Component:\n \"scheduler\", Host:\"\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197394, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197394, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.008093       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster-jgxi4.140fce14ec254690\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster-jgxi4.140fce14ec254690\", UID:\"3ed3c189-797b-11e5-8507-0016c200016e\", ResourceVersion:\"2736\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197394, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208267500), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"xxxx-system\", Name:\"heapster-jgxi4\", UID:\"3e9b5e11-797b-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2729\", FieldPath:\"implicitly required container POD\"}, Reason:\"pulled\", Message:\"Pod container image \\\"gcr.io/google_containers/pause:0.8.0\\ http://gcr.io/google_containers/pause:0.8.0%5C\" alre\n ady present on machine\", Source:api.EventSource{Component:\"kubelet\", Host:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197394, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197394, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.008137       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster-jgxi4.140fce14ff823dbf\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster-jgxi4.140fce14ff823dbf\", UID:\"3f05473a-797b-11e5-8507-0016c200016e\", ResourceVersion:\"2737\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197394, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208267620), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"xxxx-system\", Name:\"heapster-jgxi4\", UID:\"3e9b5e11-797b-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2729\", FieldPath:\"implicitly required container POD\"}, Reason:\"created\", Message:\"Created with docker id 98f69c198083\", Source:api.EventSource{Com\n ponent:\"kubelet\", Host:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197394, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197394, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.008202       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster-jgxi4.140fce1509f8edbf\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster-jgxi4.140fce1509f8edbf\", UID:\"3f201ea3-797b-11e5-8507-0016c200016e\", ResourceVersion:\"2738\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197395, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208267740), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"xxxx-system\", Name:\"heapster-jgxi4\", UID:\"3e9b5e11-797b-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2729\", FieldPath:\"implicitly required container POD\"}, Reason:\"started\", Message:\"Started with docker id 98f69c198083\", Source:api.EventSource{Com\n ponent:\"kubelet\", Host:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197395, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197395, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.008248       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster-jgxi4.140fce151b29c8b4\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster-jgxi4.140fce151b29c8b4\", UID:\"3f4c1e99-797b-11e5-8507-0016c200016e\", ResourceVersion:\"2739\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197395, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208267860), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"xxxx-system\", Name:\"heapster-jgxi4\", UID:\"3e9b5e11-797b-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2729\", FieldPath:\"spec.containers{heapster}\"}, Reason:\"created\", Message:\"Created with docker id 0fa3de331de1\", Source:api.EventSource{Component:\"\n kubelet\", Host:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197395, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197395, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.008313       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster-jgxi4.140fce1522e704b3\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster-jgxi4.140fce1522e704b3\", UID:\"3f60078e-797b-11e5-8507-0016c200016e\", ResourceVersion:\"2740\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197395, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208267980), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"Pod\", Namespace:\"xxxx-system\", Name:\"heapster-jgxi4\", UID:\"3e9b5e11-797b-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2729\", FieldPath:\"spec.containers{heapster}\"}, Reason:\"started\", Message:\"Started with docker id 0fa3de331de1\", Source:api.EventSource{Component:\"\n kubelet\", Host:\"slave-2.xxx.xxx.xxx\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197395, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197395, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.008348       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster.140fcdd93b1ea423\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster.140fcdd93b1ea423\", UID:\"a6044b31-797a-11e5-8507-0016c200016e\", ResourceVersion:\"2640\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197138, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208267aa0), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"ReplicationController\", Namespace:\"xxxx-system\", Name:\"heapster\", UID:\"6481d845-7964-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"56\", FieldPath:\"\"}, Reason:\"successfulCreate\", Message:\"Created pod: heapster-jcqpp\", Source:api.EventSource{Component:\"replication-controller\", H\n ost:\"\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197138, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197138, nsec:0, loc:(_time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.008378       1 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:api.ObjectMeta{Name:\"heapster.140fce14d6826ebd\", GenerateName:\"\", Namespace:\"xxxx-system\", SelfLink:\"/api/v1/namespaces/xxxx-system/events/heapster.140fce14d6826ebd\", UID:\"3e9c599b-797b-11e5-8507-0016c200016e\", ResourceVersion:\"2730\", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63581197394, nsec:0, loc:(_time.Location)(0x11e7c40)}}, DeletionTimestamp:(_unversioned.Time)(0xc208267bc0), DeletionGracePeriodSeconds:(_int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil)}, InvolvedObject:api.ObjectReference{Kind:\"ReplicationController\", Namespace:\"xxxx-system\", Name:\"heapster\", UID:\"3e9ad1bf-797b-11e5-8507-0016c200016e\", APIVersion:\"v1\", ResourceVersion:\"2728\", FieldPath:\"\"}, Reason:\"successfulCreate\", Message:\"Created pod: heapster-jgxi4\", Source:api.EventSource{Component:\"replication-controller\",\n  Host:\"\"}, FirstTimestamp:unversioned.Time{Time:time.Time{sec:63581197394, nsec:0, loc:(_time.Location)(0x11e7c40)}}, LastTimestamp:unversioned.Time{Time:time.Time{sec:63581197394, nsec:0, loc:(*time.Location)(0x11e7c40)}}, Count:1}\nI1023 11:43:20.008425       1 kube_events.go:216] Fetched list of events from the master\nI1023 11:43:20.040539       1 pods.go:134] Ignoring pod kibana-logging-v1-cf792 with namespace xxxx-system since namespace object was not found\nI1023 11:43:20.040595       1 pods.go:134] Ignoring pod fluentd-elasticsearch-slave-2.xxx.xxx.xxx with namespace kube-system since namespace object was not found\nI1023 11:43:20.040613       1 pods.go:134] Ignoring pod nginx-hcr3s with namespace ntest since namespace object was not found\nI1023 11:43:20.040629       1 pods.go:134] Ignoring pod elasticsearch-logging-v1-0pvmr with namespace xxxx-system since namespace object was not found\nI1023 11:43:20.040659       1 pods.go:134] Ignoring pod heapster-jgxi4 with namespace xxxx-system since namespace object was not found\nI1023 11:43:20.040675       1 pods.go:134] Ignoring pod infludb-grafana-didcq with namespace xxxx-system since namespace object was not found\nI1023 11:43:20.040692       1 pods.go:134] Ignoring pod kube-dns-v8-asywl with namespace xxxx-system since namespace object was not found\nI1023 11:43:20.040710       1 pods.go:134] Ignoring pod fluentd-elasticsearch-slave-1.xxx.xxx.xxx with namespace kube-system since namespace object was not found\nI1023 11:43:20.040800       1 kube_nodes.go:126] Fetched list of nodes from the master\nI1023 11:43:20.040885       1 kubelet.go:110] about to query kubelet using url: \"http://xxx.xxx.xxx.108:10255/stats/default/nginx-1hsi6/b306a93a-7977-11e5-8507-0016c200016e/nginx\"\nI1023 11:43:20.125467       1 manager.go:175] completed scraping data from sources. Errors: []\nI1023 11:43:25.000274       1 manager.go:162] starting to scrape data from sources start: 2015-10-23 11:43:20 +0000 UTC end: 2015-10-23 11:43:25 +0000 UTC\nI1023 11:43:25.000419       1 manager.go:103] attempting to get data from source \"Kube Pods Source\"\nI1023 11:43:25.000740       1 manager.go:103] attempting to get data from source \"Kube Node Metrics Source\"\nI1023 11:43:25.000886       1 manager.go:103] attempting to get data from source \"Kube Events Source\"\nI1023 11:43:25.000926       1 kube_events.go:216] Fetched list of events from the master\nI1023 11:43:25.092743       1 pods.go:134] Ignoring pod fluentd-elasticsearch-slave-2.xxx.xxx.xxx with namespace kube-system since namespace object was not found\nI1023 11:43:25.092800       1 pods.go:134] Ignoring pod nginx-hcr3s with namespace ntest since namespace object was not found\nI1023 11:43:25.092820       1 pods.go:134] Ignoring pod kibana-logging-v1-cf792 with namespace xxxx-system since namespace object was not found\nI1023 11:43:25.092842       1 pods.go:134] Ignoring pod fluentd-elasticsearch-slave-1.xxx.xxx.xxx with namespace kube-system since namespace object was not found\nI1023 11:43:25.092857       1 pods.go:134] Ignoring pod elasticsearch-logging-v1-0pvmr with namespace xxxx-system since namespace object was not found\nI1023 11:43:25.092871       1 pods.go:134] Ignoring pod heapster-jgxi4 with namespace xxxx-system since namespace object was not found\nI1023 11:43:25.092886       1 pods.go:134] Ignoring pod infludb-grafana-didcq with namespace xxxx-system since namespace object was not found\nI1023 11:43:25.092901       1 pods.go:134] Ignoring pod kube-dns-v8-asywl with namespace xxxx-system since namespace object was not found\nI1023 11:43:25.093124       1 kube_nodes.go:126] Fetched list of nodes from the master\nI1023 11:43:25.093306       1 kubelet.go:110] about to query kubelet using url: \"http://xxx.xxx.xxx.108:10255/stats/default/nginx-1hsi6/b306a93a-7977-11e5-8507-0016c200016e/nginx\"\nI1023 11:43:25.173166       1 manager.go:175] completed scraping data from sources. Errors: []\nI1023 11:43:25.626728       1 external.go:138] Storing Timeseries to \"InfluxDB Sink\"\nI1023 11:43:25.626728       1 external.go:142] Storing Events to \"InfluxDB Sink\"\nI1023 11:43:30.000321       1 manager.go:162] starting to scrape data from sources start: 2015-10-23 11:43:25 +0000 UTC end: 2015-10-23 11:43:30 +0000 UTC\nI1023 11:43:30.000475       1 manager.go:103] attempting to get data from source \"Kube Pods Source\"\nI1023 11:43:30.001144       1 manager.go:103] attempting to get data from source \"Kube Node Metrics Source\"\nI1023 11:43:30.001314       1 manager.go:103] attempting to get data from source \"Kube Events Source\"\nI1023 11:43:30.001344       1 kube_events.go:216] Fetched list of events from the master\nI1023 11:43:30.077561       1 pods.go:134] Ignoring pod fluentd-elasticsearch-slave-1.xxx.xxx.xxx with namespace kube-system since namespace object was not found\nI1023 11:43:30.077625       1 pods.go:134] Ignoring pod elasticsearch-logging-v1-0pvmr with namespace xxxx-system since namespace object was not found\nI1023 11:43:30.077645       1 pods.go:134] Ignoring pod heapster-jgxi4 with namespace xxxx-system since namespace object was not found\nI1023 11:43:30.077661       1 pods.go:134] Ignoring pod infludb-grafana-didcq with namespace xxxx-system since namespace object was not found\nI1023 11:43:30.077677       1 pods.go:134] Ignoring pod kube-dns-v8-asywl with namespace xxxx-system since namespace object was not found\nI1023 11:43:30.077696       1 pods.go:134] Ignoring pod nginx-hcr3s with namespace ntest since namespace object was not found\nI1023 11:43:30.077730       1 pods.go:134] Ignoring pod kibana-logging-v1-cf792 with namespace xxxx-system since namespace object was not found\nI1023 11:43:30.077746       1 pods.go:134] Ignoring pod fluentd-elasticsearch-slave-2.xxx.xxx.xxx with namespace kube-system since namespace object was not found\nI1023 11:43:30.077815       1 kube_nodes.go:126] Fetched list of nodes from the master\nI1023 11:43:30.086011       1 kubelet.go:110] about to query kubelet using url: \"http://xxx.xxx.xxx.108:10255/stats/default/nginx-1hsi6/b306a93a-7977-11e5-8507-0016c200016e/nginx\"\nI1023 11:43:30.126842       1 manager.go:175] completed scraping data from sources. Errors: []\nI1023 11:43:35.000695       1 manager.go:162] starting to scrape data from sources start: 2015-10-23 11:43:30 +0000 UTC end: 2015-10-23 11:43:35 +0000 UTC\nI1023 11:43:35.000880       1 manager.go:103] attempting to get data from source \"Kube Events Source\"\nI1023 11:43:35.000917       1 kube_events.go:216] Fetched list of events from the master\nI1023 11:43:35.000880       1 manager.go:103] attempting to get data from source \"Kube Node Metrics Source\"\nI1023 11:43:35.000835       1 manager.go:103] attempting to get data from source \"Kube Pods Source\"\nI1023 11:43:35.089471       1 kube_nodes.go:126] Fetched list of nodes from the master\nI1023 11:43:35.089576       1 pods.go:134] Ignoring pod kibana-logging-v1-cf792 with namespace xxxx-system since namespace object was not found\nI1023 11:43:35.089607       1 pods.go:134] Ignoring pod fluentd-elasticsearch-slave-2.xxx.xxx.xxx with namespace kube-system since namespace object was not found\nI1023 11:43:35.089624       1 pods.go:134] Ignoring pod nginx-hcr3s with namespace ntest since namespace object was not found\nI1023 11:43:35.089639       1 pods.go:134] Ignoring pod elasticsearch-logging-v1-0pvmr with namespace xxxx-system since namespace object was not found\nI1023 11:43:35.089654       1 pods.go:134] Ignoring pod heapster-jgxi4 with namespace xxxx-system since namespace object was not found\nI1023 11:43:35.089668       1 pods.go:134] Ignoring pod infludb-grafana-didcq with namespace xxxx-system since namespace object was not found\nI1023 11:43:35.089683       1 pods.go:134] Ignoring pod kube-dns-v8-asywl with namespace xxxx-system since namespace object was not found\nI1023 11:43:35.089701       1 pods.go:134] Ignoring pod fluentd-elasticsearch-slave-1.xxx.xxx.xxx with namespace kube-system since namespace object was not found\nI1023 11:43:35.089786       1 kubelet.go:110] about to query kubelet using url: \"http://xxx.xxx.xxx.108:10255/stats/default/nginx-1hsi6/b306a93a-7977-11e5-8507-0016c200016e/nginx\"\nI1023 11:43:35.183477       1 manager.go:175] completed scraping data from sources. Errors: []\nI1023 11:43:35.818493       1 external.go:142] Storing Events to \"InfluxDB Sink\"\nI1023 11:43:35.818418       1 external.go:138] Storing Timeseries to \"InfluxDB Sink\"\nI1023 11:43:40.000322       1 manager.go:162] starting to scrape data from sources start: 2015-10-23 11:43:35 +0000 UTC end: 2015-10-23 11:43:40 +0000 UTC\nI1023 11:43:40.000499       1 manager.go:103] attempting to get data from source \"Kube Node Metrics Source\"\nI1023 11:43:40.000453       1 manager.go:103] attempting to get data from source \"Kube Pods Source\"\nI1023 11:43:40.002049       1 manager.go:103] attempting to get data from source \"Kube Events Source\"\nI1023 11:43:40.002298       1 kube_events.go:216] Fetched list of events from the master\nI1023 11:43:40.046790       1 pods.go:134] Ignoring pod fluentd-elasticsearch-slave-2.xxx.xxx.xxx with namespace kube-system since namespace object was not found\nI1023 11:43:40.046809       1 pods.go:134] Ignoring pod nginx-hcr3s with namespace ntest since namespace object was not found\nI1023 11:43:40.046818       1 pods.go:134] Ignoring pod kibana-logging-v1-cf792 with namespace xxxx-system since namespace object was not found\nI1023 11:43:40.046829       1 pods.go:134] Ignoring pod fluentd-elasticsearch-slave-1.xxx.xxx.xxx with namespace kube-system since namespace object was not found\nI1023 11:43:40.046836       1 pods.go:134] Ignoring pod elasticsearch-logging-v1-0pvmr with namespace xxxx-system since namespace object was not found\nI1023 11:43:40.046844       1 pods.go:134] Ignoring pod heapster-jgxi4 with namespace xxxx-system since namespace object was not found\nI1023 11:43:40.046851       1 pods.go:134] Ignoring pod infludb-grafana-didcq with namespace xxxx-system since namespace object was not found\nI1023 11:43:40.046858       1 pods.go:134] Ignoring pod kube-dns-v8-asywl with namespace xxxx-system since namespace object was not found\nI1023 11:43:40.046947       1 kubelet.go:110] about to query kubelet using url: \"http://xxx.xxx.xxx.108:10255/stats/default/nginx-1hsi6/b306a93a-7977-11e5-8507-0016c200016e/nginx\"\nI1023 11:43:40.088566       1 kube_nodes.go:126] Fetched list of nodes from the master\nAs you can see, there are several lines like the following: \"Ignoring pod\nPUT POD NAME HERE with namespace NAMESPACE since namespace object was not\nfound\".\nDo you know what does it means?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/668.\n. I will try to find the original issue.\n\nOn Mon, Oct 26, 2015 at 1:08 AM, R\u00e9my Phelipot notifications@github.com\nwrote:\n\nIndeed, I'm running kubernetes v1.0.4, so the issue is probably related to\nkubernetes. Do you have the link of this issue?\nI will update kubernetes and check if it solves the problem.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/668#issuecomment-151055644\n.\n. In general I'm thinking logging and forging ahead is better than\ncrashlooping. Imagine a user adding a sink dynamically - we do not want\nheapster to crash in such a scenario right?\n\nOn Sat, Oct 24, 2015 at 1:13 AM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nLGTM if this behaviour is intended\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/670#issuecomment-150774872.\n. What if one of the sinks were to crash if its endpoint isn't available?\nThat is not acceptable in a dynamic cluster environment.\nFrom that perspective, even if I were to configure a sink via flags, it\nmust continue to retry ideally, and not cause heapster to crash.\nAlso sinks are just one functionality of heapster. The http APIs are\nexpected to function even if the sinks fail. May be we can raise events, if\nrunning in a kube environment, in case of sink failures.\n\nOn Sun, Oct 25, 2015 at 11:22 AM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nNote that this just returns an error, it doesn't exit the program. It's up\nto the caller to decide whether to treat the error as fatal.\nheapster.go does error and exit, since you want that if you cannot bring\nup the sinks specified via the command line.\nOTOH, api/v1/api.go will just return a StatusInternalServerError to the\nHTTP client and not exit.\nI think both cases are correct, so I don't see the point of this PR at the\nmoment. It would make --sinks unintuitive and hide errors from the API -\nif you tell it to add a sink and it fails, it would not return the error\nresponse.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/670#issuecomment-150951526.\n. Ah I get what you are saying now.. Will fix the PR!\n. Crashlooping is not desirable. It adds unnecessary overhead to the orchestration system - dead containers lying around, signals firing in all directions, etc.\n\nHeapster now supports multiple sources and sinks. It is possible for some of the sources or sinks to not be available when heapster starts. In addition to that the sources and sinks lifecycles are decoupled from that of heapster. I think a good cluster application should be resilient to the dynamic nature of cluster workloads.\nThis PR deals with the configuration issues specifically. A user can look at the logs and then debug any issues that they encounter with heapster.\nI have been updating a few sinks to be resilient to backend outages as well. As part of this I also added a utility library that can help manage backend availability for sinks.\n. @mwielgus: Can you review this PR? This will help prevent heapster from crashlooping if GCM APIs or scopes are not enabled. \n. cc @dchen1107 \n. cc @piosz @mwielgus \n. Setup GOPATH as per instructions on Golang's website.\nThen run\nbash\ngo get -d k8s.io/heapster`.\ncd $GOPATH/src/k8s.io/heapster\nmake\nOn Fri, Oct 23, 2015 at 9:04 PM, Juneja notifications@github.com wrote:\n\nI try to install heapster in standalone on kubernetes cluster Master.\nwhen i make heapster,then entry question:\n[root@wlan-cloudserver31 heapster]# pwd\n/etc/kubernetes/heapster\n[root@wlan-cloudserver31 heapster]# make\nrm -f heapster\nrm -f deploy/docker/heapster\ngo get -u github.com/tools/godep\nGOOS=linux GOARCH=amd64 CGO_ENABLED=0 godep go build ./...\nhandlers.go:24:2: cannot find package \"k8s.io/heapster/api/v1\" in any of:\n/usr/lib/golang/src/k8s.io/heapster/api/v1 (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io/heapster/api/v1\n(from $GOPATH)\n/home/gopath/src/k8s.io/heapster/api/v1\nhandlers.go:25:2: cannot find package \"k8s.io/heapster/expapi/v2\" in any\nof:\n/usr/lib/golang/src/k8s.io/heapster/expapi/v2 (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io/heapster/expapi/v2\n(from $GOPATH)\n/home/gopath/src/k8s.io/heapster/expapi/v2\nsources.go:20:2: cannot find package \"k8s.io/heapster/extpoints\" in any\nof:\n/usr/lib/golang/src/k8s.io/heapster/extpoints (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io/heapster/extpoints\n(from $GOPATH)\n/home/gopath/src/k8s.io/heapster/extpoints\nhandlers.go:26:2: cannot find package \"k8s.io/heapster/manager\" in any of:\n/usr/lib/golang/src/k8s.io/heapster/manager (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io/heapster/manager\n(from $GOPATH)\n/home/gopath/src/k8s.io/heapster/manager\nhandlers.go:27:2: cannot find package \"k8s.io/heapster/sinks\" in any of:\n/usr/lib/golang/src/k8s.io/heapster/sinks (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io/heapster/sinks\n(from $GOPATH)\n/home/gopath/src/k8s.io/heapster/sinks\nheapster.go:32:2: cannot find package \"k8s.io/heapster/sinks/cache\" in\nany of:\n/usr/lib/golang/src/k8s.io/heapster/sinks/cache (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io/heapster/sinks/cache\n(from $GOPATH)\n/home/gopath/src/k8s.io/heapster/sinks/cache\nhandlers.go:28:2: cannot find package \"k8s.io/heapster/sources\" in any of:\n/usr/lib/golang/src/k8s.io/heapster/sources (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io/heapster/sources\n(from $GOPATH)\n/home/gopath/src/k8s.io/heapster/sources\nhandlers.go:29:2: cannot find package \"k8s.io/heapster/sources/api\" in\nany of:\n/usr/lib/golang/src/k8s.io/heapster/sources/api (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io/heapster/sources/api\n(from $GOPATH)\n/home/gopath/src/k8s.io/heapster/sources/api\nhandlers.go:30:2: cannot find package \"k8s.io/heapster/validate\" in any\nof:\n/usr/lib/golang/src/k8s.io/heapster/validate (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io/heapster/validate\n(from $GOPATH)\n/home/gopath/src/k8s.io/heapster/validate\nheapster.go:34:2: cannot find package \"k8s.io/heapster/version\" in any of:\n/usr/lib/golang/src/k8s.io/heapster/version (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io/heapster/version\n(from $GOPATH)\n/home/gopath/src/k8s.io/heapster/version\napi/v1/api.go:21:2: cannot find package \"k8s.io/heapster/api/v1/types\" in\nany of:\n/usr/lib/golang/src/k8s.io/heapster/api/v1/types (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/\nk8s.io/heapster/api/v1/types (from $GOPATH)\n/home/gopath/src/k8s.io/heapster/api/v1/types\napi/v1/model_handlers.go:27:2: cannot find package \"k8s.io/heapster/model\"\nin any of:\n/usr/lib/golang/src/k8s.io/heapster/model (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io/heapster/model\n(from $GOPATH)\n/home/gopath/src/k8s.io/heapster/model\napi/v1/api.go:23:2: cannot find package \"k8s.io/heapster/sinks/api\" in\nany of:\n/usr/lib/golang/src/k8s.io/heapster/sinks/api (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io/heapster/sinks/api\n(from $GOPATH)\n/home/gopath/src/k8s.io/heapster/sinks/api\napi/v1/model_handlers.go:28:2: cannot find package \"\nk8s.io/heapster/store/statstore\" in any of:\n/usr/lib/golang/src/k8s.io/heapster/store/statstore (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/\nk8s.io/heapster/store/statstore (from $GOPATH)\n/home/gopath/src/k8s.io/heapster/store/statstore\napi/v1/api.go:24:2: cannot find package \"k8s.io/heapster/util\" in any of:\n/usr/lib/golang/src/k8s.io/heapster/util (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io/heapster/util (from\n$GOPATH)\n/home/gopath/src/k8s.io/heapster/util\nexpapi/v2/metrics_handlers.go:27:2: cannot find package \"\nk8s.io/heapster/expapi/v2/types\" in any of:\n/usr/lib/golang/src/k8s.io/heapster/expapi/v2/types (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/\nk8s.io/heapster/expapi/v2/types (from $GOPATH)\n/home/gopath/src/k8s.io/heapster/expapi/v2/types\nmodel/aggregation.go:21:2: cannot find package \"\nk8s.io/heapster/store/daystore\" in any of:\n/usr/lib/golang/src/k8s.io/heapster/store/daystore (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/\nk8s.io/heapster/store/daystore (from $GOPATH)\n/home/gopath/src/k8s.io/heapster/store/daystore\nsinks/modules.go:18:2: cannot find package \"k8s.io/heapster/sinks/gcl\" in\nany of:\n/usr/lib/golang/src/k8s.io/heapster/sinks/gcl (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io/heapster/sinks/gcl\n(from $GOPATH)\n/home/gopath/src/k8s.io/heapster/sinks/gcl\nsinks/modules.go:19:2: cannot find package \"k8s.io/heapster/sinks/gcm\" in\nany of:\n/usr/lib/golang/src/k8s.io/heapster/sinks/gcm (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io/heapster/sinks/gcm\n(from $GOPATH)\n/home/gopath/src/k8s.io/heapster/sinks/gcm\nsinks/modules.go:20:2: cannot find package \"\nk8s.io/heapster/sinks/gcmautoscaling\" in any of:\n/usr/lib/golang/src/k8s.io/heapster/sinks/gcmautoscaling (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/\nk8s.io/heapster/sinks/gcmautoscaling (from $GOPATH)\n/home/gopath/src/k8s.io/heapster/sinks/gcmautoscaling\nsinks/modules.go:21:2: cannot find package \"k8s.io/heapster/sinks/hawkular\"\nin any of:\n/usr/lib/golang/src/k8s.io/heapster/sinks/hawkular (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/\nk8s.io/heapster/sinks/hawkular (from $GOPATH)\n/home/gopath/src/k8s.io/heapster/sinks/hawkular\nsinks/modules.go:22:2: cannot find package \"k8s.io/heapster/sinks/influxdb\"\nin any of:\n/usr/lib/golang/src/k8s.io/heapster/sinks/influxdb (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/\nk8s.io/heapster/sinks/influxdb (from $GOPATH)\n/home/gopath/src/k8s.io/heapster/sinks/influxdb\nsinks/cache/impl.go:23:2: cannot find package \"k8s.io/heapster/store\" in\nany of:\n/usr/lib/golang/src/k8s.io/heapster/store (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io/heapster/store\n(from $GOPATH)\n/home/gopath/src/k8s.io/heapster/store\nsinks/gcl/driver.go:29:2: cannot find package \"k8s.io/heapster/util/gce\"\nin any of:\n/usr/lib/golang/src/k8s.io/heapster/util/gce (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io/heapster/util/gce\n(from $GOPATH)\n/home/gopath/src/k8s.io/heapster/util/gce\nsinks/gcm/core.go:32:2: cannot find package \"k8s.io/heapster/util/gcstore\"\nin any of:\n/usr/lib/golang/src/k8s.io/heapster/util/gcstore (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/\nk8s.io/heapster/util/gcstore (from $GOPATH)\n/home/gopath/src/k8s.io/heapster/util/gcstore\nsources/cadvisor.go:32:2: cannot find package \"\nk8s.io/heapster/sources/datasource\" in any of:\n/usr/lib/golang/src/k8s.io/heapster/sources/datasource (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/\nk8s.io/heapster/sources/datasource (from $GOPATH)\n/home/gopath/src/k8s.io/heapster/sources/datasource\nsources/cadvisor.go:33:2: cannot find package \"\nk8s.io/heapster/sources/nodes\" in any of:\n/usr/lib/golang/src/k8s.io/heapster/sources/nodes (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/\nk8s.io/heapster/sources/nodes (from $GOPATH)\n/home/gopath/src/k8s.io/heapster/sources/nodes\nstore/daystore/day_store.go:26:2: cannot find package \"\nk8s.io/heapster/third_party/window\" in any of:\n/usr/lib/golang/src/k8s.io/heapster/third_party/window (from $GOROOT)\n/etc/kubernetes/heapster/Godeps/_workspace/src/\nk8s.io/heapster/third_party/window (from $GOPATH)\n/home/gopath/src/k8s.io/heapster/third_party/window\ngodep: go exit status 1\nmake: *** [build] Error 1\n[root@wlan-cloudserver31 heapster]#\n[root@wlan-cloudserver31 heapster]# ll /home/gopath/bin/\ntotal 8836\n-rwxr-xr-x. 1 root root 9047088 Oct 24 09:07 godep\n[root@wlan-cloudserver31 heapster]# ll /home/gopath/pkg/\ntotal 4\ndrwxr-xr-x. 3 root root 4096 Oct 23 17:36 linux_amd64\n[root@wlan-cloudserver31 heapster]#\n[root@wlan-cloudserver31 heapster]# find / -name \"k8s.io\" -print\n/etc/kubernetes/heapster/Godeps/_workspace/src/k8s.io\n[root@wlan-cloudserver31 heapster]# ll Godeps/_workspace/src/k8s.io/\ntotal 0\ndrwxr-xr-x. 5 root root 47 Oct 23 16:49 kubernetes\n[root@wlan-cloudserver31 heapster]#\nwhere can i get the package \"k8s.io/heapster/api/v1\nhttp://k8s.io/heapster/api/v1\"?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/673.\n. Can you run heapster with --vmodule=*=4 flag and post its logs?\n\nOn Sun, Oct 25, 2015 at 8:10 PM, Juneja notifications@github.com wrote:\n\n@vishh https://github.com/vishh thanks for you help, i got the heapster\nrun in standalone with curl results.\ni have another doubt, i config three nodes in /var/run/heapster/hosts, but\ni only get curl results of only one node!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/673#issuecomment-151009520\n.\n. pass the flag arguments within \"\".\n./heapster --vmodule=*=4 --source=\"kubernetes:\nhttp://172.27.8.210:8080?inClusterConfig=false&auth=''&useServiceAccount=false\nhttp://172.27.8.210:8080/?inClusterConfig=false&auth=%27%27&useServiceAccount=false\n\" --sink=\"influxdb:http://172.27.8.109:8086\"\n\nOn Tue, Oct 27, 2015 at 5:06 AM, Juneja notifications@github.com wrote:\n\n@vishh https://github.com/vishh add \u2018--vmodule=\n=4\u2019 can get more nodes stats. but i have another doubt in standalone\ntype\uff0cthen --sink option doesn't validated, it didn't write any data to\ninfluxdb [root@wlan-cloudserver31 heapster]# ./heapster --vmodule==4\n--source=kubernetes:\nhttp://172.27.8.210:8080?inClusterConfig=false&auth=''&useServiceAccount=false\n--sink=influxdb:http://172.27.8.109:8086\n[1] 131344\n[2] 131345\nbash: --sink=influxdb:http://172.27.8.109:8086: No such file or directory\n[2]+ Done auth=''\n[root@wlan-cloudserver31 heapster]# I1027 19:59:23.922591 131344\nheapster.go:61] ./heapster --vmodule=*=4 --source=kubernetes:\nhttp://172.27.8.210:8080?inClusterConfig=false\nI1027 19:59:23.922779 131344 heapster.go:62] Heapster version 0.18.0\nI1027 19:59:23.923265 131344 kube_factory.go:172] Using Kubernetes client\nwith master \"http://172.27.8.210:8080\" and version \"v1\"\nI1027 19:59:23.923280 131344 kube_factory.go:173] Using kubelet port 10255\nI1027 19:59:23.923807 131344 external.go:214] Updated sinks: []\nI1027 19:59:23.925529 131344 external.go:116] no timeseries data between\n0001-01-01 00:00:00 +0000 UTC and 0001-01-01 00:00:00 +0000 UTC\nI1027 19:59:23.933108 131344 heapster.go:72] Starting heapster on port 8082\nI1027 19:59:30.000694 131344 manager.go:162] starting to scrape data from\nsources start: 2015-10-27 19:59:25 +0800 CST end: 2015-10-27 19:59:30 +0800\nCST\nI1027 19:59:30.000938 131344 manager.go:103] attempting to get data from\nsource \"Kube Events Source\"\nI1027 19:59:30.001311 131344 kube_events.go:81] Received new event:\napi.Event{TypeMeta:unversioned.Type\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/673#issuecomment-151470319\n.\n. I see some metrics aggregation errors:\n\n```\nI1028 09:39:00.006839 142911 kube_nodes.go:59] Failed to get container\nstats from Kubelet on node \"wlan-cloudserver35\"\nI1028 09:39:00.028767 142911 kube_nodes.go:64] No container stats from\nKubelet on node \"wlan-cloudserver34\".\nTake a look at kubelet logs.\nOn Tue, Oct 27, 2015 at 7:06 PM, Juneja notifications@github.com wrote:\n\n@vishh https://github.com/vishh pass flag within '\"\"' then it run ok,\nbut there is no data in influxdb\nmore logs:\n[root@wlan-cloudserver31 heapster]# ./heapster --vmodule=\n=3\n--source=\"kubernetes:http://172.27.8.210:8080?inClusterConfig=false&auth=''&useServiceAccount=false\nhttp://172.27.8.210:8080?inClusterConfig=false&auth=''&useServiceAccount=false\"\n--sink=\"influxdb:http://172.27.8.109:8086 http://172.27.8.109:8086\" I1028\n09:37:54.097134 142891 heapster.go:61] ./heapster --vmodule==3\n--source=kubernetes:\nhttp://172.27.8.210:8080?inClusterConfig=false&auth=''&useServiceAccount=false\n--sink=influxdb:http://172.27.8.109:8086\nI1028 09:37:54.097414 142891 heapster.go:62] Heapster version 0.18.0\nF1028 09:37:54.097524 142891 heapster.go:68] stat '': no such file or\ndirectory\n[root@wlan-cloudserver31 heapster]# lsof -i:8082\n[root@wlan-cloudserver31 heapster]# ./heapster --vmodule==3\n--source=\"kubernetes:http://172.27.8.210:8080?inClusterConfig=false&auth=''&useServiceAccount=false\nhttp://172.27.8.210:8080?inClusterConfig=false&auth=''&useServiceAccount=false\"\n--sink=\"influxd[root@wlan-cloudserver31 heapster]# ./heapster --vmodule==3\n--source=\"kubernetes:\nhttp://172.27.8.210:8080?inClusterConfig=false&auth=''useServiceAccount=false\"\n--sink=\"influxdb[root@wlan-cloudserver31 heapster]# ./heapster --vmodule==3\n--source=\"kubernetes:http://172.27.8.210:8080?inClusterConfig=false&auth='useServiceAccount=false\nhttp://172.27.8.210:8080?inClusterConfig=false&auth='useServiceAccount=false\"\n--sink=\"influxdb:[root@wlan-cloudserver31 heapster]# ./heapster --vmodule==3\n--source=\"kubernetes:\nhttp://172.27.8.210:8080?inClusterConfig=false&auth=useServiceAccount=false\"\n--sink=\"influxdb:h[root@wlan-cloudserver31 heapster]# ./heapster\n--vmodule==3\n--source=\"kubernetes:http://172.27.8.210:8080?inClusterConfig=false&authuseServiceAccount=false\nhttp://172.27.8.210:8080?inClusterConfig=false&authuseServiceAccount=false\"\n--sink=\"influxdb:ht[root@wlan-cloudserver31 heapster]# ./heapster\n--vmodule==3 --source=\"kubernetes:\nhttp://172.27.8.210:8080?inClusterConfig=false&autuseServiceAccount=false\"\n--sink=\"influxdb:htt[root@wlan-cloudserver31 heapster]# ./heapster\n--vmodule==3\n--source=\"kubernetes:http://172.27.8.210:8080?inClusterConfig=false&auuseServiceAccount=false\nhttp://172.27.8.210:8080?inClusterConfig=false&auuseServiceAccount=false\"\n--sink=\"influxdb:http[root@wlan-cloudserver31 heapster]# ./heapster\n--vmodule==3 --source=\"kubernetes:\nhttp://172.27.8.210:8080?inClusterConfig=false&auseServiceAccount=false\"\n--sink=\"influxdb:http:[root@wlan-cloudserver31 heapster]# ./heapster\n--vmodule==3\n--source=\"kubernetes:http://172.27.8.210:8080?inClusterConfig=false&useServiceAccount=false\nhttp://172.27.8.210:8080?inClusterConfig=false&useServiceAccount=false\"\n--sink=\"influxdb:http:/[root@wlan-cloudserver31 heapster]# ./heapster\n--vmodule==3 --source=\"kubernetes:\nhttp://172.27.8.210:8080?inClusterConfig=false&useServiceAccount=false\"\n--sink=\"influxdb:http://172.27.8.109:8086\"\nI1028 09:38:42.706153 142911 heapster.go:61] ./heapster --vmodule=*=3\n--source=kubernetes:\nhttp://172.27.8.210:8080?inClusterConfig=false&useServiceAccount=false\n--sink=influxdb:http://172.27.8.109:8086\nI1028 09:38:42.706631 142911 heapster.go:62] Heapster version 0.18.0\nI1028 09:38:42.707107 142911 kube_factory.go:172] Using Kubernetes client\nwith master \"http://172.27.8.210:8080\" and version \"v1\"\nI1028 09:38:42.707123 142911 kube_factory.go:173] Using kubelet port 10255\nI1028 09:38:42.708459 142911 external.go:116] no timeseries data between\n0001-01-01 00:00:00 +0000 UTC and 0001-01-01 00:00:00 +0000 UTC\nI1028 09:38:42.735340 142911 driver.go:316] created influxdb sink with\noptions: {root root 172.27.8.109:8086 k8s false}\nI1028 09:38:42.804073 142911 driver.go:245] Created database \"k8s\" on\ninfluxDB server at \"172.27.8.109:8086\"\nI1028 09:38:42.804360 142911 external.go:214] Updated sinks: [0xc2082ae580]\nI1028 09:38:42.813886 142911 heapster.go:72] Starting heapster on port 8082\nI1028 09:38:50.000694 142911 manager.go:162] starting to scrape data from\nsources start: 2015-10-28 09:38:45 +0800 CST end: 2015-10-28 09:38:50 +0800\nCST\nI1028 09:38:50.000998 142911 manager.go:103] attempting to get data from\nsource \"Kube Node Metrics Source\"\nI1028 09:38:50.001022 142911 manager.go:103] attempting to get data from\nsource \"Kube Pods Source\"\nI1028 09:38:50.001080 142911 manager.go:103] attempting to get data from\nsource \"Kube Events Source\"\nI1028 09:38:50.002672 142911 kube_events.go:216] Fetched list of events\nfrom the master\nI1028 09:38:50.005782 142911 kube_nodes.go:126] Fetched list of nodes from\nthe master\nI1028 09:38:50.007247 142911 kubelet.go:110] about to query kubelet using\nurl: \"\nhttp://172.27.8.212:10255/stats/default/jenkins-master-rlzrl/907d1380-7c4f-11e5-8c76-fa163e77e286/jenkins-master\n\"\nI1028 09:38:50.007350 142911 kubelet.go:110] about to query kubelet using\nurl: \"\nhttp://172.27.8.211:10255/stats/default/portal-v2-isiip/f4a674c2-7c7c-11e5-8c76-fa163e77e286/portal-v2\n\"\nI1028 09:38:50.009014 142911 kube_nodes.go:59] Failed to get container\nstats from Kubelet on node \"wlan-cloudserver35\"\nI1028 09:38:50.033857 142911 kube_nodes.go:64] No container stats from\nKubelet on node \"wlan-cloudserver34\"\nI1028 09:38:50.082159 142911 manager.go:175] completed scraping data from\nsources. Errors: []\nI1028 09:38:52.713540 142911 external.go:138] Storing Timeseries to\n\"InfluxDB Sink\"\nI1028 09:38:52.713591 142911 external.go:142] Storing Events to \"InfluxDB\nSink\"\nI1028 09:38:55.000807 142911 manager.go:162] starting to scrape data from\nsources start: 2015-10-28 09:38:50 +0800 CST end: 2015-10-28 09:38:55 +0800\nCST\nI1028 09:38:55.001104 142911 manager.go:103] attempting to get data from\nsource \"Kube Node Metrics Source\"\nI1028 09:38:55.001144 142911 manager.go:103] attempting to get data from\nsource \"Kube Events Source\"\nI1028 09:38:55.001186 142911 kube_events.go:216] Fetched list of events\nfrom the master\nI1028 09:38:55.001084 142911 manager.go:103] attempting to get data from\nsource \"Kube Pods Source\"\nI1028 09:38:55.002206 142911 kube_nodes.go:126] Fetched list of nodes from\nthe master\nI1028 09:38:55.003151 142911 kubelet.go:110] about to query kubelet using\nurl: \"\nhttp://172.27.8.212:10255/stats/default/jenkins-master-rlzrl/907d1380-7c4f-11e5-8c76-fa163e77e286/jenkins-master\n\"\nI1028 09:38:55.003261 142911 kubelet.go:110] about to query kubelet using\nurl: \"\nhttp://172.27.8.211:10255/stats/default/portal-v2-isiip/f4a674c2-7c7c-11e5-8c76-fa163e77e286/portal-v2\n\"\nI1028 09:38:55.006374 142911 kube_nodes.go:59] Failed to get container\nstats from Kubelet on node \"wlan-cloudserver35\"\nI1028 09:38:55.007326 142911 kube_nodes.go:64] No container stats from\nKubelet on node \"wlan-cloudserver34\"\nI1028 09:38:55.025368 142911 manager.go:175] completed scraping data from\nsources. Errors: []\nI1028 09:39:00.000691 142911 manager.go:162] starting to scrape data from\nsources start: 2015-10-28 09:38:55 +0800 CST end: 2015-10-28 09:39:00 +0800\nCST\nI1028 09:39:00.000910 142911 manager.go:103] attempting to get data from\nsource \"Kube Pods Source\"\nI1028 09:39:00.000998 142911 manager.go:103] attempting to get data from\nsource \"Kube Node Metrics Source\"\nI1028 09:39:00.001051 142911 manager.go:103] attempting to get data from\nsource \"Kube Events Source\"\nI1028 09:39:00.001133 142911 kube_events.go:216] Fetched list of events\nfrom the master\nI1028 09:39:00.002700 142911 kube_nodes.go:126] Fetched list of nodes from\nthe master\nI1028 09:39:00.003155 142911 kubelet.go:110] about to query kubelet using\nurl: \"\nhttp://172.27.8.212:10255/stats/default/jenkins-master-rlzrl/907d1380-7c4f-11e5-8c76-fa163e77e286/jenkins-master\n\"\nI1028 09:39:00.003169 142911 kubelet.go:110] about to query kubelet using\nurl: \"\nhttp://172.27.8.211:10255/stats/default/portal-v2-isiip/f4a674c2-7c7c-11e5-8c76-fa163e77e286/portal-v2\n\"\nI1028 09:39:00.006839 142911 kube_nodes.go:59] Failed to get container\nstats from Kubelet on node \"wlan-cloudserver35\"\nI1028 09:39:00.028767 142911 kube_nodes.go:64] No container stats from\nKubelet on node \"wlan-cloudserver34\"\nI1028 09:39:00.082110 142911 manager.go:175] completed scraping data from\nsources. Errors: []\nI1028 09:39:02.808511 142911 external.go:138] Storing Timeseries to\n\"InfluxDB Sink\"\nI1028 09:39:02.808516 142911 external.go:142] Storing Events to \"InfluxDB\nSink\"\nI1028 09:39:05.000817 142911 manager.go:162] starting to scrape data from\nsources start: 2015-10-28 09:39:00 +0800 CST end: 2015-10-28 09:39:05 +0800\nCST\nI1028 09:39:05.001132 142911 manager.go:103] attempting to get data from\nsource \"Kube Node Metrics Source\"\nI1028 09:39:05.001152 142911 manager.go:103] attempting to get data from\nsource \"Kube Pods Source\"\nI1028 09:39:05.001375 142911 manager.go:103] attempting to get data from\nsource \"Kube Events Source\"\nI1028 09:39:05.001499 142911 kube_events.go:216] Fetched list of events\nfrom the master\nI1028 09:39:05.002861 142911 kube_nodes.go:126] Fetched list of nodes from\nthe master\nI1028 09:39:05.003097 142911 kubelet.go:110] about to query kubelet using\nurl: \"\nhttp://172.27.8.212:10255/stats/default/jenkins-master-rlzrl/907d1380-7c4f-11e5-8c76-fa163e77e286/jenkins-master\n\"\nI1028 09:39:05.003151 142911 kubelet.go:110] about to query kubelet using\nurl: \"\nhttp://172.27.8.211:10255/stats/default/portal-v2-isiip/f4a674c2-7c7c-11e5-8c76-fa163e77e286/portal-v2\n\"\nI1028 09:39:05.007803 142911 kube_nodes.go:59] Failed to get container\nstats from Kubelet on node \"wlan-cloudserver35\"\nI1028 09:39:05.009057 142911 kube_nodes.go:64] No container stats from\nKubelet on node \"wlan-cloudserver34\"\nI1028 09:39:05.030858 142911 manager.go:175] completed scraping data from\nsources. Errors: []\nI1028 09:39:10.000753 142911 manager.go:162] starting to scrape data from\nsources start: 2015-10-28 09:39:05 +0800 CST end: 2015-10-28 09:39:10 +0800\nCST\nI1028 09:39:10.001124 142911 manager.go:103] attempting to get data from\nsource \"Kube Pods Source\"\nI1028 09:39:10.001183 142911 manager.go:103] attempting to get data from\nsource \"Kube Node Metrics Source\"\nI1028 09:39:10.001475 142911 manager.go:103] attempting to get data from\nsource \"Kube Events Source\"\nI1028 09:39:10.001524 142911 kube_events.go:216] Fetched list of events\nfrom the master\nI1028 09:39:10.002558 142911 kube_nodes.go:126] Fetched list of nodes from\nthe master\nI1028 09:39:10.004382 142911 kubelet.go:110] about to query kubelet using\nurl: \"\nhttp://172.27.8.212:10255/stats/default/jenkins-master-rlzrl/907d1380-7c4f-11e5-8c76-fa163e77e286/jenkins-master\n\"\nI1028 09:39:10.004385 142911 kubelet.go:110] about to query kubelet using\nurl: \"\nhttp://172.27.8.211:10255/stats/default/portal-v2-isiip/f4a674c2-7c7c-11e5-8c76-fa163e77e286/portal-v2\n\"\nI1028 09:39:10.008187 142911 kube_nodes.go:59] Failed to get container\nstats from Kubelet on node \"wlan-cloudserver35\"\nI1028 09:39:10.024337 142911 kube_nodes.go:64] No container stats from\nKubelet on node \"wlan-cloudserver34\"\nI1028 09:39:10.109099 142911 manager.go:175] completed scraping data from\nsources. Errors: []\nI1028 09:39:12.970630 142911 external.go:138] Storing Timeseries to\n\"InfluxDB Sink\"\nI1028 09:39:12.970916 142911 external.go:142] Storing Events to \"InfluxDB\nSink\"\nI1028 09:39:15.000446 142911 manager.go:162] starting to scrape data from\nsources start: 2015-10-28 09:39:10 +0800 CST end: 2015-10-28 09:39:15 +0800\nCST\nI1028 09:39:15.000640 142911 manager.go:103] attempting to get data from\nsource \"Kube Pods Source\"\nI1028 09:39:15.000650 142911 manager.go:103] attempting to get data from\nsource \"Kube Node Metrics Source\"\nI1028 09:39:15.000756 142911 manager.go:103] attempting to get data from\nsource \"Kube Events Source\"\nI1028 09:39:15.000777 142911 kube_events.go:216] Fetched list of events\nfrom the master\nI1028 09:39:15.001544 142911 kube_nodes.go:126] Fetched list of nodes from\nthe master\nI1028 09:39:15.002470 142911 kubelet.go:110] about to query kubelet using\nurl: \"\nhttp://172.27.8.212:10255/stats/default/jenkins-master-rlzrl/907d1380-7c4f-11e5-8c76-fa163e77e286/jenkins-master\n\"\nI1028 09:39:15.002564 142911 kubelet.go:110] about to query kubelet using\nurl: \"\nhttp://172.27.8.211:10255/stats/default/portal-v2-isiip/f4a674c2-7c7c-11e5-8c76-fa163e77e286/portal-v2\n\"\nI1028 09:39:15.005073 142911 kube_nodes.go:59] Failed to get container\nstats from Kubelet on node \"wlan-cloudserver35\"\nI1028 09:39:15.007979 142911 kube_nodes.go:64] No container stats from\nKubelet on node \"wlan-cloudserver34\"\nI1028 09:39:15.027275 142911 manager.go:175] completed scraping data from\nsources. Errors: []\nI1028 09:39:20.000651 142911 manager.go:162] starting to scrape data from\nsources start: 2015-10-28 09:39:15 +0800 CST end: 2015-10-28 09:39:20 +0800\nCST\nI1028 09:39:20.000921 142911 manager.go:103] attempting to get data from\nsource \"Kube Pods Source\"\nI1028 09:39:20.001026 142911 manager.go:103] attempting to get data from\nsource \"Kube Node Metrics Source\"\nI1028 09:39:20.001210 142911 manager.go:103] attempting to get data from\nsource \"Kube Events Source\"\nI1028 09:39:20.001231 142911 kube_events.go:216] Fetched list of events\nfrom the master\nI1028 09:39:20.002299 142911 kube_nodes.go:126] Fetched list of nodes from\nthe master\nI1028 09:39:20.002857 142911 kubelet.go:110] about to query kubelet using\nurl: \"\nhttp://172.27.8.211:10255/stats/default/portal-v2-isiip/f4a674c2-7c7c-11e5-8c76-fa163e77e286/portal-v2\n\"\nI1028 09:39:20.003004 142911 kubelet.go:110] about to query kubelet using\nurl: \"\nhttp://172.27.8.212:10255/stats/default/jenkins-master-rlzrl/907d1380-7c4f-11e5-8c76-fa163e77e286/jenkins-master\n\"\nI1028 09:39:20.007041 142911 kube_nodes.go:59] Failed to get container\nstats from Kubelet on node \"wlan-cloudserver35\"\nI1028 09:39:20.028139 142911 kube_nodes.go:64] No container stats from\nKubelet on node \"wlan-cloudserver34\"\nI1028 09:39:20.075010 142911 manager.go:175] completed scraping data from\nsources. Errors: []\nI1028 09:39:23.032114 142911 external.go:138] Storing Timeseries to\n\"InfluxDB Sink\"\nI1028 09:39:23.032117 142911 external.go:142] Storing Events to \"InfluxDB\nSink\"\nI1028 09:39:25.001630 142911 manager.go:162] starting to scrape data from\nsources start: 2015-10-28 09:39:20 +0800 CST end: 2015-10-28 09:39:25 +0800\nCST\nI1028 09:39:25.001743 142911 manager.go:103] attempting to get data from\nsource \"Kube Pods Source\"\nI1028 09:39:25.001795 142911 manager.go:103] attempting to get data from\nsource \"Kube Node Metrics Source\"\nI1028 09:39:25.002217 142911 manager.go:103] attempting to get data from\nsource \"Kube Events Source\"\nvery luckly, the data has write to influxdb v0.9 . i can query results\nwith \u2018show series\u2019 in db k8s.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/673#issuecomment-151698647\n.\n``````\n. - You need to run kube-dns addon.\n- We recommend setting up service accounts as part of kube cluster setup.\n\nOn Mon, Oct 26, 2015 at 4:28 AM, Juneja notifications@github.com wrote:\n\nwhen i got run with ./heapster --source=kubernetes:\nhttp://172.27.8.210:8080?inClusterConfig=false&auth=\u201d\u201d&useServiceAccount=false\n--sink=influxdb:http://172.27.8.109:8086?user=root&pw=root&db=k8s\nit run ok .\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/674#issuecomment-151105597\n.\n. Kindly re-open this issue if the documentation under docs/source-configuration.md does not help you resolve your current issues :)\n. Yeah. kube-dns addon needs to be run. If you choose to not run that, you\nwill have to pass the IP of monitoring-influxdb service to heapster.\n\nOn Mon, Oct 26, 2015 at 6:40 AM, Thuc Nguyen notifications@github.com\nwrote:\n\nFrom your list of pods, it looks like skydns wasn't there. You'll need to\nenable the DNS add-on to access InfluxDB service via monitoring-influxdb.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/675#issuecomment-151135961\n.\n. ok to test\n. Thanks for the cleanup @huangyuqi :+1: \n. The base image will be missing for containers that were not started via\nkubernetes APIs. In kubernetes, the kubelet (node agent), creates cgroups\nto track resource usage of certain node daemons (docker, sshd, etc). This\ninformation is useful to monitor the nodes. Filtering out non-interesting\ncontainers is possible, but it will be difficult to implement in practice\nunless we can somehow discover only the interesting ones.\nIf this is not an issue as of now, I'd recommend not attempting to fix this.\n\nOn Tue, Oct 27, 2015 at 6:10 AM, yuqi huang notifications@github.com\nwrote:\n\nheapster store timeseries of PODs(including their containers ) and events\nto the external sink.\nhttps://github.com/kubernetes/heapster/blob/master/sinks/external.go#L139\nfor idx := range esm.externalSinks {\n        sink := esm.externalSinks[idx]\n        go func(sink sink_api.ExternalSink) {\n            glog.V(2).Infof(\"Storing Timeseries to %q\", sink.Name())\n            errorsChan <- sink.StoreTimeseries(timeseries)\n        }(sink)\n        go func(sink sink_api.ExternalSink) {\n            glog.V(2).Infof(\"Storing Events to %q\", sink.Name())\n            errorsChan <- sink.StoreEvents(kEvents)\n        }(sink)\n    }\nhere, just saving the pods' and containers' metric information, but the\ntimeseries contains some redundant data about some process and cgroup,\ni debug this issue,and got these:\n*cgroup\n{\n    \"Point\": {\n        \"Name\": \"uptime\",\n        \"Labels\": {\n            \"container_base_image\": \"\",\n            \"container_name\": \"/user/0.user/38.session\",\n            \"host_id\": \"192.168.11.77\",\n            \"hostname\": \"192.168.11.77\",\n            \"labels\": \"\"\n        },\n        \"Start\": \"2015-10-26T17:14:56Z\",\n        \"End\": \"2015-10-26T20:04:10Z\",\n        \"Value\": 10172528\n    },\n    \"MetricDescriptor\": {\n        \"Name\": \"uptime\",\n        \"Description\": \"Number of milliseconds since the container was started\",\n        \"Labels\": null,\n        \"Type\": 0,\n        \"ValueType\": 0,\n        \"Units\": 2\n    }\n}\n*other process\n{\n    \"Point\": {\n        \"Name\": \"cpu/usage\",\n        \"Labels\": {\n            \"container_base_image\": \"\",\n            \"container_name\": \"kubelet\",\n            \"host_id\": \"192.168.11.77\",\n            \"hostname\": \"192.168.11.77\",\n            \"labels\": \"\"\n        },\n        \"Start\": \"2015-10-13T19:05:12Z\",\n        \"End\": \"2015-10-26T20:04:10Z\",\n        \"Value\": 9654496406507\n    },\n    \"MetricDescriptor\": {\n        \"Name\": \"cpu/usage\",\n        \"Description\": \"Cumulative CPU usage on all cores\",\n        \"Labels\": null,\n        \"Type\": 0,\n        \"ValueType\": 0,\n        \"Units\": 3\n    }\n}\nThe redundant data has common feature is that their container_base_image\nis empty.\nShall we filter these redundant data?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/677.\n. ok to test\n. LGTM. Thanks for the PR @bluebreezecf :+1: \n. Jenkins should be accessible here.\n\nOn Fri, Oct 30, 2015 at 5:44 AM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nMerged #679 https://github.com/kubernetes/heapster/pull/679.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/679#event-450345473.\n. We run a test server to validate PRs. My previous comment was for @mvdan.\n\nOn Fri, Oct 30, 2015 at 1:51 PM, Jianwei Shi notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh\nYour guys are talking about the Jenkins, what is it about?\nIs the newly added test cause your inner Jenkins down?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/679#issuecomment-152648329.\n. @bluebreezecf: Thanks for offering help! This PR is fine as-is.\n. > The actual used listener,which is created by model.GetCacheListener() , just does nothing (https://github.com/kubernetes/heapster/blob/master/model/impl.go#L387).\n\nIt is deleting objects right?\n\nNow, we can figure out that the aggregated pod metrics may not be so accurate. Besides, the aging pod container data in realModel will not be removed until the belonging pod is ageing and removed\nin the PodEvicted(), which is also only triggered by the runGC().\n\nBy ageing, I assume you are referring to terminated pods. Is that correct?\n\nSo, i add deletePodContainer() for realModel, trigger to call it in PodContainerEvicted in model/impl.go, and add the corresponding test for deletePodContainer in model/impl_test.go. The details can be shown in the commit details of this PR.\n\nAFAIK, k8s does not allow adding/removing containers in pods. That was probably the reason why the existing logic did not handle pod container deletions. \nIn general though, if we are abstracting out k8s concepts in the model, the proposed change makes sense.\nI will let @mwielgus and @piosz also take a look before this change is merged.\n. cc @thucatebay \n. cc @piosz @mwielgus \n. cc @mwielgus @piosz \n. cc @mwielgus \n. I'm merging this assuming that there are no objects from @piosz and @mwielgus. \n. @jfoy: I'd appreciate a PR in the future to fix the docs section I have added for Riemann sink.\n. I tested this sink myself. This should be good to go.\n. @piosz: Thanks for the review. Added comments.\n. This is an issue with the kubernetes api server. Can you try accessing\ngrafana directly, without the api-server proxy? The configuration files\nshould contain information that will guide you to skip the proxy.\nOn Thu, Nov 5, 2015 at 1:42 AM, tsn77130 notifications@github.com wrote:\n\nresult when I'm lucky\n[image: containers_ok]\nhttps://cloud.githubusercontent.com/assets/8789835/10964862/eacb89b0-83a9-11e5-89cd-affc01b5885d.png\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/688#issuecomment-154007416\n.\n. In that case, @thucatebay might be able to help you.\n. I don't know the overall plan behind these PRs. FYI: Certain metrics can have multiple values per container - for example, there can be multiple disks per container and we currently use resource identifiers to distinguish those disks. This can be applicable to per-cpu usage, network, etc.\n. Yeah. The best we can do for now is document what we test against.\nSetting up integration tests for some of the sinks like kafka and riemann isn't that difficult. We can create a heapster pod with these sinks, re-configure heapster sinks from e2e tests, and then run the necessary sink validation logic.\nWe can also encourage the community to add e2e tests for those sinks, if they happen to use those sinks in production.\n. cc @piosz \n. ok to test\n. cc @piosz\n\nOn Tue, Nov 10, 2015 at 1:54 PM, Chris Love notifications@github.com\nwrote:\n\nSo this is continuing #694\nhttps://github.com/kubernetes/heapster/issues/694\nNow the pod keeps restarting, with very little logging information. All I\nhave now is:\nW1110 14:48:19.665252 12536 cmd.go:149] log is DEPRECATED and will be\nremoved in a future version. Use logs instead.\nI1110 21:48:19.802383 1 heapster.go:61] /heapster --source=kubernetes:\nhttps://kubernetes.default --sink=gcm --sink=gcl --stats_resolution=1m\nI1110 21:48:19.802509 1 heapster.go:62] Heapster version 0.18.0\nF1110 21:48:19.802525 1 heapster.go:64] stats resolution '60000000000' is\nnot less than model resolution '60000000000'\nI am see multiple restarts of the\nImage: kubernetes/heapster:canary\non the heapster pod\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/702.\n. AFAIK, GCE does not allow updating scopes (permissions to Google Cloud\nAPIs) dynamically. When you create nodes on GCE, you need to add the\nmonitoring scope. If you had setup the cluster using the kubernetes scripts\n(kube-up.sh), then the monitoring scope should have been added by default.\n\nTo get around this, you will have to add new nodes with the monitoring\nscope enabled, and then delete the old nodes.\nOn Thu, Nov 12, 2015 at 10:08 AM, Chris Love notifications@github.com\nwrote:\n\nAnother error:\nLet me know what else you need\nkubectl logs heapster-j9w02 --namespace=kube-system\nI1112 18:06:02.054151 1 heapster.go:61] /heapster --source=kubernetes:\nhttps://kubernetes.default --sink=gcm --sink=gcl --stats_resolution=30s\nI1112 18:06:02.054215 1 heapster.go:62] Heapster version 0.18.0\nI1112 18:06:02.054659 1 kube_factory.go:172] Using Kubernetes client with\nmaster \"https://kubernetes.default\" and version \"v1\"\nI1112 18:06:02.054669 1 kube_factory.go:173] Using kubelet port 10255\nI1112 18:06:02.069390 1 driver.go:113] created GCM sink\nI1112 18:06:02.070631 1 driver.go:242] Project ID for GCL sink is:\n\"decent-slice-104819\"\nI1112 18:06:02.070647 1 driver.go:262] creating GCL sink\nE1112 18:06:02.070654 1 manager.go:257] encountered following errors while\nsetting up sinks - Current instance does not have the expected scope (\"\nhttps://www.googleapis.com/auth/monitoring\"). Actual scopes:\nhttps://www.googleapis.com/auth/compute\nhttps://www.googleapis.com/auth/devstorage.read_only\nhttps://www.googleapis.com/auth/logging.write\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal 0xb code=0x1 addr=0x0 pc=0x703ff7]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/702#issuecomment-156186714\n.\n. Also #272 implements a Bosun Sink which is essentially a OpenTsdb sink. Can\nwe combine those PRs? Maybe we can use the client libraries used by the PR?\n\nOn Thu, Nov 12, 2015 at 12:59 PM, Marcin Wielgus notifications@github.com\nwrote:\n\n@bluebreezecf https://github.com/bluebreezecf Is there any particular\nreason why you had to write an opentsdb client on your own, from scratch?\nWhat are your plans regarding new version, updates of the client, etc?\nPlease be aware that we will have to review the client library before\nallowing it in, as the project (\nhttps://github.com/bluebreezecf/opentsdb-goclient) has 0 forks and just 1\ncontributor.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/703#issuecomment-156232248.\n. ok to test\n. add to whitelist\n. LGTM\n. ok to test\n\nOn Thu, Nov 12, 2015 at 4:43 AM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/705#issuecomment-156094259.\n. add to whitelist\n\nOn Thu, Nov 12, 2015 at 4:49 AM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/706#issuecomment-156095228.\n. Can you try with the latest versions of all the components?\nHeapster is v0.19.0, InfluxDB v0.6 and Grafana v2.6.0\n\nOn Thu, Jan 21, 2016 at 4:23 AM, tyler274 notifications@github.com wrote:\n\nwould appreciate this as well\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/710#issuecomment-173555184\n.\n. What version of heapster are you using? Heapster will retry connecting to\nInfluxDB until it can succeed.\n\nOn Mon, Nov 16, 2015 at 5:21 PM, Chris notifications@github.com wrote:\n\nInstructions here point to starting up all services/RCs simultaneously:\nhttps://github.com/kubernetes/heapster/blob/master/docs/influxdb.md.\nIf heapster comes up before influxdb is ready, I see the following error\nin the heapster logs:\nE1117 00:54:03.431077 1 manager.go:257] encountered following errors while\nsetting up sinks - failed to ping InfluxDB server at\n\"monitoring-influxdb:8086\" - Get http://monitoring-influxdb:8086/ping:\ndial tcp: lookup monitoring-influxdb: no such host\nAnd influxdb never gets the 'k8s' database added.\nIf I manually order the startup of these containers, everything works\nproperly.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/711.\n. @piosz @mwielgus Can one of you try out the HEAD configs and see if we are able to repro this issue?\n. This issue has been fixed on HEAD. The next release of heapster should be\nresilient to backend failures.\n\nOn Wed, Nov 25, 2015 at 8:38 AM, perhapszzy notifications@github.com\nwrote:\n\nAnd there's a similar problem:\nHeapster will not work correctly if the influxdb pod restart.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/711#issuecomment-159665834\n.\n. https://github.com/kubernetes/heapster/blob/master/sinks/influxdb/driver.go#L230\n\nOn Wed, Dec 2, 2015 at 10:12 PM, Piotr Szczesniak notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh could you please link PR with fix? I\ncan't find any.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/711#issuecomment-161527845\n.\n. +1 for keeping the existing configuration. The config is already complex.\nWe should not complicate it any further.\nMay be the problem to be addressed here is improved documentation.\nWe should provide one or more examples for using multiple tests.\n\nOn Wed, Nov 18, 2015 at 1:43 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nI think it makes more sense to keep it separate so that commas can be used\nin sink configuration if required in future. If you're OK with that then\nI'd like to close this PR.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/712#issuecomment-157658486.\n. Cadvisor exposes docker image labels. Make sure we preserve that. \n. This is cute. What was the motivation? What is the purpose of the new branch? Are we planning on doing a giant merge sometime soon into master?\n. Thanks for the info!\n\nOn Mon, Nov 30, 2015 at 2:11 PM, Marcin Wielgus notifications@github.com\nwrote:\n\nYes, as mentioned in #684\nhttps://github.com/kubernetes/heapster/issues/684. We are changing lots\nof internals which cannot be done at once. Right now the status of the\nbranch is working prototype but once tested and on par with the current\nversion we hope to merge it with the master. We will soon send 2 proposals:\none with the long term vision and one which describes the changes in\ndetails.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/722#issuecomment-160778173.\n. @mwringe: I have added you to the kubernetes github org. Issues and PRs can be assigned to you now. \n. ok to test\n\nOn Wed, Nov 25, 2015 at 6:33 PM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/733#issuecomment-159785205.\n. cc @kubernetes/heapster-maintainers \n. needs a rebase\n. Can you rebase to HEAD and try again?\n\nOn Wed, Dec 2, 2015 at 6:53 PM, yuqi huang notifications@github.com wrote:\n\nHi, @vishh https://github.com/vishh ,Is there something wrong about the\nCI? I got the error in checking process,\nI1203 02:14:44.359848   15679 heapster_api_test.go:79] Failed to create ns: namespaces \"heapster-e2e-tests\" already exists\n--- FAIL: TestHeapster (51.98s)\n    assertions.go:154:\n```\nLocation:   heapster_api_test.go:784\nError:      No error is expected but got namespaces \"heapster-e2e-tests\" already exists\n```\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/733#issuecomment-161500687.\n. cc @piosz. Is our test infra broken?\n\nOn Thu, Dec 3, 2015 at 5:17 PM, yuqi huang notifications@github.com wrote:\n\nHi, @vishh https://github.com/vishh, I have rebased, but the checks\nwere still not successful.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/733#issuecomment-161843754.\n. ok to test\n\nOn Fri, Dec 11, 2015 at 12:11 PM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/733#issuecomment-164037114.\n. @huangyuqi: @mwielgus might be able to assist you with that.\n. ok to test\n\nOn Mon, Dec 21, 2015 at 11:48 PM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/733#issuecomment-166544400.\n. @huangyuqi @mwielgus: This patch will be useful for shipping events to elasticsearch and view them via Kibana. Can we prioritize this?\n. Can you post heapster logs? You can try following the debugging guide if\nthe logs don't contain any useful information.\n\nAlso note that there is currently a 2 minute delay for the /model\nendpoints to start working.\nOn Mon, Nov 30, 2015 at 9:39 AM, Dan Di Spaltro notifications@github.com\nwrote:\n\nheapster is currently only sending log/events measurements to influxdb,\nso all of the out of the box metrics in the grafana/influxdb don't really\nwork.\nI think I narrowed it down to a heapster somehow not working correctly (vs\na Influx or grafana problem)\nHere's my config https://gist.github.com/ddispaltro/c29bbcaa1eb4d3a0149e\nHere's the heapster api response\nroot@heapster-v10-dxl8f:/go# curl http://localhost:8082/api/v1/model/stats/{  \"uptime\": 0,  \"stats\": {} }\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/744.\n. What version of kubernetes are you running? Heapster required versions >=\n1.0.6\n\nOn Mon, Nov 30, 2015 at 10:29 AM, Dan Di Spaltro notifications@github.com\nwrote:\n\nIt's been up for 15 hrs.\nHere's what /validate responds with:\ncurl localhost:8082/validate\nHeapster Version: 0.18.0\nSource type: kube-pod-metrics\n    Pod Errors: map[{name:busybox id:195725ea-96f0-11e5-a065-6805ca2dac7c ip:127.0.0.1}:1581 {name:k8s-master-127.0.0.1 id:c2e10198-96ee-11e5-a065-6805ca2dac7c ip:127.0.0.1}:4743]\nSource type: Kube Node Metrics\nKubernetes Nodes plugin:\n    Healthy Nodes:\n        127.0.0.1\n    No node errors\nSource type: kube-events\nExternal Sinks\n    Exported metrics:\n        uptime: Number of milliseconds since the container was started\n        cpu/usage: Cumulative CPU usage on all cores\n        cpu/limit: CPU hard limit in millicores.\n        cpu/request: CPU request (the guaranteed amount of resources) in millicores. This metric is Kubernetes specific.\n        memory/usage: Total memory usage\n        memory/working_set: Total working set usage. Working set is the memory being used and not easily dropped by the kernel\n        memory/limit: Memory hard limit in bytes.\n        memory/request: Memory request (the guaranteed amount of resources) in bytes. This metric is Kubernetes specific.\n        memory/page_faults: Number of page faults\n        memory/major_page_faults: Number of major page faults\n        network/rx: Cumulative number of bytes received over the network\n        network/rx_errors: Cumulative number of errors while receiving over the network\n        network/tx: Cumulative number of bytes sent over the network\n        network/tx_errors: Cumulative number of errors while sending over the network\n        filesystem/usage: Total number of bytes consumed on a filesystem\n        filesystem/limit: The total size of filesystem in bytes\n    Exported labels:\n        hostname: Hostname where the container ran\n        host_id: Identifier specific to a host. Set by cloud provider or user\n        container_name: User-provided name of the container or full container name for system containers\n        container_base_image: User-defined image name that is run inside the container\n        pod_name: The name of the pod\n        pod_id: The unique ID of the pod\n        pod_namespace: The namespace of the pod\n        namespace_id: The UID of namespace of the pod\n        labels: Comma-separated list of user-provided labels\n        resource_id: Identifier(s) specific to a metric\n    External Sinks:\n        Sink Type: InfluxDB\n    client: Host \"monitoring-influxdb:8086\", Database \"k8s\"\n    Number of write failures: 0\nAnd here's some excerpt from the logs with added verbosity\nI1130 18:28:00.000578       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager\"\nI1130 18:28:00.000575       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox\"\nI1130 18:28:00.000726       1 kube_nodes.go:59] Failed to get container stats from Kubelet on node \"127.0.0.1\"\nI1130 18:28:00.000750       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:28:00.000767       1 kube_pods.go:110] failed to get stats for container \"controller-manager\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:28:00.000753       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox - Get http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:28:00.000786       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver\"\nI1130 18:28:00.000810       1 kube_pods.go:110] failed to get stats for container \"busybox\" in pod \"default\"/\"busybox\"\nI1130 18:28:00.000917       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:28:00.000931       1 kube_pods.go:110] failed to get stats for container \"apiserver\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:28:00.000946       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler\"\nI1130 18:28:00.001104       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:28:00.001129       1 kube_pods.go:110] failed to get stats for container \"scheduler\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:28:00.001151       1 manager.go:175] completed scraping data from sources. Errors: []\nI1130 18:28:30.000238       1 manager.go:162] starting to scrape data from sources start: 2015-11-30 18:28:00 +0000 UTC end: 2015-11-30 18:28:30 +0000 UTC\nI1130 18:28:30.000316       1 manager.go:103] attempting to get data from source \"Kube Pods Source\"\nI1130 18:28:30.000324       1 manager.go:103] attempting to get data from source \"Kube Node Metrics Source\"\nI1130 18:28:30.000361       1 manager.go:103] attempting to get data from source \"Kube Events Source\"\nI1130 18:28:30.000392       1 kube_events.go:216] Fetched list of events from the master\nI1130 18:28:30.000405       1 kube_nodes.go:128] Fetched list of nodes from the master\nI1130 18:28:30.000400       1 kube_events.go:217] []\nI1130 18:28:30.000436       1 pods.go:139] Ignoring pod heapster-v10-liajt with namespace kube-system since namespace object was not found\nI1130 18:28:30.000450       1 pods.go:139] Ignoring pod kube-dns-v9-582p1 with namespace kube-system since namespace object was not found\nI1130 18:28:30.000457       1 pods.go:139] Ignoring pod kube-ui-v3-60fz9 with namespace kube-system since namespace object was not found\nI1130 18:28:30.000463       1 pods.go:139] Ignoring pod monitoring-influxdb-grafana-v2-140h2 with namespace kube-system since namespace object was not found\nI1130 18:28:30.000469       1 pods.go:152] selected pods from api server [{pod:0xc208383800 nodeInfo:0xc2081f2980 namespace:0xc2081df3b0} {pod:0xc2083839f0 nodeInfo:0xc2081f29c0 namespace:0xc2081df3b0}]\nI1130 18:28:30.000608       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox\"\nI1130 18:28:30.000617       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager\"\nI1130 18:28:30.000720       1 kube_nodes.go:59] Failed to get container stats from Kubelet on node \"127.0.0.1\"\nI1130 18:28:30.000804       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox - Get http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:28:30.000808       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:28:30.000823       1 kube_pods.go:110] failed to get stats for container \"busybox\" in pod \"default\"/\"busybox\"\nI1130 18:28:30.000833       1 kube_pods.go:110] failed to get stats for container \"controller-manager\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:28:30.000860       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver\"\nI1130 18:28:30.001014       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:28:30.001031       1 kube_pods.go:110] failed to get stats for container \"apiserver\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:28:30.001052       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler\"\nI1130 18:28:30.001175       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:28:30.001197       1 kube_pods.go:110] failed to get stats for container \"scheduler\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:28:30.001228       1 manager.go:175] completed scraping data from sources. Errors: []\nI1130 18:28:54.640620       1 external.go:116] no timeseries data between 0001-01-01 00:00:00 +0000 UTC and 0001-01-01 00:00:00 +0000 UTC\nI1130 18:29:00.000186       1 manager.go:162] starting to scrape data from sources start: 2015-11-30 18:28:30 +0000 UTC end: 2015-11-30 18:29:00 +0000 UTC\nI1130 18:29:00.000266       1 manager.go:103] attempting to get data from source \"Kube Pods Source\"\nI1130 18:29:00.000302       1 manager.go:103] attempting to get data from source \"Kube Node Metrics Source\"\nI1130 18:29:00.000309       1 manager.go:103] attempting to get data from source \"Kube Events Source\"\nI1130 18:29:00.000372       1 kube_nodes.go:128] Fetched list of nodes from the master\nI1130 18:29:00.000361       1 kube_events.go:216] Fetched list of events from the master\nI1130 18:29:00.000377       1 pods.go:139] Ignoring pod heapster-v10-liajt with namespace kube-system since namespace object was not found\nI1130 18:29:00.000391       1 pods.go:139] Ignoring pod kube-dns-v9-582p1 with namespace kube-system since namespace object was not found\nI1130 18:29:00.000398       1 pods.go:139] Ignoring pod kube-ui-v3-60fz9 with namespace kube-system since namespace object was not found\nI1130 18:29:00.000413       1 pods.go:139] Ignoring pod monitoring-influxdb-grafana-v2-140h2 with namespace kube-system since namespace object was not found\nI1130 18:29:00.000384       1 kube_events.go:217] []\nI1130 18:29:00.000423       1 pods.go:152] selected pods from api server [{pod:0xc208383800 nodeInfo:0xc2083c0bc0 namespace:0xc208140690} {pod:0xc2083839f0 nodeInfo:0xc2083c0c00 namespace:0xc208140690}]\nI1130 18:29:00.000533       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager\"\nI1130 18:29:00.000533       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox\"\nI1130 18:29:00.000700       1 kube_nodes.go:59] Failed to get container stats from Kubelet on node \"127.0.0.1\"\nI1130 18:29:00.000715       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:29:00.000728       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox - Get http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:29:00.000742       1 kube_pods.go:110] failed to get stats for container \"controller-manager\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:29:00.000747       1 kube_pods.go:110] failed to get stats for container \"busybox\" in pod \"default\"/\"busybox\"\nI1130 18:29:00.000763       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver\"\nI1130 18:29:00.000919       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:29:00.000949       1 kube_pods.go:110] failed to get stats for container \"apiserver\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:29:00.000974       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler\"\nI1130 18:29:00.001109       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:29:00.001125       1 kube_pods.go:110] failed to get stats for container \"scheduler\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:29:00.001151       1 manager.go:175] completed scraping data from sources. Errors: []\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/744#issuecomment-160715635\n.\n. ```\n\nI1130 18:28:30.000400       1 kube_events.go:217] []\nI1130 18:28:30.000436       1 pods.go:139] Ignoring pod heapster-v10-liajt\nwith namespace kube-system since namespace object was not found\nI1130 18:28:30.000450       1 pods.go:139] Ignoring pod kube-dns-v9-582p1\nwith namespace kube-system since namespace object was not found\nI1130 18:28:30.000457       1 pods.go:139] Ignoring pod kube-ui-v3-60fz9\nwith namespace kube-system since namespace object was not found\nI1130 18:28:30.000463       1 pods.go:139] Ignoring pod\nmonitoring-influxdb-grafana-v2-140h2 with namespace kube-system since\nnamespace object was not found\nI1130 18:28:30.000469       1 pods.go:152] selected pods from api server\n[{pod:0xc208383800 nodeInfo:0xc2081f2980 namespace:0xc2081df3b0}\n{pod:0xc2083839f0 nodeInfo:0xc2081f29c0 namespace:0xc2081df3b0}]\nI1130 18:28:30.000608       1 kubelet.go:110] about to query kubelet using\nurl: \"\nhttp://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox\n\"\nI1130 18:28:30.000617       1 kubelet.go:110] about to query kubelet using\nurl: \"\nhttp://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager\n\"\nI1130 18:28:30.000720       1 kube_nodes.go:59] Failed to get container\nstats from Kubelet on node \"127.0.0.1\"I1130 18:28:30.000804       1\nkubelet.go:96] failed to get stats from kubelet url:\nhttp://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox\nhttp://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox\n- Get\nhttp://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox\nhttp://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox:\ndial tcp 127.0.0.1:10255 http://127.0.0.1:10255: connection refused\n```\n1. The Api server is not including kube-system namespace when heapster\n   attempts to list namespaces.\n2. The kubelet is not allowing heapster to query container metrics. I\n   suspect an issue with auth configuration.\nOn Mon, Nov 30, 2015 at 10:31 AM, Dan Di Spaltro notifications@github.com\nwrote:\n\n1.1 using the docker setup + cluster-dns\n\u2014\nReply to this email directly or view it on GitHub.\n. AFAIK, v0.18.2 should not have this issue. v0.18.2 is what has been bundled\nwith kubernetes v1.1.\n\nOn Mon, Nov 30, 2015 at 12:03 PM, Matthew Wringe notifications@github.com\nwrote:\n\nThere was a commit about a month ago (063ce38\nhttps://github.com/kubernetes/heapster/commit/063ce38deb63d1d66629a3811ef508a49860b08d)\nwhich breaks compatibility with OpenShift.\nSpecifically it removes the ability to connect to OpenShift in a secure\nmanner.\nThe problem is that it is using the kubernetes v1.1.0 build, where the\nsecurity implementation is in the v1.2.0 builds.\nWe need to update our godeps to use kubernetes v1.2.0\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/746.\n. It might be helpful to setup an e2e test against all releases and master\nand post the results to github. I can help setup such an e2e.\n\nOn Mon, Nov 30, 2015 at 12:04 PM, Vishnu Kannan vishnuk@google.com wrote:\n\nAFAIK, v0.18.2 should not have this issue. v0.18.2 is what has been\nbundled with kubernetes v1.1.\nOn Mon, Nov 30, 2015 at 12:03 PM, Matthew Wringe <notifications@github.com\n\nwrote:\nThere was a commit about a month ago (063ce38\nhttps://github.com/kubernetes/heapster/commit/063ce38deb63d1d66629a3811ef508a49860b08d)\nwhich breaks compatibility with OpenShift.\nSpecifically it removes the ability to connect to OpenShift in a secure\nmanner.\nThe problem is that it is using the kubernetes v1.1.0 build, where the\nsecurity implementation is in the v1.2.0 builds.\nWe need to update our godeps to use kubernetes v1.2.0\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/746.\n. Ok. We can cut a v0.18.3 if the PRs you care about are already merged into\nrelease-0.18 branch. It is just an option.\n\n\nOn Mon, Nov 30, 2015 at 12:11 PM, Matthew Wringe notifications@github.com\nwrote:\n\nv0.18.2 is almost 2 months ago and is out of date with our needs. We did\nget a bunch of the commits we required into the release-0.18.0 branch, but\nno release has happened yet which contains those commits. Even if a release\nwere to happen for 0.18.0 we would still need to go in and back port a\ncommit or two.\nI almost have a PR which will bring master up to the latest v1.2.0 alpha,\nif that makes sense for the master branch.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/746#issuecomment-160745547\n.\n. ok to test\n\nOn Tue, Dec 1, 2015 at 10:59 AM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/749#issuecomment-161062972.\n. add to whitelist\n. ok to test\n. LGTM\n\nOn Mon, Jan 11, 2016 at 6:15 AM, Matthew Wringe notifications@github.com\nwrote:\n\n@jimmidyson https://github.com/jimmidyson I don't believe the\nheapster-scalability branch has this issue. I can run that and see\nmetrics coming in (I can't get it to recognize that the Hawkular sink is\nconfigured, but the log sink is showing metrics). This was something\nspecific to the master branch and the release-0.18.0 didn't have this\nproblem (at least at the last time I checked)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/749#issuecomment-170563417.\n. Can you file an issue with the errors you are experiencing with e2e tests? It is supposed to work.\n. @k8s-bot: ok to test\n. cc @mwielgus @piosz: The integration test seems to be broken. Can you fix that?\n. @mwringe: I guess @mwielgus is primarily focussing on the new scalability branch and I guess that branch should include the new deps? \n@mwielgus please advice what to do with this PR.\n. ok to test\n. ok to test\n\nOn Mon, Dec 14, 2015 at 1:01 PM, Kubernetes Bot notifications@github.com\nwrote:\n\nCan one of the admins verify that this patch is reasonable to test? (reply\n\"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/753#issuecomment-164557995.\n. Ping @mwringe @mwielgus : How do we make progress on this PR?\n. We might have to define GCM labels which might be a subset of all the\nlabels.\n\nOn Fri, Dec 18, 2015 at 10:48 AM, Matthew Wringe notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh I am not sure how exactly to make\nprogress with this PR. There is an issue where the GCM sink can only\nsupport 10 labels and this PR pushes the label count to 11.\nSo with this PR the GCM sink wont be able to function properly. I don't\nhave a GCM account setup to test, and I don't know how exactly to handle\nthis situation.\nIt seems like someone familiar with the GCM sink should be making the\ndecisions on how it should work. You may need to open up a new\nconfiguration option for GCM to allow admins to specify which labels they\nactually want or not, or perhaps just hardcode it in the sink.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/753#issuecomment-165867920.\n. We are still discussing this decision in the design doc.. Given that, I'd\nprefer not going ahead with this change now..\n\nOn Wed, Dec 2, 2015 at 2:22 AM, Marcin Wielgus notifications@github.com\nwrote:\n\nMerged #754 https://github.com/kubernetes/heapster/pull/754.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/754#event-480007165.\n. @jimmidyson: Its a draft of a doc that @mwielgus is planning to post on github anytime now.. \n. This issue is already fixed on HEAD. The next release will fix this issue.\n\nOn Wed, Dec 2, 2015 at 9:15 AM, Patrick Hemmer notifications@github.com\nwrote:\n\nThe guide at\nhttps://github.com/kubernetes/heapster/blob/master/docs/influxdb.md\nprovides a command (kubectl create -f deploy/kube-config/influxdb/) which\nbrings up all resources near simultaneously. This apparently can cause\nissues with the heapster pod when it tried to connect to influxdb:\nI1201 20:32:58.289427       1 heapster.go:61] /heapster --source=kubernetes:https://kubernetes.default --sink=influxdb:http://monitoring-influxdb:8086\nI1201 20:32:58.289620       1 heapster.go:62] Heapster version 0.18.0\nI1201 20:32:58.290218       1 kube_factory.go:172] Using Kubernetes client with master \"https://kubernetes.default\" and version \"v1\"\nI1201 20:32:58.290283       1 kube_factory.go:173] Using kubelet port 10255\nE1201 20:33:28.294551       1 manager.go:257] encountered following errors while setting up sinks - failed to ping InfluxDB server at \"monitoring-influxdb:8086\" - Get http://monitoring-influxdb:8086/ping: dial tcp 10.3.0.103:8086: i/o timeout\nI1201 20:33:28.298463       1 heapster.go:72] Starting heapster on port 8082\nThis results in grafana throwing 500 errors when it tries to access\nhttps://kubernetes.example.com/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/api/datasources/proxy/1/query?epoch=ms&q=SELECT+last(value)+FROM+%22memory%2Flimit_bytes_gauge%22+WHERE+%22container_name%22+%3D~+%2F%7B%7D%2F+AND+time+%3E+now()+-+6h+GROUP+BY+time(10s)+fill(null)%0ASELECT+last(value)+FROM+%22memory%2Fusage_bytes_gauge%22+WHERE+%22container_name%22+%3D~+%2F%7B%7D%2F+AND+time+%3E+now()+-+6h+GROUP+BY+time(10s)+fill(null)%0ASELECT+last(value)+FROM+%22memory%2Fworking_set_bytes_gauge%22+WHERE+%22container_name%22+%3D~+%2F%7B%7D%2F+AND+time+%3E+now()+-+6h+GROUP+BY+time(10s)+fill(null)\n...since the k8s db was never created by heapster.\nRestarting the heapster container fixed the issue.\nIt seems either heapster needs to periodically retry connecting to\ninfluxdb, some sort of wait mechanism, or the docs need to be adjusted to\nnot launch them all simultaneously. Though I think the latter isn't a good\nchoice, as I imagine this issue can recur if the heapster & influxdb pods\never go down and come up at the same time (such as a node reboot).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/756.\n. SGTM. This issue shows up very frequently these days, that it warrants\nraising the logging level by default.\nDo you have some spare cycles to post a PR?\n\nOn Wed, Dec 2, 2015 at 12:19 PM, Patrick Hemmer notifications@github.com\nwrote:\n\nI was trying to track down why heapster wasn't gathering any stats, but\nthe logs weren't proving much help, only:\nE1202 20:04:54.955140       1 model_handlers.go:620] unable to get pod list metric: the model is not populated yet\nAfter increasing the logging level, I started seeing other errors such as:\nI1202 20:04:52.247233       1 kubelet.go:96] failed to get stats from kubelet url: http://10.0.0.220:10255/stats/default/firebirds-xjhq2/656abaa4-9855-11e5-98be-0e3733bda1bf/firebirds - Get http://10.0.0.220:10255/stats/default/firebirds-xjhq2/656abaa4-9855-11e5-98be-0e3733bda1bf/firebirds: dial tcp 10.0.0.220:10255: connection timed out\nI1202 20:04:52.247244       1 kube_pods.go:110] failed to get stats for container \"firebirds\" in pod \"default\"/\"firebirds-xjhq2\"\nI1202 20:04:52.247264       1 kube_nodes.go:59] Failed to get container stats from Kubelet on node \"ip-10-0-0-217.ec2.internal\"\nAs these are error messages, I think they should be logged at the error\nlevel, so that you don't have to adjust the log level on the daemon to see\nthem.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/757.\n. This is an issue with older versions of InfluxDB. A more recent version,\nInfluxDB v0.9.x, does not have this issue. The next heapster release will\ninclude support for v0.9.x and it should be released this week.\n\nOn Wed, Dec 2, 2015 at 1:14 PM, Patrick Hemmer notifications@github.com\nwrote:\n\nI'm new to heapster, influxdb, grafana, so I apologize if this isn't the\nright place, but this seems like an issue with heapster.\nThe problem is: I have an app running inside a pod which is consuming\nabout 40% CPU (according to docker stats on the container), though it\ndoes bounce up and down a bit. However when I look at the pod in grafana,\nthe chart is all over the place, including showing negative values. And the\nspikes grow larger over time.\n[image: image]\nhttps://cloud.githubusercontent.com/assets/1826947/11544044/d1bd8fda-990d-11e5-9cf4-7ba93707f01f.png\nDocker stats:\nCONTAINER           CPU %               MEM USAGE/LIMIT     MEM %               NET I/O\na990                46.93%              8.126 MB/3.951 GB   0.21%               0 B/0 B\nVersion stuff:\nkubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"2+\", GitVersion:\"v1.2.0-alpha.4\", GitCommit:\"4a9b0fc7153c733832d70c9f6d0796db177643a6\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"1\", GitVersion:\"v1.1.1\", GitCommit:\"92635e23dfafb2ddc828c8ac6c03c7a7205a84d8\", GitTreeState:\"clean\"}\nkubectl get rc heapster influxdb-grafana --namespace=kube-system\nCONTROLLER         CONTAINER(S)   IMAGE(S)                             SELECTOR                      REPLICAS   AGE\nheapster           heapster       kubernetes/heapster:canary           k8s-app=heapster,version=v6   1          1h\ninfluxdb-grafana   influxdb       kubernetes/heapster_influxdb:v0.5    name=influxGrafana            1          1h\n                   grafana        kubernetes/heapster_grafana:v2.1.0\nkubectl get pod heapster-8xek5 --namespace=kube-system -o jsonpath='{.status.containerStatuses[].imageID}'\ndocker://0a433ef5a50163c7b1552d7a478fee995f23f8679d853a8e1e9ea877900ce36e\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/759.\n. Either works for me. Thanks for investigating!\n\nOn Wed, Dec 23, 2015 at 1:41 AM, Antoine Cotten notifications@github.com\nwrote:\n\nThis is not yet sufficient, the patterns look right but the CPU usage\ncalculation doesn't.\nComparison of CPU metrics from New Relic, Kubedash and Grafana for one\nsingle node over 1 hour:\nhttps://cloud.githubusercontent.com/assets/3299086/11973798/6a607d1e-a95e-11e5-868c-c6d5ac7d33ed.png\nhttps://cloud.githubusercontent.com/assets/3299086/11973796/6a5e9f76-a95e-11e5-904d-1321c48a4240.png\nhttps://cloud.githubusercontent.com/assets/3299086/11973797/6a5f9408-a95e-11e5-9198-ddbdd77fa0b5.png\nThis results in a displayed value nearing or above 100% (8000m, in my\ncase), which is most likely wrong:\nhttps://cloud.githubusercontent.com/assets/3299086/11973988/0f4af6aa-a960-11e5-8079-ed2fcfaf8b7a.png\nhttps://cloud.githubusercontent.com/assets/3299086/11973989/0f6c38b0-a960-11e5-82fb-480b75f0492f.png\nFiddling with the \"Group by interval\" and changing it from >30s to\nprecisely 16s in this case gives me the following results:\nhttps://cloud.githubusercontent.com/assets/3299086/11974075/fcebd7ee-a960-11e5-8e94-193644418ef7.png\nThe higher the interval, the higher the value, which sounds wrong from the\nuser's perspective. Here with 120s:\nhttps://cloud.githubusercontent.com/assets/3299086/11974112/65d0e736-a961-11e5-8075-2af0f9bd2ebb.png\nI'll investigate further how we could determine something accurate\nregardless of the chosen interval. @vishh https://github.com/vishh\nshould I create a new issue or do you prefer to re-open that one?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/759#issuecomment-166847508\n.\n. ok to test\n\nOn Thu, Dec 3, 2015 at 7:13 AM, Tim Gossett notifications@github.com\nwrote:\n\nthe following lines might also be elevated to error level:\nintegration/framework.go:211:       glog.V(1).Infof(\"cluster validation failed - %q\\n %s\", err, out)\nintegration/framework.go:221:       glog.V(1).Infof(\"kube client creation failed - %q\", err)\nintegration/framework.go:358:       glog.V(2).Infof(\"Cannot delete namespace %q. Skipping deletion.\", ns)\nmanager/manager.go:137:             glog.V(1).Infof(\"Model housekeeping returned error: %s\", err.Error())\nmanager/manager.go:186:     glog.V(1).Infof(\"housekeeping resulted in following errors: %v\", errors)\nmodel/impl.go:149:      glog.V(2).Infof(\"nil namespace pointer passed to addPod\")\nmodel/impl.go:154:      glog.V(2).Infof(\"nil node pointer passed to addPod\")\nsinks/gcm/core.go:119:      glog.V(2).Infof(\"[GCM] Deleting metric %q failed: %v\", metricName, err)\nsinks/riemann/driver.go:199:            glog.V(2).Infof(\"Failed sending event to Riemann: %+v: %+v\", event, err)\nsources/kube_events.go:211:         glog.V(1).Infof(\"Event watch loop was terminated due to error. Will restart it. Error: %v\", err)\nsources/kube_nodes.go:64:       glog.V(3).Infof(\"No container stats from Kubelet on node %q\", host)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/762#issuecomment-161670787.\n. cc @piosz\n\nOn Mon, Dec 7, 2015 at 11:30 AM, andrejvanderzee notifications@github.com\nwrote:\n\nI have tried with volumes hostDir like this but it does not create\nanything in the host's path:\napiVersion: v1\nkind: ReplicationController\nmetadata:\nlabels:\nname: influxGrafana\nname: influxdb-grafana\nnamespace: kube-system\nspec:\nreplicas: 1\nselector:\nname: influxGrafana\ntemplate:\nmetadata:\nlabels:\nname: influxGrafana\nspec:\nnodeSelector:\nrole: monitor\ncontainers:\n- name: influxdb\n  image: kubernetes/heapster_influxdb:v0.5\n  volumeMounts:\n- mountPath: /data\n  name: influxdb-storage\n- name: heapster\n  image: kubernetes/heapster:canary\n  imagePullPolicy: Always\n  command:\n- /heapster\n- --source=kubernetes:http://172.31.7.28:8080\n- --sink=influxdb:http://127.0.0.1:8086\n- name: grafana\n  image: kubernetes/heapster_grafana:v2.1.0\n  ports:\n- containerPort: 3000\n  hostPort: 3000\n  env:\n- name: INFLUXDB_SERVICE_URL\n  value: http://127.0.0.1:8086\n  volumeMounts:\n- mountPath: /var\n  name: grafana-storage\n  volumes:\n- name: influxdb-storage\n  source:\n  hostDir:\n  path: /var/lib/monitor/influxdb\n- name: grafana-storage\n  source:\n  hostDir:\n  path: /var/lib/monitor/grafana\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/768#issuecomment-162633833\n.\n. ok to test\n. LGTM. Thanks for the PR @roboll \n. Yes.\n\nOn Tue, Dec 8, 2015 at 10:31 AM, rob boll notifications@github.com wrote:\n\n[image: :+1:] any time. assuming you can take care of pushing the updated\nimage once this merges?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/772#issuecomment-162971903.\n. Let's get this merged for now. Once 2.6 is available, feel free to post a PR :)\n. LGTM. We will have to update the config files also. \n. That error is returned by InfluxDB. We have tested HEAD against InfluxDB v0.9.4 and it used to work in the past. \nWe are in the process of releasing a new version of heapster with InfluxDB v0.9 support. \ncc @mwielgus: We should test v0.19.0 with the InfluxDB v0.9.4 before officially releasing it. \n. Heapster from HEAD supports InfluxDB v0.9 only. Are you running InfluxDB\nv0.8?\n\nOn Thu, Dec 10, 2015 at 3:25 AM, Romain Vrignaud notifications@github.com\nwrote:\n\nJust for the record, I've been running Heapster (commit d500aa9\nhttps://github.com/kubernetes/heapster/commit/d500aa975889f6ba2c2093b2bd518e17266c42fd)\nsuccessfully for a while. Since my cluster got upgraded to 1.1.2 I have\nthis kind of error with the d500aa version:\nE1210 11:22:08.390460       8 external.go:87] failed to sync data to sinks - encountered the following errors: unable to parse '...': missing fields\nThis is why I tried to upgrade heapster version to master without luck\n(see first comment).\nI can send privately full logs if needed.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/775#issuecomment-163583599\n.\n. Thanks for clarifying. I'm trying the new release v0.19.0 now.\n\nOn Thu, Dec 10, 2015 at 10:45 AM, Romain Vrignaud notifications@github.com\nwrote:\n\nAs I said I'm running Influxdb 0.9.4.1. Heapster d500aa9\nhttps://github.com/kubernetes/heapster/commit/d500aa975889f6ba2c2093b2bd518e17266c42fd\nhas successfully sent data for a while when I was running GKE 1.0.7.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/775#issuecomment-163714275\n.\n. I have been running heapster v0.19.0 with InfluxDB v0.9.6 successfully for\nthe last few days.\nI'm using the configs from HEAD.\n\nOn Wed, Dec 16, 2015 at 12:25 AM, Romain Vrignaud notifications@github.com\nwrote:\n\nI still have same problem (bad timestamp error) with heapster 0.19.1\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/775#issuecomment-165029969\n.\n. Setting heapster logging level to --vmodule=*=4 will be very helpful.\n\nOn Wed, Dec 16, 2015 at 11:16 PM, Romain Vrignaud notifications@github.com\nwrote:\n\nIs there anything I can provide to help you debug ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/775#issuecomment-165365561\n.\n. LGTM\n. Thanks for the fix @mwielgus! Can we use this ClientInitializer for this purpose?\n. Sure :) \n. heapster:canary might be built using master branch instead of\nheapster-scalability branch. Most of the upstream work is happening on the\nlatter branch.\n\nOn Thu, Mar 10, 2016 at 11:09 AM, Chris Kleban notifications@github.com\nwrote:\n\nI'm seeing this issue right now and I'm using the heapster:canary docker\nimage.\nHere are my logs. I've waiting about 28 minutes and still I don't see a\nretry.\nkubectl --namespace=kube-system logs heapster-v10-8y7i1 --follow\nI0310 18:40:14.541729 1 heapster.go:61] /heapster --source=kubernetes:\nhttps://kubernetes.default --sink=influxdb:http://monitoring-influxdb:8086\nI0310 18:40:14.541818 1 heapster.go:62] Heapster version 0.19.1\nI0310 18:40:14.543349 1 kube_factory.go:172] Using Kubernetes client with\nmaster \"https://kubernetes.default\" and version \"v1\"\nI0310 18:40:14.543363 1 kube_factory.go:173] Using kubelet port 10255\nE0310 18:40:19.678442 1 manager.go:299] encountered following errors while\nsetting up sinks - issues while creating an InfluxDB sink: failed to ping\nInfluxDB server at \"monitoring-influxdb:8086\" - Get\nhttp://monitoring-influxdb:8086/ping: dial tcp: lookup\nmonitoring-influxdb: no such host, will retry on use\nI0310 18:40:19.680930 1 heapster.go:72] Starting heapster on port 8082\nE0310 18:42:00.005793 1 kubelet.go:102] failed to get stats from kubelet\nurl:\nhttp://10.56.184.130:10255/stats/default/web-zelhp/c539e4e9-e6ef-11e5-8a2d-000c29fb6370/web\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E0310 18:42:00.010569 1 kube_pods.go:110] failed to get stats for\n  container \"web\" in pod \"default\"/\"web-zelhp\"\n  E0310 18:42:05.011524 1 kubelet.go:102] failed to get stats from kubelet\n  url:\n  http://10.56.184.130:10255/stats/default/web-zelhp/c539e4e9-e6ef-11e5-8a2d-000c29fb6370/web\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E0310 18:42:05.011578 1 kube_pods.go:110] failed to get stats for\n  container \"web\" in pod \"default\"/\"web-zelhp\"\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/780#issuecomment-195004407.\n. LGTM. We need to cherrypick this change into the release-0.18 branch.\n. I tested this change manually over the weekend and it works. merging.\n. I tested this change manually. Merging.\n. That's not expected. Heapster will continue its operations even if it encounters any errors.\ncc @mwielgus \n. cc @mwielgus: This seems like a concerning bug. Should we cherry-pick this into v0.19?\n. @pwittrock: Feel free to self-merge once you address the comment.\n. Yeah. Perhaps we can enhance the debugging.md doc with this information?\n\nOn Tue, Dec 15, 2015 at 10:57 PM, yuqi huang notifications@github.com\nwrote:\n\nSometimes, in our production environment, the timestamp of the cluster\nnodes are nonsynchronous for some reasons(such as NTP server do not work\ni.e).\nFor this reason, heapster can not get the metrics in two kinds of scene,\nas follows:\n-\ntimestamp of heapster is sooner than other nodes\nif TimeSpan > cache_duration, all the metrics data will be set to\n   tooOld,and be discarded.\nfunc (rc *realCache) isTooOld(lastUpdated time.Time) bool {\n    if time.Now().Sub(lastUpdated) >= rc.bufferDuration {\n        return true\n    }\n    return false\n}\n-\ntimestamp of heapster is later than other nodes\nAFAIK, heapster calls kubelet's api to get metrics data, such as:\n// Get stats for all non-Kubernetes containers.\nfunc (self *kubeletSource) GetAllRawContainers(host Host, start, end time.Time) ([]api.Container, error) {\n        ...\n    url := fmt.Sprintf(\"%s://%s:%d/stats/container/\", scheme, host.IP, host.Port)\n    return self.getAllContainers(url, start, end)\n}\nfunc (self *kubeletSource) getAllContainers(url string, start, end time.Time) ([]api.Container, error) {\n    // Request data from all subcontainers.\n    request := statsRequest{\n        ContainerName: \"/\",\n        Start:         start,\n        End:           end,\n        Subcontainers: true,\n    }\n    body, err := json.Marshal(request)\n        ...\n    req, err := http.NewRequest(\"POST\", url, bytes.NewBuffer(body))\n        ...\nBecause the start is later than the kubelet's timestamp, and if *TimeSpan\n\ncache_duration* ,\nthe response of POST request above will be empty. The reason is that, the\nkubelet does not hold the ageing metrics data.\n\nfunc (rm *realManager) Housekeep() {\n    for {\n        start := rm.lastSync\n        end := start.Add(rm.resolution)\n        ...\n        }\n    }\n}\nGenerally, users often spend much time to resolve issues deriving from\nthis reason.\nSo, should we add a logic in heapster to check the timestamp synchronous\nor not in the init process? And give a tips for users.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/802.\n. Sure post a PR. We can discuss the specifics over the PR.\n\nOn Wed, Dec 16, 2015 at 6:27 PM, yuqi huang notifications@github.com\nwrote:\n\nYes, I agree with you @vishh https://github.com/vishh , enhancing the\ndocs is a practicable way, i will add this tips.\nBut I think it's not a forcing method ^_^.\nMaybe, In my opinion, let the heapster to check in the init process is\nmore better, if the timestamp is nonsynchronous, it will return a warning\nor an error to users.\nBTW, all these come from usability of heapster, it's assistant .So what's\nyour opinion about add this to heapster?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/802#issuecomment-165317107\n.\n. LGTM. Thanks for the PR @tomdee \n. LGTM\n. Thanks for the PR @vicki-c. For CPU usage, I think we need to use non_negative_derivative function instead of derivative.\n. LGTM\n. Docker Image updated.\n\nOn Mon, Dec 21, 2015 at 3:19 AM, Marcel Klemenz notifications@github.com\nwrote:\n\n@vicki-c https://github.com/vicki-c thank you for your work.\n@vishh https://github.com/vishh can you please make a release of that?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/810#issuecomment-166274679.\n. cc @thucatebay \n. @huangyuqi: Kubelet APIs are also supposed to contain timestamps. Each stats sample provided by cAdvisor contains a timestamp. Why not re-use that? \nAnd BTW, why are we extending heapster to detect time skew, instead of say running a daemon per cluster (or per node) that detects time skew? That daemon for example could mark a node to be NotReady if time skew is detected. \n. LGTM. Thanks for the fix @antoineco. \n. ok to test\n. The pod and/or container is mostly not running on the kubelet.\n\nOn Tue, Dec 22, 2015 at 5:54 PM, Wu Fei notifications@github.com wrote:\n\nI1223 01:40:10666821 1 heapstergo:61] /heapster --source=kubernetes:\nhttps://kubernetesdefault --sink=influxdb:http://monitoring-influxdb:8086\nI1223 01:40:10666943 1 heapstergo:62] Heapster version 0191\nI1223 01:40:10668758 1 kube_factorygo:172] Using Kubernetes client with\nmaster \"https://kubernetesdefault\" and version \"v1\"\nI1223 01:40:10668798 1 kube_factorygo:173] Using kubelet port 10255\nI1223 01:40:10693724 1 drivergo:351] created influxdb sink with options:\n{root root monitoring-influxdb:8086 k8s false}\nI1223 01:40:10773177 1 drivergo:272] Created database \"k8s\" on influxDB\nserver at \"monitoring-influxdb:8086\"\nI1223 01:40:10777331 1 heapstergo:72] Starting heapster on port 8082\nE1223 01:40:15018363 1 kubeletgo:96] failed to get stats from kubelet url:\nhttp://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:40:15018396 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:40:20072006 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:40:20072094 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:40:25022254 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:40:25022317 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:40:30020519 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:40:30020553 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:40:35018795 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:40:35018855 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:40:40033585 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:40:40033693 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:40:45014130 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:40:45014204 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:40:50064095 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:40:50065104 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:40:55017329 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:40:55017350 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:41:00021505 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:41:00023014 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:41:05015772 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:41:05016693 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:41:10017381 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:41:10017508 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:41:15010812 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:41:15010914 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:41:20019273 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:41:20019331 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:41:25017475 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:41:25017609 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:41:30016801 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:41:30017041 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:41:35014202 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:41:35014287 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:41:40012590 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:41:40012686 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:41:45023101 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:41:45023418 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:41:50011450 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:41:50011617 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:41:55019194 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:41:55019417 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:42:00014253 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:42:00014276 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:42:05019330 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:42:05019432 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:42:10021359 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:42:10021388 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:42:15016257 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:42:15016401 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:42:20025086 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:42:20025278 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:42:25011700 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:42:25011721 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:42:30010230 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:42:30010350 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:42:35073396 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:42:35073422 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:42:40014557 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:42:40014725 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n  E1223 01:42:45011833 1 kubeletgo:96] failed to get stats from kubelet url:\n  http://100012:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello\n- request failed - \"404 Not Found\", response: \"no matching container\\n\"\n  E1223 01:42:45011962 1 kube_podsgo:110] failed to get stats for container\n  \"hello\" in pod \"default\"/\"hello-world\"\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/820.\n. ok to test\n. @antoineco: Thanks a lot for cleaning this up! \n. Can you create a new release by updating grafana/RELEASES.md? I can then push the corresponding images.\n. Yeah sure. \n. It is expected to show CPU in milli-cores. Kubernetes does not support normalizing CPUs as of now. So it is whatever the nodes are provisioned to use.\n. LGTM\n. Relevant docker images have been built and pushed.\n. cc @jimmidyson \n. General structure LGTM. Thanks @jimmidyson !!\n. The only issue with this PR is that of exposing the infra container. I'd prefer adding pod level metrics and exposing network as a pod level metrics.\n. Ok then. No issues in that case.\n\nOn Tue, Jan 12, 2016 at 8:00 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nThis is how's it's been done in heapster-scalability branch & I'm not\ngoing to duplicate that functionality as this is just a quick fix until\nheapster-scalability branch becomes master.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/852#issuecomment-170957230.\n. cc @jimmidyson \n. This was added originally to simplify configuration. Instead of supporting 10 different configuration files and making our salt scripts complex, we can deploy a standalone heapster and then dynamically add/remove sinks. \n. I meant standalone configuration, not a pod. The default kube setup can\nbring up heapster without any sink and once the sinks are setup, admins can\nregister those sinks.\nStatic configuration is one of the biggest pain points today with existing\nmonitoring solutions.\nOn Jan 8, 2016 11:58 PM, \"Marcin Wielgus\" notifications@github.com wrote:\nThank you for the explanation. Allowing non-persistent configuration is\njust asking for troubles/outage. And crashes/restarts do happen. For that\nreasons we run Heapster under replication controller and not as a\nstandalone Pod.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/867#issuecomment-170208171.\n. Config needs to be persisted.\n\nOn Mon, Jan 11, 2016 at 9:32 AM, Piotr Szczesniak notifications@github.com\nwrote:\n\nAnd what would happen in case when Heapster is restarted?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/867#issuecomment-170627218.\n. LGTM\n. @piosz: Feel free to self merge once the tests are passing.\n. Shouldn't this be handled as part of golang setup? If required, we can\nreference all binaries relative to $GOPATH.\n\nOn Wed, Jan 27, 2016 at 10:27 AM Matthew Wringe notifications@github.com\nwrote:\n\nCurrently as part of the Makefile we install godeps (\nhttps://github.com/kubernetes/heapster/blob/master/Makefile#L11), but\nthis is not automatically added to the path, so its not used without manual\nconfiguration.\nIf we are installing godeps as part of the build, we should be using this\nversion for the build\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/906.\n. FYI: The pod is not deemed running until all containers have started and\nreadiness checks are satisfied. For monitoring, we might want to track\nindividual containers even if their parent pod is not ready.\n\nOn Fri, Jan 29, 2016, 7:05 AM Jimmi Dyson notifications@github.com wrote:\n\nIf pods aren't running, i.e. containers aren't started, then there are no\nstats for them. Docker creates & cleans up cgroups that stats are gathered\nfrom on start/stop of containers.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/919#issuecomment-176800965.\n. FYI: @jimmidyson: https://github.com/kubernetes/kubernetes/pull/20687 combined with the new metrics API in the kubelet should solve the issue of identifying interesting system containers.\n. @timstclair: Similar to network stats, can we add volume disk usage as well? We might need an additional field/label to qualify volume name.\n. Completed my first pass through this PR.\n. @timstclair: What's the status on this PR?\n. I wish clients have to include only the API schema :( \nLGTM.\n. Yeah. It is annoying.\n\nOn Wed, Feb 3, 2016 at 1:05 PM Marcin Wielgus notifications@github.com\nwrote:\n\nLGTM\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/932#issuecomment-179466638.\n. ok to test\n. LGTM\n. @k8s-bot: test again\n. @pwittrock: Docker daemon on the jenkins node running heapster e2e is possibly broken. Can you take a look?\n. @timstclair: Are you able to run make test-integration locally?\n. The test infra seems to be stable. I tried building against HEAD and it\npasses.\n\nOn Tue, Feb 9, 2016 at 11:17 AM Dawn Chen notifications@github.com wrote:\n\nTest infra failure here?\n--- FAIL: TestHeapster (514.39s)\nassertions.go:154:\nLocation:   heapster_api_test.go:683\nError:      No error is expected but got failed to build docker binary (\"exit status 1\") - \"/jenkins-master-data/workspace/project/src/k8s.io/heapster/deploy/docker /jenkins-master-data/workspace/project/src/k8s.io/heapster/deploy/docker\\nCannot http://k8s.io/heapster/deploy/docker%5CnCannot connect to the Docker daemon. Is the docker daemon running on this host?\\n\"\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/956#issuecomment-182014472.\n. ok to test\n. Tests have passed. This is good to go. @mwielgus ok to merge?\n. Lol :)\n\nOn Tue, Feb 9, 2016 at 2:40 PM, Marcin Wielgus notifications@github.com\nwrote:\n\nMerged #956 https://github.com/kubernetes/heapster/pull/956.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/956#event-545049822.\n. LGTM.\n. ok to test\n\nOn Tue, Feb 16, 2016 at 10:47 AM, Kubernetes Bot notifications@github.com\nwrote:\n\nCan one of the admins verify that this patch is reasonable to test? (reply\n\"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/982#issuecomment-184822476.\n. @k8s-bot https://github.com/k8s-bot test this\n\nOn Tue, Feb 16, 2016 at 12:19 PM, Kubernetes Bot notifications@github.com\nwrote:\n\nJenkins GCE e2e\nBuild/test failed for commit ec18050\nhttps://github.com/kubernetes/heapster/commit/ec180504eb069a592382c3a1adc0ffc563954225\n.\n- Build Log\n  https://storage.cloud.google.com/kubernetes-jenkins/pr-logs/pull/982/heapster-pull-build-test-e2e/198/build-log.txt\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/982#issuecomment-184860881.\n. Tests have passed. @mwielgus I'm merging this PR to make progress on #927. If you have any issues with this PR, we can revert this and address your concerns.\n. LGTM\n. I started putting together a patch for this issue. I'm yet to test it. I can own this issue.\n. @k8s-bot test this\n. the canary image is updated everyday. We should use an explicit version.\n\n@fgrzadkowski @mwielgus We should get this resolved soon since Kubernetes v1.2 is already out.\n. LGTM\n. Build failed because the PR was merged before the build started. I did test\nthis PR manually though.\nOn Thu, Mar 3, 2016 at 1:33 PM, Kubernetes Bot notifications@github.com\nwrote:\n\nJenkins GCE e2e\nBuild/test failed for commit e7bad40\nhttps://github.com/kubernetes/heapster/commit/e7bad40eefb48a0d51d0fcfc85608a1a7be05bb0\n.\n- Build Log\n  https://storage.cloud.google.com/kubernetes-jenkins/pr-logs/pull/1041/heapster-pull-build-test-e2e/247/build-log.txt\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1041#issuecomment-191973070.\n. Did you accidentally include two other files?\n. LGTM\n. Let's wait for the tests to pass.\n. I tested this on a real cluster. Sample output here using the summary API - https://gist.github.com/vishh/c5ca39ede110d38a8051\n. Thanks @piosz!\n\nOn Thu, Mar 10, 2016 at 9:28 AM, Kubernetes Bot notifications@github.com\nwrote:\n\nJenkins GCE e2e\nBuild/test passed for commit 8c3aa0e\nhttps://github.com/kubernetes/heapster/commit/8c3aa0e11aeaae8eb178af3dc9dbd18026f8542e\n.\n- Build Log\n  https://storage.cloud.google.com/kubernetes-jenkins/pr-logs/pull/1055/heapster-pull-build-test-e2e/264/build-log.txt\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1055#issuecomment-194967002.\n. The data surfaced from the kubelet is valid. Here is a sample output. I suspect an issue with heapster aggregation.\n. I could not find any issues in the code either. Closing this issue.\n. Is this data for a pod or a container? Limits and Requests are per container. \nAs for filesystem metrics, what is your node configuration? We don't support device mapper docker storage driver as of now.\n. https://github.com/google/cadvisor/issues/959\n. For what numbers?\n. We haven't changed disk usage semantics since the last release.\n. @DirectXMan12 \nJust to reiterate, I'm not suggesting that all application-level metrics go though Heapster. The stated goal of custom metrics support has been to support only those custom metrics needed for autoscaling and related things. This proposal does not seek to change that.\n\nHow do you envision enforcing this logic? What would prevent users from exposing monitoring specific metrics via this model?\n. > Taking this from another angle:\n\nMaking a clear separation between your regular monitoring and your metrics\nfor auto-scaling/scheduling to me means that they should not end up in the\nsame storage.\n\nThis might not be a valid assumption. AFAIK, there are use cases where\nservice metrics for auto-scaling can be surfaced via metrics monitoring\nsystems - sysdig, GCM, etc.\n. I'd recommend holding off this PR until the API from cAdvisor & kubelet perspective is decided.\n. +1 for adding it to the summary API. We need to expose it in a more user\nfriendly fashion than what cAdvisor does though.\nOn Thu, Jun 23, 2016 at 1:01 PM, Tim St. Clair notifications@github.com\nwrote:\n\nThese actually are collected by cAdvisor (\nhttps://github.com/google/cadvisor/blob/b4f1d7b82f87459070b58052b71ac4524a4344c3/info/v1/container.go#L301),\nbut not surfaced in the summary API since heapster wasn't using them. If we\nwant to include them in heapster, we could add them to the summary.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/1155#issuecomment-228166566,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AGvIKIQn8MLNh_ojjEDEeHsCvYnF0dh9ks5qOuYvgaJpZM4Icdgg\n.\n. Have you looked at kube-state-metrics project?\n. cc @mwielgus \n. https://cloud.google.com/monitoring/api/ref_v3/rest/\n. That will require some changes to how auth is handled.\n\nOn Fri, Jun 17, 2016 at 10:26 AM, activars notifications@github.com wrote:\n\n@vishh https://github.com/vishh Will this enable Heapter writing custom\nmetrics from a kube cluster running outside GCE?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/1175#issuecomment-226830395,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AGvIKAhk9nx9UOV8zY812rJ1A6tUHuzVks5qMtjKgaJpZM4In1sr\n.\n. \ud83e\udd17 \ud83d\udc4d \n. I can review the build aspects of this PR.. @luxas updating influxdb and grafana would be great. We don't have a\nrelease process for those images since they meant to serve as a reference\nimplementation only. If the build process is robust, then just documenting\na release process for these images should suffice.\n\nOn Tue, Jan 3, 2017 at 3:16 PM, k8s-ci-robot notifications@github.com\nwrote:\n\nJenkins GCE e2e failed\nhttps://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/heapster/1387/pull-heapster-e2e/80/\nfor commit 4a1c976\nhttps://github.com/kubernetes/heapster/commit/4a1c9764479aceb2b007b923aced83f1c3001b43.\nFull PR test history http://pr-test.k8s.io/1387.\nThe magic incantation to run this job again is @k8s-bot test this. Please\nhelp us cut down flakes by linking to an open flake issue\nhttps://github.com/kubernetes/heapster/issues?q=is:issue+label:kind/flake+is:open\nwhen you hit one in your PR.\nIf you have questions or suggestions related to my behavior, please file\nan issue against the kubernetes/test-infra\nhttps://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:\nrepository. I understand the commands that are listed here\nhttps://github.com/kubernetes/test-infra/blob/master/prow/commands.md.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1387#issuecomment-270253105,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKO9xYBXEJ8P4jprOkNJtCaKU85V_ks5rOtaygaJpZM4K12-1\n.\n. Is make push the magic command to build and push for all architectures? If yes, I'd suggesting having a dry run mode where a real push doesn't happen, just to verify images locally. . +1. Just ensure that the default dashboards continue to work after the\nupgrade.\n\nOn Tue, Jan 3, 2017 at 2:37 PM, Lucas K\u00e4ldstr\u00f6m notifications@github.com\nwrote:\n\nI'm in favor for updating as well, let's see if we can get it into the new\nrelease!\ncc @piosz https://github.com/piosz @vishh https://github.com/vishh\n@DirectXMan12 https://github.com/DirectXMan12\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1415#issuecomment-270245766,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKCgJs-6msFVRSYctHO3UEunZAr2oks5rOs3HgaJpZM4LMLvN\n.\n. In on-prem clusters, disk is often a contended resource. We do not provide\nany isolation for disk yet. The best we can do is to expose usage metrics.\nDisk IO is a common issue and exposing that metrics to users will enable\nthem to identify offending pods and take action. So I'd recommend adding\nmetrics for \"space\", \"inodes\" and \"IO\" for disk\n\nOn Fri, Jan 13, 2017 at 12:48 AM, Piotr Szczesniak <notifications@github.com\n\nwrote:\n@timstclair https://github.com/timstclair @vishh\nhttps://github.com/vishh can one of you take a look? Does it make sense\nto have those metrics?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1450#issuecomment-272391229,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKFT_w-7tp_eSX_LeMdlS9Q7eukwQks5rRzp0gaJpZM4LePK3\n.\n. It is already using the cadvisor client. Are you referring to some other part of the client that is not being consumed?\n. Good catch. My go version is older than that of what @vmarmol ran when he originally ran godeps. I am updating my local go version\n. I don't know what this field actually does. This is the config that I uploaded to the repo and it seems to be fine.\n. Done.\n. This dashboard is now only for kubernetes clusters. \n. Nope. They are all 'go fmt' changes.\n. :+1: \n. It is just a minor inconvenience. go fmt would keep correcting these files until they are fixed. \n. Can you update Readme.md to reflect the hostport change?\n. Update deps. I don't think the version number here matches the cadvisor's versioning system. \n. I don't know. I am intending to fix the deps to v0.7.2.\n. Removed.\n. Done\n. no :) removed.\n. The intent is to reuse kube releases across runs. That is the reason for using the same tempdir.\n. Done\n. Done.\n. Done\n. Done\n. Done.\n. Done.\n. nit: self.writeFailures++ instead?\n. Pod UID might be a better to differentiate between different instances of the same Pod.\n. Ahh I see the purpose. Lets add the UID also just to be sure.\n. Kube DNS service assumes port '80' to be the default port for all services. It is not identifying the port specified for the service in the Spec. This might be a bug that has already been fixed. Since every service has its own IP, the port here doesn't really matter much.\n. Done.\n. Done.\n. These variables are populated by the kubelet as part of static service\ndiscovery. We can add more additional checks for sure, but it might not be\nnecessary.\n\nOn Wed, Jan 21, 2015 at 2:35 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nIn deploy/run.sh\nhttps://github.com/GoogleCloudPlatform/heapster/pull/66#discussion_r23340861\n:\n\nif [ ! -z $KUBERNETES_RO_SERVICE_HOST ]; then\n-    echo \"Detected Kube specific args. Starting in Kube mode.\"\n-    KUBE_ARGS=\"--kubernetes_master $KUBERNETES_RO_SERVICE_HOST:$KUBERNETES_RO_SERVICE_PORT\"\n  -fi\n-  # TODO(vishh): add support for passing in user name and password.\n-  INFLUXDB_ADDRESS=\"\"\n-  if [ ! -z $MONITORING_INFLUXDB_SERVICE_HOST ]; then\n-    INFLUXDB_ADDRESS=\"${MONITORING_INFLUXDB_SERVICE_HOST}:${MONITORING_INFLUXDB_SERVICE_PORT}\"\n\nDo we need to check if a port was passed in\n(MONITORING_INFLUXDB_SERVICE_PORT)?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/66/files#r23340861.\n. This might result in linear memory growth since we don't prune the activePods map at all.\n. Yeah. we can do that.\n\nOn Wed, Jan 21, 2015 at 3:21 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nIn sources/kube.go\nhttps://github.com/GoogleCloudPlatform/heapster/pull/69#discussion_r23343819\n:\n\n@@ -259,6 +272,7 @@ func newKubeSource() (KubeSource, error) {\n        client:      kubeClient,\n        lastQuery:   time.Now(),\n        kubeletPort: argKubeletPort,\n-       activePods:  make(map[PodInstance]bool),\n\nyes, this is a problem because we don't keep a list of pods being tracked.\nWould it be good enough to regenerate the list in getPods() - purge all old\nentries that are not present in the new listing from getPods()?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/69/files#r23343819.\n. It just occurred to me that another approach to handle this is to ignore Pods that are not in \"Running\" state. \n. I removed the flag defaults here and added the flag to integration/.jenkins.sh\n. Kubelet is not expected to set the Pod state to running unless all the containers are in running state. If you notice a different behavior its probably a bug.\n. YUP. AFAIK kubelet waits until all containers are running. On the other hand what is the intended purpose of this counter? Is it to capture infrastructure (kubelet/cadvisor) errors while trying to retrieve stats information for a \"running\" pod?\n. nit: Wrap err around a more descriptive error message. \n. nit: Add units here. As discussed offline, we should hopefully not have housekeeping intervals grater than 2 minutes.\n. May be a TODO to export more debug information?\n. Added a file level comment.\n. Done. \n. Yeah. We poll on a in-memory cache that gets updated asynchronously. In the future we can move to a model where we identify creation and deletion of pods and have a handler per pod. I will have to dig into the watch implementation for that.\n. Done\n. Done\n. To be unambiguous this would be the \"hostname of the node where the container ran\"\n. How about moving this to a separate file since it will grow quite a bit? It will also help maintain unit tests in the future.\n. Makes sense. Done.\n. Done.\n. lastQuery is not accurate as of now since we poll cadvisor/kubelet serially. \n. +1. Specifying time duration instead of a numeric value might be more useful.\n. Should we instead try and figure out why we get duplicate data? A sink should not necessarily care about all this. We can add a filter in between source and sink to filter such duplicate data.\n. Since this method pushes the data in addition to decoding it, consider renaming it or splitting the pushing part into a separate method.\n. +1 for sorting. Very thoughtful!\n. Why is the code commented?\n. Considering adding this to validate output.\n. Is this a custom cluster setup where the node name is its IP? Why so? Why not give the node an unique hostname that can be resolved to an IP?\n. Just so that I understand clearly, the hostname of the machines in the cluster matches the IP? Does the rest of kubernetes work given the resolution issues?\nSorry for being a pain, I am just trying to understand the underlying problem!\n. Ahh. Thanks for explaining. +1 to splitting the PRs and thanks for identifying and fixing the namespace handling bug :+1: \nLet us know if you are not able to find a way to get around the hostname resolution issue and I can explore to see if the proxy server on the kube-master can proxy traffic to kubelets instead of having to talk to the kubelets directly.\n\nWhat does no.Status.HostIP point to in the case of Rackspace? Does it point to an internally accessible IP with the kubelet port exposed?\n. Done.\n. nit: relaunch the controller's'\n. Do we need any other auth information for gcm? What will the user have to do to get GCM to work?\n. Ok. We can clarify that in the Readme as well.\nOn Wed, Feb 18, 2015 at 12:31 PM, Victor Marmol notifications@github.com\nwrote:\n\nIn sinks/types.go\nhttps://github.com/GoogleCloudPlatform/heapster/pull/112#discussion_r24937616\n:\n\n@@ -19,7 +19,7 @@ import (\n    \"fmt\"\n )\n-var argSink = flag.String(\"sink\", \"memory\", \"Backend storage. Options are [memory | influxdb | bigquery]\")\n+var argSink = flag.String(\"sink\", \"memory\", \"Backend storage. Options are [memory | influxdb | bigquery | gcm ]\")\n\nToday we only support running in GCE (which we check and error out on in\nNew) and we get the auth information from the machine's service accounts\n(which we also check and error out on).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/112/files#r24937616\n.\n. This is a public repo. Any suggestions for other repos? I can use kubernetes, if you think thats ok.\n. Done\n\nOn Thu, Feb 19, 2015 at 9:22 AM, Victor Marmol notifications@github.com\nwrote:\n\nIn clusters/coreos/build.sh\nhttps://github.com/GoogleCloudPlatform/heapster/pull/114#discussion_r25006093\n:\n\n@@ -0,0 +1,9 @@\n+#!/bin/bash\n+\n+set -e\n+\n+pushd $( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\n+godep go build -a\n+\n+docker build -t vish/heapster-buddy-coreos:canary .\n\njust \"heapster-buddy-coreos\" which keeps it local. If you want to push it\nto a repo you can do:\n$ docker tag heapster-buddy-coreos kubernetes/heapster-buddy-coreos\n$ docker push kubernetes/heapster-buddy-coreos\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/114/files#r25006093\n.\n. That is true. If users set up https, then we will have to pass auth info. I don't want to do it unless some one needs it.\n. Depends on if we have access to the 'filesystem'.\n\nOn Thu, Feb 19, 2015 at 7:05 PM, Victor Marmol notifications@github.com\nwrote:\n\nIn sources/nodes/coreos.go\nhttps://github.com/GoogleCloudPlatform/heapster/pull/116#discussion_r25046759\n:\n\n\nself.nodes = nodeList\nreturn nodeList, nil\n  +}\n  +\n  +func (self *fleetNodes) DebugInfo() string {\noutput := fmt.Sprintf(\"Fleet Nodes plugin: Aggregate error count: %d; recent error: %v\", self.apiErrors, self.recentApiError)\nif self.nodes != nil {\noutput = fmt.Sprintf(\"%s\\nCadvisor Nodes: %v\", output, self.nodes.Items)\n}\nreturn output\n  +}\n  +\n  +func getFleetRegistryClient(fleetEndpoints []string) (fleetClient.API, error) {\nvar dial func(string, string) (net.Conn, error)\n  +\ntlsConfig, err := fleetPkg.ReadTLSConfigFiles(\"\", \"\", \"\")\n\n\nI think we can just get this from the filesystem.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/116/files#r25046759\n.\n. Great!\n\nOn Thu, Feb 19, 2015 at 7:09 PM, Victor Marmol notifications@github.com\nwrote:\n\nIn sources/nodes/coreos.go\nhttps://github.com/GoogleCloudPlatform/heapster/pull/116#discussion_r25046850\n:\n\n\nself.nodes = nodeList\nreturn nodeList, nil\n  +}\n  +\n  +func (self *fleetNodes) DebugInfo() string {\noutput := fmt.Sprintf(\"Fleet Nodes plugin: Aggregate error count: %d; recent error: %v\", self.apiErrors, self.recentApiError)\nif self.nodes != nil {\noutput = fmt.Sprintf(\"%s\\nCadvisor Nodes: %v\", output, self.nodes.Items)\n}\nreturn output\n  +}\n  +\n  +func getFleetRegistryClient(fleetEndpoints []string) (fleetClient.API, error) {\nvar dial func(string, string) (net.Conn, error)\n  +\ntlsConfig, err := fleetPkg.ReadTLSConfigFiles(\"\", \"\", \"\")\n\n\nWe do from the container :)\nI have to send a PR to add a hostVolume for Heapster so we can use HTTPS.\nWe need it for GCM and I forgot to add it :P\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/116/files#r25046850\n.\n. Noob question: Is this the default location for storing certs for most applications? \n. Ok.\n. Why add labels to the service?\n. Having container results in lesser code inside the loop. :)\n. Which log line? The two ones below seem to be fine.\n. Agreed! Done.\n. yeah. It is indeed confusing.\n. Coreos support no longer requires the buddy. This section needs to be updated. We can do this in a later PR.\n. nit: The comment here should be \"check if we are running in kubernetes\". i see that it was incorrect to begin with.\n. Should we do the master service discovery ahead of time and prepare heapster args before checking for storage drivers?\n. Not related to this PR: Can we consider stashing the Has* info into cadvisor Stats instead of requiring the Spec in the latest API?\n. This could apply to local cadvisor or kubelet. How do we differentiate that?\n. I think we can get rid of this check.\n. nit: InfluxDB service address not specified.\n. Ahh ignore my comment. I see below why its necessary.\n. Done\n. Done\n. is it cluster or context? Any naive user would have issues with this.\n. I updated it to cluster-name.\n. Ack.\n. Calling it a \"Point\". A timeseries here is a wrapper around a point and descriptor.\n. I want it to be an interface{} because not all backends can handle all input types. For example events cannot be handled by GCM. \n. As of today we understand stats, but events and config data are not yet defined. That is the reason for generic interface. Is there a reason why you are suggesting a stronger type? Testing?\n. Done\n. Done\n. Done\n. Count make sense\n. Ack.\n. Ack.\n. Done\n. Added a TODO :)\n. Done\n. Done\n. Ahh din't realize that. Done\n. Done\n. Should we rename it to kube-config instead?\n. This might be worth doing in the central decoder\n. Should we add a TODO to batch writes?\n. I think we can round to seconds everywhere. \n. True. Once we start adding custom metrics, wouldn't we hit this condition?\n\nOn Fri, Mar 20, 2015 at 3:59 PM, Victor Marmol notifications@github.com\nwrote:\n\nIn sinks/gcm/driver.go\nhttps://github.com/GoogleCloudPlatform/heapster/pull/176#discussion_r26883098\n:\n\n+\n-       err := self.pushMetrics(&request)\n-       if err != nil {\n-           lastErr = err\n-       }\n-   }\n  +\n-   return lastErr\n  +}\n  +\n  +func (self gcmSink) pushMetrics(request metricWriteRequest) error {\n-   if len(request.Timeseries) == 0 {\n-       return nil\n-   }\n-   if len(request.Timeseries) > maxTimeseriesPerRequest {\n-       return fmt.Errorf(\"unable to write more than %d metrics at once and %d were provided\", maxTimeseriesPerRequest, len(request.Timeseries))\n\nThis is just a sanity check, by now we should have done this.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/176/files#r26883098\n.\n. Poll duration is how often we poll for stats. Updated the description.\n. Good point. Done.\n. nit: s/GRAGANA/GRAFANA\n. nit: s/GRAGANA/GRAFANA\n. This comment is invalid. But I fixed cadvisor.go which is the correct place for this comment.\n. Thats true. But that would require us passing in last timestamp all the way through. I resorted to requiring the resolution to be relative to the polling duration. If you think it is too confusing for users, I can plumb last timestamp as well.\n. Why add this file?\n. For testing purposes, can we can spawn an influxdb container and use that instead?\n. nvm. I see a testing use case below. Should we move to this under a '/test' directory?\n. Runtime expansion sounds awesome. But DNS should take care of this. So in kubernetes, this could just be the Influxdb service name.\n. \"INFLUXDB_URL\" is not specified in the dockerfile. Is this intentional? Can this be defaulted to the influxdb service name?\n. I remember changing it to ':' some time back.\n. Ohh yeah :) Fixed\n. Fixed\n. Ahh :) Time to go home I guess. Fixed.\n. Good idea. Done.\n. Any reason to delete this file?\n. The 'url' needs to be that of where Grafana is accessible (ex. :https://<master>/api/v1beta1/proxy/services/monitoring-grafana). \nThe intention is to make Grafana client side re-use its URL for accessing influxdb.\n. So I see two separate config requirements here.\n1. How to access Influxdb internally from Grafana container.\n2. How to access Influxdb externally from outside the cluster (which is that of reaching back to the proxy endpoint in the Grafana container).\nIIUC, both of those configs are conflated now.\n. nit: Why not return here?\n. In case of an error, we should bubble up the error and try again. \n. Should we continue and return stats even if we cannot retrieve events?\nThe existing code also needs to change as well. So its fine to deal with this in a separate PR.\n. Ack. I don't mind leaving it as-is.\n. Done\n. Done\n. Done\n. Yup. Filed #194.\n. Fixed. Removing \"Error - \" makes sense.\n. Done\n. Ack.\n. Shall we use this script to deploy heapster with different sinks? \nIdeally I want to have a go binary for deployment, but this script isn't a bad place to start. \n. Should we instead state that GCM will work only when running on GCE?\n. We don't access cadvisor directly anymore in heapster. Should we even mention cadvisor port here? \n. This should go in a central location. May be in the landing page?\n. Sounds good.\n. @vmarmol: Can you update this to v0.10.0 since its released?\n. Done\n. Good suggestion\n. Added some documentation.\n. Done\n. Done\n. Travis needs to be updated. Adding to .gitignore sgtm\n. Precision is not included here.\n. nit: How about reversing the condition and continuing if its nil?\n. Ditto as above.\n. Why do we need this check? Why can't we use the precision that is passed via timeseries.TimePrecision?\n. This log line is misleading. Why are we logging precision in place of number of timeseries?\n. Is this for optimization? Why can't we use all the labels defined as part of the point?\n. How about sharing setting up of these standard columns between the two methods here?\n. Should we round the timestamp here to a millisecond?\n. I don't think GCM will like diverging from the registered series names. What ever series name we used while registering the events is what we should use here.\nIf Influxdb decides to mangle the series name with additional metadata, then it must be local to Influxdb.\n. Hmm. Maybe rephrase to Successfully wrote timeseries to InfluxDB with precision: %q?\n. Ah! Why do we care about precision at an API level? Why not pass timestamps with max precision and round to a sane precision in the sinks?\n. I found the same pattern in a few other files. Can you make the same change in other files as well?\n. These labels might not show up as part of LabelKeys. Why not include all metric labels by default as part of labelkeys?\n. I think this can be just the dns name: source=kubernetes:https://kubernetes-ro. Does the SkyDNS setup work in OpenShift?\n. s/Replac/Replace\n. Should we check for atleast one event from all pods?\n. Note that it says unreleased. I am just tracking all the changes as we make\nthem under a new release label.\n\nOn Mon, Apr 13, 2015 at 5:39 PM, Victor Marmol notifications@github.com\nwrote:\n\nIn grafana/RELEASES.md\nhttps://github.com/GoogleCloudPlatform/heapster/pull/236#discussion_r28292850\n:\n\n@@ -1,5 +1,8 @@\n # Release Notes\n+## 0.7 (Unreleased)\n\nWas this meant to be here? Maybe do in another PR or commit? We should\nprobably mention some of the GCM changes as well.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/236/files#r28292850\n.\n. Nice job with these tests! :100: \n. @vmarmol: This never seems to work. Deletion works via the API explorer. Can you try this code once and see if it works for you?\n. Ok. Updated all metric to be doubles. The only downside is that now all the existing metrics are invalid. To solve this, I have added code to query existing metrics, check if they differ from requirements and delete if they do, and re-add the metric.\n. WIP :) Fixed it.\n. I am relying on the kubernetes.io namespace to isolate just our metrics. The prefix check makes sense.\n. Done\n. Deleted it\n. Done\n. Done\n. How to access and graph custom metrics; A snapshot of all the metrics stored in gcm would help.\n. The pointer should do the trick ;)\n. Done\n. Done\n. Agreed! \n. Good point. I meant to do it and somehow forgot to do it.\n. Ah! I did not know that. updated!\n. Done\n. Done\n. I would prefer posting a full list just to make it easy. May be just list all the rate metrics that are specific to GCM.\n. +1. We can also group gcl and gcm into a google sink to share some common code.\n. We should surface write errors here to aid in debugging.\n. Should this be \"kubernetes.io\" to match the naming for GCM?\n. nit: Use http.StatusOK instead\n. By flags are you referring to the opaque part of the url? These drivers will require more options once they are enhanced to work outside of GCE as well.\n. Can you elaborate a bit more? \n. Done. Its actually port '80' since its a kubernetes service.\n. Good catch! Done\n. Done\n. Done\n. You have not specified any hostport. Why add just the container port?\n. Good catch :) Fixed\n. I am getting ahead of myself! Multiple sources isn't supported yet. Fixed\n. I assume #269 is required for this PR to just work?\n. nit: Take a look at the go style guide here and update the code.\n. Since this is a one time operation, I think we can disregard the expense. WDYT? Have updates would make user experience nicer. Very soon, we hope to lock down the current storage schema with appropriate versioning.\n. Why are you not performing the actual registration here?\n. Why? \n. Why not combine the two methods into one? \n. nvm. That requires kubelet change.\n. Can we rename this to GetAllRawContainers?\n. #279 takes care of kube auth plumbing. Once that goes in, can you rebase this PR on HEAD and then this will be good to go!\n. #279 is merged.\n. Currently we have gauges and cumulative metrics. Wouldn't storing cumulative metrics as gauges be confusing?\n. @burmanm: Ping\n. I intend to get rid of the source API structs and make the sources write PodElements directly to the cache.\n. What about raw cgroups? They are not system containers in the literal sense right?\n. nit: Can you add a V(4) log here that states that we are using the public IP as-is?\n. I was thinking of keeping the cache as part of heapster manager instead of here since it holds data that is not in metrics form.\n. Discussed offline. This is working as intended. Needs documentation though.\n. Fixed\n. Done\n. Yeah. As discussed offline, I will try that once the basic framework is in.\n. Moved real gc to here. Ignoring this nit.\n. Not necessary. Fixed\n. Done\n. Makes sense. Done\n. Makes sense done.\n. Not needed. Fixed.\n. Ack.\n. Done\n. Done\n. Can this API be \"metrics\" instead of \"metric-export\"?\n. Done\n. Done\n. Yeah. Thats true\n. Good catch. There is an issue. I think I have fixed it this time. \n. Done\n. True. Fixed.\n. Ah. Thats very helpful. Done\n. Done\n. There is one as part of TestDelete()\n. Done\n. We can store points out of order. Based on how we end up storing data, we might end up clearing more points that we intended to. We can take the most recent point's timestamp for GC. I wonder if it is necessary to keep data points that are older than the buffer duration?\n. Done\n. Ah! Missed that. Fixed\n. Done\n. Fixed.\n. I don't think I intended to make this change. Merge issue maybe? Will fix it anyways.\n. Done.\n. This needs to be UnitsCount since cores is not in nanoseconds.\n. Will /var/run/secrets' be auto mounted for the heapster pod?\n. How about adding an example config for running heapster in OpenShift? Does it already exist elsewhere? \n. Can you provide a pointer to an example PodSpec or may be some pod that uses service accounts?\n. We use device id's as a label to differentiate between filesystems.\n. Done\n. Any reason why the 'compute.googleapis.com' prefix is necessary? We can add that when we push to GCM. The labels here are meant to be generic to monitoring backends.\n. The heapster core is in a bit of a flux. I apologize for the inconvenience. \nWe currently push node usage as part ofcpu/usagemetric, withcontainer_namelabel set tomachine. Will that suffice your use case? \nIf not, this change will not expose node_usage if resourceId is not set, which might be the case in baremetal deployments. Should we instead annotate ContainerSpec with either a Container name or isNode boolean and use that here?\nIf resource ID is available we can push that label as part of node_usage.\n. As mentioned above, 'memory/usage' should include node stats as well. Label \"container_name=machine\" is the identifier. If that is inconvenient, we can avoid including node metrics inmemory/usage. WDYT?\n. Why not call itExternalID`? That way it remains generic.\n. Removed these changes.\n. Unintentional change. Removed.\n. Its for developer use.\n\nOn Fri, Jun 12, 2015 at 3:03 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nIn Makefile\nhttps://github.com/GoogleCloudPlatform/heapster/pull/335#discussion_r32359189\n:\n\ntest-unit-cov: clean deps sanitize build\n    hooks/coverage.sh\n-test-integration: clean deps\n-   godep go test -v --timeout=30m github.com/GoogleCloudPlatform/heapster/integration/... --vmodule==2\n  +test-integration: clean deps build\n-   godep go test -v --timeout=30m github.com/GoogleCloudPlatform/heapster/integration/... --vmodule==2 $(FLAGS)\n\nwhere do we fill the FLAGS? I don't see that anywhere.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/335/files#r32359189\n.\n. Essentially then, node/usage is the kernel overhead?\n. The GCM driver under sinks/gcm/ already prefixes individual metrics. Can we move this logic there?\n. stat.Memory.Usage is the entire's node's usage right? If your goal is to calculate system overhead, then it has to be node_usage - usage of all containers under '/'\n. Done\n. Good point. Done. Settled with 'host_id'.\n. You can start with the existing GcStore which is not bounded and then replace that with a Bounded one.\n. Should we keep the kubernetes versioned namespace objects around as well?\n. This is essentially a MetricsStore right?\n. Create a separate package under heapster and place all the derived logic in there. Let the 'api' package only deal with REST endpoints.\n. Why is this wrapper needed? Can NewRealCluster() be private?\n. Should the timestamp be time.Now()?\n. Is this required? Why cant the lock be a variable?\n. Ack\n. Why not return nil, stamp, fmt.Errorf(...) instead? \nAnother style nit: The pattern that we usually follow is that of handling the failure case first and then moving on with the rest of the logic.\n. +1. This needs to be part of the cluster object. Is the intention to not return the lock as well when Cluster object is returned via APIs? If so, we need an internal and an external object. How about having an internal object that wraps the external object and adds a lock?\n. Ah! Naming is hard :( Lets not overload \"resource_id\", since it is required for filesystem metrics.\nDoes \"instance id\" have to be called \"resource_id\"? Can we call it \"host_id\" instead in GCM side?\n. Can you elaborate your comment?\n. Good catch. Fixed.\n. Update Put API to take in TimePoint.\n. Validate the timestamps as well.\n. latest instead of max may be?\n. Change to GetClusterData(start, end time.Time) instead?\n. Can we remove the 'if' condition here?\n. Ack.\n. nit: Can we have consts for resource types and \"limits\" or \"usage\" or \"%tage\"?\n. Better to have sub-structure, which will help with derived stats. Store spec and stats separately in a composite struct.\n. How about a generic resource struct { usage int, limit int } and have one such struct for every resource type? Essentially we will have a static struct with all the metrics defined. Ok to tackle in a future PR.\n. All your points are valid! One option is to have a map of resource type to\na timeseries of resource metrics (spec + stats). This is closer to the\nkubernetes model.\nAs long as this is not what we expose via APIs, I am totally fine deferring\nthe refactoring to a later stage.\n\nOn Wed, Jun 24, 2015 at 3:35 PM, Alexandros Mavrogiannis \nnotifications@github.com wrote:\n\nIn schema/types.go\nhttps://github.com/GoogleCloudPlatform/heapster/pull/365#discussion_r33204976\n:\n\n@@ -77,3 +78,12 @@ type PodInfo struct {\n type ContainerInfo struct {\n    InfoType\n }\n+\n+// Supported metric names, used as keys for all map[string]*store.TimeStore\n+const cpuLimit = \"cpu/limit\"\n\nThat is the eventual goal, however there are a few issues to consider,\nsuch as:\n- time resolution of limits vs time resolution of usage.\n- mismatch of limit/usage timestamps.\n- input resolution versus output resolution for historical data.\n- instantaneous derived stats (e.g. current utilization percentage)\n  and the required conversions of limits/usage values.\nFor those reasons, I believe it's better if we address the format of the\nresource struct together with the design of the derived stats.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/365/files#r33204976\n.\n. You don't need a pointer to an interface in go.\n. As of now, heapster expects an auth file at defaultKubeConfigFile path to exist. Is that required moving forward with service accounts? Can we make auth an optional item - user cluster without auth, can run vanilla heapster without any issues?\n. My understanding is that service accounts are created by default on all clusters. If thats true, I think breaking backwards compatibility is fine, since the new model will work off the box.\nEssentially I am suggesting getting rid of the auth config completely, since service accounts do the right thing.\n. This should be 8082 I think. \n. Can you add an example for dynamically adding a sink?\n. Yes please. \n. +1\n. Good catch @mvdan \n. Good eyes again. Fixed. PTAL\n. The entire file was auto-indented. Do you want me to alter the indentation?\n. @dchen1107 can you clarify your comments?\n. Good point! Manually controlling gc duration makes sense. Updated code. \n. No. Fuzz might not be generating free containers. So its flaky as of now.\n. The unit tests are supposed to cover the corner cases. I intend to add an e2e test for this. But with the time crunch, that might not happen. \n. This will reduce the memory usage. I'd recommend taking this in. WDYT?\n. Users have reported that memory usage being a concern. This is an easy way to mitigate that. Moreover we don't have to store stats for more than 3 minutes in reality, assuming we scrape it faster than that. \n. What percentile?\n. what is the 20 minutes value for?\n. nit: lock can be embedded into rc.\n. Why are all these values the same?\n. Make it explicit that you are providing 95%ile in the field name.\n. nit: This can be embedded.\n. Why not just the last sample here? Reading all the data back seems wasteful. It might also lead of memory bursts.\n. Can we place a symlink to an existing service file instead of creating a new one? \n. Does this have to be maintained here or can we reuse an existing reimann docker image?\n. Is annotations a TODO here?\n. How about \"Container base image\" instead?\n. Can you add this in sinks/api/v1/decoder.go as well? The code is in a state of flux. Thats the reason for duplication.\n. Take a look at this library: https://github.com/jbrukh/window\n. Does this mean you are storing in unsorted fashion?\n. Why is this method needed? Doesn't make sense on a lossy storage driver.\n. Why add numRes here? IIUC, we did not get any data points for the duration between lastPut and current timestamp?\n. nit: Add a todo to consider garbage collecting outside of Put as well.\n. Why not calculate the new max?\n. Why move the maxIdx here? Can you add a small example as comments here?\n. Isn't it a day?\n. nit: Suggest Returns in favor of Displays\n. Can we define a top level resources supported and then just use that resources keyword everywhere else?\n. For now this will be just 'cpu' and 'memory'\n. What if the user wants all the metrics in a namespace?\n. I assume this is per resource. Mention that.\n. Ah! Got it.\n. Yeah. You are stating that in every API. If we add another resource, it requires updating all the API  docs. Instead if we declare something like 'Supported Resources' and refer to that keyword in the rest of the doc, it might be easy to maintain in the long run. WDYT?\n. Ok.\n. \"/api/v1/model/namespaces/{namespace-name}/stats/\": Exposes the average, max and 95th percentile over the past minute, hour and day for each namespace metric.\n\nThere is no reference to the specific metrics like CPU and Memory here.\n. One more option - /namespaces/{namespace-name}/pods can return metrics for all pods in that namespace. The client can them perform the summation. Heapster can then focus only in distributions for all logical entities. WDYT?\n. If we go with the option I proposed, we should do the same for /nodes as well, but thats for a separate PR.\n. @afein: Care to send a PR to invoke the local sync method instead of the one that spawns a go thread?\n. I did try building and running the image. The e2e test also builds it and runs it in a cluster.\n. No problem :) Updated to 8 as suggested. \n. @mwielgus: I chatted with @afein offline and that discussion is captured here #476. The current API format is not well suited for adding more use cases. This PR is a classic example of that. \nEssentially if we had a /api/v1.1/stats/namespaces/<namespace>/pods to begin with which returned all the stats for pods under the given namespace, that should satisfy your use case, I hope. In the future, we can add support for label selectors as query params, which should help result in querying only a subset of pods. \nIn general, +1 for doing client side summation for now. Once we understand the use cases more, we can consider moving operations to the server side. WDYT?\n. +1 for returning a list of timepoints and performing summation at client side. \n. @mvdan: The use of interfaces in our use case, will result in copying pointers only. If we store structures, we will end up copying structures between regions of heap. WDYT?\n. +1 for switching over to our own impl if we have issues with this.\n. Why is that so? The max should be inclusive of the maximum during the entire day right?\n. Why?\n. nit: Can we do this in a loop?\n. +1 for rewriting if there is a performance issue. \n. SUggestion: How about storing pointers as floats? Will that help us avoid having to fork this package?\n. Storing a pointer to hourEntry as a float64 :)\nOn Mon, Aug 17, 2015 at 3:39 PM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nIn third_party/window/window.go\nhttps://github.com/kubernetes/heapster/pull/471#discussion_r37245593:\n\n+package store\n+\n+// An array-based moving window; a moving window\n+// is a queue with a maximum size and the property\n+// that when the size is reached, pushing a new\n+// element into the queue causes the head to be\n+// popped.\n+//\n+// You can optimize the amount of slice copying\n+// that the MovingWindow will be doing by trading\n+// off with space complexity. Namely, the underlying\n+// array is allocated with a size that is the\n+// multiple of the intended capacity of the queue\n+// so that copying is less frequent.\n+type MovingWindow struct {\n-   arr  []interface{}\n\n@vishh https://github.com/vishh what do you mean? Storing float64\ninstead of hourEntry?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/471/files#r37245593.\n. Does this Getter copy the entire model?\n. Ah! Good point @mvdan.\n\nOn Mon, Aug 17, 2015 at 3:44 PM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nIn third_party/window/window.go\nhttps://github.com/kubernetes/heapster/pull/471#discussion_r37246106:\n\n+package store\n+\n+// An array-based moving window; a moving window\n+// is a queue with a maximum size and the property\n+// that when the size is reached, pushing a new\n+// element into the queue causes the head to be\n+// popped.\n+//\n+// You can optimize the amount of slice copying\n+// that the MovingWindow will be doing by trading\n+// off with space complexity. Namely, the underlying\n+// array is allocated with a size that is the\n+// multiple of the intended capacity of the queue\n+// so that copying is less frequent.\n+type MovingWindow struct {\n-   arr  []interface{}\n\nI don't see why that would help. I don't think Go would allow it either,\nunless you use the unsafe package. You can't cast stuff around in Go like\nyou would with void* in C.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/471/files#r37246106.\n. Why is this required if the model is turned on by default?\n. Do you have a follow up PR for epsilon? 100 defeats the whole effort.\n. Ideally, only nodes and namespaces must be accessible from cluster. Those objects should then expose the pods that are under them. I am ok with this flat interface for now, but add a TODO.\n. ok. Ping me once you have updated.\n. Getting points from the hour store will be an expensive operation since the store does not return stats for custom time durations. I am concerned about spikes that we will miss if we don't do it though. I don't see why we will calculate percentile for more than a day. My understanding if that the day store will hold 23 hourly stats and the 24th sample will be derived from the hourly stats store.\n. Thanks for the explanation @afein. Existing implementation is fine for now. Let's document the limitation though. In the future, if it affects users, we can improve it. \n. Why is this necessary?\n. nit: rename this to api_v1.\n. Hmm. Thats wierd! \n. This is useful when compiling on OSX. If we switch to docker based builds, we can get rid of this safely.\n. by cast it to uint16 instead of uint32?\n. I assume this logic still requires input in a chronological order. Make sure you document that.\n. Is the initial check required?\n. Why not return both timepoint and max?\n. uint should be OK here. \n. If we maintain a count, we would not have this issue right?\n. We currently use stats_resolution to down sample stats. Have you looked through the rest of the code to remove the downsampling, since we are choosing to poll more frequently?\n. This name is very hard to interpret. What is this meant for?\n. Is it ok to update the model more often? Can we derive model updates based on polling durations instead?\n. What is an external entity?\n. What is the purpose for tracking earliest timestamp?\n. What are the units?\n. Why declare the test one here?\n. What is an entity?\n. How did you arrive at the 25 seconds delta? The model resolution of 1 minute is OK. I am more interested in making the updates happen more frequently to ensure that we can serve more recent data to users. \n. Got it.\n. Yes please :)\n. ok.\n. Ok. Can you add a comment about this? Not necessarily in this PR. \n. If this type is exposed over the wire, can we choose an alternative name? Entity is hard to interpret. How about Point or MetricPoint or something thats more descriptive?\n. Without resolution, we will end up handling metrics of 1 second granularity. Is this something we want? It will lead to additional memory usage?\n. Got it. I agree with everything you mentioned! I intend to make the new stats APIs in kubelet support resolution. One option we have is to query more often, but ask for only the most recent stat. WDYT of that option?\nWith this change, my understanding is that heapster will drop all but one stat entry from every query. Have you tested that?\n. I guess the underlying issue here is that of combining metrics storage and aggregation. We can store container and node metrics at a much higher granularity. But we do not want to aggregate at higher granularities. Update() needs to be broken down. Can you file an issue?\n. How about \n\ngo\ntype LatestMetric struct {\n     Kind string // oneof: Node, Pod, Namespace, Container\n     Name string\n     CPU uint64 // millicores\n     Memory uint64 // bytes\n}\n. Why is this required? Why don't we make the default config work for kubedash? The reason for not preferring another config is that it is hard to keep all the configs in sync. It makes testing hard as well.\n. Ditto as above. Why is this needed?\n. Restating my comment from your previous PR: If we split update to have a separate container and node level metrics storage section and aggregation section, it might be easier to deal with these durations.\n. is it possible to post incremental changes as separate commits and then squash them at the end? Github makes it hard to track comments if commits are rebased :(\n. +1. We can get rid of cadvisor's Spec and Stats completely. The model does that.\n. We will eventually need both request and limits I guess. Why not follow kube api semantics and make requests and limits a map[resourceType]Quantity?\n. Should we clarify request vs limit? If we were to run with cadvisor in standalone mode, what will this metric be?\n. Clarify what request means since it is currently specific to just kubernetes.\n. That makes sense. We should handle both request and limit though. Values from cAdvisor will translate to limits, but that can be overridden in the case of Kubernetes.\n. Introducing internal abstractions for stats and spec should solve this issue. \n. nit: We can embed the mutex into the parent struct.\n. How about placing this in a separate file?\n. That doesn't work. Since we append and not replace entries, we end up with an array of twice the size. One option is to pass in a reference to individual elements and overwrite them. WDYT? Is it worth it?\n. Commented the logic for not doing that below.\n. I meant \ngo\ntype realAuthTokenProvider struct {\n    sync.RWMutex\n    ...\n}\nSorry if my previous comment was confusing!\n. This can fail if the DB is not up yet. I am trying to create the DB here just as an optimization.\n. Good catch. Done\n. Done\n. Yes. Removed\n. Done\n. Do we want to verify that all the expected resources are present in the response?\n. What about all the other model APIs?\n. Why do we need a separate endpoint for available? Why not display the available sub resources for /nodeMetrics/?\n. May be I misunderstood the purpose. Is this API returning all the available metrics like cpu, memory, etc? If yes, why do we need it?\n. I assume a big chunk of this function will be common to other type of metrics as well. Why make it local to node metrics?\n. json tag for this?\n. nit: Let's stick to camelCase starting with lower case for json tags.\n. Are the meta objects required from heapster perspective? Since it provides read-only APIs I don't see a reason. I could be wrong.\n. We should specify that experimental means it will be deleted anytime. Also why not use experimental/v0.1 ?\n. Yeah. That makes sense.\nOn Tue, Sep 15, 2015 at 1:59 PM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nIn integration/heapster_api_test.go\nhttps://github.com/kubernetes/heapster/pull/558#discussion_r39564367:\n\n@@ -190,6 +207,31 @@ func getSchema(fm kubeFramework, svc _kube_api.Service) (_api_v1.TimeseriesSchem\n    return &timeseriesSchema, nil\n }\n+func getModelMetrics(fm kubeFramework, svc _kube_api.Service, pod *kube_api.Pod) (_api_v1.MetricResultList, error) {\n-   url := fmt.Sprintf(\"/api/v1/model/namespaces/%s/pod-list/%s/metrics/%s\",\n\nMaybe the issue should be closed once all endpoints are tested. Is that\nwhat you mean?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/558/files#r39564367.\n. Would unit-like tests validate the restful layer as well? If not, at-least\npoking those endpoints to make sure they work will be useful.\n\nOn Tue, Sep 15, 2015 at 2:42 PM, Marcin Wielgus notifications@github.com\nwrote:\n\nIn integration/heapster_api_test.go\nhttps://github.com/kubernetes/heapster/pull/558#discussion_r39569407:\n\n@@ -190,6 +207,31 @@ func getSchema(fm kubeFramework, svc _kube_api.Service) (_api_v1.TimeseriesSchem\n    return &timeseriesSchema, nil\n }\n+func getModelMetrics(fm kubeFramework, svc _kube_api.Service, pod *kube_api.Pod) (_api_v1.MetricResultList, error) {\n-   url := fmt.Sprintf(\"/api/v1/model/namespaces/%s/pod-list/%s/metrics/%s\",\n\nI would rather check the other endpoints in a more mocked and unit-like\ntest for api/v1/model_handler.go where I can verify if the returned\nvalues are correct.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/558/files#r39569407.\n. Fair point! I see value for all of them and in terms of priority, the\nbounding of methods might be the final thing we want to test, with the\nassumption that code reviews would have caught that.\n\nOn Tue, Sep 15, 2015 at 3:11 PM, Marcin Wielgus notifications@github.com\nwrote:\n\nIn integration/heapster_api_test.go\nhttps://github.com/kubernetes/heapster/pull/558#discussion_r39572424:\n\n@@ -190,6 +207,31 @@ func getSchema(fm kubeFramework, svc _kube_api.Service) (_api_v1.TimeseriesSchem\n    return &timeseriesSchema, nil\n }\n+func getModelMetrics(fm kubeFramework, svc _kube_api.Service, pod *kube_api.Pod) (_api_v1.MetricResultList, error) {\n-   url := fmt.Sprintf(\"/api/v1/model/namespaces/%s/pod-list/%s/metrics/%s\",\n\nWith the E2E approach as here i can only check if the output is returned\nat all and if it is sane, not if the endpoints are correctly bound to model\nfuncs.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/558/files#r39572424.\n. I am perfectly fine with adding checks for the other endpoints in a future\nPR. I just wanted to bring that up.\n\nOn Tue, Sep 15, 2015 at 3:19 PM, Vishnu Kannan vishnuk@google.com wrote:\n\nFair point! I see value for all of them and in terms of priority, the\nbounding of methods might be the final thing we want to test, with the\nassumption that code reviews would have caught that.\nOn Tue, Sep 15, 2015 at 3:11 PM, Marcin Wielgus notifications@github.com\nwrote:\n\nIn integration/heapster_api_test.go\nhttps://github.com/kubernetes/heapster/pull/558#discussion_r39572424:\n\n@@ -190,6 +207,31 @@ func getSchema(fm kubeFramework, svc _kube_api.Service) (_api_v1.TimeseriesSchem\n   return &timeseriesSchema, nil\n }\n+func getModelMetrics(fm kubeFramework, svc _kube_api.Service, pod *kube_api.Pod) (_api_v1.MetricResultList, error) {\n-  url := fmt.Sprintf(\"/api/v1/model/namespaces/%s/pod-list/%s/metrics/%s\",\n\nWith the E2E approach as here i can only check if the output is returned\nat all and if it is sane, not if the endpoints are correctly bound to model\nfuncs.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/558/files#r39572424.\n. ok\n\n\nOn Tue, Sep 15, 2015 at 1:25 PM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nIn expapi/v2/metrics_handlers.go\nhttps://github.com/kubernetes/heapster/pull/549#discussion_r39560080:\n\n\nmodel_api \"k8s.io/heapster/model\"\n\"k8s.io/heapster/store/statstore\"\n\"k8s.io/kubernetes/pkg/api/resource\"\n  +)\n  +\n  +// errModelNotActivated is the error that is returned when manager.cluster\n  +// has not beed initialized.\n  +var errModelNotActivated = errors.New(\"the model is not activated\")\n  +\n  +// RegisterMetrics registers the Metrics API endpoints.\n  +// All endpoints that end with a {metric-name} also receive a start time query parameter.\n  +// The start and end times should be specified as a string, formatted according to RFC 3339.\n  +func (a Api) RegisterMetrics(container restful.Container) {\nws := new(restful.WebService)\nws.\nPath(\"/experimental/v2\").\n\n\nIt's experimental/v2 since this may become api/v2.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/549/files#r39560080.\n. This change might cause timeouts in the go test framework. We will have to update the timeout in the makefiles.\n. Fair point. Lets go with this for now.\n. Hmm. gofmt doesn't seem to change it. The unit tests will fail in case it fails. \n. If InfluxDB is not up before heapster starts, db creation will fail in Register. To avoid the race condition and to avoid db being accidentally deleted while heapster is running, heapster tries to re-create db is required.\n. Sure.\n. I could not compile any of the code because of missing godeps. If this PR looks good in general I can add godeps again.\n. Good catch.\n. Registration is done synchronously as part of startup. Even if we\nsuccessfully create db as part of startup, what if influxDB crashed and\ncame back up? The previously created DB would no longer exist. These\nchanges make heapster resilient to DB failures.\n\nOn Wed, Sep 16, 2015 at 3:19 PM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nIn sinks/influxdb/driver.go\nhttps://github.com/kubernetes/heapster/pull/548#discussion_r39692705:\n\n}\n// Stores events into the backend.\n func (sink influxdbSink) StoreEvents(events []kube_api.Event) error {\n-   dataPoints := []influxdb.Series{}\n-   if err := sink.createDatabase(); err != nil {\n\nI would expect heapster to refuse to use a sink if its registration fails,\nor to retry a few times.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/548/files#r39692705.\n. Done.\n. Do what? I don't think users want to remove DBs ever. They will lose the data. \n. Thanks for the tip! Updated!\n. nit: We should have explicit names for the return arguments or at-least document their meaning. \n. Also the returns types are essentially the same. Is it worth unifying them into a single struct and re-use in all methods?\n. cc @mvdan \n. Can we avoid adding a separate controller? I'm trying to get rid of the existing ones and document using the remote API for enabling sinks. For now, just documenting the reimann sink should work I think. \n. Same as above\n. Got it! Thanks for explaining!\n. Does reimann have a web UI that needs to be exposes outside of the cluster?\n. Can this be private?\n. Updated with tentative timelines.\n. Done\n. Ok. Shall we ship the UI by default in the same Pod? It will definitely help users. \nAlso, can we route data to InfluxDB via Reimann or is it recommended to directly store in InfluxDB (or some other timeseries store)?\n. nit: s/source/sources.\nHighlight the options standalone and coreos.\n. nit: One sentence per line. It helps with readability.\n. Can we somehow autodetect this? For example, if one of the sources is kubernetes, we can use that as an hint. Default values for flags like this affect usability. WDYT?\n. Was this intentional?\n. Done.\n. Nit: Remove trailing white space here and in other places.\n. Ah! Got it!\n. automated.\n\nOn Tue, Sep 22, 2015 at 2:46 PM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nIn docs/Grafana Dashboard.json\nhttps://github.com/kubernetes/heapster/pull/597#discussion_r40148781:\n\n@@ -145,8 +116,9 @@\n                 }\n               ],\n               \"groupByTags\": [],\n-              \"hide\": false,\n\nI can't help but notice that some changes are simply moving lines around,\nlike hide and interval. Are these changes automated or manual?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/597/files#r40148781.\n. What is the markdown syntax here?\n. Agreed. Any thoughts on the UI?\n. Ah! Ok. Thanks for explaining. In that case just some pointers to existing\ndocumentation should do.\n\nOn Tue, Sep 22, 2015 at 4:00 PM, Jack Foy notifications@github.com wrote:\n\nIn deploy/kube-config/riemann/riemann-service.json\nhttps://github.com/kubernetes/heapster/pull/379#discussion_r40155265:\n\n@@ -0,0 +1,36 @@\n+{\n-  \"apiVersion\": \"v1beta3\",\n-  \"kind\": \"Service\",\n-  \"metadata\": {\n-    \"labels\": {\n-      \"kubernetes.io/cluster-service\": \"true\",\n-      \"name\": \"riemann-heapster\"\n-    },\n-    \"name\": \"riemann-heapster\"\n-  },\n-  \"spec\": {\n\nIt's a good idea, but I'm tempted to defer. We have run with riemann-dash\nin the past, and we're not currently doing so; it's a useful tool for\nexploration and certain visualizations. In practice, we stream our metrics\nfrom Riemann to Grafana dashboards.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/379/files#r40155265.\n. Yes. That is the current state.\n\nOn Thu, Sep 24, 2015 at 12:18 PM, Jimmi Dyson notifications@github.com\nwrote:\n\nIn docs/standalone.md\nhttps://github.com/kubernetes/heapster/pull/607#discussion_r40360524:\n\nIf you got debug information as before, means that the heapster is working normally.You can test it by calling its RESTful API.\n You can use some restful tools to test, or just run a curl command ( heapster implements gzip encoding, so we need add gunzip in command \"curl\").\n-shell\n-curl http://0.0.0.0:8082/api/v1/model/stats/ | gunzip\n-\n-   curl http://0.0.0.0:8082/api/v1/model/stats/ | gunzip\n\nSo does heapster compress every response, even if the client hasn't\nrequested compression?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/607/files#r40360524.\n. Yeah. The response size was the concern. It makes sense to let clients\nhandshake though.\n\nOn Thu, Sep 24, 2015 at 12:49 PM, Jimmi Dyson notifications@github.com\nwrote:\n\nIn docs/standalone.md\nhttps://github.com/kubernetes/heapster/pull/607#discussion_r40363926:\n\nIf you got debug information as before, means that the heapster is working normally.You can test it by calling its RESTful API.\n You can use some restful tools to test, or just run a curl command ( heapster implements gzip encoding, so we need add gunzip in command \"curl\").\n-shell\n-curl http://0.0.0.0:8082/api/v1/model/stats/ | gunzip\n-\n-   curl http://0.0.0.0:8082/api/v1/model/stats/ | gunzip\n\nIs that just due to the potential response size? We could just let\ngo-restful handle the content encoding negotiation & let the client choose?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/607/files#r40363926.\n. Just curious: Does kubernetes also import this package?\n. Can we move all the grafana image related files to a top-level grafana directory similar to how InfluxDB is managed?\n. nit: You can drop the three \"``\" in favor of a tab.\n. Can you explain why this is necessary? Kubernetes supports DNS names and so Grafana should be able to connect to pre-defined service namemonitoring-influxdb` and expect to have InfluxDB answering its queries right?\n. Why do we need an external IP? Grafana 2.0 is capable of proxying requests to InfluxDB. So we can use a cluster local IP to talk to InfluxDB.\n. I will set up a nightly build for heapster soon. Until then, can you revert this change? \nI will also make a heapster v0.19.0 release soon, that you can use with influxDB v0.9.4.\n. Once the review is complete, I can push the image as per your suggestion.\n. Why use the endpoint directly? Eventually, if we run an InfluxDB cluster, you'd want to loadbalance across all the instances right?\n. Thanks @jimmidyson :100: \nUpdated docs.\n. I'm wondering if this is necessary. If we default to using the proxy mode,\nusers can easily switch to direct access via the UI. Even without the\nloadbalancer discovery part, this PR makes the default user experience\nfantastic.\nConsider the scenario where instead of loadbalancer, K8s users end up using\nnode ports. Are we going to extend support for that as well?\nI'm just a bit weary of maintaining more logic.\n\nOn Fri, Sep 25, 2015 at 12:37 PM, Thuc Nguyen notifications@github.com\nwrote:\n\nIn grafana/influxdb_service_discovery.go\nhttps://github.com/kubernetes/heapster/pull/599#discussion_r40467732:\n\n@@ -0,0 +1,76 @@\n+// Copyright 2014 Google Inc. All Rights Reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n\n@vishh https://github.com/vishh you asked a question about why this was\nneeded. Grafana supports both proxy and direct methods to access InfluxDB\nbackend. This is mainly for deployments where InfluxDB is exposed as an\nexternal service and can be accessed directly from the browser. This code\nlooks up the external service URL for InfluxDB and returns it if one is\navailable.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/599/files#r40467732.\n. Awesome!\n. While you are here, should we symlink the service definitions instead of duplicating them?\n. +1\n\nOn Mon, Sep 28, 2015 at 11:07 AM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nIn deploy/kube-config/google/heapster-service.json\nhttps://github.com/kubernetes/heapster/pull/614#discussion_r40584624:\n\n@@ -1,23 +0,0 @@\n-{\n\nWhat about having it as deploy/kube-config/heapster-service.json, if it\napplies to all different configs?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/614/files#r40584624.\n. Why retain this file?\n. nit: Update this line based on recent changes.\n. Why mention heapster version here?\n. I guess you missed this. Its fine. I will replace this, after making a release.\n. nit: Can we name it heapster_grafana just to be clear?\n. What is the output of this endpoint? \n. This should be a different release version. May be 2.1.1?\n. Just noticed this @piosz. The last time I tested, I did not see any duplicate stats. We can add a Vlog here to validate my claim.\n. go get will not work because of godep\n. fixed it.\n. One issue is that we use the release version as docker image labels. More characters will make the label complex. \n. Yeah\n. Good catch. fixed.\n. Makes sense. Updated.\n. https://github.com/kubernetes/heapster/issues/642#issuecomment-146935609\n. We rely on cAdvisor's predictable performance as of now. \nTo be on the safe side, increasing the window a bit SGTM.\nAlso - will 1 sec window work with metrics compaction/deduplication strategy in cAdvisor?\n\nI don't understand this question. Can you elaborate?\n. I also think the right solution here will be for cAdvisor or kubelet to support resolution.\n. You are exposing an internal data structure TimeSeries over the wire here. If we update the data structure in the future, this will break all client of this sink. That is not desirable. \nWhat is the minimum amount of data that you need? Are there any standard formats for exposing metrics and events kind of data through kafka?\n. Do you want to retry here? In a typical cluster, components can go down anytime. If you were to keep re-trying periodically to connect to kafka hoping for it to come up, that might provide better user experience. WDYT?\n. Yeah. If the client doesn't attempt to connect to kafka brokers here, that is fine. If it attempts to establish connectivity to brokers, chances are it will fail most of the time, if kafka is also running as a cluster service.\n. You can take a look at the influxdb sink which retries instead of failing hard.\n. SGTM. You can create a versioned structure in this sink.\nOn Mon, Oct 26, 2015 at 4:32 AM, yuqi huang notifications@github.com\nwrote:\n\nIn sinks/kafka/driver.go\nhttps://github.com/kubernetes/heapster/pull/648#discussion_r42982284:\n\n\nreturn fmt.Errorf(\"failed to produce Kafka messages: %s\", err)\n}\n}\nreturn nil\n  +}\n  +\n  +// produceKafkaMessage produces messages to kafka\n  +func (self *kafkaSink) produceKafkaMessage(v interface{}, topic string) error {\nif v == nil {\nreturn nil\n}\njsonItems, err := json.Marshal(v)\nif err != nil {\nreturn fmt.Errorf(\"failed to transform the items to json : %s\", err)\n}\nmessage := &proto.Message{Value: []byte(string(jsonItems))}\n\n\nThanks for your comments. @vishh https://github.com/vishh .\nI need all the ops information, includes metrics and events.\nExposing an internal data structure TimeSeries over the wire is not well\nadvised. I will pick out the effective information about metrics and events\nto give the kafka sink.Include: timestamp, value of metrics and events.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/648/files#r42982284.\n. If the brokers do not respond within the timeout, will the sink setup fail? I'd expect the sink to continue setup, and keep retrying to establish connection. From the code, that is not clear to me. Can you document the behavior using comments?\n. Was this intentional?\n. nit: I don't think we want to spam log files with info logs.\n. nit: Rephrase as Options can be set in ...\n. Why did you remove the defaults?\n. What will happen if the tsdb server isn't available at this stage?\n. Can you use the [ClientInitializer](https://github.com/kubernetes/heapster/blob/master/sinks/util/client.go#L28) interface to abstract out this Ping? \nSince this driver does not require any special setup, you can pass func() {return nil} as initializer and use this tsdbSink.client.Ping method as ping argument.\nExample usage here.\n. Really? Is auth supported by opentsdb?\n. nit: why not events instead?\n. Thanks for clarifying @bluebreezecf \n. Can we create a new version of storage schema instead of deprecating labels in the existing version?\nIt will be much easier for users to understand. WDYT?\n. This statement is vague. We as kube developers haven't put enough energy into making kubedash primetime ready. Why throw away a good piece of software, when no alternative exists? \n. Disk will be required before net stats..\n\nOn Tue, Dec 8, 2015 at 12:49 PM, Marcin Wielgus notifications@github.com\nwrote:\n\nIn docs/proposals/vision.md\nhttps://github.com/kubernetes/heapster/pull/769#discussion_r47011414:\n\n+OpenTSDB, Google Cloud Monitoring, Hawkular, Kaflka, Riemann, ElasticSearch (some of them are\n+not yet submitted).\n+\n+In addition to gathering metrics  Heapster is responsible for handling Kubernetes events - it\n+reads them from Kubernetes API server and writes them, without extra processing, to a selection\n+of persistent storages: Google Cloud Logging, Influxdb, Kafka, OpenTSDB, Hawkular,\n+ElasticSearch, etc.\n+\n+There is/was a plan to add resource prediction components (Initial Resources, Vertical\n+Pod Autoscaling) to Heapster binary.\n+\n+## Separation of Use Cases\n+From the current state description (see above) the following use cases can be extracted:\n+\n+* [UC1] Read metrics from nodes and write them to an external storage.\n+* [UC2] Expose metrics from the last 2-3 minutes (for HPA and GKE)\n\nAll data is written to some (hopefully) permanent storage. If you need\nthem you should query the storage directly. AFAIK there is no current need\nfor a bigger window for ALL metrics in Heapster, UI needs cpu and memory\nonly (and maybe some net stats in the future).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/769/files#r47011414.\n. Let's just say InfluxDB v0.9. That will let us rev up InfluxDB without a heapster release.\n. BTW the schema hasn't changed. The way columns are stored was changed. \n. last two minutes as of now to be specific.\n. Shouldn't this cherry-pick be a separate minor release :) ?\n. Should we just return nil, err here instead?\n. Can we include all the configs in a single file?\n. What's special about this image? Is there an existing image that we can use?\n. How is alerting possible?\n. Can you add a link to an issue that tracks the replacement? \nWe should place that issue in the kubedash repo as well to make it clear for the existing kubedash users.\n. The original plan was to combine events and metrics data and build more interesting signals for end users. I guess even if we split heapster into separate binaries, if we want to build such models, we will have to combine the data somewhere else.\n. Currently, max, avg and 95%ile is made available for last minute, hour and day. \n. Would it be possible to let users decide what metrics they want to retain? Essentially a whitelist? That will require standardizing metric names though, which will not work for non system metrics well.\n. Although this is true today, what if we wanna combine events with metrics? Is the suggestion to aggregate the same data in a separate service?\n. One other use case has been that of letting heapster act as a source for events instead of using etcd. \ncc @smarterclayton \n. So does kubedash. If heapster API can hide oldtimer and gracefully reduce the amount of available data, then I guess oldtime is only an implementation detail.\n. https://github.com/kubernetes/kubernetes/issues/18770 proposes using cAdvisor directly for metrics.\nSo I don't see why we want to get rid of cadvisor support.\n. Instead, can we define what the requirements are for submitting a sink to heapster? Kafka for example is also a popular deployment option for users. \n. Why not? What if custom metrics are required for the GKE pipeline in the near future?\n. A lot of the information here is still being debated in various issues and PRs. I'd suggest adding a TBD for this section.\n. Will OldTimer be abstracted using Heapster APIs?\n. Storage backends have the option of downsampling data as well. \n. nit: Add package level comments that describe what this package is supposed to do.\n. Backwards compatibility could be opt-in. If a user cares about kubedash, they can then pay the extra resource overhead. I'd prefer calling that out explicitly in the doc though to be clear.\n. If we assume that none of those custom-metrics are cached, do we still expect to have a huge resource impact to proxy metrics over to a sink?\n@jimmidyson: What you suggest is an alternative for sure. Excepting a simple aggregation, I'm not suggesting any additional features. Prometheus is a monitoring system by itself, whereas heapster is only an aggregation agent.\n. So is the answer \"no\", there will not be a common API in the future, or, \"maybe\", there can potentially be a common API through heapster?\n. Do we need network metrics in the model or is it necessary only for monitoring purposes?\nThe cost for adding metrics to the model is kindda high for now.\n. I think we want to collect the system containers like /kubelet, /docker-daemon, etc which should have been the existing behavior.\n. Yeah. Is it required to be exposed via APIs as well?\n. I'd personally prefer exposing this and filesystem stats as well. Its just that the default resource limits in Kube has to change once these metrics are added. So as long as we can go fix the limits, then adding these metrics is OK by me :)\n. SGTM. So is monitoring the primary use case?\n. Ideally, the pod infra container should be hidden inside heapster. That way if we collect metrics from rocket for example, which does not need an infra container, we will not break users of heapster.\n. The reason is that these labels are not intended to be exposed :)\n. We need stats at the pod level in addition to container level.\n. Dont change go version\n. Godep changes look a bit wonky. I'd suggest not updating any deps that this PR does not depend on.\n. What about available? Will these metrics be serialized and pushed to sinks automatically?\n. By sinks I meant the storage backends. Will these metrics be stored into\nInfluxDB automatically or does it need a schema change somewhere?\nOne more want is to expose these new metrics via the GKE specific REST\nendpoint. It can be done in a separate PR though to make it easy to review.\n\nOn Tue, Feb 2, 2016 at 3:41 PM Tim St. Clair notifications@github.com\nwrote:\n\nIn metrics/sources/kubelet/summary.go\nhttps://github.com/kubernetes/heapster/pull/927#discussion_r51656180:\n\n\nreturn\n}\n  +\nfsLabels := map[string]string{LabelResourceID.Key: fsKey}\nif fs.UsedBytes != nil {\nusage := LabeledMetric{\nName:       MetricFilesystemUsage.Name,\nLabels:     fsLabels,\nValueType:  ValueInt64,\nMetricType: MetricFilesystemUsage.Type,\nIntValue:   int64(*fs.UsedBytes),\n}\nmetrics.LabeledMetrics = append(metrics.LabeledMetrics, usage)\n}\n  +\nif fs.CapacityBytes != nil {\n\n\nIt looks like we need to add a new metric for available. I'm not sure I\nunderstand your second question?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/927/files#r51656180.\n. Also should we update the metrics list here to include available?\n. Will rate or delta metrics be silently dropped?\n. Given that the value is float64 wouldn't be always less than MaxInt64?\n. Should we instead round up if frac is greater than 0.5?\n. If float values are supported, why are we converting to an integer in the first place?\n. Should the key include the keyword vol in there? \n. Are we planning to maintain a schema of some sort for all the metrics supported similar to this\n. My bad. I overlooked the float32() cast. This logic LGTM\n. :+1: \n. Some of the storage backends use the metric type as a hint. Google Cloud Monitoring on the other hand does recognize Delta as an explicit type of metric\n. Isn't 1.0-epsilon a number that is tending to 1? Can you explain why it is logically equivalent?\n. BTW, GCM supports double values. Should we avoid limiting precision in heapster core and instead let the storage backends deal with the precision?\nThat way we can store doubles here.\n. Acknowledged.\n\nOn Wed, Feb 3, 2016 at 4:57 PM, Tim St. Clair notifications@github.com\nwrote:\n\nIn metrics/sources/kubelet/summary.go\nhttps://github.com/kubernetes/heapster/pull/927#discussion_r51814713:\n\n\nreturn\n}\n  +\nfsLabels := map[string]string{LabelResourceID.Key: fsKey}\nif fs.UsedBytes != nil {\nusage := LabeledMetric{\nName:       MetricFilesystemUsage.Name,\nLabels:     fsLabels,\nValueType:  ValueInt64,\nMetricType: MetricFilesystemUsage.Type,\nIntValue:   int64(*fs.UsedBytes),\n}\nmetrics.LabeledMetrics = append(metrics.LabeledMetrics, usage)\n}\n  +\nif fs.CapacityBytes != nil {\n\n\nDone. Re: schema - It looks like that schema is out of date (it's missing\na handful of metrics). I can update it in a separate PR, but I think it\nwould ideally be generated from the metrics definitions.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/927/files#r51814713.\n. Really? IN its current state, it should run the e2e tests against an existing cluster.\n. nit: Why not just pod? \n. What about flow control, resource isolation across sources?\n. nit: do retain a maintainer to track the author. . \n",
    "lavalamp": "Wish granted, I think we removed that yesterday or maybe it was last week. :)\n. What @nikhiljindal says is correct, but you need to make a dedicated GOPATH first, or you risk cross-contamination.\n. OK, I think I may have figured it out, let's see...\n. OK, this time I updated only the bare minimum. Let's see if this one works. (it passes tests this time...)\n. Looks like this one is going to pass. Although jenkins can't run the boilerplate checker for some reason?\n. hm, integration test failed... @vmarmol do you know why?\n. I'll run integration locally and see if that works?\n. I think the integration test is broken because we moved addons into the kube-system namespace.\n. Running again with kube-system namespace.\n. Fixing a bug in the integration test...\n. Location looks fine to me. I can't comment as to whether the contents are right or not :)\n. > So that eventually the authorization stuff in Heapster should be the same as in Kubernetes.\nRight, we want to expose the two behind a single access point, so they have to have compatible security models.\n\nBasically, the goal is to not have to rewrite a bunch of the push metrics code when we implement a more robust authorization system, but to also not have to make having the more robust system implemented a requirement to merging push metrics.\n\nThis seems like a reasonable goal, however if the timeframe is short, is it necessary? Why not just start out with the kube auth concepts?\n. I think that apiserver right now exposes endpoints for you to check auth-- I would go that way.\nAlso, if you can't express your auth concepts in the system then we probably need to tweak the system and we should learn that asap.\n. What about using a key containing \"working set\" so this is clear?\n. ",
    "doublerr": "It's because the scripts link grafana to other services by querying the metadata service http://metdata/... that is only available on GCE.\n. ",
    "akramh": "thanks. Is there anything special about the docker container in GCE? or it\nis just the scripts that query this metadata server?\nOn Sat, Aug 30, 2014 at 4:55 PM, Vish Kannan notifications@github.com\nwrote:\n\nThe influxdb docker container is setup to work on GCE only as of now. We\nmight be able to solve this issue by moving grafana into a separate\nservice, thereby enabling discovery of InfluxDB container natively.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/3#issuecomment-53974022\n.\n. \n",
    "bmaltais": "Forget it. The pod is back in waiting state... look like those \",\" are fine afterall... something else might be causing this issue then ;-( Sorry for jumping the gun.\n. ",
    "vmarmol": "LGTM\n. LGTM, was this a recent addition to Kubernetes?\n. LGTM\n. LGTM\n. LGTM\n. Are you guys deploying your own cAdvisors in the cluster? cAdvisor is started by the Kubelet by default on that port so it should always be running.\n. @ryaneleary that is the port of the Kubelet.\n. Yes, the cAdvisor pod is placed on the manifests directory so that it is run on all nodes. The intended behavior is for it to start on all nodes on all setups. If that is not happening, its a bug and we should fix it. Are you not seeing the static manifests read and executed by the Kubelet on vagrant?\n. @ryaneleary sorry for the delay getting back to you. Is the issue still happening? If so, lets file an issue and get it fixed since it is not intended behavior :)\n. There are a couple of bugs outstanding in 0.6.2 (fixed for the now-in-alpha) 0.7.0 that don't correctly detect Docker containers in systemd systems. If you're running with 0.6.2 you're probably running into this.\n. LGTM\n. LGTM\n. Changes LGTM, will try out in a cluster now.\n. Finally got around to testing this. I works as expected. The only change would be to update the README with the new instructions :)\nFeel free to do that in another PR. Merge away!\n. Should we remove the integration.test file?\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Can we change the commit message to provide more detail about the change? Seems like it is cluster-destruction related.\n. LGTM, although aren't there some scripts that automatically add the user/password? Those may need changing.\n. WDYT of building the images as files and shipping those manually? Instead of depending on the registry and a specific account?\n. I've been working on something similar for cAdvisor (target is Jenkins), but I haven't quite finished. Take a look here:\nhttps://github.com/vmarmol/cadvisor/commit/20046d6d629052d809c88b234dfbcf2d063c2265\nSpecifically, docker save and docker load. That code saves a specific container into a tar file, ships it over to another machine and loads it there.\n. LGTM\n. LGTM\n. Tests failing\n. LGTM\n. Failed :( flaky?\n. LGTM\n. LGTM\n. I also think its fine to add an HTTP server and listen on it. It should be light enough that we won't notice.\n. run integration tests\n. test this please :)\n. test this please\n. Both CI and integration tests are failing.\n. Chane LGTM though\n. Just nit, otherwise LGTM\n. LGTM\n. LGTM\n. :D\n. LGTM, waiting on integration tests for merge.\n. LGTM\n. Should we just track this with issues?\n. LGTM\n. LGTM\n. LGTM\n. LGTM, although needs rebase.\n. Can we split that into a separate PR? Its's not even mentioned in the commits.\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Does this need to be rebased with the PR you sent a couple of days ago?\n. I guess not since you built the images :)\n. LGTM\n. Sink is not complete, but this is the first big part of it.\n. Ping @vishh @rjnagal I have a few followups to this one ready :)\n. Thanks for the quick review @vishh! Will do :)\n. Just some nits and questions for me.\nNext time, can we make this two PRs? I'd be way easier to review :)\nTravis is showing vet errors. and can we remove the nodes.test file that snuck in?\n. LGTM, but waiting on #88 \n. One minor note we can address in another PR. LGTM, merging.\n. Here are the first couple of metrics. More will come, but I wanted to get the flow right first.\n. Cleanup from @vishh's review on my last PR.\n. LGTM\n. LGTM, waiting on @rjnagal\n. Minor cleanups suggested by @vishh on my last PR\n. LGTM, merging now since it's only a nit.\n. ok to test\n. LGTM\n. LGTM\n. I think we can merge the last two commits. Outside of the types question, LGTM\n. With this PR we can push reliably to GCM. The next PR will add GCM as an supported storage backend :)\n. All changes made, PTAL\n. Rebased and all tests pass. Thanks @vishh!\n. LOL you sound too excited for that :)\nOn Tue, Feb 17, 2015 at 10:31 AM, Vish Kannan notifications@github.com\nwrote:\n\nYayy!! Now its time to add an integration test.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/104#issuecomment-74722726\n.\n. ok to test\n. ok to test\n. LGTM, will merge when the CI passes. Thanks for the patch @cedbossneo!\n. Looks like Jenkins is failing\n. ```\n+ godep go test -a -v '--vmodule=*=1' --timeout=30m --namespace=default --kube_versions=0.10.0 github.com/GoogleCloudPlatform/heapster/integration/...\n=== RUN TestHeapsterInfluxDBWorks\nI0217 15:02:11.331626   25980 framework.go:248] Disabled cluster monitoring\nI0217 15:02:11.332002   25980 framework.go:251] Setting up new kubernetes cluster version: 0.10.0\nI0217 15:02:11.332060   25980 framework.go:142] Bringing down any existing kube cluster\nI0217 15:08:20.974312   25980 framework.go:270] Successfully setup new kubernetes cluster version 0.10.0\nI0217 15:08:24.448101   25980 framework.go:192] Kubernetes master IP is 130.211.153.5\nI0217 15:08:24.620576   25980 framework.go:369] Cannot find Replication Controller \"monitoring-heapster-controller\". Skipping deletion - replicationController \"monitoring-heapster-controller\" not found\nI0217 15:08:24.625657   25980 framework.go:369] Cannot find Replication Controller \"monitoring-influx-grafana-controller\". Skipping deletion - replicationController \"monitoring-influx-grafana-controller\" not found\nI0217 15:08:24.630485   25980 framework.go:354] cannot find service \"monitoring-influxdb\" - service \"monitoring-influxdb\" not found\nI0217 15:08:24.774790   25980 kube_test.go:166] waiting for pods to be running\nI0217 15:09:09.901397   25980 kube_test.go:84] 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0]\n--- FAIL: TestHeapsterInfluxDBWorks (418.57 seconds)\n    assertions.go:154: \n\nLocation:   kube_test.go:205\n\nError:      No error is expected but got 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0]\n\nFAIL\nexit status 1\nFAIL    github.com/GoogleCloudPlatform/heapster/integration 418.578s\n```\n. Sadly I think it is more subtle than that from my restart of the build before. But maybe it likes you more so we'll see :)\n. Oh wow Jenkins fine, work for @vishh and not for me. I don't know what to think of this betrayal Jenkins.\n. LGTM\n. LGTM, waiting on CIs\n. Jenkins failed:\n+ deploy/build-test.sh\n~/workspace/project/src/github.com/GoogleCloudPlatform/heapster/deploy ~/workspace/project/src/github.com/GoogleCloudPlatform/heapster\n../Godeps/_workspace/src/github.com/GoogleCloudPlatform/kubernetes/pkg/api/context.go:22:2: cannot find package \"github.com/GoogleCloudPlatform/kubernetes/pkg/auth/user\" in any of:\n    /var/lib/jenkins/tools/org.jenkinsci.plugins.golang.GolangInstallation/go1.3.3/src/pkg/github.com/GoogleCloudPlatform/kubernetes/pkg/auth/user (from $GOROOT)\n    /var/lib/jenkins/workspace/project/src/github.com/GoogleCloudPlatform/heapster/Godeps/_workspace/src/github.com/GoogleCloudPlatform/kubernetes/pkg/auth/user (from $GOPATH)\n    /var/lib/jenkins/workspace/project/src/github.com/GoogleCloudPlatform/kubernetes/pkg/auth/user\ngodep: go exit status 1\n. It's still unhappy it seems :(\n. Just saw it had gone green :) LGTM\n. LGTM\n. LGTM, just that one nit.\n. Needs a rebase is seems.\n. Thanks @vishh :) LGTM, waiting on green to merge\n. ```\n$ godep go test -a -test.short ./...\ngithub.com/GoogleCloudPlatform/heapster/sources/nodes\nsources/nodes/external_test.go:53: too few values in struct initializer\n?       github.com/GoogleCloudPlatform/heapster [no test files]\n?       github.com/GoogleCloudPlatform/heapster/clusters/coreos [no test files]\nok      github.com/GoogleCloudPlatform/heapster/integration 0.016s\n?       github.com/GoogleCloudPlatform/heapster/sinks   [no test files]\n?       github.com/GoogleCloudPlatform/heapster/sinks/gcm   [no test files]\nok      github.com/GoogleCloudPlatform/heapster/sources 0.018s\n?       github.com/GoogleCloudPlatform/heapster/sources/api [no test files]\nok      github.com/GoogleCloudPlatform/heapster/sources/datasource  0.019s\nFAIL    github.com/GoogleCloudPlatform/heapster/sources/nodes [build failed]\n?       github.com/GoogleCloudPlatform/heapster/validate    [no test files]\n?       github.com/GoogleCloudPlatform/heapster/version [no test files]\ngodep: go exit status 2\nThe command \"godep go test -a -test.short ./...\" exited with 1.\n```\n. LGTM, just a question we can continue in another PR\n. The default of the flag is a specific file, so you'd have to specifically set it to the empty string.\n. :)\n. LGTM\n. LGTM\n. /cc @vishh WDYT?\n. The absence of this metric is useful to track, but we can also track container restarts with this. If one of the containers in your pods crashes and gets brought back up, the uptime will go down, although the pod will not change name and the rest of the containers will stay up.\n. LGTM\n. One of the unit tests seems to be failing. That's odd?\n. LGTM. Looks like the CI is unhappy with the gcloud invocations.\n. LGTM\n. LGTM\n. Failing due to the last command failing. Maybe ignore it's return?\n. LGTM, will merge on green.\n. Can we add more description as to the bugs in the commit message? :)\n. LGTM\n. LGTM\n. Will wait for green.\n. :( why did Jenkins report green?\n. Let me know what you think @vishh :) I will fill out the GCM and standalone ones in the coming days as we finalize how to run those.\n. Tested on a new cluster and it comes up as expected.\n. +1, this logic can probably live in the Heapster binary itself. We don't need a wrapper per say.\n. Thanks @vishh! Will merge on green.\n. Looks like the minions have a full disk :P free'd up some space and kicked off the build again. Still investigating...\n. I wanted to get this in and then I'll write the doc :)\n. Failing due to disk space, cleaning up and kicking again.\n. For some reason the web trigger is not firing, but this passes on Jenkins:\nhttp://104.154.52.74/job/heapster-e2e-gce/547/\n. LGTM\n. ok to test\n. LGTM thanks @imjacobclark!\n. +1 to splitting up :) the split sounds good and smaller in scope\n. LGTM, but can we add a note as to why? :)\n. Only remaining comments are around the interfaces that use interface{}\n. LGTM, will merge on green.\n. Looks like we're missing the gofuzz dep?\n. Minor nits and {subtle} this could have been 2-3 smaller PRs {/subtle} :)\n. LGTM. Haha no worries, didn't want to let you off without a passive-aggressive comment ;) It was shorter than expected.\n. Hehehehe :D\n. LGTM\n. nit: Can we add a \"why\" to the commit message? :)\n. Minor nits, LGTM otherwise. Thanks for the update! I missed it :)\n. LGTM, will merge on green. Thanks @vishh!\n. Merging.\n. LGTM\n. ok to test\n. Ah, I figured I'd miss something :) should be fixed. Will let the integration test decide. Thanks for the quick review @vishh!\n. Finally Travis no longer hates me :)\n. LGTM.\n. LGTM\n. Pushed a new version PTAL @vishh\n. All the Kubernetes changes are in. Is this ready?\n. Ah I thought we were waiting on the new Kubernetes version. Just minor comments.\n. LGTM, thanks for fixing nits :D will merge on green\n. @vishh the integration test has failed twice now in TestHeapsterInfluxDBWorks is there a known issue?\n. :D :D\n. ok to test\n. This looks good, although I have a feeling that it will fail the integration test since we have an older release on Kubernetes up.\n. LGTM, thanks @vishh!\n. LGTM\n. ok to test\n. /cc @ggarson\n. PTAL @vishh, went with separate config files to make the getting started easier.\n. Thanks for the review @vishh! We can wait or push it before, either should be fine :)\n. Rebased, waiting on green.\n. Green :D\n. LGTM, wait on green.\n. :D\n. LGTM, is the Jenkins failure expected?\n. LGTM\n. LGTM, thanks @vishh :) this will help as we scale.\n. Jenkins failing :(\n. LGTM\n. @vishh is that build live? It's having trouble pulling it. Maybe lets use 0.14.0 for now?\n. Green :D\n. CI error:\n$ go vet github.com/GoogleCloudPlatform/heapster/...\nsources/events.go:110: result of fmt.Errorf call not used\nexit status 1\n. LGTM, minor nits. Thanks for getting this in so quickly :)\n. @vishh \n```\n--- FAIL: TestHeapsterInfluxDBWorks (9.84 seconds)\n    assertions.go:148: \nLocation:   kube_test.go:370\n\nError:      No error is expected but got failed to build go binary (\"exit status 1\") - \"# github.com/GoogleCloudPlatform/heapster/sinks/api\\n../../sinks/api/supported_metrics.go:171: undefined: stats\\ngodep: go exit status 2\\n\"\n\nMessages:   failed to build and push images\n\n```\n. LGTM overall\n. LGTM :) merging!\n. LGTM\n. LGTM\n. ok to test\n. LGTM\n. LGTM\n. LGTM\n. Outside of that problem, it looks good!\n. LGTM, thanks @vishh!\n. ok to test\n. Thanks @cedbossneo! This looks good.\nWe're missing gofmt btw:\nRun gofmt on the following files:\n ./sinks/api/decoder_test.go\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Will let the CI's run before merge to make sure the release will be green :)\n. @vishh Travis is dead today (as is Shippable :( ), but Jenkins runs everything so we're good!\n. Sure :D \n. Jenkins is failing.\n. LGTM\n. LGTM\n. LGTM\n. Just nits\n. LGTM\n. LGTM, thanks!\n. LGTM\n. LGTM\n. e2e has been failing, can you take a look @vishh?\n. LGTM\n. Jenkins failures are expected, Jenkins is broken right now.\n. Yeah np :) let me know if I can help\n. Build is green after a rebase!\n. LGTM\n. Jenkins is failing btw\n. Jenkins still fails reliably :( is this something we should worry about?\n. Will take a look\n. FWIW, we've tested it on larger cluster and it has been okay. Since we only scrape data every few mins.\n. IIRC we tested on a 100node cluster but more likely with 5 pods/node\n. LGTM\n. LGTM\n. LGTM\n. Sent comments, but I will patch this and build off of it :)\n. Note the new comments btw :)\n. LGTM, thanks for fixing all the nits @vishh!\n. Thanks @rjnagal! We can merge since Jenkins runs what Travis runs too :)\n. This should be ready for review. It is the interface and the logic to get the data from the cache. But there is no cache so we don't return useful information today :)\n/cc @vishh @a-robinson\n. @a-robinson that doesn't seem unreasonable. I changed the data structure PTAL. We not expose all the metrics for the same target schema together.\n. PTAL at the API @a-robinson, still working through integration issues, but the API should be good to go.\n. Rebased and retested. All is working as expected and we return metrics as expected. PTAL @vishh \n. LGTM, I thought some of those changes had been made for some reason.\n. LGTM, thanks @vishh!\n. Passed, although I forgot to update the storage schema, will do that now.\n. Pushed the new commit with the doc updates.\n. Ooops my bad, should be updated now @vishh \n. LGTM\n. LGTM\n. Thanks @vishh! :D\n. Those profiles are a bit hard to read :) we usually need a symbolized one. Can you run the following comand:\n$ go tool pprof -svg -output=output.svg http://localhost:8082/debug/pprof/heap\nand attach output.svg?\nThat command requires having go installed and it assumes you're in localhost. Feel free to change that to a remote host and port.\nThanks! :)\n. Heapster needs to generate some configs before building try running make build from the root (or take a look at the steps the Makefile takes).\n. LGTM\n. The Jenkins build is currently broken sadly :( need to take a look. Feel free to ignore it for now. I don't think this PR would affect it anyways.\n. Looks good overall, just minor comments and suggestions.\n. One nit, LGTM otherwise\n. Thanks @afein! Merging.\n. We're having issues with the e2e, I'd ignore it for now.\n. ok to test\n. LGTM, thanks!\n. @mikedanese have you had a change to run this through its paces btw?\n. The Jenkins setup is a bit broken right now, so feel free to ignore it and\ntest by hand.\nOn Mon, Jul 6, 2015 at 3:25 PM, Mike Danese notifications@github.com\nwrote:\n\nI will look into test failures and ping you\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/382#issuecomment-119015528\n.\n. Ping @mikedanese had a chance to test btw?\n. Thanks @mikedanese! @mvdan also tested the changes and all looks good. LGTM, merging.\n. LGTM, thanks @saad-ali!\n. ok to test\n. LGTM, sorry for the delay. Just minor edits. Will merge to unblock you, please send another PR for the fixes :)\n. What are you looking to run @mboussaa? If you're interested in a Heapster that collects from various (non-Kubernetes) cAdvisors it should be possible to set that up with Heapster today.\n. LGTM, @krousey have you had a chance to try this out on a Kubernetes cluster manually? Our integration setup is a bit broken at the moment...\n. ok to test\n. LGTM, thanks @lavalamp!\n. LGTM, thanks @rjnagal!\n. LGTM, will wait for build\n. Apologies, we uploaded to gcr.io, but not Docker Hub. Should be up now!\n. @afein you were working on something similar no?\n. LGTM\n. LGTM, thanks for being thorough @mvdan!\n. LGTM\n. Thanks @saad-ali!\n. I know @afein had some work in this area as well\n. Ah no, disregard that @afein :)\n. ok to test\n. Hmmm this code is 2 months old, what issues have you found from it? Or was it just by inspection?\n. Ah I see, this will eventually get to cAdvisor which only keeps ~2min of data anyways. This should be safe. Have you had a chance to manually test?\n. we shouldn't duplicate the exported data (we remove it before export), but it is using more resources than necessary to get this information.\n. Thanks @piosz! That's what I meant. We'll merge now and try to get into Kubernetes. We'll need extra testing for that anyways.\n. Lets to the integration test in another PR\n. Also take a look at the code for the existing ExternalSinks and lets see if any of them should do something in their Unregister()\n. Looks like we need a rebase, otherwise LGTM!\n. Thanks @mvdan! Will wait for green and merge.\n. /cc @jszczepkowski @piosz \n. ok to test\n. LGTM, thanks!\n. Needs a rebase btw\n. LGTM\n. ok to test\n. LGTM\n. ok to test\n. LGTM\n. Outside the issue with shares vs. cores, LGTM\n. LGTM, sorry this dropped out of my radar!\n. LGTM\n. Thanks :) LGTM\n. Will need a rebase from the other PR btw\n. Other than that, LGTM\n. LGTM, will merge on green\n. ok to test\n. LGTM, thanks @a-robinson!\n. ok to test\n. Thanks @piosz! Looks like we're missing gofmt on some files:\n\nRun gofmt on the following files:\n ./sinks/gcmautoscaling/driver.go\n. LGTM\n. np :) will merge on green.\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Jenkins is failing :(\n. We should use the cAdvisor client someday :)\n. Ah I see nvm. Must have been from before :D\n. Are all these godep changes by hand?\n. So awesome to cut the hardest step :D\n. We probably shouldn't change Godep unless it is part of  a Godep update... Maybe revert these?\n. we don't chmod /app/app/dashboards/*.json does that matter?\n. Why have this function over just ioutil.ReadFile?\n. nit: return ioutil...\n. maybe: changeClusterConfig() and document what it changes?\n. Do we need the \"_ =\"?\n. Consider using TempDir.\n. %q and %v instead. Same below\n. nit: s/External/external/\n. runKubeClusterCommand()? Here and in tearDownCluster\n. Maybe more comment on what \"this\" is? :)\n. consider doing:\nrequire = require.New(t)\nwhich lets you no longer have to specify t in the function\n. Ah kk. WDYT of using TempDir to generate workDir? We can have the flag's default value be empty which means \"make one for me\"\n. DestroyCluster?\n. nit: self.podErrors[...]++\nhere and above\n. nit: Can we constify \"v1beta1\" here, below, and in the struct above so they all come from one variable?\n. Should we get at least 1? Or waiting for 0.9.0?\n. Ah I see\n. nit: Can we say what we mean by an external source?\n. Remove empty file?\n. nit: can remove temp var\n. How about merging this into newPodsApi?\n. So we watch to populate a cache which we then poll on?\n. Do you mean to put it inside the if? It won't trigger the first time. Moved it closer to the if in case that is what you meant :)\n. Done\n. Done\n. nit: these unrelated changes should be in another commit\n. Why not just always do this instead? We'd probably want a little buffer too.\n. This assumes that each stat is collected at 1s interval which is not true. This is something broken in the cAdvisor API that I've meant to fix. Will probably be fixed for API v2.0.\nDoes not affect this PR :)\n. Looks like the only difference is getting the InternalIP, can we move that code above and not duplicate the append and adding to nodeList?\nSorry to partially be a pain :)\n. Done\n. We should figure out what is going on I just filed #107 to track that, but this makes this driver move forward. We can always remove it later.\n. Found out the hard way during testing :)\n. Unrelated testing, my mistake.\n. I'm planning to instrument this with Prometheus metrics (we can for all of Heapster). To know how many timeseries we've pushed and the error rate. We can keep the last N of these in Validate too.\n. nit: s/grafana/Grafana/\n. Today we only support running in GCE (which we check and error out on in New) and we get the auth information from the machine's service accounts (which we also check and error out on).\n. nit: build to something local since vish is only you :)\n. just \"heapster-buddy-coreos\" which keeps it local. If you want to push it to a repo you can do:\n$ docker tag heapster-buddy-coreos kubernetes/heapster-buddy-coreos\n$ docker push kubernetes/heapster-buddy-coreos\n. This won't let us do HTTPS no? What is the alternative to this? Do you actually have to give it the file paths?\n. I think we can just get this from the filesystem.\n. We do from the container :)\nI have to send a PR to add a hostVolume for Heapster so we can use HTTPS. We need it for GCM and I forgot to add it :P\n. From documentation I've found and references by @kelseyhightower, that is indeed the case.\n. index is unused\n. Ah I see now, this GitHub diff is terrible :P\n. Lets not use container and only index?\n. Is the log line below right? It will display \"/ \"\n. Should we use both for this file too? It seems confusing and error prone.\n. Yes, I think that makes sense. Similar to has_ in proto. Will file an issue :)\n. Done, looks cleaner.\n. Done.\n. this will try to remove the existing kubernetes containers and fail (which is fine). Maybe just ignore the error?\n. Same thing here with images\n. Maybe just mention the kubernetes auth? Now-a-days they're in ~/.kube/<cluster>/.kubernetes_auth but that's too much detail for here :)\n. Done\n. Today we can't target the local Kubelet since we get the pods from the apiserver no? When we change that we can move the flag to heapster.go and take it as a param here I think.\n. Is there such a thing? Everything we measure has to have a unit no?\n. I wanted to rename this since it was getting confusing. This is arguably a Timeseries or a point in time or something along those lines. A lot of the monitoring systems call it \"timeseries\" so that makes sense. WDYT?\n. What about having Store*() for each type we plan instead of Store of a CompositeData type? Probably doesn't matter either way, but worth considering.\n. Ah, I think you want a Count type as well. That takes care of the None usecase above.\n. remove mention to GCM\n. Can we try to make this of a stronger type than interface{}? I think we have a pretty good idea of what those could be today.\n. We can remove this TODO, the pushing is what we should do in parallel :)\n. Add a TODO to remove this when we fix the issue.\n. Or MetricValue?\n. We can have MetricsFrom functions here too. It'll be easier to extend :)\n. MetricPoint?\n. But the backend should drop those requests silently instead of having an overly generic interface. It's fine that some backends don't handle it.\n. Stronger interfaces make it easier for the implementer and for the person writing the code. With an interface I have no idea what types of inputs I get and what I need to support. A strong type tells me exactly what that is and I can choose what to do if I can't handle it.\n. Don't we have another version of this in API? Lets make it public and use it here. Or place it in util.\n. LOL this slipped in, I saw flaked with OnGCE() so we should probably try it for ~15s before quitting. TODO is fine\n. nit: encountered \"the\" following\n. nit: newExternalSinkManager?\n. nit: don't embed else the Lock() and Unlock() are public members of the struct.\n. You shouldn't need this since the empty value is 0.\n. Done.\n. We already do, but this is used here (again) because GCM rounds to seconds. We can enforce that on all backends or re-do this here where it is enforced.\n. This is just a sanity check, by now we should have done this.\n. typo: \"Yes\"\n. I think its key=value\n. typo: Gauge here and elsewhere\n. Ah yes, you're right I just checked.\nOn Tue, Mar 24, 2015 at 5:57 PM, Vish Kannan notifications@github.com\nwrote:\n\nIn docs/storage-schema.md\nhttps://github.com/GoogleCloudPlatform/heapster/pull/189#discussion_r27087812\n:\n\n+| network/rx         | Cumulative number of bytes received over the network                                               | Cumulative | Bytes       | v0.9            |\n+| network/rx_errors  | Cumulative number of bytes received over the network                                               | Cumulative | Count       | v0.9            |\n+| network/tx         | Cumulative number of bytes sent over the network                                                   | Cumulative | Bytes       | v0.9            |\n+| network/tx_errors  | Cumulative number of errors while sending over the network                                         | Cumulative | Count       | v0.9            |\n+\n+Note: Guage refers to instantaneous metrics\n+\n+## Labels\n+\n+Heapster tags each metric with the following labels.\n+\n+| Label Name     | Description                                                                   | Supported Since | Kubernetes specific |\n+|----------------|-------------------------------------------------------------------------------|-----------------|---------------------|\n+| pod_id         | Unique ID of a Pod                                                            | v0.9            | yes                 |\n+| container_name | User-provided name of the container or full cgroup name for system containers | v0.9            | No                  |\n+| labels         | Comma-separated list of user-provided labels. Format is 'key:value'           | v0.9            | Yes                 |\n\nI remember changing it to ':' some time back.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/189/files#r27087812\n.\n. link the the other way around in Markdown\n. Can we separate into Heapster and Sinks(InfluxDB) sections? :)\n. nit: Go errors strt with lowercase, also why do we need to append error to all of these?\n. You probably want to run a few hundred fuzz iterations.\n. Note that we use the canary here until 0.10.0 is released :)\n. Plan SGTM, mind we do it in a followup PR?\n. We should probably have it in a separate troubleshooting page. Seems weird to be in the landing page...Added to the debugging page. WDYT?\n. Yes, removed (and in influxdb.md)\n. Yes, done.\n. nit: s/fs/FS/\n. nit: make([]internalPoint, 0, len(stats.Filesystem))\n\nSorry, I know I'm a pain :)\n. Can we document auto-population of these? I'm guessing we only set the specified labels and may set the values of the rest\n. nit: \"metric specific\" (without the s)\n. Was this meant to be here? Maybe do in another PR or commit? We should probably mention some of the GCM changes as well.\n. We just turned all cumulative into rates. I don't think it's easy to use either way... Maybe keeping cumulative is less worse\n. Sadly this won't work for now. Since it is not omitempty we'd generate both types which will confuse the GCM API. We can't make them omitempty since 0 is a valid value. We need to have two structs we define for this (probably a base embedded into an int or double type). Easiest thing may be to just use double for all the metrics for now. It should not reduce the accuracy of our numbers I believe.\n. commented out?\n. You have to send the fullMetricName() I believe, today you're sending the short one.\n. I think we can move the URL to a separate function (or a const) since we use it in multiple places\n. nit: [GCM] second is a bracket right now\n. Check prefix instead of contains?\n. Why the function redirect? Fine either way :)\n. We should check the metric domain as well. Also: prefix instead of contains?\n. Shouldn't we not need to do this btw? We should know if the metrics coming in are ours or not no?\n. That looks like it is still not sufficient. It is interesting that we're getting a 404.\n. nit: each \"metric\"\n. What are we missing?\n. can build outside the for loop?\n. lol :)\n. we can't do omitempty unfortunately since zero is a valid value :( We need two different structs one for each type.\n. The first one is in the GCM started guide no? and the second is this document?\n. Ah, sneaky, I like it :)\n. nit: remove\n. nit: remove\n. You can check for only type, if the description is different the add below would update it.\n. We should only need to do this once in register no? Since it builds the map of supported metrics.\n. We should consider consolidating these two auth implementations (GCM, GCL) into util in Heapster\n. I guess I mean that we have both already :)\n. possible to export to?\n. I think you mean it is now possible to have multiple sources and sinks. We don't export to sources :)\n. nit: InfluxDB\n. on \"the\" Kubernetes apiserver\n. nit: InfluxDB we're not consistent :)\n. nit: RCs\n. Done!\n. nit: Element here and below.\n. Any reason not to use the source API structs?\n. We call them SystemContainers elsewhere.\n. Why public?\n. GCStore starts a thread for garbage collection, this would be a thread per container we are storing which could get quite huge. WDYT if we enforce the eviction age on the TimedStore in an insert and we then GC ContainerElements?\n. Should we also populate the metadata for these containers?\n. We may be able to share some of the logic for the container insertion.\n. nit: return gcs.store.Put(...)\n. Why public fields?\n. Temp file.\n. Wonder whether we should unify this with the cAdvisor one?\n. nit: StorePods and StoreContainers since we're giving a slice\n. Unused bufferDuration?\n. Not a huge fan of that either, ExternalContainers was another suggestion but that was shut down. It doesn't matter as long as we're consistent though :)\n. Should this be the other way around?\n```\nList has\n[1, 2, 4]\nI do:\nPut(3)\n1: 3 is after 1? yes\nInsertAfter()\n``\n. Not null, but zero. Here and below.\n. ping\n. ping\n. Done.\n. Can we document that the buffer is a linked list in reverse chronological order? :)\n. nit: zeroTime\n. You can dotime.Time{}` as you do above\n. I think there is an issue here with start time.\n```\nTake: [3, 2, 1]\nGet(2, 5)\nI would expect it to return [2, 3], but it actually returns []\nWe check:\n- is 1 before 2? Yes, break.\n```\nIssue with [4, 3, 2, 1,] I want 2 - 3?\n. nit: s/End/end/ s/Start/start/\n. I think this has the same start problem as above.\n. FYI you can do:\nassert := assert.New(t)\n// use assert without t\n. You can probably write a helper that is like:\nfunc expectElements(t *testing.T, actual interface{}, expected []int)\n. We should add a test that specifies a start time\n. Add a test that specifies a start time in the middle of the range too\n. I think this is purpose-built for this usecase, I'd leave metrics for a more generic API we build for querying. Worst case, we symlink them in the future.\n. It will also do by metric-specific labels, we can do this way or by target labels only if you prefer.\n. Also, fault_type we expose as different metrics today, so we'll need to sync up on that.\n. It shouldn't be, we know what labels are for the target schema. Will update and ping again.\n. nit: newContainerElement, same below with the other three. The function is still internal so it probably differentiates itself from a possible NewContainerElement\n. Should this be from the new point and not from now? The difference in behavior is: \"the last 2min of data that has been exported\" vs \"the data exported in the last 2min of walltime\". The first one would keep 2, 1min data points whereas the second will probably only keep one if the delay from collection to storing is high-ish.\n. nit: add error to the log.\nAlso, not sure this should be fatal? Maybe just error?\n. This could be race, since we control the timestamp going in, why not customize it? Take now and subtract an hour, make the gc time an hour.\n. Seems weird to have that in TestDelete though...instead of TestInsert. Wouldn't be where you would expect to find it.\n. Yeah using the newest point makes sense. What do you mean about keeping the\nolder data points? Isn't that what we want to gc?\nOn Fri, Jun 5, 2015 at 12:53 PM, Vish Kannan notifications@github.com\nwrote:\n\nIn sinks/cache/gc_store.go\nhttps://github.com/GoogleCloudPlatform/heapster/pull/300#discussion_r31845856\n:\n\n\nreturn err\n}\ngcs.reapOldData()\nreturn nil\n  +}\n  +\n  +func (gcs *gcStore) Get(start, end time.Time) []interface{} {\nreturn gcs.store.Get(start, end)\n  +}\n  +\n  +func (gcs *gcStore) Delete(start, end time.Time) error {\nreturn gcs.store.Delete(start, end)\n  +}\n  +\n  +func (gcs *gcStore) reapOldData() {\nend := time.Now().Add(-gcs.bufferDuration)\n\n\nWe can store points out of order. Based on how we end up storing data, we\nmight end up clearing more points that we intended to. We can take the most\nrecent point's timestamp for GC. I wonder if it is necessary to keep data\npoints that are older than the buffer duration?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/300/files#r31845856\n.\n. This is the last remaining commend, otherwise looks good :D\n. nit: time.Time{} or was there an issue with that?\n. Ah good catch, I was overwriting them today. Should be fixed.\n. nit: document these please\n. Any particular reason this one is public?\n. document here too :)\n. I'd say to just use an int, it is generally simpler and we don't need the optimization\n. Go uses camelCase, here and elsewhere\n. maybe rename to current? tp vs tpc gets hard for readability\n. Not a huge fan of the duplication between here and in_memory.go. If we find a bug in one we'll have to update both and I'm sure we'd forget to do so. We should try to merge the implementations more, but we don't have to do that now. Can we add a TODO to look into merging them?\n. nit: you shouldn't need to do this\n. How about we use a value that averages out to a round number? Easier to reason about and less comment :) {0, 30, 10, 1000} = 1040 / 4 = 260. / 8 = 130\n. Ah I see what you mean. Long term, we may want to look at how this affects the accuracy of the data. SGTM for now.\n. Agree, but we should make a note of this. Many times these \"temporary\" things outlive us all.\n. Saw the TODO you added, looks good!\n. nit: camelCase (and in old_avg)\n. This should probably only override if it was provided no?\n. Just SinkManager?\n. We should document this (don't follow the bad example we set above :) ). Lets also documen tthe ExternalSinkManager\n. \"Configuration for Heapster sinks for exporting data\"\n. We should document this format and provide examples. Something more simple here, but we should add something to docs/\n. NewSinkManager\n. How we do this is so nasty... (not your code). This whole overloading of a URI and using a string instead of a strongly-typed thing really doesn't sit well with me. WDYT of making this a slightly stronger and friendlier API? At the very least we can do:\n\nGo\ntype SinkConfig struct {\n    // Name of the sink.\n    Name string\n    // Arguments for configuring the sink.\n    ConfigArgs string\n}\nIt won't be as nice for flags, but it'll work much better for a REST API. I'd even argue to make the SinkArgs more concrete. Maybe make this a struct with configs for all the sinks?\nGo\ntype SinkConfig struct {\n    GCM GCMConfig `json:\"gcm,omitempty\"`\n    InfluxDB InfluxDBConfig `json:\"gcm,omitempty\"`\n    ...\n}\n. We can make the interface SinkManager and have the implementation be ExternalSinkManager\n. I guess we don't have to add anything here, we're as verbose as we are elsewhere. We should have more docs for it though. Questions always come :)\n. more docs -> docs/\n. I think this was meant as an ExternalSink being a sink of other sinks. That didn't quite pan out, it is more of a manager of sinks.\n. I'm OK with doing in another PR. I don't think we should change the flags though. Writing JSON to flags is not the nicest...\n. I'd say: s/ExternalSink/sinkManager/\n. rename this file to manager.go?\n. comment is stale\n. Don't embed the Mutex since it makes its functions public on the object\n. What does this mutex protect? Let's document it :)\n. unrelated: Can remove this TODO\n. Document both functions please\n. SetSinks is probably fine actually, since it implements the interface\n. Actually, now that I read more of the code on the side it looks like the code is currently transitioning to ExternalSink as the interface. So yes, let's not rename ExternalSinkManager -> SinkManager. Sorry :) at least you have the separate commit!\n. What do you mean by shares? What units?\n. just heapster?\n. Won't we need the makefile for the generate stuff?\n. Ah maybe my bash is weak :) I intended for this to be \"heapster:canary\", but this seems like we're selecting a var with that name?\n. Ah thanks! I've seen the second but not the first notation. \n. Why change to this instead of what we did before with the makefile?\n. SGTM, I see you made the fixes to the Makefile as well.\n. Just curious about the units :)\n. Seems like we should start using defined values for these strings. We can do in another PR if we'd like.\n. Can we document this function and the one below it?\n. This is exporting in shares? At the Heapster level (and for nodes) we should probably export in whole cores. Shares is hard to reason about at a node level.\n. I would have expected we still do this? We fixed it for node containers but not for general containers no?\n. Hmmm I tend to like the alternative we mentioned better: \nGo\ntype SinkConfig struct {\n    GCM GCMConfig `json:\"gcm,omitempty\"`\n    InfluxDB InfluxDBConfig `json:\"gcm,omitempty\"`\n    ...\n}\nIt is stronger typed, self-documented, and easier on a REST API. It totally sucks for a flag interface though. WDYT? I am not against having the flag interface be different from the REST API.\n. ",
    "rjnagal": "LGTM\n. We should start adding tests and travis build here too :)\n. Minor nits. LGTM\n. LGTM\n. I like the templated dashboards :+1: \n. LGTM\n. LGTM\n. LGTM\n. heapster is not GCE specific. You can run heapster with a Kubernetes or CoreOS cluster on any platform.\n. LGTM\n. Also add a binary to the repo.\n. LGTM\n. Added a release and a binary. Let me know if that works for you.\nOn Tue, Dec 2, 2014 at 2:55 PM, Craig Wickesser notifications@github.com\nwrote:\n\n@rjnagal https://github.com/rjnagal @vishh https://github.com/vishh\nis a binary going to be added, perhaps a 0.4 \"release\" (as in github\nrelease)?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/21#issuecomment-65321234\n.\n. LGTM\n. Can we just use the stats from root container and take a diff?\n. True. These should be exported by Kubelet as a standard API. Otherwise, we\nwill end up picking lot of unrelated stuff that'll need to throw out. That\nis lot of wasted bandwidth at cluster level.\n\nOn Tue, Jan 6, 2015 at 10:24 AM, Vish Kannan notifications@github.com\nwrote:\n\nIf we run the system daemons in separate cgroups, it is useful to track\neach of the raw cgroups separately.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/30#issuecomment-68907976\n.\n. godep go build ./... was failing before the update. Works fine after adding CoreOs deps.\n. LGTM\n. @vishh Is the travis build set up? I don't see it running for this PR.\n. LGTM\n. go vet has moved. You nedd to update travis. We already did that for cadvisor yesterday.\n. LGTM\n. ha, now we have vet errors :)\n. LGTM\n. I am going to try this patch tonight.\n. Can you rebase to remove Godeps and squash other patches? It'll make it easier to cherrypick.\n. @dipankar Can you remove the coreos binary from the patch? Thanks.\n. LGTM\n. btw, it requires credentials to view jenkins failure.\n. Merging, @vishh is still adding more fixes to the test.\n. This time it timed out ... took 10m. I think its close to be fixable.\n. I think its getting stuck somewhere. Timed out again after 15mins.\n\nOn Tue, Jan 13, 2015 at 9:17 AM, Vish Kannan notifications@github.com\nwrote:\n\nI have restarted the build with an increased timeout.\nOn Mon, Jan 12, 2015 at 7:26 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nThis time it timed out ... took 10m. I think its close to be fixable.\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/GoogleCloudPlatform/heapster/pull/48#issuecomment-69690274>\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/48#issuecomment-69781531\n.\n. LGTM, finally :)\n. test this please\n. LGTM\n. LGTM\n. LGTM.\n\nMerging, since CI passed. kube cluster failed to come up for integration test.\n. Updated with added pod ID and other fixes.\n. @vishh can you rebase?\n. LGTM\n. @vishh Fixed this to work on created->active pod transitions.\n. Updated the change to purge old pods from activePods and podError maps.\n. LGTM\n. LGTM.\nNeeds rebase.\n. LGTM.\nCan we auto-generate the js files? 80% of the file is boilerplate.\n. LGTM\n. Thanks for the fix.\nLGTM\n. LGTM.\nMinor questions for my understanding. Merging now.\n. LGTM\n. ok :) LGTM\n. LGTM\n. LGTM\n. Is the integration expected to fail?\nThe changes look good otherwise\n. LGTM\n. LGTM.\n. Would it be a problem if we accidentally forgot to add the flag and heapster just continued assuming localhost?\n. I just realized that after sending the comment.\nLGTM\n. LGTM\n. +1 there are too many steps in current monitoring setup for user/admin to run through themselves.\nThe tool should also support cleanly shutting down monitoring for people who don't use it. The subcommands I'd like to see:\n- start monitoring\n- stop monitoring\n- validate (get validate output from heapster and cadvisors, also healthchecks sinks)\n. Heapster currently supports kubernetes and fleet. Extending to add other types is pretty simple. We'd love to have PRs for marathon support and can help with any issues. We won't have time to get to it ourselves for a while though.\n@samek would you like to give it a shot?\n. @samek I was advocating adding a new source. Since the setup uses cadvisor, the only part we need to implement is a watch or poll interface for getting list of nodes from marathon. I'd love to help with any question about approach or guidelines.\n. LGTM\n. travis fails:\n    github.com/GoogleCloudPlatform/heapster/sinks/gcm   [no test files]\n--- FAIL: TestWorkflowSuccess (0.00s)\n    Location:   cadvisor_test.go:101\n    Error:      Not equal: \"/b\" (expected)\n                    != \"/a\" (actual)\nLocation:   cadvisor_test.go:102\nError:      Not equal: \"/a\" (expected)\n                != \"/b\" (actual)\nFAIL\nFAIL    github.com/GoogleCloudPlatform/heapster/sources 0.047s\n. That's a weird flake. Might be a bad test. We should look it up.\n. LGTM. Thanks for the quick fix :)\n. LGTM\n. LGTM\nAll green :)\n. Is the failure expected:\ngithub.com/GoogleCloudPlatform/heapster/sinks/api\nsinks/api/decoder_test.go:32: cannot use struct {} literal (type struct {}) as type api.AggregateData in argument to NewDecoder().Timeseries\n. LGTM\nMerging\n. Test failure in vmodule revert, haha. Suitable punishment for bundling all patches together :)\n. Still failing integration:\nI0313 17:53:28.043127    6702 framework.go:393] Cannot find Replication Controller \"monitoring-heapster-controller\". Skipping deletion - replicationController \"monitoring-heapster-controller\" not found\nI0313 17:53:28.045736    6702 framework.go:393] Cannot find Replication Controller \"monitoring-influx-grafana-controller\". Skipping deletion - replicationController \"monitoring-influx-grafana-controller\" not found\nI0313 17:53:28.049257    6702 framework.go:378] cannot find service \"monitoring-influxdb\" - service \"monitoring-influxdb\" not found\nI0313 17:53:41.043606    6702 kube_test.go:252] waiting for pods to be running\n--- FAIL: TestHeapsterInfluxDBWorks (260.81 seconds)\n    assertions.go:154: \n```\nLocation:   kube_test.go:311\nError:      No error is expected but got pod not in running state after 60 seconds\n```\nFAIL\nexit status 1\nFAIL    github.com/GoogleCloudPlatform/heapster/integration 260.821s\ngodep: go exit status 1\n. LGTM.\nwaiting on integration tests.\n. LGTM, although I don't understand half of the mysterious ways of these scripts :)\n. LGTM, ignoring build failure for doc change (cluster is already setup when integration test starts).\n. Does heapster handle auth today? State modifying APIs would require some kind of Auth setup option - maybe just like the basic one we have in cAdvisor.\n. LGTM\n. ok to test\n. :+1: holding PR SG.\n. ok to test\n. LGTM\n. LGTM\n. ok to test\n. LGTM.\nWill merge after test comes back.\n. ok to test\n. sources/kube.go:242: undefined: kubeClientVersion\nsources/kube.go:260: undefined: kubeClientVersion\n. LGTM. Waiting for tests.\n. The test failed :(\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Do we have any certs available outside secrets today?\n. Discussed with @vishh offline.\n. LGTM\nmostly nits.\n. LGTM\n. Probably easier to put it in your global git preferences.\n. I don't mind it, I just gave up trying to update repos some time ago :)\n. LGTM\n. It's failing with the same error we saw before:\nkube_test.go:350] failed to query list of pods from influxdb. Query: \"select distinct(pod_id) from /cpu./\", Err: expected only one series from Influxdb for query \"select distinct(pod_id) from /cpu./\". Got []\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Awesome, LGTM\nThanks @saad-ali \n. LGTM\n. Can you clarify what you mean by standalone? Without kubernetes or fleet?\n. ah, got it. Will try to get to it this week.\nOn Thu, May 28, 2015 at 9:49 AM, shilpapadgaonkar notifications@github.com\nwrote:\n\nYes. There is a link for the same in the docu\nhttps://github.com/GoogleCloudPlatform/heapster/blob/master/docs/standalone.md\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/299#issuecomment-106482626\n.\n. LGTM\n. LGTM\n. Doesn't that mean we need to update travis to 1.5\n\nFor Go 1.4 you still need to go install golang.org/x/tools/cmd/vet. For Go 1.5 this may not be necessary.\n. LGTM\n. LGTM. Needs rebase to remove other patches.\n. LGTM. waiting for travis.\n. LGTM\n. LGTM\n. @Yashu5 If you are not running heapster with kubernetes, you need to configure heapster to discover the cadvisor port to talk to. See documentation for cadvisor/external in https://github.com/GoogleCloudPlatform/heapster/blob/master/docs/source-configuration.md\nThe relevant portion to your setup:\nExternal\nExternal cadvisor source \"discovers\" hosts from the specified file. Use it like this:\n--source=cadvisor:external[?]\nThe following options are available:\nstandalone - only use localhost (default: false)\nhostsFile - file containing list of hosts to gather cadvisor metrics from (default: /var/run/heapster/hosts)\ncadvisorPort - cadvisor port to use (default: 8080)\nHere is an example:\n./heapster --source=\"cadvisor:external?cadvisorPort=4194\"\nLet us know if that works for you.\n. @Yashu5 yes, you don't need to run another cadvisor. It is built into kubernetes node agent - kubelet.\nWhat platform are you running on? Is your kubernetes cluster up? Can you get\nkubectl cluster-info\nkubectl get pods\nI suspect your node is not configured properly. You can also try to get cAdvisor debug info from:\ncurl -L localhost:4194/validate\n. The test failed:\n. The test is failing:\n- godep go test -a -v --timeout=30m github.com/GoogleCloudPlatform/heapster/integration/... '--vmodule=*=1' --namespace=default --kube_versions=0.18.2\n  === RUN TestHeapster\n  I0611 23:42:27.122513   18657 framework.go:254] Disabled cluster monitoring\n  I0611 23:42:27.123087   18657 framework.go:222] checking if existing cluster can be used\n  I0611 23:42:27.125972   18657 framework.go:225] failed to get kube version info - \"the server could not find the requested resource\"\n  I0611 23:42:27.125992   18657 framework.go:261] Setting up new kubernetes cluster version: 0.18.2\n  I0611 23:42:27.126001   18657 framework.go:153] Bringing down any existing kube cluster\n  panic: test timed out after 30m0s\ngoroutine 19 [running]:\ntesting.func\u00b7008()\n    /usr/local/go/src/testing/testing.go:681 +0x12f\ncreated by time.goFunc\n    /usr/local/go/src/time/sleep.go:129 +0x4b\ngoroutine 1 [chan receive, 27 minutes]:\ntesting.RunTests(0xba62e0, 0xd86190, 0x1, 0x1, 0xd90501)\n    /usr/local/go/src/testing/testing.go:556 +0xad6\ntesting.(*M).Run(0xc20805a000, 0xd98180)\n    /usr/local/go/src/testing/testing.go:485 +0x6c\nmain.main()\n    github.com/GoogleCloudPlatform/heapster/integration/_test/_testmain.go:52 +0x1d5\ngoroutine 5 [chan receive]:\ngithub.com/golang/glog.(*loggingT).flushDaemon(0xd8fb20)\n    /var/lib/jenkins/workspace/project/src/github.com/GoogleCloudPlatform/heapster/Godeps/_workspace/src/github.com/golang/glog/glog.go:879 +0x78\ncreated by github.com/golang/glog.init\u00b71\n    /var/lib/jenkins/workspace/project/src/github.com/GoogleCloudPlatform/heapster/Godeps/_workspace/src/github.com/golang/glog/glog.go:410 +0x2a7\ngoroutine 17 [syscall, 30 minutes, locked to thread]:\nruntime.goexit()\n    /usr/local/go/src/runtime/asm_amd64.s:2232 +0x1\n. LGTM\n. needs rebase.\n. LGTM\n. ok to test\n. LGTM\n. LGTM\n. Agree w/ the naming comments. Applies to ResourceID too?\n. LGTM\n. This will need updating if you update external_id on the other PR.\n. LGTM,\nwill merge after deps are in.\n. Need to trigger tests again.\n. LGTM\n. ok to test\n. What other endpoint do we export host_id at?\n. LGTM\n. LGTM\n. Thanks for catching this, @njuicsgz \nWe should just push a new image tag v0.14.2 to be consistent with the other tags.\n@vishh Can you do a push today?\n. the link is still not accessible.\nOn Wed, Jun 17, 2015 at 12:45 PM, Vish Kannan notifications@github.com\nwrote:\n\nOutput here: http://104.197.27.4:8082/api/v1/metric-export\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/356#issuecomment-112925721\n.\n. LGTM\n. Let's merge this now, assuming we'll fix all comments in the followup PRs.\n. Can you describe all the pieces in this PR a bit more for my reference.\n. LGTM\n. nit about the date, fine other than that.\n\nLGTM\n. LGTM\n. LGTM\nThanks for the example.\n. LGTM\n. ok to test\n. LGTM \nI think this needs to wait for #389 though\n. Thanks @krousey, merging.\n. ok to test\n. LGTM with a file-naming nit.\nLet's squash the changes and merge.\n. LGTM\n. LGTM\n. LGTM\n. Handling end time and listing in separate PR is fine. It'll be easier to review :)\nThe only remaining nit I have around API is that the doc part of most endpoints feel under-specified. eg does nodename in the endpoint correspond to a fqdn, kube host id, or maybe ip?\nWould it also help if we bundled all params for a request into a single structure, eg NodeRequest for the implementation methods. That way we can have a single well-documented types.go and addition of new params would be a bit cleaner. WDYT?\n. ok, let's merge this and get going with the follow on changes.\n. LGTM\n. Curious: Why go back from 1.3 to 1.2.2?\n. Might be useful to add versioning policies here. Is every release an increment of 0.1, or you want to make a convention?\n. Is the default always Kubernetes, or can it be fleet too ?\n. Why not hide controls by default?\n. Why the commented line?\n. Ah, sorry. This is go source :)\n. Do we want to move it to 0.7.1?\n. How do we tell if v0.7.0-13-* is moving forward from v0.7.0-256? Shouldn't it be v0.7.2-smth?\n. done.\n. The errors are probably due to problem from host. Do you want to add UID in addition to pod and host name?\n. done.\n. Why change all ports to 80?\n. slices don't need initialization\n. Do you want to separate out release PR\n. Do we need to check if a port was passed in (MONITORING_INFLUXDB_SERVICE_PORT)?\n. yes, this is a problem because we don't keep a list of pods being tracked. Would it be good enough to regenerate the list in getPods() - purge all old entries that are not present in the new listing from getPods()? \n. kubelet will set pod to running when at least one container instance is running. Will that return it back to the earlier state?\n. SG. I think the comment in pod_cache differentiates between started and running, but if kubelet does it, we can trust that.\n. The purpose is to record any errors retrieving pod data once the pod is up and running. I think this should do. I need to look at pod restart behavior, but it doesn't need to be done here.\n. nit: move to !first check.\n. nit: reword - 'Cumulative stats have container creation time as start time'\n. nit: maybe just spell lbl out, eg labelPodId?\n. nit: Make it ParseRC to match DeleteRC and CreateRC?\n. nit: The signature looks unnatural. We don't need to return *api.Service. I think we can hide the k8 api format and do the simpler version of returning just the error. Same for CreateRC.\n. doh! why did we start with a post?\n. What's the difference between poll duration and stats resolution?\n. Does this mean we get 5s worth of stats every 10s ?\n. nvmd... it gets all stats between lastGet and now. \n. do we need to check resolution against poll duration? \n. s/runing/running/\n. Shouldn't we bet setting it to the timestamp of the last stored datapoint to get a consistent resolution? \n. s/ddebug/debug\n. 2015 in new files :)\n. You need to run godep save with ./... to include dependencies for subpackages.\n. curious: why is uptime a rate?\n. remove units as we have a separate field for it.\n. How do we handle multiple fs?\n. Why is it updating a single godep file without any update to Godeps.json?\n. where do we fill the FLAGS? I don't see that anywhere.\n. Can you expand the name for readability. Here, and in all methods.\nDoes it stand for representation? You can also name it topology or schema.\n. This is going to print on every request. Probably make it glog.V(2).Infof?\n. I'd prefer a single comment style - \" // \"\n. Time should be zero by default. We can skip updating it and just fill in the cinfo below.\n. Inline zero as time.Time{}\n. How do we take care of nodes that got deleted?\n. do we need the lock around lookup?\n. We don't check the return type? Do we need it to retutn the nodeInfo? Shall we return an error (eg if node exists).\n. All Update*() should return error.\n. It might be better if we are consistent with the other methods here and add an UpdateTime() call. The lock can then be subsumed there and we don't need to worry about addition of extra code after timestamp causing a deadlock.\n. Why do we split ClusterInfo from realCluster with just the timestamp at the top-level?\nIt's harder to see that lock here is protecting the timestamp in realCluster.\nCan we push all fields of ClusterInfo into realCluster, or do we plan to reuse ClusterInfo somewhere else?\n. why is this a package global? It only seems to be protecting realCluster?\n. s/Unable/unable\ngo convention for errors is to start with lower case. Here and below.\n. why do we use \"All\" in every GetData()? It makes me think we'll add interfaces to pull out partial data (field-specific). Is that true?\n. Are there any restrictions on hostname string? Is this internal hostname from heapster, or does it have to be fqdn? Doesn't matter if it is called after getting hostname from heapster, but we should document it.\n. Document the return types - what is the returned time - time of data collection? Is that relevant to caller of GetAllData()? Why do they need that?\nAdding header comments to methods would help the users understand how to use the interface.\n. Is the default namespace also part of rc.Namespaces? In that case, this shouldn't ever happen if we have any data?\n. Is this check necessary? If we didn't have this one, user would get a better error message that the namespace they asked for does not exist.\n. Also add namespace we are looking at in this error message.\n. Add  //TODO(afein): Unimplemented to indicate that this is WIP.\nThis is a general pattern that will help us easily figure out which pieces are coming.\n. Is returning pointers for Node and Pods etc safe in case the caller mutates it? We should add a comment if it is.\n. optional nit: remove the else to remove a branch.\n. fix date: 6-19\n. should we read from kube_c also before returning?\n. do we need to reap all channels before returning?\n. I prefer full names for readability: schema/aggregation.go\n. specified start time implies there are params that we should describe here?\n. space between / and endpoint.\n. what type is the start time?\n. does /node give data about all nodes? or list available nodes? How does the caller discover node names?\n. same comment for namespace and pods: how do you find all available namespaces and pods? Does the /namespace endpoint gives data for all namespaces?\n. For all endpoints, why don't we need an end time? It looks like some of the calls can return too much data and ability to exactly chunk might be useful?\n. pod between {}\n. ah, you meant /namespace/{namespace-name}/pod .\n. that this calls returns only free containers and not all containers is confusing. rename?\n. use your github id in TODOs for better name resolution :)\n. what does 'default analogy' mean?\n. update to freecontainers.\n. ",
    "sebzimmermann": "Hey, I'm also currently facing this issue. Are there any updates or possible workarounds?\n. ",
    "viklas": "Thanks - needed a pointer. \n. ",
    "mindscratch": "I am running kubernetes 0.6, i have cadvisor deployed as pods (to each minion) and I deployed a heapster pod (as shown in the example.\nWhen I tail the logs of the heapster container, I see:\nfailed to get stats from kubelete url: http://192.168.4.5:10250/stats?num_stats=10 - Get http://192.168.4.5:10250/stats/: EOF\nIf I:\ncurl -v http://192.168.4.5:10250/stats?num_stats=10\nI see I get a 301 response which tells me to try \"/stats/\" , so I try:\ncurl -v http://192.168.4.5:10250/stats/\nAnd I get an \"empty reply from server\"\n. The problem I had was cadvisor wasn't deployed with hostPort being 4194, now heapster is not complaining.\n. @mikedanese the port 8080 is configurable using -port on the command-line, looks like kubernetes needs to fix their end.\n. @ryaneleary the problem was the pod definition I used for running cAdvisor didn't specify the hostPort, I updated the pod definition like so:\nyaml\nports:\n  - {name: cadvisor-port, containerPort: 8080, hostPort: 4191}\n. @vmarmol are you saying the kubelet process starts a cAdvisor process? I'm running kubernetes on a private set of VM's (not AWS, GCE, etc), so I thought I had to also deploy cAdvisor myself. Is that not the case?\n. @ryaneleary perhaps that's the case when using their vagrant setup or cloud provider, but that doesn't seem to be the case when running kubernetes on VM's otherwise.\n. @vmarmol I installed kubernetes manually (meaning, dropping the binaries onto the hosts) so /etc/kubernetes/manifests did (and does) not exist.\n@ryaneleary it's possible to \"pin\" pods by specifying a \"hostPort\" in the podTemplate and potentially by using labels (having some discussion with people with regards to whether this is working yet). I do like the idea of dropping configs into /etc/kubernetes/manifests and letting kubelet handle it.\n. @rjnagal @vishh is a binary going to be added, perhaps a 0.4 \"release\" (as in github release)?\n. ",
    "ryanleary": "Can you expound on that @mindscratch ? What did you end up changing?\n@jeinwag Did you have any luck solving your issue? I see the same invalid character messages you do.\n. Looks like for whatever reason salt didn't put the appropriate file in my /etc/kubernetes/manifests directory.\nWhat is running on port 10250 then? I'm a bit confused.\n. @mindscratch When I started up kubernetes using vagrant, cAdvisor didn't start for me either. However, it appears that it probably should have. If you look in cluster/saltbase/salt/cadvisor, there is cadvisor.manifest which is supposed to be copied to each minion's /etc/kubernetes/manifests/ directory. Once this file is deployed, the kubelet running on that node will automatically spawn a cAdvisor pod. This guarantees that a cAdvisor runs on every minion.\n. That may be true. I think the intent of the manifests directory is so that you may deploy pods that must run on a particular machine. In the case of cAdvisor, you want to run it on every minion and not have to depend on a replication controller or something (which wouldn't be deterministic). The kubelet process has a watch on the manifests directory and starts the pods defined in there automatically. Perhaps @vmarmol could confirm this is the desired behavior and correct rationale?\n. Yes, when I checked out from master (recently, post 0.6.0 prerelease) and provisioned the cluster, /etc/kubernetes/manifests/ was not created, and the cadvisor manifest, obviously then, wasn't there. The kubelet was properly configured to look in /etc/kubernetes/manifests/ and correctly logged that the directory didn't exist and was to be ignored. Manually creating the manifests directory and copying the manifest started the cadvisor process properly.\nI can try to track down the underlying issue and submit a PR to the kubernetes project.\n. ",
    "mikedanese": "It looks like the port number for the kubelet cadvisor client is hardcoded to 4194 here: https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/kubelet/util.go#L95\nbut the cadvisor run doc here, cadvisor runs on 8080. This was confusing for me as well.\n. @vishh \n. @vishh I've made requested changes and ran make test-unit and make test-integration. Is there anything else the jenkins config is doing?\n. @googlebot I am okay with these commits being contributed. Thanks @vishh \n. I will look into test failures and ping you\n. Ok sounds good.\n. Whoops forgot. Yes I tested and seems to work. Without change:\n$ sudo docker run gcr.io/google_containers/heapster:v0.15.0 --sink=gcm --sink=gcl                                                                                                                                                    \nI0714 00:53:05.419413       1 heapster.go:52] /heapster --sink=gcm --sink=gcl\nI0714 00:53:05.419845       1 heapster.go:53] Heapster version 0.15.0\nE0714 00:53:08.434191       1 driver.go:120] [GCM] list metrics failed Get https://www.googleapis.com/cloudmonitoring/v2beta2/projects/mikedanese-k8s/metricDescriptors: x509: failed to load system roots and no roots provided\nI0714 00:53:08.434433       1 driver.go:524] created GCM sink\nE0714 00:53:08.434537       1 heapster.go:59] Get https://www.googleapis.com/cloudmonitoring/v2beta2/projects/mikedanese-k8s/metricDescriptors: x509: failed to load system roots and no roots provided\nWith change:\n$ sudo docker run  gcr.io/mikedanese-k8s/heapster:test --sink=gcm --sink=gcl\nI0714 00:53:21.887427       1 heapster.go:53] /heapster --sink=gcm --sink=gcl\nI0714 00:53:21.887679       1 heapster.go:54] Heapster version 0.16.0\nE0714 00:53:24.934109       1 core.go:118] [GCM] list metrics failed request &{Method:GET URL:https://www.googleapis.com/cloudmonitoring/v2beta2/projects/mikedanese-k8s/metricDescriptors Proto:HTTP/1.1 ProtoMajor:1 ProtoMinor:1 Header:map[Authorization:[Bearer ya29.sAFs2CENSFl9c5IpDMoYGNcrJ6eDGHEsAH7GCgLJAPGuhXCSiM-n2zSOwgnTQ-KdChJP] Content-Type:[application/json]] Body:<nil> ContentLength:0 TransferEncoding:[] Close:false Host:www.googleapis.com Form:map[] PostForm:map[] MultipartForm:<nil> Trailer:map[] RemoteAddr: RequestURI: TLS:<nil>} failed with status \"403 Forbidden\" and response: &{Status:403 Forbidden StatusCode:403 Proto:HTTP/1.1 ProtoMajor:1 ProtoMinor:1 Header:map[Alternate-Protocol:[443:quic,p=1] Vary:[Origin X-Origin] Content-Type:[application/json; charset=UTF-8] Date:[Tue, 14 Jul 2015 00:53:24 GMT] Expires:[Tue, 14 Jul 2015 00:53:24 GMT] Cache-Control:[private, max-age=0] X-Content-Type-Options:[nosniff] X-Frame-Options:[SAMEORIGIN] X-Xss-Protection:[1; mode=block] Server:[GSE]] Body:0xc208325d80 ContentLength:-1 TransferEncoding:[chunked] Close:false Trailer:map[] Request:0xc20811ad00 TLS:0xc208073680}, Body: \"{\\n \\\"error\\\": {\\n  \\\"errors\\\": [\\n   {\\n    \\\"domain\\\": \\\"usageLimits\\\",\\n    \\\"reason\\\": \\\"accessNotConfigured\\\",\\n    \\\"message\\\": \\\"Access Not Configured. The API (Cloud Monitoring API) is not enabled for your project. Please use the Google Developers Console to update your configuration.\\\",\\n    \\\"extendedHelp\\\": \\\"https://console.developers.google.com\\\"\\n   }\\n  ],\\n  \\\"code\\\": 403,\\n  \\\"message\\\": \\\"Access Not Configured. The API (Cloud Monitoring API) is not enabled for your project. Please use the Google Developers Console to update your configuration.\\\"\\n }\\n}\\n\"\nI0714 00:53:24.934368       1 driver.go:93] created GCM sink\nE0714 00:53:24.934484       1 heapster.go:60] request &{Method:GET URL:https://www.googleapis.com/cloudmonitoring/v2beta2/projects/mikedanese-k8s/metricDescriptors Proto:HTTP/1.1 ProtoMajor:1 ProtoMinor:1 Header:map[Authorization:[Bearer ya29.sAFs2CENSFl9c5IpDMoYGNcrJ6eDGHEsAH7GCgLJAPGuhXCSiM-n2zSOwgnTQ-KdChJP] Content-Type:[application/json]] Body:<nil> ContentLength:0 TransferEncoding:[] Close:false Host:www.googleapis.com Form:map[] PostForm:map[] MultipartForm:<nil> Trailer:map[] RemoteAddr: RequestURI: TLS:<nil>} failed with status \"403 Forbidden\" and response: &{Status:403 Forbidden StatusCode:403 Proto:HTTP/1.1 ProtoMajor:1 ProtoMinor:1 Header:map[Alternate-Protocol:[443:quic,p=1] Vary:[Origin X-Origin] Content-Type:[application/json; charset=UTF-8] Date:[Tue, 14 Jul 2015 00:53:24 GMT] Expires:[Tue, 14 Jul 2015 00:53:24 GMT] Cache-Control:[private, max-age=0] X-Content-Type-Options:[nosniff] X-Frame-Options:[SAMEORIGIN] X-Xss-Protection:[1; mode=block] Server:[GSE]] Body:0xc208325d80 ContentLength:-1 TransferEncoding:[chunked] Close:false Trailer:map[] Request:0xc20811ad00 TLS:0xc208073680}, Body: \"{\\n \\\"error\\\": {\\n  \\\"errors\\\": [\\n   {\\n    \\\"domain\\\": \\\"usageLimits\\\",\\n    \\\"reason\\\": \\\"accessNotConfigured\\\",\\n    \\\"message\\\": \\\"Access Not Configured. The API (Cloud Monitoring API) is not enabled for your project. Please use the Google Developers Console to update your configuration.\\\",\\n    \\\"extendedHelp\\\": \\\"https://console.developers.google.com\\\"\\n   }\\n  ],\\n  \\\"code\\\": 403,\\n  \\\"message\\\": \\\"Access Not Configured. The API (Cloud Monitoring API) is not enabled for your project. Please use the Google Developers Console to update your configuration.\\\"\\n }\\n}\\n\"\nJust rebased.\n. @SergeyCherepanov what version of kubernetes are your running? are you using GKE or ./cluster/kube-up.sh to setup the cluster?\n. Are you running heapster on kubernetes? If so can you show me the pod or rc yaml\n. @SergeyCherepanov the host and port are pulled from the KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT env variables. You can't use InClusterConfig unless heapster is running as a pod (or you make heapster think it's running inside a pod by mimicing the environment). We should definitely fail more gracefully than having an empty host and port.\nRegarding the heapster-controller.json error, I think this is fixed by #382. @vishh perhaps another release? \n@SergeyCherepanov In the mean time can you see if this controller spec works:\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/master/cluster/addons/cluster-monitoring/google/heapster-controller.yaml\n(it's the exact same but yaml, with a volume mount)\n. @SergeyCherepanov ok. What version of kubernetes are you running?\n. It should not cause cert issues.\n. @Andre-Freitas are you running on kubernetes? If so, can you link or paste the rc-spec.yaml you are using to run heapster?\n. > you must enable root_ca_file (in case you don't use default installation methods) for kube-controller-manager\nTrue, this should probably be documented somewhere.\n\nYou must remove currently used secret (this is necessary for installation which go through update process from early version)\n\nAlso true for the release but this was fixed in GoogleCloudPlatform/kubernetes#11303\n. I will try to writeup a doc about requirements and known pitfalls.\n. @AlexeyKupershtokh how did you generate the cert? It looks like you need to fix add a SAN that is the kubernetes service's cluster IP.\n. @AlexeyKupershtokh see how we generate the cert to include the service IP?\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/master/cluster/gce/util.sh#L550\n. Please see doc on configuring kubernetes source: https://github.com/kubernetes/heapster/blob/master/docs/source-configuration.md#kubernetes\nLet me know if it needs clarification.\n. can you post a link to the heapster-controller.yaml\n. @y00181991 can you add\nvolumeMounts: \n            - name: ssl-certs\n              mountPath: /etc/ssl/certs\n              readOnly: true\n      volumes: \n        - name: ssl-certs\n          hostPath: \n            path: \"/etc/ssl/certs\"\nTo the end of your yaml?\n. It seems like a bug then if insecure=true used to work. Are you using --root-ca-file on the kube-controller-manager? It should point to a root CA signed for the apiserver's cert\n. Should be fixed by https://github.com/GoogleCloudPlatform/heapster/pull/445?\n. I've separated automated import rewrite from manual build fix into separate commits intentionally. This is recommended by the guideline here. I think it makes sense not to squash.\n. All green again.\n. P.S., this will require people to update their gopath layout.\nmkdir -p $GOPATH/src/k8s.io\nmv $GOPATH/src/github.com/GoogleCloudPlatform/heapster $GOPATH/src/k8s.io\n. Yes, I can add that. I've also sent out an announcement on kubernetes-dev, google-containers and kubernetes-announce.\nhttps://groups.google.com/forum/#!topic/kubernetes-announce/d5Yq_y4hbac\n. but does it break dns-search paths? @thockin what should be aware of when switching to alpine? is this an ok usage?\n. LGTM.\n. #492 is required but we should also use heapster's vanity domain when we go get:\ngo get k8s.io/kubernetes/heapster\n. @mwielgus 's command in this issue does a go get github.com/GoogleCloudPlatform/heapster\n. Yes. I left the rest of makeKubeConfig in for backwards compatibility, but eventually we should be able to replace all of it with kube_client.InClusterConfig() or a kubeconfig file without the extra decoration that makeKubeConfig() and getConfigOverrides() currently does.\n. Ok. I can do that. Service accounts are on by default but the tls crt is only distributed if the optional --root-ca-cert flag is set on the kube-controller-manager. Is that okay?\n. ",
    "Fandekasp": "Hi, I've just done this: my cloud-config drops the cadvisor.manifest in /etc/kubernetes/manifests/, and I run my kubelet with --config=/etc/kubernetes/manifests, so now all nodes have k8s containers for cadvisor.\nBut I have a few issues:\n- cadvisor isn't listed by kubecfg. How can I get kubecfg to list a pod for the running cadvisor k8s?\n- I can only get a couple of graphs working:\n  \nHere is the head of the docker logs: http://sprunge.us/eLeB\n  I'm running kubernetes  v0.5.3 with the default cadvisor.manifest \n  And I can curl localhost:10250/stats/  from a minion: http://sprunge.us/QiJL\n  Any idea what could be wrong ?\n. > Here is the head of the docker logs: http://sprunge.us/eLeB\nThose where the heapster logs. As you can see, it raises errors like invalid character 'i' in literal false (expecting 'l')\n. @vishh I migrated to v0.6.2 yesterday before answering you. Sorry, but I had the same issue afterwards.\nPlease note that I'm also running heapster from the kubernetes/heapster image, which was updated 3 days ago.\n. @vishh yes, I use CoreOS which runs systemd. \nHere is what I see when querying a minion cadvisor UI:\n\nThe \"Dockers containers\" link fails with error:\nFailed to get container \"\" with error: file not found\nHere is a docker ps from the same minion:\ncore@ip-10-7-169-214 ~ $ docker ps\nCONTAINER ID        IMAGE                                 COMMAND                CREATED             STATUS              PORTS                                                                                                                                        NAMES\n682e3ab7ccbb        kubernetes/pause:go                   \"/pause\"               3 hours ago         Up 2 hours                                                                                                                                                       k8s_net.c1939112_93244b2e-84e6-11e4-a3f6-22000b3b0f35.default.etcd_93244b2e-84e6-11e4-a3f6-22000b3b0f35_1a86a225        \na0dc7c345ac4        kubernetes/pause:go                   \"/pause\"               3 hours ago         Up 3 hours                                                                                                                                                       k8s_net.c1939112_93241766-84e6-11e4-a3f6-22000b3b0f35.default.etcd_93241766-84e6-11e4-a3f6-22000b3b0f35_f4069c3e        \nd85578a12e09        dockerfile/elasticsearch:latest       \"/elasticsearch/bin/   3 hours ago         Up 3 hours                                                                                                                                                       k8s_elasticsearch.1c3ec218_influx-grafana.default.etcd_8e1eb823-84e4-11e4-a3f6-22000b3b0f35_0a346927                    \ncc4c2eaa89d7        kubernetes/heapster_grafana:canary    \"/run.sh\"              3 hours ago         Up 3 hours                                                                                                                                                       k8s_grafana.24b8c47e_influx-grafana.default.etcd_8e1eb823-84e4-11e4-a3f6-22000b3b0f35_b5de211e                          \nc04d8836bcd8        kubernetes/kibana:latest              \"/usr/local/bin/run_   3 hours ago         Up 3 hours                                                                                                                                                       k8s_kibana-image.8c725922_elasticsearch-kibana-pod.default.etcd_9a83e37c-84e4-11e4-a3f6-22000b3b0f35_17a9b9ba           \n2eb27a4adfaa        dockerfile/elasticsearch:latest       \"/elasticsearch/bin/   3 hours ago         Up 3 hours                                                                                                                                                       k8s_elasticsearch.7c027b5c_elasticsearch-kibana-pod.default.etcd_9a83e37c-84e4-11e4-a3f6-22000b3b0f35_4fdad99f          \nab696dd08569        kubernetes/heapster:0.4               \"/run.sh\"              3 hours ago         Up 3 hours                                                                                                                                                       k8s_heapster.e68e2043_heapster.default.etcd_966b51d7-84e4-11e4-a3f6-22000b3b0f35_ac8c8d96                               \n89c4d850ef2a        kubernetes/heapster_influxdb:latest   \"/run.sh\"              3 hours ago         Up 3 hours                                                                                                                                                       k8s_influxdb.e38ff5e1_influx-grafana.default.etcd_8e1eb823-84e4-11e4-a3f6-22000b3b0f35_8e93306d                         \n43ca1e525331        google/cadvisor:0.6.2                 \"/usr/bin/cadvisor\"    3 hours ago         Up 3 hours                                                                                                                                                       k8s_cadvisor.b0dae998_cadvisormanifes12uqn2ohido76855gdecd9roadm7l0.default.file_cadvisormanifes12uqn2ohido76855gdecd9roadm7l0_02aede60\nfc5af8dd1ef6        kubernetes/pause:go                   \"/pause\"               3 hours ago         Up 3 hours                                                                                                                                                       k8s_net.1df2cc77_elasticsearch-kibana-pod.default.etcd_9a83e37c-84e4-11e4-a3f6-22000b3b0f35_f8c9e497                    \n2e94a4a21041        kubernetes/pause:go                   \"/pause\"               3 hours ago         Up 3 hours                                                                                                                                                       k8s_net.dbcb7509_heapster.default.etcd_966b51d7-84e4-11e4-a3f6-22000b3b0f35_4a78a857                                    \nba2fd96a81ed        kubernetes/pause:go                   \"/pause\"               3 hours ago         Up 3 hours          0.0.0.0:80->80/tcp, 0.0.0.0:8083->8083/tcp, 0.0.0.0:8086->8086/tcp, 0.0.0.0:8090->8090/tcp, 0.0.0.0:8099->8099/tcp, 0.0.0.0:9200->9200/tcp   k8s_net.380d25a7_influx-grafana.default.etcd_8e1eb823-84e4-11e4-a3f6-22000b3b0f35_f82a1af9                              \n19e472b7b32b        kubernetes/pause:go                   \"/pause\"               3 hours ago         Up 3 hours          0.0.0.0:4194->8080/tcp                                                                                                                       k8s_net.a0f18f6e_cadvisormanifes12uqn2ohido76855gdecd9roadm7l0.default.file_cadvisormanifes12uqn2ohido76855gdecd9roadm7l0_b589f27f\n31002c02f0ce        kubernetes/pause:go                   \"/pause\"               3 hours ago         Up 3 hours                                                                                                                                                       k8s_net.dbcb7509_fluentdesmanife2u464h05heqcpotoddodpnehjaqsde.default.file_fluentdesmanife2u464h05heqcpotoddodpnehjaqsde_3e3e3cbd\ncore@ip-10-7-169-214 ~ $\nWhen querying the minion kubelet stats API from the master, I get this: http://sprunge.us/BRgi\nAs you can see at the bottom, the /stats/heapster/heapster query fails:\ncore@ip-10-164-151-207 ~ $ curl http://10.7.169.214:10250/stats/heapster/heapster\nInternal Error: unable to unmarshal container info for \"/docker/ab696dd085698004b4b79cbe7e5ffb726c74daed31ca3c8d8d8fa11412181d0a\" (failed to get container \"/docker/ab696dd085698004b4b79cbe7e5ffb726c74daed31ca3c8d8d8fa11412181d0a\" with error: unknown container \"/docker/ab696dd085698004b4b79cbe7e5ffb726c74daed31ca3c8d8d8fa11412181d0a\"\n): invalid character 'i' in literal false (expecting 'l')\nI wonder why cadvisor isn't able to retrieve the containers information. It has a host volume for /var/lib/docker.\n. Great, thank you guys\n. In order to write a script that will upgrade cadvisor on a running cluster, I manually tried the following:\n* ssh in each minions\n$ cd /etc/kubernetes/manifests && sudo rm cadvisor.manifest && sudo wget https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/v0.7.0/cluster/saltbase/salt/cadvisor/cadvisor.manifest\n$ sudo pkill kubelet\n$ docker rm -f <id for k8s_cadvisor.b0dae998_cadvisormanifes...>\n$ docker rm -f <id for k8s_net.a0f18f6e_cadvisormanifes...>\n$ sudo /opt/bin/kubelet --address=0.0.0.0 --port=10250 --config=/etc/kubernetes/manifests --hostname_override=<ip> --etcd_servers=http://127.0.0.1:4001 --logtostderr=true\nBut this doesn't seem to fix the issue:\ncore@ip-10-165-74-145 ~$ curl http://localhost:4194/docker/\nFailed to get container \"\" with error: unable to find data for container /system.slice/docker-e9561984c4282af68df8fd228bb45b8a6c48794e89e2e0669b7abc1edc1d44a8.scope\ncore@ip-10-165-74-145 ~ $ docker ps | grep cadvisor\n100a930ef5bc        google/cadvisor:0.7.0     \"/usr/bin/cadvisor\"   8 minutes ago       Up 8 minutes                                 k8s_cadvisor.b0dae998_cadvisormanifes12uqn2ohido76855gdecd9roadm7l0.default.file_cadvisormanifes12uqn2ohido76855gdecd9roadm7l0_cb16d227\n73c9809d414a        kubernetes/pause:go       \"/pause\"              17 hours ago        Up 17 hours         0.0.0.0:4194->8080/tcp   k8s_net.a0f18f6e_cadvisormanifes12uqn2ohido76855gdecd9roadm7l0.default.file_cadvisormanifes12uqn2ohido76855gdecd9roadm7l0_1fb596ea\ncore@ip-10-165-74-145 ~ $\nThe docker logs of those containers are empty.\nAlso note that the cadvisor.manifest uses the docker image google/cadvisor:latest, when I would expect it to use google/cadvisor:v0.7.0  (I believe cadvisor is following the same versions as kubernetes).\nSince I only upgraded cadvisor in the operation, should I also upgrade kubernetes from v0.6.2 to v0.7.0 in order to fix that issue? Please let me know if I missed any step, thank you\n. @vishh Seems to work well now, thank you !\n\n. I'm also having issues with heapster, all graphs returning  InfluxDB Error : Clouldn't look up columns\nHere is my output: http://sprunge.us/TiiS?sh\nI'm only getting Moved Permanently or Empty reply from server. Do you have any idea why? Where should I investigate?\nThank you in advance\n. I'm running the kubernetes/heapster image, which was updated yesterday. And I use binaries from kubernetes v0.5.3 (running kubernetes from head never worked properly for me). Should I use a previous release of heapster compatible with kubernetes v0.5.3 ? I can see tags 0.1, 0.2 and 0.4 available besides latest.\nHere is the head of heaspter logs: http://sprunge.us/bPJZ?console\n. I've partially fixed this issue (I can only see cluster logs), and moved my discussion to #15. Unsuscribing from this ticket\n. ",
    "nlamirault": "Ok i will try with the new release. \n. Same errors. I use the kubernetes configuration in the deploy directory. How can i see heapster logs ?\n. Thanks. The container logs : \nbash\n$ docker logs 839369e62c08\nDetected Kube specific args. Starting in Kube mode.\nI1204 10:05:24.006500 00006 heapster.go:18] /usr/bin/heapster --kubernetes_master 10.0.0.148:80 --sink influxdb --sink_influxdb_host 10.0.0.190:8085\nI1204 10:05:24.007621 00006 heapster.go:19] Heapster version 0.4\nI1204 10:05:24.201017 00006 influxdb.go:180] Database creation failed - Server returned (409): database k8s exists\nE1204 10:05:34.381961 00006 heapster.go:22] Get http://10.0.0.148:80/api/v1beta1/pods?labels=&namespace=default: net/http: transport closed before response was received\n. This is output logs of some container  \n``` bash\n$ docker ps\nCONTAINER ID        IMAGE                                 COMMAND                CREATED             STATUS              PORTS                                                                                                                                        NAMES\ne7033172a1fc        dockerfile/elasticsearch:latest       \"/elasticsearch/bin/   4 days ago          Up 4 days                                                                                                                                                        k8s_elasticsearch.7b8bd84_influx-grafana.default.etcd_89c5fea9-7ad9-11e4-858c-0800275fbc80_c72879f1 \n17825c3b6332        kubernetes/heapster_grafana:canary    \"/run.sh\"              4 days ago          Up 4 days                                                                                                                                                        k8s_grafana.e02bbfea_influx-grafana.default.etcd_89c5fea9-7ad9-11e4-858c-0800275fbc80_cc7cc951      \n9187b792e127        kubernetes/heapster_influxdb:latest   \"/run.sh\"              4 days ago          Up 4 days                                                                                                                                                        k8s_influxdb.c319f14d_influx-grafana.default.etcd_89c5fea9-7ad9-11e4-858c-0800275fbc80_05d606c2     \n755dcef95d77        kubernetes/pause:go                   \"/pause\"               4 days ago          Up 4 days           0.0.0.0:80->80/tcp, 0.0.0.0:8083->8083/tcp, 0.0.0.0:8086->8086/tcp, 0.0.0.0:8090->8090/tcp, 0.0.0.0:8099->8099/tcp, 0.0.0.0:9200->9200/tcp   k8s_net.380d25a7_influx-grafana.default.etcd_89c5fea9-7ad9-11e4-858c-0800275fbc80_e71f6313          \nad571e1dadd0        google/cadvisor:0.6.2                 \"/usr/bin/cadvisor -   4 days ago          Up 4 days           8080/tcp, 0.0.0.0:4194->4194/tcp                                                                                                             cadvisor                                                                      \n$ docker logs 9187b792e127\n=> About to create the following database: k8s\n=> Starting InfluxDB ...\n=> Waiting for confirmation of InfluxDB service startup ...\n+---------------------------------------------+\n|  _  _            _    |\n| | |      / _| |          |   \\|  _ \\  |\n|   | |  _  | || |  | |  | | |) | |\n|   | | | ' \\|  | | | | \\ \\/ / |  | |  _ <  |\n|  | || | | | | | | || |>  <| || | |_) | |\n| |_|| ||| ||_,/_/__/|____/  |\n+---------------------------------------------+\n[12/03/14 10:47:13] [INFO] Loading configuration file /config/config.toml\n=> Waiting for confirmation of InfluxDB service startup ...\n{\"status\":\"ok\"}\n=> Creating database: k8s\nexec /usr/bin/influxdb -config=${CONFIG_FILE}\n$ docker logs 17825c3b6332\n=> Creating basic auth for \" admin\" user with preset password\nAdding password for user admin\n=> Done!\n========================================================================\nYou can now connect to Grafana with the following credential:\nadmin:admin\n\n========================================================================\n=> Configuring InfluxDB\n=> InfluxDB has been configured as follows:\n   InfluxDB ADDRESS:  \"+window.location.hostname+\"\n   InfluxDB PORT:     8086\n   InfluxDB DB NAME:  k8s\n   InfluxDB USERNAME: root\n   InfluxDB PASSWORD: root\n    Please check your environment variables if you find something is misconfigured. \n=> Done!\n=> Found Elasticsearch settings.\n=> Set Elasticsearch url to \"http://\"+window.location.hostname+\":9200\".\n=> Done!\n=>Setting default dashboard to \\/dashboard\\/file\\/kubernetes.json\n=>Done\n=> Starting and running Nginx...\n$ docker logs e7033172a1fc\n[2014-12-03 10:53:41,200][INFO ][node                     ] [Dragonwing] version[1.4.0], pid[1], build[bc94bd8/2014-11-05T14:26:12Z]\n[2014-12-03 10:53:41,205][INFO ][node                     ] [Dragonwing] initializing ...\n[2014-12-03 10:53:41,228][INFO ][plugins                  ] [Dragonwing] loaded [], sites []\n[2014-12-03 10:53:48,572][INFO ][node                     ] [Dragonwing] initialized\n[2014-12-03 10:53:48,576][INFO ][node                     ] [Dragonwing] starting ...\n[2014-12-03 10:53:48,750][INFO ][transport                ] [Dragonwing] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.100.8.3:9300]}\n[2014-12-03 10:53:48,821][INFO ][discovery                ] [Dragonwing] elasticsearch/Ec_2wTKkQ56MNS0DhTPvgw\n[2014-12-03 10:53:52,935][INFO ][cluster.service          ] [Dragonwing] new_master [Dragonwing][Ec_2wTKkQ56MNS0DhTPvgw][influx-grafana][inet[/10.100.8.3:9300]], reason: zen-disco-join (elected_as_master)\n[2014-12-03 10:53:53,001][INFO ][http                     ] [Dragonwing] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.100.8.3:9200]}\n[2014-12-03 10:53:53,003][INFO ][node                     ] [Dragonwing] started\n[2014-12-03 10:53:53,047][INFO ][gateway                  ] [Dragonwing] recovered [0] indices into cluster_state\n$ \n```\n. I had to recreate the cluster. \nbash\ncore@minion-2 $ docker ps   \nCONTAINER ID        IMAGE                     COMMAND                CREATED             STATUS              PORTS                              NAMES\n03bfecae3fc0        kubernetes/heapster:0.4   \"/run.sh\"              9 seconds ago       Up 8 seconds                                           k8s_portefaix-heapster.4701ac2_portefaix-heapster-pod.default.etcd_41bb2e71-7f80-11e4-9c0c-0800275fbc80_6445f2c0   \n28d5198dcd86        kubernetes/pause:go       \"/pause\"               15 minutes ago      Up 15 minutes                                          k8s_net.dbcb7509_portefaix-heapster-pod.default.etcd_41bb2e71-7f80-11e4-9c0c-0800275fbc80_5d569f21                 \n7a629ddf5576        google/cadvisor:0.6.2     \"/usr/bin/cadvisor -   31 minutes ago      Up 31 minutes       8080/tcp, 0.0.0.0:4194->4194/tcp   cadvisor                                                                                                           \ncore@localhost ~ $ docker logs 03bfecae3fc0\nDetected Kube specific args. Starting in Kube mode.\nI1209 09:05:08.200574 00006 heapster.go:18] /usr/bin/heapster --kubernetes_master 10.0.0.227:80 --sink influxdb --sink_influxdb_host 10.0.0.5:8085\nI1209 09:05:08.201711 00006 heapster.go:19] Heapster version 0.4\nI1209 09:05:08.308501 00006 influxdb.go:180] Database creation failed - Post http://10.0.0.5:8085/db?u=root&p=root: net/http: transport closed before response was received\nE1209 09:05:18.417711 00006 heapster.go:22] Get http://10.0.0.227:80/api/v1beta1/pods?labels=&namespace=default: net/http: transport closed before response was received\nbash\ncore@minion-2 $ ping 10.0.0.5\nPING 10.0.0.5 (10.0.0.5) 56(84) bytes of data.\nFrom 10.133.0.253: icmp_seq=1 Destination Host Unreachable\nFrom 10.133.0.253: icmp_seq=3 Destination Host Unreachable\n^C\n--- 10.0.0.5 ping statistics ---\n3 packets transmitted, 0 received, +2 errors, 100% packet loss, time 2001ms\nSame thing if i try to find the master on 10.0.0.227 : Destination Host Unreachable\nan on the other Pod : \nbash\ncore@minion-1 $ docker ps\nCONTAINER ID        IMAGE                                 COMMAND                CREATED             STATUS              PORTS                                                                                                                                                                NAMES\ne500b2bb14df        dockerfile/elasticsearch:latest       \"/elasticsearch/bin/   22 minutes ago      Up 22 minutes                                                                                                                                                                            k8s_elasticsearch.d62bbe30_influx-grafana.default.etcd_550b08b6-7f7e-11e4-9c0c-0800275fbc80_6596fee2   \n6f0c7d4db0c8        kubernetes/heapster_grafana:canary    \"/run.sh\"              25 minutes ago      Up 25 minutes                                                                                                                                                                            k8s_grafana.eba1bffa_influx-grafana.default.etcd_550b08b6-7f7e-11e4-9c0c-0800275fbc80_cbe7a617         \nbbba1b54f60a        kubernetes/heapster_influxdb:latest   \"/run.sh\"              28 minutes ago      Up 28 minutes                                                                                                                                                                            k8s_influxdb.ce8ff15d_influx-grafana.default.etcd_550b08b6-7f7e-11e4-9c0c-0800275fbc80_e7dd9dee        \n49c1c7ce010a        kubernetes/pause:go                   \"/pause\"               30 minutes ago      Up 30 minutes       0.0.0.0:80->80/tcp, 0.0.0.0:8083->8083/tcp, 0.0.0.0:8086->8086/tcp, 0.0.0.0:8090->8090/tcp, 0.0.0.0:8099->8099/tcp, 0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   k8s_net.3b912643_influx-grafana.default.etcd_550b08b6-7f7e-11e4-9c0c-0800275fbc80_518a8709             \ndeb94159bb8a        google/cadvisor:0.6.2                 \"/usr/bin/cadvisor -   34 minutes ago      Up 33 minutes       8080/tcp, 0.0.0.0:4194->4194/tcp                                                                                                                                     cadvisor\nthe ES container logs : \n``` bash\ncore@minion-1 $ docker logs e500b2bb14df\n[2014-12-09 08:43:43,666][INFO ][node                     ] [Random] version[1.4.0], pid[1], build[bc94bd8/2014-11-05T14:26:12Z]\n[2014-12-09 08:43:43,669][INFO ][node                     ] [Random] initializing ...\n[2014-12-09 08:43:43,683][INFO ][plugins                  ] [Random] loaded [], sites []\n[2014-12-09 08:43:51,080][INFO ][node                     ] [Random] initialized\n[2014-12-09 08:43:51,081][INFO ][node                     ] [Random] starting ...\n[2014-12-09 08:43:51,244][INFO ][transport                ] [Random] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.100.14.3:9300]}\n[2014-12-09 08:43:51,308][INFO ][discovery                ] [Random] elasticsearch/T5EkabxpT_G-GcMDtJLOmQ\n[2014-12-09 08:43:55,453][INFO ][cluster.service          ] [Random] new_master [Random][T5EkabxpT_G-GcMDtJLOmQ][influx-grafana][inet[/10.100.14.3:9300]], reason: zen-disco-join (elected_as_master)\n[2014-12-09 08:43:55,511][INFO ][http                     ] [Random] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.100.14.3:9200]}\n[2014-12-09 08:43:55,515][INFO ][node                     ] [Random] started\n[2014-12-09 08:43:55,543][INFO ][gateway                  ] [Random] recovered [0] indices into cluster_state\n```\nThe Grafana logs : \n``` bash\ncore@minion-1 $ docker logs 6f0c7d4db0c8\n=> Creating basic auth for \" admin\" user with preset password\nAdding password for user admin\n=> Done!\n========================================================================\nYou can now connect to Grafana with the following credential:\nadmin:admin\n\n========================================================================\n=> Configuring InfluxDB\n=> InfluxDB has been configured as follows:\n   InfluxDB ADDRESS:  \"+window.location.hostname+\"\n   InfluxDB PORT:     8086\n   InfluxDB DB NAME:  k8s\n   InfluxDB USERNAME: root\n   InfluxDB PASSWORD: root\n    Please check your environment variables if you find something is misconfigured. \n=> Done!\n=> Found Elasticsearch settings.\n=> Set Elasticsearch url to \"http://\"+window.location.hostname+\":9200\".\n=> Done!\n=>Setting default dashboard to \\/dashboard\\/file\\/kubernetes.json\n=>Done\n=> Starting and running Nginx...\n```\nThe InfluxDB logs : \n``` bash\ncore@minion-1 $ docker logs bbba1b54f60a\n=> About to create the following database: k8s\n=> Starting InfluxDB ...\n=> Waiting for confirmation of InfluxDB service startup ...\n+---------------------------------------------+\n|  _  _            _    |\n| | |      / _| |          |   \\|  _ \\  |\n|   | |  _  | || |  | |  | | |) | |\n|   | | | ' \\|  | | | | \\ \\/ / |  | |  _ <  |\n|  | || | | | | | | || |>  <| || | |_) | |\n| |_|| ||| ||_,/_/__/|____/  |\n+---------------------------------------------+\n[12/09/14 08:37:07] [INFO] Loading configuration file /config/config.toml\n=> Waiting for confirmation of InfluxDB service startup ...\n{\"status\":\"ok\"}\n=> Creating database: k8s\nexec /usr/bin/influxdb -config=${CONFIG_FILE}\n```\n. No.\nbash\ncore@minion-1 ~ $ ping 10.0.0.227\nPING 10.0.0.227 (10.0.0.227) 56(84) bytes of data.\nFrom 10.133.0.253: icmp_seq=1 Destination Host Unreachable\nFrom 10.133.0.253: icmp_seq=2 Destination Host Unreachable\nbash\ncore@minion-2 ~ $ ping 10.0.0.227\nPING 10.0.0.227 (10.0.0.227) 56(84) bytes of data.\nFrom 10.133.0.253: icmp_seq=1 Destination Host Unreachable\nFrom 10.133.0.253: icmp_seq=2 Destination Host Unreachable\n. I use Virtualbox/CoreOS/Docker/Flannel. Which tests can i perform to check Kubernetes network configuration ?\n. do you know how minions retrieve the master IP (10.0.0.227) ?\n. @vishh : not really. I will destroy the cluster and create it again to watch etcd logs.\n. ",
    "mwielgus": "Obsolete.\n. We track running pods.\n. Obsolete.\n. done\n. CoreOS support is currently suspended.\n. done\n. support for non-kubernetes deployments is currently suspended.\n. /cc\n. Kafka sink is avaiable for metrics.\n. Possible with custom metrics. Support in Heapster is done.\n. There should be no volatile configuration changes. Cycling heapster with new configuration is much easier today as the internal state is relatively small. \n. Obsolete\n. Obsolete, retries were added.\n. Obsolete\n. GCM sink was rewritten.\n. Done.\n. Done.\n. Obsolete.\n. Obsolete. Eventer handles events in 0.20.\n. cc: @piosz \n. cc: @piosz \n. @afein Thanks, much better with this flag:\ncurl 10.244.3.6:8082/api/v1/model/metrics/ | gunzip\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   146  100   146    0     0  19891      0 --:--:-- --:--:-- --:--:-- 24333\n[\n  \"memory-limit\",\n  \"fs-limit-dev-sdb\",\n  \"memory-usage\",\n  \"fs-limit-dev-disk-by-uuid-3e07d02c-c373-4daf-ac26-d29550f95545\",\n  \"cpu-limit\",\n  \"fs-usage-dev-sdb\",\n  \"memory-working\",\n  \"fs-usage-dev-disk-by-uuid-3e07d02c-c373-4daf-ac26-d29550f95545\",\n  \"cpu-usage\"\n ]\n. cc: @vishh\n. Ping :)\n. Completed (for now) with #518 #522 #527\n. #508 \n. I can take a look.\n. First test: #553 \n. Custom build from head ~1 weak old.\n. Fixed with #494\n. #490\n. @korczis What Heapster/influxdb version?\n. All checks are green - can we merge this PR?\n. LGTM\n. Fixed with #522\n. This can be postponed.\n. For now it is synced with cache:\n527 #518 #522\n. In updatePod (#508):\ngo\n    // Get Pod pointer\n    pod_ptr := rc.addPod(pod.Name, pod.UID, namespace, node)\nDoes pod.Name contain in fact namespce + name?\n. I will have the fix in PR for #513 (which is based/depends on #508).\n. Fixed in #522\n. cc: @piosz  @jszczepkowski\n. Unfortunately, the cache is used to populate the model storage so right now it is in fact a source/transmitter of pod lifecycle events. \n. cc: @vishh @piosz @jszczepkowski\n. One of the tests is flaky - see: #523\n. Fixed in #532\n. Obsolete.\n. cc: @vishh @piosz @jszczepkowski\n. LGTM\n. Failing test is flaky, merging.\n. Apparently some other, this time related test is failing too:\n```\n--- FAIL: TestSyncLastUpdated (3.47s)\n        Location:       external_test.go:191\n    Error:      Not equal: time.Time{sec:0, nsec:0, loc:(time.Location)(nil)} (expected)\n                    != time.Time{sec:63577396626, nsec:894450989, loc:(time.Location)(0xb90f00)} (actual)\n    Messages:   now: 2015-09-09 13:48:06.894450989 +0200 CEST, expected: 0001-01-01 00:00:00 +0000 UTC, actual: 2015-09-09 13:57:06.894450989 +0200 CEST\nFAIL\n```\n. cc: @vishh \n. cc: @vishh @piosz \n. Yep, you are correct. I blindly copied sleep from the below where it is used just after sync. All sleeps (new and the old one) can be removed as there is a sync after starting goroutines.\n. #534 removes the sleeps.\n. @huangyuqi. Than you for the PR!. Overall it looks good. Apart from the comments above there is a few minor grammar and formatting errors but I think the fastest way to fix them would be if I (or @vishh) correct them in a separate PR.\n. Can you squash the commits into 1?\n. Travis problems are not relevant to the PR - created an issue to track it - #537. \nThe rest looks fine - merging.\n. Please squash the commits.\n. LGTM\n. Adding some debugs in #558 so the executed steps are printed and it is possible to tell which of the sub-tests failed.\n. Added even more debugs in #562. Closing for now.\n. Shouldn't we rather fix it in Kubernetes?\n. Actually events do have UID set. I generated the similar event and it looks OK. The above is not also lacking UID but also creationTimestamp - I would suggest dropping broken events or giving them heapster generated uid - if uid & creation time is broken why should we assume that the name is fine?\njson\n        {\n            \"kind\": \"Event\",\n            \"apiVersion\": \"v1\",\n            \"metadata\": {\n                \"name\": \"cassandra.1404cab927c4af5f\",\n                \"namespace\": \"default\",\n                \"selfLink\": \"/api/v1/namespaces/default/events/cassandra.1404cab927c4af5f\",\n                \"uid\": \"b014a366-5d49-11e5-bdca-42010af07fef\",\n                \"resourceVersion\": \"120398\",\n                \"creationTimestamp\": \"2015-09-17T14:37:57Z\",\n                \"deletionTimestamp\": \"2015-09-17T15:37:57Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Pod\",\n                \"namespace\": \"default\",\n                \"name\": \"cassandra\",\n                \"uid\": \"b00af05b-5d49-11e5-bdca-42010af07fef\",\n                \"apiVersion\": \"v1\",\n                \"resourceVersion\": \"120395\",\n                \"fieldPath\": \"implicitly required container POD\"\n            },\n            \"reason\": \"pulled\",\n            \"message\": \"Container image \\\"gcr.io/google_containers/pause:0.8.0\\\" already present on machine\",\n            \"source\": {\n                \"component\": \"kubelet\",\n                \"host\": \"kubernetes-minion-knk4\"\n            },\n            \"firstTimestamp\": \"2015-09-17T14:37:57Z\",\n            \"lastTimestamp\": \"2015-09-17T14:37:57Z\",\n            \"count\": 1\n        },\n. Do you know who is assigning the uids and how it could fail?\n. Fixed in #563\n. All green, merging.\n. cc: @vishh @afein @piosz @jszczepkowski\n. It seems there is no big conflict if Kubernetes is not used. #536 shows Heapster in a standalone mode using model API. \n. cc: @vishh @piosz \n. Fixes #487\n. Probably fixed with #563\n. Probably fixed.\n. In Jenkins, Heapster service has repeating problems with finding a matching pod (in the namespace) what is quite odd as it works perfectly fine on my computer/cluster.\n. I bumped the version to 1.0.5. Integration tests pass (travis failed because of flaky test - filled an issue for that #565). I will start yet another test run to confirm that we are good now.\n. Getting rid of system-like stuff from the configs and changing the labels a bit helped. All tests pass. Merging\n. obsolete\n. LGTM\n. +1, I can do it on Monday. Kube-dns is not (or at least was not) obligatory so I would rather stick to the flag.\n. LGTM, all PRs are already merged to master, merging.\n. Can you provide more context? I don't understand how this issue is related to Heapster.\n. Obsolete.\n. Personally, I would rather not include cAdvisor structs in Heapster api. What is exactly needed? Maybe we could expose the required data through some generic api?\n. cc: @piosz @fgrzadkowski @jszczepkowski\n. Personally I would double check whether we are not writing too much data to influxdb or doing it in the suboptimal way. From a brief & tired look at how the influxdb sink is implemented I would suspect that we are are writing samples with the default 5 sec resolution. What about changing it to 30 sec (not perfect but better than switching off infuxdb)? \nOther idea is to play with how batches are constructed and sink frequency - if the underlying storage is key-value like it might be beneficial to write data less often than every 10s so that more data is written under a single key in one batch and the key lookup/disk seeks happen less often.\nIn influxdb 0.8 there seem to be pluggable backends which deeply differ in terms of write performance https://influxdb.com/blog/2014/06/20/leveldb_vs_rocksdb_vs_hyperleveldb_vs_lmdb_performance.html\nwith HyperLevelDB appearing to be the best choice for write-intensive workloads. Did we try it?\n. I personally don't care whether we use Influxdb or some other time series database like GCM. I just want to have some kind of permanent storage (cloud-provider permitting) for metrics. And I'm convinced that we should not write another one as there is lots of other, more important challenges in K8s. If influxdb is not the best choice for big customers - lets change it in 1.2 and for now provide an extra flag in kube-up to run a heapster without any storage. \nAnd if there are performance issues with Influxdb 0.8, and we cannot upgrade to 0.9 (for K8s 1.1), lets try some simple workarounds first (increasing resolution to 30 sec will decrease the load 6x) so it is usable for 80-90% of customers with small/moderate clusters before dropping it completely.\n. After a discussion with @piosz we agreed that 30 sec resolution would be even more handy for him. We also took a look at the metrics that are exported - there are 16 but InitialResources need only 6 of them (but network may also be handy so lets count 8). So if we added a flag to control what metrics are propagated to influxdb we could reduce the load even more.\nAssuming the K8S 1.0 scalability target of 100 nodes with 30 pods each we have:\n(100 nodes * 30 pods/node * 8 metrics/pod) / 30 sec resolution = 800 data points per second. \n. LGTM\n. ok to test\n. cc: @fgrzadkowski @piosz @wojtek-t @gmarek\n. Fixed in heapster-scalability.\n. cc: @fgrzadkowski @piosz @gmarek @wojtek-t\n. This, combined with #637, means that we transfer up to 60x more data than we actually need.\n. cc: @piosz\n. Sinks are fed with timeseries. Model storage does some processing on its own but it is mainly targeted at fighting with cumulative cpu usage. \nI think it is bad. The metrics should come from sources/cache in one common way that doesn't require any extra processing to make them usable.\n. I have a strong feeling that the responsibility and isolation between sources, sinks, and data-passing layer should be drawn more clearly. Sources should produce ready-to-use container metrics. Another layer should group the container data into pods, namespaces, nodes etc. Another calculate any derivative metrics that are needed (cummulative cpu -> average cpu). And the final layer: sinks and model api would pass it to the external data stores and expose the data to clients.\ncc: @jszczepkowski @fgrzadkowski \n. Fixed in heapster-scalability.\n. Obsolete.\n. cc: @vishh @piosz @jszczepkowski @fgrzadkowski \n. The only thing that is taken from pod - cpu/mem reservation can be added either to the api or to docker kubelet-generated labels, just like \"io.kubernetes.pod.terminationGracePeriod\" is right now. \n. Which metadata (that are not returned by cAdvisor/kubelet) are needed?\n. Fixed in heapster-scalability.\n. The target scope and vision documented in #769. Closing this for now.\n. LGTM\n. The code is +- complete in heapster-scalablity branch.\n. LGTM, thanks for writing our discussion down. Most likely we will change/move the core types around.\nI will take care of fixing the makefile.\n. Good idea. I will take a look.\n. related to: #665\n. Postponed until after 1.2\n. cc: @vishh \n. LGTM\n. Tests for the whole flow will come in the next PR.\n. ```\nk8s.io/heapster/sources\nsources/cadvisor_test.go:59: unknown datasource.Host field 'Resource' in struct literal\nsources/cadvisor_test.go:60: unknown datasource.Host field 'Resource' in struct literal\nsources/kube_events_test.go:35: undefined: body\nk8s.io/heapster/sources/datasource\nsources/datasource/kubelet_client_test.go:49: undefined: kubeletSource\nsources/datasource/kubelet_client_test.go:92: undefined: kubeletSource\nsources/datasource/kubelet_client_test.go:163: undefined: kubeletSource\n```\n. fixed.\n. @bluebreezecf Is there any particular reason why you had to write an opentsdb client on your own, from scratch? What are your plans regarding new versions, updates of the client, etc? \nPlease be aware that we will have to review the client library before allowing it in, as the project (https://github.com/bluebreezecf/opentsdb-goclient) has 0 forks and just 1 contributor.\n. LGTM, please squash the commits.\n. I will take a look.\n. Fixed with #780. \nThe fix is available in release v0.19.1.\n. LGTM\n. Yes, as mentioned in #684. We are changing lots of internals which cannot be done at once. Right now the status of the branch is working prototype but once tested and on par with the current version we hope to merge it with the master. We will soon send 2 proposals: one with the long term vision and one which describes the changes in details.\n. LGTM\n. LGTM\n. Thank you too :)\n. LGTM\n. LGTM\n. Fixed.\n. We can merge it to master if you feel it is needed for pre-heapster-scalability patch release. AFAIR heapster-scalability is already using 1.2-alpha-something.\n. does not compile\n. LGMT\n. LGTM\n. Can we somehow fix hostname instead of adding another label which will do more-or-less the same?\n. Can you make sure that this label doesn't go to GCM - we are already hitting the limit (10) of labels AFAIR.\n. Unfortunately there is no unit test\nFilter it in these places:\nhttps://github.com/kubernetes/heapster/blob/master/sinks/gcm/core.go#L254\nhttps://github.com/kubernetes/heapster/blob/master/sinks/gcm/core.go#L281\nhttps://github.com/kubernetes/heapster/blob/master/sinks/gcm/driver.go#L33\n. LGTM\n. LGTM\n. @vishh It is still not fixed. Moreover there should be retries in the sink.\nkubectl logs heapster-v10-o6n83 --namespace=kube-system\nI1211 14:33:56.148091       1 heapster.go:61] /heapster --source=kubernetes:'' --sink=influxdb:http://monitoring-influxdb:8086 --stats_resolution=30s --sink_frequency=1m\nI1211 14:33:56.148205       1 heapster.go:62] Heapster version 0.19.0\nI1211 14:33:56.149073       1 kube_factory.go:172] Using Kubernetes client with master \"https://10.0.0.1:443\" and version \"v1\"\nI1211 14:33:56.149088       1 kube_factory.go:173] Using kubelet port 10255\nE1211 14:34:16.220762       1 manager.go:257] encountered following errors while setting up sinks - failed to ping InfluxDB server at \"monitoring-influxdb:8086\" - Get http://monitoring-influxdb:8086/ping: dial tcp: lookup monitoring-influxdb: no such host\nI1211 14:34:16.223077       1 heapster.go:72] Starting heapster on port 8082\n. cc: @piosz \n. I'm fixing a bunch of issues in influxdb code. The fixing PR should come soon.\n. Fixed with #780. \nThe fix is available in release v0.19.1.\n. LGTM\n. LGTM\n. Build/test problems are not related to the PR.\n. re: kafka, thanks for the heads up.\n. Merging the proposal as is. Most of the proposal is already implemented in heapster-scalability branch. For other stuff I'm happy to set up different issues/document or have a vc. Please let me know if you feel a strong need to discuss a particular case.\n. Fixed\n. LGTM\n. Fixes #756\ncc: @piosz \n. I would rather submit this (tested) fix and then refactor the whole influxdb sink (+ClientInitializer a bit) and retest the whole thing.\n. sinks/influxdb/driver.go - there is no such file, something got mis-sync...\n. 1. Can you tell me the exact Heapster version (last git commit)?\n2. Can you share some more details about your deployment? Is it Amazon, GCE or some bare-metal cluster.\n3. Did Heapster stop writing the metrics completely? (No metrics for other pods as well)\n4. Did the node come back to life? If yes, did Heapster recover?\n. LGTM\n. LGTM\n. works, merging\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. cc: @piosz \n. Not sure if I understand the PR correctly. Can you explain why this change is needed?\n. Ok, I see your point. Although I'm not sure if we want to use an extra API to check for clock synchronization (there are lots of discussions about Kubelet metrics API and using an extra one doesn't help). Let me think about it. \nWhat about checking metric timestamps or lack of metrics in the existing Kubelet response processing? Would it work?\n. Closing as we are not adding feature to the old heapster codebase.\n. Yes, we are significantly refactoring Heapster. It was announced in #684.\nMore details on the future of Heapster can be also found here: #769 \n. Sure, any help will be more than welcome. I will try to produce a list of pending tasks Today but if I don't (Christmas is coming) you can start with porting Kafka and Riemann sinks to the new api.\n. LGTM\n. Please rebase the PR. Otherwise LGTM.\n. Something went wrong with the rebase. There are 3 commits that should not belong to this PR.\n. lgtm\n. Thank you for porting Kafka and Riemann.\n. @huangyuqi  I created couple issues and cc-ed you on them (#826, #827, #828). Feel free to pick the one you like (just add a comment if you want to start working on it).\n. ok to test\n. lgtm\n. I'm looking at it right now.\n. Fixed by @huangyuqi .\n. Fixed with #846.\n. cc: @piosz \nTest failure is justified. \n. There were \"utils\" at two levels - one at the \"global\" level and one in metrics and events. \n. The change is trivial and doesn't go to the master branch. Submitting as TO BE REVIEWED. @piosz @fgrzadkowski - please review at the earliest convenience. \n. Known e2e issue, merging.\n. LGTM, thank you for the PR\n. So:\n- We need the similar set of stats thing for Events/Eventer:\n  - last event timestamp\n  - last housekeep timestamp\n  - total number of events\n  - number of runs in the current houskeep interval\n  - time spent for scraping events\n  - time spent for writing events (total and on per sink basis, please add this to metrics as well)\n- We are moving close to code complete stage but some packages have poor test coverage. Please look around and check where tests can be added (I created an issue for Influxdb sink but there are probably other places that require tests as well)\n. Yeah, the problem seems to be unrelated. BTW, we will have to redo this work in heapster-scalability branch.\n. If the new API is delivered on time then yes we will switch, if not we will keep using the old one.\n. Merging to get e2e tests green. @piosz please review at earliest convenience. \n. This change goes to heapster-scalability branch, not master.\n. LGTM, thanks for the PR.\n. Sinks get all metricsets (containers as well as pod, namespace and cluster aggregates) and all data that are in metricsets. And of course they can write only a subset of data. Moreover, the exactly same data are available through the API (api is implemented as a sink). \n. LGTM\n. Well not really. I don't want to create another path for metrics to go within Heapster. So this should be rather handled by defining the Prometheus internal metrics as Custom Metrics and using the upcoming mechanism to export Custom Metrics from Kubelet. The same applies to storing internal metrics of other Kubernetes components, like scheduler.\n. Some strange jenkins flakiness - ignoring for now as it seems to be unrelated to the pr.\n. Thank you for the explanation. Allowing non-persistent configuration is just asking for troubles/outage. And crashes/restarts do happen. For that reasons we run Heapster under replication controller and not as a standalone Pod.\n. LGTM\nWe will merge the PR as soon as we switch Kubernetes HEAD to use new Heapster (alpha). Right now we are fixing bugs and don't want to introduce any new code. \n. Please fix the tests\n. LGTM\n. All comments addressed.\n. Merging to unblock further work as the 1.2 code complete deadline is near. @piosz please take a look - I will fix your concerns in the next PR.\n. Looks good, just one minor comment.\n. LGTM, thank you for the PR.\n. LGTM\n. All test pass (default is broken for other reason), merging to uncomment some other e2e tests, @piosz please review in the morning your time.\n. Merging to unblock tests in #887.\n. LGTM\n. LGTM\n. LGTM\n. You can start adding the new API as a separate source (leaving the existing one intact). If all elements are in (including the code that picks the right api based on kubelet version and CM) and we still have time to redo the tests then we will definitely consider using the new source in K8S 1.2.\n. Yes, please make it completely separate.\n. Log, Influxdb, Riemann and Kafka are done.\nHawkular, GCM are pending.\n. Does #922 fix the problem?\n. LGTM\n. Thanks for fixing this!\n. fixed with 918\n. I think i agree with @jimmidyson.  Just in case someone needs more data - can have a flag that disables status checking (set as default to false => pass only the running pods)?\n. Or to be more precise - enrich whatever is already there but don't create any missing/not running stuff unless the flag is set.\n. Ok, but it is possible that a pod is running but cadvisor doesn't know about it yet which will result in creating a half-empty metric set. Your PR doesn't handle that.\n. Can we please not have \"/\" in container name? It will create issues in the REST api as container name can be a part of the path.\n. OK, lets see what happens.\n. LGTM\n. Do you have an idea how these pods are created?\n. OK, lgtm\n. As requested in #907, please leave the default kublet source as is. We want to reduce the risk of introducing bugs/breaking heapster just before the launch during addition of the new api support. \n. cc: @piosz @fgrzadkowski \n. Please make a full copy of kubelet source (name it kubelet2) and make your changes there. I know that the code will be duplicated but only for some time and I accept it. Once we are done with the new api we will delete the old source (kubelet) and rename kubelet2.\n. @timstclair Ok, if these are the only changes to the old source. But move summary.go to a separate dir/package.\n. LGTM\n. do not merge yet (per chat with @timstclair )\n. all green, merging\n. metrics/processors/pod_based_enricher.go:55: possible formatting directive in Error call\nmake: *** [sanitize] Error 1\nBuild step 'Execute shell' marked build as failure\n. I'm not aware of any\n. LGTM\n. ref: #910\n. ref #910\n. Yep, year has to be updated in the boilerplate. Will do it tomorrow.\n. LGTM, will merge it tomorrow.\n. reopening to trigger travis\n. metrics/sinks/hawkular/driver.go:274: arg v[0] for printf verb %d of wrong type: string\nmetrics/sinks/hawkular/driver.go:282: arg v[0] for printf verb %d of wrong type: string\nmake: *** [sanitize] Error 1\n. LGTM\n. Nothing is tagged yet. We can add #930 and #940.\n. LGTM\n. Looks good except for the mapping removal. It has to remain there.\n. cc: @gmarek \n. LGTM\n. ok to test\n. LGTM\nThanks for the PR. Could you please send a similar PR for heapster-scalability branch (which will soon replace master)?\n. It was probably ported before the reliability code was added. I don't think it was removed on purpose. I can take a look on it tomorrow and try to sync the codebase.\n. Done\n. Heapster v0.20.0 (currently we have an alpha release candidate) will have OpenTSDB enabled.\n. LGTM\n. Hope we don't get any trojan horse in.\n. cc: @piosz \n. LGTM but we will have to reorganize managers and factories after 1.2.\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. @liggitt AFAIR this code has never been in heapster-scalability.\n. Ok sorry, I must have missed it during the review. Yes, definitely a test would be helpful.\n. LGTM\n. Heapster 0.18 doesn't work with Influxdb >= 0.9.\n. LGTM\n. lgtm\n. LGTM\n. @piosz Only cpu is converted, plus the code is prone for map size overflow. \nhttps://github.com/kubernetes/heapster/blob/heapster-scalability/metrics/sources/kubelet/kubelet.go#L218 \n. Unit test added\n. Added couple other changes required for this to work:\n- do not aggregate container cumulative metrics \n- print scrape time in logsink\n. LGTM\n. Yep should be Infof instead of Info. Will fix it in a minute.\n. Fixed.\n. You may want to check if you have the same account/email set in your git client.\n. LGTM, please rebase.\n. No rebase for 15 days - closing the PR. \n. Fixes #988.\n. LGTM\n. I guess that the first entry is for pod and the second is for the container. Please add container_name to the query to check.\n. ok to test\n. E2e failure is not related to the pr.\n. LGTM\n. I will port the code but someone should test if it really works.\n. #1000 needs porting.\n. #1007 - #1000 ported.\n. Ok, so we have a basic support for metrics in opentsdb. Events are pending/TBD.\n. @innovia @fgrzadkowski \nA tiny correction - grafana + influxdb is in a separate docker image. New version is here gcr.io/google_containers/heapster_grafana:v2.6.0-2. \n. You will have to port the config files manually. Heapster and grafana should work on the older cluster as well. BTW, please pay attention to new mini-releases of Heapster - 0.20.0 (a big rewrite/refactoring) is still in alpha stage and we are pushing new version with bugfixes every other day. Today we pushed alpha10.\n. Please check if the problem persists in the new Heapster version candidate gcr.io/google_containers/heapster:v0.20.0-alpha10 and reopen if needed.\n. LGTM\n. LGTM\n. Ok, it is container capacity limit. There should be some mem capacity metrics for nodes.\n. LGTM\n. LGTM\n. LGTM\n. Can you please switch to the newest version? gcr.io/google_containers/heapster:v0.20.0-alpha9\nJust FYI heapster 0.20.0 is a big refactoring/rewrite and there were numerous bugs in early alpha versions. Issue in kubelet.go:154 was fixed some time ago.\n. LGTM\n. Fixes #934\n. Understood. Will be done today.\n. https://github.com/kubernetes/kubedash/pull/57\n. Not only in GKE.\n. lgtm\n. lgtm\n. LGTM, thanks for the PR.\n. Yep, removed.\n. Please move the code to heapster-scalability branch. The master branch will be soon overwritten by heapster-scalability.\n. Yes, please.\n. No this is not intdended state. Can you please provide more details about the crashes?\n. LGTM\n. Thank you for the PR.\n. @mwringe  please rebase\n. Thanks for fixing this.\nLGTM\n. LGTM\n. Btw, is pretty printing turned on?\n. cc: @jfoy @piosz \n. Please try v0.20.0-alpha12.\n. cc: @piosz @fgrzadkowski \n. LGTM\n. Could you please be a bit more specific on what you are exactly trying to do? And what is missing?\nI guess that s390x stands for some IBM architecture but I've personally never heard much about it.\n. Well, it is not entirely missing - most likely we are just using Influxdb client/db that is about half a year old that doesn't depend on the newest boltdb that has the mentioned file. We can definitely think about upgrading the deps a bit to have this file on-board but this is not a matter of putting an extra file there.\n. LGTM\n. Heapster doesn't do any node disk aggregations. Disk metrics in containers are not touched at all so the problem is either in kubelet response parsing or with GKE internal code.\n. From the shared node doc it is not clear how do you want to handle custom metrics. As long as the presence of custom metrics in the summary api is not fully confirmed and agreed I won't drop the old API that has them.\n. Grafana seems to have issues with updating templates. If you go to manage dashboard, click on templates, edit namespace templates and click on update the new namespace will pop in. I will take a look on how to workaround this problem. \nAs of pods not being filtered - there is no show tag values filtering possible in influxdb. But we can create an additional tag for time serieses that will contain namespace and pod name concatenated together.\n. @jfoy please verify that this fix helps.\n. No, you are not missing anything. Due to the way pods work and some current technical limitations the network stats are gathered at the pod level.\n. Limit is the sum of memory explicitly asked by your pods in the Limit section of container resources.\nIn the same way Request is the sum of memory explicitely asked by you pods in Request section. If you don't specify limit but only request then request sum can be higher than limit.\nAdding @vishh to handle filesystem metric question.\n. The PR only removes obsolete configuration options. Merging without LGTM. @piosz please review on your earliest convenience.\n. Are these warnings repeatable?\n. Heapster gets a list all namespaces from the API server and adds some namespace information to the pods and containers. If the pod's namespace is not on the namespace's list then the warning is printed (although it is badly formatted - i will fix it right now). \nI don't have a clear answer why this was possible given that it had 30 sec to get the namespaces.\n. Log fixed with #1086\n. Docs should be fixed now.\n. This is more-or-less working as intended. Request are used for scheduling. So the total requested value should not exceed node capacity. Limits on the other hand are used to keep containers/pods in some boundaries. \nPlease take a look https://github.com/kubernetes/kubernetes/blob/master/docs/proposals/resource-qos.md for more details.\n. The documentation isn't clear:\nhttp://kubernetes.io/docs/admin/resourcequota/\nIn term of resources, Kubernetes checks the total resource requests, not resource limits of all containers/pods in the namespace.\nSo IIUC your namespace is set to accept pods with combined sum of requests up to 1gb. So the sum of pod limits is 2gb which is ok.\n. Can you please provide more information? What Heapster version, what deployment, where do you miss cpu/usage_rate, etc.\n. Ok, can you enable log sink (--sink=log)for a moment and send us a sample from the logs?\n. Thanks for the log. Looks like for unknown reason the rates are not generated for nodes and node system containers but are for pods and containers. One reason is that something is missing in the node metadata in cAdvisor.\nWhat cloud are you running on?\n. Please add -v=8 for more logging - if my suspicions are correct you should find: \"Skipping rates for %s - different crate time new: xxxx  old: xxx\"  there. Please paste these lines and/or all logs if possible.\n. I spotted a bug in our code. We will push a new image tomorrow. \n. Thanks for reporting!\n. LGTM\n. LGTM\n. Fixes #1088.\n. Merging without LGTM to move #1090 forward. @piosz - please review at the earliest convenience. \n. LGTM\n. ```\nGOOS=linux GOARCH=amd64 CGO_ENABLED=0 godep go build ./...\ngoogle.golang.org/cloud/compute/metadata\nGodeps/_workspace/src/google.golang.org/cloud/compute/metadata/metadata.go:191: req.Cancel undefined (type http.Request has no field or method Cancel)\ngodep: go exit status 2\nmake: ** [build] Error 1\n```\n. LGTM\n. LGTM\n. Travis picked up 1.6, merging.\n. LGTM\n. LGTM\n. There is a test that checks whether the exported metrics have the labels mentioned in the schema. But if a label is not mentioned in the schema (as it was in this case) then the test won't indicate any troubles.\nIf you have a separate list of labels that have to be in the dump we can use it to double-check that everything is exported as expected.\n. Yeah, kind of :). \nI will add the explicit checks in couple minutes in the next PR. Merging this one.\n. cc: @a-robinson \n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Sorry, I was on vacation. Will take a look soon.\n. Regarding 100s of custom metrics - this was never the target and most likely won't be. But we are trying to follow Kubernetes scalability goals in Heapster, which for 1.2 were 1000 nodes x 30 pods in reasonable CPU/mem. So if your cluster is smaller then you have enough capacity to move some custom metrics around.\n. @DirectXMan12 What is your preferred option? I don't have a super strong opinion on how it should be done as I will not use it personally. Option 3 (1+2) seems to give the wider spectrum than the other although you can start with 1 or 2. \n. LGTM\n. Yes, we dropped support for non-Kubernetes deployments.\n. Thank you for this contribution. I understand that this is cleaner but unfortunately some users already depend on the current layout and we don't want to break things (again) just because of naming change. So we will not merge the PR, however we can revisit it and release with some bigger refactoring/update. \ncc: @piosz \n. Just one comment to apply, otherwise LGTM.\n. Yes, please.\n. LGTM\n. Thank you for the PR (and sorry that it took so long)!\n. Please sign the CLA.\n. Please sign the CLA.\n. LGTM\n. It is quite strange but it seems that heapster gets too few data. Can you curl heapster_ip:8082/api/v1/model/debug/allkeys  as described here https://github.com/kubernetes/heapster/blob/master/docs/debugging.md ?\n. LGTM\n. LGTM\n. What about tests?\n. I don't like this approach but OK.\n. LGTM\n. LGTM, please add it to release notes.\n. LGTM\n. lgtm\n. Tests fail\n. Can you fix it anyhow? I cannot merge a pr that breaks unit test. \n. LGTM\n. @k8s-bot test this please\n. lgtm\n. 1. Node utilization is calculated based on pod cpu metrics. If they are 0 then the utilization will also be 0.\n2. Yes, that is correct. We don't support labeled metrics in the api as no client needed it before. Contributions are welcome :). \n. LGTM\n. LGTM\n. Please sign the CLA.\n. LGTM\n. Why do you need to customize the processors?\nBTW, please sign the CLA. \n. lgtm\n. @k8s-bot test this\n. LGTM\n. lgtm\n. LGTM\n. LGTM\n. lgtm\n. lgtm\n. LGTM\n. LGTM\n. lgtm\n. LGTM\n. LGTM\n. lgtm, please squash commits to 2 before submit.\n. LGTM\n. lgtm\n. LGTM\n. Looks good - 1 question. LGTM will be given after the rebase.. LGTM. LGTM. lgtm. @DirectXMan12 \nYeah, unfortunately I have to agree with you :). We had a conversation with @piosz and @KarolKraskiewicz, @GFilipek and decided that reversing the data traffic makes sense. . LGTM. /lgtm. @crassirostris \nFrom the concurrency perspective the new version is slightly better but i agree that it doesn't solve the problem completely. My main concern is that we still mix adding things for execution with timeouts. And because of that we still have convoluted logic. The main improvement that was made was logging the timeouted requests in case of small clusters.\nI don't understand the second part of your message  \n. /lgtm (as a cherry-pick). /lgtm. Done\n. Added here and to the cache.\n. /namespaces/{namespace-name}/pods/sum/metrics/{metric-name} will also match to \n/namespaces/{namespace-name}/pods/{pod-name}/metrics/{metric-name} causing path ambiguity.\nReusing /namespaces/{namespace-name}/pods/{pod-name}/metrics/{metric-name} will require a change in the result type in this (an other paths for consistency). Are you OK with that?\n. Ok, should I expand TimePoint or create a new type (sample count is needed for the sum to be meaningful)? Other option is to return something like [][]store.TimePoint.\n. There is no CPU/MEM information in /namespaces/{namespace-name}/pods/:\ncurl 10.0.98.141:80/api/v1/model/namespaces/kube-system/pods | gunzip\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   237  100   237    0     0  69277      0 --:--:-- --:--:-- --:--:-- 79000\n[\n  \"fluentd-cloud-logging-kubernetes-minion-yxir\",\n  \"kube-ui-v1-8b1lb\",\n  \"fluentd-cloud-logging-kubernetes-minion-ihlf\",\n  \"fluentd-cloud-logging-kubernetes-minion-a5u3\",\n  \"kube-controller-manager-kubernetes-master\",\n  \"monitoring-influx-grafana-v1-s97to\",\n  \"kube-scheduler-kubernetes-master\",\n  \"kube-apiserver-kubernetes-master\",\n  \"kube-dns-v8-vdsrr\",\n  \"monitoring-heapster-v7-czp8j\",\n  \"fluentd-cloud-logging-kubernetes-minion-9y8h\",\n  \"etcd-server-kubernetes-master\",\n  \"monitoring-heapster-v7-p9aze\",\n  \"fluentd-cloud-logging-kubernetes-master\",\n  \"monitoring-heapster-v7-iio5r\",\n  \"monitoring-heapster-v7-r1ddh\"\n ]\n. In the end I decided to try with [][]store.TimePoint. It seems clearer to get all metrics and sum them elsewhere than summing them up inside the db, under lock. Please take a look.\n. What about /namespaces/{namespace-name}/pod-list/{pod-list}/metrics/{metric-name} (heapster will return a list of MetricResult, not a sum)\n. For now we are fine with summation on the client (the PR is updated).\n. Updated.\n. Not sure but without this the test fails. Probably a fuzzer checks what packages are initialized or so. \n```\n--- FAIL: TestSetSinksStore (1.00s)\n        Location:       external_test.go:168\n    Error:      Not equal: 1 (expected)\n                    != 0 (actual)\n    Location:       external_test.go:169\nError:      Not equal: 1 (expected)\n                != 0 (actual)\n\n    Location:       external_test.go:173\nError:      Not equal: 1 (expected)\n                != 0 (actual)\n\n    Location:       external_test.go:174\nError:      Not equal: 1 (expected)\n                != 0 (actual)\n\n    Location:       external_test.go:177\nError:      Not equal: 1 (expected)\n                != 0 (actual)\n\n    Location:       external_test.go:178\nError:      Not equal: 1 (expected)\n                != 0 (actual)\n\n    Location:       external_test.go:188\nError:      Not equal: 2 (expected)\n                != 0 (actual)\n\n    Location:       external_test.go:189\nError:      Not equal: 2 (expected)\n                != 0 (actual)\n\nFAIL\nFAIL    k8s.io/heapster/sinks   1.067s\n``\n. Done.\n. What delay?\n. Why this fragment is redundant?\n. i cannot see any sampling here.\n. Done\n. Done\n. Both deleteNode and pod operate on the same lock. Here the lock is already taken and deletePod would deadlock (at least i think so - mutex doc doesn't say anything about possibility to acquire a lock multiple times - \"Lock locks m. If the lock is already in use, the calling goroutine blocks until the mutex is available.\")\n. Should rather runmakeinstead ofgodep go build` - right now it is more or less equivalent but this may possibly change in the future.\n. poll_duration has been removed recently - see: #512\n. Not sure about your curl but my (7.35.0 (x86_64-pc-linux-gnu)) doesn't automatically gunzip the response.\nResponse headers: \n```\n\nGET /api/v1/model/stats/ HTTP/1.1\nUser-Agent: curl/7.35.0\nHost: localhost:8082\nAccept: /\n< HTTP/1.1 200 OK\n< Content-Encoding: gzip\n< Content-Type: application/json\n< Date: Fri, 11 Sep 2015 09:35:21 GMT\n< Content-Length: 625\n```\n\nResponse content:\n\ufffd\ufffd\u0012\u001c\u0016_\ufffd\ufffd\u0001WA\ufffd\ufffdq0\ufffds\ufffdP3\ufffd\u0012[...]\n. This TODO should not be removed.\n. What is the purpose of this struct?\n. Can we have these 2 grouped in a struct?\nWould it be good to have CpuLimit as well?\n. Actually I've found cpu limit inside of cadvisor cpu spec. Adding the new cpu-related fields will create a mess. I would opt for removing cadvisor dependency in sources api (and completing the below TODO).\n. Limits are inside of cadvisor structs :/\n. done\n. Fuzzed structs are already used in unit tests to check corner cases. Here I would like to check if nothing is lost/dropped on the way if the values are correct. The values set below are (mostly) needed and the rest is irrelevant from the point of view of this test.\n. Done\n. Not a big difference but OK. Changed to NotEmpty and InEpsilon.\n. Added more checks.\n. I would rather check the other endpoints in a more mocked and unit-like test for api/v1/model_handler.go where I can verify if the returned values are correct.\n. Well, I hope so. I can still poke all of the endpoints and check if they don't crash (but probably in the next PR).\n. With the E2E approach as here i can only check if the output is returned at all and if it is sane, not if the endpoints are correctly bound to model funcs.\n. If this test doesn't complete in <10min then something is terribly broken (or the connection is dead slow). Increasing the timeout beyond 30m will not make any good. \n. There is Heapster running ...\n. This is a temporary fix for unreliable delete - see #559. Delete should mean delete and there should be no reason for retries. I will refactor this completely in the next PR.\n. As above.\n. Just to check if this helps. There were multiple runs (only in Jenkins) where service could not find his endpoints within the previous timeouts.\n. Reverted\n. Added detection.\n. Why it has to be >2*modelResultion?\n. Actually I think there is no reason for it to be > 2* modelResolution. If model resolution is 1 minute then it should sync every minute with the cache. Timestamp checking in update should do the rest (and if it doesn't then it should be fixed ASAP).\n. What if cadvisor didn't manage to catch the sample in the last second (or the samples are offseted slightly). Lets make the window slightly bigger. Also - will 1 sec window work with metrics compaction/deduplication strategy in cAdvisor?\n. Can we make it 1 min + offset?\n. So, I heard from @piosz that cAdvisor/Kubelet doesn't generate a new sample if the new one is more-or-less equal to the the previous and the previous was taken in the last 15 sec or so. I'm not sure where and how this logic is exactly implemented.  It is also possible that I misunderstood something.\n. It should not be called aggregator. Lets call it DataBatchProcessor and we will have aggregators implementing this interface. \n. it should rather be Int64Gauge, Int64Cumulative etc so that we know what is the interpretation of the value.\n. There should be a list of DataBatchProcessors, that are fired one after another.\n. it should return a list of data batches. Otherwise secifying start doesn't make any sense.\n. Lets move name outside of the struct and keep it only as map key.\n. Repalce with metrics map[string]MetricValue\n. NewDummyDataProcessor\n. dummyDataProcessor\n. Done\n. Done\n. Done.\n. Done\n. Done.\n. Done.\n. This is already enforced in Housekeep. Simple sanity check is probably more than enough.\n. Done.\n. Done.\n. We would have two functions that take start/end and do housekeeping. Lets leave it as is.\n. Of course,  should be end.Add(scrapeOffset).Sub(now). Done.\n. Remove or add todo with info when to uncomment it?\n. 10255 - make constant or take port from somewhere\n. We will support all metrics, not only these mentioned below. Name it as StandardMetrics or so.\n. Is it still used?\n. I had to pick some. 10000 points sounds more-or-less reasonable. We will fine-tune it soon and put it behind a flag.\n. Fixed. Although I'm not sure what consequences this change will have for GKE/GCM. We will have to put this in the release notes. \n. done\n. done\n. done\n. will do (me or @jszczepkowski)\n. done\n. This was done for KubeDash but we should get the list of pods from APIserver only. I will add more info on that. \n. All data is written to some (hopefully) permanent storage. If you need them you should query the storage directly. AFAIK there is no current need for a bigger window for ALL metrics in Heapster, UI needs cpu and memory only (and maybe some net stats in the future).\n. OK, I can rephrase this sentence.\n. Yep, right now metrics and events are combined into one tool but we are planning to split them.\n. Well this is a compromise. Storing all of the data for 15 min will consume lots of memory for nothing (no use case). UI and some Vish's experimental scheduler needs cpu and mem info for a longer time window (preferably 1h). \n. Maybe. We don't have clear requirements for Oldtimer so for now we will keep it separate. Heapster and Oldtimer can be glued together at any time if needed.\n. It was not my decision to start yet another UI. But this is the situation right now. Kubernetes Dashboard is actively developed (https://github.com/kubernetes/dashboard/graphs/contributors) by Google and Fujitsu, has some working prototype and will probably be delivered for 1.2 to some extent. On the other hand there is KubeDash that is \"stable\" from early October. It has some very specific requirements, like 1-day-log cpu usage average, which may or may not be relevant once Kubernetes Dashboard becomes the default/main Kubernetes user interface.\n. Do you have any specific use case that could justify the extra memory?\nRight now we have 2 flags/constants that control this behavior. We can measure what is the cost of storing all metrics for 15 min. We hope to run some more performance test late this/early next week. Then we can decide based on hard data, but I have a feeling that being consistent will make memory consumption >4x bigger. \n. Actually there is a need for it. Initial Resources Estimator is kindof piggybacked in Kubernetes Master. There is/was a plan to move it to Heapster. Vish's experimental scheduler also wants some long-term stats. And there is UI. But all of them are a bit vague and not top priority.\n. Yes, I know that long term storage will most likely have troubles with this amount of data. Hopefully we will run some tests soon to check whether the targets are doable at all. \n. Agree, it can be outside of the cluster. \n. It means that if we scale to 1000 nodes x 30 pods with 5 custom metrics but we fail with 6 custom metrics then we will be happy and we will not try to improve.\n. All metrics should be aggregated.\n. This should rather be a constant.\n. Inconsistent type. Please zero the metric. Otherwise it can be random.\nPlease also continue the aggregation on error. One nasty user should not break the entire flow.\n. Same here.\n. Do not crash.\n. Kindof. We don't want to use more than 60kb per pod (with 1 container) in total. In theory you can divide it by the number of samples you want to store and get some number (like 4kb for 15 samples for 15 minutes).\nIn our internal discussions we did it the other way round. We calculated the weight of all information and data structs needed to keep it and we got around 3.5 kb per pod sample.\nWhile storing multiple samples of the same pod we can be smarter - we can store labels only once (if they didn't change) and reduce the needed memory. There also has to be some extra space reserved for parsing requests and building responses (and some/most formats are very inefficient - like storing all labels per metric/single value). So overall we are rather thinking about the whole system not about a single sample in isolation.\n. Ok, will add it when needed.\n. AFAIK there are no immediate plans to for any event/metrics combining. Once we decide to do it we can revisit this item and decide what is the best:\n- having a separate component\n- having a sink that also listens to Kubernetes events\n- glueing the two binaries again (unlikely)\nIf you want to discuss this now please schedule a VC.\n. Yes, it will be possible. You can tweak 2-3 min, 15 min and whitelisted metrics names.\n. @DirectXMan12 Yes\n@vishh We will try to keep the api and also hack some stuff around stats in Heapster to temporary support some stats (avg/max, 95% is more tricky to squeeze into small memory) in the core/new Heapster but everything will depend on how much time we will have left. This is not the top priority feature.\n. Sure, but I would rather downsample/drop metrics in Heapster and not push unneeded data through the wire.\n. Yep, max will be there too.\n. See my comment above. Happy to chat about it over a VC.\n. @wojtek-t What is the current plan regarding events storage?\n. At the time of writing the plan was to have a Kubelet specific api. Then the direction changed couple times and I'm personally not sure what is the current \"approved\" plan and how stable it is. For now we will talk with Kubelet via the old (but fixed) cAdvisor-specific api and see what the Node Team will provide. \n. Kafka is not a \"trivial\" deployment and anyone who has it will probably be able to build his own image. With 1.2 the sinks will remain compiled in. We can work out some specific requirements later (tests, e2e tests, performance checks, only \"official\" client libraries etc).\n. Because we cannot scale that much to say that we can take arbitrary number of custom metrics. 100+ metrics per pod with 1000 nodes and 30 pods on each node will probably require sharded/clustered Heapster for which we will likely not get \"time budget\" anytime soon.\n. There is no official Heapster/Metrics api yet. There is some api for stats but I'm not sure if will be exactly reproduced in Oldtimer. \n. We have timestamp here:\nhttps://github.com/kubernetes/heapster/blob/heapster-scalability/metrics/core/types.go#L76\n. Added.\n. Done\n. Done\n. We don't go to v4 in heapster.\n. Done.\n. Done.\n. This is only for debugging. No need for aligning.\n. Added.\n. Yes. core.EventSource, added comment.\n. done\n. done\n. Ok, 1 is also good. 10 was made just in case someone sometime in the future put more errors there. \n. Please fix the indentation.\n. Done\n. Done\n. I have a plan to make sink creation error a fatal one and force all sinks to handle retries/reconnections appropriately. In some next PR.\n. They are interfaces/pointers.\n. Done. Need to change this file so i have the same on mac and linux.\n. Done\n. Ack.\n. Done\n. Done\n. It is how it is written in the old/current Heapster.\n. Added. This code was copy-pasted from the old heapster.\n. Comment replaced.\n. Done\n. Added.\n. Yes it is not, you can can specify what to start in yaml. By default Heapster4Metrics is started.\n. Actually I planned to remove all comments (they are duplicated in doc, couple lines below).\n. Next PR.\n. Done.\n. Ack.\n. Done\n. Why do we need this?\n. I guess that the Heapster API traffic will be in most cases smaller than 10qps and this code is a bit of overkill. \n. Same as below.\n. This is only for tests.\n. We hope to have exactly same data. Just FYI - this we are rewriting Heapster so some functionality is not yet present. In the mean time we want to have the tests passing so some of them (that will obviously fail) are commented out.\n. Ok good point.\n. Can we have this in common//influxdb and reuse in metrics/ and events/ ?\n. No\n. We don't have node aggregator so there is no duplication, but i can add it :).\n. OK\n. There is no host id in pod/container.\n. m? 007?\n. Are you sure you want to treat containers without name as system? \n. Yes, when everything works then everything has a name :).\nBut if you take a look couple lines above you will see that we sometimes extract the container name from an obfuscated docker container name and sometimes from a kubelet-assigned label. Both mechanisms can possibly fail and in that case I would rather drop the metric set/container with a warning instead of renaming it as a system.\n. Yes, but at that moment we don't know to which pod the container belongs. Ideally we would take container id and look up the correct pod/container, but here we don't use this mechanisms (for simplicity, maybe we should) and we need a name. In Kubernetes 1.0.x the name was NOT available as an annotation so we extract it from docker-specific container name, what can possibly (but unlikely) fail.\n. I believe that we have to support 2 last major releases.\n. Done.\n. Heapster listens on events and writes them to external data stores (Google Cloud Logging, Influxdb etc)\n. We wrote some code that predicts resource consumption for a container\nhttps://github.com/kubernetes/kubernetes/blob/master/docs/proposals/initial-resources.md\nBut further work is suspended at this moment due to higher-priority tasks. \n. Are all the values float?\n. Is it guaranteed that we will always have cpu usage rate? How it is calculated?\n. Is it possible for cAdvisor that a PodStatsContainer is not present but regular pod containers are? If yes then we should clean up the mess somewhere - either create the missing pod or drop the \"orphaned\" containers.\n. I wonder what to do if the status is different (deleted/failed/whatever)? Should we drop the remains?\n. I was under impression that cAdvisor's custom metrics were either int64 or float.\n. Yes, if everything is in perfect sync. But there is a chance that the state of some pod/containers differs in api server and in kubelet when a pod is killed, started etc. What then?\n. Name the sink so that it has something in common with kubernetes/kubelet/etc.\n. Place this file in a separate directory. One source per dir, please.\n. Most of the sinks have some metric registration code in place if required. So in theory you can add any metric you like. Influxdb has a support for both normal and labeled metrics. GKE/GCM not yet but it is coming soon (probably tomorrow). \n. I would rather suggest that you stick to one type of the value for a metric. We can consider moving from float32 to float64 in MV using extra bytes saved from switching Time.Time (which is 20b) to int32 with unix timestamp.\n. Unfortunately you cannot remove cpu-usage mapping. This is a legacy constant used by Horizontal Pod Autoscaler when talking to Heapster.\n. Other way round, but yes. First check for mapping.\n. Some metrics may not be yet present in a metric set. Please do not enforce that.\n. Same here. \n. I'm just saying that the lack of a metric in one of the pods should not break the whole batch request. Right now the result for that pod will be empty and I would rather keep this behavior. \n. There can also be custom metrics that can have arbitrary name. Please only hardcode the deprecated metrics that require conversions.\n. Done.\n. Done.\n. Yes.\n. set float\n. Can you apply the comment?\n. Or let's do it other way round, I will merge and apply a quick fix. I need enable custom metrics ASAP and the current code blocks them.\n. Added\n. Done.\n. Done\n. Scrape time is (at least now) used only for converting rate metrics to cumulative. Do you have any cumulative metric that is scraped at a very different time? This is an internal structure that can be changed in the future. Right now I'm trying to reduce a size of a single metric as we are adding couple more of them.\n. Good idea - will take a look.\n. Which yaml is used in integration tests?\n. Can we please have it in a separate package? \n. Add delta handling in pod aggragtor. We should not aggregate delta metrics as we don't aggregate cumulatives.\n. set create and scrape time\n. I guess something should remain here for manual runs.\n. done\n. Done.\n. Done.\n. Done.\n. What do you exactly mean by additional scoping? Do you want to include CA owner in the metric name?\n. How will you exactly attach metrics to pod/containers/namespaces/etc? /api/v1/push seeems to look like a generic bulk push.\n. Could you please include an example of how the push content should look like?\n. The current heapster works on batches. Every 30/60sec it scrapes all the metrics from the nodes packs into a \"batch\" and pushes together down the processing stream. In the batch all metrics  have the same timestamp which is rounded to 30/60sec (although there is also information about the exact scrape time). If you push them often enough (>>=2x heapster scrape time) then this should not matter too much.\n. Eventually we will use a \"rounded\" timestamp for the metrics. So for example if you push your metrics at 12:01:40 we will grab it and use in a batch with timestamp 12:02:00 along with metrics scraped by Kubelet at 12:01:51. The original timestamp can also be available somewhere in the data for some custom processing (like converting cumulative to gauge) but the idea in Heapster is that all data/metrics from one batch share the same timestamp for consistency/simplicity. \n. Is MetricResult the best structure to pass this data? \n. Would you consider having multiple \"MetricAggregationResult\" and then a single data point in average, max, min, median, count etc? \n. Which sinks are you planning to support? \n. Yeah, something like that. Although I would introduce start/end time to MetricAggregationBucket. The question is also whether you want to name all the aggregations upfront or have a map[string]MetricResult (I don't have a super strong opinon on that).\n. I would suggest to include it in the default deployment. I think that for 99% deployments a single instance will be enough. It should rather not consume lots of memory. \nAnyone who needs to perform large-scale data analysis will do it in some other way (not via this very basic api). \n. Oldtimer should rather run as a separate executable/container in the main Heapster pod.\n. Please move these 2 files to a subdirectory and place a readme explaining what is the difference between these and the original files.\n. 2016 :)\n. Please write why we had so crazy path.\n. Please add user information only if it is not empty.\n. What is and what is not covered by this test?\n. Are you planning any tests for the handlers?\n. Yeah, it would be nice. Can be in a following PR but please do them.\n. What exactly is pod id. Please document.\n. what aggregations are supported?\n. what about sql injection. can we test that the field name is sane?\n. same here, can we escape the parameters?\n. what if left empty?\n. Actually, lets name the default set \"default\".  As Heapster works only in kubernetes name \"kubernetes\" doesn't mean anything.\n. default\n. We pass pod listener for a reason. In large clusters with thousands of pods podsListeners are quite big (they contain a cache of ALL pods in the system with all metadata) and having 2 copies of them would be a big waste of memory. \n. Do we need to build this or is it already done?. Let's move these comments to a separate pr.. I guess we should probably use maps here. CPU and Memory are not the only resources we may want to use. . Scrape. PodID (as uid) is useless. Replace with podname + podnamespace.. UpperCase. Add an information that this set may be only a pice of a multipart batch.. Add batching.. Please check if we have a helper function for this.. Because the whole metric pass assumes that it is synchronous. We don't want to start another metric scraping if there are too many pending sink sync operations. Heapster, especially in large  clusters, has memory limits set with assumption that there are at most 2 (or 3, don't remember exactly) full scrape results in the memory. Having more may cross the limit and if unlucky with free resources, crash the whole service.  . This will create DDOS on StackDriver infrastructure. All Heapsters in GKE clusters scrape  nodes at roughly same time and at the same time they will start pushing the metrics. Please add a random sleep for 5-10 second somewhere. . It is a bad idea. We scrape metrics at the slightly different times but we round things up to the closest minute for clarity and for being consistent with the subsequent Heapster runs and different clusters, should the user want do some serious performance debugging.\nSo it is important that we start scraping nodes at the right, consistent time.. I would set timeouts irrespective of the offset and definitely not random. If we have to introduce up to 5 sec offset we should reduce the total timeout by 5 sec.. This is misleading. You can add all requests to the queue but it doesn't mean that the operation completed. . 3 second random latency might be too small to prevent ddos in the future. Please increase the value to 5-10 and make it a flag.. Info?\n. make it uninitialized and set to timeout in timeout case. This piece hasn't improve :/. you can move this if condition to for, will be cleaner . I would remove the timeout here and use buffered channels instead. There is no point in having timeout control in 2 places.. How useful this information is?. ok.. Just FYI, switching to buffered channels will require you to change the stop method and introduce an extra check around line 295, so it is not a single line :/. \nAnyway as this is a cherry-pick I will not push for any changes but please consider refactoring it in the future.. ",
    "kelonye": "Recreated the services and seems to working OK now thanks.\n. ",
    "dipankar": "Done!\n. ",
    "cadvisorJenkinsBot": "Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. I am working on fixing the integration framework. The fuzzed test is an\nissue. I will fix that test via a separate PR.\nOn Thu, Jun 4, 2015 at 12:09 PM, Jack Foy notifications@github.com wrote:\n\nThe failed integration builds appear to be an infrastructure error:\nfailed to load docker image \"heapster:e2e_test\" using temp file \"/tmp/kubernetes-minion-71yx171296728\" on host \"kubernetes-minion-71yx\" (\"exit status 1\") -\n\"Warning: Permanently added '104.197.24.127' (ECDSA) to the list of known hosts.\ntime=\\\"2015-06-04T19:01:01Z\\\"\nlevel=\\\"fatal\\\"\nmsg=\\\"Error: Untar fork/exec /usr/bin/docker: argument list too long\\\"\nERROR: (gcloud.compute.ssh) [/usr/bin/ssh] exited with return code [1].\"\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/310#issuecomment-109014540\n.\n\nYou received this message because you are subscribed to the Google Groups\n\"cadvisor jenkins bot\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an\nemail to cadvisor-jenkins-bot+unsubscribe@google.com.\nTo post to this group, send email to cadvisor-jenkins-bot@google.com.\nTo view this discussion on the web visit\nhttps://groups.google.com/a/google.com/d/msgid/cadvisor-jenkins-bot/GoogleCloudPlatform/heapster/pull/310/c109014540%40github.com\nhttps://groups.google.com/a/google.com/d/msgid/cadvisor-jenkins-bot/GoogleCloudPlatform/heapster/pull/310/c109014540%40github.com?utm_medium=email&utm_source=footer\n.\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Build finished. No test results found.\n. Build finished. No test results found.\n. Build finished. No test results found.\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. \n",
    "sourav82": "These minions are on local intranet (running ubuntu). Not using any cloud infrastructure. It is local Kubernetes cluster having two minions.\n. @vishh, Yes, cadvisor is reachable using http://:4194/. \n. @vishh, I was using kubernetes-mesos framework with mesos and came to know that cadvisor is not yet integrated with kubernetes-mesos. https://github.com/mesosphere/kubernetes-mesos/issues/137#issuecomment-72027269\n. ",
    "antmanler": "I think it's better to make trade off, since a monitoring service should not cost much computing resources in cluster.\nAnd I've no idea why sending data to influxdb will cause so much CPU usage, I checked the network payload in influxdb's container and there is not a lot of data incomes every 10 sec. I wonder maybe this issue is related to influxdb itself or its golang client? \n. Thanks, sorry so busy these days, I'll try to put restrictions on InfluxDB and tune its resources consumption\n. ",
    "arkadijs": "For InfluxDB sink I suggest to use Go channels and a separate flusher goroutine together with GOMAXPROCS.\nhttps://github.com/arkadijs/heapster/blob/master/sinks/influxdb.go#L209\n. Also, adding to (1), numStats is not a cAdvisor HTTP query parameter, but must be supplied via POST JSON: https://github.com/arkadijs/heapster/commit/2fc23c2a890e6c77c56ee0640e5601178f461df6#diff-d506dd5ceb65926ead27b28cd6c5eca3L95\n. An example.\nThere are two kinds of containers: user apps and infra. I want to show a stacked graph of infra containers accross the cluster. What do I do?\nselect container_name, derivative(cpu_cumulative_usage) as cpu_usage \n  from stats where container_name =~ '^infra-'\n  group by time(10s), container_name\nDoesn't work.\nderivative() if operating on non-related values that comes from different machines.\nselect container_name, derivative(cpu_cumulative_usage) as cpu_usage \n  from stats where container_name =~ '^infra-'\n  group by time(10s), container_name, hostname\nIf Gafana would have a support for that - ie. grouping by more than 1 non-time column - which it doesn't suport with InfluxDB, that would give me a line per machine.\nThus I need a two-stage process. A continuous query:\nselect container_name, derivative(cpu_cumulative_usage) as cpu_usage\n  from stats where container_name =~ '^infra-'\n  group by time(10s), container_name, hostname into cpu_stats_infra\nthen:\nselect container_name, sum(cpu_usage)\n  from cpu_stats_infra where $timeFilter\n  group by time($interval), container_name order asc\nin Grafana.\nSimilar pattern repeats over and over again, hitting the limitations of InfluxDB query language and Grafana.\nAnyway, graphing from stats is really slow. Testing is done on m3.large AWS instances, timeseries data fits in InfluxDB memory cache. InfluxDB 0.8.8.\n\nWith diskio and filesystems I was thinking of separating them into separate series. Does that approach sound good?\n\nYes. Please think about separating as much as possible into different series. Maybe even a series table per container (name) / pod, eliminating a repetitive scans over stats. I don't know if InfluxDB would be able to cleanup that effectively, which is probably could, but InfluxDB can join series into single data set - select * from /.*/, thus providing performance and/or aggregated data when required.\nIIRC, InfluxDB 0.9 will be better by storing columns as integers to refer to unique values, but that won't solve the performance problem in general.\n. 1. +1 container to deploy and explain to newcomer. I prefer to code a solution that just works. Please avoid microservices envy.\n2. Exchanging live data via JSON formatted data on the filesystem? Really?\n3. The buddy is overwriting the hosts file in-place, thus a race condition with the reader (heapster). Use rename().\n4. There is code in Heapster that queries Kubernetes nodes API. following the logic it should be removed and placed into separate container.\n   https://github.com/GoogleCloudPlatform/heapster/blob/master/sources/kube.go#L110\n. ",
    "piosz": "Obsolete.\n. Obsolete.\n. closing in favor of #690. Obsolete.\n. Fixed by @mwielgus \n. closing in favor of #1197\n. Obsolete/Done\n. Out of scope. Heapster is about monitoring Kubernetes cluster.\n. Closing as Kafka is already supported.\n. I can take this (with medium priority).\n. closing as obsolete. There is already a support for template dashboards. \n. Working as intended.\n. There is no run.sh script right now.\n. Capacity doesn't make sense as a metric. If needed it can be used for computing other metrics eg. currently we export node utilization as node usage divided by node capacity.\n. Currently there is no way to export the same event twice, however it is possible to miss some of them. Closing in favor of new issue.\n. PR in flight #733\n. Thanks @huangyuqi. Looking forward for you change.\n. #733 is merged.\n. closing in favor of #1133\n. Fixed.\n. closing as obsolete. Out of the scope of the project - only Kubernetes deployments are supported. See more details: https://github.com/kubernetes/heapster/blob/master/docs/proposals/vision.md\n. closing as obsolete. @vishh do you still want to get this in? If yes please re-open and rebase.\n. This is obsolete. Try to use the new Heapster 1.0.\n. Out of the scope of the project - only Kubernetes deployments are supported. See more details: https://github.com/kubernetes/heapster/blob/master/docs/proposals/vision.md\n. I can take a look.\n. Ok. Thanks for the update. Did you test it with many pods (i.e. 100 nodes, 10 pods/node)?\n. Sounds reasonable. Should this issue be closed?\n. According to https://github.com/kubernetes/heapster/blob/master/docs/sink-owners.md we are going to remove support for Riemann unless we will find owners for it.\ncc @DirectXMan12 @jsoriano. No more config files.\n. Out of the scope of the project - only Kubernetes deployments are supported. See more details: https://github.com/kubernetes/heapster/blob/master/docs/proposals/vision.md\n. Billing will not be supported.\n. This is obsolete. Please try the new Heapster 1.0.2.\n. Eventer is now separated from Heapster.\n. This is obsolete. Please try the new version of Heapster 1.0.2.\n. Obsolete. Please try the newest version.\n. cc @vishh \n. Yes.\n. Out of the scope of the project - only Kubernetes deployments are supported. See more details: https://github.com/kubernetes/heapster/blob/master/docs/proposals/vision.md\n. Fixed tests. Some minor changes. Tested the code manually. Seems to work.\n. cc @jszczepkowski\n. The snapshot is here: http://pastebin.com/uNJjZLNr\nI'm not sure if it is what you wanted.\n. Thanks!\n. closing this in favor of #687\n. No problem. Thanks for review! Added follow up PR.\n. Out of the scope of the project - only Kubernetes deployments are supported. See more details: https://github.com/kubernetes/heapster/blob/master/docs/proposals/vision.md\n. This is obsolete. The document no longer uses v1beta3 objects.\n. @vmarmol, @vishh are you ok with such change? If yes I'll add unit test. \n. @afein maybe you can review this PR?\n. Thanks!\n. @vmarmol \n. I've found this bug while developing #394. There seems to be no issue probably because of what you wrote about cAdvisor, but it always requests stats from the interval begin on the time of Heapster creation.\nWhat do you mean by manually testing? I ran Heapster with this change and seems to work but I may perform more advances tests tomorrow.\n. I can take a look on Monday\n. @vmarmol\n399 was only about fixing cpu, but I've also fixed memory to make both resources exported consistently.\n. Changed to use millicores internally. We probably don't want to use whole core (not millicores) to support also machines with fractional part of virtual CPU. Please let me know if it addressed your comment.\n. ping\n. No worries. Thanks you!\n. This is not related to Heapster. Please file an issue in Kubernetes.\n. This is not related to Heapster project.\n. Addressed @vmarmol comments from  #384 \n. Fixed. Sorry for missing this.\n. @vishh could you please take a look?\n. @vishh \n. Please try to use the new stable version. Please reopen in case of the problems.\n. This seems to be no longer a problem. Also please try to use a new stable version of Heapster.\n. Try to use stable Heapster version. Closing as obsolete.\n. Closing as obsolete. Try to use a new stable version of Heapster.\n. I assume it was fixed by  #445. Please use a stable version of Heapster and reopen in case of problems.\n. I assume this is no longer a problem. Closing the issue.\n. Obsolete. Try to use the stable version of Heapster.\n. cc @mwielgus @jszczepkowski\n. closing in favor of #1056\n. cc @fgrzadkowski\n. Out of the scope of Heapster.\n. cc @vishh @afein \n. This is out of the scope of Heapster.\ncc @fgrzadkowski @mwielgus \n. @jszczepkowski?\n. Closing as obsolete.\n. As for now Heapaster is built with go 1.6\n. This is obsolete. Cluster autoscaler for Kubernetes is implemented and it no longer needs signals from Heapster.\ncc @mwielgus \n. Closing as obsolete. Please try to use the latest Heapster release.\n. cc @mwielgus @jszczepkowski\n. I still can see a problem here. Let's call scraping goroutine S and populating goroutine P. Let's assume that poll_duration=1m and sink_frequency=2m.\nS scrapped already data for time inteval [10:00, 10:01] and started scraping for [10:01, 10:02]. Right after P started populating data for [10:00,10:02]. It found only the first part of the data, populated them and marked [10:00, 10:02] interval as done. Right after that S provided data for [10:01, 10:02] interval which will be never exported.\nIt can happen also in situation when poll_duration=sink_frequency\nSo there is no more races in terms @mvdan mentioned but some data may be lost.\n. @mvdan could you please take a look?\n. Thanks!\n. I can add a follow up PR which simplifies interface Source.GetInfo() method by removing resolution field there, since it's now longer needed if this PR is merged.\ncc @vishh @mvdan \n. cc @mwielgus \n. I've tested it on Kubernetes already. How can I test the cadvisor case?\n. Tested manually on local setup with cadvisor and Heapster. Seems to work.\n. Obsolete.\n. After Heapster big refactoring effort this is no longer an issue.\n. LGTM, just a few nits\n. LGTM\n. closing as obsolete\n. LGTM\n. Thank you so much @vishh for this PR! I'll take a look into it today.\n. LGTM from me, but wait for @mwielgus LGTM as well\n. LGTM. Thanks for the fix.\n. LGTM\n. Obsolete\n. You have to implement DataSink interface. LogSink is a good example since it doesn't require knowledge in any specific backend.\n. cc @vishh \n. PTAL @mwielgus @vishh \nHow about defining an internal Spec&Stats API in a follow up PR?\n. I think the commits are meaningful so let's leave them.\n. LGTM just a nit\n. I should do it during Kubernetes \"code freeze\" for 1.1.\ncc @mwielgus @vishh \n. We no longer use cadvisor types internally. Closing as obsolete\n. @vishh I took a look briefly and I have no obvious concerns. Please let me know what is a timeline with bumping the version of Heapster.\n. @vishh I wouldn't say it was a review;)\n. Fixed in new Heapster.\n. LGTM feel free to merge on green\n. This is fixed in new Heapster.\n. LGTM\n. SGTM\n. Heapster by default provides 15 minutes of data in its API. In theory this duration could be extended, but it would cause Depending on the configuration Heapster can export data to InfluxDB (or many others metric backends) and there should remain for a longer period of time. I suggest using the second option.\n. We are happy to accept a contribution, though we have higher priority task to do.\n. LGTM\n. This is fixed in new Heapster.\n. LGTM\n. Do you want to cherry-pick integration tests as well?\n. LGTM. Feel free to merge on green.\n. @vishh not now. Probably not in the nearest future.\n. As for now we aggregate limit/request metric for node. It's no longer a machine capacity.\n. cc @jszczepkowski @fgrzadkowski @wojtek-t\n. This issue has been inactive for a year so closing it. Please reopen if you think it's still wanted.\n. Those informations are available in api server. It doesn't make sense to duplicate api server functionality in Heapster.\n. closing in favor of https://github.com/kubernetes/kubernetes/issues/27630\n. It seems to be not a big problem, so closing the issue\n. fixed by #634\n. cc @vishh \nThis should be cherry-picked to 0.18 and then enabled in Kubernetes.\n. LGTM\n. Thanks for doing it!\n. Thanks @vishh!\n. cc @mwielgus @vishh @wojtek-t\n. Sure. Changing in the description. Thanks for catch.\n. Heapster scales now to 2k nodes.\n. LGTM\n. ok to test\n. This will be fix by introducing Metrics API in Kubeletet https://github.com/kubernetes/kubernetes/issues/12483\n. @biskup noticed similar issue in Grafana dashboard. I'll try to fix #568 soon.\n. There was an int overflow in cadvisor https://github.com/kubernetes/kubernetes/issues/27194. This is fixed now\n. AFAIK this change won't help because NumStats field is ignored if Start and End fields are specified. See more details here:\nhttps://github.com/google/cadvisor/blob/master/info/v1/container.go#L94\n. In this case you should decrease maxHousekeepingInterval to 1s to ensure we don't omit any stats. I'm not sure what would be the outcome of this change.\n@vmarmol @rjnagal any thoughts?\n. @vishh I know, but we don't have mechanism implemented which will use the previous value in case of lack the current one, so indeed there will be gaps in metrics. This will cause incorrect computation of autoscaling metrics.\n. This will be fix by introducing Metrics API in Kubeletet https://github.com/kubernetes/kubernetes/issues/12483\n. We no longer support 1 hour of historical data. To gather it you should query your metric backend. Heapster provides only the latest values.\n. Since there was no follow up, I assume @vishh's clarification fixed the issue.\n. Support for docker compose is out of the scope of the Heapster.\n. The solution is posted by the asker http://stackoverflow.com/a/38039069\n. This seems like a problem with proxy on master. AFAIR I've seen similar problems with accessing web pages via the master proxy. Please create an issue on https://github.com/kubernetes/kubernetes\n. cc @gmarek\n. Thanks @Rastusik for the update. Closing the issue.\n. cc @fgrzadkowski\n. Closing as obsolete.\n. So what is the current plan to fix this?\n. LGTM\n. cc @jimmidyson @burmanm\n. LGTM\n. Closing as this working as intended.\n. I don't mind with this PR, but let @mwielgus to also take w look.\n. @bluebreezecf we will take a look today together with @mwielgus \n. LGTM\nThanks for the change!\n. Thanks @vishh!\n. PTAL\n. LGTM\n. LGTM, just 2 nits. I've taken a look only into the last commit.\nAlso please bear in mind #692\n. LGTM\n. cc @bgrant0607 @davidopp\n. closing as obsolete. LGTM thanks for the fix!\n. LGTM. Thank you @huangyuqi for the contribution!\n. cc @chrislovecnm\n. LGTM\n. unit test doesn't compile. please fix\n. LGTM\n. LGTM\n. fixed\n. please fix:\nsinks/manager_test.go:54: newDummySink returns Lock by value: sinks.dummySink contains sync.Mutex\n. LGTM\n. Thanks for reporting @chrislovecnm. You can decrease stats_resolution to 30s. I'll create a fix.\n. LGTM\n. please rebase\n. LGTM\n. LGTM\nMerging, please fix typo in other PR,\n. LGTM\n. closing as obsolete. Closing as obsolete. @vishh could you please link PR with fix? I can't find any.\n. I added TODO to export all supported labels.\n. Oh, I can see Image field. I missed it previously. Will add in follow-up PR.\n. fail not related\n. LGTM\nThanks for the contribution!\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. We do have a label hostname which is usually set to node name\n. When do you need it? Should this be fixed in the HEAD right now or can we do it once we will refactor Heapster (late Dec/early Jan).\n. It seems to be broken. How can I get there to fix it?\n. @huangyuqi  Is this ready for review?\n. @mwielgus could you please take a look?\n. @biswars, @moserke, @titilambert if you are familiarized with elasticsearch could you please also take a look?\n. @titilambert please comment in the code.\n. @huangyuqi we have a common directory for the common logic. For example for influxdb you can find it here https://github.com/kubernetes/heapster/tree/master/common/influxdb\n. Any progress on this? It would be great to include it in Heapster 1.1/Kubernetes 1.3. The code freeze is in 2 weeks from now.\n. this is a known flake\n. @k8s-bot test this\n. @k8s-bot test this\n. @huangyuqi I fixed it yesterday #1159\n. LGTM. Feel free to merge on green\n. LGTM\n. ok to test\n. LGTM\n. @huangyuqi I assigned @mwielgus to it.\n. LGTM\n. LGTM\n. pls fix:\nintegration/heapster_api_test.go:31:2: cannot find package \"k8s.io/heapster/model\" in any of:\n    /home/travis/.gimme/versions/go1.4.linux.amd64/src/k8s.io/heapster/model (from $GOROOT)\n    /home/travis/gopath/src/k8s.io/heapster/Godeps/_workspace/src/k8s.io/heapster/model (from $GOPATH)\n    /home/travis/gopath/src/k8s.io/heapster/model\n. LGTM\n. @cboggs yes, it definitely should be included in Kubernetes 1.2 so a few weeks from now.\n. LGTM\n. The code compiles now. PTAL\n. It's easy to revert the change and I had a problem with dependencies it uses so I decided to (temporary) remove it.\n. LGTM\n. LGTM, thanks for the PR!\n. LGTM\n. cc @burmanm @jimmidyson @mwringe \n. LGTM\n. LGTM\n. LGTM modulo small nit\n. cc @mwielgus \n. cc @bgrant0607 @davidopp @dchen1107 @fgrzadkowski @jimmidyson @jszczepkowski @vishh \n. cc @bryk \n. LGTM\n. please fix\nmd5sum: extpoints/extpoints.go: No such file or directory\n. Thanks. Feel free to merge on green.\n. PTAL\n. Closing as obsolete. LGTM\n. LGTM\n. Run gofmt on the following files:\n ./integration/framework.go\n. LGTM\n. Could you please try the latest build gcr.io/google_containers/heapster:v0.20.0-alpha12? It may required some configuration changes, for reference take a look into https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/cluster-monitoring/standalone/heapster-controller.yaml\n. Some information you can find here: https://github.com/kubernetes/heapster/releases. Not sure whether there is a single source of knowledge here.\nIn general the default version in Kubernetes 1.1 is 0.18 (the latest one is v0.18.5) and in Kubernetes 1.2 we plan to release Heapster 1.0 (currently pre-released as v0.20.0-alpha12). \n. You should see in logs something like this Heapster version 0.20.0-alpha12. It doesn't guarantee the exact version, but it's some approximation.\n. @ckleban thanks for verifying.\n. LGTM\n. We should probably make a patch release for v0.18 with the change.\n. LGTM\n. sinks/influxdb/driver.go:306: result of fmt.Errorf call not used\n. LGTM\n. fixed in #862. LGTM\n. LGTM\n. LGTM\n. ok to test\n. LGTM\n. LGTM\n. LGTM\n. Closing as obsolete.\n. LGTM\n. LGTM\n. LGTM\n. Feel free to merge on green.\n. LGTM\n. LGTM, just a few nits.\n. LGTM\n. LGTM\n. LGTM\n. LGTM\ntest fail unrelated\n. LGTM\n. LGTM\n. LGTM\n. Did you have a chance to test the build manually (with empty local gopath)?\n. LGTM. Hopefully the tests will pass.\n. default failed as expected\n. LGTM\n. LGTM\nThanks for the change, although I'm not sure whether it actually fixes #787, so no closing the issue before it's verified.\n. add to whitelist\n. @k8s-bot ok to test\n. Fail not related.\n. LGTM\n. LGTM\n. @a-robinson could you please take a look?\n. LGTM, please rebase\n. LGTM\n. And what would happen in case when Heapster is restarted?\n. @k8s-bot ok to test\n. Both fails seem to to a flakiness. Re-running tests.\n. @k8s-bot test this please\n. Filled an issue #871. The fail seems to be unrelated to the change.\n. LGTM, just a nit. Feel free to fix it some time in the future.\n. The issue is fixed.\n. @k8s-bot test this\n. @k8s-bot test this\n. Tests passed on Jenkins.\n. cc @burmanm @jimmidyson \n. The problem takes place in release-0.18 branch only\n. fixed by #877\n. @k8s-bot test this\n. @k8s-bot test this\nEnabled Cloud Monitoring API\n. tests passed on Jenkins\n. @k8s-bot test this\n. Integration tests passes on Jenkins. default timed-out. Merging without LGTM.\n. @k8s-bot test this\n. @k8s-bot test this\n. default timed out\njenkins failing due to different reason\nMerging without LGTM\n. Known fail #876\n. Merging without LGTM\n. cc @mwielgus \n. @k8s-bot test this\n. LGTM, just one comment.\n. LGTM\n. LGTM\n. @k8s-bot test this\n. LGTM, please fix the tests before merge.\n. @k8s-bot ok to test\n. LGTM\nThanks for the catch and fix!\n. LGTM\n. LGTM\n. LGTM\n. cc @mwielgus @vishh \n. LGTM\n. LGTM\n. LGTM\n. This is fixed now after #1046 is merged\n. cc @mwielgus @fgrzadkowski \n. version v0.20.0-alpha2\n. cc @mwielgus @fgrzadkowski \nversion v0.20.0-alpha2\n. another failure http://kubekins.dls.corp.google.com/view/Critical%20Builds/job/kubernetes-e2e-gce/10579/\n. In gce suite the problem is with heapster container in waiting state.\n. Heapster is no longer failing in reboot tests.\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Any progress?\n. @mwringe @jimmidyson Is Hawkular sink working correctly? If yes let's close this.\n. Thanks for the update.\n. This should be fixed in the new Heapster 1.0.0 released in Kubernetes 1.2.\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Heapster in versions 1.0, 1.1, 1.2 support OpenTSDB. Here you can find an info how to configure the sink. Providing image to with OpenTSDB itself is our of the scope of Heapster project.\n. LGTM\n. from logs\nE0210 20:19:52.372961       1 factory.go:65] Failed to create sink: Post https://www.googleapis.com/cloudmonitoring/v2beta2/projects/k8s-jnks-e2e-gce-autoscalin\ng/metricDescriptors?alt=json: oauth2/google: can't get a token from the metadata service; not running on GCE\n. To fix the problem we should add waiting for token right after it's created https://github.com/kubernetes/heapster/blob/heapster-scalability/metrics/sinks/gcm/gcm.go#L239\nI'll create the fix tomorrow.\n. Let's discuss offline tomorrow.\n. LGTM\n. Closing as discussed offline\n. Merging without LGTM. Please take a look post-merge.\n. LGTM\n. Merging without LGTM. PTAL later\n. @muzzy82 thanks for reporting this.\ncurl http://localhost:8082 is not working by design. Please try curl http://localhost:8082/api/v1/model/namespaces.\nWe decided to change storage schema in influxdb starting from version 0.20 to make it consistent with our REST endpoint and others sinks. It hasn't been documented yet (see #1009). Apologies for that.\nI'm closing the issue. Feel free to re-open in case of further problems.\n. LGTM, though we should decide what we want to log.\n. cc @gmarek \n. LGTM\n. LGTM\n. LGTM :+1: \n. fixed in #985\n. LGTM\n. LGTM\n. LGTM\n. This is fixed now.\ncc @a-robinson \n. The tests were performed with Kubernetes 1.1. We need to test it with 1.2 as well.\n. It seems everything works besides #1069 which is not considered as a blocker for Kubernetes 1.2.\n. Fixed by @mwielgus \n. LGTM Though I'm not familiarized with OpenTSDB\n. LGTM\n. @k8s-bot test this please\n. I'm not sure what should be done in this issue. @fgrzadkowski @mwielgus any ideas?\n. The names have changed. We should document this. Closing in favor of https://github.com/kubernetes/heapster/issues/1009#issuecomment-190739123\n. Storage schema documentation should be updated (see #https://github.com/kubernetes/heapster/issues/970#issuecomment-184599624).\n. A bunch of PRs was merged:\n1071\n1072\n1073\n1074\n1075\n1077\n1078\nThis seems to be fixed.\n. fixed in #1059\n. LGTM\n. LGTM\n. Isn't grafana/.DS_Store by mistake here?\n. LGTM\n. fixed by #1023\n. ref #997\n. This is not fixed actually. #1023 fixed #1020\n. ref #997\n. LGTM\n. LGTM\n. LGTM\n. ```\n--- FAIL: TestAllExportsInTime (2.00s)\n    :31: \nLocation:   manager_test.go:55\n\nError:      Not equal: 3 (expected)\n                != 2 (actual)\n\n```\n. @k8s-bot test this\n. LGTM\n. We are not aware of any way. If you will figure it out please let us know:)\n. @andyxning what do you mean? \nHeapster 1.1.0 is released and available in GCR as gcr.io/google_containers/heapster:v1.1.0.\nHeapster starting from version 1.0 doesn't support kube-dash, the effort is deprecated and is replaced by dashboard project.\nOpenTSDB sink is supported both in Heapster 1.0 and 1.1, though it's community driven feature and we can't promise it works.\n. New dashboard was introduced in Kubernetes 1.2 and is by default installed in Kubernetes cluster. In the same Kubernetes version we released stable Heapster version 1.0 which doesn't support kube-dash. We strongly encourage you to use dashboard instead of kube-dash however it's still possible to use also kube-dash (more details here).\n@bryk can provide more details on dashboard.\n. cc @a-robinson @fgrzadkowski @mwielgus\n. ref #997\n. cc @a-robinson  @fgrzadkowski @mwielgus\n. ref #997\n. This is fixed now. The reason was #1034\n. cc @a-robinson  @fgrzadkowski @mwielgus\n. ref #997\n. fixed in #1036 \n. @k8s-bot test this\n. @a-robinson could you please take a look and verify whether the difference is acceptable?\ncc @mwielgus @fgrzadkowski \nref #997\n. fixed in #1060\n. Yes. The current schema is:\n{\n  \"metrics\": [\n   {\n    \"name\": \"uptime\",\n    \"description\": \"Number of milliseconds since the container was started\",\n    \"type\": \"cumulative\",\n    \"value_type\": \"int64\",\n    \"units\": \"ms\"\n   },\n   {\n    \"name\": \"cpu/usage\",\n    \"description\": \"Cumulative CPU usage on all cores\",\n    \"type\": \"cumulative\",\n    \"value_type\": \"int64\",\n    \"units\": \"ns\"\n   },\n   {\n    \"name\": \"memory/usage\",\n    \"description\": \"Total memory usage\",\n    \"type\": \"gauge\",\n    \"value_type\": \"int64\",\n    \"units\": \"bytes\"\n   },\n   {\n    \"name\": \"memory/working_set\",\n    \"description\": \"Total working set usage. Working set is the memory being used and not easily dropped by the kernel\",\n    \"type\": \"gauge\",\n    \"value_type\": \"int64\",\n    \"units\": \"bytes\"\n   },\n   {\n    \"name\": \"memory/page_faults\",\n    \"description\": \"Number of page faults\",\n    \"type\": \"cumulative\",\n    \"value_type\": \"int64\"\n   },\n   {\n    \"name\": \"memory/major_page_faults\",\n    \"description\": \"Number of major page faults\",\n    \"type\": \"cumulative\",\n    \"value_type\": \"int64\"\n   },\n   {\n    \"name\": \"network/rx\",\n    \"description\": \"Cumulative number of bytes received over the network\",\n    \"type\": \"cumulative\",\n    \"value_type\": \"int64\",\n    \"units\": \"bytes\"\n   },\n   {\n    \"name\": \"network/rx_errors\",\n    \"description\": \"Cumulative number of errors while receiving over the network\",\n    \"type\": \"cumulative\",\n    \"value_type\": \"int64\"\n   },\n   {\n    \"name\": \"network/tx\",\n    \"description\": \"Cumulative number of bytes sent over the network\",\n    \"type\": \"cumulative\",\n    \"value_type\": \"int64\",\n    \"units\": \"bytes\"\n   },\n   {\n    \"name\": \"network/tx_errors\",\n    \"description\": \"Cumulative number of errors while sending over the network\",\n    \"type\": \"cumulative\",\n    \"value_type\": \"int64\"\n   },\n   {\n    \"name\": \"cpu/limit\",\n    \"description\": \"CPU hard limit in millicores.\",\n    \"type\": \"gauge\",\n    \"value_type\": \"int64\"\n   },\n   {\n    \"name\": \"memory/limit\",\n    \"description\": \"Memory hard limit in bytes.\",\n    \"type\": \"gauge\",\n    \"value_type\": \"int64\",\n    \"units\": \"bytes\"\n   },\n   {\n    \"name\": \"filesystem/usage\",\n    \"description\": \"Total number of bytes consumed on a filesystem\",\n    \"labels\": [\n     {\n      \"key\": \"resource_id\",\n      \"description\": \"Identifier(s) specific to a metric\"\n     }\n    ],\n    \"type\": \"gauge\",\n    \"value_type\": \"int64\",\n    \"units\": \"bytes\"\n   },\n   {\n    \"name\": \"filesystem/limit\",\n    \"description\": \"The total size of filesystem in bytes\",\n    \"labels\": [\n     {\n      \"key\": \"resource_id\",\n      \"description\": \"Identifier(s) specific to a metric\"\n     }\n    ],\n    \"type\": \"gauge\",\n    \"value_type\": \"int64\",\n    \"units\": \"bytes\"\n   },\n   {\n    \"name\": \"filesystem/available\",\n    \"description\": \"The number of available bytes remaining in a the filesystem\",\n    \"labels\": [\n     {\n      \"key\": \"resource_id\",\n      \"description\": \"Identifier(s) specific to a metric\"\n     }\n    \"type\": \"gauge\",\n    ],\n    \"type\": \"gauge\",\n    \"value_type\": \"int64\",\n    \"units\": \"bytes\"\n   }\n  ],\n  \"common_labels\": [\n   {\n    \"key\": \"nodename\",\n    \"description\": \"nodename where the container ran\"\n   },\n   {\n    \"key\": \"hostname\",\n    \"description\": \"Hostname where the container ran\"\n   },\n   {\n    \"key\": \"host_id\",\n    \"description\": \"Identifier specific to a host. Set by cloud provider or user\"\n   },\n   {\n    \"key\": \"container_name\",\n    \"description\": \"User-provided name of the container or full container name for system containers\"\n   },\n   {\n    \"key\": \"container_base_image\",\n    \"description\": \"User-defined image name that is run inside the container\"\n   }\n  ],\n  \"pod_labels\": [\n   {\n    \"key\": \"pod_name\",\n    \"description\": \"The name of the pod\"\n   },\n   {\n    \"key\": \"pod_id\",\n    \"description\": \"The unique ID of the pod\"\n   },\n   {\n    \"key\": \"pod_namespace\",\n    \"description\": \"The namespace of the pod\"\n   },\n   {\n    \"key\": \"labels\",\n    \"description\": \"Comma-separated list of user-provided labels\"\n   }\n  ]\n }\n. LGTM\n. @k8s-bot test this\n. LGTM\n. @k8s-bot test this\n. Please fix e2e\n```\n--- FAIL: TestHeapster (341.16s)\n    assertions.go:154: \nLocation:   heapster_api_test.go:689\n\nError:      No error is expected but got unexpected metric \"cpu/limit\"\n\nFAIL\nexit status 1\n. @jimmidyson could you please take a look?\n. ping @jimmidyson \n. @jimmidyson thanks!\n.\nk8s.io/heapster/metrics/sinks/monasca\nmetrics/sinks/monasca/data_test.go:55: missing type in composite literal\nmetrics/sinks/monasca/data_test.go:67: missing type in composite literal\n```\n. LGTM\n. The PR is in flight https://github.com/kubernetes/kubernetes/pull/22851\n. Mentioned PR is merged.\n. our target is v1.0\n. PR in flight https://github.com/kubernetes/kubernetes/pull/22993\n. https://github.com/kubernetes/kubernetes/pull/22993 is merged so closing this\n. dup of #1009\n. LGTM\n. @mwielgus merging without LGTM. PTAL later.\n. @mwielgus merging without LGTM.\n. Merging without LGTM\n. @k8s-bot test this\n. cc @fgrzadkowski \n. @k8s-bot e2e test this\n. @k8s-bot test this\n. @jimmidyson could you please take a look?\n. LGTM\n. LGTM just a nit\n. LGTM just a typo\n. LGTM\n. LGTM\n. cc @mwielgus @timstclair @vishh \n. LGTM, though we should probably improve this document since summary-api is now a default source in Kubernetes and it seems to be an optional one from what is written here.\n@timstclair would you like to elaborate a little bit about summary-api? \n. Not yet. My concern here is that after reading the doc it seems that kubernetes source is a default one while kubernetes.summary_api is just an alternative. It'd great to update the docs thought it's rather low priority.\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. @k8s-bot test this\n. LGTM\n. LGTM\n. LGTM\n. The version 1.1.0-beta1 haven't been released yet, so closing this.\n. fixed in #1169\n. @mwielgus we should cherry-pick this, make patch release and bump Heapster version in Kubernetes.\n. LGTM\n. @mwielgus could you please take a look? Thanks!\n. Thanks for reporting.\nv0.20.0-alpha9 is an experimental version. Could you please try to use a stable one like v1.0.2? In the meantime I'm closing the issue. Please re-open in case of troubles.\n. Unfortunately this is out of the scope of the Heapster.\ncc @mwielgus\n. cc @mwielgus \n. I agree with @jimmidyson.\n. cc @fgrzadkowski \n. @bryk @kubernetes/dashboard-maintainers this is something you are interested in. PTAL\n. Let's imagine the situation where we created a pod with some name, it was deleted and then let's say a week later we created another pod with the same name which is a totally different pod. What will be the semantic of historical metrics in this case? Do you plan to verify also pod-uid somehow? \n. But operating on pod uids is a bad user experience.\nI'm ok with saying (possibly in the first version): if you have two pods with the same name historically the data will be mixed. Better approach is to return data only from the newest pod with the given name. For example I created on Monday pod with name my-name and then killed it on Tuesday. I created also a pod with the same name on Friday and it is still running. When querying for historical data for the whole week I'd like to get only metrics for the second pod, so there won't be any data for Mon-Thu. WDYT?\n. fixed in #1332. cc @dchen1107\n. cc @jszczepkowski \n. Which version of Heapster do you run @euprogramador? Released one like v1.1.0-beta1, v1.0.2 or from HEAD?\n. @titilambert please have in mind that we changed the schema in Kubernetes 1.2 just a few weeks ago.\n. @titilambert Heapster is a dedicated solution to monitor Kubernetes cluster. Its requirements and release process are tightly coupled with Kubernetes release process. In Kubernetes 1.1 Influxdb was in version 0.8, while in Kubernetes 1.2 in version 0.9. Also the format of data in InfluxDB between 0.8 and 0.9 has changed, so we had to change the scheme. It was just a few weeks ago and I'm against making another breaking change right now.\nAFAIK in July there should be InfluxDB 1.0. We want to migrate to the new version, so we can add your change then. WDYT?\n. LGTM. Thanks for the change!\n. Version 1.0 should be available soon, so we should migrate to it.\nRelated issue #1197\n. Thank you guys for confirming.\n. Any progress on this? It would be great to include it in Heapster 1.1/Kubernetes 1.3. The code freeze is in 2 weeks from now.\n. ping\n. Just wanted to ensure that this PR doesn't introduce anything new. \n. LGTM, thanks for catching it. Could you please sign the CLA?\n. Closing the PR because CLA is not signed. If you decide to continue working on it please re-open.\n. Closing the PR because CLA is not signed. If you decide to continue working on it please re-open.\n. Could you please try to do what @mwielgus wrote in https://github.com/kubernetes/heapster/issues/1154#issuecomment-226475676. Also please use a stable version of Heapster.\n. @timstclair @vishh could one of you please take a look since you are more familiarized with disk metrics?\n. Closing as obsolete. Please reopen if you still want to work on this.. cc @davidopp @mwielgus\n@lavalamp as we briefly discussed f2f in MTV few weeks ago metrics api types will be defined in Heapster. Is this still valid agreement?\n. @k8s-bot test this\n. @erictune to make it clear (at least what I understood from sync with CSI team):\n1. CSI team is working on 'discovery merger' project which is about to allow other components than apiserver to serve Kubernetes API. There will be a kind of proxy introduced in front of apiserver which will redirect user's queries to appropriate component, so that the user won't notice any difference while talking with the api provider. If you need more details please ask @lavalamp \n2. Heapster will act as 3rd party api server which will handle metrics requests. The types will be defined there to make client generation possible (@caesarxuchao did I understand correct this part?) \n3. For 1.3 we decided with @davidopp and @fgrzadkowski that we don't want to implement those mechanisms and only define first version of the api and make it available in Heapster, that's why I haven't written the implementation proposal.\n. friendly ping @davidopp \n. comments applied\n. Thanks!\n. LGTM, thanks for the contribution!\n. There was a lot of problems with gathering metrics on CoreOS (see https://github.com/kubernetes/kubernetes/issues/30939, https://github.com/kubernetes/kubernetes/issues/27194). Most of them should be fixed in Kubernetes 1.4. Please try to upgrade. I'm closing now, please reopen in case of more issues.\n. This is not tracked by Kubelet/cadvisor so we can't do it easily in Heapster.\n@dchen1107 @timstclair @vishh is there any plan to track such metrics on Kubelet side?\n. Ups, sorry @timstclair for the confusion.\n. We'd be more than happy to accept the contribution. Please contact me or @mwielgus before starting the work.\n. LGTM, just a nit.\nThank you for the contribution!\n. @k8s-bot test this\n. Merging without LGTM to fix integration tests.\n. We'd be more than happy to accept the contribution.\n. @k8s-bot test this\n. I'll add in a next PR.\n. @k8s-bot test this\n. contribution is welcome\n. Will do. Thanks for the review!\n. Ok\n. @k8s-bot test this please\n. cc @mwielgus \n. cc @mwielgus \n. Need to fix hooks. Marking as WIP.\n. @k8s-bot test this\n. Tests are passing so removing WIP tag. @mwielgus PTAL\n. Should be merged after #1166\n. @k8s-bot test this please\n. Thanks for reporting!\nWhich data source do you use: cadvisor or summary? Currently exactly one of the sources should be used in Heapster. Also could you please share some logs from Heapster? Did you see there:\nSummary not found, using fallback\nAnyway if you can see different values in cadvisor and summary Kubelet endpoints it seems to be a huge problem. Did you have a chance to query kubelet directly for both endpoint and see that the responses differ?\ncc @timstclair @dchen1107 @fgrzadkowski \n. closing in favor of https://github.com/kubernetes/kubernetes/issues/27194\n. It turned out to be an issue on OS level. Closing in favor of https://github.com/kubernetes/kubernetes/issues/30939\n. LGTM\nThanks for the contribution!\n. Could you please try to do the following:\n- create a pod with empty-dir mounted\n- login into the container and check whether you are able to write to the directory\nIf this won't work this bug is not related to Heapster/Influxdb. In such case please close the issue and create a one in Kubernetes.\n. Please do not merge this until we will cut release 1.1 branch.\n. LGTM, thanks for the contribution!\n. cc @fgrzadkowski @jszczepkowski \n. cc @davidopp \n. @titilambert thanks for the contribution. We do have code freeze for 1.3 right now, so the PR has to be suspend for a week or two. Apologies for that. We will back to it as soon as we release Heapster for Kubernetes 1.3 (hopefully early next week).\n. @titilambert thanks for the idea. Sorry for the delay, we need to think a bit about the scope of Heapster project. I'll keep you updated soon.\n@davidopp @fgrzadkowski \n. @titilambert @djsly I'm really sorry about the delay. I was on vacation and then on business trip.\nWe definitely want to support collecting Kubernetes objects related metrics. We'd like to have MVP for 1.5. Unfortunately the scope of the project and especially the design hasn't been discussed yet, so it's not clear whether Heapster should or should not be involved into the pipeline. Because of that there is no much sense to accept the PR right now, because we'd like to avoid situation when we will have to roll it back in the next Heapster release or support legacy functionality.\nThe discussions on the mentioned project should occur in September, I'm happy to include you guys into it, especially taking into account the fact that you have already some experience with it.\nOnce again apologies for the delay. \n. @activars we don't plan to do it soon, but feel free to contribute or at least create an issue, so maybe someone else will do it.\ncc @mwielgus \n. This is fixed.. Closing as explained in https://github.com/kubernetes/heapster/issues/1174#issuecomment-237577429. If we decide we would go this path in the future - we will reopen this.\n. By default cadvisor collect metrics every 10s so it's expected that metrics will be missed for pods when Heapster metrics_resolution is set to 10s. On the other hand aggregation across cluster requires at least one collected metric for some pod which is usually true.\nIn general if you are using default Kubelet setup you shouldn't decrease the the value of metrics_resolution. If you need to do it you should also change flag housekeeping_interval in Kubelet, but there is no guarantee it would work.\n. LGTM, Please squash commits and sign the CLA.\n. Also please make sure e2e tests are passing\n. @nickschuch This PR is inactive for a longer period of time. If you plan to work on this, please reopen.. There is no support for pod status in InfluxDB.\n. This is problem was on Kubelet side and is fixed now. Please create issue in Kubernetes in case of more such problems.\n. You should either use host networking for Heapster pod or configure your cluster in a way that the node has a regular name not 127.0.0.1. The current problem is that node name is resolved to Heapster localhost. Please reopen in case of more problems.\n. LGTM\nThanks for the contribution!\n. Try to use Heapster in stable version and reopen if it fails.\n. Thanks @boj for confirming!\n@theobolo we are about to release Kubernetes 1.3 (should be available withing few days). Are you ok to wait until then?\ncc @mwielgus \n. Looking forward for an update.\n. Haven't heard any update so closing the issue. @theobolo please re-open in case of problems.\n. Do you have enough quota in GCM to do it? AFAIK this is by ~50k requests per day which is ~30 requests per minute. In each request we can send up to 200 timeseries, which allows us to export 30*200=6000 timeseries per minute. Each container uses ~30 timeseries (each metrics for each container is a different timeseries) and every minute we export all metrics for all containers so in fact using default quota you are able to monitor up to 200 containers.\ncc @a-robinson @fgrzadkowski @mwielgus \n. I can confirm that I observe similar problem. I'll try to fix it tomorrow.\n. The problem is that filesystem metrics are exported without resource-id label and GCM treats them as duplicates.\n. I'll build a new image tomorrow.\n. The new version is released https://github.com/kubernetes/heapster/releases/tag/v1.1.0\nPlease let me know whether you problem is fixed.\n. Well it's not fixed on Kubernetes HEAD yet, it requires https://github.com/kubernetes/kubernetes/pull/27542 to be merged, which should happen pretty soon (the PR is LGTMed). You can wait or cherry-pick the change to your branch.\n. You can set ENABLE_CLUSTER_MONITORING to none and then run you own instance of Heapster.\n. As @chrislovecnm noticed heapster service already exists in GKE. Regarding problem with rc I need to update configuration files. In the meantime try to use this one though you have to get rid off jinja markdowns. Also if you will run another Heapster instance on GKE with the same label set heapster service will redirect queries to both instances in a random way. If you want to play with Heapster on GKE you should create/update a cluster to have monitoring disabled. \n. known problem: https://github.com/kubernetes/kubernetes/issues/27194\n. cc @mwielgus @fgrzadkowski \n. Not now, but we are open to the contribution. ElasticSearch sink is maintained by community.\ncc @huangyuqi @mwielgus @titilambert \n. @jamiehannaford cool. The URL path is as for now the only possible configuration option. Please agree with @huangyuqi the set of configuration params.\n. It was done by @jszczepkowski here https://github.com/kubernetes/kubernetes/pull/28109 and then reverted here https://github.com/kubernetes/kubernetes/pull/30080 due to an issue with PetSet implementation.. cc @mwielgus \n. Waiting for @huangyuqi to grant LGTM\n. Thanks @jamiehannaford for the contribution and @huangyuqi for the review!\n. @jamiehannaford I'll release v1.2.0-beta1 hopefully this week with this change included.\n. Sorry for the delay. It's here https://github.com/kubernetes/heapster/releases/tag/v1.2.0-beta.1\n. Closing in favor of #1199. I'm happy to accept a contribution.. Ups! Sorry, I meant #1274. @caesarxuchao could you please take a look into this?\n. ok to test\n. @mkumatag I prefer @huangyuqi to grant LGTM since he know the context.\n. Thanks @mkumatag for the contribution, thanks @huangyuqi for the review.\n. LGTM\n. LGTM\n. cc @jamiehannaford\n. LGTM, thanks for the fix.\nPlease sign CLA before we can merge this\n. @prashantchitta This PR is inactive for a longer period of time. If you plan to work on this, please reopen and sign CLA.. @DirectXMan12 please grant LGTM when you think it's ready to merge. Then I'll take a look.\n. closing as obsolete. cc @AlmogBaku \n. @liubog2008 This PR is inactive for a longer period of time. If you plan to work on this, please reopen.. @fgrzadkowski \n. @erictune Heapster serves Metrics API and some time in the future (for 1.5?) it will be a kind of API server (details TBC), so the latter is true. So that eventually the authorization stuff in Heapster should be the same as in Kubernetes.\n@DirectXMan12 please correct me if I'm wrong: the solution you propose is a temporary solution to make the effort of pushing custom metrics to Heapster possible rather than our final goal.\n. LGTM\n. @k8s-bot test this\n. closing as obsolete. LGTM, thanks!\n. Failure unrelated.\n. As Solly wrote you should use Heapster 1.1 with Kubernetes 1.3. Heapster 1.2 is targeted for Kubernetes 1.4.\n. LGTM. Thanks for the change!\nTest failures unrelated. \n. LGTM, just a few nits\n. LGTM, thanks!\n. LGTM\n. cc @mksalawa @mwielgus @taimir \n. cc @mksalawa @mwielgus \n. @andyxning I bumped cadvisor to v0.23.2-79-gc6c06d4 in #1252. We're trying to keep this in sync with the version in Kubernetes.\n@lattwood probably not. I posted an update on #1237 \n. fixed in #1282\n. cc @mksalawa \n. @k8s-bot test this\n. @k8s-bot test this\n. @k8s-bot test this\n. @k8s-bot test this\n. cc @mksalawa \n. closing in favor of #1282\n. cc @mksalawa \n. failure unrelated\n. Thanks for reporting. I'll take a look. Which version of Heapster/Kubernetes do you use?\n. cc @huangyuqi @AlmogBaku \n. fixed in #1319\n. There is addon updater, which periodically (every ~5 minutes) checks whether cluster services are up to date. As a side effect it removes all pods with the annotation from kube-system namespace which were not created from manifests.\nRemoving the annotations is correct workaround. Another option is to run Grafana and Heapster in a different namespace.\n. Thanks @AlmogBaku \n. This is working as intended. First it doesn't make sense to do aggregation from cumulative metrics, because usually containers have different start times. Second we are collecting metrics also for system containers: kubelet, kube-proxy, system and docker, so if we make an aggregation from all containers running on the node at least in theory this should be equal to what is collected for the node itself. In practice since we sometimes metrics for a container will be missed and cadvisor collects them at different times the current method will be more accurate.\nRequests and limits are aggregated from running pods because it's not defined for a node itself. It's a legacy functionality required by previous implementation of cluster autoscaler.\nThanks for your help. Sorry, that I haven't responded faster but I somehow missed it.\n. cc @mwielgus @mksalawa \n. fixed in #1271\n. Liveness probe need to be setup in Kubernetes as well.\n. This is fixed. Thanks @mksalawa \n. cc @mksalawa \n. @DirectXMan12 many thanks for the help!\n. @k8s-bot test this\n. cc @mwielgus \n. You're right. It works on HEAD. Sorry for the confusion!\n. LGTM\n. fixed in #1257\n. @huangyuqi  could you please take a look?\n. Thanks @huangyuqi for offering your help.\n. @AlmogBaku thanks for the contribution.\n@huangyuqi thanks for the review.\n. LGTM\n@sarahnovotny could you please also take a look? \n. LGTM, just a nit\n. LGTM\n. closing as explained in #1248\n. LGTM, thanks!\n. Please create an issue\n. As for now we support InfluxDB in 0.12 as default. Once 1.0 will be released we will migrate to it. Also you are using very old Heapster. Try the latest stable release.\n. @feelobot do you mean Heapster v1.2.0-beta.1 works with Influxdb v1.0.0-rc2?\n. Leaving closed. We have an issue to migrate to 1.0 once it will be released.\nhttps://github.com/kubernetes/heapster/issues/1143\n. LGTM, thanks Marek\n. This is working as intended. Our intend was to not duplicate logic of api server. However if you want add a logic which displays containers from the pod I'm happy to accept the contribution:). @aleksandra-malinowska this is a good initial task in Heapster.. LGTM\n. Please squash commits\n. Something went wrong with your rebase.\n. LGTM from me. Waiting for @huangyuqi \n. Thanks for the PR and the review.\n. LGTM, thanks!\n. First, could you please try to use Kubelet Summary API as the source? \n--source=kubernetes.summary_api:''\n. @bryk for dashboard releated questions\n. This should be done together with building the image in container (in the similar way to how Kubernetes is built now). I'm happy to accept contributions!\n. LGTM\n. Thanks for the contribution. Please sign CLA\n. @k8s-bot test this\n. Commits squashed\n. Thanks @bprashanth for the info. Anyway since we have our own trusted image it'd be better to use it.\n. Thanks for the info. I'm prioritizing this issue.\n. The version 0.19 is no longer supported. Please try to the latest stable release (1.1.0).\n. Let's wait with merging this until we cut release-1.2 branch (hopefully tomorrow).\n. The branch is cut now. Please update godeps.\n. LGTM. Will merge once you will update Kubernetes version in godeps\n. Feel free to drop a PR. The docs are definitely outdated.\n. LGTM\nThanks for the fix!\n. This will be split into smaller PRs. \n. LGTM\n. LGTM\n. Thanks for the PR.\n. which \"time\" do you mean? \ntimestamp is the point in time in which the value was observed. \nlatestTimestamp is the maximum value of all timestamps\nPlease of re-open in case of more questions\n. Looks reasonable. @nikhiljindal could you please take a look? Our goal is to refactor Heapster to reuse genericapiserver library. This is the first step which creates apiserver without any resource.\n. @DirectXMan12 @deads2k we are not sure how the current auth model in Heapster works, so to not break anything we are starting a new http server (using genericapiserver) library on a different port which uses exactly the same auth model that federated-apiserver uses. We will reach you to figure out the auth issues.\n. @nikhiljindal thanks for the review.\n@mksalawa could you please flag-gate this change?\n. LGTM\n. Tests on Travis passed, but Travis hangs forever.\n. We are working on overall monitoring vision in Kubernetes, which should be discussed on sig-instrumentation meetings soon. If you are interested feel invited. In the meantime I'm closing the issue, because it's about the architecture of monitoring pipeline in general.\ncc @fgrzadkowski @fabxc \n. This is working as intended. There is nothing to display at this level of the path. Try to query for something more concrete, for example take a look into https://github.com/kubernetes/heapster/blob/master/docs/model.md\n. The are two GUI-s showing metrics which comes out of the box on GCE: Grafana and dashboard.\nGrafana is showing graphs based on data in InfluxDB, where data are being exported by Heaspter.\nDashboard is directly querying Heapster using model api.\n. If you are creating your cluster with kube-up.sh, once your cluster is setup you can see a list of avaialble services, something like this:\nkubernetes-dashboard is running at https://<master-ip>/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard\nGrafana is running at https://<master-ip>/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana\nThis can be printed with kubectl cluster-info command.\nPlease have in mind that if you are using GKE there is no Grafana by default.\n. Unfortunately there is no such guide.\nThe suggested options for displaying metrics on GKE are:\n- dashboard as described above, more info here: https://cloud.google.com/container-engine/docs/oss-ui\n- Stackdriver\nBoth works out of the box in your cluster (unless you disabled monitoring).\nIf you really, really want to have Grafana in your cluster you can try to create monitoring pipeline on your own with heapster exporting to influxdb and grafana reading from it. You may want to use cluster/addons/cluster-monitoring manifests. You should change the namespace to avoid conflict with existing monitoring pipeline. Also please have in mind that heapster-controller.yaml is a template and you should evaluate it (probably manually).\nTo sum up the procedure should be:\n1. change namespace from kube-system to another one\n2. evaluate Heapster manifest\n3. create all resources form the directory\n. @wojtek-t could you please take a look?\n. LGTM from me, @nikhiljindal could you please take a look?\n. LGTM,\nThanks @mksalawa for the change!\nThanks @nikhiljindal @deads2k for the review.\n. Travis time-outed.\n. > Please visit https://cla.developers.google.com/ to sign.\n. I'm sorry, but we can't merge any pull request without CLA. Closing until the CLA issue can be resolved.\n. LGTM\n. Feel free to merge on green Jenkins.\n. Sorry, this is out of the scope of Heapster. There is an effort in Kubernetes to support not only Docker as container runtime (see https://github.com/kubernetes/kubernetes/blob/master/docs/proposals/container-runtime-interface-v1.md). I think we should rather add a functionality to CRI to proxy container runtime events to Kubernetes. You can reuse some Heapster code to create your own solution if you need it.. @commixon could you please take a look into #1313 \n. LGTM, thanks for the fix. @AlmogBaku thanks for your contribution. I'm fine with this PR, but I'm not familiarized with ES. We need someone to review the code. @commixon offered his help.\nIn the meantime could you please reorganize commits into the following pattern:\n- godeps changes\n- real changes\nIf you think you need more than 2 commits please explain why.\n. @k8s-bot ok to test\n. @AlmogBaku I don't know how to fix this. If Jenkins is green we can assume that tests pass.\n. @huangyuqi please create an issue and we can discuss it there. Thanks for offering the help\n. @andrejvanderzee is there any chance your problem with negative values is related to https://github.com/kubernetes/kubernetes/issues/30939?\n. Could you please create a new issue with you problem?\n. I'm ok with the change. Can one of the reviewers confirm that it's ready to merge by writing LGTM?. @k8s-bot test this. Jenkins failure unrelated. I'm not familiarized with Glide but I really want to get rid off Godeps, which is big pain point. Anyway I'd like to wait for Kubernetes successful migration.\n. Closing as explained in #1335. @owenmorgan did you try to use @Jaware's attitude? The default way to access Grafana is via Kubernetes proxy (use kubectl cluster-info for you address). Otherwise you have to change the config as Jaware proposed.\n. I agree, but proxy is available by default and is secured. If you want to use public IP you can easily change the configuration of both Grafana service and Grafana pod. Please have in mind that in this case you have to be able to create public IP and the connection is not secured in any way. I'm against for making the latter the default but I think we should add appropriate comment how to easily change the configuration.\n. @agsola regarding #2 could you please use Slack channel and mention me there? sig-autoscaling channel seems to be appropriate spot for this. Could you please also post there:\n- kubernetes version\n- heapster version\n- events from HPA\n. @agsola great. Could you please take a look into HPA doc http://kubernetes.io/docs/user-guide/horizontal-pod-autoscaling/ and create and issue on https://github.com/kubernetes/kubernetes.github.io which explains what is missing there from your point of view?\ncc @jszczepkowski \n. @timstclair when summary api was introduced?. Thanks @timstclair!\n@tacy could you please try what @timstclair proposed? I'm closing this, please reopen in case it won't help.. LGTM\n. LGTM\n. @nikhiljindal could you please take a look?\n. LGTM\n. Please have in mind that we are working on Kubernetes monitoring architecture. It's not clear yet what will be the role of Heapster if the vision will be agreed. \n. It won't be possible to do it due to backward compatibility and pretty short timing. I think some of the stuff would happen for 1.5 and some will be spawned across 1.6 or even 1.7. \n. @k8s-bot ok to test\n. Closing in favor of #1334\n. @k8s-bot ok to test\n. I'm fine with the test, but we need someone who understand the Monasca to take a look. Do you have any suggestion?\n. cc @cheld \n. Thanks @taimir for the contribution. Thanks @cheld for the review.\n. tests should be fixed by #1394\nfailures not related. @k8s-bot ok to test\n. @aegixx Please rebase\n. IIUC this PR adds Chronograf UI. I have no idea what is it and why do we need it, especially in the default setup. Who will maintain this? Could you please elaborate a bit? \n. @aegixx as I wrote in https://github.com/kubernetes/heapster/issues/1316#issuecomment-252897423 I'm against having Grafana with public IP in the default setup. How about just adding a comment that if you are changing service type to LoadBalancer you should also update GF_SERVER_ROOT_URL arg in Grafana pod spec?\nThere is already Grafana RC specification. I'm ok with migrating it to Deployment but please remove RC in this case.\nAlso please follow the same patterns as in https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/cluster-monitoring/influxdb (label names, etc.).\n. LGTM, thanks\n. Please sign the CLA\n. closing in favor of #1415. Which version of Heapster and Kubernetes do you use?\n. Custom metrics feature is in alpha state and is going to be removed soon. There is a new version of it in progress. @DirectXMan12 is driving the effort. You can see a proposal here https://github.com/kubernetes/kubernetes/pull/34754\n@mwielgus @jszczepkowski FYI the current implementation of HPA with custom metrics seems to not work correctly.\nClosing the issues since you resolved the problem (excepts from custom metrics). If you need more help please create an issue on Kubernetes or use slack.\n. @k8s-bot ok to test. @k8s-bot test this. Closing as obsolete. Please reopen if you still want to work on this.. LGTM\n. @k8s-bot ok to test\n. @jbdalido thanks for the contribution. Do you know anyone who is familiarized with Kafka and can review this PR?\n. @huangyuqi could you please take a look?\n. Closing as obsolete. Please reopen if you still want to work on this.. @k8s-bot ok to test\n. LGTM, thanks for the PR. Will merge on green.\n. Did you bump any deps? I'd expect this change to be transparent in term of added/removed files.\n. @luxas . As I wrote in https://github.com/kubernetes/heapster/issues/1314#issuecomment-253418553, I think Heapster should follow Kubernetes standards. According to what @thockin we want to stay with godeps for a while. Once Kubernetes is migrated we will revisit this decision. Closing the PR in the meantime.. follow up https://github.com/kubernetes/kubernetes/issues/34216\n. Please let me know once this is ready to merge.\n. I'll merge on green.\n. @bergerx yes, I'd like to have it done for Kubernetes 1.5. Unfortunately I don't have enough free cycles on my side, so any contribution is welcome. Closing in favor of #1143\n. @k8s-bot ok to test\n. Did you try git commit --amend? In the worts case you can try to just create a new PR.\nAlso I'm not familiarized with Riemann at all, so we need to find someone to review it.\n@erimatnor @jfoy @jsoriano @huangyuqi are you able to make the review?\n. @k8s-bot test this\n. Who is reviewing this? Please let me know when this is LGTMed and ready for the review. @shmish111 please sign the CLA. @k8s-bot test this. @k8s-bot ok to test\n. This is unrelated to influxdb. How about putting this into heapster/docs/debugging.md?\n. @agsola I'm fine with linking to docs/debbuging.md from docs/influxdb.md\n. Agree with @AlmogBaku . @k8s-bot ok to test\n. @jsoriano thanks for your contribution! We need someone familiarized with Graphite to review this. Do you know anyone?\n@yuvipanda @mzupan @stevezau are you interested in reviewing this?\n. Who is reviewing this PR? Please let me know once it's ready to merge by writing LGTM. @theairkit thanks for offering you help. Please review and let me know once this is ready to merge.. @jsoriano thanks for the contribution!. LGTM, thanks!\n. I'm not sure whether I understand this issue. Currently eventer reads events from Kubernetes API server and exports them to a storage backend to archive them. If you need real time events why not just watch api server directly?\n. @andyxning sorry, this is out of the scope of eventer, which is supposed to be a simple archiver for events with no logic and no api. Anyway feel free to reuse eventer's code.\n. I'm still a little bit confused. Do you want to have a push endpoint which accepts events from other sources than Kubernetes api server?\n. What is the responsibility of Api Server on your picture? Is this a sink in eventer which exports some api? If yes this is out of the scope of project, since events are already served by api server.\n. I suggest changing Alarm implementation to watch events directly from Kubernetes API server. If this is not an option for you should write a sidecar container which watches events from Kubernetes API server and push them to your alarm path. \n. @huangyuqi could you please reply?\n. @k8s-bot ok to test\n. LGTM. Thanks for the PR. Please sign CLA. \n. @farcaller it looks good\n. @k8s-bot ok to test\n. LGTM, thanks! Will merge on green\n. Currently we only support InfluxDB in version 0.12. The plan is to migrate to 1.0 for Kubernetes 1.5.\nClosing in favor of #1143\n. As a first step we can just get rid off test-unit-cov\n. Would you like to send PR with the fix?\n. Travis passed successfully in ~10 minutes\n. LGTM, thanks\n. What this chart comes from?\n. @bryk could you please take a look?\n. Take a look into https://github.com/google/cadvisor/blob/master/info/v1/container.go#L326. Shouldn't you use WorkingSet instead?\n. cc @timstclair \n. Could you please create an issue in cadvisor repository and then close this one?\n. @Ryan-Dmello see https://github.com/kubernetes/kubernetes/blob/master/CONTRIBUTING.md#contributor-license-agreements. closing in favor of #1387. This is unrelated to Heapster. Please take a look into http://kubernetes.io/docs/user-guide/kubectl/kubectl_logs/ \n. Which version of Kubernetes and Heapster do you use? Which source: summary api or cadvisor?\nIt should be rather a bug with system than with Heapster itself because it only appears on CoreOS nodes. \nHow often does the problem occur? Could you please do the similar investigation that @Random-Liu did here https://github.com/kubernetes/kubernetes/issues/27194#issuecomment-226318668?. @Thermi we haven't tested Heapster 1.2 with Kubernetes 1.3. Do you have the same issue with Heapster 1.1 nad Kubernetes 1.3? Also could you please verify cadvisor output?\n. @k8s-bot ok to test. LGTM, just a nit. @andyxning something went wrong with rebase. Now you have 100+ files modified in this PR. LGTM, thanks!. I'm happy to accept a contribution.\n. LGTM, please squash commits before merge. +1 to https://github.com/kubernetes/heapster/pull/1367#issuecomment-262653259. @k8s-bot ok to test. Thanks @andyxning. I'll merge once I'll be able to make tests green.. @k8s-bot ok to test. LGTM, will merge on green. Bumped cluster version in #1394. Let's see whether it will help. @k8s-bot test this. @kubernetes/test-infra-maintainers e2e tests are failing due to some infra problems. Could you please take a look? . @k8s-bot test this. @ixdy thanks. It's weird, because I updated Kubernetes version in #1394 and the method was invoked with \ngodep go test -v --timeout=60m ./integration/... --vmodule=*=2  --namespace=heapster-e2e-tests --kube_versions=\"1.3.6\". @k8s-bot test this. @ixdy is test bot working correctly? . @ixdy thanks for the info!. @andyxning could you please rebase the PR?. @andyxning thanks!. Thanks @huangyuqi for offering you help. Feel free to merge once it's ready.. @k8s-bot ok to test. @miaoyq thanks for the contribution. Apologies for the delay.. Closing as WAI (see @mwringe comment).. Could you please describe your problem and the solution? Thanks!. Thanks for your feedback. I'm happy to accept the contribution.. @Crassirostris could you please take a look?. Thanks @andyxning for the change\nThanks @AlmogBaku for the review. @jonaz thanks for reporting. I'm happy to accept your contribution.. Closing as you were able to resolve the issue.. Can anyone review it? @AlmogBaku?. closing per @AlmogBaku comment. If you need any sink specific config why not making this a part of the sink configuration rather than a top level flag?. I'm not familiar with ES, so assuming some will approve your change (maybe @AlmogBaku?)  I'm fine with adding a parameter to ES config. I'm against adding this as a top level flag since it's ES specific.. @k8s-bot ok to test. @AlmogBaku do you want to review this PR?. LGTM, please sign cla/linuxfoundation. @k8s-bot test this. The failure seems to be related:\n```\n--- FAIL: TestGetNodeHostnameAndIP (0.00s)\n    Error Trace:    kubelet_test.go:380\n    Error:      Not equal: \"127.0.0.2\" (expected)\n                    != \"127.0.0.1\" (actual)\nFAIL\nFAIL    k8s.io/heapster/metrics/sources/kubelet 0.631s\n```\nPlease fix. cc @AlmogBaku @huangyuqi  @jamiehannaford. cc @AlmogBaku @huangyuqi  @jamiehannaford. @AlmogBaku @andyxning is #1380 fixing this?. Could you please try to use a new stable release of Heapster v1.2.0?. @k8s-bot ok to test. @keithballdotnet please sign the cla . @luxas thanks a lot for you change. Do you have anyone in your mind who is familiarized with build stuff and can review it?. @luxas I can take a look from high level Kubernetes perspective, but I don't have enough expertise with go build.. thanks @vishh . Thanks for your contribution.. @thomasfricke did it work with Heapster 1.1?\n@timstclair has anything changed in Kubelet which may have impact to this?. @jimmidyson @mwringe @burmanm @DirectXMan12 any ideas?. Could you please try to upgrade Heapster to v1.2? Unfortunatelly we are not able to test every possible combinations of Heapster and Kubernetes versions. The following combinations should work:\n- Heapster 1.0 with Kubernetes 1.2\n- Heapster 1.1 with Kubernetes 1.3\n- Heapster 1.2 with Kubernetes 1.4, 1.5\n. @mashayev I'll try to reproduce soon. Is there any reason why your are using cadvisor endpoint instead of summary api? The endpoint you are using is deprecated and supported only due to backward compatibility. It has negative performance impact both and kubelet nad Heapster.. @k8s-bot test this. @k8s-bot test this. @k8s-bot test this. @k8s-bot test this. @Crassirostris Jenkins is passing again. PTAL. @andyxning thanks for the explanation.. How about doing it in https://github.com/kubernetes/heapster/releases? Is this sufficient solution?. @luxas . As discussed on sig-instrumentation Slack channel this is probably a bug in CoreOS cgroups. . @ezeev thanks for the contribution. We need someone familiar with the  Wavefront to review it. Do you know such person?\ncc @mrcrgl . @mrcrgl Cool, thanks! Will you be interested in becoming sink owner (see https://github.com/kubernetes/heapster/blob/master/docs/sink-owners.md) once this is merged?. It needs LGTM from @mrcrgl to be merged.. Closing as obsolete. Please reopen if you still want to work on this.. Thanks @aleksandra-malinowska @DirectXMan12 . As discussed offline SGTM. FYI @fgrzadkowski added all sink maintainers to @kubernetes/heapster-reviewers team.. @jamtur01 @mcorbin please send a PR with appriopriate docs update. To be clear, one of you can do it. I doesn't have to be two separate PRs.. @k8s-bot ok to test. @bavarianbidi can you please squash commits? This may help for the cla issues. Alternatively maybe you can close this and create a new PR?. cc @mwielgus @krzysztofgrygiel . Closing as obsolete. Please reopen if you still want to work on this.. @k8s-bot ok to test. Closing as obsolete. Please reopen if you still want to work on this.. @jackzampolin thanks a lot for your contribution! Did you have a chance to test your change with grafana?. @jackzampolin could you please rebase?\n@mwielgus could you please take a look?. I've published https://github.com/kubernetes/heapster/releases/tag/v1.3.0-beta.0. Added milestone. Will draft the new release early next week.. Sorry, I really can't understand what you mean. Please reopen with better description of your issue. @mpvaniersel could you please rebase?\n@luxas could you please take a look?. @luxas did you have such problem?. fixed in #1448. Sorry, this is out of the scope of Heapster. There is a plan to build in 2017 infrastore - a repository of data about Kubernetes infrastructure which should have such metrics. . We do not plan to maintain Riemann image. Also according to https://github.com/kubernetes/heapster/blob/master/docs/sink-owners.md we are planning to remove support for Riemann unless we will find owners for it.\ncc @DirectXMan12 @jsoriano. @AlmogBaku could you please accept the invitation to heapster-reviewers?. LGTM, please rebase. @DirectXMan12 can you please take a look?\ncc @jszczepkowski . Heapster  v0.18.0 is no longer supported. Please use a new stable release.. @timstclair @dashpole do you know what can cause this issue?. cc @AlmogBaku @luxas @chulkilee @DirectXMan12  . I'll release v1.3.0-beta.0 in a few minutes (assuming tests will pass), then try to review/merge a bunch of opened PRs and release the next version hopefully next week. . #1415 requires some work. There is a bunch of other PRs waiting to be merged, that's why I suggested to release v1.3.0-beta.0 now and v1.3.0-beta.1 early next week once all those changes will be merged. . LGTM, thanks for writing it down!\nFYI @fgrzadkowski @mwielgus . @timstclair @vishh can one of you take a look? Does it make sense to have those metrics?. Removing out of 1.3. @andyxning apologies for the delay. I'm fine with this PR. As @DirectXMan12 wrote please have in mind that /stats endpoint is deprecated in Kubelet and will be removed from there at some point, especially Heapster while using it consumes much more resources.\nAt the same time it's not deprecated in pure Heapster to some degree - we just need to tune Heapster a bit to work with standalone cadvisor.. /lgtm. LGTM. @Crassirostris could you please check this and build appropriate images if needed?. @luxas this is about old images. Once #1454 is merged we can release a new images.. Thanks @Crassirostris . LGTM. @papile see #1478\nsorry for the troubles. LGTM\ncc @DirectXMan12 . @andyxning thanks for the answer\n@zte-wangning pelase reopen in case of more questions. @k8s-bot ok to test. @AlmogBaku / @andyxning / @huangyuqi. @jgoclawski thanks for the contribution. @luxas thanks for the review.. @CallMeFoxie please squash commits before merge. LGTM. Closing as duplicate. LGTM. Apologies guys for the confusion. There an issue with gcr.io (#1478).  The images were pushed for specific architecture but there is not generic one. Please use the approach from @idcrook's https://github.com/kubernetes/heapster/issues/1469#issuecomment-274218852.. Closing in favor of #1478. @r0bj out of curiosity why do you need lower metric-resolution?. LGTM, thanks!. LGTM, thanks\nFYI @DirectXMan12 . @fgrzadkowski could you please add @jamtur01 and @mcorbin to heapster-reviewer?. @k8s-bot test this. @jamtur01 thanks!. closing in favor of #1469. LGTM, thanks!. ping @luxas . see #1478\nsorry for the troubles. LGTM, thanks.\nPlease squash 2nd and 3rd commit.. LGTM, thanks!. cc @bskiba . @AlmogBaku I've seen this issue recently. It's a flake.\ncc @aleksandra-malinowska . @k8s-bot test this. @k8s-bot test this. Grafana is available in version v4.0.2 but you need specify your architecture explicitly due to a problem with multi-arch release tool. See #1478. Sorry for the confusion.. Jenkins is green now. @theairkit PTAL. Thanks @theairkit @jsoriano . We can use a different image. @timstclair what is the recommended base image with package manager?. LGTM, thanks!. LGTM, thanks!. @k8s-bot test this. LGTM. Closing in favor of #1420. In the meantime please use:\ngcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\ngcr.io/google_containers/heapster-grafana-amd64:v4.0.2\n(or your architecture if you have something else than amd64).\nSorry for the confusion. @frankgreco this should be fixed now by #1496.. LGTM. ```\nk8s.io/heapster/metrics\nmetrics/heapster_test.go:33: imported and not used: \"k8s.io/heapster/metrics/sinks/metric\"\nmetrics/heapster_test.go:45: undefined: metricsink in metricsink.MetricSink\nPlease fix. @kgrygiel @mwielgus @DirectXMan12\n@Demonsthere @jlalouette @jmn @jonaz @judexzhu @karunakar1122v @lilnate22 @mashayev @whereisaaron . @jlalouette thanks for the update. Do you mean `heapster-grafana-amd64:v4.0.2` does not work with `heapster_influxdb:v1.1.1` or Grafana in general does not support InfluxDB in version 1.1? If the former this issue is about it, to make sure that our images play well together.. @bergquist does Grafana support InfluxDB in version 1.0+?. Great! Then we need to make sure that our images play well together.. @KarolKraskiewicz any update?. LGTM, thanks! please sign the CLA.. It's ok right now. LGTM, thanks!. LGTM, thanks!. I agree with @DirectXMan12. This will be avaialble in 1.3.0 and is not important enough to make a patch release.. LGTM, thanks!. Closing as obsolete. Please use  `gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1`. LGTM, thanks!. @astropuffin ETA? I want to cut release ASAP and this is the only PR I'm waiting for.. @k8s-bot ok to test. Thanks!. Obsolete: currently there is beta.1. @XiLongZheng today;). LGTM, thanks!. LGTM, thanks!. /lgtm. Does the following command work correctly:\nkubectl create -f deploy/kube-config/influxdb/\nwith dashes?. @k8s-bot ok to test. LGTM, please rebase.. /lgtm\n. LGTM, thanks a lot for doing it!. LGTM. LGTM. LGTM. @timstclair @vishh any ideas?. LGTM. Moving out of 1.3. Support for api-server mode is rather a proof of concept, than a proper implementation.. @k8s-bot ok to test. The problem is with CLA. @dieterdemeyer did you sign it?. @k8s-bot ok to test. @k8s-bot test this. LGTM, thanks!. @k8s-bot ok to test. Closing, as it's inactive for 6+ months. Please reopen if you still want to work on this.. @krzyzacy can you please help with debugging the issue here?. The test passed with the old version here https://github.com/kubernetes/heapster/pull/1535 a couple of minutes ago.. @krzyzacy both locally and on Jenkins I can see:\n=== RUN   TestHeapster\nE0316 23:05:18.324786   14810 framework.go:166] failed to tear down cluster - \"exit status 1\"\nBringing down cluster using provider: gce\n!!! kubectl appears to be broken or missing\n!!! Cannot find kubernetes-server-linux-amd64.tar.gz\nRequired binaries appear to be missing. Do you wish to download them? [Y/n]\nE0316 23:05:18.817638   14810 framework.go:149] failed to bring up cluster - \"exit status 1\"\n... Starting cluster in europe-west1-c using provider gce\n... calling verify-prereqs\n... calling verify-kube-binaries\n!!! kubectl appears to be broken or missing\n!!! Cannot find kubernetes-server-linux-amd64.tar.gz\nRequired binaries appear to be missing. Do you wish to download them? [Y/n]\n``. Is this somehow related to the new image?. @k8s-bot test this. @krzyzacy thanks for help. The issue was in Kuberneteskube-up.sh` script: starting from 1.5 it requires some confirmations. Jenkins is happy.. cc @kubernetes/heapster-reviewers @kubernetes/heapster-maintainers . LGTM. /lgtm. https://github.com/kubernetes/heapster/pull/1577#issuecomment-288213958\n+1. Looks good. Please rebase.. LGTM. /lgtm. Thanks for your contribution. Apologies for the delay.. @DirectXMan12 can you please investigate?. cc @mwielgus @x13n . cc @mwielgus @x13n . I was not aware of any problems with pod_nanny permissions. If there are any we need to update rbac policies so that the pod nanny has appropriate permissions to do its job.\nI agree that it's a weird situation when Heapster pod wants to modify itself and long term we may want to introduce maybe a separate controller for this purpose. See https://github.com/kubernetes/kubernetes/issues/46763. @x13n could you please verify this?. I had similar thoughts many times in the past and eventually came to the conclusion that there is a lot of template boilerplate + addon resizer out of the box in Kubernetes deployment version, so it's not possible to just create Heapster deployment without using kube-up.sh scripts. At the same time there was a number of issues filed in Heapster repo about not working deployment files so it seems there is a number of folks using them.\nI think at some point it would be great to have them in a single place but I'm afraid we are not ready for this yet.. @bowei @dashpole @timstclair @vishh do you know? . Yes, this is included in 1.4.0 as described in  https://github.com/kubernetes/heapster/releases/tag/v1.4.0. /lgtm. /lgtm\nthanks for the fix!. /lgtm\n/approve. this is fixed. /lgtm. @k8s-bot ok to test. /lgtm. Could you please rebase?. /lgtm\nfeel free to merge on green. LGTM, please rebase before merging. LGTM, please rebase before merging. cc @timstclair @dashpole . /lgtm\n/approve. /lgtm. /lgtm\n/approve\nFeel free to merge on green.. I agree with @DirectXMan12 - it doesn't look like a regular sink.. My opinion is that we can remove pod_namespace and one of nodename and hostname.\nWe should keep both pod_id and pod_name because the former is guaranteed to be unique and is the computer friendly while the latter is human friendly. Same with namespace.. /lgtm\n/approve\nThanks @loburm for the fix. Could you please cherry-pick this to release-1.3?. @k8s-bot ok to test. @igorpeshansky from Stackdriver team.. /lgtm. /lgtm. @k8s-bot ok to test. @k8s-bot pull-heapster-e2e test this. @mwielgus . Closing for now as obsolete.. I've built gcr.io/google-containers/heapster-grafana-amd64:v4.2.0. Please check the new one.. @allen12921 @saschagrunert @taylorshaulis thanks for reporting! Let's track this in #1709. I'll build 4.2.0 today or tomorrow. Sorry for the confusion, I was not aware of this issue.\n. I've build gcr.io/google-containers/heapster-grafana-amd64:v4.2.0.. I've built gcr.io/google-containers/heapster-grafana-amd64:v4.2.0. Please check the new one.. @crassirostris could you please take a look?. @cdxOo also we introduced recently event-exporter which has similar functionality.. I've built gcr.io/google-containers/heapster-grafana-amd64:v4.2.0. Please check the new one.. I've built gcr.io/google-containers/heapster-grafana-amd64:v4.2.0. Please check the new one.. /retest. @luxas timeout so I'd assume some infra problem.. /retest. /retest. Test failed with\nW0621 11:18:21.340] I0621 11:18:21.338596   12804 framework.go:222] cluster validation failed - \"exit status 1\"\nW0621 11:18:21.341]  Validating gce cluster, MULTIZONE=\nW0621 11:18:21.341] Project: kubernetes-jenkins-pull\nW0621 11:18:21.341] Zone: us-central1-b\nW0621 11:18:21.341] It looks as if you don't have a compiled kubectl binary\nW0621 11:18:21.341] \nW0621 11:18:21.341] If you are running from a clone of the git repo, please run\nW0621 11:18:21.341] './build/run.sh make cross'. Note that this requires having\nW0621 11:18:21.341] Docker installed.\nW0621 11:18:21.341] \nW0621 11:18:21.342] If you are running from a binary release tarball, something is wrong. \nW0621 11:18:21.342] Look at http://kubernetes.io/ for information on how to contact the \nW0621 11:18:21.342] development team for help.\nI'll take a look.. /retest. works for me locally. Bumped to v1.7.0-beta.2 and still I can see the same error.. @kubernetes/test-infra-maintainers any ideas what can cause the problem?. Something has changed in kube-up.sh script between 1.5 and 1.6.. It's even stranger:\nIn Kubernetes 1.5 release tarball there was no kubectl while in Kubernetes 1.6 release tarball I can see platforms/linux/amd64/kubectl.\n@krzyzacy @luxas @kubernetes/kubectl-maintainers any thoughts?. @ixdy should kube-up.sh install alpha/beta?. Thanks @ixdy !\nI added installation command and it works!\n@DirectXMan12 PTAL. To be merged after #1692 and #1691.. I've built gcr.io/google-containers/heapster-grafana-amd64:v4.2.0. Please check the new one.. I've built gcr.io/google-containers/heapster-grafana-amd64:v4.2.0. Please check the new one.. @loburm could you please take a look? Thanks!. @DirectXMan12 I asked Filip to add him to heapster-reviewers team.. @fgrzadkowski thx. Unfortunately this is not fixed yet. We've seen duplicated containers with non-zero resource usage. Will send a fix soon.\n. #1790 fixed the problem in 1.4. Decreasing the priority - we need to fix this in HEAD as well.. cc @adambkaplan @luxas. /ok-to-test. /lgtm\n/approve. closed/reopened to re-trigger cla verification. ElasticSeach is supported in eventer, so you can setup it. Instead of heapster you need to run eventer, see for example eventer container which exports events to InfluxDB here. Feel free to ping me on slack. Also I'm happy to accept your contribution to the documentation once you figure out how to do it:) . @AlmogBaku @andyxning @huangyuqi . cc @yujuhong @dashpole . /lgtm. Apologies for the problems. @loburm is investigating this.. /lgtm\nJust one minor comment. Please squash commits before merge.. /lgtm. Thanks @allencloud for your contribution!. Please bump the tag as well.\n@adambkaplan @caarlos0 you've recently bumped the grafana version. Do you have any thoughts here?. @adambkaplan we use 1.8.3 which the latest one.. /lgtm. /lgtm\n. /lgtm\nThanks a lot for the fix!. /lgtm. Instead of introducing a new concept of \"usable\" and \"not usable\" nodes, how about just re-using something from Kubernetes. Maybe node conditions? (I don't have an answer for this - I just think that introducing something new might be confusing).\nAlso the question is whether you will be able to to gather any metrics from not-ready kubelet? In the current implementation by design we do not scrape not ready nodes because there is a big chance that we will fail anyway and when something wrong is going on the node we do not want to add out 0.02$ to this problem.\nWDYT?. @andyxning ready&schedulable sounds better to me. While I'm perfectly fine with schedulable bit I'm not super-convinced that scraping metrics from not-ready nodes is the right approach.\nCan you extract the logic which adds schedulable label to a different PR, so that we can merge it and focus on the discussion on scraping not-ready nodes.\n@dashpole @timstclair @yujuhong from node team for their opinion on that.. @andyxning the other PR is merged. Looking for feedback from node team about scraping not ready nodes. . ping @kubernetes/sig-node-pr-reviews . /lgtm\nLooks like a useful thing to me.\nDisclaimer: Heapster maintainers reserve the right to remove it.. @asifdxtreme thanks for the contribution!. sounds better. I've changed also histogram to summary which is more suitable for this case.\nPTAL. done. /lgtm. PTAL. cc @crassirostris @loburm . Comments applied and squashed commits - will merge on green..  /retest. /test pull-heapster-e2e. Closing as there is not progress for 2+ months. Please reopen if you want to continue working on this.. /lgtm\nThanks!. @alok87 the images has been pushed. Could you please check whether they work?. Thanks @miry for checking this!. /lgtm. /lgtm. I've just released the following images:\ngcr.io/google_containers/heapster-grafana-<platform>:v4.4.3\ngcr.io/google_containers/heapster-influxdb-<platform>:v1.3.3\nPlease verify them.. /lgtm. cc @loburm . /ok-to-test. /lgtm. I've just released the following images:\ngcr.io/google_containers/heapster-grafana-<platform>:v4.4.3\ngcr.io/google_containers/heapster-influxdb-<platform>:v1.3.3\nPlease verify them.. cc @loburm @yujuhong . @timstclair @crassirostris any thoughs on this?. /ok-to-test. cc @luxas . @acondrat thanks for reporting. We're looking into this.\ncc @loburm @x13n . Indeed there is an issue: we export evictable memory to Stackdriver instead of non-evictable one. @loburm is going to fix this.. https://github.com/kubernetes/heapster/blob/master/metrics/sinks/stackdriver/stackdriver.go#L412. Closing as CLA is not signed - please reopen once you fix this problem.. @loburm nice fix. Thanks!. cc @loburm . /lgtm\nThanks!. @x13n could you please cherry-pick to release-1.4?. /lgtm. /lgtm. @loburm remove WIP once it's ready to merge.. @loburm could you please take a look and figure out whether we want to backport this to 1.4.3?. /ok-to-test. @mikebryant thanks for the fix!. @yujuhong @timstclair @dashpole . Merging per @DirectXMan12's approval from https://github.com/kubernetes/heapster/pull/1803#issuecomment-336969571. /lgtm. LGTM in general. Huge thanks @andyxning for doing this super grungy work.\nE2e are failing\n```\nI1018 02:50:00.002] --- FAIL: TestHeapster (591.37s)\nI1018 02:50:00.003]     assertions.go:203: \nError Trace:    heapster_api_test.go:1159\n\nI1018 02:50:00.003]       \n    Error:      Received unexpected error \"failed to parse heapster controller - no kind \\\"ReplicationController\\\" is registered for version \\\"v1\\\"\"\n```\nThe PR also needs to be rebased. Will you find some time to take a look into this?. @andyxning thanks. I'll release v1.5.0 then and include this in v1.5.1.. @andyxning no worries. As I wrote, we will include those changes in a patch release.. /lgtm\nThanks @atzannes for your contribution.. /lgtm\nThanks @andyxning for the contribution and @loburm for the review!. /ok-to-test. /lgtm. It's not fixed yet. LGTM, please clean up the commits and we can merge.. /lgtm thx. /lgtm. LGTM from me. I'll merge it on @mindprince's LGTM.\nBTW is anyone going to add support for this in Summary API? . /lgtm. /lgtm. /lgtm. ... and please squash commits before merge.. /lgtm. /lgtm. /lgtm. /lgtm. @luxas could you please take a look?. /ok-to-test. /retest. /lgtm\n@wojtek-t PTAL. Karol has some results.. /retest. /test pull-heapster-e2e. /lgtm. /lgtm. /lgtm. /ok-to-test. Please agree with Mik, whether we want to report beginning or end of the operation.\nAlso squash commits before merge.\nOtherwise LGTM.. /lgtm\n(I haven't clicked send before). Nope. It's kernel overhead + kubernetes overhead + user's pods usage.\n. Same as above.\n. I can do it, but there will be name collision. There is already label named \"resource_id\". The solution is to change the value of LabelResourceID. WDYT?\n. Changed logic to use IsNode istead of HasResourceId.\n. Done\n. Nope. According to https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#custom_metrics_beta it has to be exactly:\ncompute.googleapis.com/resource_type\ncompute.googleapis.com/resource_id\n. See https://github.com/GoogleCloudPlatform/heapster/pull/334#discussion_r32456815\n. I mean total usage divided by node capacity. It's the number from range [0, 1] without any unit.\nUpdate: If you have better name I'm happy to change it;)\n. Just wanted to make it consistent with the current unit, but it will be definitely easier to use millicores instead.\nOr do you mean just cores as a float?\n. I see the problem. Fixed.\n. How about making this image more official?\n. Do you mean this logic in sources/datasource/utils.go? That's what I'm proposing in https://github.com/kubernetes/heapster/pull/512#issuecomment-137437419. I think it worth to do it in this particular PR.\nIf you are ok with those changes I'll do it tomorrow.\n. I don't mean removing sampling logic here at all but rather simplifying since we need to take exactly one samples per time interval as for now. It might make sense in the future to support such call in cadvisor (give me the first sample from specified time range) which will significantly reduce the amount of data being sent between heapster and cadvisor. WDYT?\nEDIT: I'm not trying to remove stats_resolution flag.\n. How about using \"/\" here? I believe I've seen somewhere convention /\n. remove this empty line\n. How about reusing deletePod here?\n. As for now scrapping data from sources takes place right after the end time passed. For example scrapping data for [10:01, 10:02] takes place a few milliseconds after 10:02. If the resolution is high some data may be missed so that we may want add some delay here in the future. I'm ok with removing this comment if you think it is useless.\n. After poll_duration flag is removed we want to get exactly one sample for each time interval. There is also a possibility to add support of querying for 1 sample from time interval in cadvisor which will reduce significantly network load (see https://github.com/kubernetes/heapster/pull/512#discussion_r38791456).\n. That's fine. The problem may be solved by delegating the responsibilty of locking to the caller, but maybe this is won't simplify code here.\n. This is a good idea and a good candidate for 1.2 cleanup. I'll restore the TODO comment.\n. To make it consistent with ContainerSpec above. Also it is a first step to get rid off of cadvisor's structs.\n. Having these 2 grouped in a struct makes sense but probably the better option is what @vishh described in https://github.com/kubernetes/heapster/pull/539#discussion_r39319547.\nAs @mwielgus noticed cpu limit is already computed by cadvisor. Let's remove cadvisor dep in a new PR (probably after 1.1). AFAIK there is no requests computed by cadvisor so we need to workaround it by obtaining the data from Kubernetes.\n. Good idea.\n. Extended description. In the standalone mode no values will be exported here.\n. how about moving \"modelDuration\" somewhere above to somehow group all fields?\n. nit:remove this empty line\n. Does it make sense to use fuzzer here and then overwrite some fields required for the test?\n. I believe there is a better function to test this condition. Same below.\n. I think it is implementation specific. There is no mutex in fakeAuthTokenProvider.\n. done\n. Got it! Sorry that I haven't understood before. Done.\n. How about using here time wait pattern here (see link below)?\nhttps://github.com/kubernetes/kubernetes/blob/master/test/e2e/util.go#L394\n. Same here.\n. I'd create a dummy pod here rather than assume that at least one is running in the cluster.\n. Ok, let's assume it's enough.\n. Ack\n. I tested this both manually and the unit test below and I haven't seen any duplicates.\n. Done\n. Done\n. Discussed offline. Not changing\n. Done\n. Done\n. Done\n. Done\n. Done\n. How about changing also MetricValue to plural above?\n. I'm not sure if this is the right way to make comments;)\n. This comments is rather related to GetMetricsSources() method.\n. Same here\n. There will be nil pointer here. Create MetricSets field in response.\n. Let's have a resolution size of a window. Remove last sync\n. if end - start != resolution\n. housekeep timeout\n. Move this up\n. Move to realManager\n. It seems to be incorrect: time to next sync will be always 5 sec\n. OK\n. OK\n. Done\n. Done\n. typo\n. Done\n. Done\n. Any explanation why did you choose this number?\n. We now have 2 labels which means exactly the same. Please fix in this PR\n. should be NamespaceKey\n. should be namespaceKey\n. bad request\n. Add support for float in follow-up PR\n. Remove this as well.\n. Done\n. Done\n. I don't agree. We should have pod and namespace specified for every pod container. If there is not probably source implementation doesn't work as expected.\n. Discussed offline. Leaving as it was.\n. Done\n. Discussed offline,.\n. eventer is not started anywhere\n. ack\n. Why are you removing this comment?\n. The comment above in wrong. Please fix.\n. Remove empty line\n. remove empty line\n. Why this is changed?\n. Not sure whether you can remove coreos deps. I tried to do it some time ago and it broke the build:(\n. That's fine.\n. The tests passed, so seems to be ok.\n. I'm afraid there is too many logs that come from log sink.\n. How about just setting cpu/mem request for containers and then let the PodAggregator to do its work?\n. Also how about setting limits here rather than relying on cadvisor? Limits are tightly coupled with requests.\nEDIT: For node limit means its capacity. cadvisor doesn't handle it, so it should be set somewhere else (see https://github.com/kubernetes/heapster/blob/master/sources/nodes/kube.go#L96)\n. Again, this is just duplication of the logic of NodeAggreator.\n. Also you are 'enriching' nodes in PodEnricher which is confusing.\n. I believe you are adding mem requests and limits as well. Also you are adding them to containers rather that pods\n. Populate LabelHostID as well.\n. You are right. We should add it somewhere.\n. typo s/Memoru/Memory\n. fixed\n. filesystem/usage\n. remove this label\n. Add comment why this dead code exists here.\n. done\n. ugly formatting\n. typo: should be parameters\n. typo available\n. typo: kubernetes\n. They are specified?\n. else if? We don't want to have 2 error messages for one problem.\n. Please note that we actually don't have versioned API in Heapster. This a kind of promise that we will try to support it in some backward-compatible way but there is no guarantee. If you really want to have a versioned API it should be a part of Resource Metrics API https://github.com/kubernetes/kubernetes/pull/24253 \n. There will be another versioned API of Kubernetes standards (proposal linked in previous comment). Especially if you want model/historical api to be fully versioned they should go through the path: alpha -> beta -> stable. We don't want it here to allow higher velocity at cost of possible instability.\n. This is one of possibility although we agree in proposal (https://github.com/kubernetes/kubernetes/pull/25279) to use just ResourceMemory constant. We may revisit this decision later since this is just alpha version of the API.\n. Done\n. Done\n. Done\n. Done\n. Done\n. Shouldn't be here %v instead of %s?\n. Good catch!\n. Thanks @caesarxuchao \nTo give you more context this started failing after bumping the Kubernetes version in #1158 from https://github.com/kubernetes/kubernetes/commit/644d651c690774376112cd3f257af0308e95a392 (~late Feb 2016) to https://github.com/kubernetes/kubernetes/commit/4bb30e00978219640215ac04e84caf5bf398be92 (~mid May 2016). After that NewNonInteractiveDeferredLoadingClientConfig function started returning empty version.\n. No worries. This is not high priority. Thanks for looking into it.\n. GetPodsRunningOnNodes?\n. How about reusing GetRunningPods and just filtering out pods running on master?\n. the previous name od the test was ok;)\n. Ups, dismiss this\n. How about more clear message: Metrics API test - single pod: OK? Same below\n. Please reuse the client\n. Please remove progrium. We no longer use this, though the documentation is outdated.\n. If this is a controversial thing I'm ok with creating an issue and postponing the discussion there.\n. why this? \n. You can extract also this.\n. Please find a better name for the flag and extend the description.\n. We are not sure how to handle auth, so intially we would like to have two ports: for the native heapster api and metrics api server via api server. Eventually native api should be server on the same port as an addtional handlers in genericapiserver library.\nI agree, but this is because there is already deprecated flag in genericapiserver with the same name and we have a conflict. Eventually (before releasing next Heapster version) we want to either remove the flag from genericapiserver or move it to apiserver and then revert this change here.\n. AFAIU your objection is similar to https://github.com/kubernetes/heapster/pull/1304#discussion_r80764878. @mksalawa will fix this in a follow up PR.\n. how about using summary api here?\n. Please revert this file\n. Please revert this file\n. remove empty line\n. We have now TAG and VERSION which are supposed to be the same. Could you please use just one of them?. nit: please change to v1.2.0 to follow the current pattern. I can't see it. Did you commit your change?. Please leave gcm name here. This was named explicitly to avoid confusion with anything that can be named v3. /s/name/type. IIUC you push amd64 image as heapster:<version> without explicit specifying architecture. Why do you need an architecture in Heapster manifest then?\nAlso why don't you have the same thing for grafana/influx?. 200. IIUC we discussed to name it \"stackdriver\", though I'm fine with either name.. Also this can be shared with gcm sink. const. Please have one field per line.. This can be shared with gcm sink. this can be shared with gcm sink. remove debug code. Why namespace_id is empty?. this can be shared with gcm sink . this can be shared with gcm sink . Please use MetricUptime.MetricDescriptor.Name. Will you have a separate method for every point? At least interval can be somehow shared.. 2017. Why do you need this?. Why do you need to specify empty set of labels?. labels are not used. Stackdriver sink. ACK. FYI, this broke e2e tests\ndocker tag gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1 heapster:e2e_test\nError response from daemon: Conflict: Tag heapster:e2e_test is already set to image 1b32c9c98284, if you want to replace it, please use -f option\nMakefile:68: recipe for target 'container' failed\nmake[1]: *** [container] Error 1\nmake[1]: Leaving directory '/go/src/k8s.io/heapster\nhttps://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/heapster/1572/pull-heapster-e2e/238/\nI'm going to revert this change (only Makefile part).. ref #1572. api/v1/metric-export. Please add a simple unit test.. We support v1.1.1. remove this line. Please combine this message with the previous one and explain what type of worked you mean.. Is it actually needed?. Let's leave metrics:\nMetricNodeCpuCapacity,\nMetricNodeMemoryCapacity,       \nMetricNodeCpuAllocatable,       \nMetricNodeMemoryAllocatable,\nwhich are generally useful.\nLet's also rename NodeAutoscalingEnricher to just NodeEnricher. Why did you make this change? When I'm trying to build grafana containers I can see:\n```\ndocker run --rm -it -v /tmp/tmp.OnAJjYyL8A:/build -w /go/src/github.com/grafana/grafana gcr.io/google_containers/kube-cross:v1.8.3-1 /bin/bash -c \"\\\n        curl -sSL https://github.com/grafana/grafana/archive/v4.2.0.tar.gz | tar -xz --strip-components=1 \\\n        && curl -sSL https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana_4.2.0_amd64.deb > /tmp/grafana.deb \\\n        && mkdir /tmp/grafanarootfs && dpkg -x /tmp/grafana.deb /tmp/grafanarootfs \\\n        && CGO_ENABLED=1 GOARCH=arm CC=cc go build --ldflags=\\\"-w -X main.version=v4.2.0 -X main.commit=unknown-dev -X main.timestamp=0 -extldflags '-static'\\\" -o /tmp/grafanarootfs/usr/sbin/grafana-server ./pkg/cmd/grafana-server \\\n        && CGO_ENABLED=1 GOARCH=arm CC=cc go build --ldflags=\\\"-w -X main.version=v4.2.0 -X main.commit=unknown-dev -X main.timestamp=0 -extldflags '-static'\\\" -o /tmp/grafanarootfs/usr/sbin/grafana-cli ./pkg/cmd/grafana-cli \\\n        && cd /tmp/grafanarootfs && tar -cf /build/grafana.tar . \\\n        && cd /build && CGO_ENABLED=0 GOARCH=arm go build -o setup_grafana setup_grafana.go\"\nruntime/cgo\ncc: error: unrecognized command line option '-marm'\n```. cc @luxas . IIUC the logic is as follow:\n- if we first encounter an old container, then the new one will just replace it\n- if we first encounter the new, this check will ensure that the old will not replace it\nIt's not that obvious. Please add a comment explaining this.. How about avoiding double checking of the same condition container.StartTime.Time.Before(otherStartTime) by implementing this in the following way:\nif container.StartTime.Time.Before(otherStartTime) {\n    if *container.CPU.UsageNanoCores == 0 {\n        return true\n    }\n    glog.Warningf(...)\n}. done. Time, Duration has string method defined. It's well formated:\nI0809 09:21:05.043400       1 stackdriver.go:175] Skipping batch from 2017-08-09 09:21:00 +0000 UTC because there hasn't passed 1m40s from last export time 2017-08-09 09:20:00 +0000 UTC. V(2) seems to be good.. I prefer to have explicit default.. It is - interval is visible only within the scope of if. removed. I don't have a strong opinion here, but I think we should rather make visible that some metrics are dropped. This should be also logged rarely - it occurs with probability 0.1-1% once per container (after its start), so it's not spammy thing.\nI can change if you feel it's not useful. . SGTM, changed to V(2). done. I need to use it both as int and as time.Duration.. I'm aware of this, though it's not obvious whether we want to do this - as discussed offline they are accounting problem in such case (metrics from cancelled request might be successfully exported). Added todo to fix this.. done. to be consistent with the other parameter. 1. We don't want to DDOS SD (see Marcin's comment below).\n2. To limit memory usage (we are pretty close to the limit with the current approach).\nI've added a comment to make a it a real timeout (see above) - it's almost real right now (the number of workers is order of magnitude lower than the number of requests).\nMarcin, we store 2 full scrape results.. With the current approach the load will be spread across the whole interval. I can add 0-3s sleep which is one request processing time. WDYT?. done. When I debug a problem the first thing I take a look are logs. . done. done. This is just a copy/paste. Let's leave as it is right now. done. Error msg is not exposed as metric. We need to make it visible - we can't hide everything in logs. Changed to warning.. will explain f2f. done. reimplemented to timeout on all requests. obsolete. obsolete. done. done. done. will send a follow up PR - this is a pure cherry-pick. Without this you won't be able to distinguish when a request was not picked at all vs. the case when it's in progress.. It's for accounting purposes: we want to track number of requests/timeseries successfully delivered to SD. Those which are not picked by any thread are clearly dropped.. remove empty line. V(2)?. Also please print all requests as V(10). glog.Infof.V(2). How about adding a comment explaining this if?. ditto. glog.Infof.V(10). Sorry, I was not clear enough. When you are reading the code IMO it's easier to understand:\nif glog.V(2) {\n  glog.V(2).Info(\"log this\") }\n}\nthan\nif glog.V(2) {\n  glog.Warninf(\"log this\") }\n}\nbecause in the second case your first impression is that the log severity is Warning, but then when you look closer, it turns out the this is in fact V(2).. Why don't you export volume metrics?. Add todo to revisit how location is set.\nCurrently it's based on the zone where Heapster is running and it could be something else because we run nodes in multiple zone.. why not implement this by calling createTimeSeries an passing \"gke_container\" as resource?. Ups, sorry for not reading:|. How about using key function?\nhttps://github.com/kubernetes/heapster/blob/master/metrics/core/ms_keys.go. @crassirostris do we want this change?. Should be Milliseconds. Milliseconds here and in the definition of the metrics.. Shouldn't be the begging of the scraping?. +1 for not bumping the version, as kubectl top relied on this until very recently.. same here - see the comment above. and in many places below. Consider removing support for aggregated apiserver as this is currently implemented by metrics-server.\n@DirectXMan12 WDYT?. ",
    "pires": "@arkadijs excellent analysis. Will keep an eye on this issue.\n. Any updates on this?\n. +1\n. +1\n. +1\n. +1\n. I agree with @jimmidyson. I think it's up to the user to adopt whatever storage strategy he/she is more comfortable with. In the end, I'd really like to see something like flocker coming into Kubernetes, but right now it's too early to decide on an approach other than empty-dir.\n. @vishh as requested, I've updated the InfluxDB + Grafana replication controller.\n. @vishh done.\n. @Andre-Freitas exactly.\n. @jimmidyson please see #142 where I also made nginx - which already is used to serve grafana - to reverse-proxy influxdb.\nI find it weird that @vishh is open to accept #183 when he says that #142 is a basic PR and gives all sort of (also weird) justifications to not accept it.\n. Thanks @jimmidyson \n. What about using publicIPs flag from Services? This is what I've been advocating all the time.\n. @jimmidyson you can also go with createExternalLoadBalancer but that requires a cloud-provider.\n. ",
    "jimmidyson": "As detailed above, separating into separate series is the first step. Series name should include namespace as well so that we can perhaps provide some kind of authorisation based on series names. Separating out also will let us do clean up more easily on historic stats, say as namespaces/projects are deleted.\nContinuous queries are also going to be critical, not just for the performance reasons detailed above, but for tend analysis, etc. over long periods of time where you don't want to keep the individual metrics.\n. I think by default we should be using an empty-dir volume, but as we're seeing with lots of things this is deployment/implementation specific. A deployment binary might be a good idea, but we really need this to be in the influxdb manifest at the very least.\n. @liuhewei Sounds great!\n. Agreed that heapster should support statsd & collectd, but running those would require extra setup rather than natively supporting collection in heapster.\nAlso heapster providing discovery of what to monitor would probably be easier than configuration of collectd & statsd to do the same. My initial thoughts on this were to use selectors to collect configured metrics over http. I'm not sure how you would achieve that simply with statsd/collectd. Adding a REST service to heapster to allow for dynamic configuration of metrics collectors would make this even better IMO.\n. Thanks for raising those. Wonder if it's worth pushing on with using labels while the discussion around discovering application types via the Kubernetes API kicks off?\n. Not really a heapster problem - closing.\n. I use a grafana docker image at https://github.com/jimmidyson/docker-grafana which uses go http server to serve files, but currently uses direct browser connection to influxdb service. I need to add in a reverse proxy to influxdb.\nUsing the built in kubernetes master proxy is not great for environments that are secured & require credentials to use the kubernetes master proxy. I can try to add in the reverse proxy tomorrow if you're interested?\n. @vissh why do you need to use the apiserver proxy rather than just a reverse proxy to the kube service directly?\n. @vishh - yeah. So I've updated https://github.com/jimmidyson/docker-grafana to create a RP, similar to the nginx config one of the other PRs does, to the env var specified influxdb service so there's no need to go through the kube master proxy. I'll send in a PR for updating the Grafana image so you can take a look.\n. If you wouldn't mind holding off on this, I'm looking at putting together a generic go utility that serves static content, processes config template to file & proxies to kubernetes service. I see this as pretty generic & useful.\n. Yes we can default & use dns too. Dns wasn't available as core service when I first put this together but makes sense to use it now.\n. @vishh @pires @simon3z - I have updated this image to use kuisp (https://github.com/jimmidyson/kuisp) which I've built for this purpose: serve static content & proxy to kubernetes services. It greatly simplifies the scripts while allowing the same functionality. Please review kuisp if you feel you need to.\nThis gives heapster grafana a more easily configurable image than nginx/apache, as well as being very lightweight & small image.\n. @vishh what do you receive in your browser when you try it?\nAccessing HTML pages via the api server proxy is not really a good idea due to the amount of rewriting the HTML that is required to sort stuff like absolute links, hard coded base hrefs, etc.\n. Accessing services remotely in a consistent way across environments/clouds\nis pretty impossible IMO. Public ips is probably the easiest way to achieve\nthat but as the ip will vary across envs it makes deployment a bit harder\nthan simply deploying a few RCs & services - needs user input to deploy.\nHaving said that, it's the only way to get a nice cross-env UX where users\ncan hit a simple url, without remembering /api/v1beta1/.... Users are gonna\nwant to set up a dns entry to view grafana directly.\nI vote for publicips option but pointing to documentation on different\ncloud set ups for accessing kubernetes services, which grafana will just be\nanother one.\n. @vishh - if you've set the host port properly, the pod is actually running & you can route to the node then I don't understand how you get connection refused. The single container process is the web server - of the pod is running then it's serving on port 8080...\n. Yeah so that's the other option: document how to deploy on different clouds/envs but that's not the consistent ux that @vishh is after I don't think.\n. @vishh - reason for 8080 is to run as non-root user (nobody) in container for extra security.\nIirc how the proxy works it should still be accessible through the api server proxy, but I'm not sure how the grafana ui will look with the html rewrites that the api server proxy still probably need to do.\n. @vishh - i think this we need to eparate out influxdb & grafana into separate RCs & services. Scaling influxdb & grafana separately is definitely something users are going to want to do. Another use case is hosting grafana on publicly accessible servers, but influxdb not being publicly accessible.\n. @vishh - it depends on the implementation. I know at least one that requires authentication for both kubernetes & kubernetes-ro. Even the RO service can expose info... We at least need the capability to auth, regardless.\n. @vishh - btw this failed earlier because of the github outage :( it's not building again for some reason.\n. @vishh - yes this makes a lot more sense. I was unaware of this coming but I like it :) Are there any timescales for when this will be supported in a kubernetes release?\n. @vishh - ok i'll update this PR to use clientauth & we can wait to merge it until it's usable in Kubernetes?\n. @vishh - i guess we could merge this now as it defaults to disabled & only use when introducing secrets to containers is sorted in kubernetes?\n. @vishh - I have a prototype for this outside of heapster in kadvisor (https://github.com/jimmidyson/kadvisor) that I was working on to learn more about go. The principle is to go from multiple sources, through a pipeline & sending to multiple sinks. Currently the pipeline just acts as a intermediary to pass messages from sinks to sinks & decouple sources & sinks, but could in future be used to validate input, etc.\nThe use of channels also means that collection is done in different goroutines to sinks storing data (#110) & allows you to specify different collection intervals per source.\nWhat I'd like to do is to migrate what I've done for kadvisor to heapster & plug in the sources/sinks that you already have in heapster. I'd like to migrate the command line flags at the same time as specified in #200.\nWhat do you think? I know this is quite a significant change but I think it would bring a lot of benefits.\n. Of course this approach could also be used for monitoring multiple Kubernetes clusters, or even for federation.\n. I think that approach would work well for exposing data to be pulled/scraped by external services, e.g. prometheus. I see this in-memory cache as one example of a sink, just one that doesn't push the data anywhere, just exposes it for external reaping.\nHaving it used in process for push only sinks seems like an unnecessary overhead. If data is being pushed then why not push it as it's collected? Push sinks could do their own throttling of data, but keeping an in-memory cache of metrics when all you're trying to do is push to external sinks seems like a use of resources. Normally when pushing metrics you would configure the sampling rate at the source & push the data.\n. @vishh - saw your changes. Happy to go with the in-memory representation for now & see how it works out.\nAs for configuration of backends, how would you feel about dropping the individual -kubernetes_master, -coreos, etc flags & move to a more generic --source=<source_type>:<source_config> flag definition. We could then use the new factory methods you've added in & side-effect imports to add in source & sink extensions/plugins? e.g. if I wanted to use kubernetes source I would use something like:\n--source=kubernetes:<optional_kubernetes_master>?insecure=true&kubelet-port=10250&auth=/etc/heapster/clientauth.json\nThe --source flags & complementary --sinkflags could both be specified multiple times.\nThis approach would make adding sources/sinks much easier IMO.\n. Heapster isn't a command you're going to be running over & over again on the command line. It's normally going to be configured in a Docker container so config specified in Dockerfile or Kubernetes manifest. Or as a service where it will again only be configured in e.g. a systemd unit file. Once you have it right you're not going to change it very often.\nI'm not sure how we could structure flags to achieve the same pluggable approach?\nAs for ditching flags, I totally go for adding a config file option as an alternative, but wouldn't ditch flags as on the whole I think it will only be a handful of flags that are used.\n. Could even consider viper (https://github.com/spf13/viper) so we can combine flags, env vars & config files into one runtime configuration. As we as more functionality, multiple sinks, events, etc, having a config file might be really useful, in addition to flags, etc.\n. Think this is closeable note that sources & sinks flags are consolidated.\n. The grafana image has been updated in master & these issues should go away once we release that & update manifests. @vishh - any chance of that happening soon?\n. Can't we just store the marshalled event object as JSON rather than a string which is much harder to parse for display, etc?\n. Or we could create columns from the event data instead?\n. Understand now about the common schema. Is this schema documented anywhere? Surely the storage schema might differ between sinks?\nStoring marshalled as JSON should cover what anyone needs to do with it.\n. @vishh - so do newer versions of Kubernetes put the external IP in the NodeLegacyHostIP address? Otherwise I don't see how that could happen? Perhaps it's because heapster's using v1beta1 API & the conversion between v1beta3 & v1beta1 in Kubernetes has an issue with that? Switching heapster to version v1beta3 is probably the better solution, but of course more work.\n. @vishh - agreed v1beta3 is still in flux, as is v1beta2 although less so. I guess I an live without it, but I would like to see heapster supporting multiple versions of kubernetes once it's a bit more stable.\n. Surely we don't need to distribute the individual nice certificates but just the root CA certificate to verify the nodes address? This should be pretty easy through secrets right?\n. Let's make the kubelet scheme configurable at least?\n. I think that viper is complimentary to this - we can allow sources to be specified as flags & we can add in viper to be able to use config file too. Having both would be nice. If we decide to merge this then I can work on viper config in a subsequent PR.\n. OK I'll rebase once merged & was going to add a doc today anyway. Thanks @vishh\n. Hey! Integration tests passed woohoo! Writing doc today I promise ;) Then ready for merge?\n. Docs are already updated!\nOn 18:36, Fri, 10 Apr 2015 Vish Kannan notifications@github.com wrote:\n\nAwesome! Ping me once the docs are complete.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/223#issuecomment-91631241\n.\n. Good point. I'll update later tonight. Btw openshift bundles skydns as a service ootb.\n. Updated.\n. @vishh - squashed\n. @vishh -have you managed to review this for merge?\n. Awesome! Thanks\n\nOn 22:46, Mon, 13 Apr 2015 Vish Kannan notifications@github.com wrote:\n\nThanks for the reminder!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/223#issuecomment-92510333\n.\n. @vishh - fancy updating travis.yml at the same time? Then we're at least conistent.\n. AFAIK the default read only api service in vanilla kube doesn't use https by default (although it is in other impls like openshift). I suggest making the scheme configurable via an env var, defaulting to http.\n. One option I thought of is we could sniff whether kubernetes-ro service is https or not by just trying to connect on https, falling back to http if it fails. I've used this on other projects successfully before.\n. Could you please add the required godeps so builds are repeatable?\n. @vishh Are there any plans for heapster to use service accounts?\n\n@shilpapadgaonkar You need to create a secret containing the contents of a kubeconfig file & mount it at the appropriate place in your pod. Should work, but not tried it recently.\n. @vishh That's right - the token isn't generated by default as it is with vanilla kubernetes. Just wondering why kube config is preferred to service account?\n. @smarterclayton @detiber Heapster can't currently use bearer token auth so I'm sending in a PR now to do just that. I have it working locally against OpenShift.\n. Sent in PR #324 to enable service accounts in heapster.\n. There will be further difficulties in running Heapster on OpenShift such as #325 as access to kubelets is secured. I'll work on another PR for that to use the master proxy.\n. I have this working by adding the read only port to the kubelet config in OpenShift. However this is not recommended as it is obviously a security risk. Working with the OpenShift team to try to resolve, but this is not a priority for them so may take a while to get working.\n. There is no kubernetes-ro service, this should be https://kubernetes now, but requires a service account to be properly set up as you say. I'm documenting the deployment on openshift in the openshift docs by the end of this week (hopefully) so might be best to wait for that.\n. @shilpapadgaonkar As long as the pods are using the default:heapster service account then it should be fine. Are you saying you're only getting metrics for the pods in the default namespace? Where are you looking?\n. Where are you looking? In grafana or in influxdb? Wonder if the query in grafana is only targeting the default namespace?\n. @miheer @david-martin Would you be able to post the output of:\noc describe clusterPolicy default\noadm policy who-can get nodes\noadm policy who-can get pods\noc get rc heapster -n default | grep serviceAccount\n. Also:\noc describe clusterPolicyBindings :default\n. I've just put in a PR for some minor updates to the documentation - mainly updating to latest heapster release (0.17.0) & a necessary firewall rule on each node.\nDeployed on 3 node cluster & all worked straight OOTB with no customisations.\n. Fixes #320 too, or at least makes it easier to run on OpenShift.\n. @vishh I think this should be the default once implemented. What do you think?\n. @vishh It might be :) Let me find out.\n. Discussing in https://github.com/openshift/origin/issues/3020\n. @vishh OK agree that this shouldn't go through the API server, will work on sorting out heapster & openshift separately.\n. What version of kubernetes are you running? Have you deployed the DNS add-on? That's required if you're trying to use DNS for services as you are in this case.\n. That's a pretty old version of kubernetes & there have been significant improvements to the DNS stiff since then. Any chance you could upgrade?\n. IIRC you can use env vars in sink URLs something like:\ninfluxdb:http:// ${MONITORING_INFLUXDB_SERVICE_HOST}\n:${MONITORING_INFLUXDB_SERVICE_PORT}\nOn 07:00, Wed, 17 Jun 2015 shwetalakhimpur notifications@github.com wrote:\n\nI can upgrade. What is the best way to upgrade ?\nPrior to that, i would want to get heapster running even if rather raw\nmethod.\nIs there a way I can hard-code influxb service IP, so that heapster can\nreach it without setting up DNS?\nAdd host to this URL or add sink IP ?\ninfluxdb:http://monitoring-influxdb:80\"\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/338#issuecomment-112664372\n.\n. If you're running heapster in your cluster then you should be using the service IP (local IP as you call it), not the public IP. Service IPs should be fully routeable inside your cluster, if they're not then you have big problems with your cluster.\n. The service IP will not be routable from outside the cluster but should be routable from all pods in the cluster, regardless of what node they are running on. If not then you have a problem with your flannel set up.\n\nYou should not be sending data from heapster running in the cluster to a public IP, but rather to the service IP allocated by kubernetes, available in the MONITORING_INFLUXDB_SERVICE_HOST env var. The heapster pod should be able to route to the service IP.\n. If you don't specify the kubernetes source then heapster won't try to get any data from kubernetes - up to you whether you want that ;)\nAnd yes it looks like the same issue - routing to service IPs from pods.\n. Looking better! Yes you will need to set up the DNS add-on so that service names like kubernetes-ro are resolvable.\n. @vishh @timstclair What's the status of this? Any kubernetes issues/PRs I can look at?\n. @timstclair Thanks for the update. Look forward to your PR!\n. @timstclair @vishh When do you aim for a stable stats v2 endpoint in kubernetes? A couple of months perhaps?\nThinking about users who want to use network stats today & can see a simple fix to heapster to ingest them - network stats from kubelet are correctly exposed today, ready for ingestion, albeit in a raw cadvisor form.\nWhat do you think about just adding in network stats to existing heapster API while we wait for @timstclair's PR & any subsequent stabilisation of the new metrics APIs?\n/cc @jcantrill\n. So this is a bit more involved than I thought it would be. AFAIK there is no way to query the infra container in a pod with the current kubelet API & as the network metrics are associated only to the infra container that means there is no way to get the network metrics.\nI guess this will have to wait for the new metrics APIs.\n@vishh Does that sound right to you or can you think of a simple way to work around this without waiting for new metrics APIs?\n. I am erring on waiting until the new metrics APIs are finalised as this will be pod based rather than container based. However if this is a requirement in the near future then we should probably get this added to existing APIs. This will be a bit trickier (& dirtier) to do but should be doable.\n. The problem is that the infrastructure container isn't included in the pod status retrievable from the kube API (the API server is unaware of the infrastructure container from what I can see). The way that Heapster currently works is to get container info for each pod from API server then request stats for each container from the appropriate kubelet (of course this is really inefficient & addressed in the ongoing rewrite of heapster AFAIK).\nCurrently the infra container is totally ignored by heapster (https://github.com/kubernetes/heapster/blob/master/sources/datasource/kubelet.go#L180) as it's labelled as a member of a kubernetes pod, but not known to the API server. I cannot see anything else available that identifies it as the infra container for a pod, other than the format of the container name & the network mode (normal pod containers have a network mode of container:<containerID>).\nThe current kubelet APIs only allow querying for a) a container of a pod (which the infra container isn't listed as so is not queryable this way); b) a raw container via a Docker ID (no way to know); c) by getting all raw containers, extracting pod name from the container name (follows known regexp ATM) & adding this to pod info somehow. So c is the only option I can see, but it seems pretty fragile (no way to identify it's actually an infra container that I can think of).\nHow does that sound? Any other ideas? Dirty & fragile but without a major rework (the Heapster rewrite & new metrics APIs) this is the only option I can see. I would prefer to wait for the updated versions as I said before but if it's wanted I can probably hack it together.\n. So looking again it seems the infra container name is hard coded to POD (https://github.com/kubernetes/kubernetes/blob/b9cfab87e33ea649bdd13a1bd243c502d76e5d22/pkg/kubelet/leaky/leaky.go#L24 , https://github.com/kubernetes/kubernetes/blob/810181fb7b4e969d1c1aa84f32377ef1b65f8c65/pkg/kubelet/dockertools/docker.go#L41). Of course this is terrible & fragile but would simplify finding infra containers & pairing them to a pod. Not at all happy with this though.\n. It may be enough to just rebase & do godep save ./... github.com/progrium/go-extpoints now that godeps are cleaned up a bit. I'd try that first.\n. @DreadPirateShawn As heapster needs to query the API server to retrieve the list of hosts, some kind of authorization has to take place, be it cert/token via service account/basic auth. There's no reason not to be able to use heapster externally from the cluster, but currently you would need to provide the same config as if it was inside a cluster. Looking at the code, it might be a good idea to add more options for auth to facilitate what you're doing.\n. Are you using master? If so can you try with inClusterConfig=false as an extra option to the kube source?\n. @DreadPirateShawn Glad it's working-ish for you now.\n. The CA cert is auto mounted in kubernetes >= 0.20. IIRC you can configure heapster with the insecure=true option on the kubernetes source.\n. If the CA cert isn't mounted in the pod then that looks like a bug in Kubernetes as this should have been happening since 0.20 I believe.\nAs I said above, you can try adding insecure=true as an option to the kubernetes source config, so in your case it would look like --source=kubernetes:https://kubernetes:443?useServiceAccount=true&auth=&insecure=true\n. Was this an upgraded environment? I think I've seen service accounts that existed before the upgrade to 0.20+ not have the CA cert secret added to mounts.\n. Yes it will. Install glibc & you still get a small image but with expected DNS behaviour. See https://github.com/anapsix/docker-alpine-java/blob/master/Dockerfile#L8 for how to do it.\n. @shilpapadgaonkar This is a bug in cadvisor which kubernetes embeds - fixed in latest cadvisor yesterday, will try to get kubernetes godeps bumped today. See https://github.com/google/cadvisor/issues/822 & https://github.com/google/cadvisor/pull/865. This is due to a previous incompatibility with cadvisor & docker >= 1.7 when libnetwork was introduced.\n. Might be worth adding the godep command to a Makefile target? make godeps sounds good to me.\n. One for @mwringe I think.\n. @shilpapadgaonkar The api server certificate has been created with a different FQDN, master4.exampletest.com. Did you do this manually or use the ansible scripts or some other way? There are a couple of SANs (alternative names) that should be added to your api server certificate to help make services that run on your cluster portable & the api server address validate successfully. These are:\nkubernetes.default.svc\nkubernetes.default.svc.cluster.local\nYou will need to regenerate your api server certificates with these SANs. IIRC you can use oadm ca a bit like this:\noadm ca create-server-cert --cert=master.server.crt --key=master.server.key --hostnames=kubernetes.default.svc,kubernetes.default.svc.cluster.local,localhost,openshift.default.svc.cluster.local,127.0.0.1\nYou can also add master4.exampletest.com to the hostnames flag if that's the public or externally resolvable address for your cluster & you will need to update the certificate/key paths appropriately.\n. @shilpapadgaonkar Glad that worked for you.\n. @lorenzvth7 Try adding the public hostname you're trying to connect to to the list of hostnames passed to the --hostnames flag. Once you've done that & restarted the API server, can you check it's running successfully before trying oc login again?\n. @lorenzvth7 Sorry - I mean whatever address you hit the API server at remotely, if that's the public hostname of the server or a different DNS that you have set up.\n. Did you overwrite the master.server.crt & master.server.key files wherever they are stored? Possibly in /etc/origin/master/ looking at your logs.\n. Have you deployed influxdb in the same namespace? Has it started properly?\n. What does oc get services influxdb-monitoring show?\n. There should be no need to manually start influxdb though...\n. @vishh skydns is embedded in the openshift binary so instead of the DNS addon deployed as a pod it is embedded in the master.\n@lorenzvth7 404 is normal when using the API port (8086). InfluxDB does have a built-in console on port 8083 - if you want to use that then create another service using that as the target port & connect to the service IP (or OpenShift route if you're running in a full-on cluster).\n. Shouldn't it be kubernetes.default.svc?\n. Let me sort out the godeps in a separate PR before even looking at this.\n. See #601 for clean up of godeps.\n. Rebased, ready for review.\n. That's just a godep special... no real harm done ;)\n. Yeah godep seems to have some problem with transitive side effect imports but doing it from a clean Godeps dir seemed to work nicely.\n. The majority of information should be available once #941 is merged - this will be in the heapster API format. Is that enough? Supported metrics shown in https://github.com/kubernetes/heapster/pull/941/files#diff-7ee408da48cb16f010268e3b72643f00R37\n. LGTM - @mvdan any more comments before I merge?\n. @mvdan Thanks.\n@burmanm Could you please squash the commits then I'll merge?\n. Thanks for squashing - merging.\n. @mvdan @vishh Would love a quick review of this if possible - trivial fix with big impact to us :(\n. @mvdan Much nicer. Updated.\n. @mvdan OK to merge?\n. @mvdan I don't know the rules here yet - each community has different rules. But I know for next time - thanks!\n. @googlebot These are my commits.\n. @googlebot Happy to contribute these commits.\n. @mvdan @vishh Any comments before I merge? \n. @mvdan There's one line change in heapster which LGTM ;) Rest is just kubernetes deps updates.\n. Ah OK - @burmanm can you do the opposite of what I asked & submit as 2 separate commits rather than squashing? Sorry - my mistake.\n. Yeah - see https://github.com/google/cadvisor/blob/893a5c15f59972d5ddb4d61e2630863ce6a98dd7/utils/timed_store.go#L95-L98\n. @thucatebay Thanks for the feedback! I was thinking of Prometheus in this scenario as a day store, similar to how Heapster operates now, with rules for aggregating metrics as they come in to keep storage & memory requirements low. This would keep the management of it simple but bring with it extra benefits of:\n- application level metrics (future work for heapster)\n- persistence (afaik if heapster pod dies you lose all stats which could affect autoscaling)\n- sharding & federation (future work for heapster)\nThe only difference for you would be to write an external storage plugin for Prometheus as opposed to a Heapster sink.\nIt would also mean that those places that don't have their own monitoring & alerting system as you do have would be able to expand the environment by adding in Prometheus alert manager if they wanted, but certainly not a requirement.\n. @cadvisorJenkinsBot: test this please\n. The only test run is Travis, no e2e run so leaving this to @vishh to decide. fwiw lgtm...\n. @mvdan Aha! Thanks. Merging.\n. LGTM\n. Well it's highlighted a potential race in hawkular sink...\n. @mwringe @burmanm Can you look at this please? Needs to be fixed for opensihft heapster release too.\n. @burmanm Is this closed now?\n. Feels like there should be a prettier solution to this, but should work.\n. @vishh Would you be able to kick off test? Would be good to be added as admin in Jenkins ghprb (which I assume you're using) so I can ok to test PRs if possible?\n. Come on Jenkins! You can do it!\n. @vishh how's Jenkins looking?\n. @danmcp Did you mean to close this?\n. @mwringe What does kubectl get nodes show?\n. How about kubectl get nodes -oyaml? Just want to think of the best way to get the node address.\n. @liggitt What do you suggest for this? Node name doesn't have to be a resolvable address afaik & node IP isn't a SAN in node TLS cert.\n. By default, does kubernetes connect to the node via the IP & does it validate the node certificate?\n. #664 \"fixes\" it for a cluster that doesn't have certificates correctly set up as you had. Heapster cannot fix up poorly configured clusters but we can work with it...\n. @mwringe This should fix your problem with connecting to kubelets without a valid cert.\n. /cc @vishh \n. @mvdan Yeah sorry about that - btw should be fixes #a, fixes #b from https://help.github.com/articles/closing-issues-via-commit-messages/#closing-multiple-issues\n. I think this is an incredibly important thing to nail down. I've previously raised a proposal in #645 to utilise Prometheus for metric collection & Heapster to become a REST API with Kubernetes API semantics over the top of Prometheus.\nI'd also like to point out the work I've done with Prometheus to provide autodiscovery for master, node, container & application targets for metric collection.\nBut that doesn't help with nailing down exactly what the function & limits of Heapster is but I feel this would provide a configurable & extensible option to users.\n. IMO I feel there are a couple of different scenarios to cater for in different ways:\n1. Sink unavailable on startup - fail startup. More likely to highlight misconfiguration.\n2. Sink unavailable when adding at runtime. Don't add sink, return appropriate error with non-200 status code (not sure quite what to use).\n3. Sink becomes unavailable after being added. Sink should buffer until sink is available again.\n. A 422, Unprocessable Entity, is probably the most appropriate error code.\n@mvdan Do you think the current behaviour is wrong? What do you think should be correct behaviour?\n. We can add in 422 as kubernetes does here.\nSo if current behaviour is correct what is this PR for @vishh?\n. LGTM\n. LGTM\n. ok to test\n. I thought this worked by specifying multiple --sink=... flags?\n. Doesn't it support something like --sink=gcm --sink=gcmautoscaling?\n. I think it makes more sense to keep it separate so that commas can be used in sink configuration if required in future. If you're OK with that then I'd like to close this PR.\n. Thanks @bluebreezecf\n. Thanks @bluebreezecf!\n. LGTM. Thanks!\n. @bluebreezecf My pleasure. However , all thanks are to you for your contributions - look forward to your next one :)\n. LGTM thanks!\n. Thanks for reporting.\nThis is a dupe of #658 fixed in master by #693 \n. The reason for the change is described in the the PR #609.\nGzip encoding isn't removed, but rather it is now using the standard approach of content negotiation rather than forcing gzip encoding on clients that may not be aware that the response is gzip encoded or indeed be able to handle that. So to request gzipped responses you use the standard Accept-Encoding header set to gzip.\n. Thanks for the doc update btw. Do you want to add info on Accept-Encoding header to this PR so that people are aware?\n. :+1: Thanks\n. Thanks! LGTM\n. Yes it seems we should just be able to cut a release from the release-0.18.0 branch as @vishh said unless there's something we're missing @mwringe?\n. :+1: to updating master to 1.2.0 alphas\n. I agree with @vishh that we need e2e against multiple versions now, including master so we can track version compatibility as much as anything else.\n. ok to test\n. Hmm Jenkins doesn't seem to be playing ball. Travis has shown up some godep issues anyway so those need fixing.\n. retest this please\n. I have no problem with this PR - should have no problem working against previous versions of Kubernetes, but I would like to see the Jenkins integration tests pass before merging.\n@vishh @mwielgus Any concerns?\n@mwringe This also needs to be done against the heapster-scalability branch please as pretty soon (hopefully) that will be the mainline code.\n. OK to test\n. @k8s-bot retest this please\n. @vishh @mwringe What needs doing to get this merged?\n. @vishh Which design doc are you referring to?\n. Adding that is an option sounds ok, but should not be the default: if users have gone to the trouble of configuring TLS then we should adhere to proper TLS handshake/certificate validation.\n. Passing a server certificate seems odd. If you have a CA then you should give the CA certificate. If you are using self-signed server certificates then that also acts as the CA for the server certificate (they're one & the same thing) so we don't need to add an option for specifying a server certificate.\nEven so, the only way to trust a badly issued certificate (e.g. one that has a hostname that isn't the same as the actual hostname used to connect to the service) is to disable TLS verification completely: it's pretty worthless anyway in this case. That is already supported.\nI'm not sure anything needs to be done here.\n. ok to test\n. The --tls_* flags are used to configure Heapster serving TLS, not connecting to a secured Kubernetes endpoint - with your current configuration you would need to curl https://heapster/...\nTo connect using TLS please take a look at https://github.com/kubernetes/heapster/blob/master/docs/source-configuration.md#kubernetes:\n\nHeapster requires an authentication token to connect with the apiserver securely. By default, Heapster will use the inClusterConfig system to configure the secure connection. This requires kubernetes version v1.0.3 or higher and a couple extra kubernetes configuration steps. Firstly, for your apiserver you must create a SSL certificate pair with a SAN that includes the ClusterIP of the kubernetes service. Look here for an example of how to properly generate certs. Secondly, you need to pass the ca.crt that you generated to the --root-ca-file option of the controller-manager. This will distribute the root CA to /var/run/secrets/kubernetes.io/serviceaccount/ of all pods. If you are using ABAC authorization (as opposed to AllowAll which is the default), you will also need to give the system:serviceaccount::default readonly access to the cluster (look here for more info).\n. I'm going to close this now as I've just tested & it works for me. @jtblin - feel free to reopen if you still have problems or queries.\n. retest this please\n. Removed leaky package dependency.\n. Got more work to do on this to expose via the API so changing to WIP - please don't merge.\n\n@vishh @mwielgus Do you know if the Jenkins GCE e2e should pass? Seeing unrelated flake in logs:\nNo error is expected but got expected [gcm] sinks, found []\n. @mwielgus Are you targeting the new metrics APIs in the scalability branch? Sorry I've not kept up to speed with it.\n. Ready for review please. Jenkins GCE e2e is flaky & needs to either disabled or fixed separately to this PR.\n. Network stats are now only sent to sinks, not retrievable via Heapster API as discussed with @vishh.\n. @vishh Please can you let me know what is outstanding for this PR to be merged?\n. This is how's it's been done in heapster-scalability branch & I'm not going to duplicate that functionality as this is just a quick fix until heapster-scalability branch becomes master.\n. Thanks @vishh.\nMerging.\n. @mwielgus @vishh Just a godep cleanup on master - no code changes. GCE e2e flake is a problem with that test environment (failing for all PRs now...).\n. retest this please\n. @k8s-bot retest this please\n. @piosz I relied on travis to do that for me :)\n. Not sure if default Jenkins build should pass? Jenkins GCE es2 passes fine...\n. Thanks @piosz!\n. @mwielgus: One question I have: with this approach (infra container metrics only being included in pod metrics, not as separate container) will the infra container metrics be sent to sinks?\n. Thanks for the explanation.\n. cc @mwringe :)\n. @mwielgus @huangyuqi Take a look when you get a chance please.\n. This is a good check but i think there must be some other problem too in the scraping loop in the case where a node becomes unavailable during scrape?\n. There is no #874?\n. Ah you mean #847. So the metrics added in #847 & enhanced in #861 could be useful to external sinks as custom metrics.\n. #861 instrumented the sink export data so you can track how long exportng to each individual sink took.\n. @mwielgus :+1: \n. :+1: \n. Travis compile error:\n```\nk8s.io/heapster/events/sources/kubernetes\nevents/sources/kubernetes/kubernetes_source.go:101: invalid argument result (type core.EventBatch) for len\n``\n. In general: namespace should be the app (in this caseeventer) & Subsystem should be the area of functionality (exporter,scraper, etc). Scraper might have a better name in this case tbh. Perhaps we should change tosources,sinks`? & do the same with heapster metrics.\n. @bh016088 Thanks for offering! PRs always appreciated.\n. @mwringe It doesn't need to be assigned for you to work on it & provide a PR :)\n. It should do. Sorry I didn't mark it as fixed in the PR.\n. I do agree it would make more sense to only keep stats for pods we have stats for & ignore pods we don't have stats for. I assume the idea was to have an in-memory representation of every pod in the cluster but that is a bit wasteful.\nThoughts @mwielgus?\n. The problem here is this bit of code - https://github.com/kubernetes/heapster/blob/heapster-scalability/metrics/sources/kubelet/cadvisor_utils.go#L23-L41. It doesn't take into account systemd based distros systems or indeed nested cgroups.\n. System containers are now properly identified.\n. /cc @mwielgus @mwringe \nPartial fix for #912 - think we need to handle this more gracefully though in general.\n. If pods aren't running, i.e. containers aren't started, then there are no stats for them. Docker creates & cleans up cgroups that stats are gathered from on start/stop of containers.\n. AFAIK readiness checks don't affect Running phase, but rather Ready condition.\nThe only phase before Running is Pending which is documented as:\n\nPodPending means the pod has been accepted by the system, but one or more of the containers has not been started. This includes time before being bound to a node, as well as time spent pulling images onto the host.\n\nSo perhaps we just have to ignore pods that we can't find details for in scraped metrics rather than crash as we currently do?\n. Due to the distributed, asynchronous nature of Kubernetes I think we should just ignore data we can't find - best effort. If there is stuff that should be visible then users should rely on their alerting infra to notify them.\n. Yes this PR was really an attempt at a band-aid - not a good idea & I will close!\nCadvisor picks up cgroups as they're created via inotify so there should be no point in time that cadvisor doesn't know about running containers/pods: cgroups are created on container start, cleaned up on container stop in the filesystem & cadvisor is notified via inotify of these changes. Having said that there could be points where containers are started but stats haven't been gathered for those containers yet. Cadvisor has no concept of pods so it doesn't aggregate pods at all right now. Stats are collected on a per-container basis so no concept of atomic pod stat collection.\nI still think the best idea is to just report stats that we know about, ignore pods we haven't got any stats for via the kubelet.\n. Can do. It's more common to keep the leading slash for cgroup hierarchies though but it dies mess with rest api as you said.\n. Removed leading / in cgroup names.\n. These are pods either in pending, waiting to be scheduled to a node, or completed, containers exited with restart policy of never (e.g. build pods, jobs pods, etc).\n. Correction: as discussed in #911 \n. I've reworked this now to only loop through collected stats, creating stubs for containers that have no metrics collected for them (e.g. partially started pod) & creating stubs for containers that have no pod metrics collected (e.g. running pod containers without pod infra container - strange but possible scenario I guess).\n. Yeah sorry about that. Also trying to update tests to use a fake StoreToPodLister - any examples of that anywhere that you know?\n. OK working on it now & expanding test coverage to test for stubbing as required.\n. Something up with e2e:\nStarting cluster using provider: gce\n... calling verify-prereqs\nsh: 0: getcwd() failed: No such file or directory\nsh: 0: getcwd() failed: No such file or directory\nsh: 0: getcwd() failed: No such file or directory\nsh: 0: getcwd() failed: No such file or directory\nsh: 0: getcwd() failed: No such file or directory\nsh: 0: getcwd() failed: No such file or directory\nsh: 0: getcwd() failed: No such file or directory\nsh: 0: getcwd() failed: No such file or directory\nRepeated a lot in the logs.\n. @k8s-bot test this please\n. @mwielgus This now only adds pods/containers that have stats collected. If there are missing pods without all containers or containers without pods, these are stubbed with requests/limits from pod spec (tests added for this).\n. @mwringe FYI in this PR the stubbed pods/containers now have proper node info for node aggregation.\n. @vishh Annoying isn't it?\nDeps look fine, waiting for tests to run.\n. @k8s-bot test this please\n. Manually merge as long as someone has LGTM & tests are green.\n. @mwringe Can you look at the Hawkular sink please & add support for LabeledMetrics as required? Thanks!\n. Thanks!\n. We've fixed the no node info ...` error that was causing it to exit, should be released with v0.20.0-alpha4 when it's released (soon I hope).\nAs for the Node 10.0.2.15 has no valid hostname and/or IP address: 10.0.2.15 error that looks like something unexpected with your node config. Can you share the output of kubectl get node 10.0.2.15 -oyaml so I can see why? Also, what version of Kubernetes are you running?\n. Thanks - fix support for legacy node IP in PR. Will try to get this in next release too.\n. You're welcome!\n. This is now fixed in master, will be in next release.\n. @mwielgus Is this just to bump the version ready to tag? We could do with a new tagged version to fix some of the issues around node info, etc. Would like to merge #930 & #940 in this release though.\n. /cc @mwielgus \n. Sorry -on vacation for a couple of weeks. Thanks for picking this up.\n. Rather than unofficial builds/images, we'd love contributions for that to heapster itself if you're up for that?\n. LGTM\n. :)\n. One nit but other LGTM.\n. LGTM - waiting for tests before merge. Thanks @mwringe.\n. Of course - thanks :)\n. Couple of stylistic things, but LGTM otherwise.\n. LGTM - merging. Thanks @mwringe.\n. Looks like ExternalID is deprecated (see https://github.com/kubernetes/kubernetes/blob/master/pkg/api/v1/types.go#L2312-L2314) so I don't think heapster should support this. Going to close this, but we can reopen it if we need more discussion on it.\n. ok to test\n. @mwielgus Question for you:\n\nThis also begs the question, why is there even two metric types? Why not remove the old map[string]MetricValue and modify this behavior to be global in the Heapster? Is there some sort of wish for different types (if so - why are they on the different depth in MetricSet type) ?\n\n@mwringe Could you take a look please?\n. @mwringe Happy?\nLGTM\n. Merged #1118 - can you rebase please so we can see changes only from this PR?\n. ok to test\n. LGTM - waiting for green\n. Merging - thanks @burmanm!\n. @DirectXMan12 @mwielgus @fabxc Perhaps we could have an open hangout (or whatever) to discuss?\n. @DirectXMan12 SGTM - where can I find details on that?\n. @fabxc From https://docs.google.com/document/d/1RvhQAEIrVLHbyNnuaT99-6u9ZUMp7BfkPupT2LAZK7w/edit#heading=h.oh2koj9sbr3x:\n\nWe have bi-weekly meetings, every second Thursday at 11:30 US East Time, starting Oct 15.\n\nI assume that's still correct?\n. > namespace scoped stats will use service account\nI thought the discussion on this issue decided that push metrics would be restricted to system/infra stats, hence the applicability of using certificates? When you say namespace scoped stats do you mean application stats? \n. This isn't running a shell, this is executing the command explicitly as pid 1. Could you share the logs? I'd expect the quotes to not be required in this case tbh.\n. > god damnit\nI doubt He/She/It cares...\nAs you feel so strongly about it, how about sending in a PR that fixes it?\n. It's more of a convenience for doing builds for those with docker but\nwithout full go dev environment set up locally.\nOn 22:14, Tue, 24 Mar 2015 Vish Kannan notifications@github.com wrote:\n\nIn grafana/Dockerfile.dev\nhttps://github.com/GoogleCloudPlatform/heapster/pull/183#discussion_r27079077\n:\n\n@@ -0,0 +1,26 @@\n+FROM gliderlabs/alpine:3.1\n\nWhy add this file?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/183/files#r27079077\n.\n. This should be /db/\n. Remove middle slash with ENV INFLUXDB_EXTERNAL_URL /db/ above\n. Remove middle slash with ENV INFLUXDB_EXTERNAL_URL /db/ above\n. Shouldn't we separate out grafana & influxdb into separate pods now for easier deployment? Might want to scale influxdb & grafana separately for example.\n. Oops! Thanks @rjnagal - fixed now.\n. @vishh - this is generated by go generate but is causing boilerplate check to fail. Should I add this file to .gitignore & not commit? I've added go generate to the Makefile so this will be created on each build. Guess we need to update travis config as well?\n. You'll need to add port 8086 in too.\n. Is there any way we could use the options map instead of the flags to keep it consistent? How about using the first param as the client id?\n. Same for GCM sink.\n. IsSecure = c.AvoidColumns? Typo?\n. Could these registrations be moved into their respective packages to keep consistent registration pattern?\n. Same with the factory methods - can these be moved into their respective patterns to keep things cleaner?\n. Yeah that's how service accounts work. You create a service account, a token is generated, then you can reference that service account in your pod spec & token is automounted.\n. I'm working on it for OpenShift Origin docs & will either link to it from here or add it here too. There are some problems to work through as OpenShift is locked down right now so accessing /stats endpoint on the kubelet isn't possible... working on that too,\n. Does this work on non-debian based distros?\n. I guess you haven't tried building this? That URL doesn't work IIRC. Try changing the 6 to 8. Relying on build artifacts isn't great but no other option right now if using alpine.\n. Interesting - failed for me, sorry. Most have been a local issue.\n. So does heapster compress every response, even if the client hasn't requested compression?\n. Is that just due to the potential response size? We could just let go-restful handle the content encoding negotiation & let the client choose?\n. OK let me put in a PR for that.\n. No. The reason for using this package is to enable more configurable auth handlers in future. Right now we only see a need for client cert auth but there may be call for other auth methods in future or more granular authzn policy. We've learnt the hard way that it's better to put the auth behind interfaces now in case we want to enhance the auth policy later.\n. Ah OK thanks @liggitt my mistake.\n. Sorted.\n. PR merged - can get rid of the | gunzip or add -H \"Accept-Encoding: gzip\" to the curl command.\n. Couldn't this just be m := append(self.modifiers, metrics.Tenant(k)) ?\n. Not that it'll make too much difference given the probable size of the slice, but do you actually want to copy the underlying data?\n. append doesn't modify the underlying slice. Of course if stuff operates on the elements then that will be reflected in both slices which isn't what you want, but copying for the sake of it is a bit of a waste - but as it's a negligible optimisation I'm not going to argue the point.\n. Why are these ports removed?\n. Ah they're in same pod.\n. I think what he means is you can drop the type declaration as type will be inferred.\n. Consider me educated! Thanks!\n. not: returns rather than receives\n. ignore me, misread sorry\n. How about:\n\nHeapster can be configured to send data to multiple sinks by specifying the`--sink=...` flag multiple times.\n. For example, to send data to both gcm and influxdb at the same time, you can use the following:\n. Might read slightly better with something like:\n\nBy default, API responses are not compressed. Cadvisor supports gzip compression of responses via standard content negotiation. To enable, set the value of the Accept-Encoding header to gzip. The corresponding curl command will be like:. Agree. I would just remove the bit about KubeDash.\n. What is the use case for this rather than querying the API server directly?\n. That is a very short queryable period. Is there some discussion somewhere on this?\n. What is the reason that event storage is part of heapster rather than a separate tool to do that?\n. Does this mean raw metrics?\n. Don't really like the idea of having different queryable windows for different metrics. Makes is hard to compare. I understand the importance of CPU & memory but would like to see it be a bit more consistent.\n. :+1: \n. Ah I see theEventerbelow.\n. I think it makes sense to keep UC1,2,4,5 together as they're dealing with related data.\n. Can you explain more aboutno extra fine-tunning effort \n+to support >5 custom metrics per pod`? Not sure what you mean.\n. Going to have to be careful with these figures as long term storage options will probably struggle with this target.\n. From this can we extract the target for a single container sample with all metrics in memory? That would be very useful when we work on the data structs.\n. Doesn't necessarily have to be a side-car container, just an endpoint that accepts the push of metrics in defined format. Might even be outside of cluster in some cases. But the idea of a side-car container is also a possibility.\n. I think we should define what \"lots\" means before we try to compromise/optimise.\n. If there's no clear requirements then no need for it all.\n. +1 Sinks should be able to filter what's required in heapster.\n. You'd likely get more mileage out of contributing to existing projects, e.g. Prometheus, if this became a requirement.\n. So are you saying it shouldn't be returned by API queries, only passed to sinks? I'd prefer it to be exposed via the REST API & that means using the model, doesn't it?\n. Sorry that's a bad comment! Will update - raw containers are of course retained.\n. Point me to the requirements :) Don't we make it up as we go along? ;)\n\nHonestly, if it's too expensive to store a couple of extra values per pod then I don't mind dropping it from there & just leave it as passed to sinks.\n. Updated\n. OK how about I remove from the model for this version & revisit it for @mwielgus' rewritten version?\n. Monitoring & accounting, yes.\n. Can't think of a use for autoscaling on network traffic right now.\n. Seems to work OK...\n. Must be a change in godep binary - this is cleanly generated via godep save so just a tooling thing I guess.\nWould like to change to a different vendoring tool like glide soon though tbh. I find godep pretty annoying to work with personally & would prefer the standard golang vendor dir anyway.\n. Suggestions on how to do that? Right now I can't think of one tbh.\n. Labels are text values in Prometheus metrics, this is converting & (normalizing where necessary) to a string in an efficient way I guess. This is lifted straight from Prometheus codebase but let me do a quick benchmark to see if it's worthwhile.\n. Fair enough.\n. Removed.\n. Removed.\n. last_housekeep_seconds?\n. This should be eventer.\n. eventer\n. eventer\n. eventer\n. eventer\n. scraper\n. scraper\n. scraper\n. manager?\n. No keep as is.\n. Bit confused by this. We're adding the infra container to the appropriate pod so this is done.\n. nit: scrapeEventsDuration\n. Can drop _number - these are all numbers :)\n. Hmm I thought all containers in pods were either explicitly named in pod spec or assigned a name when created. Is that not right?\n. From https://github.com/kubernetes/kubernetes/blob/master/pkg/api/types.go#L843-L845:\ngo\ntype Container struct {\n    // Required: This must be a DNS_LABEL.  Each container in a pod must\n    // have a unique name.\n    Name string `json:\"name\"`\n. Do we have to support Kubernetes 1.0.x btw? We could get rid of this bit of logic & rely on Docker labels if not.\nHaving said that, I don't see any chance of failure in extracting the k8s container name from the Docker container name. This is what is done in the logging component successfully. Pod name is also present in the Docker container name, as is namespace - we can extract all that info from the Docker container name, but as I said above I'd prefer to drop support for 1.0.x & rely on Docker labels in 1.1.x.\n. So I think the extraction of container, pod & namespace looks right to me & I think we should trust that this will work. I can't see any problems with it.\n. So in summary, yes I personally am happy to consider any pods without ns, pod name, container name to be system containers.\n. Other statuses shouldn't have any running containers so will not be present in cadvisor stats.\n. Cadvisor doesn't know anything about pods. Any pods that are started/running should have a pod infra container running that will ensure the PodStatsContainer exists ready here.\n. Really a float32? Not limiting?\n. Maybe we need to switch this round then & loop through the stats we've collected & correlate to pods from pod list? That way we can't miss anything at all & we only keep state for the pods we actually have stats for.\n. Ah damn. How about keeping the mappings, deprecated, & supporting all metrics in the way this PR does? So check if metric is supported, if not check if there's a mapped metric?\n. Couldn't this be s/201[0-9]/2014/g? Then we won't need to change it in 2017 :)\n. Done.\n. So what would you prefer? List all metrics that could be available? Have to remember that we're not aggregating all metrics, are we?\n. Agreed - I'll update to validate for any known metric names & it will return empty for those that are not populated.\n. So now this PR return all possible metrics in /metrics requests, validate possible ones in /metrics/{metric-name:*}, return only for unknown metrics, regardless of what is cached.\n. So what do you want to return in /metrics requests? Only available (collected) metrics for the requested resource?\n. Should this be a debug log rather than info?\n. Let's go with 4.\n. When can we get into this state? Is this really a warning or lower log level?\nI'd also prefer to just continue if not found rather than if else.\n. I meant continue as in continue with the for loop. Reads better to me but only stylistic preference.\nI'm still unsure why changing the resolution makes it likely to hit this condition. Think that needs looking at more.\n. This would be more idiomatic to write as a switch rather than if-else.\n. Would prefer to flip this check & return ms.Labels[core.LabelNodename.Key] - no need for the else part, reduces nesting.\n. Can join this in the previous condition (f && !h.recent(p, model))\n. Shouldn't this just check if labelNodeId is zero string (len(h.labelNodeId) != 0)?\n. I've just seen this in a few other places actually, unrelated to this PR - all probably need updating.\n. Same as above - should be zero length string check.\n. Zero length string check.\n. Or make optional fields *string & check against nil that way. But length check preferred.\n. Zero length string check.\n. This seems like a nice approach to me. It would mean no change to the current flow of data into Heapster, no requirement for authentication changes, no flow control to prevent overloading Heapster.\n. ",
    "cedbossneo": "Split of the pullrequest #105 \n. We can, for our cluster hosted on RackSpace, we had some hostname resolution issues so we use IP instead so you're right, maybe this customization must stay outside the project.\nLe Mon Feb 16 2015 at 19:03:03, Vish Kannan notifications@github.com a\n\u00e9crit :\n\nIn sources/nodes/kube.go\nhttps://github.com/GoogleCloudPlatform/heapster/pull/105#discussion_r24766205\n:\n\n@@ -78,8 +78,15 @@ func (self _kubeNodes) List() (_NodeList, error) {\n            nodeList.Items[Host(node.Name)] = Info{PublicIP: node.Status.HostIP, InternalIP: addrs[0].String()}\n            goodNodes = append(goodNodes, node.Name)\n        } else {\n-           glog.Errorf(\"Skipping host %s since looking up its IP failed - %s\", node.Name, err)\n-           self.recordNodeError(node.Name)\n-           ip := net.ParseIP(node.Name)\n\nIs this a custom cluster setup where the node name is its IP? Why so? Why\nnot give the node an unique hostname that can be resolves to an IP?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/105/files#r24766205\n.\n. Yes, the node names(not hostname) match the IP of the node because on RackSpace you can't resolve the name of other machines, if you want to do this, you must declare each machines in your hosts file or add a local dns mechanism.\n. I think we can split the pull request because the first changeset ( resource := filepath.Join(\"stats\", pod.Namespace, pod.Name, pod.ID, containerName)) is required if you have a kubernetes cluster with multiple namespaces. Maybe i can find a better solution for the hostname issue.\n. \n",
    "mbforbes": "Thanks for the quick review. I also fixed up a bad merge from the README update yesterday (that added port 8086 to the firewall rules).\n. Done.\n. ",
    "saad-ali": "252\n. PTAL\n. PTAL. I refactored the decoder such that there are separate decoders for events and metrics--they create common timeseries objects that the drivers (gcm and influxdb) can consume.\nAnd based on your feedback, events and metrics now share the common schema (I added \"pod_name\").\nNew screenshot showing events in InfluxDB (pulled by heapster from Kubernetes API server and pushed by heapster to influxdb):\n\n. I think there's other information in there that is useful. I'll switch it to JSON.\n. @jimmidyson The original iteration of this PR was deriving columns from event objects. But after syncing with @vishh we agreed that this was contradictory to heapster's goals of having a common schema for all metrics/events. Hence this iteration.\n. @jimmidyson The idea was to try and maintain a common schema to the extent possible, and throw any additional information in to the value. Sink will always have the option to drop data that is irrelevant to them. The schema is documented here: docs/storage-schema.md\n. PTAL.\nNew screenshot:\n\n. Closing this. #234 replaces this.\n. LGTM. Will merge on green.\n. Good point. Wouldn't hurt to have it as part of the project though (Kubernetes does).\n. Integration tests pass when run locally:\n--- PASS: TestHeapsterInfluxDBWorks (508.96s)\nPASS\nok      github.com/GoogleCloudPlatform/heapster/integration 508.996s\nLooks like all recent Travis runs are failing with\nThe command \"go get golang.org/x/tools/cmd/vet\" failed and exited with 1 during .\n. Travis is now green\n. This would be super sweet, but it it looks like sequence_number can not be an arbitrary string.\nfailed to write events to influxDB - Server returned (400): sequence_number field must be float but is string (43e8c11b-e94c-11e4-a7c2-42010af0f6db)\n. CC @vishh \n. PTAL\n. PTAL rebased\n. PTAL.\n. This is completely out of date. Closing.\n. Way too many files to look at. But I assume you did some manual testing to verify things are still working. If so, LGTM.\n. CC @vishh @vmarmol @rjnagal \n. Instead of having two identical return statements, I prefer having one return statement for the method. But, no big deal either way. If you feel strongly, I can change it.\n. Implemented logic to restart watch loop if it fails. However this is not very robust. As discussed in person we will revisit this, as it's a bigger issue.\n. We can address this as a separate issue.\n. It is required for filtering out other precisions. Each Timeseries can specify it's own precision and the influxDB client expects all points in a WriteSeriesWithTimePrecision call to have the same precision. So we filter the timeseries, and issue one WriteSeriesWithTimePrecision call per precision type (if that type is present).\n. Fixed.\n. Changed.\n. Changed.\n. See above comment about precision, essentially storeTimeseriesWithPrecision is called for each possible precision type (second, milli, micro). It filters any timeseries that match that precision, and issues a WriteSeriesWithTimePrecision for those points with that precision. So this log message is essentially saying that a time series with a given precision was written.\n. Yes, it is an optimization. Each timeseries can have multiple points, and any given point may specify some subset of labels for the timeseries. So, instead of iterating through all points and their lables to figure out the complete set of label keys, we have LabelKeys defined at the top defining all the labels that points in this timeseries may have (if any individual point defines a label that is not part of labelKeys, it is ignored, if any individual point does not provide a value for a label that is part of labelKeys, it is left empty).\n. Done.\n. Done.\n. I don't think that is necessary. We just need to verify that events are making it through the pipeline.\n. :)\n. Bad merge. Fixed.\n. Why map to a struct and not a boolean?\n. Ah, TIL. Thanks.\n. ",
    "a-robinson": "Yeah, could definitely be due to cadvisor being missing.\nroot@kubernetes-minion-8hyo:/var/log# wget http://10.240.173.100:10250/stats/default/kube-dns-0133o/70de926d-bb1a-11e4-b34e-42010af06d77/etcd\n--2015-02-23 19:33:23--  http://10.240.173.100:10250/stats/default/kube-dns-0133o/70de926d-bb1a-11e4-b34e-42010af06d77/etcd\nConnecting to 10.240.173.100:10250... connected.\nHTTP request sent, awaiting response... 500 Internal Server Error\n2015-02-23 19:33:23 ERROR 500: Internal Server Error.\n. It's happening every 10 seconds on every node, with no signs of kubelet restarts.\n. The cadvisor container has been up for 14 hours, since the cluster was created. The cadvisor logs (from docker logs) are empty. Docker's health checks for it are succeeding:\ntime=\"2015-02-23T19:43:17Z\" level=\"info\" msg=\"-job container_inspect(a3fd307f497b2418822dd83bfbffd979cfea187ad8ede24c7734dd6af1f3def2) = OK (0)\"\n. Heapster logs:\n[...]\nE0223 18:32:23.447607       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.156.10:10250/stats/default/wordpress/9c8d9846-bb1a-11e4-b34e-42010af06d77/wordpress - Got 'Pod does not exist\n': invalid character 'P' looking for beginning of value\nE0223 18:32:23.447615       9 kube.go:158] failed to get stats for container \"wordpress\"/\"default\" in pod \"wordpress\"\nE0223 18:32:23.447653       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.156.10:10250/stats/default/wordpress/9c8d9846-bb1a-11e4-b34e-42010af06d77/wordpress - Got 'Pod does not exist\n': invalid character 'P' looking for beginning of value\nE0223 18:32:23.447675       9 kube.go:158] failed to get stats for container \"wordpress\"/\"default\" in pod \"wordpress\"\nE0223 18:32:23.447721       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.156.10:10250/stats/default/wordpress/9c8d9846-bb1a-11e4-b34e-42010af06d77/wordpress - Got 'Pod does not exist\n': invalid character 'P' looking for beginning of value\nE0223 18:32:23.447728       9 kube.go:158] failed to get stats for container \"wordpress\"/\"default\" in pod \"wordpress\"\nE0223 18:32:23.451652       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.173.100:10250/stats/ - Got 'Internal Error: received empty response from \"container info for \\\"/\\\"\"\n': invalid character 'I' looking for beginning of value\nE0223 18:32:23.457987       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.137.215:10250/stats/ - Got 'Internal Error: received empty response from \"container info for \\\"/\\\"\"\n': invalid character 'I' looking for beginning of value\nE0223 18:32:23.459743       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.147.240:10250/stats/ - Got 'Internal Error: received empty response from \"container info for \\\"/\\\"\"\n': invalid character 'I' looking for beginning of value\nE0223 18:32:32.777992       9 reflector.go:85] Failed to list *api.Pod: Get http://10.0.0.1:80/api/v1beta1/pods?fields=Status.Host%21%3D&namespace=: read tcp 10.0.0.1:80: connection reset by peer\nE0223 18:32:33.367214       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.156.10:10250/stats/default/wordpress/9c8d9846-bb1a-11e4-b34e-42010af06d77/wordpress - Got 'Pod does not exist\n': invalid character 'P' looking for beginning of value\nE0223 18:32:33.367853       9 kube.go:158] failed to get stats for container \"wordpress\"/\"default\" in pod \"wordpress\"\nE0223 18:32:33.368369       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.156.10:10250/stats/default/wordpress/9c8d9846-bb1a-11e4-b34e-42010af06d77/wordpress - Got 'Pod does not exist\n': invalid character 'P' looking for beginning of value\nE0223 18:32:33.368777       9 kube.go:158] failed to get stats for container \"wordpress\"/\"default\" in pod \"wordpress\"\nE0223 18:32:33.371077       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.156.10:10250/stats/default/wordpress/9c8d9846-bb1a-11e4-b34e-42010af06d77/wordpress - Got 'Pod does not exist\n': invalid character 'P' looking for beginning of value\nE0223 18:32:33.371386       9 kube.go:158] failed to get stats for container \"wordpress\"/\"default\" in pod \"wordpress\"\nE0223 18:32:33.371813       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.156.10:10250/stats/default/wordpress/9c8d9846-bb1a-11e4-b34e-42010af06d77/wordpress - Got 'Pod does not exist\n': invalid character 'P' looking for beginning of value\nE0223 18:32:33.372137       9 kube.go:158] failed to get stats for container \"wordpress\"/\"default\" in pod \"wordpress\"\nE0223 18:32:33.372444       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.156.10:10250/stats/default/wordpress/9c8d9846-bb1a-11e4-b34e-42010af06d77/wordpress - Got 'Pod does not exist\n': invalid character 'P' looking for beginning of value\nE0223 18:32:33.372640       9 kube.go:158] failed to get stats for container \"wordpress\"/\"default\" in pod \"wordpress\"\nE0223 18:32:33.372765       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.156.10:10250/stats/default/wordpress/9c8d9846-bb1a-11e4-b34e-42010af06d77/wordpress - Got 'Pod does not exist\n': invalid character 'P' looking for beginning of value\nE0223 18:32:33.372921       9 kube.go:158] failed to get stats for container \"wordpress\"/\"default\" in pod \"wordpress\"\nE0223 18:32:33.373051       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.156.10:10250/stats/default/wordpress/9c8d9846-bb1a-11e4-b34e-42010af06d77/wordpress - Got 'Pod does not exist\n': invalid character 'P' looking for beginning of value\nE0223 18:32:33.373191       9 kube.go:158] failed to get stats for container \"wordpress\"/\"default\" in pod \"wordpress\"\nE0223 18:32:33.374618       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.173.100:10250/stats/ - Got 'Internal Error: received empty response from \"container info for \\\"/\\\"\"\n': invalid character 'I' looking for beginning of value\nE0223 18:32:33.377418       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.137.215:10250/stats/ - Got 'Internal Error: received empty response from \"container info for \\\"/\\\"\"\n': invalid character 'I' looking for beginning of value\nE0223 18:32:33.378167       9 kubelet.go:80] failed to get stats from kubelet url: http://10.240.147.240:10250/stats/ - Got 'Internal Error: received empty response from \"container info for \\\"/\\\"\"\n': invalid character 'I' looking for beginning of value\n. kubelet logs:\n[...]\nI0223 19:44:33.373258    4813 server.go:594] POST /stats/: (597.668\u00b5s) 500\ngoroutine 174444 [running]:\ngithub.com/GoogleCloudPlatform/kubernetes/pkg/httplog.(*respLogger).WriteHeader(0xc2081beba0, 0x1f4)\n        /go/src/github.com/GoogleCloudPlatform/kubernetes/_output/dockerized/go/src/github.com/GoogleCloudPlatform/kubernetes/pkg/httplog/log.go:188 +0x9a\nnet/http.Error(0x7f8fca61f748, 0xc2081beba0, 0xc2081f6b40, 0x47, 0x1f4)\n        /usr/src/go/src/net/http/server.go:1274 +0x9a\ngithub.com/GoogleCloudPlatform/kubernetes/pkg/kubelet.(*Server).error(0xc2080e9100, 0x7f8fca61f748, 0xc2081beba0, 0x7f8fca60fa80, 0xc20827a090)/go/src/github.com/GoogleCloudPlatform/kubernetes/_output/dockerized/go/src/github.com/GoogleCloudPlatform/kubernetes/pkg/kubelet/server.go:642 +0x6ff\ngithub.com/GoogleCloudPlatform/kubernetes/pkg/kubelet.(*Server).handleStats(0xc2080e9100, 0x7f8fca61f748, 0xc2081beba0, 0xc208295520)\n        /go/src/github.com/GoogleCloudPlatform/kubernetes/_output/dockerized/go/src/github.com/GoogleCloudPlatform/kubernetes/pkg/kubelet/server.go:288 +0x46\ngithub.com/GoogleCloudPlatform/kubernetes/pkg/kubelet.*Server.(github.com/GoogleCloudPlatform/kubernetes/pkg/kubelet.handleStats)\u00b7fm(0x7f8fca61f748, 0xc2081beba0, 0xc208295520)\n        /go/src/github.com/GoogleCloudPlatform/kubernetes/_output/dockerized/go/src/github.com/GoogleCloudPlatform/kubernetes/pkg/kubelet/server.go:101 +0x45\nnet/http.HandlerFunc.ServeHTTP(0xc20807a760, 0x7f8fca61f748, 0xc2081beba0, 0xc208295520)\n        /usr/src/go/src/net/http/server.go:1265 +0x41\nnet/http.(*ServeMux).ServeHTTP(0xc2080aedb0, 0x7f8fca61f748, 0xc2081beba0, 0xc208295520)\n        /usr/src/go/src/net/http/server.go:1541 +0x17d\ngithub.com/GoogleCloudPlatform/kubernetes/pkg/kubelet.(*Server).ServeHTTP(0xc2080e90e0, 0x7f8fca61f710, 0xc208116c80, 0xc208295520)\n        /go/src/github.com/GoogleCloudPlatform/kubernetes/_\nI0223 19:44:33.631877    4813 server.go:594] GET /api/v1beta1/podInfo?podID=elasticsearch-logging-controller-oh43e&podNamespace=default: (7.373463ms) 0\nI0223 19:44:33.649061    4813 server.go:594] GET /api/v1beta1/podInfo?podID=kibana-logging-controller-gziey&podNamespace=default: (13.679069ms) 0\nI0223 19:44:33.654014    4813 server.go:594] GET /api/v1beta1/podInfo?podID=monitoring-influx-grafana-controller-ls6k1&podNamespace=default: (12.459173ms) 0\nI0223 19:44:33.670365    4813 server.go:594] GET /api/v1beta1/podInfo?podID=monitoring-heapster-controller-fplln&podNamespace=default: (17.910493ms) 0\nI0223 19:44:33.677777    4813 server.go:594] GET /api/v1beta1/podInfo?podID=kube-dns-0133o&podNamespace=default: (25.190853ms) 0\nI0223 19:44:34.779222    4813 server.go:594] GET /healthz: (661.37\u00b5s) 0\nI0223 19:44:37.815668    4813 file.go:209] Got pod from file \"/etc/kubernetes/manifests/cadvisor.manifest\": file-6bb810db-kubernetes-minion-8hyo.cadvisor-agent (57d25f8c3da447436ec35b3ce7097bb5)\nI0223 19:44:37.816141    4813 file.go:209] Got pod from file \"/etc/kubernetes/manifests/fluentd-es.manifest\": file-8cd71177-kubernetes-minion-8hyo.fluentd-to-elasticsearch (1ea443fc8fed5e1d27d6916f2da1d673)\nI0223 19:44:38.725653    4813 server.go:594] GET /api/v1beta1/podInfo?podID=elasticsearch-logging-controller-oh43e&podNamespace=default: (32.014506ms) 0\nI0223 19:44:38.741443    4813 server.go:594] GET /api/v1beta1/podInfo?podID=kibana-logging-controller-gziey&podNamespace=default: (29.460761ms) 0\nI0223 19:44:38.744238    4813 server.go:594] GET /api/v1beta1/podInfo?podID=kube-dns-0133o&podNamespace=default: (50.848013ms) 0\nI0223 19:44:38.746700    4813 server.go:594] GET /api/v1beta1/podInfo?podID=monitoring-heapster-controller-fplln&podNamespace=default: (26.532633ms) 0\nI0223 19:44:38.748346    4813 server.go:594] GET /api/v1beta1/podInfo?podID=monitoring-influx-grafana-controller-ls6k1&podNamespace=default: (33.794581ms) 0\nI0223 19:44:43.492990    4813 server.go:594] POST /stats/: (2.939874ms) 500\ngoroutine 174444 [running]:\ngithub.com/GoogleCloudPlatform/kubernetes/pkg/httplog.(*respLogger).WriteHeader(0xc208169680, 0x1f4)\n        /go/src/github.com/GoogleCloudPlatform/kubernetes/_output/dockerized/go/src/github.com/GoogleCloudPlatform/kubernetes/pkg/httplog/log.go:188 +0x9a\nnet/http.Error(0x7f8fca61f748, 0xc208169680, 0xc208228050, 0x47, 0x1f4)\n        /usr/src/go/src/net/http/server.go:1274 +0x9a\ngithub.com/GoogleCloudPlatform/kubernetes/pkg/kubelet.(*Server).error(0xc2080e9100, 0x7f8fca61f748, 0xc208169680, 0x7f8fca60fa80, 0xc20827a100)\n        /go/src/github.com/GoogleCloudPlatform/kubernetes/_output/dockerized/go/src/github.com/GoogleCloudPlatform/kubernetes/pkg/kubelet/server.go:117 +0x124\ngithub.com/GoogleCloudPlatform/kubernetes/pkg/kubelet.(*Server).serveStats(0xc2080e9100, 0x7f8fca61f748, 0xc208169680, 0xc208295a00)\n        /go/src/github.com/GoogleCloudPlatform/kubernetes/_output/dockerized/go/src/github.com/GoogleCloudPlatform/kubernetes/pkg/kubelet/server.go:642 +0x6ff\ngithub.com/GoogleCloudPlatform/kubernetes/pkg/kubelet.(*Server).handleStats(0xc2080e9100, 0x7f8fca61f748, 0xc208169680, 0xc208295a00)\n        /go/src/github.com/GoogleCloudPlatform/kubernetes/_output/dockerized/go/src/github.com/GoogleCloudPlatform/kubernetes/pkg/kubelet/server.go:288 +0x46\ngithub.com/GoogleCloudPlatform/kubernetes/pkg/kubelet.*Server.(github.com/GoogleCloudPlatform/kubernetes/pkg/kubelet.handleStats)\u00b7fm(0x7f8fca61f748, 0xc208169680, 0xc208295a00)\n        /go/src/github.com/GoogleCloudPlatform/kubernetes/_output/dockerized/go/src/github.com/GoogleCloudPlatform/kubernetes/pkg/kubelet/server.go:101 +0x45\nnet/http.HandlerFunc.ServeHTTP(0xc20807a760, 0x7f8fca61f748, 0xc208169680, 0xc208295a00)\n        /usr/src/go/src/net/http/server.go:1265 +0x41\nnet/http.(*ServeMux).ServeHTTP(0xc2080aedb0, 0x7f8fca61f748, 0xc208169680, 0xc208295a00)\n        /usr/src/go/src/net/http/server.go:1541 +0x17d\ngithub.com/GoogleCloudPlatform/kubernetes/pkg/kubelet.(*Server).ServeHTTP(0xc2080e90e0, 0x7f8fca61f710, 0xc208117ae0, 0xc208295a00)\n        /go/src/github.com/GoogleCloudPlatform/kubernetes/_\nI0223 19:44:43.782783    4813 server.go:594] GET /api/v1beta1/podInfo?podID=elasticsearch-logging-controller-oh43e&podNamespace=default: (8.38694ms) 0\nI0223 19:44:43.801743    4813 server.go:594] GET /api/v1beta1/podInfo?podID=kibana-logging-controller-gziey&podNamespace=default: (27.167657ms) 0\nI0223 19:44:43.811035    4813 server.go:594] GET /api/v1beta1/podInfo?podID=monitoring-heapster-controller-fplln&podNamespace=default: (23.502468ms) 0\nI0223 19:44:43.813873    4813 server.go:594] GET /api/v1beta1/podInfo?podID=monitoring-influx-grafana-controller-ls6k1&podNamespace=default: (26.454084ms) 0\nI0223 19:44:43.815521    4813 server.go:594] GET /api/v1beta1/podInfo?podID=kube-dns-0133o&podNamespace=default: (28.229048ms) 0\nI0223 19:44:45.335495    4813 server.go:594] GET /healthz: (651.195\u00b5s) 0\nE0223 19:44:47.031101    4813 reflector.go:85] Failed to list *api.Pod: Get https://10.240.109.119:6443/api/v1beta1/pods?fields=Status.Host%3Dkubernetes-minion-8hyo.c.arob-frogger.internal&namespace=: dial tcp 10.240.109.119:6443: i/o timeout\nE0223 19:44:47.031219    4813 reflector.go:85] Failed to list *api.Service: Get https://10.240.109.119:6443/api/v1beta1/services?namespace=: dial tcp 10.240.109.119:6443: i/o timeout\nI0223 19:44:48.864251    4813 server.go:594] GET /api/v1beta1/podInfo?podID=elasticsearch-logging-controller-oh43e&podNamespace=default: (24.75714ms) 0\nI0223 19:44:48.866261    4813 server.go:594] GET /api/v1beta1/podInfo?podID=kibana-logging-controller-gziey&podNamespace=default: (26.545063ms) 0\nI0223 19:44:48.899430    4813 server.go:594] GET /api/v1beta1/podInfo?podID=kube-dns-0133o&podNamespace=default: (13.145955ms) 0\nI0223 19:44:48.908430    4813 server.go:594] GET /api/v1beta1/podInfo?podID=monitoring-heapster-controller-fplln&podNamespace=default: (12.303541ms) 0\nI0223 19:44:48.920184    4813 server.go:594] GET /api/v1beta1/podInfo?podID=monitoring-influx-grafana-controller-ls6k1&podNamespace=default: (23.910287ms) 0\n. Sorry for the delay - I'd love to test this, but the wi-fi here isn't good enough for kube-up to work for me :/ I probably won't be able to until tomorrow\n. This appears to be fixed at head. Thanks Vish!\n. Thanks for the adjustment, @vmarmol. De-duping the label values like this should also reduce the data sent over the wire by quite a bit.\n. API LGTM, thanks!\n. LGTM\n. LGTM other than the naming issue\n. I don't seem to have reopen privileges, but this appears to still not be working in 0.14.2. Only pods have host_id populated, not the non-pod containers (system, kubelet, etc.)\n. I can't hit that endpoint, but LGTM.\n. Much better, thanks!\n. Fixed by #409\n. Are those logs only from around the time that the heapster pod started up? If they're happening frequently, it could be a sign of a bug somewhere.\n. Right, that shouldn't affect functionality. A few failures to connect at startup aren't concerning as long as they stabilize after a little while, since the master is also starting up around the same time.\n. Are there any potential problems if a container is deleted from the cache while it still exists? Say, for example, heapster fails to reach a node for 5 minutes. Will it continue trying to collect that data by virtue of what it knows from the apiserver, or is this cache considered the source of truth on what needs to be scraped?\n. Perfect, thanks. This LGTM as long as you're confident that the lastUpdated time is being tracked everywhere it needs to be.\n. If you're using Container Engine, go to Cloud Monitoring's GKE page. My Stack Overflow answer on this question explains the details.\nIf you're using Kubernetes directly via the scripts in this repository or get.k8s.io, you should also have an influxdb instance running in the cluster which contains the metrics.\nPlease reopen if this doesn't answer your question!\n. I hope you don't mind me playing devil's advocate -- why is storing historical data heapster's job? Isn't that the point of the sinks?\nI do see a lot of value in being able to read historical data in a backend-agnostic way, but I have two thoughts on that:\n1. We should be careful to avoid trying to do too much. Scope how many different kinds of data munging and aggregation we're willing to do. Limit the number of backends we support unless others are willing to own them.\n2. Be careful about how much memory we use while plumbing through historical data. It'd actually make some sense for this to be a separate component, such that the reading of metrics couldn't interfere with the ingestion of metrics.\nThis honestly seems like the sort of thing that might eventually benefit from being a separate project rather than tied to kubernetes, but that's definitely too much extra work for now.\n. @jimm-porch - turning heapster off is a perfectly reasonable option. The only effect it'll have is that metrics from your cluster will no longer be available via Cloud Monitoring. If you want to turn it off, just run gcloud alpha container clusters update CLUSTERNAME --monitoring-service \"none\".\n. Also, judging by the limitations section of the addon-updater, patching the image version used won't be overwritten by the addon-updater, so that should also work.\n. Sorry I missed this the last couple days. Taking a look now\n. LGTM now\n. You can assign this one to me as well, I no longer have maintainer privileges on heapster to be able to assign myself.\n. I'm going to assume that the new schema there is outdated, since it's missing the limit and filesystem metrics, which we've resolved elsewhere. With those added, the new schema LGTM\n. LGTM\n. Sorry, the nodes actually have 100GB boot disks, which is at least on the same scale as the metrics we're seeing, but the metrics are still too high.\n. LGTM, but can we have a unit test? I thought that there used to be a test that covered all the fields we expected to be present?\n. That test sounds a little self-fulfilling :)\nWe always want:\nnamespace_id\npod_id\nhost_id\ncontainer_name\nAnd for disk metrics, we expect a resource_id field.\n. Thanks!\n. Does this only group by the target labels (the ones that identify a container), or will it also group by the metric-specific labels, like device_name and fault_type?\n. Target labels only would definitely be preferable, but we could live with this if grouping only by target labels is too difficult.\n. Doesn't this have to be a map from metric names to more than just a single point? A single target-metric pair can have more than one point if the metric has additional labels. For example, if I have one disk-bytes-used data point for device_name=/dev/sda1 and one point for device_name=/dev/sda2, both from the same container, I'm not sure how this schema supports exposing both?\n. Nit, but better phrased as \"The UID of the namespace of the pod\"\n. It's not my API, but I'd much prefer a name that makes it clear that it's the external ID of the host. Otherwise it's not clear which object the ID belongs to, and the default that I'd expect if I didn't know better is the container.\nCould be any of host_external_id, host_id, node_external_id, node_id, etc.\n. Every second seems way too often to run GC that only garbage collects things on the order of minutes old. Can you increase the period between runs? Every minute seems like it'd be more than enough, since holding data an extra minute shouldn't pose much risk.\nIf you have this set so low for the sake of testing, then just make it configurable and set it lower in testing.\n. Did you remove the assert.NotEmpty(cache.GetFreeContainers(zeroTime, zeroTime)) line by accident?\n. Or base it on the buffer duration\n. I assume you probably already checked, but can you please double check that this is set  appropriately everywhere that the pod/container elements are updated?\n. Including when it's initially created\n. Does this need to be part of the cherry-pick? It'd be ideal to keep unrelated changes to a minimum, since we can't know what effect this might have without experience using it.\n. Has memory usage been an issue that needs fixing?\n. But you did manually check in place of a test, then, right?\n. Sure, we can keep it in. I just want to minimize the risk running into bugs due to some other component unexpectedly depending on things being around for 10 minutes, or a cache array having some minimum number of data points.\n. Is this used anywhere by this PR?\n. I'm a little worried that moving the definitions of the API types into the core makes it more likely that they'll be changed in the future without considering that they're part of what needs to be a stable API.\nIs there any particular motivation for this change that justifies the risk?\n. It's an existing problem, but it wouldn't hurt to fix the \"Kuberentes\" typo while you're here\n. Without having made it through the actual code changes, commenting out an existing test scares me. Does this mean the new API output won't contain everything the old API output does?\n. Just a suggestion, but using the raw string literal (e.g. \"namespace_id\") rather than the same constant that's shared with the code would be more future-proof against accidental changes to the value of the constant.\n. ",
    "DreadPirateShawn": "For what it's worth, I'm currently seeing the same issue, using the latest heapster github code.\nJust to make sure, here's the latest pull and restart that I performed:\n```\n20:58 kube_devstack highland@kube-master1:~/heapster$ git pull origin master\nremote: Counting objects: 6, done.\nremote: Compressing objects: 100% (6/6), done.\nremote: Total 6 (delta 1), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (6/6), done.\nFrom https://github.com/GoogleCloudPlatform/heapster\n * branch            master     -> FETCH_HEAD\nUpdating d5098a9..ec1c526\nFast-forward\n sources/cadvisor_test.go |    8 ++++++--\n 1 file changed, 6 insertions(+), 2 deletions(-)\n21:06 kube_devstack highland@kube-master1:~/heapster$ deploy/kube.sh restart\nheapster pods have been removed.\nheapster pods have been setup\n```\nI see that the pods were indeed recreated, however I'm still seeing the POST to /stats/ every 10 seconds:\n[...]\n/logs/kube-minion3/INFO-174305.28825:I0226 21:08:50.271049   28825 server.go:362] POST /stats/: (323.296\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825:I0226 21:09:00.278409   28825 server.go:362] POST /stats/: (411.688\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825:I0226 21:09:10.271291   28825 server.go:362] POST /stats/: (521.926\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825:I0226 21:09:20.272693   28825 server.go:362] POST /stats/: (505.051\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825:I0226 21:09:30.265936   28825 server.go:362] POST /stats/default/monitoring-heapster-controller-2gdny/6b8cfbc5-bdfb-11e4-848e-00505629c58f/heapster: (2.417704ms) 500\n/logs/kube-minion3/INFO-174305.28825:I0226 21:09:30.267541   28825 server.go:362] POST /stats/default/monitoring-heapster-controller-2gdny/6b8cfbc5-bdfb-11e4-848e-00505629c58f/heapster: (3.741048ms) 500\n/logs/kube-minion3/INFO-174305.28825:I0226 21:09:30.268122   28825 server.go:362] POST /stats/: (170.12\u00b5s) 500\nAnd from the kubectl logs:\n21:16 kube_devstack highland@kube-minion1:~$ kubectl.sh log monitoring-heapster-controller-2gdny | tail\n2015-02-26T21:17:20.268190019Z E0226 21:17:20.268150       5 kube.go:158] failed to get stats for container \"influxdb\"/\"default\" in pod \"monitoring-influx-grafana-controller-xxvi7\"\n2015-02-26T21:17:20.272222969Z E0226 21:17:20.271986       5 kubelet.go:80] failed to get stats from kubelet url: http://10.0.0.4:10250/stats/default/monitoring-influx-grafana-controller-xxvi7/6bfa5435-bdfb-11e4-848e-00505629c58f/influxdb - Got 'Internal Error: received empty response from \"Docker container info for \\\"35c432aef492333df08db6c44eb42c4977392c4bdef6c0b33182730a82480a89\\\"\"\n2015-02-26T21:17:20.272222969Z ': invalid character 'I' looking for beginning of value\n2015-02-26T21:17:20.272222969Z E0226 21:17:20.271998       5 kube.go:158] failed to get stats for container \"influxdb\"/\"default\" in pod \"monitoring-influx-grafana-controller-xxvi7\"\n2015-02-26T21:17:20.273595492Z E0226 21:17:20.272685       5 kubelet.go:80] failed to get stats from kubelet url: http://10.0.0.7:10250/stats/ - Got 'Internal Error: received empty response from \"container info for \\\"/\\\"\"\n2015-02-26T21:17:20.273595492Z ': invalid character 'I' looking for beginning of value\n2015-02-26T21:17:20.273626242Z E0226 21:17:20.273016       5 kubelet.go:80] failed to get stats from kubelet url: http://10.0.0.4:10250/stats/ - Got 'Internal Error: received empty response from \"container info for \\\"/\\\"\"\n2015-02-26T21:17:20.273626242Z ': invalid character 'I' looking for beginning of value\n2015-02-26T21:17:20.273626242Z E0226 21:17:20.273319       5 kubelet.go:80] failed to get stats from kubelet url: http://10.0.0.3:10250/stats/ - Got 'Internal Error: received empty response from \"container info for \\\"/\\\"\"\n2015-02-26T21:17:20.273626242Z ': invalid character 'I' looking for beginning of value\nIs there more that I could check? (Still finding my way around the logs.)\nEDIT: Granted, it's possible that cadvisor is not yet running properly on my system -- that's what I'm looking into now -- but the POST logs still seem suspect, given this ticket.\n. Ah! Thanks, I didn't realize where the image was specified -- I see what you mean, now. Even better, I tried your test image, and I do see the POST actions change to GET:\n[...]\n/logs/kube-minion3/INFO-174305.28825 22:51:02.629496   16718 server.go:362] POST /stats/: (172.947\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825 22:51:12.624493   16718 server.go:362] POST /stats/: (433.335\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825 22:51:22.623663   16718 server.go:362] POST /stats/default/monitoring-influx-grafana-controller-182tv/db5fa8f2-be03-11e4-848e-00505629c58f/influxdb: (8.611277ms) 500\n/logs/kube-minion3/INFO-174305.28825 22:51:22.627992   16718 server.go:362] POST /stats/default/monitoring-influx-grafana-controller-182tv/db5fa8f2-be03-11e4-848e-00505629c58f/influxdb: (12.700853ms) 500\n/logs/kube-minion3/INFO-174305.28825 22:51:22.628858   16718 server.go:362] POST /stats/: (237.14\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825 22:51:32.615554   16718 server.go:362] POST /stats/: (449.188\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825 22:52:19.700102   16718 server.go:362] GET /stats/default/monitoring-heapster-controller-zjpez/06983ddc-be0a-11e4-848e-00505629c58f/heapster: (6.570953ms) 500\n/logs/kube-minion3/INFO-174305.28825 22:52:19.707005   16718 server.go:362] GET /stats/default/monitoring-heapster-controller-zjpez/06983ddc-be0a-11e4-848e-00505629c58f/heapster: (13.167025ms) 500\n/logs/kube-minion3/INFO-174305.28825 22:52:19.708450   16718 server.go:362] GET /stats/: (696.24\u00b5s) 500\n/logs/kube-minion3/INFO-174305.28825 22:52:29.702688   16718 server.go:362] GET /stats/default/monitoring-influx-grafana-controller-i4hx9/06e12bd6-be0a-11e4-848e-00505629c58f/influxdb: (9.018229ms) 500\n/logs/kube-minion3/INFO-174305.28825 22:52:29.709717   16718 server.go:362] GET /stats/default/monitoring-influx-grafana-controller-i4hx9/06e12bd6-be0a-11e4-848e-00505629c58f/influxdb: (16.410284ms) 500\n/logs/kube-minion3/INFO-174305.28825 22:52:29.711006   16718 server.go:362] GET /stats/: (340.775\u00b5s) 500\nI'm pretty sure now that my other issue is cAdvisor, since I'm still seeing the \"invalid character 'I' looking for beginning of value\" errors, but taking this GET/POST aspect off the table is definitely helpful.\n. Any update on this doc? Still empty. Even just a sample command or flags...\n. I can't speak for the others, but I arrived at this doc stub while attempting to run heapster as a separate process with --source pointing to kubernetes master.\nI ended up filing issue #392 during my trial-and-error phase, and ultimately @jimmidyson suggesting adding inClusterConfig=false as an extra option to the kube source.\nThis was indeed the solution to my troubles, and that's the sort of info which I would expect to be in the \"standalone\" doc.\nGranted, there may be other valid contexts for \"standalone\" heapster -- @rjnagal above questions whether this may apply to running without kubernetes or fleet, and I am definitely running with kubernetes. Perhaps the doc should include use-cases for various \"standalone\" interpretations? eg, \"standalone\" meaning \"running the heapster binary externally to a kubernetes cluster rather than as a container in the cluster\", \"running the heapster binary externally with a source other than kubernetes\" (if that's even a thing that can be done), etc... dev testing vs having a scaled out setup... basically each variation of \"standalone\" could be acknowledged in the doc, along with either a working example (eg /my/bin/heapster --v=5 --logtostderr=true --port=8082 --source=\"kubernes:http://10.0.0.2:8080?insecure=true&inClusterConfig=false\") or confirmation of lack of support.\nJust my two cents, of course.\n. I've also tried adding the insecure=true option found later in the same doc, with the same failure, eg:\n$ /my/bin/heapster --v=5 --logtostderr=true --port=8082 --source=kubernetes:http://10.0.0.2:8080?insecure=true\\&auth=''\nI0708 15:55:16.246421    5602 heapster.go:52] /my/bin/heapster --v=5 --logtostderr=true --port=8082 --source=kubernetes:10.0.0.2:8080?insecure=true&auth=\nI0708 15:55:16.246602    5602 heapster.go:53] Heapster version 0.16.0\nE0708 15:55:16.246703    5602 heapster.go:59] open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory\n. As I read through the source and issues, I'm starting to beg the question -- now that cadvisor has been integrated into kubernetes, is it even possible to run heapster as a separate process and still tie into the kubernetes cluster?\nI've got some dev work in progress which implements a new sink for heapster, so it seems like running heapster directly during dev / fix cycles would allow for more speed / flexibility, but is that even an option anymore?\n. Hmm. In the kubernetes cluster, authentication is currently disabled -- using neither cert nor token -- so it is using the same config... does that mean that even if I switch to loading heapster inside the cluster, it'll still fail in the same manner? That is, does heapster (currently) require that some authentication is in place?\n. Fantastic -- this gets me further:\n```\n/my/bin/heapster --v=5 --logtostderr=true --port=8082 --source=\"kubernes:http://10.0.0.2:8080?insecure=true&inClusterConfig=false\"\nI0708 18:45:58.212246    7140 heapster.go:53] Heapster version 0.16.0\nI0708 18:45:58.213190    7140 kube_factory.go:169] Using Kubernetes client with master \"http://10.0.0.2:8080\" and version \"v1\"\nI0708 18:45:58.213234    7140 kube_factory.go:170] Using kubelet port 10255\nI0708 18:45:58.213314    7140 kube_events.go:153] Starting \"kube-events\" source\nI0708 18:45:58.213338    7140 kube_events.go:155] Finished starting \"kube-events\" source\nI0708 18:45:58.215322    7140 heapster.go:64] Starting heapster on port 8082\nI0708 18:45:58.215526    7140 manager.go:84] starting to scrape data from sources\nI0708 18:45:58.223577    7140 manager.go:66] attempting to get data from source \"Kube Pods Source\"\n```\nGranted, I'm seeing other errors now:\nE0708 18:45:58.225355    7140 reflector.go:136] Failed to list *api.Pod: field label not supported: spec.nodeName\n... but now I can dig into the code and get more info, and file separate tickets if needed. Closing this ticket.\nMany thanks!\n. I'm seeing the same thing, attempting to run kubectl.sh create -f deploy/kube-config/influxdb out-of-the-box.\nSpot-checked via docker directly:\n$ docker run kubernetes/heapster:v0.16.0\nUnable to find image 'kubernetes/heapster:v0.16.0' locally\nPulling repository kubernetes/heapster\nFATA[0005] Tag v0.16.0 not found in repository kubernetes/heapster\n. Also relevant to this topic, for posterity:\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1421834\n\nIn Kubernetes, we specify a cAdvisor housekeeping interval of 10s.  cAdvisor has backoff logic that could move this up to 15s, plus jitter logic that could double that, leaving us with an actual max housekeeping interval of 30s:\nhttps://github.com/kubernetes/kubernetes/blob/master/vendor/github.com/google/cadvisor/manager/container.go#L382.\nThus, our Heapster image needs to have a metrics_resolution of at least 30s\nin order to avoid reading the same metrics batch twice from the node, which causes Heapster to skip rate calculation, which causes CPU metrics to not appear.. Any update on this?. \n",
    "idoshamun": "I keep getting:\nE0418 16:46:17.721174       7 kubelet.go:85] failed to get stats from kubelet url: http://172.18.0.13:10250/stats/default/monitoring-influx-grafana-controller-gkukt/6889f02a-e5e8-11e4-b87a-000d3a30177a/grafana - Get http://172.18.0.13:10250/stats/default/monitoring-influx-grafana-controller-gkukt/6889f02a-e5e8-11e4-b87a-000d3a30177a/grafana: malformed HTTP response \"\\x15\\x03\\x01\\x00\\x02\\x02\"\nWhen I wget this link it downloads a binary file.\n. Fixed it.\nThanks\n. ",
    "googlebot": "Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. ",
    "samek": "@rjnagal  Well the way I look it it's either I write it  from scratch or We add just the new source to this. \nBut we would need to discuss the approach part, get some guide lines ..\n. Yes and no\n We've change the source of cadvisor so that it sends enviroment vars to influxdb.\nEnv vars are what you need.\nSent from my iPhone\n\nOn 21 Mar 2015, at 19:44, Vish Kannan notifications@github.com wrote:\n@samek: Were you able to make any progress?\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "MikeMichel": "What does heapster need? A list of node ips where containers are running?\n. ok, then it has to check http://mesosmaster:5050/master/state.json and there the slaves array. It contains the slave ips.\n. I will start with heapster tomorrow but from what i understand i have to run cadvisor on the nodes and heapster will connect to a list of nodes to gather the metrics?\n. how do i use the external hostsfile? using --source=cadvisor:external add the cmd line is not working. it's not showing in the available option up when doing /usr/bin/heapster --help. tried the latest docker version also the canary one.\n. no, maybe the docker version is outdated. i will try from source tomorrow.\n. ",
    "exnerd": "Hi all,\nI was looking to get heapster running on a mesos cluster. I tried to insert some mesos slave running a cadvisor into the external file, but I don't know how the file must look like. By looking into the code I tried following\n{ \"items\": { \"name\": \"mesos-04.development.vaph.be\",\"ip\": \"193.53.25.110\"}}\nbut that didn't make it.\nI got the error :\nfailed to get information from source - failed to unmarshal contents of file /var/run/heapster/hosts. Error: json: cannot unmarshal string into Go value of type []api.ExternalNode\nWhat should the format of the external file be ?\n. ",
    "gtmtech": "Did anyone get anywhere with this?\n. ",
    "naxhh": "i've been playing with this a bit.\nI'm just starting to play with mesos and marathon. And I don't find a way to retrieve public and private ip's from mesos.\nMesos REST API returns this information about slaves\n{\"slaves\":[{\"active\":true,\"attributes\":{},\"hostname\":\"10.141.141.10\",\"id\":\"20150913-141231-16842879-5050-1194-S0\",\"pid\":\"slave(1)@127.0.1.1:5051\",\"registered_time\":1442153555.00333,\"resources\":{\"cpus\":2,\"disk\":34068,\"mem\":1000,\"ports\":\"[31000-32000]\"}}]}\nIs not a lot, and is not enough to fill nodes.Info.\nI've been checking the go library: https://godoc.org/github.com/mesos/mesos-go\nBut seems to be in really early stages, I don't find anything useful for what I want.\nFinally, zookeeper should have this information... so maybe a better approach will be to make requests directly to zookeper. I don't know what is the info inside zk..\nAny idea or directions in this regard?\n. I've checked the setup at work. And hostname is not always an ip. Resolving the hostname to ip's is an option. I just wanted to make it as simple as possible.\nChecking information from Zk.\n```\nslave01:\n20150902-134310-1560936451-5050-16793\ufffd\ufffd\ufffd\ufffd\ufffd'\"master@10.0.10.1:5050*hostname0120.23.0\nslave02:\n$20150902-134316-4127195142-5050-1887\ufffd\ufffd\ufffd\ufffd\ufffd'\"master@10.0.0.2:5050*hostname0220.23.0\n```\nThis is the information from /mesos/info_0000000001, /mesos/info_0000000002\nThe information I've found there is pretty much the same I can found in the API (state.json)\nMaybe doing it from ZK is more reliable than doing it from the API? In the API we could resolve hostnames or parse pid (pid: \"slave(1)@10.0.0.1:5051)\nFrom ZK, I should learn how unserialize that data and parse the master@ip:port thing....\nFrom my POV, resolving hostname to IP's is the simplest way. I don't know if requiring DNS will be a big problem for some users.\nFinal question: nodes.Info has a few fields. What is really required from there? Public and private ip? or just private ip?\n\nEdit:\nBy the way. I'm just trying to retrieve the Mesos slaves because someone suggested to do that. But I'm thinking that you could read the apps and retrieve hostnames and ports for cadvisor running apps. At the end that's what heapster needs, right?\nv2/apps/cadvisor will have this kind of information:\n```\n\"tasks\": [\n {\n   \"id\": \"cadvisor.asdasd\",\n   \"host\": \"slave1.mesos.com\",\n   \"ports\": [ 3001 ],\n   \"appId: \"/cadvisor\",\n },\n {\n   \"id\": \"cadvisor.asdasd2\",\n   \"host\": \"slave2.mesos.com\",\n   \"ports\": [ 3001 ],\n   \"appId: \"/cadvisor\",\n },\n]\n```\nAnd invoke this as:\n./heapster --source=\"cadvisor:marathon?marathonUrl=mesos02:8080\"\nThat is enough?\n. ",
    "sub-mod": "write error: No space left on device\ngodep: go exit status 2\n. @vishh done\n. Isn't it a good practice to have the selector as a label in the service too, I saw it in the  discussions on some PR\n. ",
    "agios": ":+1: \n@vishh The url you posted does not work for me either, but the one used in this PR does work\nhttps://<DBHOST>:<DBPORT>/db/\n. ",
    "Andre-Freitas": "@agios this works if you replace influxdb-grafana-controller.yaml grafana image with pires/grafana. No need to apply this PR in your stuff.\n. plus the error above i'm detecting this second error '/set_basic_auth.sh: line 10: [: ==: unary operator expected' in grafana logs\nhttp://ur1.ca/k1eox\n. @vishh I have deployed the latest version from github, but now my grafana web page is giving 404 page not found regarding at each graph http://postimg.org/image/7ahbhrzj3/ \nRegarding the logs of each container: http://ur1.ca/k4916\nRegarding heapster webpage: http://ur1.ca/k491g\n. Ok i repetead the instalation of heaspter and now i have found the cause of the problem\nLet me explain:\n1\u00ba At this moment i have the following services http://ur1.ca/k4hom\n2\u00ba When i start a browser to open grafana web page http://10.100.163.163/#/dashboard/file/kubernetes.json i get a lot of errors 404 in browser http://ur1.ca/k4hpc  (because grafana is trying to querying the apiserver using grafana ip service - which will not work)\n3\u00ba if i get one of the querys that is giving 404 (for example, http://10.100.163.163/api/v1beta1/proxy/services/monitoring-grafana/db/k8s/series?p=root&q=select+container_name,+mean(value)+from+%22memory%2Fworking_set_bytes_gauge%22+where+time+%3E+now()+-+5m+and+labels+%3D~+%2Fname:influxGrafana%2F+group+by+time(2s),+container_name+order+asc&u=root) and change it to use the ip of the service kubernetes-ro, i get the correct results http://ur1.ca/k4hrn (http://10.100.0.1/api/v1beta1/proxy/services/monitoring-grafana/db/k8s/series?p=root&q=select+container_name,+mean(value)+from+%22memory%2Fworking_set_bytes_gauge%22+where+time+%3E+now()+-+5m+and+labels+%3D~+%2Fname:influxGrafana%2F+group+by+time(2s),+container_name+order+asc&u=root)\nThe problem is in the way that grafana trys to query influxdb trough apiserver - all querys should use the kubernetes-ro ip service and instead its using is own ip on the querys\n. After a IRC chat with @vishh the problem was resolved when i removed the following env variable from influxdb-grafana-controller.yaml\n- name: \"INFLUXDB_EXTERNAL_URL\"\n  value: '/api/v1beta1/proxy/services/monitoring-grafana/db/'\n. I'm having the same error as @SergeyCherepanov as i can't connect to api using service account token - http://ur1.ca/n7f0g\nI have create a new service account token heapster and despite my cluster k8s is 0.21.4 i only have a token file in /tmp/secrets/kubernetes.io/serviceaccount\nI'm using this argments in my rc http://ur1.ca/n7f1j . how can i resolve this?\n. @SergeyCherepanov how did you fetch the ca.cert (and token) and added it to volume? atm my heaspter pod only has token file. \n. @vishh its required to have ca.crt in /var/run/secrets/kubernetes.io/serviceaccount/ even when we use service account token heapster to communicate with the API (atm \u00ed'm using this --source=kubernetes:https://kubernetes:443?useServiceAccount=true&auth= ? )\nAs i don't have ca.crt in my kube-apiserver (i only have self-signed certificate and key that were generated for the public address and saved to /var/run/kubernetes) this could be the reason that my heapster pod doesn't have ca.crt\n. @mikedanese yes i'm running in kubernetes 0.21.4 with a json rc like this http://ur1.ca/n85ks.\n@vishh i have tried to put source --source=kubernetes:'' but its the same. Monday i will try to put  --source=kubernetes:https://kubernetes:443 and see what happens. so the fact that i don't have ca.crt defined in my master is not a problem? because i only have token file in /var/run/secrets/kubernetes.io/serviceaccount/\n. ",
    "okapusta": "```\nHeapster Version: 0.8\nSource type: Cadvisor\n    NodeList: {Items:map[55a6aed1a8b947f9bd3886e6845eea7b:{PublicIP:10.21.2.131 InternalIP:10.21.2.131} 60023ec21a6643ebb0a71a54454d1bcc:{PublicIP:10.21.1.34 InternalIP:10.21.1.34} c30d3028403a4f179c9c5981759a1f1d:{PublicIP:10.21.2.132 InternalIP:10.21.2.132}]}\n    Fleet Nodes plugin: Aggregate error count: 0; recent error: \nCadvisor Nodes: map[c30d3028403a4f179c9c5981759a1f1d:{10.21.2.132 10.21.2.132} 55a6aed1a8b947f9bd3886e6845eea7b:{10.21.2.131 10.21.2.131} 60023ec21a6643ebb0a71a54454d1bcc:{10.21.1.34 10.21.1.34}]\nSink type: Influxdb\n    client: Host \"10.21.1.34:8086\", Database \"k8s\"\n    Data buffering duration: 10s\n    Number of write failures: 0\n```\ncore@ip-10-21-1-34 ~/monitoring $ docker logs -f heapster \n+ EXTRA_ARGS=\n+ '[' '!' -z true ']'\n+ EXTRA_ARGS=' --coreos'\n+ '[' '!' -z true ']'\n+ EXTRA_ARGS=' --coreos --vmodule=*=3'\n+ '[' '!' -x ']'\n+ HEAPSTER='/usr/bin/heapster  --coreos --vmodule=*=3 '\n+ '[' '!' -z ']'\n+ '[' '!' -z 10.21.1.34:8086 ']'\n+ /usr/bin/heapster --coreos '--vmodule=*=3' --sink influxdb --sink_influxdb_host 10.21.1.34:8086\nI0311 16:20:56.013186      10 heapster.go:44] /usr/bin/heapster --coreos --vmodule=*=3 --sink influxdb --sink_influxdb_host 10.21.1.34:8086\nI0311 16:20:56.013266      10 heapster.go:45] Heapster version 0.8\nI0311 16:20:56.013414      10 heapster.go:46] Flags: alsologtostderr='false' bq_account='' bq_credentials_file='' bq_id='' bq_project_id='' bq_secret='notasecret' cadvisor_port='8080' coreos='true' external_hosts_file='/var/run/heapster/hosts' fleet_endpoints='http://127.0.0.1:4001' kubelet_port='10250' kubernetes_insecure='true' kubernetes_master='' listen_ip='' log_backtrace_at=':0' log_dir='' logtostderr='true' max_procs='0' poll_duration='10s' port='8082' sink='influxdb' sink_influxdb_buffer_duration='10s' sink_influxdb_host='10.21.1.34:8086' sink_influxdb_name='k8s' sink_influxdb_password='root' sink_influxdb_username='root' sink_memory_ttl='1h0m0s' stderrthreshold='2' v='0' vmodule='*=3'\nI0311 16:20:56.013448      10 influxdb.go:255] Using influxdb on host \"10.21.1.34:8086\" with database \"k8s\"\nI0311 16:20:56.014897      10 heapster.go:57] Starting heapster on port 8082\nI0311 16:21:07.094145      10 influxdb.go:226] flushed data to influxdb sink\n. Heapster flushed data to influxdb sink one more time at 16:25:58 and then stopped\n. Container is running, it just stopped querying (I guess) cadvisor containers and pushing data to InfluxDb\n. ",
    "etcinit": "I was experiencing this issue with a Deis cluster too. I just tried using v0.10.0 and the problem seems fixed now.\n. ",
    "biwwy": "I have the same issue as described above on CoreOS cluster, using heapster:latest. \n. I did so, it was saying \"I0520 20:19:22.335361       1 heapster.go:46] flushed stats to influxDB\" about once an hour or so, instead of 10s. I've updated to v.0.10.0 as mentioned above and it seemed to fix the issue.\n. ",
    "liuhewei": "@jimmidyson We are working on a heapster kafka-sink to pub metrics to kafka, but we don't separate metrics from different ns/pod/containers to different topics as you said. Our idea now is \"one topic for one k8s cluster\". Our purpose is similar to your thought: provide metrics to different backend services such as alerting, autoscaling, not only for storage and dashboard.\n. @vishh I have a question here about the \"--sink\" flag: \nThe kafka-client usually need be configured multiple tcp endpoints for a broker cluster, typically:\nvar kafkaAddrs = []string{\"192.168.1.2:9092\", \"192.168.1.2:9093\"}\nIt's different with other existing sinks such as influxdb/hawkular which usually has only one http_url as  the endpoint.\nAnd I checked the sink extension's interface, then found it supports only one url endpoint (*url.URL):\ntype SinkFactory func(*url.URL, HeapsterConf) ([]sinksApi.ExternalSink, error)\nIt's a problem now, and what's your suggestion? Thanks.\n. Interesting features! Are there already some introduction or draft design docs to be shared? Thanks.\n. @vishh From the short-term feature list, we're very interested in the \"Support for application metrics\". Actually we're also thinking about how to collect more application-level metrics inside containers. I assume this feature is related to cAdvisor's design here? And, the scalability and performance are also key points. We're preparing to do some stress testing on our deployment later. By the way, we'll submit our kafka-sink as a PR around this weekend for review.\nAs for the long-term and future features, these policy-related features such as \"Resource usage predictions, events generating and outlier detection\" are also interesting. Actually we're doing some researches on these directions. We have our own policy-engine to analyze the data collected from heapster. If heapster can do those things, we're happy to follow the progress.\nAnd I see an ecosystem related feature: Support for Docker Swarm and Mesos. I wonder if this feature need cAdvisor also be installed on Mesos or Swarm? It will be great if heapster's source-configuration is flexible enough to connect to other container management systems. \nWe are also thinking about some other features not in the roadmap, for example, multiple heapster instances for HA, one heapster instance for multiple K8S clusters or partial nodes of one K8S cluster (for multi-tenant scenarios). \ncc @alfred-huangjian @huangyuqi \n. ",
    "alfred-huangjian": "/cc\n. ",
    "huangyuqi": "@vishh , I am developing the kafka sink. Shall I commit a wip PR to accept some guys' comments? Thanks : )\n. thanks @piosz \nI will rebase #733 , and fix it.\n. I will update the PR #733 today\nBTW, @titilambert , in PR #1144 , you can cherry-pick my commit .\n. @piosz I have done the ES sink in #733, please review.\n. Yes,all nodes works normally, some pods works on these nodes. I have entered the container, heapster can visit the kubelet's rest api of all nodes and get the metrics information.\n. @NOX73 Would you check the date of all the nodes? If the node's date is not synchronous with the node where the  heapster resided, the heapster cannot collect the metrics normally.\n. this issue will not happen when i  ensure time synchronization between nodes.  So i close this issue.\n. Thanks. I will commit this PR today.\n. Hi, @vishh would  you please kindly check this PR for me? With the issue https://github.com/kubernetes/heapster/issues/535\n. @mwielgus Thanks very much. I expect your amendment for this doc. Thanks again.\n. @mwielgus Would you please merge this two PRs for me? : )\n. Sure, i have finished the squash, please check again. Thanks.\n. The CI is failed, I will close the pr first and reopen. : )\n. @mwielgus Thanks so much. : )\n. @vishh , thank you so much. Would you please kindly check this again? : )\n. @vishh , @mvdan thank you so much. \n. @vishh , thank you so much. \n. Sorry, @vishh , : ). I have found an existing issue about the kafka sink.: https://github.com/kubernetes/heapster/issues/172. So, i close this one. \n. Thanks , @vishh , would you please kindly review this PR? : ) \n. Thanks @mvdan for your meticulous comments. \n. @mvdan , thanks for your attentive comments.  In the four remaining comments: \n1. https://github.com/kubernetes/heapster/pull/648#discussion-diff-42236519R18 : i have dealed with goimports. ^-^\n2~3. https://github.com/kubernetes/heapster/pull/648#discussion-diff-42524964R59       https://github.com/kubernetes/heapster/pull/648#discussion-diff-42524964R59   i have removed the corresponding testcase, so it may not be hidden here. ^-^\n4.    https://github.com/kubernetes/heapster/pull/648#discussion-diff-42626989R124 I think the opt of query string is often an array, so i test it by len(). \n. @vishh : this PR is ready, I have fixed all the comments. : )\n. @vishh , Would you please review the PR of kafka sink for some modification about data structure and kafka's client? : )\nBTW, Our OPS subsystem uses the ES to collect the metrics and events,  so i realize a simple ES's sink. May I take another new PR about this ? \n. Thanks, @vishh :\nsink setup\n- Kafka is run as a cluster comprised of one or more servers each of which is called a broker; So we need to assign the broker list to the sink, with the query string brokers\n- Kafka maintains feeds of messages in categories called topics. So the topics should also be specified, with the query string timeseriestopic and eventstopic\n- In kafka sink, we use a client to connect these brokers, with the below config:\n``` go\ntype BrokerConf struct {\n    // Kafka client ID.\n    ClientID string\n// LeaderRetryLimit limits the number of connection attempts to a single\n// node before failing. Use LeaderRetryWait to control the wait time\n// between retries.\n// Defaults to 10.\nLeaderRetryLimit int\n\n// LeaderRetryWait sets a limit to the waiting time when trying to connect\n// to a single node after failure.\n// Defaults to 500ms.\n// Timeout on a connection is controlled by the DialTimeout setting.\nLeaderRetryWait time.Duration\n\n// AllowTopicCreation enables a last-ditch \"send produce request\" which\n// happens if we do not know about a topic. This enables topic creation\n// if your Kafka cluster is configured to allow it.\n// Defaults to False.\nAllowTopicCreation bool\n\n// Any new connection dial timeout.\n// Default is 10 seconds.\nDialTimeout time.Duration\n\n// DialRetryLimit limits the number of connection attempts to every node in\n// cluster before failing. Use DialRetryWait to control the wait time\n// between retries.\n// Defaults to 10.\nDialRetryLimit int\n\n// DialRetryWait sets a limit to the waiting time when trying to establish\n// broker connection to single node to fetch cluster metadata.\n// Defaults to 500ms.\nDialRetryWait time.Duration\n\n...\n}\n```\nIn this sink, I set the count of all retries to 1, and waiting time to zero, means that if the client can not connect the broker, it will  return immediately\n- We'll call processes that publish messages to a Kafka topic producers.So we need instantiate a producer base on the broker. And use the producer to send message to kafka server.\nElasticsearch\nAbout retention policies, ES support gateway( gateway module stores the cluster state and shard data), data persistence. Now , we use the gateway to save the data with local file system.\n. Hi, @rvrignaud , Elasticsearch is a distributed full text search engine, ES sink is able to save metrics and events; And we can use the RestAPI to search.\n. @vishh , I think something wrong about the sink's format:   ^_^\nyour format : --sink=kafka:?brokers=localhost:2181&timeseriestopic=test&eventstopic=test\nmine : --sink=kafka:\"http://?brokers=localhost:9092&timeseriestopic=test&eventstopic=test\"\ntwo issues, let me explain : )\n- symbol \"&\"\n\"&\" is used for support multi sinks, like this:\n--sink=influxdb:http://0.0.0.0:80&gcm:http://0.0.0.0:81\nso, the kafka sink's url needs quotation mark to avoid confuse. Maybe, we can modify the use of symbol \"&\" .\n- url format\nKafka sink gets the brokers list from the url's query string.So kafka sink's url need follow the format of URL.\nSo, i use --sink=kafka:\"http://?brokers=localhost:9092&timeseriestopic=test&eventstopic=test\", the sink works normally.  Do you have a better suggestion ?\n. ./heapster --logtostderr --source=cadvisor:external?standalone=true --sink=\"kafka:?brokers=localhost:9092&timeseriestopic=test&eventstopic=test\"\nIt can work normally.Enclosing the entire sink config inside  \"\" :+1:  ^^\n. @rvrignaud , i am very sorry for delay.  I am working on this, I will open a PR in recent two days. I have finished most of the sink code. ^^\n. Hi, @rvrignaud , so sorry for delay. I have finished the ES sink, would you please help me to review it? \nOr, if you have any suggestion, please tell me. ^_^\nhttps://github.com/kubernetes/heapster/pull/733\nBTW, Hi, @vishh , I found that I can not test any sink in standalone mode.\n. @vishh ,  thanks for your explanation. \n. thanks so much, @piosz   ^_^\n. @SidneyAn , \n\nI run heapster standalone in a minion node and I change the cadvisorPort to 4194\n\nIf you want to get the response of something  about kubernetes cluster, you need  config the source to kubernetes,like this: --source=kubernetes:http://9.91.18.135:8080 or --source=kubernetes:http://kubernetes(If you run the heapster in a POD)\n\nrun heapster standalone in a minion node\n\nIn this mode, you can just get the metrics from the cadvisor, and not perceive the resource about kubernetes, such as namespace, pod and so on.\n. @biswars the source of heapster you config is kubernetes, right?   And another api can work normally?\n. @biswars I got reason of this issue\n- the version of heapster in the deployment file you use is \"1.1.0\", it is a refactor version; And the model.md maybe not update.\n- So, i go to check the code of heapster api, i found the api /api/v1/model/namespaces/{namespace-name}/pods/{pod-name}/containers/ has been removed. Maybe a omission.  You can refer the file https://github.com/kubernetes/heapster/blob/master/metrics/api/v1/model_handlers.go#L49-L217\nThis issue is a bug certainly, in my opinion. Would you bring a new issue to community, copy this comment to there and at me? I am pleasure to fix this.\n. Hi, @vishh ,Is there something wrong about the CI? I got the error in checking process,\n```\nI1203 02:14:44.359848   15679 heapster_api_test.go:79] Failed to create ns: namespaces \"heapster-e2e-tests\" already exists\n--- FAIL: TestHeapster (51.98s)\n    assertions.go:154: \nLocation:   heapster_api_test.go:784\n\nError:      No error is expected but got namespaces \"heapster-e2e-tests\" already exists\n\n```\n. OK, I will try. Thanks @vishh \n. Hi, @vishh, I have rebased, but the checks were still not successful.\n. Hi, @vishh , how can I resolve the issue about the above checks? Thanks.\n. thanks @vishh \nHi, @mwielgus , how can I resolve the issue about the above checks? Thanks.\n. Hi, @vishh \nI have taken a rebase with master.would you please help to review ? Thanks.\n. @vishh @mwielgus I will fix issues of the CI checks right now.\nBTW, some guys are talking about where to get the events in issue https://github.com/kubernetes/kubernetes/issues/19637, would you please give some suggestions?\n. Hi, @biswars , I am on vacation this month , so sorry for delay. \nI will check this, because this PR is old, may be something changed with the base code.\n. Hi, @moserke \nES sink in this PR has been finished. Maybe some reviews is in process.\n@mwielgus \n. @piosz  It's ready, please give some review coments. Thanks so much\n. Hi, @titilambert \nNow, in new version of heapster, events and metrics are divided into two different process. So, the elasticsearch sink has double. But the content putting to sink are different.\n. @piosz thanks, i will pick out the common part. \n. Hi, @piosz  I have finished this ES sink, please give me some review comments. Thanks.\n. I got an error in CI:\n```\nok      k8s.io/heapster/events/manager  5.528s\n--- FAIL: TestAllExportsInTime (2.00s)\n    :31: \nLocation:   manager_test.go:55\n\nError:      Not equal: 3 (expected)\n                != 2 (actual)\n\nW0512 06:35:16.299389   29333 manager.go:108] Failed to events data to sink: s2\nW0512 06:35:19.309638   29333 manager.go:108] Failed to events data to sink: s2\nW0512 06:35:22.310790   29333 manager.go:108] Failed to events data to sink: s2\nW0512 06:35:22.310817   29333 manager.go:108] Failed to events data to sink: s1\nW0512 06:35:25.311504   29333 manager.go:108] Failed to events data to sink: s2\nW0512 06:35:25.311505   29333 manager.go:108] Failed to events data to sink: s1\nFAIL\nFAIL    k8s.io/heapster/events/sinks    15.047s\n```\n. @piosz , some issues still exist in CI\n```\n[0;33mWaiting for 2 ready nodes. 0 ready nodes, 0 registered. Retrying.\u001b[0m\n\u001b[0;33mWaiting for 2 ready nodes. 0 ready nodes, 0 registered. Retrying.\u001b[0m\n\u001b[0;31mDetected 0 ready nodes, found 0 nodes out of expected 2. Your cluster may not be working.\u001b[0m\nI0512 07:20:20.354669   22591 framework.go:285] Retry validation after 10ns seconds.\nI0512 07:20:30.354878   22591 framework.go:212] validating existing cluster\nI0512 07:20:30.354934   22591 framework.go:133] about to run &{/tmp/heapster_test/1.0.7/kubernetes/cluster/validate-cluster.sh [/tmp/heapster_test/1.0.7/kubernetes/cluster/validate-cluster.sh] []     []     false [] [] [] [] }\npanic: test timed out after 30m0s\ngoroutine 35 [running]:\ntesting.startAlarm.func1()\n    /usr/local/go/src/testing/testing.go:703 +0x132\ncreated by time.goFunc\n    /usr/local/go/src/time/sleep.go:129 +0x3a\n``\n. thanks so much @piosz \n. thanks so much @mwielgus \n. thanks, @piosz . Would you please help me to review another PR?https://github.com/kubernetes/heapster/pull/733   ^_^\n. thanks so much, @piosz ^_^\n. Hi, @FlavioF \n- Can you request the Heapster's RESTApi to get the metrics? If you can get any data, maybe there is something wrong about the influxDB sink? You can change the log level to get more details.You can try this RESTapi:http://***:8082/api/v1/metrics- AFAIK, Heapster has aging mechanism; So if you stopped a node, it will exist innode listof cache ,until heapsterrunGcto delete. Thus, you will get the error logkube_nodes.go:59] Failed to get container stats from Kubelet on node \"node-3-samantha.internal\"` until removing the node from cache.So i think we can ignore this error log in current scene.\n. Yes, @FlavioF , I have two nodes of kubernetes cluster,\n8.8.8.237\n8.8.8.239\nwhen I stop one node 8.8.8.239 , i can also get the same issue:\nE0106 23:57:50.001755   26308 kube_nodes.go:59] Failed to get container stats from Kubelet on node \"8.8.8.239\"\ni have debugged, and got this: In heapster node function List and getNodeInfoAndHostname, heapster doesn't check the node state.  refer the code: https://github.com/kubernetes/heapster/blob/master/sources/nodes/kube.go#L110-L121. It just checks with LookupIP\n```\n    if nodeInfo.InternalIP == \"\" {\n        if hostname == nodeInfo.PublicIP {\n                   ...\n        } else {\n            addrs, err := net.LookupIP(hostname)\n            if err == nil {\n                nodeInfo.InternalIP = addrs[0].String()\n            } else {\n                ...\n            }\n        }\n    }\n```\nSo, even though the node is not ready, the heapster's nodeList also contains it. And heapster would still get metric from this not ready node.\nIn my opinion this is a bug, I will take a commit add node's state checkingto fix.\n @vishh  @mwielgus\n. Hi, @FlavioF \nI have push a commit to fix this, please check https://github.com/kubernetes/heapster/pull/862, and debug in your enviroment. Thanks\n. Yes, I agree with you @vishh , enhancing the docs is a practicable way, i will add this tips. \nBut I think it's not a forcing method ^_^.\nMaybe, In my opinion, let the heapster to check in the init process is more better, if the timestamp is nonsynchronous, it will return a warning or an error to users. \nBTW, all these come from usability of heapster, it's assistant .So what's your opinion about add this to heapster? \n. Hi, @vishh \nFor issue https://github.com/kubernetes/heapster/issues/802, I want to add some tips in debugging.md first. Please help me to check, thanks .\n. related issue #802\n@vishh any suggestion will be very appreciated. Thanks.\n. Thanks, @mwielgus , i will explain the details about this PR.\nSomething has been mentioned in issue #802 .\n- As we know, the old data will be delete in heapster, if last update timestamp - time.now > cache_duration in normal scene. \n- So, if the node's timestamp is not synchronous (earlier than)with heapster , and duration between these nodes is bigger than cache_duration,  all the metrics data will be dropped.\n- The heapster's users often have something wrong unexpected about the nodes' synchronization, because of this, they can not get any metrics and events from heapster, and can not get any debug information from heapster.\n- So, this PR used for checking synchronization between nodes. And giving a warning, if any un synchronous occurs.\n. Thanks, @mwielgus ,\nNow, in all kubelet's apis, there is no response carrying timestamps. So, in this PR, i request the casvisor's original  RESTapi directly to get timestamps of nodes,like this:\nRequst:\nGET /api/v2.0/summary\nResponse:\n{\n    \"/\": {\n        \"timestamp\": \"2015-12-21T20:43:33.893425812Z\"\n        \"latest_usage\": {\n        ...\n        }\n        \"minute_usage\": {\n        ...\n        }\n        \"hour_usage\": {\n        ...\n        }\n        \"day_usage\": {\n        ...\n        }\n    }\n}\nIt can work and be identical with expectation.\n. Hi, @vishh \nI have pushed the PR about clock synchronization check. Would you mind to give some advise? Thanks ^_^\n. > Kubelet APIs are also supposed to contain timestamps. Each stats sample provided by cAdvisor contains a timestamp. Why not re-use that? \nthanks , @vishh . Actually, i have found that kubelet APIs contains timestamp.\nkubelet request&response\n```\nRequest: \ncurl -H \"Content-Type: application/json\" -X POST -d '{\"containerName\":\"kubelet\",\"subcontainers\":true}' http://localhost:10255/stats/container\nResponse:\n{\n\"/kubelet\": {\n    \"name\": \"/kubelet\"\n    \"spec\": {...}\n    \"stats\": [60]\n    {\n        0:  {\n        \"timestamp\": \"2015-12-29T14:23:36.628136848Z\"\n        ...\n}\n```\nBut, in my opinion, the response of cadvisor is more concise. And the body parse is more simple.\ncadvisor request&response\nRequst:\nGET /api/v2.0/summary\nResponse:\n{\n    \"/\": {\n        \"timestamp\": \"2015-12-21T20:43:33.893425812Z\"\n        \"latest_usage\": {\n        ...\n        }\n        \"minute_usage\": {\n        ...\n        }\n        \"hour_usage\": {\n        ...\n        }\n        \"day_usage\": {\n        ...\n        }\n    }\n}\nHowever, if you have some  other considerations, i can change this to kubelet api. \n\nAnd BTW, why are we extending heapster to detect time skew, instead of say running a daemon per cluster (or per node) that detects time skew? That daemon for example could mark a node to be NotReady if time skew is detected.\n- Up to now, AFAK, time skew just influences  working condition of heapster. So if time skew is detected, and we mark a node to be NotReady,  in worst case, the most of kubernetes's node will be down. \n- In some production environment, heapster will be deployed in many ways:\n  1. bare mental \n  2. in docker container at the master node\n  3. in kubernetes PODs\n\nSo, i think detecting time skew in heapster, and giving a warn information may be better than intruding into kubernetes cluster. \n. Hi, @mwielgus  I inadvertently found this heapster-scalability branch ,  is there a reconstruction plan of heapster in community? would you please share me some details about this. Thanks so much.\n. Thanks, @mwielgus , shall I join some Heapster refactoring contribution work? Or assign me some task about this? ^^\n. Thanks so much for letting me join. ^^\nMay you have the best Christmas ever. @mwielgus \nI will invest in the porting work today and wait for your pending tasks list. \n. This log shows that, heapster can not get the metrics from kubelet (Cadvisor). So, In my opinion, there may be two ways to check first,\n- You can request the URL http://10.0.0.12:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello manually, with command curl or wget to check stats of kubelet. This is the RESTapi of Cadvisor,.\n- Run docker ps to check the container exists or not.\n. Can you get the container after running docker ps ?\n. Because you got the error log failed to get stats from kubelet url: http://10.0.0.12:10255/stats/default/hello-world/e8c46f4f-a54b-11e5-85d5-fa163ea22a7a/hello - request failed - \"404 Not Found\", response: \"no matching container\\n\"\nSo ,i want to know the container hello exists or not in your kubernetes nodes. Would you check again? \nAFAIK, if containers are deleted by some reason(delete by RC, update, or crash), heapster's still holds the container information in its cache during the cache_duration, and you will also get this error information until heapster execute runGc to delete\n. Hi, @mwielgus \nPlease help to review, thanks so much.\n. Sorry, I am checking this issue. \n. sorry, @mwielgus , some misoperation occur in rebase. Please merge this PR now, thanks so much.\nBTW, would you please share me the task list of heapster reconsitution? ^^\n. thanks, @mwielgus , there is another PR about porting kafka sink, Would you please help me to check? https://github.com/kubernetes/heapster/pull/822 \n. thanks, @mwielgus , I will work on this interesting issues. \n. fixed #872 \n. please refer https://github.com/kubernetes/heapster/issues/802\n. @mwielgus , Please help me to review this, thanks so much. ^^\n. thanks @mwielgus ,\nBTW, Are there other tasks or proposals to let me join?  ^_^\n. thanks, @mwielgus ,\nIn recent two days, I have anatomized all codes of reconstituted version.So i think i am suitable for adding these.\n. Yes, i will.This instrumentation is more better. :+1: \n. yes, i agree.\nWhen a node becomes unavailable during scrape, heapster will have some other problem. Like this:\nE0107 04:32:55.001706    8037 kube_nodes.go:59] Failed to get container stats from Kubelet on node \"8.8.8.239\"\nBecause, the unnormal node still exists in cache, and heapster will get metrics as before.\nBut I think when the node is delete from cache, in runGc(). Heapster will work normally.\n. thanks, @piosz \n. Yes, I agree. Do we need add the runtime profiling data to sinks at the same time?\n. thanks so much, @jimmidyson ,  i will study this and take the exporting today.\n. @jimmidyson Please take a look when you get a chance. Thanks so much.\nAnd, I  don't understand the meaning of \" number of runs in the current houskeep interval\", so i leave this to another commit. Please @mwielgus help to explain for me, thanks.\n. I got it. Thanks @jimmidyson .\nLet me check the CI issue first.\n. Hi, @mwielgus Please help to check, thanks.\n. yes, i will move these together\n. Maybe, you can get more details in this doc:\nhttps://github.com/kubernetes/heapster/blob/master/docs/influxdb.md\n. Aha, I have got the same issue ,and this is random occurrence\n. @jamiehannaford thanks for reporting the issue. ES client has many config items, but in current community version ,we just allow user to config the endpoints secret indexname;   If you want add some other config items for ES client, contributing code for this sink is appreciate.Or contacting me to add this code.\nBTW, the interface of Init ES client contains the enable variable of \"health check\".\n. @piosz @jamiehannaford  I agree with this way of adding the configuration params. Thanks\n. LGTM.\nthanks @jamiehannaford \n. @piosz ,maybe i have no authority to add LGTM. Please add it for this PR. thanks\n. LGTM.\nThanks! @mkumatag \n. From v1.2.0-beta.0, the ES sink has been added to events. Any issues about this event ES sink? @douglasader \n. @piosz @AlmogBaku yes, I'd be happy to review this PR. Thanks.\n. LGTM\n. we can solve this issue with two simple ways mentioned in https://github.com/kubernetes/heapster/issues/1349\n. LGTM\n. @helinbo2015 Please sign GLA\n. The PR about Ganglia sink is appreciate, please push to the upstream. And then someone including me should take a review about this.\n. Thanks for this PR; \nBut, would you like to give more details about this bug-fix in PR commit?\nPlease include all scene, result, and reason relating to this issue . Thanks. \n. @adelbot Thanks for feedback!\nIn current version, we set the partition with a const value 0 for simplify.  It now seems not appropriate.\nThere is one simple way to solve it:\n-  Creating HashProducer with the default producer; \n-  HashProducer publishs messages to kafka, computing partition number from message key hash, using fnv hash and [0, numPartitions) range.\nRefer to https://godoc.org/github.com/optiopay/kafka#DistributingProducer\nI will take a pull request to fix this issue.\n. I will bring a fix PR with second way.\n. @piosz I agree.  there are duplicate contents between test-unit and test-unit-cov.\n. I will review this PR @piosz \n. LGTM, @k8s-bot ok to test\nThanks, @miaoyq , I have no doubt about this PR; . Hi, @DirectXMan12 I can maintain ES and Kafka Sink; . LGTM. OK, I will fix this.\n. Thanks for your comment. I have the same question, and change to use a restapi tool to test. I will fix this.\n. Thanks for your comment. I will fix this.\n. Hi, @vishh , adding two white space at the end, aims to place the sentence in a new line : ) \n. @mvdan ,thanks for your comments so much. The adding lines of Kafka sink are copied from the above sinks to keep pace with their format. So i think these lines are not one line, i will retain this. : )\n. OK , thanks @mvdan .\n. thanks @mvdan,  but I cant catch your meaning, can you tell me more clearly?  : )\n. thanks @mvdan,  but I cant catch your meaning, can you tell me more clearly?  : )\nthe function Produce is a stub for testcase. The return values are also stub values.\n. got it, thanks so much. : )\n. thanks, it's indeed thoughtless.I will remove it.\n. OK,i will remove, thanks.\n. i will remove. thanks : )\n. It's my bad coding habit. T_T.\nI will add a \"assert\" to deal with the err. Like this: \ngolang\nassert.NoError(t, err)\n. @mvdan , thanks for your attentive comments. Here, I think the opt of query string is often an array, so i test it by len().  Do you agree with me?  ^^\n. @vishh ,thanks for your comments. Retry here is used for connecting kafka brokers  for the first time, by importing an opensource kafka client. \nAnd your opinion is to check the link to kafka periodically, during the sink process, right?\n. Thanks for your comments.  @vishh .\nI need all the ops information, includes metrics and events. \nExposing an internal data structure TimeSeries over the wire is not well advised. I will pick out the effective information about metrics and events to give the kafka sink.Include: timestamp, value of metrics and events.\n. @vishh , Sorry for the delay. I have add two internal data structures for kafka sink. Would you please check again? Thanks  ^^\n. @vishh , I think that setting the retry time to 1 is fine, I have modified. If the kafka server is not ready, the sink will return immediately.  : ) \n. sorry, i  have not noticed that you added the default topics. \ngolang\nconst (\n    ...\n    timeSeriesTopic          = \"heapster-metrics\"\n    eventsTopic              = \"heapster-events\"\n)\n. nothing special about this image.\nwe can find a similar image in docker hub. \n. Yes, the yaml file supports  delimiter --- to define a block.\nSo we can put all configs in a single file, separate with ---.Like:\n```\napiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: kafka\n ....\n\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n``\n. thanks, @vishh , i will add a description for this package.\n. this is a typo error. i will remove. Thanks.\n. Is putting all config in a single file not clearness enough?\n. yes, maybe something wrong I have done during rebase. \n.startupHealthcheckTimeoutandmaxRetrieshave default value or not?\n. yes, maybe we can add a description in thesink-configuration.mdabout these two default value.\n. another question, : )\nthe packagegithub.com/olivere/elastic/we import is a third party lib we use.  If we modify the code here, and someone use thegodepto get the package from the origin location of github, the compile will fail.\nso, the better way i think is taking a PR togithub.com/olivere/elastic/to fix this. \n. typo errorfFailed==>Failed` \n. the option \"sniff\" may be useful in some scene. Why remove here?\n. please remove the blank line here.\n. change\ngolang\nstartupFns = append(startupFns, elastic.SetHttpClient(awsClient))\nstartupFns = append(startupFns, elastic.SetSniff(false))\nto\nstartupFns = append(startupFns, elastic.SetHttpClient(awsClient),elastic.SetSniff(false))\n. change glog.Info to glog.V(3).Info\n. unify the level of glog.Info to V2\n. How to deploy heapster, kubernetes or standalone?\nAnd if using kubernetes to deploy, how to inject this environment variable? \nWould you like to explain me the details of deployment?\nThanks so much.\n. Yes, i agree, warning is more precise. thanks\n. In my opinion, when deploying a pod in k8s, some authentication info can be read from serviceaccout (secret),  equal to read from some files. I don't think putting secret information in deploy yaml file by ENVIRONMENT is a good way.\n. we can refer to this link http://kubernetes.io/docs/user-guide/service-accounts/\nwith a  conventional process,  we can put these parameters like \"AWS_ACCESS_KEY_ID\" in secret, and use these by service-account.\n. This issue will not block this PR, so I think it is ready to be merged.\nLGTM.\n@piosz Please merge this. \n. change copyright to 2016\n. moving the init action to the common folder should be better, also including the init action in kafka sink of metrics;\nthe common folder is: https://github.com/kubernetes/heapster/tree/master/common\nthe init action in metrics kafka sink: https://github.com/kubernetes/heapster/blob/master/metrics/sinks/kafka/driver.go#L117-L170\n. moving this logic of initing action to common directory; https://github.com/kubernetes/heapster/tree/master/common\n. This is also a common logic between metric and events sink;. there is a issue about the partition.\nhttps://github.com/kubernetes/heapster/issues/1344\nIt should be fixed in this PR, in my opinion.. ",
    "jszczepkowski": "CC @jszczepkowski\n. @vishh @piosz\n. This PR will be taken over by piosz@, as I'm on vacation for the next two weeks.\n. CC @jszczepkowski\n. CC @jszczepkowski\n. CC @jszczepkowski\n. CC @vishh @fgrzadkowski @piosz \n. > Should we validate this be flag? @afein mentioned that the updates to model are dependent on model resolution.\nYou are right, it should be higher. I've added validation.\n. @vishh Can you order running integration tests for this PR? It seems that folks in Warsaw have no permissions to do it.\n. @vishh\nWhy the current frequency is so high? Can we do p.2 (reduce it to 65 seconds)?\n. CC @piosz @fgrzadkowski @mwielgus \n. The problem here is that we need 65 seconds model frequency for 1.1, while #528 will rather not be solved for 1.1. Model frequency of nearly 2.5 minutes is not acceptable from the horizontal pod autoscaler point of view.\n. CC @derekwaynecarr\n. @DirectXMan12 @piosz @fgrzadkowski \nWe have some serious concerns about proposed authentication of producers. The proposed authentication,  with names of users and certificates, doesn't seem to be aligned with Kubernets authentication. We are not sure it will be useful in general case for the majority of clients.\nThe simplest option seem to be authentication on namespace level: pods can only push metrics to their own namespace.  Most probably, we will need to use ServiceAccounts/Secrets to verify this.\n. I agree it may reasonable to keep different authentication schemes: system stats w/o namespace will use CA, while namespace scoped stats will use service account. Still, this topic needs some deeper investigation.\n. >  When you say namespace scoped stats do you mean application stats?\nYes, but not only. We may have stats pushed by a pod prefixed with pod name (application stats), and more general stats prefixed only with namespace.\n. > I think we need to clarify here: when you say \"prefixed with pod name/namespace\", do you mean metrics stored internally at the keys \"namespace:ns/pod:podname\" vs \"namespace:ns\", or do you mean metrics producers prefixing the metric names with the associated pod or namespace?\nI was thinking about the former (annotations in path): keys with \"namespace:ns/pod:podname\" and \"namespace:ns\".\n. It will be nice to extend the test and check, if all pods/nodes are correctly returned by master API, however, it can be addressed in another PR.\nLGTM.\n. LGTM\n. LGTM\n. LGTM\n. Push Metrics in heapster will be mainly used by HPA. Before we proceed with this PR, let's agree on HPA API for custom/push metrics, see https://github.com/kubernetes/kubernetes/pull/28628 for details.\n. LGTM. We need labels of our metrics to be in a format compatible with google cloud autoscaler. So, we need to have prefix \"compute.googleapis.com\" for them. I'm afraid we cannot change the labels later when pushing to GCM. Maybe @tkulczynski can comment on this.\n. We don't want to use regular cpu/usage metric, as it contains info from both machines and containers. We want our metric to contain info for machines only.\nI agree that annotating ContainerSpec with isNode is better idea than basing on resourceId to be not empty. We will apply it.\n. The same as above: memory/usage is polluted with container stats.\n. I agree, we should change it.\n. Please update the comment.\n. It seems not: https://github.com/kubernetes/heapster/issues/642#issuecomment-146972283.\n. Aren't we missing return here?\n. ",
    "jjw27": "subscribe.  @jjw27\n. ",
    "thucatebay": "This should be covered in https://github.com/kubernetes/heapster/pull/599, right?\n. @vishh Please let me know what else needs to be done here. Perhaps I can help.\n. The default dashboards are available at http://104.197.125.217/. \n. I signed it!\nI'm part of the \"ebay-kubernetes\" group. We collectively signed the CLA a while ago. The email that's part of that group is Thuc.Nguyen@ebay.com. \n. @vishh I dropped the Go code that would auto discover \"Load Balancer\" InfluxDB server. I'd appreciate another pass to review the latest changes, which now includes:\n- A couple of Grafana 2.1.0 dashboards.\n- run.sh which starts Grafana in the background, creates a datasource, imports a couple of dashboards, and brings Grafana into the foreground.\n. I signed it!\n. @mvdan rebased\n@vishh thanks for the comments. They should be addressed.\nDo you know who can help me with the CLA thing? I've confirmed on eBay's side that I'm all set. Thanks.\n. I signed it!\n. @vishh Thank you for reviewing, providing valuable comments, and merging.\n. kubernetes/heapster:v0.18.1 as specified in https://github.com/kubernetes/heapster/blob/master/deploy/kube-config/influxdb/heapster-controller.yaml doesn't work with InfluxDB 0.9.4. I had to build heapster from master branch and create my own Docker image with it. \n. Similar to https://github.com/kubernetes/heapster/pull/599, except that it's for 1.1 release, which is going out with InfluxDB v0.8.9. Most of the changes were to make the default dashboards work with this version of InfluxDB as v0.9 is quite different.\n@vishh please review. I'm not sure how we should version this docker image so that it doesn't collide with kubernetes/heapster_grafana:2.1.0, which is being used in master.\n. Here's how the dashboards look on GCE, http://130.211.174.79/. \n. Since Prometheus has alerting capability, it'd make for a good out-of-the-box experience. However, as a cluster grows in terms of nodes and pods, it becomes a non-trivial task to operate and scale a metric store such as Prometheus, InfluxDB, Graphite, etc. At eBay we have our own monitoring and alerting system, which we're planning to use for Kubernetes. Heapster fits well in this model since all we have to do is to write a sink. How about making Prometheus the default metric store instead of InfluxDB? \n. Which version of Kubernetes were you using? I believe @vishh made some changes to kube-proxy on the Kubernetes side to make Grafana work via the proxy URL.\n. From your list of pods, it looks like skydns  wasn't there. You'll need to enable the DNS add-on to access InfluxDB service via monitoring-influxdb.\n. @JunejaTung https://github.com/kubernetes/kubernetes/tree/release-1.1/cluster/addons/dns.\n. Here's the query that Grafana uses to fetch the list of nodes:\nselect distinct(hostname) from \\\"memory/limit_bytes_gauge\\\" where time > now() - 10m\nfor namespace:\nselect distinct(pod_namespace) from \\\"uptime_ms_cumulative\\\" where time > now() - 1m\nfor pod:\nselect distinct(pod_name) FROM \\\"uptime_ms_cumulative\\\" where pod_namespace =~ /$namespace/ and time > now() - 1m\nand container:\nselect distinct(container_name) from \\\"uptime_ms_cumulative\\\" where pod_name =~ /$pod/ and \\\"pod_namespace\\\" =~ /$namespace/ and time > now() - 1m\nYou can change the time filter to go back further and pick up more data. These are defined under \"Templating\" section of the dashboard settings menu.\nI'll look into whether it's possible to use the selected time filter instead of hardcoding.\n. I've also seen this from time to time where heapster isn't able to get metrics from a node (that is not ready, for example), and is just stuck there. A connection timeout is needed when heapster connects to kubelet.\n. It seems that Grafana wasn't able to get the list of containers from time to time. Any errors in Grafana log? \n. If you're just starting out on using Heapster, I'd recommend using the latest, which leverages InfluxDB 0.9.\nIn the version that you use, the query to get all containers looks like this:\nselect distinct(container_name) from \\\"uptime_ms_cumulative\\\" where pod_name =~ /$pod/ and \\\"pod_namespace\\\" =~ /$namespace/ and time > now() - 1m\nYou'll find it in the dashboard settings panel under the Templating section. You can change the time interval to, say 30m. \nI have a pending task to update it to use the time filter instead of hardcoding.\n. I built this image from the Dockerfile that is part of this PR. My suggestion is:\n- docker pull thuctnguyen/grafana:2.1.0\n- docker tag xyz kubernetes/grafana:2.1.0\n- docker push kubernetes/grafana:2.1.0\nI can then test with \"kubernetes/grafana:2.1.0\" and update the PR.\n. I built this image from HEAD. I think it'd be a good idea to have kubernetes/heapster:latest, which gets built automatically from HEAD. \n. Sure. I'll do that.\n. This is to support the \"direct\" access use case where InfluxDB service is exposed as \"Load Balancer\" type. Since Grafana continues to support this method as mentioned on http://docs.grafana.org/datasources/influxdb/, I thought it would be useful to have that support here as well.\n. Yes, the intention was to go through the InfluxDB service, instead of hitting specific pods. The term \"endpoint\" is probably causing some confusion here. I'll rename it to \"URL\" instead.\nLookup order for InfluxDB service URL:\n1. If it's provided in the spec via an environment variable, use it.\n2. Call into \"influxdb_service_discovery\" to check if InfluxDB is exposed as an external service.\n3. Fall back to \"http://monitoring-influxdb\".\n. @vishh you asked a question about why this was needed. Grafana supports both proxy and direct methods to access InfluxDB backend. This is mainly for deployments where InfluxDB is exposed as an external service and can be accessed directly from the browser. This code looks up the external service URL for InfluxDB and returns it if one is available.\n. @vishh sounds good. I'll go ahead and remove it to simplify things a bit. We can add it back if needed.\n. My suggestion is to keep the Grafana version as 2.1.0 and tag on the heapster version. So for 1.1 release it would be 2.1.0-v4 and master 2.1.0-v5. \nWhat do you think?\n. ",
    "simon3z": "@vishh note that you need a new kubernetes/heapster_grafana image: v0.6\nI have some other changes in mind, for example, what do you think about the possibility to revamp the shell script (in possibly just a single one)?\nIf you're ok with that you can hold this PR and I'll go ahead with larger changes tomorrow (so I'll spare you to build the docker image twice).\n. Oops previous question was for @rjnagal  (but for @vishh as well eventually)\n. @vishh @rjnagal I am little confused by the fact that these scripts are available also in kubernetes here:\nhttps://github.com/GoogleCloudPlatform/kubernetes/tree/master/cluster/addons/cluster-monitoring\nMaintaining them in two places is painful, do you think we can centralized them here?\n. Closing this because I moved to https://github.com/GoogleCloudPlatform/heapster/pull/187\n. @vishh @rjnagal this is urgent as it's related to https://github.com/GoogleCloudPlatform/kubernetes/issues/5850\n. /cc @vishh @jimmidyson \nIf we have an agreement on using apache + its reverse proxy I can go ahead and complete that as well.\n. @vishh idea idea why it fails?\n. @vishh @jimmidyson right now stats are per-container, any idea on how to represent these per-pod network metrics? I think you mentioned the problem here above but I didn't understand the conclusion.\nYou mentioned the pod infra container, are you thinking to expose/collect them (with their network metrics) and then leave to external applications to do the grouping? (It should properly reference the pod uid, etc.).\n. @jimmidyson with regard to openshift (openshift-metrics) when is expected the new API to be finalized? Is it for 3.2? We expected to have network metrics in 3.1.z. cc @smarterclayton to keep me honest on the expectancy.\n. @mwringe thanks! It may be worth doing that in conjunction with exposing the metric_resolution option in the deployer variables (if it's not too much work).\n. @mwringe earlier I filed an issue here https://github.com/openshift/origin-metrics/issues/101 for the option in the deployer.\n. @mwringe should this be closed?\n. ",
    "fejta-bot": "Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nFreeze the issue for 90d with /lifecycle frozen.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nPrevent issues from auto-closing with an /lifecycle frozen comment.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or @fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten\n/remove-lifecycle stale. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Rotten issues close after 30d of inactivity.\nReopen the issue with /reopen.\nMark the issue as fresh with /remove-lifecycle rotten.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/close. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. Stale issues rot after 30d of inactivity.\nMark the issue as fresh with /remove-lifecycle rotten.\nRotten issues close after an additional 30d of inactivity.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle rotten. Issues go stale after 90d of inactivity.\nMark the issue as fresh with /remove-lifecycle stale.\nStale issues rot after an additional 30d of inactivity and eventually close.\nIf this issue is safe to close now please do so with /close.\nSend feedback to sig-testing, kubernetes/test-infra and/or fejta.\n/lifecycle stale. ",
    "x13n": "/remove-lifecycle stale\n/unassign @mwielgus . /remove-lifecycle stale\n/lifecycle frozen\n/help. /close. pod_nanny for heapster at the moment exists in the same pod as heapster itself, so if it is forbidden from modifying this pod requirements/limits, it won't work. However, ClusterRole is set only in yaml example from heapster repo:\nhttps://github.com/kubernetes/heapster/blob/master/deploy/kube-config/influxdb/heapster.yaml\nbut is not set in main kubernetes repo, which is used when setting up a cluster:\nhttps://github.com/kubernetes/kubernetes/blob/master/cluster/addons/cluster-monitoring/influxdb/heapster-controller.yaml\nRight now the problem will show up only when both pod_nanny and RBAC are used for Heapster at the same time. So, short term, giving those permissions looks like the way to go. Long term, as @piosz wrote, we need kubernetes/kubernetes#46763.. /remove-lifecycle rotten\nhttps://github.com/kubernetes/heapster/pull/1612 introduced a ClusterRoleBinding for heapster, but nothing for pod_nanny. Reusing https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/cluster-monitoring/heapster-rbac.yaml will be enough here.\n/cc @luxas who made the original change.. I need more context here. What jobs, where is this configured?. With https://github.com/kubernetes/test-infra/pull/6359 merged, I guess this is done? Please reopen if not.. /retest. /retest. FWIW, when I was trying to bump k8s version in Heapster tests, this is what I found in Heapster logs:\nE0115 13:51:27.283122 1 reflector.go:190] k8s.io/heapster/metrics/util/util.go:30: Failed to list v1.Node: nodes is forbidden: User \"system:serviceaccount:heapster-e2e-tests:default\" cannot list nodes at the cluster scope\nE0115 13:51:27.283299 1 reflector.go:190] k8s.io/heapster/metrics/heapster.go:328: Failed to list v1.Pod: pods is forbidden: User \"system:serviceaccount:heapster-e2e-tests:default\" cannot list pods at the cluster scope\nE0115 13:51:27.283492 1 reflector.go:190] k8s.io/heapster/metrics/processors/namespace_based_enricher.go:89: Failed to list *v1.Namespace: namespaces is forbidden: User \"system:serviceaccount:heapster-e2e-tests:default\" cannot list namespaces at the cluster scope\nLooks like Heapster running in a separate namespace can no longer list pods or nodes without being explicitly granted those rights.. Yup, that's my guess.. Yay! Looks good to me, deferring to others who previously had comments to apply lgtm label.. I'm against bumping the version again - there is already too much happening in this PR.. Looks like there is some connectivity, so I'd try checking influxdb logs to see why it returned this error.. This doesn't look to be a problem with Heapster, so closing. Please use slack/stackoverflow for this type of questions.\nThat being said, your influxdb probably doesn't expose any external IP which is why you cannot reach it. Grafana runs in the same cluster, so internal IP is enough there.. /lgtm\n/retest. /test pull-heapster-e2e-prow. @BenTheElder The new prow job is failing, but I cannot tell how or why - the \"Details\" link doesn't show anything particularly useful. Is there any other place I could check to see what failed? . Looks like it's actually not getting heapster directory from anywhere. @krzyzacy Where did you find this error?. Apparently so, this was last bumped by https://github.com/kubernetes/heapster/pull/1692 I guess this should be using 1.9 now? Anyway, the test is failing now on missing kube config, which is a different issue.. Geez, those outputs are confusing. You're right, this is a different issue. Looks like it was fixed by https://github.com/kubernetes/kubernetes/pull/51471, but by using 1.7.0-beta2 we're staying broken. I'll bump the version and see what happens.. PR with version bump: https://github.com/kubernetes/heapster/pull/1927. I don't think we should be tying ourselves to old versions, testing against Kubernetes 1.9. seems to be the right thing to do.. I think this is more of a HPA question, not a heapster question. I guess @MaciekPytel should know.. @aleksandra-malinowska @piosz Looks like we cannot migrate Heapster integration tests because metrics are missing from /api/v1/metric-export. This is probably caused by \nhttps://github.com/kubernetes/heapster/pull/1580 Can you advise on how to fix the tests now?. This is how tests are fetching metric time series, see here: https://github.com/kubernetes/heapster/blob/master/integration/heapster_api_test.go#L249:6. Ok, I ran the tests manually. It turns out the problem lies elsewhere, heapster logs are flooded with this:\nE0115 13:51:27.283122       1 reflector.go:190] k8s.io/heapster/metrics/util/util.go:30: Failed to list v1.Node: nodes is forbidden: User \"system:serviceaccount:heapster-e2e-tests:default\" cannot list nodes at the cluster scope\nE0115 13:51:27.283299       1 reflector.go:190] k8s.io/heapster/metrics/heapster.go:328: Failed to list v1.Pod: pods is forbidden: User \"system:serviceaccount:heapster-e2e-tests:default\" cannot list pods at the cluster scope\nE0115 13:51:27.283492       1 reflector.go:190] k8s.io/heapster/metrics/processors/namespace_based_enricher.go:89: Failed to list v1.Namespace: namespaces is forbidden: User \"system:serviceaccount:heapster-e2e-tests:default\" cannot list namespaces at the cluster scope. I just realized that there is https://github.com/kubernetes/heapster/pull/1849, so closing this in favor of the other PR.. Looks like a networking problem, so ccing @kubernetes/sig-network-bugs . Why?. Thanks for giving the context, makes sense.\n/lgtm. /retest\n. /lgtm. /lgtm. /lgtm. Is there a github issue for that bug?. Nope, I just wanted to avoid not linking one that exists.\n/lgtm. /lgtm. /lgtm. /lgtm. I didn't get to the root cause, unfortunately. My only finding was that in the request sent by Heapster, the container name was sometimes empty. At the same time, no such thing could be observer in the summary API. My guess was that there is a bug in Heapster causing it to sometimes lose container names, which in turn causes Stackdriver to think there are duplicates (if there was more than one container without a name set). I don't know if this is what is happening here and I don't know whether this is the only problem.. I'm pretty sure that was cpu or memory usage - do we expose them as pod metrics in addition to exposing them as container metrics?. /lgtm. /test pull-heapster-unit. Looks like a duplicate of https://github.com/kubernetes/heapster/issues/2059. /ok-to-test\n/lgtm. Heapster is deprecated and doesn't accept new features. For reference, see https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md. Whoops, you're right, I don't think we deprecated eventer as well.\n/reopen\n/ok-to-test. @DirectXMan12 @kawych What's the deprecation story for eventer? I guess we were going to deprecate it too, given that the whole repo is planned to be retired in 1.13 timeline, but what should people use instead? . /ok-to-test\n/lgtm. There is no such syntax. There is glog.V(2).Infof, however. Quoting from glog doc, those two calls are (almost) equivalent:\nif glog.V(2) { glog.Info(\"log this\") }\n\nglog.V(2).Info(\"log this\")\n\nThe only difference is, when V(2) is false, the expression inside won't get evaluated. I'm using the former, since instead of simply logging a string I'm also lazy-evaluating conversion to json.. Added.. Ack, changed.. Substituting this in old release notes doesn't seem to be the right thing to do.. ditto. Should be > 0. Or >= 1. But definitely not >= 0 :-)\nAlso, please add a warning here (and above, for cluster name) if the key is missing from opts.\n  . /cc @MaciekPytel @kawych to address the HPA question.. ",
    "k8s-ci-robot": "@x13n: \nThis request has been marked as needing help from a contributor.\nPlease ensure the request meets the requirements listed here.\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the /remove-help command.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/218):\n\n>/remove-lifecycle stale\n>/lifecycle frozen\n>/help\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Jenkins GCE e2e [**failed**](https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/heapster/1178/pull-heapster-e2e/11/) for commit 37c3c9a19e3a9f7ff6b0abce307767953c5acbe6. [Full PR test history](http://pr-test.k8s.io/1178).\n\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n. Jenkins GCE e2e failed for commit 8439e999b70210d26554a5d50698a50033922043. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @AlmogBaku: you can't request testing unless you are a [kubernetes](https://github.com/kubernetes/people) member.\n\n\nIn response to [this comment](https://github.com/kubernetes/heapster/pull/1313#issuecomment-260325392):\n\n> @k8s-bot test this\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Jenkins GCE e2e failed for commit d8894f342ecb4be1ad928bd792ef27d5db0113be. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.. Jenkins GCE e2e failed for commit a9318baf8da6966d8e55e8015cad3b168e724e8b. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line.\nRegular contributors should join the org to skip this step.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line.\nRegular contributors should join the org to skip this step.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line.\nRegular contributors should join the org to skip this step.\n. Jenkins GCE e2e failed for commit be68fee28b07488fab9f699c10d08bd4350ed430. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line.\nRegular contributors should join the org to skip this step.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line.\nRegular contributors should join the org to skip this step.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line.\nRegular contributors should join the org to skip this step.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line.\nRegular contributors should join the org to skip this step.\n. Jenkins GCE e2e failed for commit b0819bdcaaf35b77256ec220b078ea5cf5155d4c. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line.\nRegular contributors should join the org to skip this step.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line.\nRegular contributors should join the org to skip this step.\n. Jenkins GCE e2e failed for commit 04895e3a2ba790910ac0a81a9493343d17d0d723. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n. Jenkins GCE e2e failed for commit f017f92d55635e983dad4a5f3417e56e2cc7b176. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line.\nRegular contributors should join the org to skip this step.\n. @shmish111: you can't request testing unless you are a kubernetes member.\n\nIn response to [this comment](https://github.com/kubernetes/heapster/pull/1339#issuecomment-261715541):\n\n> @k8s-bot test this\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Jenkins GCE e2e [**failed**](https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/heapster/1339/pull-heapster-e2e/102/) for commit 47107c6fa484eeab163b2e6086bd1b3ce875fd93. [Full PR test history](http://pr-test.k8s.io/1339).\n\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line.\nRegular contributors should join the org to skip this step.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line.\nRegular contributors should join the org to skip this step.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line.\nRegular contributors should join the org to skip this step.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line.\nRegular contributors should join the org to skip this step.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the kubernetes/test-infra repository.\n. Jenkins GCE e2e failed for commit 2e2ce208efcd3c25cf8b67888148c9f8bd370bc3. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the kubernetes/test-infra repository.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the kubernetes/test-infra repository.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the kubernetes/test-infra repository.\n. Jenkins GCE e2e failed for commit 97380d9b515b6e5404a7b93823673bc47a7757c3. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the kubernetes/test-infra repository.\n. Jenkins GCE e2e failed for commit b0889036831e987c3f3970f9d81042097ab67044. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the kubernetes/test-infra repository.\n. @andyxning: you can't request testing unless you are a kubernetes member.\n\n\nIn response to [this comment](https://github.com/kubernetes/heapster/pull/1368#issuecomment-262633708):\n\n>@k8s-bot test this\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Jenkins GCE e2e [**failed**](https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/heapster/1368/pull-heapster-e2e/59/) for commit 06c1aa163cfcfe2bccfa18b70ac45577d278fd1a. [Full PR test history](http://pr-test.k8s.io/1368).\n\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the kubernetes/test-infra repository.\n. Jenkins GCE e2e failed for commit 29b79e84225c91d38e8caa4980e1c8080979e3e9. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the kubernetes/test-infra repository.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the kubernetes/test-infra repository.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Jenkins GCE e2e failed for commit ccb048b18aaa7e27affb7db4a0022df596b05e5b. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Can a [kubernetes](https://github.com/orgs/kubernetes/people) member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @keithballdotnet: you can't request testing unless you are a kubernetes member.\n\nIn response to [this comment](https://github.com/kubernetes/heapster/pull/1386#issuecomment-261189137):\n\n> @k8s-bot ok to test\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Jenkins GCE e2e failed for commit 628246545ba8275ed254754b22fd1ff5aafbc90c. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.. Jenkins GCE e2e failed for commit 20e35c603090377caab0543ecf026d0c8e92bde2. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n\n\nIf you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Jenkins GCE e2e failed for commit 24aabebb6716606d4365fa31f109d9622cef20a2. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.. \nCan a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n.\n\nCan a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @thaume: you can't request testing unless you are a kubernetes member.\n\n\nIn response to [this comment](https://github.com/kubernetes/heapster/pull/1399#issuecomment-264394230):\n\n>@k8s-bot ok to test\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. \nCan a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n.\n\nCan a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @zuomang. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @AlmogBaku. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @AlmogBaku. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @bavarianbidi. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Jenkins GCE e2e [**failed**](https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/heapster/1411/pull-heapster-e2e/106/) for commit b5a009d1780b9c6a37f8f6cc7e6b261d70c86d4b. [Full PR test history](http://pr-test.k8s.io/1411).\n\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @galexrt. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @jackzampolin. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @mpvaniersel. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @gadaigadai. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @seeekr. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @andyxning. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @wxb0521. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @wxb0521. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Jenkins GCE e2e failed for commit 80720912be610966ef1c76d7b33c9435d7511fe1. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @wxb0521. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @dongziming. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @rikatz. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @chancez. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nIf you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @rikatz. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @rikatz. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @gadaigadai. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Jenkins GCE e2e failed for commit ce85bf6fdf4ef0c364a96eed10a4829e4267de8d. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n\n\nIf you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @rikatz. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Jenkins GCE e2e failed for commit 670c04de35c1b20824af8a3714e0db2d5b7fa3ad. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @miaoyq. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @jgoclawski. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @CallMeFoxie. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @rikatz. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @fate-grand-order. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @fate-grand-order. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @mcorbin. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Jenkins GCE e2e [**failed**](https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/heapster/1473/pull-heapster-e2e/113/) for commit b8c938e6099c35d1b83bdec8c1260ac45aaa485b. [Full PR test history](http://pr-test.k8s.io/heapster/1473). cc @mcorbin\n\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @fate-grand-order. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. @DirectXMan12: The following test(s) failed:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\nJenkins GCE e2e | 333264429c55103e31d47ffb98e6c0f43e19a4a2 | link | @k8s-bot test this\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n test report \n. @DirectXMan12: The following test(s) failed:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\nJenkins GCE e2e | 333264429c55103e31d47ffb98e6c0f43e19a4a2 | link | @k8s-bot test this\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n test report \n. Hi @whereisaaron. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @tianshapjq. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. @AlmogBaku: The following test(s) failed:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\nJenkins GCE e2e | f6aa0b143479687e17220a7d73633bc7a36768e5 | link | @k8s-bot test this\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n test report \n. Hi @rarneson. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @mortensteenrasmussen. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @neith00. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. @krzyzacy: The following test(s) failed:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\nJenkins GCE e2e | f2c2907f9edee86bb1264ac747bec85c00d914d0 | link | @k8s-bot test this\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n test report \n. @krzyzacy: you can't re-open an issue/PR unless you authored it or you are assigned to it.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/1501#issuecomment-375808544):\n\n>/reopen\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @luxas: The following test(s) failed:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\nJenkins GCE e2e | 48ded2e8abd705b0cd2bd917aa4c419dfe79d005 | link | @k8s-bot test this\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n test report \n. Hi @jonaz. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. @krzyzacy: The following test(s) **failed**:\n\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\nJenkins GCE e2e | ba627edd0cb2983b05c5b522fd353c770a1fea16 | link | @k8s-bot test this\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n test report \n. Hi @tianshapjq. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @tianshapjq. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @jorge07. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @tianshapjq. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @ChrisBuchholz. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @ezeev. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @fate-grand-order. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @astropuffin. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @samjeyam. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @tianshapjq. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. @tianshapjq: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-e2e | 05e8d16d5a07e881653c3e7aab087552e8553904 | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Hi @nicolaiskogheim. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @johanneswuerbach. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. @AlmogBaku: you can't LGTM a PR unless you are an assignee.\n\n\n\nIn response to [this comment](https://github.com/kubernetes/heapster/pull/1540#issuecomment-281823827):\n\n>/lgtm\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @stresler. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @vjdhama. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @tianshapjq. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @tianshapjq. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. @andyxning: you can't LGTM a PR unless you are an assignee.\n\n\nIn response to [this comment](https://github.com/kubernetes/heapster/pull/1558#issuecomment-284728370):\n\n>@tianshapjq Thanks!\n>\n>/lgtm\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. @tianshapjq: The following test(s) failed:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\nJenkins GCE e2e | fd71e7348795fe76b70913a160654541daee5ba3 | link | @k8s-bot test this\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n test report \n. Hi @bruceauyeung. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. @andyxning: you can't LGTM a PR unless you are an assignee.\n\n\nIn response to [this comment](https://github.com/kubernetes/heapster/pull/1559#issuecomment-284731257):\n\n>@bruceauyeung Thanks for your contribution!\n>\n>/lgtm\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. @bruceauyeung: The following test(s) failed:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\nJenkins GCE e2e | 403801937e3af9bb7ab65ce0868db257016e67b2 | link | @k8s-bot test this\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n test report \n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @mcorbin. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @vivkong. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @foxyriver. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @bruceauyeung. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @dieterdemeyer. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @jakon89. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @tianshapjq. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @bruceauyeung. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @krzyzacy: The following test(s) failed:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\nJenkins GCE e2e | 9d338cf3c435c6d2d23492a74990b1a51d465d8e | link | @k8s-bot test this\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n test report \n. Hi @jakon89. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @cainelli. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @woodlee: you can't re-open an issue/PR unless you authored it or you are assigned to it.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/1579#issuecomment-370487717):\n\n>/reopen\n>/remove-lifecycle-rotten\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nOnce you've signed, please reply here (e.g. \"I signed it!\") and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @yannrouillard. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @caarlos0. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @tianshapjq. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @bruceauyeung. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @KarolKraskiewicz. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @mcorbin. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @mcorbin. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @kartoch. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @AlmogBaku: The following test(s) failed:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\nJenkins GCE e2e | a359af5300a0e05442eeab412f373773a60bf88f | link | @k8s-bot test this\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Hi @adambkaplan. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @watercraft. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @watercraft. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @jkinkead. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @watercraft. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @YuPengZTE. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @caarlos0. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @zioproto: you can't re-open an issue/PR unless you authored it or you are assigned to it.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/1648#issuecomment-385394953):\n\n>/reopen\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @jeffery9. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @georgebuckerfield. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @loburm. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @georgebuckerfield. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @loburm. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @yaacov. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @mmaquevice. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @hoangphuocbk. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @astef. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @vnandha. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @Kokan. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @marcinkubica. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @doubaokun. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @frankruizhi. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @frankruizhi. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @voyalab. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @matlockx. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @chancez: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-e2e | 1bcec91c20d0db07c5e34c1dd3f45b0f987b053d | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @kawych: The following test **failed**, say `/retest` to rerun them all:\n\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-e2e | cbc2b02935a5f623157532ed9e8c0b58dce9b793 | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. @piosz: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-e2e | a1df614c701d3046a6b3913c2168d81569d1f7b7 | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @4admin2root. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @basilisk487. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @Kokan. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @mtanski. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @fate-grand-order. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @dnavre. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @dnavre: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-e2e | 7ee0bedad5f86b9c13f06478c7e6da8ad4b31b56 | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Hi @allencloud. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @allencloud. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @allencloud. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @gdecroux. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @cieni. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @davidstack. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @allencloud. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @allencloud. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @ipeacocks. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @bartebor. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @asifdxtreme. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @fisherxu. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @007. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @007. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @systemizer. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @mkumatag. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @rohitsardesai83. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @mbssaiakhil. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @mbssaiakhil. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @miry. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @miry. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @Rajadeepan. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @007. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @allencloud. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @miry. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @andyxning: GitHub didn't allow me to request PR reviews from the following users: ckleban, CBR09.\nNote that only kubernetes members can review this PR, and authors cannot review their own PRs.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/pull/1788):\n\n>Fix #1058 \n>\n>This PR will fix the network metric values when there are multiple interface available for a container. Currently, heapster will use the first one(index 0) interface stats as the value for network metrics. This will be wrong when there are more than one network interface available for a container.\n>\n>What we should do is to summarize all the network interface values instead of only picking up only one.\n>\n>/cc @DirectXMan12 @piosz @ckleban @CBR09 @xiangpengzhao \n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @juanluisvaladas. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @juanluisvaladas. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @juanluisvaladas: you can't request testing unless you are a kubernetes member.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/pull/1792#issuecomment-326086572):\n\n>/ok-to-test\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @miry. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @mikebryant. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @yogeswaran. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @heckj. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @nlebas. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @loburm: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-e2e | 8db6dd919563be270b32b1cea6494579543c1a7f | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @atzannes. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @mikebryant. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @andyxning: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-e2e | 7681f26fd2d159d40564b6329c41825d696716f0 | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. [APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: andyxning, loburm\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning,loburm]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. Hi @atzannes. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @atzannes. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @TinySong. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @ldavis-ca. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @4406arthur. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @Kokan. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @kawych: The following test **failed**, say `/retest` to rerun them all:\n\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-e2e | 466e73b7696da78fdc83a9b7c6e5768d5c07478c | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @DanielMSchmidt: The following test **failed**, say `/retest` to rerun them all:\n\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-e2e | 37e5a138bd25c97306599bc89c645310d43f73c9 | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: Please follow instructions at https://github.com/kubernetes/kubernetes/wiki/CLA-FAQ to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @x13n: The following test **failed**, say `/retest` to rerun them all:\n\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-e2e | b1ee88c3232b1a6ce62dd63d5a7ee84c6e8ef7db | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. @krzyzacy: The following tests failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-e2e | 61648706c05379d53e27669e47ad2df6e176e9f4 | link | /test pull-heapster-e2e\npull-heapster-e2e-prow | 61648706c05379d53e27669e47ad2df6e176e9f4 | link | /test pull-heapster-e2e-prow\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. [APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: andyxning, DirectXMan12, loburm\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [DirectXMan12,andyxning,loburm]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. Hi @JooyoungJeong. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @srm09. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. Hi @ryarnyah. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. Hi @qhyou11. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @oilbeater: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-e2e | 73377cc66b77dd385bc41bf1c198eca728d721f6 | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Hi @qhyou11. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. Hi @ryarnyah. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @Kokan: changing LGTM is restricted to assignees, and only kubernetes org members may be assigned issues.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/pull/1950#issuecomment-363904684):\n\n>/lgtm\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: ryarnyah\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: piosz\nAssign the PR to them by writing /assign @piosz in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"piosz\"]} \n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. Hi @aduston. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. Hi @aduston. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. @acobaugh: The following test **failed**, say `/retest` to rerun them all:\n\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-e2e | f8f38c19e2469541615ba83004051b29fdaf8ce4 | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. [APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: acobaugh, andyxning, DirectXMan12, pcm32\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[grafana/OWNERS](https://github.com/kubernetes/heapster/blob/master/grafana/OWNERS)~~ [DirectXMan12,andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: acobaugh, kawych\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[grafana/OWNERS](https://github.com/kubernetes/heapster/blob/master/grafana/OWNERS)~~ [kawych]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. @DirectXMan12: you cannot LGTM your own PR.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/pull/1965#issuecomment-368047853):\n\n>/lgtm\n>/approve\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: DirectXMan12\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [DirectXMan12]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: javier-b-perez\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: huangyuqi\nAssign the PR to them by writing /assign @huangyuqi in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"huangyuqi\"]} \n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: jingxu97\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: x13n\nAssign the PR to them by writing /assign @x13n in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"x13n\"]} \n. @jingxu97: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-e2e | 0fecd6789febe1d7cf2135d66a745398cb3db664 | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. [APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: andyxning, stash1001\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: andyxning, Kokan\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: AdamDang, andyxning\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: kawych, x13n\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [kawych,x13n]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: kawych, x13n\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [kawych,x13n]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: kawych, x13n\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [kawych,x13n]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: kawych, x13n\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [kawych,x13n]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. @fejta-bot: Closing this issue.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/1980#issuecomment-429644336):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: kawych, loburm, serathius\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[metrics/sinks/stackdriver/OWNERS](https://github.com/kubernetes/heapster/blob/master/metrics/sinks/stackdriver/OWNERS)~~ [kawych,loburm]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: AdamDang, andyxning\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: kawych, serathius\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[metrics/sinks/stackdriver/OWNERS](https://github.com/kubernetes/heapster/blob/master/metrics/sinks/stackdriver/OWNERS)~~ [kawych]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: AdamDang, crassirostris\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [crassirostris]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. [APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: andyxning, loburm\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning,loburm]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. @fejta-bot: Closing this issue.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/1990#issuecomment-435251896):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: 007, andyxning\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: jiayingz, kawych\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [kawych]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: DirectXMan12, krzyzacy\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [DirectXMan12]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: hzxuzhonghu\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: andyxning\nAssign the PR to them by writing /assign @andyxning in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"andyxning\"]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: andyxning, jingxu97\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. @fejta-bot: Closing this issue.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/2000#issuecomment-435242203):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. [APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: jcharlytown, kawych\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [kawych]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. @fejta-bot: Closing this issue.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/2005#issuecomment-420660946):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @fejta-bot: Closing this issue.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/2006#issuecomment-429438143):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: andyxning, thockin\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: andyxning, kawych\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning,kawych]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: andyxning, kawych\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning,kawych]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: crassirostris, x13n\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [crassirostris,x13n]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: kawych, sanketjpatel\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [kawych]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. @fejta-bot: Closing this issue.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/2013#issuecomment-437581695):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: AdamDang\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: x13n\nAssign the PR to them by writing /assign @x13n in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"x13n\"]} \n. Hi @AdamDang. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @fejta-bot: Closing this issue.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/2017#issuecomment-429639676):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @lister2010. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. [APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: lister2010\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: x13n\nAssign the PR to them by writing /assign @x13n in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"x13n\"]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: AdamDang, andyxning\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. [APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: bruksnys, kawych\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [kawych]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. @fejta-bot: Closing this PR.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/pull/2021#issuecomment-435661638):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: DirectXMan12\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [DirectXMan12]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: AdamDang, andyxning\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. @fejta-bot: Closing this issue.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/2025#issuecomment-435242206):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: AdamDang, andyxning\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: kawych, serathius\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[metrics/sinks/stackdriver/OWNERS](https://github.com/kubernetes/heapster/blob/master/metrics/sinks/stackdriver/OWNERS)~~ [kawych]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: kawych, serathius\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [kawych]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. @AdamDang: GitHub didn't allow me to assign the following users: directxman1.\nNote that only kubernetes members and repo collaborators can be assigned.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/pull/2030#issuecomment-385234802):\n\n>/assign @directxman1\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: AdamDang, andyxning\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: zioproto\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: kawych\nAssign the PR to them by writing /assign @kawych in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"kawych\"]} \n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: zioproto\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: huangyuqi\nAssign the PR to them by writing /assign @huangyuqi in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"huangyuqi\"]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: kawych, serathius\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [kawych]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: andyxning, zioproto\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: AdamDang, andyxning\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. @fejta-bot: Closing this issue.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/2036#issuecomment-429438152):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @JuneZhao: Reiterating the mentions to trigger a notification: \n@kubernetes/sig-azure-bugs\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/2037#issuecomment-386851343):\n\n>@kubernetes/sig-azure-bugs\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @JuneZhao: Reiterating the mentions to trigger a notification: \n@kubernetes/sig-scalability-bugs\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/2037#issuecomment-386851625):\n\n>@kubernetes/sig-scalability-bugs\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @fejta-bot: Closing this issue.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/2037#issuecomment-431487082):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: AdamDang, DirectXMan12\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [DirectXMan12]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. @fejta-bot: Closing this issue.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/2039#issuecomment-442245564):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @fejta-bot: Closing this issue.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/issues/2041#issuecomment-429438148):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @yahonggu. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. [APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: yahonggu\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: x13n\nAssign the PR to them by writing /assign @x13n in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"x13n\"]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: DirectXMan12, nathanleclaire\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [DirectXMan12]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: jiayingz, kawych, mindprince\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[metrics/sinks/stackdriver/OWNERS](https://github.com/kubernetes/heapster/blob/master/metrics/sinks/stackdriver/OWNERS)~~ [kawych]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: kawych, serathius\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [kawych]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: kawych, serathius\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approvers: \nAssign the PR to them by writing /assign in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/release-1.4/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. @serathius: The following tests failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-unit | 30246dee52aebe8786c5dad55a165b000e0e1a72 | link | /test pull-heapster-unit\npull-heapster-e2e | 30246dee52aebe8786c5dad55a165b000e0e1a72 | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. @fejta-bot: Closed this PR.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/pull/2049#issuecomment-437692264):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: kawych, serathius\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approvers: \nAssign the PR to them by writing /assign in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/release-1.5/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. @serathius: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-e2e | 1049f4226411b79198df664d774cd4ce3fdc3d69 | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. @fejta-bot: Closed this PR.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/pull/2050#issuecomment-437692261):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: bmoyles0117, loburm\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[metrics/sinks/stackdriver/OWNERS](https://github.com/kubernetes/heapster/blob/master/metrics/sinks/stackdriver/OWNERS)~~ [loburm]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: AdamDang, andyxning\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: AdamDang, andyxning\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: kawych, loburm\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [kawych,loburm]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. New changes are detected. LGTM label has been removed.. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: kawych, loburm\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approvers: \nIf they are not already assigned, you can assign the PR to them by writing /assign in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/release-1.5/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. @kawych: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-e2e | e5f14450d9ca563251b7ef562bdac70af39e112f | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: AdamDang, andyxning\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. [APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: DirectXMan12, jakubbujny\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [DirectXMan12]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: andyxning, kawych\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning,kawych]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nApproval requirements bypassed by manually added approval.\nThis pull-request has been approved by: kawych, loburm\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/release-1.5/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nApproval requirements bypassed by manually added approval.\nThis pull-request has been approved by: kawych, loburm\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/release-1.5/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nApproval requirements bypassed by manually added approval.\nThis pull-request has been approved by: kawych, loburm\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/release-1.5/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nApproval requirements bypassed by manually added approval.\nThis pull-request has been approved by: DirectXMan12, kawych\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/release-1.5/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: kawych, loburm\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [kawych,loburm]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: fqsghostcloud, kawych\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [kawych]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. Hi @ringtail. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: ringtail\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: andyxning\nIf they are not already assigned, you can assign the PR to them by writing /assign @andyxning in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[metrics/sinks/influxdb/OWNERS](https://github.com/kubernetes/heapster/blob/master/metrics/sinks/influxdb/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"andyxning\"]} \n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. [APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: ringtail\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: andyxning\nIf they are not already assigned, you can assign the PR to them by writing /assign @andyxning in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[metrics/sinks/influxdb/OWNERS](https://github.com/kubernetes/heapster/blob/master/metrics/sinks/influxdb/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"andyxning\"]} \n. Hi @ringtail. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. [APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: andyxning, ringtail\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[metrics/sinks/influxdb/OWNERS](https://github.com/kubernetes/heapster/blob/master/metrics/sinks/influxdb/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: andyxning, ringtail\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[metrics/sinks/influxdb/OWNERS](https://github.com/kubernetes/heapster/blob/master/metrics/sinks/influxdb/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: AdamDang, x13n\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [x13n]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: ringtail\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: andyxning\nIf they are not already assigned, you can assign the PR to them by writing /assign @andyxning in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[metrics/sinks/influxdb/OWNERS](https://github.com/kubernetes/heapster/blob/master/metrics/sinks/influxdb/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"andyxning\"]} \n. @ringtail: The following tests failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-unit | 7629078fdeaa67924eecd265190c797a1422375c | link | /test pull-heapster-unit\npull-heapster-e2e | 7629078fdeaa67924eecd265190c797a1422375c | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: ringtail\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: andyxning\nIf they are not already assigned, you can assign the PR to them by writing /assign @andyxning in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[metrics/sinks/influxdb/OWNERS](https://github.com/kubernetes/heapster/blob/master/metrics/sinks/influxdb/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"andyxning\"]} \n. @ringtail: Cannot trigger testing until a trusted user reviews the PR and leaves an /ok-to-test message.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/pull/2086#issuecomment-417620862):\n\n>/ok-to-test\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @ringtail: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-unit | 3ad3941f8a4519defae3e1dd62aee56f04b526ae | link | /test pull-heapster-unit\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: AdamDang, andyxning\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: andyxning, ringtail\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[events/sinks/influxdb/OWNERS](https://github.com/kubernetes/heapster/blob/master/events/sinks/influxdb/OWNERS)~~ [andyxning]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: ringtail\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: andyxning\nIf they are not already assigned, you can assign the PR to them by writing /assign @andyxning in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"andyxning\"]} \n. @x13n: Reopening this PR.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/pull/2089#issuecomment-418642085):\n\n>Whoops, you're right, I don't think we deprecated eventer as well.\n>/reopen\n>/ok-to-test\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. Hi @ohookins. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: ohookins\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: directxman12\nIf they are not already assigned, you can assign the PR to them by writing /assign @directxman12 in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"directxman12\"]} \n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: mirake, x13n\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [x13n]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. @mirake: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-unit | 23e2300be813e768b4f9404fc0268874361f01a8 | link | /test pull-heapster-unit\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Hi @AdamDang. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: AdamDang\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: almogbaku\nIf they are not already assigned, you can assign the PR to them by writing /assign @almogbaku in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[metrics/sinks/elasticsearch/OWNERS](https://github.com/kubernetes/heapster/blob/master/metrics/sinks/elasticsearch/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"almogbaku\"]} \n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: mirake\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: loburm\nIf they are not already assigned, you can assign the PR to them by writing /assign @loburm in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"loburm\"]} \n. @mirake: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-unit | 322a84e68f8bdad9ec699428fe431b4f5a7fc56f | link | /test pull-heapster-unit\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: ringtail\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: andyxning\nIf they are not already assigned, you can assign the PR to them by writing /assign @andyxning in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[metrics/sinks/influxdb/OWNERS](https://github.com/kubernetes/heapster/blob/master/metrics/sinks/influxdb/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"andyxning\"]} \n. @ringtail: The following tests failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-unit | e16f18bc2783ff19613929d56aea3e41217ef7df | link | /test pull-heapster-unit\npull-heapster-e2e | e16f18bc2783ff19613929d56aea3e41217ef7df | link | /test pull-heapster-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Hi @xichengliudui. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @xichengliudui: Cannot trigger testing until a trusted user reviews the PR and leaves an /ok-to-test message.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/pull/2102#issuecomment-435308139):\n\n>/ok-to-test\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @xichengliudui: GitHub didn't allow me to assign the following users: mwieglus.\nNote that only kubernetes members and repo collaborators can be assigned. \nFor more information please see the contributor guide\n\n\nIn response to [this](https://github.com/kubernetes/heapster/pull/2102#issuecomment-435598050):\n\n>/assign @mwieglus\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: xichengliudui\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: andyxning\nIf they are not already assigned, you can assign the PR to them by writing /assign @andyxning in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"andyxning\"]} \n. @xichengliudui: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-unit | 26b3183d46a14004417c9352d067bf22b75cf219 | link | /test pull-heapster-unit\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Hi @mikeweiwei. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: mikeweiwei\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: andyxning\nIf they are not already assigned, you can assign the PR to them by writing /assign @andyxning in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"andyxning\"]} \n. Hi @xiezongzhe. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: xiezongzhe\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: andyxning\nIf they are not already assigned, you can assign the PR to them by writing /assign @andyxning in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"andyxning\"]} \n. @xiezongzhe: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-heapster-unit | ffc6ba7f1459e507954d905375b148b36d0eb76c | link | /test pull-heapster-unit\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Hi @jesseshieh. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: jesseshieh\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: x13n\nIf they are not already assigned, you can assign the PR to them by writing /assign @x13n in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"x13n\"]} \n. Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please follow instructions at https://git.k8s.io/community/CLA.md#the-contributor-license-agreement to sign the CLA.\nIt may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please sign in with your organization's credentials at https://identity.linuxfoundation.org/projects/cncf to be authorized.\nIf you have done the above and are still having issues with the CLA being reported as unsigned, please email the CNCF helpdesk: helpdesk@rt.linuxfoundation.org\n\n need_sender_cla \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n. Hi @pdericson. Thanks for your PR.\n\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: pdericson\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: jsoriano\nIf they are not already assigned, you can assign the PR to them by writing /assign @jsoriano in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[metrics/sinks/graphite/OWNERS](https://github.com/kubernetes/heapster/blob/master/metrics/sinks/graphite/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"jsoriano\"]} \n. Hi @SataQiu. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: SataQiu\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: andyxning\nIf they are not already assigned, you can assign the PR to them by writing /assign @andyxning in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"andyxning\"]} \n. Hi @gaozhenhai. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: gaozhenhai\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: x13n\nIf they are not already assigned, you can assign the PR to them by writing /assign @x13n in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"x13n\"]} \n. @DirectXMan12: you cannot LGTM your own PR.\n\n\nIn response to [this](https://github.com/kubernetes/heapster/pull/2110#issuecomment-442959566):\n\n>/lgtm\n>/retest\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is APPROVED\nThis pull-request has been approved by: brancz, DirectXMan12\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)~~ [DirectXMan12]\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[]} \n. Hi @wangzhuzhen. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. [APPROVALNOTIFIER] This PR is NOT APPROVED\nThis pull-request has been approved by: wangzhuzhen\nTo fully approve this pull request, please assign additional approvers.\nWe suggest the following additional approver: x13n\nIf they are not already assigned, you can assign the PR to them by writing /assign @x13n in a comment when ready.\nThe full list of commands accepted by this bot can be found here.\nThe pull request process is described here\n\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https://github.com/kubernetes/heapster/blob/master/OWNERS)**\n\nApprovers can indicate their approval by writing `/approve` in a comment\nApprovers can cancel approval by writing `/approve cancel` in a comment\n\n META={\"approvers\":[\"x13n\"]} \n. ",
    "DirectXMan12": "There's no description here, and we already support that, so I'm closing this issue.. I'm not certain we want to do this, and there hasn't been any movement on this for a while now.. I believe we have these mechanisms now. Please paste configuration and logs in a fenced code block so that they are actually readable.. this is quite old.  please re-open if you are still experiencing the issue. (to be more specific, this was fixed in the Heapster refactor that happened a while ago.  The default update interval is currently 1 minute, but, if you're willing to deal with an increased memory footprint, can go as low as 10-15 seconds (you'll want to go higher than 10s, though, otherwise you'll have timing issues with cAdvisor's housekeeping interval in Kubernetes))\n. --metric_resolution argument on the Heapster executable will do it (see heapster --help).  You can use it like --metric_resolution 10s.\n. We're currently transitioning away from Heapster as the defacto solution, as per the new monitoring vision in the community repo.  One of the results of that will be an end-to-end setup with Prometheus that does not involve Heapster.. (in light of that, I'm closing this issue). https://github.com/kubernetes/community/blob/master/contributors/design-proposals/monitoring_architecture.md should be what you're looking for.. Alternatively, you can use the Heapster historical API, if your backend supports it (currently only InfluxDB, Hawkular support coming soon).\n. this is quite old.  please re-open if you are still experiencing the issue. this is quite old.  please re-open if you are still experiencing the issue. model is no longer supported, and this issue is stale, closing. this is quite old.  please re-open if you are still experiencing the issue. this is quite old.  please re-open if you are still experiencing the issue. Don't think this is still an issue.  closing.. this is quite old.  please re-open if you are still experiencing the issue. please reopen if you plan on working on this. this is quite old.  please re-open if you are still experiencing the issue. Yes, I would accept a PR to do this.. this is quite old.  please re-open if you are still experiencing the issue. this is quite old.  please re-open if you are still experiencing the issue. this is quite old.  please re-open if you are still experiencing the issue. /remove-lifecycle stale\nwe should have some concrete action items if we're going to work on this. this should be covered under the disk io metrics work. this is quite old.  please re-open if you are still experiencing the issue. this is quite old.  please re-open if you are still experiencing the issue. this is quite old.  please re-open if you are still experiencing the issue. if you're working in a GOPATH, sane practice is to have the GOPATH's bin directory on your path.. this is quite old.  please re-open if you are still experiencing the issue. this is quite old.  please re-open if you are still experiencing the issue. feel free to reopen if someone wants to work on this.  Trying to prune open issues as noticed by the bot to stuff that we're actually likely to work on.. this is quite old.  please re-open if you are still experiencing the issue. I don't think we want to ever try and handle this in Heapster.\nFeel free to reopen if you disagree.. We have support for this in our Makefile now, closing.. This sounds like a cAdvisor question, not a Heapster question -- does the node summary API report stats for your physical machines?  If not, you should file this against Kubernetes and/or cAdvisor.. > Yes, the cAdvisor has reported stats for my physical machines.\nBut, does the node summary API report the right thing (curl $NODE:10250/stats/summary or $NODE:10255/stats/summary)?  We just pull from the summary API more-or-less directly.. ok, so I also just noticed this:\n\nv0.20.0-alpha12\n\nThat's a really old version of Heapster.  While I doubt this is a Heapster problem specifically, you should update so that we're working against an up-to-date Heapster.  If you're not already using the summary API source, you should be using that, and then get the results from /stats/summary.. so, to use the summary API, you need --source=kubernetes.summary_api instead.  It's newer and better maintained, and avoids a whole class of problems.. Yeah, there's currently no route to list the containers in a pod, but if you know the container name, the route to view a metric for a container is still there.  It doesn't actually break HPA or anything like that, it just makes it slightly more annoying to debug metrics issues.\n. this is quite old at this point, and the model API is not really supported.  closing. this is quite old.  please re-open if you are still experiencing the issue. @fabxc \n\nI would be very interested in the use case for this and the benefit within the next 6 months.\nAs I've heard there are companies with very elaborate scheduling and auto-scaling that hardly had a need for long-term data.\nMy instinct tells me to defer such features until its absolutely clear what's required.\n\nHere's an example use case (in fact, this is an actual thing that we'd like to be able to do):\nYou would like to write an auto-idler component for Kubernetes.  This component would run at some sort of long-term regular interval (say, for instance, every night).  Then, it would, for some set of scalables (e.g. RCs, deployments, etc), check to see if there had been any activity on a particular metric (e.g. network activity or hits on an HTTP server) over the past 24h by using oldtimer.  Then, if they had no traffic, you could idle them, and set up an auto-unidler.\nWithout oldtimer, you could build this component to talk directly to one particular metrics sink, but that would mean that you could not upstream the component, since it would not work with just any Kubernetes deployment.  Alternatively, we could keep longer-term aggregated metrics in Heapster, but the Heapster vision doc specifically talks about moving such functionality to a different component (which I think is valuable, for reasons that I talked about a bit in the proposal).\n\nI'm worried that adding a \"generic\" read-path at this point will lead to Heapster eventually growing a full meta-QL for all its sinks. If it's trying to be the system through which writes happen, it just makes sense to read through it as well.\n\nI really do not want a full query language for Heapster.  This proposal is about a minimal set of aggregations that would be useful for writing Kubernetes components that deal with historical data (e.g. the idling controller talked about above).\n\nAlso all these sinks have different query languages. I see a lot of time being spent working on mapping features onto each other and being a general contention point. You touched that point already at the end of your document.\n\nSo, hopefully this came across in the proposal, but if not I can make it clearer there:  This should basically represent the lowest-common-denominator amongst the sinks.  There should not be any \"this sink does not support feature X, so fake it in the Oldtimer code\".  I did spend some time looking at what the different sinks are capable of, but let me know if I missed anything major.\n. @fabxc you can also look at the original discussion around the Heapster long-term vision document (#769) to see some other proposed use cases for Oldtimer.\n. Ok, I've tried to address all of the comments.  I've added in an example aggregation query, and made a few more things more explicit, and rearranged the proposal a bit.  Please let me know if I've missed anything ;-)\n. Had to tweak the paths a bit so that go-restful could work with them (it turns out go-restful doesn't like wildcards in the middle of a path, and it's probably not the best idea as far as ambiguity is concerned, anyway)\n. > What will be the semantic of historical metrics in this case? Do you plan to verify also pod-uid somehow?\nGood point.  We probably want /pod-ids/{pod_id} and /pod-ids/{pod_id}/containers/{container} API endpoints.  Keeping the namespace-pod API endpoint as well might be nice, though, since it mirrors the model API.\n. > Better approach is to return data only from the newest pod with the given name\nThis gets a bit complicated, since effectively we have to then know the appropriate pod uid for a pod name before making a query, so we have to make two queries against the database.  That might not be so bad, but it does adds another round trip for every query.\nIf we're ok with the extra round trip, the optimal setup would probably be that you can refer to pod UID to get a specific pod, or refer to pod name to get the newest pod with that name.\n. I've added in the pod-id-based endpoints, and added a note about the pod-name-based endpoints.  I think there are some cases for certain backends in which we can get away with only one trip to the backend, but there are several cases where two will be required.\nFor instance, to get raw metrics from Hawkular, you need to know the pod id, AFAICT (@mwringe et all might be able to correct me on this).  For aggregations, I'm not sure how Hawkular deals with a query which could return multiple time series, but I suspect we'll need it there as well.\n. I've also clarified a couple points for posterity's sake:\n- Oldtimer is intended to work only with data written in the Heapster storage schema (i.e. data originally written by Heapster)\n- Oldtimer is not mean to be a general-purpose query tool that maps arbitrary queries to the sink query language -- it's just supposed to support a basic set of aggregations on single time series.\ncc @fabxc \nI think at this point the proposal is pretty much ready for merge.  @mwielgus anything else you want changed?\n. as in \"Kube system components can't talk to Heapster\" or \"Heapster can't talk to Kubelet\"?\n. We've fixed this a couple times.  This particular issue should be fixed.. cc @piosz @mwielgus @fgrzadkowski \n. P.S. I'm open to changing the proposed initially supported format.  I chose the prometheus one since it's probably familiar to people, and it's pretty easy to write from just about any language (including scripts, etc).\n. @fabxc \nJust to clarify, this proposal is not about pushing any and all metrics through Heapster.\nThis proposal is about giving cluster admins an easy and flexible path to push additional metrics that they would like to use to for autoscaling (and perhaps even other cluster components) at various levels of the hierarchy (whether it be pushing some sort of special aggregate metric composed of many smaller metrics at a per-pod level, or being able to push namespace-level metrics, and then consume those from cluster components and potentially autoscaling in the future).\n\nFrom my understanding the purpose of Heapster is to hold metrics that are relevant for autoscaling and scheduling.\n\nYes, we are in agreement here.\n\nThis is a fairly recent document stressing that this is the main purpose:\nhttps://github.com/kubernetes/heapster/blob/master/docs/proposals/vision.md#custom-metrics-status\nI'm confused why Heapster needs a feature to push service level metrics through its pipeline if it's a) not its purpose\n\nThis is where we start to disagree.  I would argue that custom metrics are important for autoscaling.  The above cited document says \"The support for custom metrics is focused on auto-scaling and critical functionality monitoring (and potentially scheduling)\".  This proposal is still targeting those metrics that would be used for autoscaling.\nAdditionally, there's nothing that says that custom metrics here are necessarily application-level metrics.  Sure, current custom metrics are application-level, but this proposal makes it easy for cluster admins to add in system metrics specific to their cluster.\n\nand b) not willing to give any guarantees around it or put effort into it.\n\nI'd like for @mwielgus to chime in here, but my understanding is less that we're not willing to put any effort into it, and more that we're not going to optimize for having 100s of different custom metrics.\n\nThere are a variety of projects working hard to apply sane semantics and decent APIs for metrics collection \u2013 be that push or pull.\n\nI don't think that we want/need another component running on top of the cluster that is required for the cluster to function.  If it's metrics-related and a cluster component is going to make use of it (e.g. the HPA controller), it should go through Heapster or a Heapster-related component.\n\nFor the system-level metrics needed for autoscaling and scheduling it seems very feasible to just pick a single format and either push or pull.\nThe specified performance target will also certainly suffer from people routing all the app-level metrics (which are the majority) through Heapster (for no reason).\n\nJust to reiterate, I'm not suggesting that all application-level metrics go though Heapster.  The stated goal of custom metrics support has been to support only those custom metrics needed for autoscaling and related things.  This proposal does not seek to change that.\n. @vishh  so, there's really no way to distinguish between the two types of metrics short of asking the producer.  To be fair, with the current pull model custom metrics, there's also technically no way to tell if the user is exposing monitoring-specific metrics, either.\nHowever, we could probably add options for restricting metrics on a per-producer level fairly easily (\"which custom metrics are we currently keeping track of with the prefix '$PRODUCER', compare that to the new names that would be added here, check to make sure the end result wouldn't be more than 5\"), if the cluster admin want to hand out push certificates without having to worry about someone pushing in way to many metrics.\n. I've added in an example, reorganized a bit, and made a couple things a bit clearer.\n. @jimmidyson we can discuss at the upcoming SIG Autoscaling meeting, if that works for you and fabxc, or we can schedule a hangout for some other time next week\n. @jimmidyson @fabxc yeah, the next one is this upcoming Thursday (a week from today).\n. I've clarified the \"target audience\" a bit (the proposal is geared towards cluster components, not arbitrary user applications, and is not intended to replace the existing custom metrics flow for user apps), and also added in a discussion of some of the proposed alternatives, and what I see to be their disadvantages.\n@fabxc @jimmidyson please let me know if I've misrepresented any of the alternatives that you proposed.\n. > The target audience section says this is for cluster-level metrics, and is not for arbitrary cluster users. The rest of the doc, arguments and examples are however for custom pod-level application/user metrics.\nAs I noted above, it's for cluster components.  Those components could be providing pod-level metrics, namespace-level metric, node-level metrics, etc for a bunch of different pods, namespaces, nodes, etc.\n\nThis currently looks like putting Heapster on the path to a fully-general custom metrics solution\n\nThe intention is that the metrics are only those used for autoscaling and other cluster component requirements. This proposal does nothing to change what kind of metrics Heapster stores/collects, it just provides an avenue for getting them into Heapster when you have one source generating metrics for multiple different pods/namespaces/etc.\n\nMy straw man proposal would be to have the autoscaler directly scrape what it needs, as the request rate shown in the example is something that the pod will already be exposing in some fashion.\n\nThe \"request rate\" in the example above is not coming from the pod hosting the application.  The intention (as I attempted to show by using two different pod names) was that it was coming from some sort of cluster-level cache/loadbalancer/etc which provides functionality for the entire cluster.\n. > The \"request rate\" in the example above is not coming from the pod hosting the application. The intention (as I attempted to show by using two different pod names) was that it was coming from some sort of cluster-level cache/loadbalancer/etc which provides functionality for the entire cluster.\nFWIW, the examples of service-level metrics below the example of pod-level metrics probably fit better with this example -- I was simply also trying to show what the label-set for pod-level metrics looks like.\n. > This raises the question of what you propose to do for application level metrics that aren't going through a loadbalancer or other centralised infrastructure component if this design isn't meant to handle them. For say a batch system you might want resources scaled based on the size of your backlog, which would most likely come from a monitoring system.\nJust to make sure I'm understanding your scenario here: both the batch system and the monitoring system are user-controlled (i.e. neither are cluster infrastructure components), and the monitoring system knows the backlog length, and you wish to be able to scale the number of workers based on the backlog length?\nIf that's the case, that's a bit of a tricky scenario for either the proposal, or any of proposed alternatives, IMO, because it involves just one metric (being produced by the monitoring agent) that needs to affect multiple pods.\nI think, in that case, the optimal solution might involve a bit of a tweak to the way the HPA reads custom metrics -- you'd allow the HPA to scale not just on pod-level metrics, but on namespace-level metrics (which is useful anyway).  Then you'd point the HPA scaling the workers' ReplicationController at the namespace-level aggregate corresponding to that custom metric.\n. @brian-brazil Also re: the pushgateway API: do you have any thoughts on what I said in https://github.com/kubernetes/heapster/pull/1129#discussion_r61651447)?\n. Also, just to add another use case in, one of the components that this would be useful for is an idler.  We can set up a pod, daemon, or cron job to push in some metrics gathered about the various pods, services, etc for idling (be they special network activity metrics, some sort of metrics from a load balancer, a computed aggregate metrics, etc) and use Heapster push metrics to store them such that they're associated with the pods, services, etc that they describe.  Then the idler can later read them back (via Oldtimer, for instance) when it needs to make its idling decision.\nWith push metrics, we can get the relevant metrics into Heapster from different sources (since HTTP requests can be made from just about any language/environment easily), in a way that is not tied to a particular TSDB.  Additionally, since the metrics are being fed and retrieved through Heapster, and not accessed directly from the producer or from the TSDB, the idler can easily be made generic across different metrics, sources, storage (since it does not need to care about the TSDB or the original source -- it just needs be given the metric name and access it at service or RC scope).\n. Re: the pushgateway compatiblity, there's four options:\n1. make the pushgateway-style path available at a subpath (e.g. /api/v1/push/prometheus/metrics/...): it looks like the official clients would support doing this, except for the Ruby client, although I'm not sure whether this is \"accidental\" support, or whether it was on purpose.  This option is nice because it preserves the feel of the Heapster API, while still allowing us to work with some of the pushgateway clients OOTB.  The downside is that it's not clear whether this was intended to be supported.\n2. Use the base URL without the pushgateway-style path, and just use the clients to generate the format itself, and just have them submit the request separately.  AFAICT, all the official clients support just generating the metrics into a string, except the Go client, which has libraries for generating the format, but doesn't actually support taking the metrics registered with the client and generating those to a string.  This appears to be better/more intentionally supported amongst the clients, but it means that we don't work OOTB with the pushgateway clients.\n3. Support 1 & 2, and hope that between the two something works for everyone's language and client of choice.\n4. Just use the raw push gateway URL at the root.  This has the benefit of being guaranteed to work with the pushgateway clients, but it looks completely different from the rest of the Heapster APIs, and does not provide a clear way to support different formats in the future, if we so choose.\n(it also occurs to me that the clients do not appear to make it easy to push via HTTPS with client certs, which would make the benefits of 1, 3, and 4 somewhat less advantageous).\n@brian-brazil @mwielgus @piosz what do you think?\n. @mwielgus I'm in favor of option 3 as well -- I think it provides the best path forwards in the future.  I'll update the docs.\n. Ok, I've updated the spec to support option 3.\n. > We have some serious concerns about proposed authentication of producers. The proposed authentication, with names of users and certificates, doesn't seem to be aligned with Kubernets authentication. We are not sure it will be useful in general case for the majority of clients.\n\nThe simplest option seem to be authentication on namespace level: pods can only push metrics to their own namespace.\n\nThis will not work, since it defeats the main purpose of the proposal, which custom \"system components\" to push metrics about all pods, etc.\n\nMost probably, we will need to use ServiceAccounts/Secrets to verify this.\n\nI'm not opposed to changing the proposed authentication model, but it's currently more or less the same authentication model used by the Heapster model API (I have it have the option of using a different CA for flexibility, but otherwise it's the same).  Is that changing as well?\nIt might not be a bad idea to put authentication behind an interface, and then just allow multiple different authentication methods -- we could keep the certificate-based system for backwards compat/where it makes sense, drop in a new ServiceAccount based system, and, for example, provide an easy place to allow Heapster to be hooked up to the OpenShift policy system. WDYT?\n\nApart from being aligned with k8s (which is important), I think that another problem is that it will work well for very static setups. When we have more dynamic entities (e.g. pods) who will want to push metrics to heapster than I think that static certificate is not very user friendly, unless I'm missing something.\n\nWell, if you don't specify a list of names, and just have it trust anything from the CA, you can just distribute certs signed by the CA (e.g. in a secret).  What exactly did you have in mind for the service accounts, etc? \n. > Yes, but not only. We may have stats pushed by a pod prefixed with pod name (application stats), and more general stats prefixed only with namespace.\nI think we need to clarify here: when you say \"prefixed with pod name/namespace\", do you mean metrics stored internally at the keys \"namespace:ns/pod:podname\" vs \"namespace:ns\", or do you mean metrics producers prefixing the metric names with the associated pod or namespace?  Push metrics mainly intended for pushing \"bulk\" metrics that may cross the namespace from the originating pod (e.g. the pod might be in kube-system, but might describe pods from all namespaces).\n. So specifically for push metrics, both of these can be pushed in a single push, so it doesn't make sense to provide different auth mechanisms for different metrics storage scopes, unless I'm misunderstanding you.\n. Perhaps we should find some time for a quick hangout at some point this week?  It might be more productive than trying to play comment tag.\nAlso, I think it would be reasonable to proceed with push metrics auth as specified in the documentation, and then overhaul Heapster auth in a separate PR (i.e. I don't think we need to tie auth changes/a new auth interface to push metrics, necessarily).\n. closing due to staleness. this is quite old.  please re-open if you are still experiencing the issue. this is quite old.  please re-open if you are still experiencing the issue. Events in general are currently undergoing some updates, and there are other tools that allow a bit better control of things over eventer.  Closing. /remove-lifecycle stale. Heapster is now deprecated.  No new sinks will be added.  See https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more details.. that's just for events -- you'll also need to modify the metrics sink (and related historical source) and common utils as well:\n- https://github.com/kubernetes/heapster/tree/master/metrics/sinks/influxdb\n- https://github.com/kubernetes/heapster/tree/master/common/influxdb\n. P.S. InfluxDB 1.0 has the bound parameters code in (I think), so you should probably be able to drop the manual sanitizing in favor of using bound parameters in the historical queries\n. closing due to no activity/staleness. closing due to no response. diskio work is partially completed and being tracked elsewhere.  closing. Fixed by #1788 . fixed in #1591 . kubernetes: kubernetes/kubernetes@3f2fe8b5b87e233a161830ce67b9d9d012e6b288\nheapster: kubernetes/heapster@a543cfde8d9ff7abb413ba97627d0ef30c1ac53b\n. cc @onorua \n. cc @piosz @mwielgus\nAt the moment, I'm not entirely sure if this is a Heapster issue or a node issue, although I didn't notice it with the older version of Heapster, which seems to suggest it's at least partially a Heapster issue.\n. IIRC, I saw it using both the summary and the normal one.  I'll double check this afternoon.  I recall that when I added in extra logging, I saw heapster thinking it had received two different entries, and was just using the first of the two (which sometimes happened to be the correct values, sometimes the weird value described above).  I'll do a bit of digging this afternoon.\n. As far as sinks go, this PR includes InfluxDB, and Hawkular will probably be next (IIRC, @burmanm was interested in helping out there).  After that, I'm assuming GCM is the next priority.  Depending on time constrains, I can then work on OpenTSDB (or Reimann or Elasticsearch, but I haven't investigated those quite yet).\n. Ok, I've removed the [WIP] tags from this, since it was decided that additional sinks will get their own PRs.\n. Ok, I've added tests for the model handlers.\n. Oddly, it passes locally (go version go1.6.1 linux/amd64). Looks like timezones in the timestamps in the test are changing across serialization and deserialization.\n. @mwielgus yep, just need to figure out what's going on ;-)\n. Ok, I've pushed one final change, which makes the start time parameter mandatory (and mandatorily non-zero), since different sinks treat a zero time differently, and some sinks have issues with excessively large duration (making an explicit start time mandatory makes sense here).  Thanks to @burmanm for pointing out the issue.\nThanks to @ncdc for tracking down the issue with the times.  It had to do with Go having a special \"Local\" timezone internally that was different than the actually local timezone (but \"equivalent\"), so DeepEqual considered them different (not entirely sure why it worked in east coast time, though).\n. @mwielgus should be good to go!\n. With the advent of metrics-server, this is probably better done elsewhere in Kubernetes docs. It's also possible that this is related to #1168.  @onorua what do you see for cpu/usage?  A negative/overflowed cpu/usage_rate could indicate a cpu/usage which is not continuously increasing (like it should be).  Do you see something similar to what's in #1168?\nI wounder if this is on Heapster's end, or if we're getting sent odd values from the node...\n. cc @piosz @mwielgus \n. @onorua yeah, I think we can just use the bug that I opened, since it looks like you're hitting a similar issue.  I'll see if I can find to some time to track it down a bit more.\n. Hmm... do the pods appear eventually, or do they just never appear?  You could try running at a high-verbosity log level (e.g. 6), which should show you more of what's going on under the hood, and the requests that kubernetes is making.  Also, can you please report your kubernetes and Heapster versions (if you're running from master on either, indicate the git SHA)\n. @ApsOps and I discussed further, and it looks like one of his nodes is not reporting any pods at its stats endpoint.\n. @dchen1107 do you have any ideas about what could cause this?\n. this is quite old.  please re-open if you are still experiencing the issue. /remove-lifecycle stale. Heapster is now deprecated.  No new sinks or features will be added.  See https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more details.. @piosz: this is #1199 ;-). for the API proxy, you would have needed to ensure that system:anonymous had permissions to proxy that service, if you're on a recent version of kube.  Please look at the documentation on the Kubernetes RBAC system.. In general, it looks like a good start.  I've got some questions about a couple of things, although a couple might be due to not knowing the specifics of the Hawkular Go API.\n. Just did another quick pass, and I think this mostly looks good.  The only thing I'm worried about here is assuming the gauge type for unknown metrics -- the push metrics PR allows for custom metrics of both the cumulative and gauge type, so assuming gauge for custom metrics will miss some metrics.  Is there any way to search both gauge and counter metrics in Hawkular?  Alternatively, perhaps we could put some sort of marker in the name (e.g. anything ending in _rate is always a gauge, etc)?  WDYT?\n. >  one could have \"cpu/usage_rate\" \"GAUGE\" and \"cpu/usage_rate\" \"COUNTER\", thus it would be quite difficult to find out which one is the correct one.\nInternally, Heapster keys on metric name only (https://github.com/kubernetes/heapster/blob/master/metrics/core/types.go#L123), so data fetched via Oldtimer, which was written originally by Heapster, should not have issues in that regard.\n\nI'm not sure what you mean with \"miss some metrics\", as long as we store it with same assumption. And at this point we store unknown metricType as Gauge also (so transformation happens two ways in identical way). \n\nI've updated the push metrics PR to properly read which type (cumulative vs gauge) a metric is marked as, and store in Heapster accordingly.  Unless the Hawkular sink implementation ignores the MetrcType field (https://github.com/kubernetes/heapster/blob/master/metrics/core/types.go#L90), you're going to have some custom metrics stored as gauge and some stored as cumulative.  Ergo, if you assume all custom metrics are gauges, and you only search for gauges, you're going to miss cumulative custom metrics.\n. So, push metrics are transient -- there's no guarantee that the pushed metrics waiting to be scraped have the same names as the ones stored in Hawkular.\nHowever, I just remembered seeing something: doesn't the Hawkular sink keep metrics definitions around: https://github.com/kubernetes/heapster/blob/master/metrics/sinks/hawkular/types.go#L52\nCould that information be used here?\n~~(also, it occurs to me, is there going to be a problem with the Hawkular sink for push metrics, since not all push metric names are know at startup time?)~~ EDIT: just tested, works fine\n. The push metrics PR is #1217.\n. @burmanm did you get a chance to take a look at the push metrics PR?  I'd like to get this in soon.\n. ack, I'll add that in.  I think I left it out because I wasn't sure if it was redundant (b/c there's already namespace_name as a different label).\n. @burmanm fixed\n. cc @ncdc @piosz @mwielgus @jszczepkowski \n. This is a rough draft, but I wanted to get the ideas out there so that debate on the general design could begin.\n. I chatted with @deads2k earlier to day, and he brought up the issue of the API server HTTP service proxy, which would not work with this proposal out of the box -- the proxy currently strips off authentication information (e.g. the bearer token).\nTo solve this, from the Heapster end, you'd have to explicitly not authorize the proxy user by itself to do anything (since you have no idea which user the request came from, so you can't properly make decisions).\nIn order to get the proxy to continue to work properly, we'd probably have to change the proxy itself a bit.  @deads2k and @liggitt  suggested that we could have have the proxy accept credentials in the Proxy-Authenticate header (like squid) instead, and simply pass through existing credentials (new clients would have to do a version check to figure out if they were talking to an old or new server).\n. @liggitt the alternative is to just say \"if you want to have any sort of authorization rules, mark the service proxy user as untrusted by Heapster, and always go through the service IP/service DNS name directly, or through a Route/Ingress/what have you\".  This would mean the HPA would have to go through the service IP (or DNS name) directly (which seems reasonable to me), but I'm not sure what we'd do about kubernetes/kubernetes#28844 (having an oc command require a route seems suboptimal).  I feel a bit like we're between a rock and a hard place here -- we're going to have to make some use case more difficult.\n@deads2k re heapster setup, Heapster supports serving over HTTPS, so it would be quite possible to have heapster have a verifiable identify (and is recommended, IIRC).\nIn OpenShift, we do something similar with Hawkular, where Hawkular authenticates and authorizes in a single step by performing a SubjectAccessReview against OpenShift using the user's Authorization header, AFAICT.\nWhich brings me to a question: we'd eventually probably want to write something similar to what Hawkular does as an option for Heapster authentication/authorization.  In this case, the \"authenticate\" step would effectively just have to return an empty user, and pass the bearer token to the authorizer.  This could potentially be done via the GetExtra method on the auth.User interface, but it seems a little hacky, so I was wondering if people had thoughts on that (e.g. we could use an interface that either returned auth.User or an indication that the authorizer should handle things).\n. @deads2k neat-o!  The webhook auth makes my above question a non-problem in the future, it looks like.\n. @deads2k did a bit of work, and found an issue with the proxy change idea (see openshift/origin#9826).  So, I think, for the time being, the answer has to basically be \"if you want fine-grained authorization, block the system:master-proxy user, and have things connect through the Heapster service IP/Route/Ingress\".  If you just want to use the existing setup (client certs, no fine-grained authorization), then you can continue to use the master service proxy with the new interfaces.\nDoes that seem reasonable to people?\n. > Something similar could be done by verifying the client cert's signature, then remotely checking the client cert's subject's access.\nYeah, for the client certs, you could just use the current implementation (the right CA gets injected into Heapster, and you just verify the cert, and use the CN).\nSupporting both (and using SubjectAccessReview for authz) would look like\n--authn=cert:/path/to/ca.ct --authn=kubetoken:https://kubernetes.default.svc?someoption=something --authz=sar:https://kubernetes.default.svc?someoption=something\nThen, if the request had client certs, it would try verifying the signature on the client certs, and using the CN from the client cert as the user name (using the same code as today).  If the request did not, it would look for the token, check the token against kubernetes, and use the returned user info.\n. @piosz @erictune the goal was to propose an interface that could be coded against currently that would make it so that we could quickly reimplement the current system (client certs and user list), and then later implement a system based on RBAC/SAR (or what have you) without having to rewrite the code which uses the interface.\nBasically, the goal is to not have to rewrite a bunch of the push metrics code when we implement a more robust authorization system, but to also not have to make having the more robust system implemented a requirement to merging push metrics.\n. > conform more to the Kube RBAC style\nPutting aside the \"not needing RBAC auth as a prerequisite for merging push metrics\" issue, there's also the issue that that there's not necessarily a direct mapping between a single action and a single (verb, resource) pair like in Kubernetes (see below).  IMO, we should make the methods which the various parts of Heapster call into speak in Heapster terminology, and then whatever is behind them can do some conversion into the appropriate Kubernetes terminology.\nFor instance, while most queries (both model and historical) can be expressed directly as a (verb, resource) pair (i.e. GET, with the subresource representing either model or historical), when you get to lists and pod IDs the mapping is slightly less straightforward (although still pretty clear -- you'd probably just map lists to GET ALL IN NAMESPACE, and since it's not clear what name and namespace a UID belonged to, fetching by UID would probably just be mapped to GET ALL IN CLUSTER).\nOnce you get the push metrics, the mappings break down further: push metrics are intended to allow bulk submission of metrics, so in a single push you could have: metrics on the namespace, metrics on pods, metrics on containers, and metrics on the cluster.  If all the metrics are in a particular namespace, you might be able to notice that and make a more direct query, and fall back to POST ALL IN CLUSTER for everything else.\nThe various Heapster components, however, should not have to know anything about these mappings.  They should just be able to say \"here are the affected Heapster keys, can I preform creation/model get/historical get\" and get a response.\n. That's not to say that we shouldn't have RBAC mappings for Heapster resources at some point (I think we should have them), I just don't think the internal Heapster components should specifically have to think about the mappings, unless the RBAC system can speak more in terms of Heapster terminology.\n. @liggitt @deads2k @erictune thoughts?\n. > namespace name -> this seems to map directly to our existing namespace concept, so in the attributes it would be namespace=namespace.Name\nYeah, that's probably the easiest one ;-). \n\nPodName/PodID - I don't understand the distinction between these. We key based on PodName now. Are we worried about re-using a name and an old credential having rights to update it? Drawing an ACL boundary on namespace seems to make sense, so this distinction seems to matter less. I would expect apiGroup: heapster, resource: pod, name: name\n\nPodID is the Pod's UUID.  It's used in the historical API to allow users to disambiguate between several historical pods with the same name.  Queries with UUIDs have no associated namespace/name information, so I'd imaging they'd have to be a more privileged operation (e.g. GET POD METRICS in namespace \"*\")\n\nContainerName - I'm not sure I see the need to acl down to this level. Why would I want to subdivide my access based on containers inside pods? 1 NodeName - apigroup: heapster, resource: nodes ?\n\nThat information was mainly intended to allow ACLing on system containers, which are identified by node name and pod name.  In general, I'd imagine that you'd probably just want system container metrics to be a yes/no switch, although I'm not entirely certain how you'd represent that.  The field is mostly there for completeness -- pods are ACLed on NAMESPACE and POD NAME, while the \"corresponding\" attributes for system containers seemed to be NODE and CONTAINER NAME.\nThe biggest wrinkle that I see is push metrics -- from what I've seen of the ACL system (and it's entirely possible I've missed something), we basically need to decide how we want to break up the requests for push metrics.  How do we deal with multi-namespace requests (do we make multiple requests, do we just say any number of namespaces greater than one requires permissions in to push in any namespace (which seems like it could yield confusing results)), what does the prefix map to, etc?\n. > Hmm... we need to subdivide at that level of granularity? I could be done via subresource, but that would be unpleasant for all concerned.\nI imagine in most cases it would be something like \"is this user allowed to push with no prefix, is this user allowed to push with a custom prefix (maybe), or is this user allowed to push with a prefix equivalent to their user name\" (so, effectively, one of two or three options, so a boolean or three-way enum).  There will need to be some Heapster-side logic to figure out user prefix vs empty prefix, but I suspect the ACL could probably just be another YES/NO to start out with, if that makes it easier.\n. > Is there a case where one group of people can see containers on some nodes and not others?\nIs there a case where a node might only be allowed to see resources for its own node?  That's probably more fine-grained than you want, right?  Otherwise, I think that looks reasonable.\n. > For example tagging an image in another namespace requires permission to tag in ns1 and permission to pull the image from ns2.\nThat might be a lot of requests, conceivably.  Is it a reasonable operation to first check for \"ANY NAMESPACE\" permissions on more that one request, and then if you don't have that, make the individual requests (but have a cap of some sort)?\n. @deads2k @erictune I've updated the PR to just work with Kube SAR and TokenReviews+client certs.  Does that look closer to what you're looking for?\n. I've added some examples and hopefully clarified a couple of points.\nRE: prefixed vs unprefixed metrics, I'm imaging the metrics will look the same (i.e. submitted metrics will never have a username prefix), but the URL may end up differing slightly, although that will have to be a change to the push metrics PR.\n. After some lengthy discussion with @deads2k (thanks!), I've tweaked the section about prefixed and unprefixed metrics, and will be updating the push metrics API PR correspondingly.\n. closing in light of the new monitoring vision. the direct cadvisor source hasn't been supported for quite a while.  closing.. Before merging, I'd like to get #1213 in and supported here, but that may not happen.\n. cc @ncdc @mwielgus @piosz @jszczepkowski @fgrzadkowski \n. (the test failure looks like a flake, AFAICT)\n. I added better handling of labeled metrics (and refactored a bit).  The new code is in a separate commit, but I can squash if it desired.\n. @piosz @mwielgus PTAL\n. @k8s-bot test this\n. that's a weird failure -- looks like one of the metrics API integration tests is failing...\n. While it's technically possible to run Heapster outside of a Kubernetes cluster, it's more complicated and error prone.  You should run it inside the cluster in a pod (probably created through a replicationcontroller or deployment) instead.. this is quite old.  please re-open if you are still experiencing the issue. @anthonyhaussman for Kube 1.4, the design of the master metrics API changed a bit.  It looks like that's what you're hitting, unfortunately.  Please stick to Heapster 1.1.0 for Kubernetes 1.3.0.\n. this is quite old.  please re-open if you are still experiencing the issue. @Kokan we should probably have that modification, if we don't already. ack, closing. cc @piosz @mwielgus \ncc @burmanm depending on if/when this gets merged, you might have to implement this for #1209 \n. While there are not currently many labeled metrics pushed through Heapster (although, IIRC, there are a few), there may be more in the future (e.g. per-port network stats on pods and services), and push metrics also opens up a new avenue for adding more labeled metrics.\n. @k8s-bot test this\n. WRT to the rate calculation issue: I'm seeing a weird issue on my setup where, after inserting some debug statements, it appears that occasionally, Heapster receives multiple instances of the same container with slightly different start times (subsecond differences).  This seems to happen much more rarely on the summary API (which appears to round to the second for the timestamps), but still happens occasionally. \nHowever, I'm not entirely certain that this is also the cause of your missing metrics.  I'll continue to dig.\n. WRT to the rate calculation issue: traced back to kubelet, which seems to be somewhere receiving duplicate container entries from cAdvisor, and is happily just appending to its list, assuming that never happens.  The duplicates seem to have slightly different start times -- off by less than a second.  I'll continue digging tomorrow.\n. Depending  on versions of things involved, this may be a bug in the interaction between systemd and docker, causing a container to show up in at two different keys.  After I updated my system, and then dealt with this: https://github.com/projectatomic/docker/pull/181, the rate calculation problem went away.\n. (if you docker exec into one of your pods and check /proc/self/cgroup, what what do you see there?)\n. That could also be causing your missing metrics issue, as \"missing metrics\" can mean missing \"cpu/usage_rate\", which is put in place by the rate calculator.\n. yeah, based on having both the /init.scope prefix and and the plain /system.slice prefix, I'd guess that containers are getting doubly reported by Kubelet (with slightly different start times), and thus confusing Heapster.\n. @janwillies @lattwood it's also fixed in a newer Docker version, there's this patch (https://github.com/projectatomic/docker/pull/181) if you're running on Fedora, and you could also patch your copy of Kubelet to merge those results.\n. closing this in favor of the ongoing discussion about a 1.3/1.4 solution in kubernetes/kubernetes#30939\n. Confirmed -- GetAggregation is missing the check that's in GetMetric and GetLabeled.\n. I'll have a patch up shortly\n. Ok, so, I think I figured out why this is failing -- there are a couple of returns missing after calls to WriteError.  I suspect that something changed in go-restful that caused it to previous ignore all writes after WriteError, but the new version allows for writes after WriteError.  I've got a patch coded up (it's just a couple lines).  I'll post it in a couple minutes.\n. @piosz NP.  The PR above should fix it.  Sorry about that ;-)\n. I can only repo this with the PR that that this test run was testing (#1252), so I think it's actually caused by that.  I'll take a look, and leave a comment there.\n. this is quite old.  please re-open if you are still experiencing the issue. It seems reasonable to attach related information together.  Requests and limits seem reasonable to attach as well.  Does attaching capacity, though, increase storage space much?  You'd be duplicating it quite a bit if it appears everywhere cpu/usage_rate does, I believe.\n. Do you see anything in the logs for the Heapster pod?  Could you put logs from the Heapster container in some sort of pastebin-like area (a gist, pastebin, uploaded to an accessible server, etc) and post a link here so we can take a look and make sure there are no errors there?\n. can you run Heapster under a higher verbosity (add an additional argument in your Heapster RC of --v=5)?  That might show more of what's going on.\n. hmm... if you make the queries manually, do you see the incorrect information?\nFor instance /metrics/api/v1/model/namespaces, or /metrics/api/v1/model/namespaces/default/pods?\n. @piosz what's the status of this. I think this looks good to me.  I'm glad we're explicitly passing around the options object instead of using globals now.  I'd prefer if the commit had a description (e.g. \"This commit brings Heapster inline with the conventions used by Kubernetes' GenericAPIServer\")\n. Do you see any values on individual containers?\n. This is quite old now.  Please re-open if you're still experiencing the issue. I believe this has been fixed.. What are the candidates here?  I would guess probably at least the current HEAD and current released.\n. please add a small description (even if it's just something like \"we should have a /healthz endpoint in eventer like we do in Heapster so that Kubernetes can properly health-check it\") for future viewers to look back on.\n. That seems reasonable -- those two together seem sufficient justification.. cc @deads2k @liggitt for auth stuff.  I think as long as we're transitioning to genericapiserver, we need to stick a full auth story in here.  I think some of that ties into the auth proposal (mainly some of the discussions about the Kube auth model there) -- #1213 \n. Closed by #1298, AFAICT.\n. this appears to be the same as #1316.  Closing in favor of that.\n. What's the relationship between this, #1296 (which looks to be an extension of this), and #1294?\n. > Separate authentication powered by the webhook authorizer.\nI think we're agreeing here -- my point was that you don't necessarily need/want the other auth mechanisms -- you just want the webhook one.\n. Please file a bug or follow-up PR to discuss and/or fix the auth as per my comments and @deads2k's comments.\n. Yeah, so the point there was that you generally don't want all of the authorization options available on the Heapster API -- you just want the webhook authorization option (which would get set by the user to the Kube API server hosting the auth APIs).  We (probably) don't want Heapster having separate authn/authz from the rest of the API servers.\n. closing.  If you want proper auth, use metrics-server.. Just to clarify, if you do kubectl get nodes, what addresses are listed there?\n. @huangyuqi can you please do a review?  I'm not particularly familiar with ElasticSearch\n. none-the-less, I agree with @piosz: we should probably wait for and/or assist with the Kubernetes migration, and then move ourselves once that happens.  It would be unfortunate if we ended up with a different workflow than Kubernetes, and had to change again to be in line with the main repo.\n. Please, in the future (and here for posterity) add a description for why this change is necessary (what the issue was, etc).  It should be in the PR description, and (potentially in reduced form) in the commit message as well.\n. > I'm not quite sure what is the status of the Heapster project as a part of the Kubernetes ensemble of products though\nCurrently, it's required for the Horizontal Pod Autoscaler to function, and otherwise is useful for collecting metrics and placing those into sinks (InfluxDB being one of them).  We're currently re-evaluating the monitoring pipeline, however.\nAs for the state of the grafana configs, they are most likely out of date.  Grafana is not really a part of Heapster (those configs are mostly just provided as an example of how to set up the InfluxDB sink).  If you have updates that work, we'd be willing to accept them in pull-request form.\n. I don't think we officially support InfluxDB 1.0.0 yet, but this does look somewhat like a bug nonetheless.\nPlease try with an older version of InfluxDB and see if it works.\n. hmm... this probably needs a fix in the InfluxDB sink, then\ncc @mwielgus \n. This needs to have a commit author matching your account (looks like it's just your machine's root user right now).\n. cc @mwielgus \n. cc @mvernimmen \n. Perhaps glide isn't doing the filtering based on used vs unused files, like Godep does?  Do we need glide-vc?\n. closing as per above comment.. @bryk all the data comes from Kubelet (which it's getting from cAdvisor).\n. I think you'd need to weigh in on the cAdvisor bug. kubectl top just uses data from Heapster, so it would have the same problem. The concept looks good.  Does the race detector not work on s390x?\n. > Is this anywhere on the roadmap or known workarounds?\nNot that I know of.  @AlmogBaku was recently working with the ES sink, so he might know of a workaround, but I there's nothing on the roadmap that I know of.  We'd probably be willing to accept a PR adding that functionality, though.\n. Closing this due to the \"All good\" comment.. LGTM :+1: [has LGTM from AlmogBaku, merging...]. @k8s-bot test this. That looks like either you're not running in a Kubernetes pod, or something is wrong with your Kubernetes setup (since Heapster can't find the auto-injected service account secret).  Are you running with service account secret creation disabled?. looks like your container networking might not be working properly (since your Heapster can't connect to your API server through the service proxy).. @piosz what needs to be done to get these posted?  Who has access?. ping @mrcrgl . @mrcrgl agreed about the unit tests\n@ezeev alright, please add some unit tests, and then I think this has the LGTM stamp after those are done (should be something like \"given these metrics in Heapster internal format, do we get the right output metrics in Wavefront format\").\nOnce those tests are in place (and I've had a chance to glance over them), I'll merge this.. Generally LGTM.  Please squash the commits together and add a commit updating the state of the Wavefront sink in the sink-owners.md, and then it'll be good to merge.  Thanks!. Heapster can run fine in sinkless (in-memory-only) mode, though.  It's useful for testing, or if you only want to support the HPA, and have a different metrics collection method.  You should probably have some logic that says \"if a sink was asked for, but doesn't exist, then fail\". LGTM :+1:, but we might want to mention both sig-instrumentation and sig-autoscaling (since many questions are HPA-related). this is a known issue with CoreOS when using the systemd-cgroups driver with docker.  It is recommended to use the cgroupfs driver instead.. please provide a description of the PR (what fixes, how it fixes it, etc).  Also, please remove the extraneous .iml file.. please add a description of what this is fixing.  It would appear that this includes several different changes.. @k8s-bot ok to test. LGTM :+1:. These are the current likely candidates based on a quick check of git history.  The ones with ? next to them I was unsure about, general due to lack of recent activity on the sink.  If you're on this list, please chime in and let us know if you're ok being pinged for reviews on the listed sink.\n\n[x] ElasticSearch: @AlmogBaku / @andyxning / @huangyuqi\n[x] GCM: @kubernetes/heapster-maintainers \n[x] Hawkular: @burmanm / @mwringe \n[x] InfluxDB: @kubernetes/heapster-maintainers / @andyxning\n[x] Metric: @kubernetes/heapster-maintainers \n[x] Kafka: @huangyuqi\n[ ] Monasca: ~~@taimir / @cheld ?~~ could be candidate for deprecation (https://github.com/kubernetes/heapster/issues/1407#issuecomment-266008730)\n[x] OpenTSDB: ~~@petergardfjall~~ @bluebreezecf\n[ ] Riemann: ~~@jsoriano ?~~ could be candidate for deprecation once Graphite merges (https://github.com/kubernetes/heapster/issues/1407#issuecomment-265850414)\n[x] Graphite (once #1341 has merged): @jsoriano / @theairkit \n[x] Wavefront (once it merges): @ezeev\n. @jsoriano are you still using the Riemann sink?  If nobody is actively using it, we might want to put out the call for interest, and if nobody is interested, simply deprecate it.. I've updated the list above.  It seems Monasca and Riemann could soon become candidates for deprecation, although I'd like to hear from @shmish111 about the latter.  The original author for the OpenTSDB sink seems to have been @bluebreezecf, so if they're no longer interested, we may wish to consider our options for that one as well.\n\nPing @piosz @mwielgus any other comments?. Thanks, I've updated the list.. @piosz  @fgrzadkowski ok to officially commit this list in the repo?  Thoughts on the deprecation?. It would be nice to get that, or something like the main Kubernetes repo OWNERS functionality.  I'll keep you posted :-). @k8s-bot ok to test. LGTM :+1:. Please also provide a description of the change in the PR description field, and use a more descriptive title.  If you want to mark that this is fixes an issue, write Fixes #566 in the PR description field.\n. This should be opened against https://github.com/kubernetes/kubernetes. > But if someone wants to ignore/filter out specific types, the person should understand that this also disables metrics. The problem in my project case is that we don't want/need metrics of any containers (as the InfluxDB \"dies\" after X container series metrics).\nAt the very least, can you add in a warning message if cpu and/or memory at the container level are skipped?  Alternatively, if there's a way to turn this on for specific sinks, that would also work (but might require a bit more work).. Specificially, we care about, AFAICT (cc @piosz anything I'm forgetting?)\n\ncontainer cpu/usage_rate\ncontainer memory\n\nif those get ignored, then we'll have issues.. > So I should just add a warning when the label type=container is ignored?\nYeah, that'll work.. let's trying again cc @piosz @krzyzacy is this expected due to the CI work that's being done?\n@k8s-bot test this. hmm... the Godeps file seems to have been updated, but the corresponding vendor directory was not.. We should have one soon.  Sorry for the delay.  Going forward, we'll have a better procedure for releases :-). I'm working on a rebase right now as well, which we'll probably want in a near-future release.. I've edited the title a bit, but please fill in a few more details in the PR description.. gah, apparently GitHub's web resolve-conflicts tool doesn't quite do what I want it to do (sorry about that extraneous commit).  I'll merge the changes as soon as you rebase.\nI'd like to apologize for the delay -- I know long review delays can be frustrating, and I'm sorry I caused one. . Ok, fixed the rebase issues and undid the commit that was confusing the CNCF CLA bot (\"Allow Edits From Maintainers\" taketh away, \"Allow Edits from Maintainers\" giveth :-P ).  Will merge as soon as Travis finishes.  Thanks!. Thanks for the contribution.  Sorry it took so long :-/. @jsoriano thanks for the repo save!  @piosz sound fine to substitute @jsoriano's repo in the mean time?. which variable are you talking about?  Also, if they're sink-specific, they'd need to be sink URI parameters. Ah, it's a change to Graphana.  Those are mainly just examples, not really part of Heapster.  You can build your own Graphana image pretty easily to make that change.. We'd take a pull request to make it an environment variable, though.. I think you may be misunderstanding me -- I was suggesting that if you, or someone else, submitted a pull request to make the change, we'd be happy to accept it :-). closing due to inactivity.  We can always re-open this later.. LGTM :+1:. IIRC, network is measured at the pod level, since all containers are in the same network namespace.  Ergo, no container name is recorded on network metrics.. LGTM :+1:. > If those containers aren't represented in the data stored from stats/summary (say because only total pod data was returned in a single \"opaque\" container) then heapster will return no data via the metrics api. while it will will show usage in the model api.\nYeah, do to current requirements from the consumers of the resource metrics API (the HPA), we need to have per-container usage stats (this is not strictly necessary, but it's how the HPA currently works).\nWhat's the situation in your cluster in which cAdvisor is returning only per-pod usage stats? . Hmm... ok.  Not sure how much we support such a setup where the stats provider doesn't get visibility into the pod.  cc @kubernetes/sig-node-feature-requests what's the current story around setups like that and/or around CRI and stats integration?. Closing.  This was due to not running a standard metrics setup, which Heapster doesn't really support.. LGTM :+1:. We're not adding new sources at the moment.  In the future, such a usecase will be covered as part of the new monitoring vision by the third-party monitoring pipeline components (see https://github.com/kubernetes/community/blob/master/contributors/design-proposals/monitoring_architecture.md). @AlmogBaku / @huangyuqi / @andyxning please review. That's... weird.  Presumably there's not a null byte in the JSON that json_reformat and/or is ignoring?. cc @AlmogBaku in case I've missed anything special to the ES sink, but should be good for merge.. @AlmogBaku / @huangyuqi / @andyxning . No, Heapster does not normally calculate averages.  Some average data is calculated in the legacy model API, but IIRC that's only the past hour and day.  You can calculate them yourself, or if your sink supports the Oldtimer interface (currently only InfluxDB), you can also use that to fetch averages.. err... what's up with the \"add missing files\" commit?  Did you not use the godep tool to pull in the new dependency?  Also, you can squash the Godeps related commits into a single commit, just as long as the commit which updates the main code is separate from the one that updates the Godeps.json/vendor directory.. This fixes issue #1419 (which has another repo location posted).  I don't particularly care which repo location we use, but I think we're probably going to have to deprecate (then remove) the riemann sink.. cc @jsoriano . @piosz PTAL\nAs a follow up, could you fill out the \"release\" section with a bit more policy on timing with relation to Kubernetes releases?. (P.S. the docs and sinks/* labels don't exist yet.  Once this merges, I'll create them).. we can reopen once cAdvisor gets support. it needs to be rebased before it can be merged.. I'd very much prefer not to add anything new to the legacy source, especially if it's not in the summary API.  Please get it added to the summary API.. We're removing the riemann image, so I'm going to close this.. (see #1455). ping @huangyuqi . looks like we actually have a Kafka events sink too, but that page didn't get updated.  Can you please also update sink-owners.md in this PR?. I don't think the Graphana dashboard is not designed to be accessed through the Kubernetes API server HTTP proxy, and will attempt to load it's resources from the root of wherever the paged is accessed through.  You should access it through its service IP instead (or through an Ingress object).. What log level was Heapster running at?  Can you run at a higher one?  Did you see any messages about skipping rate calculations in your logs?  How about other errors?. Do you have thin_ls enabled?. yeah, I'm not sure how rancher works, but basically, Heapster gets its metrics via the Kubelet stats summary API.  Kubelet derives these stats via a built-in copy of cAdvisor, so I'm not sure how much good a sidecar cAdvisor will do.  Does Rancher just not support the HPA at all?. > and funny thing is, the pods it says \"no metric from\" are the 2 pods I applied HPA on their deployment. it doesn't seem to even try to fetch info about other pods, which is weird. Will be reading about how all this works...\nYeah, that message is actually logged from the API handlers when they can't find the info in the in-memory cache.\n\n. so i'm trying to understand now how to make heapster receive it's needed data.\nHeapster log shows this non stop:\n\nIf the Kubelet doesn't serve the summary API, there's not much we can do.  If the Kubelet does serve the summary API, then we have something to work with.  Is the Rancher Kubelet serving the summary API?. @r0bj \nW0209 12:54:37.625232 1 listers.go:68] can not retrieve list of objects using index : object has no meta: object does not implement the Object interfaces\nIf I had to guess, there's a version mismatch in your cluster, or something else is up -- the \"skipping rate calculation\" message you posted is generally caused by stub pod entries.. >  btw, what are \"stub pods\" @DirectXMan12 ?\nStub pods are fake pod entries created when Heapster has some data from one source, but not from another.  They cause all sorts of issues, but you usually don't see them with the summary API\n\nI'm doing manual cleanup of any exited containers on hosts, and manual cleanup of unused images, can this cause such issue?\n\nThat maybe could cause issues... not sure.  Is the built-in Kube cleanup not working sufficiently?  Kube should do that stuff automatically as necessary.. > As you mentioned, the \"skipping rate\" message is caused by a version mismatch in the cluster, do you know witch version is mismated, the heapster vs. k8s or k8s vs. etcd?\nI was referring to heapster vs k8s.\n\nAnd in this message the old create time is \"0001-01-01 00:00:00 +0000 UTC\", looks like not set.\nAccording to my understanding, the metric data is retrieved from cadvisor, witch is embed in kubelet, are there any wrong with kubelet?\n\nThat happens usually when heapster has to create a placeholder entry because it doesn't have some data when it needs it.. HPA specifically needs requests.  This is documented in the HPA documentation.  Simply accessing the metrics from Heapster should not require requests to be set.. @r0bj which source are you using? --sink=kubernetes or --sink=kubernetes.summary_api?. Ok, so first try with --source=kubernetes.summary_api:https://kubernetes.default.svc.  If that doesn't work, check if you can you manually curl the summary API endpoint from the Heapster pod.  The summary API endpoint is at https://$NODE:10255/stats/summary.. so, I'm guessing you're hitting a bug where your Heapster collection interval is not always larger than the cAdvisor collection interval.  What's your Heapster collection interval?  Default?  We had a bug a while back where thin_ls stats were causing issues, but that should have gotten merged a while ago.... hmm... you're using 1.5.2... that doesn't have the patch for disabling thin_ls stats...\ntry bind-mounting /dev/null over the thin_ls binary and see if that fixes your issue.. I really need to put this in a document somewhere:\nWith a set collection interval of 10s in cAdvisor, the actual cAdvisor collection interval can be up to 30s, due to jitter and backoff mechanisms.  Your Heapster collection interval should be no lower than 30s.. closing since the issue seems to be resolved.. If Riemann is going to move to maintained, we need to do something about the client library -- it can't live quasi-unmaintained in @rikatz's GitHub forever.. That's fine with me, as long as we have a maintained Riemann library :-). So, you can curl the URL that the dashboard fails on from other pods, but not from the dashboard pod?. That usually means the kubernetes is not able to talk to your copy of Heapster.  Can you please show the events from your HPA object?  Is Heapster deployed in the right namespace?. closing (as it seems to be fixed). cc @deads2k I have to make sure I didn't actually break anything, but the tests pass, etc.\nLet me know if you see anything that could be improved re: the split repositories, generic API server, etc.. > \"please look through 1 million changed lines to find something funny looking\" sniff\nHeh, sorry.  The \"important\" changes are now in their own commit, separate from the \"find-replace\" commit on the API.. cc @piosz . @piosz this looks like it's failing to build because it's missing the context package, which entered Go in 1.7.  Is the CI not building in 1.7?. @deads2k thanks for the review!\n@piosz PTAL (also what's up with the e2e test -- it looks like it's not running 1.7, perhaps?). @kubernetes/test-infra-maintainers do you have any idea what's going on here?  Nothing useful seems to be in the logs, but symptoms seem to point to this test being run under Go 1.6, AFAICT.. FYI, I discovered an issue in this PR that only appears at runtime with the API server \"storage\".  It's fixed in the update PR that I posted ( #1537 ), which may end up superseding this PR.. superseded by #1537. cc @piosz . @k8s-bot test this. @jsoriano yeah, the above invocation should re-run the tests, you might not have permissions to invoke the bot. please stop +1-ing.  It doesn't help the situation.  If you really feel the need to +1, use the reaction button.\n@piosz @kawych @loburm @x13n I think we need to switch the base image.  I suspect alpine is probably fine, since it doesn't look like there are any other recommendations...\nEveryone else: in the mean time, I'd recommend building your own image if you care about this functionality.. @piosz there's been some upheaval with the images lately.  WDYT?. > That coupled with the poor searchability of gcr.io images/tags makes for a rather unfortunate first impression (IMHO)\nNot in disagreement here, just wanted to check and make sure this matches the intended image setup ;-). We should already do this if you enable the kubeletHttps option in the source config.  We appear to try to connect with the same client certificates that we use to connect to Kubernetes.. (added another to the list). @krzyzacy we can close this now, right?. apologies, I mistyped my earlier comment -- its metric_resolution (metric is singular).  Otherwise, LGTM. Failure looks unrelated, not sure what's going on. @k8s-bot ok to test. once this passes, please squash the commits together and then I'll merge. This shouldn't matter if you're launching in a pod, I think.  What did your YAML look like?. they should be in the pod's standard output -- just kubectl logs $HEAPSTER_POD_NAME. Does anybody have any last minute complaints, or are we good to merge this?. SGTM. They actually were supported before then, IIRC, there was just period where they incorrectly got dropped.. You should note that instead.. yeah. I think this looks good now.  Not sure if we really need v1.2.1 -- I think just fixing it in v1.3.0 is find.\nWDYT @piosz ?. some of the sinks do, not sure about OpenTSDB.  Assigning to the sink maintainers: @bluebreezecf . Heapster reports the working set, and the total usage.  The summary API also sends us RSS, but we don't expose that currently (it would be a pretty simple patch to do so, though).. err... there are still like 7 commits in this PR that say revert or fix.  Each commit should be a single piece of mostly self-contained new functionality, so this PR should probably just have one commit (git rebase -i, mark everything as a fixup except the first commit, git push --force-with-lease).. Alright, I've squashed everything down for you.  Looks good now, will merge once Travis finishes.. > this should be sink/elasticsearch, not influxdb.\nso it should, I think I was triaging too fast :-P. @piosz this should merge before 1.3.. cc #1507 . ping @piosz . I think this makes sense -- IIRC, \"system containers\" can actual contain other containers due to cAdvisor's definition of \"system containers\".  cc @piosz PTAL as well.. We should probably get secret information as a Kubernetes secret object (pass in a secret name).. It shouldn't be too hard to change over the various sinks (even without knowledge of how the particular sinks work).\nWe could either have heapster actually fetch the secret, or just rely on secret mounting, and have Heapster look in a specific (configurable) spot.\n@piosz WDYT?. /remove-lifecycle stale. Heapster is now deprecated.  No new sinks or features will be added.  See https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more details.. That work went in place, but never really left alpha.  It's being deprecated in favor of the monitoring vision work.  While we don't have a direct replacement for it yet, we keep an eye in sig-autoscaling and sig-instrumentation in the 1.7 or 1.8 timeframe.. follow up with @andyxning's comments, but otherwise looks good.. Following up, the code looks decent.  It should be good to merge once we've confirmed that it passes tests and you fix the nits.. @k8s-bot ok to test. cc @piosz PTAL. client-go finally synced, so this is unblocked. You'll need to also update the sink owners file.. I think this LGTM :+1:, but if we're going to land the Godeps update, I'd like to get that landed first.. > There is no Godeps update. Any clue?\n1537 contains a major Godeps update that pulls in the latest dependencies, and therefore changes a lot of import paths.. err... considering eventer no longer builds when rebased onto master, I'd say a rebase is in order (not sure why the unit tests aren't showing it, but just tested locally).  Nothing from k8s.io/kubernetes (with the exception of one package of types from kubelet)) is imported, so you'll need to change that.. Please rebase on top of the latest master.. looks good in general to me.\nAny thoughts, @piosz . merging, seems good to me.. any comments @piosz ?. clusters tend to not react well in general to differing times.  I was under the general impression that you need to have system time synced across your cluster for Kubernetes in general to work properly.. what do the heapster logs say?. closing based on Slack conversation -- this might be due to having a collection interval below 30s, which can interact poorly with cAdvisor, which may jitter it's collection interval up to 30s.. heh, LGTM :+1:.  Can you sign the linuxfoundation CLA please, and then we'll be all set to merge.. closing due to inactivity. something is not set up properly with your IDE -- it looks like it's not pulling in the rest of the packages in the project, if I had to guess.  Take a look at the Makefile do see how we normally build Heapster.. we should probably skip empty metric labels in the OpenTSDB sink, then.  Assigning to the OpenTSDB maintainer.. cc @bluebreezecf . @piosz PTAL\nI've found these additions to be very useful in debugging weird issues with Heapster that we've had over the past few months, so I figured they should probably go in to Heapster.. is other pod networking working fine?  Can you reach the master from other pods?. do you have multiple different API servers backing your API endpoint?  Maybe one of them is using different certs?. will merge once @burmanm give LGTM.\n@burmanm @mwringe if you want this to make the next release, please try to resolve by tomorrow evening (6pm PST).. (had meant to mark this 1.3) merging due to LGTM from sink maintainer.. just merged the dep, please rebase.. looks reasonable to me. @k8s-bot ok to test\nwill merge once tests pass due to LGTM from sink-owner.\nWe're probably going to want a release note on the label changes @piosz . Thanks for this!. @piosz @luxas opinions on this?. seems fine to me as well, merging.. Heapster only reads from Kubelet, actually.  Kubelet re-serves the cAdvisor information in a different format.\nIf you want to fetch the /metrics endpoint metrics, you'll have to have something else fetch them (e.g. Prometheus itself, or another tool that speaks prometheus format).. --secure-port only affects the apiserver mode -- you'll need to run with --api-server. Post-rebase, it's not proof-of-concept any more.  I'm not sure what you're getting at here.... looks like an infrastructure flake.\n@k8s-bot test this. Everything looks good, merging.. cc @bluebreezecf . LGTM from maintainer, looks reasonable to me.  merging.. Just FYI, metric resolution really shouldn't be lower than 30s anyway, because of issues with cAdvisor.. @andyxning it's been a few months since I went through that code, but IIRC, most of the important bits are here: https://github.com/kubernetes/kubernetes/blob/master/vendor/github.com/google/cadvisor/manager/container.go#L359.\nKubelet sets the default housekeeping \"interval\" as 10 seconds, sets the max dynamic housekeeping interval as 15 seconds (https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cadvisor/cadvisor_linux.go#L54), and enables dynamic housekeeping.\nThe duration between housekeeping intervals can be, even without dynamic housekeeping, up to 2 times the default housekeeping interval (that's what jitter does), meaning even if we turned off dynamic housekeeping, we get an interval that's actually anywhere between 10s and 20s.\nWith dynamic housekeeping, we can increase the housekeeping interval from 10s to 15s.  This is applied before the jitter, so theoretically, we can get an actual time between housekeepings of up to 30s.. I'd like to get an ok from @piosz as well (since this is a \"front page\" change, so to speak), but it looks good to me.. Oldtimer was never widely implemented, and is very likely going to be deprecated and removed over the next couple of releases.  I'm not entirely certain it's worth providing these instructions (especially without a deprecation notice).\n@piosz WDYT. > So will the main endpoints extract metrics older than 15 min. or if we want to do that we'll have to query the backend sink?\nEventually we'll have something along the same lines as the new custom metrics API -- a set of type definitions and API paths that will be implemented separately for each sink, except in a clearly historical context (as opposed to the custom metrics API, which is for recent metrics).. we'd welcome a patch which added support, but the Heapster maintainers are unlikely to do it ourselves.. @mpvaniersel @piosz PTAL. yes, that's expected, I believe. First of all, I'd reccomend that you match Heapster versions with Kubernetes/OpenShift versions -- Heapster 1.3 is much newer than Kubernetes/OpenShift 1.3.\nPlease also ensure proper network connectivity from the pod to the master.. closing due to inactivity. you should be able to control that via the ignore_label and store_label flags.. what's your Heapster configuration look like?  Are you using the old source --sink=kubernetes: or the summary source --sink=kubernetes.summary_api?. so, first things first, I'd reccomend switching to --source=kubernetes.summary_api.  If I had to guess, based on the error message, what's going on is this: the non-summary API source can require multiple HTTP requests for each node.  Due to something (network congestion, the number of requests, load, etc), Heapster isn't able to complete all of the HTTP requests in a reasonable mount of time.  The summary source uses a single HTTP request per node, so it shouldn't have that problem.. @kaijparo can pods access the node IPs with canal networking?\n@ntquyen what version of Kubernetes are you using?  CoreOS stable version doesn't really tell me anything.  It looks like the watch cache of nodes isn't populating correctly.. @k8s-bot ok to test. if you're still seeing this, please re-open.. this should probably be filed against the main kube repo (since the problem is policy and non Heapster), although I'd argue that the RBAC is correct in denying the Heapster pod the ability to modify its own configuration.  That feels poor from a security perspective.. cc @piosz . Is this intended to be a temporary solution, or a long-term one?  In the long term, I don't think we should tie the VPA to Heapster, since Heapster will eventually no longer be a necessary cluster component.\nIs there any reason the VPA can't pull this information from the resource metrics API instead?  That way, everything will continue to work if people run metrics-server or something else.\n. We should close this, then. would like to see a review from @jamtur01 before merge, but if we don't get one soon, just ping me and I'll merge this.. Awesome.  Merging!. IIRC, \"free containers\" returns things that look vaguely like containers to cAdvisor (in this case, a bunch of cgroups).  At any rate, I would not rely on this functionality to monitor containers started outside of Kubernetes, as that use case is not well supported.. IIRC, there was, at one point, some issues double-counting things at the node and cluster levels.  That should be fixed in master, I believe.\nA heads up though -- you might have some issues running Heapster 1.3.0 against Kubernetes 1.3.0 -- the Heapster versioning doesn't match up with Kubernetes versioning, so Heapster 1.3.0 is quite newer than Kubernetes 1.3.0.  Heapster 1.3.0 is designed to work with Kubernetes 1.5 and Kubernetes 1.6.. (I'm closing this now, but feel free to re-open if you have further questions or if the latest Heapster does not solve your issue). > How does heapster collect its metrics? Especially for node and cluster level, pods seems to be ok.\nSo, it looks like for the node, we get the memory/usage value directly from Kubelet/cAdvisor -- that's not one of the ones that we aggregate up from pods.  So whatever you're seeing, that's what Kubelet is telling us.\n\nIs there a way to run all its api calls manually?\n\nWhen configured to use the summary endpoint (the recommended way -- --source=kubernetes.summary_api), Heapster will hit the summary endpoint of each node using a GET.  That's served at http://$NODE_IP:10255/stats/summary.  Otherwise, it will repeatedly hit each of the stats endpoints, which is a bit more difficult to replicate yourself.  It's probably easiest to start looking at the data provided by the summary endpoint and check that it matches what you'd expect.\n\nyou also mentioned some issues double-counting things at the node and cluster levels\n\nSpecifically, at the cluster level, there was an issue at one point involving double-counting due to the way cAdvisor interprets what a \"system container\" is (a collection of cgroups), and the fact that the cluster-level metrics include system containers.  I would recommend against trusting the numbers produced as \"cluster level\" numbers.  The node-level ones should be fine, though. Note that the sum of all pods is not necessarily the memory usage on the node.  The sum of all pods should be at most the memory usage of the node, but the node should have other processes that are using memory on it as well.. Why should the system:heapster role be able to GET and PUT extensions.deployments?  The Heapster code should only be able to GET/LIST and even then only what it needs to know about to function: Events, Namespaces, Nodes, and Pods.. @piosz might know. @piosz do we have the grafana-cli in our images?  Should we?. If grafana-cli is just for managing plugins, I'm tempted to say that things like installing plugins should be done at image build time, not at runtime, and that if you want to install plugins, you can pull in grafana-cli separately.\nI'm open to other opinions though. > cause we can not confirm what plugins to install at compile time.\nSure, but presumably the admin/user deploying grafana does, right?  So they should have an image\n```dockerfile\nFROM  gcr.io/google_containers/heapster-grafana-amd64:v4.0.2\ninstall our plugins and then remove grafana-cli to avoid runtime security issues\nCOPY grafana-cli /tmp/grafana-cli\nRUN /tmp/grafana-cli do-some-stuff\nRUN rm /tmp/grafana-cli\n```\nand use that for running Grafana on their cluster.. Closing as per my response above.  If you want to do something special for your cluster, build your own image.. This looks reasonable to me, any comments @piosz ?. @piosz is there any reason for us to keep around two separate copies of the deployment information?  I'd tend to agree with @kartoch here.... hmm... probably the underlying metrics are labeled metrics in Heapster, but the model API doesn't expose those so well.... In the internal representation, they're probably labeled per network adapter.. no,  we do not support that. closing since this seems to be a duplicate.. cc @AlmogBaku @andyxning @huangyuqi . this is not an issue with Heapster.  It's probably a version issue with your Kubernetes cluster.. Let me know when you guys are ready to merge this one, and I will.. @AlmogBaku could you squash some of the \"gofmt\" and \"fix\" commits?  Each commit should be one logical unit of features (or an update to Godeps).  After that, I'll merge.  Thanks for the hard work!. I'll squash myself so we can get this in the upcoming release. Alright, squashed into three commits (Godeps, ES5, ingest).  Should be good to merge once it passes the tests.. /retest. looks like a flake unrelated to this PR\n/retest. looks good, merging. Thanks everyone for all their hard work on this :-). for positerity, you can also use --logtostderr to only log to std err.. Please note that this is specifically in relation to GKE in the PR title.. please also put [GKE] in the commit message title.. Please actually write was was originally wrong and what the correct behavior is for changes like this (also, probably put \"Stackdriver\" somewhere in your commit message title).. It's not important enough to revert, but please keep an eye on this in the future.. @kubernetes/heapster-maintainers PRs should not be merged until they have an appropriate commit description and title.. cc @acobaugh. Fixed by    05f739e. Can you please indicate which version of Heapster and which version of Graphana and InfluxDB you are using?. Are there any issues in the Heapster logs?  Have you tried asking the dashboard repo?. > Also is cAdvisor part of the equation ?\nHeapster makes use of the cAdvisor built in to the Kubelet.\nIf you run Heapster at high verbosity (--v=10), what do the Heapster logs say?. Which version of Kubernetes are you using?  Can you run Heapster at a higher log level (add the --v=8 argument).. The first thing I'd do is try using the summary source (--kubernetes.summary_api) instead.  That often solves many issues.\nThen, please attempt to query the Heapster API directly $HEAPSTER_SERVICE_IP/apis/metrics/v1alpha1/nodes to determine if you're having Heapster issues or connectivity issues.. So, the way kubernetes DNS is structured is that services are exposed as <service>.<namespace>.svc.<suffix>.  Generally, the shortened forms <service> (which expands to <service>.<current-namespace>.svc.<suffix>) and <service>.<namespace> are acceptable.  I'm not sure why <service> works fine but <service>.<namespace> does not.  Does <service>.<namespace>.svc work for you?. > It's possible this is a red herring and I'm just not waiting long enough (it seems to take at least 3mins for the hpa to recoginze the current CPU usage)\nThat's somewhat expected, depending on your configuration of Heapster, the HPA, etc.  Heapster needs two collection intervals to determine CPU usage (since it's a rate, we need two data points to calculate it).  Depending on your HPA resync interval, 3 minutes is high, but not terribly so.\n\nI also have the issue that the hpa is never able to recover current cpu usage for a statefulset.\n\nDo statefulsets even implement the scale subresource?  AFAICT, they do not.  The HPA will only work on scalable resources (those which implement the /scale subresource).. This is not really an appropriate question for the Heapster issue tracker.  StackOverflow, the mailing list, or Slack are more appropriate venues for this discussion.  FWIW, we are moving in a direction of encouraging alternatives, as opposed to having all data flow through Heapster.  Check out the monitoring vision for more information.. We don't usually use the GitHub squash, because generally you want several distinct commits for different portions of work, but you don't want any \"fixup\"-style commits.  For example, keep \"Update grafana to xzy\" but get rid of \"fix gofmt\".. I do seem to recall seeing this issue before, possibly on another sink.  Can you reproduce with the latest Heapster?. @k8s-bot ok to test. This one looks squashed, now, actually.  For future reference, you don't have to open a new PR -- just git rebase -i the current PR to squash, and then git push --force-with-lease.. This is only supported for the resource metrics API (/apis/metrics/v1alpha1/...).  In that case, you can pass label selectors.  However, that API only provides CPU and memory.  The custom metrics API (not served through Heapster) also supports label selectors, but there are no providers for that yet (hopefully, we'll have one soon).. merged the original, closing this one.. Did you mean to run eventer instead?. I'm unclear as to what you mean by \"Can't get the events data\".  Can you elaborate a bit?. The table was not correctly updated -- we do have a Kafka sink.  @huangyuqi please submit a PR updating the table.. LGTM :+1:. No, that's not a metric that Heapster exports.  Please see https://github.com/kubernetes/kube-state-metrics for metrics like that.. This would be a better request for cAdvisor or the Kubelet.  Heapster just collects the metrics.. No.  That would be part of the \"third-party\" pipelines described in the monitoring design document: https://github.com/kubernetes/community/blob/master/contributors/design-proposals/monitoring_architecture.md. Thanks for the PR.  Can you please update the description to describe things a bit more?  For instance:\n```\nGCM: check error when parsing config URL\nMake sure to check to see if parsing the config URL returned an error in the GCM sink.\n```\n@k8s-bot ok to test. Please do so for the commit message as well. Otherwise LGTM :+1:. Is there a reason that the elasticsearch sink for Heapster wouldn't work for you?. I'm a little hesitant to support this, since it seems not quite as generally useful (since most people can probably just use the elasticsearch sink), and every new sink in Heapster adds maintenance burden to us.\nWDYT @piosz . looks like your kubeconfig file is not set up properly.  I'd check that (the error is from client-go, validating the kubeconfig).  Alternatively, if you're running on cluster, you shouldn't have to manually specify a kubeconfig -- you can just rely on in-cluster config.. Try running Heapster at --v=8 or --v=10.  That should give you more details.. closing due to lack of response.. So, there error that you got doesn't look like an RBAC error, it looks like a networking error.. In your networking setup, are pods able to access the API server?. sounds like you have cluster networking issues, not Heapster issues.  Please ensure your cluster networking and cluster DNS works as expected, and then re-open this issue if you're still having problems. I just checked, and it looks like there is a role in the bootstrap policy for Heapster (https://github.com/kubernetes/kubernetes/blob/master/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/policy.go#L201) which already contains these rules.\nIs your cluster set up with the bootstrap policy?. I'm going to close this PR.  If you have other problems, please file a new issue.. Uh... can you rephrase your question?\nAlso, what version of Kubernetes are you running?  What version of Heapster?. Please separate Godeps updates into their own commit, otherwise it's difficult to review the PR.. For future reference, no need to close the PR, you can just git push --force-with-lease. there should be a Heapster role automatically created as part of the bootstrap policy.  Make sure that role is present (if not, sync your bootstrap policy), and make sure there's a clusterrolebinding binding the kube-system:heapster serviceaccount to that clusterrole.. (feel free to re-open the issue if that doesn't work).. make sure you sync your bootstrap policy -- the policy is definitely in the set of bootstrap policy for 1.6.3 (I just checked): https://github.com/kubernetes/kubernetes/blob/v1.6.3/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/policy.go#L199. can you just inject the CA roots via a volume (hostpath or otherwise)?. https://kubernetes.io/docs/concepts/storage/volumes/#hostpath provides instructions for how to use volumes (specifically, hostpath volumes).  You could also place them in a secret, and mount them in that way.. Here's the list of where go looks for root certs: https://github.com/golang/go/blob/master/src/crypto/x509/root_linux.go#L9\nLooks like your setup doesn't quite match what it's expecting -- you've mixed an matched paths and file names from a couple different distros, AFAICT.. this seems reasonable to me.  I'd welcome a patch.. you also need to make sure you've set up the right flags on your Kubernetes cluster to auto-inject the DNS info.  IIRC, it's a flag on the Kubelet.. ~~@piosz didn't you just fix this?~~ whoops, I think I just saw this issue before. . Please provide a more detailed description in both the commit body and the PR description.  Please try also to not have too long of a commit message title (otherwise it wraps).. Tests pass, has LGTM from sink maintainer.  Merging. This is probably not a Heapster issue.  Are other pods working properly on your cluster?  Heapster is using the standard DNS code from the Kubernetes libraries here.  Are you actually injecting cluster DNS?. by default, Heapster with the API server will attempt to create self-signed certificates if none are present in /var/run/kubernetes or specified with flags.  Either make /var/run/kubernetes writable, or, better yet, inject certificates via a secrets mount.. do we even need host_id at all?  Is nodename not sufficient?. sure, but as has been previously suggested, if you know one, you can probably find the other -- they're effectively two parallel axes, so we shouldn't store both and cause extra indices to be created in the TSDB.. I've updated the PR description a bit to match the latest discussion.  Merging.. merging since this has lgtm and tests pass. @mwringe can you do a review?. I think I was waiting for an ok from @mwringe before merging.  If this is good and rebased, I can merge it.. Merging, since this is from the sink owner and seems to have a generally positive review otherwise.. this is not a Heapster issue.  Please ask about it on StackOverflow (or the Kubernetes bug tracker if it seems like this is actually a bug).. I'd expect that Heapster 1.0 should work with Kubernetes 1.4, but I'm not sure.  I'd have test it out to see.  At any rate, I'd avoid become dependent on any of those APIs -- they're not official APIs, and are likely to disappear.  If you want aggregation, please use one of the sinks.. it's sink-dependent -- each sink has it's own query language that you could use to do that.. it should still be there, according to the code.  I'd warn that Heapster 1.3 is relatively new, so is mostly tested against Kubernetes 1.5/1.6, not 1.4. v1.2.0 is probably fine.. LGTM.  Merging post tests.. @k8s-bot ok to test. needs to be @piosz -- I can't check gcr.io, AFAIK. #1689 and friends all seem to duplicate the work here.. cc @miaoyq / @Kokan if this is still the case, feel free to act on it.. Something like that sounds fairly reasonable.  Tag me when you submit the PR.. looks like a duplicate of the referenced issue.  Closing. You'd probably need to build a new image off of the base image with the changes baked in, or somehow inject the changes via a mounted-in configmap.. I'm not super familiar with how Grafana works, but I'd guess that it's because the series still exists in the TSDB -- the old metrics are still there, so you can still access the information for that pod.  Perhaps someone else can clarify, though.. you should probably just scope your queries to a particular span of time... that should prevent deleted pods from showing up, I believe.. no, it's not supposed to be exclusively as a cluster addon.. We'd welcome a PR for this :-).. This seems reasonable to me. Assigning to the Kafka sink maintainer for a second review.. LGTM since the maintainer isn't responding.  Merging.. > my cluster is up and running fine but heapster is unable to detect the information from the cluster, grafana is not connecting to influxdb also\nLet's do this one at a time.  What are the symptoms of Heapster not being able to gather metrics?  You'll need to provide more information.  Please also provide versions of Kubernetes and Heapster.. So, is Heapster itself working?  Is it just not grafana that's working?. Wait, how have you confirmed that Heapster isn't connected to Kubernetes, then?  Are you able to access the Heapster API?. Wait, how have you confirmed that Heapster isn't connected to Kubernetes, then?  Are you able to access the Heapster API?. ok, so first let's figure out if Heapster is working.  You might just have a Grafana issue.  You should try running kubectl proxy in one terminal window, and then curl localhost:8001/api/v1/namespaces/kube-system/services/proxy/apis/metrics/v1alpha1/namespaces/kube-system/pods and see if that returns anything.. you appear to have missed putting an actual service name in your curl -- it should be \ncurl localhost:8001/api/v1/namespaces/kube-system/services/heapster/proxy/apis/metrics/v1alpha1/namespaces/kube-system/pods. This and #1689 are duplicates.. @piosz can you confirm the image versions are as they should be?. duplicate of #1689.  Closing.. Please provide a more descriptive commit/PR title.. Sorry, that message is a bit misleading.  It looks like you're missing the requestheader client CA file.  The API server code in Heapster isn't new enough to automatically fetch it from Kubernetes, so you'll need to pass --requestheader-client-ca-file as well (see https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md for more info on the different CA files).. Seems fine to me.  Will merge once test runs finish.. Seems fine to me, will merge once tests runs finish and #1691 merges.. we need to make sure we log this if it happens, among other things.  In the past, we've had too many spots in the collection code which silently ignores errors or missing data.. @piosz: it looks @loburm isn't yet on the list of people that I can assign to. First off: please provide a PR description when you file a PR.\nSecondly: looks like your goal here is to support InsecureSkipVerify?  I'd much rather see a field for easily pointing at a CA file to use to verify (which is a bit more specific that just generally adding to the known list).. I'm (personally) fine with this (but you'll have to convince @AlmogBaku, since they're the sink owner) -- it's a fairly common option on things that does development/test clusters easier to work with.  However, you have to make it just as easy to configure a CA specifically for accessing ES, otherwise people will use this as an escape hatch for all their SSL problems (which reduces overall security).  If the ES sink doesn't have a config option for specifying a particular file to use for the CA for ES connections, it should @AlmogBaku.. can we close this because of the new PR?. @piosz did something go wrong with the latest release of the Grafana pod?. looks like a duplicate of https://github.com/kubernetes/heapster/issues/1709. looks good to me, will merge soon.. I believe @piosz is working on it, and it's marked as high priority.  We should have a fix soon.. This is a known issue with the current Grafana image, it looks like: duplicate of https://github.com/kubernetes/heapster/issues/1709. cc @ezeev . ping @ezeev . merging as per owner response. /ok-to-test. LGTM as long as it passes tests.. Merging since tests pass.. @kubernetes/heapster-maintainers was this ever fixed?. @piosz have you seen anything like this before?. please ensure that your container networking is set up properly, and that in your networking setup, pods have permission to access the nodes.  Further ensure that you haven't disabled access to Kubelet on that port.. Yeah, open in the main Kubernetes repo.  Heapster merely scrapes the metrics provided by Kubelet. We should really have documentation for that somewhere (if we don't, I'd gladly accept a PR).  The summary source uses the summary API, while the non summary source uses a series of calls to the old stats API.  The summary source is the newer and more supported of the two.. /ok-to-test. I'm fine with testing the volume scraping logic, but can you adapt this to just work in the main decoding test?. LGTM.  merging.. /ok-to-test. oh dear, we're being really flaky\n/retest. It's passed.  Merging.. /ok-to-test. /retest\nmight have been a flake. This looks like it passes tests, so I think I'm fine merging this.. yep! :-)\n/ok-to-test. Hmm... I though this was already created.\ncc @luxas. merging with LGTM from sink maintainer. LGTM :+1:\n/ok-to-test. can you figure out which file in that directory the issue is coming from?  The error alone doens't help much.. also, please state the version of Kubernetes that you are using. just for filesystem/usage?  What version of Heapster?  What do the Heapster logs say?. first off, try with --source=kubernetes.summary_api=https://kubernetes.default.  Using kubernetes.summary_api as the source often solves a lot of issues.. Ok, I'm going to close this since it isn't Heapster-related.. you'd have to use the deprecated model API (/api/v1/model) or look at the data directly in whatever sink you're using.. you should see network usage with the summary API.  If not, please file a separate bug.  However, I will note that the model API is deprecated, so no support is guaranteed.. LGTM :+1:. That is the correct answer.  You should build a custom image with your own grafana.ini.  You can even use the existing image as a base image.. The model API is deprecated.  Please use the resource metrics API (/apis/metrics/v1alpha1/nodes) instead.. Without knowing details about your cluster, there's nothing we can do.  Please indicate your versions of Heapster and Kubernetes, and check the Heapster logs.. For DNS, Heapster will use whatever the pod is autoconfigured to use.  Make sure your cluster DNS is properly configured for whatever setup you have.\nThe default configurations all point at using the kubernetes.default cluster DNS alias, but if this doesn't suit you, you can override it by changing the --source option.. I like the general concept of this, but I think maybe I'd prefer to separate not_usable into unschedulable and unready.  If you want usuable nodes, you can then say usability=usable\nThoughts @piosz ?. as I'm thinking about this a bit more, is there a reason not to just always report node metrics, and then correlate this after the fact?  adding an extra label like that does weird things to the metrics \"model\", since whether or not something's schedulable shouldn't affect its identity.... /lgtm. This seems reasonable.  We commonly use these errors to diagnose issues with Heapster, but V(2) is sufficiently low that it's probably fine to just run with that when we care about it.. LGTM, will merge once tests pass\n/ok-to-test. err... can you provide some justification for this?  The criteria for this \"report card\" seems rather unclear, and while it's nice and all to have some random \"A+\" on your page, it's also nice to have it actually mean something.. apparently, I was in a foul mood when I wrote that comment.  Apologies for that :-).  While I'm not entirely convinced on the matter, I'm not opposed to merging it.  Let's run it by @piosz and then we'll go from there.. please add a more descriptive title and PR description.. @piosz do you remember why we don't aggregate cumulative metrics together?. honestly, I'm not sure.  It's possible that it was just to save space in-memory, because mostly we only care about rate metrics.. I'm tempted to close this as I'm not particularly keen on adding new features to Heapster and/or increasing the memory bloat (which is already fairly large).  If you have a compelling use case and are still interested, please share.. lgtm.  Thanks!. I think that might be an artifact of the kubectl describe code.  Does the actual pod configuration look normal?. Also, this is not a Heapster question.  Please ask on the Kubernetes StackOverflow for Kubernetes support.. I suspect you'd have to change the cluster addon configuration.. I want to make this clear: if you add a sink, you're on the hook for maintaining it.  If there are issues, they'll be assigned to you, and it's your responsibility to deal with them in a timely fashion.  If issues are assigned to you and not dealt with, or if there are dependency issues that aren't dealt with, we (the Heapster maintainers) may remove the sink.\nI don't mean to sound harsh, but I'd like to avoid maintenance issues in the future.\nIf you'd like to go forward with this, add yourself to the sink-owners.md file as part of this PR.. /ok-to-test. This has passed tests, so I'm going to go ahead an merge.. merging due to lgtms from appropriate reviewers.. This is not really a Heapster question.  Please look at the InfluxDB docs and/or file a bug with Influx. Heapster is able to talk to InfluxDB as a sink, if configured.  What does your Heapster config look like?. closing b/c no response\n/close. Does the reproduce on the sample API server?  This might not be a Heapster issue.. hmm... we need to update the API server code anyway, since we're on an older version.  I'll try to get to that soon and see if it fixes the issue.. you should be using metrics-server if you want resource metrics API support\n/close. @piosz @loburm @x13n I can try and take a look at this next week, unless one of you wants to handle it.. /remove-lifecycle stale. yeah, that should be the only requirement. @piosz @loburm @x13n we need to migrate testing it looks like.  Can one of you take a look at this?. /lgtm. Please update the description of this PR.. merging due to LGTM from @luxas . Please add a description to this PR.. looks like there's now a conflict, please rebase if still appropriate.. looks like there's now a conflict, please rebase if still appropriate.. which version of Heapster are you running?  That looks like one of the overflow issues that was fixed a while ago.. What are you referring to?  Grafana?  Influx?  Please be specific and provide more information about your setup.. Honestly, if you need to customize Grafana, I'd recommend building your own Docker image with the customizations.  Note that since grafana is running in a container, you probably want to build the plugins into the image and restart the image, not try to restart the server within the image.. @piosz @kawych et all can you take a look at this. Sorry it took me so long to get back to this.  Shouldn't we just make network a labeled metric now, instead of aggregating like this, since we now have per-interface stats?  People can aggregate together in their sink.. I'm going to merge this since @huangyuqi seems to be AWOL.\nIn other news, this sink needs a new maintainer.. Can you clarify what, exactly, you're trying to accomplish here?. I want to make this clear: if you add a sink, you're on the hook for maintaining it. If there are issues, they'll be assigned to you, and it's your responsibility to deal with them in a timely fashion. If issues are assigned to you and not dealt with, or if there are dependency issues that aren't dealt with, we (the Heapster maintainers) may remove the sink.\nI don't mean to sound harsh, but I'd like to avoid maintenance issues in the future.\nIf you'd like to go forward with this, add yourself to the sink-owners.md file as part of this PR.. Please squash so that they're aren't commits of \"rebased\" \"updated for review\" (it's ok to have multiple commits, but each commit should be somewhat self-contained), and then we can merge this.. Please remove the \"rebased and resolved conflicts\" commit, and then we're good to go here.. /cc @piosz \nPerhaps something in the config is triggering autodiscovery of IP address and getting confused?  Maybe we should hard-code the IP address to 0.0.0.0, since we're in a container anyway.. > I suppose a relevant question - should \"localhost\" have been set to reference something - is that a step that I missed somewhere in setting up my bare-node kubeadm cluster, or should be added?\nIf localhost doesn't resolve to a loopback address, you're going to have a bad time with more than just Heapster.  Plus, RFC 6761 section 6.3 states\n\nUsers may assume that IPv4 and IPv6 address\nqueries for localhost names will always resolve to the respective\nIP loopback address.\n\nFurthermore, there's a draft in place to further solidify this: https://tools.ietf.org/html/draft-west-let-localhost-be-localhost-06.\nI'd consider this to be a case of a misconfigured environment, and not anything we should try to work around locally.. (for future reference, I'd guess that CenturyLink was not returning NXDOMAIN for localhost.  Shame on them ;-) ). I'd prefer we actually figure out what the issue is in the new images, if we can.. we'll need one of the Google folks to make any image changes.  I'll poke one of them and see what we can do.. did you check the logs of the pod?. closing issue as per the above comment. Would like another review on it.\n@mwringe, can you take a look?  If not, I'll try and do a deeper review.. is this still relevant @burmanm ?  If so, we can make sure it's still good to go, and merge it.. I really think it's fine to leave this as localhost, and call any machine that doesn't have localhost as the loopback misconfigured.\nRFC 6761 section 6.3 states\n\nUsers may assume that IPv4 and IPv6 address\nqueries for localhost names will always resolve to the respective\nIP loopback address.\n\nFurthermore, there's a draft in place to further solidify this: https://tools.ietf.org/html/draft-west-let-localhost-be-localhost-06.\nClosing.. I'd need to see the heapster logs (kubectl logs $HEAPSTER_POD_NAME) to figure much out.. might be a bug in our influx sink. this is not the appropriate forum for grafana-specific questions, but I'd note that installing plugins at runtime like that is probably a bad idea, since they will not last through pod restart.. this almost certainly indicates that the certificates on your nodes are mismatched.  Try regenerating the certificates for those nodes.  Furthermore, this is not the correct place to ask about OpenShift deployment issues.. Heapster is now deprecated, so no new features or sinks will be added.  Please see https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more information.  Please re-open this if you believe it's been erroneously closed.. you're going to have to clarify what you mean.  It's not clear from what you've posted.. please sign the CLA. if you want support, please provide more details than that.  Specifically, what do the logs look like?  How long did you wait before looking for metrics?. IIRC, at one point there was some feature where Kubernetes could collect metrics from CoreOS absent an installation of Kubernetes.  We probably need to update that part of the README.. feel free to submit a PR removing the \"and CoreOS\" part.. What do you mean by \"can't access\"?  It doesn't load?  It's not exposed on the right port?  Which guide did you follow?. I have read your post again, and I'm still unclear as to what you mean by \"can't access\".  It never hurts to be more specific.  Do you mean that it does not seem to be exposed on the right port?  If you try curling from inside the cluster to the service IP and port, does it work?  Is it an issue only with your ingress controller.\nPlease remain civil when participating in discussions or I will have to close this issue.. I believe certain sinks have that option currently, but there's no global option.  I'd be willing to accept a patch for that, however.  What do you need to get started?. It looks like there's already something similar in the --ignore_label flag.  It's not quite what you want, but it's similar.  That's probably a good starting point.. /lgtm. looks reasonable.  Can you also add support in the summary source?. Ah, my bad.  It's already there.\n/ok-to-test. /retest. yep, retest was for flaky infrastructure, actually.  Should be all set now.. cc @johanneswuerbach. Can you please try with the latest version, so that we can determine if it's still an issue?  Looking at the relevant code, it looks like the only way it could panic is if the amount of time taken is less than zero, which should never happen unless times are changing inconsistently on your cluster. No, that is correct.  Heapster is deprecated for that purpose.  You should use metrics-server instead.  It was noted in the 1.8 release notes as a required action.. ah, sorry about that.  The release notes may have gotten a bit out of sync with the plans for Heapster as things got shuffled around.. you'll naturally see such messages (which occur when something tries to access Heapster, and Heapster can't find metrics) for the first 2 collection intervals after you start Heapster (may be up to 2ish minute, or for a particular pod, after you start that pod.  Does it persist after that?. If you're still experiencing the issue, you can run at --v=10 to see exactly what Heapster is seeing in terms of responses from Kubelet, aggregation, etc.  That should give more insight as to what the issue is.. Most people seem to be using something like https://github.com/heptiolabs/eventrouter.  Eventer doesn't really get much love these days, so it's probably more worthwhile looking there.\nFurthermore, collecting Docker events would make Eventer significantly more complicated, so I suspect we'd would not be incredibly likely to accept such a patch, unfortunately.  This is probably the kind of thing that you'd want to set up separately in your monitoring pipeline.. closed as duplicate. hmmm...\nFirst things first, double check that the times are identical between all your nodes and masters (NTP, etc).  Once you've confirmed that, does anything jump out at you at log-level 9?  Look for messages about creating placeholder entries and similar.. most of the things that call Heapster do so by making a request to the API server's HTTP proxy (the services/proxy subresource).  Is the issue that you can't access that, or that the API server is configured to go through a proxy when contacting services?. Sorry for the delay\n/ok-to-test. Will merge once the test finishes. /retest. oh dear, @kubernetes/heapster-maintainers any idea what's going on with the test infrastructure?\n/retest. for future reference, why does node-problem-detector depend on Heapster?. /retest. /ok-to-test\nplease squash the fixup down into the original commit.  Otherwise looks good.. /ok-to-test\nseems pretty reasonable. yep!. hmm... first, seems we always error out if we don't get an IP address (which seems wrong), but you should probably still report a node IP on the node, even with an IP v6 address.  What does kubectl get node -o yaml show under the addresses section?. whoops, I missed a line when I was reading over the code last time: https://github.com/kubernetes/heapster/blob/e90c142b41c709bf47403e5756536ac961294f3d/metrics/sources/kubelet/kubelet.go#L323.\nWe explicitly try to convert to an IPv4 address.  That seems wrong, so we should fix that.  There's at least one other IPv6 issue with Heapster and nodes though (Heapster uses fmt.Sprintf to join host and port, which doesn't work for IPv6, since you need to surround the address with square brackets).\nWe'll need to fix both those issues to get anywhere on IPv6 support.. Closed by #1861. code in vendor is included from other projects.  You'll need to change the code there.. looks like a bug in the legacy kubelet source, which really just needs to be removed.  Please try using --source=kubernetes.summary_api=... and see if you hit the bug.  A quick glance seems to indicate that it's due to a hack put in place to extract container names. updated the description for clarity.\n/lgtm. /ok-to-test. /lgtm. /ok-to-test. /retest. In order to debug this, you're going to have to give more information than that.  Please indicate Heapster version, Kubernetes version, and Heapster configuration, as well as the results of actually trying to fetch the metrics directly.\nPlease also bear in mind that the Heapster model API is deprecated, and should not be used for new projects.. eventer is the sister project to Heapster for events.  It's more-or-less deprecated.. Also, based on that snippet, it looks like you'ver got the summary source and the influx sink, but I don't see any log sink.. Your issue is listed in the pod status conditions that you pasted in \n\nthe HPA was unable to compute the replica count: missing request for cpu on container thumbor in pod dev/dev-thumbor-4162317122-70rgf\n\nYou'll need to set a request for CPU in your pod spec for that container, like\nyaml\ncontainers:\n- image: xxx/thumbor\n  ...\n  resources:\n    requests:\n      cpu: 500m\nThat should fix your issue.. @miaoyq @Kokan awesome.  Can one of you two file a PR to update the sink-owners doc?  Remove the old Kafka maintainer, and add yourselves as the current ones.. just merged the PR.  Looks good.. @piosz we'll need to pick this over to metrics-server, if you haven't already. Please address @loburm's comments and then this will be good to go.. The Heapster model API needs to be accessed on the Heapster pod/service itself, not your main API server.  If you want to use the API server proxy, use the service name http:heapster:.  However, I would not recommend using the model API at all.  It's deprecated.. Heapster is being replaced by metrics-server.  The model API never left alpha, and we have no plans to do so.  I believe we marked it as deprecated at some point, but you're correct, the model document should probably also mention it.  Feel free to make a PR :-). closing as per comments. The model API never left alpha, and is deprecated (I need to put a large warning on the Heapster README).  It's typically because labeled metrics don't show up as normal in the model API.  If they're not showing up in you sink, that would be a problem, but it's more-or-less expected in the model API, and we have no plans to improve that.. cpu/usage is not the hard limit in millicores -- it's the cumulative CPU time used by the process.  These metrics are still useful for people who want to do their own rate calculations.  Depending on the sink, it's almost better to provide the counter metrics only, and let the sink to a better rate calculation than Heapster can.  I don't think we should remove either, or, at the very least, should just make it an option.. the best would probably be to deprecate it after a Kube release (e.g. when 1.10 releases). Deprecate in 1.9, remove in 1.10, probably, although we could leave it deprecated for another release if you want and not remove until 1.11.  I will say though, I'm not entirely convinced that this change is a good idea, having read over the original issue (see my comment there).. We no longer support the model API.  Please see https://github.com/kubernetes/heapster/pull/1898 for more details.. Hmm... I'd very much just like to say \"always use the summary API\", and just deprecate the \"legacy\" stats API usage entirely.  WDYT, @piosz @kubernetes/heapster-maintainers ?. > Until kubernetes disable cAdvisor integration, we should support legacy stats API usage or otherwise this is also a backward breaking change\nWhether or not Kubernetes has cAdvisor integration is independent of whether we support the legacy stats API.  We can certainly choose to deprecate it and/or not add new features.  Ceasing to add new features is not a breaking change.\nMy vote would be to simply not add new features to our legacy API support, and mark it as deprecated.. this is not an appropriate question for the Heapster repository.  Questions about Kubernetes itself should be directed at StackOverflow, or by reading the Kubernetes documentation: https://kubernetes.io/docs/tasks/run.../horizontal-pod-autoscale-walkthrough/. Hmm... since we're now sometimes deploying Heapster alongside metrics-server, this actually makes sense.  I think I'm in favor.  WDYT @piosz . Heapster is now deprecated, so no new features or sinks will be added.  Please see https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more information.. /retest. this is a bit confusing, but cluster usage is the sum of all namespace usage, while node usage, IIRC, is at least partially just recorded directly, so thats probably what you're seeing (just pod usage vs pod+system usage).. cc @kubernetes/heapster-maintainers . FYI, you can configure a single Heapster to dump to multiple sinks.. seems reasonable, but will wait for one of the Google folks to weigh in on the stackdriver change. merging on confirmation from the Google folks.. we'd have to make sure whatever is running the builds supports a sufficiently recent version of Docker. o_O.  Heapster shouldn't itself need those permissions.  pod-nanny probably does, but that shouldn't prevent Heapster from working, IIRC.. cc @AlmogBaku @huangyuqi @andyxning . Heapster is now deprecated, so we will no longer be adding new features or sinks.  Please see https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more information.  I apologize for any in progress sinks or features that will now no longer be merged.. yeah, there's a few different solutions, depending on what your backend is.  Someone could also try to adapt https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/event-exporter, or fork eventer.. LGTM.  Please squash the commits so that each represents a distinct feature or fix (so squash the last commit, \"Address PR review comments\"), and then I'll merge. looks like Heapster can't reach the nodes.  Make sure you're not using an overlay network that prevents this communication.. cc @piosz @kawych. First of all, please elaborate in the commit and PR description about what a hybrid cluster is, and what it is that you're doing (it appears \"hybrid\" refers to a multi-OS cluster, but you need to be clear in the description for posterity).\nSecondly, if we're going to do this, we should change all the non-GCE configs, not just the Influx one.. Might not be a bad idea.  My pet peeve with PR templates is that people tend not to fill them out properly and then leave a bunch of cruft in the commit log.... Heapster is now deprecated, so no new features or sinks will be added.  Please see https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more information.. Heapster is now deprecated.  No new sinks or features will be added.  See https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more details.. cc @miaoyq @Kokan . Cool, now that I actually understand what's going on here, LGTM. cc @miaoyq @Kokan . merging due to approval from sink owners. Do you have an overlay network set up?  Does network policy prevent pods from communicating with masters or nodes?. Generally looks good, but can you put a note in asking people to remove extraneous boilerplate information from the template (for instance, we don't want the HTML comments showing up in the commit log, and we don't want people to leave unfilled fields in their request, since it's just visual noise at that point).. I'd prefer it also have a note that said \"please remove empty sections and comments before submitting\", but other than that it's fine.. /approve\n/lgtm. If you're adding this sink, you'll need a sink owner (this may be you).  A sink owner is responsible for responding to issues, fixing bugs, and updating dependencies.  If these tasks are not done in a timely manner, or if we go to update Heapster and cannot resolve incompatibilities and the sink owner cannot be reached, this sink may be removed.\nIf you are ok with these responsibilities, please also update the sink-owners.md file with the relevant information.. Heapster is now deprecated, so we will no longer be adding new features or sinks.  Please see https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more information.  I appologize for any in progress sinks or features that will now no longer be merged.. Since you've already tracked down the issue, can you submit a PR to fix it?  Thanks!. Huh, fascinating.  Based on the initial symptoms, I'd guess something's being starved for CPU, probably either the kubelet or Heapster itself, but since you don't lose memory stats, that probably indicates a rate calculation issue (since CPU usage is actually a calculated rate metric).\nThat would imply that it's probably Kubelet that's starved for CPU, and collection is taking more than the Heapster metric collection interval.  This would in turn cause Heapster to get the same timestamps on two different pieces of data, meaning it would skip rate calculation.  This should show up in the Heapster logs if run with sufficiently high verbosity (--v=10 is pretty much the highest we care about).\n\nlower the metric resolution to various intervals, down to the minimum of 5s.\n\nPerhaps somewhat counterintuitively, this could make the problem worse if my theory is correct.  In that case, you'd actually want to increase the collection interval.\n\nDisclaimer: I can't be sure if this is heapster's fault or not, so please don't get angry if I am in the wrong spot.\n\nI do try not to bite, I swear :-P. so... that means that two different metrics from the same container have different start times listed.  Generally, it means that there's a bug in Kubelet.  The only time I've ever seen it like that before was when we had a prefixing bug in cAdvisor where it wasn't stripping off cgroup prefixes for systemd, but that was fixed a long time ago.  If you can manage to get the logs from level 10, you should see the exact responses received from the Kubelet, which might shed some light on the situation.. Oof.  Yeah, localhost should resolve to the loopback address.  Does your DNS server not respond with NXDOMAIN for localhost?  That seems strange...\nAnyway, I'm closing this be.  Feel free to re-open if you disagree.. LGTM. sounds good.  I'll cc you on a couple of issues for triage just so you can get a feel for it, and then if you're up for it we'll add you to the sink owners documentation.. I'm going to close this now, since @acobaugh is in the Influx owners file.. Does percent-escaping (URL escaping) the password work?  It probably should.  Please try that.. ok, I'm closing the issue since URL-escaping works.. merging as per sink maintainer.. Heapster is now deprecated, so we will no longer be adding new features or sinks.  Please see https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more information.  I apologize for any in progress sinks or features that will now no longer be merged.\n. merged the other PR.  Thanks, though!. cc @acobaugh. cc @acobaugh . Heapster is now deprecated, so no new features or sinks will be added.  Please see https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more information.  I apologize for any in-progress sinks or features that will not be merged.. cc @acobaugh. /ok-to-test. Heapster is now deprecated, meaning no new features will be accepted.  Please see https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md for more information.. cc @acobaugh. Sorry about that.  Don't know why @acobaugh was insufficient here.  \n/approve. It will attempt to authenticate with Kubelet the same way it authenticates with the Kubernetes API server: https://github.com/kubernetes/heapster/blob/b0a73e1c64a2787242368b55efdb542a632ef726/metrics/sources/kubelet/configs.go#L67, so make sure that you've set up Kubelet to accept similar auth.. cc @cblecker step one.  What's step 2?. Currently, I intend all reviewers to also be approvers (effectively, that's currently the case, except I just manually hit merge and leave a comment to the effect of \"merging on LGTM from maintainer\").  We can always change it later.. I don't need to manually duplicate the list of approvers to reviewers, right?. > Yes, you do. blunderbuss (the plugin that assigns reviewers) doesn't look at the approvers section\nAh, ok.  Thanks!\n\nYou should be able to open a PR against k/test-infra with all of these in the same PR\n\nAwesome, thanks for all your help!\n. Don't know why this didn't auto-merge.  Manually merging.. FYI @kubernetes/heapster-maintainers @AlmogBaku @andyxning @huangyuqi @burmanm @mwringe @miaoyq @Kokan @bluebreezecf @jamtur01 @mcorbin @jsoriano @theairkit @ezeev @johanneswuerbach @emfree @yogeswaran \nYou should now all be approvers on the relevant sinks and/or the root of the repository.  You're all also listed as a reviewer on the relevant parts of the repo.  Let me know if there's a directory that you think you should own that you're not listed in.  From now on, you should be able to /lgtm and /approve.\nIn the sage words of Uncle Ben: \"with great power comes great responsibility\".. /lgtm\n/approve. > @DirectXMan12: you cannot LGTM your own PR.\nWhat now, robot? :-P. can't really help you without more details about your setup, Heapster logs, etc. closing as per the above comment. check the times on your nodes.  Make sure they're all in sync.  . Are you referring to the subpath volume CVE?  That doesn't affect clients, I believe.. Heapster is now deprecated, so we will no longer be adding new features or sinks.  Please see https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more information.  I apologize for any in progress sinks or features that will now no longer be merged.. They seem to pass for me, on the master branch.  Please try again with master and go 1.10, and report back if there is still an issue.. Heapster is now deprecated, so no new features or sinks will be added.  Please see https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more information.. Heapster is now deprecated, so no new features or sinks will be added.  Please see https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more information.. are you trying to build Heapster with a different version of Prometheus than is listed in the vendor directory?  That's not supported.  Please reopen this issue if that's not what's going on.. is the shell interpolation being done?  I suspect that's the issue.  I don't think there's anything that'll do shell interpolation unless you explicitly launch a shell which launches Heapster.. /approve\n/lgtm. /hold\nuntil the other PR merges. Closed by https://github.com/kubernetes/heapster/pull/1849. No, I don't believe that is supported.  Heapster is now deprecated, so we will no longer be adding new features or sinks.  Please see https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more information.  I apologize for any in progress sinks or features that will now no longer be merged.\n. Heapster is now deprecated, so we will no longer be adding new features or sinks.  Please see https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more information.  I apologize for any in progress sinks or features that will now no longer be merged.. It comes from this line here: https://github.com/kubernetes/heapster/blob/784a4b55e34060b622acc9442b0bd454644f5732/Makefile#L71.. EDIT: whoops, replied to that on autopilot too quickly.  Sounds like this is caused by cronjob pods lasting for a shorter period of time than the Heapster scrape interval.  This is a fundamental limitation of the way we collect metrics, so it's unlikely to be fixed.  You can lower your metrics collection interval to 30s, but that's about it.. cc @kubernetes/heapster-maintainers @kubernetes/sig-instrumentation-misc @brancz @piosz. /hold\nto make sure we don't accidentally merge this before people have a chance to voice concerns.. P.S. I've written up the deprecation timeline as a draft.  I'd like it to be similarly aggressive to what's listed, if possible, but I'm open to suggestions.. reviewers/approvers, please take a look:\n@acobaugh, @almogbaku, @andyxning, @bluebreezecf, @burmanm, @crassirostris, @emfree, @ezeev, @huangyuqi, @jamtur01, @johanneswuerbach, @jsoriano, @kawych, @Kokan, @loburm, @mcorbin, @miaoyq, @mwielgus, @mwringe, @piosz, @theairkit, @x13n, @yogeswaran. > For instance, Heapster is currently a go-to monitoring agent for Stackdriver.\n@brancz beat me to the punch here, but I'll follow up with extended thoughts.\nStackdriver using Heapster is all well and good, but, IMO, that's not a good reason to keep Heapster around as a maintained Kubernetes project -- it's a good reason for GCE to keep around a Heapster fork until they can migrate to a different monitoring solution (or really as long as they want).\nOne of the key observations of the new monitoring plan was that the Heapster model of development (vendor dump) is unsustainable.  Even apart from that, the Heapster codebase needs significant love.  It has serious design issues (ask me about these later, if you're interested), years accumulated cruft with no real policy around what we can remove, and serious usability issues (I'd be happy to expand on these later as well).\nFurthermore, non-deprecated Heapster poses problems from an organization perspective.  People continue to request that we add new sinks, and with the current policies, we don't have a good reason to say no.  It's unclear if this is because they believe that Heapster is still the recommended solution for monitoring Kubernetes (it's not), because they want their monitoring solution to reach a larger audience (which is, in-and-of-itself, also a problem -- see below), or because they don't want to maintain collection code.\nOn that subject, from a Kubernetes perspective, we want people to invest in a healthy monitoring ecosystem with solutions designed to work with their tools from the beginning, as opposed to shoehorning the tools into Heapster's architecture -- we have a number of PRs like \"this specific sink has issues with this way the metrics are structured in Heapster, so we need to work around it\".  I'm not try to cast a bad light on our sink maintainers -- they do a wonderful job with the resources they have, but it's tough trying to keep things working, between the core Heapster code and the sink-specific hacks.\nAs an added benefit, this'll help ensure that, as we move forward with features that require use of the new monitoring architecture, we can be more certain that distributions have either switched over to using it, or made a conscious decision to maintain their own copy of Heapster.\nTo summarize: one of the main reasons for the new monitoring architecture was to remove Heapster as a bottleneck to development, and to ensure more maintainable code.  Keeping Heapster around as  a maintained Kubernetes project hinders that effort.. Let me try and address a few of the above comments, mainly about the \"suddenness\" of this move:\n\nAgreed that we need also promote metrics-server to kubernetes organization alongside the deprecation of Heapster if we do need to deprecate Heapster.\n\nIt's reasonable to want to promote metrics-server from kubernetes-incubator to somewhere (most likely the kubernetes organization, but we'll have to figure out what the current policy is on that, and whether the kubernetes organization is the right place).\n\nIt also seems to me that keep the scrape and sink ability for Heapster should be ok.\n\nOne of the key reasons that we want to deprecate Heapster is precisely that the sink functionality is unmaintainable in it's current form.  Keeping the scrape and sink functionality would defeat on of the major purposes of deprecating Heapster, as it would continue to support that unmaintainable code, and continue to perpetuate the idea the adding new sinks to Heapster is the recommended way of getting metrics to you sink.\n\nIMHO, Heapster deprecation is too sudden\n\nPlease let me know how this could have been communicated better.\nWe've be discussing the possibility of deprecating Heapster basically since we merged the new monitoring vision (a few releases ago), we've been encouraging people to move to metrics-server with steadily increasing intensity over the past few releases, and this monitoring timeline delays the actual removal .  Even conservatively saying that nobody realized that metrics-server was the future until we made use of metrics-server default in Kubernetes for the HPA (in 1.9), that still gives 1.9, 1.10, 1.11, and 1.12, plus the development cycle for 1.13 to figure out what each cluster/distribution/admin wants to do instead.\n\nThere are no simple and easy setup guides for metrics api, metrics server and third party tsdb servers pipeline available to end users to learn and try for now which is a big gap between Heapster.\nBTW, what is the replacement for eventer. People may still be using it.\n\nI don't recall the name, at the moment, but last time I checked, there was a recommended event exporter that was more stable and robust than eventer.  I'll try to find it later.  We've been unofficially recommending against using eventer for a while now.\n\nBut maybe we should think a way for Heapster to support a plugin mechanism just like out-of-tree cloud provider support or different device support with device plugin instead of kicking it off and setup a new one.\n\nBut, that doesn't really buy us much.  If, for instance, we agree that either way we should deprecate serving APIs, we get:\n\n\nthe basic discovery of nodes and node IPs.  This logic isn't actually super-complicated to start out with, but could be pulled out into a library.\n\n\nsome scrape logic with translates the relatively straightforward structure of the summary API to a format which vaguely makes sense internally to Heapster, but is fairly complicated.\n\n\na bunch of processors that transform data.  Most of this transformation is either a) no longer needed (it augments with additional data in ways that aren't necessary any more, or aren't recommended) or b) performs ahead-of-time aggregation, which goes against best practices for many modern monitoring pipelines (you can always aggregate later, but you can't \"unaggregate\" preprocessed data)\n\n\nsome rather complicated interval logic that doesn't quite do what it says on the tin, and has been the source of some subtle bugs\n\n\nsome boilerplate setup code which is way more complicated than it needs to be\n\n\nOf those bullets, I think the only one that's actually worth keeping around is the first bullet point, and that could just be abstracted out into a small one-file library.  Ergo, I don't think that just trying to move all sinks out of tree would be a useful exercise. \n\nThere are no simple and easy setup guides for metrics api, metrics server and third party tsdb servers pipeline available to end users to learn and try for now which is a big gap between Heapster.\n\nPutting custom-metrics-based-autoscaling aside (since Heapster doesn't do that either), I'd dispute this assertion.  For metrics-server, there's the in-repo deployment manifests, linked from the official documentation (https://kubernetes.io/docs/tasks/debug-application-cluster/core-metrics-pipeline/), plus the manifests in the cluster addons directory, both of which are fairly straightforward to use (kubectl create -f /path/to/manifests).\nFor third-party monitoring solutions,there are extensive guides on how to set up Prometheus on Kubernetes, from a myriad of sources, with a myriad of perspectives.  There's even the Prometheus Operator, to make it even simpler.  Prometheus aside, there are other open-source and close-source solutions for cluster monitoring.\n\nAlso there are some existing add-ons or functionalities depends on the data from Heapster. What is the procedure to help those components migrating to the new pipeline smoothly.\n\nPlease be more specific.  kubectl top and HPA no longer need to use Heapster.  Last I heard, the dashboard team was at least aware that Heapster was not a long term solution, and was thinking about alternatives.  I don't know of any official kubernetes-organization projects that are using Heapster.  Other projects that use Heapster will have to consider if the existing metrics APIs are sufficient, or if they need to talk directly to a monitoring pipeline, or propose a new metrics API, or an extension to an existing one.  I can't make suggestions on those, though, without a concrete example.\n\nYeah, our current ongoing Poseidon/Firmament scheduler effort leverages Heapster real-time metrics for workload scheduling. We created a new sink for pushing metrics information out of heapster to Firmament scheduler.\nAs Dujun mentioned, we were about to initiate new Poseidon sink upstreaming process in the next couple of days.\n\nThat's unfortunate, but we've been mentioning that Heapster was not a reasonable path forward for a while now, and this very incident underscores the need to official deprecate Heapster to communicate that it should not be counted on as a path forward.. > Add the deprecation summary for Heapster also to the Kubernetes release v1.11 release note and update the deprecation info for Kubernetes release v1.12 and v1.13. Most people will try to read the summary release note for Kubernetes to learn the trends for Kubernetes. So, i think it is also a good place to make the heapster deprecation widely known.\nYes, definitely!\n\nAdd a detailed setup docs for metrics api, metrics server and third party monitoring system to learn.\n\nI'd love to improve the docs.  Can you put together a specific list of complaints you have with the official setup docs in the Kubernetes documentation?\n\nAdd the replacement binary for eventer in case some people still using it.\n\nWe should definitely link to that somewhere.  Does anybody remember exactly what it was?  I can't recall.  I'll need to track it down otherwise.\n\nFor the influxdb case, I believe telegraf with its prometheus input plugin ought to be able to replace the functionality of heapster by pulling metrics straight from metrics-server.\n\nIt shouldn't be pulling from metrics-server.  It should query the nodes instead.  At any rate, we should have follow-up PRs to build up a list of alternatives for the third-party monitoring solutions.\n@piosz any objections to this timeline.  Otherwise, I'll plan on merging by the end of the week.\n. Since there are no further maintainer objections lodged, I'm going to merge this.. It differs based on sink.  I believe there was a fluentd event exporter at one point, and there's now a stackdriver event exporter, plus what appears to be a more generic one at https://github.com/alauda/event-exporter (not endorsing that one, but it does appear to be a potential solution).. There's also Heptio's eventrouter, it appears: https://github.com/heptiolabs/eventrouter. Heapster is now deprecated, so we will no longer be adding new features or sinks.  Please see https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more information.. Sounds like you haven't correctly set of the auth for the nanny.  Does the service account have the right permissions, etc?. kubectl get roles and kubectl get rolebindings.  Please read up on cluster roles in the official Kubernetes documentation. /ok-to-test\n/lgtm\n/approve. You don't have RBAC rules set up to allow unauthenticated users to proxy to the grafana service.  Please read up on RBAC in the official Kubernetes documentation.  You'll need to allow anonymous user GET the services/proxy resource for the grafana service.. I apologize for hastily closing this issue, I did not intend to be rude.  I often get 40-100 new/updated issues a day that I have to triage, and leaving issues open after I think they've been answered just makes triage later more difficult.  Please feel free to reopen issues if you think they were closed incorrectly.\nOn the subject of docs updates, can one of the influxdb owners (@andyxning @acobaugh) please update the docs to mention the trade offs in allowing unauthenticated access to grafana?. > Am I not able to re-open the issue. There's no reopen button.\nAh, ok.  Let me do that, then.. Heapster is now deprecated.  No new sinks will be added.  See https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md#initial-deprecation-kubernetes-111 for more details.. /remove-lifecycle stale. /ok-to-test. merging, since this is a bugfix.. /lgtm\n/approve. Hold on, why did this merge?.  Heapster is deprecated.  No new features are supposed to go in.  This is a new feature.  Ergo, it shouldn't have gone in.. This seems like it's adding something new to the sink.  The idea is that if something that exists doesn't work, we'll fix it, but otherwise, no new code.  Feel free to elaborate with more details, though.. > In my mind, this was a missing piece of #1891 where these metrics were added in the old resource model but don't work in the new resource model. But I see your point as well.\nI see your point, but that line of logic could be used to justify a lot of new code.  I suggest that you fork Heapster if you want to add anything else along this line.\n\nThe heapster deprecation timeline says Heapster is deprecated from 1.11 which is not released yet\n\nHeapster was deprecated officially as soon as we merged the deprecation PR.  The 1.11 on the timeline was intended to indicate that it was deprecated \"as part of the 1.11 development cycle\".  Apologies if that wasn't clear.\n\nSo, I think we need to make one final release of Heapster from the master branch that will go in 1.11.\n\nOur existing policy so far has been that no new features will merge into the 1.11 cycle releases of Heapster that weren't already in place by the time the deprecation PR merged.  There were a few features merged before the deprecation PR was in place, so those'll be in the next release.. > Specially because the expectation is that deprecations are aligned with the Kubernetes release schedule\nI'll try and clarify in the doc, but Heapster releases haven't traditionally been officially tied to Kubernetes releases.. Please provide more information about your setup, including your Kubernetes pod manifest.. @damianoneill forgive me if I've missed something here, but what's the expected result?  Network can only be measured on a per-pod basis (with cadvisor, at least), since it looks at the interfaces in the network namespace, and all containers in a pod share a netns.. Heapster is now deprecated, so no new features will be accepted.  Please see https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md for more information.. /ok-to-test. you're... going to have to be more specific here.  That's not a lot of information to reproduce.  How did you set up email alerts?  What sink are you using?. cc @andyxning @acobaugh . @kubernetes/heapster-maintainers does anyone know what happened to the newer grafana images?. If this is a bugfix, what's the actual bug here?  What was the issue being caused?. Isn't that specific to the stackdriver sink, though?  Shouldn't we just have a workaround there?  It feels heavy-handed and hacky to add an entire new option and pull this from a completely optional label.  Kubernetes did remove that field for a reason, and I don't think it's a particularly good idea to hack it back in in the general case in Heapster.. Hmm... I'd argue the fix should have been to remove the HostID label, then, since Kubernetes removed the HostID field.  Does that cause significant breakage?\nSeparately, if a change is adding a new flag, there should be discussion, or at least time for discussion.  Merging within a couple hours of the change landing on GitHub is not acceptable for Heapster-wide changes.  It's fine if you want to do that within the Stackdriver sink, but for this kind of thing, other people should have a chance to comment.. > Sorry for that, I considered it a small change with no effect on performance and transparent to anyone that doesn't depend on HostID label.\nNo worries, I'm just more concerned about new config flags, etc.\n\nWrt. to removing HostID label completely, I wouldn't consider it as an option because it's already used by some (Stackdriver) sink. It does cause significant breakage for Stackdriver integration, i.e. all node metrics have now indistinguishable labels\n\nhmm... could we possibly choose a replacement to plumb through?  Is NodeName insufficiently unique there, or is there some wrinkle in stackdriver that makes that not work?. Ok, since this is already in, and that's a reasonable explanation, I think it's fine to keep it as-is.  As much as I would like an alternate path, I also don't want to cause too much of a headache for you guys.  In the future, please include that explanation in the PR as well.. What's going on with these PRs?  They don't seem to have a bug attached. Can you clarify the actual issue?. replied in #2057, lets continue the convo there.. CPU usage (i.e. not cpu/usage_rate) is measured over time -- it's the amount of time that the process was running on the given processor, so if your pod was running for at 500 millicores for 2 seconds, you'd have a usage of 1 second.  IIRC, usage is actually measured here in nanoseconds, so large numbers are reasonable  If you're seeing this on usage_rate, then you have an issue.. I'd be happy merge a PR with that documentation update, since, yeah, that's poorly worded.  In general, though, we're in the process of deprecating Heapster, and the replacements have working that make it a bit more obvious, IMO. /ok-to-test\n/lgtm. I'd have to check exact compatibility, but I would not recommend it.. cc @andyxning @acobaugh . I'd guess those are pseudo-containers (cgroup hierarchies) being created by systemd, and cadvisor is picking up on those and exporting them.\ncc @acobaugh @andyxning is there an influx mechanism to deal with this, or filter these out?. SGTM.. Generally, when Heapster can't connect to your nodes, it's a problem with your networking set.  More specifically, certain networking vendors don't allow pods to connect to node IPs by default, and you have to manually allow it.  Unfortunately, there's not much advice I can provide here, since it's highly specific to your networking setup.. > So there were actually a couple of things going wrong but it is worth pointing out that heapster requires kubelet to have its readOnlyPort set to 10255\nAlternatively, you can just tell Heapster not to use the read-only port.  There's flags on the source to do this -- see https://github.com/kubernetes/kubernetes/pull/65102.\nSorry for the initially terse response -- we end up getting a lot of issues that boil down to \"My CNI plugin/overlay network blocks pod<->communication out of the box\".\n. It's been the default in at least a couple of distros for a while now, but hasn't been in upstream kubernetes for various reasons.  It'll be the default in metrics-server soon.. /lgtm\n/approve. @jsoriano @theairkit . There's also something like Heptio's eventrouter.  Don't know too much about it, but it seems like it might do what you want.. Closing this PR. unfortunately, I'm not super-familiar with the grafana setup.  Are there any logs in the grafana pod?. Unfortunately, Heapster is now deprecated, and is thus closed to new features.  See docs/deprecation.md for more information.. It looks like your cluster is set up do RBAC on the Kubelets, but you've disabled inClusterConfig, so it's not submitting credentials to the Kubelet.. >  don't know about your disabled cluster configuration, so I hope you can help me figure out how to configure it.\nYou've passed inClusterConfig=false to Heapster, which means you're not trying to authenticate with the Kubelet.  Try switching that to true.. /kind bug. Unfortunately, Heapster is in the last stage of its deprecation, and is slated for removal this release of kubernetes.  Thank you for your contribution, though!. cc @piosz @brancz @kubernetes/sig-instrumentation-misc . @AishSundar \"retiring\" means moving to kubernetes-retired, no longer fixing bugs, adding features, or building new releases (Heapster doesn't quite follow the release cadence of Kubernetes).  Existing images will continue to work until they don't, but we provide no guarantees when that is.\nAs for migration: all new installations of Kubernetes using the recommended setup options in the Kubernetes repo are using metrics-server instead.  We've been mentioning this in community meetings and Kubernetes release notes, and have had the deprecation notice up for several releases now, so hopefully, this hasn't caught anyone off guard.\nAs for the dashboard, it's unclear what the story is beyond the immediate implications.  The dashboard could continue to consume Heapster until it no longer works, but once Heapster is retired, there's no guarantees about compatibility.  I know the dashboard team has been aware of this change for a while now, but I haven't really heard anything on their path forward.. /lgtm\n/retest. @loburm @piosz @brancz  can one of you LGTM?  Also, I think the e2e failure is from running different versions of the Go between tests when running gofmt.  I'm disabling that test for now, since this should be literally the last PR we'll make to this repository.\n. The \"max\" value is also important here, because it lets you see if there was any activity at all (not particularly useful for things like CPU and memory, but for net, or certain custom metrics like hits per second, it could be used to determine if the pod was useful)\n. Also, long-term stats are useful for things like idling (e.g. \"did this metric have a non-zero value over the past 24 hours ever?\")\n. :heavy_check_mark: \n. :heavy_check_mark: \n. Fair.  I like \"historical\" as well. :heavy_check_mark: \n. I was trying to keep it \"readable\".  IMO, \"perc\" is more readable than \"pct\".  I'd prefer either \"perc\" or not abbreviating it.\n. :heavy_check_mark:\n. Metrics pushers can't overwrite/interfere with each other, since each metric is stored as prefixed by pusher's name, as determined during the auth step, unless you had multiple sources that shared the same name but were not otherwise coordinated.  That scenario seems like something that should just be avoided in this case.  I can make that explicit if need be. \nIn general, I imagined that the use cases for this are more designed for either cluster-admin-created components, or admin-blessed components, and thus some of this would be a case of \"does the admin trust themselves\".\nNonetheless, I could be desirable to limit metrics per pusher, pushes per second, etcd.  It would be fairly easy to do so on a global scale, but I could see a case for having per-pusher controls on those variables.  The question becomes how complicated we want to make this.\n. So, AFAICT, the pull metrics on Heapster use the time provided by the kubelet APIs (one of the Heapster core devs can feel free to correct me here :wink:).  I don't really want to have one part of Heapster using time from the metrics source, and another part using time from Heapster.\n. @mwielgus ok, sounds good.  I'll remove the timestamp bit then.  Thanks!\n. Sure, I'll add in an example.\n. Yes, it's a bulk push.  Labels are used to indicate which metrics go where.  I talk about that a bit below.  I can clarify a bit, though.\n. So, while I mainly chose the prometheus format since it's really pretty easy to write from just about any language (including shell scripts), I do like the idea of actually supporting the Prometheus push gateway URL as well.  My main issue is that the URL format (/metrics/job/{jobname}) doesn't really fit with the rest of the Heapster API (which looks like /api/v1/model and so on).  I would not be averse to the Prometheus format handler accepting something like /api/v1/push/metrics/job/{jobname}, but I'm not sure if the Prometheus clients would be able to handle it (and thus if it would be worth trying to implement).\n. Kind of -- I don't want anyone to be able to accidentally (or purposefully, for that matter) overwrite someone else's metrics by using the same name.  The current auth framework extracts the common name from the client cert to check against a list of allowed users.  This proposal would have that name be passed on, and all metrics submitted would be prefixed with that.\n. So, I'd definitely support Hawkular, and I'm also willing to write the initial support for other sinks that people think are important.  I've tried to make sure all the features talked about in the proposal work across InfluxDB, Hawkular, GCM, and OpenTSDB (the main metrics sinks, AFAICT).\n. The goal is to have a set of features that all the current metrics sinks support, and which seem likely for any future metrics sinks to support (if you think I should trim of any features in favor of this goal, please let me know).\n. I'm definitely open to changes here.  Were you thinking something like:\n``` go\ntype MetricAggregationResultList struct {\n    Items []MetricAggregationResult\n}\ntype MetricAggregationResult struct {\n    Average MetricPoint\n    Maximum MetricPoint\n    ...\n}\n```\nThat seems reasonable to me.\n. Whoops, that last comment needs bucket support:\n``` go\ntype MetricAggregationResultList struct {\n    Items []MetricAggregationResult\n}\ntype MetricAggregationResult struct {\n    Buckets []MetricAggregationBucket\n    // this below isn't strictly necessary, but puts the result into context nicely\n    BucketSize time.Duration\n}\ntype MetricAggregationBucket struct {\n    Average MetricPoint\n    Maximum MetricPoint\n    ...\n}\n``\n. Alternatively, the timestamp could be moved into theMetricAggregationBucket` type,\nand have each field just be a numeric value.\n. > Although I would introduce start/end time to MetricAggregationBucket\nAFAICT, Influx doesn't actually provide an end time -- it just has a timestamp (otherwise I would have).\n. So, you'd probably want to run it separately from Heapster so that you could scale it, but we might want to also provide an easy option to run it as part of Heapster as well (some sort of \"allinone\" installation for quickly getting up and running).\n. Listing the available items is to be roughly in line with the Heapster model API.  In general, I think both APIs should follow the same conventions.\n. I was envisioning Oldtimer always querying the storage.  Otherwise, how will we know if \"everything\" is in memory?  Things could have changed since our last fetch.  The way things are tagged in the sinks shouldn't make it too bad to list all pods for a namespace, I would image, although I'd welcome additional data/feedback there.\n. Sure, I'll add in a brief overview of the differences.\n. They're all specified slightly below (after the definition of {prefix}), but if that's not clear, I can more the definitions around a bit.  An example probably wouldn't hurt either ;-)\nHourly 95-percentiles from the last 30 days: GET /api/v1/historical/namespaces/default/pods/somepod/metrics/cpu-usage/95-perc?start=2016-03-20T10:57:37-04:00&bucket=1h\n. Sounds good to me.\n. It looks like the Go client and the Python client can somewhat accidentally support a having /metrics/job/{jobname} at a subpath, while the Python client, for instance, does not really support this (the gateway argument is only used for it's host and port, not the path).\nSince I'd really like to preserve the base Heapster API path format (/api/v{version}/{api_name}/...), I'm not sure it's worth it to try to support the Prometheus push gateway API.  @brian-brazil do you have any thoughts on this?\n. Yeah, if it's running in the same process with Heapster proper, it could just use the info from Heapster proper.\n. ah, ok, I think I misunderstood you above.\n. That would effectively mean that it would have to be on a different port.  Why not just allow it to run in the same executable, if you're going to be running it as part of the same pod anyway, so that https://heapster.kube-system/api/v1/model is for the model, while https://heapster.kube-system/api/v1/historical is for oldtimer?\n. I'll tweak the wording here.  I figured since there was an API version in the path, the intention was that you'd have a versioned API at some point.\n. :heavy_check_mark: \n. As far as I understand it, the node team did not really want Kubelet in charge of collecting custom metrics.\n. Presumably, even with the prefixing, the admin probably still does not want to allow such behavior globally, even if it's just for cosmetic reasons.\n. As I noted above in response to your earlier note, I would like to support the Prometheus Pushgateway's URL scheme (and I wouldn't mind supporting protobuf either, I just wanted to limit the initial scope).  However, I do not think it's a good idea to have one URL that looks completely different from the rest of Heapster's APIs, so it would have to be something like /api/v1/push/prometheus/metrics/jobs/..., and AFAICT, a couple of the Prometheus clients seem to support a URL prefix, while others do not (IIRC the Ruby client looked like it did not), and the specification in the Pushgateway repo does not seem to allow it either.\nErgo, I wasn't sure if it was worth it to include it at a subpath, so I left it out barring further comments on the matter (see https://github.com/kubernetes/heapster/pull/1129#discussion_r60298969).\nI'm certainly willing to add it in as a subpath, and I'm also willing to change the format (for instance, we could do a JSON format instead). \n. The next set of metrics is an example of a service-level metric.  As I mentioned above, this is for cluster components -- sources producing metrics tied to other pods/services/namespaces/etc.\nCurrently, we have a method for individual pods to expose information about themselves (the cAdvisor-Kubelet-Heapster-Pull chain) This proposal would allow exposing metrics about other entities (pods, services, namespaces, etc) in the cluster.\n. When I mentioned \"cluster-scoped\" above, I mean metrics not describing the producer itself (as I note on line 171).  Those could be pod-level, node-level, etc.\n. I meant more of \"you don't just have the option to push whenever you're ready, instead, you have to have a daemon always running\".  I'll reword a bit.\n. Ah, ok, I misunderstood what you were suggesting.  If you didn't want to use a hostDir, you'd have to have the daemon looking for a particular emptyDir, or something of that sort.  Is that what you had in mind?  That seems more complicated to me.\n. With the main proposal, since producers already need to authenticate with Heapster, we get it for free -- you could potentially just say \"whoever has the certificate that allows them to authenticate is allowed to push\".  Since cAdvisor doesn't currently check certificates AFAIK, you could not use the same mechanism with cAdvisor -- you'd either have to teach cAdvisor to check certificates (which seems like tying cAdvisor closer to Kube than it wants to be), or set a list of pod names (or some sort of other identifier) at the Heapster level.\n. I apologize if that was not clear.  I'll reword a bit and make it clearer.\n. :heavy_check_mark: \n. :heavy_check_mark: \n. Just a type alias around string?  Sure.  I often think that Go could use a proper enum type, though...\n. I'll add in documentation to the interface, good catch.\n. (but yes, they can have zero/empty values, which indicate no bounds for time, and to use only a single bucket for bucket size).\n. In Influx, it's the start time IIRC, so we should stick with that for consistency.  I'll add a note in.\n. This tests turning the JSON returned by InfluxDB for a  \"query row\" (which is actually a set of results) into internal Heapster form (both for the normal queries and the aggregated queries), which represents a decent chunk of the logic in the historical implementation for Influx.  Not tested here is the \"turn these parameters into a query string\", but I should add some tests for those as well.\n. I can write some if desired.\n. ack, will do.  It's the UID of the pod (as recorded in the pod metadata in Kubernetes).  I'll just change the text there to say UID.\n. The aggregations documented in the proposal.  Do you want that documented elsewhere, or a subpath which just returns that list?\n. For Influx, key values are checked in checkSanitizedKey above, and an error is thrown if anything besides /^[a-zA-Z0-9_.-]+$/ is found.\nEach of the sinks is individually responsible (instead of sanitization being at the API layer) since some sink driver will support bound parameters, which are more flexible.\n. It is disabled.  I've added a note.\n. aggregationName values are checked against a fixed list at the API level (see getAggregations in metrics/api/v1/historical_handlers.go:694)\n. Do we need to get definitions?  Isn't the name that the driver stores metrics under effectively a function of the metric key (or is the goal here to support changing the naming scheme in the future)?\n. This concerns me.  We should decide what zero means across all drivers (8 hours, since the epoch, etc) and always set it correctly, or decide to prevent unspecified start times at the API level.  I don't want different drivers having different results when we can avoid it.\n. Nit: please avoid all-acronym variable names like dps and mds when you can, since it generally makes code much harder to read at a glance (in this case, you could use something like metricDefs and datapoints instead).\n. This... is suboptimal.  We should probably just compose a map based on AllMetrics  in core (e.g. core.AllMetricsByName) or have a function, since looking up metric descriptors by name is a useful thing to be able to do and having to loop over all metrics every time is not the best.\n. How do custom metrics factor in here?  If, in the future, we have a way to have custom metrics of different types (e.g. push metrics), how would that be dealt with here?\n. can you elaborate on what the question is here?\n. can you add docs to this method to explain what it does/why it's necessary, please?\n. Please drop a comment here about what isNamespaceTenant is doing here\n(something like // if we're also separating namespaces by tenant, make sure to also filter on the tenant name, if I'm reading this correctly)\n. what happens here if we have some tenants that don't represent namespaces (e.g. this Hawkular setup is being shared with other metrics sources), or is such a setup not generally allowed?\n. Ack, that makes sense.\n. Yeah, I think that will probably avoid the most issues here.  I'll update my original PR.\n. I'll add it to the Oldtimer PR.  I see it being useful across different sinks, etc.\n. So, currently in my push metrics prototype, I just use one type (gauge, IIRC), but that's mainly because the Prometheus ingester API doesn't expose the type annotation in the right places.\nEventually, I'd like the be able to respect the annotations on the pushed metrics, so that gauges are stored as a gauge, and cumulative metrics are stored as cumulative.  I'll have to think a bit about how to resolve that.\n. actually, can you just add it in a separate commit in this PR, but in the same place that core.AllMetrics is?  That way there's some context, and not just a change in the Oldtimer PR that does not appear to have justification.\n. As in placing the configuration in a separate file?  While I'm not necessarily against that, I suspect that most of the auth mechanisms would work fine with just a \"URL\" configuration (note, this doesn't necessarily mean that the authentication or authorization is based on a remote HTTP call -- Heapster just uses a quasi-URL scheme for specifying configuration of sinks and sources, so this would probably work fine here as well).\n. that's just an example.  It's difficult to track in the current setup, but I can envision a future in which we can somehow identify the pod pushing, and I'd like to be able not have to change the interface of the auth mechanism (this is mainly the motivation for including the whole \"key\" to the metrics, and not just the namespace -- it allows us the option to make more complex decisions like this, should we so choose).\n. Somewhat.  The motivation comes from several related issues:\n1. We want to be able to restrict who can push metrics, read recent metrics (via the model), and read historical metrics (via Oldtimer) -- it's desirable to allow different sets of people to have permissions for those things.\n2. We potentially want to be able to restrict who can push metrics associated with certain \"objects\" (e.g. you might want some to be able to push metrics for any cluster objects, while other might be only allowed to push metrics in their own namespace).\n3. We seen concern over the fact that you cannot restrict reading of metrics to a certain scope for different users.  A mechanism to restrict users to reading things only in their own namespace, for example, would allow Heapster access to be more \"safely\" granted to different applications and users on the cluster.\n. Ack.\n. The decision is mostly on the \"prefix\" vs \"no prefix\" -- it could allow for a extension to push metrics where certain applications are trusted to push metrics without the \"custom/\" prefix, for example (which could be desirable for certain users).  I think it probably makes sense just to drop down to just having the prefix specified.\n. It's mainly for convenience of the auth mechanism implementation.  A common case would probably be \"is the user authorized to push metrics prefixed with the given prefix\" (e.g. generally, users are only allowed to push metrics prefixed with their name, but some users might not have that restriction).  If you pass the prefix separately, you can do 1 string compare.  If you don't pass the prefix in, you have to run through the whole list of metrics separately. \n. Partially -- create is POST, but the URL doesn't say anything about the particular metrics, resources, etc involved (and multiple metrics and resources may be involved).\nQuery is GET, and the URL does say something about the resources involved.\nHowever, that seems like a detail for the particular auth mechanism.  An authorization mechanism like the current one (just using a list of user names) probably doesn't work in terms of HTTP verb -- it just wants to know which action broad (create vs query model vs query historical) is being performed, so it can check the list of users allowed.\n. Exactly.\n. Yeah, that was the plan (that's how the Kube auth works, right?).  I'll make it clearer in the proposal\n. Note here that not all information is present for all key types -- for pod queries for instance, either NamespaceName and PodName are present, or PodID is present.  Node is used for node-level metrics and system-container metrics (which also use ContainerName).\n. The Oldtimer PR as it went in requires start time (and end time is actually set at the API level to be time.Now()), so you should probably amend this comment to say \"... but this is prevented at the API level\"\n. > IF heapster was not designed to be multitenant from the start, and I don't think it was, this is going to be whack-a-mole for a long time. \n\nThe alternative is to run a heapster per namespace. This solves both authz and qos problems. If you have a free tier where you can't afford to run heapster per namespace, you can use a shared instance, but restrict access to it.\n\nDoes the Heapster multitenancy story need work?  Definitely.  Does that mean we shouldn't work towards it?  No. \nIt's not just a \"free-tier\" problem -- if you have a lot of paying users, or you're running a company wide cluster with a lot of different consumers, etc, it's going to be a problem.  With Heapster-per-namespace as the auth solution, you're basically say \"if you're running a small cluster, you get fine grained auth, otherwise \u00af_(\u30c4)_/\u00af\".\nIf you have a bunch of namespaces and heapster-per-namespace, get problems of:\n- load (kubelet has to serve many more HTTP requests, Heapster sinks have to be able to handle many more writers)\n- extra work for Heapster consumers (pushers now have to divide their metrics by namespace, discover the Heapster in each namespace, and make a request to each Heapster, and the HPA now has to know how to, and be permitted to, talk to many different Heapsters)\n- user-friendliness (there's now a RC in every namespace not under each user's control), \n- admin-friendliness (you now need a component to auto-provision each namespace with a Heapster, provide secrets to talk to the backend, etc)\nIt also entirely sidesteps the problem of making auth easier for the user -- plugging into something like the Kube RBAC system makes it easier to manage your permissions, and easier for users to auth -- you don't have to have a side-channel for distributing certificates, etc.\n. Yeah, that's more or less what I was envisioning.  I can add a couple of examples to the proposal.\n. When you're querying historical metrics, there's no guarantee that you'll have that info (you may have been restarted, etc).  It's possible for model metrics, but the model doesn't support UID queries.\n. probably just reject -- we don't want someone to be able to DoS the cluster and or Heapster like that in a case with a lot of namespaces.\n. The intended pattern is/was nginx pushes against metrics with total-requests.  Heapster checks if you're permitted to push unprefixed metrics, and if not, sees that you're permitted to push prefixed metrics, and thus prepends custom/$USERNAME/.  Also, $USERNAME needs to have permissions allowing it to create pod/metrics\nAlternatively, \"trusted system component X\" may be allowed to have unprefixed-metrics permissions.  \"trusted system component X\" pushes metrics, Heapster sees the permission for unprefixed-metrics, and uses the permissions as-is.  The username of \"trusted system component X\" needs to have permissions on to create pod/unprefixed-metrics.\nUnfortunately, this requires two requests unless you cache the result of prefixed vs unprefixed, and assume uniformity among prefixed vs unprefixed across different resources.  I'm open to suggestions here (up to and including removing unprefixed metrics, but at the very least not have the \"custom/\" prefix for things intended to be cluster system components is nice...)\n. Yeah, the client certs stuff will require having the CA available, as I noted below, unless I'm misunderstanding you.\n. That won't work when Heapster gains the ability to support metrics on objects other than pods.  How do you distinguish between querying for metrics on an object in two different API groups if you can't pay attention to the SubjectAccessReview's API group?\n. That would be great, except that it doesn't work for push metrics.  It could work for model and historical queries, though.\n. I replied to a similar question by @erictune above, but I'll add in some more examples.\n. > You cannot assume that there isn't an actual pods/metrics endpoint that needs to be ACLed in a different group.\nAh, fair point. \n\napiGroup=heapster.k8s.io, resource=pods.legacy.k8s.io\n\nYeah, that would work.\n. The push metrics implementation PR hasn't actually merged yet, so I can still change the semantics a bit there.  I'll figure something out.\n. It's possible (although it requires keeping a list of all pods ever seen), but it seems like it makes it less confusing to users just to blanket say \"pod uid access is more privileged\" vs saying \"pod uid access is sometimes more privileged\".\n. Also, apparently I had an extraneous not in that description.\n. \u2714\ufe0f \n. ack.\n. missed this on my first pass through.  What's the point of calling this function?  We've already established that val converts to a float32 directly above, so why call this function?\n. this cast is redundant\n. remove this commented out code, please\n. It didn't occur to me until just now, but this may be problematic when you have multiple pods with the same name, but that were alive during different time periods.  They could have different UIDs (meaning different entries in Hawkular), but since they have the same name, they'll all be returned here.  Since the definition query doesn't restrict to \"time that the pod is alive\" I think you may need to just perform multiple queries, and then figure out if there were multiple pods alive during that time period and error out.\n. Should this have separate authorization, or just bounce off of the main API server use SARs, etc?\ncc @deads2k\n. if no retention policy is set in the sink config, should we still run this query?. This doesn't actually change the compiler, IIRC -- it's basically just a record of which version of go was used to run Godep.  Please set it back.. The library updated from taking an interface{} type to taking a pre-encoded blob of json.  Presumably, it was just calling json.Marshal before, but the library was doing it internally.  I can switch it over to runtime.Encode. ack, will do.. I think it just tried to support \"all\" auth before, but I'm also not entirely certain that anyone was actually using -- I think the HPA is normally deployed to point to the non-generic-API-server variant of the API, and the Kube cluster monitoring addon doesn't enable the generic API server code in Heapster.. done, the rewiring stuff having to do with the generic API server code is now in it's own commit.. I thought when I was looking there was something in the PrepareRun() code of the generic API server code that did this for us. > Also, your main looks to be fighting with some of the API server endpoints like heath\nThat shouldn't be the case, since we're on a different port. it's a legacy thing -- you run --source=kubernetes.summary_api=$KUBE_CONFIGURATION_IN_A_URL, but you can specify to use the inClusterConfig option which should just pull from the injected kubeconfig, IIRC.. > There's a way to add health checks. Did you accidentally lose this entirely?\nWhoops, yeah, looks like this was installing the HTTP handler with a custom checker.  I'll add the custom checker back in. You should note that monitoring-influxdb is an in-cluster DNS name.. I don't think this change should be here.... go\nif *testUser != \"\" {\n    nodeHost = fmt.Sprintf(\"%s@%s\", *testUser, nodeHost)\n}. Please change this to \"... and Horizontal Pod Autoscaling will not function\".. we should probably do this ahead of time, and make it a map. you might even make this a processor that can be plugged in. :new: means it's in development/not-yet-merged this should probably just be :ok:. did you mean to change the Makefile?. this doesn't need to be in common unless you also plan on writing an eventer sink.. does this really need to be a global?. this should probably be health-check-ip and health-check-port. ack, alright :-). huh, you appear to be correct, this should be fine.. don't we still only want to return the error if it's not \"already exists\"?. I'm not entirely certain what the intention here is of multiplying by 30.  Please, let's not have magic numbers (at least not without better comments).. Heapster supports the pluggable... (s/ a / the /).. now that I think about it, I don't see much reason to actually respect the minimum metric resolution with the dummy sink -- this is just going to make the tests take way longer to run.. (so, I don't think there's much reason to change the tests here). small nit: I'd pre-initialize this with the size of metricSet.Labels. why?  Heapster should not need to do anything with deployments.. Heapster should never need to update anything.. agreed with @andyxning . please please please always log conditions like this (at high log level, like 8).  Otherwise, it's impossible to debug user deployments without sending a patched Heapster container to the user.. if !found. Use should not, cannot, or must not.  could not implies (in this particular case) that it's no longer true, because of the past tense.. add some godoc here for clarity.. godoc should be of the form\n<method name> does a thing, so here it should look like:\n// CopyLabels copies the given set of pod labels into a set of metric labels, using the following logic:\n// - all labels....\n// .... if possible, I'd love to avoid globals here.  Don't worry about it too much, but if you can find a nice way to avoid globals, that would be great.. this doesn't need to marked as new unless you plan to do follow-up PRs.. we shouldn't be returning a result at all if there's an error.  Change the signature of the method to also return an error (you might also have to change the signature of the interface for metric sources).. it's generally better to handle the short case in an if:\n```go\nif err != nil {\n    glog.Errorf(\"unable to scrape metrics: %v\", err)\n    return\n}\nfor _, p := range ...\n``. remove this line, we'll log it later on.  Just return an error likereturn fmt.Errorf(\"unable to scrape Kubelet for container metrics: %v\", err). again,if err != nil`:\ngo\nif err != nil {\n    glog.Errorf(\"error scraping source: %v\")\n    continue\nmake sure to log the error, and not just discard it. remove this line, and just return a more descriptive error.. just make this == 0 for clarity.. you already checked allowAllLabels above, no need to check it again.. you can just glog.V(4).Infof -- this isn't in a hot loop. Also, since you're dumping the entire metrics string, probably use a bit higher than 4. it's not safe to assume that you can always listen on this port.  Either using something similar to the go testing HTTP server, or make it so that you can pass a fake client (I'd prefer the latter).. if resourceId, ok := labels[core.LabelResourceID.Key]; ok {. move this so that it reads like\ngolang\nmetricType, hasMetricType := labels[...]\nif !hasMetricType {\n    return \"\", fmt.Errorf(...)\n}\nswitch metricType {\n...\n}. make this a compound if statement. is only UDP supported?. this phrasing is not incorrect.  It's fine to say \"persist things to disk\".. this comma was not incorrect, and the pause makes the sentence easier to parse, since it's easy to see the the last and is a clarification.. also not incorrect.. you've changed the meaning of the sentence.  Whereas earlier the sentence implied that we intended that you only expose useful metrics for collection, the new sentence says that we intend all metrics exposed to be useful.  In the earlier sentence, the intention is that metrics have usefulness coming into Heapster, but with your change the metrics are granted inherent usefulness coming out of Heapster, having none to begin with.. plus, in the earlier case, \"itself\" refers to Heapster, while in the later case, itself is ambiguous.. you should validate that it's greater than zero here.. don't embed this, it looks weird calling sink.Add. this needs more comments -- it's not immediately clear that it's being used to limit the number of sends that can go on at once. add an inline comment here: // use the channel to block until there's less than the maximum number of concurrent requests running. and here: // empty an item from the channel so the next waiting request can run. I'd return the URL itself.. please don't do unrelated changes in the same commit.. I don't think this is necessary.  The model API is more-or-less deprecated (see #1898), and we don't really want to encourage its use by adding new stuff to it.. nope, don't do this.  Leave it broken up per-pod.  People can aggregate in their sinks if they want it aggregated, but it's difficult-to-impossible to \"unaggregate\". might want this to say \"Hawkular pre-caching\" instead of just \"pre-caching\" to make things move obvious in the logs. please make this name something like hashDefinition to be a bit more obvious and avoid naming conflicts in the future.\n  . toCache sounds like a conversion function, name should probably be a bit different (storeInCache for example?). should just remove this commented-out line. should these be removed?. This seems to have updated without a corresponding change to the Godeps file.. this should just be \"k8s.io/api/core/v1\".SchemeGroupVersion. ditto here. Nope, we can't do this -- the legacy client still expects the alpha API version on the non-aggregated endpoint.  If we're going to change this, we'd have to just remove it.  We can't really do that unless we also say that the most recent version of Heapster is the last to be supported as a source of metrics for HPA (FWIW, I'm fine with saying that, but it's something that we need to decide as a team @kubernetes/heapster-maintainers ).. ",
    "roberthbailey": "@jimmidyson For Kubernetes, we are working on securing the master <-> node communication (see #3168) so we want the kubelet listening on an HTTPS port. We would eventually like to be able to run Kubernetes on a less trusted (or untrusted) network where even exposing a read-only interface in the system infrastructure is considered a bad idea. \n@rjnagal I don't believe that we distribute any certs today (using secrets or otherwise). \n. I don't think this will just work. You need to configure an http.Client that will connect to the kubelet without verifying the server's SSL cert (by setting InsecureSkipVerify to true in the TLSClientConfig). \n. ",
    "rohitsardesai83": "@vishh : I am interested in contributing towards this. Do let me know I can pick this up . Thanks !\n. Hello @vishh , I can take this up. Do let me know if I can proceed. Thanks !\n. I signed it!. @andyxning, please review and merge the PR.. @roberthbailey , @andyxning , is there anything I need to do here ?. ",
    "djsly": "HI @huangyuqi , we are also interested in the this PR. We though it was dead and planned in trying to see how easy it was to bring it back to life. Happy to see that you are back. Looking forward to this change!\n. Thanks @vishh for the pointer! and @ghodss great work on https://github.com/kubernetes/kube-state-metrics. \nI was wondering if there were any reasons to keep those projects separated ? Would it make sense to have the state metrics also handled by heapster ?\n. @ghodss  Thanks for the explanation! While this all make sense, I am still trying to see if it wouldn't be beneficial for Heapster to also gather the metrics from the API Server (it already connects to it to gather the node list) and allow the state data to be kept inside influxDB out of the box. \n@titilambert has already create the necessary code change and the associated dashboards in grafana. A PR should come in early next week. It comes with supports for deployment / replicaSet / DaemonSet / and other\n@vishh @ghodss Are you ok for a review once it is available ?\nAlso, could we use the approach of running the kibe-state-metric binary in the same Monitoring POD as heapster / eventer / grafana / influxdb.  I agree that having it as a separate project makes sense due to the differences in business logic. Generator VS consumer.\n. @vishh @piosz @ghodss \nWould anyone be ok to give an update on this topic ?\nWe would like to not diverge to far from the official hipster releases and with the following PR thats unmerged, its making it a bit hard to handle those local build and merge .\nhttps://github.com/kubernetes/heapster/issues/1176\nThanks and sorry for the trouble. \n. sure no problem. Please include us in the discussions regarding the new MVP that you are planning for 1.5; that would be greatly appreciated.\nIn the mean time we will keep collecting what we beleive are important metrics from the kubernetes apiserver. \nRegards,\n. @titilambert does that ring a bell ? was this the reason why we had to upgrade to the latest InfluxDB ?\n. Unfortunately, the ElasticSearch cluster is provided by a different team. We are not giving the credentials for it. Their entry point to the cluster is LogStash at the moment. . ",
    "titilambert": "Hello !\n@huangyuqi I don't know if you already started the work, but due to my job deadlines, I need it quickly.\nSo, I started the merge for ES sink from your commits.\nIt's here: https://github.com/titilambert/heapster/tree/sink_es\nEvents are working. Metrics and tests are missing...\nTell me if you want I create an other PR or I can try to make a PR on you own PR ?\n. PR created here: https://github.com/kubernetes/heapster/pull/1144\n. @huangyuqi It seems I was not clear. I already did the job: I cherry-pick your commits (from #733), I merge it in 2 commits and create PR #1144.\nJust missing the metric parts (which is not really useful, generally people use influxDB for metrics)\nYou can rebase on PR #1144, before adding your changes, in order to avoid to redo the done work...\n. @moserke I make a PR to be able to get events directly in InfluxBD: https://github.com/kubernetes/heapster/pull/1145\nI also added a Grafana dashboard, called events_withfields.json (available in the PR), to get data from those events.\n. @piosz the first thing I see, it seems there is code duplication (two driver.go and two driver_test.go files)\n(functions: SaveDataIntoES, NewElasticSearchSink, ...)\n. @huangyugi you can check in #1144 I did this split for elasticsearch...\n. Documentation updated\n. @piosz I'm not sure to understand what you mean by \"the schema\" and the link with Kubernetes 1.2. Could you give me more context about \"the schema\" ?\n. @mwielgus what about add an option to get the new schema with influxdb (?withfieds=true) in the URL.\nSomething like: --sink=influxdb:http://monitoring-influxdb:80/?withfields=true\n. @piosz Thanks ! I understand now the schema issue. What do you think about withfields option ? This could permit smooth transitions waiting breaking change. (Of course default value should be old schema)\n. Hello,\nCan someone explain how I can add @huangyuqi to this pull request ?\nThanks\nedit:\nDONE\n. Sorry I was off for 2 weeks ...\nI guess this PR could be close since #733 is merged.\n@piosz did you find any stuff in this PR that could be merged even with #733 merged ?\n. Last message:\n\n@piosz Thanks ! I understand now the schema issue. What do you think about withfields option ? This could permit smooth transitions waiting breaking change. (Of course default value should be old schema)\n. I also rework the influxdb schema for events. We can now get all events directly from Grafana.\n@piosz Should I post a Grafana dashboard example ?\n. @piosz @mwielgus I added the withfields options which set new schema for influxdb sink.\nBy default the value is false so there is NO breaking changes.\nNew Heapster users can get the new influxdb schema with something like:\n-sink=influxdb:http://localhost:8086?withfields=true\n. Hello !\n@mwielgus Any feedback on this one ?\n. @mwielgus changes done !\n. @mwielgus do you want I squash all of this ?\n. @mwielgus squashed and rebased ;)\n. The PR is ready ! (tests OK)\n. @piosz Hello, I just rebased the PR #1176. Waiting for feedback\n. @piosz Any news about the Heapster scope ?\n. @davidopp @fgrzadkowski Any news about the issue ?\n. NewElasticSearchSink is already defined in events/sinks/elasticsearch\n. SaveDataIntoES is already defined in events/sinks/elasticsearch\n. \n",
    "jmreicha": "@vishh is there an example somewhere that I can follow?  I was having issues browsing to the url you pointed out.\n. When I run a curl to the above master address I get the following Error: 'dial tcp 10.244.103.11:8080: no route to host'.\nI am running the Kubernetes master on port 8080.\n. That's what I'm confused about.  Maybe I am not following a correct Kubernetes setup?\nI am using the CoreOS config guide - https://github.com/GoogleCloudPlatform/kubernetes/tree/master/docs/getting-started-guides/coreos/cloud-configs\n. No firewall rules, right now internally it is pretty wide open.  I can curl http://master-ip:8080 okay.\n. Maybe I will just hold off until Kubernetes stabilizes a little bit :/\n. Yeah a link to your config would be great.\n. ",
    "jonlangemak": "My setup is on bare metal CentOS7 but the pod deployment for the cluster add-on should be the same.  I've been using branch/release 0.13 though since I had issues with Kubernetes in anything after that.  I havent tried .15 yet though.  Let me know if you want the link for my config based on .13\n. I did a write up on how I deployed Heapster here -> http://www.dasblinkenlichten.com/installing-cadvisor-and-heapster-on-bare-metal-kubernetes/  It's a little dated (but updated) since the initial install relied on cadvisor running as a manifest on each host before it got baked into the kubelet.  \nAs far as the configs for replications controllers and services go you can checkout my github page here ->  https://github.com/jonlangemak/saltstackv2  The premise of that repo is to use SaltStack to deploy a baremetal cluster so there's a lot of templating in the configs but there's a very limited amount (if any) in the heapster YAML files so it should be pretty straight forward.  See this page -> https://github.com/jonlangemak/saltstackv2/tree/master/salt/pods/heapster specifically for the Heapster replication controller and service definitions.  \nWould love to hear how you're testing goes.  Like I said, a lot changed after .13 and I couldnt get the API server service proxy to work in .14 so I stuck with .13.  Im hoping it's fixed in .15 but I havent checked yet.  A lot also changed after .13 in regards to how things like etcd are deployed and what services run where etc.  \nAnd Im rambling.  Feel free to shoot me a email with further questions or comments\njon at interubernet dot com\n. ",
    "biswars": "I observe the same issue with v1beta3. However, I am trying to get heapster run on kubernetes. Can you let me know which versions of the kube clients can I go for along with heapster ? So that I can set it up without exploring with various versions. Can I go for kube client 0.15 and master branch of heapster ?\n. Here is the Heapster log:\n/opt/bin/kubectl log monitoring-heapster-controller-9ihq3\n2015-05-06T07:09:23.181452490Z + EXTRA_ARGS=\n2015-05-06T07:09:23.181452490Z + '[' '!' -z --kubernetes_version=v1beta3 ']'\n2015-05-06T07:09:23.181452490Z + EXTRA_ARGS=--kubernetes_version=v1beta3\n2015-05-06T07:09:23.181452490Z + '[' '!' -z 10.100.0.1 ']'\n2015-05-06T07:09:23.181452490Z + EXTRA_ARGS='--kubernetes_master 10.100.0.1:80 --kubernetes_version=v1beta3'\n2015-05-06T07:09:23.181452490Z + HEAPSTER=/usr/bin/heapster\n2015-05-06T07:09:23.181452490Z + case $SINK in\n2015-05-06T07:09:23.181452490Z + HEAPSTER='/usr/bin/heapster --sink influxdb'\n2015-05-06T07:09:23.181452490Z + '[' '!' -z 10.100.0.1 ']'\n2015-05-06T07:09:23.181452490Z + INFLUXDB_ADDRESS=\n2015-05-06T07:09:23.181452490Z + '[' '!' -z 10.100.44.240 ']'\n2015-05-06T07:09:23.181452490Z + INFLUXDB_ADDRESS=10.100.44.240:80\n2015-05-06T07:09:23.181452490Z + /usr/bin/heapster --sink influxdb --sink_influxdb_host 10.100.44.240:80 --kubernetes_master 10.100.0.1:80 --kubernetes_version=v1beta3\n2015-05-06T07:09:24.244084682Z I0506 07:09:24.213734       8 heapster.go:45] /usr/bin/heapster --sink influxdb --sink_influxdb_host 10.100.44.240:80 --kubernetes_master 10.100.0.1:80 --kubernetes_version=v1beta3\n2015-05-06T07:09:24.244084682Z I0506 07:09:24.242699       8 heapster.go:46] Heapster version 0.10.0\n2015-05-06T07:09:24.252171510Z I0506 07:09:24.249727       8 kube.go:237] Using Kubernetes client with master \"http://10.100.0.1:80\" and version v1beta3\n2015-05-06T07:09:24.252171510Z I0506 07:09:24.249780       8 kube.go:238] Using kubelet port \"10250\"\n2015-05-06T07:09:24.252171510Z I0506 07:09:24.250028       8 driver.go:164] Using influxdb on host \"10.100.44.240:80\" with database \"k8s\"\n2015-05-06T07:09:24.551762140Z E0506 07:09:24.532776       8 reflector.go:115] Failed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:24.551867724Z E0506 07:09:24.544043       8 driver.go:175] Database creation failed: Post http://10.100.44.240:80/db?u=root&p=root: net/http: transport closed before response was received. Retrying after 30 seconds\n2015-05-06T07:09:25.541940354Z E0506 07:09:25.541854       8 reflector.go:115] Failed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:26.568185275Z E0506 07:09:26.550427       8 reflector.go:115] Failed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:27.562186149Z E0506 07:09:27.561886       8 reflector.go:115] Failed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:28.570102593Z E0506 07:09:28.569595       8 reflector.go:115] Failed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:29.576157889Z E0506 07:09:29.575924       8 reflector.go:115] Failed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:30.582672970Z E0506 07:09:30.582291       8 reflector.go:115] Failed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:31.589976589Z E0506 07:09:31.589723       8 reflector.go:115] Failed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:32.596224621Z E0506 07:09:32.595927       8 reflector.go:115] Failed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:33.606609261Z E0506 07:09:33.606137       8 reflector.go:115] Failed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:34.616152119Z E0506 07:09:34.615970       8 reflector.go:115] Failed to list api.Node: the server could not find the requested resource\n2015-05-06T07:09:35.648468652Z E0506 07:09:35.647990       8 reflector.go:115] Failed to list api.Node: the server could not find the requested resource\nKubernetes Setup:\nVersion - v0.16.0-1-gfc76242f4afd2c\nNetwork Set up - Test set up with virtual box's NAT (not GCE). Kubernetes set up is done with private etcd discovery and post set up able to list the minions and also able to run pods. Able to access the minions from the host system and able to access cadvisor at port 4194\nyaml files used: https://github.com/GoogleCloudPlatform/heapster.git commit -e3b3b0584de41e1da1d58c9b8b99313ee0812f4d \nLet me know if any further info needed on this.\n. Thanks for all your responses so far; much appreciated. In fact, the flag issue of --kubernetes_version=v1beta3 was observed and that was the very reason for settling to a working commit version for this set up. Also, had done some testing by copying the heapster binary from the docker container and running without the flag that resulted in some other error. But I can confirm this back exactly on the error once I am back to my work network after few hours from now.\nBefore that (sorry but since it seems I am into a different time zone, so taking some feedback early), could you let me know which version of kubernetes, coreos, etcd (private discovery or clustered) and heapster (you have already mentioned to try from the latest) you have run the set up successfully ? So that I can straight away work on that and confirm you back.\n. When checked on the heapster repo, I did not observe any update as mentioned by you (on the flag change). This perfectly makes sense as the flag is set inside the docker image \"kubernetes/heapster:v0.11.0\" in order to run as container. But When I run the pods & services using the config files from HEAD, I still get the flag error. So, there is no change to the image either.\nHowever, as per my earlier method, I could extract the heapster binary from the exited docker container (thanks to \"docker cp\" command) and run heapster without --kubernetes_version=v1beta3. \nBut the following is the error in heapster:\nMay 07 05:03:32 core-02 kubelet[1113]: E0507 05:03:32.559349    1113 kubelet.go:1288] Unable to mount volumes for pod \"monitoring-heapster-controller-jlu5p_default\": secrets \"token-system-monitoring\" not found; skipping pod\nMay 07 05:03:32 core-02 kubelet[1113]: E0507 05:03:32.791071    1113 pod_workers.go:103] Error syncing pod 5c00e9a1-f476-11e4-8dd9-0800279924c6, skipping: secrets \"token-system-monitoring\" not found\nMay 07 05:03:32 core-02 kubelet[1113]: I0507 05:03:32.815953    1113 event.go:200] Event(api.ObjectReference{Kind:\"Pod\", Namespace:\"default\", Name:\"monitoring-heapster-controller-jlu5p\", UID:\"5c00e9a1-f476-11e4-8dd9-0800279924c6\", APIVersion:\"v1beta3\", ResourceVersion:\"2020\", FieldPath:\"\"}): reason: 'failedSync' Error syncing pod, skipping: secrets \"token-system-monitoring\" not found\nMay 07 05:03:42 core-02 kubelet[1113]: E0507 05:03:42.587797    1113 secret.go:116] Couldn't get secret default/token-system-monitoring\nMay 07 05:03:42 core-02 kubelet[1113]: E0507 05:03:42.597299    1113 kubelet.go:1288] Unable to mount volumes for pod \"monitoring-heapster-controller-jlu5p_default\": secrets \"token-system-monitoring\" not found; skipping pod\nMay 07 05:03:42 core-02 kubelet[1113]: E0507 05:03:42.757533    1113 pod_workers.go:103] Error syncing pod 5c00e9a1-f476-11e4-8dd9-0800279924c6, skipping: secrets \"token-system-monitoring\" not found\nMay 07 05:03:42 core-02 kubelet[1113]: I0507 05:03:42.762674    1113 event.go:200] Event(api.ObjectReference{Kind:\"Pod\", Namespace:\"default\", Name:\"monitoring-heapster-controller-jlu5p\", UID:\"5c00e9a1-f476-11e4-8dd9-0800279924c6\", APIVersion:\"v1beta3\", ResourceVersion:\"2020\", FieldPath:\"\"}): reason: 'failedSync' Error syncing pod, skipping: secrets \"token-system-monitoring\" not found\nAnd the Heapster POD status is always in Pending state.\n(This is aparantly due to the secrete as a volume addition in the HEAD tag. Any changes need to be made in the host files/directories ?)\nThen I continued with the just prior commit 331d9228cb464d50cb3414fc9f81cbda7ebfc8af\nThis time able to run Heapster successfully listening to the port 8082\nBut following is the error in heapster:\nI0507 04:50:38.403239    8612 heapster.go:50] hs/heapster --sink influxdb:http://10.100.116.226:80 --source=kubernetes:http://10.100.0.1:80\nI0507 04:50:38.408021    8612 heapster.go:51] Heapster version 0.11.0\nI0507 04:50:38.412493    8612 kube_factory.go:97] Using Kubernetes client with master \"http://10.100.0.1:80\" and version v1beta1\nI0507 04:50:38.434382    8612 kube_factory.go:98] Using kubelet port 10255\nW0507 04:50:38.450605    8612 request.go:288] field selector: v1beta1 - pods - DesiredState.Host - : need to check if this is versioned correctly.\nI0507 04:50:38.520599    8612 driver.go:371] created influxdb sink with options: {root root 10.100.116.226:80 k8s false}\nI0507 04:50:38.528436    8612 heapster.go:62] Starting heapster on port 8082\nW0507 04:50:38.634983    8612 request.go:288] field selector: v1beta1 - pods - DesiredState.Host - : need to check if this is versioned correctly.\nE0507 04:55:38.499291    8612 reflector.go:158] watch of api.Node ended with: very short watch\nW0507 04:55:38.618639    8612 request.go:288] field selector: v1beta1 - pods - DesiredState.Host - : need to check if this is versioned correctly.\nE0507 04:55:38.623958    8612 reflector.go:158] watch of api.Pod ended with: very short watch\nE0507 04:55:39.000137    8612 kube_events.go:168] Event watch loop was terminated due to error. Will restart it. Error: Events watchLoop failed with error: watchLoop channel closed\nE0507 04:55:39.001577    8612 heapster.go:121] failed to get information from source - Events watchLoop failed with error: watchLoop channel closed\nW0507 04:55:39.650032    8612 request.go:288] field selector: v1beta1 - pods - DesiredState.Host - : need to check if this is versioned correctly.\nW0507 04:55:39.768627    8612 request.go:288] field selector: v1beta1 - pods - DesiredState.Host - : need to check if this is versioned correctly.\nSo failed again :-(\nThanks for you suggestion on running heapster from kubernetes repo. So do you mean, I need to clone the repo and build the kubernetes binary and then run the heapster, influxDB and grafana PODs ? (Assume I don't need to build the heapster, influxDB and grafana as these are referred as docker images in the .yaml config files)\nHowever, since I had the kubernetes 0.16.0 binary, I tried runing heapster with that and from the config files present in the \"kubernetes repo\" but still faced the heapster issue with similar logs as above. This time, I observed the follwoing error in the kubernetes apiserver logs which was not there in the earlier cases:\nError in kube-apiserver:\nI0507 06:16:26.871017   22089 server.go:400] Using self-signed cert (/var/run/kubernetes/apiserver.crt, /var/run/kubernetes/apiserver.key)\nI0507 06:16:44.716810   22089 logs.go:41] http: TLS handshake error from 172.17.8.1:63329: remote error: unknown certificate authority\nE0507 06:27:39.949671   22089 etcd_helper_watch.go:401] unknown action: expire\nNote: In all the above cases I can list down the minions, run/get the POD and Service details so I am quite sure the kuberenetes clusture is up and running as expected but always welcome your inputs.\nAlso, I can open the cAdvisor at port 4194 and influxDB at 8083 with two DBs grafana and k8s. But no data as heapster fails to pull stats from kubelet.\nInputs on how to make this work is greatly appreciated.\n. Did you setup a cluster with kubernetes version 0.16.0 from scratch or did you upgrade the different components manually?\nAns: I am using kube-apiserver, kubelet, proxy, kube-register, kube-controller-manager, kube-scheduler binaries and setting up the clustur manaually. The cluster runs fine and I can list/run the pods\nFew observations here while using the kubernetese repo and the config files present there.\nInside the file heapster-controller.yaml I see the heapster is started using --sink influxdb:http://monitoring-influxdb.default.kubernetes.local:8086. How will it resolve the domain here ? Do I need to change the domain to my local IP as influxDB is running on 8086 on my local IP. After changing this (hope am at right path), when I access grafana using https://172.17.8.102:6443/api/v1beta1/proxy/services/monitoring-grafana/ I don't see grana UI but just an empty page with text \"{{alert.title}}\". Following is the screenshot of this.\n\nNext, I inspected the port where grafana ui is running by listing out the kubernetes services. When I opened the ui directly with the grafana ui port, I can see the grafana page but empty graphs as shown in the follwoing screen shot.\n\nWhen I move my cursor over the empty graph, it shows influx DB 404.\n\nWhen I analysed with network analyser for the first graph, I see the following URL is getting hit\nhttp://172.17.8.102:53370/api/v1beta1/proxy/services/monitoring-grafana/db/k8s/series?p=root&q=select+container_name,+derivative(value)+from+%22cpu%2Fusage_ns_cumulative%22+where+time+%3E+now()+-+5m+group+by+time(5s),+container_name+order+asc&u=root\nand Response is 404\n(Here 53370 is where the grafana ui is listening).\nNext, when I log in to influxdb ui using port 8083, I can log in successfully and list the series with \"list series\" query. So I feel somehow grafana is not able to get the data from influxdb.\nI am stuck at this point for a long time today. Really appreciate any input to progress.\n. I have not set up the dns in the kubernetes cluster so far but yeah, heapster is deployed as per the command you have mentioned above (the only change I made was replaced the domain name with the IP address of the influxDB host inside heapster-controller.yaml).\nThe dns part, I can confirm once I am back to my work network after few hours.\nIn the mean time, I can confirm you that I have seen the metrics data in the InfluxDB (the value in the \"time\" column was zero always though not sure why) with values populated for all the pods, container_name, cpu, io, memory stats and multiple rows. But could not get these reflected in the grafana ui. The IP address you see above is the IP address of the minion where the influxDB pod is running.\nAs it seems I am in a different time zone, highly appreciate if you could provide any input on the grafana ui issue before I get to my work network and run the dns set up (somehow I feel, setting up the dns would not resolve the grafana ui issue and there is some other reason that I am not able to find out yet).\n. I am taking this point and will be exercising this as well along with dns set up. But the set up I am running is completely on my local machine with VMs running by virtual box. So, it is not a production set up and might not need to add public IP to the node as I can access it directly.\nAny other inputs you would like to provide here for the 404 error in grafana dashboard graphs ?\n. Did some more analysis and able to get the grafana ui. Here are my observations:\nAs mentioned above, I was trying to access grafana by hitting the url https://172.17.8.102:6443/api/v1beta1/proxy/services/monitoring-grafana/. Observed that the internal urls getting hit in the browser is https://172.17.8.102:6443/api/v1beta1/proxy/namespaces/default/services/monitoring-grafana/. (this always gives HTTP 404)\nWhen I access the url https://172.17.8.102:6443/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/ then the ui comes up.\nSo, a question here - who does this url redirection ? Is it due to the pods used here are with v1beta3 ?\nQueries:\nA: The \"All containers CPU usage - stacked shows \"No datapoints\" initially with only grafana, influxdb and heapster running. The query run is \"select container_name, derivative(value) from \"cpu/usage_ns_cumulative\" where time > now() - 5m group by time(5s), container_name \" which gives empty result when I tried in influxdb ui. Is it the normal behavior ? The memory usage graph shows the gauge value properly.\nB: The grafana ui shows two rows distinctively for the pods heapster and influx-grafana. When I run more pods onto the cluster it does not add similar rows for each pod. Is this the normal behavior ?\nC: I used the 0.16.2 kubernetes binaries so that I can use the secrets as volume as you had mentioned earlier. But I still get error due to missing \"token-system-monitoring\" and pod does not get deployed. Just out of curiosity, where is this secret defined ? In heapster-controller.json, it is just referenced but I think it should be defined somewhere isn't it ? (Note, as mentioned earlier I am using the kubernetes binaries manually and not on GCE. So, not sure if I need to define the secrete separately)\nD: While checking the kube-apiserver logs, two errors are observed. Can these be ignored ?\nD1:  etcd_helper_watch.go:401] unknown action: expire (this comes even when left idle for sometime)\nD2: 872 logs.go:41] http: TLS handshake error from 223.239.132.241:57670: remote error: unknown certificate authority (this comes at times while accessing the grafana ui)\n. Today I tried the same set up with kubernetes head (1.0.3) and getting error in running heapster. The error log shows the followings:\nI0817 10:25:38.552690       1 heapster.go:55] /heapster --source=kubernetes:'' --sink=influxdb:http://172.17.8.102:8086\nI0817 10:25:38.552754       1 heapster.go:56] Heapster version 0.17.0\nF0817 10:25:38.552885       1 heapster.go:62] open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory\nNote: I am not using any secrets as I was not using with kubernetes version 0.16 earlier. Any suggestion on how to make this work ?\n. Thanks Vishh for pointing this out. I could proceed to the extent of setting up the service accounts with secrets but it seems I am still missing something to make it work. I am providing the the set up details (I know I need to struggle as am setting up locally but hopeful as it was working fine with earlier version of kubernetes).\nSet up with observation:\n$ kubectl get svc\nNAME                 CLUSTER_IP      EXTERNAL_IP   PORT(S)         SELECTOR             AGE\nkubernetes           10.100.0.1              443/TCP                        6h\nI believe the RO service is now deprecated. But not able to hit this using \"curl https://10.100.0.1:443\" instead curl with -k (ssl connection without certs) option works. This is discussed in #4290 but not too sure on the solution on how to place the certs.\nService account set up:\nAs per the suggestion above, api-server started with admission-control plugin as mentioned in https://github.com/kubernetes/kubernetes/blob/master/docs/admin/admission-controllers.md. Able to get the service accounts but could not get the secrets associated with it.\nAs could not get any guide within the community, had searched and got this useful link #11355. Post creation of the self-signed key manually and restarting the api-server and controller manager able to get the secrets associated with the service account (namespace \"default\")\nHowever, running heapster shows the followings logs. Anything missing still ?\nI0818 10:23:15.669769       1 heapster.go:55] /heapster --source=kubernetes:'' --sink=influxdb:http://172.17.8.102:8086\nI0818 10:23:15.675855       1 heapster.go:56] Heapster version 0.17.0\nE0818 10:23:15.677549       1 helper.go:247] expected to load root ca config from /var/run/secrets/kubernetes.io/serviceaccount/ca.crt, but got err: open /var/run/secrets/kubernetes.io/serviceaccount/ca.crt: no such file or directory\nI0818 10:23:15.679510       1 kube_factory.go:168] Using Kubernetes client with master \"https://10.100.0.1:443\" and version \"v1\"\nI0818 10:23:15.679679       1 kube_factory.go:169] Using kubelet port 10255\nE0818 10:23:16.231170       1 reflector.go:136] Failed to list api.Namespace: Get https://10.100.0.1:443/api/v1/namespaces: x509: failed to load system roots and no roots provided\nE0818 10:23:16.262881       1 kube_events.go:96] Failed to load events: Get https://10.100.0.1:443/api/v1/events: x509: failed to load system roots and no roots provided\nE0818 10:23:16.269705       1 reflector.go:136] Failed to list api.Pod: Get https://10.100.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D: x509: failed to load system roots and no roots provided\nE0818 10:23:16.272718       1 reflector.go:136] Failed to list *api.Node: Get https://10.100.0.1:443/api/v1/nodes: x509: failed to load system roots and no roots provided\nI0818 10:23:16.274954       1 driver.go:376] created influxdb sink with options: {root root 172.17.8.102:8086 k8s false}\n. Just a query here - even cAdvisor does not show the limit set to the container (at least in the web interface). So, are there any means to get the container limits using cAdvisor directly ?\n. Thanks. Yeah, able to get the limits directly now and also able to co-relate with the data as sampled by heapster. For the memory usage, I am using the series \"memory/usage_bytes_gauge\" and works perfect.\nHowever, having some issue with the cpu usage using the series \"cpu/usage_ns_cumulative\". Post some analysis, I guessed the \"value\" field in series \"cpu/usage_ns_cumulative\" gives the time in nano secs spent in the form of cpu cycles on specific container. Any suggestion on how to transform this value to the %age of cpu usage for a container ?\nNote: I am able to get the cpu limit also now which gives the cpu share per container (viz. 1024, 512 etc as set in the spec). But getting the %age using the limit value and \"cpu/usage_ns_cumulative\" is where am not able to find any co-relation.\n. Thanks. Very useful comments. So, If I want to relate the above two parts to get the cpu usage per container can I take like this way;\na: Result of the new series from part 1 above gives the nano secs of cpu usage per 1m container-wise. From this, get the number of seconds of cpu usage per second. Assign this value as \"m\".\nb: Based on the cpu shares per node (taking 1000 as per part 2 above) and limit set by the container (ex: 500 which is half of the cpu), if m == 0.5 then the container's cpu usage is 100%.\nWould this be the right way ?\n. Thanks. Able to get the cpu % usage based on your suggestions and it works to certain extent. The issue I am facing is, I need to wait for more than 3/5 minutes before actually getting/applying the instantaneous cpu usage.\nThe followings are the steps used:\nStep 1: Every 1 minute interval, execute the query \"select container_name, derivative(value) from \"cpu/usage_ns_cumulative\" where time > now() - 3m group by container_name order asc\".\nStep 2: Check the derivative as obtained in Step 1 against the threshold.\nStep 3: Take action as appropriate.\nProblem:\nIn Step 1 above, I very often get empty (\"[]\") json response. So if one cycle is lost, I need to wait for the second cycle (wait for 1 more minute).\nObservation:\nIf the \"3m\" is replaced with \"5m\", I almost always get the valid json response. This is acceptable, if it is fine to wait for 5 minutes before getting the cpu instantaneous usage value. But I would like to take action as soon as the threshold is reached, which means getting the cpu usage derivative every 1 minute. But if I replace \"3m\" with \"1m\", I always get empty json response.\nWhat could be the problem ?\nIs it because heapster fails sometimes in updating the values to influxdb ? Are there any means of reducing the time interval at which heapster dumps the metrics to influxdb so that I can get the cpu derivatives every 1 minute ?\n. To answer above query, heapster polling interval can be changed in the pod definition file by cofiguring \"--poll_duration\" and with this the data points are populated. Does \"--stats_resolution\" also need to be set.\nBy setting these values as \"10s\" and \"5s\" respectively, the data points can be retrieved from influxDB if externally polled at certain interval (using 60 sec).\nThe only observation here is sometimes the data points are empty for time series \"cpu/usage_ns_cumulative\". Any configuration changes need to be made so that the data points can be retrieved always without fail ?\n. @huangyuqi , I am getting the same error while trying to access the container metrics.\nUsing /api/v1/model/namespaces/default/pods/{pod-name}/containers to access the containers but getting 404 error. Containers are not listed.\nNote: The set up is running in kubernetes cluster with version \"heapster-v1.1.0.beta1\" and I am able to get node and pod level metrics successfully. Only Container level metrics fails.\nAny special flags to be set to enable container level metrics ?\n. @huangyuqi yes, the source of heapster is kubernetes. In fact, I am using the heapster-controller.yaml as present in the kubernetes repo (one of the previous versions) heapster-controller.yaml\nReferring to model.md, I tried getting the metrics using Rest API and Cluster-level Metrics, Pod and Node-level Metrics work. However, Container-level Metrics does not work.\nExample:\n/api/v1/model/namespaces/kube-system/pods returns\n[\n  \"heapster-v1.1.0.beta1-3062717326-3u7h4\",\n  \"monitoring-influxdb-grafana-v3-ukr6v\",\n  \"kube-dns-v11-p6jiq\"\n ]\n/api/v1/model/namespaces/kube-system/pods/kube-dns-v11-p6jiq/metrics returns the available metrics\nBut, /api/v1/model/namespaces/kube-system/pods/kube-dns-v11-p6jiq/containers returns 404 error, while the containers are running fine as expected inside the pod kube-dns-v11-p6jiq\n. Hi @huangyuqi , I just tried to test this commit with elasticsearch but getting connect error always.\nNote: The elasticsearch service is reachable without user credentials using curl 172.31.0.71:9200\nThis is how I use it \"./heapster -v=3 --source=kubernetes:https://kubernetes.default --sink=elasticsearch:?nodes=172.31.0.71:9200\"\nThe log shows the follows:\nI0203 11:43:45.468969   22695 heapster.go:61] ./heapster -v=3 --source=kubernetes:https://kubernetes.default --sink=elasticsearch:?nodes=172.31.0.71:9200\nI0203 11:43:45.469054   22695 heapster.go:62] Heapster version 0.19.1\nE0203 11:43:45.469130   22695 heapster.go:122] failed to setup sinks - encountered following errors while setting up sources - open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory\nI0203 11:43:45.469208   22695 driver.go:252] initializing elasticsearch sink with ES's nodes - [172.31.0.71:9200]\nI0203 11:43:45.469239   22695 driver.go:194] attempting to setup elasticsearch sink\nI0203 11:43:45.469373   22695 external.go:116] no timeseries data between 0001-01-01 00:00:00 +0000 UTC and 0001-01-01 00:00:00 +0000 UTC\nE0203 11:43:50.470230   22695 client.go:58] Failed to initialize client \"elasticsearch\"- failed to connect any node of ES cluster\nI0203 11:43:50.470332   22695 driver.go:194] attempting to setup elasticsearch sink\nI0203 11:43:50.470475   22695 external.go:223] Updated sinks: [0xc20805ad00]\nI0203 11:43:50.471495   22695 manager.go:162] starting to scrape data from sources start: 2016-02-03 11:43:45 +0000 UTC end: 2016-02-03 11:43:50 +0000 UTC\nI0203 11:43:50.472490   22695 manager.go:175] completed scraping data from sources. Errors: []\nI0203 11:43:50.473409   22695 heapster.go:72] Starting heapster on port 8082\nI don't see any indices getting created in elaasticsearch and the line in the log above shows \"Failed to initialize client \"elasticsearch\"- failed to connect any node of ES cluster\"\n. ",
    "EranGabber": "I prefer that node metrics will be easily distinguishable from container metrics.\nDo you plan to have the node label as the \"hostname\" or somewhere else?\nI suggest that node metrics will have an empty container name (instead of \"/\").\n. The 'type' label is a much better idea. In this way you can have different label names for different resource types, and you could simply omit labels that do not make sense for the particular resource.\n. I entered a few comments on this change on GitHub.\nEran\nOn Fri, Apr 17, 2015 at 12:45 AM, Rohit Jnagal notifications@github.com\nwrote:\n\nMerged #244 https://github.com/GoogleCloudPlatform/heapster/pull/244.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/244#event-283625425\n.\n. Thank you for fixing it.\n. \n",
    "george-angel": "Excellent, running docker with --net=host fixed the issue, thank you.\n. ",
    "eliaslevy": "The issue is that the HTML ends up with a doubled proxy path:\nhtml\n<link rel=\"stylesheet\" href=\"/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/css/grafana.dark.min.4efc02b6.css\">\n<link rel=\"icon\" type=\"image/png\" href=\"/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/img/fav32.png\">\n<base href=\"/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/\"/>\n<script src=\"/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/app/app.3c38f44f.js\"></script>\n. Grafana is correctly making use of the GF_SERVER_STATIC_ROOT_PATH config env variable to to prepend the proxy path to the link, base, and script elements, but the apiserver proxy is rewriting the HTML to insert the path as well.  Thus, you end up with a duplicate path that messes up Grafana.  \nIf you don't use GF_SERVER_STATIC_ROOT_PATH, then the static assets load fine, but the XHR requests fail, as now they don't know the correct URL to use for requests. \nI am guessing the solution to this is to change the apiserver proxy so that it checks the URLs its trying to rewrite to determine if they already have the proxy path prepended to them, and if so, skip rewriting them.  Otherwise, Grafana won't run in proxy mode.\n. I'll see what I can do tomorrow.\n. As of k8s v.1.1.1 Grafana 2.1.0 is working.  Looks like @vishh fixed the double prefix issue in kubernetes/kubernetes@2a075cd4bd04a003045d4946065b4cf153803446, which made it into v1.1.1.  I think this issue can be closed.  Thanks @vishh.\n. See my explanation of the problem in #249.\n. I believe this has been fixed by kubernetes/kubernetes@2a075cd4bd04a003045d4946065b4cf153803446, which made it into v1.1.1.  I think this issue can be closed. \n. ",
    "erictune": "@vishh asked for this issue\n. This is the first I have heard that Master Metrics types would be in Heapster.  Can I see a link to the discussion where the change in plans was made?\n. Seems reasonable.\nAllowing a pod to only push metrics that are for that Pod or Namespace seems pretty useful, and a common thing to want to do.\nSome areas I that would be interesting to think about are:\n1. should a Container that is pushing metrics use a credential that identifies it as being in a namespace, having a service account, having a certain pod name, or being a particular container of a particular pod?\n2. how do users want to aggregate metrics?  Presumably sometimes by Service or ReplicaSet that the pod is part of.  But, then how does we join pod-provided metrics with the Service or ReplicaSet dimensions?  By endpoints?  Or by labels?  If labels, how are those labels validated as being the right labels for the pusher?\n3. If namespace-level authentication is sufficient, then the pushing program could present its service account JWT from which namespace can be inferred.\n. IIUC, webhook auth only supports token, and not client cert (which a fair number of people use).\n. I like the overall direction and the parts about authentication.  I'd suggest more thought about concrete Authorization use cases, and what is the minimal amount of information that crosses the interface in order to make authorization decisions for those use cases.\n. I'm not sure what our overall goals are for how heapster fits into a Kube system. \nIf heapster is going to remain an optional, replace-able component in a Kube cluster, that just happens to share some authentication code, then I think this proposal is just fine.\nHowever, if heapster is going to look more and more like just another part of the Kube api to the point where the heapster API becomes an expected part of any Kube cluster, then I feel somewhat strongly that the Heapster authz needs to conform more to the Kube RBAC style (hence comments in this PR.).\nThese other PRs:\n- https://github.com/kubernetes/kubernetes/pull/26858 and\n- and especially https://github.com/kubernetes/kubernetes/pull/28844\n  make me think it is more of the latter.\n. If Heapster is going to serve a Kubernetes-style API soon, as @piosz says, then Heapster will need to adjust its concepts to fit the Kube authorization model.  \nIf Heapster is going to forever remain a separate, optional, system, then it can keep its own concepts (as @DirectXMan12 suggests in https://github.com/kubernetes/heapster/pull/1213#issuecomment-233412472).  \nI haven't seen a proposal on what the future of Heapster is within Kubernetes.\nI honestly don't know what the plan is, but I think there should be a firm decision, involving the Heapster and Kubernetes communities -- a decision on whether heapster is required, and if it serves an API that is required for essential kubernetes APIs to work right.   Once that is decided, we will know what to do about heapster authorization.\n. subjectaccessreview has these fields:\n- subject:\n  - user making a request, (service account if called from a pod), \n  - the user's group memberships \n- \"verb\" (create, delete, etc).\n- object:\n  - namespace of the object\n  - name of the object (exact match, no pattern match)\n  - kind of the object\n  - subresource\nHeapster wants to authorize their requests based on this set of attributes:\n- NodeName\n- Namespace\n- PodName\n- ContainerName\n- PodId string\n- QueryType (historical or model)\n-  metric name (prefix match)\n- ObjectType in {MetricSetTypePodContainer, MetricSetTypePod, core.MetricSetTypeNamespace}\nHow can we fit the heapster attributes into the subjectaccessreview object?\n@lavalamp @ericchiang @deads2k \n. For the authentication, is the idea to set up a completely separate CA for heapster, and a separate distribution path for heapster-CA-signed-client-certs, which is parallel to but independent of the service-account and service-account-token framework?\n. Are you trying to grow Heapster into a robust multi-tenant system?  Or is there a narrower motivation for this?\n. How important is \"own RC\".  That is a complex thing to track.\n. You say URL.  Do you want to support file-based as well?\n. Is there a way to look at Creation and Query as POST and GET or \"create\" and \"get\", like in RBAC, so that and RBAC rule could express Heapster authorization, and so that for example, in the future, an RBAC role for a pod's service-account could encompass not only the ability to access the K8s API if needed, but the ability to push metrics?\n. Why doesn't the caller do the prepending?\n. I could not discern any concrete use case from the above paragraph.\n. And I think dropping metric-names from the authorization model will make it easier to make it match the RBAC model.\n. If just Heapster does this, it is not so bad maybe, but if lots of peripheral systems to this, it makes the meaning of \"GET pods\" encompass a lot of things, which makes it hard for someone to understand this permission.  \nOnce you have a fairly general IAM system like RBAC, you might as well express the permissions within the RBAC framework.  And then extend the default roles to have the appropriate permissions.\nThis also allows you to show a users permissions explicitly in a clear way in a CLI/GUI.  (e.g. when you list \"roleFoo\" permissions, you see both \"GET pods in this namespace\" and \"GET modelMetrics in this namespace\" and \"GET historicalMetrics in this namespace\".\n. Do you really need all this information?  Excessive information makes it hard to cache authorization decisions, and makes it harder to integrate with a general authorization system like RBAC.\n. Thanks for clarification.  So, it sounds like you would be able to reuse, for example, the local-file-based token authentication module from Kube, but you would use a pseudo-URL to name that module via a flag, e.g. --authn=password:localfile:/etc/heapster/passwords.csv.\nSounds fine.\n. Define how it works with multiple:  presumably, each is tried in order, and success if any succeeds, with short circuit?\n. So, you have both kubernetes namespace, and \"username prefix\" as namespacing mechanisms.  Sorta confusing.\n. Number 3 sounds like \"yes\" to my question about multi-tenancy.\nSay you add authorization by namespace.\nThen, the next thing that happens is someone sends lots of high-cpu queries and makes the system slow for everyone else.  Or sends to many metrics, and  uses up all the storage.  IF heapster was not designed to be multitenant from the start, and I don't think it was, this is going to be whack-a-mole for a long time. \nThe alternative is to run a heapster per namespace.  This solves both authz and qos problems.  If you have a free tier where you can't afford to run heapster per namespace, you can use a shared instance, but restrict access to it.\n. Let me test my understanding.   A request for $PODNAME metrics in namespace $NS would do a  SAR like this:\nyaml\nkind: subjectAccessReview\napiVersion: authorization.k8s.io/v1beta\nspec:\n  resourceAttributes:\n    namespace: $NS\n    verb: get\n    group: \"\"\n    version: \"v1\"\n    subresource: \"historical-metrics\"\n    resource: \"pod\"\n    name: $PODNAME\n  subject:\n    user: directxman12\n. Can you not extend heapster to remember the namespace when it stores stats by pod UID?\nhmm.  I guess that should be the namespace UID?\n. Would you fall back to individual requests if that failed, or just reject?\n. Say I have an nginx pod, and it wants to push some metrics about itself, such as number of requests handled since starting.   Say that the pod has a credential with username  $USERNAME.\nIs the recommended pattern:\n1. the nginx pushes metrics prefixed with that username, like $USERNAME-total-requests.  Also, $USERNAME need to have a role binding which gives it create metrics permission. Or is it...\n2.  the nginx pod  pushes unprefixed metrics, such as total-requests.  Also  $USERNAME needs to have a role binding which gives it create unprefixed-metrics permission.\n. ",
    "k8s-bot": "Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 26521da4aeb4440c154c9b3379dbd28590a275c0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 1e4b24d328180f931e49e7a6bd3373fb3e43c95f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit bf8dcc095b720cfdde821856b2e5e77fdde823f8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 19f8710693e81f5c860f15d1431e3662b5050603.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f8e29bc467165a432c939440142145f675d257db.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 2862c344670568a7c04f26bcdb0b595a819038a3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 38eb9000a1c9eb1dc2497d5b1a8fefcd1f519d8a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 7bc89a2148b79d7b7e3f61ff480edf99988d2af9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2b851ce919d4f8d0a88e548a292b29f60a162b59.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit a212a9dd5a8906b922bd6639665c76f1aa98bfaf.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4eb06643eca3d9fa405ece4b01beeefadacb2959.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4eb06643eca3d9fa405ece4b01beeefadacb2959.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4eb06643eca3d9fa405ece4b01beeefadacb2959.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a77549162e98898e79343b927e6a581305e2daf3.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 0dc333ad62f2d8c64016f6b92dd52b2c15b31629.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 0dc333ad62f2d8c64016f6b92dd52b2c15b31629.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit c0d82aab6736ab64805584ad5d2d88ba41c9ea9b.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 7400e1c44d577e727504644b4b5959e8b09094b8.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 1c2c1092c060384fcffceebcf385c6bc80400e87.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 955832eec805fc45a2c132b705506f188fae21d2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 6f7a367234570f5544ee672e267d5ecb4d076354.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 332551cfe3bb9383ca1c39d5e55e3d37054e51e7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit cae2b3a386a25043e061756fbb8709e15966cf79.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit b5c136e0c6e836eb0818cd84d32d30713e86278c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 5552fcebbfd8a0bf21bb571685796672dbf232f2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit f1bc44b40c5b91789573b5b67a256ff7e3dcac79.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit cf6f29b5b068faa568f0072004ae30fc5a17e974.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 42d3c598bde44fc8ca116dbab3961a0121c86624.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 44cd51350349460922a052f7e3df7ddd4291a86b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit b445c4f27972f15ad623b0321c5832e56ba98d5b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e412d1ad56e47664d8792278af6738b96e255985.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 69ed10931552335a3b28f998e5612c9a3568a148.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit b4d34849f553c48624cb2486100338ce7945e3b6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 9d12a2efc52de26ec91b955655c0430aa41e1862.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ab3618aaaaedb0156c1f6f8bfd43eb5fc7ee505b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4211ea190bff9be08dee2b7564caf31566271208.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 477e222cfd431d30a67ac948978ddaad5290c9c9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 59e46a7853a99a14ab9ab00c6e12879d6d3363ba.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 90a48c657a4d936cd08ba415ee8a9a05d408789f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9340760983d833e320d5ab13094928be42db7853.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 6666ed37cdf7013040c0526e390bd8da82182855.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ceb7679c4d5799d589dfb47e621fdc74e2642d0d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 38b402decf9fb222b0eab822b7e9b6e8946f8180.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 688072fec75156edbe391918fa7b748b4de1f843.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit cf544ac62010b57e434a7512c388f47e736d6818.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a6a946e278e8fb04a19e30648b72adbcd9f0c4f5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 84b3be88f85e6a621aa1181f777be92564d48431.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 295f1a07c263c019e5e9da45f4ebe24b1927418d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9f164343ef22677ea88db29361c6b2ad939af69e.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 13d5968e00d84ed28ce24214edf4a782fbcb449f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ce46681a7adf1b2673e28a93b23526da7aa103f1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit da014e6bab8d772ff00a42c317b895087bcfe5aa.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3f59137c1f4a9bc0bb557ae2fba0e4943f59f88d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 079e21f4ff3e83b3df9a7d0d4faecb6cfbe5417c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit fee46ceeb4734cf1630592a8f01bacf6e8a4ac91.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b8b97ce168c8a27ed0f99cecdaff84a4063d233f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4b7595aa442ebb0da54144666dd863a5f20120fd.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8ac7eed69bf9acddeba8bf30bbb1b63f087574a5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 5aaa7c56f97806dcc264be1fe583cdcc192146fb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e7e4b021c8961dc216eedfae38e74b20ffa4d07f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 426a00bf87dce5bc3ace02c46fe54eb745659f67.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a9d66d34ed9396be2c725b28fc606c103767b653.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit ec6b172153e47d05e0f7b1e61232d37019131481.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ec6b172153e47d05e0f7b1e61232d37019131481.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 454a3e9aa4bd84465cdd90399d1d6f359e341e48.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 7e56086afe8f7afc8df76ddb8cad89200fc6a326.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 7e56086afe8f7afc8df76ddb8cad89200fc6a326.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ec4b623e20cb0cc27f18e9e885d0315cd7f34e0a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 267e6698337bae837be30c8d5b93a6d46f106fd8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e94190797b98a2533615c258b692956ac523ded5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6179e77541e60c0812456e84f0da6b6fae66c809.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0042425f9c6baaa55c8e1d4921f794c0a33075f2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4aba0926b36dfcd2b2179e9a961585d0e031420d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d204da0cf40396dc33ec58b5ac8f2cc755327b87.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 709546787803570cc078c7c300e0ca6e2b862ba7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3372a34240a32a2febf9861c8190cd98eeb66879.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 25e58c6b791726098ee39b22149793dcf1e968ed.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 1e03b1d3ae7e74ff7dbfb0f4f3fdca961589e00e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 1e03b1d3ae7e74ff7dbfb0f4f3fdca961589e00e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 22ceb4f72f8b7fa31d3292bdcb7ad4e6d6145d8a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 22ceb4f72f8b7fa31d3292bdcb7ad4e6d6145d8a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 609d3b5fb832af9a5a658cd65c8463116d48ab10.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4219b77ba984fd5a9761e879425969125cb5a620.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4219b77ba984fd5a9761e879425969125cb5a620.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 41d9a9697fd437f14d8da7f12448a425f08eea5c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 41d9a9697fd437f14d8da7f12448a425f08eea5c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 07bc9fc371350390f8990447fa056b467e61e55a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 07bc9fc371350390f8990447fa056b467e61e55a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 07bc9fc371350390f8990447fa056b467e61e55a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 917f991b123b14fba1cfacf7640d8a15e6cea60e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit b718196728d1d74e353f08bfd7306569a185dfa3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b718196728d1d74e353f08bfd7306569a185dfa3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 131b76afac5ebe6d8867b9ce4eb0d9ede64b97b2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 80909305193a4418884157bb5c443157d8d67cdf.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 683d5ce272a002b8bde7156843f4ae0df641eca6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 2d119e369e783aac6d12b07ea312889ea4c6a510.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 18a19b0d8a13359ea73e927df2693d95a4afc963.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 428cd31397e963a3cf34870365541e23e25a691a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 764cacd8d2ee7baf768f0b9f61ebd41422580219.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit fdeede1c519cfa1953669181efeee8fd985f545c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 5619ee6fb52d7b0f38ac04e51518da25eff7d186.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 603d2ef4280715a079c7d4d524c2516a38641e55.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 23fa2991a79d25d68fda6efb6b1ec44d77a937dd.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 1ebf8eb8c700c17bc2be37242225bb8440520367.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c65927677d57111cfcce91da98c013600ede1da6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5d40b1284e801a8a7759ec96b8a98633acba50bf.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 4b8d10a4b88168fd3b215e5f45afcdaa96afda43.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit fbd9f69f531b1d64dba9ca8b48db437d14be24a4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 1a13e9a3e815a61b677af9bbda44f9253a17f0cb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a64e9c66992e5ec819f098f0a54a58e1632c041c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 57465505de9538cc5fbd8bdc219f229ff5e5c96a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 238961f12d4f05e195ecb975824c6a4628b33c9d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d925143161015c4709793f19cbb5ec70d38d97ff.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ce56c703916732d3624939992dec61a188f2e197.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 548454d36f4e40102cf3d9c3e51f7ee35ea8fdd0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 57e5f016c584552e3c2708f515fa7140de1d3544.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e674dd2c89a16a99b6e0a52667a286686a2b20f0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 580bbd784fe4be4111dc6cb344cd4bb4a4603c34.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a1571435ecf89b78c70bddf7b152c89df4f2f7b3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e4964fd53e1a4d58efe131a51cc54fd3fa138642.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit bceb5bf1dbf4b9cbf565ffc5a856c5ac4ae29f3d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a162de915383e959de10267bee66dcb1bc0daa57.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 132e31626a6e273df908c650bb2d0385e9b27427.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 7480778448cb08352ee95db847b3fbb0a85c8ef9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6e700159ac9bc556bc8d2954064dd99b895c3fc6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6e4e2f3b33186160640c6571eada55435914557a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f4a24c14e1e7ec18984f05ea8f1550ec5ae8a56d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d672b39b18ef0ed97181f07234e2bf5e76cc3a01.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit c422d6185b05e53f9b8bab0fd15470284d0b2f99.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 08ffa9d764c98fe84c62b009d02a1ab6a9919bd0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit af078d44b026d32348261c8d7a6710f379af2540.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2191b993eec1b22df2716d173a7073b3b3b8594d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ee91e150700fdacd5b5a7bfbec1089be73ac1be2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 919b94b1c119079f2ed6dbdea080a734acad2a02.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit a3c15c57e20ce19fb9cf1e4fb260ccc1fcf2af69.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 7b9a61a487512a68611425409c8512e0b0653edc.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit a66c0750acf2b8bd08d5c208a3c576da02acaf1f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a66c0750acf2b8bd08d5c208a3c576da02acaf1f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 51866a93a6f0a1d4efdcfd900d97513327de45f4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 8de2a3e2bd17a7d3203b04733a073173f22e1e81.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9a1c9ffe4c3d9c064fb32b2d9aaf7ccfa405c467.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 1466378bf99768f29bd58a32b0f4e1a0de6323ea.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 464e36abc054d8f8098203f71d92bed9ab8faf52.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 00160a3af5f9f31339c31c578312e33a4f113da7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 1031d8f9f05997d49c1a5990d2a6cc38fee352e5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 995c39c51c86b938f4c1b7d5a543b330f11c7d15.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d4fc3785b65bbdc80bb1590d155b6d8b033bb338.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3baae41c86226489bd58603fe3f31a1f6175d9ca.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit adf76b9acb97c8c23fb24b1ad75564770d900190.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b9104e7a4c475794b60428152518de33d25da734.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ea8337ce6e3be1ae9f1f9fb2905cb5021a2fce0f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6cb3a77e37d9d501def28da495cd65ac248253b2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5b1ee7f0a001ab6bfcfebf72cea8aee0243e2b65.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 616515faf3b64b8c1261b6593140cb2b6b49e0b8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8331d5bdf2fcbe6c17e5ef9e0ad139f1b76e2562.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4ac27bc60a1c50badb89766306286bf92f2ffc82.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit c743e04a398c104c455f3925ab371ac10bf24309.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4cbd820f2b9d8cf5e8f4218f579baad82c90655f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2a37865eea1913ba7e1316a78cef1cb4a52dc15a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 09ba00ca0d20cda6ddd31e350bd2e7aeee46b987.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 5776db0a361c4062933fb04eba2322e52d08db4b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5776db0a361c4062933fb04eba2322e52d08db4b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0fdf64c293c48334d872bc1045cd8db0592f1e99.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8737d1b0246509d1c276bc8c1482abe4b45bbfea.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 19dcc97342f180ddd0b86c1813a618f4261cd017.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ed5c20c759b8e87c6e7b01bbbd18f98391a154ed.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 36b256342586c9613169aea0955652f3cca15daf.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 1c8587046d462cad1fe61d13552d4e9dee901ca4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 1c8587046d462cad1fe61d13552d4e9dee901ca4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 98a84ea66a0166afa2bb7b6ccef7fb667739c7e9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit bcc33f234486b4607a6974b39014891173ab205f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5abd0667e58e50c94d2fbe7b87251c043f58d65d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 2014cae0901b301b2be7cb94d7966a2fe566a6aa.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f356e2e5257af12de58b62c162728f9c162a8440.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 99d6f0f93c55e5945877c9fc6dba241bad584087.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5d22fa1f0f979b24f306f7b3453c76f784f33d88.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 21b425dce1f63cbdb37898c4d431e2551ffb04f9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 131a2baea05f210a560c1b282e369121f44309d4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit db4747fa25b987cd41b07659a357de6baf845e43.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 09702b3a4e096f93a0d72a352a4abd21a36b17f8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ebc8cdb4c4b02c4b167bda2e7193d7fc2434e3dd.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 5728cd8d10c1a71e1d223bde509971533ab36827.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 87cfef7e307baac0b090195fae5c636c8c34cd22.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit ec180504eb069a592382c3a1adc0ffc563954225.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ec180504eb069a592382c3a1adc0ffc563954225.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ec180504eb069a592382c3a1adc0ffc563954225.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ec180504eb069a592382c3a1adc0ffc563954225.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ec180504eb069a592382c3a1adc0ffc563954225.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 0f774d805a4c380dfdcda5e9dce0e2b2d1f0a002.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ca78b14421dff80c65c4d5fb63eca7eb495074b6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit a6906390d127c724d742d2c45095e338ef6b4c95.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit bb96db21da600108540f8d6081bfaff35799d383.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 015905631ccc08b305873272fbe64ee2dd6f4160.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 671b17a47ab607701803948d39219c7221a08ac2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 46f7c37bef4606cd43d1a105f58d903b4898775c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 76464e269fa54f17e6b3c69486639a37d33f5a44.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 96038526fea57a61c53bf68da519e49ea69e543e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 73264b9038be600f214ede46eed16c73536ccc03.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4e9226459c7bb78dc1a8a9d233527ba3fee7b0ee.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit b5290703dd6fdc60800584a9ea3f47691693966a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8d79b2ef912a2f40971c6409fb4df35aa611c6f4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 35120a685febbf6455034b61ff540d29e473af6f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit bee36df94038793d93641812d44796a9db550907.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit bee36df94038793d93641812d44796a9db550907.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit bab3d42f2bef0eff8a2e5c89e1ee649d1764dff7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2576a258b07ff9c28d3a8a3fe1e08d385ba8f743.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit bf93fa4969121ac7817ef27eb2fa95bdfd38bb0d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5ca0561d5722728d71bbfc129e24a5037d16c5a5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e59deec9725d387a23db3226609c07bc764dfc20.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5ad3b3721533e79906650bf311fe7635f6053a08.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit cd1d7bff2281ec2efb8dc8453bd178c6589dabb7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 236e63177e0c2918ce993ed954ea88bbc85a2d8b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b029aba3a8d9f85d3fe21f6b25c7abd2043dd0c8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 76ac097862878483e68a950e13c4ad27e8b045f9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 9e69d1044882cd5e5e2edad62b29b29e6725d7d3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9e69d1044882cd5e5e2edad62b29b29e6725d7d3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 590fd546a21bfd0d23ffbec4915c7d10913fc4ba.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit cc658ecf24ed8dd93a8aa9ec0222af510b0214cd.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit a471d154a41806b7619f3a773149ff94518ea0e5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d29713b38d7cad00572214849c9860ae6dfb45f7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6b08e3a609ae11d49f0c2ee4292d051303a5df33.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 069c2f5bfea203b3c3c3a8f148509372ffab9aa2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 8a770c757c5e7fc4326dd0bdfb05b13d1b78fd33.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8a770c757c5e7fc4326dd0bdfb05b13d1b78fd33.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3ced92ca1d47f0c0bc090fbd086aec122d7e2f06.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 2e615c8cc80d7c79cac474033ebfb842bb03aa56.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2e615c8cc80d7c79cac474033ebfb842bb03aa56.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0546247d84091e2700e044d0463eaff2c40e08ed.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e7bad40eefb48a0d51d0fcfc85608a1a7be05bb0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b8d642a6ea34be3549247f7ca9e9eedd29d66374.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3cdb1d2f49683eec258f5838f826959744ee2a4d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3000fd191b0512b0916a5065796a09588690602d.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 8f102e1bf63d4921c262538e27cfcb21709a1c18.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 8f102e1bf63d4921c262538e27cfcb21709a1c18.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit a00e465dcd78a2d2a28ec4d583ca28b50874aa7b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0b5008136a9b748bb8ba15509c0ddaadeedaf6a5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ff625be3c348d5ea255cb33454d52a4044466a98.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3f851bc077bf503b6ae21b1b577819ef19faa27f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b978199543c3bc5f56a35c87f24fd5d9b98c8d97.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 5685148a3fde112dbe1499964ab26f947c27c696.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 5321cae2a273d46158fc955f4b29fb2c6f45cd84.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 735c745e99783acf25c816678fcc428cc64b735b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4863453316b920d803914874781e4917dd5747ce.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3826e2102f4b25c87f866876ab7e0d6335167991.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8c3aa0e11aeaae8eb178af3dc9dbd18026f8542e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 25c78a901ab7e943786bc6793004c6689a4cca51.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 7e874d171a32eac23170633d9785fc10b49b5ada.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 401604481eedfef339f48cb90c7752eebe6139d9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0e34cdb03ba45a3b3412890b418bb49fe09c290f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c85f377263b8c5d17848f1c10619886c13a63a4c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6e0457dc8ac230114555dec4c0961190756e6f4e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 0d1bba2429be5cd0c35025fd0c21c06b4e49ce04.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0d1bba2429be5cd0c35025fd0c21c06b4e49ce04.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 09c9c269102a528674ed762bbb957ba6f5a54d7e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 09c9c269102a528674ed762bbb957ba6f5a54d7e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 325304051aae02066caed5a19d13394bc4a1e9bb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 14da9ee6ea477d650b3a137e23ff164843845304.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5b59c06a5da63f0098c290d7785caa9a581d3501.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0306eb14674613f7a65277807029b5bdfae5e1a7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9365912462916b87bfc4c30d0b4b094ba9bcb0bc.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 899cea1e32ebbfc892a0285ee3732782787b23fd.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3ca8b4fb0b7d469e5334772e6b4a736834bce03c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 73bed9b1b4dc3dc5092726cc8d129cb1d44519fd.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3242c3bebaa8c381338bad65476eb2ac35e137ba.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e781e65e1eaa3d8aec55ccf65a11d644ce939039.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2a8ce727c9ca41a6cff9951a2d61542c498344ff.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e597d80d71aa4991b7d5c4e54a7c12f8943308a0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4096c5eb2b3cd272abbc5003445751c8af5be26d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ce296da0dcf1b85f0be7a6d09b546d7d377a5741.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 948642f1d65c4a116322b44eaa0e5db8a6cea02a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit fe2585155251b892b9905e54f98aea1fb5a67541.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 24fb1740ef5fc60fcdfd9893afca90e2812e6d00.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 756f0d5f2c08bdc5e5ca4e19b5f841720a1fd4ad.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 756f0d5f2c08bdc5e5ca4e19b5f841720a1fd4ad.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a4b66834ae78d687188fb5465d618a0cad13fe7c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 84301deb3a1652097c1969b2b6da4691e982c26d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8bac5b435d2f75946ca69b6919ae653af3aa1b87.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit dca27245b29088050c13e70182ea47616159c347.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 60f38d3677c0b2b96ec8236baf81a8169e6a9dd0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 48ee6fc3b997baf345f9b18675df81794a2735ec.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 54ecf0e9b4d078b7d901b0cf9edad9e66f5d5f31.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b884fb90746dd240cdc056d3f4ab745110807814.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 1263880ae9dd0e17fcf7656600d3b2c5eb7fd355.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 258c7533bd9ccc1c637bfe13e908f40f1e78b6f5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ff2c190f73632eddac26059db7f6afdd739ae848.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 7f79318c74cbdacd0b05ae8fa261fed7e9a9c5c2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b6f50e7919379e7688b226f41b79627f23ef5757.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 2ed506d9c3d5bc370b1ccd37ab32b4b826cfbc0c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit bf8ddb30aedb8421207f08b7c58223704a939c0b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f234c0859ef9017474e5bbd85a187e2a226f5fb2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6591927ed1274201fbba82a3e2183d19ba958da7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3c3d0c07fef1c51dc53e0a28037e61c8d7e85051.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 1969a6f6bd56f3ccf128ac15c61f7a945a60911e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2de173c30c70947578d0230b4561dd7d6ea49d73.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8de977389de88ef425fce4a268dc812f03211f18.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 55cbbb4a0efb82d60f7541eaec84211050062919.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 744b5ccb363e2af2a4279b1d47aa055cdcbdbc28.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 4d653385e8b4f41f77af5e897d91d7878590ad6d.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit c49a0662e7317cdc31a150369346dd0a37fa71fc.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c49a0662e7317cdc31a150369346dd0a37fa71fc.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 77d7660c3e314042e613ed73e4ecc6a1fb9ce3bd.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d58a4c60cdd6aab714a55b5043337fd2b6557d4c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ad14bd2d72fb86ec2b2e90daa251ca6ac58abf1d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 0c1fa173a45f20837baee6571ec46b55f61f07e5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d646a0ed7d26d05bf10fbbde5158a34b51de77c0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4310fa9bc4073b6f417c0d93a0d74f783cdd2a13.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 6f96e580fab05196a48c789f4780d623dd4acab2.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit d22e01c70960e4310636ea466d877cc55eeee641.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d22e01c70960e4310636ea466d877cc55eeee641.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 86147e7fc6ab61caa83dc96bcf48a67393a352f1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit b81086eb86bad9fe677b7b435a376cd7c29e66c3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit b81086eb86bad9fe677b7b435a376cd7c29e66c3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit f979239bde59f73217fd6abe5b70e810962b0097.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c23569bbbedb5ca0cab0491bc673a899c706d909.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 56780bcd78f3eb8cf59c7edf7e0276a5badc521b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c7b78823e1ded1b119b19d510b0c12e0050ded4d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3834a5f5ddd93b82de9aa893279d1ad3075edfcd.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3834a5f5ddd93b82de9aa893279d1ad3075edfcd.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 8dc8e3c76802516b60116b8ae1597e9fc5ff0dd5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4f162f407907703d731fd5037eec052085d7e63c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 66b8f6866de6fd256d8cbfd19e951bf584c824e9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 66b8f6866de6fd256d8cbfd19e951bf584c824e9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit a3a804843fbe45bb42682dd2dbd027b56cadbb05.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a3a804843fbe45bb42682dd2dbd027b56cadbb05.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 28b15ae7dfe343e1d3bb8c8e1477bfb74e1801b6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6a7dce2ffc6f710fc13fa62b6c219657e5cdcdc1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 58341085aede7d3e6312b1edeb095089b556bba4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e84ab8f68c559f93504c3fc14890d727b50c7dfa.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d642641ccde111d2df2cd893ee77abb8d7dd2345.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f355bb93dfc10a7ed64a27f4e59a977ec2cd7a3b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b36ca14c109b1bb31f4871e80b66db66f2231e8b.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 1202b19972940e25647d2a8a8d94d6c2ab0bd734.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 1202b19972940e25647d2a8a8d94d6c2ab0bd734.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 1202b19972940e25647d2a8a8d94d6c2ab0bd734.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 37c3c9a19e3a9f7ff6b0abce307767953c5acbe6.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 0caa4badd6bdcf577736482812113fea15aa4330.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit bdbb6b17aabe89a2865f6f3f03eded76a30a9452.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit dfd777d14912292edee6464c675d7d1375fd1e60.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit ffe80578e491b13ddd178dd5a3c9fbc420ab0d55.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b91a8c453346d4424cc7c10a3cef9bbb6e8fd6d0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 54fb40a93f00982ff7c9a31ea2f66a4c7c61b344.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a875bad2b9e6a5c016b40f0926b59918b615db81.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e13d75946d41b81e5489044f39c393da7effc92f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 82fbb8a2cd81913b90b6cbd08429ec4a5c8c6861.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 352b8e7a43164126587f0284bd2f8a6d55d31227.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4498d47f254dbbc915b19a4708f9926554fbbec2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5d367690b0b19c65c2b41f97048dbf913fb780b3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3dbf2642b1ebc4f7af3c5c2f291b9a12b45f99d2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5d367690b0b19c65c2b41f97048dbf913fb780b3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit bdfc4d9bbef27df0cb011c47098dc9d5bf05e61e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d7e6d5cfae6a53be10eb2cfd079ae942a850d156.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f42437e85d57a2a0888a25b07ce002349f9c51f8.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 423d261ee3309fffdd82ece37b081c551b9b01df.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit fc4966880a56a6a3ab158e7ef5b090d2fca5b8e8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 25dde94d2bbb69505d820660c0dc55bbdef3d584.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 8a664375e73c202ec3b3fa0e287145f59fd8b760.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0e38ba16814e23bfc8e95bb3c3f3e6573c94605c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0b77b1d72ab2ce3e55935af51fe62565c58eaeb8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d7f76cbff1928fde111c2e083be65c2c24830331.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b2430677d331a1e0729b562824e1c873bd575cff.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 30f7bccf1d250895bca07a0d7dfa9354136857e7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c5d9b6fd25e8dd88463ce2b1f361e3b3e833ff1a.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 25b6856a187fde74f8e175b4b271d56a1fba3749.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3c8b4490e94d762bc16813dd8abb521d1a60875e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3c8b4490e94d762bc16813dd8abb521d1a60875e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3c8b4490e94d762bc16813dd8abb521d1a60875e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d2448fc005732fcfb2c022463d4c6514118ec708.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3594bb24b11a06cff505e1c536a9899552aab6d3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 61ba9df120af327c45674d8cf6e68198bfd2549a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 61ba9df120af327c45674d8cf6e68198bfd2549a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 849162c79e9a73cbd96f6a7764e09e78add97abb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 849162c79e9a73cbd96f6a7764e09e78add97abb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 96218b21d098f59919a6bb72321680723a1bbcdf.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d42b7537891eb890efe1526a11a606f21b493c92.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 176bb0383d321284aed5da559d1ac9c8a1839e81.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 176bb0383d321284aed5da559d1ac9c8a1839e81.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 11395a544718a2a9a273e1cbd877d53e2661b66c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit efb8b1c134ca8fb98312229154003c78f05ece56.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 9d113cb276e8347ffc6d86f7b42ce3f0511a402e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 831122304d87986131bcc9a4960afc9e719edf52.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 458871835da54a3e6fbda505448cce4f3819629e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 9086bd542d4af44c66760693b4f92c9201007e99.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 666abd365826ca81678d47b967f7e5aa1b61c0dd.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 7ddb53325489d0760f4a1aac27ec16656536c16e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 7f942e81d153ce99a86404d9ae40c1b1b90bf98f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e9657592c6690c2a443e0c9928ae26759f101db3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d8d7bbda12fada302d884c17fe83454ec4a436cc.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 35b9337f0ff0a5de88cb2e658c0699d0b5af3742.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 503de76c0019161564c24e18c149d1e04e741e2c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e6c4df81e2c6bf01fd0152c19577a2e36973dd65.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit cef9009dfa07deb4a4adfc4c82067c974a9dac28.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 750254170485e2b839aa9d0a7c85d0e7cb2cba5b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 0b1f92f16bc39ea8ac5adb1065d86416e2ac2a0a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 03e992e87a9d62c3ee90b230d9b0125a5523cfff.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c5525b72360b69e1ea80f03b844ee42245568db1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c5525b72360b69e1ea80f03b844ee42245568db1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c5525b72360b69e1ea80f03b844ee42245568db1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c5525b72360b69e1ea80f03b844ee42245568db1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c5525b72360b69e1ea80f03b844ee42245568db1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4b9668d3a2cf8a932a2c49614fd0d2f6c9a83027.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 24219b04f0c5eb18db7e64d76eba97d713b9f11c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit bca81f7aeebebc53f3f8a284931c5a699ed5abbd.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit eb506cde94799a112b65deb4efd3000b835f7a71.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 3605e8304a5eed0437300a7489ad10586e950d4b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 7a7ed540807fc2f2c788ab087ca1933d3a2606fa.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 7a7ed540807fc2f2c788ab087ca1933d3a2606fa.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3a360cf082b9c47592eebac7c8792485203afba6.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 08bfa79218b662919f5c20e86cc59344c7e3009d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 405624aedc5ab33120ef92429d78b1c2a3abecfb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 636c24aef0b8a63168bad858d3179f795d4b21cf.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e02c51e95053da40c17d95d3505913cf48700d4b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e88daf6a88bae1b3b967b1e7dc81269f99540fd5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit bf9892a453fec1140c61389f40e1992feb26d2ae.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ddd126e5d1a545bd8a61e999297d1a25445d1864.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6481a3364f2e6367972256a69b976ae8b198e5e3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a3f1a5c7193b116560d8e7d35c1d54fd6d73632d.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 6f4533b05bcbacbab11dc782867f5b75c2d33a60.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 522c8113148f15ac78006f5fb87e8601ca7da8ab.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 162fe811cf48f865a55dcbda7fc16dad30c91730.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 162fe811cf48f865a55dcbda7fc16dad30c91730.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 48925dcfdf7567167a2cecf376cb4fea1b8eecb1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit bd1db1e55f33c17381df19fa595c75a22b9cdf37.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 77c6e2314d69410f034ca6c2ad68e61d6f002b0e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ae8b6ca271102f68a3db8325544279cb6f6b72a3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e61cc25e8b1a991970f4b1010337b14e811509e2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c51eb6cec98cf842de9c8fd24c9a629005297676.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 075ad3fca5ab449226a5ec867f09794ab8162706.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e73af98277168e33c0d810a35e08ff67da7375f4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ae429a4203db8641a2b7f4ee802b64a59d1a1abc.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 1c09ed503548fc019d7eceabae785bb84f80a526.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit a53f7c9bc9416d3e1f5586e257db124be92502b3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit a53f7c9bc9416d3e1f5586e257db124be92502b3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 7903529be926bdc980f2090768efafc16a56d4aa.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5e9b31d257065f047f96dbe92c73461316991767.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 113ac8c8de8da72382e5fc209350d6a368eff0d2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 88dce7bbd7a094bb5ddda1c496a83b76cf91e816.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 89f9973285e1f7a1a6542171187c346d9323843e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit de80757ff497c218ed1b276ed58363a3f65bac72.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c2f01d646627bec659fa9b33aa694c8e1d0fd758.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e0ee44bba8efe044af99b74da9467495aa626545.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 27c18351d10f46797dfd0df7ba46e5c15638d044.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a1195a608fce21ff060853dd06e2a2edfe7ab0e0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a6ecb472eebe5b3d648fed38c10fbd3a634fc615.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 93b007151b20de87f7673d7b2c540737973439ee.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit fbbc2fc1a81cb102118de468df75e0e80cf31c37.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 760233b7436fd104e5bf9110236ddb9cbf0ceafa.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit f725ca2a5d578b4a5951a64a6758d71658fd34af.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit b42db1ae934579b2924f4a9bf5c6cdbeb001dbcf.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e82d17fd71a7aa9002f37b2da5606ad1a753c1fe.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4fff3d28b0d69dbaf9448a94ebc1d45603de87b0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9ed22f1023993456d0308052fed47b02605947b0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 160b74bafca2c048ae2e2be34fb043bf73c6459b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 2e358de382f6be7e3778021edcdd98f01d38b0fc.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit cc543aae2c1f52e7845820d422ecd0648a2e5789.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 212ae1891ce8365d6f07e07527136db6a5aad567.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d73b35465f52c9327a065fbfdb306a3387cb1b13.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c3aeaec8774015e3caeec612f5e7d8970c99add9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 62b67d30c2178e90b6007bd2e2ff1ac8053df34e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a31b407edc117e03f91e1cd3bb64a71a526c9875.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 823ff3db89d9fba6cd577153b5013016f85c3d5f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 9c159bacc51a6c7d6e2df3766a1911262e996042.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 237f719a2b7d59f9b29ae9c6fc1c3a75db81288d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d96372b1199d8d795ae166cc9d6f69b23a465204.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 81686b6d162890db850abc6330ce0f17e88b80ea.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 09b58e675440a422e7b34e44e862140cf6a1ef34.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit fbdca238694f7ae56406ae7de1f12f5ae17b7523.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 138ec3e46564b557cfce398ebf4b2d149b4cc160.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 55717422be56fe298b9dbec8fd3c5ae536eb75ed.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3bb5ef4b4f0f37312b57a95c917b15039c17c7f9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 17ce5bb8211772e14db29528be00b2008ac53738.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 2ead1cb65f5b4ef229a51f0a5ee509a76376dcc0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 2f2ba9cf004ce53ce3f5415fec4e54213a37a943.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4f41393e3019690a69f41772278b30d7b41e8da5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5765f363412881b56c681526a005382c2d98aa4c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b62b9591aea2d5672da1c40666f86ee9905e62d5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3b049a693da15f3918f7fa326032630c3f0fe335.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 2aa39384be73a78639c71b19f49536a84008bfac.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 57cacac3cc5db7e0b8ab5193797fc70326f6e214.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c0ab50732411c3840c27d07132b357ce8e406ca2.\n- Build Log\n. ",
    "rvadim": "Thank you, it's now works.\nsystemd unit file now looks like this:\n```\n[Unit]\nDescription=Heapster monitoring tool for cluster\nAfter=docker.service\nRequires=docker.service\n[Service]                                                                                                                                                   \nExecStartPre=-/usr/bin/docker kill heapster\nExecStartPre=-/usr/bin/docker rm -f heapster\nExecStart=/bin/bash -c \"source /etc/environment && /usr/bin/docker run --name heapster -p 8082:8082 -e SINK=influxdb -e INFLUXDB_HOST=http://10.21.12.35:8086 -e FLAGS=\\\"--source=cadvisor:coreos?fleetEndpoint=http://$COREOS_PUBLIC_IPV4:4001\\\" kubernetes/heapster:v0.11.0\"\nExecStop=/usr/bin/docker stop -t 2 heapster\n```\n. ",
    "luksa": "I've force-pushed the rebased commits. \n. No, I wasn't even aware of #269. \nI'm runnin heapster like this: heapster --source=\"kubernetes:https://ip:port?auth=clientauth&kubeletPort=10250\", where clientauth is a json file that defines paths to the cert files (docs on this are here: https://github.com/GoogleCloudPlatform/heapster/blob/master/docs/source-configuration.md)\n. ",
    "shwetalakhimpur": "@biswars - I am unable to access the Grafana UI\nYou stated above - \"When I access the url https://172.17.8.102:6443/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/ then the ui comes up.\"\nPlease help me understand the IP:port here. \nIs this is IP which is running monitoring-influx-grafana-controller-y3qn3 ? This is the IP listed under IP or HOST when we do \"kubectl get pods\"\n172.17.8.102:6443\nThe port 6443, is this the port listed in \"grafana-service.json\" ?\n. Thanks Vish, we may continue troubleshooting on the other thread, I have provided more details\nhttps://github.com/GoogleCloudPlatform/heapster/issues/338\n. I seem to be on the same issues as this thread - stuck at 404 Not found, \nhttps://github.com/GoogleCloudPlatform/heapster/issues/203\n. IBM Softlayer, CentOS Linux release 7.1.1503 (Core)\netcd-2.0.9-2.el7.x86_64\nkubernetes-0.15.0-0.3.git0ea87e4.el7.x86_64\nThis is a cluster of 3 nodes, master is not set as a minion\n\n\nkubectl get pods\nkubectl get pods\nmonitoring-heapster-controller-t8p51         172.17.39.12   heapster       kubernetes/heapster:v0.13.0         119.x.x.x/119.x.x.x   name=heapster                                Running   8 hours\nmonitoring-influx-grafana-controller-y3qn3   172.17.39.13   influxdb       kubernetes/heapster_influxdb:v0.3   119.x.x.x/119.x.x.x   name=influxGrafana                           Running   8 hours\n                                                            grafana        kubernetes/heapster_grafana:v0.7  \n\n\nI tried this, it says 'Unable to Connect'\nhttps:///api/v1beta3/namespaces/default/services/monitoring-grafana/\nIt is not clear to me on what should this IP be - the Host machine IP of the Kubernetes master or the IP listed under services.\nI am using unauthenticated version, per your notes in \"heapster-controller.json\"\n\"command\": [\n                            \"/heapster\",\n                            \"--source=kubernetes:http://kubernetes-ro?auth=\",\n                            \"--sink=influxdb:http://monitoring-influxdb:81\"\n]\nAlso, removed token-ring from volumes.\n. [root@minion01 2015Jun11]# kubectl log monitoring-influx-grafana-controller-t3m8h grafana\n2015-06-12T18:21:11.817897874Z Creating config file:  /opt/grafana/config.js.tmpl => /opt/grafana/config.js\n2015-06-12T18:21:11.818159522Z\n2015-06-12T18:21:11.818173274Z Creating service proxy: /db/ => http://monitoring-influxdb:80/db/\n2015-06-12T18:21:11.818179873Z\n2015-06-12T18:21:11.818185007Z Listening on :8080\n2015-06-12T18:21:11.818190369Z\n. The kube config does not have much data, is that a problem ? What should it look like ?\nI have been following your guide, will try to make it externally accessible.\nkubectl config view\napiVersion: v1\nclusters: []\ncontexts: []\ncurrent-context: \"\"\nkind: Config\npreferences: {}\nusers: []\nI am trying to understand the issue here. All the required heapster services and pods are running OK. \nWhat are the potential things which could go wrong -\na - I am not hitting the right host/port/url. I am certain about my Master IP and trying with that.\nb - Some settings are incomplete and missing ?\nc - Which logs should I be looking to troubleshoot ?\n. I have set the Public IP grafana-service.json. \nThis shows me the service config on master nose 8080.\nhttp://MASTER NODE IP:8080/api/v1beta3/namespaces/default/services/monitoring-grafana/\nAND I am finally seeing the Grafana UI at:\nhttp://MASTERNODE IP:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/#/dashboard/file/kubernetes.json\nI am on the next piece on how to make it get data from the InfluxDB.\nThanks Vish for your support so far.\nhttp://MASTER NODE IP:8080/api/v1beta3/namespaces/default/services/monitoring-grafana/\n{\n  \"kind\": \"Service\",\n  \"apiVersion\": \"v1beta3\",\n  \"metadata\": {\n    \"name\": \"monitoring-grafana\",\n    \"namespace\": \"default\",\n    \"selfLink\": \"/api/v1beta3/namespaces/default/services/monitoring-grafana\",\n    \"uid\": \"28227eb9-11f9-11e5-b3a8-06b8863a81b8\",\n    \"resourceVersion\": \"340974\",\n    \"creationTimestamp\": \"2015-06-13T18:22:32Z\",\n    \"labels\": {\n      \"kubernetes.io/cluster-service\": \"true\",\n      \"kubernetes.io/name\": \"monitoring-grafana\"\n    }\n  },\n  \"spec\": {\n    \"ports\": [\n      {\n        \"name\": \"\",\n        \"protocol\": \"TCP\",\n        \"port\": 80,\n        \"targetPort\": 8080\n      }\n    ],\n    \"selector\": {\n      \"name\": \"influxGrafana\"\n    },\n    \"portalIP\": \"10.254.231.84\",\n    \"createExternalLoadBalancer\": true,\n    \"sessionAffinity\": \"None\"\n  },\n  \"status\": {}\n}\n. Hi Vish,\nIf you could please help me with the next step.\nWhat works -\nhttp://MASTER-IP:8080/api/v1beta3/namespaces/default/services/monitoring-grafana/\n==> Returns the \"monitoring-grafana\" service\nhttp://INFLUXDB-IP:8083/\n==> InfluxDB UI\nhttp://MASTER-IP.166:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/#/dashboard/file/kubernetes.json\n==> Grafana UI\nWhat does NOT work -\nJS Error on The Grafana UI - click on the Orange/Red Error image on the top left of all the graphs.\nMessage:\n            e.columns is undefined\nStack trace:\n            c.getTimeSeries/<@http://MASTER-IP:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/app/app.496fdc2e.js:18:7167\nm/he@http://MASTER-IP:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/app/app.496fdc2e.js:7:30701\nYb@http://MASTER-IP:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/app/app.496fdc2e.js:7:17685\nc.getTimeSeries@http://MASTER-IP:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/app/app.496fdc2e.js:18:7129\nk@http://MASTER-IP:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/app/app.496fdc2e.js:18:10178\nb@http://MASTER-IP:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/app/app.496fdc2e.js:7:10053\ni@http://MASTER-IP:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/app/app.496fdc2e.js:9:30859\nj/<@http://MASTER-IP:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/app/app.496fdc2e.js:9:31046\nRc/this.$get</l.prototype.$eval@http://MASTER-IP:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/app/app.496fdc2e.js:10:6185\nRc/this.$get</l.prototype.$digest@http://MASTER-IP:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/app/app.496fdc2e.js:10:4706\nRc/this.$get</l.prototype.$apply@http://MASTER-IP:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/app/app.496fdc2e.js:10:6450\nh@http://MASTER-IP:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/app/app.496fdc2e.js:9:14669\nq@http://MASTER-IP:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/app/app.496fdc2e.js:9:16489\nkc/</u.onload@http://MASTER-IP:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/app/app.496fdc2e.js:9:17029\n. Per the heapster/docs/storage-schema.md, list series should show the all the metrics being pushed by heapster into influxdb. When I logon to the InfluxDB Admin, click on 'grafana' db, Explore Data,\n\nlist series\nI do not see the metrics. How do I check if there is trouble in heapster pushing data into the db ?\n\nhttps://github.com/GoogleCloudPlatform/heapster/blob/master/docs/storage-schema.md\n. The issue is that the heapster process is not able to connect to apiserver. \n\n\ndocker ps -f \n\n\nE0616 15:27:29.027202       1 reflector.go:123] Failed to list *api.Pod: Get http://kubernetes-ro/api/v1beta3/pods?fieldSelector=spec.host%21%3D: dial tcp: lookup kubernetes-ro: no such host\nE0616 15:27:29.821323       1 driver.go:321] Database creation failed: Post http://monitoring-influxdb:80/db?u=root&p=root: dial tcp: lookup monitoring-influxdb: no such host. Retrying after 30 seconds\n. I am using a kubernetes cluster of 3 minions, where 1 is MASTER and 2 are MINIONS. \nI have listed the Public IPs to list the 2 minions in \"monitoring-influxdb\" service.\nAble to access the Influx DB UI on the Public IP http://:8083/\nThe PUBLIC_IP here is the IP of the minion where the docker has deployed by Kubernetes. When Kubernetes deploys to another minion, I need to use that IP to access the DB UI.\n\"heapster-controller.json\", has the following command for heapster, where it says - influxdb:http://monitoring-influxdb:80\". I was ideally expecting that it will discover the API service as the API-service is configured for the kube service on the minion.\nNow, that when we have the Public IPs defined, should this URL influxdb:http://monitoring-influxdb:80\" change OR will it discover the influxdb server's IP by looking into the iptable.\n\"command\": [\n                            \"/heapster\",\n                            \"--source=kubernetes:http://kubernetes-ro?auth=\",\n                            \"--sink=influxdb:http://monitoring-influxdb:80\"\n. IBM Softlayer, CentOS Linux release 7.1.1503 (Core)\netcd-2.0.9-2.el7.x86_64\nkubernetes-0.15.0-0.3.git0ea87e4.el7.x86_64\nOK. I will setup the SkyDNS - https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/dns.md\n. I can upgrade. What is the best way to upgrade ?\nPrior to that, i would want to get heapster running even if rather raw method.\nIs there a way I can hard-code influxb service IP, so that heapster can reach it without setting up DNS?\nAdd host to this URL or add sink IP ?\ninfluxdb:http://monitoring-influxdb:80\" \n. MONITORING_INFLUXDB_SERVICE_HOST has the local IP and not the Public IP.\nBecause of this the heapster server is not able to connect to the monitoring-influxdb. I could not find any environment variable which holds the public ip information which has been specified in the services.\nWhat else can I try (before getting to the DNS setup), or force the two docker containers to launch on the same host.\n. @jimmidyson - I am not sure what you mean. I followed this guide on Kubernetes cluster setup and it does say here that the Service IP is Kubernetes internal IP address defined in /etc/kubernetes/apiserver. I have the flannel setup which is in the 172.17.x.x range, and this is the IP being assigned as the docker containers come up. \nThis IP is accessible from outside. However, the service IP which is  MONITORING_INFLUXDB_SERVICE_HOST, is not accessible.\nIs there something amiss here - should service IP be the flannel range. \n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nhttp://www.severalnines.com/blog/installing-kubernetes-cluster-minions-centos7-manage-pods-services\nYou should get a 10.254.x.x IP range assigned to the mysql service. This is the Kubernetes internal IP address defined in /etc/kubernetes/apiserver. This IP is not routable outside, so we defined the public IP instead (the interface that connected to external network for that minion):\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nI was hoping that am getting close to seeing Heapster push data into the InfluxDB, but your question did throw me off on the basic setup.\n. OK. then I do need to review setup. I did cleanup all the public IPs, and I have only the service IPs for all the services. My flannel network is 172.17.23.0/16, the Service IP is 10.254.x.x\nThanks for your prompt responses. Appreciate it.\nAnother question  - on heapster command, do I absolutely must have the source=kubernetes ?\n\"/heapster\",\n                        \"--source=kubernetes:http://kubernetes-ro?auth=\",\n                        \"--sink=influxdb:http://${MONITORING_INFLUXDB_SERVICE_HOST}:80\"\nI get this error. I suppose this is because of the same issue where my serviceIP is not accessible across the cluster.\nError: reflector.go:123] Failed to list *api.Node: Get http://kubernetes-ro/api/v1beta3/nodes\n. Thanks :). Will continue tomorrow, past midnight for me. \n. @jimmidyson\nMy updates, based on yesterday's discussion, I cleaned up my cluster to remove all Public IPs, and changed the \nKubernetes API server inside /etc/kubernetes/apiserver, this is the same as my Flannel network\nKUBE_SERVICE_ADDRESSES=\"--portal_net=172.254.0.0/16\"\nI am able to ping the Service IPs across the nodes in the cluster\nNow the issues - \n1) The kubernetes api-server is still picking up the old IP , and I have no idea from where.\nI could not find the cache for this. I rebooted the machine, and restarted all kubernetes,flannel, etcd services but no effect.\nNAME                     LABELS                                                                     SELECTOR             IP                PORT(S)\nkubernetes               component=apiserver,provider=kubernetes                                                   10.254.0.2        443/TCP\nkubernetes-ro            component=apiserver,provider=kubernetes                                                   10.254.0.1        80/TCP\n2) Heapster when it starts continues to give this error - this is expected as my kubernetes-ro is not discoverable service\nThe API server is listed in the kubelet config, but not sure why it does not take the info from there.\nHow do I make this work ? Will I have to setup the DNS service ?\nE0618 13:29:52.448165       1 reflector.go:123] Failed to list api.Node: Get http://kubernetes-ro/api/v1beta3/nodes: dial tcp: lookup kubernetes-ro: no such host\nE0618 13:29:52.448165       1 reflector.go:123] Failed to list api.Pod: Get http://kubernetes-ro/api/v1beta3/pods?fieldSelector=spec.host%21%3D: dial tcp: lookup kubernetes-ro: no such host\n. Hi, Please provide your inputs ob the following issue -\nI am in this interesting situation, and let me try to explain. \nSituation A - \n1 - I can get the heapster to start with the influxdb properly, where I am able to see the data in the influxdb.\n2 - I am unable to access the Grafana UI -\nhttp://:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/#/dashboard/file/kubernetes.json\nError: 'dial tcp 172.17.39.55:8080: i/o timeout'\nTrying to reach: 'http://172.17.39.55:8080/'\nwhere 172.17.39.55 is the IP assigned to Grafana\nSituation B -\n1 - I am able to access the Grafana UI OK on URL\nhttp://:8080/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana/#/dashboard/file/kubernetes.json\nThe UI shows up but cannot show any data as the error shown is - influxDb Error: object Object not found\n2 - This is because in this instance the influxdb is not populatd with data\nIn summary, I can either access the UI or my database is populated properly by heapster. Both are not happening together.\nWhat could be problem here ? Port clash issue ? I have influxdb on port 4050. Grafana and Heapster on port 80\nmonitoring-grafana       kubernetes.io/cluster-service=true,kubernetes.io/name=monitoring-grafana   name=influxGrafana   172.254.85.167    80/TCP\nmonitoring-heapster      kubernetes.io/cluster-service=true,name=monitoring-heapster                name=heapster        172.254.17.236    80/TCP\nmonitoring-influxdb      kubernetes.io/cluster-service=true,name=monitoring-influxdb                name=influxGrafana   172.254.240.32    4050/TCP\nmonitoring-influxdb-ui                                                                        name=influxGrafana   172.254.172.226   8083/TCP\n                                                                                                                                           8086/TCP\n. ",
    "imesh": "Hi @vishh, I'm also seeing this problem on Kubernetes 1.0.6. Anyway to resolve this?\nThanks\n. AFAIU the problem is with the SSL certificate configuration in Heapster. The HTTPS call to Kubernetes API fails due to a certificate issue. I was able to get around this by specifying the HTTP Kubernetes API URL in the heapster controller as below:\n--source=kubernetes:http://172.17.8.101:8080?inClusterConfig=false\n. ",
    "lgfausak": "i've been trying to deploy heapster on my coreos/kubernetes cluster. i can't get heapster to start using the deploy/influxdb resource files. I figured out i need to add a token-system-monitoring secret, but, i don't know what the format of that secret is.  without the secret the heapster pod hangs with a 'Waiting' status, it is waiting on the mount resource.  I added a blank one with:\n{\n  \"apiVersion\": \"v1beta2\",\n  \"kind\": \"Secret\",\n  \"id\": \"token-system-monitoring\",\n  \"data\": {\n  }\n}\nAlthough this got me past the hanging, now the heapster just restarts constantly, \n2015-05-13T15:32:32.663447125Z + /usr/bin/heapster --sink influxdb:http://10.100.95.142:80 --source=kubernetes:http://10.100.0.1:80\n2015-05-13T15:32:32.663454073Z I0513 15:32:32.662521       7 heapster.go:52] /usr/bin/heapster --sink influxdb:http://10.100.95.142:80 --source=kubernetes:http://10.100.0.1:80\n2015-05-13T15:32:32.663459690Z I0513 15:32:32.662590       7 heapster.go:53] Heapster version 0.12.0\n2015-05-13T15:32:32.663466175Z E0513 15:32:32.662661       7 heapster.go:59] The config file /etc/kubernetes/kubeconfig/kubeconfig does not exist\nthe mount path is declared here:\n\"volumeMounts\": [\n                        {\n                            \"name\": \"ssl-certs\",\n                            \"mountPath\": \"/etc/ssl/certs\",\n                            \"readOnly\": true\n                        },\n                        {\n                            \"name\": \"monitoring-token\",\n                            \"mountPath\": \"/etc/kubernetes/kubeconfig\",\n                            \"readOnly\": true\n                        }\n                    ]\nso i am guessing that i have the configuration incorrect for the secret definition.  where can i look to see how that should be defined?  I posted a question about this on google containers as well.  I have been trying to get heapster started on and off for a couple of weeks now :-)\n. standing by, thank you.\nOn Wed, May 13, 2015 at 2:34 PM, Vish Kannan notifications@github.com\nwrote:\n\n@lgfausak https://github.com/lgfausak: Tokens are required for access\nto the kube api-server. Kubernetes exposes two default services to access\nthe apiserver - one requires no auth and is read-only (kubernetes-ro);\nthe other requires auth (kubernetes) and is read-write. Kubernetes is in\nthe process of dropping support for kubernetes-ro service. Heapster used\nto use the kubernetes-ro service until recently.\nFor the short-term, until you figure out how to setup tokens for cluster\nservices, I am making a change to heapster to make it possible to run\nheapster using kubernetes-ro service. I will get back to you with\nspecific instructions once the required changes are in.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/279#issuecomment-101787478\n.\n\n\nGreg Fausak\nlgfausak@gmail.com\n. ",
    "mvdan": "@saad-ali could you please re-do this PR?\n. Also an option :)\n. I'll try to have a PR by the end of the day.\n. @jfoy I can take care of the godep stuff if you like - remove all the godep changes and I'll do a PR on top of yours.\n. Running exactly those commands, the answer is simple - you have to give ./... as an argument to godep save. If you don't, it will assume ., thus saving only the dependencies for the main heapster package.\n. @jfoy funnily enough, I get the same thing you get when running save on master. It obviously breaks:\nsources/kube_factory.go:30:2: cannot find package \"k8s.io/kubernetes/pkg/client/unversioned/clientcmd\" in any of:\nSome of the things it does, like removing go-uuid, are fine - but others aren't. The best I can come up with now is that godep must be broken. We might be doing something wrong here, but the problem is just too obvious.\n. @jfoy see #601 - could you rebase your PR and try his trick of removing Godeps before running save?\n. Godep is working fine for me now from a clean GOPATH, without removing Godeps/:\nGOPATH=/tmp/go\nmkdir $GOPATH\ngodep restore\ngodep save -v ./... github.com/progrium/go-extpoints\n. I just fixed integration tests in #431, so let's run them again.\n. retest this please\n. retest this please\n. The last jenkins job succeeded. Should we merge?\n. Maybe I should have clarified in the commit message that this is for the kubernetes source.\n. @vmarmol just updated the patch with an empty string check. Made sure both the default and overriding cases work.\n. FWIW I'll update this PR with integration tests for the new endpoint. And we might want to fix the Unregister() TODOs before merging too.\n. Pushed the four changes in four extra commits, will amend into original commit once you're OK with them.\n. I'll try :)\n. Seems like gcm needs code in its Unregister(), since its Register() then calls addMetric() in gcm/core.go which sends a POST. We probably want to send another request for Unregister().\nhawkular also does save some stuff in self.reg and self.models, so we probably want to do something there as well.\ngcmautoscaling looks very much like gcm. Although it ignores the metrics passed via Register(), so I'm guessing nothing needs to be done? Or perhaps just unregistering the autoscaling metrics?\nThe rest all have no-ops, so I'm betting on no-ops for their counterparts too.\n. I'll be back soon with another commit for the unregister bits, think I got the hang of it.\n. Unit tests added and I'm pretty confident with the Unregister() code for the gcm and gcmautoscaling sinks.\nDon't know how the hawkular one works so I've tried putting something together and left a TODO. @burmanm you seem to have written the Register() code, any ideas?\n. @burmanm Cool, thanks! I can't do that with GCM because of the GET/POST requests, but since as you say it's all local here, the GC will just wipe it :)\n. @vmarmol done. Thanks again @burmanm!\n. FWIW, I'm having issues with gofmt because I'm running 1.4.2 and my gofmt makes lots more changes that travis isn't complaining about.\n. @vmarmol just added the last unit test. If you confirm everything is ok I'll rebase the commits and they'll be ready for merging.\n. @vmarmol last issues fixed and commits rebased.\n. Rebased and opened another PR for the integration test.\n. Hang on a sec, the integration tests don't work with this. Don't know why though.\n. The code in the integration tests that runs docker and make is kinda broken, currently fixing it. Will be back with a larger patch.\n. Fixed and updated.\n. Rebased (it conflicted with another PR of mine, duh) and fixed the generate issue.\n. Yet another timeout :( but other than the container name thing this should be ready to merge.\n. This is really weird. I couldn't find a reference to this issue anywhere, so to confirm that indeed reading from an empty buffered channel doesn't block, I set up a play.golang.org example: https://play.golang.org/p/abMIOJa0FJ\nAnd it does indeed block. So this issue seems to come from someplace else.\n. @vmarmol now it is ready to merge :)\n. Whoops yeah, I left stuff around. Will rebase and fix.\n. Done.\n. FWIW, the old code used to do an url.Parse() to get the type, opaque url and options for the sink/source. Then each sink/source factory would do another url.Parse() out of the opaque part. The new code simply splits by : and makes a url.URL out of the part in the right. This results in simpler code, IMHO, and doesn't break anything.\n. PR updated with the two fixes.\n. Amended.\n. Cool! That was just a quick read, haven't gone through the code extensively FWIW. Also, you probably need to rebase since there are a couple of merge conflicts due to #413. Easy to fix though.\n. You mean using a github commit http link on a commit message? That's way too long IMHO. For a pull request name/description sure, but for a git commit summary/message I've never seen it done.\nI assumed github would pick up the hash and link to it. I could've used the link for the PR title, yeah.\n. Amended. Thanks for picking that up!\n. FWIW, the PR page doesn't pick up the link but the commit history page does pick it up.\n. retest this please\n. Jenkins is trying this on kubernetes 0.20.0, this is weird.\n. Note: I tried using a sync.RWMutex but I got stuck at escalating RLock() into Lock() when the cache needs updating. A sync.Mutex is probably good enough here. The fact that the better option is not feasible probably means that we should use some other strategy in the long run.\n. Don't merge yet, will update this PR after I've exposed the percentile code in cadvisor so as to not duplicate code.\n. I split up the commits on purpose, so as to have one change per commit. This is easier to find and read, and also easier to revert if any of these changes are wrong.\n. test this please\n. #431 fixed this.\n. CC @vmarmol @vishh @afein\n. FWIW, this is passing on a Kubernetes 1.0.1 cluster.\n. Included #426 into this PR.\n. Still failed, with the same error that #426 had. Agh!\n. Forgot to close this - was fixed by #450\n. Very similar to #422, perhaps a duplicate.\n. LGTM except that nit, which can be ignored.\n. Jenkins barfed again. But that doesn't matter, because Travis failed for a good reason. Looks like you're trying to use the code I exported in cadvisor/summary, but that code isn't in Godeps yet. You probably want to update cadvisor too, besides adding that subpackage.\n. LGTM\n. Okay, the gcm sink integration test is still failing with the cert issue. @vishh, you mentioned a PR that should fix this, which was it?\n. @vishh I'm guessing those changes need to be done on the other configs - especially the standalone one - for them to matter here.\n. Reping @vishh\n. @vishh looks like it failed all the same.\n. So that's the thing - the integration tests pass on my machine, both before and after that commit.\n. @vishh with that last commit we should see what the error is without having to go through the logs ourselves.\n. @vishh okay, so one can now see that the failure is that GCM isn't enabled for the account:\n\nAccess Not Configured. The API (Cloud Monitoring API) is not enabled for your project. Please use the Google Developers Console to update your configuration.\n. Can't fix it from home, but will do so tomorrow morning.\n. retest this please\n. Huzzah!\n. @vishh cleaned up my commits, jenkins should succeed again in a few minutes. Could you review and merge?\n. It failed for some weird issue. retest this please\n. @vishh hooks/check_boilerplate.sh complains about the generated file not having the copyright. It's a generated file, so I'm thinking we should make the script ignore that file. Thoughts?\n. As said above, made hooks/check_boilerplate.sh ignore go generated files, which is only *extpoints/extpoints.go for now.\n. See my commit messages - go generate is not supposed to be run by the user, but by the developer(s).\n\nWhen what package is modified, go-extpoints or heapster? At the moment, neither. It is up to whomever changes the go-extpoints input to re-run the tool. And if the tool ever changes, which it shouldn't do often, someone would eventually run the updated tool and notice.\nI thought about making travis run go generate and check if it is up to date. I could bundle that with a go-extpoints in Godep/ so that it never breaks in the future. What do you think?\nSide note: master currently has this problem. If go-extpoints breaks, heapster breaks, because we just go get it via master.\n. I'll vendor go-extpoints and add the thing to travis.\n. retest this please\n. retest this please\n. @vishh any idea why jenkins failed twice with the same error message?\n. Otherwise this is ready to be merged\n. The new jenkins build failed, but an earlier one succeeded. Jenkins is not being very deterministic.\n. green again!\n. LGTM, thanks @mikedanese for fixing travis/jenkins. Could you squash?\n. Fair enough.\nJenkins is acting up again... retest this please\n. Thanks @mikedanese!\n. Many people will run into this issue, so I'm thinking we better have it in the README for a couple of weeks. Thoughts?\n. Just as well, the README should specify that people should use k8s.io/* for go get et al instead of the full github.com/GoogleCloudPlatform/* url.\n. Right, didn't know.\n. retest this please\n. @vishh I think jenkins and travis have broken since the switch from GoogleCloudPlatform/ to kubernetes/.\nCC @mikedanese\n. With @vishh we were just commenting that it would be a good idea to move this into a separate package so that kubelet could use it without having to import all of heapster/store. What do you think?\nOtherwise LGTM.\n. If you squash and fix Travis, it's fine by me.\n. @vishh this is the panic that we were encountering too\nedit: probably not, actually\n. LGTM\n. @vishh it's failing here like in all PRs, so looks like something is wrong with jenkins independently of the PR.\n. :+1: \n. retest this please\n. Careful there. You are relicensing code written by someone else.\n. Why are the tests passing now? Are the travis gods aware of license violations? :)\n. Thanks for opening this!\n. Makes sense, it's a dependency of fleet, which isn't removed by this cleanup. Then the question should be, why is it the local build working fine?\n. Sounds like a bug in godep. godep build et al falling back to GOPATH is also very confusing and sounds like a misfeature to me.\n. Hang on, are you running the latest godep? I already did the cleanup here: https://github.com/kubernetes/heapster/commit/8523d38ab030e9cc65864d7b0638f12d6a36639d\nIt cleaned up some libraries, but not that one. So your godep is doing extra cleanups compared to mine, which are wrong.\n. Ping @mwielgus \n. @mikedanese git grep 'github\\.com/kubernetes' shows no result. Where is that go get?\n. Oh right, your \"we\" confused me for a second :)\n. If you do git grep GoogleCloudPlatform/kubernetes | grep -v Godeps, you'll see that a few docs need updating.\nOtherwise LGTM.\n. LGTM\n. Update - kubernetes is not ready yet. See the races still present that were found in https://github.com/kubernetes/kubernetes/pull/13838\n. :) Just adding the note in case anyone is thinking kubernetes is ready since it builds with 1.5.\n. Maybe wrap the http links in blockquotes?\n. LGTM, but perhaps squash?\nAlso, looks like there's a flaky unit test - Travis failed.\n. LGTM\n. Edited the last comment to fix formatting.\n@sanjana-bhat for multiline quotes, to keep newlines, use triple backquotes.\n. @jimmidyson updating the godeps is not as simple as a single command, it usually involves godep restore, updating the library you want, and then godep save. Or a separate GOPATH if you don't want to modify your main one. So I wouldn't have it as a make target.\n@afein jenkins isn't complaining, and I can't see why it should be :)\n. @vishh this makes two flaky tests fail way more often, which I think is a good thing. They are TestSetSinksStore and TestRealCacheData.\n. Just fixed TestRealCacheData. Left it running in a forever loop for nearly an hour, no test failures.\n. @vishh the GC bug is a serious regression in the latest stable, perhaps a release is due?\n. There was no race, it was a bug in the garbage collection that I just fixed. That is what I meant with \"GC bug\". It's a serious regression because it removes all nodes at every runGC().\n. Sure.\nBTW, the TestSetSinksStore flake was introduced in 6d4d6e2251fb42e355. It was definitely not flaky when I made my \"changing sinks at runtime\" changes.\n. Mentioning the flake because it now happens almost always thanks to -race. In the long run we want -race enabled and the flake fixed. In the short run, if we can't fix the flake, we can hold off -race for now.\n. All green now thanks to #510, merging\n. Agreed with @vishh, uint is better than uint32. I keep on assuming 32-bit... :)\nLGTM too\n. Could you split up the PR in two commits, one for the prod code and another for the unit tests? This way code reviewing the prod code first should be a lot easier, especially since the unit test changes are so large.\n. I actually meant just splitting the changes in two commits, which would mean travis wouldn't fail :) But this works too\n. Added -race to go test, so reopening to trigger the tests again.\n. I ran into the very same issue when adding -race to go test here: https://github.com/kubernetes/heapster/pull/504\n. Confirmed it is fixed since the tests now pass with -race consistently.\n. LGTM, thanks for the fix @vishh!\n. LGTM. Looks like -race found another race bug, but completely unrelated to this change. I'll dig a bit.\n. @Naresht-vedams edited your comments to fix formatting. For readability, use three backquotes around a multiline paste, not around every line.\n. Adding sleeps to fix flaky tests seems wrong - shouldn't there be a blocking call somewhere?\n. Thanks @mwielgus \n. Thanks for the cleanup! Will merge once CI passes.\n. Skipping the test for now, but leaving this issue open as it still needs fixing.\n. Don't worry about the flaky test in sources - lots of these are arising since we started using go test -race. Reopening to trigger a rebuild.\n. All green now. LGTM, merging.\n. @vishh what's the exact syntax to make it happy? Let me have a try...\n@googlebot cofirming I am okay with this\n. Sometimes errors are just logged and ignored, sometimes they are returned and logged, and sometimes they are just returned. Perhaps we should be more consistent about that?\n. @vishh note that I had made some comments on your pre-rebase commits and they got lost. I can't make comments on the diff directly because it's too large since the godep update and github refuses to let me see the diffs.\nPerhaps remove the godep update, take comments, and then re-add the godep update commit?\n. No worries, I remember them and can re-do them.\n. Comments re-done. Also see https://github.com/kubernetes/heapster/pull/548#issuecomment-140200401\n. Note that 0.9.4 was just released, so we might as well switch to that.\n. Could you rebase to fix the conflicts?\n. The build seems to still be failing\n. LGTM\n. @vishh could we make the heapster jenkins run multiple builds at a time? if some build fails, which is what is happening now, it takes a long time retrying until it gives up. That blocks all the other builds for hours at a time.\n. @vishh comments amended and WIP removed. Checked that the endpoint works. Note the two TODOs.\n. @vishh just applied your stats result struct suggestion, makes everything a lot simpler :)\nThere's only one TODO left, the system containers bit.\n. Re-adding WIP until the TODO is fixed.\n. @vishh progress update:\n- Rebased on master\n- Updated the experimental api types to match the ones in nodeMetrics (replace map with list et al)\n- Updated kubernetes in godeps to my nodeMetrics PR\n- Add kube-heapster-bridge\n. FWIW, I'll keep WIP since I'm pushing my progress to this PR.\n. @vishh rebased on master and removed the bridge changes - they'll come in a separate PR. Ready to merge.\n. go vet prints errors to stderr, so the output caught by the shell script was always empty.\n. Why would I? It uses make unit-test just like travis. The Makefile itself uses the script.\n. Took a stab at it - #614\n. Note that this collides with #554\n. LGTM\n. This is probably due to -race being added to go test, in which case the flakyness might have been around for much longer than you would think.\n. Skipping the test for now, but leaving this issue open as it still needs fixing.\n. See https://github.com/google/cadvisor/pull/861. They are derived from the accumulated cpu usage, so they are updated exactly as often as the accumulated cpu usage is. \n. LGTM\n. Duplicate of #584\n. Whoops, sorry about that.\n. CC @vishh\n. @vishh all green, PTAL\n. One nit, otherwise LGTM.\n. Correction - the current behaviour does print the errors, but in a messy way.\n. Just noticed that we have a file with a space, docs/Grafana Dashboard.json. That should be docs/grafana-dashboard.json.\n. Only that filename nit, otherwise LGTM.\n. Yeah, that's pretty much why :)\n. Note that this will need rebasing and conflict fixing since the configs are now yaml. Sorry about that!\n. LGTM, thanks @jimmidyson. I tried doing this after semi-manually updating the kubernetes dep, but it kept on removing necessary dependencies.\n. Oh, nice find.\nJenkins is failing in a weird way, but since all code and tests compile this is good to merge.\n. LGTM\n. After my docs fixes PR, this PR needs rebasing.\n. LGTM. Flaky test unrelated to this PR - investigating atm.\n. LGTM\n. LGTM, thanks for the fix!\n. The linters have way too many false positives. They do much more than go vet, but the advantage of go vet is that it has no false positives.\nSo what I do is run them manually from time to time and apply some of their suggestions. They are definitely too heavy for pre-commit. As for CI, they would be a nightmare due to the many false positives.\n. As proof, one of the \"code simplifications\" that golint suggested broke the build :) undone.\n. In fact, some of the fields weren't kept in the right order. I can manually fix that if it's a problem.\n. @vishh PTAL. Also, by looking at integration/.jenkins.sh, it looks like jenkins is cloning/getting the old GoogleCloudPlatform/heapster repo. If it's fixed there too, those lines could be removed.\n. I cleaned up .jenkins.sh regarding the make commands, but not the GOPATH handling as it requires changing how jenkins gets the source code. Which AFAIK is not handled by any shell script in this repo.\n. @vishh PTAL, all green\n. LGTM\n. @vishh jenkins is failing on this line:\nmv /var/lib/jenkins/workspace/project/src/github.com/GoogleCloudPlatform/heapster /var/lib/jenkins/workspace/project/src/k8s.io/heapster\nWhich was removed in #617. Issue is, it wasn't removed from the stable branches. I forgot about them. I'll open PRs to address them ASAP.\n. Now, why on earth is this jenkins failing...\nWill investigate.\n. How did you fix jenkins?\nTwo nits, otherwise LGTM\n. Let me get the jenkins fix PR in first. Otherwise LGTM.\n. Cool, thanks. LGTM\n. You opened a pull request to merge the stable branch into master. You probably don't want that.\n. ok to test\n. LGTM once commits are squashed.\n. Thank you both!\n. LGTM with a nit.\n. LGTM\n. LGTM. Nit though - it's usually preferred to define the default value then override it with special cases, like:\nscheme := \"http\"\nif self.config != nil && self.config.EnableHttps {\n    scheme = \"https\"\n}\n. Sure :) I thought you would self-merge since I gave my LGTM.\n. ok to test\nLet's wait for jenkins. Has anyone reviewed the code?\n. I see. We generally keep godep updates in different commits, which makes things a lot easier.\n. FWIW, jenkins isn't showing here but the build is at http://104.154.52.74/job/heapster-e2e-gce/2700/.\nLGTM once commits are split up.\n. No problem :) The general rule is, keep granularity unless commits fix issues introduced in other commits. So if you take input from comments, these commits should be squashed. But otherwise, pieces of independent work - especially huge godep diffs - should be separate.\n. Build passed on jenkins - merging.\n@vishh you might want to look into why jenkins isn't properly wired back to GitHub - it doesn't show as a check anymore like Travis does. The builds do start though.\n. ok to test\n. The pipe error happens all the time, it's harmless for now. The error is just after those:\n\ncannot find package \"github.com/pborman/uuid\"\n. fwiw it did succeed on jenkins: http://104.154.52.74/job/heapster-e2e-gce/2803/\n. @huangyuqi if this PR is ready on your end, please remove WIP from the title\n. You applied some of the fixes by amending your original commit, i.e. rebasing the history. Please don't do that until the PR is ready, so that the history of changes is kept.\n. Thanks for all the fixes! The code looks a lot better now. Someone with a better understanding of kafka can probably speak as to the quality of the implementation.\n. Note that four comments are still standing - they will be hidden once fixed.\n. ok to test\n. Oh, no jenkins run required. Merging.\n. Fixed by #664.\n. LGTM!\n\nIn the future, could you please do the \"Fixes #n\" in the commit message? When you open a pull request, if it's a single commit, github will use the commit description as PR description.\nThanks for the double fix!\n. Oh, and I think the syntax is \"Fixes #a, #b\" given that the second issue wasn't closed by github.\n. Huh. I'm usually lazy and just do one closes/fixes per line in the commit description :)\n. Thanks for picking this up! You have to sign the CLA, though.\n. Thanks @mdshuai!\n. My bad, I forgot to update the markdown docs when changing the formats. Thanks for noticing!\n. LGTM if this behaviour is intended\n. Note that this just returns an error, it doesn't exit the program. It's up to the caller to decide whether to treat the error as fatal.\nheapster.go does error and exit, since you want that if you cannot bring up the sinks specified via the command line.\nOTOH, api/v1/api.go will just return a StatusInternalServerError to the HTTP client and not exit.\nI think both cases are correct, so I don't see the point of this PR at the moment. It would make --sinks unintuitive and hide errors from the API - if you tell it to add a sink and it fails, it would not return the error response.\n. If you want sinks to not cause an exit when used via flags, then change heapster.go. I still think that modifying it this way is not the right way to do it. If you do an HTTP POST changing the sinks and you get a 200 response when setting the new sinks failed, that is definitely broken.\n. > Sink unavailable on startup - fail startup. More likely to highlight misconfiguration.\nThis was already in place, and I did not change it when adding sinks at runtime.\n\nSink unavailable when adding at runtime. Don't add sink, return appropriate error with non-200 status code (not sure quite what to use).\n\nI made it work this way when I added sinks at runtime. I made it return InternalServerError (500), suggestions on a better status code welcome. But we definitely want 4xx/5xx.\n\nSink becomes unavailable after being added. Sink should buffer until sink is available again.\n\nAFAIK nothing is done in this regard. Nothing changed about this when I added sinks at runtime, so it didn't even occur to me.\n. @jimmidyson good suggestion. 1.5 is limited as far as http status codes, since they stick to the widely approved ones. So the 400s only go up to 418 on 1.5, for example. Maybe leave it as a TODO? I recently saw a commit in golang master that added many more status codes, so 422 will probably land in 1.6. Although starting to use it would mean dropping 1.5, so probably not going to happen soon.\nI think the current behaviour on cases 1 and 2 is correct, hence me not understanding this PR. You raised a valid point on case 3, but that is a different issue - this issue is about \"coming up\".\n. ok to test\n. Looks like jenkins is down, or at least unreachable for me: http://104.154.52.74/job/heapster-e2e-gce/\nEither way, the only change is a unit test, which are by Travis. Jenkins isn't really needed for this PR, merging.\n. Fair point.\n. ExternalSinkManager was already there, so I didn't touch it. Not sure what the difference between Sink and ExternalSink is, so I didn't change that.\n. Sure.\n. Sure.\n. What do you mean by \"something more simple here\"?\nBy examples, you mean examples with json data? Should they include curl commands, or is json enough?\n. Then we'd have to change lots more, like externalSinkManager to sinkManager. This goes hand in hand with \"don't know the difference between Sink and ExternalSink\".\n. SGTM\n. Agreed on the string stuff. I saw that Uris was just []string, so I went with that. I've worked with the flag.Value interface before, I could get that working.\nHaven't looked into sink configs yet, but I guess this is for exporting them on a GET /api/v1/sinks, right?\nI'd actually prefer to do all of this in a separate PR if that's OK. Since this is just continuing with what we were doing already, really.\n. So then we're keeping Sink, ExternalSink and SinkManager, correct?\n. Hadn't noticed that you were describing the same struct in two different ways. What about it being some sort of interface with the underlying type being the actual config, be it GCMConfig or whatever?\nThe only problem then would be the json key names.\n. Right, so sinkManager is for now the only implementation of SinkManager.\n. OK!\n. Yep.\n. Good point.\n. Sure.\n. Sure.\n. That's what was in the original build.sh:\n-docker build -t heapster:canary .\n+docker build -t $IMAGE .\nWhat would you call it? The Makefile uses $PREFIX/heapster:$TAG, but I didn't know what prefix to use here.\n. Right yes. I'll add a Makefile target for that and fix the PR.\n. ${1-heapster:canary} means $1 if $1 is empty (or undefined, same thing) otherwise \"heapster:canary\". There is an alternate and more popular notation, ${1:-heapster:canary}, which does the same.\n. Firstly, because both the script and this go func do essentially the same. So why duplicate code? For example, we found out that the bash code was wrong (it was missing the go generate stuff).\nSecondly, because now that the root Makefile no longer uses absolute build paths, running make in the root dir will create a heapster binary in the root directory. So we'd have to copy it over. Which complicates the Go code in this function even more.\nSo it's more a question of whether to reimplement all of build.sh in Go, which seemed unnecessary.\n. Fair enough.\n. Even if they don't have any formatting? I'm guessing this is to be consistent and to not add the errors import, correct?\n. I don't think this is good for the flag interface, yeah. Gets too much done too soon.\nI didn't do the REST API part yet, what I did for now is simply change the underlying type to be stronger without changing any of the behaviour.\nI seem to recall you said that you'd like the endpoint to use something other than the strings that the flags use. Would that be for GET, POST or both? In a way I'd rather keep it the way it is now, since it's more intuitive and it uses the same format everywhere.\nAlthough at the same time, a json detailed description of each sink would be useful. Perhaps a separate endpoint, something like /api/v1/sink-configs?\n. nit: you can probably remove this line\n. And also the others in this file.\n. I just noticed that the other flags don't end with a period. Perhaps do the same?\n. I've seen people use \"zero\" variables, like zt := time.Time{} at the beginning of the function, so that you don't repeat the typed zero value many times. Reduces columns and makes it easier to change the zero value or type if it ever needs to change.\n. Damn, forgot about that when moving from ln to f.\n. Period should probably go.\n. Same here and a few more occurences below.\n. nit: shouldn't extra arguments be at the end? Also, looks like its doc needs updating.\n. nit: , _ can be removed here and elsewhere.\n. Something like the following?\nmarkdown\necho '[\"gcm\", \"influxdb:http://monitoring-influxdb:8086\"]' | curl \\\n    --insecure -u admin:<password> -X POST -d @- \\\n    -H \"Accept: application/json\" -H \"Content-Type: application/json\" \\\n    https://<master-ip>/api/v1/proxy/namespaces/kube-system/services/monitoring-heapster/api/v1/sinks\n. Note that --insecure is necessary for curl to work with the self-signed SSL certificate.\n. See #427 \n. I can't help but think that these three lines are unnecessarily duplicated across the file. What about a new method/function, something like:\ngo\nfunc (a *Api) getNonNilCluster() (model.Cluster, error) {\n    cluster := a.manager.GetCluster()\n    if cluster == nil {\n        return nil, fmt.Errorf(\"the model is not activated\")\n    }\n    return cluster\n}\nAnd then:\ngo\ncluster, err := a.getNonNilCluster()\nif err != nil {\n    response.WriteError(400, err)\n}\n[...]\n. Granted that this pretty much just deduplicates the error string.\n. In fact, for the sole purpose of deduplicating the error string you could just have the error string defined as a const:\ngo\nconst errModelNotActivated = errors.New(\"the model is not activated\")\n[...]\nresponse.WriteError(400, errModelNotActivated)\nWhich is much simpler than adding the new method.\n. Correction - apparently errors can't be constants, so var instead.\n. I know I wrote this, but perhaps we should note that at this stage inc is already sorted in a comment. Or sort it ourselves explicitly before grabbing the max.\n. nit: you can do if newVal := tp.Value.(uint64); newVal > curMax {\n. This is actually a question - do you generally accept if cond { return false } return true instead of return !cond? I know cond in this case is rather large, but still wondering.\n. I see more duplicated error strings :)\n. typo?\n. another typo?\n. @saad-ali an empty struct takes up no memory. The only reason why using a boolean would be better is if you wanted to have three states instead of two, which isn't really necessary for a set.\n. nit: use var result []TimePoint to avoid allocation if no elements are to be added\n. surely this must be ss.rewind()?\n. :=? (assuming you haven't tried compiling any of this now)\n. nit: var result []Timepoint\n. Without looking at the code, it's hard to tell what each argument does. And in generated docs, since the fields are not exported, it will be for the user to guess.\nI suggest something like NewStatStore(epsilon uint64, resolution time.Duration, duratoin uint8, percentiles []float64)\n. An alternative would be to keep the current ones and add a proper comment explaining what each argument is.\n. +1\n. comparing to an empty time seems wrong to me... are you sure you cannot do if !ss.start { if that is its zero value?\n. Could it be that due to a loss of precision you end up with this error when you shouldn't?\n. Why did you choose to make epsilon uint64 instead of float64?\n. Actually, for strings you do if s != \"\". Maybe if ss.start != time.Time{}? Although it's essentially the same.\n. :+1: \n. FWIW, I fixed a race condition in https://github.com/kubernetes/heapster/pull/405 - there might be another one, and if similar it could be causing something like this. Since Store() would return before it's actually finished.\n. I'd probably leave the import as statstore\n. You don't need to override the import name here, right?\n. Correction - you do because you named that package store. Please rename it to window and remove this name override here.\n. Why use a pointer to a window? The structure is light.\n. Looks like this is the normal flow of the code. How about inverting the if, making it return and then moving the rest of the code out of the block?\n. nit: var to avoid allocation\n. If we're only going to be using this for the StatStore, you might want to use the specific type that we need. The only reason why would be so that we can use []hourEntry, which is contiguous in memory, instead of []interface{} which uses pointers under the covers.\nIn general, unless each element is very large and thus expensive to swap/move/copy, it's better to store its value directly.\n. Building on that - this code is rather simple, we could probably reimplement it for our needs under our own license in the store/daystore package with the usage of []hourEntry instead of []interface{}.\n. That's why I said that we could write an implementation of our own to suit our needs :)\nAnyway, I could do this if needed if I encounter performance issues in the future.\n. Could you change the import here too?\n. Copying structures is only a problem if they are noticeably heavier than a pointer. In this case, hourEntry is three uint64s. Even if assuming the worst - a 32bit system - that is far from a large struct.\nThe data being compact and local in memory is very important. You should only give those up if the structure is very heavy.\nBut as said, I'm fine with using this implementation for now and rewriting it later myself if I find that this is a problem.\n. Bump! This comment got hidden in an outdated diff.\n. Wait, so should this be closed in favour of #478?\n. @vishh what do you mean? Storing float64 instead of hourEntry?\n. I don't see why that would help. I don't think Go would allow it either, unless you use the unsafe package. You can't cast stuff around in Go like you would with void* in C.\n. Oh. Fair enough.\n. 0, len(list)\n. What percentile?\n. Why not use time.Time or util.Time?\n. uint8/16 will be padded if the struct is accompanied by a uint32/64. So if your only reason to use uint8/16 here is space optimization, I would just use uint32.\n. As a side note - if your goal is to optimize memory at all costs, what you can do to avoid padding is to use slices or arrays instead of multiple fields of different sizes. But I don't think we need that level of premature optimization - at least not until it is proved necessary.\n. I was thinking of Duration, yes :) Sounds fair.\n. All the uint64(0) assignments are unnecessary since that's the zero value.\n. Aren't all of these new endpoints missing Writes()?\nAlso, many other endpoints say Writes(types.MetricResult{}) but that's just wrong, e.g. GET(\"/nodes/\")\n. return error?\n. return error?\n. You can use make([]influxdb.Point, 0, len(events) since you already know how much to allocate\n. This line can be removed, I think\n. same as above, make() with len(timeseries)\n. You're confusing length and capacity. See https://golang.org/ref/spec#Slice_types - in my comment above, the length is 0 and the capacity is the len(...).\n. It was just copied over from the model. As a means to check if the model was running. I'll remove it.\n. Sure.\nBTW, this is a WIP, so don't review yet as this is still being written.\n. It's experimental/v2 since this may become api/v2.\n. We can remove them.\n. Maybe the issue should be closed once all endpoints are tested. Is that what you mean?\n. This looks like it's missing gofmt\n. Do we want to create a database at each call?\nAlso, since we return nil if there are no events, perhaps this could be moved down.\n. It would be more idiomatic go to do return err after recordWriteFailure(), drop the else and return nil here.\n. Does this compile? It sounds like you want make([]influxdb.Point, ...\n. Same as above.\n. Like the other err comment, it would be more idiomatic to not have an else and do return nil at the end.\n. Now that Register does create a database, shouldn't Unregister do something too?\n. Also - Register should be called before any StoreEvents, so creating the database here seems redundant.\n. I would expect heapster to refuse to use a sink if its registration fails, or to retry a few times.\n. You're right: https://play.golang.org/ (press the format button)\nIt looks like gofmt treats composite literals differently.\n. Fair enough.\n. Right. It just caught my eye.\n. Good point.\n. Why are we bumping the timeout here and below?\n. This should not be here since we use the vendored go-extpoints version as seen in https://github.com/kubernetes/heapster/blob/8e4a8ba63e75fe234bedb4171b35553d6ea18981/Godeps/Godeps.json#L6\n. Yes.\n. These should probably be vendored. If not, I'm thinking that the riemann tests may break at any point.\nDidn't godep save ./... add them?\n. I would still specify what version to use.\n. I can't help but notice that some changes are simply moving lines around, like hide and interval. Are these changes automated or manual?\n. Fair enough then.\n. You can use four spaces or a tab to do code blocks just like you can with surroinding text with three backticks.\nThis is especially useful for very few lines, since it requires two less lines and it's more readable in plaintext.\n. What about having it as deploy/kube-config/heapster-service.json, if it applies to all different configs?\n. This isn't an endpoint - it's the path for the whole WebService. Basically, the prefix and other common parameters for the endpoints under it. So /nodeMetrics below becomes /experimental/v2/nodeMetrics, etcetera. \n. Perhaps specify a version, namely 1.4.3?\n. You can probably simplify building and running by doing instead:\n```\nRUN apt-get install -y git\nRUN go get k8s.io/heapster\nENTRYPOINT [\"/go/bin/heapster\"]\n``\n. You're right. In that case, instead of git clone you can dogo get -u -d k8s.io/heapster` instead of the git clone, but essentially it's the same.\n. switch?\n. You could split up the append in three parts since two are exactly the same, as in:\ngo\nn = append(n, p.Labels[sink_api.LabelContainerName.Key])\nif p.Labels[sink_api.LabelPodId.Key] != \"\" {\n    n = append(n, p.Labels[sink_api.LabelPodId.Key])\n} else {\n    n = append(n, p.Labels[sink_api.LabelHostID.Key])\n}\nn = append(n, p.Name)\n. remove this?\n. This will not make a full copy of the []metrics.Modifier, is this on purpose?\n. That doesn't copy the underlying data, fwiw.\n. mv heapster /heapster instead?\n. Please indent via four spaces if it's just one line.\n. Same as above.\n. None of these require typing.\n. I see logs and errors duplicated everywhere. Is this necessary? If it is, please deduplicate the strings.\n. Please run goimports -l -w . to fix the import grouping.\n. You can drop the named returns.\n. This loop and all the rest are a bit weird. Why not just do for _, t := range timeseries and then t instead of timeseries[index]?\n. What is this checking? The last generated err was already checked.\n. See https://github.com/golang/go/wiki/CodeReviewComments#indent-error-flow - the else is unnecessary.\n. Not exactly - constants are only typed when used, not when declared. So it's a simplification of the code, but also dropping an unnecessary strict typing. For example, you could use 10 as both int and uint, but if you specify int, you can only use it for the former.\n. Note that the format was changed weeks ago, the other sink configuration lines are indented with four spaces.\n. Named returns are only useful for naked returns, which you don't use here. So return (int64, error).\nSee https://github.com/golang/go/wiki/CodeReviewComments#named-result-parameters.\n. Lots of interesting stuff at blog.golang.org :) https://blog.golang.org/constants\n. This duplicate error string remains.\n. I'm not sure whether ignoring errors here makes sense. Maybe panic if err != nil, similar to regexp.MustCompile?\n. len() of a nil slice is zero, so the test against nil is redundant.\n. Same as above.\n. Ignored error?\n. This is really nitpicky, but usually people test if strings are empty like if s == \"\"\n. ",
    "burmanm": "Squashed and added the documentation to storage-schema.md\n. @mvdan I think it might be simpler if you just recreate &HawkularSInk, as the Register() takes care of the initialization process (by fetching definitions from the server), so there's nothing valuable left. I assume at least that the GC would then take care of removing old maps from the memory.\n. Rebased and client updated (ready for merge)\n. @vishh For now I believe the next release will come in time anyway (around mid of October is the cut-date for new features), if you had planned a later release date, then I'll need to ask someone who knows more ;)\n. @jimmidyson Need you to confirm the squashed commit for cla..\n. @vishh Something fishy with the Jenkins? dial unix /var/run/docker.sock: no such file or directory.\n. I think the issue with reconnection is that the Register method will return an error and Heapster will die in these cases. Restart policy for Heapster is probably the best workaround for now.\n. Okay, it is theoretical data race in a test (if the test would be run in a multithreaded way and the same instance of httptest-server would be used) as the method used inside the goroutine does \"++\" operator to an outside variable.. This can't be triggered by the test itself as it does not spawn more threads. I'll fix it.\n. Yep. \n. Do you need something from me?\n. \"./metrics/sinks/hawkular/client.go does not have the correct copyright notice.\"\nHas the copyright notice changed or should I use 2015 still as year-number?\n. Yep, that seems to be the issue:\nhttps://github.com/kubernetes/heapster/blob/heapster-scalability/hooks/check_boilerplate.sh#L27\n. I figured leaving it as \"LabeledMetric\" was easier to read since we're transforming older metrics to the \"Labeled\" ones (and not the other way)\n. Done\n. It is possible to search for both types, but since the type is part of the unique key, one could have \"cpu/usage_rate\" \"GAUGE\" and \"cpu/usage_rate\" \"COUNTER\", thus it would be quite difficult to find out which one is the correct one.\nI'm not sure what you mean with \"miss some metrics\", as long as we store it with same assumption. And at this point we store unknown metricType as Gauge also (so transformation happens two ways in identical way). \nAlso, Gauge type supports the same operations, such as rate calculation on the server side (but does not have understanding for something like counter reset obviously) so one is not missing any functional behavior.\n. No, Hawkular does not ignore MetricType, it uses it. Could the push metrics expose those defined metrics in some way, so that we could use that information here to search for correct type also?\n. Yes, the map of registered metrics can be used here, but there's the issue of backtracking the correct one, as the metrics.Definition includes a generated id. registerLabeledIfNecessary calls the idName() to generate the identifier.\nHowever, it also uses the heapsterTypeToHawkularType so the typing should be the exact same as what we're trying to find here. Because of this, I'm finding it quite difficult to think of a situation where we would end up with different result on different iterations..\nAlthough you strikeout the pushmetrics/startup time, I'll clarify here if someone wonders. Hawkular-Metrics Registers only models in the startup, not actual metricDefinitions (those are stored as we process metrics). However, I must read your push metrics to see if there's something that needs working (to avoid unnecessary calls / registerations / etc)\n. @DirectXMan12 Hi, yes I did.. I couldn't find the type as an issue, however there's one thing that caught my eye:\n// TODO: core.LabelPodNamespace?\nThis is quite important label for us, as it's used in Openshift by default as the namespace divider currently:\nhttps://github.com/openshift/origin-metrics/blob/master/deployer/templates/heapster.yaml#L83\nWithout the information, things will be written to the system tenant and that's a problem and not just for Oldtimer API.\n. Rebased from master and fixed the issues @DirectXMan12 mentioned\n. Using without quotes causes your shell to interpret \"&\" as background job and probably not what you want.\n. @DirectXMan12 Yes, I can take care of the Hawkular sink (and I assume @mwringe will also). So, the reason is that we still use in the origin-metrics the pod_namespace label, which isn't stored when Heapster stores the namespace-level metrics:\nhttps://github.com/kubernetes/heapster/blob/96d9dbf9ee20c200bb7703a8dff0b5ea7c1d1ba1/metrics/processors/namespace_aggregator.go#L60\nAnd for that reason, the matching fails.\n. Fixed here instead: https://github.com/openshift/origin-metrics/pull/319. /close. \n\nReview status: 0 of 5 files reviewed at latest revision, 2 unresolved discussions.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \n\nReviewed 4 of 5 files at r1, 1 of 1 files at r2.\nReview status: all files reviewed at latest revision, all discussions resolved.\n\nComments from Reviewable\n Sent from Reviewable.io \n. And what happened to this..?. @DirectXMan12 I have more improvements done for this in my branch (waiting for some QE work) - so don't merge yet.. I'll open a new one from another branch.. Sorry for the delay, hopefully all the comments are now solved.. I merged all three as the second one was a fix also.. Actually, yes, this would be doable in the register phase (here it isn't as we run this process on every occasion of metric store - I'll explain in other comment why) -  compare the metricDescriptor labels with the stuff we've already stored and mark stuff to be updated. Versioning would make this a lot simpler process as we could store the version on the Hawkular side.\n. In this model, we would store the metric with a name: containerName/containerId/metricName and only the last one is available on the Register() phase. If the register would be called when new container is detected, then it would make sense to register here. But as we now want to create a metric per each container+metric combination it can't be done here, so that's why we store the model only here.\n. Our gauge support fits better these currently collected metrics than our counter model would. This might or might not change in the future, but for now that's the case. Also, there weren't any Boolean values yet so modeling them would be quite premature.\n. Yep, the underlying self.modifiers should not be modified in this case.\n. I'm not fond of the idea of passing []string type here. Couldn't we make it a type that automatically restricts the available values and makes it easier to read?\nNow each sink writer has to know that these strings are actually vars from historical_types.go\nConsidering that every sink writer has to modify them to their internal naming / functions in any case.\n. This question applies to all of these methods. Are there parameters which can be assumed to be passed on with invalid value? Such as end time 0 or start time 0 (or both), bucketSize 0? I see there's checks for these in the example influx implementation, but nothing in these interface definitions.\nAnd if there are, how should they be handled? I can make a lot of assumptions (and derive something from the influx example), but it would be nice to have these documented to make it easier to implement proper HistoricalSource.\n. Is this the end time of the bucket, or start time (for Hawkular we return both timestamps to the user) ? Or user definable as long as it's consistent?\n. Currently such setup is not supported and we do not have any metadata associated to filter out non-Heapster tenants at this point.\n. Yes, I'll add (it's an Openshift specific behavior to store each namespace on separate tenant)\n. This is included in several places, so I'll add to the method of isNamespaceTenant a longer description if that's alright. It's not actually filtering as by default all the requests go to a default tenant. So it's required to know which tenant has the data to find the metric (requesting from wrong tenant would return wrong data or no data - each metricId+type is unique inside a tenant, so different tenants can have same metricIds)\n. An extra question mark ;). It's just a reminder that \"TypeCluster\" must always target the default tenant in Openshift (that's where the cluster level information is stored in)\n. I assume push metrics have a type as well, being either Gauges or Cumulatives? Gauge datatype allows richer possiblities as it uses float64 instead of int64, so that's why I've set it as default when nothing else matches. As long as the matching is consistent it shouldn't matter much.\n. Agreed. The 8 hours is the default in Hawkular when user gives no range restrictions, but I'd much rather have some application specific restriction here. Should it be added to the API level that if both are empty, the API would set them with some default? Instead of doing the same filtering on each historicalSource?\n. Technically it is, but with some confusing methods (I have no idea on the history of why), if you look at https://github.com/kubernetes/heapster/blob/master/metrics/sinks/hawkular/client.go#L102 that one could be still resolved back in most cases, however there's for some reason the following addition at some point in the past:\nhttps://github.com/kubernetes/heapster/blob/master/metrics/sinks/hawkular/client.go#L154\nAnd here we don't know if there was a ResourceID set or not. I don't personally like to use metricIds for anything as they provide no structure at all (and should I have my wish, they would be random strings..).\nHowever, after 0.18.0 of HWKMETRICS is out, this Definitions() fetching can be removed and the tags are used with ReadRaw to find the metrics directly - reducing the amount of calls. I just didn't want to use the non-stable API in this integration. Equal to the way GetAggregations is doing the work (and e I could probably reuse a lot more code).\n. I'll change (the acronyms are from returned data types).\n. Yep, agreed. Will you add that in the Oldtimer PR or will I add something to create a map in the start of the HawkularSink and use that instead of looping everytime?\n. I would make this \"labelTagPrefixOpts\" or something to indicate it's not actually in the label value. This is confusing. Why would we want to store the original labels-line and the splitted instance on the server side?. This does not disable the caching itself, only the preloading from Hawkular-Metrics. For that reason, the disableCache would be quite misleading.. ",
    "satheessh": "we also need it\n. ",
    "erimatnor": "+1\n. ",
    "ivanthelad": "+1\n. @cboggs I was able to get rieman sink working on OSE 3.1 by rebuilding the origin heapster-base  and heapster images based on the following commit id. https://github.com/kubernetes/heapster/commit/c7910881b1a5acde8fe7b1e8b93e6d1b828f6d0c\nSee, https://github.com/ivanthelad/origin-metrics/blob/master/heapster-base/Dockerfile#L30\nand then building the 'heapster' image  https://github.com/ivanthelad/origin-metrics/tree/master/heapster\nYou will then need to edit the heapster RC accordingly to reference the new image. \nIt is a bit of  a hack but you did mention its hack week \n. ",
    "duttadeep55": "Riemann\nThis sink supports metrics only. To use the reimann sink add the following flag:\n--sink=\"riemann:[?]\"\nThe following options are available:\nttl - TTL for writes to Riemann. Default: 60 seconds\nstate - FIXME. Default: \"\"\ntags - FIXME. Default. none\nstoreEvents - Control storage of events. Default: true. ",
    "jamtur01": "@piosz @mcorbin and I are going to own the Riemann sink.. I'd be happy to take care of the Riemann sink.. @DirectXMan12 I'd be happy to move the client library into the Riemann org.. @mcorbin Do you want to throw your eyes over the client library and talk to @rikatz about moving it to github.com/riemann? Let me know. I can look next week if you don't have time.. I created https://github.com/riemann/riemann-go-client as a placeholder.. @DirectXMan12 Apologies - didn't realize I was the blocker. This LGTM.. ",
    "lswith": "I think a better alternative could be the use of the difference aggregate. If you do the difference rather than the derivative you get better resolution and it correctly takes in the group by time \n. the same thing is occurring in v0.16.0 as well.\n. ",
    "Thiru-Arasu": "Hi Guys, I have doubt in calculating the CPU usage of Kube in percentage. I need to convert cpu/usage_ns_cumulative value to percentage with help of cpu/limit_gauge. Plz give some example to analyze....\n. ",
    "huliz": "I signed it!\n. ",
    "krousey": "If everything passes, how long would it take to push a new container image so I can update the Pod specs in Kubernetes?\n. @bgrant0607\n. I updated the change to reflect minions being replace by nodes and DesiredState being replaced by spec.\n. @vmarmol can you refresh my memory on how?\n. @vmarmol I'm running some Kubernetes e2e tests against krousey/heapster:canary right now. Hopefully this is what you wanted? Also it seems the Jenkins build timed out on the integration tests. It ran fine locally.\n. @bgrant0607 I don't understand. Doesn't this change explicitly ask for the v1 API?\n. Or are we defaulting to whatever kubernetes wants somewhere?\n. @bgrant0607 it seems weird to have heapster not tied to a concrete version. Shouldn't it be?\nAs far as updating Kubernetes, I have heapster in a separate GOPATH. Should I clone kubernetes directly, sync to tag v.20.0 and godep restore kubernetes then godep save heapster?\n. I think I'm giving up on trying to get this working tonight. I will be in tomorrow to ask for help, because what I did isn't working.\n. @vmarmol I added a change to make heapster use the v1 API and ignore the preferred version in the client in the godeps version of the client. Is this ok?\n. Also https://github.com/GoogleCloudPlatform/kubernetes/pull/10785 tests the kubernetes e2e with a docker image I built from this. It passed and did not use the v1beta3 API.\n. Integration also passed locally.\n. @rjnagal This does not depend on that. The second commit in this PR forces v1.\n. @jlowdermilk Why should this be removed? Heapster is a client of the API and should only use versions it supports. In fact, this should go in before #389 because it would break if heapster defaulted to v1 without switching it's node query label from spec.host to spec.nodeName. I think it should use the version it was coded to use.\n. Done\n. Done.\n. ",
    "bgrant0607": "Thanks\n. Note: We should still update them, but we don't run using the json files here. There was an earlier unmerged (and now out-of-date) PR #283 to re-sync the entire files with Kubernetes.\n. LGTM\n. Oh, you need to update Kubernetes packages in godeps\n. Only in the integration test.\n. Heapster looks like it attempted to specify v1 already:\nhttps://github.com/GoogleCloudPlatform/heapster/blob/master/sources/kube_factory.go#L35\nWe need to understand why that didn't work.\ncc @nikhiljindal \nI'm not an expert on godeps, unfortunately. Perhaps @lavalamp could provide some advice. \nAs for what version of Kubernetes -- I believe there were fixes more recent than v.20.0 that we need. I'd grab head.\n. Heapster looks like it should be logging which api version it thinks it's using:\nhttps://github.com/GoogleCloudPlatform/heapster/blob/master/sources/kube_factory.go#L166\n. cc @jlowdermilk, in case he can help with the kubeconfig in any way\n. @ghodss \nhttps://github.com/kubernetes/community/blob/master/sig-instrumentation/README.md\n. This is not our v1beta3 API.\n. Ditto\n. See:\nhttps://cloud.google.com/logging/docs/api/ref/rest/index\n. Please revert this file.\n. Please revert this file.\n. ",
    "shilpapadgaonkar": "Yes. There is a link for the same in the docu https://github.com/GoogleCloudPlatform/heapster/blob/master/docs/standalone.md\n. Thanks!\n. I could not get pod logs as the pod is not running yet. See attached pic\n\nThe openshift master logs showed some errors though...\n\n. Thanks @detiber for your help.\nI modified a couple of things in the heapster-controller such as source to kkubernetes as --source=kubernetes:http://kubernetes-ro.default.local.\nNow the pod atleast gets into a running state.\nBut now I face the same issue as you where the logs say that it cant find kubeconfig\nE0609 10:36:59.761253       1 heapster.go:59] The config file /etc/kubernetes/kubeconfig/kubeconfig does not exist\nWith my openshift version, volume mounts are not yet supported.\nI am hoping to upgrade this version in a day or 2 after which I will try to specify the path of auth as  /var/lib/openshift/openshift.local.certificates/kube-client/.kubeconfig and see if this works\n. Could you recommend some other working solution to gather metrics for openshift?\n. @jimmidyson \nI now try to use the serviceAccount as specified by you in the source config docu and the logs show me the following:\nE0617 14:10:25.465443       1 kube_events.go:96] Failed to load events: Get http://kubernetes-ro/api/v1beta3/events: dial tcp: lookup kubernetes-ro: no such host\nE0617 14:10:25.702882       1 reflector.go:123] Failed to list api.Node: Get http://kubernetes-ro/api/v1beta3/nodes: dial tcp: lookup kubernetes-ro: no such host\nE0617 14:10:25.702945       1 reflector.go:123] Failed to list api.Pod: Get http://kubernetes-ro/api/v1beta3/pods?fieldSelector=spec.host%21%3D: dial tcp: lookup kubernetes-ro: no such host\n. Ok. sounds good\n. @jimmidyson \nI can see only the pods running in the default namespace. Do I need a different role than the one mentioned in your documentation?\noadm policy add-cluster-role-to-user \\\n    cluster-reader \\\n    system:serviceaccount:default:heapster\n. @jimmidyson \nYes I am getting metrics for only the pods within the default namespace. I also have some pods running in demo namespace and i dont see any metrics from these pods\n. i am looking in grafana.\nquery looks fine to me\nwhere pod_namespace =~ /.+/\nOn Fri, Aug 7, 2015 at 2:29 PM, Jimmi Dyson notifications@github.com\nwrote:\n\nWhere are you looking? In grafana or in influxdb? Wonder if the query in\ngrafana is only targeting the default namespace?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/issues/320#issuecomment-128687819\n.\n. @deads2k \nI deleted all running pods, and redeployed the complete service and now I can see metrics of pods from all namespaces\n. @smarterclayton : Is there some documentation explaining how openshift will use hawkular to implement auto-scaling. Is this the right place --https://github.com/kubernetes/kubernetes/blob/master/docs/proposals/autoscaling.md\n. @jimmidyson: Currently we are using the read-only port without auth to make heapster work on openshift. When authentication is introduced, I am guessing this should solve the problem?\n\nI also saw this https://trello.com/c/xWFvS0xc/45-8-hawkular-metrics-security-metrics\nfor hawkular. Is there something similar planned for heapster?\n. @vishh \nDoes heapster now support influxdb v0.9?\nCould you kindly give me a hint how I can make heapster get this data into namespace specific DBs?\nThanks\n. @mwringe : I would like to try out the heapster-hawkular sink for my openshift env. Could you kindly point me to some documentation link for the same?\n. Thanks.\n. ",
    "roshan-zot": "Is there a half-written documentation somewhere?\n. ",
    "acconsta": "And apparently solves some data corruption issues related to clustering. Is there a time frame for this issue?\n. (duplicate)\n. ",
    "rvrignaud": "Hello,\nIs there anyone working on this one ? Influxdb 0.9.3 is coming and 0.8 branch will not live for a long time.\n. @huangyuqi I really would be interested to see a PR for an ES sink.  Is this for events only or also metrics ? This would fix https://github.com/kubernetes/heapster/issues/227 ?\n. @huangyuqi Hi. Would you mind open a PR or at least send a gist for the Elasticsearch sink as you mentioned above ? It would be very useful to us to get that feature ! The issue is describe here : https://github.com/kubernetes/heapster/issues/227.\nThanks !\n. @huangyuqi : Hi. Thanks a lot for the PR. Unfortunately I'm not a Golang developer but I'll try to find some time to build and test it.\n. Just for the record, I've been running Heapster (commit d500aa975889f6ba2c2093b2bd518e17266c42fd) successfully for a while. Since my cluster got upgraded to 1.1.2 I have this kind of error with the d500aa975889f6ba2c2093b2bd518e17266c42fd version:\nE1210 11:22:08.390460       8 external.go:87] failed to sync data to sinks - encountered the following errors: unable to parse '...': missing fields\nThis is why I tried to upgrade heapster version to master without luck (see first comment).\nI can send privately full logs if needed.\n. As I said I'm running Influxdb 0.9.4.1. Heapster d500aa975889f6ba2c2093b2bd518e17266c42fd has successfully sent data for a while when I was running GKE 1.0.7.\n. Hi @vishh. Did you reproduce ?\n. I still have same problem (bad timestamp error) with heapster 0.19.1\n. Is there anything I can provide to help you debug ? I can PM you full logs for instance ?\n. @vishh I sent you the log file with vmodule=*=4 privately as it's very complicated to remove sensible parts on this huge file.\n. @antoineco Thank you very much. Removing all lifecycle definitions, fixed indeed the heapster problem.\n@vish did you get anything from the logs I sent you ? Do you reproduce when adding lifecycle fields ?\n. @mwielgus thank you for the clarification. It wasn't clear for me in documentation. Closing this one\n. Hi,\nIt seems that GKE doesn't enforce hard CPU limits. Nothing wrong with heapster here.\nClosing\n. I have same problem with or without specifying scheme.\n. I did succeed to send metric to elasticsearch using 1.2.beta0.\nBut it seems that events are not pushed and specifying index is not working.\nAny clue on that ?\n. Still same issue with 1.2.beta1.\nDid anyone succeed in pushing events to ES and specifying index name ?\n. Quick note: 0.9.4.1 has been released (see https://github.com/influxdb/influxdb/releases/tag/v0.9.4.1)\n. ",
    "szubster": "Any news?\n. ",
    "hyperbolic2346": "I signed it!\nOn Tue, Jun 2, 2015 at 2:55 AM, googlebot notifications@github.com wrote:\n\nThanks for your pull request. It looks like this may be your first\ncontribution to a Google open source project, in which case you'll need to\nsign a Contributor License Agreement (CLA).\n[image: :memo:] Please visit https://cla.developers.google.com/\nhttps://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll\nverify. Thanks.\n\nIf you've already signed a CLA, it's possible we don't have your\n  GitHub username or you're using a different email address. Check your\n  existing CLA data https://cla.developers.google.com/clas and verify\n  that your email is set on your git commits\n  https://help.github.com/articles/setting-your-email-in-git/.\nIf you signed the CLA as a corporation, please let us know the\n  company's name.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/304#issuecomment-107831182\n.\n. Added fleet files with updated versions from @vishh .\n. Any update on this?. /remove-lifecycle stale. /remove-lifecycle stale. /remove-lifecycle stale. \n",
    "alexmavr": "I signed it!\n. LGTM\n. @rjnagal Addressed all comments in the new commit.\nThe newer commit moved the external types to the \"info\" subpackage, so they can be easily exposed to external consumers.\nUnit tests for everything are incoming on the next commit.\nNote on locking: I will initially implement cluster-level locking, with plans to eventually support a more fine-grained update procedure\n. @rjnagal @vishh @vmarmol Added Unit tests for the schema skeleton, currently at 92.6% coverage.\nThe untested branches correspond to error handling/logging routines in the Update and updatePod methods.\n. To facilitate the code review process, I will submit the same content as independent pull requests of 50-200 lines of code. \nThe PRs will be tagged as [Schema N], with N representing the number in the PR sequence.\n. @vishh addressed comments in the latest commit\n. @vishh @rjnagal ready to merge\n. @rjnagal @vishh ready to merge\nRegarding the comment, please look at my line note reply: https://github.com/GoogleCloudPlatform/heapster/pull/365#discussion_r33193039\n. squashed, tests pass locally in case you want to merge early\n. addressed most comments, squashed with previous commit. Please view line note answers for nits that were not addressed in the new commit.\n. ready to merge, merge build fail is irrelevant to this PR\n. inline comments addressed in the latest commit\n. renamed file and squashed changes. Thanks for the review!\n. Hey @piosz, indeed I am working on something very similar which involves both stat alignment and aggregation. \nFeel free to take a look at how aggregation and alignment is implemented in #388 and let us know if that satisfies your use cases. After that PR is merged, the next PR involves REST endpoints for all metrics.\nHere are some example REST endpoints:\n/api/v1/model/cluster/cpu-usage\n/api/v1/model/pod/monitoring-heapster-xk12d/memory-usage\n/api/v1/model/namespace/default/cpu-limit\nThe JSON response for these endpoints would be a single timeseries, represented as a list of timestamp-value pairs. These endpoints also have an optional starting time parameter, so you can only get metrics that are newer than your specified time.\nYou can take a look at the core of the next PR here:\nhttps://github.com/afein/heapster/compare/schema6...schema7\nLet me know if that model would work in your use case, in which case we can speed up the merge process to let you access that functionality faster!\n. LGTM, but @vmarmol should review it as well\n. overall LGTM, just two nits\n. just one nit on consistency, sorry I missed it before.\n. nits addressed in the latest commit (squash before merge pending)\n. Addressed all points in the latest commit. Also, renamed the /pod, /namespace, /node, /container endpoints to plural (/pods, /namespaces, /nodes, /containers), so that hitting api/v1/model/nodes would present a list of all the available hostnames and so on.\nThe actual implementation of these endpoints will be sent out on a different PR (otherwise this one will bloat up to around 1.5k LOC)\n. squashed all commits, resolved merge conflict and addressed the last inline note (update to freecontainers).\nThe hostname is the exact hostname that is reported by cadvisor, which I believe is the prefix of a machine's fqdn. For GCE clusters, it's kubernetes-minion-****. \nI will document this more in the followup PR, but any confusion caused by that shouldn't be a problem as the API consumers are expected to retrieve a list of available node names at the /nodes endpoint.\nI agree that bundling params into Request structs would be cleaner, I will address that in the followup PR.\n. overall LGTM\nFYI, you can use a commit link to point to a specific commit: https://github.com/mvdan/heapster/commit/67415bd031b1191e0fa0a9902d6d1c8e1cf47b73\n. LGTM. For the commit link, I meant only for the description, so whoever reviews your PR can just click through to the previous commit. You could mention a PR number in a commit message, though, like #418\n. updated the API to provide metrics under /metrics for each entity, thus enhancing browsability of the API and allowing a future /stats extension to be added to each entity.\nAlso addressed most of mvdan's comments.\n. LGTM!\nThanks a lot for fixing these, most of them were introduced during debugging and I omitted to clean them up. Can you squash?\n. Fair enough, even though they're definitely correct\n. Jenkins is behaving non-deterministically :/\n. @vishh squashed\n. Fixed according to mvdan's last suggestion and squashed. Sorry for the delay!\n. LGTM\n. Two more commits:\n- Separated all the getter implementations and tests outside of impl.go and impl_test.go . Also, addressed the nits pointed by @mvdan\n- Updated the cadvisor dependency to the current master version, as I was building against a version on my local GOPATH apparently. Hopefully, Travis will be compliant.\n. ping @vishh @mvdan \n. closing this PR to split it into multiple ones and reimplement the StatStore.\n. LGTM\n. Hi, which version of Kubernetes are you using? you can find out with kubectl version\n. LGTM, just one comment. No need to address it atm, I will revisit that file in a future PR to improve readability and maintainability\n. yes, please don't nit pick at this time, the code does not compile yet. This PR was opened mostly to initiate discussion. I will remove the (WIP) prefix when it's ready for an actual review\n. Implementation is finished. Feel free to review as normal.\n\\cc @vishh @mvdan \n. Addressed comments. Another commit with further documentation will be added soon.\n. moved to a separate package and added more documentation\nrebase pending, do not merge\n. rebased, no further changes pending on my part\n. Thanks for submitting this mwielgus, the panic will be fixed soon.\nThe error message that should have been reported is \"the model is not activated\".\nPlease set -use_model=true to use the model, as it is not enabled by default as of yet.\nIf the flag was set to true, then please try accessing the following endpoints and post the responses:\n/api/v1/model/stats/\n/api/v1/model/nodes/\n/api/v1/model/namespaces/default/metrics\n/api/v1/model/namespaces/default/stats\n. LGTM\n. Posted two primary concerns.\nFor future reference, I am currently in the process of refactoring some portions of the model and writing documentation, which might prove useful for future extensions such as this one.\nPS: the Travis test that is failing is a flaky test.\n. retest this please\n. LGTM\n. I am unable to reproduce the same test failure as Travis on my machine. Any clues?\n. it was a flaky test: https://github.com/kubernetes/heapster/pull/315/files#diff-e4dfa1a125bfbc4332db2c3b31a2b471L150\n. ping @mvdan @vishh\nunless there are any nits, this PR is ready to merge\n. addressed the two issues on behavior documentation and test readability\n. squashing and self-merging\n. As of Docker 1.8 we can use docker cp to copy files outside of a container.\nBecause of that, we can build a heapster-build container based on the golang image and copy the heapster binary outside of the container when the build is finished.\nThen, we can normally build the alpine-based image using the existing dockerfile, which will copy the new binary into the final container image.\n. sure, expect a PR within the week\n. An update on this PR:\nAt the moment, I'm trying to bring this to a condition where tests are passing.\nAfter that, most of the points that @vishh has mentioned will be also addressed, along with other leftover issues.\nThe majority of this PR includes Model10, which was submitted independently 4-5 weeks ago.\nNow that the underlying stores have changed, a large set of past tests are failing, mostly due to assumptions taken on the behavior of Get and Put with the old stores.\nI invited an initial review on this PR mostly for structural comments on the implementation of derived stats. However, there are still a few things that are leftover:\n- make tests passing\n- compare accuracy/memory footprint with that of the the older stores\n- cleanup of CAdvisor dependencies\n- breakdown of this PR into readable commits.\n- definition of epsilon per metric (initially hardcoded in types.go, with a TODO for a configuration object argument on the model constructor)\nI will ping @vishh and @mvdan when these tasks are completed and I will remove the (WIP) prefix from this PR then. At that point, please review the PR commit-by-commit for readability.\nETA: Tuesday 08/25\n. Quick Update:\nThis PR is almost fully operational. I've had to change the behavior of a few components to comply with the new stores.\nAs soon as this PR is mergeable, I will start submitting separate smaller PRs that link to this one to facilitate the code review process.\nThe changes that will be included in this set of PRs are:\n- usage of the new StatStore and DayStore\n- static epsilon configuration\n- Uptime calculation bugfix\n- Unit test refactoring for compliance\n- renaming of the top-level model interface to \"Model\" instead of \"Cluster\" to avoid confusion with the ClusterInfo struct\nFurthermore, based on preliminary testing, the memory usage of the Heapster Pod is now consistently between 30-40MB for a not-too-noisy cluster. Using the previous stores, the usage was 200MB instead, leading to an improvement of 5x (rough estimate). \n. @mvdan @vishh these are the travis and jenkins errors I was getting after my initial godep cleanup, which is why I closed that original PR.\nI am able to build and test locally, so I'm guessing there is some mismatch between what the expected testing environment is and what it should be\n. fleet is being used in sources/nodes/ and clusters/, therefore it is reasonable that the build should fail.\nThe reason that the local build was passing is that the googleapi dependency was located in my gopath.\nThe main concern around this issue is the following:\n- During the godep save ./... operation, the googleapi dependency is not included in Godeps, even though it is dependent by fleet\nI will run a secondary check on a clean gopath to see if reinstating the googleapi dependency will resolve this issue alone\n. You're right, seems to be a bug on my Godep version. I checked out all the Godeps from master and it seems to be working\n. Closing this PR. The feature set that was included has been broken down into #508 and #525\n. LGTM\n. Working on this as part of #478.\nETA: Tuesday\n. As of now, limits are being treated as metrics and they are, therefore, stored in a DayStore\nI propose that, initially, we should utilize the same epsilon for usages and limits.\nThen, after the current set of P0 issues is completed, we can edit the model to handle limits as constants, rather than timeseries.\n. Opened limits handling as #493 \n. Yes, static epsilon configuration was resolved as part of #508.\nMore dynamic options could be explored for future milestones, if that is deemed useful.\n. I am currently busy for the remainder of this week. If noone else picks this up I could work on it over the weekend\n. could you please provide the commit hash for your HEAD?\n. Travis complained on the googleapi dependency at #478 so I'm guessing this cleanup is not actually required\n. Do you get any response when accessing http://{master-ip}/api/v1/proxy/namespaces/kube-system/services/heapster/api/v1/model/? \nAlso, could you tell us the heapster version you're using through kubectl logs <heapster-pod>?\n. Hey @sanjana-bhat, sorry for the delay. The internal error is actually not supposed to occur outside of unit tests, which is quite confusing. It represents a case where your CPU usage metrics are actually flowing into the heapster model backwards in time.\nWe are currently in the process of refactoring the underlying codebase of the APIs (#478) so this issue might only affect v0.17.0.\nCan you try deploying heapster using the docker image located at afein/heapster-model:latest?\nIf your setup is still not operational with that docker image then the underlying problem would be related to your deployment environment, which we can try to dig into further.\nIf your setup does becomes operational with that docker image then the underlying problem will be fixed in the next heapster release.\nAlso, can you post the replication controller file that you are using to spawn heapster?\n. There seems to be some sort of issue with the data that is being scraped from the nodes.\nCould you let us know what version of kubernetes you are running through kubectl version?\n. From my understanding, it appears that heapster is constantly reading 0 for cpu usage while parsing model data.\nCan you try accessing \nhttp://{master-ip}/api/v1/proxy/namespaces/kube-system/services/heapster/api/v1/metric-export and searching for cpu/usage values that are zero?\n. @sanjana-bhat I added such a condition on #508, which is planned to be merged shortly.\n. @sanjana-bhat yes, the /stats endpoint has been removed temporarily to facilitate code review. It will be reintroduced in a future PR and it is definitely going to be included in the next heapster release.\nIs everything else fully operational?\n. The /stats endpoints were merged today in master, so you should be able to access the full feature set by building the current HEAD.\nI am not aware about the exact date of the next release (\\cc @vishh), but any further changes until that release would most probably involve internal quality\n. As of now, the model is updated roughly every 2.5 * model_resolution (which defaults to 2.5 minutes).\nThis means that the latest data available through both /stats and /metrics is the average for the last minute that is stored, which could be either 23:28, 23:29 or 23:30. If you are interested in the exact time, you can invoke the /metrics endpoint as well\n. The model_resolution flag is always considered, however 1 min is also the default value for model_resolution (https://github.com/kubernetes/heapster/blob/master/heapster.go#L43).\n. Jenkins was complaining after I removed the googleapi Godeps. I closed the PR to avoid confusion as I wasn't sure at the time if they are somehow used in integration tests\n. retest this please\n. addressed all comments except the change from uint16 -> uint32 .\n@vishh please let me know if you believe this change will be beneficial\n. addressed comments and squashed\n. rebased to split implementation and tests into separate commits, as per @mvdan's request.\n. as soon as we get an LGTM from @vishh\n. Added a fix for the panic error mentioned in #501 \n. Addressed comments in the latest commit.\nPending work on this feature set:\n- Filesystem Epsilon\n- Finalize the period of the Update method\n. This PR effectively addressed #484\n. Node pods are currently keyed by concatenating the namespace and the pod name such as default/frontend-xkyz. \nThis assures no pod name collisions can occur, but it is not the cleanest way of exposing that information. A PodPath struct can be created to hold the pod's namespace and its name.\n. actually you are right, pod.Name is the pod's name, as provided by the PodElement struct that is consumed from the cache.\nThe GetNodePods method manually constructs the <namespace>/<podname> path by calling the findPodNamespace method. Instead of that, we could directly key the NodeInfo.Pods map by <namespace>/<podname> \n. LGTM\n. Is this behavior present in other Getter methods as well?\n. Perhaps we should leave the flag as false by default, to avoid conflicts with users of non-Kubernetes clusters. The heapster deployment files that are used by Kubernetes can set the flag to true instead.\n. LGTM, finally the model is enabled by default :)\n. Finally! What is the time interval that is used in the calculation of the instantaneous CPU metric?\n. That is certainly possible. We would have to create a \"Kubernetes mode\" on the model which would cause the Kubernetes-specific API endpoints to return an appropriate message instead of invoking their corresponding Getter methods. \nOther than that, no further changes should be needed, as no PodElements should be received by the cache in non-Kubernetes clusters.\n. That would be also possible by editing the API handlers to skip endpoint registration according to a certain flag.\nThis behavior could be also automated by attempting to access kube-dns from within heapster, instead of asking for a CLI flag.\n. For reference, the memory limits are currently being set to -1 and the cpu limits are being set to 99 millicores.\n. This is defined in CAdvisor. \nMachine containers are populated here: https://github.com/google/cadvisor/blob/master/container/raw/handler.go#L168\nThe behavior you're describing could be handled here as well\n. As an additional note, this behavior also occurs if the container is restarted between measurements of cpu/usage_ns_cumulative. \n. Even though this is a very drastic change, I agree that it would lead to a more intuitive structure for the project and a reduced memory footprint.\nAlternatively, the model could replace the cache as it already maintains a set of compressed timeseries in memory.\n. I would strongly recommend resolving #528 alongside #635. The reason is that if the update period is a multiple of the model resolution then some derived model metrics (pods, namespaces, cluster) could incorrectly disregard values during aggregation. \nThis behavior is caused by the fact that the same model timestamp is used for both aggregation and updates and it was a tradeoff that was required to avoid adding complex logic that would cause derived metrics to intentionally lag behind in time.\n. Fair enough, but do keep in mind that reducing the model frequency before #528 is resolved might potentially reduce the accuracy of aggregated metrics in a periodic fashion.\nSpecifically, I suspect that every 1 in LCM(model_frequency, model_resolution) samples will be incorrectly aggregated to 0.\n. LGTM, thanks for catching this\n. Regarding metric aging, the model uses the self-managing DayStore structure which does not require external garbage collection for old metrics.\nAlso, as @vishh said, it is not possible to add/remove containers from Pods at runtime, which is probably why this functionality was originally left out.\nIf you are observing inaccuracies in aggregated Pod metrics, the root cause might be different than the garbage collection system. Feel free to elaborate more on your use case and your observations and I'll try to help pinpoint the problem\n. Thanks for the explanation of your concerns @bluebreezecf,\nAFAIK, when a Pod is terminated, the existing deletePod method should remove all references to the corresponding PodInfo struct, which should eventually trigger Go's garbage collection for the referenced DayStore altogether.\nBecause of that, I suspect that the underlying cause of your observations may not be caused by the existing garbage collection mechanisms. \nMay I ask the following:\n- How did you observe that old metrics are not removed for pods that have been deleted?\n- What types of inaccuracies did you observe in aggregated pod metrics?\n. The namespaces endpoint refers to Kubernetes namespaces. If you are not running heapster within a Kubernetes cluster then this endpoint is not enabled. \nIn standalone mode, you should be able to view your containers on a per-node basis under /api/v1/model/nodes/{node-name}/freecontainers\n. Yes, along with Labels. Labels are only provided at the Pod level at the moment, and maintaining them inside the InfoType will be helpful in propagating them up and down the hierarchy.\n. The timestamp of the realCluster signifies the last update from the cache. Therefore, for the first invocation we would like to invoke the Cache methods with parameters (start=zeroTime, end=zeroTime) to extract all the contents\n. addressed on the next commit\n. My initial thinking was to have two methods for each exposable resource, \"All\" and \"New\". I don't believe there is any reason to explore field-specific interfaces, at least not at this stage.\nGetAllXData() -> returns all metrics and structure under X\nGetNewXData(start time.Time) -> returns structure under X and metrics with timestamp > start\n\"Get\" functions return the cluster timestamp at the time of the request. Therefore, the user will be able to get only fresh data by calling the GetNewX methods with that timestamp as a parameter. This can prove useful for periodic AJAX calls, when the browser needs only newer data.\nNote: The timestamp signifies the latest metric \"end\" time, as provided by cadvisor, rather than time.Now(). This allows us to skip timezone conversions and calculations of the time difference from metric generation on cadvisor, until consumption through the heapster API\n. - Hostname string is the internal hostname from heapster,\n- For the Timestamp, please look at my answer at the previous inline comment\n- Returning pointers is NOT safe, the interface for getters is planned to change to return copies. I will make that change on the same PR as the unit tests of the getters.\nFor everything else, thorough comments are incoming on the next commit\n. @vishh @rjnagal The lock was moved outside the internal cluster object in commit https://github.com/afein/heapster/commit/32b54bd90a9f4a731ed5f449f9ca617e098a70eb , after  comment https://github.com/GoogleCloudPlatform/heapster/pull/358#discussion_r32685465 and the followup offline discussion.\nDue to a dependency with the upcoming [Schema 3] PR, the lock will be re-inserted into the realCluster as part of [Schema 4].\n. As mentioned offline, this check represents only the case where GetAllPodData is called before any data has been inserted in the cluster. \n. Interface-level methods need to be redesigned alongside the REST endpoints.\nThe existing Getters serve mostly as proof-of-concept endpoints. After the redesign, these methods will most probably be used as internal methods  for testing. Estimated PR: [Schema 6]\nFundamental ideas:\n- Different methods for historical and derived data\n- (start, end) parameters for historical data\n- Getters return (time, error) and accept a callback function which performs the appropriate conversion and copy of the internal types to external types.\n. done\n. done\n. constants nit are now addressed in the latest commit.\nI'm already planning to change the structure of the existing InfoType object at [Schema 8], to facilitate derived stats. Let's discuss offline and postpone structural concerns related to derived stats for a future PR.\n. That is the eventual goal, however there are a few issues to consider, such as:\n- time resolution of limits vs time resolution of usage.\n- mismatch of limit/usage timestamps.\n- input resolution versus output resolution for historical data.\n- instantaneous derived stats (e.g. current utilization percentage) and the required conversions of limits/usage values.\nFor those reasons, I believe it's better if we address the format of the resource struct together with the design of the derived stats.\n. addressed in the latest commit\n. The problem in that case is that (0+30+10)/3 is not round, as well as 1040/5, 1040/6 etc. As this is a cumulative average with unsigned values, the number of testcases that will actually match the exact average of all values is very limited, due to consecutive rounding for each individual average calculation. Even if we find such a testcase, that behavior is part of the the uint64 implementation and should require testing.\n. in_memory.go is used only for the cache implementation, it is barely documented and it is not as well-tested as cma_store. As the cache might (or might not) be eventually deprecated, in_memory might be removed as well, or just replaced by cma_store. Duplication was used now to avoid taking such deprecation decisions. \nI expect that the functionality of in_memory.go, if not deprecated, can be implemented within cma_store with a simple configuration flag and a conditional statement.\n. nit: creating the struct array before the for statements would help a lot with readability, both here and in the other test cases.\n. nit: we primarily use fmt.Errorf for new errors in Heapster.\n. For consistency with the rest of the codebase, can you use assert.Equal instead? \n. same here\n. right! The var definition was left over before I ported some code to parseRequestStartParam\n. Good call. We generally define a zeroTime variable if there are multiple instances of time.Time{} within the same scope\n. aren't you missing a newline before \\tExternal Sinks?\n. same here, the format string was \"\\n\\t\\t%s\" in the Sprintf version of the file\n. Most use cases of the API are meant for fresh data (scheduling, analytics, resource recommendations). As the model only stores a short total duration by default, exporting all the data  is not particularly wasteful.\nNonetheless, adding an end time to the APIs and getters is less than trivial, should I do it?.\n. default analogy means the ratio of model_duration to cache_duration (passed as bufferDuration to NewManager). As of now, model_duration = 5 * cache_duration for no specific reason. However, other options include using a flag to determine the exact duration, while defaulting to a multiple of cache_duration.\n. I think that parseRequestParam(\"start\", request, response) is more readable than parseRequestParam(request, response, \"start\"), especially for consecutive calls with the same request/response arguments.\n. I guess due to the size of the condition both could work. However, I will be deleting this function instead, as the most requested feature seems to be providing up-until-24-hours stats, which does not require the dayPassed check.\n. The changes in this file do not appear to have any impact on unit test success or coverage of either the cache or the model. Was this change made for the sanity of not having an empty pod?\nIf so, the comments of cacheFactory need to be updated as well. I can do this myself in a follow-up PR, if needed.\nPS: the pod was originally empty to test pod-level error-handling branches. However, further unit tests have covered these branches so there is no impact from this change.\n. It is the 95th percentile. The underlying Statstore only provides one hardcoded percentile value as of now, the 95th percentile, due to a pending integration with CAdvisor's percentile derivations.\nI can rename the struct field to NinetyFifth and consider reworking the StatStore implementation\n. this library assumes that the window size is a constant number of elements. In this case, we want an arbitrary number of tpEntries whose count fields sum up to the window size. For example, for a 1-minute resolution, 1-hour duration statStore, we might have just one tpEntry with a count of 60, if the value of the metric has not changed more than epsilon, on average.\nDue to that, our corresponding rewind operation would subtract values from the count of the earliest tpEntry, and it would be removed only if the count is zero.\n. totally agree, i'll make the names more explicit and add comments as well.\n. I don't believe that could be the case. Percentiles are declared in the 0-1 range. If someone is requesting the 95.4853th percentile then it might be a problem but for two decimal digits this should be accurate\n. epsilon represents error margin in value. Since all TimePoint values are assumed to be uint64, then having a floating point epsilon makes no sense. For example, setting epsilon to 50.53412 would effectively be the same as setting it to 50. Also, epsilon cannot be a percentage offset as that would cause bucket overlaps. For example, a bucket with average value of 100 and epsilon of 0.1 would overlap with a bucket with value of 110.\n. agree, will address.\n. invalid operation: ! time.Time\n. Time struct equality is only supported through the .Equal method of time.Time, as the method has to compare each field of the two structs\n. As we discussed offline, absence of data points is assumed as the value remaining constant at the last bucket. If an actual deletion/shutdown has happened, a higher-level event handler should take care of that and insert zero-valued timepoints.\n. this is the bucket max, and we cannot recalculate it as we have lost\ninformation of the other maxes in that bucket\nOn Aug 12, 2015 12:14 PM, \"Vish Kannan\" notifications@github.com wrote:\n\nIn store/statstore/stat_store.go\nhttps://github.com/kubernetes/heapster/pull/461#discussion_r36901283:\n\n\nif ss.start.Equal(time.Time{}) {\nss.start = ss.lastPut.stamp\n}\n  +}\n  +\n  +// rewind deletes the oldest one resolution of data in the StatStore.\n  +func (ss *StatStore) rewind() {\nfirstElem := ss.buffer.Back()\nfirstEntry := firstElem.Value.(tpBucket)\n// Decrement number of TimePoints in the earliest tpBucket\nfirstEntry.count--\n// Decrement total number of TimePoints in the StatStore\nss.tpCount--\nif firstEntry.maxIdx == 0 {\n// The Max value was just removed, lose precision for other maxes in this bucket\nfirstEntry.max = firstEntry.value\n\n\nWhy not calculate the new max?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/461/files#r36901283.\n. The existing API structure is meant to resemble the Kubernetes API, by having subpaths that indicate increased specificity in the cluster structure. \n\nBecause of that, something like /namespaces/{namespace-name}/pods/{pod-list}/metrics/{metric-name} would be cleaner. To implement that, we could change the existing handler for that path to attempt to extract a comma-separated list of podnames.\nAlternatively, we could try something like /namespaces/{namespace-name}/pods/sum/metrics/{metric-name}, having pod-list being a query parameter instead.\n. This is a much more \"fat\" API handler than the rest of the model handlers, which does not consider model locking. Due to the consecutive calls to GetPodMetric, it is possible that a model Update operation may be performed between these calls, leading to potential concurrency issues. \nFor example, if \"end\" is specified as zero, it is possible that some of the pods will return a []TimePoint with some timestamps further in the future than the result of other pods.\nTo avoid this issue, I would recommend following the pattern used in the other handlers, where a single model method is called by the handler. This would involve the creation of the following model method:\nGetPodListMetric(PodListRequest) []store.TimePoint\ntype PodListRequest struct {\n    NamespaceName string\n    PodNames []string\n    MetricName: string\n    Start: time.Time\n    End: time.Time\n}\n. For this case I would recommend creating a new struct in model/types.go that encapsulates a TimePoint and the count. \nFor further reference, on the next release of Heapster the model will always return TimePoints at every resolution for all entities that are active in the cluster, so at that point the sample count might not be as meaningful.\nIn general, feel free to modify any external type you may need at api/v1/model_types.go .\nHowever, please do not edit the TimePoint type or any existing types in model/types.go, as we are trying to minimize the total memory footprint of the model at this point.\n. You're right, the /pods/sum subpath could cause issues in case there is pod named \"sum\" in that namespace.\nA change in external result types is perfectly fine for this change.\n. not for historical data, meaning that you can't extract a timeseries for more than 1 hour ago.\nAs we discussed offline, the DayStore only provides derived stats using the hourly derived stats\n. That is a more complicated use case, assuming you mean that they want exactly the same start and end times for each metric. \nAs for now they can call each individual metric's API with the desired start and end.\nTo enrich the API, we could add a model method that extracts all metrics for an entity given the same start and end times\n. It is mentioned that these stats are exposed for each X metric, where X is {cluster, node, namespace, pod, container}. If that is not clear I can change the phrasing\n. could you elaborate? I am trying to be very explicit in the documentation and I'm not avoiding repetition. As latest CPU and Memory usage values are the only two things that are shown in the listing, I'm explicitly stating that.\n. At the moment, /namespaces/{namespace-name}/pods/ returns a list of all the pods, along with the latest cpu and memory usage values. If we change this behavior, then the API will not be fully browsable, as there would be no way for a consumer to know which pods are represented in the model.\n. @vishh: I've seen some test runs where s1.StoredTimeseries is 1, instead of 2.\n. fixed\n. changed\n. using []hourEntry would cause the daystore implementation to be fragmented to the third party package, for the definition of hourEntry. interface{} is cleaner\n. that was the initial plan but apparently this library performs really well. If the interface{} causes it to slow down we might as well scrap it for a ring buffer. \\cc @vishh\n. ETA: 3 hours\n. No, #478 uses this daystore.\nOn Fri, Aug 14, 2015 at 1:54 PM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nIn store/daystore/day_store_test.go\nhttps://github.com/kubernetes/heapster/pull/471#discussion_r37118362:\n\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package store\n+\n+import (\n-   \"testing\"\n-   \"time\"\n  +\n-   \"github.com/stretchr/testify/assert\"\n  +\n-   ss \"k8s.io/heapster/store/statstore\"\n\nWait, so should this be closed in favour of #478\nhttps://github.com/kubernetes/heapster/pull/478?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/471/files#r37118362.\n. please do not review this PR yet, there are 650 failing assertions and a\nset of refactoring changes that need to be put in first\nOn Aug 17, 2015 4:10 PM, \"Vish Kannan\" notifications@github.com wrote:\nIn model/aggregation.go\nhttps://github.com/kubernetes/heapster/pull/478#discussion_r37248065:\n\nfor key, tpSlice := range newMetrics {\n    _, ok := target.Metrics[key]\n    if !ok {\n        // Metric does not exist on target InfoType, create TimeStore\n-           newTS := rc.tsConstructor()\n-           target.Metrics[key] = &newTS\n-           // TODO(afein): configure epsilon\n-           newDS := daystore.NewDayStore(100, rc.resolution)\n\nDo you have a follow up PR for epsilon? 100 defeats the whole effort.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/478/files#r37248065.\n. the interface can be renamed to Model to reflect its holistic scope of the cluster's structure. Having sub-objects would require a redesign of the current locking mechanisms, as we have discussed in the past\n. this should have been a pointer on the manager. Thanks for catching this!\n. I guess the comment is misleading. What I meant to say is the following (will be edited in a new commit):\n\nif the DayStore has data within the past 0-1 hour:\n- The Average, Max and 95th pecentile are the corresponding values of the underlying StatStore\nif the DayStore has more than 1 hour of data then hourly stats have been flushed to the window buffer. Then:\n- The Max is calculated as the max of all entries in the window, and the max of the underlying StatStore\n- The Average and 95th percentiles are calculated from the entries in the window. the values in the underlying StatStore are NOT considered as some datapoints will be accounted for twice. For example, if a flush happened 10 minutes ago, the Hour StatStore will hold 50 minutes of data that have been accounted for in the latest hourEntry of the window. \nIf this is a concern on the accuracy of daily stats, I could Get all the TimePoints from the Hour StatStore that are later than the last hourly flush, and weigh these points appropriately for the calculation of the average. For the 95th percentile, we could calculate the 95th percentile of these points and use that to calculate the daily 95th percentile, along with the 95th percentiles from the window. \nThe problem in that case is that we will essentially be calculating the average/95th for a duration longer than one day, as we cannot partially discard the earliest value in the window. WDYT?\n. please view my answer on the previous line comment\n. I edited the above comment, please view on Github\n. We can still capture spikes through the Max, which considers the current hour, and their effect on the overall daily percentile/average will be reflected when that specific hour is flushed, as of now.\nIf we hold 23 hourly stats and use the StatStore for the past hour then we can only guarantee that the derived stats correspond to the range of (-24,-23) hours ago. The reason is that if the last flush has happened 1 minute ago, then the StatStore holds one hour of information, 59 minutes of which we do not need. Therefore, we would calculate the average,max and 95th percentile of the past 23 hours and 1 minute, in that case. This would lead to the daily Max not reflecting a spike that has occured 23 hours and 10 minutes ago.\nIf we hold 24 points to avoid that and we still utilize values from the StatStore, then these values are over the 24 hour range that is stored. As we cannot account for partial data in the earliest hour, we would essentially be calculating the max, average and 95th for the past 24 hours and 1 minute instead.\nBecause of these reasons, I believe that the current implementation is a fair compromise of precision over time length. We use exactly 24 hours of data for the average and 95th, even though these values are at most 59 minutes in the past. For the max, we consider up to 24 hours and 59 minutes in the past, which leads to increased flexibility for spike detection. WDYT?\n. I have observed this test being fuzzy even at the current master. Not sure what the cause is\n. That's a fair point. I no longer have access to a gcr.io bucket so someone else would have to actually deploy this.\nThese deployment files can be eventually deprecated, as soon as the next milestone is deployed in all kubernetes clusters. In that case, the model should be accessible by default in the heapster pod running in the kube-system namespace.\n. time.Time represents a time format, however the Uptime is a time.Duration field.\nSince there is no standardized format for durations (unlike RFC3339 for time.Time), I opted to output the total number of seconds for that duration\n. 95th. I'll change that name, as well as the Stats struct in model/types.go\nHowever, I will not change the json representation as of now to avoid conflicts in Kubedash\n. I opted to use the next largest possible type than uint8. I am not very familiar with Go's struct padding so I am not sure if that will result in any space optimization .\nIf you believe that uint32 is more appropriate, I'd gladly change it\n. done\n. If the initial check is not performed, then an empty StatStore would return an array with one empty TimePoint. Instead, we would want to skip the lastPut field as well and return an empty array, which would represent that no Put operations have been performed in the StatStore as of yet\n. fair point, will do so\n. will do\n. we are actually maintaining a count within lastPut to be able to calculate the cumulative average. I will change the implementation to use that counter instead.\n. roger that\n. it's actually an external (entity list), rather than an (external entity) list\nThis function converts a slice of internal entityListEntry to a slice of ExternalEntityListEntry.\nThe internal and external structs are identical, as of now, but they are distinct to avoid outputting internal types through the API\n. An entity is something that holds metrics in the model and can be provided as part of a listing. It \ncould be a namespace, a pod, a node, or a container.\nAn entity could also be the cluster itself, but since there is only one cluster entity it cannot be part of a listing\n. it is the external type for a single entry of a list of entities.\n. Model updates should ideally occur every 1 - 3 resolutions. We cannot update more often than the model resolution with the current implementation.\nMoreover, the update operation should not be an exact multiple of resolution, to be able to serve more accurate values at the latest minute. For that reason, I set the update frequency to min(cacheDuration, 2 * ModelResolution + 25 sec). This minimizes the number of Update operations that occur at the beginning of the latest resolution, which could decrease accuracy for metrics that are extracted on the very last minute.\nI can open an issue with the required steps, if more often updates are required.\n. Tracking the earliest creation timestamp during aggregation helps identify the uptime for abstract entities.\nFor example, the creation time of a namespace is the creation time of the first pod that was created in that namespace\n. The same units as the values for that specific metric.\nIt's millicores for all CPU metrics and bytes for all other metric. Should I document that?\n. As of now, filesystem metrics default to defaultEpsilon. I will address this in an upcoming PR\n. A delta is required due to the existing implementation of the StatStore, where values of the past 1 minute are being averaged together before being placed into buckets. The Get and Last methods of the StatStore expose that last one minute and we are, therefore, interested in its accuracy.\nBy having the 25 second delta, the values of the last one minute represent a larger set of samples than when not having an offset. That value was derived empirically, as only one in every 12 updates would align perfectly with the current resolution. Of course, there are other delta values to consider due to that.\nFor example, if Updates were happening at every resolution, then the very last resolution of every metric might represent only one actual sample that was used to generate it, depending on the alignment of metrics flowing in.\nIf no delta is used, then a mechanism would have to be added to force each metric to lag behind in time for the appropriate duration.\nFor example, a container would be lagging by 1 resolution, its parent pod would be lagging by 2 resolutions and its parent namespace would be lagging behind by 3 resolutions.\nTherefore, if we are intestered in serving the most recent data we can, we should retain a delta rather than forcing the updates to happen more often, as the second option would require us to delay the exposure of metrics by 1-3 resolutions, in order to retain accuracy\n. I'm not sure that Point is descriptive of what this struct represents, as Point sounds like a single point in time of a timeseries.\nEssentially this struct could be represent any of the following sets of values:\n(Node_name, latest_CPU, latest_Mem) \n(Pod_name, latest_CPU, latest_Mem) \n(Namespace_name, latest_CPU, latest_Mem)\n(Container_name, latest_CPU, latest_Mem)\nI was using the word Entity to describe Node | Pod | Namespace | Container, which are essentially the entities of the cluster that are represented in the model. Do you think we should look for a better word to describe these \"entities\"?\n. You're right, I included these configs accidentally in this PR.\n. Totally agree, opened #528 \n. Agree, #528 \n. ",
    "jfoy": "We have a CCLA as whitepages.com .\n. Stand by -- integration testing shows this didn't solve the issue.\n. Looks like there's a deeper issue for us here -- node.Status.Addresses is not getting populated. I'm tracing through why this may be.\n. This issue turned out to be very close to #202, but I think we've addressed the underlying problem that forced reverting that change with #210.\nLegacyHostIP is documented as equivalent to PublicIP in the cloud providers. With this change, the algorithm for choosing a node's IP is as follows:\nIf the node has an InternalIP, use that address;\notherwise, if the node has a hostname that differs from its PublicIP, resolve the hostname and use that address;\notherwise, use the node's PublicIP (which may have been passed as LegacyHostIP.)\nThis version is working as expected in our cluster. \n. Looks like I need to add more unit tests to pass the coverage check.\n. make test-unit-cov is passing on my box; what am I missing here?\n. Unit tests are green on my system. (Note: I've changed GOOS=linux to GOOS=darwin for local testing.)\n201345m0:heapster jfoy$ make test-unit\nrm -f heapster\nrm -f ./extpoints/extpoints.go\nrm -f ./deploy/docker/heapster\ngo get github.com/tools/godep\ngo get github.com/progrium/go-extpoints\ngo get golang.org/x/tools/cmd/vet\nhooks/check_boilerplate.sh\nhooks/check_gofmt.sh\nhooks/run_vet.sh\nGOOS=darwin GOARCH=amd64 CGO_ENABLED=0 go generate github.com/GoogleCloudPlatform/heapster\nextpoints: Processing file extpoints/interfaces.go\nextpoints: Found interfaces: []string{\"SourceFactory\", \"SinkFactory\"}\nextpoints: Writing file extpoints/extpoints.go\nGOOS=darwin GOARCH=amd64 CGO_ENABLED=0 godep go build -a github.com/GoogleCloudPlatform/heapster/...\nGOOS=darwin GOARCH=amd64 CGO_ENABLED=0 godep go build -a github.com/GoogleCloudPlatform/heapster\nGOOS=darwin GOARCH=amd64 CGO_ENABLED=0 godep go test --test.short github.com/GoogleCloudPlatform/heapster/...\n?       github.com/GoogleCloudPlatform/heapster [no test files]\n?       github.com/GoogleCloudPlatform/heapster/clusters/coreos [no test files]\n?       github.com/GoogleCloudPlatform/heapster/extpoints   [no test files]\nok      github.com/GoogleCloudPlatform/heapster/integration 0.015s\n?       github.com/GoogleCloudPlatform/heapster/manager [no test files]\n?       github.com/GoogleCloudPlatform/heapster/sinks   [no test files]\nok      github.com/GoogleCloudPlatform/heapster/sinks/api   0.063s\nok      github.com/GoogleCloudPlatform/heapster/sinks/gcl   0.012s\n?       github.com/GoogleCloudPlatform/heapster/sinks/gcm   [no test files]\nok      github.com/GoogleCloudPlatform/heapster/sinks/influxdb  0.011s\nok      github.com/GoogleCloudPlatform/heapster/sources 0.014s\n?       github.com/GoogleCloudPlatform/heapster/sources/api [no test files]\nok      github.com/GoogleCloudPlatform/heapster/sources/datasource  1.321s\nok      github.com/GoogleCloudPlatform/heapster/sources/nodes   0.014s\n?       github.com/GoogleCloudPlatform/heapster/util/gce    [no test files]\nok      github.com/GoogleCloudPlatform/heapster/util/gcstore    0.015s\n?       github.com/GoogleCloudPlatform/heapster/validate    [no test files]\n?       github.com/GoogleCloudPlatform/heapster/version [no test files]\n201345m0:heapster jfoy$ echo $?\n0\n. The failing test appears to get non-deterministic input:\nfunc TestFuzzInput(t *testing.T) {\n    var input source_api.AggregateData\n    fuzz.New().Fuzz(&input)\n    timeseries, err := NewDecoder().Timeseries(input)\n    assert.NoError(t, err)\n    assert.NotEmpty(t, timeseries)\n}\nIs there a way to get the tests to show a seed, to reproduce a particular fuzzed input?\n. The failed integration builds appear to be an infrastructure error: \nfailed to load docker image \"heapster:e2e_test\" using temp file \"/tmp/kubernetes-minion-71yx171296728\" on host \"kubernetes-minion-71yx\" (\"exit status 1\") - \n\"Warning: Permanently added '104.197.24.127' (ECDSA) to the list of known hosts.\ntime=\\\"2015-06-04T19:01:01Z\\\"\nlevel=\\\"fatal\\\"\nmsg=\\\"Error: Untar fork/exec /usr/bin/docker: argument list too long\\\"\nERROR: (gcloud.compute.ssh) [/usr/bin/ssh] exited with return code [1].\"\n. Thanks!\n. CCLA signed under Whitepages.\n. /cc @aphyr\n. Seems I can't see the failing e2e test; is there a way to retrieve the result? \n. OK. What else do we need to review before this can be merged? \n. Great -- I'll get this caught up.\n. I will -- thanks for your patience. \n. Not forgotten. We've been iterating on this internally, and I have a set of changes ready to go, which I'll push with the rebase and squash.\n. Done and squashed.\n. What's going on with our CCLA? We signed one from Whitepages, which seems to have vanished.\n. Fixing dependencies to unblock unit tests. (How embarrassing.)\n. That was weird. Reverting to the prior state until I figure out what's going on with godep.\n. @mvdan Awesome, thank you. I would like to understand where the variation comes from, though -- if I do a 'godep restore' followed by a 'godep save -t', why does the resulting Godeps/Godeps.json show so many deleted packages? \n. Here's what I see:\n```\n201345m0:heapster jfoy$ godep restore\n201345m0:heapster jfoy$ git status\nOn branch riemann_sink\nYour branch is up-to-date with 'jfoy/riemann_sink'.\nnothing to commit, working directory clean\n201345m0:heapster jfoy$ godep save -t ./... github.com/progrium/go-extpoints\n201345m0:heapster jfoy$ git status\nOn branch riemann_sink\nYour branch is up-to-date with 'jfoy/riemann_sink'.\nChanges not staged for commit:\n  (use \"git add/rm ...\" to update what will be committed)\n  (use \"git checkout -- ...\" to discard changes in working directory)\nmodified:   Godeps/Godeps.json\ndeleted:    Godeps/_workspace/src/code.google.com/p/go-uuid/uuid/LICENSE\ndeleted:    Godeps/_workspace/src/code.google.com/p/go-uuid/uuid/dce.go\ndeleted:    Godeps/_workspace/src/code.google.com/p/go-uuid/uuid/doc.go\ndeleted:    Godeps/_workspace/src/code.google.com/p/go-uuid/uuid/hash.go\ndeleted:    Godeps/_workspace/src/code.google.com/p/go-uuid/uuid/node.go\ndeleted:    Godeps/_workspace/src/code.google.com/p/go-uuid/uuid/time.go\ndeleted:    Godeps/_workspace/src/code.google.com/p/go-uuid/uuid/util.go\ndeleted:    Godeps/_workspace/src/code.google.com/p/go-uuid/uuid/uuid.go\ndeleted:    Godeps/_workspace/src/code.google.com/p/go-uuid/uuid/uuid_test.go\ndeleted:    Godeps/_workspace/src/code.google.com/p/go-uuid/uuid/version1.go\ndeleted:    Godeps/_workspace/src/code.google.com/p/go-uuid/uuid/version4.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/mount/flags.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/mount/flags_freebsd.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/mount/flags_linux.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/mount/flags_unsupported.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mount.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mount_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mounter_freebsd.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mounter_linux.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mounter_unsupported.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mountinfo.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mountinfo_freebsd.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mountinfo_linux.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mountinfo_linux_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mountinfo_unsupported.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/mount/sharedsubtree_linux.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/mount/sharedsubtree_linux_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/units/duration.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/units/duration_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/units/size.go\ndeleted:    Godeps/_workspace/src/github.com/docker/docker/pkg/units/size_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/cgroups.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/cgroups_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/cgroups_unsupported.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/apply_raw.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/blkio.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/blkio_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/cpu.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/cpu_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/cpuacct.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/cpuset.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/cpuset_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/devices.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/devices_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/freezer.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/freezer_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/fs_unsupported.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/hugetlb.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/hugetlb_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/memory.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/memory_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/net_cls.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/net_cls_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/net_prio.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/net_prio_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/perf_event.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/stats_util_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/util_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/utils.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/fs/utils_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/stats.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/systemd/apply_nosystemd.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/systemd/apply_systemd.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/utils.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/configs/cgroup.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/configs/config.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/configs/config_test.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/configs/config_unix.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/configs/device.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/configs/device_defaults.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/configs/hugepage_limit.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/configs/interface_priority_map.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/configs/mount.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/configs/namespaces.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/configs/namespaces_syscall.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/configs/namespaces_syscall_unsupported.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/configs/namespaces_unix.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/configs/namespaces_windows.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/configs/network.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/configs/validate/config.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/system/linux.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/system/proc.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/system/setns_linux.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/system/syscall_linux_386.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/system/syscall_linux_64.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/system/syscall_linux_arm.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/system/sysconfig.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/system/sysconfig_notcgo.go\ndeleted:    Godeps/_workspace/src/github.com/docker/libcontainer/system/xattrs_linux.go\nmodified:   Godeps/_workspace/src/github.com/prometheus/client_golang/text/bench_test.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/apis/experimental/install/install.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/client/unversioned/clientcmd/api/helpers.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/client/unversioned/clientcmd/api/latest/latest.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/client/unversioned/clientcmd/api/register.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/client/unversioned/clientcmd/api/types.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/client/unversioned/clientcmd/api/v1/conversion.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/client/unversioned/clientcmd/api/v1/register.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/client/unversioned/clientcmd/api/v1/types.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/client/unversioned/clientcmd/auth_loaders.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/client/unversioned/clientcmd/client_config.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/client/unversioned/clientcmd/doc.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/client/unversioned/clientcmd/loader.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/client/unversioned/clientcmd/merged_client_builder.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/client/unversioned/clientcmd/overrides.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/client/unversioned/clientcmd/validation.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/probe/doc.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/probe/exec/exec.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/probe/http/http.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/probe/probe.go\ndeleted:    Godeps/_workspace/src/k8s.io/kubernetes/pkg/probe/tcp/tcp.go\n\nUntracked files:\n  (use \"git add ...\" to include in what will be committed)\nGodeps/_workspace/src/github.com/onsi/\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n201345m0:heapster jfoy$\n``\n. @vishh I'm pulling together some config samples.\n. I can't tell by looking at the output -- is this another artifact of the e2e pipeline?\n. I would love help addressing the godeps issue. I'm anxious to get this merged. \n. Will do! Thanks for updating this.\n. @mwielgus Did this make it into heapster-scalability?\n. Done.\n. We'll need to build our own image even if we derive it from an existing base, because we still need to supply an appropriate/etc/riemann/riemann.config.\n. Yes, done.\n. Not in the engine itself. There's a separately-distributed [dashboard](https://github.com/aphyr/riemann-dash), which in practice I would package as a separate container. It wouldn't be crazy for it to be in the same pod, though.\n. Done.\n. Done. I'll add corresponding documentation.\n. Done.\n. Done.\n. No, my mistake. I hadn't rungodeps save`.\n. We can route to InfluxDB via Riemann, and you probably typically would. It's our intent to do so. Setting up that configuration is a larger task than I wanted to take on in this PR.\n. It's a good idea, but I'm tempted to defer. We have run with riemann-dash in the past, and we're not currently doing so; it's a useful tool for exploration and certain visualizations. In practice, we stream our metrics from Riemann to Grafana dashboards. \n. ",
    "krancour": "\nHeapster does not use etcd.\n\nBut Fleet uses etcd.  From the Fleet documentation:\n\nfleetctl currently requires direct communication with the etcd cluster that your fleet machines are configured to use.\n\nI see this repeatedly in my logs when I try to start heapster on a machine that isn't also running etcd:\nJun 08 19:28:28 ip-10-0-1-251 bash[1673]: INFO client.go:291: Failed getting response from http://:4001/: dial tcp :4001: connection refused\nJun 08 19:28:28 ip-10-0-1-251 bash[1673]: ERROR client.go:213: Unable to get result for {Get /_coreos.com/fleet/machines}, retrying in 1s\nPort 4001 is an etcd port.\n. Thanks.  That helped!\n. ",
    "detiber": "@shilpapadgaonkar I've been working on this myself.  I've gotten a bit further than you have, but am still hitting a roadblock on the correct service URL to use internal to the cluster.  Here is my modified replication controller:\n[openshift@ip-172-19-47-10 ~]$ cat heapster-rc.json \n{\n    \"apiVersion\": \"v1beta3\",\n    \"kind\": \"ReplicationController\",\n    \"metadata\": {\n    \"labels\": {\n        \"name\": \"heapster\"\n    },\n    \"name\": \"monitoring-heapster-controller\"\n    },\n    \"spec\": {\n    \"replicas\": 1,\n    \"selector\": {\n        \"name\": \"heapster\"\n    },\n    \"template\": {\n        \"metadata\": {\n        \"labels\": {\n            \"name\": \"heapster\"\n        }\n        },\n        \"spec\": {\n        \"containers\": [\n            {\n            \"image\": \"kubernetes/heapster:v0.13.0\",\n            \"name\": \"heapster\",\n            \"command\": [\n                \"/heapster\",\n                \"--source=kubernetes:http://kubernetes-ro.default.local?auth=\",\n                \"--sink=influxdb:http://ip-172-19-51-208.ec2.internal:8086\"\n            ]\n            }\n        ]\n        }\n    }\n    }\n}\nYou will probably not want to edit the --sink flag for the heapster command I'm trying to route the metrics to an external InfluxDB instance instead of one running in a pod.\nThe first issue that I ran into was that /etc/kubernetes/kubeConfig/kubeConfig and /etc/ssl/certs do not exist on the OpenShift nodes.\nThe kubernetes-ro service is exposed, but I do not seem to be able to reach it using the skydns provided name (kubernetes-ro.default.local) or via the default service ip address (172.30.0.1), but I believe that is because the deployment I am running (OSE Beta3 or Beta4 builds should not suffer from that issue).\n. @vishh as far as I can tell that is correct. @smarterclayton am I missing something, or do you have thoughts on how to bridge the gap (if any)\n. ",
    "smarterclayton": "We don't generate that token, but the token should be replaceable with a service account.  One sec, let me poke.\n. Ok, so the token should be injected by the service account in openshift into the standard service account location /var/run/secrets/kubernetes.io/serviceaccount.  If you ENVVAR=$(cat $file) you can then pass it as an argument in the shell.\n. @deads2k is there any reason why the service account token for a cluster reader wouldn't be able to access pods?  Can you help debug here\n. Any update on this?  Haven't seen it working yet - not sure if I missed it.\n. We would certainly like to have network metrics, since it can be a\nsignificant bit of info for administrators.  How dirty and ugly do we think\nit will be?\nOn Tue, Dec 29, 2015 at 4:37 PM, Federico Simoncelli \nnotifications@github.com wrote:\n\n@jimmidyson https://github.com/jimmidyson with regard to openshift\n(openshift-metrics) when is expected the new API to be finalized? Is it for\n3.2? We expected to have network metrics in 3.1.z. cc @smarterclayton\nhttps://github.com/smarterclayton to keep me honest on the expectancy.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/368#issuecomment-167883053\n.\n. Yes - it would be through the hawkular sync.\n. @mwringe\n. Yes, the OpenShift team members are trying to work with the autoscaling\nteam.  It's still in progress so unfortunately we don't have a single\ndocument yet.\n\nOn Fri, Aug 28, 2015 at 4:26 AM, shilpapadgaonkar notifications@github.com\nwrote:\n\n@smarterclayton https://github.com/smarterclayton : Is there some\ndocumentation explaining how openshift will use hawkular to implement\nauto-scaling. Is this the right place --\nhttps://github.com/kubernetes/kubernetes/blob/master/docs/proposals/autoscaling.md\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/502#issuecomment-135661078\n.\n\n\nClayton Coleman | Lead Engineer, OpenShift\n. If leaky is imported just to get pod infra container name, it's not worth it.  I'd just have it be a constant or parameter in this code.\n. Does this need sign off from sig auth?\n. ",
    "deads2k": "\n@deads2k is there any reason why the service account token for a cluster reader wouldn't be able to access pods? Can you help debug here\n\nSeems to be working fine:\n```\n[deads@deads-dev-01 origin]$ export KUBECONFIG=openshift.local.config/master/admin.kubeconfig \n[deads@deads-dev-01 origin]$ oc get secrets\nNAME                       TYPE                                  DATA      AGE\nbuilder-dockercfg-goyxb    kubernetes.io/dockercfg               1         21s\nbuilder-token-8xvwf        kubernetes.io/service-account-token   2         21s\nbuilder-token-j9uw0        kubernetes.io/service-account-token   2         21s\ndefault-dockercfg-3xc56    kubernetes.io/dockercfg               1         21s\ndefault-token-czoly        kubernetes.io/service-account-token   2         21s\ndefault-token-iwx0m        kubernetes.io/service-account-token   2         21s\ndeployer-dockercfg-qdiio   kubernetes.io/dockercfg               1         21s\ndeployer-token-n4i7n       kubernetes.io/service-account-token   2         21s\ndeployer-token-pg41k       kubernetes.io/service-account-token   2         21s\n[deads@deads-dev-01 origin]$ oc describe secrets/default-token-czoly\nName:       default-token-czoly\nNamespace:  default\nLabels:     \nAnnotations:    kubernetes.io/service-account.name=default,kubernetes.io/service-account.uid=fa6f4ad4-44f3-11e5-ae1e-28d2447dc82b\nType:   kubernetes.io/service-account-token\nData\nca.crt: 1066 bytes\ntoken:  eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tY3pvbHkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImZhNmY0YWQ0LTQ0ZjMtMTFlNS1hZTFlLTI4ZDI0NDdkYzgyYiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.Q7C2DQwQEy4oTalRpUDjSPtiZKfBNvp9aV7eSCcSseulxxF1EQNb5lmnvnOQDnnaku3gYRsAs3G_6VzBI3QssgVJGBY_O1vFOA0pXm8vRs4GE9e0fZ11UDRJXvOktMXwPPERfJ8-ufd58ENBxr1P2gH8R8gN0pUDKB9uf6D-nW685ZKWG5wPFK-OYL-kNLeB-V0zbxHJ37CU_H5jC8Nc9h-7B1jD9-3LGNhl0UneDr4VvZ1vEchacKnzbXRjvMsNLtHD2G6PEFoZRwKmzcUkG5oUwCrwfwbiYHzBAugKaeH9K2fPU7PD5EZPxmepXi05BrCa2mf_f9B6TRnMmdDRpw\n[deads@deads-dev-01 origin]$ oc get pods --all-namespaces --token=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tY3pvbHkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImZhNmY0YWQ0LTQ0ZjMtMTFlNS1hZTFlLTI4ZDI0NDdkYzgyYiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.Q7C2DQwQEy4oTalRpUDjSPtiZKfBNvp9aV7eSCcSseulxxF1EQNb5lmnvnOQDnnaku3gYRsAs3G_6VzBI3QssgVJGBY_O1vFOA0pXm8vRs4GE9e0fZ11UDRJXvOktMXwPPERfJ8-ufd58ENBxr1P2gH8R8gN0pUDKB9uf6D-nW685ZKWG5wPFK-OYL-kNLeB-V0zbxHJ37CU_H5jC8Nc9h-7B1jD9-3LGNhl0UneDr4VvZ1vEchacKnzbXRjvMsNLtHD2G6PEFoZRwKmzcUkG5oUwCrwfwbiYHzBAugKaeH9K2fPU7PD5EZPxmepXi05BrCa2mf_f9B6TRnMmdDRpw\nError from server: User \"system:serviceaccount:default:default\" cannot list all pods in the cluster\n[deads@deads-dev-01 origin]$ oadm policy add-cluster-role-to-user \"cluster-reader\" system:serviceaccount:default:default\n[deads@deads-dev-01 origin]$ oc get pods --all-namespaces --token=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tY3pvbHkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImZhNmY0YWQ0LTQ0ZjMtMTFlNS1hZTFlLTI4ZDI0NDdkYzgyYiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.Q7C2DQwQEy4oTalRpUDjSPtiZKfBNvp9aV7eSCcSseulxxF1EQNb5lmnvnOQDnnaku3gYRsAs3G_6VzBI3QssgVJGBY_O1vFOA0pXm8vRs4GE9e0fZ11UDRJXvOktMXwPPERfJ8-ufd58ENBxr1P2gH8R8gN0pUDKB9uf6D-nW685ZKWG5wPFK-OYL-kNLeB-V0zbxHJ37CU_H5jC8Nc9h-7B1jD9-3LGNhl0UneDr4VvZ1vEchacKnzbXRjvMsNLtHD2G6PEFoZRwKmzcUkG5oUwCrwfwbiYHzBAugKaeH9K2fPU7PD5EZPxmepXi05BrCa2mf_f9B6TRnMmdDRpw\nNAMESPACE   NAME      READY     STATUS    RESTARTS   AGE\n[deads@deads-dev-01 origin]$ \n```\n. > @jimmidyson \n\nYes I am getting metrics for only the pods within the default namespace. I also have some pods running in demo namespace and i dont see any metrics from these pods\n\n@shilpapadgaonkar Can you show me the URLs you're requesting and the return codes you're getting back?  I wonder if you're accidentally specifying the namespace on your request.  You should be hitting https://<yourip>:8443/api/v1/pods.\n. For general authorization, we've said the boundary is at the namespace level. I think that's probably sufficient since it will contain the damage to a scope where the person affected has the power to fix the situation.  Basically, the project admin shouldn't run pods that mess with his other pods' metrics in the same namespace.\nI'm less certain about using the SA credentials, but its probably just a lack of familiarity with set up.  Using the SA tokens would allow heapster (and by side effect anyone who can masquerade as heapster or has edit level access to the heaspter namespace) direct access to credentials that can be used against the API server.  When I set up heapster, is it configured as a tightly controlled component where this sort of interaction is safe?  You'd have to require SSL and the serving cert should be signed in way that allows the heapster callers to validate it.\n. > worse than that, it would mean changing the auth layer to authenticate requests to proxy subresources differently than the rest of the API (with fallback to support older clients)\nThis isn't a particularly difficult thing to do.\n\nit would also mean changing clients to authenticate requests to proxy subresources differently than the rest of the API (with fallback to support older servers, hoping the older server isn't passing through credentials intended for the API to the backend pods)\n\nYes, it would.  However, it would allow token based authentication for services through the proxy in a consistent way.\n. > In this case, the \"authenticate\" step would effectively just have to return an empty user, and pass the bearer token to the authorizer.\nSee https://github.com/kubernetes/kubernetes/pull/28788 for webhook authentication endpoint in the API server and https://github.com/kubernetes/kubernetes/pull/20573 for kube SAR.\n. > IIUC, webhook auth only supports token, and not client cert (which a fair number of people use).\nYes, but you could still want to support both: client certs and/or API tokens.\n. > How can we fit the heapster attributes into the subjectaccessreview object?\nIn openshift we've used synthetic access checks where we would model those concepts something like this:\n1. historical query -> verb: get\n2. model query -> verb: get, subresource: model\n3. creation -> verb: create\nAssuming you model the things you're getting and updating as resources, that allows clean ACL for your verbs\nI'm less sure about the resources\n1. namespace name -> this seems to map directly to our existing namespace concept, so in the attributes it would be namespace=namespace.Name\n2. PodName/PodID - I don't understand the distinction between these.  We key based on PodName now.  Are we worried about re-using a name and an old credential having rights to update it?  Drawing an ACL boundary on namespace seems to make sense, so this distinction seems to matter less.  I would expect apiGroup: heapster, resource: pod, name: name\n3. ContainerName - I'm not sure I see the need to acl down to this level.  Why would I want to subdivide my access based on containers inside pods?\n   1 NodeName - apigroup: heapster, resource: nodes ?\n. Ok, so if we model resources like this:\n1. group=heapsterGroup, namespace=normal, resource=pod, name=podname, subresource=model, for most requests, then we're able to use the get/create/update verbs like we'd expect\n2. group=heapsterGroup, namespace=\"\", resource=pod, name=\"\", for cases where you're using pod UID, that would make it a privileged operation\n3. group=heapsterGroup, namespace=\"\", resource=container, name=name, for cases where you're using container without a pod, that would make it a privileged operation\nIs there a case where one group of people can see containers on some nodes and not others?\n. > How do we deal with multi-namespace requests (do we make multiple requests, do we just say any number of namespaces greater than one requires permissions in to push in any namespace (which seems like it could yield confusing results)), what does the prefix map to, etc?\nIn openshift, when an operation requires multiple permissions, we check for permission to perform each action.  For example tagging an image in another namespace requires permission to tag in ns1 and permission to pull the image from ns2.\n\nwhat does the prefix map to, etc?\n\nHmm... we need to subdivide at that level of granularity?  I could be done via subresource, but that would be unpleasant for all concerned.\n. > That might be a lot of requests, conceivably. Is it a reasonable operation to first check for \"ANY NAMESPACE\" permissions on more that one request, and then if you don't have that, make the individual requests (but have a cap of some sort)?\nEh, you could, but generally you're running an LRU with a TTL to keep from making remote calls.  Since kube and openshift now agree on the Authorize interface, I think you can just lift our layers.\n. > Should this have separate authorization, or just bounce off of the main API server use SARs, etc?\nSeparate authentication powered by the webhook authorizer.\n. > I think we're agreeing here -- my point was that you don't necessarily need/want the other auth mechanisms -- you just want the webhook one.\nYeah, your launch command probably doesn't need other options.\n. > cc @deads2k I have to make sure I didn't actually break anything, but the tests pass, etc.\n\nLet me know if you see anything that could be improved re: the split repositories, generic API server, etc.\n\n\"please look through 1 million changed lines to find something funny looking\"  sniff. A few comments on wiring, but it looks largely ok.  I'll leave detailed review to a heapster specialist.\n@DirectXMan12 nice job, this separation looks a lot cleaner.. > The alternative is to run a heapster per namespace. This solves both authz and qos problems. If you have a free tier where you can't afford to run heapster per namespace, you can use a shared instance, but restrict access to it.\nI think that heapster per-namespace is going to have similar requirements about separating readers and writers.  The kube TokenAccessReview, SubjectAccessReview and Impersonation features were intended to enable use-cases like this one.  I think this will be good experience for us as we try to move those APIs out of the beta stage.\nAlso, this feature wouldn't preclude per-namespace heapster scenarios if we want to try them out.\n. You'll have to terminate client certs yourself or support the impersonation headers from wherever they are terminated.\n. > Yeah, that's more or less what I was envisioning. I can add a couple of examples to the proposal.\nPlease add one.\n. > When you're querying historical metrics, there's no guarantee that you'll have that info (you may have been restarted, etc). It's possible for model metrics, but the model doesn't support UID queries.\nAre there some cases where you will have the information?  Is it difficult to use it when you have it?\n. > probably just reject -- we don't want someone to be able to DoS the cluster and or Heapster like that in a case with a lot of namespaces.\nThat sounds reasonable to start, but you should clarify it.\n. So if metrics == push metrics not prefixed with username and unprefixed-metrics == push metrics with no prefix, is there a permission that says, \"push metrics that are prefixed with my username\"?\nAlso, how are you going to deconflict user \"billy\" pushing \"bob-foo\" and user \"billy-bob\" pushing \"foo\"?\n. All these queries need to be setting an APIGroup of something under heapster.k8s.io\n. nodes and namespaces will be named?  It would be helpful to have one example each here too.\n. In core kube, we actually parse the URL separately to determine the authorization attributes.  It has the advantage of being very obvious that you're handling every incoming request.  I think you should strongly consider doing something similar.\n. > Yeah, the client certs stuff will require having the CA available, as I noted below, unless I'm misunderstanding you.\nYeah, I got there after I left a comment here.  Impersonation headers may still be useful and it seems like you may have an intersection with https://github.com/kubernetes/kubernetes/pull/29714.\n. > The intended pattern is/was nginx pushes against metrics with total-requests. Heapster checks if you're permitted to push unprefixed metrics, and if not, sees that you're permitted to push prefixed metrics, and thus prepends custom/$USERNAME/. Also, $USERNAME needs to have permissions allowing it to create pod/metrics\nI don't think that pattern is a good idea.  It seems like writing to different metrics based on your current level of access will produce confusion.  Seems like you should make them say what they mean.\n. > That won't work when Heapster gains the ability to support metrics on objects other than pods. How do you distinguish between querying for metrics on an object in two different API groups if you can't pay attention to the SubjectAccessReview's API group?\nWhat about apiGroup=heapster.k8s.io, resource=pods.legacy.k8s.io?  You cannot assume that there isn't an actual pods/metrics endpoint that needs to be ACLed in a different group.\n. > It's possible (although it requires keeping a list of all pods ever seen), but it seems like it makes it less confusing to users just to blanket say \"pod uid access is more privileged\" vs saying \"pod uid access is sometimes more privileged\".\nI can buy that.\n. to match normal authz rules, this should have the namespace and the name set to somens\n. These help.  Make a top level note that we're controlling access to metrics on resource X, but not subdividing ACL on individual metrics on resource X.  I think this reasonable, but its worthy of note.\n. Take a look at the other install.go files under /apis in authentication, authorization, batch.  They're much smaller.\n. This doesn't look right.  What kind of event is this?  Normally you'd using something like runtime.Encode.. break the rewiring in this file into its own commit for us? Looks more involved than the rest.. got to the end here.  At a glance, this looks good.  Did you support a general heapster webhook authentication before?. > The library updated from taking an interface{} type to taking a pre-encoded blob of json. Presumably, it was just calling json.Marshal before, but the library was doing it internally. I can switch it over to runtime.Encode\nI won't block on it, but I think that would be better.. What happened to these? Unnecessary?. There's a way to add health checks.  Did you accidentally lose this entirely?. Also, your main looks to be fighting with some of the API server endpoints like heath. This looks dirty.  doesn't have to be fixed here, but why do you have a side-channel to load the server location instead of a kubeconfig file?. wait, does this test not even attempt to close the server?. Looks like its preexisting, but you should have an issue.. you're starting a secure server without security...... > you're starting a secure server without security.....\nYeah, given a loopback identity which you have access to, you should be using that.  followup I guess since its preexisting.. ",
    "miheer": "I followed the instructions as per  https://docs.openshift.org/latest/admin_guide/cluster_metrics.html as a root i.e.system:admin user but, I'm been shown pods related to system:admin user and not the pods related to the user joe in the grafana UI.\nI see  a tile in the grafana UI only for  node1 which has pods created by system:admin i.e root but, not node2 where pods of joe reside.\n. ",
    "david-martin": "@miheer I'm seeing similar behaviour after following through that guide.\nI set it up in the default project, and exposed a route for the grafana service.\nIn the grafana UI it shows only containers that are on 1 of the 5 nodes in this OSE install.\nThe filters for showing all container cpu usage look ok to me e.g. pod_namespace =~ /.+/\n. ",
    "priyanka5": "Hi @rjnagal , thanks a lot for response , but I am trying this with kubernetes only, I followed this doc  : https://github.com/GoogleCloudPlatform/heapster/blob/master/docs/influxdb.md ,\nmy cadvisor is running fine on port 8080, i installed cadvisor as a docker container seperately. What I am understanding now is that I do not need to configure cadvisor externally, it gets created automatically. Let me know if I am wrong.\nI tried doing curl on port 4194, I assume cadvisor is not running properly :  \ncurl -L localhost:4194\nfailed to get container \"/\" with error: unknown container \"/ \n. hmm, thanks again! I am running kubernetes in CoreOS , below are the outputs I get, I think some issue with cgroups, any advice would be really helpful:\n1) cluster-info output\nKubernetes master is running at https://localhost:8443\nmonitoring-grafana is running at https://localhost:8443/api/v1beta3/proxy/namespaces/default/services/monitoring-grafana\nis running at https://localhost:8443/api/v1beta3/proxy/namespaces/default/services/monitoring-heapster\n2) get pods output\nNAME                                         READY     REASON    RESTARTS   AGE\nmonitoring-influx-grafana-controller-bafhg   2/2       Running   0          19h\n3) curl -L localhost:4194/validate\ncAdvisor version: 0.13.0\nOS version: CentOS Linux 7 (Core)\nKernel version: [Supported and recommended]\n        Kernel version is 4.0.4-303.fc22.x86_64. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported and recommended]\n        Available cgroups: map[memory:1 freezer:1 net_cls:1 perf_event:1 cpuset:1 cpu:1 cpuacct:1 blkio:1 net_prio:1 hugetlb:1 devices:1]\n        Following cgroups are required: [cpu cpuacct]\n        Following other cgroups are recommended: [memory blkio cpuset devices freezer]\n        Hierarchical memory accounting status unknown: memory cgroup not mounted.\nCgroup mount setup: [Unknown]\n        Could not locate cgroup mount point.\n        Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\nDocker version: [Supported and recommended]\n        Docker version is 1.6.0. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Unsupported]\n        Docker exec driver is native-0.2. Storage driver is devicemapper.\n        Cgroups are being created through cgroup filesystem.\n        Docker container state directory \"/var/lib/docker/containers\" is not accessible.\nBlock device setup: [Supported and recommended]\n        At least one device supports 'cfq' I/O scheduler. Some disk stats can be reported.\n         Disk \"dm-0\" Scheduler type \"none\".\n         Disk \"dm-1\" Scheduler type \"none\".\n         Disk \"dm-2\" Scheduler type \"none\".\n         Disk \"xvda\" Scheduler type \"cfq\".\n. Hi @vishh sorry for late reply, I am still struggling to get heapster working, In my kubernetes environment cAdvisor is running on port  4194. I can access this and heapster pod also runs fine, But heapster is still not showing any data in graph, do i need to run heapster to use port 4194 for cAdvisor? As I am running heapster with influx db , how would cAdvisor know to put data in influx db. Is that also the default behavior?? would be waiting for the response. Thank-you so much\n. in logs it gives \" skydns: incomplete CNAME chain: rcode is not equal to success\" would be really very helpful if u can give pointers on this.\n. ",
    "wonderfly": "@vishh I encountered the same issue today. I am new to Kubernetes and followed the getting started doc to start my first cluster on GCE. It started up correctly. However the pod(s) for heapster failed to get to the READY state in 50 minutes. I didn't have any customization to the configs.\nWhat could go wrong?\n$ kubectl cluster-info\nKubernetes master is running at https://x.x.x.x\nKubeDNS is running at https://x.x.x.x/api/v1/proxy/namespaces/kube-system/services/kube-dns\nKubeUI is running at https://x.x.x.x/api/v1/proxy/namespaces/kube-system/services/kube-ui\nGrafana is running at https://x.x.x.x/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana\nHeapster is running at https://x.x.x.x/api/v1/proxy/namespaces/kube-system/services/monitoring-heapster\nInfluxDB is running at https://x.x.x.x/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb\n$ kubectl get --all-namespaces pods\nNAMESPACE     NAME                                           READY     STATUS             RESTARTS   AGE\nkube-system   etcd-server-kubernetes-master                  1/1       Running            2          51m\nkube-system   fluentd-cloud-logging-kubernetes-master        1/1       Running            1          51m\nkube-system   fluentd-cloud-logging-kubernetes-minion-oleh   1/1       Running            0          50m\nkube-system   fluentd-cloud-logging-kubernetes-minion-pqgz   1/1       Running            0          50m\nkube-system   fluentd-cloud-logging-kubernetes-minion-rqv7   1/1       Running            0          50m\nkube-system   fluentd-cloud-logging-kubernetes-minion-tqvc   1/1       Running            0          50m\nkube-system   kube-apiserver-kubernetes-master               1/1       Running            1          51m\nkube-system   kube-controller-manager-kubernetes-master      1/1       Running            1          51m\nkube-system   kube-dns-v8-knx96                              4/4       Running            0          51m\nkube-system   kube-scheduler-kubernetes-master               1/1       Running            1          51m\nkube-system   kube-ui-v1-0y9p9                               0/1       CapacityExceeded   0          51m\nkube-system   kube-ui-v1-8t7up                               0/1       CapacityExceeded   0          51m\nkube-system   kube-ui-v1-c4clc                               0/1       CapacityExceeded   0          51m\nkube-system   kube-ui-v1-eeqe3                               0/1       CapacityExceeded   0          51m\nkube-system   kube-ui-v1-gmqvu                               1/1       Running            0          51m\nkube-system   kube-ui-v1-gocxx                               0/1       CapacityExceeded   0          51m\nkube-system   kube-ui-v1-n885p                               0/1       CapacityExceeded   0          51m\nkube-system   monitoring-heapster-v6-hld43                   0/1       Running            8          51m\nkube-system   monitoring-heapster-v6-kbc32                   0/1       CapacityExceeded   0          51m\nkube-system   monitoring-influx-grafana-v1-4ubsn             2/2       Running            0          51m\n. $ kubectl describe rc --namespace=kube-system monitoring-heapster\nName:           monitoring-heapster-v6\nNamespace:      kube-system\nImage(s):       gcr.io/google_containers/heapster:v0.16.1\nSelector:       k8s-app=heapster,version=v6\nLabels:         k8s-app=heapster,kubernetes.io/cluster-service=true,version=v6\nReplicas:       1 current / 1 desired\nPods Status:    1 Running / 0 Waiting / 0 Succeeded / 1 Failed\n$ kubectl describe pod --namespace=kube-system monitoring-heapster-v6-hld43\nName:               monitoring-heapster-v6-hld43\nNamespace:          kube-system\nImage(s):           gcr.io/google_containers/heapster:v0.16.1\nNode:               kubernetes-master/10.240.180.241\nLabels:             k8s-app=heapster,kubernetes.io/cluster-service=true,version=v6\nStatus:             Running\nReason:             \nMessage:            \nIP:             10.244.0.3\nReplication Controllers:    monitoring-heapster-v6 (1/1 replicas created)\nContainers:\n  heapster:\n    Image:  gcr.io/google_containers/heapster:v0.16.1\n    Limits:\n      cpu:      100m\n      memory:       300Mi\n    State:      Running\n      Started:      Fri, 21 Aug 2015 11:32:15 -0700\n    Ready:      False\n    Restart Count:  10\nConditions:\n  Type      Status\n  Ready     False \nEvents:\n  FirstSeen             LastSeen            Count   From                SubobjectPath           Reason  Message\n  Fri, 21 Aug 2015 10:32:23 -0700   Fri, 21 Aug 2015 10:32:23 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d43570a31644\n  Fri, 21 Aug 2015 10:32:23 -0700   Fri, 21 Aug 2015 10:32:23 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d43570a31644\n  Fri, 21 Aug 2015 10:32:33 -0700   Fri, 21 Aug 2015 10:32:33 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f98a9e6e4b8e\n  Fri, 21 Aug 2015 10:32:33 -0700   Fri, 21 Aug 2015 10:32:33 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f98a9e6e4b8e\n  Fri, 21 Aug 2015 10:32:43 -0700   Fri, 21 Aug 2015 10:32:43 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id faa648b4c4fc\n  Fri, 21 Aug 2015 10:32:43 -0700   Fri, 21 Aug 2015 10:32:43 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id faa648b4c4fc\n  Fri, 21 Aug 2015 10:32:53 -0700   Fri, 21 Aug 2015 10:32:53 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 7437407ec350\n  Fri, 21 Aug 2015 10:32:53 -0700   Fri, 21 Aug 2015 10:32:53 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 7437407ec350\n  Fri, 21 Aug 2015 10:33:03 -0700   Fri, 21 Aug 2015 10:33:03 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 10981fb5aa65\n  Fri, 21 Aug 2015 10:33:03 -0700   Fri, 21 Aug 2015 10:33:03 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 10981fb5aa65\n  Fri, 21 Aug 2015 10:33:13 -0700   Fri, 21 Aug 2015 10:33:13 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f28e21cede46\n  Fri, 21 Aug 2015 10:33:13 -0700   Fri, 21 Aug 2015 10:33:13 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f28e21cede46\n  Fri, 21 Aug 2015 10:33:23 -0700   Fri, 21 Aug 2015 10:33:23 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d09853011c94\n  Fri, 21 Aug 2015 10:33:23 -0700   Fri, 21 Aug 2015 10:33:23 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d09853011c94\n  Fri, 21 Aug 2015 10:33:33 -0700   Fri, 21 Aug 2015 10:33:33 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 28255251da98\n  Fri, 21 Aug 2015 10:33:33 -0700   Fri, 21 Aug 2015 10:33:33 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 28255251da98\n  Fri, 21 Aug 2015 10:33:43 -0700   Fri, 21 Aug 2015 10:33:43 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 1b98fab37d06\n  Fri, 21 Aug 2015 10:33:44 -0700   Fri, 21 Aug 2015 10:33:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 1b98fab37d06\n  Fri, 21 Aug 2015 10:33:53 -0700   Fri, 21 Aug 2015 10:33:53 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 2b9453b05eda\n  Fri, 21 Aug 2015 10:33:53 -0700   Fri, 21 Aug 2015 10:33:53 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 2b9453b05eda\n  Fri, 21 Aug 2015 10:34:05 -0700   Fri, 21 Aug 2015 10:34:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 1fb62a4bf25f\n  Fri, 21 Aug 2015 10:34:05 -0700   Fri, 21 Aug 2015 10:34:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 1fb62a4bf25f\n  Fri, 21 Aug 2015 10:34:14 -0700   Fri, 21 Aug 2015 10:34:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id b56204c7a30b\n  Fri, 21 Aug 2015 10:34:14 -0700   Fri, 21 Aug 2015 10:34:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id b56204c7a30b\n  Fri, 21 Aug 2015 10:34:25 -0700   Fri, 21 Aug 2015 10:34:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 3489fcf5b9c4\n  Fri, 21 Aug 2015 10:34:25 -0700   Fri, 21 Aug 2015 10:34:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 3489fcf5b9c4\n  Fri, 21 Aug 2015 10:34:34 -0700   Fri, 21 Aug 2015 10:34:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id e9eca96734c2\n  Fri, 21 Aug 2015 10:34:34 -0700   Fri, 21 Aug 2015 10:34:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id e9eca96734c2\n  Fri, 21 Aug 2015 10:34:44 -0700   Fri, 21 Aug 2015 10:34:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id c889cdb540da\n  Fri, 21 Aug 2015 10:34:45 -0700   Fri, 21 Aug 2015 10:34:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id c889cdb540da\n  Fri, 21 Aug 2015 10:34:54 -0700   Fri, 21 Aug 2015 10:34:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d6dd105477a5\n  Fri, 21 Aug 2015 10:34:55 -0700   Fri, 21 Aug 2015 10:34:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d6dd105477a5\n  Fri, 21 Aug 2015 10:35:04 -0700   Fri, 21 Aug 2015 10:35:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 2d4fe57100c1\n  Fri, 21 Aug 2015 10:35:05 -0700   Fri, 21 Aug 2015 10:35:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 2d4fe57100c1\n  Fri, 21 Aug 2015 10:35:13 -0700   Fri, 21 Aug 2015 10:35:13 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8db9e4ee5244\n  Fri, 21 Aug 2015 10:35:14 -0700   Fri, 21 Aug 2015 10:35:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8db9e4ee5244\n  Fri, 21 Aug 2015 10:35:23 -0700   Fri, 21 Aug 2015 10:35:23 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id fecd1eca7626\n  Fri, 21 Aug 2015 10:35:24 -0700   Fri, 21 Aug 2015 10:35:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id fecd1eca7626\n  Fri, 21 Aug 2015 10:35:33 -0700   Fri, 21 Aug 2015 10:35:33 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 3318d007445e\n  Fri, 21 Aug 2015 10:35:34 -0700   Fri, 21 Aug 2015 10:35:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 3318d007445e\n  Fri, 21 Aug 2015 10:35:44 -0700   Fri, 21 Aug 2015 10:35:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 4467277d6336\n  Fri, 21 Aug 2015 10:35:44 -0700   Fri, 21 Aug 2015 10:35:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 4467277d6336\n  Fri, 21 Aug 2015 10:35:54 -0700   Fri, 21 Aug 2015 10:35:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id cfcb992e6b38\n  Fri, 21 Aug 2015 10:35:54 -0700   Fri, 21 Aug 2015 10:35:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id cfcb992e6b38\n  Fri, 21 Aug 2015 10:36:04 -0700   Fri, 21 Aug 2015 10:36:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 502fe6ebcde3\n  Fri, 21 Aug 2015 10:36:04 -0700   Fri, 21 Aug 2015 10:36:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 502fe6ebcde3\n  Fri, 21 Aug 2015 10:36:15 -0700   Fri, 21 Aug 2015 10:36:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d41ffd0cfb60\n  Fri, 21 Aug 2015 10:36:15 -0700   Fri, 21 Aug 2015 10:36:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d41ffd0cfb60\n  Fri, 21 Aug 2015 10:36:25 -0700   Fri, 21 Aug 2015 10:36:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id efd1659ccd8f\n  Fri, 21 Aug 2015 10:36:25 -0700   Fri, 21 Aug 2015 10:36:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id efd1659ccd8f\n  Fri, 21 Aug 2015 10:36:34 -0700   Fri, 21 Aug 2015 10:36:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id c04ea8c8daf3\n  Fri, 21 Aug 2015 10:36:35 -0700   Fri, 21 Aug 2015 10:36:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id c04ea8c8daf3\n  Fri, 21 Aug 2015 10:36:45 -0700   Fri, 21 Aug 2015 10:36:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 264870475f28\n  Fri, 21 Aug 2015 10:36:45 -0700   Fri, 21 Aug 2015 10:36:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 264870475f28\n  Fri, 21 Aug 2015 10:36:54 -0700   Fri, 21 Aug 2015 10:36:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 471f892d352b\n  Fri, 21 Aug 2015 10:36:54 -0700   Fri, 21 Aug 2015 10:36:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 471f892d352b\n  Fri, 21 Aug 2015 10:37:04 -0700   Fri, 21 Aug 2015 10:37:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id a5918669f01a\n  Fri, 21 Aug 2015 10:37:04 -0700   Fri, 21 Aug 2015 10:37:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id a5918669f01a\n  Fri, 21 Aug 2015 10:37:14 -0700   Fri, 21 Aug 2015 10:37:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id bb8d476ebc10\n  Fri, 21 Aug 2015 10:37:14 -0700   Fri, 21 Aug 2015 10:37:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id bb8d476ebc10\n  Fri, 21 Aug 2015 10:37:25 -0700   Fri, 21 Aug 2015 10:37:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 4ff93acf5629\n  Fri, 21 Aug 2015 10:37:25 -0700   Fri, 21 Aug 2015 10:37:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 4ff93acf5629\n  Fri, 21 Aug 2015 10:37:35 -0700   Fri, 21 Aug 2015 10:37:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 612f57a73b4f\n  Fri, 21 Aug 2015 10:37:35 -0700   Fri, 21 Aug 2015 10:37:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 612f57a73b4f\n  Fri, 21 Aug 2015 10:37:45 -0700   Fri, 21 Aug 2015 10:37:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f9e230049b00\n  Fri, 21 Aug 2015 10:37:45 -0700   Fri, 21 Aug 2015 10:37:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f9e230049b00\n  Fri, 21 Aug 2015 10:37:55 -0700   Fri, 21 Aug 2015 10:37:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id de67224f372c\n  Fri, 21 Aug 2015 10:37:55 -0700   Fri, 21 Aug 2015 10:37:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id de67224f372c\n  Fri, 21 Aug 2015 10:38:04 -0700   Fri, 21 Aug 2015 10:38:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 400be2cf06c5\n  Fri, 21 Aug 2015 10:38:04 -0700   Fri, 21 Aug 2015 10:38:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 400be2cf06c5\n  Fri, 21 Aug 2015 10:38:14 -0700   Fri, 21 Aug 2015 10:38:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 6b1b62ca6811\n  Fri, 21 Aug 2015 10:38:14 -0700   Fri, 21 Aug 2015 10:38:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 6b1b62ca6811\n  Fri, 21 Aug 2015 10:38:24 -0700   Fri, 21 Aug 2015 10:38:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 3cb17dee0830\n  Fri, 21 Aug 2015 10:38:24 -0700   Fri, 21 Aug 2015 10:38:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 3cb17dee0830\n  Fri, 21 Aug 2015 10:38:34 -0700   Fri, 21 Aug 2015 10:38:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id a9f0d9c12523\n  Fri, 21 Aug 2015 10:38:34 -0700   Fri, 21 Aug 2015 10:38:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id a9f0d9c12523\n  Fri, 21 Aug 2015 10:38:44 -0700   Fri, 21 Aug 2015 10:38:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 1e689bf1d3d2\n  Fri, 21 Aug 2015 10:38:44 -0700   Fri, 21 Aug 2015 10:38:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 1e689bf1d3d2\n  Fri, 21 Aug 2015 10:38:54 -0700   Fri, 21 Aug 2015 10:38:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id aa990588d65c\n  Fri, 21 Aug 2015 10:38:54 -0700   Fri, 21 Aug 2015 10:38:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id aa990588d65c\n  Fri, 21 Aug 2015 10:39:04 -0700   Fri, 21 Aug 2015 10:39:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 4970f4684d9e\n  Fri, 21 Aug 2015 10:39:04 -0700   Fri, 21 Aug 2015 10:39:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 4970f4684d9e\n  Fri, 21 Aug 2015 10:39:15 -0700   Fri, 21 Aug 2015 10:39:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id b97ba369de93\n  Fri, 21 Aug 2015 10:39:15 -0700   Fri, 21 Aug 2015 10:39:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id b97ba369de93\n  Fri, 21 Aug 2015 10:39:24 -0700   Fri, 21 Aug 2015 10:39:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 3da88f66ed45\n  Fri, 21 Aug 2015 10:39:24 -0700   Fri, 21 Aug 2015 10:39:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 3da88f66ed45\n  Fri, 21 Aug 2015 10:39:35 -0700   Fri, 21 Aug 2015 10:39:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d16b6cb856f0\n  Fri, 21 Aug 2015 10:39:35 -0700   Fri, 21 Aug 2015 10:39:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d16b6cb856f0\n  Fri, 21 Aug 2015 10:39:45 -0700   Fri, 21 Aug 2015 10:39:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 62ad66b39ec4\n  Fri, 21 Aug 2015 10:39:45 -0700   Fri, 21 Aug 2015 10:39:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 62ad66b39ec4\n  Fri, 21 Aug 2015 10:39:55 -0700   Fri, 21 Aug 2015 10:39:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d2ed71091c51\n  Fri, 21 Aug 2015 10:39:55 -0700   Fri, 21 Aug 2015 10:39:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d2ed71091c51\n  Fri, 21 Aug 2015 10:40:05 -0700   Fri, 21 Aug 2015 10:40:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 95ec9be23fe9\n  Fri, 21 Aug 2015 10:40:05 -0700   Fri, 21 Aug 2015 10:40:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 95ec9be23fe9\n  Fri, 21 Aug 2015 10:40:14 -0700   Fri, 21 Aug 2015 10:40:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d56899de5dd3\n  Fri, 21 Aug 2015 10:40:14 -0700   Fri, 21 Aug 2015 10:40:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d56899de5dd3\n  Fri, 21 Aug 2015 10:40:24 -0700   Fri, 21 Aug 2015 10:40:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 90f0e6348dd1\n  Fri, 21 Aug 2015 10:40:24 -0700   Fri, 21 Aug 2015 10:40:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 90f0e6348dd1\n  Fri, 21 Aug 2015 10:40:34 -0700   Fri, 21 Aug 2015 10:40:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id ba364e3c9df1\n  Fri, 21 Aug 2015 10:40:34 -0700   Fri, 21 Aug 2015 10:40:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id ba364e3c9df1\n  Fri, 21 Aug 2015 10:40:45 -0700   Fri, 21 Aug 2015 10:40:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 3e3164d2d7ac\n  Fri, 21 Aug 2015 10:40:45 -0700   Fri, 21 Aug 2015 10:40:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 3e3164d2d7ac\n  Fri, 21 Aug 2015 10:40:55 -0700   Fri, 21 Aug 2015 10:40:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 7842037f1628\n  Fri, 21 Aug 2015 10:40:55 -0700   Fri, 21 Aug 2015 10:40:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 7842037f1628\n  Fri, 21 Aug 2015 10:41:05 -0700   Fri, 21 Aug 2015 10:41:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8d3c742fe42c\n  Fri, 21 Aug 2015 10:41:05 -0700   Fri, 21 Aug 2015 10:41:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8d3c742fe42c\n  Fri, 21 Aug 2015 10:41:15 -0700   Fri, 21 Aug 2015 10:41:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d3159212148a\n  Fri, 21 Aug 2015 10:41:15 -0700   Fri, 21 Aug 2015 10:41:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d3159212148a\n  Fri, 21 Aug 2015 10:41:25 -0700   Fri, 21 Aug 2015 10:41:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d856a2bcd494\n  Fri, 21 Aug 2015 10:41:25 -0700   Fri, 21 Aug 2015 10:41:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d856a2bcd494\n  Fri, 21 Aug 2015 10:41:34 -0700   Fri, 21 Aug 2015 10:41:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id c0bfee9256d2\n  Fri, 21 Aug 2015 10:41:34 -0700   Fri, 21 Aug 2015 10:41:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id c0bfee9256d2\n  Fri, 21 Aug 2015 10:41:44 -0700   Fri, 21 Aug 2015 10:41:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id ce6d87ea6cff\n  Fri, 21 Aug 2015 10:41:44 -0700   Fri, 21 Aug 2015 10:41:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id ce6d87ea6cff\n  Fri, 21 Aug 2015 10:41:54 -0700   Fri, 21 Aug 2015 10:41:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 70048779b053\n  Fri, 21 Aug 2015 10:41:54 -0700   Fri, 21 Aug 2015 10:41:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 70048779b053\n  Fri, 21 Aug 2015 10:42:04 -0700   Fri, 21 Aug 2015 10:42:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 3ca6dec72e82\n  Fri, 21 Aug 2015 10:42:04 -0700   Fri, 21 Aug 2015 10:42:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 3ca6dec72e82\n  Fri, 21 Aug 2015 10:42:15 -0700   Fri, 21 Aug 2015 10:42:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 6f6e318e5d9e\n  Fri, 21 Aug 2015 10:42:15 -0700   Fri, 21 Aug 2015 10:42:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 6f6e318e5d9e\n  Fri, 21 Aug 2015 10:42:25 -0700   Fri, 21 Aug 2015 10:42:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 19d8bd9416f5\n  Fri, 21 Aug 2015 10:42:25 -0700   Fri, 21 Aug 2015 10:42:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 19d8bd9416f5\n  Fri, 21 Aug 2015 10:42:35 -0700   Fri, 21 Aug 2015 10:42:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 3a20519bf029\n  Fri, 21 Aug 2015 10:42:35 -0700   Fri, 21 Aug 2015 10:42:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 3a20519bf029\n  Fri, 21 Aug 2015 10:42:45 -0700   Fri, 21 Aug 2015 10:42:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 4272ff3dc89a\n  Fri, 21 Aug 2015 10:42:45 -0700   Fri, 21 Aug 2015 10:42:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 4272ff3dc89a\n  Fri, 21 Aug 2015 10:42:54 -0700   Fri, 21 Aug 2015 10:42:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 74f15aafabbe\n  Fri, 21 Aug 2015 10:42:54 -0700   Fri, 21 Aug 2015 10:42:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 74f15aafabbe\n  Fri, 21 Aug 2015 10:43:04 -0700   Fri, 21 Aug 2015 10:43:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d1a68eae507d\n  Fri, 21 Aug 2015 10:43:04 -0700   Fri, 21 Aug 2015 10:43:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d1a68eae507d\n  Fri, 21 Aug 2015 10:43:14 -0700   Fri, 21 Aug 2015 10:43:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f3af325336bf\n  Fri, 21 Aug 2015 10:43:14 -0700   Fri, 21 Aug 2015 10:43:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f3af325336bf\n  Fri, 21 Aug 2015 10:43:24 -0700   Fri, 21 Aug 2015 10:43:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id cf614d9d4a0b\n  Fri, 21 Aug 2015 10:43:24 -0700   Fri, 21 Aug 2015 10:43:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id cf614d9d4a0b\n  Fri, 21 Aug 2015 10:43:35 -0700   Fri, 21 Aug 2015 10:43:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id ef8d14c6e217\n  Fri, 21 Aug 2015 10:43:35 -0700   Fri, 21 Aug 2015 10:43:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id ef8d14c6e217\n  Fri, 21 Aug 2015 10:43:45 -0700   Fri, 21 Aug 2015 10:43:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 9c58085ecdd8\n  Fri, 21 Aug 2015 10:43:45 -0700   Fri, 21 Aug 2015 10:43:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 9c58085ecdd8\n  Fri, 21 Aug 2015 10:43:55 -0700   Fri, 21 Aug 2015 10:43:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 225dc9555b59\n  Fri, 21 Aug 2015 10:43:55 -0700   Fri, 21 Aug 2015 10:43:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 225dc9555b59\n  Fri, 21 Aug 2015 10:44:05 -0700   Fri, 21 Aug 2015 10:44:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 1416528a641e\n  Fri, 21 Aug 2015 10:44:05 -0700   Fri, 21 Aug 2015 10:44:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 1416528a641e\n  Fri, 21 Aug 2015 10:44:15 -0700   Fri, 21 Aug 2015 10:44:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 896366357ed1\n  Fri, 21 Aug 2015 10:44:15 -0700   Fri, 21 Aug 2015 10:44:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 896366357ed1\n  Fri, 21 Aug 2015 10:44:24 -0700   Fri, 21 Aug 2015 10:44:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id a2d60c76cf7e\n  Fri, 21 Aug 2015 10:44:24 -0700   Fri, 21 Aug 2015 10:44:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id a2d60c76cf7e\n  Fri, 21 Aug 2015 10:44:35 -0700   Fri, 21 Aug 2015 10:44:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 72b1d855ce6b\n  Fri, 21 Aug 2015 10:44:35 -0700   Fri, 21 Aug 2015 10:44:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 72b1d855ce6b\n  Fri, 21 Aug 2015 10:44:44 -0700   Fri, 21 Aug 2015 10:44:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 12400117c64e\n  Fri, 21 Aug 2015 10:44:44 -0700   Fri, 21 Aug 2015 10:44:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 12400117c64e\n  Fri, 21 Aug 2015 10:44:54 -0700   Fri, 21 Aug 2015 10:44:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 99e3b01f8885\n  Fri, 21 Aug 2015 10:44:54 -0700   Fri, 21 Aug 2015 10:44:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 99e3b01f8885\n  Fri, 21 Aug 2015 10:45:04 -0700   Fri, 21 Aug 2015 10:45:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 36613d9f473f\n  Fri, 21 Aug 2015 10:45:04 -0700   Fri, 21 Aug 2015 10:45:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 36613d9f473f\n  Fri, 21 Aug 2015 10:45:14 -0700   Fri, 21 Aug 2015 10:45:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 06c64794e7b9\n  Fri, 21 Aug 2015 10:45:14 -0700   Fri, 21 Aug 2015 10:45:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 06c64794e7b9\n  Fri, 21 Aug 2015 10:45:24 -0700   Fri, 21 Aug 2015 10:45:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id ac64828254b9\n  Fri, 21 Aug 2015 10:45:24 -0700   Fri, 21 Aug 2015 10:45:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id ac64828254b9\n  Fri, 21 Aug 2015 10:45:34 -0700   Fri, 21 Aug 2015 10:45:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id cf7b437a3d58\n  Fri, 21 Aug 2015 10:45:34 -0700   Fri, 21 Aug 2015 10:45:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id cf7b437a3d58\n  Fri, 21 Aug 2015 10:45:44 -0700   Fri, 21 Aug 2015 10:45:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 38ce850f4088\n  Fri, 21 Aug 2015 10:45:44 -0700   Fri, 21 Aug 2015 10:45:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 38ce850f4088\n  Fri, 21 Aug 2015 10:45:54 -0700   Fri, 21 Aug 2015 10:45:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 1a29bfa73f4c\n  Fri, 21 Aug 2015 10:45:54 -0700   Fri, 21 Aug 2015 10:45:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 1a29bfa73f4c\n  Fri, 21 Aug 2015 10:46:04 -0700   Fri, 21 Aug 2015 10:46:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id bcb4d67b56ea\n  Fri, 21 Aug 2015 10:46:04 -0700   Fri, 21 Aug 2015 10:46:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id bcb4d67b56ea\n  Fri, 21 Aug 2015 10:46:14 -0700   Fri, 21 Aug 2015 10:46:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id bb8ca6e90bb6\n  Fri, 21 Aug 2015 10:46:14 -0700   Fri, 21 Aug 2015 10:46:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id bb8ca6e90bb6\n  Fri, 21 Aug 2015 10:46:24 -0700   Fri, 21 Aug 2015 10:46:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 0cf1f61fca9b\n  Fri, 21 Aug 2015 10:46:24 -0700   Fri, 21 Aug 2015 10:46:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 0cf1f61fca9b\n  Fri, 21 Aug 2015 10:46:34 -0700   Fri, 21 Aug 2015 10:46:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 74b09a7977e2\n  Fri, 21 Aug 2015 10:46:34 -0700   Fri, 21 Aug 2015 10:46:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 74b09a7977e2\n  Fri, 21 Aug 2015 10:46:44 -0700   Fri, 21 Aug 2015 10:46:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 540c6a647255\n  Fri, 21 Aug 2015 10:46:44 -0700   Fri, 21 Aug 2015 10:46:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 540c6a647255\n  Fri, 21 Aug 2015 10:46:54 -0700   Fri, 21 Aug 2015 10:46:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 87e75609d3ea\n  Fri, 21 Aug 2015 10:46:54 -0700   Fri, 21 Aug 2015 10:46:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 87e75609d3ea\n  Fri, 21 Aug 2015 10:47:04 -0700   Fri, 21 Aug 2015 10:47:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 06bacf140e66\n  Fri, 21 Aug 2015 10:47:04 -0700   Fri, 21 Aug 2015 10:47:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 06bacf140e66\n  Fri, 21 Aug 2015 10:47:14 -0700   Fri, 21 Aug 2015 10:47:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 803d3a4a4320\n  Fri, 21 Aug 2015 10:47:14 -0700   Fri, 21 Aug 2015 10:47:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 803d3a4a4320\n  Fri, 21 Aug 2015 10:47:24 -0700   Fri, 21 Aug 2015 10:47:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id bdf03cb8bfc6\n  Fri, 21 Aug 2015 10:47:24 -0700   Fri, 21 Aug 2015 10:47:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id bdf03cb8bfc6\n  Fri, 21 Aug 2015 10:47:34 -0700   Fri, 21 Aug 2015 10:47:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id c1de3429a62b\n  Fri, 21 Aug 2015 10:47:34 -0700   Fri, 21 Aug 2015 10:47:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id c1de3429a62b\n  Fri, 21 Aug 2015 10:47:44 -0700   Fri, 21 Aug 2015 10:47:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 00f2e41cedd1\n  Fri, 21 Aug 2015 10:47:44 -0700   Fri, 21 Aug 2015 10:47:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 00f2e41cedd1\n  Fri, 21 Aug 2015 10:47:54 -0700   Fri, 21 Aug 2015 10:47:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 4e31d18a2c60\n  Fri, 21 Aug 2015 10:47:54 -0700   Fri, 21 Aug 2015 10:47:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 4e31d18a2c60\n  Fri, 21 Aug 2015 10:48:04 -0700   Fri, 21 Aug 2015 10:48:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 29be60bb65fb\n  Fri, 21 Aug 2015 10:48:04 -0700   Fri, 21 Aug 2015 10:48:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 29be60bb65fb\n  Fri, 21 Aug 2015 10:48:14 -0700   Fri, 21 Aug 2015 10:48:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 6c01de46e754\n  Fri, 21 Aug 2015 10:48:14 -0700   Fri, 21 Aug 2015 10:48:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 6c01de46e754\n  Fri, 21 Aug 2015 10:48:25 -0700   Fri, 21 Aug 2015 10:48:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id b7684d1cd29a\n  Fri, 21 Aug 2015 10:48:25 -0700   Fri, 21 Aug 2015 10:48:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id b7684d1cd29a\n  Fri, 21 Aug 2015 10:48:35 -0700   Fri, 21 Aug 2015 10:48:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 6f59fc0cce84\n  Fri, 21 Aug 2015 10:48:35 -0700   Fri, 21 Aug 2015 10:48:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 6f59fc0cce84\n  Fri, 21 Aug 2015 10:48:45 -0700   Fri, 21 Aug 2015 10:48:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 0d50e7873f7b\n  Fri, 21 Aug 2015 10:48:45 -0700   Fri, 21 Aug 2015 10:48:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 0d50e7873f7b\n  Fri, 21 Aug 2015 10:48:55 -0700   Fri, 21 Aug 2015 10:48:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 192cd1fcad08\n  Fri, 21 Aug 2015 10:48:55 -0700   Fri, 21 Aug 2015 10:48:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 192cd1fcad08\n  Fri, 21 Aug 2015 10:49:04 -0700   Fri, 21 Aug 2015 10:49:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 49f1fa3a597b\n  Fri, 21 Aug 2015 10:49:04 -0700   Fri, 21 Aug 2015 10:49:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 49f1fa3a597b\n  Fri, 21 Aug 2015 10:49:14 -0700   Fri, 21 Aug 2015 10:49:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id ad6d3734de13\n  Fri, 21 Aug 2015 10:49:14 -0700   Fri, 21 Aug 2015 10:49:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id ad6d3734de13\n  Fri, 21 Aug 2015 10:49:24 -0700   Fri, 21 Aug 2015 10:49:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 4ee43cd384a9\n  Fri, 21 Aug 2015 10:49:24 -0700   Fri, 21 Aug 2015 10:49:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 4ee43cd384a9\n  Fri, 21 Aug 2015 10:49:34 -0700   Fri, 21 Aug 2015 10:49:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f2658a4a90da\n  Fri, 21 Aug 2015 10:49:34 -0700   Fri, 21 Aug 2015 10:49:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f2658a4a90da\n  Fri, 21 Aug 2015 10:49:44 -0700   Fri, 21 Aug 2015 10:49:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f490c4e2e8c5\n  Fri, 21 Aug 2015 10:49:44 -0700   Fri, 21 Aug 2015 10:49:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f490c4e2e8c5\n  Fri, 21 Aug 2015 10:49:54 -0700   Fri, 21 Aug 2015 10:49:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 247014cb4ee5\n  Fri, 21 Aug 2015 10:49:54 -0700   Fri, 21 Aug 2015 10:49:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 247014cb4ee5\n  Fri, 21 Aug 2015 10:50:04 -0700   Fri, 21 Aug 2015 10:50:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id deb356f5d175\n  Fri, 21 Aug 2015 10:50:04 -0700   Fri, 21 Aug 2015 10:50:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id deb356f5d175\n  Fri, 21 Aug 2015 10:50:15 -0700   Fri, 21 Aug 2015 10:50:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 61467326a7ff\n  Fri, 21 Aug 2015 10:50:15 -0700   Fri, 21 Aug 2015 10:50:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 61467326a7ff\n  Fri, 21 Aug 2015 10:50:25 -0700   Fri, 21 Aug 2015 10:50:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 12b07ac9043b\n  Fri, 21 Aug 2015 10:50:25 -0700   Fri, 21 Aug 2015 10:50:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 12b07ac9043b\n  Fri, 21 Aug 2015 10:50:35 -0700   Fri, 21 Aug 2015 10:50:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 5287c05e01a2\n  Fri, 21 Aug 2015 10:50:35 -0700   Fri, 21 Aug 2015 10:50:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 5287c05e01a2\n  Fri, 21 Aug 2015 10:50:45 -0700   Fri, 21 Aug 2015 10:50:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id b52e96757d8f\n  Fri, 21 Aug 2015 10:50:45 -0700   Fri, 21 Aug 2015 10:50:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id b52e96757d8f\n  Fri, 21 Aug 2015 10:50:54 -0700   Fri, 21 Aug 2015 10:50:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 767777990f20\n  Fri, 21 Aug 2015 10:50:54 -0700   Fri, 21 Aug 2015 10:50:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 767777990f20\n  Fri, 21 Aug 2015 10:51:04 -0700   Fri, 21 Aug 2015 10:51:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 9830becc6e48\n  Fri, 21 Aug 2015 10:51:04 -0700   Fri, 21 Aug 2015 10:51:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 9830becc6e48\n  Fri, 21 Aug 2015 10:51:14 -0700   Fri, 21 Aug 2015 10:51:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 4088bf6e469e\n  Fri, 21 Aug 2015 10:51:14 -0700   Fri, 21 Aug 2015 10:51:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 4088bf6e469e\n  Fri, 21 Aug 2015 10:51:25 -0700   Fri, 21 Aug 2015 10:51:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 9ec5c69bc0aa\n  Fri, 21 Aug 2015 10:51:25 -0700   Fri, 21 Aug 2015 10:51:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 9ec5c69bc0aa\n  Fri, 21 Aug 2015 10:51:35 -0700   Fri, 21 Aug 2015 10:51:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 041662d325e1\n  Fri, 21 Aug 2015 10:51:35 -0700   Fri, 21 Aug 2015 10:51:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 041662d325e1\n  Fri, 21 Aug 2015 10:51:45 -0700   Fri, 21 Aug 2015 10:51:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8de68405fefa\n  Fri, 21 Aug 2015 10:51:45 -0700   Fri, 21 Aug 2015 10:51:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8de68405fefa\n  Fri, 21 Aug 2015 10:51:55 -0700   Fri, 21 Aug 2015 10:51:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 5347ac4e7180\n  Fri, 21 Aug 2015 10:51:55 -0700   Fri, 21 Aug 2015 10:51:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 5347ac4e7180\n  Fri, 21 Aug 2015 10:52:05 -0700   Fri, 21 Aug 2015 10:52:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 32dc4dfdd2d7\n  Fri, 21 Aug 2015 10:52:05 -0700   Fri, 21 Aug 2015 10:52:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 32dc4dfdd2d7\n  Fri, 21 Aug 2015 10:52:15 -0700   Fri, 21 Aug 2015 10:52:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 32b700c61b23\n  Fri, 21 Aug 2015 10:52:15 -0700   Fri, 21 Aug 2015 10:52:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 32b700c61b23\n  Fri, 21 Aug 2015 10:52:24 -0700   Fri, 21 Aug 2015 10:52:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8e2d4f1ac395\n  Fri, 21 Aug 2015 10:52:24 -0700   Fri, 21 Aug 2015 10:52:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8e2d4f1ac395\n  Fri, 21 Aug 2015 10:52:34 -0700   Fri, 21 Aug 2015 10:52:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id b4552ebdf261\n  Fri, 21 Aug 2015 10:52:34 -0700   Fri, 21 Aug 2015 10:52:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id b4552ebdf261\n  Fri, 21 Aug 2015 10:52:44 -0700   Fri, 21 Aug 2015 10:52:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 3b8f8d40d0da\n  Fri, 21 Aug 2015 10:52:44 -0700   Fri, 21 Aug 2015 10:52:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 3b8f8d40d0da\n  Fri, 21 Aug 2015 10:52:54 -0700   Fri, 21 Aug 2015 10:52:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 5db48e7eba4e\n  Fri, 21 Aug 2015 10:52:54 -0700   Fri, 21 Aug 2015 10:52:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 5db48e7eba4e\n  Fri, 21 Aug 2015 10:53:04 -0700   Fri, 21 Aug 2015 10:53:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id e5054da8b04d\n  Fri, 21 Aug 2015 10:53:04 -0700   Fri, 21 Aug 2015 10:53:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id e5054da8b04d\n  Fri, 21 Aug 2015 10:53:15 -0700   Fri, 21 Aug 2015 10:53:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8df15d9153e0\n  Fri, 21 Aug 2015 10:53:15 -0700   Fri, 21 Aug 2015 10:53:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8df15d9153e0\n  Fri, 21 Aug 2015 10:53:25 -0700   Fri, 21 Aug 2015 10:53:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f044a947f3a2\n  Fri, 21 Aug 2015 10:53:25 -0700   Fri, 21 Aug 2015 10:53:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f044a947f3a2\n  Fri, 21 Aug 2015 10:53:35 -0700   Fri, 21 Aug 2015 10:53:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id aa8157c6b877\n  Fri, 21 Aug 2015 10:53:35 -0700   Fri, 21 Aug 2015 10:53:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id aa8157c6b877\n  Fri, 21 Aug 2015 10:53:44 -0700   Fri, 21 Aug 2015 10:53:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8c5b58aed3dc\n  Fri, 21 Aug 2015 10:53:44 -0700   Fri, 21 Aug 2015 10:53:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8c5b58aed3dc\n  Fri, 21 Aug 2015 10:53:54 -0700   Fri, 21 Aug 2015 10:53:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 570c06f8a967\n  Fri, 21 Aug 2015 10:53:54 -0700   Fri, 21 Aug 2015 10:53:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 570c06f8a967\n  Fri, 21 Aug 2015 10:54:04 -0700   Fri, 21 Aug 2015 10:54:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 634c500e93f1\n  Fri, 21 Aug 2015 10:54:04 -0700   Fri, 21 Aug 2015 10:54:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 634c500e93f1\n  Fri, 21 Aug 2015 10:54:15 -0700   Fri, 21 Aug 2015 10:54:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 208be5624aaf\n  Fri, 21 Aug 2015 10:54:15 -0700   Fri, 21 Aug 2015 10:54:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 208be5624aaf\n  Fri, 21 Aug 2015 10:54:25 -0700   Fri, 21 Aug 2015 10:54:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 4eb91b12c317\n  Fri, 21 Aug 2015 10:54:25 -0700   Fri, 21 Aug 2015 10:54:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 4eb91b12c317\n  Fri, 21 Aug 2015 10:54:35 -0700   Fri, 21 Aug 2015 10:54:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 07c1f8f078a2\n  Fri, 21 Aug 2015 10:54:36 -0700   Fri, 21 Aug 2015 10:54:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 07c1f8f078a2\n  Fri, 21 Aug 2015 10:54:45 -0700   Fri, 21 Aug 2015 10:54:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 2c72e5ce2f13\n  Fri, 21 Aug 2015 10:54:45 -0700   Fri, 21 Aug 2015 10:54:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 2c72e5ce2f13\n  Fri, 21 Aug 2015 10:54:55 -0700   Fri, 21 Aug 2015 10:54:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 07f8fc526e1b\n  Fri, 21 Aug 2015 10:54:55 -0700   Fri, 21 Aug 2015 10:54:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 07f8fc526e1b\n  Fri, 21 Aug 2015 10:55:04 -0700   Fri, 21 Aug 2015 10:55:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id a53ec97964b6\n  Fri, 21 Aug 2015 10:55:05 -0700   Fri, 21 Aug 2015 10:55:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id a53ec97964b6\n  Fri, 21 Aug 2015 10:55:14 -0700   Fri, 21 Aug 2015 10:55:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 994c80f11cf5\n  Fri, 21 Aug 2015 10:55:14 -0700   Fri, 21 Aug 2015 10:55:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 994c80f11cf5\n  Fri, 21 Aug 2015 10:55:24 -0700   Fri, 21 Aug 2015 10:55:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 3334fd1d931b\n  Fri, 21 Aug 2015 10:55:24 -0700   Fri, 21 Aug 2015 10:55:24 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 3334fd1d931b\n  Fri, 21 Aug 2015 10:55:34 -0700   Fri, 21 Aug 2015 10:55:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f3f253434399\n  Fri, 21 Aug 2015 10:55:34 -0700   Fri, 21 Aug 2015 10:55:34 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f3f253434399\n  Fri, 21 Aug 2015 10:55:44 -0700   Fri, 21 Aug 2015 10:55:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 36cdc80dd316\n  Fri, 21 Aug 2015 10:55:44 -0700   Fri, 21 Aug 2015 10:55:44 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 36cdc80dd316\n  Fri, 21 Aug 2015 10:55:54 -0700   Fri, 21 Aug 2015 10:55:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 10909551c7e5\n  Fri, 21 Aug 2015 10:55:54 -0700   Fri, 21 Aug 2015 10:55:54 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 10909551c7e5\n  Fri, 21 Aug 2015 10:56:04 -0700   Fri, 21 Aug 2015 10:56:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id dda135fadcb5\n  Fri, 21 Aug 2015 10:56:04 -0700   Fri, 21 Aug 2015 10:56:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id dda135fadcb5\n  Fri, 21 Aug 2015 10:56:14 -0700   Fri, 21 Aug 2015 10:56:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id a8d150f41ded\n  Fri, 21 Aug 2015 10:56:14 -0700   Fri, 21 Aug 2015 10:56:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id a8d150f41ded\n  Fri, 21 Aug 2015 10:56:25 -0700   Fri, 21 Aug 2015 10:56:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 22181c70c3cf\n  Fri, 21 Aug 2015 10:56:25 -0700   Fri, 21 Aug 2015 10:56:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 22181c70c3cf\n  Fri, 21 Aug 2015 10:56:35 -0700   Fri, 21 Aug 2015 10:56:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d67e2c538b4a\n  Fri, 21 Aug 2015 10:56:35 -0700   Fri, 21 Aug 2015 10:56:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d67e2c538b4a\n  Fri, 21 Aug 2015 10:56:45 -0700   Fri, 21 Aug 2015 10:56:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 332ddb5508a1\n  Fri, 21 Aug 2015 10:56:45 -0700   Fri, 21 Aug 2015 10:56:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 332ddb5508a1\n  Fri, 21 Aug 2015 10:56:55 -0700   Fri, 21 Aug 2015 10:56:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 7a74f7f6ab80\n  Fri, 21 Aug 2015 10:56:55 -0700   Fri, 21 Aug 2015 10:56:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 7a74f7f6ab80\n  Fri, 21 Aug 2015 10:57:04 -0700   Fri, 21 Aug 2015 10:57:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id feae71f49e63\n  Fri, 21 Aug 2015 10:57:04 -0700   Fri, 21 Aug 2015 10:57:04 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id feae71f49e63\n  Fri, 21 Aug 2015 10:57:14 -0700   Fri, 21 Aug 2015 10:57:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 7a0e7dbbc9a0\n  Fri, 21 Aug 2015 10:57:14 -0700   Fri, 21 Aug 2015 10:57:14 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 7a0e7dbbc9a0\n  Fri, 21 Aug 2015 10:57:25 -0700   Fri, 21 Aug 2015 10:57:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id de4026e1d704\n  Fri, 21 Aug 2015 10:57:25 -0700   Fri, 21 Aug 2015 10:57:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id de4026e1d704\n  Fri, 21 Aug 2015 10:57:35 -0700   Fri, 21 Aug 2015 10:57:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 71666682d17b\n  Fri, 21 Aug 2015 10:57:35 -0700   Fri, 21 Aug 2015 10:57:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 71666682d17b\n  Fri, 21 Aug 2015 10:57:45 -0700   Fri, 21 Aug 2015 10:57:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 104fa684b434\n  Fri, 21 Aug 2015 10:57:45 -0700   Fri, 21 Aug 2015 10:57:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 104fa684b434\n  Fri, 21 Aug 2015 10:57:55 -0700   Fri, 21 Aug 2015 10:57:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d08c2dc3be60\n  Fri, 21 Aug 2015 10:57:55 -0700   Fri, 21 Aug 2015 10:57:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d08c2dc3be60\n  Fri, 21 Aug 2015 10:58:05 -0700   Fri, 21 Aug 2015 10:58:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 4240a2f67f62\n  Fri, 21 Aug 2015 10:58:05 -0700   Fri, 21 Aug 2015 10:58:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 4240a2f67f62\n  Fri, 21 Aug 2015 10:58:15 -0700   Fri, 21 Aug 2015 10:58:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id c565eb0654cb\n  Fri, 21 Aug 2015 10:58:15 -0700   Fri, 21 Aug 2015 10:58:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id c565eb0654cb\n  Fri, 21 Aug 2015 10:58:25 -0700   Fri, 21 Aug 2015 10:58:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id e8baadd8829c\n  Fri, 21 Aug 2015 10:58:25 -0700   Fri, 21 Aug 2015 10:58:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id e8baadd8829c\n  Fri, 21 Aug 2015 10:58:35 -0700   Fri, 21 Aug 2015 10:58:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f1581cc27884\n  Fri, 21 Aug 2015 10:58:35 -0700   Fri, 21 Aug 2015 10:58:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f1581cc27884\n  Fri, 21 Aug 2015 10:58:45 -0700   Fri, 21 Aug 2015 10:58:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 9c53b74b4b16\n  Fri, 21 Aug 2015 10:58:45 -0700   Fri, 21 Aug 2015 10:58:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 9c53b74b4b16\n  Fri, 21 Aug 2015 10:58:55 -0700   Fri, 21 Aug 2015 10:58:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 3f772c1732fd\n  Fri, 21 Aug 2015 10:58:55 -0700   Fri, 21 Aug 2015 10:58:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 3f772c1732fd\n  Fri, 21 Aug 2015 10:59:05 -0700   Fri, 21 Aug 2015 10:59:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id a8fee10ac341\n  Fri, 21 Aug 2015 10:59:05 -0700   Fri, 21 Aug 2015 10:59:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id a8fee10ac341\n  Fri, 21 Aug 2015 10:59:15 -0700   Fri, 21 Aug 2015 10:59:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f7962fea97b5\n  Fri, 21 Aug 2015 10:59:15 -0700   Fri, 21 Aug 2015 10:59:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f7962fea97b5\n  Fri, 21 Aug 2015 10:59:25 -0700   Fri, 21 Aug 2015 10:59:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 1239e1a6c6a7\n  Fri, 21 Aug 2015 10:59:25 -0700   Fri, 21 Aug 2015 10:59:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 1239e1a6c6a7\n  Fri, 21 Aug 2015 10:59:35 -0700   Fri, 21 Aug 2015 10:59:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 01f84f2f34ba\n  Fri, 21 Aug 2015 10:59:35 -0700   Fri, 21 Aug 2015 10:59:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 01f84f2f34ba\n  Fri, 21 Aug 2015 10:59:45 -0700   Fri, 21 Aug 2015 10:59:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id eaf5bcf47e70\n  Fri, 21 Aug 2015 10:59:45 -0700   Fri, 21 Aug 2015 10:59:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id eaf5bcf47e70\n  Fri, 21 Aug 2015 10:59:55 -0700   Fri, 21 Aug 2015 10:59:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id e3fcf0b55e06\n  Fri, 21 Aug 2015 10:59:55 -0700   Fri, 21 Aug 2015 10:59:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id e3fcf0b55e06\n  Fri, 21 Aug 2015 11:00:05 -0700   Fri, 21 Aug 2015 11:00:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 3af7000d4dc9\n  Fri, 21 Aug 2015 11:00:05 -0700   Fri, 21 Aug 2015 11:00:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 3af7000d4dc9\n  Fri, 21 Aug 2015 11:00:15 -0700   Fri, 21 Aug 2015 11:00:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 3ff2d85d759a\n  Fri, 21 Aug 2015 11:00:15 -0700   Fri, 21 Aug 2015 11:00:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 3ff2d85d759a\n  Fri, 21 Aug 2015 11:00:25 -0700   Fri, 21 Aug 2015 11:00:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 6bb5b69940a9\n  Fri, 21 Aug 2015 11:00:25 -0700   Fri, 21 Aug 2015 11:00:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 6bb5b69940a9\n  Fri, 21 Aug 2015 11:00:35 -0700   Fri, 21 Aug 2015 11:00:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f156ba4f92a0\n  Fri, 21 Aug 2015 11:00:35 -0700   Fri, 21 Aug 2015 11:00:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f156ba4f92a0\n  Fri, 21 Aug 2015 11:00:45 -0700   Fri, 21 Aug 2015 11:00:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id e40d2bd4a3d2\n  Fri, 21 Aug 2015 11:00:45 -0700   Fri, 21 Aug 2015 11:00:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id e40d2bd4a3d2\n  Fri, 21 Aug 2015 11:00:55 -0700   Fri, 21 Aug 2015 11:00:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8175a1e65399\n  Fri, 21 Aug 2015 11:00:55 -0700   Fri, 21 Aug 2015 11:00:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8175a1e65399\n  Fri, 21 Aug 2015 11:01:05 -0700   Fri, 21 Aug 2015 11:01:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 56606e7dc2a7\n  Fri, 21 Aug 2015 11:01:05 -0700   Fri, 21 Aug 2015 11:01:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 56606e7dc2a7\n  Fri, 21 Aug 2015 11:01:15 -0700   Fri, 21 Aug 2015 11:01:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 7e64de034807\n  Fri, 21 Aug 2015 11:01:15 -0700   Fri, 21 Aug 2015 11:01:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 7e64de034807\n  Fri, 21 Aug 2015 11:01:25 -0700   Fri, 21 Aug 2015 11:01:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d383e829ceef\n  Fri, 21 Aug 2015 11:01:25 -0700   Fri, 21 Aug 2015 11:01:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d383e829ceef\n  Fri, 21 Aug 2015 11:01:35 -0700   Fri, 21 Aug 2015 11:01:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f7355f7a51f7\n  Fri, 21 Aug 2015 11:01:35 -0700   Fri, 21 Aug 2015 11:01:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f7355f7a51f7\n  Fri, 21 Aug 2015 11:01:45 -0700   Fri, 21 Aug 2015 11:01:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 1d56912760b4\n  Fri, 21 Aug 2015 11:01:45 -0700   Fri, 21 Aug 2015 11:01:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 1d56912760b4\n  Fri, 21 Aug 2015 11:01:55 -0700   Fri, 21 Aug 2015 11:01:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 97e4baace81d\n  Fri, 21 Aug 2015 11:01:55 -0700   Fri, 21 Aug 2015 11:01:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 97e4baace81d\n  Fri, 21 Aug 2015 11:02:05 -0700   Fri, 21 Aug 2015 11:02:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 6ac5bacd6474\n  Fri, 21 Aug 2015 11:02:05 -0700   Fri, 21 Aug 2015 11:02:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 6ac5bacd6474\n  Fri, 21 Aug 2015 11:02:15 -0700   Fri, 21 Aug 2015 11:02:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 7c4eed3190da\n  Fri, 21 Aug 2015 11:02:15 -0700   Fri, 21 Aug 2015 11:02:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 7c4eed3190da\n  Fri, 21 Aug 2015 11:02:25 -0700   Fri, 21 Aug 2015 11:02:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id c8641e180f5b\n  Fri, 21 Aug 2015 11:02:26 -0700   Fri, 21 Aug 2015 11:02:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id c8641e180f5b\n  Fri, 21 Aug 2015 11:02:35 -0700   Fri, 21 Aug 2015 11:02:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id fcb268534517\n  Fri, 21 Aug 2015 11:02:36 -0700   Fri, 21 Aug 2015 11:02:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id fcb268534517\n  Fri, 21 Aug 2015 11:02:46 -0700   Fri, 21 Aug 2015 11:02:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 3e96285fa48a\n  Fri, 21 Aug 2015 11:02:46 -0700   Fri, 21 Aug 2015 11:02:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 3e96285fa48a\n  Fri, 21 Aug 2015 11:02:55 -0700   Fri, 21 Aug 2015 11:02:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 82c3dffc5b7b\n  Fri, 21 Aug 2015 11:02:56 -0700   Fri, 21 Aug 2015 11:02:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 82c3dffc5b7b\n  Fri, 21 Aug 2015 11:03:05 -0700   Fri, 21 Aug 2015 11:03:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 2b0485000fb1\n  Fri, 21 Aug 2015 11:03:05 -0700   Fri, 21 Aug 2015 11:03:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 2b0485000fb1\n  Fri, 21 Aug 2015 11:03:15 -0700   Fri, 21 Aug 2015 11:03:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id b43c51f9e5f2\n  Fri, 21 Aug 2015 11:03:16 -0700   Fri, 21 Aug 2015 11:03:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id b43c51f9e5f2\n  Fri, 21 Aug 2015 11:03:25 -0700   Fri, 21 Aug 2015 11:03:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id eab6825eba5e\n  Fri, 21 Aug 2015 11:03:26 -0700   Fri, 21 Aug 2015 11:03:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id eab6825eba5e\n  Fri, 21 Aug 2015 11:03:35 -0700   Fri, 21 Aug 2015 11:03:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 979711539b91\n  Fri, 21 Aug 2015 11:03:35 -0700   Fri, 21 Aug 2015 11:03:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 979711539b91\n  Fri, 21 Aug 2015 11:03:45 -0700   Fri, 21 Aug 2015 11:03:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 9aa796f6d243\n  Fri, 21 Aug 2015 11:03:46 -0700   Fri, 21 Aug 2015 11:03:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 9aa796f6d243\n  Fri, 21 Aug 2015 11:03:55 -0700   Fri, 21 Aug 2015 11:03:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8f7f834a3422\n  Fri, 21 Aug 2015 11:03:56 -0700   Fri, 21 Aug 2015 11:03:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8f7f834a3422\n  Fri, 21 Aug 2015 11:04:06 -0700   Fri, 21 Aug 2015 11:04:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id c39d25e51281\n  Fri, 21 Aug 2015 11:04:06 -0700   Fri, 21 Aug 2015 11:04:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id c39d25e51281\n  Fri, 21 Aug 2015 11:04:15 -0700   Fri, 21 Aug 2015 11:04:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 9e131eeae4a6\n  Fri, 21 Aug 2015 11:04:16 -0700   Fri, 21 Aug 2015 11:04:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 9e131eeae4a6\n  Fri, 21 Aug 2015 11:04:26 -0700   Fri, 21 Aug 2015 11:04:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id b1e031088ab9\n  Fri, 21 Aug 2015 11:04:26 -0700   Fri, 21 Aug 2015 11:04:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id b1e031088ab9\n  Fri, 21 Aug 2015 11:04:36 -0700   Fri, 21 Aug 2015 11:04:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id a4c0638f7b0e\n  Fri, 21 Aug 2015 11:04:36 -0700   Fri, 21 Aug 2015 11:04:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id a4c0638f7b0e\n  Fri, 21 Aug 2015 11:04:45 -0700   Fri, 21 Aug 2015 11:04:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f56aa7569853\n  Fri, 21 Aug 2015 11:04:46 -0700   Fri, 21 Aug 2015 11:04:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f56aa7569853\n  Fri, 21 Aug 2015 11:04:55 -0700   Fri, 21 Aug 2015 11:04:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 1a62616ce134\n  Fri, 21 Aug 2015 11:04:56 -0700   Fri, 21 Aug 2015 11:04:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 1a62616ce134\n  Fri, 21 Aug 2015 11:05:06 -0700   Fri, 21 Aug 2015 11:05:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id eac8b10a083f\n  Fri, 21 Aug 2015 11:05:06 -0700   Fri, 21 Aug 2015 11:05:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id eac8b10a083f\n  Fri, 21 Aug 2015 11:05:16 -0700   Fri, 21 Aug 2015 11:05:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d9e7fa688c63\n  Fri, 21 Aug 2015 11:05:16 -0700   Fri, 21 Aug 2015 11:05:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d9e7fa688c63\n  Fri, 21 Aug 2015 11:05:26 -0700   Fri, 21 Aug 2015 11:05:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id c1291b91aed3\n  Fri, 21 Aug 2015 11:05:26 -0700   Fri, 21 Aug 2015 11:05:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id c1291b91aed3\n  Fri, 21 Aug 2015 11:05:36 -0700   Fri, 21 Aug 2015 11:05:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 85d67899e9fb\n  Fri, 21 Aug 2015 11:05:36 -0700   Fri, 21 Aug 2015 11:05:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 85d67899e9fb\n  Fri, 21 Aug 2015 11:05:46 -0700   Fri, 21 Aug 2015 11:05:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 719283e9a548\n  Fri, 21 Aug 2015 11:05:46 -0700   Fri, 21 Aug 2015 11:05:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 719283e9a548\n  Fri, 21 Aug 2015 11:05:56 -0700   Fri, 21 Aug 2015 11:05:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 323d3e2bb695\n  Fri, 21 Aug 2015 11:05:56 -0700   Fri, 21 Aug 2015 11:05:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 323d3e2bb695\n  Fri, 21 Aug 2015 11:06:06 -0700   Fri, 21 Aug 2015 11:06:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 0c156cb63196\n  Fri, 21 Aug 2015 11:06:06 -0700   Fri, 21 Aug 2015 11:06:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 0c156cb63196\n  Fri, 21 Aug 2015 11:06:16 -0700   Fri, 21 Aug 2015 11:06:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 4fddfabd5721\n  Fri, 21 Aug 2015 11:06:16 -0700   Fri, 21 Aug 2015 11:06:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 4fddfabd5721\n  Fri, 21 Aug 2015 11:06:26 -0700   Fri, 21 Aug 2015 11:06:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 4ae0a816ea4b\n  Fri, 21 Aug 2015 11:06:26 -0700   Fri, 21 Aug 2015 11:06:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 4ae0a816ea4b\n  Fri, 21 Aug 2015 11:06:36 -0700   Fri, 21 Aug 2015 11:06:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id e4eec3de203d\n  Fri, 21 Aug 2015 11:06:36 -0700   Fri, 21 Aug 2015 11:06:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id e4eec3de203d\n  Fri, 21 Aug 2015 11:06:46 -0700   Fri, 21 Aug 2015 11:06:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id a9549cea961c\n  Fri, 21 Aug 2015 11:06:46 -0700   Fri, 21 Aug 2015 11:06:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id a9549cea961c\n  Fri, 21 Aug 2015 11:06:56 -0700   Fri, 21 Aug 2015 11:06:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id cf6e52a7c7ba\n  Fri, 21 Aug 2015 11:06:56 -0700   Fri, 21 Aug 2015 11:06:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id cf6e52a7c7ba\n  Fri, 21 Aug 2015 11:07:05 -0700   Fri, 21 Aug 2015 11:07:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 52d290edf67c\n  Fri, 21 Aug 2015 11:07:05 -0700   Fri, 21 Aug 2015 11:07:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 52d290edf67c\n  Fri, 21 Aug 2015 11:07:15 -0700   Fri, 21 Aug 2015 11:07:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d5905b83ba0d\n  Fri, 21 Aug 2015 11:07:15 -0700   Fri, 21 Aug 2015 11:07:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d5905b83ba0d\n  Fri, 21 Aug 2015 11:07:26 -0700   Fri, 21 Aug 2015 11:07:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 603a2d41532f\n  Fri, 21 Aug 2015 11:07:26 -0700   Fri, 21 Aug 2015 11:07:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 603a2d41532f\n  Fri, 21 Aug 2015 11:07:36 -0700   Fri, 21 Aug 2015 11:07:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 793d79a89011\n  Fri, 21 Aug 2015 11:07:36 -0700   Fri, 21 Aug 2015 11:07:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 793d79a89011\n  Fri, 21 Aug 2015 11:07:46 -0700   Fri, 21 Aug 2015 11:07:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8aa165bd76b1\n  Fri, 21 Aug 2015 11:07:46 -0700   Fri, 21 Aug 2015 11:07:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8aa165bd76b1\n  Fri, 21 Aug 2015 11:07:56 -0700   Fri, 21 Aug 2015 11:07:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 7c6b40ba8d64\n  Fri, 21 Aug 2015 11:07:56 -0700   Fri, 21 Aug 2015 11:07:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 7c6b40ba8d64\n  Fri, 21 Aug 2015 11:08:05 -0700   Fri, 21 Aug 2015 11:08:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id eaec3f149997\n  Fri, 21 Aug 2015 11:08:05 -0700   Fri, 21 Aug 2015 11:08:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id eaec3f149997\n  Fri, 21 Aug 2015 11:08:15 -0700   Fri, 21 Aug 2015 11:08:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f76f550427f3\n  Fri, 21 Aug 2015 11:08:15 -0700   Fri, 21 Aug 2015 11:08:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f76f550427f3\n  Fri, 21 Aug 2015 11:08:25 -0700   Fri, 21 Aug 2015 11:08:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id fb047dd3ab2c\n  Fri, 21 Aug 2015 11:08:25 -0700   Fri, 21 Aug 2015 11:08:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id fb047dd3ab2c\n  Fri, 21 Aug 2015 11:08:35 -0700   Fri, 21 Aug 2015 11:08:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f9f07852744f\n  Fri, 21 Aug 2015 11:08:35 -0700   Fri, 21 Aug 2015 11:08:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f9f07852744f\n  Fri, 21 Aug 2015 11:08:46 -0700   Fri, 21 Aug 2015 11:08:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8fdc1da4ff24\n  Fri, 21 Aug 2015 11:08:46 -0700   Fri, 21 Aug 2015 11:08:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8fdc1da4ff24\n  Fri, 21 Aug 2015 11:08:56 -0700   Fri, 21 Aug 2015 11:08:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 1347212856b0\n  Fri, 21 Aug 2015 11:08:56 -0700   Fri, 21 Aug 2015 11:08:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 1347212856b0\n  Fri, 21 Aug 2015 11:09:06 -0700   Fri, 21 Aug 2015 11:09:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id b7735f817f84\n  Fri, 21 Aug 2015 11:09:06 -0700   Fri, 21 Aug 2015 11:09:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id b7735f817f84\n  Fri, 21 Aug 2015 11:09:16 -0700   Fri, 21 Aug 2015 11:09:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 2e942723956e\n  Fri, 21 Aug 2015 11:09:16 -0700   Fri, 21 Aug 2015 11:09:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 2e942723956e\n  Fri, 21 Aug 2015 11:09:26 -0700   Fri, 21 Aug 2015 11:09:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 89b71e7ca62a\n  Fri, 21 Aug 2015 11:09:26 -0700   Fri, 21 Aug 2015 11:09:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 89b71e7ca62a\n  Fri, 21 Aug 2015 11:09:36 -0700   Fri, 21 Aug 2015 11:09:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id e8faf26e31a6\n  Fri, 21 Aug 2015 11:09:36 -0700   Fri, 21 Aug 2015 11:09:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id e8faf26e31a6\n  Fri, 21 Aug 2015 11:09:46 -0700   Fri, 21 Aug 2015 11:09:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id dba97c74fbb2\n  Fri, 21 Aug 2015 11:09:46 -0700   Fri, 21 Aug 2015 11:09:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id dba97c74fbb2\n  Fri, 21 Aug 2015 11:09:56 -0700   Fri, 21 Aug 2015 11:09:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8ec16dbd6ac9\n  Fri, 21 Aug 2015 11:09:56 -0700   Fri, 21 Aug 2015 11:09:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8ec16dbd6ac9\n  Fri, 21 Aug 2015 11:10:06 -0700   Fri, 21 Aug 2015 11:10:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id ba135bcf7e62\n  Fri, 21 Aug 2015 11:10:06 -0700   Fri, 21 Aug 2015 11:10:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id ba135bcf7e62\n  Fri, 21 Aug 2015 11:10:16 -0700   Fri, 21 Aug 2015 11:10:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 9d8d0bfe7a8f\n  Fri, 21 Aug 2015 11:10:16 -0700   Fri, 21 Aug 2015 11:10:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 9d8d0bfe7a8f\n  Fri, 21 Aug 2015 11:10:26 -0700   Fri, 21 Aug 2015 11:10:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id cf1e7caf9cdb\n  Fri, 21 Aug 2015 11:10:26 -0700   Fri, 21 Aug 2015 11:10:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id cf1e7caf9cdb\n  Fri, 21 Aug 2015 11:10:36 -0700   Fri, 21 Aug 2015 11:10:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 898540165d0b\n  Fri, 21 Aug 2015 11:10:36 -0700   Fri, 21 Aug 2015 11:10:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 898540165d0b\n  Fri, 21 Aug 2015 11:10:46 -0700   Fri, 21 Aug 2015 11:10:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 55ebcfb6d9fb\n  Fri, 21 Aug 2015 11:10:46 -0700   Fri, 21 Aug 2015 11:10:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 55ebcfb6d9fb\n  Fri, 21 Aug 2015 11:10:56 -0700   Fri, 21 Aug 2015 11:10:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id ddafd5396596\n  Fri, 21 Aug 2015 11:10:56 -0700   Fri, 21 Aug 2015 11:10:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id ddafd5396596\n  Fri, 21 Aug 2015 11:11:06 -0700   Fri, 21 Aug 2015 11:11:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id e883974512bc\n  Fri, 21 Aug 2015 11:11:06 -0700   Fri, 21 Aug 2015 11:11:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id e883974512bc\n  Fri, 21 Aug 2015 11:11:16 -0700   Fri, 21 Aug 2015 11:11:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8f9c93813dc8\n  Fri, 21 Aug 2015 11:11:16 -0700   Fri, 21 Aug 2015 11:11:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8f9c93813dc8\n  Fri, 21 Aug 2015 11:11:26 -0700   Fri, 21 Aug 2015 11:11:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8f4de0f58425\n  Fri, 21 Aug 2015 11:11:26 -0700   Fri, 21 Aug 2015 11:11:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8f4de0f58425\n  Fri, 21 Aug 2015 11:11:36 -0700   Fri, 21 Aug 2015 11:11:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f21a540c400b\n  Fri, 21 Aug 2015 11:11:36 -0700   Fri, 21 Aug 2015 11:11:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f21a540c400b\n  Fri, 21 Aug 2015 11:11:46 -0700   Fri, 21 Aug 2015 11:11:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8130ee53797f\n  Fri, 21 Aug 2015 11:11:46 -0700   Fri, 21 Aug 2015 11:11:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8130ee53797f\n  Fri, 21 Aug 2015 11:11:56 -0700   Fri, 21 Aug 2015 11:11:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d6cf46206903\n  Fri, 21 Aug 2015 11:11:56 -0700   Fri, 21 Aug 2015 11:11:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d6cf46206903\n  Fri, 21 Aug 2015 11:12:06 -0700   Fri, 21 Aug 2015 11:12:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id fec7e8e51c1f\n  Fri, 21 Aug 2015 11:12:06 -0700   Fri, 21 Aug 2015 11:12:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id fec7e8e51c1f\n  Fri, 21 Aug 2015 11:12:16 -0700   Fri, 21 Aug 2015 11:12:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8d0a0dc7e753\n  Fri, 21 Aug 2015 11:12:16 -0700   Fri, 21 Aug 2015 11:12:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8d0a0dc7e753\n  Fri, 21 Aug 2015 11:12:26 -0700   Fri, 21 Aug 2015 11:12:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 34a0de509e62\n  Fri, 21 Aug 2015 11:12:26 -0700   Fri, 21 Aug 2015 11:12:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 34a0de509e62\n  Fri, 21 Aug 2015 11:12:36 -0700   Fri, 21 Aug 2015 11:12:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 05d0bd71631a\n  Fri, 21 Aug 2015 11:12:36 -0700   Fri, 21 Aug 2015 11:12:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 05d0bd71631a\n  Fri, 21 Aug 2015 11:12:46 -0700   Fri, 21 Aug 2015 11:12:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id cdfaa770565a\n  Fri, 21 Aug 2015 11:12:46 -0700   Fri, 21 Aug 2015 11:12:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id cdfaa770565a\n  Fri, 21 Aug 2015 11:12:56 -0700   Fri, 21 Aug 2015 11:12:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id be7aea0f90e2\n  Fri, 21 Aug 2015 11:12:56 -0700   Fri, 21 Aug 2015 11:12:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id be7aea0f90e2\n  Fri, 21 Aug 2015 11:13:06 -0700   Fri, 21 Aug 2015 11:13:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 776f12748b08\n  Fri, 21 Aug 2015 11:13:06 -0700   Fri, 21 Aug 2015 11:13:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 776f12748b08\n  Fri, 21 Aug 2015 11:13:16 -0700   Fri, 21 Aug 2015 11:13:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 9277159918a2\n  Fri, 21 Aug 2015 11:13:16 -0700   Fri, 21 Aug 2015 11:13:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 9277159918a2\n  Fri, 21 Aug 2015 11:13:26 -0700   Fri, 21 Aug 2015 11:13:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 61ad28cfb2dc\n  Fri, 21 Aug 2015 11:13:26 -0700   Fri, 21 Aug 2015 11:13:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 61ad28cfb2dc\n  Fri, 21 Aug 2015 11:13:36 -0700   Fri, 21 Aug 2015 11:13:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id ee38ebf3e303\n  Fri, 21 Aug 2015 11:13:36 -0700   Fri, 21 Aug 2015 11:13:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id ee38ebf3e303\n  Fri, 21 Aug 2015 11:13:46 -0700   Fri, 21 Aug 2015 11:13:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id debab36e560b\n  Fri, 21 Aug 2015 11:13:46 -0700   Fri, 21 Aug 2015 11:13:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id debab36e560b\n  Fri, 21 Aug 2015 11:13:56 -0700   Fri, 21 Aug 2015 11:13:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 2ef0d490f1a5\n  Fri, 21 Aug 2015 11:13:56 -0700   Fri, 21 Aug 2015 11:13:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 2ef0d490f1a5\n  Fri, 21 Aug 2015 11:14:06 -0700   Fri, 21 Aug 2015 11:14:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id a99f6fcad93f\n  Fri, 21 Aug 2015 11:14:06 -0700   Fri, 21 Aug 2015 11:14:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id a99f6fcad93f\n  Fri, 21 Aug 2015 11:14:16 -0700   Fri, 21 Aug 2015 11:14:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f2194829cf9e\n  Fri, 21 Aug 2015 11:14:16 -0700   Fri, 21 Aug 2015 11:14:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f2194829cf9e\n  Fri, 21 Aug 2015 11:14:26 -0700   Fri, 21 Aug 2015 11:14:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 91dcf4ce5ba1\n  Fri, 21 Aug 2015 11:14:26 -0700   Fri, 21 Aug 2015 11:14:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 91dcf4ce5ba1\n  Fri, 21 Aug 2015 11:14:36 -0700   Fri, 21 Aug 2015 11:14:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 6335c3ee0f7c\n  Fri, 21 Aug 2015 11:14:36 -0700   Fri, 21 Aug 2015 11:14:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 6335c3ee0f7c\n  Fri, 21 Aug 2015 11:14:46 -0700   Fri, 21 Aug 2015 11:14:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 184c0644b35d\n  Fri, 21 Aug 2015 11:14:46 -0700   Fri, 21 Aug 2015 11:14:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 184c0644b35d\n  Fri, 21 Aug 2015 11:14:56 -0700   Fri, 21 Aug 2015 11:14:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 366375549f11\n  Fri, 21 Aug 2015 11:14:56 -0700   Fri, 21 Aug 2015 11:14:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 366375549f11\n  Fri, 21 Aug 2015 11:15:06 -0700   Fri, 21 Aug 2015 11:15:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 37059475d437\n  Fri, 21 Aug 2015 11:15:06 -0700   Fri, 21 Aug 2015 11:15:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 37059475d437\n  Fri, 21 Aug 2015 11:15:16 -0700   Fri, 21 Aug 2015 11:15:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id fec6cef9212d\n  Fri, 21 Aug 2015 11:15:16 -0700   Fri, 21 Aug 2015 11:15:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id fec6cef9212d\n  Fri, 21 Aug 2015 11:15:26 -0700   Fri, 21 Aug 2015 11:15:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id cf1c0cce068f\n  Fri, 21 Aug 2015 11:15:26 -0700   Fri, 21 Aug 2015 11:15:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id cf1c0cce068f\n  Fri, 21 Aug 2015 11:15:36 -0700   Fri, 21 Aug 2015 11:15:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id e0bca02530ae\n  Fri, 21 Aug 2015 11:15:36 -0700   Fri, 21 Aug 2015 11:15:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id e0bca02530ae\n  Fri, 21 Aug 2015 11:15:46 -0700   Fri, 21 Aug 2015 11:15:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id ea9dee32935b\n  Fri, 21 Aug 2015 11:15:46 -0700   Fri, 21 Aug 2015 11:15:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id ea9dee32935b\n  Fri, 21 Aug 2015 11:15:56 -0700   Fri, 21 Aug 2015 11:15:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d50d846d1a5b\n  Fri, 21 Aug 2015 11:15:56 -0700   Fri, 21 Aug 2015 11:15:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d50d846d1a5b\n  Fri, 21 Aug 2015 11:16:06 -0700   Fri, 21 Aug 2015 11:16:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id efbe2428f5b1\n  Fri, 21 Aug 2015 11:16:06 -0700   Fri, 21 Aug 2015 11:16:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id efbe2428f5b1\n  Fri, 21 Aug 2015 11:16:16 -0700   Fri, 21 Aug 2015 11:16:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d3a36f801850\n  Fri, 21 Aug 2015 11:16:16 -0700   Fri, 21 Aug 2015 11:16:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d3a36f801850\n  Fri, 21 Aug 2015 11:16:26 -0700   Fri, 21 Aug 2015 11:16:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id b7bc49e1859e\n  Fri, 21 Aug 2015 11:16:26 -0700   Fri, 21 Aug 2015 11:16:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id b7bc49e1859e\n  Fri, 21 Aug 2015 11:16:36 -0700   Fri, 21 Aug 2015 11:16:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 957f6597f678\n  Fri, 21 Aug 2015 11:16:36 -0700   Fri, 21 Aug 2015 11:16:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 957f6597f678\n  Fri, 21 Aug 2015 11:16:46 -0700   Fri, 21 Aug 2015 11:16:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id b42634b7dc62\n  Fri, 21 Aug 2015 11:16:46 -0700   Fri, 21 Aug 2015 11:16:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id b42634b7dc62\n  Fri, 21 Aug 2015 11:16:56 -0700   Fri, 21 Aug 2015 11:16:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 79ef814a6d31\n  Fri, 21 Aug 2015 11:16:56 -0700   Fri, 21 Aug 2015 11:16:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 79ef814a6d31\n  Fri, 21 Aug 2015 11:17:06 -0700   Fri, 21 Aug 2015 11:17:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8ec10d7a4bc6\n  Fri, 21 Aug 2015 11:17:06 -0700   Fri, 21 Aug 2015 11:17:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8ec10d7a4bc6\n  Fri, 21 Aug 2015 11:17:16 -0700   Fri, 21 Aug 2015 11:17:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 08d8d123c4fa\n  Fri, 21 Aug 2015 11:17:16 -0700   Fri, 21 Aug 2015 11:17:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 08d8d123c4fa\n  Fri, 21 Aug 2015 11:17:26 -0700   Fri, 21 Aug 2015 11:17:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d6ca4bd6afa5\n  Fri, 21 Aug 2015 11:17:26 -0700   Fri, 21 Aug 2015 11:17:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d6ca4bd6afa5\n  Fri, 21 Aug 2015 11:17:36 -0700   Fri, 21 Aug 2015 11:17:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 86ac7360704a\n  Fri, 21 Aug 2015 11:17:36 -0700   Fri, 21 Aug 2015 11:17:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 86ac7360704a\n  Fri, 21 Aug 2015 11:17:46 -0700   Fri, 21 Aug 2015 11:17:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 424371953ef6\n  Fri, 21 Aug 2015 11:17:46 -0700   Fri, 21 Aug 2015 11:17:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 424371953ef6\n  Fri, 21 Aug 2015 11:17:56 -0700   Fri, 21 Aug 2015 11:17:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id bf797eb80cce\n  Fri, 21 Aug 2015 11:17:56 -0700   Fri, 21 Aug 2015 11:17:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id bf797eb80cce\n  Fri, 21 Aug 2015 11:18:06 -0700   Fri, 21 Aug 2015 11:18:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 515eff9d1a51\n  Fri, 21 Aug 2015 11:18:06 -0700   Fri, 21 Aug 2015 11:18:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 515eff9d1a51\n  Fri, 21 Aug 2015 11:18:15 -0700   Fri, 21 Aug 2015 11:18:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 0a15a4dc8986\n  Fri, 21 Aug 2015 11:18:15 -0700   Fri, 21 Aug 2015 11:18:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 0a15a4dc8986\n  Fri, 21 Aug 2015 11:18:26 -0700   Fri, 21 Aug 2015 11:18:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 9ed2d506de93\n  Fri, 21 Aug 2015 11:18:26 -0700   Fri, 21 Aug 2015 11:18:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 9ed2d506de93\n  Fri, 21 Aug 2015 11:18:36 -0700   Fri, 21 Aug 2015 11:18:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 9fd363e9ac25\n  Fri, 21 Aug 2015 11:18:36 -0700   Fri, 21 Aug 2015 11:18:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 9fd363e9ac25\n  Fri, 21 Aug 2015 11:18:46 -0700   Fri, 21 Aug 2015 11:18:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f25cd5a2fe84\n  Fri, 21 Aug 2015 11:18:46 -0700   Fri, 21 Aug 2015 11:18:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f25cd5a2fe84\n  Fri, 21 Aug 2015 11:18:56 -0700   Fri, 21 Aug 2015 11:18:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 24c3304f7844\n  Fri, 21 Aug 2015 11:18:56 -0700   Fri, 21 Aug 2015 11:18:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 24c3304f7844\n  Fri, 21 Aug 2015 11:19:06 -0700   Fri, 21 Aug 2015 11:19:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 4769ff6a50f0\n  Fri, 21 Aug 2015 11:19:06 -0700   Fri, 21 Aug 2015 11:19:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 4769ff6a50f0\n  Fri, 21 Aug 2015 11:19:16 -0700   Fri, 21 Aug 2015 11:19:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 9033ee073d63\n  Fri, 21 Aug 2015 11:19:16 -0700   Fri, 21 Aug 2015 11:19:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 9033ee073d63\n  Fri, 21 Aug 2015 11:19:26 -0700   Fri, 21 Aug 2015 11:19:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id db23a5a6867f\n  Fri, 21 Aug 2015 11:19:26 -0700   Fri, 21 Aug 2015 11:19:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id db23a5a6867f\n  Fri, 21 Aug 2015 11:19:36 -0700   Fri, 21 Aug 2015 11:19:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 587ff6dcf12c\n  Fri, 21 Aug 2015 11:19:36 -0700   Fri, 21 Aug 2015 11:19:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 587ff6dcf12c\n  Fri, 21 Aug 2015 11:19:46 -0700   Fri, 21 Aug 2015 11:19:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id ac53b333853f\n  Fri, 21 Aug 2015 11:19:46 -0700   Fri, 21 Aug 2015 11:19:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id ac53b333853f\n  Fri, 21 Aug 2015 11:19:56 -0700   Fri, 21 Aug 2015 11:19:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id ff11a3fe07d4\n  Fri, 21 Aug 2015 11:19:56 -0700   Fri, 21 Aug 2015 11:19:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id ff11a3fe07d4\n  Fri, 21 Aug 2015 11:20:06 -0700   Fri, 21 Aug 2015 11:20:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 0b7422296d1d\n  Fri, 21 Aug 2015 11:20:06 -0700   Fri, 21 Aug 2015 11:20:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 0b7422296d1d\n  Fri, 21 Aug 2015 11:20:16 -0700   Fri, 21 Aug 2015 11:20:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id bbfbe1714694\n  Fri, 21 Aug 2015 11:20:16 -0700   Fri, 21 Aug 2015 11:20:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id bbfbe1714694\n  Fri, 21 Aug 2015 11:20:26 -0700   Fri, 21 Aug 2015 11:20:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 44e2f1c9000a\n  Fri, 21 Aug 2015 11:20:26 -0700   Fri, 21 Aug 2015 11:20:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 44e2f1c9000a\n  Fri, 21 Aug 2015 11:20:36 -0700   Fri, 21 Aug 2015 11:20:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 36f9784658ce\n  Fri, 21 Aug 2015 11:20:36 -0700   Fri, 21 Aug 2015 11:20:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 36f9784658ce\n  Fri, 21 Aug 2015 11:20:46 -0700   Fri, 21 Aug 2015 11:20:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 04ff9832a17b\n  Fri, 21 Aug 2015 11:20:46 -0700   Fri, 21 Aug 2015 11:20:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 04ff9832a17b\n  Fri, 21 Aug 2015 11:20:56 -0700   Fri, 21 Aug 2015 11:20:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id fa4497bedee2\n  Fri, 21 Aug 2015 11:20:56 -0700   Fri, 21 Aug 2015 11:20:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id fa4497bedee2\n  Fri, 21 Aug 2015 11:21:06 -0700   Fri, 21 Aug 2015 11:21:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id ac4fe425ce05\n  Fri, 21 Aug 2015 11:21:06 -0700   Fri, 21 Aug 2015 11:21:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id ac4fe425ce05\n  Fri, 21 Aug 2015 11:21:16 -0700   Fri, 21 Aug 2015 11:21:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 5bd065b3bacb\n  Fri, 21 Aug 2015 11:21:16 -0700   Fri, 21 Aug 2015 11:21:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 5bd065b3bacb\n  Fri, 21 Aug 2015 11:21:26 -0700   Fri, 21 Aug 2015 11:21:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id c49f604b56aa\n  Fri, 21 Aug 2015 11:21:26 -0700   Fri, 21 Aug 2015 11:21:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id c49f604b56aa\n  Fri, 21 Aug 2015 11:21:36 -0700   Fri, 21 Aug 2015 11:21:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id acedde54e004\n  Fri, 21 Aug 2015 11:21:36 -0700   Fri, 21 Aug 2015 11:21:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id acedde54e004\n  Fri, 21 Aug 2015 11:21:46 -0700   Fri, 21 Aug 2015 11:21:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 81ec945ecccf\n  Fri, 21 Aug 2015 11:21:46 -0700   Fri, 21 Aug 2015 11:21:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 81ec945ecccf\n  Fri, 21 Aug 2015 11:21:56 -0700   Fri, 21 Aug 2015 11:21:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id c2099e9924be\n  Fri, 21 Aug 2015 11:21:56 -0700   Fri, 21 Aug 2015 11:21:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id c2099e9924be\n  Fri, 21 Aug 2015 11:22:06 -0700   Fri, 21 Aug 2015 11:22:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 3262bdbd7108\n  Fri, 21 Aug 2015 11:22:06 -0700   Fri, 21 Aug 2015 11:22:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 3262bdbd7108\n  Fri, 21 Aug 2015 11:22:16 -0700   Fri, 21 Aug 2015 11:22:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 2e46fbb71b60\n  Fri, 21 Aug 2015 11:22:16 -0700   Fri, 21 Aug 2015 11:22:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 2e46fbb71b60\n  Fri, 21 Aug 2015 11:22:26 -0700   Fri, 21 Aug 2015 11:22:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 978cd428ee6d\n  Fri, 21 Aug 2015 11:22:27 -0700   Fri, 21 Aug 2015 11:22:27 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 978cd428ee6d\n  Fri, 21 Aug 2015 11:22:36 -0700   Fri, 21 Aug 2015 11:22:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id a643f67abf4e\n  Fri, 21 Aug 2015 11:22:36 -0700   Fri, 21 Aug 2015 11:22:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id a643f67abf4e\n  Fri, 21 Aug 2015 11:22:46 -0700   Fri, 21 Aug 2015 11:22:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 7d1ffa77a8d5\n  Fri, 21 Aug 2015 11:22:46 -0700   Fri, 21 Aug 2015 11:22:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 7d1ffa77a8d5\n  Fri, 21 Aug 2015 11:22:56 -0700   Fri, 21 Aug 2015 11:22:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f0b6fc2f85b0\n  Fri, 21 Aug 2015 11:22:56 -0700   Fri, 21 Aug 2015 11:22:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f0b6fc2f85b0\n  Fri, 21 Aug 2015 11:23:06 -0700   Fri, 21 Aug 2015 11:23:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id c1ecaa34bcf8\n  Fri, 21 Aug 2015 11:23:06 -0700   Fri, 21 Aug 2015 11:23:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id c1ecaa34bcf8\n  Fri, 21 Aug 2015 11:23:16 -0700   Fri, 21 Aug 2015 11:23:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 0fd1abb2c26e\n  Fri, 21 Aug 2015 11:23:16 -0700   Fri, 21 Aug 2015 11:23:16 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 0fd1abb2c26e\n  Fri, 21 Aug 2015 11:23:26 -0700   Fri, 21 Aug 2015 11:23:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f420a4251a45\n  Fri, 21 Aug 2015 11:23:26 -0700   Fri, 21 Aug 2015 11:23:26 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f420a4251a45\n  Fri, 21 Aug 2015 11:23:36 -0700   Fri, 21 Aug 2015 11:23:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 52778a95d8b6\n  Fri, 21 Aug 2015 11:23:36 -0700   Fri, 21 Aug 2015 11:23:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 52778a95d8b6\n  Fri, 21 Aug 2015 11:23:46 -0700   Fri, 21 Aug 2015 11:23:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 0df655fbb439\n  Fri, 21 Aug 2015 11:23:46 -0700   Fri, 21 Aug 2015 11:23:46 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 0df655fbb439\n  Fri, 21 Aug 2015 11:23:56 -0700   Fri, 21 Aug 2015 11:23:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 82089c7fef90\n  Fri, 21 Aug 2015 11:23:56 -0700   Fri, 21 Aug 2015 11:23:56 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 82089c7fef90\n  Fri, 21 Aug 2015 11:24:06 -0700   Fri, 21 Aug 2015 11:24:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 33e85c5762f9\n  Fri, 21 Aug 2015 11:24:06 -0700   Fri, 21 Aug 2015 11:24:06 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 33e85c5762f9\n  Fri, 21 Aug 2015 11:24:15 -0700   Fri, 21 Aug 2015 11:24:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 65d2b7d483fb\n  Fri, 21 Aug 2015 11:24:15 -0700   Fri, 21 Aug 2015 11:24:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 65d2b7d483fb\n  Fri, 21 Aug 2015 11:24:25 -0700   Fri, 21 Aug 2015 11:24:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id eaf63e70999f\n  Fri, 21 Aug 2015 11:24:25 -0700   Fri, 21 Aug 2015 11:24:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id eaf63e70999f\n  Fri, 21 Aug 2015 11:24:35 -0700   Fri, 21 Aug 2015 11:24:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 9a0b42822fa2\n  Fri, 21 Aug 2015 11:24:35 -0700   Fri, 21 Aug 2015 11:24:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 9a0b42822fa2\n  Fri, 21 Aug 2015 11:24:45 -0700   Fri, 21 Aug 2015 11:24:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 73cccc8c3404\n  Fri, 21 Aug 2015 11:24:45 -0700   Fri, 21 Aug 2015 11:24:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 73cccc8c3404\n  Fri, 21 Aug 2015 11:24:55 -0700   Fri, 21 Aug 2015 11:24:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8320c521d2ae\n  Fri, 21 Aug 2015 11:24:55 -0700   Fri, 21 Aug 2015 11:24:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8320c521d2ae\n  Fri, 21 Aug 2015 11:25:05 -0700   Fri, 21 Aug 2015 11:25:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id fae1f12330fb\n  Fri, 21 Aug 2015 11:25:05 -0700   Fri, 21 Aug 2015 11:25:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id fae1f12330fb\n  Fri, 21 Aug 2015 11:25:15 -0700   Fri, 21 Aug 2015 11:25:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id cc7e084d3812\n  Fri, 21 Aug 2015 11:25:15 -0700   Fri, 21 Aug 2015 11:25:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id cc7e084d3812\n  Fri, 21 Aug 2015 11:25:25 -0700   Fri, 21 Aug 2015 11:25:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 8c09bb1c9f17\n  Fri, 21 Aug 2015 11:25:25 -0700   Fri, 21 Aug 2015 11:25:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 8c09bb1c9f17\n  Fri, 21 Aug 2015 11:25:35 -0700   Fri, 21 Aug 2015 11:25:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id b5d511de10d5\n  Fri, 21 Aug 2015 11:25:35 -0700   Fri, 21 Aug 2015 11:25:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id b5d511de10d5\n  Fri, 21 Aug 2015 11:25:45 -0700   Fri, 21 Aug 2015 11:25:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 0ce3796c501b\n  Fri, 21 Aug 2015 11:25:45 -0700   Fri, 21 Aug 2015 11:25:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 0ce3796c501b\n  Fri, 21 Aug 2015 11:25:55 -0700   Fri, 21 Aug 2015 11:25:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 471645d5b5e3\n  Fri, 21 Aug 2015 11:25:55 -0700   Fri, 21 Aug 2015 11:25:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 471645d5b5e3\n  Fri, 21 Aug 2015 11:26:05 -0700   Fri, 21 Aug 2015 11:26:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 976946b23c90\n  Fri, 21 Aug 2015 11:26:05 -0700   Fri, 21 Aug 2015 11:26:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 976946b23c90\n  Fri, 21 Aug 2015 11:26:15 -0700   Fri, 21 Aug 2015 11:26:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 1c5ee3d9ef3e\n  Fri, 21 Aug 2015 11:26:15 -0700   Fri, 21 Aug 2015 11:26:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 1c5ee3d9ef3e\n  Fri, 21 Aug 2015 11:26:25 -0700   Fri, 21 Aug 2015 11:26:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id c138d6a50ad0\n  Fri, 21 Aug 2015 11:26:25 -0700   Fri, 21 Aug 2015 11:26:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id c138d6a50ad0\n  Fri, 21 Aug 2015 11:26:35 -0700   Fri, 21 Aug 2015 11:26:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id dc91ca29fb04\n  Fri, 21 Aug 2015 11:26:35 -0700   Fri, 21 Aug 2015 11:26:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id dc91ca29fb04\n  Fri, 21 Aug 2015 11:26:45 -0700   Fri, 21 Aug 2015 11:26:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id aa0d6f2a2273\n  Fri, 21 Aug 2015 11:26:45 -0700   Fri, 21 Aug 2015 11:26:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id aa0d6f2a2273\n  Fri, 21 Aug 2015 11:26:55 -0700   Fri, 21 Aug 2015 11:26:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 87a930e252ee\n  Fri, 21 Aug 2015 11:26:55 -0700   Fri, 21 Aug 2015 11:26:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 87a930e252ee\n  Fri, 21 Aug 2015 11:27:05 -0700   Fri, 21 Aug 2015 11:27:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id c2008b115d29\n  Fri, 21 Aug 2015 11:27:05 -0700   Fri, 21 Aug 2015 11:27:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id c2008b115d29\n  Fri, 21 Aug 2015 11:27:15 -0700   Fri, 21 Aug 2015 11:27:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id fddad6bf983c\n  Fri, 21 Aug 2015 11:27:15 -0700   Fri, 21 Aug 2015 11:27:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id fddad6bf983c\n  Fri, 21 Aug 2015 11:27:25 -0700   Fri, 21 Aug 2015 11:27:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 914c89582045\n  Fri, 21 Aug 2015 11:27:25 -0700   Fri, 21 Aug 2015 11:27:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 914c89582045\n  Fri, 21 Aug 2015 11:27:35 -0700   Fri, 21 Aug 2015 11:27:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id a327d73e029b\n  Fri, 21 Aug 2015 11:27:35 -0700   Fri, 21 Aug 2015 11:27:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id a327d73e029b\n  Fri, 21 Aug 2015 11:27:45 -0700   Fri, 21 Aug 2015 11:27:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 7c838f6b9b0f\n  Fri, 21 Aug 2015 11:27:45 -0700   Fri, 21 Aug 2015 11:27:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 7c838f6b9b0f\n  Fri, 21 Aug 2015 11:27:55 -0700   Fri, 21 Aug 2015 11:27:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 01ea06fb428f\n  Fri, 21 Aug 2015 11:27:55 -0700   Fri, 21 Aug 2015 11:27:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 01ea06fb428f\n  Fri, 21 Aug 2015 11:28:05 -0700   Fri, 21 Aug 2015 11:28:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 32a4b55a894f\n  Fri, 21 Aug 2015 11:28:05 -0700   Fri, 21 Aug 2015 11:28:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 32a4b55a894f\n  Fri, 21 Aug 2015 11:28:15 -0700   Fri, 21 Aug 2015 11:28:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 82cef048b13e\n  Fri, 21 Aug 2015 11:28:15 -0700   Fri, 21 Aug 2015 11:28:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 82cef048b13e\n  Fri, 21 Aug 2015 11:28:25 -0700   Fri, 21 Aug 2015 11:28:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 5b9c1e65ac3f\n  Fri, 21 Aug 2015 11:28:25 -0700   Fri, 21 Aug 2015 11:28:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 5b9c1e65ac3f\n  Fri, 21 Aug 2015 11:28:35 -0700   Fri, 21 Aug 2015 11:28:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 2bbec1177cb9\n  Fri, 21 Aug 2015 11:28:36 -0700   Fri, 21 Aug 2015 11:28:36 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 2bbec1177cb9\n  Fri, 21 Aug 2015 11:28:45 -0700   Fri, 21 Aug 2015 11:28:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 9cdab7be3357\n  Fri, 21 Aug 2015 11:28:45 -0700   Fri, 21 Aug 2015 11:28:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 9cdab7be3357\n  Fri, 21 Aug 2015 11:28:55 -0700   Fri, 21 Aug 2015 11:28:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id aeb1519d8e72\n  Fri, 21 Aug 2015 11:28:55 -0700   Fri, 21 Aug 2015 11:28:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id aeb1519d8e72\n  Fri, 21 Aug 2015 11:29:05 -0700   Fri, 21 Aug 2015 11:29:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id c8befbc8886f\n  Fri, 21 Aug 2015 11:29:05 -0700   Fri, 21 Aug 2015 11:29:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id c8befbc8886f\n  Fri, 21 Aug 2015 11:29:15 -0700   Fri, 21 Aug 2015 11:29:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 20f6face384d\n  Fri, 21 Aug 2015 11:29:15 -0700   Fri, 21 Aug 2015 11:29:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 20f6face384d\n  Fri, 21 Aug 2015 11:29:25 -0700   Fri, 21 Aug 2015 11:29:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id b7bbbd6fa7f6\n  Fri, 21 Aug 2015 11:29:25 -0700   Fri, 21 Aug 2015 11:29:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id b7bbbd6fa7f6\n  Fri, 21 Aug 2015 11:29:35 -0700   Fri, 21 Aug 2015 11:29:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 0ed75fb2e61c\n  Fri, 21 Aug 2015 11:29:35 -0700   Fri, 21 Aug 2015 11:29:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 0ed75fb2e61c\n  Fri, 21 Aug 2015 11:29:45 -0700   Fri, 21 Aug 2015 11:29:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 0bbb35bd6891\n  Fri, 21 Aug 2015 11:29:45 -0700   Fri, 21 Aug 2015 11:29:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 0bbb35bd6891\n  Fri, 21 Aug 2015 11:29:55 -0700   Fri, 21 Aug 2015 11:29:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 9d0311c03d1b\n  Fri, 21 Aug 2015 11:29:55 -0700   Fri, 21 Aug 2015 11:29:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 9d0311c03d1b\n  Fri, 21 Aug 2015 11:30:05 -0700   Fri, 21 Aug 2015 11:30:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 2dc111d54e41\n  Fri, 21 Aug 2015 11:30:05 -0700   Fri, 21 Aug 2015 11:30:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 2dc111d54e41\n  Fri, 21 Aug 2015 11:30:15 -0700   Fri, 21 Aug 2015 11:30:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 73899a8b3dea\n  Fri, 21 Aug 2015 11:30:15 -0700   Fri, 21 Aug 2015 11:30:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 73899a8b3dea\n  Fri, 21 Aug 2015 11:30:25 -0700   Fri, 21 Aug 2015 11:30:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id bb442638913f\n  Fri, 21 Aug 2015 11:30:25 -0700   Fri, 21 Aug 2015 11:30:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id bb442638913f\n  Fri, 21 Aug 2015 11:30:35 -0700   Fri, 21 Aug 2015 11:30:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id f31be698f000\n  Fri, 21 Aug 2015 11:30:35 -0700   Fri, 21 Aug 2015 11:30:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id f31be698f000\n  Fri, 21 Aug 2015 11:30:45 -0700   Fri, 21 Aug 2015 11:30:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 0394091f67c6\n  Fri, 21 Aug 2015 11:30:45 -0700   Fri, 21 Aug 2015 11:30:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 0394091f67c6\n  Fri, 21 Aug 2015 11:30:55 -0700   Fri, 21 Aug 2015 11:30:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 99e50311e2c8\n  Fri, 21 Aug 2015 11:30:55 -0700   Fri, 21 Aug 2015 11:30:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 99e50311e2c8\n  Fri, 21 Aug 2015 11:31:05 -0700   Fri, 21 Aug 2015 11:31:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 4da507c0b225\n  Fri, 21 Aug 2015 11:31:05 -0700   Fri, 21 Aug 2015 11:31:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 4da507c0b225\n  Fri, 21 Aug 2015 11:31:15 -0700   Fri, 21 Aug 2015 11:31:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 3435eb9711f8\n  Fri, 21 Aug 2015 11:31:15 -0700   Fri, 21 Aug 2015 11:31:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 3435eb9711f8\n  Fri, 21 Aug 2015 11:31:25 -0700   Fri, 21 Aug 2015 11:31:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id c5a2a61a4508\n  Fri, 21 Aug 2015 11:31:25 -0700   Fri, 21 Aug 2015 11:31:25 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id c5a2a61a4508\n  Fri, 21 Aug 2015 11:31:35 -0700   Fri, 21 Aug 2015 11:31:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id bf1af945ec89\n  Fri, 21 Aug 2015 11:31:35 -0700   Fri, 21 Aug 2015 11:31:35 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id bf1af945ec89\n  Fri, 21 Aug 2015 11:31:45 -0700   Fri, 21 Aug 2015 11:31:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 99820e6b2b52\n  Fri, 21 Aug 2015 11:31:45 -0700   Fri, 21 Aug 2015 11:31:45 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 99820e6b2b52\n  Fri, 21 Aug 2015 11:31:55 -0700   Fri, 21 Aug 2015 11:31:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 0ea459f7d685\n  Fri, 21 Aug 2015 11:31:55 -0700   Fri, 21 Aug 2015 11:31:55 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 0ea459f7d685\n  Fri, 21 Aug 2015 11:32:05 -0700   Fri, 21 Aug 2015 11:32:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id 4dde3638c3ec\n  Fri, 21 Aug 2015 11:32:05 -0700   Fri, 21 Aug 2015 11:32:05 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id 4dde3638c3ec\n  Fri, 21 Aug 2015 11:32:15 -0700   Fri, 21 Aug 2015 11:32:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   created Created with docker id d2a323dce27e\n  Fri, 21 Aug 2015 11:32:15 -0700   Fri, 21 Aug 2015 11:32:15 -0700 1   {kubelet kubernetes-master} spec.containers{heapster}   started Started with docker id d2a323dce27e\n. @vishh Do you have a guess what might be wrong? I modified config-default.sh to create 2 nodes and heapster was able to start.\n. ",
    "smugcloud": "@vishh I'm having a similar problem as @wonderfly.  My K8S cluster was deployed with the kube-up script, on AWS.  The cluster seems to be working fine, and I can access the Grafana UI, but do not get any heapster data.  I am also seeing a lot of restarts with the Pod.  Looking at the logs gives me:\n$ kubectl logs -p --namespace=kube-system monitoring-heapster-v5-919mw\nI1112 19:47:10.367791       1 heapster.go:52] /heapster --source=kubernetes:'' --sink=influxdb:http://monitoring-influxdb:8086\nI1112 19:47:10.367874       1 heapster.go:53] Heapster version 0.16.0\nI1112 19:47:10.371759       1 kube_factory.go:169] Using Kubernetes client with master \"https://10.0.0.1:443\" and version \"v1\"\nI1112 19:47:10.371787       1 kube_factory.go:170] Using kubelet port 10255\nI1112 19:47:10.398029       1 driver.go:376] created influxdb sink with options: {root root monitoring-influxdb:8086 k8s false}\nI1112 19:47:10.398903       1 heapster.go:64] Starting heapster on port 8082\nE1112 19:49:23.127563       1 driver.go:231] failed to write stats to influxDB - Post http://monitoring-influxdb:8086/db/k8s/series?u=root&p=root&time_precision=s: dial tcp: lookup monitoring-influxdb: no such host\nE1112 19:50:01.464521       1 driver.go:231] failed to write stats to influxDB - Post http://monitoring-influxdb:8086/db/k8s/series?u=root&p=root&time_precision=s: dial tcp: lookup monitoring-influxdb: no such host\nIt looks like @wonderfly and I do not have an influxDB pod running.  Is that the problem?\n$ kubectl get pods --all-namespaces\nNAMESPACE     NAME                                                              READY     STATUS    RESTARTS   AGE\ndefault       mysql                                                             1/1       Running   0          15h\ndefault       wordpress                                                         1/1       Running   0          15h\nkube-system   elasticsearch-logging-v1-bz2yp                                    1/1       Running   0          16h\nkube-system   elasticsearch-logging-v1-r5jio                                    1/1       Running   0          16h\nkube-system   fluentd-elasticsearch-ip-172-20-0-67.us-west-2.compute.internal   1/1       Running   0          16h\nkube-system   fluentd-elasticsearch-ip-172-20-0-68.us-west-2.compute.internal   1/1       Running   0          16h\nkube-system   kibana-logging-v1-elr8u                                           1/1       Running   0          16h\nkube-system   kube-dns-v8-h0uk2                                                 4/4       Running   0          16h\nkube-system   kube-ui-v1-y57xg                                                  1/1       Running   0          16h\nkube-system   monitoring-heapster-v5-919mw                                      1/1       Running   2          16h\nkube-system   monitoring-influx-grafana-v1-e0xvk                                2/2       Running   0          16h\n. Thanks @vishh.  Upon spinning up my cluster, I was inspecting the summary links like @wonderfly.  Grafana renders, but heapster and InfluxDB do not.  I saw #338 and do run into some redirect issues, but I wasn't sure if that was the root cause. \nAre you saying I should stop heapster, and restart it with the definition files you referenced, or can I modify the existing service since the monitoring-heapster service is running?  \nAlso, should I create the influxDB pod?  I'm really trying to understand what's available and how to use it.  I am assuming I Should be able to hit this to view heapster details: https://x.x.x.x/api/v1/proxy/namespaces/kube-system/services/monitoring-heapster\n. Sure, here you go:\n$ kubectl get svc --namespace=kube-system\nNAME                    LABELS                                                                                              SELECTOR                        IP(S)          PORT(S)\nelasticsearch-logging   k8s-app=elasticsearch-logging,kubernetes.io/cluster-service=true,kubernetes.io/name=Elasticsearch   k8s-app=elasticsearch-logging   10.0.253.115   9200/TCP\nkibana-logging          k8s-app=kibana-logging,kubernetes.io/cluster-service=true,kubernetes.io/name=Kibana                 k8s-app=kibana-logging          10.0.110.106   5601/TCP\nkube-dns                k8s-app=kube-dns,kubernetes.io/cluster-service=true,kubernetes.io/name=KubeDNS                      k8s-app=kube-dns                10.0.0.10      53/UDP\n                                                                                                                                                                           53/TCP\nkube-ui                 k8s-app=kube-ui,kubernetes.io/cluster-service=true,kubernetes.io/name=KubeUI                        k8s-app=kube-ui                 10.0.32.40     80/TCP\nmonitoring-grafana      kubernetes.io/cluster-service=true,kubernetes.io/name=Grafana                                       k8s-app=influxGrafana           10.0.160.123   80/TCP\nmonitoring-heapster     kubernetes.io/cluster-service=true,kubernetes.io/name=Heapster                                      k8s-app=heapster                10.0.189.126   80/TCP\nmonitoring-influxdb     kubernetes.io/cluster-service=true,kubernetes.io/name=InfluxDB                                      k8s-app=influxGrafana           10.0.242.132   8083/TCP\n                                                                                                                                                                           8086/TCP\n. @vishh I don't think they're transient as I have not been able to hit the Heapster or InfluxDB URL's since the cluster was created.  If I try to hit the Influx DB UI I get:\n\"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"no endpoints available for \\\"monitoring-influxdb\\\"\",\n  \"code\": 500\nI stopped the service, enabled the logging, and logs on the pod are the same as before:\n$ k logs monitoring-heapster-v5-919mw --namespace=kube-system\nI1112 21:01:36.952166       1 heapster.go:52] /heapster --source=kubernetes:'' --sink=influxdb:http://monitoring-influxdb:8086\nI1112 21:01:36.952300       1 heapster.go:53] Heapster version 0.16.0\nI1112 21:01:36.953833       1 kube_factory.go:169] Using Kubernetes client with master \"https://10.0.0.1:443\" and version \"v1\"\nI1112 21:01:36.954911       1 kube_factory.go:170] Using kubelet port 10255\nI1112 21:01:36.977794       1 driver.go:376] created influxdb sink with options: {root root monitoring-influxdb:8086 k8s false}\nI1112 21:01:36.978631       1 heapster.go:64] Starting heapster on port 8082\nE1112 21:03:32.617864       1 driver.go:231] failed to write stats to influxDB - Post http://monitoring-influxdb:8086/db/k8s/series?u=root&p=root&time_precision=s: dial tcp: lookup monitoring-influxdb: no such host\n. I'm hitting the URL provided in cluster-info: https://x.x.x.x/api/v1/proxy/namespaces/kube-system/services/monitoring-heapster/.  Should I be hitting something else?  \nI'll try upgrading the heapster image and report back.\n. So without upgrading heapster, starting the proxy, and running these curl requests, i get the same response back for both:\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"no endpoints available for \\\"monitoring-influxdb:8086\\\"\",\n  \"code\": 500\n}\n. What's also strange is that if I hit the /validate endpoint for Heapster, I get a (unformatted) response back.  This was generated by hitting: https://master_IP/api/v1/proxy/namespaces/kube-system/services/monitoring-heapster/validate.  Everything appears healthy from this perspective.\n```\nHeapster Version: 0.16.0\nSource type: kube-pod-metrics\n    No pod errors\nSource type: Kube Node Metrics\nKubernetes Nodes plugin: \n    Healthy Nodes:\n        ip-172-20-0-67.us-west-2.compute.internal\n        ip-172-20-0-68.us-west-2.compute.internal\n    No node errors\nSource type: kube-events\nExternal Sinks\n    Exported metrics:\n        uptime: Number of milliseconds since the container was started      cpu/usage: Cumulative CPU usage on all cores        cpu/limit: CPU limit in millicores      memory/usage: Total memory usage        memory/working_set: Total working set usage. Working set is the memory being used and not easily dropped by the kernel      memory/limit: Memory limit      memory/page_faults: Number of page faults       memory/major_page_faults: Number of major page faults       network/rx: Cumulative number of bytes received over the network        network/rx_errors: Cumulative number of errors while receiving over the network     network/tx: Cumulative number of bytes sent over the network        network/tx_errors: Cumulative number of errors while sending over the network       filesystem/usage: Total number of bytes consumed on a filesystem        filesystem/limit: The total size of filesystem in bytes\n    Exported labels:\n        hostname: Hostname where the container ran      host_id: Identifier specific to a host. Set by cloud provider or user       container_name: User-provided name of the container or full container name for system containers        pod_name: The name of the pod       pod_id: The unique ID of the pod        pod_namespace: The namespace of the pod     namespace_id: The UID of namespace of the pod       labels: Comma-separated list of user-provided labels        resource_id: Identifier(s) specific to a metric\n    External Sinks:\n    Sink Type: InfluxDB\nclient: Host \"monitoring-influxdb:8086\", Database \"k8s\"\nNumber of write failures: 0\n\n```\n. Sure thing @vishh, here are the outputs.  I'll try upgrading heapster as well.\n$ kubectl describe pod monitoring-influx-grafana-v1-e0xvk --namespace=kube-system\nName:               monitoring-influx-grafana-v1-e0xvk\nNamespace:          kube-system\nImage(s):           gcr.io/google_containers/heapster_influxdb:v0.3,gcr.io/google_containers/heapster_grafana:v0.7\nNode:               ip-172-20-0-67.us-west-2.compute.internal/172.20.0.67\nLabels:             k8s-app=influxGrafana,kubernetes.io/cluster-service=true,version=v1\nStatus:             Running\nReason:\nMessage:\nIP:             10.244.1.6\nReplication Controllers:    monitoring-influx-grafana-v1 (1/1 replicas created)\nContainers:\n  influxdb:\n    Image:  gcr.io/google_containers/heapster_influxdb:v0.3\n    Limits:\n      cpu:      100m\n      memory:       200Mi\n    State:      Running\n      Started:      Wed, 11 Nov 2015 19:58:56 -0800\n    Ready:      True\n    Restart Count:  0\n  grafana:\n    Image:  gcr.io/google_containers/heapster_grafana:v0.7\n    Limits:\n      cpu:      100m\n      memory:       100Mi\n    State:      Running\n      Started:      Wed, 11 Nov 2015 19:59:17 -0800\n    Ready:      True\n    Restart Count:  0\nConditions:\n  Type      Status\n  Ready     True\nNo events.\n$ kubectl get endpoints --all-namespaces\nNAMESPACE     NAME                    ENDPOINTS\ndefault       kubernetes              172.20.0.9:443\ndefault       wordpress-frontend      10.244.1.10:80\ndefault       wordpress-mysql         10.244.1.7:3306\nkube-system   elasticsearch-logging   10.244.0.3:9200,10.244.1.4:9200\nkube-system   kibana-logging          10.244.1.3:5601\nkube-system   kube-dns                10.244.0.4:53,10.244.0.4:53\nkube-system   kube-ui                 10.244.0.5:8080\nkube-system   monitoring-grafana      10.244.1.6:8080\nkube-system   monitoring-heapster     10.244.1.5:8082\nkube-system   monitoring-influxdb     10.244.1.6:8083,10.244.1.6:8086\n$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"0\", GitVersion:\"v1.0.3\", GitCommit:\"61c6ac5f350253a4dc002aee97b7db7ff01ee4ca\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"0+\", GitVersion:\"v1.0.0-259-gf3b752d831708c\", GitCommit:\"f3b752d831708ccdad564bd188abb11822c51595\", GitTreeState:\"clean\"}\n. Thanks again for looking at this @vishh.  Unfortunately, I'm still having the same issue, with some slight differences in the ancillary components.\n1. I updated K8S to 1.1.1 across the board.\n2.  I used the kube-up.sh script to provision a cluster on AWS with 2 minions\n3. Everything appeared to be up at cluster creation but upon hitting the Grafana UI, I saw some errors pointing to an InfluxDB issue (below)\n4. Looking into this further, the Heapster RC was never created.  I manually created the RC using this manifest  (with the memory hardcoded to a few values, latest was 2000Mi).  It appears to run for a few minutes and then both the RC and pod are gone\n5. The main issue pops up in the grafana logs and those are basically the errors I'm seeing in the UI.\n6. Here are the URI's I'm trying to hit for InfluxDB, and Heapster, respectively: https://MASTERIP/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb, https://MASTERIP/api/v1/proxy/namespaces/kube-system/services/heapster\n7. The heapster URI successfully loads for a few minutes but all I see is the Temporary Redirect pointing to https://MASTERIP/validate/.  If I append /validate to the above heapster URI, I get the expected response.  After about 5 minutes, the RC and pod are gone and I get a 503 for all of those URI's\n8. The Influx URI just returns a 503 that it's unavailable during all of this\n9. So something seems off and the Heapster RC/pod won't stay alive.  I tried adding the extra logging when starting the RC but don't get anything valuable in the logs.\nAny ideas?\n$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"1\", GitVersion:\"v1.1.1\", GitCommit:\"92635e23dfafb2ddc828c8ac6c03c7a7205a84d8\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"1\", GitVersion:\"v1.1.1\", GitCommit:\"92635e23dfafb2ddc828c8ac6c03c7a7205a84d8\", GitTreeState:\"clean\"}\n$ k get pods,rc,svc --all-namespaces\nNAMESPACE     NAME                                                              READY     STATUS    RESTARTS   AGE\nkube-system   elasticsearch-logging-v1-d86vl                                    1/1       Running   0          49m\nkube-system   elasticsearch-logging-v1-ffpoi                                    1/1       Running   0          49m\nkube-system   fluentd-elasticsearch-ip-172-20-0-44.us-west-2.compute.internal   1/1       Running   0          46m\nkube-system   fluentd-elasticsearch-ip-172-20-0-45.us-west-2.compute.internal   1/1       Running   0          46m\nkube-system   heapster-v10-tgqqh                                                1/1       Running   0          1m\nkube-system   kibana-logging-v1-dham9                                           1/1       Running   2          49m\nkube-system   kube-dns-v9-tnko4                                                 4/4       Running   0          49m\nkube-system   kube-ui-v2-n9837                                                  1/1       Running   0          49m\nkube-system   monitoring-influxdb-grafana-v2-n6jx1                              2/2       Running   0          49m\nNAMESPACE     CONTROLLER                       CONTAINER(S)            IMAGE(S)                                                SELECTOR                                   REPLICAS   AGE\nkube-system   elasticsearch-logging-v1         elasticsearch-logging   gcr.io/google_containers/elasticsearch:1.7              k8s-app=elasticsearch-logging,version=v1   2          49m\nkube-system   heapster-v10                     heapster                gcr.io/google_containers/heapster:v0.18.2               k8s-app=heapster,version=v10               1          1m\nkube-system   kibana-logging-v1                kibana-logging          gcr.io/google_containers/kibana:1.3                     k8s-app=kibana-logging,version=v1          1          49m\nkube-system   kube-dns-v9                      etcd                    gcr.io/google_containers/etcd:2.0.9                     k8s-app=kube-dns,version=v9                1          49m\n                                               kube2sky                gcr.io/google_containers/kube2sky:1.11\n                                               skydns                  gcr.io/google_containers/skydns:2015-10-13-8c72f8c\n                                               healthz                 gcr.io/google_containers/exechealthz:1.0\nkube-system   kube-ui-v2                       kube-ui                 gcr.io/google_containers/kube-ui:v2                     k8s-app=kube-ui,version=v2                 1         49m\nkube-system   monitoring-influxdb-grafana-v2   influxdb                gcr.io/google_containers/heapster_influxdb:v0.4         k8s-app=influxGrafana,version=v2           1         49m\n                                               grafana                 beta.gcr.io/google_containers/heapster_grafana:v2.1.1\nNAMESPACE     NAME                    CLUSTER_IP     EXTERNAL_IP   PORT(S)             SELECTOR                        AGE\ndefault       kubernetes              10.0.0.1       <none>        443/TCP             <none>                          50m\nkube-system   elasticsearch-logging   10.0.24.121    <none>        9200/TCP            k8s-app=elasticsearch-logging   49m\nkube-system   heapster                10.0.129.241   <none>        80/TCP              k8s-app=heapster                49m\nkube-system   kibana-logging          10.0.252.56    <none>        5601/TCP            k8s-app=kibana-logging          49m\nkube-system   kube-dns                10.0.0.10      <none>        53/UDP,53/TCP       k8s-app=kube-dns                49m\nkube-system   kube-ui                 10.0.164.47    <none>        80/TCP              k8s-app=kube-ui                 49m\nkube-system   monitoring-grafana      10.0.104.179   <none>        80/TCP              k8s-app=influxGrafana           49m\nkube-system   monitoring-influxdb     10.0.35.241    <none>        8083/TCP,8086/TCP   k8s-app=influxGrafana           49m\n```\n$ k logs monitoring-influxdb-grafana-v2-n6jx1 -c influxdb --namespace=kube-system\n[11/13/15 16:22:55] [INFO] Loading configuration file /config/config.toml\n+---------------------------------------------+\n|  _  _            _    |\n| | |      / _| |          |   \\|  _ \\  |\n|   | |  _  | || |  | |  | | |) | |\n|   | | | ' \\|  | | | | \\ \\/ / |  | |  _ <  |\n|  | || | | | | | | || |>  <| || | |_) | |\n| |_|| ||| ||_,/_/__/|____/  |\n+---------------------------------------------+\n```\n$ k logs -f heapster-v10-wewa2 --namespace=kube-system\nI1113 17:32:44.355661       1 heapster.go:55] /heapster --source=kubernetes:''\nI1113 17:32:44.355748       1 heapster.go:56] Heapster version 0.18.2\nI1113 17:32:44.357093       1 kube_factory.go:169] Using Kubernetes client with master \"https://10.0.0.1:443\" and version \"v1\"\nI1113 17:32:44.357110       1 kube_factory.go:170] Using kubelet port 10255\nI1113 17:32:44.364532       1 heapster.go:66] Starting heapster on port 8082\n```\n$ k logs monitoring-influxdb-grafana-v2-n6jx1 -c grafana --namespace=kube-system\nInfluxdb service URL is provided.\nUsing the following URL for InfluxDB: http://monitoring-influxdb:8086\nUsing the following backend access mode for InfluxDB: proxy\nStarting Grafana in the background\nWaiting for Grafana to come up...\n2015/11/13 16:22:35 [I] Starting Grafana\n2015/11/13 16:22:35 [I] Version: 2.1.0, Commit: v2.1.0, Build date: 2015-08-04 14:19:48 +0000 UTC\n2015/11/13 16:22:35 [I] Configuration Info\nConfig files:\nCommand lines overrides:\nEnvironment variables used:\n\nPaths:\n  home: /usr/share/grafana\n  data: /var/lib/grafana\n  logs: /var/log/grafana\n2015/11/13 16:22:35 [I] Database: sqlite3, ConnectionString: file:/var/lib/grafana/grafana.db?cache=shared&mode=rwc&_loc=Local\n2015/11/13 16:22:35 [I] Migrator: Starting DB migration\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create migration_log table\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create user table\n2015/11/13 16:22:35 [I] Migrator: exec migration id: add unique index user.login\n2015/11/13 16:22:35 [I] Migrator: exec migration id: add unique index user.email\n2015/11/13 16:22:35 [I] Migrator: exec migration id: drop index UQE_user_login - v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: drop index UQE_user_email - v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: Rename table user to user_v1 - v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create user table v2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create index UQE_user_login - v2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create index UQE_user_email - v2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: copy data_source v1 to v2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: Drop old table user_v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create star table\n2015/11/13 16:22:35 [I] Migrator: exec migration id: add unique index star.user_id_dashboard_id\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create org table v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create index UQE_org_name - v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create org_user table v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create index IDX_org_user_org_id - v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create index UQE_org_user_org_id_user_id - v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: copy data account to org\n2015/11/13 16:22:35 [I] Migrator: skipping migration id: copy data account to org, condition not fulfilled\n2015/11/13 16:22:35 [I] Migrator: exec migration id: copy data account_user to org_user\n2015/11/13 16:22:35 [I] Migrator: skipping migration id: copy data account_user to org_user, condition not fulfilled\n2015/11/13 16:22:35 [I] Migrator: exec migration id: Drop old table account\n2015/11/13 16:22:35 [I] Migrator: exec migration id: Drop old table account_user\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create dashboard table\n2015/11/13 16:22:35 [I] Migrator: exec migration id: add index dashboard.account_id\n2015/11/13 16:22:35 [I] Migrator: exec migration id: add unique index dashboard_account_id_slug\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create dashboard_tag table\n2015/11/13 16:22:35 [I] Migrator: exec migration id: add unique index dashboard_tag.dasboard_id_term\n2015/11/13 16:22:35 [I] Migrator: exec migration id: drop index UQE_dashboard_tag_dashboard_id_term - v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: Rename table dashboard to dashboard_v1 - v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create dashboard v2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create index IDX_dashboard_org_id - v2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create index UQE_dashboard_org_id_slug - v2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: copy dashboard v1 to v2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: drop table dashboard_v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: alter dashboard.data to mediumtext v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create data_source table\n2015/11/13 16:22:35 [I] Migrator: exec migration id: add index data_source.account_id\n2015/11/13 16:22:35 [I] Migrator: exec migration id: add unique index data_source.account_id_name\n2015/11/13 16:22:35 [I] Migrator: exec migration id: drop index IDX_data_source_account_id - v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: drop index UQE_data_source_account_id_name - v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: Rename table data_source to data_source_v1 - v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create data_source table v2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create index IDX_data_source_org_id - v2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create index UQE_data_source_org_id_name - v2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: copy data_source v1 to v2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: Drop old table data_source_v1 #2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create api_key table\n2015/11/13 16:22:35 [I] Migrator: exec migration id: add index api_key.account_id\n2015/11/13 16:22:35 [I] Migrator: exec migration id: add index api_key.key\n2015/11/13 16:22:35 [I] Migrator: exec migration id: add index api_key.account_id_name\n2015/11/13 16:22:35 [I] Migrator: exec migration id: drop index IDX_api_key_account_id - v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: drop index UQE_api_key_key - v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: drop index UQE_api_key_account_id_name - v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: Rename table api_key to api_key_v1 - v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create api_key table v2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create index IDX_api_key_org_id - v2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create index UQE_api_key_key - v2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create index UQE_api_key_org_id_name - v2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: copy api_key v1 to v2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: Drop old table api_key_v1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create dashboard_snapshot table v4\n2015/11/13 16:22:35 [I] Migrator: exec migration id: drop table dashboard_snapshot_v4 #1\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create dashboard_snapshot table v5 #2\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create index UQE_dashboard_snapshot_key - v5\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create index UQE_dashboard_snapshot_delete_key - v5\n2015/11/13 16:22:35 [I] Migrator: exec migration id: create index IDX_dashboard_snapshot_user_id - v5\n2015/11/13 16:22:35 [I] Migrator: exec migration id: alter dashboard_snapshot to mediumtext v2\n2015/11/13 16:22:35 [I] Created default admin user: admin\n2015/11/13 16:22:35 [I] Listen: http://0.0.0.0:3000/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana\n.Grafana is up and running.\nCreating default influxdb datasource...\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=UTF-8\nSet-Cookie: grafana_sess=537895ba5bb655ce; Path=/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana; HttpOnly\nDate: Fri, 13 Nov 2015 16:22:37 GMT\nContent-Length: 37\n100   246  100    37  100   209   7745  43751 --:--:-- --:--:-- --:--:-- 52250\n{\"id\":1,\"message\":\"Datasource added\"}\nImporting default dashboards...\nImporting /dashboards/cluster.json ...\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 26362  100    60  100 26302   1690   723k --:--:-- --:--:-- --:--:--  733k\nHTTP/1.1 100 Continue\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=UTF-8\nSet-Cookie: grafana_sess=bb21943917db0397; Path=/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana; HttpOnly\nDate: Fri, 13 Nov 2015 16:22:37 GMT\nContent-Length: 60\n{\"slug\":\"kubernetes-cluster\",\"status\":\"success\",\"version\":0}\nDone importing /dashboards/cluster.json\nImporting /dashboards/containers.json ...\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  7342  100    52  100  7290   8453  1157k --:--:-- --:--:-- --:--:-- 1423k\nHTTP/1.1 100 Continue\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=UTF-8\nSet-Cookie: grafana_sess=36b2c0729f404270; Path=/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana; HttpOnly\nDate: Fri, 13 Nov 2015 16:22:37 GMT\nContent-Length: 52\n{\"slug\":\"containers\",\"status\":\"success\",\"version\":0}\nDone importing /dashboards/containers.json\nBringing Grafana back to the foreground\nexec /usr/sbin/grafana-server --config=/etc/grafana/grafana.ini cfg:default.paths.data=/var/lib/grafana cfg:default.paths.logs=/var/log/grafana\n2015/11/13 16:32:27 [I] Completed /db/k8s/series 400 Bad Request in 112.981582ms\n2015/11/13 16:32:27 [I] Completed /db/k8s/series 400 Bad Request in 6.372123ms\n2015/11/13 16:32:27 [I] Completed /db/k8s/series 400 Bad Request in 1.492953ms\n2015/11/13 16:32:27 [I] Completed /db/k8s/series 400 Bad Request in 3.062731ms\n2015/11/13 16:32:27 [I] Completed /db/k8s/series 400 Bad Request in 1.909597ms\n2015/11/13 16:32:27 [I] Completed /db/k8s/series 400 Bad Request in 2.731184ms\n2015/11/13 16:34:58 [I] Completed /db/k8s/series 400 Bad Request in 5.344887ms\n2015/11/13 16:34:59 [I] Completed /db/k8s/series 400 Bad Request in 2.85325ms\n2015/11/13 16:34:59 [I] Completed /db/k8s/series 400 Bad Request in 1.58361ms\n2015/11/13 16:34:59 [I] Completed /db/k8s/series 400 Bad Request in 3.314549ms\n2015/11/13 16:34:59 [I] Completed /db/k8s/series 400 Bad Request in 1.468141ms\n2015/11/13 16:34:59 [I] Completed /db/k8s/series 400 Bad Request in 8.545558ms\n2015/11/13 16:34:59 [I] Completed /db/k8s/series 400 Bad Request in 13.363428ms\n2015/11/13 16:34:59 [I] Completed /db/k8s/series 400 Bad Request in 9.343128ms\n2015/11/13 16:34:59 [I] Completed /db/k8s/series 400 Bad Request in 6.037474ms\n2015/11/13 16:34:59 [I] Completed /db/k8s/series 400 Bad Request in 1.841557ms\n2015/11/13 16:34:59 [I] Completed /db/k8s/series 400 Bad Request in 3.038935ms\n2015/11/13 16:34:59 [I] Completed /db/k8s/series 400 Bad Request in 2.91062ms\n2015/11/13 16:34:59 [I] Completed /db/k8s/series 400 Bad Request in 3.057221ms\n2015/11/13 16:34:59 [I] Completed /db/k8s/series 400 Bad Request in 1.676242ms\n2015/11/13 16:34:59 [I] Completed /db/k8s/series 400 Bad Request in 2.82611ms\n2015/11/13 16:51:11 [I] Completed /db/k8s/series 400 Bad Request in 5.54743ms\n2015/11/13 16:51:12 [I] Completed /db/k8s/series 400 Bad Request in 2.373784ms\n2015/11/13 16:51:12 [I] Completed /db/k8s/series 400 Bad Request in 2.29305ms\n2015/11/13 16:51:12 [I] Completed /db/k8s/series 400 Bad Request in 2.359016ms\n2015/11/13 16:51:12 [I] Completed /db/k8s/series 400 Bad Request in 1.476427ms\n2015/11/13 16:51:12 [I] Completed /db/k8s/series 400 Bad Request in 2.989776ms\n2015/11/13 16:51:12 [I] Completed /db/k8s/series 400 Bad Request in 1.654767ms\n2015/11/13 16:51:12 [I] Completed /db/k8s/series 400 Bad Request in 7.712052ms\n2015/11/13 16:51:12 [I] Completed /db/k8s/series 400 Bad Request in 4.644701ms\n2015/11/13 16:51:12 [I] Completed /db/k8s/series 400 Bad Request in 10.399463ms\n2015/11/13 16:51:12 [I] Completed /db/k8s/series 400 Bad Request in 2.787593ms\n2015/11/13 16:51:12 [I] Completed /db/k8s/series 400 Bad Request in 3.019632ms\n2015/11/13 16:51:12 [I] Completed /db/k8s/series 400 Bad Request in 1.681945ms\n2015/11/13 16:51:12 [I] Completed /db/k8s/series 400 Bad Request in 3.740655ms\n2015/11/13 16:51:12 [I] Completed /db/k8s/series 400 Bad Request in 3.015398ms\n2015/11/13 16:51:29 [I] Completed /db/k8s/series 400 Bad Request in 8.97373ms\n2015/11/13 16:51:29 [I] Completed /db/k8s/series 400 Bad Request in 8.91191ms\n2015/11/13 16:51:29 [I] Completed /db/k8s/series 400 Bad Request in 10.646566ms\n2015/11/13 16:51:29 [I] Completed /db/k8s/series 400 Bad Request in 2.4448ms\n2015/11/13 16:51:29 [I] Completed /db/k8s/series 400 Bad Request in 4.658329ms\n2015/11/13 16:51:29 [I] Completed /db/k8s/series 400 Bad Request in 3.716319ms\n```\n. ",
    "njuicsgz": "354\n. good to me.\n. ",
    "jekyang": "Hi, vishh\nI've tried again with --cache_duration=1m, it would still crash with same message.There are running 15 nodes in our k8s cluster, with more than 500 pods. Today, I've also noticed that the influxdb would crashed with \"fatal error: runtime: cannot allocate memory\".\nhere's my setting parameters for heapster and influxdb:\n/opt/bin/heapster --source=kubernetes:http://xx.xx.xx.xx:8080 --sink=\"influxdb:http://127.0.0.1:19186?user=root&pw=root&db=k8s\" --cache_duration=1m\ndocker run -i -t -p 19183:8083 -p 19186:8086 --name influxdbtest -v /data/:/data/ kubernetes/heapster_influxdb:v0.3\nThere are both running in one machine together. And my machine has 2 cpu and 4G memory. I'm not sure whether there's other config about memory that mightbe cause the crash, Or whether should I increase our machine memory?\nCrash log from influxdb is below, crash log for heapster is same with 1st comments.\n=> About to create the following database: k8s;grafana\n=> Starting InfluxDB ...\n[06/18/15 09:46:16] [INFO] Loading configuration file /config/config.toml\n+---------------------------------------------+\n|  _  _            _    |\n| | |      / _| |          |   |  _ \\  |\n|   | |  _  | || |  | |  | | |) | |\n|   | | | ' |  | | | | \\ \\/ / |  | |  _ <  |\n|  | || | | | | | | || |>  <| || | |) | |\n| |_|| ||| ||,/_/__/|/  |\n+---------------------------------------------+\n=> Waiting for confirmation of InfluxDB service startup ...\n=> Waiting for confirmation of InfluxDB service startup ...\n{\"status\":\"ok\"}\n=> Creating database: k8s\n=> Creating database: grafana\nexec /usr/bin/influxdb -config=${CONFIG_FILE}\nfatal error: runtime: cannot allocate memory\nruntime stack:\nruntime.gothrow(0xd4f790, 0x1f)\n        /root/.gvm/gos/go1.4/src/runtime/panic.go:503 +0x8e fp=0x7fff7f126780 sp=0x7fff7f126768\nruntime.persistentalloc(0x1000, 0x40, 0x14e1f38, 0x0)\n        /root/.gvm/gos/go1.4/src/runtime/malloc.go:824 +0x18b fp=0x7fff7f1267a8 sp=0x7fff7f126780\ngetempty(0x7f2f10630000, 0x7f2f10630000)\n        /root/.gvm/gos/go1.4/src/runtime/mgc0.c:578 +0xc6 fp=0x7fff7f1267d8 sp=0x7fff7f1267a8\nscanblock(0xc26b508000, 0x1338000, 0x0)\n        /root/.gvm/gos/go1.4/src/runtime/mgc0.c:454 +0x837 fp=0x7fff7f126918 sp=0x7fff7f1267d8\nscanframe(0x7fff7f126a20, 0x0, 0x301)\n        /root/.gvm/gos/go1.4/src/runtime/mgc0.c:719 +0x164 fp=0x7fff7f126988 sp=0x7fff7f126918\nruntime.gentraceback(0x4f36e0, 0xc258642bc0, 0x0, 0xc262b93440, 0x0, 0x0, 0x7fffffff, 0x7fff7f126ad0, 0x0, 0x0, ...)\n        /root/.gvm/gos/go1.4/src/runtime/traceback.go:311 +0x7a8 fp=0x7fff7f126a78 sp=0x7fff7f126988\nscanstack(0xc262b93440)\n        /root/.gvm/gos/go1.4/src/runtime/mgc0.c:777 +0x21c fp=0x7fff7f126ae8 sp=0x7fff7f126a78\nmarkroot(0xc208010000, 0x32)\n        /root/.gvm/gos/go1.4/src/runtime/mgc0.c:553 +0xe7 fp=0x7fff7f126b48 sp=0x7fff7f126ae8\nruntime.parfordo(0xc208010000)\n        /root/.gvm/gos/go1.4/src/runtime/parfor.c:91 +0x13b fp=0x7fff7f126bc8 sp=0x7fff7f126b48\nruntime.gchelper()\n        /root/.gvm/gos/go1.4/src/runtime/mgc0.c:1185 +0x4a fp=0x7fff7f126bf0 sp=0x7fff7f126bc8\nstopm()\n        /root/.gvm/gos/go1.4/src/runtime/proc.c:1181 +0x158 fp=0x7fff7f126c10 sp=0x7fff7f126bf0\ngcstopm()\n        /root/.gvm/gos/go1.4/src/runtime/proc.c:1351 +0xed fp=0x7fff7f126c38 sp=0x7fff7f126c10\nschedule()\n        /root/.gvm/gos/go1.4/src/runtime/proc.c:1551 +0x9b fp=0x7fff7f126c68 sp=0x7fff7f126c38\nruntime.gosched_m(0xc24e60a480)\n        /root/.gvm/gos/go1.4/src/runtime/proc.c:1674 +0xac fp=0x7fff7f126c88 sp=0x7fff7f126c68\nruntime.newstack()\n        /root/.gvm/gos/go1.4/src/runtime/stack.c:776 +0x4fa fp=0x7fff7f126d38 sp=0x7fff7f126c88\nruntime.morestack()\n        /root/.gvm/gos/go1.4/src/runtime/asm_amd64.s:324 +0x7e fp=0x7fff7f126d40 sp=0x7fff7f126d38\ngoroutine 1 [runnable]:\nruntime.makeslice(0xb05c80, 0x1000, 0x1000, 0x0, 0xffffffffffffff00, 0x710bdb)\n        /root/.gvm/gos/go1.4/src/runtime/slice.go:18 fp=0xc20815d618 sp=0xc20815d610\nnet/http.newBufioWriterSize(0x7f2f2485db60, 0xc209ed6b40, 0x1000, 0xc20815d710)\n        /root/.gvm/gos/go1.4/src/net/http/server.go:488 +0x1a2 fp=0xc20815d6b0 sp=0xc20815d618\nnet/http.(_Server).newConn(0xc208058420, 0x7f2f2485d870, 0xc21520b3e8, 0xc209ed6b40, 0x0, 0x0)\n        /root/.gvm/gos/go1.4/src/net/http/server.go:444 +0x433 fp=0xc20815d790 sp=0xc20815d6b0\nnet/http.(_Server).Serve(0xc208058420, 0x7f2f2485be30, 0xc20817a038, 0x0, 0x0)\n        /root/.gvm/gos/go1.4/src/net/http/server.go:1746 +0x2f3 fp=0xc20815d868 sp=0xc20815d790\ngithub.com/influxdb/influxdb/api/http.(_HttpServer).serveListener(0xc20803e420, 0x7f2f2485be30, 0xc20817a038, 0xc20802c9a0)\n        /root/go/src/github.com/influxdb/influxdb/api/http/api.go:202 +0xd8 fp=0xc20815d8d8 sp=0xc20815d868\ngithub.com/influxdb/influxdb/api/http.(_HttpServer).Serve(0xc20803e420, 0x7f2f2485be30, 0xc20817a038)\n        /root/go/src/github.com/influxdb/influxdb/api/http/api.go:172 +0x1219 fp=0xc20815d928 sp=0xc20815d8d8\ngithub.com/influxdb/influxdb/api/http.(_HttpServer).ListenAndServe(0xc20803e420)\n        /root/go/src/github.com/influxdb/influxdb/api/http/api.go:91 +0x207 fp=0xc20815d9d8 sp=0xc20815d928\ngithub.com/influxdb/influxdb/server.(_Server).ListenAndServe(0xc208094090, 0x0, 0x0)\n        /root/go/src/github.com/influxdb/influxdb/server/server.go:217 +0xbac fp=0xc20815dcb8 sp=0xc20815d9d8\nmain.start(0x0, 0x0)\n        /root/go/src/github.com/influxdb/influxdb/daemon/influxd.go:202 +0x127a fp=0xc20815df80 sp=0xc20815dcb8\nmain.main()\n        /root/go/src/github.com/influxdb/influxdb/daemon/influxd.go:73 +0x1f fp=0xc20815df98 sp=0xc20815df80\nruntime.main()\n        /root/.gvm/gos/go1.4/src/runtime/proc.go:63 +0xf3 fp=0xc20815dfe0 sp=0xc20815df98\nruntime.goexit()\n        /root/.gvm/gos/go1.4/src/runtime/asm_amd64.s:2232 +0x1 fp=0xc20815dfe8 sp=0xc20815dfe0\nThanks\n-Jack\n. Hi, Vishh\n  we've already placed limits on all pods in the k8s cluster during running heapster. Today, we put the heapster and influxdb into a standalone machine with 4cpu, 19G mem(including 4G swap mem). In the Machine,there no other program execpt for that both, after some time, we saw that, the total 19G memory has almost run out.\nfree -h\n                                  total       used       free     shared    buffers     cached\nMem:           15G        15G       145M         8K       520K        18M\n-/+ buffers/cache:       15G       164M\nSwap:          4.0G       2.9G       1.1G\ntop:(4CPU, 16Gmem + 4G swap)\n  PID USER      PR   NI    VIRT    RES       SHR S   %CPU  %MEM     TIME+    COMMAND\n 1427 root        20   0   10.046g  8.851g   2044 R  264.0   56.5         254:56.75 influxdb\n 1475 root        20   0   9952.9m 6.393g   4196 S   40.2    40.8         16:25.13 heapster\nfortunately, it didn't crash,but is it normal that both heapster and influxdb consuming such large number of memory? is there any way we can reduce the memory consumed?\nour cluster has 15 nodes and more than 500 pods there.\nheapster cmd:\n/opt/bin/heapster --source=kubernetes:http://xx.xx.xx.xx:8080 --sink=\"influxdb:http://127.0.0.1:8086/?user=root&pw=root&db=k8s\" --cache_duration=1s --poll_duration=1m  --logtostderr=true --log_dir=/opt/bin/ --v=6\nThanks\n-Jack\n. Hi, Vmarmol\nthe png file is attached here.\n\nThanks\n-Jack\n. hi, any suggestion or update?\n. ",
    "yoanisgil": "Yes I am totally in for it. I will look into swarm API to see if there is something better than doing a swarm list and parsing the ouput ;). Having cAdvison running on all nodes should not be difficult with Swarm if we know how many nodes there are and we could probably use filters/affinity to target all nodes (not sure though)\n. Hi @vishh \nI think it might be a good idea to update the sources documentation with an example of what the hosts files should look like for cAdvisor. Initially I though it was a host per file but I was so wrong ;). After reading a bit I cam with this:\njson\n{\n  \"Items\": [\n    {\n      \"Name\": \"N0\",\n      \"IP\": \"192.168.99.105\"\n    },\n    {\n      \"Name\": \"N0\",\n      \"IP\": \"192.168.99.105\"\n    }\n  ]\n}\nLet me know if you want me to create a PR for this ;)\nCan you please let me know how heapster will work for those two sources above? When I go to InfluxDB and run list series it seems like metrics are global to the cluster and not specific to each individual host. Is that about right? Finally I think we should put together a docker-compose file which can help people up and running in no time, which is specially useful if you're a dev ;)\nBests,\nYoanis.\n. Ok. I will try to send the PR today. In which directory should the docker-composer file should live?\n. @vishh doc PR is https://github.com/GoogleCloudPlatform/heapster/pull/373. Will create another for docker-compose.\n. @vishh docker-compose pr is https://github.com/GoogleCloudPlatform/heapster/pull/374.\n. @vishh I am having a git of a hard time setting up my dev environment and I cannot seem to find any documentation describing how to do it. I am on OSX Yosemite btw. Any ideas? This is what I get when I try to build heapster:\n```\n$ godep restore\n$ go build .\ngithub.com/GoogleCloudPlatform/heapster/sinks/gcl\n../../../go/src/github.com/GoogleCloudPlatform/heapster/sinks/gcl/driver.go:279: undefined: extpoints.SinkFactories\ngithub.com/GoogleCloudPlatform/heapster/sinks/gcm\n../../../go/src/github.com/GoogleCloudPlatform/heapster/sinks/gcm/driver.go:87: undefined: extpoints.SinkFactories\ngithub.com/GoogleCloudPlatform/heapster/sinks/hawkular\n../../../go/src/github.com/GoogleCloudPlatform/heapster/sinks/hawkular/driver.go:248: undefined: extpoints.SinkFactories\ngithub.com/GoogleCloudPlatform/heapster/sinks/influxdb\n../../../go/src/github.com/GoogleCloudPlatform/heapster/sinks/influxdb/driver.go:37: undefined: client.Series\n../../../go/src/github.com/GoogleCloudPlatform/heapster/sinks/influxdb/driver.go:37: undefined: client.TimePrecision\n../../../go/src/github.com/GoogleCloudPlatform/heapster/sinks/influxdb/driver.go:73: undefined: client.Series\n../../../go/src/github.com/GoogleCloudPlatform/heapster/sinks/influxdb/driver.go:119: undefined: client.Series\n../../../go/src/github.com/GoogleCloudPlatform/heapster/sinks/influxdb/driver.go:140: undefined: client.Millisecond\n../../../go/src/github.com/GoogleCloudPlatform/heapster/sinks/influxdb/driver.go:151: undefined: client.Series\n../../../go/src/github.com/GoogleCloudPlatform/heapster/sinks/influxdb/driver.go:174: undefined: client.Series\n../../../go/src/github.com/GoogleCloudPlatform/heapster/sinks/influxdb/driver.go:187: undefined: client.Series\n../../../go/src/github.com/GoogleCloudPlatform/heapster/sinks/influxdb/driver.go:214: undefined: client.Series\n../../../go/src/github.com/GoogleCloudPlatform/heapster/sinks/influxdb/driver.go:248: undefined: client.Series\n../../../go/src/github.com/GoogleCloudPlatform/heapster/sinks/influxdb/driver.go:214: too many errors\ngithub.com/GoogleCloudPlatform/heapster/sources\n../../../go/src/github.com/GoogleCloudPlatform/heapster/sources/cadvisor.go:49: undefined: extpoints.SourceFactories\n../../../go/src/github.com/GoogleCloudPlatform/heapster/sources/kube_factory.go:45: undefined: extpoints.SourceFactories\n```\n. I signed it!\n. Actually I think we can improve a bit more the compose file and link to\ncadvisor container. This way the hosts file will point to a working host.\nWhat do you think ?\nLe vendredi 26 juin 2015, Vish Kannan notifications@github.com a \u00e9crit :\n\nok to test\nOn Thu, Jun 25, 2015 at 8:00 PM, cadvisorJenkinsBot \nnotifications@github.com\n<javascript:_e(%7B%7D,'cvml','notifications@github.com');> wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/GoogleCloudPlatform/heapster/pull/374#issuecomment-115481018\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/374#issuecomment-115722798\n.\n. Ok. Please do not merge until I get cAdvisor service integrated and fix the port issue ;)\n. @vishh  there you are. We now have a fully functional working example with docker-compose. Users just need fire docker-compose up and they will be all set. I think we should link this somewhere in the docs so people know that this thing exists and can benefit from it.\n. @vishh Where can I check the output from e2e-gce? I would like to know why the tests are failing but when I click here:\n\nhttp://104.154.52.74/job/heapster-e2e-gce/1453/\nI get a 404. Any ideas?\n. Travis CI build is not passing.\n. @mboussaa there is some work in process for this:\nhttps://github.com/GoogleCloudPlatform/heapster/pull/374\nyou will need to install Docker/Docker Compose but I think it's worth the price. This PR is not merged yet but you can use my branch here:\nhttps://github.com/yoanisgil/heapster/tree/docker-compose-example\nand here is some documentation:\nhttps://github.com/yoanisgil/heapster/tree/docker-compose-example/deploy/docker-compose\nLet me know if you run into any issues ;).\n. Yes. I will fix this alone with adding a cadvisor service in the compose\nfile\nLe vendredi 26 juin 2015, Vish Kannan notifications@github.com a \u00e9crit :\n\nIn deploy/docker-compose/docker-compose.yml\nhttps://github.com/GoogleCloudPlatform/heapster/pull/374#discussion_r33380496\n:\n\n@@ -0,0 +1,18 @@\n+heapster:\n-    image: kubernetes/heapster:v0.14.3\n-    ports:\n-        - \"8002:8002\"\n\nThis should be 8082 I think.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/GoogleCloudPlatform/heapster/pull/374/files#r33380496\n.\n. \n",
    "fiunchinho": "Any updates?\n. ",
    "akash010": "Thanks for the reply !\nAssuming node means 'worker nodes where kubelet and proxy runs',  I am not getting any node specific network stats. Though I am getting cpu and memory based stats. \n. ",
    "timstclair": "@jimmidyson we just decided that we need to re-think the API, rather than trying to force the cAdvisor API into k8s. I should have a PR out in the next couple of days, but I suspect it will not get merged until after thanksgiving since a lot of people are out next week. You can see the draft here (actively changing): https://github.com/timstclair/kubernetes/blob/metrics-beta/pkg/apis/metrics/types.go\n. > When do you aim for a stable stats v2 endpoint in kubernetes?\nI'd like to deprecate the /stats endpoint, and put any data we need going forward into the metrics API. In order to move heapster over to the new API for v1.2, we need to have the new API in beta in a couple weeks. To this end, I'm only planning on including the minimum stats requirements for heapster.\n\nWhat do you think about just adding in network stats to existing heapster API while we wait for @timstclair's PR & any subsequent stabilisation of the new metrics APIs?\n\nWhich network stats are you looking for? It looks like heapster already has network/{tx,rx}{,_errors}. I just had a meeting this morning where we agreed to remove a lot of the more detailed networking stats from the metrics API.  cc/ @mwielgus \n. The kubelet-metrics API was finally merged\nhttps://github.com/kubernetes/kubernetes/pull/18544, and includes what\nyou're looking for (pod level network stats). There is a little more work\nto do on the metrics (most notably filesystem & volume metrics), but I can\nstart implementing what we have. This shouldn't be too much work, nor\nshould it see much push back (compared with the API), so I'm optimistic\nthat I can get an initial implementation out by the EOW, or by next week at\nthe latest.\nOn Mon, Jan 4, 2016 at 10:14 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nSo looking again it seems the infra container name is hard coded to POD (\nhttps://github.com/kubernetes/kubernetes/blob/b9cfab87e33ea649bdd13a1bd243c502d76e5d22/pkg/kubelet/leaky/leaky.go#L24\n,\nhttps://github.com/kubernetes/kubernetes/blob/810181fb7b4e969d1c1aa84f32377ef1b65f8c65/pkg/kubelet/dockertools/docker.go#L41).\nOf course this is terrible & fragile but would simplify finding infra\ncontainers & pairing them to a pod. Not at all happy with this though.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/368#issuecomment-168756465\n.\n. I know this is coming in hot, but if the Kubelet side does land it would be awesome to get this in. I'm volunteering to do all the work on this, as I know you're all overloaded right now (and it's so late!).\n\n@vishh is adding custom metrics to the API.\n. Sounds good. I've already mostly implemented this as an alternate path in\nthe existing kubelet source, since there's a lot of overlap with the\nprevious methods. I'll probably send a WIP PR out later today, so if you\nstill think it would be better to refactor it as a separate source after\nthat, let me know.\nOn Mon, Feb 1, 2016 at 7:49 AM, Marcin Wielgus notifications@github.com\nwrote:\n\nYou can start adding the new API as a separate source (leaving the\nexisting one intact). If all elements are in (including the code that picks\nthe right api based on kubelet version and CM) and we still have time to\nredo the tests then we will definitely consider using the new source in K8S\n1.2.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/907#issuecomment-178037031\n.\n. Sorry, I completely misread this. Code is correct, please forget you ever saw this! :grimacing: \n. I refactored this to minimize the changes to existing code, and make the Summary a separate metric source (as a wrapper around the kubeletMetricSource).\n. @mwielgus I think what I have here is almost the same thing (it doesn't touch kubelet.go), but without all the code duplication. My concern is that if a bug is found in the existing kubelet implementation, it would then need to be fixed in 2 places with your proposal. What is your issue with this implementation?\n. @vishh - I forgot when we discussed this in the meeting, but volume stats are already added here. FS stats on a pod-scoped MetricSet can be interpreted as volume stats, but if you think it's necessary we could add a different label or prefix the fsKey (e.g. vol:...). I'm open to suggestions here.\n. > Ok, if these are the only changes to the old source. But move summary.go to a separate dir/package.\n\nUnfortunately it's a tradeoff. The current wrapper approach reads some private fields from the Kubelet source. To move the summary source to a new package will either require more duplicate code or more changes to existing code (export internal fields). Personally I like the current approach for the balance it makes between (1) Isolating the new source, (2) minimal changes to existing code and (3) minimal duplicate code.\nThoughts?\nOn a side note, I'm happy to own the cleanup of this post-1.2\n. Ok, addressed feedback & finished testing. I added a unit test, and an alternative controller to deploy the new source for the integration test (and verified it works). I think the main remaining issue is that of separating the summary source to a different directory, but I described the tradeoffs above.\nPTAL - We'd really like to get this into the 1.2 release.\n. Also I'm aware that the Godeps changes make this hard to review. Let me know if you'd like me to separate them and/or squash the commits.\n. Rebased & squashed non-godeps commits.\n. > @timstclair: What's the status on this PR?\nFrom my perspective, this is good to go. I'm just waiting on a review.\n. Addressed some comments, responses on https://github.com/timstclair/heapster/commit/d78bdc30025871f8a21b3291ea4fd040f0bb6c45\nWill ping again once I've addressed the last 2 (side-by-side testing & smooth CPU usage)\n. I just need to pull in the new godeps & update this PR to use the timestamps added in \nhttps://github.com/kubernetes/kubernetes/pull/20990\n. I can't find the open comment for some reason, but I've addressed the cpu rate issues in the latest push. A few things to note:\n- This will need to be synced with https://github.com/kubernetes/heapster/pull/985, and the temporary code can be removed\n- This includes a temporary Godeps update - the real update is blocked on https://github.com/kubernetes/kubernetes/pull/21368 (without it most of kubernetes & cAdvisor will need to be pulled in to heapster)\nPTAL - hopefully we can work out all the remaining issues by EOD tomorrow so this will be in friday's build.\n. Adressed comments.\n. Updated with the real godeps & squashed. Unit & integration tests pass. Ready to merge!\n. FYI, integration tests pass for all combinations of:\nSUPPORTED_KUBE_VERSIONS={1.1.7,HEAD}\nsource={kubelet, summary_api}\n. Looks like I missed a few. I'll ping this PR once the fixes are in.\n. OK, I believe the issues should be resolved.\n. /cc @vishh @fgrzadkowski @piosz @mwielgus \n. @mwielgus - Could you either review this or reassign it to someone who can? (What's the heapster process for assigning reviewers?)\n. Is this ready for merge? Is there any submit queue for heapster, or just manually merge?\n. Verified locally, make test-integration passes.\n. The additional dependencies (and versions) were all copied from the k8s godeps, if that makes you feel any more comfortable :)\n. failed to build docker binary (\"exit status 1\") - \"/jenkins-master-data/workspace/project/src/k8s.io/heapster/deploy/docker /jenkins-master-data/workspace/project/src/k8s.io/heapster/deploy/docker\\nCannot connect to the Docker daemon. Is the docker daemon running on this host?\\n\"\nLooks like a flake.\n@k8s-bot test this\n. @k8s-bot test this\n. @k8s-bot test this\n. @k8s-bot test this\n. /cc @dchen1107 \n. On the positive side, the fallback code is thoroughly tested :)\n. > Tim, it seems that this code hasn't been tested at all, including fallback.\nThat's not entirely true. All the tests I ran on kubernetes < 1.2 were marked as supporting the summary API (by the version check), but then successfully fell back to the old API when it got a 404 from the summary end point. That is what I meant by saying that the fallback had been thoroughly tested.  Also, I ran a handful of manual tests where I explicitly disabled the fallback (set to fallback to nil, useFallback to true, commented out fallback code) to ensure that the summary API actually was performing as expected. I don't disagree that this needs more testing with this fix in, and I agree with the decision to rollback the flag change, but I wanted to clarify that this isn't entirely untested.\n. FYI - @fgrzadkowski @mwielgus @piosz @vishh @pwittrock @dchen1107 \n. Filed https://github.com/kubernetes/kubernetes/issues/22319 for the networking stats.\n. Do we want to make the summary API the only kubernetes source? I believe this was stated as a goal at some point. If that's still the case, then I don't think we need to add much here...\n. These actually are collected by cAdvisor (https://github.com/google/cadvisor/blob/b4f1d7b82f87459070b58052b71ac4524a4344c3/info/v1/container.go#L301), but not surfaced in the summary API since heapster wasn't using them. If we want to include them in heapster, we could add them to the summary.\n. That check is accurate, the summary was introduced in v1.2.0-alpha.8 (v1.2.0 was the first stable release that included it). I believe semver just does a lexicographical comparison for everything after the -. It looks like you're trying to build a custom version of the Kubelet? If so, a simple fix would be to just add a custom tag (e.g. v1.2.0-patch-foo), so long as it's ordered after alpha it should work. Alteranatively, you could just rebase on a new release (e.g. 1.2.1).. There are some changes to Kubelet TLS auth in 1.5, but I can't think of anything that would have affected 1.4. Yes, those images are much better. Do we need the busybox base though? Can we get away with just using scratch?. Security. Debug-ability is definitely a legitimate concern, so we may want to wait for https://github.com/kubernetes/kubernetes/issues/27140 to land before moving to it. Either way, the images in your PR are a big improvement, so moving to scratch is lower priority.\nThe one other feature I would like to see is running as a non-root user, so we may want to add a user in the docker image to run as.. Here's the relevant bit of the response:\n\"/system.slice/var-lib-docker-containers-06bb355c9f4e9e3406ba1429baac5557c4ee7e2dc2a0886074da2fc1f3bfdf78-shm.mount\": {\n   \"id\": \"06bb355c9f4e9e3406ba1429baac5557c<ee7e6dC2a0886074da2fc1f3bfdf78\",\n   \"name\": \"/system.slice/var-lib-docker-con\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00.mount\",\n   \"aliases\": [\n    \"k8s_POD.d8dbe16c_dummy-2088944543-224bz_kube-system_b7bdcbc6-cdc5-11e6-964a-fa163e4fea2b_61773dc5\",\n    \"06bb355c9f4e9e3406ba1429baac5557c4ee7e2dc2a0886074da2fc1f3bfdf78\"\n   ],\n   \"namespace\": \"docker\",\n   \"labels\": {\n    \"io.kubernetes.container.hash\": \"d8dbe16c\",\n    \"io.kubernetes.container.name\": \"POD\",\n    \"io.kubernetes.container.restartCount\": \"0\",\n    \"io.kubernetes.container.terminationMessagePath\": \"\",\n    \"io.kubernetes.pod.name\": \"dummy-2088944543-224bz\",\n    \"io.kubernetes.pod.namespace\": \"kube-system\",\n    \"io.kubernetes.pod.terminationGracePeriod\": \"30\",\n    \"io.kubernetes.pod.uid\": \"b7bdcbc6-cdc5-11e6-964a-fa163e4fea2b\"\n   },\n   \"spec\": {\n    \"creation_time\": \"2016-12-29T07:54:00.980875323-05:00\",\n    \"labels\": {\n     \"io.kubernetes.container.hash\": \"d8dbe16c\",\n     \"io.kubernetes.container.name\": \"POD\",\n     \"io.kubernetes.container.restartCount\": \"0\",\n     \"io.kubernetes.container.terminationMessagePath\": \"\",\n     \"io.kubernetes.pod.name\": \"dummy-2088944543-224bz\",\n     \"io.kubernetes.pod.namespace\": \"kube-system\",\n     \"io.kubernetes.pod.terminationGracePeriod\": \"30\",\n     \"io.kubernetes.pod.uid\": \"b7bdcbc6-cdc5-11e6-964a-fa163e4fea2b\"\n    },\n    \"has_cpu\": true,\n    \"cpu\": {\n     \"limit\": 1024,\n     \"max_limit\": 0,\n     \"period\": 100000\n    },\n    \"has_memory\": true,\n    \"memory\": {\n     \"limit\": 9223372036854771712,\n     \"reservation\": 9223372036854771712,\n     \"swap_limit\": 9223372036854771712\n    },\n    \"has_network\": true,\n    \"has_filesystem\": true,\n    \"has_diskio\": true,\n    \"has_custom_metrics\": false,\n    \"image\": \"gcr.io/google_containers/pause-amd64:3.0\"\n   },\nI have no idea what would cause the invalid characters to show up in the name, but here are a couple notes:\n\nWhat is the dummy pod? Why is it running in the kube-system namsepace?\n~~We don't support docker 12, so that might be causing a problem.~~ CORRECTION: docker 1.12.3 is validated with Kubernetes 1.5+. What version of Kubernetes are you running?\nI suspect the problem will go away if you switch to the summary API (heapster source = kubernetes.summary_api). lgtm. Friendly ping. I would like to get this in 1.6. I'm rerunning the k8s e2e's with the most recent build if you want to hold off - I'll report my results here when it's done.. Tests pass. Good to go?. It's based on eth0. It's currently hard coded, but there's a couple open bugs to fix that (https://github.com/kubernetes/kubernetes/issues/28407). Oh yeah, that should only be done on system containers. Definitely a bug.. Yes\n. Yes, it is straight from cAdvisor. Currently, it's calculated when the v1 API is translated into the v2 API (here), but that will likely change in the future.\n. Ah, good point. I added some code to convert to an int64 when possible, but if it isn't I don't think we can do anything with the current API.\n. It looks like we need to add a new metric for available. I'm not sure I understand your second question?\n. As it currently stands, yes. I suppose I should just add a new MetricType (here). Are there any consequences to simply adding a new type that I need to be aware of?\n. No. Max float64 is order 10^308, while max int64 is around 10^19.\n. It's logically equivalent here, but if you think using 0.5 is more readable, sure :)\n. Because only float32 values are supported, which only have 23 bits of precision. The integer conversion is an attempt to offer higher precision for integer values (such as counts). Do you think this is overcomplicating the conversion?\n. Because the execution can only get here if (frac < epsilon || frac > 1.0-epsilon), so if frac > 0.5 then it follows that frac > 1.0-epsilon.\n. Responded here: https://github.com/kubernetes/heapster/pull/927#issuecomment-179498431\n. I'm trying to minimize heapster changes in this PR. I think that's a good idea, but I'd prefer to handle it in a follow-up. I'll add a TODO.\n. Done. Is the . delimiter acceptable?\n. Done.\n. Done. Re: schema - It looks like that schema is out of date (it's missing a handful of metrics). I can update it in a separate PR, but I think it would ideally be generated from the metrics definitions.\n. Done.\n. Ok, sgtm. Done.\n. What about metrics that could be scraped at different times? Volume / filesystem stats will be updated on a different schedule from other stats, as will UserDefinedMetrics.\n. How about definining a scale somewhere (either with the RateMetricsMapping, or maybe as part of the core.UnitsType in the MetricsDescriptor), then you don't need to branch on the metric name here\n. (nit) prefer time.Second.Nanoseconds() or 1e9\n. This should be ok for now, especially if it's internal-only.\n. It still defaults to heapster.controller.yaml, but it's configurable with a flag\n. Done.\n. Done.\n. Done.\n. You can set the user ID to anythnig, even if it doesn't map to a user in /etc/passwd. Some distributions map nobody to UID 99, but I think 65534 is the safer choice since 99 is in the system UID block.. Thanks, done.. Note: if you put a username:group here instead, it uses the mapping in /etc/passwd, but since this is from scratch, there is no mapping there. The alternative would be to make our own, but that seems unnecessary.. \n",
    "mboussaa": "Hello,\nThank you for you reply. In fact I am getting an error when executing docker compose up:\nheapster_1 | E0706 09:30:58.868929       1 driver.go:322] Database creation failed: Server returned (404): 404 page not found\nheapster_1 | . Retrying after 30 seconds\nthen I didn't understand how can I use heapster using your configuration files.\nMore again, using google cloud engine is not that free. They give only a limited free access. Right ?\n. ",
    "nikhiljindal": "cc @vmarmol @vishh\n. cc @bgrant0607 @dchen1107\n. I could be wrong, but I believe you can do:\n$ cd <heapster_dir>\n$ godep restore\n$ go get -u github.com/GoogleCloudPlatform/kubernetes\n$ godep update github.com/GoogleCloudPlatform/kubernetes\nThis should update https://github.com/GoogleCloudPlatform/heapster/blob/master/Godeps/Godeps.json#L34\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/devel/development.md#using-godep has more details\n. Awesome Thanks @krousey !!\n. /sub\n. Didnt look too closely at the first Godeps commit.\nThe second commit mostly LGTM, apart from a comment regarding the port flag.\n. Also, what about tests?\nNot sure what testing do you have around the heapster API server.\n. Just one suggestion.\nAlso, again no tests? Will be good to add some tests :)\nNo blocking comment. Can be fixed in another PR.\nLGTM, thx!\n. @mksalawa Yes you will need that for the complete e2e test. But in the meantime you can add unit tests for the code you are adding.\nYou can also take a look at https://github.com/kubernetes/kubernetes/blob/master/test/integration/examples/apiserver_test.go, which tests an apiserver without enabling auth. It uses simple CURL commands to verify that apiserver comes up fine and expected endpoints exist on the server.\n. Thanks, LGTM assuming the tests are passing\n. Why do we want to expose the heapster specific APIs on a different port?\nChanging the meaning of an existing flag doesnt seem right to me.\nIf I was setting --port=8090 in my config and was expecting to see a heapster specific API at that port, then this change will break me.\n. FYI, @deads2k and @lavalamp have changed this install code at many places.\nYou can look at https://github.com/kubernetes/kubernetes/blob/cf7301f16c036363c4fdcb5d4d0c867720214598/pkg/apis/extensions/install/install.go for an example.\nI think @lavalamp wanted to fix it some more before updating all install.go, so this should be fine. But wanted to let you know that we will need to update it soon.\n. Is it a required param? What if you dont set it at all\n. ",
    "dchen1107": "LGTM\n. cc/ @dchen1107\n. https://github.com/GoogleCloudPlatform/kubernetes/issues/10760\n. Besides those nits, LGTM. \n. Thanks!\n. LGTM\n. LGTM thanks!\n. Thanks for fix. \n. https://github.com/kubernetes/kubernetes/pull/20990 was merged. Anything else is blocking this one?\n. Test infra failure here?\n--- FAIL: TestHeapster (514.39s)\n    assertions.go:154: \n```\nLocation:   heapster_api_test.go:683\nError:      No error is expected but got failed to build docker binary (\"exit status 1\") - \"/jenkins-master-data/workspace/project/src/k8s.io/heapster/deploy/docker /jenkins-master-data/workspace/project/src/k8s.io/heapster/deploy/docker\\nCannot connect to the Docker daemon. Is the docker daemon running on this host?\\n\"\n```\n. @fgrzadkowski disk account is blocker 1.2. \n. cc/ @timstclair This is the proposal I mentioned to you a while back. \n. I opened https://github.com/kubernetes/kubernetes/issues/30939 with more debugging detail from @ichekrygin. We need to address the issue. Thanks!\n. /lgtm\n@piosz . /lgtm\n@piosz . cc/ @fgrzadkowski . Indent here?\n. indent?\n. timeline? Kubernetes 1.2?\n. timeline?\n. ",
    "jlowdermilk": "LGTM. Regarding using config overrides to explicitly set apiversion, it's fine medium term solution, but should put a TODO in to remove it after updating client deps.\n. @krousey, I stand corrected. If it's using/referring to api objects directly then explictly setting apiversion in client config is correct.\n. ",
    "xsyr": "Sorry, it's caused by system time being out of sync.\n. ",
    "onorua": "I've tried to add my ca.crt to base system:\ncp ca.crt /usr/local/share/ca-certificates\nupdate-ca-certificates\nand ten recreated pod with no succes. \nI still get \nE0720 12:12:56.329221       1 reflector.go:136] Failed to list *api.Pod: Get https://10.116.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D: x509: certificate signed by unknown authority\nE0720 12:12:56.336256       1 reflector.go:136] Failed to list *api.Namespace: Get https://10.116.0.1:443/api/v1/namespaces: x509: certificate signed by unknown authority\nE0720 12:12:56.340452       1 reflector.go:136] Failed to list *api.Node: Get https://10.116.0.1:443/api/v1/nodes: x509: certificate signed by unknown authority\n. I've installed it the way it's described in documentation. What is the proper way to add ca.crt to the heapster? I've tried all possible ways I know without any success. The latest hint I've got is https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/user-guide/service-accounts.md which may be required, but I could not find the way to add ca.crt into it properly. \nAny hint will be highly appreciated!\n. Documentation has been updated https://github.com/kubernetes/heapster/issues/429#issuecomment-132082015 So I'm closing this one.\n. I'm running kubernetes v1.0.1  with \n--admission_control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota\nconfigured on a master, my heapster controller looks like this:\napiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: monitoring-heapster-v5\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    version: v5\n    kubernetes.io/cluster-service: \"true\"\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v5\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v5\n        kubernetes.io/cluster-service: \"true\"\n    spec:\n      containers:\n        - image: gcr.io/google_containers/heapster:v0.16.0\n          name: heapster\n          resources:\n            limits:\n              cpu: 100m\n              memory: 200Mi\n          command:\n            - /heapster\n            - --source=kubernetes:''\n            - --sink=influxdb:http://monitoring-influxdb:8086\n            - --poll_duration=2m\n            - --stats_resolution=1m\n          volumeMounts: \n            - name: ssl-certs\n              mountPath: /etc/ssl/certs\n              readOnly: true\n            - name: monitoring-token\n              mountPath: /etc/kubernetes/kubeconfig\n              readOnly: true\n      volumes: \n        - name: ssl-certs\n          hostPath: \n            path: \"/etc/ssl/certs\"\n        - name: monitoring-token\n          secret:\n            secretName: token-system-monitoring\n      nodeSelector:\n        zone: \"common\"\nIn logs I can see:\nI0727 09:59:56.918307       1 heapster.go:52] /heapster --source=kubernetes:'' --sink=influxdb:http://monitoring-influxdb:8086 --poll_duration=2m --stats_resolution=1m\nI0727 09:59:56.919631       1 heapster.go:53] Heapster version 0.16.0\nE0727 09:59:56.919777       1 helper.go:247] expected to load root ca config from /var/run/secrets/kubernetes.io/serviceaccount/ca.crt, but got err: open /var/run/secrets/kubernetes.io/serviceaccount/ca.crt: no such file or directory\nI0727 09:59:56.919853       1 kube_factory.go:169] Using Kubernetes client with master \"https://10.116.0.1:443\" and version \"v1\"\nI0727 09:59:56.919868       1 kube_factory.go:170] Using kubelet port 10255\nI0727 09:59:57.943191       1 driver.go:376] created influxdb sink with options: {root root monitoring-influxdb:8086 k8s false}\nI0727 09:59:57.954722       1 heapster.go:64] Starting heapster on port 8082\nE0727 09:59:57.967057       1 reflector.go:136] Failed to list *api.Pod: Get https://10.116.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D: x509: certificate signed by unknown authority\nE0727 09:59:57.967311       1 reflector.go:136] Failed to list *api.Namespace: Get https://10.116.0.1:443/api/v1/namespaces: x509: certificate signed by unknown authority\nE0727 09:59:57.967381       1 reflector.go:136] Failed to list *api.Node: Get https://10.116.0.1:443/api/v1/nodes: x509: certificate signed by unknown authority\nE0727 09:59:57.967674       1 kube_events.go:96] Failed to load events: Get https://10.116.0.1:443/api/v1/events: x509: certificate signed by unknown authority\nE0727 09:59:58.971602       1 reflector.go:136] Failed to list *api.Node: Get https://10.116.0.1:443/api/v1/nodes: x509: certificate signed by unknown authority\nE0727 09:59:58.972063       1 reflector.go:136] Failed to list *api.Pod: Get https://10.116.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D: x509: certificate signed by unknown authority\nit seems volume is created:\n\"Volumes\": {\n        \"/dev/termination-log\": \"/var/lib/kubelet/pods/456d4f09-3446-11e5-bad6-b083fec1a8e8/containers/heapster/0fc21f3b0e83e383198a8695f327c7b3a3cd466dcff89354092eae97e6a16cd7\",\n        \"/etc/kubernetes/kubeconfig\": \"/var/lib/kubelet/pods/456d4f09-3446-11e5-bad6-b083fec1a8e8/volumes/kubernetes.io~secret/monitoring-token\",\n        \"/etc/ssl/certs\": \"/etc/ssl/certs\",\n        \"/var/run/heapster/hosts\": \"/var/lib/docker/volumes/d6bc3b29ad6d8947c3217f88cae0a1a1ffeedde7e11251d9b2c98d4fc0a51ba5/_data\",\n        \"/var/run/secrets/kubernetes.io/serviceaccount\": \"/var/lib/kubelet/pods/456d4f09-3446-11e5-bad6-b083fec1a8e8/volumes/kubernetes.io~secret/default-token-drc86\"\n    },\nbut when I check the secret:\nkubectl get secrets  --namespace=kube-system default-token-drc86 -o yaml \napiVersion: v1\ndata:\n  token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKa1pXWmhkV3gwTFhSdmEyVnVMV1J5WXpnMklpd2lhM1ZpWlhKdVpYUmxjeTVwYnk5elpYSjJhV05sWVdOamIzVnVkQzl6WlhKMmFXTmxMV0ZqWTI5MWJuUXVibUZ0WlNJNkltUmxabUYxYkhRaUxDSnJkV0psY201bGRHVnpMbWx2TDNObGNuWnBZMlZoWTJOdmRXNTBMM05sY25acFkyVXRZV05qYjNWdWRDNTFhV1FpT2lJMVpXWmlNRFZsTlMweVpXTTFMVEV4WlRVdFlqWTFNQzFpTURnelptVmpNVGhqTXpjaUxDSnpkV0lpT2lKemVYTjBaVzA2YzJWeWRtbGpaV0ZqWTI5MWJuUTZhM1ZpWlMxemVYTjBaVzA2WkdWbVlYVnNkQ0o5LlJKTUtJYmtpWm50MGJCT2JnXzU3WHVNWDVuWXVOWWZ1U1dSNVhHb013X1huUkVBcDl2ZjBkZUtkcVZxeU9MTy1vaEhZTl83bHBLQ2t3U0Z3dDFNenZqUGlhWUVLbnF0Y2t2MTBidmlZcFlpS3hiVWdrcG1FNFp3QXI5X19RSktmMldNRThQTDZvdDlIWWtYZXUwNWJObS05VU8teU01SW1IWlJhX2w5S04tVXFLNHlzZ3dYTXY4R3AzaGlhcW9iZC04c2NqMVE1Z281Y1lvU3V5eWh2ejVVS1hEWWNzOHFncTlvMzNKZEEwNktnNUlpcnViX293TzZXRFpQRWw4bWxYeF9VekdNV3pDOTlDOHBsVEFJMzJWNEtXTzRnQUJuejJFSWpqeEtfaFF3d1ZFdVZEZGNTNnNXMUQwdlhOZGYyTEZLQmR6UHRMLUNpNjJtRGRqMERHN1BVbGpCcWpwaWR5aEllSHBQejY5V2d4Y0IyTk12dkxGREFQMmlXcV9lMEpqcXVVcTFOeENYQkgwQmt2ZUtKUWVRXzFxMTVJYWViUnNhS3lUZldnVkctbS1ZR1hJS0h6YkF5QUhOVFRMaVNIcTdQMWc3MmJCRXhPazEwaktxVU1pby1xREpxMERrWURhTmFBZUhCOVNLV09XdGFjMlNBTkVoMWxHbUJaUGtRQzcxTl9pNmhWSTFFYkdHeUpZSnd5ajgyWHlXRlRpYXNfdVlycVNPUnBsM3hPVUpkZ182NGlFSGFDdmNPQW5JVkNuZkpjTHk0dUZvWlIyRVptQTNObktqRmxPQmF4X09UM1B4WG5zZkVVMHVqZmpwUGR4WjdfbHFxcWtrWmkxOVZncy1fTjNZZnA5SF9pdVUzZWNCaEtYeTVIdUNHWGo4UmY4VG5KMjBJTXNr\nkind: Secret\nmetadata:\n  annotations:\n    kubernetes.io/service-account.name: default\n    kubernetes.io/service-account.uid: 5efb05e5-2ec5-11e5-b650-b083fec18c37\n  creationTimestamp: 2015-07-20T15:27:31Z\n  name: default-token-drc86\n  namespace: kube-system\n  resourceVersion: \"6104940\"\n  selfLink: /api/v1/namespaces/kube-system/secrets/default-token-drc86\n  uid: d6bd06ea-2ef3-11e5-bad6-b083fec1a8e8\ntype: kubernetes.io/service-account-token\nthere is not even a word about ca.crt\n. I've got it working guys!\nFirst  of all you must enable root_ca_file (in case you don't use default installation methods) for kube-controller-manager. \n2. You must remove currently used secret (this is necessary for installation which go through update process from early version). So it means if you install from scratch - no problem. If you install from the version with service accounts but without ca.crt in tokens (before 1.0) you must remove secrets in my case it was default-token-drc86 after that recreated RC and it will work. \n. Thanks @hasbro17, I've used part of your analysis techniques to proceed further in my findings.. \nIndeed I have 10seconds resolution, I have no clue why would it be missing. I can only speculate that huge values I've discovered below and missing values from your ticket are related.\nI've made raw request to heapster API:\n.../api/v1/proxy/namespaces/kube-system/services/heapster/api/v1/model/metrics/cpu/usage_rate\nand got really weird result:\n{\n  \"metrics\": [\n   {\n    \"timestamp\": \"2016-06-05T11:52:10Z\",\n    \"value\": 18446744073709040579\n   },\n   {\n    \"timestamp\": \"2016-06-05T11:52:20Z\",\n    \"value\": 25046\n   },\n   {\n    \"timestamp\": \"2016-06-05T11:52:30Z\",\n    \"value\": 18446744073708433988\n   },\n   {\n    \"timestamp\": \"2016-06-05T11:52:40Z\",\n    \"value\": 7198\n   },\n   {\n    \"timestamp\": \"2016-06-05T11:52:50Z\",\n    \"value\": 18446744073709492498\n   },\n   {\n    \"timestamp\": \"2016-06-05T11:53:00Z\",\n    \"value\": 167687\n   },\n   {\n    \"timestamp\": \"2016-06-05T11:53:10Z\",\n    \"value\": 18446744073708840237\n   },\n   {\n    \"timestamp\": \"2016-06-05T11:53:20Z\",\n    \"value\": 66257\n   },\n   {\n    \"timestamp\": \"2016-06-05T11:53:30Z\",\n    \"value\": 18446744073709183875\n   },\n...\nAccording to golang specification int64 which is used for metrics, equals 9223372036854775807, but 18446744073709040000 is bigger than int64, which causing negative values in my case. \nDo you want me to prepare patch to use uint64 for this? \nHonestly I do not think this value is something real in my cluster, there must be something extraordinary in data or the way the data gathered. Any suggestions what can be checked further to pin the root cause? \n. @DirectXMan12 yeah, actually values in cpu/usage are not constantly increasing as you suspected. Should I close this one in favour of your ticket as duplicate?\n. ",
    "AlexeyKupershtokh": "How did you install the heapster?\nHave you tried to add the certificate inside the heapster container as well?\n. @y00181991 I've just tried to do this. I've connected directly to the node where the Heapster container is running. Started a shell within it and tried to run /heapster with the insecure flag:\n/ # /heapster '--source=kubernetes:http://kubernetes-ro?insecure=true' \"--sink=influxdb:http://monitoring-influxdb:80\" -v=4\nI0727 05:32:43.108157      87 heapster.go:52] /heapster --source=kubernetes:http://kubernetes-ro?insecure=true --sink=influxdb:http://monitoring-influxdb:80 -v=4\nI0727 05:32:43.116123      87 heapster.go:53] Heapster version 0.16.0\nE0727 05:32:43.116816      87 helper.go:247] expected to load root ca config from /var/run/secrets/kubernetes.io/serviceaccount/ca.crt, but got err: open /var/run/secrets/kubernetes.io/serviceaccount/ca.crt: no such file or directory\nI0727 05:32:43.117496      87 kube_factory.go:169] Using Kubernetes client with master \"https://192.168.0.2:443\" and version \"v1\"\nI0727 05:32:43.117644      87 kube_factory.go:170] Using kubelet port 10255\nI0727 05:32:43.121150      87 kube_events.go:153] Starting \"kube-events\" source\nI0727 05:32:43.122992      87 kube_events.go:155] Finished starting \"kube-events\" source\nE0727 05:32:43.136561      87 reflector.go:136] Failed to list *api.Node: Get https://192.168.0.2:443/api/v1/nodes: x509: failed to load system roots and no roots provided\nE0727 05:32:43.160448      87 reflector.go:136] Failed to list *api.Pod: Get https://192.168.0.2:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D: x509: failed to load system roots and no roots provided\nE0727 05:32:43.176448      87 kube_events.go:96] Failed to load events: Get https://192.168.0.2:443/api/v1/events: x509: failed to load system roots and no roots provided\nE0727 05:32:43.196683      87 reflector.go:136] Failed to list *api.Namespace: Get https://192.168.0.2:443/api/v1/namespaces: x509: failed to load system roots and no roots provided\nIt still tries to connect to https. I can even write any http://nonce in the source, it still connects to the \"https://192.168.0.2:443\". It seems that options don't work correctly.\nI'm running Kubernetes v0.19.3 (installed using the ubuntu scripts) and heapster 0.16.0.\n. Found another similar issue: #429\n. Worth taking a look: https://github.com/GoogleCloudPlatform/kubernetes/issues/11000\nThough I haven't had enough time to follow the instructions and make heapster work with k8s.\n. @onorua I've followed your instructions and applied . This helped me to deliver ca.crt into pods, but heapster doesn't like the ca.crt (\"x509: certificate signed by unknown authority\"):\nE0728 11:13:39.536844       1 reflector.go:136] Failed to list *api.Node: Get https://192.168.3.1:443/api/v1/nodes: x509: certificate signed by unknown authority\nE0728 11:13:39.551661       1 reflector.go:136] Failed to list *api.Pod: Get https://192.168.3.1:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D: x509: certificate signed by unknown authority\nE0728 11:13:39.566447       1 reflector.go:136] Failed to list *api.Namespace: Get https://192.168.3.1:443/api/v1/namespaces: x509: certificate signed by unknown authority\nAny ideas how to overcome this too?\nI'd deleted all the secrets using kubectl delete secret --all and then re-created heapster's RCs\n. @mikedanese would I be wrong if I said that in the case of SANS mismatch the error message would look like this https://github.com/GoogleCloudPlatform/kubernetes/issues/10972#issuecomment-119826517 ?\nI created a certificate recently by invoking this script on the master machine:\nwget https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes/master/cluster/saltbase/salt/generate-cert/make-ca-cert.sh\nchmod +x make-ca-cert.sh\nsudo groupadd kube-cert\nsudo ./make-ca-cert.sh 192.168.3.1 IP:192.168.3.1,IP:10.0.0.1,DNS:kubernetes,DNS:kubernetes.default,DNS:kubernetes.default.svc,DNS:kubernetes.default.svc.cluster.local\nIn order to provide (almost) full info on how to reproduce my environment...\nI'm using the ubuntu provider to install kubernetes from inside a script that looks almost like this one:\ngit checkout release-1.0\ncd cluster/ubuntu\nK8S_VERSION=1.0.1 ./build.sh\ncd ..\nKUBERNETES_PROVIDER=ubuntu ./kube-up.sh\ncd ..\nkubectl create -f cluster/addons/kube-ui/ --namespace=kube-system\nkubectl delete secret --all\nkubectl delete -f cluster/addons/cluster-monitoring/influxdb/\nkubectl create -f cluster/addons/cluster-monitoring/influxdb/\nMy current kubernetes git tree is the current release-1.0 with this diff: https://gist.github.com/AlexeyKupershtokh/2127ec9611b600c5d8d2\nThe host where I'm installing kubernetes is ubuntu 14.04.2 with docker 1.7.1.\nI've had installed kubernetes 0.19.3 and 0.21.4 on the same host before 1.0.1, but then used kube-down.sh for cleanup.\nMaybe you see any issues with what I'm doing at a glance?\n. I've just followed instructions from https://github.com/GoogleCloudPlatform/kubernetes/issues/11000#issuecomment-120396771 and additionally updated apiserver options this way:  \n--- a/cluster/ubuntu/util.sh\n+++ b/cluster/ubuntu/util.sh\n@@ -212,6 +212,9 @@ KUBE_APISERVER_OPTS=\"--address=0.0.0.0 \\\n --logtostderr=true \\\n --service-cluster-ip-range=${1} \\\n --admission_control=${2} \\\n+--client-ca-file=/srv/kubernetes/ca.crt \\\n+--tls-cert-file=/srv/kubernetes/server.cert \\\n+--tls-private-key-file=/srv/kubernetes/server.key \\\n --service_account_key_file=/tmp/kube-serviceaccount.key \\\n --service_account_lookup=false \"\n EOF\nNow the certificate seems valid to the heapster. Phew.\nBut I got another problem:\nE0729 01:28:14.393329       1 reflector.go:136] Failed to list *api.Node:\nthe server has asked for the client to provide credentials (get nodes)\nE0729 01:28:14.395981       1 reflector.go:136] Failed to list *api.Namespace:\nthe server has asked for the client to provide credentials (get namespaces)\nE0729 01:28:14.397820       1 reflector.go:136] Failed to list *api.Pod:\nthe server has asked for the client to provide credentials (get pods)\nIs this something related to service accounts?\n. The same behavior:\nwicked@w:~/docker/kubernetes $ kubectl logs monitoring-heapster-v5-puw81 --namespace=kube-system\nI0731 08:01:01.413687       1 heapster.go:55] /heapster --source=kubernetes:'' --sink=influxdb:http://monitoring-influxdb:8086\nI0731 08:01:01.413780       1 heapster.go:56] Heapster version 0.17.0\nI0731 08:01:01.415605       1 kube_factory.go:168] Using Kubernetes client with master \"https://192.168.3.1:443\" and version \"v1\"\nI0731 08:01:01.415656       1 kube_factory.go:169] Using kubelet port 10255\nE0731 08:01:01.562549       1 kube_events.go:96] Failed to load events: the server has asked for the client to provide credentials (get events)\nE0731 08:01:01.563821       1 reflector.go:136] Failed to list *api.Namespace:\n   the server has asked for the client to provide credentials (get namespaces)\nE0731 08:01:01.569535       1 reflector.go:136] Failed to list *api.Pod:\n   the server has asked for the client to provide credentials (get pods)\nE0731 08:01:01.571774       1 reflector.go:136] Failed to list *api.Node:\n   the server has asked for the client to provide credentials (get nodes)\nThough I have managed to use an http connection as a workaround.\n. ",
    "y00181991": "@onorua  I met the same problem, I tried service-accounts,no good luck!\n@AlexeyKupershtokh can we add a insecure flag\uff1f\n. apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: monitoring-heapster-v5\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    version: v5\n    kubernetes.io/cluster-service: \"true\"\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v5\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v5\n        kubernetes.io/cluster-service: \"true\"\n    spec:\n      containers:\n        - image: gcr.io/google_containers/heapster:v0.16.0\n          name: heapster\n          resources:\n            limits:\n              cpu: 100m\n              memory: 200Mi\n          command:\n            - /heapster\n            - --source=kubernetes:''\n. yes!\nand my heapster-controller.yaml like this;\napiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: monitoring-heapster-v5\n  namespace: default\n  labels:\n    k8s-app: heapster\n    version: v5\n    kubernetes.io/cluster-service: \"true\"\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v5\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v5\n        kubernetes.io/cluster-service: \"true\"\n    spec:\n      containers:\n        - image: gcr.io/google_containers/heapster:v0.16.0\n          name: heapster\n          resources:\n            limits:\n              cpu: 100m\n              memory: 200Mi\n          command:\n            - /heapster\n            - --source=kubernetes:https://kubernetes:443?useServiceAccount=true&auth=&insecure=true\n            - --sink=influxdb:http://monitoring-influxdb:8086\nthe flag insecure=true sounds not to work.\nis there a way to force heapster access k8s master via http?\n. I have tried this before, but it's not work;\nk8s api service generate the self-sign cert  by under /var/run/kubernetes  , this cert is CA itself,and I add it to my ca localstore (my os is ubuntu 14.04).\nwhen I use curl to access api seriver from k8s api server , I got the error x509: certificate signed by unknown authority.\nThat means even I trust the CA on ubuntu 14.04, I will still got this error.\nand the client cert and ca cert is mounted in my heapster container.\nand I also have tried generate CA myself ,and use it to sign my server cert,config these cert in k8s api servise start shell and manager service start shell .\nI also tried use kubernetes/cluster/gce/util.sh to generate these certs.\n@mikedanese In my view ,in heapster 0.16.0 ,serviceaccount must be configed, all the cert must be mounted,otherwise heapter will raise an exception on start.\nbut on ubuntu 14.04 when self-cert used to config these k8s serviers ,  x509: certificate signed by unknown authority.\nthis block about two days, I am not sure am I clear , I am looking forward to your reponses and help~\n. yes.\n--root-ca-file=\"\": If set, this root certificate authority will be included in service account's token secret. This must be a valid PEM-encoded CA bundle.\n      --service-account-private-key-file=\"\": Filename containing a PEM-encoded private RSA key used to sign service account tokens.\nsuggest by:\nhttp://kubernetes.io/v1.0/docs/admin/kube-controller-manager.html\n. ",
    "sergeycherepanov": "Hi @mikedanese, we create Cluster via Google Cloud Console. \n$ kubelet --version\nKubernetes v0.19.3\n$ kubectl version\nClient Version: version.Info{Major:\"0\", Minor:\"20\", GitVersion:\"v0.20.2\", GitCommit:\"323fde5bc5c45e30bbb5451ccf5c1ff01b0717f7\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"0\", Minor:\"21\", GitVersion:\"v0.21.4\", GitCommit:\"3d43944bee17e7147c2ae59aa97a63fb5111d6b0\", GitTreeState:\"clean\"}\nPreviously we try to start pod with rc config provided from heapster docs: https://github.com/GoogleCloudPlatform/heapster/blob/master/deploy/kube-config/google/heapster-controller.json\nAnd it fails with:\nI0722 11:36:42.163427       1 heapster.go:52] /heapster --source=kubernetes:'' --sink=gcm --sink=gcl --poll_duration=2m --stats_resolution=1m\nI0722 11:36:42.163477       1 heapster.go:53] Heapster version 0.16.0\nE0722 11:36:42.163547       1 helper.go:247] expected to load root ca config from /var/run/secrets/kubernetes.io/serviceaccount/ca.crt, but got err: open /var/run/secrets/kubernetes.io/serviceaccount/ca.crt: no such file or directory\nI0722 11:36:42.164603       1 kube_factory.go:169] Using Kubernetes client with master \"https://10.179.240.1:443\" and version \"v1\"\nI0722 11:36:42.164667       1 kube_factory.go:170] Using kubelet port 10255\nE0722 11:36:42.169158       1 reflector.go:136] Failed to list *api.Pod: Get https://10.179.240.1:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D: x509: failed to load system roots and no roots provided\nE0722 11:36:42.169792       1 reflector.go:136] Failed to list *api.Node: Get https://10.179.240.1:443/api/v1/nodes: x509: failed to load system roots and no roots provided\nE0722 11:36:42.173394       1 kube_events.go:96] Failed to load events: Get https://10.179.240.1:443/api/v1/events: x509: failed to load system roots and no roots provided\nE0722 11:36:42.177046       1 reflector.go:136] Failed to list *api.Namespace: Get https://10.179.240.1:443/api/v1/namespaces: x509: failed to load system roots and no roots provided\nE0722 11:36:43.172365       1 reflector.go:136] Failed to list *api.Pod: Get https://10.179.240.1:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D: x509: failed to load system roots and no roots provided\nE0722 11:36:43.173273       1 reflector.go:136] Failed to list *api.Node: Get https://10.179.240.1:443/api/v1/nodes: x509: failed to load system roots and no roots provided\nE0722 11:36:43.179684       1 reflector.go:136] Failed to list *api.Namespace: Get https://10.179.240.1:443/api/v1/namespaces: x509: failed to load system roots and no roots provided\nE0722 11:36:44.175711       1 reflector.go:136] Failed to list *api.Pod: Get https://10.179.240.1:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D: x509: failed to load system roots and no roots provided\nE0722 11:36:44.176539       1 reflector.go:136] Failed to list *api.Node: Get https://10.179.240.1:443/api/v1/nodes: x509: failed to load system roots and no roots provided\nE0722 11:36:44.183545       1 reflector.go:136] Failed to list *api.Namespace: Get https://10.179.240.1:443/api/v1/namespaces: x509: failed to load system roots and no roots provided\nI0722 11:36:45.169557       1 driver.go:93] created GCM sink\nE0722 11:36:45.169651       1 heapster.go:59] Current instance does not have the expected scope (\"https://www.googleapis.com/auth/monitoring\"). Actual scopes: https://www.googleapis.com/auth/compute\nAfter, I started container via _docker run with _/bin/sh entry point. I add kubernetes host with master ip to /etc/hosts file. And I paste token content to /var/run/secrets/kubernetes.io/serviceaccount/token, and tried to run heapster directly from cli. But I received error with empty master host and port.\n. @mikedanese thanks for you response it's partly helped. But we steel have issue with ca.crt error. \nexpected to load root ca config from /var/run/secrets/kubernetes.io/serviceaccount/ca.crt, but got err: open /var/run/secrets/kubernetes.io/serviceaccount/ca.crt: no such file or directory\nIn /var/run/secrets/kubernetes.io/serviceaccount/ we have only token, how we mount ca.crt?\nIf i execure kubectl get secrets i see\ndefault-token-3xgmh               kubernetes.io/service-account-token   1\ndefault-token-9d11m               kubernetes.io/service-account-token   2\ndefault-token-hl556               kubernetes.io/service-account-token   2\nAfter describe the heapster pod, i found that default-token-3xgmh mounted to heapster\nIt's looks like we have default secret without ca.crt\n. I fixed it via defining custom secret volume (with ca.crt and token) in json template for replication controller.\n. ",
    "Icedroid": "The same behavior on ubuntu 14.04:\n\nFailed to load events: Get https://10.0.0.1:443/api/v1/events: x509: failed to load system roots and no roots provided\nE0814 08:40:24.974202       1 reflector.go:136] Failed to list api.Namespace: Get https://10.0.0.1:443/api/v1/namespaces: x509: failed to load system roots and no roots provided\nE0814 08:40:26.831647       1 reflector.go:136] Failed to list api.Pod: Get https://10.0.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%21%3D: x509: failed to load system roots and no roots provided\n. \n",
    "lucas-coelho": "@a-robinson Yes, its when i started. Now are logging this:\nE0730 02:03:58.099822       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [3780/3712]) [4779]\nE0730 05:05:50.899511       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [4989/4781]) [5988]\nE0730 08:04:55.667831       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [6182/5989]) [7181]\nE0730 11:06:36.157589       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [7391/7182]) [8390]\n. ",
    "fzu-huang": "@vishh \nversion  0.17\ni switch the version to 0.16 \uff0cthen when i create a  rc,  it is wrong  and log is : \nI0803 08:35:28.200069       1 heapster.go:52] /heapster --source=kubernetes:https://kubernetes --sink=influxdb:http://monitoring-influxdb:80\nI0803 08:35:28.200157       1 heapster.go:53] Heapster version 0.16.0\nE0803 08:35:28.200587       1 heapster.go:59] open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory\nshould i  do something in kubernetes???\n. ",
    "bh016088": "That should be referenced somewhere in the heapster config guide as service accounts are not a requirement for kubernetes.\n. One thing that is not mentioned at all in the \"getting started guide\" is configuring sources:\nhttps://github.com/kubernetes/heapster/blob/master/docs/source-configuration.md#kubernetes\nThe documentation of this thing needs some serious help.\n. This occurred on Docker 1.8.x when I tried it.  I recently upgraded to 1.9.  I will test.\n. ",
    "ScOut3R": "I have tried this on CoreOS and it does not work. I changed the path to \"/usr/share/ca-certificates\" and that works. I assume the symlinks under /etc/ssl/certs aren't resolvable inside the container.\n. ",
    "foxting1127": "hi afein,\nI used the kubernete version 1.0.1. \n. NAME                                         READY     STATUS    RESTARTS   AGE\nmonitoring-influx-grafana-controller-77ohm   2/2       Running   0          4m\nCONTROLLER                             CONTAINER(S)   IMAGE(S)                            SELECTOR             REPLICAS\nmonitoring-influx-grafana-controller   influxdb       kubernetes/heapster_influxdb:v0.3   name=influxGrafana   1\n                                       grafana        kubernetes/heapster_grafana:v0.7\nNAME                     LABELS                                                                     SELECTOR             IP(S)           PORT(S)\nkubernetes               component=apiserver,provider=kubernetes                                                   192.168.3.1     443/TCP\nmonitoring-grafana       kubernetes.io/cluster-service=true,kubernetes.io/name=monitoring-grafana   name=influxGrafana   192.168.3.50    80/TCP\nmonitoring-influxdb      name=monitoring-influxdb                                                   name=influxGrafana   192.168.3.216   80/TCP\nmonitoring-influxdb-ui                                                                        name=influxGrafana   192.168.3.205   8083/TCP\n                                                                                                                                         8086/TCP\n. --namespace-all flag seems not accepted in the command.\n. kubectl get pods,rc,svc --namespace-all\nError: unknown flag: --namespace-all\nRun 'kubectl help' for usage.\n. Error from server: error when creating \"../../heapster/deploy/kube-config/influxdb/heapster-controller.json\": ReplicationController \"heapster\" is forbidden: Namespace kube-system does not exist\nError from server: error when creating \"../../heapster/deploy/kube-config/influxdb/heapster-service.json\": Service \"heapster\" is forbidden: Namespace kube-system does not exist\n. NAMESPACE   NAME                                         READY     STATUS    RESTARTS   AGE\ndefault     monitoring-influx-grafana-controller-77ohm   2/2       Running   0          9m\nNAMESPACE   NAME                     LABELS                                                                     SELECTOR             IP(S)           PORT(S)\ndefault     kubernetes               component=apiserver,provider=kubernetes                                                   192.168.3.1     443/TCP\ndefault     monitoring-grafana       kubernetes.io/cluster-service=true,kubernetes.io/name=monitoring-grafana   name=influxGrafana   192.168.3.50    80/TCP\ndefault     monitoring-influxdb      name=monitoring-influxdb                                                   name=influxGrafana   192.168.3.216   80/TCP\ndefault     monitoring-influxdb-ui                                                                        name=influxGrafana   192.168.3.205   8083/TCP\n                                                                                                                                                     8086/TCP\nNAMESPACE   CONTROLLER                             CONTAINER(S)   IMAGE(S)                            SELECTOR             REPLICAS\ndefault     monitoring-influx-grafana-controller   influxdb       kubernetes/heapster_influxdb:v0.3   name=influxGrafana   1\n                                                   grafana        kubernetes/heapster_grafana:v0.7\n. Hi vishh,\nThanks for your reply. I am quite new to heapster and kubernetes, could you give me more guidelines? For example, how do I change heapster config to run it in the default namespace? \nthanks a lot!\n. I changed kubernetes to a prior version v.0.18.0, it gives me different error:\nroot@garen:~# kubectl create -f heapster/deploy/kube-config/influxdb/\nError from server: the server could not find the requested resource\nError from server: the server could not find the requested resource\nError from server: the server could not find the requested resource\nError from server: the server could not find the requested resource\nError from server: the server could not find the requested resource\nError from server: the server could not find the requested resource\n. hi vishh, \nI see..thanks, I will try that.\n. ",
    "brendandburns": "LGTM.\n. @luxas did this ever happen?  As far as I can tell the latest ARM version of heapster is 0.8.0\nThanks!\n--brendan\n. ",
    "wulonghui": "I run the heapster on host directly, and it get the right NodeInternalIP \nI0805 18:17:02.318569   23839 kube.go:117] kube nodes found: &{Items:map[node2:{PublicIP:192.168.3.148 InternalIP:192.168.3.148 ExternalID:node2}\nIs this code[1] gets some problem inside the conatiner?\naddrs, err := net.LookupIP(hostname)\n[1]https://github.com/GoogleCloudPlatform/heapster/blob/v0.17.0/sources/nodes/kube.go#L82\n. Because my Kubernetes's cloud providers is nil, the code[1] only set NodeLegacyHostIP.\nIs that ok to add NodeInternalIP?\n[1]https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/kubelet/kubelet.go#L1974\n. @vishh OK, I will commit it soon\n. ",
    "thockin": "I will take a look at this - how big is an Alpine+glibc base?  I am\nconcerned that people will see this and use alpines package manager which\nis just not going to work.\nOn Aug 14, 2015 12:52 PM, \"Vish Kannan\" notifications@github.com wrote:\n\nMerged #470 https://github.com/kubernetes/heapster/pull/470.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/470#event-382919827.\n. Glide still has shortcomings and is sort of in stasis until the Go\nvendoring workgroup produces results.  I would not swicth yet.  Godep is\ninscrutable, but we largely understand a way to use it that works.\n\nOn Wed, Jan 11, 2017 at 1:32 PM, Lucas K\u00e4ldstr\u00f6m notifications@github.com\nwrote:\n\nI would not switch to Glide in any k8s official projects before we have\ndone it for core (if we really want that, I'm not sure). Actually I'm in\nfavor for committing the vendor dir into the project, because once you've\ndownloaded the project with git, you don't have to fetch it with glide in\nscripts or manually.\ncc @thockin https://github.com/thockin for thoughts on glide\nThis will not at least make the next release, and I don't think there is\nany argument for doing so either.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1335#issuecomment-272001391,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AFVgVFCW_WdssasHTLfI9TzJFPEhmBGLks5rRUqAgaJpZM4KU0JG\n.\n. https://github.com/kubernetes/kubernetes/issues/10834. Reverted those parts\n\nOn Thu, Dec 28, 2017 at 3:02 AM, Daniel K\u0142obuszewski \nnotifications@github.com wrote:\n\n@x13n commented on this pull request.\nIn grafana/RELEASES.md\nhttps://github.com/kubernetes/heapster/pull/1919#discussion_r158928649:\n\n@@ -5,9 +5,9 @@\n - Support Grafana 4.4.1.\n\n## 4.0.2 (04-01-2017)\n-- Formalized the image name for every arch to gcr.io/google_containers/heapster-grafana-ARCH:VERSION http://gcr.io/google_containers/heapster-grafana-ARCH:VERSION\n+- Formalized the image name for every arch to k8s.gcr.io/heapster-grafana-ARCH:VERSION http://k8s.gcr.io/heapster-grafana-ARCH:VERSION\nSubstituting this in old release notes doesn't seem to be the right thing\nto do.\n\nIn influxdb/RELEASES.md\nhttps://github.com/kubernetes/heapster/pull/1919#discussion_r158928671:\n\n@@ -4,9 +4,9 @@\n - Updated to version v1.1.1; bumped Godeps and modified some code in heapster to use the latest schema\n\n## v0.13.0 (4.1.2016)\n-- Formalized the image name for every arch to gcr.io/google_containers/influxdb-grafana-ARCH:VERSION http://gcr.io/google_containers/influxdb-grafana-ARCH:VERSION\n+- Formalized the image name for every arch to k8s.gcr.io/influxdb-grafana-ARCH:VERSION http://k8s.gcr.io/influxdb-grafana-ARCH:VERSION\nditto\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1919#pullrequestreview-85802577,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AFVgVBafmfA-TWcfV553pPP7CQ0-jfSBks5tE3VggaJpZM4RLSVH\n.\n. trailing whitespace....\n. \n",
    "fgrzadkowski": "Obsolete.\n. This will be substituted by metrics API.\n. We don't want to do this right now. We might come back to this later.\n. This will be handled by metrics API.\n. Obsolete\n. Done in new heapster\n. @vishh Who will be working on those new features in heapster?\n. @vishh @dchen1107 Let's sync on Thu about this. I think it's important who owns this so that we all properly plan work for 1.2.\n. I definitely agree we need to work properly on 100 node cluster. However this might mean changing things like requests/limits for some components.\nHowever, from discussion with @mwielgus I understood that the problem is not that we OOM, but that used 1GB of memory in ~10 minutes. This gives ~1MB per pod. Sounds a bit too much.\n. Reassigning to @mwielgus who is driving now changes in heapster.\n. LGTM\n. I'm not sure why, but lgtm :)\n. We don't want to do this for now.\n. The test doesn't timeout anymore.\n. We don't panic anymore. Fixed\n. Fixed.\n. @bryk \n. We are about to release new version of heapster (0.20) which will solve this problem. \n. How this can be reproduced?\n. This will be fixed in new release (v0.20)\n. @timstclair - Tim, thanks for adding this new API! Since we are almost at the code freeze for 1.2 I think we won' be able to work on this for this release. I think we should come back to this once we are done with 1.2.\nBtw - does the new API export custom metrics?\n@dchen1107 @mwielgus @piosz \n. LGTM, except one comment.\n. LGTM\n. LGTM\n. Fixed in #968\nPlease let us know if this is still the problem.\n. It seems that there are two separate issues:\n- It seems that DNS is not working in your cluster. This means that you'll not be able to use addresses like http://monitoring-grafana:8086\n- Internal error in kubelet is somewhat separate story, but please report it in kubernetes/kubernetes and cross reference this issue.\n. Assigning to @mwielgus for further debugging.\n. @tsn77130 What exactly do you mean? Would it be a change in Heapster or just a configuration change in Influxdb?\n. Dup https://github.com/kubernetes/heapster/issues/1003\n. This sounds like out of scope for kubernetes monitoring.\n. Fixed in #992\n. P2 as it's not a blocker for 1.2\n. @dchen1107 I didn't know that. Thanks for noticing this. Bumping up priority and reassigning to @timstclair as it seems he owns this now.\n. Fixed\n. We have release new version that should include this fix v0.20.0-alpha9.\n. I meant heapster release. If you want to update this manually you should update RC definition for heapster on master in /etc/kubernetes/addons/cluster-monitoring or you can update 'cluster/addons/cluster-monitoring/...` and redeploy your cluster.\n. Tim, it seems that this code hasn't been tested at all, including fallback. The only part that has been tested is using old kubelet API from new source, which doesn't bring much value and information. Summary API is a very significant change and we need to be sure we don't break anything.\nInitially we decided not to rollback your previous PR (https://github.com/kubernetes/kubernetes/pull/22101) as we believed that the tests were passing. Now it seems that nothing of this is true.\nI propose to do the following:\n- revert https://github.com/kubernetes/kubernetes/pull/22101 \n- review & merge this PR\n- enable summary API only in a separate jenkins job (you can use GCE autoscaling suite)\n- do some thorough tests and write down the scenario with results\nIf it turns out that the new API is working we can enable it back.\nIf it's needed I'm happy to have a VC to discuss it further.\n. @timstclair Thanks for the explanation! Please let us know if you need any assistance with these tests.\n. @mwielgus Can you please review this PR?\n. @vishh +1\n@mwielgus Can you please document this? This is really high priority.\n. @andyxning Are you saying you'd like new heapster with kube-dash? I don't think it's possible. CC-ing @piosz for more details.\n. Since @mwielgus is busy with designing cluster autoscaling, reassigning to @piosz \n. Apart from being aligned with k8s (which is important), I think that another problem is that it will work well for very static setups. When we have more dynamic entities (e.g. pods) who will want to push metrics to heapster than I think that static certificate is not very user friendly, unless I'm missing something.\n. I disagree. We should implement a general mechanism that will serve multiple purposes. Your use case - single pusher for the whole cluster - is one of them, but no the only. Before we will actually implement it, we should make sure it will also work in other use-case, such as \"I have a pod that should be scaled based metric my-load and I'd like to push it somewhere so that it's available to HPA`.\n. One workaround could to show only those timeseries that have samples within the last X minutes. I'm not sure how expressive InfluxDB queries are though.\n. @piosz. @DirectXMan12 @piosz Fixed.. /lgtm. /approve. Adding @loburm who is working on some heapster related changes anyway. Something doesn't work with reopening. @andyxning, I think you deleted your branch?. @piosz Can you please review? As discussed offline, since you're back, I think you're much better person to review it then me :). Why do you want to indent this? I think it will consume more bytes as a result and since you sent it to influxdb anyway it probably doesn't matter how it's formatted.\n. It took quite some time to understand this statement. Can you please add a comment? Something like:\nWe want to return error only if it is not \"already exists\" error\n. What does it mean influxdb interactions? Does interactions have some special meaning in heapster?\n. Interesting... Maybe a TODO to check this?\n. Please add space after // and . at the end.\n. I guess this should be only in gcmNodeAutoscalingLabels\n. What does this timestamp represent? Please add comments.\n. Can you use int64Store here?\n. Start with capital letter.\n. s/takes/taken\n. Please update this comment.. For my own education - where does this name come from?. That's a strange unit for number of restarts.... Can you explain how does it work? What is metricLabels?. UnitsBytes.. UnitBytes. It might be good to somehow distinguish names for old and new resources model to avoid silly errors where someone intended to use a new metric, but instead IDE suggested old variable name.. Why are prom metrics defined in a file with metada describing metrics? That's surprising.. This should be a flag I guess.. Prom metrics should stay in this file. Why this if? I'm not very familiar with this code, but a comment would help.. Can you combine those two \"ifs\" into one to iterate over metrics only once and batch requests for old and new resource model together?. Please reuse code for parsing boolean flags. It's exactly the same here and few lines below.. I'd prefer not to introduce such refactoring. First it's not obvious for me, it'll actually help with readability. Secondly, I'd like this PR to be minimal. We can refactor later.. ",
    "dotNetDR": "@vishh \nThank you for your reply.\nI don't know how to find instantaneous values in that file.\nis /sinks/gcm/core.go ?\nsome metrics i see (ExternalSink interface) can only get cumulative.\nWhen I know how to get the right instantaneous values(or calculates), I will post a pull request.\n. ",
    "sanjana-bhat": "I signed it!\n. I will raise a PR again with commit including my email id associated with the CLA\n. Yes, I do get response for http://{master-ip}/api/v1/proxy/namespaces/kube-system/services/heapster/api/v1/model/\n[\n  \"metrics/\",\n  \"namespaces/\",\n  \"nodes/\"\n]\nHeapster version 0.17.0\nI do see these errors in the log\nW0827 03:52:04.018268       1 impl.go:515] Internal Error: reached unreachable CPU Usage parsing flow\nW0827 03:52:04.018483       1 impl.go:515] Internal Error: reached unreachable CPU Usage parsing flow\nW0827 03:52:04.018612       1 impl.go:515] Internal Error: reached unreachable CPU Usage parsing flow\nE0827 03:52:04.021118       1 util.go:69] Recovered from panic: \"invalid memory address or nil pointer dereference\" (runtime error: invalid memory address or nil pointer dereference)\n/usr/local/google/home/vishnuk/go/src/github.com/GoogleCloudPlatform/heapster/Godeps/_workspace/src/github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:63\n/usr/local/google/home/vishnuk/go/src/github.com/GoogleCloudPlatform/heapster/Godeps/_workspace/src/github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:54\n/usr/lib/google-golang/src/runtime/asm_amd64.s:401\n/usr/lib/google-golang/src/runtime/panic.go:387\n/usr/lib/google-golang/src/runtime/panic.go:42\n. Thanks @mvdan \n@afein, is there something that I'm missing for the APIs to work?\n. Heapster RC:\njson\n{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"ReplicationController\",\n    \"metadata\": {\n        \"labels\": {\n            \"k8s-app\": \"heapster\",\n            \"name\": \"heapster\",\n            \"version\": \"v6\"\n        },\n        \"name\": \"heapster\",\n        \"namespace\": \"kube-system\"\n    },\n    \"spec\": {\n        \"replicas\": 1,\n        \"selector\": {\n            \"k8s-app\": \"heapster\",\n            \"version\": \"v6\"\n        },\n        \"template\": {\n            \"metadata\": {\n                \"labels\": {\n                    \"k8s-app\": \"heapster\",\n                    \"version\": \"v6\"\n                }\n            },\n            \"spec\": {\n                \"containers\": [\n                    {\n                        \"image\": \"kubernetes/heapster:v0.17.0\",\n                        \"name\": \"heapster\",\n                        \"command\": [\n                            \"/heapster\",\n                            \"--source=kubernetes:http://{k8s-master-host}:6443?inClusterConfig=false&auth=\",\n                            \"--sink=influxdb:http://monitoring-influxdb:8086\",\n                            \"--use_model=true\",\n                            \"--model_resolution=1m\"\n                        ],\n                        \"ports\": [\n                            {\n                                \"containerPort\": 8082\n                            }\n                        ]\n                    }\n                ]\n            }\n        }\n    }\n}\nI will try with the docker image located at afein/heapster-model:latest\n. @afein, I still see the same errors\nW0901 01:44:30.442023       1 impl.go:171] failed to parse ContainerMetricElement: failed to add cpu-limit metric: failed to add metric to DayStore: the provided timepoint has a timestamp in the past\nW0901 01:44:30.445863       1 impl.go:171] failed to parse ContainerMetricElement: failed to calculate instantaneous CPU usage: the provided value 0 is less than the previous one 341040435488\nW0901 01:44:30.446051       1 impl.go:171] failed to parse ContainerMetricElement: failed to calculate instantaneous CPU usage: the provided value 0 is less than the previous one 341183797709\nW0901 01:44:30.446162       1 impl.go:171] failed to parse ContainerMetricElement: failed to calculate instantaneous CPU usage: the provided value 0 is less than the previous one 341311764352\nW0901 01:44:30.446275       1 impl.go:171] failed to parse ContainerMetricElement: failed to calculate instantaneous CPU usage: the provided value 0 is less than the previous one 341445209247\nE0901 01:44:30.446528       1 util.go:69] Recovered from panic: \"invalid memory address or nil pointer dereference\" (runtime error: invalid memory address or nil pointer dereference)\nE0901 01:46:07.622115       1 kube_events.go:127] failed to store events in cache: failed to store event {{NodeReady default  %node% map[] } Node %node% status is now: NodeReady kubelet 2015-09-01 01:35:30 +0000 UTC {{ } {%node%.13ffb6bd3e5c0336  default /api/v1/namespaces/default/events/%node%.13ffb6bd3e5c0336  333599 0 0001-01-01 00:00:00 +0000 UTC 2015-09-01 02:35:30 +0000 UTC map[] map[]} {Node  %node% %node%   } NodeReady Node %node% status is now: NodeReady {kubelet %node%} 2015-09-01 01:35:29 +0000 UTC 2015-09-01 01:35:30 +0000 UTC 2}} - UID cannot be empty\n. Heapster service file:\njson\n{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Service\",\n    \"metadata\": {\n        \"labels\": {\n            \"kubernetes.io/cluster-service\": \"true\",\n            \"kubernetes.io/name\": \"Heapster\"\n        },\n        \"name\": \"heapster\",\n        \"namespace\": \"kube-system\"\n    },\n    \"spec\": {\n        \"ports\": [\n            {\n                \"port\": 8082,\n                \"targetPort\": 8082\n            }\n        ],\n        \"selector\": {\n            \"k8s-app\": \"heapster\"\n        }\n    }\n}\n. @afein \nkubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"1+\", GitVersion:\"v1.1.0-alpha.0.2003+b3bce3aa608834\", GitCommit:\"b3bce3aa6088345f57c9ae8c6be5802b114a181d\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"1+\", GitVersion:\"v1.1.0-alpha.0.2003+b3bce3aa608834\", GitCommit:\"b3bce3aa6088345f57c9ae8c6be5802b114a181d\", GitTreeState:\"clean\"}\n. @afein \nI cleaned up etcd to remove all the dead nodes and I don't see the cpu related errors. Also I explicitly create kube-system namespace as that was missing in etcd. After this, only /api/v1/model/nodes/%node%/stats/ returns me non-empty response. All the other endpoints including /api/v1/model/metrics return empty response.\njson\n{\n  \"uptime\": 18446744064486179580,\n  \"stats\": {\n   \"cpu-limit\": {\n    \"minute\": {\n     \"average\": 2000,\n     \"percentile\": 2000,\n     \"max\": 2000\n    },\n    \"hour\": {\n     \"average\": 2000,\n     \"percentile\": 2000,\n     \"max\": 2000\n    },\n    \"day\": {\n     \"average\": 2000,\n     \"percentile\": 2000,\n     \"max\": 2000\n    }\n   },\n   \"cpu-usage\": {\n    \"minute\": {\n     \"average\": 255,\n     \"percentile\": 255,\n     \"max\": 255\n    },\n    \"hour\": {\n     \"average\": 913,\n     \"percentile\": 1590,\n     \"max\": 1650\n    },\n    \"day\": {\n     \"average\": 1085,\n     \"percentile\": 1650,\n     \"max\": 1650\n    }\n   },\n   \"fs-limit-dev-vda\": {\n    \"minute\": {\n     \"average\": 26355142656,\n     \"percentile\": 26355142656,\n     \"max\": 26355142656\n    },\n    \"hour\": {\n     \"average\": 26355142700,\n     \"percentile\": 26355142700,\n     \"max\": 26355142700\n    },\n    \"day\": {\n     \"average\": 26355142700,\n     \"percentile\": 26355142700,\n     \"max\": 26355142700\n    }\n   },\n   \"fs-usage-dev-vda\": {\n    \"minute\": {\n     \"average\": 22341485909,\n     \"percentile\": 22341632000,\n     \"max\": 22341632000\n    },\n    \"hour\": {\n     \"average\": 22317888664,\n     \"percentile\": 22339956200,\n     \"max\": 22341632000\n    },\n    \"day\": {\n     \"average\": 22312987970,\n     \"percentile\": 22327245300,\n     \"max\": 22336712704\n    }\n   },\n   \"memory-limit\": {\n    \"minute\": {\n     \"average\": 4145524736,\n     \"percentile\": 4145524736,\n     \"max\": 4145524736\n    },\n    \"hour\": {\n     \"average\": 4148166656,\n     \"percentile\": 4148166656,\n     \"max\": 4148166656\n    },\n    \"day\": {\n     \"average\": 4148166656,\n     \"percentile\": 4148166656,\n     \"max\": 4148166656\n    }\n   },\n   \"memory-usage\": {\n    \"minute\": {\n     \"average\": 2607667882,\n     \"percentile\": 2607861760,\n     \"max\": 2607861760\n    },\n    \"hour\": {\n     \"average\": 2720761118,\n     \"percentile\": 2864709632,\n     \"max\": 2873098240\n    },\n    \"day\": {\n     \"average\": 2748737126,\n     \"percentile\": 2873098240,\n     \"max\": 2873098240\n    }\n   },\n   \"memory-working\": {\n    \"minute\": {\n     \"average\": 1656687274,\n     \"percentile\": 1656778752,\n     \"max\": 1656778752\n    },\n    \"hour\": {\n     \"average\": 1959578828,\n     \"percentile\": 2378170368,\n     \"max\": 2390753280\n    },\n    \"day\": {\n     \"average\": 2034656870,\n     \"percentile\": 2390753280,\n     \"max\": 2390753280\n    }\n   }\n  }\n. @afein and @vishh \nI increased the log level and noticed the model data structure is not being updated because of the following warnings \"Internal Error: reached unreachable CPU Usage parsing flow\" and a nil pointer dereference. After debugging I found out that prevRoundedStamp and roundedStamp values are the same and the following condition https://github.com/kubernetes/heapster/blob/master/model/impl.go#L541 is false most of the time. \nThis is the panic error caused by https://github.com/kubernetes/heapster/blob/master/model/impl.go#L531:\nE0903 07:16:32.222075 1 util.go:69] Recovered from panic: \"invalid memory address or nil pointer dereference\" (runtime error: invalid memory address or nil pointer dereference)\n/home/sanjanab/go/bin/src/k8s.io/heapster/Godeps/_workspace/src/k8s.io/kubernetes/pkg/util/util.go:63\n/home/sanjanab/go/bin/src/k8s.io/heapster/Godeps/_workspace/src/k8s.io/kubernetes/pkg/util/util.go:54\n/home/sanjanab/go/src/runtime/asm_amd64.s:401\n/home/sanjanab/go/src/runtime/panic.go:387\n/home/sanjanab/go/src/runtime/panic.go:42\n/home/sanjanab/go/src/runtime/sigpanic_unix.go:26\n/home/sanjanab/go/bin/src/k8s.io/heapster/model/impl.go:531\nI use the default poll_duration and stats_resolution values whereas model_resolution is set to 2m. Is there anything wrong with the settings/setup? \n. @afein, it does seem that way. But cpu/usage values are zero only for specific containers under /system.slice cgroup. But this shouldn't cause the API to return empty response at the cluster, node or namespace level right? Not all values are zero. \nMay be add a condition here https://github.com/kubernetes/heapster/blob/master/model/impl.go#L530 else if ok so that nil pointer dereference doesn't occur? \nHere is a sample response from the above API call:\njson\n[\n    {\n        \"labels\": {\n            \"container_base_image\": \"\",\n            \"container_name\": \"k8s_etcd.aa8dfebb_kube-dns-v8-kfmsf_kube-system_86936fe4-5207-11e5-9c3e-fa163e56b235_39cdc306\",\n            \"host_id\": \"illusioninfusion\",\n            \"hostname\": \"illusioninfusion\"\n        },\n        \"metrics\": {\n            \"cpu/limit\": [\n                {\n                    \"start\": \"2015-09-03T18:41:39Z\",\n                    \"end\": \"2015-09-03T18:41:39Z\",\n                    \"value\": 99\n                }\n            ],\n            \"cpu/usage\": [\n                {\n                    \"start\": \"2015-09-03T06:46:50Z\",\n                    \"end\": \"2015-09-03T18:41:39Z\",\n                    \"value\": 146200165151\n                }\n            ],\n            \"memory/limit\": [\n                {\n                    \"start\": \"2015-09-03T18:41:39Z\",\n                    \"end\": \"2015-09-03T18:41:39Z\",\n                    \"value\": 52428800\n                }\n            ],\n            \"memory/major_page_faults\": [\n                {\n                    \"start\": \"2015-09-03T06:46:50Z\",\n                    \"end\": \"2015-09-03T18:41:39Z\",\n                    \"value\": 63\n                }\n            ],\n            \"memory/page_faults\": [\n                {\n                    \"start\": \"2015-09-03T06:46:50Z\",\n                    \"end\": \"2015-09-03T18:41:39Z\",\n                    \"value\": 12408\n                }\n            ],\n            \"memory/usage\": [\n                {\n                    \"start\": \"2015-09-03T18:41:39Z\",\n                    \"end\": \"2015-09-03T18:41:39Z\",\n                    \"value\": 19165184\n                }\n            ],\n            \"memory/working_set\": [\n                {\n                    \"start\": \"2015-09-03T18:41:39Z\",\n                    \"end\": \"2015-09-03T18:41:39Z\",\n                    \"value\": 12181504\n                }\n            ],\n            \"network/rx\": [\n                {\n                    \"start\": \"2015-09-03T06:46:50Z\",\n                    \"end\": \"2015-09-03T18:41:39Z\",\n                    \"value\": 0\n                }\n            ],\n            \"network/rx_errors\": [\n                {\n                    \"start\": \"2015-09-03T06:46:50Z\",\n                    \"end\": \"2015-09-03T18:41:39Z\",\n                    \"value\": 0\n                }\n            ],\n            \"network/tx\": [\n                {\n                    \"start\": \"2015-09-03T06:46:50Z\",\n                    \"end\": \"2015-09-03T18:41:39Z\",\n                    \"value\": 0\n                }\n            ],\n            \"network/tx_errors\": [\n                {\n                    \"start\": \"2015-09-03T06:46:50Z\",\n                    \"end\": \"2015-09-03T18:41:39Z\",\n                    \"value\": 0\n                }\n            ],\n            \"uptime\": [\n                {\n                    \"start\": \"2015-09-03T06:46:50Z\",\n                    \"end\": \"2015-09-03T18:41:39Z\",\n                    \"value\": 42900007\n                }\n            ]\n        }\n    },\n    {\n        \"labels\": {\n            \"container_base_image\": \"\",\n            \"container_name\": \"/system.slice/mdmonitor.service\",\n            \"host_id\": \"illusioninfusion\",\n            \"hostname\": \"illusioninfusion\"\n        },\n        \"metrics\": {\n            \"cpu/limit\": [\n                {\n                    \"start\": \"2015-09-03T18:41:33Z\",\n                    \"end\": \"2015-09-03T18:41:33Z\",\n                    \"value\": 1000\n                }\n            ],\n            \"cpu/usage\": [\n                {\n                    \"start\": \"2015-09-02T18:48:42Z\",\n                    \"end\": \"2015-09-03T18:41:33Z\",\n                    \"value\": 0\n                }\n            ],\n            \"uptime\": [\n                {\n                    \"start\": \"2015-09-02T18:48:42Z\",\n                    \"end\": \"2015-09-03T18:41:33Z\",\n                    \"value\": 85988219\n                }\n            ]\n        }\n    }\n]\n. @afein, I pulled in this condition in my heapster docker image and I see non-empty response for /metrics. Has /stats being removed recently? I built the docker image from the master branch.\n. Great! Yes, everything looks okay now. Thanks!\n. @afein, when is the next heapster release? Can I expect these changes to be a part of that?\n. @afein, /stats gives average, max and 95th percentile over the past minute. Suppose time now is 23:30, /stats returns average for 23:29?\n. I have specified model_resolution to 1 min. Is that considered or it always takes default? And if that is the case the aggregation should be for 23:29 through 23:30 right?\n. @afein and @vishh, what are the recommended resource specifications for heapster container? \n. ",
    "ntquyen": "@vishh I'm using heapster 0.19.0 in k8s 1.2.0 and getting this error couple times in a few days\n...\nW0420 04:09:44.699088       1 impl.go:225] failed to parse ContainerMetricElement: failed to calculate instantaneous CPU usage: the provided value 345625445296 is less than the previous one 370196565249\nW0420 04:09:44.699281       1 impl.go:225] failed to parse ContainerMetricElement: failed to calculate instantaneous CPU usage: the provided value 356329092212 is less than the previous one 370196565249\nW0420 04:09:44.699440       1 impl.go:225] failed to parse ContainerMetricElement: failed to calculate instantaneous CPU usage: the provided value 359914434773 is less than the previous one 370196565249\nW0420 04:09:44.699537       1 impl.go:225] failed to parse ContainerMetricElement: failed to calculate instantaneous CPU usage: the provided value 364883164555 is less than the previous one 370196565249\n...\nheapster pod yaml config:\n...\n- image: kubernetes/heapster:v0.19.0\n  name: heapster\n  resources:\n    limits:\n      memory: 300Mi\n  ports:\n    - name: heapster-api\n      containerPort: 8082\n      hostPort: 8082\n  command:\n    - /heapster\n    - --source=kubernetes:http://api-server:8080\n    - --sink=influxdb:http://influxdb-server:8086\n    - --stats_resolution=10s\n    - --sink_frequency=15s\n...\nheapster stop sending metrics to influxdb when this error occurred unless I restart the container. \nSo please help me more clear on the problem. Is it an async problem between cadvisor and heapster?\n. CoreOS support is currently suspended. https://github.com/kubernetes/heapster/issues/115#issuecomment-195324116\n. @DirectXMan12 I got the same problem, heapster not collect haft of my nodes. Switching to --source=kubernetes.summary_api not help. And heapster keep printing:\nE0414 06:46:19.750431       1 streamwatcher.go:109] Unable to decode an event from the watch stream: object to decode was longer than maximum allowed size\nE0414 06:46:19.884893       1 streamwatcher.go:109] Unable to decode an event from the watch stream: object to decode was longer than maximum allowed size\nW0414 06:46:19.885646       1 reflector.go:330] k8s.io/heapster/metrics/heapster.go:327: watch of *api.Node ended with: very short watch\nE0414 06:46:29.480993       1 streamwatcher.go:109] Unable to decode an event from the watch stream: object to decode was longer than maximum allowed size\nI use gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1, all my nodes running CoreOS stable versions. ",
    "mwringe": "As far as I can tell, the gcmautoscaling sink is just a scaled down version of the gcm sink. Its only meant to gather the specific metrics autoscaling needs (cpu and memory) instead of all the metrics.\nThe Hawkular sink is already storing all the metrics coming in from Heapster, so it has access to the same cpu and memory metrics.\nIs there a link to a card for autoscaling in OpenShift you can post?\n. > I would like to integrate hawkular support for auto-scaling through this mechanism.\nAwesome :)\n. What we are doing here for Hawkular will be specific to the Hawkular sink in Heapster, so its not a generic solution for other sinks.\nWe separate out the metrics from each individual project and store them into separate tenants in Hawkular (https://github.com/kubernetes/heapster/pull/555). In Hawkular will then require a token to be sent with any request and it will be used to authenticate against the OpenShift server.\nWe need to run some more tests on it though before the Hawkular container will support this security setup.\n. Is the issue with the check failing due to tests failing, or the test environment not working. I am not sure what this broken pipe error seems to mean in the logs: http://104.154.52.74/job/heapster-e2e-gce/2787/console\n. Is there any thing we can do to speed this up?\nWe really need a released version soon if possible.\n. All checks passed, so does that mean we are good?\n. NAME      LABELS                          STATUS    AGE\nmynode    kubernetes.io/hostname=mynode   Ready     18h\n. Note that it will work if I happen to name my node the same as its IP address (which is why it just happens to work on my machine but not on any others)\n. The node yaml output:\napiVersion: v1\nkind: Node\nmetadata:\n  creationTimestamp: 2015-10-20T19:24:54Z\n  labels:\n    kubernetes.io/hostname: mynode\n  name: mynode\n  resourceVersion: \"944\"\n  selfLink: /api/v1/nodes/mynode\n  uid: 3e05f4bd-7760-11e5-a89f-3c970e88a56f\nspec:\n  externalID: mynode\nstatus:\n  addresses:\n  - address: 192.168.122.1\n    type: LegacyHostIP\n  - address: 192.168.122.1\n    type: InternalIP\n  capacity:\n    cpu: \"4\"\n    memory: 11997300Ki\n    pods: \"40\"\n  conditions:\n  - lastHeartbeatTime: 2015-10-21T13:46:35Z\n    lastTransitionTime: 2015-10-21T13:01:13Z\n    message: kubelet is posting ready status\n    reason: KubeletReady\n    status: \"True\"\n    type: Ready\n  daemonEndpoints:\n    kubeletEndpoint:\n      Port: 10250\n  nodeInfo:\n    bootID: c8a62c9a-c8b2-4f61-beff-9578eaf25e23\n    containerRuntimeVersion: docker://1.8.1.fc21\n    kernelVersion: 4.1.6-100.fc21.x86_64\n    kubeProxyVersion: v1.2.0-alpha.1-1107-g4c8e6f4\n    kubeletVersion: v1.2.0-alpha.1-1107-g4c8e6f4\n    machineID: 3cb44cba06684ed79dd063d7fa1734b2\n    osImage: Fedora 21 (Twenty One)\n    systemUUID: 81DD6B6F-5C52-CB11-B83D-8C85C64DBBDB\nThe Heapster code looks for the IP address when connecting to the node and not the name\n. > What setup are you using to create the node's certificate?\nopenshift start --write-config=openshift.local.config --hostname=myNode --public-master=$IP_ADDRESS --master=$IP_ADDRESS\nAs a work around, if I set --hostname=$IP_ADDRESS then it can connect\n. I don't believe #664 does not fix this issue, I believe it only allows insecure to work with the kubelet endpoint and this is a separate issue.\n. > #664 \"fixes\" it for a cluster that doesn't have certificates correctly set up as you had. Heapster cannot fix up poorly configured clusters but we can work with it...\nAre we sure this is a broken setup? Not specifying the IP address in a certificate may not be the best idea, but I don't know if I would say its an invalid setup.\n. Ok, makes sense. Do we have a corresponding issue in OpenShift to track this?\n. @liggitt Ok, I created one here: https://github.com/openshift/origin/issues/5294\n. I already have a patch getting ready for this, the bigger issue is that heapster broke compatibility with OpenShift about a month ago (which I also have a patch almost ready to commit, I just need to update the tests). \nHow does one get added as someone who can take tasks?\n. The hostname is only set to the node name if the hostname is empty. Which I don't think should be the case in most situations.\nThere is also the host_id which corresponds to the external id of the node. If the system doesn't overwrite the external id, then it should default to the node name. The external id is also deprecated and probably not something we should continue to rely on (https://github.com/kubernetes/kubernetes/blob/release-1.0/pkg/api/v1/types.go#L1256)\nThe problem is that in both the case of the hostname and host_id, its non deterministic that you will ever get back the node name.\n. There are a lot of people running into the problem where they cannot connect to the kubelet endpoints due to this. OpenShift should soon have the ability to update the certificates when performing an update, so it may be less of an issue when new releases.\n. v0.18.2 is almost 2 months ago and is out of date with our needs. We did get a bunch of the commits we required into the release-0.18.0 branch, but no release has happened yet which contains those commits. Even if a release were to happen for 0.18.0 we would still need to go in and back port a commit or two.\nI almost have a PR which will bring master up to the latest v1.2.0 alpha, if that makes sense for the master branch.\n. Yeah, getting a proper release from the release-0.18.0 branch would be the best solution :)\nWe probably want to backport a few commits from master in there, and to get the commits from https://github.com/kubernetes/heapster/issues/731 in there\nUpdating the heapster master to the v1.2.0 alphas is probably also a good idea\n. The patch I have is not that complex, other than updating the godeps to the newer 1.2.0 alphas, there is just some minor changes which need to be made. \n. I signed it!\n. It looks like with the last test failure that there was a timeout with github.com\nIs there anyway to rerun the tests?\nSorry for the multiple commits, I am still trying to get a handle on godeps. I will squash once the tests pass\n. retest this please\n. Anyone know what is going on with the CI here? It doesn't look like 'retest this please' is working (or at least not for me :)\n. Bump. What exactly is the hold up from getting this merged?\n. @jimmidyson I don't believe the heapster-scalability branch has this issue. I can run that and see metrics coming in (I can't get it to recognize that the Hawkular sink is configured, but the log sink is showing metrics). This was something specific to the master branch and the release-0.18.0 didn't have this problem (at least at the last time I checked)\n. @cboggs If you are using Heapster master on OpenShift then unfortunately you will not be able to properly receive metrics until this PR is merged. The symptoms you are describing is what I would be expecting. \n. @cboggs I tried again with the very latest from heapster-scalability (9582c86) and I could see metrics being properly propagated into the Hawkular Metrics sink. Not sure if I was doing something strange before, or if a more recent commit fixed things.\nLets open a new issue to discuss\n. Anyone know what is going on with the e2e tests? I see that they have been failing for other PRs but in a different manner than this one. I have never been able to get the integration tests to run properly locally.\n. This PR has been opened for over 2 months now. What exactly is the holdup?\nThe Jenkins GCE e2e tests seem to be completely broken, and they are not passing with other PRs which have been merged.\n. @k8s-bot: retest this please\n. @vishh @mwielgus @piosz What is the update on this?\n. This has nothing to do with the hostname. The hostname and the node name are completely separate.\nThis more similarities to the existing 'host_id' label. We could change the host_id label to be node name, but this changes existing behaviour and may cause backwards compatibility issues.\nThe problem is that there is currently no label which will always give you the name of the node. Being able to query based on the node name is an important feature, especially if you end up migrating your node from one machine to another.\n. I don't really have a setup for testing with GCM. I can add in something to prevent it from being added to the GCM sink if that is required, but I can't easily test and verify it.\nIs there a unit test to take into account this limitation with GCM?\n. This sounds like a very specific GCM issue, which I think should be handled by someone who is more familiar with the GCM sink and this issue. I don't use GCM and I don't have a system configured to test on GCM at the moment.\n. @vishh I am not sure how exactly to make progress with this PR. There is an issue where the GCM sink can only support 10 labels and this PR pushes the label count to 11.\nSo with this PR the GCM sink wont be able to function properly. I don't have a GCM account setup to test, and I don't know how exactly to handle this situation.\nIt seems like someone familiar with the GCM sink should be making the decisions on how it should work.  You may need to open up a new configuration option for GCM to allow admins to specify which labels they actually want or not, or perhaps just hardcode it in the sink.\n. The GCM sink needs to be updated to support handling more than 10 labels. Or make it more generic and add an option to be able to filter to any sink the list of labels to be considered.\n. It wont be by default. The user would have to explicitly configure the sink to trust very specific certificates.\nLooking at the current options, we don't actually pass the server certificate but the CA certificate. As part of this we will need to add a new option to specify the server certificate to trust.\n. It could be related to https://github.com/kubernetes/heapster/pull/749\nHeapster broke compatibility with OpenShift when it refactored some of its dependencies and for some reason the PR to get it back running have not yet been accepted.\n. @cboggs do you mean what to use for the --source option?\nThe standalone Heapster configuration file we uses this https://github.com/openshift/origin-metrics/blob/master/deployer/templates/heapster-standalone.yaml#L71\nSee also https://github.com/openshift/origin-metrics#deploying-only-heapster\n. Do you know what problems you are running into with performing your builds? Godeps can be interesting to deal with.\nI just tried out these steps and it built:\nexport GOPATH=/tmp/heapster\nmkdir -p $GOPATH/src/k8s.io/\ncd $GOPATH/src/k8s.io/\ngit clone https://github.com/mwringe/heapster\ncd heapster\ngit checkout update-1.2.0\nmake\nAt this point I don't know what to do about trying to get https://github.com/kubernetes/heapster/pull/749 merged in any faster. They are already even using the same 1.2.0 alpha in the heapster-scalability branch.\n. If you run into issues building, please let us know what the command used and what the error message is. We might be able to help debug the build problem.\nI am almost tempted to start carrying the patch in the OpenShift metric images for Heapster, just so we can get something up and running again on OpenShift which is a bit newer. But I would really prefer to get the patch properly merged.\n. > Had to add steps to install godep\nAh, you need to add $GOPATH/bin to $PATH for godeps to automatically be available. I just created an issue for this (https://github.com/kubernetes/heapster/issues/906). Since its being installed as part of the build, we should configure it to be actually used by the build.\n\nbut I'm getting flooded with 404's for containers that never managed to spin up inside the pending pods that kubernetes sees.\n\nFrom the Heapster logs or from somewhere else? How verbose the Heapster logs are can be configured, and when its configured to be verbose it can be very verbose.\n. Ah, ok, I am seeing those as well. It looks like its throwing an error when its trying to fetch metrics for containers which are no longer running\n. @burmanm Can you take a look?\n. Looks to be this issue: https://github.com/golang/go/issues/13032\nWhich should be fixed in the next Go release. Cannot reproduce with Go 1.6 Beta 2\n. Its not part of the go install as far as I can tell: https://golang.org/doc/install\nAnd its not something that the system installed go does on RHEL or Fedora systems either.\n. Is there anyway I could get this assigned to me?\n. Can we please re-open this issue?\n922 just adds a check to get around the issue and is not addressing the main problem.\nThere are a few things happening here and it has to do with how we handle metrics coming in from /stats endpoint and how we handle metrics which are not available under /stats.\nFor metrics which we get from the /stats endpoint, we go through and add the hostname label to them (https://github.com/kubernetes/heapster/blob/heapster-scalability/metrics/sources/kubelet/kubelet.go#L94)\nBut, we also get the list of pods from the cluster. If any of those pods are not currently accounted for in the list of metrics, then we try and add them: https://github.com/kubernetes/heapster/blob/heapster-scalability/metrics/processors/pod_based_enricher.go#L56\nThe issue is we are forgetting to add the hostname attribute here. So all that we need to do is to add core.LabelNodename.Key: pod.Spec.NodeName, and then this issue goes away (at least for completed containers, I suspect it would be the same for the other scenarios mentioned in #922). At the very least we should be logging a warning in #922 and not logging this situations as info level.\nI would have submitted a PR to add in the nodename to the pods coming in from the pods list and not /stats, but I am unclear why exactly we are trying to add these types of pods (https://github.com/kubernetes/heapster/blob/heapster-scalability/metrics/processors/pod_based_enricher.go#L56)\nI would have assumed that since there are no metrics for these pods, that we would have just ignored these pods instead of trying to add them back in.\nAnyone know why we are trying to add metric-less pods?\n. Can we get metrics for pods which are not currently in the running state? Wondering if other states, such as pending, may also have metrics which could be exposed\n. @gravis How exactly does this relate to origin-metrics? This doesn't sound like the issue with the yaml file would apply to origin-metrics. Could you please open a new issue for the issue you are seeing when running with the origin-metrics components (https://github.com/openshift/origin-metrics/issues)?\n. @gravis its not mixing up projects and if you have specific questions or issues about Heapster, then please file them here :) The origin-metrics project does things a bit differently in terms of deployment and if you hare having problems deploying components than its likely the fault of origin-metrics and not related to Heapster directly.\n. @jimmidyson ok\n. Sorry, I have been busy with some other stuff, and wow, I can't believe its been so long since I was looking at this. I should hopefully have time to get back to this in the next couple of days.\nI had it partially working with the Hawkular sink, but the lack of differentiation labels on LabeledMetrics meant that they could not be properly categorized and stored. I think I had to add the host labels and then it was working correctly, but I would have to go back and check what I exactly I had working.\n. @piosz I think we can close this, we have labeledMetric support\n. @mwielgus It believe it did work in the heapster-scalablity branch before this commit: https://github.com/kubernetes/heapster/commit/8ad5353a3dd6dc00eeb24eb4da778e566d26178c\n@liggitt Yeah, we should really have a test for this to prevent regressions\n. rebased\n. Any objections?\n. Updated to use V(4) logging as suggested. Squashed commits.\n. Fixed by https://github.com/kubernetes/heapster/pull/1111\nWe are just waiting for that PR to go through and then we can update the origin metrics components. If we don't get a response from that today, we may just have to fork it in a private branch until Heapster approved the PR\n. @simon3z yeah, its probably a good idea to make it configurable in the deployer. By default we are going to set it back to 10s like before, but we should also allow a user to configure it if required.\n. @simon3z Yeah, I should be closed but I don't have the power to close it :)\n@jimmidyson can you close this issue? The pr to fix it has already been merged for a while now: https://github.com/kubernetes/heapster/pull/1111\n. Updated\n. Is there any reason we are calling the function 'pointToLabeledMetricHeader' instead of the old 'pointToMetricHeader'? If it is now handling both normal metrics and labeled metric the same, having 'LabeledMetric' in the function name might be a bit confusing.\nOther than that, LGTM\n. LGTM\n. > For instance, to get raw metrics from Hawkular, you need to know the pod id, AFAICT (@mwringe et all might be able to correct me on this). For aggregations, I'm not sure how Hawkular deals with a query which could return multiple time series, but I suspect we'll need it there as well.\nHmm, you shouldn't need to know the pod id to get the raw data, you should be able to do a query based on any of the labels. But there may have been an issue preventing this in the past.\nCan you get in touch with Stefan Negrea (he is stefan_n in #hawkular on freenode) to answer your questions?\n. FYI, it looks like the test failure is not related to this commit.\n. There is no 'heapster version' command. Heapster always prints out the version as part of its startup, and as you can see in both casese it ends in an error. All that has changed between 1.2.0 and master is the error message.\n. We are using 1.2 in OpenShift (based on Kubernetes 1.4), but with kubernetes source and not the kubernetes.summary_api.\nYou can see our option here: https://github.com/openshift/origin-metrics/blob/master/deployer/templates/heapster.yaml#L85\nWhere url we use is \"https://kubernetes.default.svc:443\"\nIf you use the kubernetes source, does this work for you?\n. @burmanm can you take a look if you have the chance?. I signed it!. @DirectXMan12 can you take a look? Thanks. Ok, I updated the name of the field as requested by @burmanm \nI still believe we need to keep the \"labels\" tag around for backwards compatibility reasons. But we can wait for @burmanm's response.. @DirectXMan12 Can this be merged now?. @DirectXMan12 bump. Which V level exactly then? just V(1) or something higher?\n. > When can we get into this state?\nIf you set the metric_resolution to 10s then this error occurs. The default 30s doesn't have this issue. Not sure why exactly its happening in this situation, but the bug still remains that we trying to get the node, finding it doesn't exist, and then trying on the next line to use this nil object when we shouldn't be using it.\n\nIs this really a warning or lower log level?\n\nI left the log level at its current state, if you think it should be a lower log level then we can change it.\n\nI'd also prefer to just continue if not found rather than if else.\n\nWe can't just continue because the next 'if' line tries to call the aggregate method with the nil node which causes a nil pointer exception. That is what this simple patch is trying to avoid.\n. I updated this to remove one of the else statements in there, not sure if that was exactly what you were expecting or not though.\n. The main reason is for backwards compatibility. We don't want to break things for people who are currently using the \"labels\" tag.\nAnother reason is that with the \"labels\" tag we can use regex on the label names themselves (eg find me metrics which have a label which starts with \"foo\"), which I don't think we can do if we only have the split labels being stored.. Ok, changed. should this be 'disablePreCache' or just 'disableCache'. Do we have multiple types of caches?. ",
    "plamer": "Hey @vishh ,\nSomething like: \"labels =~/k8s-app:heapster/\"? In the web  Because that doesn't seem to work either.\n. For both, the error is the same: label doesn't exist. This is the pod:\n[root@k8s-master ~]# kubectl --namespace=kube-system describe pod monitoring-heapster-v8-c4ms9\nName:               monitoring-heapster-v8-c4ms9\nNamespace:          kube-system\nImage(s):           gcr.io/google_containers/heapster:v0.17.0\nNode:               .../...\nLabels:             k8s-app=heapster,kubernetes.io/cluster-service=true,version=v8\nStatus:             Running\nReason:\nMessage:\nIP:             10.16.2.5\nReplication Controllers:    monitoring-heapster-v8 (1/1 replicas created)\nContainers:\n  heapster:\n    Image:  gcr.io/google_containers/heapster:v0.17.0\n    Limits:\n      cpu:      100m\n      memory:       300Mi\n    State:      Running\n      Started:      Sat, 29 Aug 2015 16:56:14 +0300\n    Ready:      True\n    Restart Count:  0\nConditions:\n  Type      Status\n  Ready     True\nNo events.\n. Hm.. am I looking at the right place? I'm searching in k8s database, with the following query:\nhttp://drive.cmailpro.net/~p.kalchev@icn.bg/protected/pwd/27a14288a5d571af65969c580ea39e/Screen%20Shot%202015-09-04%20at%206.01.58%20PM.png\nCan't see \"label\" field?\n. ",
    "korczis": "Any updates? I am still experiencing same problems.\n. Heapster 0.13, influx 0.3. I am following this tutorial\nhttps://github.com/vishh/heapster-1/blob/master/docs/coreos.md\nOn Thu, Mar 24, 2016, 16:07 Marcin Wielgus notifications@github.com wrote:\n\n@korczis https://github.com/korczis What Heapster/influxdb version?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/507#issuecomment-200877833\n. \n",
    "NOX73": "I have the same issue. I have three nodes, but it shows metrics only from two of those.\n. @vishh sure. I run heapster with influxsb & grafana:\n``` bash\n$ kubectl get nodes\nNAME            LABELS                                 STATUS\n192.168.30.11   kubernetes.io/hostname=192.168.30.11   Ready\n192.168.30.12   kubernetes.io/hostname=192.168.30.12   Ready\n192.168.30.14   kubernetes.io/hostname=192.168.30.14   Ready\n```\n``` bash\n$ kubectl get pods\nNAME                    READY     STATUS    RESTARTS   AGE\nheapster-i1p6s          1/1       Running   0          23h\ninfludb-grafana-n7r10   2/2       Running   0          1d\nkube-dns-v4-rwohe       3/3       Running   3          3d\nkube-ui-v1-xtsti        1/1       Running   0          3d\n```\nbash\n$ kubectl logs heapster-i1p6s\nI0905 10:15:51.614496       1 heapster.go:55] /heapster --source=kubernetes:http://192.168.30.2:8080 --sink=influxdb:http://monitoring-influxdb:8086\nI0905 10:15:51.614557       1 heapster.go:56] Heapster version 0.17.0\nE0905 10:15:51.614632       1 helper.go:247] expected to load root ca config from /var/run/secrets/kubernetes.io/serviceaccount/ca.crt, but got err: open /var/run/secrets/kubernetes.io/serviceaccount/ca.crt: no such file or directory\nI0905 10:15:51.614706       1 kube_factory.go:168] Using Kubernetes client with master \"http://192.168.30.2:8080\" and version \"v1\"\nI0905 10:15:51.614722       1 kube_factory.go:169] Using kubelet port 10255\nI0905 10:15:51.622892       1 driver.go:376] created influxdb sink with options: {root root monitoring-influxdb:8086 k8s false}\nI0905 10:15:51.626697       1 heapster.go:66] Starting heapster on port 8082\nE0905 11:41:33.543960       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [74377/73811]) [75376]\nE0905 12:40:39.647016       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [75452/75377]) [76451]\nE0905 14:18:21.181591       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [77229/76453]) [78228]\nE0905 15:17:40.914829       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [78308/78229]) [79307]\nE0905 15:46:16.775288       1 driver.go:235] failed to write stats to influxDB - Post http://monitoring-influxdb:8086/db/k8s/series?u=root&p=root&time_precision=s: dial tcp: lookup monitoring-influxdb: no such host\nE0905 16:17:36.695847       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [79400/79309]) [80399]\nE0905 17:32:39.776462       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [80765/80401]) [81764]\nE0905 18:29:33.833213       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [81801/81766]) [82800]\nE0905 18:46:22.582508       1 driver.go:235] failed to write stats to influxDB - Post http://monitoring-influxdb:8086/db/k8s/series?u=root&p=root&time_precision=s: dial tcp: lookup monitoring-influxdb: no such host\nE0905 19:52:36.871247       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [83319/82802]) [84318]\nE0905 21:37:56.957708       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [85234/84319]) [86233]\nE0905 21:46:28.278056       1 driver.go:235] failed to write stats to influxDB - Post http://monitoring-influxdb:8086/db/k8s/series?u=root&p=root&time_precision=s: dial tcp: lookup monitoring-influxdb: no such host\nE0905 23:27:05.381356       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [87220/86234]) [88219]\nE0906 00:46:38.663397       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [88669/88220]) [89668]\nE0906 02:13:33.865975       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [90249/89669]) [91248]\nE0906 03:45:51.049016       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [91926/91250]) [92925]\nE0906 05:16:11.084423       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [93570/92928]) [94569]\nE0906 07:05:17.620307       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [95558/94570]) [96557]\nE0906 08:26:19.942406       1 reflector.go:183] watch of *api.Pod ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [97032/96558]) [98031]\nIt's a bit strange, but sometimes lookup to influxdb is failed.\n. @huangyuqi you are right, after time synchronization and restarting heapster all goes wll, thanks.\n. ",
    "Naresht-vedams": "@vishh \n$ iptables -n -t nat -L\nDNAT       tcp  --  0.0.0.0/0            192.168.3.203        / default/monitoring-grafana: / tcp dpt:80 to:10.20.0.72:56627\n- Grafana is accessible from 172.16.2.179:56627 but for 3000 port not accessible. \n- Here I can't access private network like 192.168.3.90.\n$ kubectl logs heapster-66mz1\nI0905 05:02:37.712569       1 heapster.go:55] /heapster --source=kubernetes:https://kubernetes --sink=influxdb:http://monitoring-influxdb:8086\nI0905 05:02:37.712629       1 heapster.go:56] Heapster version 0.17.0\nI0905 05:02:37.723263       1 kube_factory.go:168] Using Kubernetes client with master \"https://kubernetes\" and version \"v1\"\nI0905 05:02:37.723325       1 kube_factory.go:169] Using kubelet port 10255\nE0905 05:02:57.926683       1 reflector.go:136] Failed to list *api.Node: Get https://kubernetes/api/v1/nodes: dial tcp: lookup kubernetes: no such host\nE0905 05:02:57.926817       1 reflector.go:136] Failed to list *api.Pod: Get https://kubernetes/api/v1/pods?fieldSelector=spec.nodeName%21%3D: dial tcp: lookup kubernetes: no such host\nE0905 05:02:57.926932       1 kube_events.go:96] Failed to load events: Get https://kubernetes/api/v1/events: dial tcp: lookup kubernetes: no such host\nE0905 05:02:57.927004       1 reflector.go:136] Failed to list *api.Namespace: Get https://kubernetes/api/v1/namespaces: dial tcp: lookup kubernetes: no such host\nE0905 05:03:18.016648       1 driver.go:326] Database creation failed: Post http://monitoring-influxdb:8086/db?u=root&p=root: dial tcp: lookup monitoring-influxdb: no such host. Retrying after 30 seconds\nE0905 05:03:19.026478       1 reflector.go:136] Failed to list *api.Node: Get https://kubernetes/api/v1/nodes: dial tcp: lookup kubernetes: no such host\nE0905 05:03:19.026555       1 reflector.go:136] Failed to list *api.Pod: Get https://kubernetes/api/v1/pods?fieldSelector=spec.nodeName%21%3D: dial tcp: lookup kubernetes: no such host\n. @vishh \nYou said in this video https://www.youtube.com/watch?v=SZgqjMrxo3g&feature=youtu.be\n- For my case it is not displaying monitoring-grafana.\n$ kubectl cluster-config\nKubernetes master is running at http://172.16.2.179:8080\n$ cat ~/.kube/config\napiVersion: v1\nclusters:\n- cluster:\n    insecure-skip-tls-verify: true\n    server: http://172.16.2.179:8080\n  name: ubuntu\ncontexts:\n- context:\n    cluster: ubuntu\n    user: ubuntu\n  name: ubuntu\ncurrent-context: ubuntu\nkind: Config\npreferences: {}\nusers:\n- name: ubuntu\n  user:\n    password: ****\n    username: admin\nCan you explain a bit more what needs to do to get the cluster-info perfectly \n. @vishh \ncurl http://172.16.2.179:8080/api/v1/namespaces\nfor above URL  I am getting the response like this\n{\n  \"kind\": \"NamespaceList\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"selfLink\": \"/api/v1/namespaces\",\n    \"resourceVersion\": \"20840\"\n  },\n  \"items\": [\n    {\n      \"metadata\": {\n        \"name\": \"default\",\n        \"selfLink\": \"/api/v1/namespaces/default\",\n        \"uid\": \"9246d446-5389-11e5-a511-0800272179e9\",\n        \"resourceVersion\": \"5\",\n        \"creationTimestamp\": \"2015-09-05T04:50:03Z\"\n      },\n      \"spec\": {\n        \"finalizers\": [\n          \"kubernetes\"\n        ]\n      },\n      \"status\": {\n        \"phase\": \"Active\"\n      }\n    }\n  ]\n}\nBut in heapster logs , it is trying to connect like https://kubernetes/api/v1/namespaces this so it is giving error. what I have to do to fix this. \n. @vishh \nNow I got it.\nThanks @vishh \n. Yes I am running the config from heapster repo....I tried admin:admin, but no luck......\n. ",
    "pakuni": "Hey Naresht / Vishh, I am having the same issue what's the fix?. ",
    "Naveen1186": "Hi Naresht / Vishh, we are also getting same problem ,heapster is getting failed .\nBelow are our logs , could you please let us know the fix\n53:36 PME0206 10:53:36.798791       1 reflector.go:203] k8s.io/heapster/metrics/processors/node_autoscaling_enricher.go:100: Failed to list api.Node: Get https://Kubernetes.kubernetes:6443/api/v1/nodes?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:53:36 PME0206 10:53:36.800519       1 reflector.go:203] k8s.io/heapster/metrics/processors/namespace_based_enricher.go:84: Failed to list api.Namespace: Get https://Kubernetes.kubernetes:6443/api/v1/namespaces?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:53:36 PME0206 10:53:36.802125       1 reflector.go:203] k8s.io/heapster/metrics/heapster.go:319: Failed to list api.Pod: Get https://Kubernetes.kubernetes:6443/api/v1/pods?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:53:40 PMW0206 10:53:40.131358       1 listers.go:68] can not retrieve list of objects using index : object has no meta: object does not implement the Object interfaces2/6/2017 6:53:49 PME0206 10:53:49.957914       1 reflector.go:203] k8s.io/heapster/metrics/sources/kubelet/kubelet.go:339: Failed to list api.Node: Get https://Kubernetes.kubernetes:6443/api/v1/nodes?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:54:05 PME0206 10:54:05.000391       1 kubelet.go:270] No nodes received from APIserver.2/6/2017 6:54:07 PME0206 10:54:07.793478       1 reflector.go:203] k8s.io/heapster/metrics/heapster.go:327: Failed to list api.Node: Get https://Kubernetes.kubernetes:6443/api/v1/nodes?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:54:07 PME0206 10:54:07.799488       1 reflector.go:203] k8s.io/heapster/metrics/processors/node_autoscaling_enricher.go:100: Failed to list api.Node: Get https://Kubernetes.kubernetes:6443/api/v1/nodes?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:54:07 PME0206 10:54:07.801454       1 reflector.go:203] k8s.io/heapster/metrics/processors/namespace_based_enricher.go:84: Failed to list api.Namespace: Get https://Kubernetes.kubernetes:6443/api/v1/namespaces?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:54:07 PME0206 10:54:07.802846       1 reflector.go:203] k8s.io/heapster/metrics/heapster.go:319: Failed to list api.Pod: Get https://Kubernetes.kubernetes:6443/api/v1/pods?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:54:10 PMW0206 10:54:10.130822       1 listers.go:68] can not retrieve list of objects using index : object has no meta: object does not implement the Object interfaces2/6/2017 6:54:20 PME0206 10:54:20.959021       1 reflector.go:203] k8s.io/heapster/metrics/sources/kubelet/kubelet.go:339: Failed to list api.Node: Get https://Kubernetes.kubernetes:6443/api/v1/nodes?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:54:38 PME0206 10:54:38.794490       1 reflector.go:203] k8s.io/heapster/metrics/heapster.go:327: Failed to list api.Node: Get https://Kubernetes.kubernetes:6443/api/v1/nodes?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:54:38 PME0206 10:54:38.800115       1 reflector.go:203] k8s.io/heapster/metrics/processors/node_autoscaling_enricher.go:100: Failed to list api.Node: Get https://Kubernetes.kubernetes:6443/api/v1/nodes?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:54:38 PME0206 10:54:38.802081       1 reflector.go:203] k8s.io/heapster/metrics/processors/namespace_based_enricher.go:84: Failed to list api.Namespace: Get https://Kubernetes.kubernetes:6443/api/v1/namespaces?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:54:38 PME0206 10:54:38.803496       1 reflector.go:203] k8s.io/heapster/metrics/heapster.go:319: Failed to list api.Pod: Get https://Kubernetes.kubernetes:6443/api/v1/pods?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:54:40 PMW0206 10:54:40.132858       1 listers.go:68] can not retrieve list of objects using index : object has no meta: object does not implement the Object interfaces2/6/2017 6:54:51 PME0206 10:54:51.960146       1 reflector.go:203] k8s.io/heapster/metrics/sources/kubelet/kubelet.go:339: Failed to list api.Node: Get https:/\n. Also \ncat ~/.kube/config    result\napiVersion: v1\nkind: Config\nclusters:\n- cluster:\n    api-version: v1\n    server: \"http://kubernetes.kubernetes.rancher.internal\"\n  name: \"Default\"\ncontexts:\n- context:\n    cluster: \"Default\"\n  name: \"Default\"\ncurrent-context: \"Default\". ",
    "pramodshaw": "Failed to list *api.Node: Get https://Kubernetes.kubernetes:6443/api/v1/nodes?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:53:36 PME0206 10:53:36.800519 1 reflector.go:203] k8s.io/heapster/metrics/processors/namespace_based_enricher.go:84:\nFailed to list *api.Namespace: Get https://Kubernetes.kubernetes:6443/api/v1/namespaces?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:53:36 PME0206 10:53:36.802125 1 reflector.go:203] k8s.io/heapster/metrics/heapster.go:319:\nFailed to list *api.Pod: Get https://Kubernetes.kubernetes:6443/api/v1/pods?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:53:40 PMW0206 10:53:40.131358 1 listers.go:68] can not retrieve list of objects using index : object has no meta: object does not implement the Object interfaces2/6/2017 6:53:49 PME0206 10:53:49.957914 1 reflector.go:203] k8s.io/heapster/metrics/sources/kubelet/kubelet.go:339:\nFailed to list *api.Node: Get https://Kubernetes.kubernetes:6443/api/v1/nodes?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:54:05 PME0206 10:54:05.000391 1 kubelet.go:270] No nodes received from APIserver.2/6/2017 6:54:07 PME0206 10:54:07.793478 1 reflector.go:203] k8s.io/heapster/metrics/heapster.go:327:\nFailed to list *api.Node: Get https://Kubernetes.kubernetes:6443/api/v1/nodes?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:54:07 PME0206 10:54:07.799488 1 reflector.go:203] k8s.io/heapster/metrics/processors/node_autoscaling_enricher.go\nFailed to list *api.Node: Get https://Kubernetes.kubernetes:6443/api/v1/nodes?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:54:07 PME0206 10:54:07.801454 1 reflector.go:203] k8s.io/heapster/metrics/processors/namespace_based_enricher.go:84:\nFailed to list *api.Namespace: Get https://Kubernetes.kubernetes:6443/api/v1/namespaces?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:54:07 PME0206 10:54:07.802846 1 reflector.go:203] k8s.io/heapster/metrics/heapster.go:319:\nFailed to list *api.Pod: Get https://Kubernetes.kubernetes:6443/api/v1/pods?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:54:10 PMW0206 10:54:10.130822 1 listers.go:68] can not retrieve list of objects using index : object has no meta: object does not implement the Object interfaces2/6/2017 6:54:20 PME0206 10:54:20.959021 1 reflector.go:203] k8s.io/heapster/metrics/sources/kubelet/kubelet.go:339:\nFailed to list *api.Node: Get https://Kubernetes.kubernetes:6443/api/v1/nodes?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:54:38 PME0206 10:54:38.794490 1 reflector.go:203] k8s.io/heapster/metrics/heapster.go:327:\nFailed to list *api.Node: Get https://Kubernetes.kubernetes:6443/api/v1/nodes?resourceVersion=0: dial tcp: i/o timeout2/6/2017 6:54:38 PME0206 10:54:38.800115 1 reflector.go:203] k8s.io/heapster/metrics/processors/node_autoscaling_enricher.go\n.cat ~/.kube/config result\napiVersion: v1\nkind: Config\nclusters:\n- cluster:\n    api-version: v1\n    server: \"http://kubernetes.kubernetes.rancher.internal\"\n  name: \"Default\"\ncontexts:\n- context:\n    cluster: \"Default\"\n  name: \"Default\"\ncurrent-context: \"Default\". Formatted the logs.2017/2/14 \u4e0a\u53486:33:49dnsmasq[1]: started, version 2.76 cachesize 10002017/2/14 \u4e0a\u53486:33:49dnsmasq[1]: compile time options: IPv6 GNU-getopt no-DBus no-i18n no-IDN DHCP DHCPv6 no-Lua TFTP no-conntrack ipset auth no-DNSSEC loop-detect inotify2017/2/14 \u4e0a\u53486:33:49dnsmasq[1]: using nameserver 127.0.0.1#100532017/2/14 \u4e0a\u53486:33:49dnsmasq[1]: read /etc/hosts - 7 addresses2017/2/14 \u4e0a\u53486:33:50dnsmasq[1]: nameserver 127.0.0.1 refused to do a recursive query`. similar issue, I am facing. How to fix this?\n\n. ",
    "yuvipanda": "(I might get to this in a few weeks if nobody else does)\n. Also unsure if it should be a graphite sink or a statsd sink. Will investigate heapster more and update issue.\n. nope, sorry! Gotta uncookielick it - we've just moved to prometheus instead.\nOn Tue, Sep 6, 2016 at 7:23 AM, Steve Adams notifications@github.com\nwrote:\n\n@yuvipanda https://github.com/yuvipanda are you still planning to work\non this? just trying to understand my options as i'm looking to ship\nmetrics via graphite\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/538#issuecomment-244965996,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAB23pRSvx8sZ46JoAJ3lhvDwt48bafyks5qnXdTgaJpZM4F76Kc\n.\n\n\nYuvi Panda T\nhttp://yuvi.in/blog\n. I won't have time anytime soon, unfortunately :( Sorry!\nOn Mon, Oct 17, 2016 at 6:22 AM, Piotr Szczesniak notifications@github.com\nwrote:\n\n@jsoriano https://github.com/jsoriano thanks for your contribution! We\nneed someone familiarized with Graphite to review this. Do you know anyone?\n@yuvipanda https://github.com/yuvipanda @mzupan\nhttps://github.com/mzupan @stevezau https://github.com/stevezau are\nyou interested in reviewing this?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1341#issuecomment-254205592,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAB23jTtoRiWB4j6xpKcjbrkZOohi7uMks5q03aogaJpZM4KYD-U\n.\n\n\nYuvi Panda T\nhttp://yuvi.in/blog\n. ",
    "mzupan": "I'd love to help.. It should send right to carbon since you can set a -model_resolution time which I assume relates to the time the data would get flushed out to a sink. \n. ",
    "stevezau": "+1 for this. I'm looking to forward metrics to third party tools using graphite.\n. @yuvipanda are you still planning to work on this? just trying to understand my options as i'm looking to ship metrics via graphite\n. I might give it a go.. Are you able to point me in the right direction? As in how were you planning to support it?\n. Can anyone point me where I should start to develop statsd support? Just point me to an example of one of the other supported backends and i'll translate it into statsd.\nBeen having a look at the code repo for heapster and i'm getting a bit lost\n. Nevermind, just found them here https://github.com/kubernetes/heapster/tree/master/metrics/sinks\n. I'll take a look when i get a spare second, this is of particular interest for me! In fact, i was going to do it myself :D\n. ",
    "pendoragon": "Any updates on this?\n. ",
    "moserke": "Seems like the retention policy name, like db name, should be configurable through the sink.  At sink startup should be able to pass in a retention policy.  Then users can create retention policies in influx themselves for their own cases.\nAlternatively could expose the duration value and let the sink build the policy on the fly configured to that duration.  This to me seems more flexible.  The default value of the duration can be 5 days, but users can set the duration to a different number if they so deem.\nThoughts?\n. Any progress on this?  Would be great to have an elasticsearch sink\n. @huangyuqi Have you looked at the modifications needed to grafana to produce the same dashboards in the grafana package with influx?  Wanted to see if you had anything there before digging in to it myself.\n. @ddispaltro Can you expand on \"better options in hyperkube\"?  Seem to be having the same issue and can not determine resolution\n. On the CPU graphs, what is the limit actually showing?  Is the units really ms or is it KCU, or ECU from amazon, or?  The graph is great to show the shaped of the usage, but in terms of \"exhausting\" of resources how does that compare?\n. Have verified that this is a docker 1.10.x issue.  Don't have a resolution to date, but went back to the beta channel of coreos.\n. That shows our DNS name for the nodes.  But the kubelet source seems to not be trying to use the hostname but instead using the IP.\nAt https://github.com/kubernetes/heapster/blob/v1.2.0/metrics/sources/kubelet/kubelet.go#L293 it is looking through the nodes internal IPs and using them.  There are two internal IPs on our nodes, one that is ipv4 and one that is ipv6.   The ipv6 ends up overwriting the ipv4 as it's the second one found.\nThen at https://github.com/kubernetes/heapster/blob/v1.2.0/metrics/sources/kubelet/kubelet_client.go#L126 when scraping occurs it is using the IP not the hostname.  So even though the hostname is set to something resolvable, that doesn't get used and IP is used instead.  And since the IP gets set to ipv6 because that is the second one in the node objects IP array for InternalIP that's the one that gets attempted.\nThis ends up with a log message like: error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://fe80::250:56ff:feb0:553a:10255/stats/container/\": Post http://fe80::250:56ff:feb0:553a:10255/stats/container/: dial tcp: too many colons in address fe80::250:56ff:feb0:553a:10255\nSince the http client being used is likely an ipv4 client.\nSo the only option I see at this point is to disable ipv6 or change to use the hostname for the connection to the kubelet instead of the IP\n. ",
    "dcolens": "\nby retention policy does it mean keeping the data x days after the container is deleted or x-days after the data is created ? From the influxdb doc it sounds like it is the later.\nin the case of short-lived container, it would be nice to have an aggressive retention policy. What would be a way to specify different retention policies based on the container/label ?\n. \n",
    "Crapworks": "Any news here? That would be pretty valuable to us, since we are spinning k8s clusters in openstack und start an influxdb as a heapster backend in a pod. Right now I just fire up influxdb with a custom config. the database that heapster is creating has a retention policy for 168h (7 days). I would like to keep the data for 30days. Is there still work on this?\n. ",
    "mpvaniersel": "It looks like I started working on this in parallel with @bavarianbidi. I can rework this in whichever way necessary to get this upstreamed.. I'm comparing the two pull requests to see what I can learn here - The one of @bavarianbidi makes it possible to override the retentionpolicy selected when sending a batch of points (i.e. makes it possible to put measurements under a non-default policy), but still requires you to somehow create and set that non-default retention policy elsewhere. On the other hand my pull request is designed to override the duration on the default retention policy, covering all measurements. Disclaimer: I'm pretty new to go, influxdb and heapster so I might be misunderstanding things.. I signed the CLA. Not sure why the linuxfoundation CLA needs signing, but I went ahead and signed that one  as well.. @DirectXMan12 Now that I have your attention - right now this code only applies to influxdb as a metrics sink, not when it's used as an events sink. Do you think I should apply these changes to both? I hesitate to do so as I can't test that in my current setup.. I rebased. The code is much simpler now as creation of retention policy has also been added by somebody else in the mean time. Now it's just a matter of passing the configuration parameter to createRetentionPolicy.\n@luxas: the requested fix is no longer relevant after rebase.. I rebased this a couple of weeks ago, nobody looked at it, and now it's conflicting again.\nI would rebase this again IF I can get a promise from a developer that this will be merged in a short time frame. I'm not going to put in more work if it's only going to be ignored.. I fixed the whitespace issue that was tripping up the CI. Please go ahead and merge!\n. Thanks! Don't worry about it.. Sorry, I don't have my dev environment for heapster set up anymore. I can't be much help I'm afraid.. Good point. Since there is a default defined of \"0\", I thought it would not be necessary to check if there is a retention policy set. But it would be easy to wrap it in a \"if (sink.c.RetentionPolicy) {}\". ",
    "lvthillo": "I have the same issue. I'm using OpenShift Origin (installed with ansible). I want to enable metrics using Influxdb, grafana and heapster. I'm using the same user guide.\nI have this error logs in the Heapster pod.\nip-10-0-0-104.eu-west-1.compute.internal, not kubernetes.default.svc\nE1026 10:07:10.011991       1 reflector.go:136] Failed to list *api.Namespace: Get https://kubernetes.default.svc/api/v1/namespaces: x509: certificate is valid for ec2-52-18-247-x.eu-west-1.compute.amazonaws.com, ip-10-0-0-x.eu-west-1.compute.internal, not kubernetes.default.svc\nE1026 10:07:10.211997       1 reflector.go:136] Failed to list *api.Pod: Get https://kubernetes.default.svc/api/v1/pods?fieldSelector=spec.nodeName%21%3D: x509: certificate is valid for ec2-52-18-247-x.eu-west-1.compute.amazonaws.com, ip-10-0-0-x.eu-west-1.compute.internal, not kubernetes.default.svc\nE1026 10:07:10.235825       1 reflector.go:136] Failed to list *api.Node: Get https://kubernetes.default.svc/api/v1/nodes: x509: certificate is valid for ec2-52-18-247-x.eu-west-1.compute.amazonaws.com,\nSo very similar and I executed your command:\noadm ca create-server-cert --cert=master.server.crt --key=master.server.key --signer-cert=/etc/origin/master/ca.crt --signer-key=/etc/origin/master/ca.key --signer-serial=/etc/origin/master/ca.serial.txt --hostnames=kubernetes.default.svc,kubernetes.default.svc.cluster.local,localhost,openshift.default.svc.cluster.local,127.0.0.1\nBut it really messed up my OpenShift:\n[root@ip-10-0-0-104 centos]# oc login\nThe connection to the server hostname:8443 was refused - did you specify the right host or port?\nIt said I was not able to login anymore and I had to run the ansible installer again to recover. \nAfter the recover I had still the same error for Heapster. \n. @jimmidyson I'm sorry but what do you mean with the public hostname? The domainname?: ec2-52-18-247-x.eu-west-1.compute.amazonaws.com\n. I did \noadm ca create-server-cert --cert=master.server.crt --key=master.server.key --signer-cert=/etc/origin/master/ca.crt --signer-key=/etc/origin/master/ca.key --signer-serial=/etc/origin/master/ca.serial.txt --hostnames=kubernetes.default.svc,kubernetes.default.svc.cluster.local,localhost,openshift.default.svc.cluster.local,127.0.0.1,ec2-52-18-247-x.eu-west-1.compute.amazonaws.com\nAnd after restarting openshift I've just the same issue.\nLogs:\nOct 26 11:12:29 ip-10-0-0-104 origin-node: E1026 11:12:29.957845    1185 reflector.go:209] pkg/kubelet/config/apiserver.go:43: Failed to watch *api.Pod: Get https://ip-10-0-0-104.eu-west-1.compute.internal:8443/api/v1/watch/pods?fieldSelector=spec.nodeName%3Dip-10-0-0-104.eu-west-1.compute.internal&resourceVersion=325453: dial tcp 10.0.0.104:8443: connection refused\nOct 26 11:12:29 ip-10-0-0-104 origin-node: E1026 11:12:29.957863    1185 reflector.go:180] /builddir/build/BUILD/origin-git-5812.e2a02a8/_build/src/github.com/openshift/origin/plugins/osdn/osdn.go:84: Failed to list *api.HostSubnet: Get https://ip-10-0-0-104.eu-west-1.compute.internal:8443/oapi/v1/hostsubnets: dial tcp 10.0.0.104:8443: connection refused\nOct 26 11:12:29 ip-10-0-0-104 origin-node: E1026 11:12:29.957889    1185 reflector.go:209] pkg/proxy/config/api.go:60: Failed to watch *api.Endpoints: Get https://ip-10-0-0-104.eu-west-1.compute.internal:8443/api/v1/watch/endpoints?resourceVersion=326298: dial tcp 10.0.0.104:8443: connection refused\n. Okay I did it a few times an I think it works now. I don't know why it wasn't working from the beginning. Maybe because I did not manually delete the files first before recreating them. I'm able to login. But there is a new error:\nE1026 11:39:53.383614       1 kube_events.go:96] Failed to load events: Get https://kubernetes.default.svc/api/v1/events: dial tcp: lookup kubernetes.default.svc: no such host\nE1026 11:39:54.378966       1 driver.go:235] failed to write stats to influxDB - Post http://influxdb-monitoring:8086/db/k8s/series?u=root&p=root&time_precision=s: dial tcp: lookup influxdb-monitoring: no such host\n. I think so,  they are in the same openshift-project (=same namespace). The pod is running fine. I can access the influxdb container. \n[root@influxdb-monitoring-a86fq /]#  /etc/init.d/influxdb start    \nSetting ulimit -n 65536\nStarting the process influxdb [ OK ]\ninfluxdb process was started [ OK ]\n. [root@ip-10-0-0-104 master]# oc get services influxdb-monitoring\nNAME                  CLUSTER_IP     EXTERNAL_IP   PORT(S)    SELECTOR                                         AGE\ninfluxdb-monitoring   172.30.62.53   <none>        8086/TCP   component=influxdb-monitoring,provider=fabric8   3d\n. The logs are changed:\nE1026 11:39:54.378966       1 driver.go:235] failed to write stats to influxDB - Post http://influxdb-monitoring:8086/db/k8s/series?u=root&p=root&time_precision=s: dial tcp: lookup influxdb-monitoring: no such host\nE1026 11:49:24.100784       1 driver.go:235] failed to write stats to influxDB - Post http://influxdb-monitoring:8086/db/k8s/series?u=root&p=root&time_precision=s: EOF\nThe last log is several minutes ago. I think the manual start of influxdb (in the container) did the job. I'm also seeing the statistics in grafana in my web browser. So I think it's working now? Thanks!\n. Weird, maybe I just had to wait a bit longer? Is it possible to curl to the service of influxdb? I get a 404 but I read that's normal? But sometimes I'm seeing some UI for influxdb on tutorialsites.\n. ",
    "jonaz": "Also ran in to this problem. Source should be\n--source=kubernetes:https://kubernetes.default\n. According to influx issue tracker this has been the same sine 0.9.4 and is\nsame issue on 0.12.0\nIve added labels to all my pods to workarround the problem for now\nOn Fri, 30 Sep 2016, 21:40 Solly Ross, notifications@github.com wrote:\n\nI don't think we officially support InfluxDB 1.0.0 yet, but this does look\nsomewhat like a bug nonetheless.\nPlease try with an older version of InfluxDB and see if it works.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/1320#issuecomment-250834336,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABF-FfxFsVMHsM3UWnQ9EQY2QUmWzg8mks5qvWWfgaJpZM4KJpRm\n.\n. /remove-lifecycle rotten. Isn't this related to https://github.com/kubernetes/kubernetes/issues/30939#issuecomment-244454951 \nand https://github.com/kubernetes/kubernetes/issues/28192\n\nI had this in coreos until latest stable (last week) where they stopped using systemd cgroup driver. \n. Those referenced commits can be ignored. I initially put the wrong issue number in the commit message. . @KarolKraskiewicz the datamodel of influxdb states that once a field is created it cannot ever change type. Thats the problem here. Between heapster 1.1 and 1.2 the datatype has changed. Thats a breaking change and should not be allowed. The preferred way to store influxdb is always float64. \nI have not tried v1.3.0-beta.1. Have heapster changed back to float64 there?. I wonder if any early influxdb client defaulted to float64 for integers...  i will try to dig through some code! \nIt might be related to this: \"Integers can be written to line protocol by specifying an i after the number while any field value without an i is considered a float\"\nif i remember correctly the old influxdb client heapster build with did not use the updated lineprotocol which appends \"i\" to the value for integers. Then i used a more recent influxdb with an older heapster and that probably why i have floats created in my database. . Im closing this based on my last comment. Pretty confident that was what happened!. We are running influxdb v1.1.1 and grafana 4.0.2 in production with heapster 1.1 and its working fine. \nNot using the heapster-grafana-amd64:v4.0.2 but a standalone grafana installation from rpm packages. . @piosz  cla/linuxfoundation signed. How do i trigger re-check here?. ",
    "liggitt": "much better... still shouldn't need to pull in bearertoken/bearertoken.go\n.  If the node is accessed by IP, then the IP should be in the node's certificate as a SAN. What setup are you using to create the node's certificate? \n. If a ca is provided to the master for constructing the kubelet client, it verifies the cert. \nIt looks like connections to the node are IP based (https://github.com/kubernetes/kubernetes/blob/782102d4370f85228ca6aabbc84b998c266b6e6d/pkg/registry/node/strategy.go#L139)\nI am not sure how the various setup scripts invoke the master, if they provide a kubelet CA or not\n. I'm pretty sure the ansible installer adds the node IP to the serving cert (@sdodson, confirm?). Can you open an issue against openshift origin to fix the write-config option to include the IP?\n. > Are we sure this is a broken setup? Not specifying the IP address in a certificate may not be the best idea, but I don't know if I would say its an invalid setup.\nThere are degrees of invalid. Serving with a cert that doesn't include an address you expect to be contacted on is not a good setup\n. Not that I'm aware of. Can you open an issue against openshift origin to add the IP to the node's certificate when running the all-in-one? I think the ansible install already builds the node's serving certificate correctly\n. Are we sure we always have DNS resolution of hostnames to nodes?\n. Any chance we could add a test to make sure this doesn't regress again?\n. > we'd probably have to change the proxy itself a bit\nworse than that, it would mean changing the auth layer to authenticate requests to proxy subresources differently than the rest of the API (with fallback to support older clients)\nit would also mean changing clients to authenticate requests to proxy subresources differently than the rest of the API (with fallback to support older servers, hoping the older server isn't passing through credentials intended for the API to the backend pods)\n. Correct, because client cert info is not transitive. Something similar could be done by verifying the client cert's signature, then remotely checking the client cert's subject's access.\n. Yes: https://github.com/kubernetes/kubernetes/blob/master/pkg/apiserver/authn.go#L31\nedit: oops, misread... you should be importing the kubernetes package\n. the service account and deployment are logically paired. the RBAC objects are not, since they would not apply if RBAC authz was not in use.. if RBAC is not in use, default roles/rolebindings are not created. ",
    "gerritbinnenmars": "Hello all,\nThanks for picking this up. We would like to have the machine information like the Node structure with number of cores, the disk Info structure and the NetInfo structure of cAdvisor available. This is needed to build a custom scheduler that is based on the OSGi requirement-capability resolver.\nE.g. a specific POD is only placed on machines that have a 10 Gb interface.\nHope it is more clear now.\nGreetings Gerrit\n. ",
    "jordannielsen": "I am using the following options in the rc file:\ncommand:\n        - /heapster\n        - --source=kubernetes:http://IP_Address?insecure=true&inClusterConfig=false\n        - --sink=influxdb:http://IP_Address:8086\n. ",
    "jkurz": "same issue here.\nI can create databases in influx with GETS to influxdbIp:8086/query?q=CREATE+DATABASE+\"tester\"&db=_internal\nbut heapster keeps failing to create a database.\n. ",
    "prateekrastogi": "Hi, I don't have any memory limit set in my config file. Also, I tried running only heapster-rc alone and it is unable to start. docker ps -a shows no trace of heapster container whatsoever. My docker version in 1.7.1, build 2c2c52b-dirty. And the cluster is running on coreos. \n. i was running 7.3.0 of coreos with k8 1.0.5. After your response I used dmesg and saw some error lines in it, so i tried upgrading the whole cluster first os and then k8, but it broke down whole setup. Then, i created a new setup with latest realeases and heapster working fine on it. Unfortunately, i was not able to log dmesg at that time, nor able to comprehend how mere upgrade broke down whole cluster setup. Thanks for your efforts anyway.\n. ",
    "lachlanbull": "Hi Vishh,\nI had been using heapster from the kubernetes releases, i think around release 1.05 it broke. I'm reasonably sure it's the same issues.\nI've been using a relatively straight forward AWS build from the included /cluster/aws scripts.\nCurrently on Release 1.0.6, grafana 2.1 starts but never configures a data source. \n. Hi Vish, \nAppreciate you checking, Yes so i manually specified the service address endpoint but it's same behaviour as using localhost in a single node setup. \nLater today i'll try manually building the docker images to see if i can get it working.\n. \"Grafana will no longer be accessible using apiserver proxy\" - Yeah fair enough, I've been advertising the routes for service range from VPC so i can reach them easily enough without the ELB.\nI'll let you know how it goes. \n. ",
    "chancez": "@lachlanbull I can also confirm the same. Works on v1.0.4 but not after. Looks like they added a new env to Grafana which broke things in v1.0.6\n. I signed it!. @AlmogBaku I for what it's worth, I got this working just fine. I had to make a small addition to the PR to add an option to disable SSL verification on the http.Client since I'm using a self signed TLS cert for ES (using the elasticsearch-operator) and didn't bother with getting the certs correctly mounted in the eventer container, but otherwise it works great.\nHere's the manifest I'm using: https://gist.github.com/e725dea50300bafc781382546bfe6f2b\nThe additional code is just this:\n```\nhttpClient := &http.Client{}\nif len(opts[\"verifySSL\"]) > 0 {\n    verifySSL, err := strconv.ParseBool(opts[\"verifySSL\"][0])\n    if err != nil {\n        return nil, errors.New(\"Failed to parse URL's verifySSL value into a bool\")\n    }\n    httpClient.Transport = &http.Transport{\n        TLSClientConfig: &tls.Config{\n            InsecureSkipVerify: !verifySSL,\n        },\n    }\n}\nstartupFnsV2 = append(startupFnsV2, elastic2.SetHttpClient(httpClient))\nstartupFnsV5 = append(startupFnsV5, elastic5.SetHttpClient(httpClient))\n```. @AlmogBaku Yeah, I'm waiting for this one to get merged and I'll open a PR for it. Mostly wanted to say this is working great perfectly for me.. @rikatz I'm using https://github.com/upmc-enterprises/elasticsearch-operator and the manifest I'm using is in the gist above. Just know I've made a minor modification to this code to work with my elasticsearch, as mentioned, so I can disable SSL verification.. @SupreethKadalur This got merged after that pre-relase, you'll have to wait for another pre-release or build it yourself.. @DirectXMan12 Sorry, forgot to copy the title into the description, there's not more than that. I didn't expect there to be much need for a body.\n@AlmogBaku I've found this useful when switching between multiple test clusters where each is using a different auto created CA, and am trying to avoid reconfiguring everything to use the new certs, some of which I often don't have the CA lying around. This is mostly for cases where I'd typically use http, but don't want to reconfigure the servers. \nSpeaking to the security aspect, it's insecure only if someone explicitly chooses to disable https. The default is still verifying SSL. I don't see how this is any different for example than how users can disable SSL verification when talking to the API Server for most applications.\n. @DirectXMan12 I can understand the concerns, but I'm not going to implement what's being asked here (being able to specify a specific CA), as it's out of scope for this change, and I'm able to get by without needing to specify a specific CA (for typical use we have real certs included in the OS bundle we can just leverage). \nAdditionally, while I get the security aspect, I can't believe this is an issue given that the sink allows http connections. If we're arguing about whether skipping SSL verification is insecure, then why does the sink even allow plain HTTP? \nYou can take the argument any direction, it's just exposing additional well known configuration options, and I'd really rather not get bogged down on whether or not a very standard option is or isn't an escape hatch, when people have surely also been using plain HTTP which is basically the same level of security as disabling verification.. ",
    "andrewmichaelsmith": "I've just loaded cluster/addons/cluster-monitoring/influxdb. When I go to: :8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/datasources Grafana loads but I get the\nInfluxDB Error: 404 page not found error.`\nI read what you said about LoadBalancers - I just set a nodePort on my grafana-service.yaml but when I go to :nodePort Grafana doesn't even load because all the HTML is referencing /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/css/.\nDid I miss something here?\n. So is the problem that the yml files define heapster v0.20 and influxdb 0.5?\n. With that configuration I get the 404 error above.\nFairly sure I'm on the right images:\n$ kubectl --namespace=kube-system get rc | grep -e heap\nheapster-v14                     heapster                      gcr.io/google_containers/heapster:v0.20.0-alpha7             k8s-app=heapster,version=v14                     1          19h\n                                 eventer                       gcr.io/google_containers/heapster:v0.20.0-alpha7                                                              \nmonitoring-influxdb-grafana-v3   influxdb                      gcr.io/google_containers/heapster_influxdb:v0.5              k8s-app=influxGrafana,version=v3                 1         19h\n                                 grafana                       gcr.io/google_containers/heapster_grafana:v2.1.1\n. ",
    "peteperl": "I just signed the cla.\nI will push the code onto my git early next week for the admins to review.\nWe can push it around on there if we need to first.\nThanks,\nPete\nOn Fri, Oct 2, 2015 at 2:19 PM, googlebot notifications@github.com wrote:\n\nThanks for your pull request. It looks like this may be your first\ncontribution to a Google open source project, in which case you'll need to\nsign a Contributor License Agreement (CLA).\n[image: :memo:] Please visit https://cla.developers.google.com/\nhttps://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll\nverify. Thanks.\n\nIf you've already signed a CLA, it's possible we don't have your\n  GitHub username or you're using a different email address. Check your\n  existing CLA data https://cla.developers.google.com/clas and verify\n  that your email is set on your git commits\n  https://help.github.com/articles/setting-your-email-in-git/.\nIf you signed the CLA as a corporation, please let us know the\n  company's name.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/629#issuecomment-145157484.\n. I can't see where to find the other agreement.\nI only have this:\nGoogle Individual CLA\nhttps://cla.developers.google.com/about/google-individualPete PerlegosOct\n02, 2015 14:51 PDT\nWhat is the commit author agreement?\nMy only choices are the following:\nSign a new agreement:\n- required\n  - You plan to submit contributions on behalf of \n    -  Only yourself\n    -  Your employer\n  - You plan to submit contributions to \n    -  Android OS\n\n## \\-  Any other Google project\n\nOn Fri, Oct 2, 2015 at 2:55 PM, googlebot notifications@github.com wrote:\n\nWe found a Contributor License Agreement for you (the sender of this pull\nrequest), but were unable to find agreements for the commit author(s). If\nyou authored these, maybe you used a different email address in the git\ncommits than was used to sign the CLA (login here\nhttps://cla.developers.google.com/ to double check)? If these were\nauthored by someone else, then they will need to sign a CLA as well, and\nconfirm that they're okay with these being contributed to Google.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/629#issuecomment-145165515.\n. Daniel,\n\nWhat is the best method to interact with Kubernetes contributors to determine the interest in a potential Hempstead extension and to evaluate what I am building?\nI would like to see the community interest as well so I can setup the cost with the right roadmap in mind. \nThanks,\nPete\n\nOn Oct 5, 2015, at 11:22 PM, Daniel Mart\u00ed notifications@github.com wrote:\nClosed #629.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "wojtek-t": "\n@gmarek did some experiments with 100 nodes and he noticed that Kubernetes is crash-looping without any suspicious logs there. My hypothesis is that the cause is high mem usage but it has to be verified.\n\nYou mean \"heapster is crash-looping\", not \"kubernetes is crash-looping\", right?\n. LGTM\n. @fgrzadkowski . LGTM. /lgtm. This looks fine.\n@kawych @piosz - did you manage to run any benchmarks/load tests with this change?. ",
    "gmarek": "After some experiments it turned out to be OOM.\n. Well, we promised everything to be working on 100 Node cluster with 30 pods/node. I don't think that 100 Node cluster with 10 pods/node counts as 'large cluster/workload'.\n. cc @fgrzadkowski @mwielgus \n. @jimm-porch FYI: for pre v1 I don't think that changing limits to requests will help. I don't remember when we implemented requests, but it was rather later than sooner.\n. LGTM\n. LGTM\n. Few nits. Generally LGTM\n. One suggestion, but otherwise LGTM. Ping me if you decide to use tabwriter.\n. LGTM\n. LGTM\n. Only nits. LGTM after you fix them.\n. LGTM.\n. One bigger issue and few nits. Otherwise LGTM.\n. I guess that heapster-integration errors are still expected. LGTM in such case.\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. I guess it's just FYI:)\n. LGTM\n. LGTM\n. Thanks @piosz. For the context: Heapster in Kubemark started crashlooping after recent version bump with error:\n```\nI0620 13:55:09.060836       1 heapster.go:65] /heapster --source=kubernetes:https://146.148.102.90:443?inClusterConfig=0&useServiceAccount=0&auth=/kubeconfig/kubeconfig\nI0620 13:55:09.060931       1 heapster.go:66] Heapster version 1.1.0\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal 0xb code=0x1 addr=0x18 pc=0x5daaeb]\ngoroutine 1 [running]:\npanic(0x1786d00, 0xc820012060)\n        /usr/lib/google-golang/src/runtime/panic.go:483 +0x3ef\nk8s.io/heapster/common/kubernetes.GetKubeClientConfig(0xc820275930, 0x0, 0x0, 0x0)\n        /usr/local/google/home/pszczesniak/go/src/k8s.io/heapster/common/kubernetes/configs.go:123 +0x3db\nk8s.io/heapster/metrics/sources/kubelet.GetKubeConfigs(0xc820275930, 0xc8201aa350, 0x417101, 0x0, 0x0)\n        /usr/local/google/home/pszczesniak/go/src/k8s.io/heapster/metrics/sources/kubelet/configs.go:39 +0x42\nk8s.io/heapster/metrics/sources/kubelet.NewKubeletProvider(0xc820275930, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/google/home/pszczesniak/go/src/k8s.io/heapster/metrics/sources/kubelet/kubelet.go:319 +0x62\nk8s.io/heapster/metrics/sources.(SourceFactory).Build(0xc820275a40, 0x7fffe9b38dee, 0xa, 0x7fffe9b38df9, 0x5, 0x0, 0x0, 0x0, 0x7fffe9b38e01, 0x12, ...)\n        /usr/local/google/home/pszczesniak/go/src/k8s.io/heapster/metrics/sources/factory.go:32 +0xaf\nk8s.io/heapster/metrics/sources.(SourceFactory).BuildAll(0xc820275a40, 0xc8201b78c0, 0x1, 0x1, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/google/home/pszczesniak/go/src/k8s.io/heapster/metrics/sources/factory.go:46 +0xda\nmain.main()\n        /usr/local/google/home/pszczesniak/go/src/k8s.io/heapster/metrics/heapster.go:76 +0x4c3\n```\nIt seems that it's caused by the changes in kubeclient.\n. /lgtm\n. /lgtm. Remove space.\n. We're not using -stamp suffix in k8s API. This is not an external API, but I guess it'd be nice to be consistent: \nhttps://github.com/kubernetes/kubernetes/blob/master/docs/devel/api-conventions.md#naming-conventions\n. Please add comment on assumed semantics - I guess that it lets in-flight ExportData to finish before killing the Sink (in case someone would want to implement Stop() by os.Exit(0))\n. s/Sinkexport/SinkExport/\n. s/sinkexportEventsTimeout/exportEventsTimeout/\n. It's probably a V(4) kind of log - I don't think this should be logged by default, especially because we also log in line 86.\n. Add a comment that this should be noticeably longer than expected running time of ExportEvents function.\n. Please extract common parts into 'processBatch(sink1sleep, sink2sleep) (elapsed, sink1export, sink2export) or sth like that.\n. An idea: maybe use text/tabwriter for this?\n. Add an info that it's in number of objects (my first thought was bytes).\n. I don't see any call for this function. Is there an interface somewhere?\n. remove empty line\n. remove empty line\n. Why 10? It should be either 1, as we always return after seeing an error, or more, but then we need to handle the case when it's full.\n. Please add a comment why you're doing it this way, instead of using time.After(rm.frequency). My guess is that you want to do polls in the same time and synchronize housekeeps with them.\n. Please add an explicit verbosity level - even if you want it to be 0.\n. Wouldn't it be useful to somehow expose those errors? Eg. return list of them or slice of pairs, or sth? I don't want to block this PR on this, so feel free to ignore this comment if you feel it'd be pointless.\n. s/eventes/events/\n. Generally it's more efficient to use for i := range bla { bla[i] } as this way you avoid making copies. I guess that Sinks are small enough for it to doesn't matter though. \n. Add \"Requires exactly one source\"\n. Maybe 'validateFlagValues'? My first thought was that it's some flag-library thingy.\n. V(2)\n. V(2)\n. Why do we care if everything builds? Isn't it enough that the binary builds?\n. ",
    "jimm-porch": "In our production cluster we have 11 nodes, with a total of 173 pods (using --all-namespaces) and we were seeing Heapster getting OOM-killed after about 2 minutes, with default configuration.\nYesterday we started using kubectl patch to change the memory limit for Heapster:\n- At 600Mi, it still OOM'd about every 2 minutes.\n- At 900Mi, it OOM'd about every 4 minutes.\n- At 1500Mi, it OOM'd after 22 minutes.\nIt didn't seem to be 100% linear, sometimes it would OOM after 10 minutes and sometimes as long as 35.\n. There is no doubt that it is OOM, for example when I was watching it yesterday, in a 7-minute period I saw 5 OOMs.  That was with our default memory configuration (which I'm told was 400Mi):\n$ sudo docker events\n2015-10-12T21:55:01.000000000Z 19f8f9e8bf72a2336a0120ecfd42d4936773ed636bbdd1ab22f592f29af8f62a: (from gcr.io/google_containers/heapster:v0.16.1) oom\n2015-10-12T21:55:01.000000000Z 19f8f9e8bf72a2336a0120ecfd42d4936773ed636bbdd1ab22f592f29af8f62a: (from gcr.io/google_containers/heapster:v0.16.1) die\n2015-10-12T21:55:03.000000000Z 34e0327e6a3f9e24573d1f6913655c398a1ed3c58e86b1941299a868f292f60f: (from gcr.io/google_containers/heapster:v0.16.1) create\n2015-10-12T21:55:03.000000000Z 34e0327e6a3f9e24573d1f6913655c398a1ed3c58e86b1941299a868f292f60f: (from gcr.io/google_containers/heapster:v0.16.1) start\n2015-10-12T21:55:41.000000000Z 7c20735e25e0770050d495a13ca16ae4048519e6af1f4c7b72e7d453deab4f02: (from gcr.io/google_containers/heapster:v0.16.1) destroy\n2015-10-12T21:56:50.000000000Z 34e0327e6a3f9e24573d1f6913655c398a1ed3c58e86b1941299a868f292f60f: (from gcr.io/google_containers/heapster:v0.16.1) oom\n2015-10-12T21:56:50.000000000Z 34e0327e6a3f9e24573d1f6913655c398a1ed3c58e86b1941299a868f292f60f: (from gcr.io/google_containers/heapster:v0.16.1) die\n2015-10-12T21:56:53.000000000Z 9142463e503f15ab54c4bed1b84393fb1e3ef8e10fd0e3ab3530bf7dc358349a: (from gcr.io/google_containers/heapster:v0.16.1) create\n2015-10-12T21:56:53.000000000Z 9142463e503f15ab54c4bed1b84393fb1e3ef8e10fd0e3ab3530bf7dc358349a: (from gcr.io/google_containers/heapster:v0.16.1) start\n2015-10-12T21:57:42.000000000Z 8e88efb133e797e75fc067c0866c6cfd8847c6a15bdd77023f9630826ac44f98: (from gcr.io/google_containers/heapster:v0.16.1) destroy\n2015-10-12T21:58:41.000000000Z 9142463e503f15ab54c4bed1b84393fb1e3ef8e10fd0e3ab3530bf7dc358349a: (from gcr.io/google_containers/heapster:v0.16.1) oom\n2015-10-12T21:58:41.000000000Z 9142463e503f15ab54c4bed1b84393fb1e3ef8e10fd0e3ab3530bf7dc358349a: (from gcr.io/google_containers/heapster:v0.16.1) die\n2015-10-12T21:58:42.000000000Z 19f8f9e8bf72a2336a0120ecfd42d4936773ed636bbdd1ab22f592f29af8f62a: (from gcr.io/google_containers/heapster:v0.16.1) destroy\n2015-10-12T21:58:44.000000000Z 0427889be1549276e47ffef6869df098b3195ed2cc4e0707f18e5624b6c0e6d1: (from gcr.io/google_containers/heapster:v0.16.1) create\n2015-10-12T21:58:44.000000000Z 0427889be1549276e47ffef6869df098b3195ed2cc4e0707f18e5624b6c0e6d1: (from gcr.io/google_containers/heapster:v0.16.1) start\n2015-10-12T22:00:31.000000000Z 0427889be1549276e47ffef6869df098b3195ed2cc4e0707f18e5624b6c0e6d1: (from gcr.io/google_containers/heapster:v0.16.1) oom\n2015-10-12T22:00:31.000000000Z 0427889be1549276e47ffef6869df098b3195ed2cc4e0707f18e5624b6c0e6d1: (from gcr.io/google_containers/heapster:v0.16.1) die\n2015-10-12T22:00:34.000000000Z 8ddc3b361603e8b9f7725fa6e6612e949ce0db14fdc92dd7c9ad3593aa95a7c6: (from gcr.io/google_containers/heapster:v0.16.1) create\n2015-10-12T22:00:34.000000000Z 8ddc3b361603e8b9f7725fa6e6612e949ce0db14fdc92dd7c9ad3593aa95a7c6: (from gcr.io/google_containers/heapster:v0.16.1) start\n2015-10-12T22:00:43.000000000Z 34e0327e6a3f9e24573d1f6913655c398a1ed3c58e86b1941299a868f292f60f: (from gcr.io/google_containers/heapster:v0.16.1) destroy\n2015-10-12T22:02:23.000000000Z 8ddc3b361603e8b9f7725fa6e6612e949ce0db14fdc92dd7c9ad3593aa95a7c6: (from gcr.io/google_containers/heapster:v0.16.1) oom\n2015-10-12T22:02:23.000000000Z 8ddc3b361603e8b9f7725fa6e6612e949ce0db14fdc92dd7c9ad3593aa95a7c6: (from gcr.io/google_containers/heapster:v0.16.1) die\n2015-10-12T22:02:24.000000000Z 93050f75e0f158eb9b7aa0a9fa228cb0dccc6026a74fdfc5745fe6d12c233d37: (from gcr.io/google_containers/heapster:v0.16.1) create\n2015-10-12T22:02:24.000000000Z 93050f75e0f158eb9b7aa0a9fa228cb0dccc6026a74fdfc5745fe6d12c233d37: (from gcr.io/google_containers/heapster:v0.16.1) start\n2015-10-12T22:02:43.000000000Z 9142463e503f15ab54c4bed1b84393fb1e3ef8e10fd0e3ab3530bf7dc358349a: (from gcr.io/google_containers/heapster:v0.16.1) destroy\n. @vishh in our production environment we are still on v0.21.4 (but we will migrate to 1.x soon).  In our dev & qa environments we are on v1.0.6.  We see this Heapster issue in all environments.\nInterestingly we see the same version of Heapster running in our environments even though they are using different versions of Kubernetes.\n- The image is gcr.io/google_containers/heapster:v0.16.1.\n- The selector is k8s-app=heapster,version=v6\n. We'll try your suggestion (using \"requests\" in place of \"limits\") to see how that fares today, and upgrading the version.\nI have a completely naive and silly question - what would happen if Heapster was completely turned off?\n. @vishh / @tylerd-porch would it be possible to change the image via kubectl patch?  For example under items.spec.containers.image:\njson\n                \"containers\": [\n                    {\n                        \"name\": \"heapster\",\n                        \"image\": \"gcr.io/google_containers/heapster:v0.16.1\",\n                        \"command\": [\n                            \"/heapster\",\n                            \"--source=kubernetes:''\"\n                        ],\n. ",
    "tylerd-porch": "Hi @vishh  -- How would we go about upgrading heapster on GKE?\n. ",
    "derekwaynecarr": "/cc @kubernetes/rh-cluster-infra \n. ",
    "awarude": "when can we expect this fix? is there anyway around this problem so that the hpa doesnt wait for 2.5 mins?\n. can you point to to the right thread or documentation which shows how that can be configured.\n. ",
    "spiffxp": "@vishh I'm a bit confused, are you saying that currently the heapster addon's presence is required for kube-scheduler to be working properly?\n. @vishh yeah but how about today?  is this required for proper functioning of v 1.0.x or 1.1.x?\n. try https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/monitoring_architecture.md\nhttps://github.com/kubernetes/community/pull/1010 shuffled around the contents of the design-proposals dir. ",
    "jayunit100": "There seems to be pretty close coupling to prometheus on the metrics front already.  sorta seems like overkill to maintain a separate timeseries framework ?  but i see both sides of the coin here.\n. ",
    "jimmycuadra": "What's the current state of this? I've read the vision document, but it's still not clear if there is or will be support for Prometheus as a sink for heapster. It's confusing that Prometheus has emerged as the go-to monitoring system for Kubernetes, especially given that it's also a member of the CNCF, and yet when you deploy the cluster monitoring addon for Kubernetes, it uses an InfluxDB sink for Heapster, plus Grafana for visualizations. This means that cluster operators who want metrics with a larger scope than Heapster is intended for must maintain two separate time series databases.. ",
    "davidkarlsen": "@DirectXMan12 \"We're currently transitioning away from Heapster as the defacto solution, as per the new monitoring vision in the community repo.\" - do you have any references /docs for that (I guess it's not the vision doc mentioned above since that one refers to heapster). ",
    "monotek": "The links is dead :-(\nAny mirror available?\nI'm currently trying to find out whats the standard / best practice monitoring solution which is used in Kubernetes. \nI thought its Cadvisor + Prometheus. Then i've read about Heapster which seems to be dead regarding @davidkarlsen post.\nI'm a bit confused now. Where to start?\n. ",
    "lvsha1989": "Have you sloved the problem?\n. ",
    "shayeeb-ahmed": "@vishh Can u please still elaborate about the cpu metric shown in :10255/stats/summary . ",
    "z0rc": "Also, maybe I'm doing something wrong, but even after launching docker containers manually (with the same options as in compose), I cannot to find any data from cadvisor in hawkular.\n. ",
    "gosharplite": "I have tried using nodePort as shown below. Same result, no luck.\ngrafana-service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: monitoring-grafana\n  name: monitoring-grafana\n  namespace: kube-system\nspec:\n  # In a production setup, we recommend accessing Grafana through an external Loadbalancer\n  # or through a public IP. \n  # type: LoadBalancer\n  type: \"NodePort\"\n  ports:\n  - port: 80\n    targetPort: 3000\n    nodePort: 30015\n  selector:\n    name: influxGrafana\n. It might be a href issue. Html source code shows href has wrong relative url.\nSo I changed influxdb-grafana-controller.yaml as shown below.\n- name: GF_SERVER_ROOT_URL\n            value: /\n            #value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/\nNow I can see the Grafana UI with proxy url, but can't get any content. Browser pop-up shows the server could not find the requested resource\nhttps://10.128.112.11/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/\nLuckily I have nodePort access too.\nhttp://10.128.112.11:30015/\n It works!\nHopefully there will be a fix for both proxy url and nodePort url.\n. I'm using v1.0.6, here are some proves.\n$ ./kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"0\", GitVersion:\"v1.0.6\", GitCommit:\"388061f00f0d9e4d641f9ed4971c775e1654579d\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"0\", GitVersion:\"v1.0.6\", GitCommit:\"388061f00f0d9e4d641f9ed4971c775e1654579d\", GitTreeState:\"clean\"}\n$ ./kube-proxy --version\nKubernetes v1.0.6\nAm I missing some parameters in kube-proxy?\nkube-proxy \\\n  --master=https://10.128.112.11:443 \\\n  --kubeconfig=/srv/kube-proxy/kubeconfig \\\n  --logtostderr=true\n. I'll take your word for it!\nSince K8S v1.1.1 upgrade to docker 1.8.3 (#15719) and CoreOS stable is still on 1.7.1, I'm not ready to check this issue.\n. ",
    "maty21": "the problem still happens to me  in K8S 1.1.7 so i suggest to open that issue again\n. ",
    "gabe-ochoa": "+1\n. ",
    "juner417": "+1\n. ",
    "yujuhong": "This happened to another user with k8s 1.2:\nhttps://stackoverflow.com/questions/37993263/grafana-not-showing-in-kubernetes-heapster\nCan we reopen the issue?\n/cc a couple of maintainers: @vishh @mwielgus @piosz \n. How are you going to interpret metrics from the not-ready nodes? I don't think they are particularly useful especially because kubelet may simply be non-reachable. \nAlso, there are ongoing efforts to sort through node conditions (e.g., https://github.com/kubernetes/kubernetes/issues/45717). I'd suggest not adding this until we are clear what \"not ready\" means. . lgtm. CPU usage may not be accurate. It's possible that the process didn't run for the past time period. Could you use memory instead?. ",
    "autostatic": "Flanneld is running on my master but I'm getting 404's too and an empty page with a button and {{alert.title}}. Kubernetes 1.2.4 with ubuntu as K8s provider.\n1.2.3.4 - - [29/Jul/2016:12:22:11 +0200] \"GET /monitoring HTTP/1.1\" 200 1247 \"-\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\"\n1.2.3.4 - - [29/Jul/2016:12:22:11 +0200] \"GET /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/public/css/grafana.dark.min.a95b3754.css HTTP/1.1\" 404 19 \"https://mytestingdomain.net/monitoring\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\"\n1.2.3.4 - - [29/Jul/2016:12:22:11 +0200] \"GET /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/public/app/app.ca0ab6f9.js HTTP/1.1\" 404 19 \"https://mytestingdomain.net/monitoring\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\"\nMaybe I've made an error in my nginx proxy configuration but the K8s Dashboard works so I think it should be OK:\nlocation /monitoring {\n                # First attempt to serve request as file, then\n                # as directory, then fall back to displaying a 404.\n                #try_files $uri $uri/ =404;\n                # Uncomment to enable naxsi on this location\n                # include /etc/nginx/naxsi.rules\n                proxy_pass http://10.10.100.1:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/;\n                include /etc/nginx/proxy_params;\n        }\n. Having the same issue here with Kubernetes 1.2.4 while the same deployment works fine with Kubernetes 1.4.0 and higher.\n. ",
    "rxwen": "I'm using kubernetes 1.4.4 with weave overlay network. And I encountered this problem too, how can I make it work?\nI tried setting GF_SERVER_ROOT_URL to \"/\", but the grafana pages showed no metrics at all.. Never mind, my mistake. I can see metrics.\nBut I guess modify GF_SERVER_ROOT_URL isn't the desired way to fix the problem. Any better way when using weave network?. ",
    "zsustar": "faced same issue.  \nAs I am running K8s on AWS. I created a ELB as the load balance to bypass this issue.\nkubectl expose --namespace=kube-system  deployment monitoring-grafana --port=80 --target-port=3000 --name=monitoring-grafana-newservice --type=LoadBalancer\nThe command above will create a ELB base on the deployment monitoring-grafana, and ELB will listen on port 80. \nTo access, hit your ELB URL directly or matching the ELB name to your public/private DNS name via Route53.\nTo me, the link is: http://a0cd682e50aa611e79a0202ddbfd8337-xxxxxx.ap-southeast-2.elb.amazonaws.com/. ",
    "jonathan-kosgei": "I get this error with kubectl proxy but using kubectl port-forward --namespace=kube-system <grafana-pod> 3000:3000 & works. I get the same error\nTemplate variables could not be initialized: InfluxDB Error: undefined\nTo get it to work I had to comment out\n# kubernetes.io/cluster-service: 'true'\nin\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)\n    # If you are NOT using this as an addon, you should comment out this line.\n    # kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: monitoring-grafana\n  name: monitoring-grafana\n  namespace: kube-system\nspec:\n  # In a production setup, we recommend accessing Grafana through an external Loadbalancer\n  # or through a public IP.\n  # type: LoadBalancer\n  # You could also use NodePort to expose the service at a randomly-generated port\n  # type: NodePort\n  ports:\n  - port: 80\n    targetPort: 3000\n  selector:\n    k8s-app: grafana\nIs this repo supposed to be deployed exclusively as a cluster add-on?. Ran into this too on GKE, your work-around works for me too +1. ",
    "ravilr": "I tried to debug this more. what i found out was, one of the node in the cluster was returning empty stats for rawContainerstats call  http://kubelet:10255/stats/container/ \n. @vishh, i remember checking the kubelet cadvisor port 4914 directly and it was showing stats at that time. the kubelet logs are rotated now, will check next time this issue recurs.\n. this is same panic reported in https://github.com/kubernetes/heapster/issues/658\nCan we move the these 2 lines inside the loop:\nhttps://github.com/kubernetes/heapster/blob/20f9a2eac6f64d305a71d3c392ee9209c2e65e91/sources/kube_nodes.go#L86-L87\n. xref https://github.com/kubernetes/kubernetes/issues/12968\n. ",
    "danmcp": "I have signed the CLA as Red Hat, Inc.\n. @jimmidyson I did.  I opened: https://github.com/kubernetes/heapster/pull/660 in its place to fix the cla warning.\n. ",
    "remy-phelipot": "I have the same problem with the kubernetes endpoint configured in unsecured mode. Here is the configuration for the replication controllers:\nGrafana/InfluxDB\nyaml\napiVersion: v1\nkind: ReplicationController\nmetadata:\n  creationTimestamp: 2015-10-22T08:42:33Z\n  generation: 1\n  labels:\n    name: influxGrafana\n  name: infludb-grafana\n  namespace: kube-system\n  resourceVersion: \"2249\"\n  selfLink: /api/v1/namespaces/kube-system/replicationcontrollers/infludb-grafana\n  uid: d6ab1f6e-7898-11e5-8753-0016c2000115\nspec:\n  replicas: 1\n  selector:\n    name: influxGrafana\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        name: influxGrafana\n    spec:\n      containers:\n      - image: kubernetes/heapster_influxdb:v0.4\n        imagePullPolicy: IfNotPresent\n        name: influxdb\n        ports:\n        - containerPort: 8083\n          hostPort: 8083\n          protocol: TCP\n        - containerPort: 8086\n          hostPort: 8086\n          protocol: TCP\n        resources: {}\n        terminationMessagePath: /dev/termination-log\n      - image: grafana/grafana:2.1.0\n        imagePullPolicy: IfNotPresent\n        name: grafana\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n        resources: {}\n        terminationMessagePath: /dev/termination-log\n        volumeMounts:\n        - mountPath: /var/lib/grafana\n          name: grafana-store\n      dnsPolicy: ClusterFirst\n      restartPolicy: Always\n      volumes:\n      - emptyDir: {}\n        name: grafana-store\nstatus:\n  observedGeneration: 1\n  replicas: 1\nHeapster\nyaml\napiVersion: v1\nkind: ReplicationController\nmetadata:\n  creationTimestamp: 2015-10-22T08:42:33Z\n  generation: 1\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\n  namespace: kube-system\n  resourceVersion: \"2250\"\n  selfLink: /api/v1/namespaces/kube-system/replicationcontrollers/heapster\n  uid: d6a5d2ee-7898-11e5-8753-0016c2000115\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - command:\n        - /heapster\n        - --source=kubernetes:http://212.xxx.xx.21:8888?insecure=true&auth=&inClusterConfig=false\n        - --sink=influxdb:http://monitoring-influxdb.kube-system.xxx.xxxx.net:8086\n        image: kubernetes/heapster:v0.18.2\n        imagePullPolicy: IfNotPresent\n        name: heapster\n        resources: {}\n        terminationMessagePath: /dev/termination-log\n      dnsPolicy: ClusterFirst\n      restartPolicy: Always\nstatus:\n  observedGeneration: 1\n  replicas: 1\nHere are the logs:\n```\nW1022 11:17:00.412871   18307 cmd.go:161] log is DEPRECATED and will be removed in a future version. Use logs instead.\nI1022 09:16:32.109403       1 heapster.go:55] /heapster --source=kubernetes:http://212.xxxxx.21:8888?insecure=true&auth=&inClusterConfig=false --sink=influxdb:http://monitoring-influxdb.kube-system..xxxx.xx.net:8086\nI1022 09:16:32.109576       1 heapster.go:56] Heapster version 0.18.2\nI1022 09:16:32.109681       1 kube_factory.go:169] Using Kubernetes client with master \"http://212.xxxxxxxxx.21:8888\" and version \"v1\"\nI1022 09:16:32.109709       1 kube_factory.go:170] Using kubelet port 10255\nI1022 09:16:32.151442       1 driver.go:376] created influxdb sink with options: {root root monitoring-influxdb.kube-system.xxxxxxxxx.net:8086 k8s false}\nI1022 09:16:32.154352       1 heapster.go:66] Starting heapster on port 8082\n```\n```\n[10/22/15 08:42:41] [INFO] Loading configuration file /config/config.toml\n+---------------------------------------------+\n|  _  _            _    |\n| | |      / _| |          |   \\|  _ \\  |\n|   | |  _  | || |  | |  | | |) | |\n|   | | | ' \\|  | | | | \\ \\/ / |  | |  _ <  |\n|  | || | | | | | | || |>  <| || | |_) | |\n| |_|| ||| ||_,/_/__/|____/  |\n+---------------------------------------------+\n```\nThe InfluxDB contains an empty 'k8s' table and the kubernetes endpoint is accessible from the container.\nshell\n/ # wget http://212.xxxx.21:8888 && cat index.html\n{\n  \"paths\": [\n    \"/api\",\n    \"/api/v1\",\n    \"/healthz\",\n    \"/healthz/ping\",\n    \"/logs/\",\n    \"/metrics\",\n    \"/resetMetrics\",\n    \"/swagger-ui/\",\n    \"/swaggerapi/\",\n    \"/ui/\",\n    \"/version\"\n  ]\n}Connecting to 212.xxx.xx.21:8888 (212.xxxx.xx.21:8888)\n. I've \"solved\" this issue by using the canary version of heapster and I can't go back to the previous version.. I'm sorry vishh.\n. Indeed, I'm running kubernetes v1.0.4, so the issue is probably related to kubernetes. Do you have the link of this issue?\nI will update kubernetes and check if it solves the problem.\n. ",
    "Alpanakabra": "@remy-phelipot  i face the same problem my k8s is empty . can you share the link or share more info on what did you change?\n. ",
    "Rastusik": "I'm having the same issue in my local k8s cluster, any suggestions? Grafana and InfluxDB are configured as NodePort.\nWhen I test the connection to my database from grafana, I'm getting a green status, that the connection is ok, but when I run the same request in curl, I get this output:\n{\"results\":[{\"error\":\"database not found: k8s\"}]}\n. I managed to solve the problem, but I needed to create the database manuall from the container\n. turns out the real problem was something with kube-dns and some secret (maybe a bug - kubernetes/kubernetes#23111). After fixing this according to the guide from the referenced issue, the problem wasn't there anymore.\n. ",
    "sdodson": "\nI'm pretty sure the ansible installer adds the node IP to the serving cert (@sdodson, confirm?). Can you open an issue against openshift origin to fix the write-config option to include the IP?\n\nNo it does not add the node's ip to the node's serving cert. I've added this to https://github.com/openshift/openshift-ansible/pull/609\n. I defer to @mwringe on priority here. But yeah basically folks who have upgraded from Origin 1.0 / OSE 3.0 to Origin 1.1 / OSE 3.1 will have trouble with this.\n. ",
    "mdshuai": "I signed it!\n. @ncdc  @DirectXMan12 help check this bug\n. Master branch works correctly.\n[root@ip-172-18-6-200 heapster]# ./heapster version\nI1107 04:34:24.818192   18164 heapster.go:65] ./heapster version\nI1107 04:34:24.818241   18164 heapster.go:66] Heapster version 1.2.0\nF1107 04:34:24.818248   18164 heapster.go:73] Failed to get kubernetes address: No kubernetes source found.\n@DirectXMan12 @mwringe help check, thanks\n. @DirectXMan12 @derekwaynecarr . @DirectXMan12 --api-server : Enable API server for the Metrics API. If set, the Metrics API will be \nserved on --insecure-port (internally) and --secure-port (externally).\nThere is no  --insecure-port in help info, it's same with --heapster-port? Need we update help info or rename heapster-port?. ",
    "sashgorokhov": "Having the same issue on \nClient Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.4\", GitCommit:\"3eed1e3be6848b877ff80a93da3785d9034d0a4f\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.4\", GitCommit:\"3eed1e3be6848b877ff80a93da3785d9034d0a4f\", GitTreeState:\"clean\"}\nHeapster ignores pods on kube-system namespace\n. ",
    "JunejaTung": "@vishh thanks for you help, i got the heapster run in standalone with curl results. \ni have another doubt, i config three nodes in /var/run/heapster/hosts, but i only get curl results of only one node!\n. @vishh  add \u2018--vmodule==4\u2019 can get more nodes stats. but i have another doubt in standalone type\uff0cthen --sink option doesn't validated, it didn't write any data to influxdb\n[root@wlan-cloudserver31 heapster]# ./heapster --vmodule==4 --source=kubernetes:http://172.27.8.210:8080?inClusterConfig=false&auth=''&useServiceAccount=false --sink=influxdb:http://172.27.8.109:8086\n[1] 131344\n[2] 131345\nbash: --sink=influxdb:http://172.27.8.109:8086: No such file or directory\n[2]+  Done                    auth=''\n[root@wlan-cloudserver31 heapster]# I1027 19:59:23.922591  131344 heapster.go:61] ./heapster --vmodule==4 --source=kubernetes:http://172.27.8.210:8080?inClusterConfig=false\nI1027 19:59:23.922779  131344 heapster.go:62] Heapster version 0.18.0\nI1027 19:59:23.923265  131344 kube_factory.go:172] Using Kubernetes client with master \"http://172.27.8.210:8080\" and version \"v1\"\nI1027 19:59:23.923280  131344 kube_factory.go:173] Using kubelet port 10255\nI1027 19:59:23.923807  131344 external.go:214] Updated sinks: []\nI1027 19:59:23.925529  131344 external.go:116] no timeseries data between 0001-01-01 00:00:00 +0000 UTC and 0001-01-01 00:00:00 +0000 UTC\nI1027 19:59:23.933108  131344 heapster.go:72] Starting heapster on port 8082\nI1027 19:59:30.000694  131344 manager.go:162] starting to scrape data from sources start: 2015-10-27 19:59:25 +0800 CST end: 2015-10-27 19:59:30 +0800 CST\nI1027 19:59:30.000938  131344 manager.go:103] attempting to get data from source \"Kube Events Source\"\nI1027 19:59:30.001311  131344 kube_events.go:81] Received new event: api.Event{TypeMeta:unversioned.Type\n. @vishh  pass flag within '\"\"' then it run ok, and has data in influxdb\nmore logs:\n[root@wlan-cloudserver31 heapster]# ./heapster --vmodule==3 --source=\"kubernetes:http://172.27.8.210:8080?inClusterConfig=false&auth=''&useServiceAccount=false\" --sink=\"influxdb:http://172.27.8.109:8086\"\nI1028 09:37:54.097134  142891 heapster.go:61] ./heapster --vmodule==3 --source=kubernetes:http://172.27.8.210:8080?inClusterConfig=false&auth=''&useServiceAccount=false --sink=influxdb:http://172.27.8.109:8086\nI1028 09:37:54.097414  142891 heapster.go:62] Heapster version 0.18.0\nF1028 09:37:54.097524  142891 heapster.go:68] stat '': no such file or directory\n[root@wlan-cloudserver31 heapster]# lsof -i:8082\n[root@wlan-cloudserver31 heapster]# ./heapster --vmodule==3 --source=\"kubernetes:http://172.27.8.210:8080?inClusterConfig=false&auth=''&useServiceAccount=false\" --sink=\"influxd[root@wlan-cloudserver31 heapster]# ./heapster --vmodule==3 --source=\"kubernetes:http://172.27.8.210:8080?inClusterConfig=false&auth=''useServiceAccount=false\" --sink=\"influxdb[root@wlan-cloudserver31 heapster]# ./heapster --vmodule==3 --source=\"kubernetes:http://172.27.8.210:8080?inClusterConfig=false&auth='useServiceAccount=false\" --sink=\"influxdb:[root@wlan-cloudserver31 heapster]# ./heapster --vmodule==3 --source=\"kubernetes:http://172.27.8.210:8080?inClusterConfig=false&auth=useServiceAccount=false\" --sink=\"influxdb:h[root@wlan-cloudserver31 heapster]# ./heapster --vmodule==3 --source=\"kubernetes:http://172.27.8.210:8080?inClusterConfig=false&authuseServiceAccount=false\" --sink=\"influxdb:ht[root@wlan-cloudserver31 heapster]# ./heapster --vmodule==3 --source=\"kubernetes:http://172.27.8.210:8080?inClusterConfig=false&autuseServiceAccount=false\" --sink=\"influxdb:htt[root@wlan-cloudserver31 heapster]# ./heapster --vmodule==3 --source=\"kubernetes:http://172.27.8.210:8080?inClusterConfig=false&auuseServiceAccount=false\" --sink=\"influxdb:http[root@wlan-cloudserver31 heapster]# ./heapster --vmodule==3 --source=\"kubernetes:http://172.27.8.210:8080?inClusterConfig=false&auseServiceAccount=false\" --sink=\"influxdb:http:[root@wlan-cloudserver31 heapster]# ./heapster --vmodule==3 --source=\"kubernetes:http://172.27.8.210:8080?inClusterConfig=false&useServiceAccount=false\" --sink=\"influxdb:http:/[root@wlan-cloudserver31 heapster]# ./heapster --vmodule==3 --source=\"kubernetes:http://172.27.8.210:8080?inClusterConfig=false&useServiceAccount=false\" --sink=\"influxdb:http://172.27.8.109:8086\"\nI1028 09:38:42.706153  142911 heapster.go:61] ./heapster --vmodule==3 --source=kubernetes:http://172.27.8.210:8080?inClusterConfig=false&useServiceAccount=false --sink=influxdb:http://172.27.8.109:8086\nI1028 09:38:42.706631  142911 heapster.go:62] Heapster version 0.18.0\nI1028 09:38:42.707107  142911 kube_factory.go:172] Using Kubernetes client with master \"http://172.27.8.210:8080\" and version \"v1\"\nI1028 09:38:42.707123  142911 kube_factory.go:173] Using kubelet port 10255\nI1028 09:38:42.708459  142911 external.go:116] no timeseries data between 0001-01-01 00:00:00 +0000 UTC and 0001-01-01 00:00:00 +0000 UTC\nI1028 09:38:42.735340  142911 driver.go:316] created influxdb sink with options: {root root 172.27.8.109:8086 k8s false}\nI1028 09:38:42.804073  142911 driver.go:245] Created database \"k8s\" on influxDB server at \"172.27.8.109:8086\"\nI1028 09:38:42.804360  142911 external.go:214] Updated sinks: [0xc2082ae580]\nI1028 09:38:42.813886  142911 heapster.go:72] Starting heapster on port 8082\nI1028 09:38:50.000694  142911 manager.go:162] starting to scrape data from sources start: 2015-10-28 09:38:45 +0800 CST end: 2015-10-28 09:38:50 +0800 CST\nI1028 09:38:50.000998  142911 manager.go:103] attempting to get data from source \"Kube Node Metrics Source\"\nI1028 09:38:50.001022  142911 manager.go:103] attempting to get data from source \"Kube Pods Source\"\nI1028 09:38:50.001080  142911 manager.go:103] attempting to get data from source \"Kube Events Source\"\nI1028 09:38:50.002672  142911 kube_events.go:216] Fetched list of events from the master\nI1028 09:38:50.005782  142911 kube_nodes.go:126] Fetched list of nodes from the master\nI1028 09:38:50.007247  142911 kubelet.go:110] about to query kubelet using url: \"http://172.27.8.212:10255/stats/default/jenkins-master-rlzrl/907d1380-7c4f-11e5-8c76-fa163e77e286/jenkins-master\"\nI1028 09:38:50.007350  142911 kubelet.go:110] about to query kubelet using url: \"http://172.27.8.211:10255/stats/default/portal-v2-isiip/f4a674c2-7c7c-11e5-8c76-fa163e77e286/portal-v2\"\nI1028 09:38:50.009014  142911 kube_nodes.go:59] Failed to get container stats from Kubelet on node \"wlan-cloudserver35\"\nI1028 09:38:50.033857  142911 kube_nodes.go:64] No container stats from Kubelet on node \"wlan-cloudserver34\"\nI1028 09:38:50.082159  142911 manager.go:175] completed scraping data from sources. Errors: []\nI1028 09:38:52.713540  142911 external.go:138] Storing Timeseries to \"InfluxDB Sink\"\nI1028 09:38:52.713591  142911 external.go:142] Storing Events to \"InfluxDB Sink\"\nI1028 09:38:55.000807  142911 manager.go:162] starting to scrape data from sources start: 2015-10-28 09:38:50 +0800 CST end: 2015-10-28 09:38:55 +0800 CST\nI1028 09:38:55.001104  142911 manager.go:103] attempting to get data from source \"Kube Node Metrics Source\"\nI1028 09:38:55.001144  142911 manager.go:103] attempting to get data from source \"Kube Events Source\"\nvery luckly, the data has write to influxdb v0.9 . i can query results with \u2018show series\u2019 in db k8s.\n. @vishh  wlan-cloudserver35 is notready for kubernetes cluster.  my heapster has run ok in standalone type, can get stats from ready kubelets. thanks.\n. when i got run with ./heapster --source=kubernetes:http://172.27.8.210:8080?inClusterConfig=false&auth=\u201d\u201d&useServiceAccount=false --sink=influxdb:http://172.27.8.109:8086?user=root&pw=root&db=k8s\nit run ok .\n. @vishh thanks for your help, i run heapster in standalone type (make the heapster source code, adn ./heapster ...), then the doubt hasn't encounter! \n. @vishh @thucatebay  I'm sorry \uff0ci can't find any documention for kube-dns\uff0chow to start kube-dns\uff1f\n. @thucatebay  thanks, i had try to lauch kube-dns, encounter many questions no luck. then i try to pass the ips to all services for refer, then all pods and services (heapster , influxdb , granafa) run normal. i will try to resolve other troubles such as grafana web returns no figures, inlfuxdb no data . \n. @vishh  now i can access web ui of influxdb and grafana services through  config NodePort in yaml file, when i ' kubectl create  -f  heapster-controller.yaml ' , the heapster have to restart for underly error logs, i pass clusterIP to \u2018--sink=influxdb:http//\u2019 option, but the heapster didn't use the pass ip but another localhost, how can i resolve this new trouble? heapster version is canary .\n```\n[root@wlan-cloudserver32 home]# docker logs cd50b5b3e7b4\nI1028 07:58:33.024362       1 heapster.go:60] /heapster --source=kubernetes:http://172.27.8.210:8080?inClusterConfig=false&useServiceAccount=false --sink=influxdb:http:///10.254.248.100:8086\nI1028 07:58:33.024555       1 heapster.go:61] Heapster version 0.18.0\nI1028 07:58:33.024840       1 kube_factory.go:168] Using Kubernetes client with master \"http://172.27.8.210:8080\" and version \"v1\"\nI1028 07:58:33.024861       1 kube_factory.go:169] Using kubelet port 10255\nF1028 07:58:33.028741       1 heapster.go:67] failed to ping InfluxDB server at \"localhost:8086\" - Get http://localhost:8086/ping: dial tcp 127.0.0.1:8086: connection refused\n[root@wlan-cloudserver32 home]# \n[root@wlan-cloudserver31 influxdb-test]# kubectl get services --all-namespaces \nNAMESPACE     NAME                  LABELS                                                                      SELECTOR               IP(S)            PORT(S)\ndefault       kubernetes            component=apiserver,provider=kubernetes                                                      10.254.0.1       443/TCP\ndefault       maintain-master       name=maintain-master                                                        name=maintain-master   10.254.98.56     6379/TCP\ndefault       portal-master         name=portal-master                                                          name=portal-master     10.254.173.124   6379/TCP\ndefault       portal-slave          name=portal-slave                                                           name=portal-slave      10.254.217.179   6379/TCP\nkube-system   heapster              kubernetes.io/cluster-service=true,kubernetes.io/name=Heapster              k8s-app=heapster       10.254.99.54     8082/TCP\nkube-system   monitoring-grafana    kubernetes.io/cluster-service=true,kubernetes.io/name=monitoring-grafana    name=influxGrafana     10.254.205.99    3000/TCP\nkube-system   monitoring-influxdb   kubernetes.io/cluster-service=true,kubernetes.io/name=monitoring-influxdb   name=influxGrafana     10.254.248.100   8083/TCP\n                                                                                                                                                        8086/TCP\n```\nwhen i try the heapster:latest version , add a new troub\uff1afail to list kubernetes master nodes, but i can curl it .\n```\n[root@wlan-cloudserver32 wlanuser]# docker ps -a | grep heapster\n562d70372de4        kubernetes/heapster:latest                      \"/heapster --source=   About a minute ago   Up About a minute                                 k8s_heapster.9e3dffa0_heapster-8t8il_kube-system_9e3036c2-7e36-11e5-8c76-fa163e77e286_76fd5d37    \n2e40e3034cee        gcr.io/google_containers/pause:0.8.0            \"/pause\"               2 minutes ago        Up 2 minutes                                      k8s_POD.e4cc795_heapster-8t8il_kube-system_9e3036c2-7e36-11e5-8c76-fa163e77e286_5502bb56          \n[root@wlan-cloudserver32 wlanuser]# docker logs 562d70372de4\nI1029 12:15:53.749185       1 heapster.go:55] /heapster --source=kubernetes:http://172.27.8.210:8080?inClusterConfig=false&useServiceAccount=false --sink=influxdb:http:///10.254.48.38:8086\nI1029 12:15:53.749392       1 heapster.go:56] Heapster version 0.18.2\nI1029 12:15:53.749572       1 kube_factory.go:169] Using Kubernetes client with master \"http://172.27.8.210:8080\" and version \"v1\"\nI1029 12:15:53.749589       1 kube_factory.go:170] Using kubelet port 10255\nE1029 12:16:23.752924       1 reflector.go:136] Failed to list api.Pod: Get http://172.27.8.210:8080/api/v1/pods?fieldSelector=spec.nodeName%21%3D: dial tcp 172.27.8.210:8080: i/o timeout\nE1029 12:16:23.754250       1 reflector.go:136] Failed to list api.Node: Get http://172.27.8.210:8080/api/v1/nodes: dial tcp 172.27.8.210:8080: i/o timeout\nE1029 12:16:23.754375       1 kube_events.go:123] Failed to load events: Get http://172.27.8.210:8080/api/v1/events: dial tcp 172.27.8.210:8080: i/o timeout\nE1029 12:16:23.754871       1 reflector.go:136] Failed to list api.Namespace: Get http://172.27.8.210:8080/api/v1/namespaces: dial tcp 172.27.8.210:8080: i/o timeout\nE1029 12:16:33.803530       1 driver.go:326] Database creation failed: Post http://localhost:8086/db?u=root&p=root: dial tcp 127.0.0.1:8086: connection refused. Retrying after 30 seconds\nE1029 12:16:54.756132       1 reflector.go:136] Failed to list api.Node: Get http://172.27.8.210:8080/api/v1/nodes: dial tcp 172.27.8.210:8080: i/o timeout\nE1029 12:16:54.756397       1 reflector.go:136] Failed to list api.Pod: Get http://172.27.8.210:8080/api/v1/pods?fieldSelector=spec.nodeName%21%3D: dial tcp 172.27.8.210:8080: i/o timeout\nE1029 12:16:54.756876       1 reflector.go:136] Failed to list api.Namespace: Get http://172.27.8.210:8080/api/v1/namespaces: dial tcp 172.27.8.210:8080: i/o timeout\n[root@wlan-cloudserver32 wlanuser]# \n[root@wlan-cloudserver32 wlanuser]#  curl http://10.254.48.38:8083\ncurl: (56) Recv failure: Connection reset by peer\n[root@wlan-cloudserver32 wlanuser]# curl http://172.27.8.210:8080/api/v1/nodes\n{\n  \"kind\": \"NodeList\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"selfLink\": \"/api/v1/nodes\",\n    \"resourceVersion\": \"586846\"\n  },\n  \"items\": [\n    {\n      \"metadata\": {\n        \"name\": \"wlan-cloudserver32\",\n        \"selfLink\": \"/api/v1/nodes/wlan-cloudserver32\",\n        \"uid\": \"24713843-79ff-11e5-8c76-fa163e77e286\",\n        \"resourceVersion\": \"586842\",\n        \"creationTimestamp\": \"2015-10-24T03:27:23Z\",\n        \"labels\": {\n          \"kubernetes.io/hostname\": \"wlan-cloudserver32\"\n        }\n      },\n``\n. @vishh  sorry, for my clerical errors in--sink=influxdb:http:///....option inheapster-controller.yaml,  more a char/. then entry that  strange question:F1028 07:58:33.028741       1 heapster.go:67] failed to ping InfluxDB server at \"localhost:8086\" - Get http://localhost:8086/ping: dial tcp 127.0.0.1:8086: connection refused. @thucatebay  to now, I config the query in Grafana not very good! I make some chages to the node172.27.8.212, make itready. then find some interesting things . i doubt there may be bug when has nodenotreadyin kubelets or no up containers, in this scene, heapster can't get metrics for thereadynodes after thatnotready` node.\n[root@wlan-cloudserver31 influxdb-test]# \n[root@wlan-cloudserver31 influxdb-test]# kubectl get nodes\nNAME                 LABELS                                      STATUS\n172.27.8.211         kubernetes.io/hostname=172.27.8.211         Ready\n172.27.8.212         kubernetes.io/hostname=172.27.8.212         Ready\n172.27.8.214         kubernetes.io/hostname=172.27.8.214         Ready\n[root@wlan-cloudserver31 influxdb-test]# kubectl get pods --all-namespaces -o wide\nNAMESPACE     NAME                     READY     STATUS    RESTARTS   AGE       NODE\ndefault       busybox                  1/1       Running   2          4d        172.27.8.211\ndefault       redis-master-rze58       1/1       Running   2          6d        172.27.8.214\nkube-system   heapster-zcd76           1/1       Running   0          4d        172.27.8.214\nkube-system   influxdb-grafana-h9rnx   2/2       Running   0          6d        172.27.8.214\nkube-system   kube-dns-v9-u07x6        4/4       Running   0          2d        172.27.8.211\n[root@wlan-cloudserver31 influxdb-test]#\n\n\nthe logs for container heapster.\nI1104 11:53:30.007492       1 kubelet.go:99] url: \"http://172.27.8.211:10255/stats/default/busybox/d22d309b-82c4-11e5-a8b4-fa163e77e286/busybox\", body: \"{\\\"num_stats\\\":60,\\\"start\\\":\\\"2015-11-04T11:53:25Z\\\",\\\"end\\\":\\\"2015-11-04T11:53:30Z\\\"}\", data: {ContainerReference:{Name:/system.slice/docker-e1a52f29c1bd116ecf599527f6c0d9e08b625b6c4edc8f69d3d1568d5299e382.scope Aliases:[k8s_busybox.d1c8ce40_busybox_default_d22d309b-82c4-11e5-a8b4-fa163e77e286_13b42095 e1a52f29c1bd116ecf599527f6c0d9e08b625b6c4edc8f69d3d1568d5299e382] Namespace:docker} Subcontainers:[] Spec:{CreationTime:2015-11-04 11:23:30.664343161 +0000 UTC Labels:map[io.kubernetes.pod.name:default/busybox] HasCpu:true Cpu:{Limit:2 MaxLimit:0 Mask:0-7} HasMemory:true Memory:{Limit:18446744073709551615 Reservation:0 SwapLimit:18446744073709551615} HasNetwork:true HasFilesystem:false HasDiskIo:true HasCustomMetrics:false CustomMetrics:[]} Stats:[]}\nI1104 11:53:30.054508       1 manager.go:175] completed scraping data from sources. Errors: []\nI1104 11:53:35.000745       1 manager.go:162] starting to scrape data from sources start: 2015-11-04 11:53:30 +0000 UTC end: 2015-11-04 11:53:35 +0000 UTC\nI1104 11:53:35.000915       1 manager.go:103] attempting to get data from source \"Kube Node Metrics Source\"\nI1104 11:53:35.000940       1 manager.go:103] attempting to get data from source \"Kube Events Source\"\nI1104 11:53:35.001114       1 kube.go:79] Only have PublicIP 172.27.8.214 for node 172.27.8.214, so using it for InternalIP\nI1104 11:53:35.001104       1 kube_events.go:216] Fetched list of events from the master\nI1104 11:53:35.001188       1 kube_events.go:217] []\nI1104 11:53:35.001279       1 kube.go:79] Only have PublicIP 172.27.8.211 for node 172.27.8.211, so using it for InternalIP\nI1104 11:53:35.001307       1 kube.go:79] Only have PublicIP 172.27.8.212 for node 172.27.8.212, so using it for InternalIP\nI1104 11:53:35.001275       1 manager.go:103] attempting to get data from source \"Kube Pods Source\"\nI1104 11:53:35.001323       1 kube_nodes.go:126] Fetched list of nodes from the master\nI1104 11:53:35.001367       1 kube.go:79] Only have PublicIP 172.27.8.214 for node 172.27.8.214, so using it for InternalIP\nI1104 11:53:35.001409       1 kube.go:79] Only have PublicIP 172.27.8.211 for node 172.27.8.211, so using it for InternalIP\nI1104 11:53:35.001450       1 kube.go:79] Only have PublicIP 172.27.8.212 for node 172.27.8.212, so using it for InternalIP\nI1104 11:53:35.001546       1 pods.go:152] selected pods from api server [{pod:0xc20830e5d0 nodeInfo:0xc2082bc140 namespace:0xc2081400e8} {pod:0xc20830e7c0 nodeInfo:0xc2082bc180 namespace:0xc2081400e8} {pod:0xc20830e000 nodeInfo:0xc2082bc1c0 namespace:0xc208140000} {pod:0xc20830e1f0 nodeInfo:0xc2082bc200 namespace:0xc208140000} {pod:0xc20830e3e0 nodeInfo:0xc2082bc240 namespace:0xc2081400e8}]\nI1104 11:53:35.001927       1 kubelet.go:110] about to query kubelet using url: \"http://172.27.8.214:10255/stats/kube-system/influxdb-grafana-h9rnx/7801146f-81d8-11e5-a8b4-fa163e77e286/influxdb\"\nI1104 11:53:35.002068       1 kubelet.go:110] about to query kubelet using url: \"http://172.27.8.214:10255/stats/kube-system/kube-dns-v9-oaep5/06e1de3d-82c2-11e5-a8b4-fa163e77e286/etcd\"\nI1104 11:53:35.002145       1 kubelet.go:110] about to query kubelet using url: \"http://172.27.8.211:10255/stats/default/busybox/d22d309b-82c4-11e5-a8b4-fa163e77e286/busybox\"\nI1104 11:53:35.002244       1 kubelet.go:110] about to query kubelet using url: \"http://172.27.8.214:10255/stats/default/redis-master-rze58/92a91592-8138-11e5-a8b4-fa163e77e286/master\"\nI1104 11:53:35.002346       1 kubelet.go:110] about to query kubelet using url: \"http://172.27.8.214:10255/stats/kube-system/heapster-zcd76/3a99af72-82ea-11e5-a8b4-fa163e77e286/heapster\"\nI1104 11:53:35.003034       1 kubelet.go:96] failed to get stats from kubelet url: http://172.27.8.214:10255/stats/kube-system/influxdb-grafana-h9rnx/7801146f-81d8-11e5-a8b4-fa163e77e286/influxdb - Get http://172.27.8.214:10255/stats/kube-system/influxdb-grafana-h9rnx/7801146f-81d8-11e5-a8b4-fa163e77e286/influxdb: dial tcp 172.27.8.214:10255: connection refused\nI1104 11:53:35.003081       1 kube_pods.go:110] failed to get stats for container \"influxdb\" in pod \"kube-system\"/\"influxdb-grafana-h9rnx\"\nI1104 11:53:35.003091       1 kube_nodes.go:59] Failed to get container stats from Kubelet on node \"172.27.8.214\"\nI1104 11:53:35.003139       1 kubelet.go:110] about to query kubelet using url: \"http://172.27.8.214:10255/stats/kube-system/influxdb-grafana-h9rnx/7801146f-81d8-11e5-a8b4-fa163e77e286/grafana\"\nI1104 11:53:35.003165       1 kubelet.go:96] failed to get stats from kubelet url: http://172.27.8.214:10255/stats/kube-system/kube-dns-v9-oaep5/06e1de3d-82c2-11e5-a8b4-fa163e77e286/etcd - Get http://172.27.8.214:10255/stats/kube-system/kube-dns-v9-oaep5/06e1de3d-82c2-11e5-a8b4-fa163e77e286/etcd: dial tcp 172.27.8.214:10255: connection refused\nI1104 11:53:35.003200       1 kube_pods.go:110] failed to get stats for container \"etcd\" in pod \"kube-system\"/\"kube-dns-v9-oaep5\"\nI1104 11:53:35.003221       1 kubelet.go:110] about to query kubelet using url: \"http://172.27.8.214:10255/stats/kube-system/kube-dns-v9-oaep5/06e1de3d-82c2-11e5-a8b4-fa163e77e286/kube2sky\"\nI1104 11:53:35.003350       1 kubelet.go:96] failed to get stats from kubelet url: http://172.27.8.214:10255/stats/kube-system/influxdb-grafana-h9rnx/7801146f-81d8-11e5-a8b4-fa163e77e286/grafana - Get http://172.27.8.214:10255/stats/kube-system/influxdb-grafana-h9rnx/7801146f-81d8-11e5-a8b4-fa163e77e286/grafana: dial tcp 172.27.8.214:10255: connection refused\nI1104 11:53:35.003416       1 kube_pods.go:110] failed to get stats for container \"grafana\" in pod \"kube-system\"/\"influxdb-grafana-h9rnx\"\nI1104 11:53:35.003476       1 kubelet.go:96] failed to get stats from kubelet url: http://172.27.8.214:10255/stats/kube-system/kube-dns-v9-oaep5/06e1de3d-82c2-11e5-a8b4-fa163e77e286/kube2sky - Get http://172.27.8.214:10255/stats/kube-system/kube-dns-v9-oaep5/06e1de3d-82c2-11e5-a8b4-fa163e77e286/kube2sky: dial tcp 172.27.8.214:10255: connection refused\nI1104 11:53:35.003515       1 kube_pods.go:110] failed to get stats for container \"kube2sky\" in pod \"kube-system\"/\"kube-dns-v9-oaep5\"\nI1104 11:53:35.003532       1 kubelet.go:110] about to query kubelet using url: \"http://172.27.8.214:10255/stats/kube-system/kube-dns-v9-oaep5/06e1de3d-82c2-11e5-a8b4-fa163e77e286/skydns\"\nI1104 11:53:35.003700       1 kubelet.go:96] failed to get stats from kubelet url: http://172.27.8.214:10255/stats/default/redis-master-rze58/92a91592-8138-11e5-a8b4-fa163e77e286/master - Get http://172.27.8.214:10255/stats/default/redis-master-rze58/92a91592-8138-11e5-a8b4-fa163e77e286/master: dial tcp 172.27.8.214:10255: connection refused\nI1104 11:53:35.003767       1 kube_pods.go:110] failed to get stats for container \"master\" in pod \"default\"/\"redis-master-rze58\"\nI1104 11:53:35.003774       1 kubelet.go:96] failed to get stats from kubelet url: http://172.27.8.214:10255/stats/kube-system/kube-dns-v9-oaep5/06e1de3d-82c2-11e5-a8b4-fa163e77e286/skydns - Get http://172.27.8.214:10255/stats/kube-system/kube-dns-v9-oaep5/06e1de3d-82c2-11e5-a8b4-fa163e77e286/skydns: dial tcp 172.27.8.214:10255: connection refused\nI1104 11:53:35.003844       1 kube_pods.go:110] failed to get stats for container \"skydns\" in pod \"kube-system\"/\"kube-dns-v9-oaep5\"\nI1104 11:53:35.003871       1 kubelet.go:110] about to query kubelet using url: \"http://172.27.8.214:10255/stats/kube-system/kube-dns-v9-oaep5/06e1de3d-82c2-11e5-a8b4-fa163e77e286/healthz\"\nI1104 11:53:35.003986       1 kubelet.go:96] failed to get stats from kubelet url: http://172.27.8.214:10255/stats/kube-system/heapster-zcd76/3a99af72-82ea-11e5-a8b4-fa163e77e286/heapster - Get http://172.27.8.214:10255/stats/kube-system/heapster-zcd76/3a99af72-82ea-11e5-a8b4-fa163e77e286/heapster: dial tcp 172.27.8.214:10255: connection refused\nI1104 11:53:35.004023       1 kube_pods.go:110] failed to get stats for container \"heapster\" in pod \"kube-system\"/\"heapster-zcd76\"\nI1104 11:53:35.004200       1 kubelet.go:96] failed to get stats from kubelet url: http://172.27.8.214:10255/stats/kube-system/kube-dns-v9-oaep5/06e1de3d-82c2-11e5-a8b4-fa163e77e286/healthz - Get http://172.27.8.214:10255/stats/kube-system/kube-dns-v9-oaep5/06e1de3d-82c2-11e5-a8b4-fa163e77e286/healthz: dial tcp 172.27.8.214:10255: connection refused\nI1104 11:53:35.004250       1 kube_pods.go:110] failed to get stats for container \"healthz\" in pod \"kube-system\"/\"kube-dns-v9-oaep5\"\nI1104 11:53:35.007198       1 kube_nodes.go:59] Failed to get container stats from Kubelet on node \"172.27.8.212\"\nI1104 11:53:35.010137       1 kubelet.go:99] url: \"http://172.27.8.211:10255/stats/default/busybox/d22d309b-82c4-11e5-a8b4-fa163e77e286/busybox\", body: \"{\\\"num_stats\\\":60,\\\"start\\\":\\\"2015-11-04T11:53:30Z\\\",\\\"end\\\":\\\"2015-11-04T11:53:35Z\\\"}\", data: {ContainerReference:{Name:/system.slice/docker-e1a52f29c1bd116ecf599527f6c0d9e08b625b6c4edc8f69d3d1568d5299e382.scope Aliases:[k8s_busybox.d1c8ce40_busybox_default_d22d309b-82c4-11e5-a8b4-fa163e77e286_13b42095 e1a52f29c1bd116ecf599527f6c0d9e08b625b6c4edc8f69d3d1568d5299e382] Namespace:docker} Subcontainers:[] Spec:{CreationTime:2015-11-04 11:23:30.664343161 +0000 UTC Labels:map[io.kubernetes.pod.name:default/busybox] HasCpu:true Cpu:{Limit:2 MaxLimit:0 Mask:0-7} HasMemory:true Memory:{Limit:18446744073709551615 Reservation:0 SwapLimit:18446744073709551615} HasNetwork:true HasFilesystem:false HasDiskIo:true HasCustomMetrics:false CustomMetrics:[]} Stats:[]}\nand i try run heapster with the same config on another kubernetes , all the nodes are ready , has containers, the grafana can view whole containers and nodes.\n. ",
    "bluebreezecf": "@vishh \nYour guys are talking about the Jenkins, what is it about?\nIs the newly added test cause your inner Jenkins down?\n. @vishh \nIf what I can do to help just let know, bro.\nI just wish this merged PR has no harm to the stability and safety of master branch.\nIf it does, please let know and I will try to fix it.\n. @vishh: With my pleasure bro! \nIt's my responsibility to take care of the quality of my submitted codes.\nLet's make the heapster more stable, useful and powerful!   : \uff09\n. @vishh @mwielgus  @piosz \nAs far as I know, heapster has two level cache of k8s metrics in some extent. The first level is cache.realCache  (in sinks/cache/impl.go), which is holding the original and raw metrics collected from k8s. It has an ageing and clean-up mechanism triggered by listener.PodContainerEvicted\n(https://github.com/kubernetes/heapster/blob/master/sinks/cache/impl.go#L98). The runGC() method\ncleans the metrics data of pod containers/pods/free containers/node for ageing in every other gcDuration (the duration of *argCacheDuration).\n``` go\nfunc (rc *realCache) runGC() {\n    rc.Lock()\n    defer rc.Unlock()\n    for podName, podElem := range rc.pods {\n        for contName, contElem := range podElem.containers {\n            if rc.isTooOld(contElem.lastUpdated) {\n                delete(podElem.containers, contName)\n                for , listener := range rc.cacheListeners {\n                    if listener.PodContainerEvicted != nil {\n                        listener.PodContainerEvicted(podElem.Namespace, podElem.Name, contName)\n                    }\n                }\n            }\n        }\n        if rc.isTooOld(podElem.lastUpdated) {\n            delete(rc.pods, podName)\n            for , listener := range rc.cacheListeners {\n                if listener.PodEvicted != nil {\n                    listener.PodEvicted(podElem.Namespace, podElem.Name)\n                }\n            }\n        }\n    }\nfor nodeName, nodeElem := range rc.nodes {\n    for contName, contElem := range nodeElem.freeContainers {\n        if rc.isTooOld(contElem.lastUpdated) {\n            delete(nodeElem.freeContainers, contName)\n            for _, listener := range rc.cacheListeners {\n                if listener.FreeContainerEvicted != nil {\n                    listener.FreeContainerEvicted(nodeName, contName)\n                }\n            }\n        }\n    }\n\n    if rc.isTooOld(nodeElem.node.lastUpdated) {\n        delete(rc.nodes, nodeName)\n        for _, listener := range rc.cacheListeners {\n            if listener.NodeEvicted != nil {\n                listener.NodeEvicted(nodeName)\n            }\n        }\n    }\n}\n\n}\nfunc NewCache(bufferDuration, gcDuration time.Duration) Cache {\n    rc := &realCache{\n        pods:           make(map[string]podElement),\n        nodes:          make(map[string]nodeElement),\n        events:         store.NewGCStore(store.NewTimeStore(), bufferDuration),\n        eventUIDs:      make(map[string]struct{}),\n        bufferDuration: bufferDuration,\n    }\n    go util.Until(rc.runGC, gcDuration, util.NeverStop)\n    return rc\n}\n```\nThe second level cache of heapster is model.realModel (in model/impl.go), which stores the recomputed and aggregated metrics used by the api server of heapster to reply the outer-side\nrequests.\nThese two level caches are synchronized data by two ways:\n- Add data periodically from cache.realCache to model.realModel by realModel.Update(), which is \n  triggered by realManager.HousekeepModel()\ngo\nfunc (rm *realManager) HousekeepModel() {\n    for {\n        select {\n        //TODO: better timing here\n        case <-time.After(rm.modelDuration):\n            if err := rm.model.Update(rm.cache); err != nil {\n                glog.V(1).Infof(\"Model housekeeping returned error: %s\", err.Error())\n            }\n        case <-rm.modelStopChan:\n            return\n        }\n    }\n}\n- Callback the methods of listener to remove data from model.realModel, after runGC removes (ages)\n  the original metrics.The listener is initialized by realModel.GetCacheListener() in manager.NewManager(https://github.com/kubernetes/heapster/blob/master/manager/manager.go#L80). So it means heapster firstly removes the ageing of original metrics in cache.realCache, and secondly removes the corresponding aggregated metrics in model.realModel.\n``` go\nfunc NewManager(sources []source_api.Source, sinkManager sinks.ExternalSinkManager, res, bufferDuration time.Duration,\n    c cache.Cache, useModel bool, modelRes, modelDuration time.Duration) (Manager, error) {\nvar newModel model.Model\nif useModel {\n    newModel = model.NewModel(modelRes)\n    // Temporary semi-hack to get model storage garbage-collected.\n    c.AddCacheListener(newModel.GetCacheListener())\n}\n\n```\n\n\nThe actual used listener,which is created by model.GetCacheListener() , just does nothing \n(https://github.com/kubernetes/heapster/blob/master/model/impl.go#L387).\n\nIt is deleting objects right?\n\nSo i think the added removal of  aggregated metrics data in model.realModel is just aiming at be \naccordance with the corresponding original metrics in cache.realCache, and it may make the aggregated metrics more accurate. \n\n\nNow, we can figure out that the aggregated pod metrics may not be so accurate. Besides, the \naging pod container data in realModel will not be removed until the belonging pod is ageing and removed in the PodEvicted(), which is also only triggered by the runGC().\n\nBy ageing, I assume you are referring to terminated pods. Is that correct?\n\nI don't mean ageing is terminated pods, but just mean current realization of old data removal mechanism, which is triggered by cache.runGC(). Only the original metrics of pods containers/\npods/nodes/free containers are out of the date and will be removed from the cache.realCache beyond cache duration since current detecting(https://github.com/kubernetes/heapster/blob/master/sinks/cache/impl.go#L76).\ngo\nfunc (rc *realCache) isTooOld(lastUpdated time.Time) bool {\n    if time.Now().Sub(lastUpdated) >= rc.bufferDuration {\n        return true\n    }\n    return false\n}\n. @afein: Thanks for your kindly explanation.\nAFAIK, the self-managing ability of DayStore used in model.realModel just takes effects in the scene of adding newly metrics, just like the reapOldData operation of gcStore, which is used in cache.realCache\nto holds the original metrics. If currently something wrong has just happened to the related k8s cluster, or just in a certain duration, there may be no newly original metrics to be queries by heapster from k8s, and there may also be no newly aggregated metrics to store into model.realModel, this self-managing ability will not work in this scene.\nThe runGC runs periodically and never to stop to clear ageing original metrics in cache.realCache, as well as triggers the removal of ageing aggregated metrics stored in model.realModel. \nThese two kinds of metrics ageing mechanism run with cooperation, and the runGC one of aging operations can be a good insurance of triggering removal of old (original/aggregrated) metrics.\nBesides the inaccuracies of aggregated pod metrics, making runGC take care of the old metrics in\nmodel.realModel is my PR's  consideration.\n. @afein:\n\nwhen a Pod is terminated, the existing deletePod method should remove all references to the corresponding PodInfo struct\n\nActually the pod original metrics in cache.realCache are deleted by runGC, just because the difference of now time and podElement's laste updated time is larger than realCache.bufferDuration.\nThe real pod entity in k8s is removed will do nothing to the corresponding data structure in heapster cache immediately; while the metrics are just ageing by realCache.runGC (ageing for original metrics in cache.realCache directly, and for aggregated metrics in model.realModel indirectly), gcStore.reapOldData() (for original metrics in cache.realCache) or StatStore.rewind() (ageing for aggregated metrics in model.realModel)\n\n\nHow did you observe that old metrics are not removed for pods that have been deleted?\nWhat types of inaccuracies did you observe in aggregated pod metrics?\n\n\nActually I do not observe the inaccuracies or un-removal Influences, I just dig into the sources\ncodes of heapster and suppose there may exist such inaccuracies of aggregated metrics. \nI am curious if we can forcast it may happen something bad, maybe we can do some discussions\nor precautions against it to make things better. \uff1a\uff09\nThank so much for your replies and discussions @afein @vishh \n. @mwielgus \nCould you have spare time to check this PR and the above discussions ?\n. Thanks a lot, bro! @piosz\n. @vishh @piosz @afein @mwielgus:  Appreciate for your kindness and rigorous reviews, discussions and supports! Thanks Bros!\n. @vishh @piosz @afein @mwielgus \nCould someone review this PR ?\n. > @bluebreezecf Is there any particular reason why you had to write an opentsdb client on your own, from scratch? What are your plans regarding new versions, updates of the client, etc?\n\nAlso #272 implements a Bosun Sink which is essentially a OpenTsdb sink. Can\nwe combine those PRs? Maybe we can use the client libraries used by the PR?\n\n@mwielgus @vishh  I am just using OpenTSDB V2 as the backend to store K8S Metrics&Events. \nI have checked and used the little sdk of 'bosun.org/opentsdb' (https://godoc.org/bosun.org/opentsdb), but it does't meet my requirements and the apis implementations are not matching all the defined rest apis in OpenTSDB V2 doc (http://opentsdb.net/docs/build/html/api_http/index.html#api-endpoints).\nSo I just develop the opentsdb client (https://github.com/bluebreezecf/opentsdb-goclient) from scratch. I have most of the apis:\nshell\nGET             /api/aggregators\nGET,POST,DELETE /api/annotation\nPOST,DELETE     /api/annotation/bulk\nGET             /api/config\nGET             /api/dropcaches\nPOST            /api/put\nGET             /api/query\nGET             /api/serializers\nGET             /api/stats\nGET             /api/suggest\nPOST            /api/uid/assign\nGET,POST,DELETE /api/uid/tsmeta\nGET,POST,DELETE /api/uid/uidmeta\nGET             /api/version\nLater I will test it fully to make opentsdb-goclient more strong and fully matching the rest apis of OpenTSDB V2.X.\n\nPlease be aware that we will have to review the client library before allowing it in, as the project (https://github.com/bluebreezecf/opentsdb-goclient) has 0 forks and just 1 contributor.\n\n@mwielgus @vishh I am willing to see opentsdb-goclient will be review by your guys, which will make it more efficiency and strong. I think if the opentsdb-goclient will make go-lang developers more easy\nto use opentsdb, it will be bound to be popular with people \uff1a\uff09. \n. @vishh @piosz @mwielgus \nBro,could someone go on to check this pr?  With great appreciations!\n. @mwielgus \nThe involved commits for this pr come across for a long time. I once merged some latest codes into the add-opentsdb-sink branch,and  there are some commits from other contributors in my local dev branch. My own commits in this pr are separated,  so it may be hard for me to squash my own commits into one commit.\nCould you give me some advice to do the squash work, or just pass this pr :\uff09\n. @mwielgus  Thanks bro!\n. > I thought this worked by specifying multiple --sink=... flags?\n@jimmidyson \nCurrently heapster doesn't support this scene with the format like '--sink=gcm,gcmautoscaling'.\nAfter merging this pr, heapster can support multiple sinks by using 'sink' parameter like the format\nof '--sink=sink1:xxxx,sink2:yyyy'.\n. > Doesn't it support something like --sink=gcm --sink=gcmautoscaling?\n@jimmidyson  It really supports this kind of sink configuration. Tks for your suggestion, bro!\nSo my PR could be a supplements to current sink configuration. \uff1a\uff09\n. @vishh @jimmidyson \nI am quite aggree with your advice to keep current usage of --sink flag but modify the corresponding sink document to present how to use multiple sinks with existing configuration.\nHere is the pr to fix this scene(https://github.com/kubernetes/heapster/pull/715). Maybe we can close current pr and track the issue of how to activate multiple sinks by using existing multiple --sink flag by\nthis newly created pr.\n. @jimmidyson here is the revised pr.\n. @jimmidyson Thanks for your kindness and rigorous reviews, bro!\n. @jimmidyson Tks, bro! We will see \uff1a\uff09\n. @jimmidyson @vishh @piosz @afein @mwielgus \nCould someone review this pr? Tks a lot!\n. @piosz Thanks for your review, bro!\n. @jimmidyson Thanks a lot for you kindness explanations. After digging into your whole pr, i find your modification is really nice and friendly!\nI will add some info of Accept-Encoding into this pr, thanks for your advice.\n. @jimmidyson Could you check out whether the added description in the end of the doc is good?\n. @DirectXMan12 I can take care of OpenTSDB sink. Sorry to reply so late.. @jakon89\nIn my preview tries, the OpenTSDB does not allow empty tags. So i prefer to send defaultTagValue if the value if empty.. @jakon89  It seems good to me.. @jakon89 Thanks for your meaningful advice. I wil dig into the doc and refresh the OpenTSDB client if it needs. 'opentsdbclient.NewClient(cfg)' currently returns error only in the scene that there is no value of cfg.OpentsdbHost. \nSo in this quoted stage, there is no error if currently the tsdb server isn't available.\nEach time calling StoreTimeseries() or StoreEvents(), it will use tsdbSink.client.Ping() to check\nwhether the opentsdb server is available. This will make sure push datapoints to opentsdb when it is available in most cases.\n``` java\nfunc (tsdbSink *openTSDBSink) StoreTimeseries(timeseries []sink_api.Timeseries) error {\n    if timeseries == nil || len(timeseries) <= 0 {\n        return nil\n    }\n    if err := tsdbSink.client.Ping(); err != nil {\n        return err\n    }\n    dataPoints := make([]opentsdbclient.DataPoint, 0, len(timeseries))\n    for , series := range timeseries {\n        dataPoints = append(dataPoints, tsdbSink.timeSeriesToPoint(&series))\n    }\n    , err := tsdbSink.client.Put(dataPoints, opentsdbclient.PutRespWithSummary)\n    if err != nil {\n        glog.Errorf(\"failed to write timeseries to opentsdb - %v\", err)\n        tsdbSink.recordWriteFailure()\n        return err\n    }\n    return nil\n}\nfunc (tsdbSink openTSDBSink) StoreEvents(events []kube_api.Event) error {\n    if events == nil || len(events) <= 0 {\n        return nil\n    }\n    if err := tsdbSink.client.Ping(); err != nil {\n        return err\n    }\n    dataPoints := make([]opentsdbclient.DataPoint, 0, len(events))\n    for _, event := range events {\n        datapointPtr := tsdbSink.eventToPoint(&event)\n        if datapointPtr != nil {\n            dataPoints = append(dataPoints, datapointPtr)\n        }\n    }\n    _, err := tsdbSink.client.Put(dataPoints, opentsdbclient.PutRespWithSummary)\n    if err != nil {\n        glog.Errorf(\"failed to write events to opentsdb - %v\", err)\n        tsdbSink.recordWriteFailure()\n        return err\n    }\n    return nil\n}\n```\n. @vishh \nThe opentsdb sink uses bluebreezecf/opentsdb-goclient to store k8s metrics and events into opentsdb v2.\nCurrently opentsdb v2 lacks an authentication and access control system. Therefore no authentication is required when accessing the API.(http://opentsdb.net/docs/build/html/api_http/index.html#authentication-permissions).\nSo there are no authentication info in the parameters of opentsdb sink.\n. @vishh \nOk, i will fix it according to your advice.\n. @vishh \nOk, i will modify it.\n. @jimmidyson Tks bro. Your description is more vivid.\n. @jimmidyson Your way of description looks more formal and better. I have refreshed the doc, please have a look, bro.\n. ",
    "tsn77130": "result when I'm lucky\n\n. Hi vishh, thanks for answer.\nI'm already using public IP to access grafana\n. No, nothing apperas with kubectl logs monitoring-influxdb-grafana-v2-vamvb grafana --namespace=kube-system\n. Hi, \n@thucatebay , thanks for your reply.\nI just have recreated services, rc & pods, and indeed it works great with 0.5 vertsion of influxDB.\nIt is a little confusing, because I used kubernetes documentation for install monitoring, and it refers to another github location , with 0.4 version in (see https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/cluster-monitoring/influxdb )\nMaybe it would be beneficial to group sources to avoid confusion \n. I was thinking a way to enable this change in influxdb configuration \n. ",
    "suchisubhra": "i have couple of issues...we have our own  name spaces.. It doesn't show up in  namespace list.\nAlso kube-system pod show nothing.. \nI just  downloaded and deployed heapster..\n. I also check\nhttp://master:8080/api/v1/proxy/namespaces/kube-system/services/heapster/api/v1/model/namespaces/\nand it is empty....Can somebody help me? where should I start looking\n. ",
    "hardikdr": "Any update on this ??\n. ",
    "andyxning": "@vishh @mwielgus @HardikDR\nI am now working on adding disk io stats metrics to Heapster. Did you have any thought about this? For example, which metrics should be collected and the metric names.\nI  prefer to add both read and write bytes per second metric separately. With this original data, we can also get the aggregated bytes per second including both read and write. . /remove-lifecycle stale. Close this as disk io has been added to heapster with legacy source.\n/close. @jimmidyson  This should be addressed by #1364 . Hi, guys, any plans for supporting heapster 1.1.0. We need the OpenTSDB sink. :)\n. @piosz what is the difficulty for migrating to supporting new heapster. :)\n. @piosz thanks for your info. If i am not misunderstanding, you mean that from heapster 1.0 we should use dashboard instead of kube-dash. If then, will kube-dash will be deprecated in the future? :)\n. @Bryk Ok, i will have a try. :)\n\nNing Xie\n2016-06-28 15:52 GMT+08:00 Piotr Bryk notifications@github.com:\n\n@andyxning https://github.com/andyxning Dashboard UI eng here. Some\nkube-dash functionality is not supported yet in the Dashboard UI. But we\nare committed to have all monitoring functionalities that kubedash had +\nmore features this quarter. In the meantime, the Dashboard UI still has\nmany nice non-monitoring features that you should try out :)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/1031#issuecomment-228977449,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/ACSxaM4nw7EC9fgHAIc_CiKHy_u65Hrwks5qQNK4gaJpZM4HnMfo\n.\n. Once #1788 is merged. We this should not be a problem again. . +1, any details about this?\n. @tedcy what is the .mount containers.\n. We should absolutely update cAdvisor dependency in order to work with K8s 1.3 version.\n\nThis is useful for use to get rss and cache memory info for containers. :-)\nI have tried update the cadvisor dependency info in godep.json to:\n\"ImportPath\": \"github.com/google/cadvisor/info/v1\",\n\"Comment\": \"v0.23.6\",\n\"Rev\": \"4dbefc9b671b81257973a33211fb12370c1a526e\"\nand then run godep update github.com/google/cadvisor/info/v1. I finally got the godep.json with cAdvisor version with v0.23.2-6-g1c8d789:\n\"ImportPath\": \"github.com/google/cadvisor/info/v1\",\n\"Comment\": \"v0.23.2-6-g1c8d789\",\n\"Rev\": \"1c8d7896a5225400df51b47330b50368e548eb94\"\nHowever, this is already good enough to keep in touch with the release step of cAdvisor. :)\n. @piosz that's ok and we should keep in sync with kubernets. \ud83d\udc4d \n. Since the RP #1245 has been merged, this issue is closed. :)\n. /cc @vishh @mwielgus\n. Since the PR #1247 has been merged in master branch. This issue is closed. :)\n. /cc @vishh @mwielgus\n. /cc @piosz  @mwielgus\n. @AlmogBaku i think it is related to your permitted. it seems that you have no write permition to the specified dir\n. /ping @mwielgus @vishh\n. @AlmogBaku, yep, i agree that we should not use sudo to run make build. Can you please paste your ll -h on the /usr/local/go/pkg/ and what is your current user to run make build.\nAnd, i am wonder whether you can install other binaries to /usr/local/go/pkg/linux_amd64. :)\n. @AlmogBaku it seems it is related to cross compile in OSX.\n. /cc @vishh @mwielgus\n. /cc @piosz @mvdan @vmarmol @huangyuqi can any guys can have a look at this. :)\n. @AlmogBaku @DirectXMan12 \nI think we should add an http server. As currently we add prometheus metrics to eventer, however we have not enabled an http server to serve /metric endpoint which makes adding prometheus metrics useless. \nWDYT.. @AlmogBaku @DirectXMan12 I will make a PR later.. @DirectXMan12 @AlmogBaku PR has been fired. PTAL~. ping @piosz \n. ping @piosz @mvdan @vmarmol @huangyuqi \n. ping @thockin @piosz @DirectXMan12\n. I will work on this in the next few days. \n. @AlmogBaku can you guys please have a look at #1379 #1374. I also make some RP based on the current master branch. BTW, creating index daily is really a wonderful thing as i also searched for how to make ES store time-limited events. :)\n. Can we just split the new features into small ones and small PRs. In this way maybe we can push this forward faster. :)\n. @AlmogBaku Can you also please have a look at #1380 also. Ignore the  build failure status, it seems that Heapster build will burst above the memory limit Travis CI container set and the build will be OOM killed. So, in #1379 i have updated the Travis CI configuration to use a VM which according to Travis CI has about 7.5GB memory except longer build start time. :)\n. @piosz yes, Eventer reads Kubernetes API for events. I want to reuse Eventer for error handling when connecting to Kubernetes API and also the capability for sinking to other destinations at the same time. With Api sink enabled, we can do this job in one program. :)\n. @piosz i think may be i express the api sink idea not so good. It will not add any new api to eventer. :)\nWith api sink, we can config an external api server to POST the received events. just like we can store the events to the es.\n. @piosz yes. the other sources is Eventer. I just want one program to watch the Kubernetes api server. Then, Eventer can sink the events to multiple sinks at the same time. :)\n```\n                                     PUSH\n               watch(PUSH)         |------ ES          POST(HTTP)\nKubernetes Api    --->  Eventer    ------- Api Server  -----> Alarm(http://localhost:3080/api/events)\n                                   |------ InfluxDB \n``\n. @piosz I have updated the description map. \n- What api server sink does is when it receives events from Eventer it will call HTTP POST method on **Alarm** with api url http://localhost:2080/api/events. \n- What **Alarm** does is receive events and then judge according to predefined alarm rules. Such as, when an system oom event is received, it will send an alarm message via Slack.\n. Just to mention that i have write [eventarbiter](https://github.com/andyxning/eventarbiter) which utilizeseventerto filter event alarm. Have fun with it. :-). It seems that memory usage metric for container also contains cache. This should also be fixed as now cadvisor has already contains this info about usage, rss, cache. Aso, with aufs, the page cache may be large. Maybe rss is the really useful metric to determine whether we should make more instances or enlarge memory request. i signed it\n. @piosz  @AlmogBaku @huangyuqi . @piosz Fixed. PTAL.. @piosz  @AlmogBaku  @huangyuqi . @piosz Done. PTAL.. @AlmogBaku @piosz Doc about--label-seperatorcommand line has been added todocs/storage-schema.md. PTAL.. @piosz @AlmogBaku @huangyuqi . @AlmogBaku yes one of my pr is reported failed with gofmt error. After gofmt them, everthing is good. \ud83d\ude02\n. see the first and the second commit in #1367 to learn more info about this.\n. @piosz Can you please take a look at the Jenkins GCE e2e test fail message. I have no idea about how to fix it. . @piosz Maybe this PR can be merged with no worries. The Travis CI failure is about OOM which should be addressed by merging #1364 .. @piosz Done.. @piosz @huangyuqi . @googlebot i run it correctly in my laptop. \n. I don't know why the testTestOneExportInTimeink8s.io/heapster/metrics/sinkswill fail. I runs ok in my laptop. :( \n. it turns out that memory is out and something likego build testmain: /home/travis/.gimme/versions/go1.6.linux.amd64/pkg/tool/linux_amd64/link: signal: killedis reported. For more info please see [Travis CI build envrionment](https://docs.travis-ci.com/user/ci-environment/#Virtualization-environments) and [Travis CI FAQ about memory limit](https://docs.travis-ci.com/user/common-build-problems/#My-build-script-is-killed-without-any-error)\n. This PR should be merged after #1379 cause i have update the Travis CI build environment from container-based to vm-based to avoid OOM while building and testing\n. @AlmogBaku Another Index other than the default Heapster? I am not quite familiar with ES, can you please explain this?  . you mean that we should change to another index instead of Heapster? Am you worried that this change will break backward capacity or something else? Sorry for not get the point why we should change to another index. :). Ok, i see. But there are still some limitations in using multi-index instead of adding a cluster tag inHeapster` index. \nIf we use multi-index scheme and Grafana as the WebUI for ElasticSearch, According to current Grafana document, wen can only specify one index in one ElasticSearch datasource. If we have three  clusters in each datacenter, we should add three datasource in order to query events from them for each datacenter. \nMost importantly, we can not query aggregated events from all the clusters in each datacenter.. @piosz If that's suitable for you, i will try to change the PR. :). @piosz @AlmogBaku I have changed the PR to support ElasticSearch cluster URL parameter for both Heapster and Eventer. PTAL.. @AlmogBaku PTAL.. \nReview status: 0 of 7 files reviewed at latest revision, 14 unresolved discussions.\n\ncommon/elasticsearch/elasticsearch.go, line 38 at r1 (raw file):\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\nI think we should call it ClusterName and not just cluster.. it'll be easier to understand imo\n\nDone.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 0 of 8 files reviewed at latest revision, 14 unresolved discussions.\n\ncommon/elasticsearch/elasticsearch.go, line 115 at r1 (raw file):\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\nI think we should call it cluster_name and not just cluster.. it'll be easier to understand imo\n\nDone.\n\ncommon/elasticsearch/elasticsearch_test.go, line 26 at r1 (raw file):\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\ncan you add a test with a different cluster_name value?\n\nDone.\n\ncommon/elasticsearch/mapping.go, line 184 at r1 (raw file):\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\nalso, as i mentioned- should be CluserName imo\n\nDone.\n\ncommon/elasticsearch/mapping.go, line 293 at r1 (raw file):\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\nI think we should call it ClusterName and not just cluster.. it'll be easier to understand imo\n\nDone.\n\ncommon/elasticsearch/mapping.go, line 125 at r2 (raw file):\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\nplease fix indentation. and use cluster_name (to keep a common case for the tags)\n\nDone.\n\ncommon/elasticsearch/mapping.go, line 209 at r2 (raw file):\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\nand use cluster_name (to keep a common case for the tags)\n\nDone. It turns out that  sometabs is inside.\n\ndocs/sink-configuration.md, line 187 at r1 (raw file):\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\nI think we should call it cluster_name and not just cluster.. it'll be easier to understand imo\n\nDone.\n\nmetrics/sinks/elasticsearch/driver.go, line 95 at r2 (raw file):\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\nLet's use cluster_name (to keep a common case for the tags)\n\nDone.\n\nmetrics/sinks/elasticsearch/driver.go, line 115 at r2 (raw file):\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\nLet's use cluster_name (to keep a common case for the tags)\n\nDone.\n\nmetrics/sinks/elasticsearch/driver.go, line 116 at r2 (raw file):\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\nLet's use cluster_name (to keep a common case for the tags)\n\nDone.\n\nmetrics/sinks/elasticsearch/driver.go, line 131 at r2 (raw file):\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\nLet's use cluster_name (to keep a common case for the tags)\n\nDone.\n\nmetrics/sinks/elasticsearch/driver_test.go, line 192 at r1 (raw file):\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\nvalues should be added to all the metrics under \"metricstags\"..\n\nDone.\n\nmetrics/sinks/elasticsearch/driver_test.go, line 192 at r2 (raw file):\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\nwhy is it empty? shouldn't the default set to \"default\"?\nalso, can you also modify the \"Events\" test\n\n\nIn test we use ElasticSearchService to generate an esSvc instance instead of CreateElasticSearchService which will add cluster_name.\n\n\nComments from Reviewable\n Sent from Reviewable.io \n. @AlmogBaku @DirectXMan12 @piosz PTAL.. /cc @mwielgus . I think so. I will try to make it tomorrow. The requirement is the same. @matthughes Do you mean that you want this behavior for both events(eventer) and metric(heapster)? . This has been implemented in #1380. Please take a look and review it. :)\n=======\nNing Xie\n2016-12-02 21:49 GMT+08:00 Matt Hughes notifications@github.com:\n\nBoth I would think. It would be a configuration flag of the ES sink.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/1383#issuecomment-264457602,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACSxaA8VYALSvF9o1kOCmGmqK6Im-zsPks5rECHQgaJpZM4Ky1IV\n.\n. @r2d4 Why not push to both and let the users choose which one they will use. BTW, i agree that we should be consistent. :)\n. @piosz it seems that the Travis CI is failed because OOM. See this comment for more info. \n\nTL;DR, we should move to sudo-enabled Travis CI by remove sudo: disable in .travis.yml. \nAfter this, i will update all my PR to base on this to make Jenkins GCE e2e pass.. @piosz @DirectXMan12 This is convenient for end users. I have no idea about whether there is a tool can help us  adding mac OSX and Linux binary release easily when releasing.. @kadamvandita you can simply compile heapster and eventer by make build under top level dir. It will compile heapster and eventer according to your compile machine's arch and os.. @kadamvandita \n\na spec file\n\nwhat is a spec file? Could you please paste it here?\n\nI get warning saying unstripped binary for s390x.\n\nwhat is your compile machine's arch and os.. @piosz  PTAL.. /cc @DirectXMan12 . @DirectXMan12 You're right. I will change the PR later. :). @DirectXMan12 @piosz PTAL.. /cc @mwielgus . @piosz @DirectXMan12 @mwielgus It seems that we can merge this. :). @DirectXMan12 I may maintain InfluxDB, too. We currently use InfluxDB to store metrics internally.. @bavarianbidi  Thanks for the PR. :)\nI think we need this PR. With this we can customize the name of retention policy. \ud83d\udc4d \n@bavarianbidi Could you please rebase the code and resolve the conflicts. :)\nBTW, According to what @piosz said, you need to squash your commits, this may help resolve CLA issue. :). @jackzampolin could you please compress your commits to just one commit. :). @DirectXMan12 This issue should be closed for now.. @AlmogBaku Should we close this?. \n\nReviewed 1 of 1 files at r1.\nReview status: all files reviewed at latest revision, all discussions resolved.\n\nComments from Reviewable\n Sent from Reviewable.io \n. /ping @DirectXMan12 @AlmogBaku @huangyuqi @piosz . @AlmogBaku \n\nWhere do you fill the data in the driver? you haven't committed any changes to the eventer driver...\n\nJust like name and namespace field under MetaData, labels field is also contained in ObjectMeta\n\nWhy do you need the RAW data of this field.. it's seems classy just for search...\n\nYep. I want to search some meta labels with wildcard or regexp. This is the main purpose to add labels field.. @AlmogBaku \nIt's my fault for misconsidering Labels filed in ObjectMeta as string type. In that situation, eventer will not write event's ObjectMeta labels.\nI have updated the PR by making labels field in the same level with Reason and Message instead of putting it under ObjectMeta cause we need to convert a labels map to a string.\nPTAL.. @AlmogBaku I have test this feature in my local environment. It finally turns out that the ObjectMeta is contains data about the event itself not the corresponding pod or some other resource  types. The Labels is always empty. :(\nBTW, do you know the other way that we can get the corresponding pod's labels for an event.. @AlmogBaku \n\nI don't think this information is available without changing the functionality.\n\nAgree. To make this work. we need to make a change to kubernetes. I will make it later to see whether kubernetes community can agree on this.\nAt this moment, we can close this PR. :)\n. I am curious about this, too. IMO, this is mainly related to cAdvisor or libcontainer. Heapster is just collecting time series data from cAdvisor. :)\nAfter cAdvisor supports GPU, iiuc, nothing need to change in heapster.. It seems that @jianzhangbjz has already post an issue about this in cAdvisor#1436.. @piosz @DirectXMan12 @mwielgus PTAL.. Sorry for the delay.\n@vishh \n\nSo I'd recommend adding metrics for \"space\", \"inodes\" and \"IO\" for disk\n\n\nThere already exists filesystem/usage, filesystemd/limit and filesystem/available metrics for space. \nAs for inodes, it seems that blkio cgroup has no info about this. I have found that inodes info is available in cAdvisor FsStats. Will add this in another PR(#1542). \nblkio cgroup supports setting resource Throttling/Upper limit policy for IO. This is what this PR does.\n. Comments addressed. Friendly ping @timstclair @vishh . Friendly ping again. :) @timstclair @vishh. @jingxu97 Yes. We need this also. I will rebase and resolve the conflicts in next week. \n\nBTW, any thoughts on the implementations or suggestions.. @DirectXMan12 @piosz @loburm @vishh @dnavre @weikinhuang @jingxu97 This PR is ready to review. :) \nPTAL.. Ping @DirectXMan12 @piosz @loburm since #1879 has been merge. This PR is ready to review and should be ready to merge too. PTAL.. Ping @kubernetes/heapster-maintainers \nSorry for the complain. But can someone help me to review this PR instead of just reviewing and merging Google related ones. :(. > sorry for the delay. This needs to add metrics from the summary source, since that's the reccomended source these days.\nAccording to https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/stats/v1alpha1/types.go#L110-L135, the disk io related data has not been added to kubelet stats api. I prefer to add legacy stats api support for now. Adding summary api support is another long term plan just like network metrics #1788 .\nSo, for now please help to review this PR.. Ping @DirectXMan12 . Ping @piosz .\nAdding summary api support is another long term plan just like network metrics #1788, so i suggest we first review this one.. Thanks @DirectXMan12 @piosz . \nYes, it is quite a question when the cluster becomes more and more large. With the original cAdvisor stats endpoint, more and more redundant data needs to be scraped as the cluster grows. This is quite a huge overhead. \nBtw, the summary api needs to be enhanced to accomplish our basic metrics requirements. :) Will do some works on that. :). @weikinhuang \ndisk/io_read_bytes_rate: Number of bytes read from a disk partition per second\ndisk/io_write_bytes_rate: Number of bytes written to a disk partition per second\nThis needs legacy source for kubelet instead of the summary one.. @weikinhuang How is your configuration for --source?. @weikinhuang As for now there is not pr for adding disk io metric in summary api. I am planning to add this in next week. If you can help and make a PR about this, i will be ok to review it. :). @weikinhuang To that situation, i suggest to wait for another week maybe for this feature added to summary api source. I will give it a try.. Any logs for heapster?. > just wondering that are you still working on adding disk metrics to summary api source?\nYes, i will work on this. But no time recently available working on this. :( \nMaybe one week later.. @Gimi Which sink backend do you use?. @Gimi Just have thought that do you use the summary data source? The disk io is currently only available on legacy data source. The summary api is used when start with heapster with --source=kubernetes.summary_api:''. @Gimi I have test master branch with statsd sink and legacy data source and the disk io metrics is available like:\n\nnode.192_168_0_1.batch.sandbox;beta_kubernetes_io/arch.disk/io_read_bytes_rate./dev/sdb:15381.686|g\nnode.192_168_0_1.batch.sandbox;beta_kubernetes_io/arch.disk/io_read_bytes_rate./dev/sda:-1.3200837e+09|g\nnode.192_168_0_1.batch.sandbox;beta_kubernetes_io/arch.disk/io_write_bytes_rate./dev/sdb:2.1783398e+06|g\nnode.192_168_0_1.batch.sandbox;beta_kubernetes_io/arch.disk/io_write_bytes_rate./dev/sda:-5.901337e+10|g. @zte-wangning \nI would like to know whether the values collected by Heaspter are averages over the sampling interval and can represent the average of the sampling intervals, or just a single point in the sampling interval.\n\nHeapster's metric values are collected from cAdvisor actually. cAdvisor reads and parses values from cgroups files at specified interval. So, the answer is values collected by Heapster are not averages over the sampling, they are just single points in the sampling interval.\n\nFor example, set the heapster sampling interval to 1 minute, can the reported points represent the average of one minute? \n\nIf you want to monitor more precisely, you can adjust following command line arguments. For example, we collects metrics every 10s.\n Heapster: -metric_resolution 10s\n Kubelet(cAdvisor): --global-housekeeping-interval=5s --housekeeping-interval=5s(Note: housekeeping interval should be less than metric_resolution.)\nThis will make Kubelet(cAdvisor) collects metric values from cgroups files every 5s, and heapster will query these metrics every 10s. \n\nIf the point can't represent the average,how can I get the average value in one minute.\n\nYou should use one of the sinks heapster supports, such as InfluxDB, to store metric values. After then, you can use the aggregation functions InfluxDB supports to query average metric values in one minute.  \n. @DirectXMan12 @huangyuqi @piosz \nAccording to the sink-owners.md, Kafka only supports Metric, not supporting Event. So @huangyuqi , could you please confirm this?. @miaoyq \n\nlooks like we actually have a Kafka events sink too, but that page didn't get updated. Can you please also update sink-owners.md in this PR?\n\nCould you please update sink-owners.md?\nOverall SGTM.. @qianzhangxa \n\nIn Elasticsearch, I see there are some metrics sent from Heapster\n\nDoes the time of metrics successfully stored in ES sit between the ones for warning log or ahead of them.\n\nIt seems it will fail every 3 mins. Is there anything wrong in the options that I used to start Heapster?\n\nES sink will only try to store metrics to ES every 30s(according to your settings with metric_resolution) and the log should be recorded every 30s. According to your pasted log, it seems that you sample the warning logs, can you please paste the full log here.\n\nCan you please make a  verbose log with making -v=4 and paste the log here.\nI have encountered the same warning log in Eventer and finally it turns out that there is something wrong in ES. Can you please make sure that your ES are always in green or yellow state especially during the warning log is recorded.. @qianzhangxa If that is true, maybe it is related with network between heapster and ES.\n\nwhat is the avg RTT time and the result of ping to ES address?. @merqurio I have run into this error for sink to ES with timeout error. I am not sure if it is true for your situation. \nBut we definitely need to add more error detail info here instead of just a generic error description.. @merqurio \nAccording to Heapster metrics ES sink impl, the root reason for error Failed to push data to sink is because the ES sink operation is synchronous and when export timeout is expired, an error log is logged.\nSo i think the root reason for this is that the write operation to ES is timeout. You may need to investigate the response time for ES and the data size for every write operation.. seems like a bug in heapster. @merqurio which version of heapster do you use?. @fate-grand-order You should sign google's CLA also according to comment-272777135. \n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address. Check your existing CLA data and verify that your email is set on your git commits.\n\nIf you have signed it with another email address other than the one used to sign google CLA. You should make it coherent. :)\nBTW, the PR SGTM.. LGTM\n@fate-grand-order Thanks a lot!\n/cc @piosz @mwielgus @DirectXMan12 . @neith00 \ud83d\udc4f \n/lgtm. Will Review this later.\n=======\nAndy Xie\n2017-02-16 4:24 GMT+08:00 Joseph Schneider notifications@github.com:\n\n@DirectXMan12 https://github.com/DirectXMan12 this should be\nsink/elasticsearch, not influxdb.\n\u2014\nYou are receiving this because you were assigned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1525#issuecomment-280128054,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACSxaG4_QK3zMB5sX-ZIGMUoSX6H42Suks5rc179gaJpZM4MBY3u\n.\n. @astropuffin Seems you have not successfully signing linux foundation cla yet. You need to do that first. :). @astropuffin \n\nWe need add more info for the root cause. This is useful for us to firstly get what this PR will do and why the original code can not satisfy our goal. :) \n\nRoot cause:\nOpaque would work if it was set, but uri.Parse does not actually set Opaque.\n\nAccording to golang url doc, Opaque will only be setted when uri is in the format like below:\n\nscheme:opaque[?query][#fragment]\n\nSo, we can not use opaque as the es url when actually the specified es url is in valid http or https schema.. /cc @AlmogBaku PTAL.. @astropuffin Yes, you're right. This PR will solve your empirical usage. :) \nThe last step you need is to resolve your linuxfoundation CLA. :). @astropuffin \ud83d\udc4f Overall LGTM. Just two nits\n could you please squeeze you commits with git rebase. :)\n IMO, we need to document this usage when just using one es backend.. Code rebased. Friendly ping @AlmogBaku @DirectXMan12 @piosz . @k8s-bot test this. @DirectXMan12 \n\nbut if we're going to land the Godeps update, I'd like to get that landed first.\n\nThere is no Godeps update. Any clue?. @DirectXMan12 #1537 has been merged. And, it seems nothing needs to rebase. :). @DirectXMan12 This PR will only import k8s.io/kubernetes/pkg/healthz. It is a utility package. IMO, this may be fine. If we need to change this to k8s.io/metrics package, we may need first add the healthz package to k8s.io/metrics first and just wrap all the things what has been done in k8s.io/kubernetes/pkg/healthz without any changes. This seems not so good. :)\nBTW, what is the purpose of creating k8s.io/metrics. . Finally, i find that healthz package has been change to k8s.io/apiserver/pkg/server/healthz. So, in order to keep consistency, i have renamed the healthz package to k8s.io/apiserver/pkg/server/healthz too. @DirectXMan12 PTAL.. @DirectXMan12 Done. PTAL.. @k8s-bot test this. Friendly ping. :) @DirectXMan12 @piosz . @DirectXMan12 PTAL~.. Friendly ping. :) @piosz\n. @AlmogBaku Addressed. I have forgotten to do that. :). Friendly ping. :) @piosz. Agree with @DirectXMan12. Time synchronization for all node in a distributed system is necessary and a preliminary.. @tianshapjq I am afraid that we may not add this to docs. @DirectXMan12 any thoughts?. SGTM. Ping @DirectXMan12  @piosz . @vjdhama Thanks for you contribution! It seems that your linux foundation CLA has not signed properly. Please you please take a look at blow tips and recheck your sign. :)\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address. Check your existing CLA data and verify that your email is set on your git commits.\n. /lgtm. @tianshapjq Thanks!\n\n/lgtm. @DirectXMan12 PTAL.. @bruceauyeung Thanks for your contribution!\n/lgtm. @DirectXMan12 \n\nmetric resolution really shouldn't be lower than 30s anyway, because of issues with cAdvisor.\n\nCould you please point out the what issues prevent metric resolution from being set lower than 30s.\nFYI, we currently use 10s. :(. LGTM. @k8s-bot ok to test. I agree with @dasher to provide a configuration method to provide a way to install plugins at runtime cause we can not confirm what plugins to install at compile time.. @lcnsir heapster and eventer uses olivere/elastic as the ES client. I have find nothing info about ssl for it.\nHowever, after diving into the code, it seems that olivere/elastic uses golang default http client to process http requests. So, the question can be addressed by confirm that whether http default client supports ssl. \nIt seems that golang http default client supports well known certificates. As for self-issued certificates, i am afraid that there is no way to do this for olivere/elastic and even heapster and eventer. \nyou can try it with well known certificastes according elasticsearch doc.\nAny feedback is appreciated. :). @alexandrst88 glog can not disable log. \n without any --v option, output generic log like glog.Info or glog.Warning\n with setting --v=1 option, output verbose log glog.V(1).Info\n* with setting --v=2 option, output verbose log glog.V(2).Info\nyou can take a look at glog usage info.. You can only adjust stderrthreshold=WARNING command line argument to control the log level when logging to stderr and files by setting alsologtostderr=true. This way you can only adjust stderr log level. Logs in files will contains  all leveled logs.\nAs for logging to files, i am afraid that there is no easy way to accomplish this.\nBTW, i am curious that why you want to disable info logs. It is helpful for debugging and learning the status of heapster from logs. At the same time, heapster will not print much log.. /lgtm. Currently we do not support cluster tag. Heapster supports cluster_name argument for ElasticSearch sink to differentiate different Kubernetes clusters. \nOne way to work around is to add cluster=esat labels to pods, nodes and resource you want to differentiate, cause we do really collect all labels into a single labels tag value with comma(default) seperated key=value. After that you should use regex search syntax to query InfluxDB.. I also think so. Will do it recently.. /ping @DirectXMan12 @piosz . I agree with @DirectXMan12 since it seems ES has already meet the demands.. @caarlos0 This PR can be closed as we have update the grafana version to v4.2.0 in #1624 .\nThe only problem is that we just merge the PR without actually compile and push the new image to gcr.io registry. This requires Google people to do. @piosz @DirectXMan12 PTAL.. @jeffery9 Could you do wget https://kubernetes.default/api/v1/namespaces?resourceVersion=0 in the heapster pod?. @DirectXMan12 I will work on a patch recently. I think we should just do this just for Influxdb sink for now. Once all other sinks make an agreement on this, we should refactor this from the original source.. AFAIK, kubernetes has no api having a uuid as a query parameter(please let me know if i am wrong). If we need uuid info, by composing time, namespace, and pod name we can query kubernetes for corresponding uuid.. namespace_name + pod_name can also uniquely determine a pod.. > namespace_name + pod_name is not unique in historical metrics.\nThis quite be true. But i am curious about your demand on what you need a unique identification among the total time window metrics.. IIUC, you mean that you want to query historical pod metrics, right?\nSince deployment name in a namespace is unique and ReplicaSet and Pod name are both derived from deployment name, it should be ok to uniquely identify the pods for different releases plus time range since only one deployment release should exist in a namespace at a time and the same for pods and resplicasets.\nI have no see any circumstance where we need use pod uid to distinguish different pods.\n@ringtail. @bluishpe Have you tried to access http://monitoring-influxdb:8086/ping in the chrome address bar or wget  http://monitoring-influxdb:8086/ping. According to the response, it seems that it is the wrong configuration in the reverse proxy about monitoring-influxdb instead of heapster.\nYou should check and confirm that you setting up an influxdb instance correctly.. * check if the influxdb instance ping endpoint is accessible by ip:port/ping.\n If influxdb is accessible through ip:port/ping, then you should check your reverse proxy settings. If that is the truth, could you please paste the configuration. \n If influxdb can not be accessed by ip:port/ping, then you should firstly check whether the influxdb instance is running.. Sorry for not making it clear that what i have proposed for checking is targeted for influxdb. Comments has been updated, please take a look at this.. Seems that you may try this wget http://monitoring-influxdb/ping. Remove the 8086 port.. I am not faimilar with kube-dns. Have you tried minotoring-influxdb without the port 8086?\nYOU deploy influxdb in kubernetes cluster?. Could you please paste the k8s rc/pod configuration about influxdb. you should try to wget inside heapster pod. wget http://monitoring-influxdb:30322/ping. Have you tried the wget within the heapster pod?\nWhy your wget resulting 16.*, i.e. public ip address. I guess maybe this is related to your /etc/resolv.conf settings. Can you please run the following command and paste the response.\nkubectl --namespace=kube-system exec heapster-1865471235-7sbfx -- cat /etc/resolv.conf. Your influxdb pod should be accessible by monitoring-influxdb.kube-system.svc.cluster.local/ping according to doc.. It seems that the kube-dns is not working properly. You should check if it is set up properly.. Sorry. I am not familiar with kube-dns. You should also check for the logs of kube-dns and whether the service for kube-dns is available. . @andrey01 \n\nDo you have any other special settings for DNS? \nWhat is the content of /etc/resolv.conf?. @andrey01  IIUC, when query an dns, client should not query 127.0.0.1:53, it should consult nameservers directly. Seems something is wrong.. @andrey01 What is the result of nslookup monitoring-influxdb.kube-system.svc inside the heapster pod?\n\nWhat is the result of nslookup monitoring-influxdb.kube-system.svc.cluster.local. inside the heapster pod with the fully qualified domain name(special note for the suffix dot after local)?\nSeems that it can not do a dns query for monitoring-influxdb.kube-system.svc.. @andrey01 \n\nis there a way to see what /etc/resolv.conf uses the heapster container itself?\n\nI am afraid that you can only docker inspect heapster-POD_NON_POD_INFRA_CONTAINER and search for ResolvConfPath mapping to get the content for resolv.conf.. @andrey01 IIUC, you mean that the root reason for heapster to perform a DNS query against 127.0.0.1 is that heapster can not read the /etc/resolv.conf file?. @andrey01 The docker file used to build heapster is at here.\n```\nFROM scratch\nCOPY heapster eventer /\nCOPY ca-certificates.crt /etc/ssl/certs/\nnobody:nobody\nUSER 65534:65534\nENTRYPOINT [\"/heapster\"]\n```\nActually, according to the settings comment, it is expected that 65534 is the UID of nobody. Maybe this is not true across all linux distributions.. @andrey01 According to the user Dockerfile doc. \n\nWarning: When the user does doesn\u2019t have a primary group then the image (or the next instructions) will be run with the root group.\n\nWhen there are no primary group then the heapster command will be run as root group. You need to check the primary group for the nobody user.. @piosz Many thanks. \ud83d\udc4f . @AlmogBaku pod_namespace has been commented as deprecated for more than two years. It has been deleted in #1659. This PR will clean all the references about pod_namespace. :). Friendly ping @piosz @DirectXMan12 . @piosz @DirectXMan12 PTAL. :). I am not sure whether both host_id and nodename are all the same always not matter deployed on bare metal or GCE or AWS. \nhost_id is the value of node.Spec.ExternalID. nodename is the value of node.Name. Friendly ping @piosz @DirectXMan12 :). @piosz @DirectXMan12 PTAL. :). @DirectXMan12 Comments addressed. PTAL.\nAfter reading code about heapster and kubernetes. The value for host_id is the value of node.Spec.ExternalID.  The value for ExternalID is specific to vendors. Its value maybe the same as node.Name(i.e., nodename in heapster) and may not.\nSo, if we can have an endpoint to query ExternalID from node.Name(i.e., nodename in heapster), we can remove host_id from InfluxDB.\n. @DirectXMan12 Many thanks. \ud83d\udc4f . @yaacov \nFirstly, you should sign the CLA. :). /assign @DirectXMan12 . @yaacov you should follow the instructions in this page. @yaacov \ud83d\udc4d \n@DirectXMan12 PTAL.. @yaacov You also need to sign the Google CLA according to this comment.. /lgtm. @mmaquevice Thanks for your PR. :)\nAccording to #1624 , we have updated the heapster-grafana-amd64 image to v4.2.0. We may forget to push the corresponding image to gcr.io registry. @piosz @DirectXMan12 Could one of you check this? :)\nOnce v4.2.0 heapster-grafana-amd64 image pushed to gcr.io, we should close this and #1669 \nBTW, we should also update the readme file about v4.2.0 release. . Friendly ping @piosz :). @marcinkubica Yes. The desired status is 4.2.0 image is also in the gcr.io. This can only be done for Google people. :). Agree with @DirectXMan12 \nIt seems that the time range you specify contains both deleted pods and currently running pods. Once you specify show series from the time pods deleted, they should not show up.\n\nIf the PODS are deleted, grafana -> podname drop down does not display the POD is deleted. \n\nThis is because deleted podname is still in Influxdb within the time range.. @ompraash \nThis may be related with Grafana top-down query. I guess the template variable does not apply the time range. It just do show tag values instead of show tag values ... where time > now() - 1h.. @ompraash \nPlease take a look at this Grafana PR #3472 and Grafana PR #4281. I suggest you give On Time \n Range Change a try. \n\n. This is confusing. Hope you make some feedback after recreate you env and test again.. @astef Please take a look at #1676 .. @vnandha This is duplicated with #1676 .. @kachkaev Sorry for the indirect info about this. I think we should use #1671 as the primary PR to follow this. :). @prat0318 Please take a look at #1671 .. Seems like unexpected change.. @luxas Could you please make a PR to change this. :). @luxas I have no idea why we need other updates instead of just fixing armhfrm to arm for #1698. If you need to update grafana and influxdb, can you please split the update commits in another PR?  \nWe should make this PR suitable to just fix #1698 :). @luxas No special tastes for me. Just to KISS. :)\nIf we update both, how about we update the description to doc the update. :). @piosz When this PR is merged, we should build and push heapster-grafana-amd64:v4.3.2 and heapster-influxdb-amd64:v1.3.0 to gcr.io.. @luxas @piosz We should also add new relesse note about this new version in RELEASES.md. . @luxas Please resolve the conflicts. :). @Robbilie there is a PR will add a default and empty grafana.ini if not exist already. See this diff for more info.\nBTW, you can mount a configuration file into /etc/grafana/grafana.ini at runtime.. @Robbilie The fix can only add an empty grafana.ini which may not be what you want. The desired way is to mount a customized grafana.ini. :). @robbilie Having not know that we have a grafana.ini in previous release. Will check this later.. @Robbilie You mean gcr.io/google_containers//heapster-grafana-amd64:v4.0.2. @DirectXMan12 @piosz @loburm we should cherrypick #1728 to release-1.4.\n@jpds As a workaround, give v1.4.1 a try.. /lgtm. @fate-grand-order Sorry for the typos. :) \nAccording to PR #1367 where i proposed and introduced label separator, you should also correct the typo note paragraph.. @fate-grand-order Could you take some time to enhance the PR with the comment above.. @gdecroux Why would you want to change the version to 4.0.2. #1676 has been resolved, i.e., we have pushed 4.2.0 version to gcr.io. :). This seems something related with the gcr.io or network. Can you pull other images on gcr.io?. SGTM. Sorry for the inconvience. According to tthe release doc about ES 5.0, https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_50_mapping_changes.html#_literal__timestamp_literal_and_literal__ttl_literal.\n_ttl has been removed in favor of time-based index or cron query-by-delete. So close this as we have upgraded to ES 5.. @AlmogBaku According to the current master branch code, both index and alias are all suffix with a date.. @AlmogBaku We use heapster-events as the alias for different heapster-2017.xx.xx indexes.\nalias                      index               filter routing.index routing.search \nheapster-events            heapster-2017.07.24 -      -             -              \nheapster-events            heapster-2017.07.26 -      -             -              \nheapster-events            heapster-2017.07.23 -      -             -              \nheapster-events            heapster-2017.08.06 -      -             -              \nheapster-events            heapster-2017.07.25 -      -             -              \nheapster-events            heapster-2017.08.02 -      -             -              \nheapster-events            heapster-2017.08.08 -      -             -              \nheapster-events            heapster-2017.07.28 -      -             -              \nheapster-events            heapster-2017.08.04 -      -             -              \nheapster-events            heapster-2017.08.03 -      -             -              \nheapster-events            heapster-2017.08.09 -      -             -              \nheapster-events            heapster-2017.08.07 -      -             -              \nheapster-events            heapster-2017.07.30 -      -             -              \nheapster-events            heapster-2017.08.05 -      -             -              \nheapster-events            heapster-2017.07.22 -      -             -               \nheapster-events            heapster-2017.07.31 -      -             -              \nheapster-events            heapster-2017.07.27 -      -             -              \nheapster-events            heapster-2017.08.01 -      -             -              \nheapster-events            heapster-2017.07.29 -      -             -\nWith this we can use heapster-events to query indexed data in both heapster-2017.07.29 and heapster-2017.07.30. \nWith heapster-events-2017.xx.xx as the alias, If we want to query both heapster-2017.07.29 and heapster-2017.07.30 indexed data, we need to query twice. One for heapster-events-2017.07.29 and the other for heapster-events-2017.07.30\nSo, i recommend we should use heapster-events as the alias. This is the basic usage for alias, imo. WDYT.\nI am not sure i explain it clearly. :). > \"hepaster-\"(o), \"heapster-events\"(a), \"heapster-events-\"(a)\nWhat is the meaning of (o), (a). Could you please detail it? @AlmogBaku . Got it. @AlmogBaku Do you have some usage that needs an alias which does not change with days pass? Having an alias the same as the index seems unreasonable and useless.. I have only one usage by alias for eventer, i use it to search something, i.e., alias(or index)/type/_search. \nAccording to the ES alias doc, where an index is used in es, it is ok to replace the index with an alias. So, i think we directly change to heapster-events alias name is fine and have no backward incompatibility.\nBTW, at the last of the alias doc, it is said that An alias cannot have the same name as an index..  I am not sure what is the consequence about having an alias and index with the same name, as currently we did having the same name for index and alias and it seems we have no problem. But in order to have an healthy ES usage, we should follow the directions in doc. So, just for this, we also should at least have a different alias name from index. :)\nI have not used heapster with ES, so i am not sure changing alias name from heapster-events-xxxx.xx.xx to heapster-events is ok. But imo, this should also be fine, as alias can be expanded by es automatically.. @AlmogBaku Let me explain it detailly.\nWhen a pod is scheduled and before it is running, a Pulling event will be fired and the event message will contain the image name.\nOur use case is that we want to query and analysis Pulling event messages to answer where the service is running on from start to end time. If the start and end time is beyond one day, let's assume the start and end time are 2017.08.14-12:30:00 and 2017.08.15-12:30:00, we need to query bothheapster-events-2017.08.14andheapster-events-2017.08.15`. This is inconvenient.\nBy using an heapster-events alias, we should not take care of the spread indexes and query all of them to compose the result as it will be automatically expanded to the possible indexes.\nI am not sure this is clear enough. Feel free to challenge me if something is still not clear. :). Friendly ping @AlmogBaku .. @AlmogBaku Thanks for the review. \n@piosz @DirectXMan12 PTAL.. SGTM. @DirectXMan12 I have thought about that. If we drain a knode, the knode will both in unschedulable and notready status. I have a hesitation about which one to chose to represent the knode status. \nIn fact, when we want to compute the remain resource and total available resource, notready and unschedulable is the same, we can not use node in either two status. . > Also the question is whether you will be able to to gather any metrics from not-ready kubelet?\nThe reason for mark a knode as notready may be a network problem or some other problem happens on a knode, such as docker hung will make a knode notready for now. We may not enumerate all the situations. \n\nInstead of introducing a new concept of \"usable\" and \"not usable\" nodes, how about just re-using something from Kubernetes. Maybe node conditions? \n\nHow about we introduce two labels ready and schedulable. As they can co-exists. The possible combination is like below:\n|label|value|\n|-----|-----------|\n| ready | true/false/unknown |\n|schedulable| true/false |\n@DirectXMan12 @piosz . Yep. One of the key points is that we need the node status at least about ready and schedulable status to calculate cluster resource usage and alarm.  \nAlways check a node can solve the first problem described above. As for the change to metrics model, it is not backward compatible indeed.. Friendly ping @DirectXMan12 @piosz :). Again Ping @piosz .PTAL.. @piosz @DirectXMan12 I have proposed a sub-pr about adding schedulable label to node metrics. #1782 PTAL.. Friendly ping @yujuhong @Random-Liu @dchen1107 .\nCould you please take a look at this about scraping not ready nodes.. @piosz How about we use ready label like this:\n add ready label to node metrics. The available values for ready label are true/false/unknown\n Stop scraping metrics for nodes with ready label values be false/unknown but still emit the node level metrics. With this heapster will not scrape not ready nodes but we still get node level metrics indicating that a node is not ready.. @yujuhong You're right. After a deep thinking about this, the not-ready condition is complicated and we can not simply conclude a clear reason for it. So, agreed on not scraping not-ready condition nodes.\nclosing this. @piosz @DirectXMan12 . Awesome! Thanks for adding goreport.\nBefore this pr is merged, you should resolve the CLA.\n/ok-to-test. /lgtm\n\ud83d\udc4d . Maybe it is straight to gophers just like the other two badges. \ud83d\ude0a\nBadges can not take too much info at first glance. It is just a indicator, once we need to known more exact meaning of a badge, we should consult it by following the link.. @asifdxtreme Noop. I have no rights to merge. We should wait for the review from @DirectXMan12 @piosz :). /lgtm. /ok-to-test. Seems flakes.\n/test pull-heapster-e2e. @fisherxu Thanks for you contribution! However, i have no merge right. Let's ping @DirectXMan12 @piosz .:). @ixdy Neither do i. Could you please help us contact @piosz as you are all Google guys. :). @007 \nThere are some other places should be updated at the same time.\n https://github.com/kubernetes/heapster/blob/master/deploy/kube-config/standalone/heapster-controller.yaml#L23 \n https://github.com/kubernetes/heapster/blob/master/deploy/kube-config/google/heapster.yaml#L23\n https://github.com/kubernetes/heapster/blob/master/deploy/kube-config/influxdb/heapster.yaml#L24\n https://github.com/kubernetes/heapster/blob/master/deploy/kube-config/standalone-test/heapster-controller.yaml#L20\n https://github.com/kubernetes/heapster/blob/master/deploy/kube-config/standalone-test/heapster-summary-controller.yaml#L31\n https://github.com/kubernetes/heapster/blob/master/deploy/kube-config/standalone-test/heapster-summary-controller.yaml#L20\n* https://github.com/kubernetes/heapster/blob/master/deploy/kube-config/standalone-with-apiserver/heapster-deployment.yaml#L26. @007 Please squash your commits as they are apparently doing the same job. . > is there a release script or other \"do a release\" doc that should be updated so these aren't forgotten?\nI am afraid not for now. @DirectXMan12 I also think we need to add a doc about this.. Thanks for complete the release procedure @007 . I have thought we did not have one. :(\n/lgtm. /ok-to-test. SGTM. @rohitsardesai83 \nThanks for your contribution!\nPlease add some description in the PR message it is empty for now.\nOverall looks good to me. \nPlease resolve the cla, firstly. :)\n. @roberthbailey Could you please squash commits.. Sorry for the error @. Could you please squash the commits.. @rohitsardesai83 . /ok-to-test. Friendly Ping @rohitsardesai83 :-)\nCould you please squash commits.. seems flakes.\n/test pull-heapster-e2e. Since PR lgtm except commits not squashed. Multi commits does not impact the correctness. :)\n/lgtm. @DirectXMan12 Could you please merge this PR since it lgtm.. sgtm. /ok-to-test. /ok-to-test. Thanks for correct this! @allencloud \n/lgtm. @khoing1111 First you need to check whether there are metrics available in Influxdb as what described by @miry above.. /ok-to-test\nThanks for updating @miry !. /lgtm. @xiangpengzhao IIUC, you mean that we should add a network interface label to network interface metrics?\nIt it is true, I suggest we postpone it as it will break the backward compatibility.. /ping @DirectXMan12 @piosz . > GetValue only applies for the non-summary source, which is deprecated.\n@DirectXMan12 you mean that none-summary source has been deprecated? or summary source is deprecated? I am sorry i am not getting it.\nAfter a looking at the summary source response, i have not found any info corresponding to different interface values from it. summary source response only contains one NetworkStats. I think we should first fix this in kubelet.\nAs for now, kubernetes 1.8 has been code freezed, it may not be added to 1.8. I will give it a try. But for now, we need this fix for normal kuberbnetes source. So would it ok for you to merge this PR first. :) I will open an issue for summary source to keep it tracked.\n. @DirectXMan12 After looking into Kubernetes #22542, it turns out that heapster addon uses summary api by default.. @DirectXMan12 A fix PR has been fired to Kubernetes #52144. PTAL.. /ping @DirectXMan12 @piosz PTAL. Can this PR be merged?. @DirectXMan12 @piosz I have updated the PR in kubernetes #52144. PTAL.. @bartebor The network settings will be complicated to be coherent to all different possible network setup. For now, we should make the atomic data as detail as possible and let the upper application, like heapster, to query and make the decision about how to aggregate the original data.\nFor now, this PR will only add the atomic network data. Maybe we should talk about how to aggregate or display network data in a heapster issue. :). @DirectXMan12 The PR in kubernetes to change summary api has been merged. Could you please take a look at this.. @kubernetes/heapster-maintainers PTAL.. > Shouldn't we just make network a labeled metric now, instead of aggregating like this, since we now have per-interface stats? People can aggregate together in their sink.\n@DirectXMan12 IMO, make network a labeled metric is not a backward compatible change. I'd like to postpone making this change. For this PR i suggest we only fix the network metrics values. I agree that for a long run, we need to split the network metrics according to different interfaces. This need another PR, however.\nBTW, by adding interface label to network metrics needs some change which has been added but not merged in other PRs. I'd like to add this later after all my other PRs has been merged.. Ping @DirectXMan12 @piosz . Thanks @piosz @DirectXMan12 . /ok-to-test\n/lgtm. @piosz @DirectXMan12 PTAL.. @thedebugger IIUC, you mean that grafana dashboard is not compatible with Grafana v4.3?\nCan you please post the error scene?. @thedebugger IIUC, you mean that there is an additional dashboard element?. @thedebugger Just tested this in our env and it is true. But this is a breaking change, imo. Have you test that after delete dashboard element it will still work with older grafana version?. @thedebugger How do you find the solution to fix this problem. i.e., delete dashboard element. This is a breaking change and i think grafana should document it somewhere.. @DirectXMan12 @piosz This lgtm. PTAL.\n/ok-to-test. @DirectXMan12 comments addressed. PTAL.. @DirectXMan12 Comments addressed. PTAL.. ping @DirectXMan12 . @DirectXMan12 Does this related with kube-dns?. @heckj According to the ipinfo, 198.105.244.23 maybe an ip  from your network provider. \nThe reason for why you are running correctly when you just run influx without config.toml is that the default value for bind-address is 127.0.0.1:8088 which is specified in ip:port instead of domain_name:port which needs to do DNS query to turn to ip:port format.\n@heckj Can you please comment the bind-address=localhost:8088 in config.toml and run influx -c config.toml and give us more feedback.\n@DirectXMan12 I think we should just comment the bind-address=localhost:8088 line in config.toml file and use the default settings for bind-address.. > is that a step that I missed somewhere in setting up my bare-node kubeadm cluster, or should be added?\nI am not familiar with kubeadm. You can ask this in the kubernetes-user mailing list or fire an issue in Kubernetes.. @DirectXMan12 @heckj I have just make an PR about moving localhost:8088 to 127.0.0.1:8088 for the value of bind-address. This can help us avoid DNS problems. :). @heckj I don't think we have changed the configuration for influxdb image, so maybe this is related with the influxdb itself.. @jmgao1983 \n\ncould it be the image's fault?\n\nYes. There is already fixed in #1709 . And, an new version for heapster-grafana-amd64 has been pushed to gcr.io and you should try heapster-grafana-amd64 with v4.4.1 and above.. /test pull-heapster-e2e. @DirectXMan12 Thanks for reviewing this. According to the RFC we should think it is ok to use localhost in places where 127.0.0.1 is expected. \nHowever, this needs another DNS query from translating localhost to 127.0.0.1. If a machine can not do this translation correctly just as the situation in #1804 , It is always ok use 127.0.0.1 to work make influxdb work properly.\nWe can not make sure all machines's DNS work properly. :)\nActually, In circumstances where IP is ok, convenient and will not push more operation burden, I will prefer Ip instead of a DNS name.. @pgayvallet I am afraid not for now. Every metrics sink will receive all the metrics.. @JuneZhao @arindam00 \nAccording to the kubelet log:\n\nSep 18 05:44:55 k8s-agent-ECC3CC6B-0 docker[3056]: I0918 05:44:55.506240    3104 kuberuntime_manager.go:370] No sandbox for pod \"heapster-2708163903-nd4hw_kube-system(7f67b525-9c34-11e7-9335-000d3a804764)\" can be found. Need to start a new one\nSep 18 05:44:50 k8s-agent-ECC3CC6B-0 docker[3056]: W0918 05:44:50.630850    3104 docker_sandbox.go:263] NetworkPlugin kubenet failed on the status hook for pod \"heapster-2708163903-bpwlj_kube-system\": Unexpected command output nsenter: cannot open /proc/54947/ns/net: No such file or directory\n\nSeems this is related with you Kubernetes settings.. @arno01 I am not quite familiar with the CLA and Google account. But i am afraid you may need to apply for one Google account to accomplish signing the CLA.\nAs for adding nobody user and group with UID and GID 65534, an easier way is to use alpine:3.6 which is smallest linux distribution for image.\n@DirectXMan12 @piosz WDYT.\nSo, What i am preferred is changing FROM scratch to FROM alpine:3.6.. @arno01 alpine is small enough and should not add too much overhead to heapster image. At the same time, heapster will get the full linux environments.\nCould you please adjust the PR with changing scratch to alpine:3.6?. @arno01 \n\nshould we add some sort of a test which would be always ensuring the expected UID:GID pair is present in the /etc/passwd file?\n\nSure, this is definitely good. In fact i am not sure whether it is easy to add such a test. \nBTW, i also think that alpine is stated to be Alpine Linux is a security-oriented, lightweight Linux distribution based on musl libc and busybox. and nobody seems to be a de-facto user with uid and gid 65534 on all linux distribution. So, i think it should be fine to just use alpine and let alpine to keep a promise about having a nobody user with 65534 uid and gid.\n\nabout doing RUN apk --update add ca-certificates\n\nAgreed. @arno01 maybe this should be \nRUN apk update\nRUN apk --no-cache add ca-certificates\nmore easy to accomplish updating ca goal.. /remove-lifecycle stale. @Joseph-Irving Seems to be what you said. \nActually updating the dependencies is painful. :(\nI have tried more than one day to resolve the changes in apimachinery&&api&&metrics&&others. . @piosz @DirectXMan12 According to the error info in pull-heapster-e2e:\n\nI1013 15:39:23.746] go version\nI1013 15:39:23.789] go version go1.7.4 linux/amd64\nI1013 15:39:23.789] GOARCH=amd64 CGO_ENABLED=0 go build -ldflags \"-w -X k8s.io/heapster/version.HeapsterVersion=v1.5.0-beta.0 -X k8s.io/heapster/version.GitCommit=b75729e\" -o heapster k8s.io/heapster/metrics\nW1013 15:39:27.133] # k8s.io/heapster/vendor/k8s.io/apimachinery/pkg/util/strategicpatch\nW1013 15:39:27.133] vendor/k8s.io/apimachinery/pkg/util/strategicpatch/patch.go:520: undefined: sort.SliceStable\nW1013 15:39:39.302] # k8s.io/heapster/vendor/k8s.io/apimachinery/pkg/util/proxy\nW1013 15:39:39.303] vendor/k8s.io/apimachinery/pkg/util/proxy/dial.go:78: tlsConfig.Clone undefined (type tls.Config has no field or method Clone, but does have tls.clone)\nI1013 15:39:59.985] Makefile:46: recipe for target 'build' failed\nW1013 15:40:00.086] make: ** [build] Error 2\n\nThe go version used for pull-heapster-e2e is go1.7.4 which is too old for sort.SliceStable.\nI am not sure who to ping to fix or update the golang version used in pull-heapster-e2e.. ping @loburm @fgrzadkowski Could you please help to fix the pull-heapster-e2e error described in comment above.. @fgrzadkowski Could you please help to make the  pull-heapster-e2e test PR  merged first. We need it to run pull-heapster-e2e test.\nIt is really hard to update the complicated dependencies. I am not 100 percent sure that it is right to update all the dependency but i have test the PR with our 1.6.3 cluster. And, seems everything is ok.\n@fgrzadkowski @loburm Please review the PR for the key points as it is really big to review. :). @fgrzadkowski Could you please help to make the  pull-heapster-e2e test PR  merged first. We need it to run pull-heapster-e2e test.\nIt is really hard to update the complicated dependencies. I am not 100 percent sure that it is right to update all the dependency but i have test the PR with our 1.6.3 cluster. And, seems everything is ok.\n@fgrzadkowski @loburm Please review the PR for the key points as it is really big to review. :). /reopen. @fgrzadkowski Could you please help to reopen this PR as it is accidentally closed by the @k8s-ci-robot  . It is quite confused me that i am the PR author and i have no rights to reopen it. :(. /reopen. /cc @loburm . @fgrzadkowski No. I have not deleted it. It still exists.\nCould you help me to reopen this. . i will give it try. If we can not make it i will open another PR to make it.. /reopen. As this PR can not be opened again. I opened another PR #1849 to continue the work on it. :)\n@loburm @fgrzadkowski . @loburm I have tried to do so. But api&&apimachinery&&metrics&&client-go are referencing each other. It is hard or impossible to only update one. . @loburm I have tried to do so. But api&&apimachinery&&metrics&&client-go are referencing each other. It is hard or impossible to only update one. . @loburm Thanks.\nSome significant changes are documented in the PR description. \n. /test pull-heapster-e2e. @loburm Done. Both has been updated to 1.8.0\n Makefile: https://github.com/kubernetes/heapster/pull/1849/files#diff-b67911656ef5d18c4ae36cb6741b7965\n integration/.jenkins.sh: https://github.com/kubernetes/heapster/pull/1849/files#diff-ee3c240603fa28e187c659a556bb782b. @piosz Will handle this in about 3 days. :). @piosz Sorry for the delay. But too busy these days. In case you have a schedule for releasing v1.5.0 for heapster, i may have not enough time to make this happen this week. I will make this happen later.. @piosz @loburm @DirectXMan12 Sorry for the late and finally i got it. For now the ci and e2e test are both green. PTAL.. @DirectXMan12 \n\nfor future reference, why does node-problem-detector depend on Heapster?\n\nThis is used to add running NPD in standalone mode. FYI: https://github.com/kubernetes/node-problem-detector/pull/49. @x13n Is this related with RBAC?. /retest. @piosz @DirectXMan12 @loburm @x13n Thank God. I finally make it. PTAL. All CIs are green now.. ping @DirectXMan12 @piosz @loburm @x13n @BenTheElder . Ping @kubernetes/heapster-maintainers . PTAL. @piosz @DirectXMan12 \n\nTwo comments:\nplease revert change bumping metrics api from v1alpha1 to v1beta1\nconsider removing support for aggregated apiserver in Heapster\nOtherwise LGTM\n\n\nplease revert change bumping metrics api from v1alpha1 to v1beta1. Done with using k8s.io/metrics/pkg/apis/metrics/v1alpha1. The endpoint for metrics is under apis/metrics.k8s.io instead of apis/metrics.\nconsider removing support for aggregated apiserver in Heapster. Pending as i am not familiar with this. I think we can do this in another PR. :)\n\nPing @jingxu97 for reviewing this PR. :) . /retest. The integration failed. Can anyone see it and give me some advise? Too complicated. :(. /retest. @kawych Thanks for the diagnose and suggestions. After all, there are still some problems. Could you please help to take a look at this?. Finally ping @loburm @piosz @DirectXMan12 @x13n @jingxu97 \nSorry for the quite long time delay but with the help from @kawych we finally make all the ci green. PTAL.. @x13n @kawych I am also suggest we first review and merge this PR.\n/cc @piosz @DirectXMan12 PTAL ASAP. :). @loburm @kawych @x13n Can anyone of you give piosz a DM since you are both Googlers. :). @TinySong Could you please also change --metric_resolution to --metric-resolution. As heapster command line argument will replace all _ with -.. /ok-to-test. @Kokan @miaoyq Would you guys help us to maintain kafka sink(answer questions and review PRs)? . Sorry for the reconfirm aside from #1860 . Thanks for taking care about kafka.\n/lgtm\n/cc @DirectXMan12 @loburm . @DirectXMan12 Comments addressed. PTAL.. Friendly ping @DirectXMan12 @loburm . Ping @kubernetes/heapster-maintainers . @loburm @DirectXMan12 Comments addressed. PTAL.. @piosz @DirectXMan12 Could you please take a look at this?. @piosz Conflicts are addressed.. Note:\nThe disk io metrics PR #1450 is also depends on this PR for the reason to use the new Device field instead of Major:minor as resourceID.. /lgtm. @loburm Thanks for reviewing this. Could you help me to merge this since @DirectXMan12 and @piosz are not available for now. :). @DirectXMan12 Make it an option is good for me. By giving a switch, it should be fine to let users to choose whether to disable or enable those counter metrics depends on their demand. Will change the PR.. Friendly ping @DirectXMan12 @loburm @piosz PTAL.. Ping @kubernetes/heapster-maintainers. @loburm I have checked that cpu/usage related metrics are not used in grafana dashboards. I agree that this is not a backward compatible change. We should document this in release note and change log clearly. But aside from this, i have not thought any good way to inform users about this change.. @loburm @DirectXMan12 Maybe we can add a warning deprecation release note in the next heapster release and state that after the next 1~2 heaspter release, those counter original metrics will be deleted for InfluxDB. \nI prefer to deprecate in the next 1~2 heapster release instead of kubernetes release since this is not related with Kubernetes. \nIf this is ok, i'd like to postpone this PR until then.. @loburm @DirectXMan12 I have added a comment to the original issue. I will add a knob to control the sinking of counter metrics.. /retest. @DirectXMan12 @loburm I have added disable_counter_metrics for influxdb uri param and updated the description for this PR. PTAL. By default, disable_counter_metrics is false, i.e., all counter metrics will be sinked to influxdb.. Ping @DirectXMan12 @piosz . @piosz @DirectXMan12 Could you please take a look at this.. @mindprince \n\nWhy use the cAdvisor endpoint instead of the summary API?\n\nFor now at least i use the original cAdvisor api to collect metrics. I also think that there are some people use normal cAdvisor endpoint instead of summary endpoint. :)\n\nwhen we expose these metrics, we would probably expose them as\naccelerator/...\n\nDone. PTAL.. @mindprince @piosz @DirectXMan12 comments addressed. PTAL.. @mindprince \n\nI also have a question if we even want to do this or just want to do through the summary API? Can both co-exist?\n\nnormal api and summary api should both be supported, IMO. And, they can co-exist.. Ping @mindprince . @DirectXMan12 @mindprince \nUntil kubernetes disable cAdvisor integration, we should support legacy stats API usage or otherwise this is also a backward breaking change. Just like what we have discussed in #1881 , this deprecation of legacy stats API is depends on the kubernetes. And, for this time i agree that we should make an deprecation statement along side kubernetes release on which cAdvisor integration is disabled.. @DirectXMan12 @loburm @mindprince Please take a look at this PR. Thanks.. @mindprince All comments are addressed. PTAL.. @mindprince Comments addressed.. Ping @piosz @DirectXMan12 . Ping @piosz @DirectXMan12 . /retest. @DirectXMan12 What is the deprecation schedule for legacy stats api?\nPing @piosz @kubernetes/heapster-maintainers I want to hear more voice on this.. @piosz Could you please take a look at this?. Ping @kubernetes/heapster-maintainers . @loburm @DirectXMan12 Conflicts addressed. PTAL.\nSince @loburm and you have agreed on the PR, can we merge this?. Ping @piosz . Seems we need to delete the google.md file instead of patch it.. /lgtm. @DirectXMan12 Should we add a PR and issue template to heapster?. This seems good to me. Ping @DirectXMan12 @piosz for option about changing to google/heapster.yaml. /retest. @loburm Comments addressed. PTAL.. > we don't want people to leave unfilled fields in their request, since it's just visual noise at that point).\nI agree that we should remove visual noise. But, IMO, this will increase the burden for users/contributors to fire an issue or PR. This may make users/contributors discouraged. Just like what we do for Kubernetes, i think this should be fine as we have already split the key points for issue/PR into several items, it should be clear and easy to find the useful info. :)\n\nremove extraneous boilerplate information from the template (for instance, we don't want the HTML comments showing up in the commit log\n\nI think git will record the git commit message in git log instead of PR description. \n. @piosz @DirectXMan12 @loburm Any suggestions on this? Maybe we can just merge this and see the effects. :)\nping @kubernetes/heapster-maintainers . @DirectXMan12 Done. The please remove empty sections and comments before submitting has been added to the comments sections, i.e., issue and PR :) PTAL. . /retest. @DirectXMan12 @loburm maybe we should give it a try and merge this for now. :). Close this via #2008 \n/close. I can help to do some job in this but must be after Spring Festival. Too busy for now.. /ok-to-test. @qhyou11 Thanks for the fix. I have missed this. Overall looks good to me except the comment. I thing the problem is exposed only when a container can access more than one disk device, right?. Close this via #2008 \n/close. /lgtm. @jingxu97 I have already fired a PR to bump kubernetes to 1.9 in https://github.com/kubernetes/heapster/pull/1849. But there is a pending review comment to be addressed. I will update that PR in about these two days.. /ok-to-test. /lgtm\n/approve\nThanks for correct this @stash1001. /ok-to-test. /lgtm. /ok-to-test. /lgtm\n/approve. @kawych @x13n Should the version used in heapster-summary-controller.yaml also be updated? There are two heapster-amd64 images in it.. /ok-to-test. /reset\n/lgtm\n/approve\n. /retest. seems flakes.\n/retest. /cc @DirectXMan12 @piosz @loburm . /retest. /ok-to-test. Will take a look at this later today.. Sorry, later today. @Nodraak . /lgtm\n@007 @Nodraak Sorry for the late.. /approve. @hzxuzhonghu \nThere is a pending to merge PR which bumping kubernetes dependency to 1.9.3 in #1849 . It has done the same job in that PR. :)\nI suggest we close this in favor of #1849 . . @jingxu97 The heapster-unit error has been addressed in master,  rebase should address the pull-heapster-unit error in this PR.. /lgtm. @jcharlytown There are also a PR to fix this https://github.com/kubernetes/heapster/pull/1947. I have left some comments there but it seems there are no any progress. \nI suggest you first check that PR and make some adjustment. If it is ok to you, i will in favor of this PR.. @jcharlytown You need first sign the CLA.\nYou need to run make fmt and make sanitize first.. @jcharlytown Thanks for the contribution! As #2009 has been merged, i suggest we prefer #2008. I have rebased it. \nI have done nothing else besides copying and applying diff from this PR.   Thanks @qhyou11. /retest. @thockin This PR needs to be rebased after #2009 has been merged.. /retest. /test pull-heapster-e2e. /approve. /lgtm. /cc @kawych \nPing @jcharlytown @qhyou11. /cc @piosz @DirectXMan12 @loburm . /cc @piosz @DirectXMan12 @loburm . /retest. /ok-to-test. /lgtm\n/approve. > Is metrics-server going to be promoted from incubator?\nAgreed that we need also promote metrics-server to kubernetes organization alongside the deprecation of Heapster if we do need to deprecate Heapster.\n\nI suggest to deprecate only Heapster API and work on replacing it with metrics API served by Metrics Server. My understanding of current monitoring architecture for k8s: https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/monitoring_architecture.md is that Heapster is going to become just another third-party monitoring agent.\n\nIt also seems to me that keep the scrape and sink ability for Heapster should be ok. IMHO, moving to metrics api, metrics server is good for HPA to better support custom metrics which is the currently the big pain point for instrumentation. So, i think we should move the api functionality from Heapster to metrics api.\nIMHO, Heapster deprecation is too sudden. There are no simple and easy setup guides for metrics api, metrics server and third party tsdb servers pipeline available to end users to learn and try for now which is a big gap between Heapster. Actually i, even as a developer, has no clear thinking about how to set up a monitor system with metrics api, metrics server and third party tsdb to replace the existing one backed by Heapster with no big changes. If we want end users move to the new monitor pipeline, we should add some docs or guides to help them to migrate from Heapster backed monitoring system to the new one.\nAlso there are some existing add-ons or functionalities depends on the data from Heapster. What is the procedure to help those components migrating to the new pipeline smoothly.\nBTW, what is the replacement for eventer. People may still be using it.\n\none of the main reasons for the new monitoring architecture was to remove Heapster as a bottleneck to development, and to ensure more maintainable code. Keeping Heapster around as a maintained Kubernetes project hinders that effort.\n\nAgreed that maintaining Heapster with multi sinks is painful. But maybe we should think a way for Heapster to support a plugin mechanism just like out-of-tree cloud provider support or different device support with device plugin instead of kicking it off and setup a new one. :). @DirectXMan12 Thanks for the detailed and excellent explanation.\nSorry for not catching up with the latest usage about HPA/kubectl top and Heapster. :)\nMaybe we could do this better in the following aspects:\n Add the deprecation summary for Heapster also to the Kubernetes release v1.11 release note and update the deprecation info for Kubernetes release v1.12 and v1.13. Most people will try to read the summary release note for Kubernetes to learn the trends for Kubernetes. So, i think it is also a good place to make the heapster deprecation widely known.\n Add a detailed setup docs for metrics api, metrics server and third party monitoring system to learn.\n* Add the replacement binary for eventer in case some people still using it.. /ok-to-test. /lgtm\n/approve. /ok-to-test. /lgtm\n/approve. /ok-to-test. /lgtm\n/approve. @zioproto Could you please help to update all the heapster image version under deploy to v1.5.3.. Thanks @zioproto for updating this.\n/lgtm\n/approve. /ok-to-test. /lgtm\n/approve. /ok-to-test. /lgtm\n/approve. /lgtm\n/approve. Seems phantomjs is missing.\n@LynnChen1989 Can you confirm whether phantomjs exists?. /ok-to-test. /lgtm\n/approve. This is the desired result, imo. As all the metrics data are stored in influxdb. If the data was not deleted from influxdb in a retention policy interval, it should be visible to end users.. /close . /lgtm. /ok-to-test. /test pull-heapster-e2e. /lgtm\n/approve. /check-cla. /ok-to-test. /lgtm\n/approve. /ok-to-test. /ok-to-test. /ok-to-test. /lgtm\n/approve. /ok-to-test. @ringtail run make sanitize to check the error.. /lgtm\n/approve. /ok-to-test. /test pull-heapster-unit. /test pull-heapster-unit. @mirake Thanks for updating this. \nBut could you first run make fmt to update format the go source code files first. make sanitize is reporting error.. @mirake Yes. . /ok-to-test. @ringtail Sorry for the late. You need to also run make fmt because make sanitize is reporting error.. /ok-to-test. @xichengliudui Run make sanitize to find more.. /ok-to-test. @xiezongzhe You need to run make sanitize locally to find the way to correct the ci error.. /ok-to-test. @SataQiu Run make sanitize. @SataQiu According to the error log.\n\nI1121 05:08:26.469] Run gofmt on the following files:\nI1121 05:08:26.470]  ./metrics/sources/summary/summary_test.go ./metrics/sinks/opentsdb/driver_test.go ./metrics/sinks/wavefront/driver_test.go\nI1121 05:08:26.470] Makefile:50: recipe for target 'sanitize' failed\nW1121 05:08:26.476] make: *** [sanitize] Error 1\n\nYou need to run make sanitize to find that you need to run gofmt ./metrics/sources/summary/summary_test.go ./metrics/sinks/opentsdb/driver_test.go ./metrics/sinks/wavefront/driver_test.go. /retest. @SataQiu \n\nI1121 10:05:31.474] # k8s.io/heapster/common/kafka\nI1121 10:05:31.474] common/kafka/glogadapter.go:29: missing ... in args forwarded to print-like function\nI1121 10:05:31.474] common/kafka/glogadapter.go:33: missing ... in args forwarded to printf-like function\nI1121 10:05:31.474] common/kafka/glogadapter.go:37: missing ... in args forwarded to print-like function\nI1121 10:05:31.474] # k8s.io/heapster/events/sinks/riemann\nI1121 10:05:31.475] events/sinks/riemann/driver.go:106: Warningf call has arguments but no formatting directives\nI1121 10:05:31.475] events/sinks/riemann/driver.go:140: Warningf call has arguments but no formatting directives\nI1121 10:05:31.475] # k8s.io/heapster/metrics/sinks/riemann\nI1121 10:05:31.475] metrics/sinks/riemann/driver.go:78: Warningf call has arguments but no formatting directives\nI1121 10:05:31.475] metrics/sinks/riemann/driver.go:132: Warningf call has arguments but no formatting directives\nI1121 10:05:31.475] # k8s.io/heapster/metrics/sources/kubelet\nI1121 10:05:31.475] metrics/sources/kubelet/kubelet.go:226: Verbose.Infof call needs 1 arg but has 2 args\n\nYou need to adjust the go source file according to the error message.\n. \n\n\n. /test pull-heapster-e2e. /retest. /retest. @SataQiu Sorry, you first need to address the conflicts. :(. Done. PTAL.. Done. The above check for basicAuthUsername duplicates.. SGTM.\nBTW, i think we need to document this behavior in the elasticsearch sink document about supporting not settings nodes when we only use one es address.. Will do.. @DirectXMan12 How about healthz-ip and healthz-port. :). I am afraid yes since manager is initialized in main package. We can not get the same manager instance.. LGTM. One nit:\nyou should also change FakeESSink := NewFakeSink() in line 69 to FakeESSink = NewFakeSink(). \nWe need all tests in driver_test.go to use the global FakeESSink variable and re-initialize it before each tests using, as we use the global FakeESSink in SaveDataIntoES_Stub.. Backward capability may be broken. We should just add gf_security_admin_user and gf_security_admin_user to the map instead of replacing grafana_user and grafana_passwd.. The logic maybe like this:\nIn case we make the default value for gf_security_admin_user and gf_security_admin_user be empty string. \n\nif gf_security_admin_user and gf_security_admin_user is not empty, i.e., user specify a gf_security_admin_user and gf_security_admin_user explicitly, we should use gf_security_admin_user and gf_security_admin_user to construct grafanaURL.\notherwise, we fallback to the original way, i.e., we should use grafana_user and grafana_passwd to construct grafanaURL. . In grafana/run.sh, we have transported GRAFANA_PORT  to GF_SERVER_HTTP_PORT. . The same for gf_server_http_port and grafana_port.. This should be removed. Otherwise we will get an empty GF_SERVER_HTTP_PORT in run.sh cause GRAFANA_PORT is not set.. Why would you change this vendor file without updating godeps.. > Am I allowed to sign CLA ?\n\nYes, everyone can and should sign the CLA before contributing to Kubernetes. :)\n\nDo you know who can verify the meaning of rss here ?\n\nThe way you explain about rss is right. rss contains memory excluding cached ones. We can understand usage ~= rss + cached.\n. How about we use this http://INFLUXDB_HOST:INFLUXDB_PORT. monitoring-influxdb is ambiguous as this is only the dns name of Influxdb server when deploy Influxdb in Kubernetes. Once we deploy Influxdb server standalone, this may not be right. \nWith http://INFLUXDB_HOST:INFLUXDB_PORT, users should known that they should replace corresponding value.\n@DirectXMan12 @neith00 WDYT?. @neith00 \nCould you please change the PR according to the comment? :). > So we need to fix this vendor file, right ?\nI guess you just need to checkout the file changes. Seems that you change file manually.\n\nHow / Where do I update the godeps ? sorry I'm new to this.\n\nYou should not and need not do this. Just leave the godeps unchanged. :). Not sure why CI not check out the conflicts. But, this fix has been merged in #1511 .\nThis change should be removed. :). SGTM. @luxas Should (ARCH) be replaced with *?. Good catch! And glad to see that i have learned another info about make.  \ud83d\ude04 \nThe link below is just for any user interesting in this to consult: https://www.gnu.org/software/make/manual/html_node/Automatic-Variables.html. I am not familiar with armhf, so i have no idea about this change. Others seem good to me.. sgtm. How about s/greater than or equal to/no less than? @allencloud . How about s/greater than or equal to/no greater than? @allencloud . AFAIK, some implementations, i.e., the etsy statsd, of statsd protocol supports tcp but not all of them because it is not part of the statsd protocol.\nBut according to the introduction page of statsd:\n\nSo, why do we use UDP to send data to StatsD? Well, it\u2019s fast \u2014 you don\u2019t want to slow your application down in order to track its performance \u2014 but also sending a UDP packet is fire-and-forget. Either StatsD gets the data, or it doesn\u2019t. The application doesn\u2019t care if StatsD is up, down, or on fire; it simply trusts that things will work. If they don\u2019t, our stats go a bit wonky, but the site stays up.\n\nudp is preferred and dup server is the default server for statsd.. > running in K8s.\nThis is also required in Docker not only in Kubernetes, IMO.. nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin is more preferable. nobody should not be able to login.. Will do.. Will update sync.WaitGroup use a field wg instead of embedding.. Have add some comments in wg and conChan fields. :) . Will do.. Will do.. Will do.. > ResourceNvidiaGPU is a deprecated resource and is going away. The new resources are exposed through device plugins.\nDoes the resources has a predefined or patterned names? @mindprince . As for now, It seems that alpha.kubernetes.io/nvidia-gpu is still available. So, i think it is fine to add this for now. \nWDYT @piosz @mindprince . Will do.. Will do.. Will do.. I have added a existence test for ResourceNvidiaGPU resource name before emitting a capacity metric, so even it is not supported and removed in the future, everything should work fine.. Will do.. Will do. Just be curious about why Goland does not gofmt the go source files. Anyway addressed.. Will do. Nice refactor advice.. Will do.. I think it is a good thing to make all accelerator related labels accelerator prefixed. This may be useful for classification. I make only two changes for accelerator lables:\n make -> accelerator_make\n model -> accelerator_model. Will do.. Will do.. Will do.. Seems Reasonable. Will do.. Will do.. It should be \"k8s.io/apimachinery/pkg/apis/meta/v1\". SchemeGroupVersion.\nDone. Done. > the legacy client still expects the alpha API version on the non-aggregated endpoint.\nThe client is HPA?\n/cc @piosz @loburm . sgtm. Will do.. Fix negative value on \"disk/io_read_bytes_rate\" and \"disk/io_write_bytes_rate\" -> Fix negative value on \"disk/io_read_bytes_rate\" and \"disk/io_write_bytes_rate\" when multiple disk devices are available. s/\"resource_id\"/core.LabelResourceID.\nAvoid hard code magic string. :). Will do.. Added a new heapster-rbac.yaml file to standalone-test dir.. Will do.. Will do.. @loburm Done. PTAL.. Done.. Done. PTAL.. New metrics should be added to https://github.com/kubernetes/heapster/blob/master/docs/storage-schema.md#metrics. s/sstorage/storage/. Shouldn't the ValueType and Units be the same as ephemeral_storage/limit.. Ignore this since other capacity metrics are also defined like this. :). s/check the specific database is exist or not/check whether the specific database exist or not. Check whether resp is nil like https://github.com/kubernetes/heapster/pull/2086/files#diff-a83c1db1f245f8fb4c41e99889717525L265. Replace this line with:\n//check whether the specific database exist or not. Why we cut the with name \"default\"?. This is actually not a valid Golang comment syntax. \nGolang\n// heapster should create retention policy on the creation of database.\n// although you can change the retention duration in params,\n// but heapster will not alter an existing database's retention policies.. The value for dbExists should be false since there is a check at the beginning of the function. https://github.com/kubernetes/heapster/pull/2086/files#diff-a83c1db1f245f8fb4c41e99889717525R257. Add a blank between // and check. Add an info log about the existence about the database.. Add an error log about the query for all the database error. This is mainly used to distinguish between the error below for creating the database.. Add an error log for recording the creating database error. This is mainly used to distinguish between with the error above for querying all the databases. This is useful for debug when something is wrong.. Add an error log for recording the creating database error. This is mainly used to distinguish between with the error above for querying all the databases. Detailed error log is useful for debug when something is wrong.. Also add an error log here to clearly recording the specific error for create retention policy.. Then you need to fix the ut. Make sure you implement the logic correct first and adjust your ut.. Just test this locally. Yes, you're right. Thanks for pointing this out.. exist -> exists. Replace //If the database is exist,we should exit right now. with // If the database exists,we should exit right now.. is exist -> exists. influxdb, because of -> influxdb:.\nYou can check the log formatter in heapster for reference.. You have not change this. :). database,because of -> database:. //If the database exists,we should exit right now. -> // If the database exists, we should exit right now.. is exist -> exists. client:%s -> client: %s. influxdb:%s -> influxdb: %s. ",
    "carmark": "I signed it!\n. ",
    "chrislovecnm": "That is great!  Maybe the docs should be a bit more clear??\n. Another error:\nLet me know what else you need\nkubectl logs heapster-j9w02 --namespace=kube-system\nI1112 18:06:02.054151       1 heapster.go:61] /heapster --source=kubernetes:https://kubernetes.default --sink=gcm --sink=gcl --stats_resolution=30s\nI1112 18:06:02.054215       1 heapster.go:62] Heapster version 0.18.0\nI1112 18:06:02.054659       1 kube_factory.go:172] Using Kubernetes client with master \"https://kubernetes.default\" and version \"v1\"\nI1112 18:06:02.054669       1 kube_factory.go:173] Using kubelet port 10255\nI1112 18:06:02.069390       1 driver.go:113] created GCM sink\nI1112 18:06:02.070631       1 driver.go:242] Project ID for GCL sink is: \"decent-slice-104819\"\nI1112 18:06:02.070647       1 driver.go:262] creating GCL sink\nE1112 18:06:02.070654       1 manager.go:257] encountered following errors while setting up sinks - Current instance does not have the expected scope (\"https://www.googleapis.com/auth/monitoring\"). Actual scopes: https://www.googleapis.com/auth/compute\nhttps://www.googleapis.com/auth/devstorage.read_only\nhttps://www.googleapis.com/auth/logging.write\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal 0xb code=0x1 addr=0x0 pc=0x703ff7]\n. Nooooooooooooooo .... Have time setup with Google to discuss.  Arg\n. what was the cause of this??\n. Closing - filed twice by accident\n. @piosz who can assist on this? This is for a demo for the k8s 1.3 release ... Would love to have this working \ud83d\ude04 \n. I can check on quota, but the errors concern me:\nE0610 07:09:07.851088       1 gcm.go:179] Error while sending request to GCM googleapi: Error 400: Duplicated timeseries: Timeseries 39 and timeseries 37, invalidParameter\nE0610 07:09:07.969058       1 gcm.go:179] Error while sending request to GCM googleapi: Error 400: Duplicated timeseries: Timeseries 15 and timeseries 12, invalidParameter\nE\nEven without enough quota I should be getting something in monitoring :)\n. @piosz much thanks!!!!\n. We need to reopen this ... It is not fixed, the 1.3 beta has a deployment package, and it keeps deploying.  The deployment is the last beta.\nconsole\n$ kubectl --namespace=kube-system get deployments\nNAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nheapster-v1.1.0.beta2   1         1         1            1           4m\nDid you guys not get that tag into the beta build?\nHow do I remove the beta, and install this tag by hand?\n. How do I remove the deployment that is installed with kube-up and install my own?\n. @jesucarr heapster is deployed in gke if monitoring is enabled.\n. Can you sign google and linuxfoundation CLA?. @k8s-bot ok to test. ",
    "tyler274": "would appreciate this as well\n. ",
    "binarybana": "These version numbers did indeed fix things for me (although 0.19.1 heapster also fixed the race between heapster and influxdb). Thanks!\n. This is related to the kubelet's hairpin mode or promiscuous mode: https://github.com/kubernetes/kubernetes/issues/20475\nWe also have switched to localhost as a foolproof workaround.\n. Though there is a bug with the --hairpin-mode flag not taking effect in\ncertain cases that is fixed for inclusion into 1.2.1. (don't have time to\nlook up the issue number atm)\nOn Wed, Mar 30, 2016 at 10:57 AM Zihao Yu notifications@github.com wrote:\n\n@antoineco https://github.com/antoineco No I did not set that flag.\nkubelet runs with default values for --configure-cbr0 and --hairpin-mode\noptions.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/1079#issuecomment-203552705\n. \n",
    "chrisleck": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb:8086\n. Cool!  When is the next release of heapster happening?\n. ",
    "perhapszzy": "And there's a similar problem: \nHeapster will not work correctly if the influxdb pod restart.\n. @mwielgus  Thanks, that makes much more sense. \n@vishh  It should be the problem that device  mapper is not counted now. The code writes the disk in the container but the pod is not mounted on any hostpath. Is there any plan to support the device mapper storage in near future?\n. Thanks for the pointer.it seems that heapster v0.19.0 worked correctly on getting those numbers, I'm curious is there anything dramatically changed for v1.0.0?\n. The disk usage for containers\n. Yes, I'm aware of the limits and quotas that could be set to every pod/namespace; actually my biggest concern is that heapster should respect the quotas set on namespace. For example, if a namespace is set to have a limit of 1G memory, it's totally valid under the current model that 2 pods have (512M request and 1G limit). However, in this case, heaspter will show the memory limit for the namespace as 2G (the sum of the two pods) instead of 1G. \n. Yeah, I understand that this is totally valid in k8s. What I concerned is the limit number in heapster (or on grafana UI). User had explicitly set the limit of memory for a namespace to be 1G, but the number shows up on grafana would be 2G. This might cause little confusion for users\n. @mwielgus  If you also think that the limit for namespace should respect the quota set, we can make the adjustment for heapster.\n. I'm running heapster v1.0.0 on k8s v1.1. The k8s is set up on ubuntu 14.04 machines on a cloud. \nI can find result with this query:\nselect hostname,type,value from \"cpu/usage\"  where type='node' limit 40\nbut not this one:\nselect hostname,type,value from \"cpu/usage_rate\"  where type='node' limit 40\n. Log is enclosed:\nlog.txt\nThe usage is present but not usage_rate\n. I'm running k8s on a local IaaS provider, could the problem be caused by the version of k8s or the way I'm running the k8s on the infrastructure?\n. Right, I did find something like:\nI0323 17:54:05.088835       1 rate_calculator.go:51] Skipping rates for node:i-075twq8l - different crate time new:2016-03-22 20:06:16.842345 +0800 +0800  old:2016-03-22 20:06:16.842345 +0800 +0800\n. All logs are enclosed:\nlog1.txt\n. Sure. Thanks for fixing the problem so quick :)\n. ",
    "SidneyAn": "@afein  I retry it within a Kubernetes cluster. I run heapster standalone in a minion node and I change the cadvisorPort to 4194, which is the port provided by kubelet. But the namespaces endpoint still returns file not found.\nMy test containers are running under the default namespace. Is there any problem with it?\n. @huangyuqi  get it. Thanks a lot!\n. ",
    "antoineco": "I'm working on it as part of #824, however Network metrics are not yet available for containers (see #368)\n. On a stack running heapster:v0.19.1, influxdb:v0.6 (v0.9.6) and grafana:v2.5.0, CPU metrics are even simply absent:\n\n@vicki-c: I'm using the dashboards @master (here) and my assumption is that a \"Group by time interval\" of 10s might be too low for most use cases. If I replace set \"Group by time interval\" to > 30s I seem to get more meaningful results:\n\nNot sure if that makes sense for every kind of app though.\n. This is not yet sufficient, the patterns look right but the CPU usage calculation doesn't.\n\nComparison of CPU metrics from New Relic, Kubedash and Grafana for one single node over 1 hour:\n\n\n\n\nThis results in a displayed value nearing or above 100% (8000m, in my case), which is most likely wrong:\n\n\n\nFiddling with the \"Group by interval\" and changing it from >30s to precisely 16s in this case gives me the following results:\n\nThe higher the interval, the higher the value, which sounds wrong from the user's perspective. Here with 120s:\n\n\nI'll investigate further how we could determine something accurate regardless of the chosen interval. @vishh should I create a new issue or do you prefer to re-open that one?\n. I found the issue and pushed the changes as part of #824\n. @rvrignaud I have a similar issue with a group of pods, can you check the :point_up: referenced issue and see if we have a pattern in common? I suspect it has to do with a lifecycle: block but I could be completely wrong.\n. Extra info: the query used on InfluxDB 0.8 (Kubernetes 1.1)\nSELECT distinct(container_name) from \"uptime_ms_cumulative\"\n  WHERE pod_name =~ /api-12345/\n  AND \"pod_namespace\" =~ /my_namespace/\n  AND time > now() - 5m\nResult: -> {rails,logrotate,logforwarder}\n. Found it.\nHeapster is struggling to write data for this one group of pods. My heapster logs are full of:\ndriver.go:207] failed to write stats to influxDB - {\"error\":\"partial write:\\nunable to parse 'uptime_ms_cumulative,container_base_image=example.com/image:tag,container_name=rails,...\\\"containerID\\\":\\\"docker://08b6bbb923aef450d50fee92038132601f34425a1b475311b3a4d47e40a82252\\\"}}\\\\,\\\"ready\\\":true\\\\,\\\"restartCount\\\":2\\\\,\\\"image\\\":\\\"example.com/image:tag\\\"\\\\,\\\"imageID\\\":\\\"docker://47299f016d3729dcc5d8033c3db7d9ddf130f22cc9e7a3008cdcd00320ac094b\\\"\\\\,\\\"containerID\\\":\\\"docker://3edcf5685d7961336d4f180dfdd3d976c60f987a4247c00fbef74f6176a26016\\\"}]}}': missing fields\"}\nCurrent pod definition:\nyaml\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    kubernetes.io/created-by: |\n      {\"kind\":\"SerializedReference\",\"apiVersion\":\"v1\",\"reference\":{\"kind\":\"ReplicationController\",\"namespace\":\"bodyweight-api\",\"name\":\"api\",\"uid\":\"7dec1239-a3ff-11e5-9f88-0a59d1e77755\",\"apiVersion\":\"v1\",\"resourceVersion\":\"40275725\"}}\n  creationTimestamp: 2015-12-18T22:40:00Z\n  generateName: api-\n  labels:\n    app: fl-backend-rails\n    component: api\n    deployment: \"33\"\n  name: api-3p9gu\n  namespace: bodyweight-api\n  resourceVersion: \"40275903\"\n  selfLink: /api/v1/namespaces/bodyweight-api/pods/api-3p9gu\n  uid: 45b03770-a5d8-11e5-9f88-0a59d1e77755\nspec:\n  containers:\n  - env:\n    - name: ROLE\n      value: api\n    - name: RAILS_ENV\n      value: production\n    - name: POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          apiVersion: v1\n          fieldPath: metadata.namespace\n    image: example.com/image:tag\n    imagePullPolicy: IfNotPresent\n    lifecycle:\n      preStop:\n        exec:\n          command:\n          - /usr/sbin/nginx\n          - -s\n          - quit\n    name: rails\n    ports:\n    - containerPort: 9080\n      name: http\n      protocol: TCP\n    resources:\n      limits:\n        cpu: 400m\n        memory: 1800Mi\n      requests:\n        cpu: 400m\n        memory: 1800Mi\n    terminationMessagePath: /dev/termination-log\n    volumeMounts:\n    - mountPath: /run/secrets/example.com/rails\n      name: rails-secrets\n      readOnly: true\n    - mountPath: /app/log\n      name: rails-logs\n    - mountPath: /var/log/nginx\n      name: nginx-logs\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: default-token-68jsr\n      readOnly: true\n  - image: apopelo/logstash-forwarder\n    imagePullPolicy: IfNotPresent\n    name: logstash-forwarder\n    resources:\n      limits:\n        cpu: 5m\n        memory: 15Mi\n      requests:\n        cpu: 5m\n        memory: 15Mi\n    terminationMessagePath: /dev/termination-log\n    volumeMounts:\n    - mountPath: /var/log/containers/rails\n      name: rails-logs\n      readOnly: true\n    - mountPath: /var/log/containers/nginx\n      name: nginx-logs\n      readOnly: true\n    - mountPath: /etc/logstash-forwarder\n      name: logstash-conf\n      readOnly: true\n    - mountPath: /etc/ssl/logstash-forwarder\n      name: logstash-ssl\n      readOnly: true\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: default-token-68jsr\n      readOnly: true\n  - image: example.com/logrotate\n    imagePullPolicy: IfNotPresent\n    name: logrotate\n    resources:\n      limits:\n        cpu: 5m\n        memory: 60Mi\n      requests:\n        cpu: 5m\n        memory: 60Mi\n    terminationMessagePath: /dev/termination-log\n    volumeMounts:\n    - mountPath: /var/log/containers/rails\n      name: rails-logs\n    - mountPath: /var/log/containers/nginx\n      name: nginx-logs\n    - mountPath: /etc/logrotate.d\n      name: logrotate-d\n      readOnly: true\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: default-token-68jsr\n      readOnly: true\n  dnsPolicy: ClusterFirst\n  imagePullSecrets:\n  - name: docker-registry\n  nodeName: ip-10-0-0-1.eu-west-1.compute.internal\n  restartPolicy: Always\n  serviceAccount: default\n  serviceAccountName: default\n  terminationGracePeriodSeconds: 30\n  volumes:\n  - name: rails-secrets\n    secret:\n      secretName: rails\n  - emptyDir: {}\n    name: rails-logs\n  - emptyDir: {}\n    name: nginx-logs\n  - name: logstash-conf\n    secret:\n      secretName: logstash-conf\n  - name: logstash-ssl\n    secret:\n      secretName: logstash-ssl\n  - name: logrotate-d\n    secret:\n      secretName: logrotate-d\n  - name: default-token-68jsr\n    secret:\n      secretName: default-token-68jsr\nstatus:\n  conditions:\n  - lastProbeTime: null\n    lastTransitionTime: null\n    status: \"True\"\n    type: Ready\n  containerStatuses:\n  - containerID: docker://5e35e8d3d447e21a2f48473cc92008704ae6cac2412b1e74c1b93a736a028766\n    image: example.com/logrotate\n    imageID: docker://6c9c7a7a9c779eafa7123550b44967a18f1d43b5125acbcc317903b82b5800cf\n    lastState: {}\n    name: logrotate\n    ready: true\n    restartCount: 0\n    state:\n      running:\n        startedAt: 2015-12-18T22:40:04Z\n  - containerID: docker://0181d898943aef21d9da96f413586c1f63bb401f1629a027e08d7f886fba6f5d\n    image: apopelo/logstash-forwarder\n    imageID: docker://32be67e30853d07971c5df6e7cc55607c946aa3c4f1d4b408a5aca18ff760fd5\n    lastState: {}\n    name: logstash-forwarder\n    ready: true\n    restartCount: 0\n    state:\n      running:\n        startedAt: 2015-12-18T22:40:04Z\n  - containerID: docker://159ebe374104eb05ef3ee8ca469788240a6508e3938f2b6be9c78600f409610d\n    image: example.com/image:tag\n    imageID: docker://f365817002eb3ccb8f91ead008dfee26e5b073dfa301b8b10589bd7632ad86d8\n    lastState: {}\n    name: rails\n    ready: true\n    restartCount: 0\n    state:\n      running:\n        startedAt: 2015-12-18T22:40:04Z\n  hostIP: 10.0.0.1\n  phase: Running\n  podIP: 172.17.9.8\n  startTime: 2015-12-18T22:40:00Z\n. Looks very related to #775\n. I suspect the lifecycle hook to be the culprit, the following block doesn't exist in a similar pod, and the metrics are exported properly.\nyaml\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"]\nedit: @vishh my assumption was right, I removed that block from my RC and the metrics are now forwarded.\n. @piosz closing this as well, see #775. Sample, before/after:\n\n\n. Samples, before/after:\n\n\n. My pleasure, I use this \"HIG stack\" extensively so I'm glad to contribute :)\nI wouldn't create a new release right now since I have two other things on my list:\n- Grafana 2.6.0 (2015-12-14) with the new InfluxDB Editor\n- Network metrics panel (cluster + containers dashboards)\nCould you give me 1 more day to finalize this?\n. Everything is ready on my side, I pushed an image for testing: toni0/heapster_grafana:2.6.0\n. @zihaoyu did you set the --hairpin-mode=hairpin-veth kubelet flag? After that just kill your pod and it should work.\nYou can check on your hosts whether the hairpin mode is enabled or not on each virtual interface of your docker bridge:\nfor intf in /sys/devices/virtual/net/docker0/brif/*; do\n  cat $intf/hairpin_mode\ndone\n. --hairpin-mode doesn't fallback properly to hairpin-veth in case --configure-cbr0 is false, however @zihaoyu said --configure-cbr0 was set to true in his cluster, so this might be yet a different case.\ncf. kubernetes/kubernetes#23322\n. ",
    "ncdc": "@jszczepkowski @piosz @mwielgus \n. I'm guessing this can wait for the refactor, but let's get at least 1 of @smarterclayton @liggitt @deads2k @mwringe @sdodson to weigh in.\n. @liggitt if not, using the proposed flag to indicate using the IP address would suffice, wouldn't it?\n. cc @kubernetes/rh-cluster-infra @jeremyeder @timothysc @smarterclayton @mwringe \n. cc @kubernetes/rh-scalability \n. What's the status on this? @DirectXMan12 is there more code to do from your end? @mwielgus does your team have any other comments? I see that the release-1.1 branch exists, so how close do you think we are to merging this?\n. @pmorie \n. Can't hurt. cc @bgrant0607 @kubernetes/sig-auth \n. remove the \"to\" after interface\n. Link to proposal?\n. I can't say I particularly like \"old-timer\". /api/v1/historical or something along those lines sounds better to me.\n. Is perc currently in use? If not, maybe pct or not abbreviating at all would be better?\n. Suggest ordering both in ascending or descending instead of mixing them\n. identically\n. intended\n. ",
    "ixdy": "@k8s-bot test this please\n. @k8s-bot test this please (reconfigured Jenkins job)\n. ok to test\n. trying again.\n@k8s-bot test this please\n. @piosz it looks more like a broken test than broken infra to me, but I'm not familiar with the heapster integration test.. @piosz Jenkins got pretty DOS'd with the code freeze unthawing. We're restarting it now.. @k8s-bot test this. don't have write access here, so I can't reopen this PR. created #1456 instead.. FYI I don't have write access to this repo, so I'll need someone else to merge.. there shouldn't be a kubectl in the 1.6 kubernetes.tar.gz tarball. it should only be in the kubernetes-client-$platform.tar.gz tarballs.. (this has been the case since 1.5.). kube-up.sh should be downloading the binaries for you (https://github.com/kubernetes/kubernetes/pull/38730).\nIt looks like other things might be amiss:\nW0622 08:58:19.654] missing required gcloud component \"alpha\"\nW0622 08:58:19.654] missing required gcloud component \"beta\". that was disabled in https://github.com/kubernetes/kubernetes/pull/36292 (which I think ended up in 1.6). I think if you set KUBE_PROMPT_FOR_UPDATE=y you can reenable the old behavior. cc @jlowdermilk \nfor the kubernetes e2e jobs, we've just installed the components in the docker image we use.. /lgtm. but I have no write access here. /lgtm. /ok-to-test. I think we still want this. now there's nothing verifying that everything in the repo builds.\n. ",
    "galexrt": "@jimmidyson Sry for the dupe, I didn't saw those issues.\n. @DirectXMan12 I know what you mean.\nBut if someone wants to ignore/filter out specific types, the person should understand that this also disables metrics. The problem in my project case is that we don't want/need metrics of any containers (as the InfluxDB \"dies\" after X container series metrics).. For now I would go with adding a warning when someone ignores labels, but for what labels in specific?\nI think showing an alert for the labels below when ignored show suffice or are there other labels that should also show an warning?\n pod\n pod_container. So I should just add a warning when the label type=container is ignored?. The last build failure doesn't seem to be related with the PR. . @DirectXMan12 The warning log message has been added and CI is green now.\nIs it okay now?. I added a warning as wanted.. ",
    "jcantrill": "/cc @jimmidyson \n. apparently a dup of https://github.com/kubernetes/heapster/issues/731\n. ",
    "dispalt": "It's been up for 15 hrs.\nHere's what /validate responds with:\n```\ncurl localhost:8082/validate\nHeapster Version: 0.18.0\nSource type: kube-pod-metrics\n    Pod Errors: map[{name:busybox id:195725ea-96f0-11e5-a065-6805ca2dac7c ip:127.0.0.1}:1581 {name:k8s-master-127.0.0.1 id:c2e10198-96ee-11e5-a065-6805ca2dac7c ip:127.0.0.1}:4743]\nSource type: Kube Node Metrics\nKubernetes Nodes plugin:\n    Healthy Nodes:\n        127.0.0.1\n    No node errors\nSource type: kube-events\nExternal Sinks\n    Exported metrics:\n        uptime: Number of milliseconds since the container was started\n        cpu/usage: Cumulative CPU usage on all cores\n        cpu/limit: CPU hard limit in millicores.\n        cpu/request: CPU request (the guaranteed amount of resources) in millicores. This metric is Kubernetes specific.\n        memory/usage: Total memory usage\n        memory/working_set: Total working set usage. Working set is the memory being used and not easily dropped by the kernel\n        memory/limit: Memory hard limit in bytes.\n        memory/request: Memory request (the guaranteed amount of resources) in bytes. This metric is Kubernetes specific.\n        memory/page_faults: Number of page faults\n        memory/major_page_faults: Number of major page faults\n        network/rx: Cumulative number of bytes received over the network\n        network/rx_errors: Cumulative number of errors while receiving over the network\n        network/tx: Cumulative number of bytes sent over the network\n        network/tx_errors: Cumulative number of errors while sending over the network\n        filesystem/usage: Total number of bytes consumed on a filesystem\n        filesystem/limit: The total size of filesystem in bytes\n    Exported labels:\n        hostname: Hostname where the container ran\n        host_id: Identifier specific to a host. Set by cloud provider or user\n        container_name: User-provided name of the container or full container name for system containers\n        container_base_image: User-defined image name that is run inside the container\n        pod_name: The name of the pod\n        pod_id: The unique ID of the pod\n        pod_namespace: The namespace of the pod\n        namespace_id: The UID of namespace of the pod\n        labels: Comma-separated list of user-provided labels\n        resource_id: Identifier(s) specific to a metric\n    External Sinks:\n        Sink Type: InfluxDB\n    client: Host \"monitoring-influxdb:8086\", Database \"k8s\"\n    Number of write failures: 0\n```\nAnd here's some excerpt from the logs with added verbosity\nI1130 18:28:00.000578       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager\"\nI1130 18:28:00.000575       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox\"\nI1130 18:28:00.000726       1 kube_nodes.go:59] Failed to get container stats from Kubelet on node \"127.0.0.1\"\nI1130 18:28:00.000750       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:28:00.000767       1 kube_pods.go:110] failed to get stats for container \"controller-manager\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:28:00.000753       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox - Get http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:28:00.000786       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver\"\nI1130 18:28:00.000810       1 kube_pods.go:110] failed to get stats for container \"busybox\" in pod \"default\"/\"busybox\"\nI1130 18:28:00.000917       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:28:00.000931       1 kube_pods.go:110] failed to get stats for container \"apiserver\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:28:00.000946       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler\"\nI1130 18:28:00.001104       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:28:00.001129       1 kube_pods.go:110] failed to get stats for container \"scheduler\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:28:00.001151       1 manager.go:175] completed scraping data from sources. Errors: []\nI1130 18:28:30.000238       1 manager.go:162] starting to scrape data from sources start: 2015-11-30 18:28:00 +0000 UTC end: 2015-11-30 18:28:30 +0000 UTC\nI1130 18:28:30.000316       1 manager.go:103] attempting to get data from source \"Kube Pods Source\"\nI1130 18:28:30.000324       1 manager.go:103] attempting to get data from source \"Kube Node Metrics Source\"\nI1130 18:28:30.000361       1 manager.go:103] attempting to get data from source \"Kube Events Source\"\nI1130 18:28:30.000392       1 kube_events.go:216] Fetched list of events from the master\nI1130 18:28:30.000405       1 kube_nodes.go:128] Fetched list of nodes from the master\nI1130 18:28:30.000400       1 kube_events.go:217] []\nI1130 18:28:30.000436       1 pods.go:139] Ignoring pod heapster-v10-liajt with namespace kube-system since namespace object was not found\nI1130 18:28:30.000450       1 pods.go:139] Ignoring pod kube-dns-v9-582p1 with namespace kube-system since namespace object was not found\nI1130 18:28:30.000457       1 pods.go:139] Ignoring pod kube-ui-v3-60fz9 with namespace kube-system since namespace object was not found\nI1130 18:28:30.000463       1 pods.go:139] Ignoring pod monitoring-influxdb-grafana-v2-140h2 with namespace kube-system since namespace object was not found\nI1130 18:28:30.000469       1 pods.go:152] selected pods from api server [{pod:0xc208383800 nodeInfo:0xc2081f2980 namespace:0xc2081df3b0} {pod:0xc2083839f0 nodeInfo:0xc2081f29c0 namespace:0xc2081df3b0}]\nI1130 18:28:30.000608       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox\"\nI1130 18:28:30.000617       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager\"\nI1130 18:28:30.000720       1 kube_nodes.go:59] Failed to get container stats from Kubelet on node \"127.0.0.1\"\nI1130 18:28:30.000804       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox - Get http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:28:30.000808       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:28:30.000823       1 kube_pods.go:110] failed to get stats for container \"busybox\" in pod \"default\"/\"busybox\"\nI1130 18:28:30.000833       1 kube_pods.go:110] failed to get stats for container \"controller-manager\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:28:30.000860       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver\"\nI1130 18:28:30.001014       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:28:30.001031       1 kube_pods.go:110] failed to get stats for container \"apiserver\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:28:30.001052       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler\"\nI1130 18:28:30.001175       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:28:30.001197       1 kube_pods.go:110] failed to get stats for container \"scheduler\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:28:30.001228       1 manager.go:175] completed scraping data from sources. Errors: []\nI1130 18:28:54.640620       1 external.go:116] no timeseries data between 0001-01-01 00:00:00 +0000 UTC and 0001-01-01 00:00:00 +0000 UTC\nI1130 18:29:00.000186       1 manager.go:162] starting to scrape data from sources start: 2015-11-30 18:28:30 +0000 UTC end: 2015-11-30 18:29:00 +0000 UTC\nI1130 18:29:00.000266       1 manager.go:103] attempting to get data from source \"Kube Pods Source\"\nI1130 18:29:00.000302       1 manager.go:103] attempting to get data from source \"Kube Node Metrics Source\"\nI1130 18:29:00.000309       1 manager.go:103] attempting to get data from source \"Kube Events Source\"\nI1130 18:29:00.000372       1 kube_nodes.go:128] Fetched list of nodes from the master\nI1130 18:29:00.000361       1 kube_events.go:216] Fetched list of events from the master\nI1130 18:29:00.000377       1 pods.go:139] Ignoring pod heapster-v10-liajt with namespace kube-system since namespace object was not found\nI1130 18:29:00.000391       1 pods.go:139] Ignoring pod kube-dns-v9-582p1 with namespace kube-system since namespace object was not found\nI1130 18:29:00.000398       1 pods.go:139] Ignoring pod kube-ui-v3-60fz9 with namespace kube-system since namespace object was not found\nI1130 18:29:00.000413       1 pods.go:139] Ignoring pod monitoring-influxdb-grafana-v2-140h2 with namespace kube-system since namespace object was not found\nI1130 18:29:00.000384       1 kube_events.go:217] []\nI1130 18:29:00.000423       1 pods.go:152] selected pods from api server [{pod:0xc208383800 nodeInfo:0xc2083c0bc0 namespace:0xc208140690} {pod:0xc2083839f0 nodeInfo:0xc2083c0c00 namespace:0xc208140690}]\nI1130 18:29:00.000533       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager\"\nI1130 18:29:00.000533       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox\"\nI1130 18:29:00.000700       1 kube_nodes.go:59] Failed to get container stats from Kubelet on node \"127.0.0.1\"\nI1130 18:29:00.000715       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/controller-manager: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:29:00.000728       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox - Get http://127.0.0.1:10255/stats/default/busybox/195725ea-96f0-11e5-a065-6805ca2dac7c/busybox: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:29:00.000742       1 kube_pods.go:110] failed to get stats for container \"controller-manager\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:29:00.000747       1 kube_pods.go:110] failed to get stats for container \"busybox\" in pod \"default\"/\"busybox\"\nI1130 18:29:00.000763       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver\"\nI1130 18:29:00.000919       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/apiserver: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:29:00.000949       1 kube_pods.go:110] failed to get stats for container \"apiserver\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:29:00.000974       1 kubelet.go:110] about to query kubelet using url: \"http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler\"\nI1130 18:29:00.001109       1 kubelet.go:96] failed to get stats from kubelet url: http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler - Get http://127.0.0.1:10255/stats/default/k8s-master-127.0.0.1/c2e10198-96ee-11e5-a065-6805ca2dac7c/scheduler: dial tcp 127.0.0.1:10255: connection refused\nI1130 18:29:00.001125       1 kube_pods.go:110] failed to get stats for container \"scheduler\" in pod \"default\"/\"k8s-master-127.0.0.1\"\nI1130 18:29:00.001151       1 manager.go:175] completed scraping data from sources. Errors: []\n. 1.1 using the docker setup + cluster-dns\nClient Version: version.Info{Major:\"1\", Minor:\"1\", GitVersion:\"v1.1.2+3085895\", GitCommit:\"3085895b8a70a3d985e9320a098e74f545546171\", GitTreeState:\"not a git tree\"}\nServer Version: version.Info{Major:\"1\", Minor:\"1\", GitVersion:\"v1.1.2\", GitCommit:\"3085895b8a70a3d985e9320a098e74f545546171\", GitTreeState:\"clean\"}\n. 1) Not sure I understand that, when I use kubectl I have to use --all-namespaces when I get pods,rc, etc to see everything\n2) It appears to be an issue with docker local install giving the kubelet ip 127 etc.\nLooks like the fix needs to be on my end, thanks!\n. Fixed by using better options in hyperkube, thanks for the help!\n. ",
    "YaoZengzeng": "I also meet similar problem recently.I use hack/local-up-cluster.sh to launch a single node kubernetes,and then I install the heapster+influxdb+grafana.But I can't get the data.The real problem is showed in the heapster log as follows:\nE0426 08:46:05.000440       1 kubelet.go:230] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://127.0.0.1:10255/stats/container/\": Post http://127.0.0.1:10255/stats/container/: dial tcp 127.0.0.1:10255: getsockopt: connection refused\n          Finally I find the reason: through local-up-cluster.sh ,the cluster get a single node, but the node's internal IP and External IP both are 127.0.0.1\u3002So when heapster want to get data, it will access the cluster's nodes\u3002But the only node IP is 127.0.0.1, therefore it will connect to 127.0.0.1:10255 as above.Unfortunately , Heapster is in a pod and has dependent net namespace,so it is enable to visit the port 10255 on the host machine.\n         So my solution is to modify the local-up-cluster.sh to make the node's ip is the machine's eth0 ip,then heapster will visit the eth0 ip instead of 127.0.0.1.For example my ip is 10.10.105.29,then use this ip to replace 127.0.0.1 in follow items in local-up-cluster,sh:\n.....\nHOSTNAME_OVERRIDE=${HOSTNAME_OVERRIDE:-\"127.0.0.1\"}\n.....\n      sudo -E \"${GO_OUT}/kubelet\" ${priv_arg}\\\n        --v=${LOG_LEVEL} \\\n        --chaos-chance=\"${CHAOS_CHANCE}\" \\\n        --container-runtime=\"${CONTAINER_RUNTIME}\" \\\n        --rkt-path=\"${RKT_PATH}\" \\\n        --rkt-stage1-image=\"${RKT_STAGE1_IMAGE}\" \\\n        --hostname-override=\"${HOSTNAME_OVERRIDE}\" \\\n      *  --address=\"127.0.0.1\" *\n        --api-servers=\"${API_HOST}:${API_PORT}\" \\\n        --cpu-cfs-quota=${CPU_CFS_QUOTA} \\\n        ${dns_args} \\\n        ${net_plugin_args} \\\n        ${kubenet_plugin_args} \\\n        --port=\"$KUBELET_PORT\" >\"${KUBELET_LOG}\" 2>&1 &\n      KUBELET_PID=$!\nIn the end, everything is OK\n. ",
    "cboggs": "@mwringe, I started up Heapster in my OpenShift cluster (per #860) and it does indeed seem to be able to query Kubernetes successfully. That said, none of the sinks seem to be shipping metrics as I'd expect - Riemann receives only an initial client connection, but no events, and InfluxDB only seems to receive stats for \"docker-daemon\" and \"kubelete\", whereas the 0.18.0 release would at least populate InfluxDB appropriately.\nThe sink issue seems unrelated to this particular PR, for sure, but not sure if it's appropriate (yet) to open an issue regarding the sink issues. Happy to do so if you'd like, or I can hold off until this is all merged.\n. Ah, my apologies @mwringe, I meant that I was running HEAD from heapster-scalability and seeing this behavior. Seems to be successfully querying Kubernetes but not sinking correctly.\nI'll avoid clouding the waters any further here though, sorry again! :-)\n. @mwielgus, is there a reasonably confident ETA on a stable release from heapster-scalability?\n. Excellent! Thanks so much for the update. Certainly seems related to the PR mentioned, happy to wait until that's merged and released. In the meantime I can get by with using cadvisor as my source (it's hack week, so a less-than-ideal fix is totally fine for a while).\nI'll monitor that PR and update here if my issue goes away when it's released.\nThanks again!\n. @ivanthelad, did you get it to work with Kubernetes as the source?\nThe Riemann sink has actually been working quite nicely, it's just been the source side that I'm struggling with. :-)\n. @mwringe, sorry to have stalled out on this one - planning and meetings, meetings and planning, blah.\nThat said, I've run back into this particular issue, even with the sha that you provided in PR #749. At one point I'd managed to make it work, but we've since blown away our test OpenShift environment and I can't seem to get the same version to work with auth'd kubernetes endpoints anymore (same behavior as mentioned in this issue's initial post).\nI tried to build from mwringe:update-1.2.0, but I'm running into all sorts of pain in getting godep get k8s.io/kubernetes and godep get k8s.io/heapster to successfully complete.\nAny ideas? We're soon to be at a point where missing the Kubernetes metadata for our container metrics is going to be very painful, happy to tackle whatever suggestions you might have. :-)\n. Interesting. I'll try those steps, though they look awfully familiar - then again, it's been more than a few sleeps since I tried. I'll give it another go.\nI also recall trying to build from the head of heapster-scalability, but no love there either.\nThanks for the input! Will loop back around after I give it a shot tonight or tomorrow.\n. @mwringe, it worked! Had to add steps to install godep in my container, but it worked this time. I haven't a clue what I did wrong before, but that's fine, I have a reproducible build now, thanks!\nI think it's working with Kubernetes appropriately at this point, but I'm getting flooded with 404's for containers that never managed to spin up inside the pending pods that kubernetes sees. Need to figure out how to quiet that down, and verify that my metrics are shipping as I expected, but I think I'm in a good spot. Thanks for your help!\n. Yea the godep path issue was an easy fix, I had just forgotten it at first. No biggie.\nThe 404's are in the heapster log. I've tried various flags to quiet it down, but no dice yet. Eventually just piped everything to /dev/null in the wrapper script for now, though that's clearly not a viable solution long-term.\nI believe the flags I've tried are -stderrthreshold=10 and -alsologtostderr=false, but to no avail.\n. Yea it's really not a big problem in itself, just ends up filling up my docker volume on whatever host happens to be hosting heapster at the time. Would be down for a better fix than piping to /dev/null, that's for sure. :-)\n. ",
    "phemmer": "Perhaps. I'm sure it's a trivial change. Will take a look.\n. ",
    "stefanodoni": "Hi,\nI have the same problem,\nI see negative CPU usage in Grafana dashboard:\n\nHere is kubernetes version:\n$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.2\", GitCommit:\"528f879e7d3790ea4287687ef0ab3f2a01cc2718\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.3+coreos.0\", GitCommit:\"c2d31de51299c6239d8b061e63cec4cb4a42480b\", GitTreeState:\"clean\"}\nAs regard the monitoring, I have used the YAML bundled with kubernetes 1.2.4 official tar, installed with:\n$ kubectl create -f addons/cluster-monitoring/influxdb\nI'm having hard time to get a correctly working heapster + grafana + influxdb setup. The official monitoring guide (https://github.com/kubernetes/heapster/blob/master/docs/influxdb.md) yelds a setup where Grafana is not showing anything, due to wrongly named metrics (e.g. memory/limit_gauge_something). I have then used the YAML included as part of the official K8S release, but I'm still getting wrong values.\nAm I missing something?\nThank you in advance for any help.\n. Great,\nThanks!\nWhat about the other questions:\n1) node_utilization always 0: is that related to the bug?\n2) no filesystem metrics available in Heapster API (but available in InfluxDB)?\n. Hi,\nI have opened this issue against the wrong project I guess, as it is not really heapster related.\nReopening it against kubernetes.\n. ",
    "MrGossett": "the following lines might also be elevated to error level:\nintegration/framework.go:211:       glog.V(1).Infof(\"cluster validation failed - %q\\n %s\", err, out)\nintegration/framework.go:221:       glog.V(1).Infof(\"kube client creation failed - %q\", err)\nintegration/framework.go:358:       glog.V(2).Infof(\"Cannot delete namespace %q. Skipping deletion.\", ns)\nmanager/manager.go:137:             glog.V(1).Infof(\"Model housekeeping returned error: %s\", err.Error())\nmanager/manager.go:186:     glog.V(1).Infof(\"housekeeping resulted in following errors: %v\", errors)\nmodel/impl.go:149:      glog.V(2).Infof(\"nil namespace pointer passed to addPod\")\nmodel/impl.go:154:      glog.V(2).Infof(\"nil node pointer passed to addPod\")\nsinks/gcm/core.go:119:      glog.V(2).Infof(\"[GCM] Deleting metric %q failed: %v\", metricName, err)\nsinks/riemann/driver.go:199:            glog.V(2).Infof(\"Failed sending event to Riemann: %+v: %+v\", event, err)\nsources/kube_events.go:211:         glog.V(1).Infof(\"Event watch loop was terminated due to error. Will restart it. Error: %v\", err)\nsources/kube_nodes.go:64:       glog.V(3).Infof(\"No container stats from Kubelet on node %q\", host)\n. ",
    "andrejvanderzee": "I have tried with volumes hostDir like this but it does not create anything in the host's path:\napiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    name: influxGrafana\n  name: influxdb-grafana\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    name: influxGrafana\n  template:\n    metadata:\n      labels:\n        name: influxGrafana\n    spec:\n      nodeSelector:\n        role: monitor\n      containers:\n      - name: influxdb\n        image: kubernetes/heapster_influxdb:v0.5\n        volumeMounts:\n        - mountPath: /data\n          name: influxdb-storage\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:http://172.31.7.28:8080\n        - --sink=influxdb:http://127.0.0.1:8086\n      - name: grafana\n        image: kubernetes/heapster_grafana:v2.1.0\n        ports: \n        - containerPort: 3000\n          hostPort: 3000\n        env:\n          - name: INFLUXDB_SERVICE_URL\n            value: http://127.0.0.1:8086\n        volumeMounts:\n        - mountPath: /var\n          name: grafana-storage\n      volumes:\n      - name: influxdb-storage\n        source:\n          hostDir:\n            path: /var/lib/monitor/influxdb\n      - name: grafana-storage\n        source:\n          hostDir:\n            path: /var/lib/monitor/grafana\n. The current master version is not useful for making views in Kibana because related data is spread over multiple log entries which we can only group with custom UI implementations (and even then we rather have pre-grouped data).\nTherefor we tried this feature branch on our DEV clusters and grouping the data in \"metric families\" makes perfect sense in context of Elastic Search and Kibana;  now we can actually make views and dashboards, which is not possible with the current master branch (it is actually rather useless).\nAnother big improvement is splitting up the index per day like in heapster-YYYY.MM.DD. In Elastic Search we do not want to keep metric data forever and with this change we can delete old indices based on date-bucket, which is common practice in Elastic.\n. Thanks for the visualizations this is really great stuff!\n. @AlmogBaku We do see occasionaly negative values for Metrics.cpu/usage_rate.value. Not sure what causes this.\n. @AlmogBaku I think the problem is missing values (there is no cpu/request and cpu/limit):\n{\n  \"_index\": \"heapster-2016.11.02\",\n  \"_type\": \"cpu\",\n  \"_id\": \"8719b52c-a0f8-11e6-b156-6692f8478d50\",\n  \"_score\": null,\n  \"_source\": {\n    \"CpuMetricsTimestamp\": \"2016-11-02T12:33:00Z\",\n    \"Metrics\": {\n      \"cpu/usage\": {\n        \"value\": 0\n      },\n      \"cpu/usage_rate\": {\n        \"value\": -469\n      }\n    },\n    \"MetricsTags\": {\n      \"container_base_image\": \"abc:v0\",\n      \"container_name\": \"abc\",\n      \"host_id\": \"192.168.64.74\",\n      \"hostname\": \"192.168.64.74\",\n      \"namespace_id\": \"5ae5cad8-00ad-11e6-a24f-fa163e48ccfb\",\n      \"namespace_name\": \"default\",\n      \"nodename\": \"192.168.64.74\",\n      \"pod_id\": \"a9bff453-a0e6-11e6-ae93-fa163e48ccfb\",\n      \"pod_name\": \"abc-2147496296-uvry3\",\n      \"pod_namespace\": \"default\",\n      \"type\": \"pod_container\"\n    }\n  },\n  \"fields\": {\n    \"CpuMetricsTimestamp\": [\n      1478089980000\n    ]\n  },\n  \"sort\": [\n    1478089980000\n  ]\n}\n. @Thermi We do get the values 99,9% of the time for the same POD instances. But sometimes we get  huge negative outliers (I assume cause of the missing metrics) which makes our graphs unreadable. Moreover, we do use limits and requests for all our POD definitions (as we have a default limitrange).\nUnfortunately we are on Rhel Atomic Host which now runs Kube 1.2.\n. @Thermi Yes, we get cpu/request and cpu/limit 99,9% of the time, but when it is missing we get a negative cpu/usage_rate like in the Elastic Search sample I posted above (probably because they are calculated in terms of these missing values).\n. @piosz We are running CentOS Atomic with systemd v219. The issue you mentioned is caused by  newer versions of systemd (>= v226). \n. @piosz See #1363.\n. Review status: 0 of 100 files reviewed at latest revision, 2 unresolved discussions.\n\ncommon/elasticsearch/elasticsearch.go, line 37 at r4 (raw file):\n\nGo\n  bulkProcessor *elastic.BulkProcessor\n  base_index    string\n}\n\nWhy underscore instead of camelcase?\n\ncommon/elasticsearch/elasticsearch.go, line 44 at r4 (raw file):\n\nGo\nfunc (esConfig ElasticSearchService) IndexAlias(date time.Time, typeName string) string {\n  return date.Format(fmt.Sprintf(\"%s-%s-2006.01.02\", esConfig.base_index, typeName))\n}\n\nThis should be esSvc and not esConfig like below. Probably also better to use a pointer type as a receiver like below.\n\nComments from Reviewable\n Sent from Reviewable.io \n. Review status: 0 of 100 files reviewed at latest revision, 3 unresolved discussions.\n\ncommon/elasticsearch/mapping.go, line 177 at r4 (raw file):\n\nGo\n      \"index\": \"not_analyzed\"\n    }\n  }\n\nHave you considered to disable _all (its enabled by default)? That will save disk space:\n\"_default_\": {\n            \"_all\": {\n                \"enabled\": false,\n            },\n\nComments from Reviewable\n Sent from Reviewable.io \n. Review status: 0 of 100 files reviewed at latest revision, 3 unresolved discussions.\n\ncommon/elasticsearch/mapping.go, line 177 at r4 (raw file):\n\nPreviously, andrejvanderzee wrote\u2026\n\nHave you considered to disable _all (its enabled by default)? That will save disk space:\n\"_default_\": {\n            \"_all\": {\n                \"enabled\": false,\n            },\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-all-field.html\n\n\nComments from Reviewable\n Sent from Reviewable.io \n. @AlmogBaku I have looked through the PR and added few comments.\n. Reviewed 5 of 9 files at r1, 90 of 91 files at r2, 3 of 4 files at r3, 2 of 2 files at r4, 4 of 4 files at r5.\nReview status: all files reviewed at latest revision, all discussions resolved.\n\nComments from Reviewable\n Sent from Reviewable.io \n. LGTM. How can I help? I can acknowledge the created indices in the current version pushes me to write my own UI client for visualization, which obviously sucks. If this PR enables us to create proper charts in Kibana like the PR suggests, that would be all great.\n. @AlmogBaku Would be happy to help out. Hopefully I can find the time this week or otherwise will do in the weekend. Would it be possible that you export/send the chart objects from Kibana so that I can quickly import and view them? That would safe me some tedious work.\n. @Thermi @AlmogBaku We are running your feature branch now on our DEV cluster and the indices look much, much more useful right now. \nWould it be possible to receive the complete dashboard and all visualizations? It will be useful to others as well so it might be an option to add those to the source base.\nIn the coming days I will review the code. \n. Duplicate of #1313.\n. @jonaz We are running CentOS Atomic with systemd v219. The issue you mentioned is caused by newer versions of systemd (>= v226).\n. What is the state on this PR. My company will migrate to ES v5 soon too. Is the PR working? Can I help out somehow?. ",
    "paulbakker": "Any news on this issue? I tried this with an EBS volume. When I delete the pod and attach the EBS to another container, I see it still contains the log/grafana and lib/grafana folders, but they are both empty.\n. ",
    "andreikop": "This is a bug in the influxdb-grafana-controller.yaml. It tries to mount volume to /var, but grafana dockerfile overwrites this by mounting volume to /var/lib/grafana.\nNext peace of yaml works for me:\ninfluxdb-grafana-controller.yaml:\n...\n          volumeMounts:\n          - name: grafana-persistent-storage\n            mountPath: /var/lib/grafana\n      volumes:\n      - name: influxdb-persistent-storage\n        emptyDir: {}\n      - name: grafana-persistent-storage\n        hostPath:\n          path: /var/lib/grafana\nNote that grafana dashboards are preserved, but not data in influxdb\n. ",
    "ScubaDrew": "Thanks @hlamer -- this seems to work for me so far :)\nI have a basic question about the influxdb persistent storage. If this is tied to an AWS ebs and you have many replicas of this controller -- won't replicas be accessing the same data?\nI dug into the Dockerfile for this and can't determine how this could work.\n. This is causing me a serious issue as users are forced to check dozens of terminated pods to find the/a active pod when you have pods that cycle frequently.\n. ",
    "kvz": "I'm new to his but I believe multiple instances of influxes would corrupt data if they both would write to the same thing. I think what you could do is create an awsElasticBlockStore volume and point a single influxdb container to its volume-id. This should give persistence. And kubernetes should recreate a pod and remount that volume if it blows up. Just a few minutes downtime. Acceptable maybe\nIf you need to scale up, increase your millicores and/or provisioned i/o on the volume. If that does not suffice, I think you'd need to shard. \nThis is all mostly guesswork based on what I've consumed so far as a newcomer, so I'd love for someone more seasoned to correct me, as I'm about to implement/verify it myself this way :). ",
    "carlosedp": "I'm trying to persist Grafana and Influx data on a StorageClass backed by NFS but the Grafana PCV fails.\nIt mount's, binds the PV to the PVC but Grafana gives an error on start. Influx works perfectly:\n```\n[root@kube-master-1 influx-grafana]# kubectl get storageclass\nNAME                  TYPE\nmanaged-nfs-storage   fuseim.pri/ifs\n[root@kube-master-1 influx-grafana]# kubectl get pv\nNAME                                                                     CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                           STORAGECLASS          REASON    AGE\nkube-system-grafana-pv-claim-pvc-2d14d769-35c5-11e7-97a1-fa163e5e86fb    1Gi        RWX           Delete          Bound     kube-system/grafana-pv-claim    managed-nfs-storage             15m\nkube-system-influxdb-pv-claim-pvc-2d22b3e9-35c5-11e7-97a1-fa163e5e86fb   5Gi        RWX           Delete          Bound     kube-system/influxdb-pv-claim   managed-nfs-storage             15m \n[root@kube-master-1 influx-grafana]# kubectl get pvc --all-namespaces\nNAMESPACE     NAME                STATUS    VOLUME                                                                   CAPACITY   ACCESSMODES   STORAGECLASS          AGE\nkube-system   grafana-pv-claim    Bound     kube-system-grafana-pv-claim-pvc-2d14d769-35c5-11e7-97a1-fa163e5e86fb    1Gi        RWX           managed-nfs-storage   15m\nkube-system   influxdb-pv-claim   Bound     kube-system-influxdb-pv-claim-pvc-2d22b3e9-35c5-11e7-97a1-fa163e5e86fb   5Gi        RWX           managed-nfs-storage   15m\n```\nGrafana Logs:\n```\nPod: monitoring-grafana-2742367454-pqwhf\nStarting a utility program that will configure Grafana\nStarting Grafana in foreground mode\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp [::1]:3000: getsockopt: connection refused. Retrying after 5 seconds...\nt=2017-05-10T21:25:18+0000 lvl=info msg=\"Starting Grafana\" logger=main version=v4.0.2 commit=unknown-dev compiled=2017-05-10T21:25:18+0000\nt=2017-05-10T21:25:18+0000 lvl=info msg=\"Config loaded from\" logger=settings file=/usr/share/grafana/conf/defaults.ini\nt=2017-05-10T21:25:18+0000 lvl=info msg=\"Config loaded from\" logger=settings file=/etc/grafana/grafana.ini\nt=2017-05-10T21:25:18+0000 lvl=info msg=\"Config overriden from command line\" logger=settings arg=\"default.paths.data=/var/lib/grafana\"\nt=2017-05-10T21:25:18+0000 lvl=info msg=\"Config overriden from command line\" logger=settings arg=\"default.paths.logs=/var/log/grafana\"\nt=2017-05-10T21:25:18+0000 lvl=info msg=\"Config overriden from Environment variable\" logger=settings var=\"GF_SERVER_PROTOCOL=http\"\nt=2017-05-10T21:25:18+0000 lvl=info msg=\"Config overriden from Environment variable\" logger=settings var=\"GF_SERVER_HTTP_PORT=3000\"\nt=2017-05-10T21:25:18+0000 lvl=info msg=\"Config overriden from Environment variable\" logger=settings var=\"GF_SERVER_ROOT_URL=/grafana\"\nt=2017-05-10T21:25:18+0000 lvl=info msg=\"Config overriden from Environment variable\" logger=settings var=\"GF_AUTH_ANONYMOUS_ENABLED=true\"\nt=2017-05-10T21:25:18+0000 lvl=info msg=\"Config overriden from Environment variable\" logger=settings var=\"GF_AUTH_ANONYMOUS_ORG_ROLE=Admin\"\nt=2017-05-10T21:25:18+0000 lvl=info msg=\"Config overriden from Environment variable\" logger=settings var=\"GF_AUTH_BASIC_ENABLED=false\"\nt=2017-05-10T21:25:18+0000 lvl=info msg=\"Path Home\" logger=settings path=/usr/share/grafana\nt=2017-05-10T21:25:18+0000 lvl=info msg=\"Path Data\" logger=settings path=/var/lib/grafana\nt=2017-05-10T21:25:18+0000 lvl=info msg=\"Path Logs\" logger=settings path=/var/log/grafana\nt=2017-05-10T21:25:18+0000 lvl=info msg=\"Path Plugins\" logger=settings path=/usr/share/grafana/data/plugins\nt=2017-05-10T21:25:18+0000 lvl=info msg=\"Initializing DB\" logger=sqlstore dbtype=sqlite3\nt=2017-05-10T21:25:18+0000 lvl=info msg=\"Starting DB migration\" logger=migrator\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp [::1]:3000: getsockopt: connection refused. Retrying after 5 seconds...\nt=2017-05-10T21:25:23+0000 lvl=eror msg=\"Fail to initialize orm engine\" logger=sqlstore error=\"Sqlstore::Migration failed err: database is locked\\n\"\n```\nI mounted the drive and an empty file is created:\n-rw-r--r--+ 1 96 96 0 May 11  2017 grafana.db\nIf I start the pod with \"emptyDir: {}\", it starts but I can't persist it's data. Maybe it's a permission issue.. ",
    "shivangipatwardhan2018": "Any updates on this? I have a EBS backed Persistent Volume and each time I try to mount that using: \nvolumes:\n      - name: influxdb-persistent-storage\n        persistentVolumeClaim:\n          claimName: influxdb-persistent-volume-claim\nI am getting an InfluxDB:error. . ",
    "bryk": "CC @joeatwork\n. @andyxning Dashboard UI eng here. Some kube-dash functionality is not supported yet in the Dashboard UI. But we are committed to have all monitoring functionalities that kubedash had + more features this quarter. In the meantime, the Dashboard UI still has many nice non-monitoring features that you should try out :)\n. @piosz @DirectXMan12 I'll review this on behalf of Dashboard UI team. \n. Looks nice. A few questions. \n. LGTM from Dashboard UI perspective \n. @hitman99 That's a valid observation. I agree with the issue here. \n@piosz How can we make heapster return metrics for a node without cache reservations? \n. cc @rf232 \n. Will this be built into Heapster? Or maybe it'll be running in a separate Pod/Replication Controller/Service? \n. Can you highlight the differences? Is this the same API, but with different prefix and bucketing options? Or something else? \n. Out of curiosity, what is this for? \n. Does Oldtimer always talk to a persistent storage? Or, when it knows that everything is in memory already, it returns data from memory? \nAsking to asses performance for lists of pods. \n. Can you formalize X, Y and B?\nWhats the URL to get, say, hourly 95-percentiles for last 30 days?\n. LGTM\n. All right, sounds good.\n. > Otherwise, how will we know if \"everything\" is in memory?\n@mwielgus does heapster have this info? \n. All right, thanks. Sounds good. \n. ",
    "timothysc": "re: events, this overlaps a lot with work folks want to do to enable direct sharding of data to kafka. \nhttps://github.com/kubernetes/kubernetes/issues/19637\n. s/Kaflka/Kafka\n. What do you mean by \"handling\"?  Events are supposed to be non-operational.  Listing alone has a huge performance impact b/c events outnumber pod-count ~ 5-10:1 on a large churn cluster. \n. What does that mean is/was?\n. events are too heavy imo, I would vote to not try to amalgamate, but leave that to back-end systems to ETL and learn from operational data.\n. @mwielgus - Folks have wanted to shard events to different systems, e.g. kafka.\n. ",
    "roboll": ":+1: any time. assuming you can take care of pushing the updated image once this merges?\n. https://hub.docker.com/r/grafana/grafana/tags/\nlooks like grafana doesn't have their docker pushes automated, latest published version is 2.5.0.\n. ",
    "Krylon360": "2.6 would be better since it adds in the derivative function to the query editor\n. ",
    "ckleban": "I'm seeing this issue right now and I'm using the heapster:canary docker image. \nHere are my logs. I've waiting about 28 minutes and still I don't see a retry. \n\n\nkubectl --namespace=kube-system logs heapster-v10-8y7i1 --follow\nI0310 18:40:14.541729       1 heapster.go:61] /heapster --source=kubernetes:https://kubernetes.default --sink=influxdb:http://monitoring-influxdb:8086\nI0310 18:40:14.541818       1 heapster.go:62] Heapster version 0.19.1\nI0310 18:40:14.543349       1 kube_factory.go:172] Using Kubernetes client with master \"https://kubernetes.default\" and version \"v1\"\nI0310 18:40:14.543363       1 kube_factory.go:173] Using kubelet port 10255\nE0310 18:40:19.678442       1 manager.go:299] encountered following errors while setting up sinks - issues while creating an InfluxDB sink: failed to ping InfluxDB server at \"monitoring-influxdb:8086\" - Get http://monitoring-influxdb:8086/ping: dial tcp: lookup monitoring-influxdb: no such host, will retry on use\nI0310 18:40:19.680930       1 heapster.go:72] Starting heapster on port 8082\nE0310 18:42:00.005793       1 kubelet.go:102] failed to get stats from kubelet url: http://10.56.184.130:10255/stats/default/web-zelhp/c539e4e9-e6ef-11e5-8a2d-000c29fb6370/web - request failed - \"404 Not Found\", response: \"no matching container\\n\"\nE0310 18:42:00.010569       1 kube_pods.go:110] failed to get stats for container \"web\" in pod \"default\"/\"web-zelhp\"\nE0310 18:42:05.011524       1 kubelet.go:102] failed to get stats from kubelet url: http://10.56.184.130:10255/stats/default/web-zelhp/c539e4e9-e6ef-11e5-8a2d-000c29fb6370/web - request failed - \"404 Not Found\", response: \"no matching container\\n\"\nE0310 18:42:05.011578       1 kube_pods.go:110] failed to get stats for container \"web\" in pod \"default\"/\"web-zelhp\"\n. I will try that out and let you know.\n\n\nAlso, are the various versions of the heapster images available for\nbrowsing/listing somewhere? Ie, where can I go look to find the various\nversions? I'd like to possibly test against a couple versions and the only\nplace I can see a listing is on the docker hub, but I guess you aren't\nstoring new images there.\nOn Thu, Mar 10, 2016 at 11:45 AM, Piotr Szczesniak <notifications@github.com\n\nwrote:\nCould you please try the latest build\ngcr.io/google_containers/heapster:v0.20.0-alpha12? It may required some\nconfiguration changes, for reference take a look into\nhttps://github.com/kubernetes/kubernetes/blob/master/cluster/addons/cluster-monitoring/standalone/heapster-controller.yaml\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/780#issuecomment-195016076.\n. Thanks. Very helpful.\n\nDo you know what version is in dockerhub's, kubernetes/heapster:canary ?\nThe reason I ask is that I also might try\ngcr.io/google_containers/heapster:v0.18.5 if this is newer\nthanks\nchris\nOn Thu, Mar 10, 2016 at 12:49 PM, Piotr Szczesniak <notifications@github.com\n\nwrote:\nSome information you can find here:\nhttps://github.com/kubernetes/heapster/releases. Not sure whether there\nis a single source of knowledge here.\nIn general the default version in Kubernetes 1.1 is 0.18 (the latest one\nis v0.18.5) and in Kubernetes 1.2 we plan to release Heapster 1.0\n(currently pre-released as v0.20.0-alpha12).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/780#issuecomment-195040906.\n. So, 0.20.0-a12 does retry correctly and create the influxDB database.\n\nHowever, I'm not getting any metrics to show in Grafana. So I assume I have\na configuration error.\nI'm running k8s 1.1.7.\nI'm going to try the heapster/influxdb/grafana config found here and see if\nit works:\nhttps://github.com/kubernetes/kubernetes/tree/master/cluster/addons/cluster-monitoring/influxdb\nthanks\nOn Thu, Mar 10, 2016 at 1:04 PM, Piotr Szczesniak notifications@github.com\nwrote:\n\nYou should see in logs something like this Heapster version 0.20.0-alpha12.\nIt doesn't guarantee the exact version, but it's some approximation.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/780#issuecomment-195048466.\n. Okay. I've confirmed that this fixes the issue. Thanks\nChris\n\nOn Thu, Mar 10, 2016 at 1:16 PM, Chris Kleban ckleban@gmail.com wrote:\n\nSo, 0.20.0-a12 does retry correctly and create the influxDB database.\nHowever, I'm not getting any metrics to show in Grafana. So I assume I\nhave a configuration error.\nI'm running k8s 1.1.7.\nI'm going to try the heapster/influxdb/grafana config found here and see\nif it works:\nhttps://github.com/kubernetes/kubernetes/tree/master/cluster/addons/cluster-monitoring/influxdb\nthanks\nOn Thu, Mar 10, 2016 at 1:04 PM, Piotr Szczesniak \nnotifications@github.com wrote:\n\nYou should see in logs something like this Heapster version\n0.20.0-alpha12. It doesn't guarantee the exact version, but it's some\napproximation.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/780#issuecomment-195048466.\n. Retrying works. Closing request\n. \n\n",
    "pwittrock": "ok to test\n. ok to test\n. SGTM, will wait until the checks are green.\n. Probably?  The comment below says it is ok for the client to be nil, but I think you are right and it should return a nil sink in case of error.\n. Done\n. ",
    "FlavioF": "\nI was using the canary docker image in docker hub, but now I changed the version to 0.19.0 and it is still happening\nGCE using my own kubernetes cluster (not GKE)\nYes\nYes\n\nUsing 0.19.0 this log message doesn't appear but heapster didn't send any data when I shut down one of the nodes\nkube_nodes.go:59] Failed to get container stats from Kubelet on node \"node-3-samantha.internal\"\n. Hi, @huangyuqi \nI have tried the Heapster's RESTApi: http://.../api/v1/model/namespaces/default/pods. In the same scenario described above.\nRest results with all nodes alive:\n[\n  {\n   \"name\": \"hazelcast-3.5-rk8es\",\n   \"cpuUsage\": 2,\n   \"memUsage\": 305459200\n  },\n...\n]\nRest results with one node down are always:\n[\n  {\n   \"name\": \"hazelcast-3.5-rk8es\",\n   \"cpuUsage\": 0,\n   \"memUsage\": 0\n  },\n...\n]\nI waited like 1 hour and there is still any data, so I believe it is not related to cache.\nIs it possible that Heapster isn't removing the NotReady nodes from the list?\nHonestly, Idk whats happening here. Is it only happening to me?\n. ",
    "chaen": "Just to say that I would definitely love to be able to have my labels stored as separate tag in InfluxDB ! :-) so +1000 ! \n. ",
    "bartebor": "A year has passed...\nI also would really like to have some labels stored as tags in influxdb. We could pick the labels via flag(s) to keep this flexible.\nMaintainers, could you state your opinion on this issue? Are you willing to accept PR if it becomes available?. I signed cncf-cla.. Looks like I missed a release, that's what happen when you take a vacation :wink:\n@DirectXMan12, would you mind merging this PR before new conflicts emerge?. @DirectXMan12 @piosz Will this PR make it into next release?. Summarizing network interfaces' stats can (will) give wrong results for given node when machine uses bonding for enhancing bandwith or link reliability.\nFor example our kubernetes nodes have two physical intefraces eno1 and eno2 bonded into bond0.\nIn that case we will double stats since bond0 and its physical interfaces essentially share the same statistics.\nI think we should have possibility to manually pick network interface(s) to summarize on given node. It probably should be done in kubelet, since interface names may vary from node to node. Heapster could use values from /stats/summary, but it is broken at the moment (searches for eth0) or /stats/, which is broken too (takes first interface from the list).\nI am not sure if this applies also to container stats; I think there is no case in kubernetes where such complicated newtork setup is created.\n. @andyxning I agree that we should have and provide detailed network info. Heapster of course can make decision how to treat such data. The problem is nodes are different and cannot be treated in the same way.\nThat's why I suggested to have this done in kubelet, because we could set those policies locally while node provisioning and do not make clients aware of such peculiarities ;) \nOtherwise we should teach heapster to differentiate nodes somehow and apply specific algorithm in each case. Given that nodes come and go somewhat frequently this should be done in some general way (node labels?) to eliminate need of, say, heapster restart.\nDiscussion of the latter should happen in heapster issue of course.. The same happens in version 1.5.2.. #1672 looks like a some DNS cache problem, so after IP change it no longer works. It is possible that OP is in this situation because of using cluster DNS.\nIn my case however there is no IP change (\"static\" DNS), just broker restart, possibly with some maintenance measures, i.e. topic truncation.\n. ",
    "vicki-c": "I signed it!\n. @vishh changed!\n. ",
    "maklemenz": "@vicki-c thank you for your work.\n@vishh can you please make a release of that?\n. ",
    "DImuthuUpe": "Is this a bug? I'm also having the same issue and once the lifecycle block was removed stats were correctly published. But I need the lifecycle hook to be present in the configuration.\n. I'm also getting the same output\n. ",
    "Fei-Guang": "ubuntu@k8s:~$ sudo netstat -ntp |grep 10255\ntcp6       0      0 10.0.0.12:10255         172.16.70.5:60664       ESTABLISHED 30150/kubelet\ntcp6       0      0 10.0.0.12:10255         172.16.70.5:60651       ESTABLISHED 30150/kubelet\nand curl http10.0.0.12:4194/containers/   is ok\n. ubuntu@k8s:~$ docker ps --format={{.Image}}\nkubernetes/heapster:canary\nkubernetes/heapster_grafana:v2.5.0\nkubernetes/heapster_influxdb:v0.6\ngcr.io/google_containers/pause:0.8.0\ngcr.io/google_containers/pause:0.8.0\nnginx\ngcr.io/google_containers/pause:0.8.0\ngcr.io/google_samples/gb-frontend:v3\ngcr.io/google_samples/gb-redisslave:v1\ngcr.io/google_containers/pause:0.8.0\ngcr.io/google_containers/pause:0.8.0\n. ",
    "aclisp": "I signed it!\n. ",
    "jtblin": "Thanks @jimmidyson for the detailed info. I'm on leave so cannot verify but that looks like it should solve my problems. \n. ",
    "Toplife": "yes,\nI have flow this document to setup.\nbut look like can not working...\nMay be need I print some log to here.\n. ",
    "geoah": "Having the same issue, currently on canary.\n. ",
    "gravis": "I have the same issue with os 1.1.3\nOnly heapster pod is being deleted, no error in the logs, just:\nKilling container with docker id 97b2aa607729: Need to kill pod.\nin the events.\n. Ok, I'll wait for the image openshift/origin-metrics-deployer to be updated and will try again then.\nThanks!\n. /cc @mwringe\n. Thanks @mwringe, sounds like I mixed up projects :(\n. ",
    "ichekrygin": "Update: this issue is most likely due to \"corrupted\" heapster-controller.yaml during kube up. Which resulted in installation where heapster-controller.yaml was entirely missing on kube master under: /etc/kubernetes/addons/cluster-monitoring/influxdb/. They way I understand it k8s is using salt to keep installed kube-system components in check. Since heapster-controller.yaml was originally missing, adding it after the fact violated the state captured in salt, hence the controller was deleted.\nI stood up new cluster with \"corrected\" heapster-controller.yaml and heapster pod is running as expected.\n. This is no longer an issue after hipster controller was fixed. Also, this does not repro in 1.2.x.\nClosing.\n. Hi guys, this appears to be still happening in v1.3.5\n{\n  \"metrics\": [\n   {\n    \"timestamp\": \"2016-08-18T21:59:00Z\",\n    \"value\": 12256\n   },\n   {\n    \"timestamp\": \"2016-08-18T22:00:00Z\",\n    \"value\": 18446744073709537136\n   }\n  ],\n  \"latestTimestamp\": \"2016-08-18T22:00:00Z\"\n }\nwhich results in \n[\n  {\n   \"metadata\": {\n    \"name\": \"rain-377063289-zpx8n\",\n    \"namespace\": \"production\",\n    \"creationTimestamp\": \"2016-08-18T22:00:28Z\"\n   },\n   \"timestamp\": \"2016-08-18T22:00:00Z\",\n   \"window\": \"1m0s\",\n   \"containers\": [\n    {\n     \"name\": \"rain\",\n     \"usage\": {\n      \"cpu\": \"-14480m\",\n      \"memory\": \"0\"\n     }\n    },\n    {\n     \"name\": \"statsd\",\n     \"usage\": {\n      \"cpu\": \"0\",\n      \"memory\": \"0\"\n     }\n    }\n   ]\n  }\n ]\n. per comments in https://github.com/kubernetes/kubernetes/issues/27194#issuecomment-240874347 this could be an issue on the heapster side\n. Hi guys, it appears this issue prevents us from using HPA. \n49m           0s              395     {horizontal-pod-autoscaler }                    Warning         FailedGetMetrics        failed to get CPU consumption and request: failed to unmarshall heapster response: json: cannot unmarshal object into Go value of type []v1alpha1.PodMetrics\n. ",
    "vklonghml": "you must configure the --source=kubernetes:'' in heapster-controller.yaml,  configure it in heapster yaml with \"- --source=kubernetes:'http://$kube-apiserver' \" for example.\n. ",
    "muzzy82": "Thanks for the hint!\nI've configured the source as was suggested, but no luck yet:\nI0204 09:33:09.291775       1 heapster.go:59] /heapster --source=kubernetes:https://10.0.2.15:6443 --sink=influxdb:http://monitoring-influxdb:8086 --metric_resolution=60s\nI0204 09:33:09.292406       1 heapster.go:60] Heapster version 0.20.0-alpha3\nI0204 09:33:09.292738       1 configs.go:59] Using Kubernetes client with master \"https://10.0.2.15:6443\" and version \"v1\"\nI0204 09:33:09.292751       1 configs.go:60] Using kubelet port 10255\nI0204 09:33:09.345396       1 influxdb.go:165] created influxdb sink with options: host:monitoring-influxdb:8086 user:root db:k8s\nI0204 09:33:09.345433       1 heapster.go:86] Starting with InfluxDB Sink sink\nI0204 09:33:09.345438       1 heapster.go:86] Starting with MetricSink sink\nI0204 09:33:09.357588       1 heapster.go:145] Starting heapster on port 8082\nI0204 09:34:05.000208       1 manager.go:76] Scraping metrics start: 2016-02-04 09:33:00 +0000 UTC, end: 2016-02-04 09:34:00 +0000 UTC\nE0204 09:34:05.000319       1 kubelet.go:283] Node 10.0.2.15 has no valid hostname and/or IP address: 10.0.2.15 \nI0204 09:34:05.000330       1 manager.go:139] ScrapeMetrics: time: 1.183\u00b5s size: 0\nI0204 09:34:05.000337       1 manager.go:141]    scrape  bucket 0: 0\nI0204 09:34:05.000340       1 manager.go:141]    scrape  bucket 1: 0\nI0204 09:34:05.000343       1 manager.go:141]    scrape  bucket 2: 0\nI0204 09:34:05.000346       1 manager.go:141]    scrape  bucket 3: 0\nI0204 09:34:05.000348       1 manager.go:141]    scrape  bucket 4: 0\nI0204 09:34:05.000351       1 manager.go:141]    scrape  bucket 5: 0\nI0204 09:34:05.000354       1 manager.go:141]    scrape  bucket 6: 0\nI0204 09:34:05.000357       1 manager.go:141]    scrape  bucket 7: 0\nI0204 09:34:05.000359       1 manager.go:141]    scrape  bucket 8: 0\nI0204 09:34:05.000362       1 manager.go:141]    scrape  bucket 9: 0\nI0204 09:34:05.000365       1 manager.go:141]    scrape  bucket 10: 0\nF0204 09:34:05.000804       1 manager.go:135] Error in processor: No node info in pod namespace:default/pod:monitoring-influxdb-grafana-v3-4s5sm: map[pod_name:monitoring-influxdb-grafana-v3-4s5sm pod_id:1a48f95a-cb22-11e5-88b9-080027f6b007 labels:k8s-app:influxGrafana,kubernetes.io/cluster-service:true,version:v3 namespace_id:eff755ad-ca88-11e5-88b9-080027f6b007 type:pod namespace_name:default pod_namespace:default]\nMy kube-apiserver indeed listens on port 6443:\nbash\n[root@kubernetes-monitoring kubernetes]# netstat -nlp | grep 6443\ntcp6       0      0 :::6443                 :::*                    LISTEN      9313/kube-apiserver\nDetected/default Kubelet port seems to be correct, too:\nbash\n[root@kubernetes-monitoring kubernetes]# netstat -nlp | grep 10255\ntcp6       0      0 :::10255                :::*                    LISTEN      9467/kubelet\nFurthermore, using the above provided URL i was able to connect to kube-apiserver from an arbitrary pod using the auto-mounted service account.\n. Node info and kubernetes version:\nbash\n[root@kubernetes-monitoring vagrant]# kubectl get node 10.0.2.15 -oyaml\napiVersion: v1\nkind: Node\nmetadata:\n  creationTimestamp: 2016-02-03T15:15:20Z\n  labels:\n    kubernetes.io/hostname: 10.0.2.15\n  name: 10.0.2.15\n  resourceVersion: \"6196\"\n  selfLink: /api/v1/nodes/10.0.2.15\n  uid: f0c51a66-ca88-11e5-88b9-080027f6b007\nspec:\n  externalID: 10.0.2.15\nstatus:\n  addresses:\n  - address: 10.0.2.15\n    type: LegacyHostIP\n  capacity:\n    cpu: \"3\"\n    memory: 3882260Ki\n    pods: \"40\"\n  conditions:\n  - lastHeartbeatTime: 2016-02-03T20:17:53Z\n    lastTransitionTime: 2016-02-03T15:15:20Z\n    reason: kubelet is posting ready status\n    status: \"True\"\n    type: Ready\n  nodeInfo:\n    bootID: 5e182318-29e0-48b7-8bf9-d44cdfd029ec\n    containerRuntimeVersion: docker://1.8.2-el7.centos\n    kernelVersion: 3.10.0-327.4.5.el7.x86_64\n    kubeProxyVersion: v1.1.0-alpha.0.1464+2bfa9a1f98147c\n    kubeletVersion: v1.1.0-alpha.0.1464+2bfa9a1f98147c\n    machineID: 02ab91023f244febb8c38be42e37cc2d\n    osImage: CentOS Linux 7 (Core)\n    systemUUID: 9182636F-5EEF-404A-8B9B-0A8647AA1640\n[root@kubernetes-monitoring vagrant]# kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"1+\", GitVersion:\"v1.1.0-alpha.0.1464+2bfa9a1f98147c\", GitCommit:\"2bfa9a1f98147cfdc2e9f4cf50e2c430518d91eb\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"1+\", GitVersion:\"v1.1.0-alpha.0.1464+2bfa9a1f98147c\", GitCommit:\"2bfa9a1f98147cfdc2e9f4cf50e2c430518d91eb\", GitTreeState:\"clean\"}\n. OK, thanks for your kind help!\n. Further investigating it seems that heapster successfully connects to kubernetes master, kubelet and influxdb:\nbash\nActive Internet connections (servers and established)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    \ntcp        0      0 172.17.0.3:59005        10.254.0.1:443          ESTABLISHED 1/heapster\ntcp        0      0 172.17.0.3:59007        10.254.0.1:443          ESTABLISHED -\ntcp        0      0 172.17.0.3:59003        10.254.0.1:443          ESTABLISHED 1/heapster\ntcp        0      0 172.17.0.3:53683        10.0.2.15:10255         ESTABLISHED 1/heapster\ntcp        0      0 172.17.0.3:38328        10.254.51.113:8086      ESTABLISHED -\ntcp        0      0 172.17.0.3:58999        10.254.0.1:443          ESTABLISHED 1/heapster\ntcp        0      0 172.17.0.3:38365        10.254.51.113:8086      ESTABLISHED 1/heapster\ntcp        0      0 172.17.0.3:59000        10.254.0.1:443          ESTABLISHED 1/heapster\ntcp        0      0 172.17.0.3:58996        10.254.0.1:443          ESTABLISHED 1/heapster\ntcp        0      0 :::8082                 :::*                    LISTEN      1/heapster\nActive UNIX domain sockets (servers and established)\nHowever the measurements collected by heapster are somewhat strange:\n``` bash\nInfluxDB shell 0.9.6\n\nuse k8s\nUsing database k8s\nshow measurements\nname: measurements\n\n\nname\ncpu/limit\ncpu/node_reservation\ncpu/node_utilization\ncpu/request\ncpu/usage\ncpu/usage_rate\nfilesystem/limit\nfilesystem/usage\nmemory/limit\nmemory/major_page_faults\nmemory/node_reservation\nmemory/node_utilization\nmemory/page_faults\nmemory/request\nmemory/usage\nmemory/working_set\nnetwork/rx\nnetwork/rx_errors\nnetwork/tx\nnetwork/tx_errors\nuptime\n```\nAccording to storage schema, measurements should've been postfixed with metric type and unit, e.g.:\nuptime_ms_cumulative\n. ",
    "galal-hussein": "@jimmidyson I got the same issue of kubelet.go:279] Node node-02 has no valid hostname and/or IP address: node-02 But in my case the node is configured with ExternalIP:\nstatus:\n  addresses:\n  - address: x.x.x.x\n    type: ExternalIP\nI am new to Kubernetes, so correct me if i am wrong but it  should be a part of ExternalIP in this snippet , right?\n. ",
    "kiyose": "@mwielgus, it isn't clear to me when the metrics/sinks/riemann/driver.go file was copied into the heapster-scalability branch. That copy doesn't appear to have the reliability code that I fixed in this PR but I'm not sure if that is because it was intentionally removed or just not ported.\n. ",
    "llarsson": "I am unclear about how I would go about making a build that gets pushed out through the official channels.\nThe code for OpenTSDB is there (not mine), there are no bug reports against it as far as I can see, and it's just not made available in any of the official images.\n. ",
    "harryge00": "Is there an official docker image about OpenTSDB?\n. ",
    "theobolo": "Same issue here\nKubernetes v 1.1.7\n. Ohhhh at least i think i've figured out why : \nAs it was said in an other issue they changed the db architecture inside the InfluxDB.\nThe problem is just that the container and kubernetes metrics templates are not requesting the correct paths ...\nLet's see : https://github.com/kubernetes/heapster/issues/970\n. Was working good with the kubernetes/cluster/addons/cluster-monitoring/influxdb****\nBut with all the folder even heapster.\nIt's working from that folder. \n. @piosz Hello guys ! I tried with the stable 1.1.0 and the 1.0.2 it's the same problem.\nObject {data: \"\", status: 500, config: Object, statusText: \"Internal Server Error\"}\nI tried with the DNS proxy name http://monitoring-influxdb:8086 and also with the Service IP but not working. For me it's not working since maybe 1 or 2 month.\nI use Azkube to deploy Kubernetes like Otavio and we are on the Kubernetes 1.2.4 version :)\nbtw @boj if you face the blank page it's maybe because you try to reach Grafana from an other path than /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/.\nIf you want to use a different path you need to change the : \n- name: GF_SERVER_ROOT_URL\n. @piosz Yes of course ! i'm waiting for that too (1.3 release) so it's alright. It's not an emergency case :P \nI'll try it soon as possible and give you my feedback. \nThanks for your answers @boj @piosz \n. ",
    "tommyknows": "My DNS is working, when I start a busybox in the kube-system Namespace and execute a nslookup monitoring-grafana, I get the correct IP address of the server. I can curl to that IP and get a WebSite back...\n//Edit:\nI tested with other names than monitoring-grafana and with direct and proxy mode;\nThe heapster/deploy/kube-config/influxdb/grafana-service.yaml doesn't open the 8086 Port. But the influxdb Service does. That's why I tested. No success...\nOnly localhost works, doesn't matter if direct or proxy...\n//Edit 2:\nInserted the Service-IP of the InfluxDB Service. When cURL to it from a CoreOS-Instance:\nshell\ncore@server ~ $ curl [Service-IP]:8086\n404 page not found\nIt also works from inside a container. \nBut I get a response, so I think this is correct. However, when added as the datasource and tested, I just get \"Unkown Error\" and \"Dashboard init failed\nTemplate variables could not be initialized: undefined\". \nChecking the logs of the Grafana Container, I see a \"http: proxy error: dial tcp [Service-IP]:8086: i/o timeout\"\nAlso, I tried to execute the same SQL-Command that is used to query some container information;\nshell\ncurl -G 'http://[Service-IP]:8086/query?pretty=true' --data-urlencode \"db=k8s\" --data-urlencode 'q=SELECT last(value) AS \"value\" FROM k8s.\"default\".\"memory/limit_bytes_gauge\" WHERE container_name =~ /{}/ AND time > now() - 6h GROUP BY time(10s)'\n{\n    \"results\": [\n        {}\n    ]\n}\n. Ok, that explains the error 500s, but the DNS-problem still persists, right?\n. ",
    "ZhuPeng": "Encounter same issue with kubernetes building from master HEAD source(commit id: 7ee8dde1e3afc9fc68616c54833f96d6391e0b22)\n. The problem was same with this issue https://github.com/kubernetes/kubernetes/issues/18868.\n. ",
    "Morriz": "I have the same issue (tested with 1.1.2 all the way up to 1.2.0-alpha.8). Could it be networking contraints that prevent it? Or would we see 404s then?\n. I found that my issues were related to firewalling, so I advise to check all routes...\n. ",
    "zjx-immersion": "I also have the same issue which in 1.1.8 and docker 1.10.2\n. ",
    "git001": "Well. I accept the cla but I have no option currently to print and sign ;-/\nAccept the contribution or not up to you ;-)\nI signed it!\n. cool yes you are right.\nthanks.\n. ",
    "janwillies": "maybe mention in https://github.com/kubernetes/heapster/blob/master/docs/source-configuration.md that the public apiserver address has to be used when running without ServiceAccount/InClusterConfig: --source=kubernetes:http://foo.bar.lan:8080?inClusterConfig=false&auth=\n. @DirectXMan12 how did you workaround this? I tried with coreos beta channel which has the systemd-cgroup driver, and also with coreos alpha channel, which has the docker-cgroup driver. No luck though\n. ",
    "elastisys": "I signed the CLA.\n. Closing this pull request since I messed up and submitted it with the wrong github account. It has been replaced by #1000.\n. ",
    "innovia": "cf kubernetes/kubernetes#487 same issue...\n. Thank you!\n. @fgrzadkowski  how do I build only heapster to test on my deployed  alpha 1.2 cluster?\n. great thank you so much\n. @fgrzadkowski sorry but I don't see that in the releases for kubernetes \nyou mean a release on heapster ? \nif so how do I add it to the alpha8 deployed cluster / or how do i redeploy with the new release?\n. thanks for the explanation - i really appreciate your help\n. @fgrzadkowski  i've deleted my running rc's for garafana and heapster and re-ran  the create on them and it's working great! thank you so much for the fix\n. +1 for pod filtering\n. ",
    "dmcnaught": "I updated our 1.2alpha8 cluster to use heapster v0.20.0-alpha9 and heapster_grafana:v2.6.0-2.\nIt's looking great now - thanks!\n. It looks like there is a similar issue in 1.1.7 and 1.1.8. Is there is a patch for that release?\n. ",
    "cheld": "ok, thanks. It seems to run stable. \nBTW stupid question: Is there a way to see the available versions of a container in gcr like on Docker Hub?\n. CC @taimir \n. manually tested and code reviewed. lgtm\n. Ready to merge\n. ok, I have to format my files...:)\n. I think the Monasca sink could be removed, because support for Kuberentes has been added directly to Monasca.\nWDYT?. ",
    "r8k": "@fgrzadkowski I think the easiest way is to use the older image docker.io/kubernetes/heapster:canary\n. ",
    "taimir": "@mwielgus so should I close this pull request and create a new one with target branch heapster:heapster-scalability?\n. O.K, I'm on it then. Closing.\n. @mwielgus ok, I've adapted the code. BTW, heapster seems rather unstable in this branch. I am running it on a local test cluster (./hack/local-up-cluster.sh) with the guestbook example on and with source: \n--source=kubernetes:http://127.0.0.1:8080?inClusterConfig=false&auth=\"\". On my machine it crashes after a minute or so (even with just the metrics sink). Is this indeed the current state?\n. LGTM as well.\n. @witekest maybe you can have a look\n. Just for reference: the change is necessary because of the way gophercloud/rackspace processes the AuthOpts when connecting to keystone. See here (TenantID not allowed in opt) and here for how one can achieve scoped auth.\nI believe this library was revisited a couple of weeks ago, so it might be worth to consider bumping to the newest version: https://github.com/gophercloud/gophercloud\n. Two small nits, then LGTM :)\n\nReview status: 0 of 2 files reviewed at latest revision, 2 unresolved discussions, some commit checks failed.\n\nmetrics/sinks/monasca/monasca_client_test.go, line 100 at r1 (raw file):\n\n``` Go\n// assert\n  assert.Nil(t, err)\n```\n\nYou can use assert.NoError(t, err) here, it's more explicit.\n\nmetrics/sinks/monasca/monasca_client_test.go, line 116 at r1 (raw file):\n\n``` Go\n// assert\n  assert.NotNil(t, err)\n```\n\nSame as above, assert.Error(t, err) here.\n\nComments from Reviewable\n Sent from Reviewable.io \n. @piosz ping\n. ",
    "CBR09": "Hi, any workaround for that?. It will useful to me.. Yes, the cAdvisor has reported stats for my physical machines.\nI got issue when my physical machine has multiple network interfaces, cAdvisor has reported network metrics of all network interfaces, but Heapster didn't, I don't know how to specify Heapster select right network interface.. ```\ncurl http://x.x.x.x:10255/stats/ | jq '.stats[0].network'\n{\n  \"tcp6\": {\n    \"Closing\": 0,\n    \"Listen\": 0,\n    \"LastAck\": 0,\n    \"Established\": 0,\n    \"SynSent\": 0,\n    \"SynRecv\": 0,\n    \"FinWait1\": 0,\n    \"FinWait2\": 0,\n    \"TimeWait\": 0,\n    \"Close\": 0,\n    \"CloseWait\": 0\n  },\n  \"tcp\": {\n    \"Closing\": 0,\n    \"Listen\": 0,\n    \"LastAck\": 0,\n    \"Established\": 0,\n    \"SynSent\": 0,\n    \"SynRecv\": 0,\n    \"FinWait1\": 0,\n    \"FinWait2\": 0,\n    \"TimeWait\": 0,\n    \"Close\": 0,\n    \"CloseWait\": 0\n  },\n  \"interfaces\": [\n    {\n      \"tx_dropped\": 0,\n      \"name\": \"eno3\",\n      \"rx_bytes\": 0,\n      \"rx_packets\": 0,\n      \"rx_errors\": 0,\n      \"rx_dropped\": 0,\n      \"tx_bytes\": 0,\n      \"tx_packets\": 0,\n      \"tx_errors\": 0\n    },\n    {\n      \"tx_dropped\": 0,\n      \"name\": \"eno4\",\n      \"rx_bytes\": 0,\n      \"rx_packets\": 0,\n      \"rx_errors\": 0,\n      \"rx_dropped\": 0,\n      \"tx_bytes\": 0,\n      \"tx_packets\": 0,\n      \"tx_errors\": 0\n    },\n    {\n      \"tx_dropped\": 3427,\n      \"name\": \"flannel.1\",\n      \"rx_bytes\": 36771186124,\n      \"rx_packets\": 59340479,\n      \"rx_errors\": 0,\n      \"rx_dropped\": 0,\n      \"tx_bytes\": 248020988479,\n      \"tx_packets\": 37138742,\n      \"tx_errors\": 0\n    },\n    {\n      \"tx_dropped\": 0,\n      \"name\": \"eno1\",\n      \"rx_bytes\": 0,\n      \"rx_packets\": 0,\n      \"rx_errors\": 0,\n      \"rx_dropped\": 0,\n      \"tx_bytes\": 0,\n      \"tx_packets\": 0,\n      \"tx_errors\": 0\n    },\n    {\n      \"tx_dropped\": 0,\n      \"name\": \"enp3s0f4d1\",\n      \"rx_bytes\": 0,\n      \"rx_packets\": 0,\n      \"rx_errors\": 0,\n      \"rx_dropped\": 0,\n      \"tx_bytes\": 0,\n      \"tx_packets\": 0,\n      \"tx_errors\": 0\n    },\n    {\n      \"tx_dropped\": 0,\n      \"name\": \"enp3s0f4\",\n      \"rx_bytes\": 685314787276,\n      \"rx_packets\": 1243755757,\n      \"rx_errors\": 0,\n      \"rx_dropped\": 0,\n      \"tx_bytes\": 706715945029,\n      \"tx_packets\": 1490150638,\n      \"tx_errors\": 0\n    },\n    {\n      \"tx_dropped\": 0,\n      \"name\": \"eno2\",\n      \"rx_bytes\": 0,\n      \"rx_packets\": 0,\n      \"rx_errors\": 0,\n      \"rx_dropped\": 0,\n      \"tx_bytes\": 0,\n      \"tx_packets\": 0,\n      \"tx_errors\": 0\n    }\n  ],\n  \"tx_dropped\": 0,\n  \"name\": \"eno3\",\n  \"rx_bytes\": 0,\n  \"rx_packets\": 0,\n  \"rx_errors\": 0,\n  \"rx_dropped\": 0,\n  \"tx_bytes\": 0,\n  \"tx_packets\": 0,\n  \"tx_errors\": 0\n}\n``\nI can see cAdvisor has reported multiple interfaces innetwork.interfaces, but at the end ofnetworkI see one interface (eno3) that I think heapster picked it. How can I select it?, mean heapster select one of these innetwork.interfaces. I've used heapster v1.2.0, but I got issue as I mentioned above and I don't understand what you said about using the summary API source. My heapster configuration is:heapster --source=kubernetes:http://kube-api.kube-system:8080?inClusterConfig=false&useServiceAccount=false --sink=influxdb:http://influxdb.kube-system:8086?db=${INFLUX_DB}&user=${INFLUX_USER}&pw=${INFLUX_PASS}`\n. @DirectXMan12 I've tried summary API, but I still got issue.\n@xiangpengzhao : thank you. Hi, any workaround for that?. It will useful to me.. ",
    "xiangpengzhao": "This issue may be due to cAdvisor. See my comment of the issue posted by @CBR09 in cAdvisor repo.\n@ckleban @DirectXMan12 . Besides this PR does, we may also want to collect and show the metrics of every network interfaces, right? i.e., the metrics can be shown in the array interfaces in the example of https://github.com/google/cadvisor/issues/1593. @andyxning correct. agreed  :). ",
    "rposts": "My apologies.  Yes, s390x is Linux on IBM's System z.  \nI am working on validating OpenShift on s390x architecture and as part of that effort I had to use Heapster for OpenShift autoscaling.  I found that I was unable to create Heapster Docker images unless I copied bolt_s390x.go from Kubernetes master.  Here is Dockerfile reference: https://github.com/openshift/origin-metrics/blob/master/heapster-base/Dockerfile\nLet me know if it makes sense.\nThanks.\n. So what is missing is bolt_s390x.go in https://github.com/kubernetes/heapster/tree/master/Godeps/_workspace/src/github.com/boltdb/bolt\n. Sure - I understand. Perhaps we can prioritize this issue accordingly.  In the meantime I will continue with mentioned workaround on s390x. Thanks.\n. ",
    "remoe": "Same here on bare-metal/coreos created with some small modifications of:\nhttps://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb\nand the current kubedash does also not work (stats 404)\n. @theobolo , right thanks! It works when one use this:\nhttps://github.com/kubernetes/kubernetes/blob/2b026ec103dabaced0e4bcc378d9b4c3bd690c08/cluster/addons/cluster-monitoring/influxdb/influxdb-grafana-controller.yaml\nBut it doesn't get the pod names:\n\n\"heapster_grafana:v2.6.0-2\" is the important version.\nUPDATE 1: I think i need to change also the other pods.\nUPDATE 2: It works!\n. @Calpicow, I've deleted the whole influxdb and garfana-data before I installed the new container.\n. ",
    "Calpicow": "+1. Updated to heapster_grafana:v2.6.0-2. Cluster view works fine, but pod view is completely broken.\n. Ok, it works when I select the podname directly, but the namespace filter is broken.\n. This issue appears to be resolved now.. ",
    "fonk": "I fixed the dashboards for heapster_grafana:v2.6.0 (and possibly earlier versions with new influxdb scheme) as far as I could - only the two Cluster Network Graphs are missing right now:\ndashboards.zip\nMaybe someone with more influxdb knowledge can fix the last two...\n. ",
    "zihaoyu": "@antoineco No I did not set that flag. kubelet runs with default values for --configure-cbr0 and --hairpin-mode options.\n. Not an issue any more. \n. ",
    "sekka1": "I am having this problem also.  Is this related to this issue?  https://github.com/kubernetes/heapster/issues/1120\n. ",
    "fragtom": "Having same issue - just default and kube-system displayed. @mwielgus - where to find \"templates\" - could find out location of namespace templates.\nBy adding query-parameter var-namespace e.g. \n?var-namespace=dev\nwe are able to drill in the namespace.\n. Update with elasticsearch 5.2 \nW0228 15:57:25.243988       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\nW0228 15:58:25.214041       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\nW0228 15:59:25.241463       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\nW0228 16:00:25.299252       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\nW0228 16:01:25.223711       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\nW0228 16:02:25.256168       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\nW0228 16:04:25.249021       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\nW0228 16:05:25.222035       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\nW0228 16:06:25.275842       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\nexactly every minute the push fails.. But on kibana data are visible, exactly grouped on timestamp \n```\nGeneralMetricsTimestamp     February 28th 2017, 17:10:00.000 \nGeneralMetricsTimestamp     February 28th 2017, 17:10:00.000 \nGeneralMetricsTimestamp     February 28th 2017, 17:10:00.000 \nGeneralMetricsTimestamp     February 28th 2017, 17:10:00.000 \nGeneralMetricsTimestamp     February 28th 2017, 17:10:00.000 \n```\n. \n. ",
    "saiwl": "thank you very much for you reply!\nAccording to you explanation, I think I find the reason. There are two pods running in kube-system namespace, but kubectl get namespaces command only shows the default namespace. I will test is later, thanks for your help!\n. ",
    "k8s-reviewable": " Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. \n Reviewable:start \nThis change is\u2002\n Reviewable:end \n. ",
    "barkbay": "I have opened a PR here https://github.com/bluebreezecf/opentsdb-goclient/pull/1\n. ",
    "euprogramador": "I'm having the same problem\n. i am running heapster in kubernetes 1.3.0.Alpha.2 is working\n. heapster:canary image it is?\n. ",
    "tspeth": "Hi guys,\nis there any update on this issue? I have the same problem as well.\nThanks!\n. ",
    "sputnick": "Same here using the deployment files example for InfluxDB from the Heapster github repo.  The code in Master seems to still have the route but it doesn't work. I have Kube-cluster.app with kubernetes 1.4 - everything but the container metrics work.\nhttps://github.com/kubernetes/heapster/blob/master/metrics/api/v1/model_handlers.go#L178\n. @DirectXMan12 what I meant was that even the direct path does not work - returns nothing.\n. This seems to be still broken. \n. The heapster cluster service /metrics gives me this\n```# HELP etcd_helper_cache_entry_count Counter of etcd helper cache entries. This can be different from etcd_helper_cache_miss_count because two concurrent threads can miss the cache and generate the same entry twice.\nTYPE etcd_helper_cache_entry_count counter\netcd_helper_cache_entry_count 0\nHELP etcd_helper_cache_hit_count Counter of etcd helper cache hits.\nTYPE etcd_helper_cache_hit_count counter\netcd_helper_cache_hit_count 0\nHELP etcd_helper_cache_miss_count Counter of etcd helper cache miss.\nTYPE etcd_helper_cache_miss_count counter\netcd_helper_cache_miss_count 0\nHELP etcd_request_cache_add_latencies_summary Latency in microseconds of adding an object to etcd cache\nTYPE etcd_request_cache_add_latencies_summary summary\netcd_request_cache_add_latencies_summary{quantile=\"0.5\"} NaN\netcd_request_cache_add_latencies_summary{quantile=\"0.9\"} NaN\netcd_request_cache_add_latencies_summary{quantile=\"0.99\"} NaN\netcd_request_cache_add_latencies_summary_sum 0\netcd_request_cache_add_latencies_summary_count 0\nHELP etcd_request_cache_get_latencies_summary Latency in microseconds of getting an object from etcd cache\nTYPE etcd_request_cache_get_latencies_summary summary\netcd_request_cache_get_latencies_summary{quantile=\"0.5\"} NaN\netcd_request_cache_get_latencies_summary{quantile=\"0.9\"} NaN\netcd_request_cache_get_latencies_summary{quantile=\"0.99\"} NaN\netcd_request_cache_get_latencies_summary_sum 0\netcd_request_cache_get_latencies_summary_count 0\nHELP get_token_count Counter of total Token() requests to the alternate token source\nTYPE get_token_count counter\nget_token_count 0\nHELP get_token_fail_count Counter of failed Token() requests to the alternate token source\nTYPE get_token_fail_count counter\nget_token_fail_count 0\nHELP go_gc_duration_seconds A summary of the GC invocation durations.\nTYPE go_gc_duration_seconds summary\ngo_gc_duration_seconds{quantile=\"0\"} 0.000123518\ngo_gc_duration_seconds{quantile=\"0.25\"} 0.00029807600000000004\ngo_gc_duration_seconds{quantile=\"0.5\"} 0.0009081720000000001\ngo_gc_duration_seconds{quantile=\"0.75\"} 0.005321064\ngo_gc_duration_seconds{quantile=\"1\"} 0.097289208\ngo_gc_duration_seconds_sum 0.544830574\ngo_gc_duration_seconds_count 27\nHELP go_goroutines Number of goroutines that currently exist.\nTYPE go_goroutines gauge\ngo_goroutines 42\nHELP go_memstats_alloc_bytes Number of bytes allocated and still in use.\nTYPE go_memstats_alloc_bytes gauge\ngo_memstats_alloc_bytes 1.88864e+07\nHELP go_memstats_alloc_bytes_total Total number of bytes allocated, even if freed.\nTYPE go_memstats_alloc_bytes_total counter\ngo_memstats_alloc_bytes_total 2.50509832e+08\nHELP go_memstats_buck_hash_sys_bytes Number of bytes used by the profiling bucket hash table.\nTYPE go_memstats_buck_hash_sys_bytes gauge\ngo_memstats_buck_hash_sys_bytes 1.487863e+06\nHELP go_memstats_frees_total Total number of frees.\nTYPE go_memstats_frees_total counter\ngo_memstats_frees_total 1.936333e+06\nHELP go_memstats_gc_sys_bytes Number of bytes used for garbage collection system metadata.\nTYPE go_memstats_gc_sys_bytes gauge\ngo_memstats_gc_sys_bytes 1.292288e+06\nHELP go_memstats_heap_alloc_bytes Number of heap bytes allocated and still in use.\nTYPE go_memstats_heap_alloc_bytes gauge\ngo_memstats_heap_alloc_bytes 1.88864e+07\nHELP go_memstats_heap_idle_bytes Number of heap bytes waiting to be used.\nTYPE go_memstats_heap_idle_bytes gauge\ngo_memstats_heap_idle_bytes 7.790592e+06\nHELP go_memstats_heap_inuse_bytes Number of heap bytes that are in use.\nTYPE go_memstats_heap_inuse_bytes gauge\ngo_memstats_heap_inuse_bytes 2.0717568e+07\nHELP go_memstats_heap_objects Number of allocated objects.\nTYPE go_memstats_heap_objects gauge\ngo_memstats_heap_objects 82332\nHELP go_memstats_heap_released_bytes_total Total number of heap bytes released to OS.\nTYPE go_memstats_heap_released_bytes_total counter\ngo_memstats_heap_released_bytes_total 0\nHELP go_memstats_heap_sys_bytes Number of heap bytes obtained from system.\nTYPE go_memstats_heap_sys_bytes gauge\ngo_memstats_heap_sys_bytes 2.850816e+07\nHELP go_memstats_last_gc_time_seconds Number of seconds since 1970 of last garbage collection.\nTYPE go_memstats_last_gc_time_seconds gauge\ngo_memstats_last_gc_time_seconds 1.487877285271342e+19\nHELP go_memstats_lookups_total Total number of pointer lookups.\nTYPE go_memstats_lookups_total counter\ngo_memstats_lookups_total 235\nHELP go_memstats_mallocs_total Total number of mallocs.\nTYPE go_memstats_mallocs_total counter\ngo_memstats_mallocs_total 2.018665e+06\nHELP go_memstats_mcache_inuse_bytes Number of bytes in use by mcache structures.\nTYPE go_memstats_mcache_inuse_bytes gauge\ngo_memstats_mcache_inuse_bytes 2400\nHELP go_memstats_mcache_sys_bytes Number of bytes used for mcache structures obtained from system.\nTYPE go_memstats_mcache_sys_bytes gauge\ngo_memstats_mcache_sys_bytes 16384\nHELP go_memstats_mspan_inuse_bytes Number of bytes in use by mspan structures.\nTYPE go_memstats_mspan_inuse_bytes gauge\ngo_memstats_mspan_inuse_bytes 271040\nHELP go_memstats_mspan_sys_bytes Number of bytes used for mspan structures obtained from system.\nTYPE go_memstats_mspan_sys_bytes gauge\ngo_memstats_mspan_sys_bytes 360448\nHELP go_memstats_next_gc_bytes Number of heap bytes when next garbage collection will take place.\nTYPE go_memstats_next_gc_bytes gauge\ngo_memstats_next_gc_bytes 2.4443671e+07\nHELP go_memstats_other_sys_bytes Number of bytes used for other system allocations.\nTYPE go_memstats_other_sys_bytes gauge\ngo_memstats_other_sys_bytes 539905\nHELP go_memstats_stack_inuse_bytes Number of bytes in use by the stack allocator.\nTYPE go_memstats_stack_inuse_bytes gauge\ngo_memstats_stack_inuse_bytes 851968\nHELP go_memstats_stack_sys_bytes Number of bytes obtained from system for stack allocator.\nTYPE go_memstats_stack_sys_bytes gauge\ngo_memstats_stack_sys_bytes 851968\nHELP go_memstats_sys_bytes Number of bytes obtained by system. Sum of all system allocations.\nTYPE go_memstats_sys_bytes gauge\ngo_memstats_sys_bytes 3.3057016e+07\nHELP heapster_exporter_duration_microseconds Time spent exporting data to sink in microseconds.\nTYPE heapster_exporter_duration_microseconds summary\nheapster_exporter_duration_microseconds{exporter=\"InfluxDB Sink\",quantile=\"0.5\"} 3.886\nheapster_exporter_duration_microseconds{exporter=\"InfluxDB Sink\",quantile=\"0.9\"} 7.083\nheapster_exporter_duration_microseconds{exporter=\"InfluxDB Sink\",quantile=\"0.99\"} 22.431\nheapster_exporter_duration_microseconds_sum{exporter=\"InfluxDB Sink\"} 161.24600000000004\nheapster_exporter_duration_microseconds_count{exporter=\"InfluxDB Sink\"} 29\nheapster_exporter_duration_microseconds{exporter=\"Metric Sink\",quantile=\"0.5\"} 5.203\nheapster_exporter_duration_microseconds{exporter=\"Metric Sink\",quantile=\"0.9\"} 6.848\nheapster_exporter_duration_microseconds{exporter=\"Metric Sink\",quantile=\"0.99\"} 33.837\nheapster_exporter_duration_microseconds_sum{exporter=\"Metric Sink\"} 3565.7509999999997\nheapster_exporter_duration_microseconds_count{exporter=\"Metric Sink\"} 29\nHELP heapster_exporter_last_time_seconds Last time Heapster exported data since unix epoch in seconds.\nTYPE heapster_exporter_last_time_seconds gauge\nheapster_exporter_last_time_seconds{exporter=\"InfluxDB Sink\"} 1.487877285e+09\nheapster_exporter_last_time_seconds{exporter=\"Metric Sink\"} 1.487877285e+09\nHELP heapster_kubelet_summary_request_duration_microseconds The Kubelet summary request latencies in microseconds.\nTYPE heapster_kubelet_summary_request_duration_microseconds summary\nheapster_kubelet_summary_request_duration_microseconds{node=\"i-47b798b5\",quantile=\"0.5\"} 1379\nheapster_kubelet_summary_request_duration_microseconds{node=\"i-47b798b5\",quantile=\"0.9\"} 1956\nheapster_kubelet_summary_request_duration_microseconds{node=\"i-47b798b5\",quantile=\"0.99\"} 14791\nheapster_kubelet_summary_request_duration_microseconds_sum{node=\"i-47b798b5\"} 67046\nheapster_kubelet_summary_request_duration_microseconds_count{node=\"i-47b798b5\"} 29\nheapster_kubelet_summary_request_duration_microseconds{node=\"i-5875cbbe\",quantile=\"0.5\"} 1378\nheapster_kubelet_summary_request_duration_microseconds{node=\"i-5875cbbe\",quantile=\"0.9\"} 2225\nheapster_kubelet_summary_request_duration_microseconds{node=\"i-5875cbbe\",quantile=\"0.99\"} 2331\nheapster_kubelet_summary_request_duration_microseconds_sum{node=\"i-5875cbbe\"} 74042\nheapster_kubelet_summary_request_duration_microseconds_count{node=\"i-5875cbbe\"} 29\nheapster_kubelet_summary_request_duration_microseconds{node=\"i-bf56a8a1\",quantile=\"0.5\"} 1278\nheapster_kubelet_summary_request_duration_microseconds{node=\"i-bf56a8a1\",quantile=\"0.9\"} 2837\nheapster_kubelet_summary_request_duration_microseconds{node=\"i-bf56a8a1\",quantile=\"0.99\"} 11185\nheapster_kubelet_summary_request_duration_microseconds_sum{node=\"i-bf56a8a1\"} 81725\nheapster_kubelet_summary_request_duration_microseconds_count{node=\"i-bf56a8a1\"} 29\nheapster_kubelet_summary_request_duration_microseconds{node=\"i-f194eedb\",quantile=\"0.5\"} 1253\nheapster_kubelet_summary_request_duration_microseconds{node=\"i-f194eedb\",quantile=\"0.9\"} 2358\nheapster_kubelet_summary_request_duration_microseconds{node=\"i-f194eedb\",quantile=\"0.99\"} 5200\nheapster_kubelet_summary_request_duration_microseconds_sum{node=\"i-f194eedb\"} 55299\nheapster_kubelet_summary_request_duration_microseconds_count{node=\"i-f194eedb\"} 29\nHELP heapster_processor_duration_microseconds The Time spent in a processor in microseconds.\nTYPE heapster_processor_duration_microseconds summary\nheapster_processor_duration_microseconds{processor=\"cluster_aggregator\",quantile=\"0.5\"} 1.326\nheapster_processor_duration_microseconds{processor=\"cluster_aggregator\",quantile=\"0.9\"} 2.885\nheapster_processor_duration_microseconds{processor=\"cluster_aggregator\",quantile=\"0.99\"} 5.584\nheapster_processor_duration_microseconds_sum{processor=\"cluster_aggregator\"} 60.461999999999996\nheapster_processor_duration_microseconds_count{processor=\"cluster_aggregator\"} 29\nheapster_processor_duration_microseconds{processor=\"namespace_aggregator\",quantile=\"0.5\"} 2.005\nheapster_processor_duration_microseconds{processor=\"namespace_aggregator\",quantile=\"0.9\"} 3.714\nheapster_processor_duration_microseconds{processor=\"namespace_aggregator\",quantile=\"0.99\"} 12.923\nheapster_processor_duration_microseconds_sum{processor=\"namespace_aggregator\"} 89.02899999999998\nheapster_processor_duration_microseconds_count{processor=\"namespace_aggregator\"} 29\nheapster_processor_duration_microseconds{processor=\"namespace_based_enricher\",quantile=\"0.5\"} 2.566\nheapster_processor_duration_microseconds{processor=\"namespace_based_enricher\",quantile=\"0.9\"} 3.571\nheapster_processor_duration_microseconds{processor=\"namespace_based_enricher\",quantile=\"0.99\"} 5.559\nheapster_processor_duration_microseconds_sum{processor=\"namespace_based_enricher\"} 90.99\nheapster_processor_duration_microseconds_count{processor=\"namespace_based_enricher\"} 29\nheapster_processor_duration_microseconds{processor=\"node_aggregator\",quantile=\"0.5\"} 1.638\nheapster_processor_duration_microseconds{processor=\"node_aggregator\",quantile=\"0.9\"} 3.166\nheapster_processor_duration_microseconds{processor=\"node_aggregator\",quantile=\"0.99\"} 5.564\nheapster_processor_duration_microseconds_sum{processor=\"node_aggregator\"} 68.347\nheapster_processor_duration_microseconds_count{processor=\"node_aggregator\"} 29\nheapster_processor_duration_microseconds{processor=\"node_autoscaling_enricher\",quantile=\"0.5\"} 1.357\nheapster_processor_duration_microseconds{processor=\"node_autoscaling_enricher\",quantile=\"0.9\"} 2.281\nheapster_processor_duration_microseconds{processor=\"node_autoscaling_enricher\",quantile=\"0.99\"} 4.775\nheapster_processor_duration_microseconds_sum{processor=\"node_autoscaling_enricher\"} 60.327999999999996\nheapster_processor_duration_microseconds_count{processor=\"node_autoscaling_enricher\"} 29\nheapster_processor_duration_microseconds{processor=\"pod_aggregator\",quantile=\"0.5\"} 1.392\nheapster_processor_duration_microseconds{processor=\"pod_aggregator\",quantile=\"0.9\"} 2.066\nheapster_processor_duration_microseconds{processor=\"pod_aggregator\",quantile=\"0.99\"} 2.93\nheapster_processor_duration_microseconds_sum{processor=\"pod_aggregator\"} 58.40999999999998\nheapster_processor_duration_microseconds_count{processor=\"pod_aggregator\"} 29\nheapster_processor_duration_microseconds{processor=\"pod_based_enricher\",quantile=\"0.5\"} 2.019\nheapster_processor_duration_microseconds{processor=\"pod_based_enricher\",quantile=\"0.9\"} 3.1\nheapster_processor_duration_microseconds{processor=\"pod_based_enricher\",quantile=\"0.99\"} 21.356\nheapster_processor_duration_microseconds_sum{processor=\"pod_based_enricher\"} 107.82100000000003\nheapster_processor_duration_microseconds_count{processor=\"pod_based_enricher\"} 29\nheapster_processor_duration_microseconds{processor=\"rate calculator\",quantile=\"0.5\"} 5.267\nheapster_processor_duration_microseconds{processor=\"rate calculator\",quantile=\"0.9\"} 6.15\nheapster_processor_duration_microseconds{processor=\"rate calculator\",quantile=\"0.99\"} 30.269\nheapster_processor_duration_microseconds_sum{processor=\"rate calculator\"} 247.80799999999994\nheapster_processor_duration_microseconds_count{processor=\"rate calculator\"} 29\nHELP heapster_scraper_duration_microseconds Time spent scraping sources in microseconds.\nTYPE heapster_scraper_duration_microseconds summary\nheapster_scraper_duration_microseconds{source=\"kubelet_summary:10.0.192.102:10255\",quantile=\"0.5\"} 6.161\nheapster_scraper_duration_microseconds{source=\"kubelet_summary:10.0.192.102:10255\",quantile=\"0.9\"} 21.463\nheapster_scraper_duration_microseconds{source=\"kubelet_summary:10.0.192.102:10255\",quantile=\"0.99\"} 300.35\nheapster_scraper_duration_microseconds_sum{source=\"kubelet_summary:10.0.192.102:10255\"} 920.0159999999997\nheapster_scraper_duration_microseconds_count{source=\"kubelet_summary:10.0.192.102:10255\"} 29\nheapster_scraper_duration_microseconds{source=\"kubelet_summary:10.0.192.156:10255\",quantile=\"0.5\"} 9.922\nheapster_scraper_duration_microseconds{source=\"kubelet_summary:10.0.192.156:10255\",quantile=\"0.9\"} 51.762\nheapster_scraper_duration_microseconds{source=\"kubelet_summary:10.0.192.156:10255\",quantile=\"0.99\"} 393.482\nheapster_scraper_duration_microseconds_sum{source=\"kubelet_summary:10.0.192.156:10255\"} 1454.0420000000001\nheapster_scraper_duration_microseconds_count{source=\"kubelet_summary:10.0.192.156:10255\"} 29\nheapster_scraper_duration_microseconds{source=\"kubelet_summary:10.0.192.192:10255\",quantile=\"0.5\"} 7.556\nheapster_scraper_duration_microseconds{source=\"kubelet_summary:10.0.192.192:10255\",quantile=\"0.9\"} 16.318\nheapster_scraper_duration_microseconds{source=\"kubelet_summary:10.0.192.192:10255\",quantile=\"0.99\"} 127.691\nheapster_scraper_duration_microseconds_sum{source=\"kubelet_summary:10.0.192.192:10255\"} 54203.30199999999\nheapster_scraper_duration_microseconds_count{source=\"kubelet_summary:10.0.192.192:10255\"} 29\nheapster_scraper_duration_microseconds{source=\"kubelet_summary:10.0.192.90:10255\",quantile=\"0.5\"} 7.371\nheapster_scraper_duration_microseconds{source=\"kubelet_summary:10.0.192.90:10255\",quantile=\"0.9\"} 30.261\nheapster_scraper_duration_microseconds{source=\"kubelet_summary:10.0.192.90:10255\",quantile=\"0.99\"} 117.587\nheapster_scraper_duration_microseconds_sum{source=\"kubelet_summary:10.0.192.90:10255\"} 1051.808\nheapster_scraper_duration_microseconds_count{source=\"kubelet_summary:10.0.192.90:10255\"} 29\nHELP heapster_scraper_last_time_seconds Last time Heapster performed a scrape since unix epoch in seconds.\nTYPE heapster_scraper_last_time_seconds gauge\nheapster_scraper_last_time_seconds{source=\"kubelet_summary:10.0.192.102:10255\"} 1.487877285e+09\nheapster_scraper_last_time_seconds{source=\"kubelet_summary:10.0.192.156:10255\"} 1.487877285e+09\nheapster_scraper_last_time_seconds{source=\"kubelet_summary:10.0.192.192:10255\"} 1.487877285e+09\nheapster_scraper_last_time_seconds{source=\"kubelet_summary:10.0.192.90:10255\"} 1.487877285e+09\nHELP http_request_duration_microseconds The HTTP request latencies in microseconds.\nTYPE http_request_duration_microseconds summary\nhttp_request_duration_microseconds{handler=\"availableClusterMetrics\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"availableClusterMetrics\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"availableClusterMetrics\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"availableClusterMetrics\"} 0\nhttp_request_duration_microseconds_count{handler=\"availableClusterMetrics\"} 0\nhttp_request_duration_microseconds{handler=\"availableContainerMetrics\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"availableContainerMetrics\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"availableContainerMetrics\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"availableContainerMetrics\"} 0\nhttp_request_duration_microseconds_count{handler=\"availableContainerMetrics\"} 0\nhttp_request_duration_microseconds{handler=\"availableMetrics\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"availableMetrics\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"availableMetrics\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"availableMetrics\"} 0\nhttp_request_duration_microseconds_count{handler=\"availableMetrics\"} 0\nhttp_request_duration_microseconds{handler=\"availableNamespaceMetrics\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"availableNamespaceMetrics\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"availableNamespaceMetrics\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"availableNamespaceMetrics\"} 0\nhttp_request_duration_microseconds_count{handler=\"availableNamespaceMetrics\"} 0\nhttp_request_duration_microseconds{handler=\"availableNodeMetrics\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"availableNodeMetrics\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"availableNodeMetrics\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"availableNodeMetrics\"} 0\nhttp_request_duration_microseconds_count{handler=\"availableNodeMetrics\"} 0\nhttp_request_duration_microseconds{handler=\"availablePodMetrics\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"availablePodMetrics\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"availablePodMetrics\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"availablePodMetrics\"} 0\nhttp_request_duration_microseconds_count{handler=\"availablePodMetrics\"} 0\nhttp_request_duration_microseconds{handler=\"clusterMetrics\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"clusterMetrics\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"clusterMetrics\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"clusterMetrics\"} 0\nhttp_request_duration_microseconds_count{handler=\"clusterMetrics\"} 0\nhttp_request_duration_microseconds{handler=\"debugAllKeys\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"debugAllKeys\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"debugAllKeys\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"debugAllKeys\"} 0\nhttp_request_duration_microseconds_count{handler=\"debugAllKeys\"} 0\nhttp_request_duration_microseconds{handler=\"freeContainerMetrics\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"freeContainerMetrics\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"freeContainerMetrics\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"freeContainerMetrics\"} 0\nhttp_request_duration_microseconds_count{handler=\"freeContainerMetrics\"} 0\nhttp_request_duration_microseconds{handler=\"namespaceList\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"namespaceList\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"namespaceList\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"namespaceList\"} 0\nhttp_request_duration_microseconds_count{handler=\"namespaceList\"} 0\nhttp_request_duration_microseconds{handler=\"namespaceMetrics\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"namespaceMetrics\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"namespaceMetrics\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"namespaceMetrics\"} 0\nhttp_request_duration_microseconds_count{handler=\"namespaceMetrics\"} 0\nhttp_request_duration_microseconds{handler=\"namespacePodList\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"namespacePodList\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"namespacePodList\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"namespacePodList\"} 0\nhttp_request_duration_microseconds_count{handler=\"namespacePodList\"} 0\nhttp_request_duration_microseconds{handler=\"nodeList\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"nodeList\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"nodeList\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"nodeList\"} 0\nhttp_request_duration_microseconds_count{handler=\"nodeList\"} 0\nhttp_request_duration_microseconds{handler=\"nodeMetrics\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"nodeMetrics\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"nodeMetrics\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"nodeMetrics\"} 0\nhttp_request_duration_microseconds_count{handler=\"nodeMetrics\"} 0\nhttp_request_duration_microseconds{handler=\"podContainerList\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"podContainerList\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"podContainerList\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"podContainerList\"} 0\nhttp_request_duration_microseconds_count{handler=\"podContainerList\"} 0\nhttp_request_duration_microseconds{handler=\"podContainerMetrics\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"podContainerMetrics\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"podContainerMetrics\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"podContainerMetrics\"} 0\nhttp_request_duration_microseconds_count{handler=\"podContainerMetrics\"} 0\nhttp_request_duration_microseconds{handler=\"podListMetric\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"podListMetric\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"podListMetric\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"podListMetric\"} 0\nhttp_request_duration_microseconds_count{handler=\"podListMetric\"} 0\nhttp_request_duration_microseconds{handler=\"podMetrics\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"podMetrics\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"podMetrics\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"podMetrics\"} 0\nhttp_request_duration_microseconds_count{handler=\"podMetrics\"} 0\nhttp_request_duration_microseconds{handler=\"pprof\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"pprof\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"pprof\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"pprof\"} 0\nhttp_request_duration_microseconds_count{handler=\"pprof\"} 0\nhttp_request_duration_microseconds{handler=\"prometheus\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"prometheus\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"prometheus\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"prometheus\"} 0\nhttp_request_duration_microseconds_count{handler=\"prometheus\"} 0\nhttp_request_duration_microseconds{handler=\"systemContainerList\",quantile=\"0.5\"} NaN\nhttp_request_duration_microseconds{handler=\"systemContainerList\",quantile=\"0.9\"} NaN\nhttp_request_duration_microseconds{handler=\"systemContainerList\",quantile=\"0.99\"} NaN\nhttp_request_duration_microseconds_sum{handler=\"systemContainerList\"} 0\nhttp_request_duration_microseconds_count{handler=\"systemContainerList\"} 0\nHELP http_request_size_bytes The HTTP request sizes in bytes.\nTYPE http_request_size_bytes summary\nhttp_request_size_bytes{handler=\"availableClusterMetrics\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"availableClusterMetrics\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"availableClusterMetrics\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"availableClusterMetrics\"} 0\nhttp_request_size_bytes_count{handler=\"availableClusterMetrics\"} 0\nhttp_request_size_bytes{handler=\"availableContainerMetrics\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"availableContainerMetrics\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"availableContainerMetrics\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"availableContainerMetrics\"} 0\nhttp_request_size_bytes_count{handler=\"availableContainerMetrics\"} 0\nhttp_request_size_bytes{handler=\"availableMetrics\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"availableMetrics\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"availableMetrics\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"availableMetrics\"} 0\nhttp_request_size_bytes_count{handler=\"availableMetrics\"} 0\nhttp_request_size_bytes{handler=\"availableNamespaceMetrics\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"availableNamespaceMetrics\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"availableNamespaceMetrics\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"availableNamespaceMetrics\"} 0\nhttp_request_size_bytes_count{handler=\"availableNamespaceMetrics\"} 0\nhttp_request_size_bytes{handler=\"availableNodeMetrics\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"availableNodeMetrics\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"availableNodeMetrics\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"availableNodeMetrics\"} 0\nhttp_request_size_bytes_count{handler=\"availableNodeMetrics\"} 0\nhttp_request_size_bytes{handler=\"availablePodMetrics\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"availablePodMetrics\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"availablePodMetrics\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"availablePodMetrics\"} 0\nhttp_request_size_bytes_count{handler=\"availablePodMetrics\"} 0\nhttp_request_size_bytes{handler=\"clusterMetrics\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"clusterMetrics\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"clusterMetrics\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"clusterMetrics\"} 0\nhttp_request_size_bytes_count{handler=\"clusterMetrics\"} 0\nhttp_request_size_bytes{handler=\"debugAllKeys\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"debugAllKeys\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"debugAllKeys\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"debugAllKeys\"} 0\nhttp_request_size_bytes_count{handler=\"debugAllKeys\"} 0\nhttp_request_size_bytes{handler=\"freeContainerMetrics\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"freeContainerMetrics\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"freeContainerMetrics\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"freeContainerMetrics\"} 0\nhttp_request_size_bytes_count{handler=\"freeContainerMetrics\"} 0\nhttp_request_size_bytes{handler=\"namespaceList\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"namespaceList\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"namespaceList\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"namespaceList\"} 0\nhttp_request_size_bytes_count{handler=\"namespaceList\"} 0\nhttp_request_size_bytes{handler=\"namespaceMetrics\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"namespaceMetrics\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"namespaceMetrics\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"namespaceMetrics\"} 0\nhttp_request_size_bytes_count{handler=\"namespaceMetrics\"} 0\nhttp_request_size_bytes{handler=\"namespacePodList\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"namespacePodList\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"namespacePodList\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"namespacePodList\"} 0\nhttp_request_size_bytes_count{handler=\"namespacePodList\"} 0\nhttp_request_size_bytes{handler=\"nodeList\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"nodeList\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"nodeList\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"nodeList\"} 0\nhttp_request_size_bytes_count{handler=\"nodeList\"} 0\nhttp_request_size_bytes{handler=\"nodeMetrics\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"nodeMetrics\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"nodeMetrics\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"nodeMetrics\"} 0\nhttp_request_size_bytes_count{handler=\"nodeMetrics\"} 0\nhttp_request_size_bytes{handler=\"podContainerList\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"podContainerList\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"podContainerList\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"podContainerList\"} 0\nhttp_request_size_bytes_count{handler=\"podContainerList\"} 0\nhttp_request_size_bytes{handler=\"podContainerMetrics\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"podContainerMetrics\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"podContainerMetrics\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"podContainerMetrics\"} 0\nhttp_request_size_bytes_count{handler=\"podContainerMetrics\"} 0\nhttp_request_size_bytes{handler=\"podListMetric\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"podListMetric\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"podListMetric\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"podListMetric\"} 0\nhttp_request_size_bytes_count{handler=\"podListMetric\"} 0\nhttp_request_size_bytes{handler=\"podMetrics\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"podMetrics\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"podMetrics\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"podMetrics\"} 0\nhttp_request_size_bytes_count{handler=\"podMetrics\"} 0\nhttp_request_size_bytes{handler=\"pprof\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"pprof\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"pprof\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"pprof\"} 0\nhttp_request_size_bytes_count{handler=\"pprof\"} 0\nhttp_request_size_bytes{handler=\"prometheus\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"prometheus\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"prometheus\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"prometheus\"} 0\nhttp_request_size_bytes_count{handler=\"prometheus\"} 0\nhttp_request_size_bytes{handler=\"systemContainerList\",quantile=\"0.5\"} NaN\nhttp_request_size_bytes{handler=\"systemContainerList\",quantile=\"0.9\"} NaN\nhttp_request_size_bytes{handler=\"systemContainerList\",quantile=\"0.99\"} NaN\nhttp_request_size_bytes_sum{handler=\"systemContainerList\"} 0\nhttp_request_size_bytes_count{handler=\"systemContainerList\"} 0\nHELP http_response_size_bytes The HTTP response sizes in bytes.\nTYPE http_response_size_bytes summary\nhttp_response_size_bytes{handler=\"availableClusterMetrics\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"availableClusterMetrics\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"availableClusterMetrics\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"availableClusterMetrics\"} 0\nhttp_response_size_bytes_count{handler=\"availableClusterMetrics\"} 0\nhttp_response_size_bytes{handler=\"availableContainerMetrics\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"availableContainerMetrics\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"availableContainerMetrics\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"availableContainerMetrics\"} 0\nhttp_response_size_bytes_count{handler=\"availableContainerMetrics\"} 0\nhttp_response_size_bytes{handler=\"availableMetrics\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"availableMetrics\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"availableMetrics\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"availableMetrics\"} 0\nhttp_response_size_bytes_count{handler=\"availableMetrics\"} 0\nhttp_response_size_bytes{handler=\"availableNamespaceMetrics\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"availableNamespaceMetrics\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"availableNamespaceMetrics\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"availableNamespaceMetrics\"} 0\nhttp_response_size_bytes_count{handler=\"availableNamespaceMetrics\"} 0\nhttp_response_size_bytes{handler=\"availableNodeMetrics\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"availableNodeMetrics\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"availableNodeMetrics\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"availableNodeMetrics\"} 0\nhttp_response_size_bytes_count{handler=\"availableNodeMetrics\"} 0\nhttp_response_size_bytes{handler=\"availablePodMetrics\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"availablePodMetrics\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"availablePodMetrics\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"availablePodMetrics\"} 0\nhttp_response_size_bytes_count{handler=\"availablePodMetrics\"} 0\nhttp_response_size_bytes{handler=\"clusterMetrics\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"clusterMetrics\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"clusterMetrics\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"clusterMetrics\"} 0\nhttp_response_size_bytes_count{handler=\"clusterMetrics\"} 0\nhttp_response_size_bytes{handler=\"debugAllKeys\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"debugAllKeys\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"debugAllKeys\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"debugAllKeys\"} 0\nhttp_response_size_bytes_count{handler=\"debugAllKeys\"} 0\nhttp_response_size_bytes{handler=\"freeContainerMetrics\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"freeContainerMetrics\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"freeContainerMetrics\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"freeContainerMetrics\"} 0\nhttp_response_size_bytes_count{handler=\"freeContainerMetrics\"} 0\nhttp_response_size_bytes{handler=\"namespaceList\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"namespaceList\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"namespaceList\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"namespaceList\"} 0\nhttp_response_size_bytes_count{handler=\"namespaceList\"} 0\nhttp_response_size_bytes{handler=\"namespaceMetrics\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"namespaceMetrics\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"namespaceMetrics\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"namespaceMetrics\"} 0\nhttp_response_size_bytes_count{handler=\"namespaceMetrics\"} 0\nhttp_response_size_bytes{handler=\"namespacePodList\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"namespacePodList\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"namespacePodList\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"namespacePodList\"} 0\nhttp_response_size_bytes_count{handler=\"namespacePodList\"} 0\nhttp_response_size_bytes{handler=\"nodeList\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"nodeList\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"nodeList\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"nodeList\"} 0\nhttp_response_size_bytes_count{handler=\"nodeList\"} 0\nhttp_response_size_bytes{handler=\"nodeMetrics\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"nodeMetrics\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"nodeMetrics\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"nodeMetrics\"} 0\nhttp_response_size_bytes_count{handler=\"nodeMetrics\"} 0\nhttp_response_size_bytes{handler=\"podContainerList\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"podContainerList\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"podContainerList\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"podContainerList\"} 0\nhttp_response_size_bytes_count{handler=\"podContainerList\"} 0\nhttp_response_size_bytes{handler=\"podContainerMetrics\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"podContainerMetrics\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"podContainerMetrics\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"podContainerMetrics\"} 0\nhttp_response_size_bytes_count{handler=\"podContainerMetrics\"} 0\nhttp_response_size_bytes{handler=\"podListMetric\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"podListMetric\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"podListMetric\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"podListMetric\"} 0\nhttp_response_size_bytes_count{handler=\"podListMetric\"} 0\nhttp_response_size_bytes{handler=\"podMetrics\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"podMetrics\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"podMetrics\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"podMetrics\"} 0\nhttp_response_size_bytes_count{handler=\"podMetrics\"} 0\nhttp_response_size_bytes{handler=\"pprof\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"pprof\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"pprof\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"pprof\"} 0\nhttp_response_size_bytes_count{handler=\"pprof\"} 0\nhttp_response_size_bytes{handler=\"prometheus\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"prometheus\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"prometheus\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"prometheus\"} 0\nhttp_response_size_bytes_count{handler=\"prometheus\"} 0\nhttp_response_size_bytes{handler=\"systemContainerList\",quantile=\"0.5\"} NaN\nhttp_response_size_bytes{handler=\"systemContainerList\",quantile=\"0.9\"} NaN\nhttp_response_size_bytes{handler=\"systemContainerList\",quantile=\"0.99\"} NaN\nhttp_response_size_bytes_sum{handler=\"systemContainerList\"} 0\nhttp_response_size_bytes_count{handler=\"systemContainerList\"} 0\nHELP process_cpu_seconds_total Total user and system CPU time spent in seconds.\nTYPE process_cpu_seconds_total counter\nprocess_cpu_seconds_total 2.32\nHELP process_max_fds Maximum number of open file descriptors.\nTYPE process_max_fds gauge\nprocess_max_fds 65536\nHELP process_open_fds Number of open file descriptors.\nTYPE process_open_fds gauge\nprocess_open_fds 13\nHELP process_resident_memory_bytes Resident memory size in bytes.\nTYPE process_resident_memory_bytes gauge\nprocess_resident_memory_bytes 5.8474496e+07\nHELP process_start_time_seconds Start time of the process since unix epoch in seconds.\nTYPE process_start_time_seconds gauge\nprocess_start_time_seconds 1.48787671439e+09\nHELP process_virtual_memory_bytes Virtual memory size in bytes.\nTYPE process_virtual_memory_bytes gauge\nprocess_virtual_memory_bytes 9.1435008e+07\nHELP ssh_tunnel_open_count Counter of ssh tunnel total open attempts\nTYPE ssh_tunnel_open_count counter\nssh_tunnel_open_count 0\nHELP ssh_tunnel_open_fail_count Counter of ssh tunnel failed open attempts\nTYPE ssh_tunnel_open_fail_count counter\nssh_tunnel_open_fail_count 0```. I tore down the cluster :/ - will test some more and update the ticket when I can replicate again.. ",
    "ezeev": "Looking for guidance on this as well - especially for testing a local heapster build. It would be useful to be able to test against a running cAdvisor.\n. Hi @mrcrgl ,\nWe maintain an image of Heapster that's packaged with a Wavefront sink. We also have a template dashboard you can use:\nhttps://community.wavefront.com/docs/DOC-1204\nI would love to sync up with you to help get this running. I'll also submit a PR with the Wavefront sink.\nThanks,\nEvan. Closed and re-opened so the bot picks up signed cncf-cla. @mrcrgl @piosz - Thank you for getting to this!\nI'm an employee at Wavefront so yes I am quite familiar with it. I would like to be the sink owner or at least collab with @mrcrgl closely.\n@mrcrgl  - thanks for the review! Also - perhaps we can synch together on getting this running in your Wavefront instance?. Hi @mrcrgl \nThanks again for reviewing this. Please take a look at the changes and let me know. Also, I'll send you an email about getting this setup on your Wavefront instance. We're maintaining a fork of Heapster with this sink for users currently.\nThanks,\nEvan. Sorry, when I say \"fork\" - I really meant to say that we're maintaining a container image of heapster packaged w/ this sink (for now).. @mrcrgl @piosz is this ok to merge?. Thanks @DirectXMan12 @mrcrgl \nI'm working on the tests now. They will include a test that ensures metrics are in the proper line format for Wavefront.. @DirectXMan12 any idea what caused this travis-ci error? the last commit was a very small change I tested locally. Did something timeout?. @mrcrgl let me know if this LGTY thank you!. I forced this through. The latest commit is the squashed version.. @DirectXMan12 is this good to go? Let me know if it needs anything else. Thanks!. I'll maintain the Wavefront sink (once it is merged). Thanks @DirectXMan12 - This was definitely not intended. I just corrected this. Cheers.. Hi @DirectXMan12 ,\nIs this good to go? I'd really like to get this in before the next Heapster release.\nCheers. @DirectXMan12 @piosz \nJust looking for someone to take another look at this since I took care of the change request a few days ago.. @DirectXMan12 the last commit squashes the Makefile cleanups.\n. LGTM @basilisk487 @DirectXMan12 . I don't disagree, other than the maps being of different types. We would still need three separate functions because they have different signatures. We could add a single function that accepts a map[string]interface{} then do type assertion. This may be less performant though.. Hi @DirectXMan12 I fixed this, is this good to merge now?. ",
    "codeb2cc": "Are you using Docker 1.11.0? It's probably related to google/cadvisor#1206. Try to build kubernetes from head or just wait for the next release  kubernetes/kubernetes/pull/24113.\n. ",
    "teddymaef": "I have the same problem. the /namespaces/ endpoint always returns []\n. ",
    "k82cn": "It seems the field name was updated in mainline, but the docker image is not updated. Anyway I also updated the template according to current InfluxDB fields.\nhttps://github.com/k82/eos/blob/master/kubernetes/tutorial/monitor/cluster.json\nhttps://github.com/k82/eos/blob/master/kubernetes/tutorial/monitor/container.json\n. ",
    "Malmee": "The data retention Policy can be updated by moving to the influxDB-grafana pod and altering the retention policy. \n. ",
    "RobbieXie": "@Malmee Hi , I am using the heapster API to get performance data of my K8S. I find it only stores 15mins data and i want to know how can i get more data through api, like 1d 's data or 7 days' data.. I got the same issue, filesystem and disk metircs are missing while others seems good. \nAnyone has an solution to get filesystem & disk metrics through heapster?. ",
    "fabxc": "As hinted in #1129 I have my concerns about service-level metrics not being clearly separated. Or better, Heapster not holding up to its intent to not be a general purpose monitoring system.\nThis was previously touched in #665 and I don't feel it ever actually happened. I'm worried that adding a \"generic\" read-path at this point will lead to Heapster eventually growing a full meta-QL for all its sinks. If it's trying to be the system through which writes happen, it just makes sense to read through it as well.\nAside from the scope-issue, this has many semantical issues as different sinks have different evaluation behaviors (e.g. extrapolation), which can have drastic effects.\nAlso all these sinks have different query languages. I see a lot of time being spent working on mapping features onto each other and being a general contention point. You touched that point already at the end of your document.\nI would be very interested in the use case for this and the benefit within the next 6 months.\nAs I've heard there are companies with very large-scale scheduling and auto-scaling that hardly had a need for long-term data.\nMy instinct tells me to defer such features until its absolutely clear what's required.\n. From my understanding the purpose of Heapster is to hold metrics that are relevant for autoscaling and scheduling.\nThis is a fairly recent document stressing that this is the main purpose:\nhttps://github.com/kubernetes/heapster/blob/master/docs/proposals/vision.md#custom-metrics-status\nI'm confused why Heapster needs a feature to push service level metrics through its pipeline if it's a) not its purpose and b) not willing to give any guarantees around it or put effort into it.\nThere are a variety of projects working hard to apply sane semantics and decent APIs for metrics collection \u2013 be that push or pull.\nFor the system-level metrics needed for autoscaling and scheduling it seems very feasible to just pick a single format and either push or pull.\nThe specified performance target will also certainly suffer from people routing all the app-level metrics (which are the majority) through Heapster (for no reason).\nThis proposal seems out of scope to me.\n. I'm sure you have the right intentions here. But people will use what is possible and if enough people use it, eventually development will cater to these uses.\nI'd consider the file pattern @brian-brazil described, which keeps things pull-based.\nTaking this from another angle:\nMaking a clear separation between your regular monitoring and your metrics for auto-scaling/scheduling to me means that they should not end up in the same storage.\nThe overhead of storing them twice (given that you'll probably want them in your normal monitoring too) would be fairly negligible considering your total amount of monitoring data.\nWhen nothing but Heapster has to read or write these metrics, there suddenly seems to be little reason to not have a single backend. The whole concern of different semantics, different feature-sets, and storage guarantees falls away.\nOnly concern left would be deployment of the backend. But if it's just part of Heapster, this seems like not much of an issue.\n. I just want to make clear that I don't have much background in auto-scaling. I'm poking in all these directions because the goals and scope separations seem very blurry to me and I want to better understand the desired and existing state.\nInitially I understood Heapster collected metrics itself because the feedback loop through a monitoring system/TSDB might be too long. That would be an understandable point but it doesn't seem to be the case. Elsewhere 30/60s rounding is mentioned and apparently we want to also read metrics not ingested via Heapster from the monitoring system/TSDB directly.\nNaturally, I also want system level metrics in my monitoring system. So why retrieve them via two different systems, i.e. why does Heapster need a write-path at all? Making the distinction to not collect the same metrics twice introduces a lot of operational complexity.\nNot only are there scope conflicts with people's regular monitoring systems but even directly with cAdvisor, which I've heard also wants to handle application metrics now (different discussion).\nMy concerns about different semantics of underlying storage still hold.\nSomething I do know about auto-scaling that it's incredibly hard to get right and every additional complexity in it might may cause unexpected feedback loops. That makes me question whether enabling more and more things with unclear semantics in quick succession is beneficial for the greater cause.\nThis is a more fundamental discussion and I'm happy to take it elsewhere.\n. Sounds good depending on the time :) That's certainly a better format.\n. I'll probably still be traveling by that time unfortunately.\n. ",
    "mogthesprog": "Hey, we can close this.\nI think there was an issue with the secrets/serviceaccounts in my kubernetes instance. I recreated both of the above and it seemed to work fine. I have a different error now (configuring LDAP Auth). If i have no further joy with it tomorrow i'll raise another ticket.\nCheers\n. ",
    "lxb31": "No reply !?\n. ",
    "brian-brazil": "Reading this I'm very unclear what the goal of this is.\nThe target audience section says this is for cluster-level metrics, and is not for arbitrary cluster users. The rest of the doc, arguments and examples are however for custom pod-level application/user metrics.\nI think we need to step back and see what the real world problem is that we're trying to solve here, and then see what the best approach is. This currently looks like putting Heapster on the path to a fully-general custom metrics solution, which at first glance doesn't seem like a proportionate solution to autoscaling and not something you'd want on a critical control path like that.\nMy straw man proposal would be to have the autoscaler directly scrape what it needs, as the request rate shown in the example is something that the pod will already be exposing in some fashion. This would give us an idea of what the real world usability/auth/maintainability/performance problems are, and then we can design solutions for them rather than prematurely optimising.\n. > The intention (as I attempted to show by using two different pod names) was that it was coming from some sort of cluster-level cache/loadbalancer/etc which provides functionality for the entire cluster.\nThat doesn't change my strawman, I think it makes it a bit more sensible as there's far fewer systems that you'd need to integrate with than I'd initially thought.\nThis raises the question of what you propose to do for application level metrics that aren't going through a loadbalancer or other centralised infrastructure component if this design isn't meant to handle them. For say a batch system you might want resources scaled based on the size of your backlog, which would most likely come from a monitoring system.\n. Prometheus developer here.\nNone of the Prometheus client libraries support timestamps and we don't plan on changing that until https://github.com/prometheus/prometheus/issues/398 is resolved. There is no timeframe on that currently.\nInstead you should do as our clients do and not specify the timestamp, and leave Heapster to use the current time. This also avoids clock sync issues.\n. This path does not follow the Prometheus Pushgateway API, and thus a Prometheus client library can't be used to push to this. If you want to offer this API then it should end with /metrics/job/jobvalue/label1/value1/label2/value2 etc. which will be supported by the various existing Prometheus client libraries.\nThe Go client will push protobuf rather than text, so you should choose the parser based on the Content-Type header. The Go client has the required APIs to handle this for you.\n. You may wish to take a look at how the Prometheus Node Exporter Textfile Collector works as an alternate implementation option. It's how we handle machine level metrics that aren't produced at every scrape, which seems to be the scenario here and has worked quite well for us.\nBasically it's a directory you can dump prometheus text format files into, and they'll be included in /metrics output.\nIn this case the kubelet could pull in files on scrape, and include them with the rest of it's metrics. Depending on directory layout, file permissions and label handling this could remove the need to do explicit auth, users having to figure out which container/pod label to apply and opens options for non-kubernetes systems on the machine to produce metrics.\nFor things you need to set only once this would avoid the need to setup a cronjob or depend on the scheduler to run one (and be in a sufficiently working state to be able to schedule). It also avoids the need for rate limiting, as it's only a write to local disk rather than hitting heapster directly.\n. limited\n. My proposal was to use the kubelet for this, so there's no new daemon.\n. I fail to see how this is the case, you just write them less often - it's the same as what you're proposing in this regard.\n. I'd presume that you'd be bind mounting this, or creating such a directory per container.\n. Above you said that this was for cluster-level metrics, however this is an application level metric. Can you give an example of cluster-level metric that you'd want to push?\n. Would the prefixing approach you're proposing in the main solution not also handle this?\n. Above you indicated that this design is for cluster-scoped metrics, why is this talking mostly about pod scoped metrics?\n. If this is for cluster-level metrics, why are you allowing for pod and container labels to be set?\n. As I've indicated before what you're proposing here is not compatible with the Prometheus ecosystem. You're effectively asking users to hand-produce the Prometheus text format. I don't think is a good idea for either the Kubernetes or Prometheus ecosystems, as it will frustrate everyone involved.\nIf you want to use our format you should be compatible with our Pushgateway's URL pattern, and support both text and protobuf formats.\n. That I can appreciate, though it would be appropriate for node-level metrics.\n. That's the same as the main proposal then. If that's a disadvantage of this approach, then it's also a disadvantage of that.\n. The stated aim of this design is cluster level metrics, service level metrics are not cluster level. Cluster level for me means things about Kubernetes and other related infrastructure of the compute cluster in and of itself.\nSimilarly user pods and services are not cluster components. Cluster components are things like the kubelet and cadvisor. Pods are (usually) not components when we're talking a system like Kubernetes.\nIf what you want to handle are per-pod or per-service user application metrics, then please make that clear throughout the document.\n. You could do it in a number of ways depending on what you want. You could have one per pod, one per node or one per service.\nService ones are actually a bit messy with this idea, though for autoscaling I'd be looking for pod rather than service-level metrics. Service-level metrics imply that aggregation has occurred, which means there's an extra hop in the control loop - which you want to avoid.\n. ",
    "aaskey": "Depending on how you access the Grafana services:\n- If you access directly via end-point. Changing GF_SERVER_ROOT_URL in deploy/kube-config/influxdb/influxdb-grafana-controller.yaml in the following way:\n- name: GF_SERVER_ROOT_URL\n      # value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/\n      value: /\n- If you use the selfLink value from Kubernetes service (You can get it by \"kubectl get services -o yaml\") through API server, there is no need to change the above config.\n. ",
    "davidopp": "What do you mean \"non-Kubernetes\"? Kubernetes runs on CoreOS.\n. Sorry, I was confused. I thought Heapster now doesn't work with Kubernetes on CoreOS. But actually, what doesn't work is Heapster without Kubernetes (on CoreOS). I never knew that ever worked. I was not suggesting we should revive that functionality. Sorry for the confusion.\n. (Thanks to @mwielgus for setting me straight.)\n. LGTM\n.   cc/ @vishh \n. Maybe just \"support 250 node clusters\" instead of <= so people know we will actually support 250.\nSame for <= 1000 later.\n. instead of \"in the following format\" I would say \"from the interval\"\n. instead of \"in the following format\" I would say \"from the interval\"\n. Comments on fields shouldn't reference to Go types, since the comment is turned into swagger documentation and the user doesn't know the Go types. I guess you can say \"corresponding to the one from pod.spec.containers\" -- at least that is a little closer to the json key names (because it uses lower case and omits the v1)\n. ",
    "raliste": "+1\n. ",
    "anzhsoft": "I just got the error message from kube desribe pods\"\n    Warning FailedSync  Error syncing pod, skipping: [failed to \"StartContainer\" for \"heapster\" with CrashLoopBackOff: \"Back-off 5m0s restarting failed container=heapster pod=heapster-v1.1.0.beta1-3608176707-edbv4_kube-system(9efc4773-054b-11e6-9624-6c92bf139d06)\"\n, failed to \"StartContainer\" for \"eventer\" with CrashLoopBackOff: \"Back-off 5m0s restarting failed container=eventer pod=heapster-v1.1.0.beta1-3608176707-edbv4_kube-system(9efc4773-054b-11e6-9624-6c92bf139d06)\"\n, failed to \"StartContainer\" for \"heapster-nanny\" with CrashLoopBackOff: \"Back-off 5m0s restarting failed container=heapster-nanny pod=heapster-v1.1.0.beta1-3608176707-edbv4_kube-system(9efc4773-054b-11e6-9624-6c92bf139d06)\"\n, failed to \"StartContainer\" for \"eventer-nanny\" with CrashLoopBackOff: \"Back-off 5m0s restarting failed container=eventer-nanny pod=heapster-v1.1.0.beta1-3608176707-edbv4_kube-system(9efc4773-054b-11e6-9624-6c92bf139d06)\"\nSo are there any other detailed information to debug this issue , and the kubelet log appears same as the kubectl describe pods' output.\n@zhouhaibing089\n. ",
    "dangzhiqiang": "How to fix it?. ",
    "dcowden": "We need this as well. I'd be willing to help build it if I knew anything at all about how to get started building a sink ( as in-- a doc that describes what interfaces I need to write.  I'm sure that would be obvious if I knew go really well, but i do not ).\n. @wizzu  Thanks, that helps, especially the go-aws-mon link.\nThis is on our k8s implementation list. Hopefully we can knock something out.\n. @wizzu that is the plan for sure. \n. ",
    "wizzu": "CloudWatch sink would be nice, might be useful to me.\n@dcowden --\nLooks like a new sink would need instance functions Name(), Stop(), ExportEvents(batch *core.EventBatch). Also the function CreateMySinkType() to create a new sink instance seems to be a convention, though not required. The EventSink interface is defined in events/core/types.go. You'd also need to update events/sinks/factory.go when adding a new sink type.\nYou can take a look at events/sinks/log/log_sink.go for a simple implementation of a sink.\nThat's all pretty straightforward, the AWS-side of the sink code might be more complex. Found this project https://github.com/a3linux/go-aws-mon which logs metrics to CloudWatch using Amazon's Go SDK (which is rather ugly). Note that it seems to be unmaintained so one of the project forks might be a better reference than the original.\nHope this helps!\n. @dcowden I'm just an interested observer, I guess you mis-attributed the previous comment. :)\nI just came by to note that there's a separate cloudwatch metrics Go lib, https://gowalker.org/github.com/crowdmob/goamz/cloudwatch (part of https://github.com/goamz/goamz) - it might be easier to use something like that than the official AWS SDK. At least, that's my experience from using other AWS services with Go. YMMV.\n. I think this would still be useful. Commenting to avoid avoid-close, hopefully - no idea how else to affect it.. ",
    "alph486": "@dcowden This is good to hear. We have limited resources, but if you aim for the repo to be open to start, definitely let me know here. It would be great to have a fully integrated AWS solution for monitoring and its possible we could dedicate some cycles in the future.\n. Definitely if you have time.\nI recently haven't had time to think about it :-(\nThanks,\nKevin Kuhl\nDevOps - WayBlazer Engineering\nkevin@wayblazer.com\nOn Jan 30, 2017, 4:48 PM -0600, Pavel Sadikov notifications@github.com, wrote:\n\nBump!\nIf needed, I can take a look at writing the sink.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub (https://github.com/kubernetes/heapster/issues/1142#issuecomment-276216942), or mute the thread (https://github.com/notifications/unsubscribe-auth/ABqrPTexhpWdHc8ywwtbgzrgzqq8F4X3ks5rXmiagaJpZM4IPcvY).\n\n. ",
    "farmdawgnation": "Has there been any movement on this?\n. ",
    "naphthalene": "Bump!\nIf needed, I can take a look at writing the sink.. ",
    "nikolay": "It seems neither Amazon, nor Google has any interest in first-class AWS support of Kubernetes.... ",
    "FrederikNS": "Amazon announced that they would start providing a managed version of Kubernetes. Lets hope they implement some stuff for integrating it with CloudWatch\nhttps://aws.amazon.com/eks/\nhttps://techcrunch.com/2017/11/29/awss-container-service-gets-support-for-kubernetes/. ",
    "jnicholls": "Would be interested in seeing this. Also interested in EKS but it's still in preview and not publicly available, thus I'm between a rock and a hard place on running with kops + something in-cluster (would have loved to see a CloudWatch integration) vs. waiting for EKS. . Agreed would like to see this\nOn Sun, May 13, 2018 at 4:39 PM Mikko H\u00e4nninen notifications@github.com\nwrote:\n\nI think this would still be useful. Commenting to avoid avoid-close,\nhopefully - no idea how else to affect it.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/1142#issuecomment-388654395,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AABPq3J9_UQ4oUja6h-XGreOBKt8Vxboks5tyJnvgaJpZM4IPcvY\n.\n-- \nSent from Gmail Mobile\n. \n",
    "007": "/remove-lifecycle rotten. CLA signed. @andyxning updated all. Sorry I was lazy the first time around.\nCC @piosz - is there a release script or other \"do a release\" doc that should be updated so these aren't forgotten?. @andyxning squashed. CLA signed.\n/joke please. Ref #1808 and #1753 - needs to be part of release process.. @piosz @DirectXMan12 @crassirostris Per https://github.com/kubernetes/heapster/pull/1753#issuecomment-321331616 can you update release process / internal scripts / docs / whatnot so that versions of dependent projects and configs get updated consistently for the 1.5 non-beta release?. /remove-lifecycle stale. Closing since https://github.com/kubernetes/heapster/ is deprecated, doesn't matter anymore.. Supporting docs for moving to busybox:glibc and bumping to 5.0.3:\nBuilding on kube-cross -> FROM golang:1.9.3\ngolang:1.9.3 -> FROM buildpack-deps:stretch-scm\nbuildpack-deps:stretch-scm -> FROM buildpack-deps:stretch-curl\nbuildpack-deps:stretch-curl -> FROM debian:stretch\nlibc on stretch is glibc.\nbusybox shows its default is uclibc, but does offer glibc and is even based on debian:stretch-slim.. Single datapoint: this launches and runs (with grafana 5.0.3) on docker 18.03.0-ce on OSX with either libc - the default busybox:uclibc version or busybox:glibc. I don't know how to exercise it further than that, or how to convince it to load the linked libraries and try to execute code with them.. I can update tomorrow, but I'm stuck on how to test the \"static\" linking in sqlite. It's easy to repro the warnings and resulting image, just switch to 5.0.4 and docker build.. @Nodraak Changing -static to -static-all doesn't work, and using the -tags static_all also seems iffy since it's throwing the same warnings. https://github.com/confluentinc/confluent-kafka-go/issues/59#issuecomment-379278816 suggests that it's unstable / unsafe, and suggests the same solution as I indicated previously: it's a Docker image, so pack the appropriate dependencies (i.e. glibc) with it.\nI pushed the changes to use the busybox:glibc version, we'll see how unit + e2e works from there.\nPer k8s-ci-robot suggestion:\n/assign @loburm . Thanks for another good data point!\nI say \"stuck\" in that I don't know EXACTLY where the sqlite code is used, or how I can prove the negative - I'd like to see a +/- where libc explodes with the wrong one and works with the right one, but I couldn't get that to happen with a different busybox libc in my limited use.. @andyxning @loburm any suggestions on how to test this to satisfy lgtm and get it merged?\nUnit and e2e both seem to be working, but it's unclear if they use the built images for the dependencies or if they're pulling whatever base they already have.. See https://github.com/kubernetes/heapster/issues/1917 but realize you probably shouldn't bother with new versions of heapster and should invest your time in implementing prometheus or similar.. ",
    "feelobot": "Its already out btw, If someone could outline the actual implementation details I wouldn't mind taking a look. \nSo far I see this file:\nhttps://github.com/kubernetes/heapster/blob/cd2301d0bca468dff796a9d26bc093efdcc1be2d/events/sinks/influxdb/influxdb.go\nSo I assume it would be a modification of this, the test, and the yaml files.\n. Interesting, I thought it would be something silly but I was told otherwise\nOn Fri, Oct 14, 2016 at 6:05 PM Bekir Dogan notifications@github.com\nwrote:\n\nWe are able to get the Heapster working with InfluxDB 1.0 with below\nchange. Heapster is still trying to use the \"default\" retention policy but\nit's renamed to \"autogen\" in InfluxDB 1.0.\nFrom: influxdata/influxdb#6825\nhttps://github.com/influxdata/influxdb/issues/6825\nThe \"default\" retention policy was renamed to \"autogen\" to avoid confusion\nwhen talking about the \"default\" retention policy versus the default\nretention policy. So now autogen is the default.\nWe are using heapster via this docker image currently in our cluster:\nopenshift/origin-metrics-heapster:v1.3.0 and it has Heapster version\n1.1.0.\nFor now we solved our problem by creating a \"default\" policy in InfluxDB\nwithout doing a modification on Heapster side:\n\nshow retention policies on heapster12_test\nname    duration    shardGroupDuration    replicaN    default\nautogen    0        168h0m0s        1        true\nCREATE RETENTION POLICY \"default\" ON \"heapster12_test\" DURATION INF REPLICATION 1 DEFAULT\nshow retention policies on heapster12_test\nname    duration    shardGroupDuration    replicaN    default\nautogen    0        168h0m0s        1        false\ndefault    0        168h0m0s        1        true\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/1143#issuecomment-253844894,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA25UQXLW-GpH9w67N7meZqxlIgDi4kUks5qz6fRgaJpZM4IP-bp\n.\n. the leading \" actually makes the sync unrecognizable \n. I am using the latest Heapster and 1.0 is already a RC, in the logs you will see the error is from heapster/1.2.0-beta.1\n\nThe reason why this is important is because you can not create any alerts using influxdata's kapacitor running in kubernetes unless influx is on 1.0\n. i think this should be left open personally...there were no breaking changes from Influx 0.13 to 1.0 so you must be using an old client driver or something.\n. No it doesn't\nOn Wed, Aug 31, 2016 at 11:06 Piotr Szczesniak notifications@github.com\nwrote:\n\n@feelobot https://github.com/feelobot do you mean Heapster\nv1.2.0-beta.1 works with Influxdb v1.0.0-rc2?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/1272#issuecomment-243704067,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA25UdRxbgo2DsudqIBbscYi3ilpWBoTks5qlUQdgaJpZM4JvpMu\n.\n. Cool thanks\n. \n",
    "bergerx": "We are able to get the Heapster working with InfluxDB 1.0 with below change. Heapster is still trying to use the \"default\" retention policy but it's renamed to \"autogen\" in InfluxDB 1.0.\nFrom: https://github.com/influxdata/influxdb/issues/6825\n\nThe \"default\" retention policy was renamed to \"autogen\" to avoid confusion when talking about the \"default\" retention policy versus the default retention policy. So now autogen is the default.\n\nWe are using heapster via this docker image currently in our cluster: openshift/origin-metrics-heapster:v1.3.0 and it has Heapster version 1.1.0.\nThis is the heapster request: POST /write?consistency=&db=heapster12_test&precision=&rp=default HTTP/1.1\nreproduced wit curl:\n```\ncurl -i -XPOST \"http://influxdbstg001iad.io.askjeeves.info:8086/write?consistency=&db=heapster12_test&precision=&rp=default\" --data-binary 'mymeas,mytag=1 myfield=91'\nHTTP/1.1 500 Internal Server Error\nContent-Type: application/json\nRequest-Id: 452d0811-921f-11e6-ab6a-000000000000\nX-Influxdb-Version: 1.0.0\nDate: Fri, 14 Oct 2016 15:02:48 GMT\nContent-Length: 48\n{\"error\":\"retention policy not found: default\"}\n```\nFor now we solved our problem by creating a \"default\" policy in InfluxDB without doing a modification on Heapster side:\n```\n\nshow retention policies on heapster12_test\nname    duration    shardGroupDuration    replicaN    default\nautogen    0        168h0m0s        1        true\nCREATE RETENTION POLICY \"default\" ON \"heapster12_test\" DURATION INF REPLICATION 1 DEFAULT\nshow retention policies on heapster12_test\nname    duration    shardGroupDuration    replicaN    default\nautogen    0        168h0m0s        1        false\ndefault    0        168h0m0s        1        true\n```\n. @sstarcher 1.2.0 is the OpenShift's version, it has Heapster 1.1.0 inside their container. \n\nSo, \"Heapster 1.1.0\" works with \"InfluxDB 1.0\" if you create a retention policy with name \"default\" in InfluxDB.\n. ",
    "sstarcher": "I second @bergerx heapster 1.2.0 works with influxdb 1.0 after creating the default retention policy.\n. I'm running kubernetes 1.4.0, heapster v1.2.0, and influxdb 1.0 currently\n. @bergerx thanks did not know that\n. ",
    "gene-telligent": "alternatively -- changing the Heapster retention policy being sent to \"autogen\" (the new alias of the autogenerated retention policy within InfluxDB) should work as well. \n. This is directly related to https://github.com/kubernetes/heapster/issues/1143 . \nThe issue is related to the influxDB default retention policy being changed from \"default\" to \"autogen\". \nYou can fix this by running (in the influxDB instance) \nCREATE RETENTION POLICY \"default\" ON \"k8s\" DURATION INF REPLICATION 1 DEFAULT\nWhich creates that old \"default\" retention policy.\n. ",
    "AlmogBaku": "hi, is this still relevant?. 1. @commixon the original issue of requiring using the ?nodes= notation was solved in #1260 which merged\n2. @douglasader you can only run the right app if you want only metrics/events (heapster - the daemon that push metrics; eventer - the daemon that push events)\n3. I tried to debug it and it seems like the index is changed at least with #1313 (which refactored big parts of the ES code)\n. @Thermi please close this issue, since you were talked about aws\n. cc: @jamiehannaford @huangyuqi do you have any idea regarding this?\n. Hi, here is our findings by far:\n1) AWS ES requires turning off sniffing - https://github.com/olivere/elastic/wiki/Using-with-AWS-Elasticsearch-Service\n2) In order to use AWS ES you need to whitelist your cluster IPs or using the AWS Client with IAM permissions, which is not supported by the current implementation.\n3) Heapster version 1.2 and above ONLY allow using AWS ES due to the updated olivere/elastic version and the support of disabling the sniffing:\n--sink=elasticsearch:?nodes=http://my-cluster.eu-west-1.es.amazonaws.com&sniff=false\n. We still experiencing issues to push to the ES server:\n2016-08-11T12:39:25.468847117Z W0811 12:39:25.468470       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n. Ok, it seems like the \"failure\" message is just a response for not getting a response after some timeout.. it doesn't really indicate about something.\nAlso I've noticed that the eventer do push data.. but the heapster(metric) doesn't, although it say it does.\nI can't really debug this out since it's not really clear for me how to build the heapster \ud83d\ude31 , but by using the code directly in a go script I wrote.. it seems like elasticsearch.SaveDataIntoES does work...\nI'll appreciate some info about building :/\n. It seems like it push it without any special error from the library(i've putted some prints there), but I still can't see it in the ES\n. fixed by #1260\n. $ make\nrm -f heapster\nrm -f eventer\nrm -f deploy/docker/heapster\nrm -f deploy/docker/eventer\nwhich godep || go get github.com/tools/godep\n/Users/AlmogBaku/Documents/Projects/golang/bin/godep\nGOOS=linux GOARCH=amd64 CGO_ENABLED=0 godep go install ./...\ngo install runtime/internal/sys: mkdir /usr/local/go/pkg/linux_amd64: permission denied\ngodep: go exit status 1\nmake: *** [build] Error 1\n. @andyxning I did that from my mac.. I can run it with sudo, but we probably don't want to run this with sudo.\n. ping @piosz @vishh @mwielgus @mvdan \n. @thermi can you help us out here and do a code review also?\n. Godeps/Godeps.json, line 7 [r2] (raw file):\n\nPreviously, piosz (Piotr Szczesniak) wrote\u2026\n\nPlease remove progrium. We no longer use this, though the documentation is outdated.\n\nDone.\n\n\nComments from Reviewable\n Sent from Reviewable.io \n. @piosz I've fixed the issue you raised (and removed the go-ext dependency).\n@huangyuqi @thermi did you had a chance to review it?\n. common/elasticsearch/elasticsearch.go, line 81 [r1] (raw file):\n\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\n\ndone\n\nDone.\n\n\nComments from Reviewable\n Sent from Reviewable.io \n. Review status: 0 of 199 files reviewed at latest revision, 7 unresolved discussions.\n\ncommon/elasticsearch/elasticsearch.go, line 133 [r1] (raw file):\n\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\n\ndone\n\nDone.\n\n\ncommon/elasticsearch/elasticsearch.go, line 142 [r1] (raw file):\n\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\n\ndone\n\nDone.\n\n\nevents/sinks/elasticsearch/driver.go, line 87 [r1] (raw file):\n\nPreviously, huangyuqi (yuqi huang) wrote\u2026\n\nYes, i agree, warning is more precise. thanks\n\nDone.\n\n\nGodeps/Godeps.json, line 7 [r2] (raw file):\n\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\n\nDone.\n\n@pio\n\n\nmetrics/sinks/elasticsearch/driver.go, line 102 [r1] (raw file):\n\nPreviously, AlmogBaku (Almog Baku) wrote\u2026\n\nI changed it to warning.. It make more sense to me. What do you think?\n\nDone.\n\n\nComments from Reviewable\n Sent from Reviewable.io \n. http://thomasardal.com/terms-aggregations-on-analyzed-fields-in-elasticsearch/\n. @huangyuqi what do you think about that?\n. @piosz  @mksalawa what about eventer? it's sounds to me like a similar solution for eventer could be great\n. cc @huangyuqi @piosz \n. @piosz @huangyuqi can you help me figure out the questions above?\n. I believe it's increase the storage.. but probably not much.\nQuestions:\n1) Should we keep the original \"capacity entry\"? or if we combine this information anyway we can remove it..\n2) Should we also attach \"units\" to the entries?\n3) Which entries are related to each other? (I'm not sure even if i was right in my example)\n. Following up to my conversation with @DirectXMan12 I came to the conclusion that a change in the schema is required.\nInstead of one \"type\" for all the metrics, we need a type for each \"metric family\"(cpu/filesystem/memory/network; and one for the rest - \"general\"; which include the \"custom\" family and uptime)\nThis change is basically not very complicated to achieve(since the DataBatch is already organized/aggregated by entities), however it'll be breaking change(and consider the fact this is a VERY new feature, it probably won't affect many people)\n. Actually, as I see now- the current eventer implementation doesn't even have an http server..\nI'm not sure if we want to create an http server just for this purpose..\n@DirectXMan12 what do you think?\n. Hi,\nI have solved that in PR #1313 \nWaiting for someone to review it for almost a month...\n@commixon it'll be very helpful if you'll help us reviewing :D\n. for some reason the tests takes too long, and it caused travis to fail..\n. @huangyuqi how can we push this forward?\n. signed the linux foundation cla\n. @piosz @huangyuqi ? how can we push this forward?\n. I fixed an issue where some of the points haven't committed due to not passing the threshold of the bulk-processor..\nNow it's flush on one of the following scenarios:\n- every 10s\n- 2MB of data\n- >= 1000 requests\n- end of the processing(end of ExportData or ExportEvents)\n. @piosz done.\n. Your test run exceeded 120 minutes.\nCan someone try to fix this?\n. @piosz @huangyuqi this can be solved by using glide #1314 (or specifically see my review comment in #1335 )\n. @commixon, have you had the chance to review it?\n. @huangyuqi have you had a chance to review it?\n. @thermi if you can export the views you created and attach them here, it'll be very useful. (i don't think we have anything sensitive there right?)\n. Review status: 0 of 100 files reviewed at latest revision, 3 unresolved discussions.\n\ncommon/elasticsearch/elasticsearch.go, line 37 at r4 (raw file):\n\nPreviously, andrejvanderzee wrote\u2026\n\nWhy underscore instead of camelcase?\n\nDone.\n\n\ncommon/elasticsearch/elasticsearch.go, line 44 at r4 (raw file):\n\nPreviously, andrejvanderzee wrote\u2026\n\nThis should be esSvc and not esConfig like below. Probably also better to use a pointer type as a receiver like below.\n\nDone.\n\n\ncommon/elasticsearch/mapping.go, line 177 at r4 (raw file):\n\nPreviously, andrejvanderzee wrote\u2026\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-all-field.html\n\nDone.\n\n\nComments from Reviewable\n Sent from Reviewable.io \n. @k8s-bot test this\n. @piosz, few people already review the code and that it functioning(see above).. can we merge this?\n. @nhlfr why even committing the vendor dir?\n. A success story with heapster can push things forward with the big project of k8s... :D\n. Also there are other k8s projects that already uses glide, such as helm\n. @DirectXMan12 @piosz @nhlfr as I wrote in #1314 there is no need to commit vendor directory at all.. that's one of the great benefits of glide.. specify semver versions for deps, and just do glide install and voila!\n. @nhlfr can you fix the PR? it looks like the tests failed\n. if im not mistaken so as godep... so why usage of hg natively here doesnt work?\n. sounds very strange... since im pretty sure that godep required me to install hg..\ncan we re-run it again? or at least just fix the integration test?\n. ping @nhlfr . This is not relevant for InfluxDB.. and afaik HPA is not depends on any sink.... @piosz @huangyuqi this can be solved by using glide #1314 (or specifically see my review comment in #1335 )\n. Duplicated of #1310 , implemented on #1313 \n_Would love and appreciate if you can help us and review this PR in order to push it forward..._\nThis is currently stuck because nobody can review it...\n. @andrejvanderzee that's exactly what this PR do.\nYou can help by:\n1. reviewing the code I've committed in the PR and to give a feedback if you have\n2. build an image with this PR, deploy it, and test it(basically see if any major issues occurs)\n. @thermi can we do that?\n. @andrejvanderzee would you kindly post your feedback(and continue the discussion) in the PR #1313 \n. this happened to us also with Metrics.memory/page_faults_rate.value\n. @thermi is that the same scenario?, maybe the problem is with Systemd version? maybe the new coreos version solves it also for us?\n. Can you add docs? Also I think adding that to metrics will be also useful\nin this scenario\nOn Thu, 24 Nov 2016 at 0:01 Ning Xie notifications@github.com wrote:\n\n@piosz https://github.com/piosz done\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1367#issuecomment-262640562,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAGCpu39WT7iUeDRxy5D2uCzXJlGT3UBks5rBLeigaJpZM4Kqesu\n.\n. Hey,\nThat's looks okay, however I'm not sure why is that required? anyway PRs\nfails in case the code isn't gofmtted..\n\nOn Sun, Nov 6, 2016 at 1:46 PM, Ning Xie notifications@github.com wrote:\n\n@piosz https://github.com/piosz @AlmogBaku\nhttps://github.com/AlmogBaku\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1368#issuecomment-258675799,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAGCpqWCRKACloufHu2cTbhpitXrcTbpks5q7b4zgaJpZM4KqfIQ\n.\n\n\nhttp://www.rimoto.net/\nwww.rimoto.com http://www.rimoto.net/\nAlmog Baku\nCTO & Cofounder *\nMobile: +972.50.2288.744\nSocial:  * http://www.facebook.com/AlmogBaku\nhttp://www.linkedin.com/in/almogbaku\n. LGTM\n. That's an interesting approach, i solved that slight differently in #1313 \n. I think it's now redundant after #1313\nIf you see any reason to implement it this way other than the method i\nused, can you elaborate?\nOn Wed, 23 Nov 2016 at 15:20 Piotr Szczesniak notifications@github.com\nwrote:\n\nCan anyone review it? @AlmogBaku https://github.com/AlmogBaku?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1379#issuecomment-262510601,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAGCpvHiwkgtYP5Mr731TDDjMSC1hZIyks5rBD2FgaJpZM4Kw6g_\n.\n. why can't you use a different index?. yep see: https://github.com/kubernetes/heapster/blob/master/docs/sink-configuration.md#elasticsearch. index is like a \"database\"..\nif you have multiple k8s clusters, and you want to use the same ES cluster,\njust use a different index\n\nOn Mon, Nov 21, 2016 at 2:06 PM, Ning Xie notifications@github.com wrote:\n\nyou mean that we should change to another index instead of Heapster? Am\nyou worried that this change will break backward capacity or something\nelse? Sorry for not get the point why we should change to another index. :)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1380#issuecomment-261919848,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAGCptFyKOX3NJyBaF-pH9IEWPYe46uoks5rAYlXgaJpZM4KyMoZ\n.\n\n\nhttp://www.rimoto.net/\nwww.rimoto.com http://www.rimoto.net/\nAlmog Baku\nCTO & Cofounder *\nMobile: +972.50.2288.744\nSocial:  * http://www.facebook.com/AlmogBaku\nhttp://www.linkedin.com/in/almogbaku\n. \n\nReview status: 0 of 8 files reviewed at latest revision, all discussions resolved.\n\nComments from Reviewable\n Sent from Reviewable.io \n. #1313 (which was merged) solved that. #1380 . LGTM. @DirectXMan12 can we merge this?. Thanks for the honor \ud83d\udde1 \nI'll be glad to do that :). @piosz @DirectXMan12 is that means we'll get writing privileges ?. who have write permissions other than @piosz ?. Upgrading to ES5 will break the current support of ES2...\nI think that  for the time being we need to support both of the sinks version..\nAlso, It looks like we'll have to replace only the ES driver and not the eventer/heapster sinks, since it's modular.\nCurrently I don't have the time/resources to build the ansible script to install ES5, so I'll have to wait for ES team to update the script in order to create a new ES Cluster... https://github.com/olivere/elastic/issues/431. @rikatz the discussion on the issue above means that we need to create a different sink for es5; however all the drivers can be the same with the current implementation since they abstract. @piosz ?. @DirectXMan12 @piosz ??. Any update?. 1) Where do you fill the data in the driver? you haven't committed any changes to the eventer driver...\n2) Why do you need the RAW data of this field.. it's seems classy just for search.... @andyxning I can't see anywhere in the driver.go where it commit the labels into this field...\ngo\nfunc eventToPoint(event *kube_api.Event, clusterName string) (*EsSinkPoint, error) {\n    point := EsSinkPoint{\n        FirstOccurrenceTimestamp: event.FirstTimestamp.Time.UTC(),\n        LastOccurrenceTimestamp:  event.LastTimestamp.Time.UTC(),\n        Message:                  event.Message,\n        Reason:                   event.Reason,\n        Type:                     event.Type,\n        Count:                    event.Count,\n        Metadata:                 event.ObjectMeta,\n        InvolvedObject:           event.InvolvedObject,\n        Source:                   event.Source,\n        EventTags: map[string]string{\n            \"eventID\":      string(event.UID),\n            \"cluster_name\": clusterName,\n        },\n    }\n    if event.InvolvedObject.Kind == \"Pod\" {\n        point.EventTags[core.LabelPodId.Key] = string(event.InvolvedObject.UID)\n        point.EventTags[core.LabelPodName.Key] = event.InvolvedObject.Name\n    }\n    point.EventTags[core.LabelHostname.Key] = event.Source.Host\n    return &point, nil\n}. I don't think this information is available without changing the functionality.. and I don't know for myself how you can dig this information.\nI guess that for the time being we can close this PR.\n. @rikatz please tag me there. LGTM. you should use the beta release. ES5 is still WIP, but I hope it will support it. This is a proof why we need glide ASAP. Godep makes the development process a nightmare.\n1314 . can you see the data in your kibana?. TBH, I don't really understand why this is happening.. especially since the pushing to the ES is async.... Also, I can't reproduce it. es 5 is not officially supported yet. please send more detailed logs(verbosity enabled), and open a different issue... since it's probably not the same\nhttps://github.com/kubernetes/heapster/blob/master/docs/debugging.md. can someone provide steps to reproduce? I can't reproduce this in my environment. @rikatz Can't we achieve the same without duplicating the whole packages? that's why there are interfaces.... also, it's not clear to me why have you duplicated also the drivers which are basically the exact same code. Ok, i've created a prototype that works with my approach.. I'll test it later (hope tomorrow). Damn, i'm really struggling with this godep. That's stopping me now from sending the PR\nahm #1314 @piosz . I also worked on a solution for that.. I still have few bugs I need to\nsolve before I can publish it..\nI'll be able to work on it after the MWC event.\nOn Tue, Feb 21, 2017 at 10:50 PM andrejvanderzee notifications@github.com\nwrote:\n\nWhat is the state on this PR. My company will migrate to ES v5 soon too.\nIs the PR working? Can I help out somehow?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1464#issuecomment-281475809,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAGCpo4X9TOdoK4abmEMa1xFSdtAcjbvks5re04CgaJpZM4LkH24\n.\n-- \n\nhttp://www.rimoto.net/\nwww.rimoto.com http://www.rimoto.net/\nAlmog Baku\nCTO & Cofounder \nMobile: +972.50.2288.744\nSocial:  * http://www.facebook.com/AlmogBaku\nhttp://www.linkedin.com/in/almogbaku*\n. cc @rikatz . the test log is odd..... This is still WIP. LGTM. Sgtm. OP please address @andyxning notes. @astropuffin can your please sign the cla?. why not just do kubectl apply -f /dir/?. /lgtm. new metrics should be added to FilesystemMetrics too. /lgtm. /assign @DirectXMan12 @piosz  \nPlease merge :). As we discussed before - I'm not in favor of providing CA file or bypassing the check. As a solution, you can attach your CA to the container's known CAs by the kubernetes manifest.\n/close. /assign @AlmogBaku @andyxning @huangyuqi. @rikatz can you help us out with reviewing this?. I added a support for Ignest pipeline.. @rikatz any update?. @rikatz \n1) please double check your config\n2) please send me logs with verbose enabled (as described here https://github.com/kubernetes/heapster/blob/master/docs/debugging.md )\n3) do you use usr/pwd?. @rikatz ?. @chancez I guess that's belong to a different PR... @rikatz \n1) that's what we do right now- we create the mapping in advance.\n2) I think the reason we created different timestamps was to allow you to create different \"types\"/\"index patterns\" in the kibana dashboard.. because otherwise you just can't see the other fields\ntake a look at #1313 \nPlease open a new ticket, since it's not related to this\n@bsafwen it's also related to my comment above, you should configure the different types..\n@chancez personally, I don't think you should disable it in any case.. you should just add your self-signed CA.. anyway - please open a new issue\nIt seems to me like it's okay to merge now @DirectXMan12 . because the index is with date pattern..... /assign @DirectXMan12 @piosz . /retest. Hallelujah!\nOn Fri, Jun 23, 2017 at 9:51 PM Solly Ross notifications@github.com wrote:\n\nThanks everyone for all their hard work on this :-)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1607#issuecomment-310745318,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAGCpoaNcw6wBGcbA2uZMuRjeWtjAbafks5sHAkegaJpZM4NFmG0\n.\n-- \n\nhttp://www.rimoto.net/\nwww.rimoto.com http://www.rimoto.net/\nAlmog Baku\nCTO & Cofounder \nMobile: +972.50.2288.744\nSocial:  * http://www.facebook.com/AlmogBaku\nhttp://www.linkedin.com/in/almogbaku*\n. /assign @DirectXMan12 . @k8s-bot test this    . @k8s-bot test this. @k8s-bot test this. @k8s-bot test this. Huh... I just saw now that's this has been fixed already on master. Why have you deleted this field from the es mapping?\nOn Thu, 25 May 2017 at 18:24 Solly Ross notifications@github.com wrote:\n\n@DirectXMan12 https://github.com/directxman12 requested your review on:\nkubernetes/heapster#1661\nhttps://github.com/kubernetes/heapster/pull/1661 fix docs&&misc about\ndelete pod_namespace.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1661#event-1097478202, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAGCponRmQrQ5xU8LtrsNQV1yPBr_sbAks5r9Z0PgaJpZM4Nl72a\n.\n-- \n\nhttp://www.rimoto.net/\nwww.rimoto.com http://www.rimoto.net/\nAlmog Baku\nCTO & Cofounder \nMobile: +972.50.2288.744\nSocial:  * http://www.facebook.com/AlmogBaku\nhttp://www.linkedin.com/in/almogbaku*\n. Personally, I'm against this idea. That's sound very insecure to me.\nIf you want to add a self-signed SSL, add it's CA to the known lists (you can do that by mounting a volume to the /opt/certs/ dir).. It make more sense to me to specify an external CA then just \"skip\" the CA verification...\nFor development use-cases just use http if you too lazy to install the CA(and don't get me wrong.. I'm lazy too, so that's exactly what I would have done). @andyxning @huangyuqi @piosz I'd love to hear your thoughts. The reason we have multiple timestamps is to allow Kibana create different\ntypes...\nOn Mon, Jun 26, 2017 at 6:09 PM Ricardo Katz notifications@github.com\nwrote:\n\n@AlmogBaku https://github.com/almogbaku need your opinion in this :)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/1701#issuecomment-311088127,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAGCpgmBBiC8Ds6EA3CIHou1uSAnx2DIks5sH8magaJpZM4OFciK\n.\n-- \n\nhttp://www.rimoto.net/\nwww.rimoto.com http://www.rimoto.net/\nAlmog Baku\nCTO & Cofounder \nMobile: +972.50.2288.744\nSocial:  * http://www.facebook.com/AlmogBaku\nhttp://www.linkedin.com/in/almogbaku*\n. I don't have an example.. but I guess adding an extra field couldn't hurt..\nHowever, it's interesting to try and see if having a unified timestamp outside of the types will also work for Kibana.. (maybe I was wrong.. I don't really recall how it works right now). 1. using keyword make sense\n1. what are the benefits of having an index per metric type?\n1. I'm still not sure how and if you can configure multiple types with only one timestamp on kibana (and also for older kibanas then version 5)\n1. I agree that we need to minimize the breaking changes and to try to create one stable mapping. We might send multiple PRs, but all of them should be released at the same time.. let's continue the discussion in #1909 and decide what's the new schema will be.\n/close. can we close it?. can you please send a ref to the ES documentation about that?. It was implemented originally without the date on the index, although that's a common practice to create date indexes.. and Kibana already knows how to aggregate them.. Yeah... I still don't understand why should we change it?\nThat's the common practice. so let's just add another alias?.. we'll have both \"hepaster-\"(o), \"heapster-events\"(a), \"heapster-events-\"(a)\nthis way we don't break anything, and it still supports kibana and fast removal of old data.. O-Original, a-alias\nOn Thu, 10 Aug 2017 at 3:20 Ning Xie notifications@github.com wrote:\n\n\"hepaster-\"(o), \"heapster-events\"(a), \"heapster-events-\"(a)\nWhat is the meaning of (o), (a). Could you please detail it? @AlmogBaku\nhttps://github.com/almogbaku\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1735#issuecomment-321416160,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAGCpr0MpOMPlHn1d0es52JtsrpnVWC5ks5sWkzIgaJpZM4OgLT2\n.\n-- \n\nhttp://www.rimoto.net/\nwww.rimoto.com http://www.rimoto.net/\nAlmog Baku\nCTO & Cofounder \nMobile: +972.50.2288.744\nSocial:  * http://www.facebook.com/AlmogBaku\nhttp://www.linkedin.com/in/almogbaku*\n. good point.\ndo you see any scenario which this kind of change will break stuff? if so let's try to have 2 aliases(because why not?).. if not- I agree, lets change it. TBH, I'm still not sure what's the use case you trying to solve here...\nOn Sun, Aug 13, 2017 at 5:21 AM Ning Xie notifications@github.com wrote:\n\nI have only one usage by alias for eventer, i use it to search\nsomething, i.e., alias(or index)/type/_search.\nAccording to the ES alias doc\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html#indices-aliases,\nwhere an index is used in es, it is ok to replace the index with an alias.\nSo, i think we directly change to heapster-events alias name is fine and\nhave no backward incompatibility.\nBTW, at the last of the alias doc, it is said that An alias cannot have the same name as an\nindex.. I am not sure what is the consequence about having an alias and\nindex with the same name, as currently we did having the same name for\nindex and alias and it seems we have no problem. But in order to have an\nhealthy ES usage, we should follow the directions in doc. So, just for\nthis, we also should have a different alias name for index. :)\nI have not used heapster with ES, so i am not sure changing alias name\nfrom heapster-events-xxxx.xx.xx to heapster-events is ok. But imo, this\nshould also be fine, as alias can be expanded by es automatically.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1735#issuecomment-322017526,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAGCpnQwUsthlQa6vlWt3lHZdjDUT8huks5sXl2UgaJpZM4OgLT2\n.\n-- \n\nhttp://www.rimoto.net/\nwww.rimoto.com http://www.rimoto.net/\nAlmog Baku\nCTO & Cofounder \nMobile: +972.50.2288.744\nSocial:  * http://www.facebook.com/AlmogBaku\nhttp://www.linkedin.com/in/almogbaku*\n. ok, make sense. sounds good :). /lgtm. I'll try to find the time to make it supports ES6. If someone can pick it up please comment. Guys we need to decide what the new schema should be for ES6 support. You're welcome to raise ideas here. Sniff is enabled by default, However it doesn't supported by AWS ES Services (see https://github.com/kubernetes/heapster/issues/1242). Therefore, in case of usage of AWS I automatically disabled this feature.\n. Hi,\nI personally recommend about deploying it into an external service rather than a k8s service to avoid a case of single-point-of-failure (especially when the ES used to help you track/resolving issues in the cluster...), but you can do both of course\nRegarding the deployment, you can set the ENV by the k8s manifest.. for more details see what I wrote in the documentation of the ES Sink(https://github.com/AlmogBaku/heapster/blob/es_sink_improvments/docs/sink-configuration.md#aws)\n. (I just moved this block to line 144)\n. done\n. done\n. done\n. I changed it to warning.. It make more sense to me. What do you think?\n. I changed it to warning.. It make more sense to me. What do you think?\n. I tried to find some way to collect the AWS information from this secret, but couldn't find anything sufficient..\nThe only idea I had(for more secured access) is to do an HTTP request to AWS endpoint in order to get the temporary AWS token associated with an EC2's role.. however that's won't work in cases of a server that doesn't uses AWS(for federation cases for instance, or for using AWS as a secondary cloud sink). Also, this method will require maintaining some mechanism of leasing, and renewing from the AWS token's endpoint, inside the heapster(=very big feature).\nDo you have a different idea how to store that?\n. I'm still not sure what the difference between exposing it via ServiceAccount's secret, and a regular secret.. also I'm not sure how I access the ServiceAccount's secret.. via reading the file /var/run/secrets/kubernetes.io/serviceaccount/aws_access_key?*\nAlso, these ENVs are the \"common practice\" by AWS GO SDK, I tried to align with them here(both with names and recommendations)\n*The documentation of k8s for SA is pretty lack of info..\n. @piosz my bad. fixed.\n(I used it to deploy to my local registry and test)\n. should be glide install\n. should be\nGOOS=linux GOARCH=amd64 go test --test.short -v -race `glide novendor` $(FLAGS)\nThis will prevent testing the vendored libs.\n. why is that needed if we are not going to commit the vendor dir?\n. you should add this tag under \"customMetricTypeSchema()\" and not specifically for the general family. I think we should call it ClusterName and not just cluster.. it'll be easier to understand imo. I think we should call it cluster_name and not just cluster.. it'll be easier to understand imo. also, as i mentioned- should be CluserName imo. I think we should call it ClusterName and not just cluster.. it'll be easier to understand imo. I think we should call it cluster_name and not just cluster.. it'll be easier to understand imo. values should be added to all the metrics under \"metricstags\"... can you add a test with a different cluster_name value?. please fix indentation :). please fix indentation. and use cluster_name (to keep a common case for the tags).  and use cluster_name (to keep a common case for the tags). Let's use cluster_name (to keep a common case for the tags). Let's use cluster_name (to keep a common case for the tags). Let's use cluster_name (to keep a common case for the tags). Let's use cluster_name (to keep a common case for the tags). why is it empty? shouldn't the default set to \"default\"?\nalso, can you also modify the \"Events\" test. Oldtimer was misspelled. ",
    "devlina": "@mwielgus I am facing similar issues, can not see any pod metrics in grafana [#26769].(https://github.com/kubernetes/kubernetes/issues/26769) Looks like I do not have api/v1/model/debug/allkeys endpoint defined. Getting logs at /metrics endpoint though.\nA side thought: I am running cluster on ubuntu, do you think platform might have something to do with the issue. Would moving to CoreOS help?\n. ",
    "somejfn": "I think I have the same issue but do not have a working heapster setup to compare with.... what would be a typical output of /api/v1/model/debug/allkeys  ?\nHere's mine:\n[\n  \"node:10.3.22.70/container:docker-daemon\",\n  \"node:10.3.22.72/container:kubelet\",\n  \"node:10.3.22.72/container:docker-daemon\",\n  \"node:10.3.22.73\",\n  \"node:10.3.22.73/container:docker-daemon\",\n  \"node:10.3.22.73/container:kubelet\",\n  \"node:10.3.22.70\",\n  \"node:10.3.22.70/container:kubelet\",\n  \"node:10.3.22.72\",\n  \"cluster\"\n ]\nOn k8s v1.2.4, coreos alpha 1097 and docker 1.11 \n. ",
    "linhbombk": "Any updates? I'm facing the similar issue with heapster can collect metrics nodes level but namespace,pod not.\nThe output of heapster_ip:8082/api/v1/model/debug/allkeys only shows cluster and node infomation.\nI can query namespace,pod,container... directly from kubernetes master.\nk8s v1.2,heapter image: canary,docker 1.11.2,Centos 7.\n. ",
    "suyogbarve": "Any update I am facing same problem under similar configuration\n. ",
    "nwind21": "I have a similar issue.  I used kube-aws to setup which by default has a heapster monitoring, tried the kubernetes/heapster instructions but that didn't pick up the pods nor cluster, and ended up taking the yaml's from kubernetes/cluster/addons/cluster-monitoring/influxdb.\nI got node cluster data but no namespaces nor pods which seems to match what others are seeing on this issue report.  I checked cAdvisor and I think everything looks good.\n- k8s: 1.3.6\n- coreos: 1122.3.0\n- docker: 1.10.3\n- heapster: gcr.io/google_containers/heapster:v1.1.0\n- heapster source:  kubernetes.summary_api:''\nNote, I did try heapster:canary and it didn't seem to scrape data to influx.  I also tried different sources, but my understanding of the sources isn't clear so it could've been something else.\n. ",
    "ajaybhande": "I see same issue with Kubernetes 1.4.6 and Heapster v1.2.0. Also, does not work with heapster-amd64:v1.3.0-beta.1.\nThe /api/v1/model/debug/allkeys shows only node and cluster.\n[\n  \"node:10.22.152.159\",\n  \"cluster\"\n ]\nHeapster logs show something like below for Pod and proper number for Node:\nreflector.go:403] k8s.io/heapster/metrics/heapster.go:319: Watch close - *api.Pod total 0 items received\nWhat could be problem?\nThank you!\n\nUpdate:\nIssue was https://github.com/kubernetes/kubernetes/issues/33192\nAfter, restarting kubelet pod metrics shown as expected. . ",
    "shilpamayanna": "@piosz I would like to contribute for this issue. \n. ",
    "jsoriano": "I signed it!\n. @piosz time type implements String(), so I think it's fine with %s, this is how it appears in logs:\nExported 4001 events to riemann in 162.355751ms\n. Thanks! :)\n. Reviewed 3 of 3 files at r1.\nReview status: all files reviewed at latest revision, 4 unresolved discussions, some commit checks broke.\n\nevents/sinks/riemann/driver.go, line 1 at r1 (raw file):\n\nGo\n// Copyright 2014 Google Inc. All Rights Reserved.\n\n2016?\n\nevents/sinks/riemann/driver.go, line 48 at r1 (raw file):\n\n``` Go\n}\ntype riemannConfig struct {\n```\n\nIt'd be nice to create a k8s.io/heapster/common/riemann package to avoid duplicating code between events and metrics sinks.\n\nevents/sinks/riemann/driver.go, line 167 at r1 (raw file):\n\nGo\n              \"component\":        event.Source.Component,\n          },\n          Metric: riemannValue(event.Count),\n\nDoes this event count make sense as metric? (Just asking, I don't have a better proposal in any case :))\n\nevents/sinks/riemann/driver_test.go, line 60 at r1 (raw file):\n\n``` Go\n}\n// Returns a fake kafka sink.\n```\n\nkafka? :)\n\nComments from Reviewable\n Sent from Reviewable.io \n. @shmish111 please, update sinks documentation too\n. \nReviewed 5 of 5 files at r2.\nReview status: 6 of 7 files reviewed at latest revision, 8 unresolved discussions, some commit checks failed.\n\nevents/sinks/riemann/driver.go, line 32 at r3 (raw file):\n\n```Go\n// Abstracted for testing: this package works against any client that obeys the\n// interface contract exposed by the goryman Riemann client\n```\n\nThis comment was for the interface you have moved to common, move also the comment please :)\n\nevents/sinks/riemann/driver.go, line 125 at r3 (raw file):\n\n```Go\n}\nfunc (sink *RiemannSink) sendData(dataEvents []riemann_api.Event) {\n```\n\nsendData could be moved to common, it's the same for metrics and events, isn't it?\n\nevents/sinks/riemann/driver_test.go, line 61 at r3 (raw file):\n\n```Go\n}\n// Returns a fake kafka sink.\n```\n\nkafka still here :)\n\nmetrics/sinks/riemann/driver.go, line 31 at r3 (raw file):\n\n```Go\n// interface contract exposed by the goryman Riemann client\ntype RiemannSink struct {\n```\n\nThis struct is defined three times, maybe a better abstraction in common is possible.\n\nComments from Reviewable\n Sent from Reviewable.io \n. I have added some comments to the review. Thanks for your changes @shmish111 !. @shmish111 are you using Riemann sinks? we were thinking about deprecating them if they are not being used, you can see the conversations about that in #1407 and #1419, we are waiting for your opinion.\nThanks!. \nReviewed 1 of 1 files at r3, 4 of 4 files at r4.\nReview status: all files reviewed at latest revision, 11 unresolved discussions, some commit checks failed.\n\nComments from Reviewable\n Sent from Reviewable.io \n. @shmish111 about the CLA, I don't know if it's related, but is it possible that you are signing it with a different email than the used in the commit messages?. I think this PR can be closed as #1591 was already merged.. Thanks @stevezau! Before this we were using riemann sink and a riemann daemon configured to forward to graphite (well, and we are still using the riemann solution in production)\n. Oh, I'll take a look.\n. I have added the code with the fix for udp by @theairkit , thanks!. @theairkit good catch! Thanks. Branch updated.. Thanks for so extensive testing @theairkit !. I wouldn't say that I'm actively maintaining the Riemann sink :) But sure, you can ping me if reviews are needed.\nAnd in principle I could also help with Graphite sink if #1341 is merged. Maybe you can also ask to @theairkit after his great work reviewing and testing my PR.. We are using it to forward metrics to Graphite, but in parallel we are also testing the Graphite sink of the PR and we could really promote it in any moment and stop using the Riemann one, so no problem for me to deprecate it.\nThere is an open PR (#1339) by @shmish111 for the events sink, I don't know if he is using heapster with Riemann for metrics too.. @jamtur01 @mcorbin great! If you have any doubt with the code of this sink I can try to help :)\nThere is also an open pull-request for the events sink (#1339). Hi @DirectXMan12,\nI had a local clone with all changesets, I have pushed it to https://github.com/jsoriano/goryman, we could use it while we deprecate the sink.\nIn any case, @cjgdev, as the last committer to goryman, do you know why the repository is not available anymore?\n. @DirectXMan12 LGTM, thanks @rikatz !. @rikatz you could maybe squash the three changesets into one. @rikatz umm, there are 5 changesets now, take a look to this http://stackoverflow.com/questions/5189560/squash-my-last-x-commits-together-using-git. @rikatz it seems that you did it right as everything seems to be in 21248d80dfd8762a38f4805182f80a12c50342ab but you merged before pushing instead of forcing the push, this should do the trick:\ngit reset --hard 21248d80dfd8762a38f4805182f80a12c50342ab\ngit push -f\n. @rikatz but no new PR is needed, 21248d80dfd8762a38f4805182f80a12c50342ab was fine, just go to your riemmanlib branch and run these two commands, this will be ok :). If we finally keep the riemman sinks I could take a look to this lib. It seems to have a quite similar interface.\nBut while we decide what to do I think that the best option is to merge your PR.. Perfect :) thanks. LGTM. @DirectXMan12 it seems that e2e tests failed starting the test cluster, is there any known problem? can tests be launched again?. @k8s-bot test this. In case it gives any clue... where I'm using the riemann sink to forward to graphite I also see this behaviour, kubernetes.X.cluster.cpu.usage_rate doubles any of the sums of nodes, pods or namespaces: \n sum(kubernetes.X.nodes.*.cpu.usage_rate)\n sum(kubernetes.X.nodes.*.pods.*.*.cpu.usage_rate)\n* sum(kubernetes.X.namespaces.*.cpu.usage_rate)\n@DirectXMan12 could you find someone to try it with other sinks to see if it also happens?\nThe sink is not doing any operation with the data provided... I'm wondering if the problem could be in the cluster aggregator.. LGTM. \nReviewed 17 of 17 files at r1.\nReview status: all files reviewed at latest revision, 4 unresolved discussions.\n\ncommon/riemann/riemann.go, line 17 at r1 (raw file):\n\n```Go\npackage riemann\nimport (\n```\n\nPlease, organize imports in groups according to goimports (https://github.com/golang/go/wiki/CodeReviewComments#imports), and do the same in all edited files.\n\nevents/sinks/riemann/driver_test.go, line 134 at r1 (raw file):\n\nGo\n      Timestamp: timestamp,\n      Events: []*kube_api.Event{\n          &event1,\n\nSet these events here inline directly?\n\nmetrics/sinks/riemann/driver.go, line 27 at r1 (raw file):\n\n```Go\n// contains the riemann client, the riemann configuration, and a RWMutex\ntype RiemannSink struct {\n```\n\nCan we reuse the definition in common?\n\nvendor/github.com/riemann/riemann-go-client/.gitignore, line 3 at r1 (raw file):\n\nriemann.log\nriemann-*\nvendor\n\nI don't think we want to ignore vendor directory.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 5 of 5 files at r2.\nReview status: all files reviewed at latest revision, 1 unresolved discussion.\n\nvendor/github.com/riemann/riemann-go-client/.gitignore, line 3 at r1 (raw file):\nPreviously, mcorbin (Mathieu Corbin) wrote\u2026\nI'm pretty new to Go, some projects consider pushing the vendor directory a bad practice, some projects dont. I don't really know what to do with the Riemann Go client.\n\nIn heapster this directory is commited into the repository.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: all files reviewed at latest revision, 1 unresolved discussion.\n\nvendor/github.com/riemann/riemann-go-client/.gitignore, line 3 at r1 (raw file):\nPreviously, mcorbin (Mathieu Corbin) wrote\u2026\nYes, but here it's riemann-go-client vendor directory, so it should not impact Heapster (i can not find a Heapster dependency in vendor with it's own vendor directory). Furthermore, riemann-go-client depends only on protobuf.\n\nOh, you're right, my apologies, I was thinking it was the .gitignore of heapster repository.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \n\nReview status: all files reviewed at latest revision, 1 unresolved discussion.\n\nComments from Reviewable\n Sent from Reviewable.io \n. Thanks!. Change LGTM, it can be merged. Is metrics-server going to be promoted from incubator?. This comment doesn't make much sense here if we don't keep the interface we had before.. Same here, this comment was related to the removed interface.. Better this way, thanks :). @DirectXMan12 @piosz graphite sink metric hasn't been merged yet (#1341), and there is no event sink in progress.. Oh, sorry, I mixed both rows, it's ok, but the referenced ticket for graphite should be #1341, isn't it?. Maybe we can add a description of what state is used for. These events could be defined here inline. No need to fix array size here. I meant to define the events inline, not in the same line :) i.e:\nexpectedEvents := []*proto.Event{\n    &proto.Event{\n        ...\n    },\n    &proto.Event{\n        ...\n    },\n    ...\n}. ",
    "shmish111": "@erimatnor I've created https://github.com/kubernetes/heapster/pull/1339\n. Completed CLA with wrong google account at first so getting commit missing GitHub user however not sure how to fix that now?\n. Sorry for the delay. I've updated the PR by extracting common logic.\nWRT the event metric, I think this makes sense when reading http://kubernetes.io/docs/api-reference/v1/definitions/#_v1_event however it is opinionated to some extent, no way round that really.\n. Build failing dur to someone else's code \nFAIL   k8s.io/heapster/metrics/sources/kubelet [build failed]\n. @piosz I don't get the problem with the git commits, they all seem fine, all assigned to me and my github picture is there.\n@jsoriano can you review my changes please.\nThanks!\n. @k8s-bot test this\n. @jsoriano the e2e tests failed due to my documentation change, travis failed for another reason, I can't work out why cla thinks my commits are missing Github user, basically the PR looks a mess however unit tests all work on my local machine. Not sure if any of this is a problem in getting this PR merged, if so then I'm not sure what to do.\n. @jsoriano I've made some changes based on the comments\n@piosz I have signed the cla, the problem is that github somehow thinks that some commits aren't mine, I have no idea how to fix this as all the commits seem to be properly associated with my github account.. @jsoriano sorry, I've had github notifications turned off for a while and didn't check your comments. \nI'm sorry to admit that I am not using heapster any more as I am working on a new project that doesn't have access to Kube (yet). Also, as a big fan of Riemann I am surprised to find myself drawn in to the new Elastic beats components, despite not being nearly as nice to configure as Riemann and being written in go (puke) ;-) I find the overall new BELK stack very appealing from an operational point of view. I am currently using Riemann but I can see it fading away and I can also see monitoring heapster with beats more likely to be allowed in to my current organization. I feel sad to say it but I think I agree with deprecating some of the sinks, including Riemann. I feel that heapster beats would be popular though, does it exist?. @mcorbin good to hear Riemann is not dead! Will your work take the place of this PR?\nI am currently torn between beats etc and Riemann, give me some ammo to push Riemann, I can't find any docs about new features coming or anything like that!. ",
    "xiaoping378": "@euprogramador \nmaybe Prometheus is right for you.\n. ",
    "GrigorievNick": "https://github.com/indigo-dc/Monitoring/blob/master/doc/heapster.md\ni use this tools for such purposes. But i install it separately on every k8s node with zabix agent. is some one can help me with DaemonSet it will be perfect. I just do not have enough time.. https://github.com/indigo-dc/Monitoring/blob/master/doc/heapster.md\nI use this tools for such purposes, but i'am install it separately(outside) from k8s service.\nOn every k8s node with zabix agent. If some one have time build daemon set over it, it will be great.. ",
    "tatroc": "I had this very same message on CentOS 7: mkdir all: mkdir /data/hh: permission denied\nI changed/etc/sysconfig/selinux to SELINUX=permissive and the issue went away. This is a proof of concept system, wouldn't disable SELINUX in production\n. ",
    "metal3d": "@tatroc exactly the same. \"chcon\" seems to not be enougth. And I really need to understand how to fix this.\nAnyone ?\n. Same on REHL7 now...\n. @piosz  I did it and I can write inside. \nStrangely, several days later, this problem disapeard and I now see that error (from heapster RC):\nError syncing pod, skipping: failed to \"StartContainer\" for \"POD\" with RunContainerError: \"runContainer: API error (500): Cannot start container 3edd1c7e1f836b7f80f27d2307da90606623fc16fb36201bc8d09cf7e79e721b: [8] System error: write /sys/fs/cgroup/devices/system.slice/docker-3edd1c7e1f836b7f80f27d2307da90606623fc16fb36201bc8d09cf7e79e721b.scope/cgroup.procs: no such device\\n\"\n. Forget my last comment, the problem is still here:\n** Failed to create docker container with error: API error (500): mkdir /var/lib/docker/overlay/38909200b9bdad3a228028e0600a17467c7d9e2cbb09644ea604808c88cd4047-init/merged/dev/shm: invalid argument\nError syncing pod, skipping: failed to \"StartContainer\" for \"influxdb\" with RunContainerError: \"runContainer: API error (500): mkdir /var/lib/docker/overlay/38909200b9bdad3a228028e0600a17467c7d9e2cbb09644ea604808c88cd4047-init/merged/dev/shm: invalid argument\\n\" **\n. The strange behavior is that dashboard hasn't monitor \"graph\" before I launch heapster, but as soon as I launch heapster RC, event if influxdb-grafana crashes with the given error in my last comment, I now see some graphs (CPU, mem) in dashboard...\n. I still have the issue... On CoreOS no problem, but with CentOS the error is still present:\nFailed to create docker container with error: API error (500): mkdir /var/lib/docker/overlay/f7d58054e4c59d933de47667fb0966cd6ac48a044c2515cdfceccf408441b5c8-init/merged/dev/shm: invalid argument\nError syncing pod, skipping: failed to \"StartContainer\" for \"influxdb\" with RunContainerError: \"runContainer: API error (500): mkdir /var/lib/docker/overlay/f7d58054e4c59d933de47667fb0966cd6ac48a044c2515cdfceccf408441b5c8-init/merged/dev/shm: invalid argument\\n\"\n. OK, Found the problem !\nCentOS 7.2 provides only kernel 3.10 version, but shm support with overlay fs should have a kernel >3.18. So I installed latest kernel via \"elrepo\" (see http://elrepo.org):\nrpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org\nrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm\nyum --enablerepo=elrepo-kernel install kernel-ml kernel-ml-headers\nRebooting (using newest kernel, 4.8 here), Heapster has no error now.\nSorry for the issue, hoping that could help others !\n. note: you can install \"lt\" kernel from elrepo to have \"long time\" support kernel, \"ml\" is for \"main line\"\n. ",
    "ghodss": "@djsly Great question. So great, in fact, I created a PR against the kube-state-metrics README to help explain the difference. Please take a look and let me know if that makes sense. https://github.com/kubernetes/kube-state-metrics/pull/2\nI'm planning to add DaemonSet support shortly, but ReplicaSet and ReplicationController support is a good idea as well.\n. @piosz I'd love to be in the discussions as well so I can know whether to continue development on kube-state-metrics.\n. @piosz I haven't heard any further discussion about this topic - is there a SIG managing it?\n. ",
    "activars": "@vishh Will this enable Heapter writing custom metrics from a kube cluster running outside GCE?\n. ",
    "dynek": "Any chance to see this merged soon? Wanted to implement heapster in my k8s cluster but it fails reaching the api server because I'm using basic auth. Thanks!\n. ",
    "heckdevice": "can be enhanced. .. is it feasible?\nOn 16 Jun 2016 20:57, \"Drew\" notifications@github.com wrote:\n\nThis is causing me a serious issue as users are forced to check dozens of\nterminated pods to find the/a active pod when you have pods that cycle\nfrequently.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/1179#issuecomment-226520695,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AHmVKq25wR1cSgkywtDFGOD_GMdsEyHjks5qMWtKgaJpZM4Ir4gJ\n.\n. \n",
    "shayderi": "Hi,\nI also suffering from this problem,\nMy best solution was : add a variable that execute select query (call it livePods for instance):\nselect pod_name,value from uptime where time > now() - 20s\nthen you need to alter the query of  podname variable : \nshow tag values from uptime with key = pod_name where \"pod_name\" =~ /$livePods$/\nbut the above solution is not satisfactory because grafana don't now how to deal with select query by displaying only pod_name (the result contains also time and value - the value is necessary for minimum one field is required in select query - from influx point of view)\nany help for how to deal with the problem in other way will by appreciated \nThanks\nS.D\n. ",
    "sakthivel1988": "Hi\nany update on this issue .\nRegards,\nSakthi\n. @ShashankYadav  its still not working for me .. can help me how to get that working \n. ",
    "ShashankYadav": "For anyone who hits the issue, until there is a proper fix, you can use @shayderi's hack with a few tweaks in the template for livepods variable,which will give you all pods which were up in the last 2 minutes:\nlivepods : select pod_name,value from uptime where time > now() - 2m \nFilter : /.-./\n. ",
    "nvartolomei": "No longer works with grafana 3.1.1 and influx 0.12 https://github.com/grafana/grafana/issues/5013\n. 4.0.2 already :). ",
    "neverfox": "Yeah, this is rough.\n. ",
    "discostur": "Any Update on this? Really annoying to klick through 50 Pods each time ...\n. @Crassirostris I think the best solution would be if you can tell Grafana to only show Pods for which are datapoints available (in relation to the given timeframe you have selected) but i'm not shure if this is possible.\nIn your solution, does it only work for a given time (like last 20 minutes) or can you adjust the timeframe by yourself? It would be really cool if you can set the scope for example to the last 6h oder last 24h and the \"dead pods\" are readjusting to that.\n. @Crassirostris thanks for your update and your effort. . @loburm just tested the new image and it works without any problems ;). ",
    "crassirostris": "@discostur Sorry, I was slowed down by other things\nI was thinking about making a template for amount of time, after which pod will be considered dead. What do you think about it?\n. @discostur Thanks for the feedback\n\nI think the best solution would be if you can tell Grafana to only show Pods for which are datapoints available\n\nMakes sense, I will investigate whether it's possible.\n\nIn your solution, does it only work for a given time (like last 20 minutes) or can you adjust the timeframe by yourself?\n\nYou can adjust.\n. @discostur To give you an update. I was trying to set it up several times, but turns out grafana is very badly suited for this. You can only easily use tags from SHOW TAG VALUES query, which doesn't support filtering by time explicitly.\nI tried to use SELECT to extract tag values, but it's certainly not designed to work out of the box. It may be possible after all, but it's not an easy task to bypass the design of grafana and its influx plugin. I will try to implement it in some way in the meantime, but can't promise anything, unfortunately.. @discostur There's a selector which results in needed information, but I still don't know how to make it work with Grafana template:\nselect pod_name, value from uptime where time > now() - 3m group by pod_name. @0111sandesh No, no progress was made so far, my last update still holds. @whereisaaron Sounds promising, could you please make a PoC in the grafana interface? The main problem I had is selecting data from query to the template.\nIf it works, I will be happy to help you contribute that change or to do it for you!. @cmachler I'm very sorry, I forgot about this PR and this one: https://github.com/kubernetes/heapster/pull/1342 actually changes the image.\nCould you please rebase?\n. @cmachler Actually I didn't change the release notes and the controller, so it may actually make sense to rebase and include in this PR only those changes.. LGTM\n@piosz Were you able to understand, what's wrong with travis?. @lattwood  I'm very sorry I forgot about this PR\nHave you seen https://github.com/kubernetes/kubernetes/pull/28279?\nThere was some problems with switching to 0.12, are you sure nothing else will be influenced by change in config?\n. @medhedifour Thanks for the submitting your request.\nI will make a new dashboard, same as Pods, but with the ability to select individual containers. Will it solve your problem?\n. @luxas No, I was swamped with other tasks and wasn't sure it's what the OP wanted. @luxas Do you want it?. @luxas It's fine, thanks. I think I will be able to address this soon, in a week or so.. Unfortunately, different efforts kept me busy and now I'm going out of office for a couple of weeks. Applying help-wanted label. @piosz Sure\n@sjug Could you please tell the versions of influxdb and grafana containers?. @luxas Are you sure we're on the same page? Your PR is about the architecture, and the original question is about grafana version (3.1.1), which you didn't change\nPlease correct me if I'm wrong. @piosz Since the comment\n\nUpdate the grafana to that. All good\n\nCan it be closed?. @euank Web Admin Interface is deprecated and will be removed, grafana should be a default way to interact with influxdb (proof). Maybe delete this section entirely?\nOtherwise, it looks great, thank you very much!\n. LGTM. @euank Thanks!. LGTM. LGTM. LGTM. LGTM. The problem is that grafana image (not even heapster-grafana) of version v2.6.0-2 was pushed to the kubernetes repo on docker hub, not to gcr\nI pushed the following images to the gcr\ngcr.io/google_containers/heapster-grafana:v2.6.0-2\ngcr.io/google_containers/heapster-influxdb:v0.13.0\nFeel free to check and close this issue if no problems occur. @luxas @piosz will handle it. @idcrook There's magic inside grafana and heapster makefiles, that will make them work without your change.\nIIUC the problem is that this magic is incompatible with gcr, we're working on resolving this issue\n@luxas Please correct me, if I'm wrong. /lgtm. /lgtm. @georgebuckerfield Sorry for the delay. I actually think that your comment about moving this code to k8s.io/heapster/common/gce looks very reasonable, then in the target code you would have clean step\nprojectId, err := gce_utils.GetProjectId()\nif err != nil {\n  return nil, fmt.Errorf(...)\n}. @georgebuckerfield Make code less nested, but I think that'll be obvious with a separate function :). @georgebuckerfield Thank you for your contribution!\nWhoops, thought it's already being tested\n/ok-to-test. @cdxOo Could you please take a look at the event-exporter Piotr mentioned? I can see more than one problem happening with eventer in the newer version of Kubernetes\nMeanwhile I'll try to take a look this week. Yup, with the newer version of SD Logging (v2), everything works\nI'll send a PR during next week. But I strongly encourage you try out event-exporter for exporting to Stackdriver. @piosz The problem will be solved if we upgrade to the latest version of https://github.com/google/google-api-go-client. But the latest version of api breaks something in monitoring. Can someone supporting sd sink do this?. @gytisgreitai Yes, it's preferred.\nAs for heapster version, you should run /eventer binary, not /heapster. And it's broken anyway, I'll fix it as soon as I find some time to take a look at it. Sorry, I don't have enough context and capacity right now. @piosz Duration -> latency mb? WDYT?. @piosz \nI'm not sure about the name change (request -> requests). See https://github.com/kubernetes/kubernetes/blob/master/pkg/client/metrics/prometheus/prometheus.go#L33 or https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/storage/etcd/metrics/metrics.go#L55\nBut otherwise LGTM\nAlso FYI, https://github.com/kubernetes/kubernetes/blob/master/pkg/client/metrics/prometheus/prometheus.go#L33 uses histogram. /lgtm. Thanks!. @mwielgus AFAIU, the latest change was addressing you comment\n\nThis is misleading. You can add all requests to the queue but it doesn't mean that the operation completed.\n\nBut I think now it's even more misleading. Now the request in-flight will be marked as timed out and the response code will be just dropped. It makes the statistics less accurate without introducing any benefits: the response will still be parsed, so it doesn't reduce the memory/cpu footprint.\nBy the way, to your original comment -- you cannot dump all requests to the queue, b/c it's non-blocking and the fact that an item went this way means implies a guarantee that it will be sent and the response will be recorded. IMO, without a hard limit, what was before is a better thing to do: sent requests will be completed and their response codes will be recorded, not sent requests will be marked as timed out. Maybe instead of the last change, to make things less confusing, just rename some stuff keeping the logic intact?. /gtm. One comment, otherwise lgtm. /lgtm. /retest. /lgtm. /lgtm. Please add some description to the PR. /lgtm. /lgtm. /ok-to-test. Known metrics metadata to be consistent with the capital letter + explain what metrics we're talking about. I would suggest rephrasing to Sink metrics or Sink performance metrics. Register sink metrics or Register sink performance metrics explain it better. Why do you need those now?. I think you can just remove those two lines. Errors should not be capitalized. Check the rest of the code for this. I think such comments should not be V(0), but rather V(4). Let's be consistent with the rest of the code and use %v for formatting errors. I strongly feel about float64(time.Since(startTime)) -> time.Since(startTime).Seconds()\nNot so sure about float64(time.Millisecond) -> time.Millisecond.Seconds(), but it's still better IMO. %s -> %v?. else is unnecessary. var minInterval time.Duration?. Extra empty line. Should it be V(0)? V(2) maybe or even V(4)?. That doesn't effectively change the behavior, so leave it to you and to the consistency. I know, it's the same with errors, I just prefer using %s when it's string and %v otherwise. Don't know how it works in heapster tho, that's why I leave it up to you. Ack. This is an explicit default in go, e.g. linter will ask to change var x int = 0 to var x int. I don't like 1 nanosecond as an initial value, but leave it up to you. Move to the top. Why not time.Duration?. It will not timeout iff len(requests) - workers are processed within batchExportTimeoutSec, not len(requests). Defer this right after channel creation mb?. Why not send them all asynchronously? I'm not saying we should do that, but srsly, what's the reasoning?. Especially having in mind that this is not a true timeout anyway, we can leave some requests hanging while exiting this routine. Why not have a duration in the parameters?. I had in mind that all outcoming requests should be canceled when timeout expires. Maybe instead select a random offest from the range [0, 1m) to start from instead? That'll guarantee the delay between two scrapes while offloading the API. OK, \"scraping start offset\" -> \"sending requests start offset\"\nMy point is, constant offset is better than constantly shrinking and expanding the collection interval, for the purposes of setting up the timeouts. > I would set timeouts irrespective of offset and definitely not random\nSure!\n\nIf we have to introduce up to 5 sec offset we should reduce the total timeout by 5 sec\n\nOTOH, if the offset is always the same, we can avoid reducing the timeout, which saves us 5 sec to send metrics with higher reliability :). Should it be V(0)?. If someone sees this in logs, what this person does? If the answer is \"nothing\", I think this definitely should not be V(0)\nIdeally, I'd prefer dropping this to V(2) at least and record a metric if it's not possible to figure it out from existing metrics already. This way the losses are visible, but the log is not flooded. for i, r := range requests and then use r instead of requests[i] below. As usual, should it be a warning, if it's exported in metrics?. for req := range requests[i:]. I'm little bit hesitant to use the existing http code to describe some client's actions. What if server starts to respond with 429? Maybe switch to using something off the RFC? -1 or -2 for example. Why not\nif apiErr, ok := err.(*googleapi.Error); ok {\n  responseCode = apiErr.Code\n}. Move -1 to constant?. Why a separate variable?. Definitely NOT an error, not even an warning, since exposed as a metric. I'd say V(2). timeout := time.Duration(sink.batchExportTimeoutSec) * time.Second\nt := time.NewTimer(timeout)\ndefer t.Stop()\nand then \nselect {\ncase rq <- r:\n  ...\ncase <- t.C:\n  ...\n}\n. Or just t := time.After(timeout). V(0) -> V(2) or Info -> Warning. Small nit: extract variable. If you implement this as a struct or a map with all metrics & translation rules, you can remove the duplication here. What if some of these metrics don't exist, as it was discovered recently by the node team?. This doesn't belong to this PR, this is not about supporting the new resource model. Adding legacy as a prefix, not in the middle of the name helps to distinguish and discourage better. Why isn't it a part of the StackdriverSink struct?. Why move?. Since @fgrzadkowski approves, I'm fine with duplication\nHowever, personally I think it's better not to write code, rather than write it and then rewrite once again. If you use the legacy notation when listing metrics, use the same notation here\nOld method should be named LegacyTransalteMetric, new one -- juse TranslateMetric. go\nif ts := sink.TranslateMetric(...); ts != nil {\n  ...\n}. Hack used only with legacy resource type \"gke_container\"\nHow is it represented in the logic? I don't see any mentions of the old resource type. go\nif err := ...; err != nil {\n  ...\n}. Move closer to the place where it's used. Same as above: make consistent with the metric names. New methods should be just methods and old methods should have a legacy prefix. This is not an old metric in a new format, this is a totally new metric that never existed and can be added in a separate PR. There's sink.useOldResourceModel in the condition, now I see. Sorry if I just missed it before. This is not the only metric there, there are also volume metrics. Synced offline, turns out there's no need in a method for the new resource model: old disk metrics will not be exported. However, it requires a mention in the PR description. Nit: I think it's easier to read when input and output is near each other. It's in the PR description. I'm not sure\n@serathius Could you please explain it somewhere, e.g. in the description?. OK, I see, arguments evaluating\nThis looks ugly anyways, can it be refactored into a single function call?. OTOH, let it be, refactoring it seems to add even more complexity and boilerplate. Why make two separate functions? One is enough. Making it consistent with metrics/sources/manager.go seems reasonable\nAmbiguity of the description is another problem, but it's outside of this PR's scope. ",
    "0111sandesh": "Any update on this issue? I have been facing same issue with Grafana and InfluxDB for EC2 instances where certain instances go down and new instances take their place with different ip.. ",
    "whereisaaron": "How about creating a continuous query in InfluxDB (using the 'select from uptime' query approach above) to keep track of the live pods, and then querying that derived measurement in Grafana?. Hi @manuelschellenberg yes I looked at it and the influxdb query syntax and I fear it may be dead end. The problem is that the output of the continuous query must be another measurement. And that measurement has the same problem that queries must return at least one field.\nI wondered if there is a way to query tags from one measurement into a string field on another measurement. The we could query just that field. However I can't see how that is possible with the available query syntax.\nPlan B would be to create a 'templated query' for the Influxdb data source, similar to those used for the CloudWatch data source. That way we could run the uptime query @heckdevice gives above and return just the tags as the result.. Thanks for the idea @Lebuin @steve21168 nice to have a workaround.\nThis remains a pain point for the grafana+influxdb combo, and certainly not a stale issue.\nLuckily there may be some progress, with @daniellee picking up PR #7919 with a strategy to achieve it.\n/remove-lifecycle stale\n. Another limitation of gcr.io is it doesn't have any human way to find tags. You can use docker search to find images, but then you need to using the API to find tags, e.g. curl https://gcr.io/v2/google-containers/heapster-influxdb-amd64/tags/list, these appear to be the current tags.\ngcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\ngcr.io/google_containers/heapster-grafana-amd64:v4.0.2\ngcr.io/google_containers/heapster-amd64:v1.3.0-beta.1. Hi @karunakar1122v, the problem with the current image builds have caught me and a lot of people\n1) The current versions are:\ngcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\ngcr.io/google_containers/heapster-grafana-amd64:v4.0.2\ngcr.io/google_containers/heapster-amd64:v1.3.0-beta.1\nAs you found, due so some image build bug, you need to add the architecture, e.g. -amd64.\n2) Once you are using influxdb v1.1.1, the image has a bug were it doesn't initialize the correction retention policy, so heapster can't write. I fix that as follows. (See #1482 and #1474).\n```\n!/bin/bash\n\nApplies to heapster influxdb image v1.1.1\nhttps://github.com/kubernetes/heapster/issues/1482\nhttps://github.com/kubernetes/heapster/issues/1474\n\nkubectl run influxcli --restart=Never --rm -it --image=cburki/influxdb-shell -- \\\n  --host=monitoring-influxdb.kube-system --port=8086 --database=k8s \\\n  --execute 'CREATE RETENTION POLICY \"default\" ON k8s DURATION 0d REPLICATION 1 DEFAULT; SHOW RETENTION POLICIES'\n```\n. @jmn the influxdb version in the example has since been updated to v1.1.1. But the example is still failing for me too. heapster 1.3.0-beta.0 appears to be failing to write to influxdb v1.1.1\nFailed the same way with heapster 1.2.0.\nThe influxdb log shows the create database went okay, but then 500 errors for POSTs from heapster:\n[httpd] 10.2.67.16 - root [25/Jan/2017:06:18:05 +0000] \"GET /query?db=&q=CREATE+DATABASE+k8s HTTP/1.1\" 200 163 \"-\" \"heapster/v1.3.0-beta.0\" 08387ab6-e2c6-11e6-80b9-000000000000 666\n[httpd] 10.2.67.16 - root [25/Jan/2017:06:18:05 +0000] \"POST /write?consistency=&db=k8s&precision=&rp=default HTTP/1.1\" 500 72 \"-\" \"heapster/v1.3.0-beta.0\" 08406030-e2c6-11e6-80ba-000000000000 2389\n[httpd] 10.2.67.16 - root [25/Jan/2017:06:18:05 +0000] \"GET /ping HTTP/1.1\" 204 0 \"-\" \"heapster/v1.3.0-beta.0\" 0840ccfd-e2c6-11e6-80bb-000000000000 60\n[httpd] 10.2.67.16 - root [25/Jan/2017:06:19:05 +0000] \"POST /write?consistency=&db=k8s&precision=&rp=default HTTP/1.1\" 500 72 \"-\" \"heapster/v1.3.0-beta.0\" 2bfe3472-e2c6-11e6-80ed-000000000000 3289\n[httpd] 10.2.67.16 - root [25/Jan/2017:06:19:05 +0000] \"GET /ping HTTP/1.1\" 204 0 \"-\" \"heapster/v1.3.0-beta.0\" 2bfec387-e2c6-11e6-80ee-000000000000 11\nThe heapster log seems happy at start-up, but no data is flowing\nI0125 06:17:01.230742       1 heapster.go:71] /heapster --source=kubernetes.summary_api:'' --sink=influxdb:http://monitoring-influxdb:8086\nI0125 06:17:01.230818       1 heapster.go:72] Heapster version v1.3.0-beta.0\nI0125 06:17:01.231026       1 configs.go:60] Using Kubernetes client with master \"https://10.3.0.1:443\" and version v1\nI0125 06:17:01.231065       1 configs.go:61] Using kubelet port 10255\nI0125 06:17:01.233219       1 influxdb.go:231] created influxdb sink with options: host:monitoring-influxdb:8086 user:root db:k8s\nI0125 06:17:01.233236       1 heapster.go:193] Starting with InfluxDB Sink\nI0125 06:17:01.233239       1 heapster.go:193] Starting with Metric Sink\nI0125 06:17:01.525442       1 heapster.go:105] Starting heapster on port 8082\nI0125 06:18:05.070935       1 influxdb.go:209] Created database \"k8s\" on influxDB server at \"monitoring-influxdb:8086\"\n. This bug appears to have been discussed and resolved in #1474 (closed already so didn't spot it).. The example failed for me to with the same 500 errors. The fix in #1474 worked for me too.\nI wasn't sure where to get the influx CLI client from, found it here: https://www.influxdata.com/downloads/\nI made the following changes and it started working.\n```\nInfluxDB shell version: 1.2.0\n\nshow databases\nname: databases\nname\n\n\n_internal\nk8s\n\nuse k8s\nUsing database k8s\nshow retention policies\nname    duration shardGroupDuration replicaN default\n----    -------- ------------------ -------- -------\nautogen 0s       168h0m0s           1        true\nCREATE RETENTION POLICY \"default\" ON k8s DURATION 0d REPLICATION 1 DEFAULT\nshow retention policies\nname    duration shardGroupDuration replicaN default\n----    -------- ------------------ -------- -------\nautogen 0s       168h0m0s           1        false\ndefault 0s       168h0m0s           1        true\n```\n. I signed it!. Hi @alirezaDavid I don't know if either of these are your problem, but two things to check:\n\n1) The image versions you quote is way older that current, maybe you cloned an old version/branch?\nCurrent versions are:\ngcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\ngcr.io/google_containers/heapster-grafana-amd64:v4.0.2\ngcr.io/google_containers/heapster-amd64:v1.3.0-beta.1\nNote, due so some image build bug, you need to add the architecture, e.g. -amd64.\n2) Once you are using influxdb v1.1.1, the image has a bug were it doesn't initialize the correction retention policy. I fix that as follows. (See #1482 and #1474).\n```\n!/bin/bash\n\nApplies to heapster influxdb image v1.1.1\nhttps://github.com/kubernetes/heapster/issues/1482\nhttps://github.com/kubernetes/heapster/issues/1474\n\nkubectl run influxcli --restart=Never --rm -it --image=cburki/influxdb-shell -- \\\n  --host=monitoring-influxdb.kube-system --port=8086 --database=k8s \\\n  --execute 'CREATE RETENTION POLICY \"default\" ON k8s DURATION 0d REPLICATION 1 DEFAULT; SHOW RETENTION POLICIES'\n```\n. This patch by @adambkaplan appears to fix (1) it is uses the GF_* variables if present.. Thanks @KarolKraskiewicz, I dealt with this problem (and other problems) by switching to the official Grafana Docker image which is based on debian:jessie. I also made a couple of tweaks to better suit Kubernetes. This has the side-benefit of upgrading to grafana the current 4.2.0 version.\nThe busybox container caused other problems for me too, e.g. it lacked a root cert collection and so couldn't send email alerts or generate Slack notifications. Alpine could work, but sticking with the tested upstream build seems a better way to go. Certainly it is working well for me when combined with the heapster and influxdb images from the project.. ",
    "manuelschellenberg": "@whereisaaron any progress on that?. ",
    "Lebuin": "@whereisaaron @manuelschellenberg I'm a bit late to the party, but for anyone coming across this thread: I did manage to create a workaround using a continuous query. The following two influxdb commands create a new measurement current_pods that contains all the pods that were active somewhere in the last 2 hours. The namespace_name, nodename and pod_name tags are included. You can add other tags to the GROUP BY statement.\nCREATE RETENTION POLICY \"2hours\" ON \"k8s\" DURATION 2h REPLICATION 1\nCREATE CONTINUOUS QUERY current_pods_query ON k8s BEGIN SELECT max(value) AS value INTO k8s.\"2hours\".current_pods FROM k8s.\"default\".uptime WHERE type = 'pod' GROUP BY time(5m), namespace_name, nodename, pod_name END. @shahbour Those 2 queries should be executed in InfluxDB itself, e.g. using the cli. ",
    "steve21168": "@Lebuin Thank you for your solution it helped me out.  \nFor anyone else not too familiar with influxDB like myself after following Lebuin's solution you want to update your template query to the following:\n\"SHOW TAG VALUES FROM k8s.\"2hours\".current_pods WITH KEY = \"pod_name\" WHERE \"namespace_name\" =~ /$namespace$/\"\n. ",
    "shahbour": "I am totally new to grafana and influxDB so excuse me if it is so obvious question, but how can I create the two queries mentioned by @Lebuin\nCREATE RETENTION POLICY \"2hours\" ON \"k8s\" DURATION 2h REPLICATION 1\nCREATE CONTINUOUS QUERY current_pods_query ON k8s BEGIN SELECT max(value) AS value INTO k8s.\"2hours\".current_pods FROM k8s.\"default\".uptime WHERE type = 'pod' GROUP BY time(5m), namespace_name, nodename, pod_name END\nShould I create them in Grafana or directly in influxDB. I am totally new to grafana and influxDB so excuse me if it is so obvious question, but how can I create the two queries mentioned by @Lebuin\nCREATE RETENTION POLICY \"2hours\" ON \"k8s\" DURATION 2h REPLICATION 1\nCREATE CONTINUOUS QUERY current_pods_query ON k8s BEGIN SELECT max(value) AS value INTO k8s.\"2hours\".current_pods FROM k8s.\"default\".uptime WHERE type = 'pod' GROUP BY time(5m), namespace_name, nodename, pod_name END\nShould I create them in Grafana or directly in influxDB. ",
    "hasbro17": "Sorry to piggyback your issue since I'm not sure why you're getting negative values, but I just noticed that you are also missing data points according to the jumps in your timestamps(I'm assuming your model resolution is set to 10s). While I'm not getting negative values, the missing data points are part of the problem in my own issue #1177. Do you have any idea why that might be happening?\n. @onorua: honestly I'm not sure how to debug this issue. I'm not even sure how much faith I can put in the metrics being reported by heapster. I can understand why there might be missing values(perhaps it's a best effort scrape by heapster and maybe the kubelet doesn't respond in time or something). But I'm getting zero values as well.\nSo for instance I have a busy loop running in one pod which uses up 100%(1000 milli-cores) of the cpu in a 4 core node. Running top on the node confirms that process/pod using up 100% consistently.\n\nBut when I query heapster for the cpu usage of that pod(for a 10s resolution over the last 15 minute interval) I get this:\nRequest: http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/heapster-custom/api/v1/model/namespaces/default/pods/my-nginx-972vk/metrics/cpu/usage_rate?start=2016-06-06T12:26:25-07:00&end=2016-06-06T12:41:25-07:00\nmy-nginx-972vk: [513 0 995 996 0 995 0 0 995 996 995 995 996 995 995 995 995 0 995 995 0 995 996 995 996 996 0 0 996 980 996 989 0 987 995 0 990 0 995 978]\nI've pasted the formatted output. First off there should have been ~90 values. But another problem is the 0's interspersed in the values reported. For whatever values reported, I should at least be seeing accurate values(in this case a steady usage above ~990).\nEven the aws node metrics(sampled at 1 min) show a constant usage of 25% (1 out of 4 cpus).\n\nI'm thinking of bypassing heapster entirely and try querying cAdvisor/kubelet directly to see if this a heapster issue. Or just sift through the heapster code to see where the aggregation is happening.\n. ",
    "xigang": "I also encountered the same problem\u3002\n~~~\n\nselect usage_rate from cpu where namespace_name='zychannelsjar-stable' and usage_rate < 0;\nname: cpu\n\n\ntime                    usage_rate\n1498103100000000000     -1134985\n1498103100000000000     -1135150\n1498103100000000000     -1135150\n1498618620000000000     -205093\n1498618620000000000     -204805\n1498618620000000000     -205117\n\n~~~. \n",
    "ApsOps": "@DirectXMan12 Thanks for replying :)\nKubernetes version v1.2.4\nHeapster version v1.0.2\nIt looks like newly created pods show up but go away randomly after some time.\nHeapster logs look okay.\nI0606 17:55:05.000211 1 manager.go:79] Scraping metrics start: 2016-06-06 17:54:00 +0000 UTC, end: 2016-06-06 17:55:00 +0000 UTC\nI0606 17:55:05.033620 1 manager.go:152] ScrapeMetrics: time: 33.313006ms size: 59\nI0606 17:56:05.000232 1 manager.go:79] Scraping metrics start: 2016-06-06 17:55:00 +0000 UTC, end: 2016-06-06 17:56:00 +0000 UTC\nI0606 17:56:05.068257 1 manager.go:152] ScrapeMetrics: time: 67.868802ms size: 59\nI0606 18:04:05.000209       1 manager.go:79] Scraping metrics start: 2016-06-06 18:03:00 +0000 UTC, end: 2016-06-06 18:04:00 +0000 UTC\nI0606 18:04:05.006531       1 manager.go:98] Querying source: kubelet_summary:10.0.61.117:10255\nI0606 18:04:05.007616       1 manager.go:98] Querying source: kubelet_summary:10.0.100.160:10255\nI0606 18:04:05.010843       1 round_trippers.go:286] GET http://10.0.100.160:10255/stats/summary/ 200 OK in 3 milliseconds\nI0606 18:04:05.012862       1 round_trippers.go:286] GET http://10.0.61.117:10255/stats/summary/ 200 OK in 6 milliseconds\nI0606 18:04:05.014526       1 manager.go:98] Querying source: kubelet_summary:10.0.100.162:10255\nI0606 18:04:05.018536       1 manager.go:98] Querying source: kubelet_summary:10.0.100.161:10255\nI0606 18:04:05.021347       1 round_trippers.go:286] GET http://10.0.100.161:10255/stats/summary/ 200 OK in 2 milliseconds\nI0606 18:04:05.023787       1 round_trippers.go:286] GET http://10.0.100.162:10255/stats/summary/ 200 OK in 9 milliseconds\nI0606 18:04:05.029525       1 manager.go:98] Querying source: kubelet_summary:10.0.58.122:10255\nI0606 18:04:05.034370       1 round_trippers.go:286] GET http://10.0.58.122:10255/stats/summary/ 200 OK in 4 milliseconds\nI0606 18:04:05.035122       1 manager.go:152] ScrapeMetrics: time: 34.746036ms size: 62\nI0606 18:04:05.035169       1 manager.go:154]    scrape  bucket 0: 5\nI0606 18:04:05.035238       1 manager.go:154]    scrape  bucket 1: 0\nI0606 18:04:05.035514       1 manager.go:154]    scrape  bucket 2: 0\nI0606 18:04:05.035738       1 manager.go:154]    scrape  bucket 3: 0\nI0606 18:04:05.035960       1 manager.go:154]    scrape  bucket 4: 0\nI0606 18:04:05.036177       1 manager.go:154]    scrape  bucket 5: 0\nI0606 18:04:05.036395       1 manager.go:154]    scrape  bucket 6: 0\nI0606 18:04:05.036606       1 manager.go:154]    scrape  bucket 7: 0\nI0606 18:04:05.036821       1 manager.go:154]    scrape  bucket 8: 0\nI0606 18:04:05.037183       1 manager.go:154]    scrape  bucket 9: 0\nI0606 18:04:05.037250       1 manager.go:154]    scrape  bucket 10: 0\nI0606 18:04:05.038466       1 manager.go:113] Pushing data to: Metric Sink\nI0606 18:04:05.038501       1 manager.go:116] Data push completed: Metric Sink\nI0606 18:04:05.038677       1 manager.go:113] Pushing data to: InfluxDB Sink\nI0606 18:04:05.038961       1 manager.go:116] Data push completed: InfluxDB Sink\nI0606 18:04:05.342816       1 influxdb.go:145] Exported 1138 data to influxDB in 283.752115ms\n. We found out that pod list not sent by one of the nodes. This node is throwing errors in kubelet logs:\nI0606 18:31:30.025970    4435 handler.go:239] HTTP InternalServerError: Internal Error: unable to find Docker container \"10bfe366a0bcd97dbce295d91c6219303115ef4e41c4c3581896c94201e27b1c\"\nI0606 18:31:30.026020    4435 server.go:1096] GET /stats/kube-system/heapster-v1.0.2-2856270887-3218g/7796e605-2c11-11e6-aed5-12bca96a847b/heapster: (150.391\u00b5s) 500\ngoroutine 1095016 [running]:\nk8s.io/kubernetes/pkg/httplog.(*respLogger).recordStatus(0xc208b761c0, 0x1f4)\n        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/httplog/log.go:214 +0xa6\nk8s.io/kubernetes/pkg/httplog.(*respLogger).WriteHeader(0xc208b761c0, 0x1f4)\n        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/httplog/log.go:193 +0x32\ngithub.com/emicklei/go-restful.(*Response).WriteHeader(0xc208efaa80, 0x1f4)\n        /go/src/k8s.io/kubernetes/Godeps/_workspace/src/github.com/emicklei/go-restful/response.go:191 +0x48\ngithub.com/emicklei/go-restful.(*Response).WriteErrorString(0xc208efaa80, 0x1f4, 0xc208e5d980, 0x72, 0x0, 0x0)\n        /go/src/k8s.io/kubernetes/Godeps/_workspace/src/github.com/emicklei/go-restful/response.go:180 +0x128\nk8s.io/kubernetes/pkg/kubelet/server/stats.handleError(0xc208efaa80, 0x7f6a08d4f040, 0xc208c55c00)\n        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/server/stats/handler.go:240 +0x29a\nk8s.io/kubernetes/pkg/kubelet/server/stats.(*handler).handlePodContainer(0xc2083c0d80, 0xc2090a9110, 0xc208efaa80)\n        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/server/stats/handler.go:216 +0x8a6\nk8s.io/kubernetes/pkg/kubelet/server/stats.*handler.(k8s.io/kubernetes/pkg/kubelet/server/stats.handlePodContainer)\u00b7fm(0xc2090a9110, 0xc208efaa80)\n        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/server/stats/handler.go:72 +0x3b\ngithub.com/emicklei/go-restful.(*Container).dispatch(0xc2083757a0, 0x7f6a08d7c788, 0xc208b761c0, 0xc20926dee0)\n        /go/src/k8s.io/kubernetes/Godeps/_workspace/src/github.com/emicklei/go-restful/container.go:249 +0xf5e\ngithub.com/emicklei/go-restful.*Container.(github.com/emicklei/go-restful.dispatch)\u00b7fm(0x7f6a08d7c788, 0xc208b761c0, 0xc20926dee0)\n        /go/src/k8s.io/kubernetes/Godeps/_workspace/src/github.com/emicklei/go-restful/container.go:95 +0x45\nnet/http.HandlerFunc.ServeHTTP(0xc2083d0cf0, 0x7f6a08d7c788, 0xc208b761c0, 0xc20926dee0)\n        /usr/src/go/src/net [[Go-http-client/1.1] 10.0.100.160:44834]\nI0606 18:31:30.041949    4435 handler.go:239] HTTP InternalServerError: Internal Error: unable to find Docker container \"0e22d9be21827606e8a2759686d8016806178e9f6f84a952a4348ed202855f22\"\nI0606 18:31:30.042020    4435 server.go:1096] GET /stats/kube-system/filebeat-ip-10-0-61-117.ec2.internal/a5d1e2cc-1e6a-11e6-aed5-12bca96a847b/filebeat: (175.214\u00b5s) 500\ngoroutine 1094943 [running]:\nk8s.io/kubernetes/pkg/httplog.(*respLogger).recordStatus(0xc208b76380, 0x1f4)\n        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/httplog/log.go:214 +0xa6\nk8s.io/kubernetes/pkg/httplog.(*respLogger).WriteHeader(0xc208b76380, 0x1f4)\n        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/httplog/log.go:193 +0x32\ngithub.com/emicklei/go-restful.(*Response).WriteHeader(0xc208efad20, 0x1f4)\n        /go/src/k8s.io/kubernetes/Godeps/_workspace/src/github.com/emicklei/go-restful/response.go:191 +0x48\ngithub.com/emicklei/go-restful.(*Response).WriteErrorString(0xc208efad20, 0x1f4, 0xc208e5db00, 0x72, 0x0, 0x0)\n        /go/src/k8s.io/kubernetes/Godeps/_workspace/src/github.com/emicklei/go-restful/response.go:180 +0x128\nk8s.io/kubernetes/pkg/kubelet/server/stats.handleError(0xc208efad20, 0x7f6a08d4f040, 0xc208c55e00)\n        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/server/stats/handler.go:240 +0x29a\nk8s.io/kubernetes/pkg/kubelet/server/stats.(*handler).handlePodContainer(0xc2083c0d80, 0xc2090a9350, 0xc208efad20)\n        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/server/stats/handler.go:216 +0x8a6\nk8s.io/kubernetes/pkg/kubelet/server/stats.*handler.(k8s.io/kubernetes/pkg/kubelet/server/stats.handlePodContainer)\u00b7fm(0xc2090a9350, 0xc208efad20)\n        /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/server/stats/handler.go:72 +0x3b\ngithub.com/emicklei/go-restful.(*Container).dispatch(0xc2083757a0, 0x7f6a08d7c788, 0xc208b76380, 0xc20926da00)\n        /go/src/k8s.io/kubernetes/Godeps/_workspace/src/github.com/emicklei/go-restful/container.go:249 +0xf5e\ngithub.com/emicklei/go-restful.*Container.(github.com/emicklei/go-restful.dispatch)\u00b7fm(0x7f6a08d7c788, 0xc208b76380, 0xc20926da00)\n        /go/src/k8s.io/kubernetes/Godeps/_workspace/src/github.com/emicklei/go-restful/container.go:95 +0x45\nnet/http.HandlerFunc.ServeHTTP(0xc2083d0cf0, 0x7f6a08d7c788, 0xc208b76380, 0xc20926da00)\n        /usr/src/go/src/net [[Go-http-client/1.1] 10.0.100.160:44696]\nI0606 18:31:30.042210    4435 handler.go:239] HTTP InternalServerError: Internal Error: unable to find Docker container \"0bd3970bef992d28f9973355a541e23df70b03ac11176c351ffd1aa6c4db9837\"\nThese containers are running fine and showing up in docker ps though.\n. Looks like I'm hitting https://github.com/kubernetes/kubernetes/issues/18868. I'm on docker v1.9.1 though.\n. Please note that restarting kubelet fixed these errors, at least for now.\n. ",
    "chowyu08": "@ApsOps I have teh same problem \ncurl http://{{ MASTER }}/api/v1/proxy/namespaces/kube-system/services/heapster/api/v1/model/namespaces/default/pods/\n and restart kubelet,no use\n. ",
    "benmathews": "I have the same issue. @piosz, where is the configuration or documentation on the changes you suggest?\n. Same issue. And mine also started about a week ago. I've exhausted my problem-solving abilities on this.. That is what I eventually did and it pointed me back to influxdb where I solved my problem. . ",
    "singram": "Heapster has got the list of nodes from Kubernetes and is now trying to pull stats from the kublete process on each node (which has a built in cAdvisor collecting stats on the node). In this case there's only one node and it's known by 127.0.0.1 to kubernetes. And there's the problem. The Heapster container is trying to reach the node at 127.0.0.1 which is itself and of course finding no kublete process to interrogate within the Heapster container.\nTwo things need to happen to resolve this issue.\n1.    We need to reference the kublete worker node (our host machine running kubernetes) by something else other than the loopback network address of 127.0.0.1\n2.   The kublete process needs to accept traffic from the new network interface/address\nTo change the hostname by which the kublete is referenced is pretty simple. You can take more elaborate approaches but setting this to your eth0 ip worked fine for me (ifconfig eth0). The downside is that you need a eth0 interface and this is subject to DHCP so your mileage may vary as to how convenient this is.\nexport HOSTNAME_OVERRIDE=10.0.2.15\nTo get the kublete process to accept traffic from any network interface is just as simple.\nexport KUBELET_HOST=0.0.0.0\nUsing these settings resolved the issue fully for me with DNS enabled.  Stats were also presented in the kubernetes-dashboard at all levels.\n. ",
    "boj": "I'm running the same versions listed above on a kube-aws built CoreOS cluster and just get an almost-big-blank nothing (a checkbox, and \"{{alert.title}}\").\nI deleted the default heapster service and used the configs provided in this project instead to spin everything up.\n. @otaviosoares Unfortunately not, stuck at that mostly white screen.\n. There were some changes made to https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/cluster-monitoring/influxdb in the last few days, I was able to get everything working yesterday.\n. ",
    "otaviosoares": "@boj Using the configs of this project did it work?\n. ",
    "jamiehannaford": "@piosz @huangyuqi I'm happy to contribute the code to get this enabled. What would be the best way for the end-user to add ElasticSearch-specific configuration, considering that there are a lot of potential options? \nI could add a few for now to the URL (?maxRetries= and ?startupHealthcheckTimeout=). I notice that Hawkular has a lot of configuration values too. \n. I signed it!\n. FAO @huangyuqi @piosz \n. @mwielgus @huangyuqi @piosz Does this need anything else from my end?\n. Thanks @piosz @huangyuqi. Will this be available on the latest Docker image tag?\n. @piosz Any update on v1.2.0-beta1? My team has some work which is contingent on the new ElasticSearch stuff :)\n. \ud83d\udc4d  Awesome, thanks!\n. @piosz Any feedback on this?\n. Yeah, the client sets them to defaults if none are provided:\n- DefaultHealthcheckTimeoutStartup\n- DefaultMaxRetries\n. I already did :) The defaults are described per option: https://github.com/kubernetes/heapster/pull/1198/files#diff-f8f98ae0a1d3912a7e0772e3ff5a7029R173\n. Yeah I totally agree. But they've already fixed this in the release-branch.v3, so it will be available soon. All I've done is implement a change that's required to get the types right, so no risk of vendor drift.\n. ",
    "mamoit": "This would be quite useful.. ",
    "markine": "This would be super nice. I've been able to get the Stackdriver monitoring agent running in Kubernetes 1.7.2 on AWS with a service account json private key mounted in /etc/google/auth/. It would be great if we could follow a similar pattern with heapster.. FYI looks like https://github.com/kubernetes/heapster/pull/1657 got merged.. I've confirmed that the above patch works by building master privately.. I signed it!. Hi @kawych , thank you for your time! I will fix the GCL typo. I have only tested this change in AWS. Because the change is derived from https://github.com/kubernetes/heapster/commit/a831b80cc381d08afc47dbffc04880e725ac40db I presumed this would be enough. Could you please help me decipher the e2e test failure? I am not familiar with your framework. Thank you!. Will fix.. Yes, it does. In the use case I demonstrated in the PR description, project info is pulled out of the environment (GOOGLE_APPLICATION_CREDENTIALS). This is core heapster code: https://github.com/kubernetes/heapster/blob/974094fed3de505402451b6282859ce91bf88691/common/gce/gce.go#L45. ",
    "mkumatag": "@jamiehannaford @piosz @huangyuqi \n. @piosz tests has been passed, can you give LGTM?\n. /cc @ixdy @luxas . /assign. Created https://github.com/kubernetes/heapster/pull/1202 to take care this comment.\n. ",
    "prashantchitta": "I have signed the CLA as a corporation under eBay Inc\nAgreement         Name            Date        Signed\nGoogle Corporate   CLA          eBay Inc    Feb 12, 2015 13:52 PST\n. ",
    "MihaiAnei": "This commit would fix heapster in our environment too.\n. ",
    "tarunsaxena79": "Thanks a lot !! Bud. It really helped.. ",
    "vtuson": "same issue here\n. I manage to work around this issue but not accessing grafana via the api proxy, but directly exposing the service. After that I still had to do a few changes.\n1) remove env variables influxdb-grafana-controller.yaml\n2) expose service as NodePort or LoadBalancer depends of your preference in grafana-service.yaml under spec add:   type: NodePort \n3) Once you do this and deploy the services and replication controller you should be able to access the monitoring-grafana using the ip of your cluster plus the export port. You can see the expose port from kubectl describe service grafana-service\n4) login to grafana as admin (admin:admin), select DataSources -> influxdb-datasource and test the connection. The connection is set up as http://monitoring-influxdb:8086, this failed for me. However as influx and grafana are both in the same pod, you can use localhost to access the service. So change the url to http://localhost:8086, save and test the connnection again. This worked for me and a minute later I was getting realtime data from nodes and pods.\n. ",
    "Parth6288": "Hi @vtuson , I am also facing the similar issue. Could you please explain in details on how you were able to resolve this. I am new to this and just set up my first Kubernets cluster.\nThanks.. ",
    "igalic": "that's not our experience, it appears as though something else is already doing the quoting for us before it hits the shell\n. showing logs is kinda a ootstrapping issue :D\n. I signed it!\n. ",
    "xyuanlu": "I think what you should do is --sink=elasticsearch:?nodes=http://elasticsearch.es.svc.cluster.local:9200 \nYou need to specify the \"nodes\" param.\n. ",
    "lacion": "i am having the same problem\nE0726 18:20:17.526386       1 factory.go:74] Failed to create sink: failed to create ElasticSearch client: no Elasticsearch node available\n--sink=elasticsearch:?nodes=http://foo.bar:9200&esUserName=foo&esUserSecret=bar\n. ",
    "commixon": "In my case it worked by specifying the nodes parameter, but also using the latest version.\n. +1. I think this is quite important. \nMaybe something simple like:\nif indexName == \"\" || typeName == \"\" || sinkData == nil {\n                return nil\n        }\n+       // Index in format indexName-yyyy-mm-dd\n+       indexName += \"-\" + time.Now().Format(\"2006-01-02\")\n+\nOr this could be done by a flag in options (e.g. ?timestampFormat=true).\nI could find some time to work on this, but as I am by no means a go expert maybe someone else is better suited and willing to take the task?\n. Will do asap! From a first look the only thing that seems to be \"difficult\" to digest is that this PR brings breaking changes. BUT, they are most welcome since they are optimized for ElasticSearch. All of the changes introduced are valid in my opinion.\n(I am the second one trying to create dashboards from the previous schema :) )\nDon't know however how can we speed up the reviewing process. \nWill try to build images from your branch and deploy them in testing to see how everything works.\n. ",
    "douglasader": "Agreed with above comments, specifying index does not change it. Also metrics get pushed to index, but no events. Version 1.2.0, any progress on this issue. Also might be nice to be able to specify if you want metrics and events or only one of them.\n. Well I'm using 1.2.0, which is newer then 1.2.0-beta.0 AFAIK and events are not sent to ES, note however the index is setup on ES for events and metrics though, but only metrics gets populated with   actual entries.\n. ",
    "liubog2008": "I signed it!\n. I want to add some labels and make data more useful for us, so that I don't need to process data again after getting data from influxdb.\n. Ok, It seems the biggest problem in my pr.  I'll try to create a public cache repository for getting cache store or indexer.\n. OK, I'll change it... In fact, I just imitate what Sources do.\n. ",
    "activeshadow": "I signed it!\n. ",
    "TracyBin": "hi, i face the same issue. would you please solve the problem?. I face the same issue. kubernetes 1.10.7 \ninfluxdb.yaml\n```\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: monitoring-influxdb\n  namespace: kube-system\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: influxdb\n    spec:\n      containers:\n      - name: influxdb\n        image: k8s.gcr.io/heapster-influxdb-amd64:v1.5.2\n        volumeMounts:\n        - mountPath: /data\n          name: influxdb-storage\n      volumes:\n      - name: influxdb-storage\n        emptyDir: {}\n\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    task: monitoring\n    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)\n    # If you are NOT using this as an addon, you should comment out this line.\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: monitoring-influxdb\n  name: monitoring-influxdb\n  namespace: kube-system\nspec:\n  ports:\n  - port: 8086\n    targetPort: 8086\n  selector:\n    k8s-app: influxdb\n```. ",
    "mksalawa": "ref #1228 \n. ref #1231 \n. #1230 - lacking the test for all pods, which is done in #1231 together with the added handler.\n. ref #1271 \n. cc @piosz \n. cc @piosz @mwielgus \n. @k8s-bot test this please\n. lgtm\n. cc @piosz\n. @DirectXMan12 thanks for having a look! I updated the description and title here and in the commit message.\n. I don't know fairly anything about TLS mechanisms here or authorization in the genericapiserver, so I would be very grateful for help here.\n@mwringe @jimmidyson @DirectXMan12 would you mind taking a look and directing me somewhere?\nAlso, maybe the genericapiserver library itself provides enough tools for that?\n. Closing this in favor of #1301\n. @DirectXMan12 about the other two PRs - they were the trials of different approaches to splitting this task into smaller parts. We ended up with this as the best solution, so those will be either closed or updated after this one is finished.\n. @nikhiljindal as for the tests - there are no resources registered yet, so once we have them, we'll reuse the existing tests for the Metrics API on a different endpoint. Would that be ok? As for now, we would actually check only if the endpoint is reachable.\n. @nikhiljindal thanks for the review! The new version looks much better. I would rather update it in the follow-up PR, as it would require godeps update I guess.\nAs for the tests, we talked about it with @piosz and from what I understand, as the connection to the API server is secured, we need the generated client first to get there in a way kube API is accessed, am I right?\n. rebased\n. The default value here is \"/var/run/kubernetes\", which fails with permission denied, so I'm not sure what should be used here. Is there any particular place the certificates should go to?\n. Sorry, it could have been unclear - if it's not set, it defaults to what I mentioned above, \"var/run/kubernetes\", which causes the problem.\n. ",
    "pmorie": "@piosz PTAL\n. @k8s-bot test this\n. ",
    "anthonyhaussman": "I just update my heapster to last stable release v1.2.0 and my kubernetes HPA does not work anymore : \n```\n  2m        4s      8   {horizontal-pod-autoscaler }            Warning   \nFailedGetMetrics    failed to get CPU consumption and request: \nfailed to unmarshall heapster response: json: \ncannot unmarshal object into Go value of type []v1alpha1.PodMetrics\n2m        4s      8   {horizontal-pod-autoscaler }            Warning   \nFailedComputeReplicas   failed to get CPU utilization: \nfailed to get CPU consumption and request: failed to unmarshall heapster response: json: \ncannot unmarshal object into Go value of type []v1alpha1.PodMetrics\n```\nI have no problem with heapter v1.1.0.\nI use kubernetes v1.3.7.\n. ",
    "Kokan": "I had such an issue with logging details (heapster 1.2.0 version), as it turned out the kafka producer used for heapster has an option to provide logger, but non given. All of the kafka producer logs are thrown away. You can modify the code to provide brokerConf.Logger (in case of 1.2.0 metrics/sinks/kafka/driver.go). The additional logging have been fixed by #1684 . @huangyuqi ping. This is still an issue, and I have a patch somewhere for it, but that is an ugly hack.\nActually I think this is a more common issue, there should be a way for each sink to \"heal\" from such issue.\nWhat I mean when the ExportData is called, it could return a status, and if the command failed the manager could re-create that  DataSink. My patch had similar to this, but actually implemented inside ExportData that is not so clean solution. Also the current RiemannSink has this re-connection inside the ExportData.\nLike DataSink could have a Restart method and ExportData a return value, and the manager could call restart if needed. Would it be okay in this way ?\n. I've signed it.. /test all. LGTM. It would be a great opportunity for me.. @DirectXMan12 yes, see #1863 . > Is there a plan to support SASL/SSL? \nI have no plan to do it in the near feature, but glad to see such PR :)\n\nIf yes, can we propose PR to migrate to sarama to support it?\n\nOf course you can. But changes that does not improve, personally I do not like. Therefore it must improve something, while still provides the same set of features.\nA few thing also important:\n active development/community\n good compatibility\n* usable api\nAt first sight it looks promising to me, but I think to review most of those things I am going to review the PR and gave my feedback about the above mentioned.\n. I did not have time to check out sarama in depth, so I am going to do it this weekend, after that from my side it is good to go. (and some testing of course :)). I have check sarama, and it is a good candidate to replace the current one.\nGenerally the PR LGTM.\nOne note would be about the usageKafkaclient function name, it would be nice to replace with something that describes what it does, for example: removeSecrestsFromOpts or simply removeSecrets, or anything better than this one :)\n@DirectXMan12 could you enable testing for this PR ?. You do not need to, but if you feel like it :) (It would help readability.). Hello,\nYes. It is good enough not to have the comment before the function :-)\nThank you. I really appreciate it.. /lgtm. Is it possible that it is the same issue as #1672 ?. I think the root cause is simple -in both cases - that in case of error, the connection is not properly re-created. You could see that kind of behavior with other sinks, in case of failure the producer is re-created.\nI had a patch, but now it is lost at my former company private gitlab, and probably the core itself moved along also. If you want to address this I could give you more guide, but I do not see that I am going to have time to fix it.. I think it is understandable, if you simply use a constant value instead of sink.DefaultSinkExportDataTimeout.  You could get rid of the import \"k8s.io/heapster/metrics/sinks\".. I would rather use the SyncProducer.  I think here the asynchronous call to the producer does not solve any kind of issue. But usally async code tend to be more complex. The ProduceKafkaMessage is only called from one place, and not so often. It does not have to run in parallel either.. I liked this error message during debugging :) Also I have an issue I am trying to resolve, which would require to detect if we could send data to kafka. (See my point also with SyncProducer). I think the user should be able to configure the property, if not. Well it should exit and not default to something valid.\nBut either way I do not think it is a style of go to return a perfectly good result an a non-nil error.\nWould it be possible to fix that also in your PR ?\n. This is pretty far from where it is used, that makes it hard to read. Would you move where you use the tlsConfig ?. Would it be possible for getTlsConfiguration to return config.Net.TLS ?\nOr something like this? \ngo\nconfig.Net.TLS.Config, config.Net.TLS.Enable, err := getTlsConfiguration(opt)\nif err != nil {\n    return nil, err\n }. Same comments as for the TLS.. It should not use the log logger, but the glog, but either way this function should have just return with an error. It should not cause call directly Fatal because other sink might be functional.. Why would you like to change the verbose level ?\nAlso maybe not the best idea to print the password part of opts, it could be replaced by *** or something.. I do not think this function must exists after the changes.. I would hope that it cannot return with a valid value, because the error indicates that the rest of the information is void. I suggested without checking the kafka.CompressionCode being int8 to return nil. (That would be a perfect value imho.). I think the next good candidate would be kafka.CompressionUknown, which does not exists.\nFollowing the call chain, in case of this branch the kafka Sink won't be created either way.\nTo conclude my comment, well I think neither kafka.CompressionNone nor -1 is better then the other, but for different reason, and because there is no better alternative to me either is acceptable in the end.. ",
    "lattwood": "Is this going to resolve the issue in #1237 ?\nI0809 17:35:05.658704       1 rate_calculator.go:51] Skipping rates for namespace:default/pod:gumco-223788322-yeys9/container:gumco - different create time new:2016-08-09 17:08:03.148174048 +0000 UTC  old:2016-08-09 17:08:03.139173623 +0000 UTC\nI0809 17:35:05.658751       1 rate_calculator.go:51] Skipping rates for namespace:kube-system/pod:influxdb-grafana-dnwdh - different create time new:2016-08-09 17:07:51.450595189 +0000 UTC  old:2016-08-09 17:07:51.458595589 +0000 UTC\nI0809 17:35:05.658788       1 rate_calculator.go:51] Skipping rates for namespace:kube-system/pod:kubernetes-dashboard-3717423461-n00hf - different create time new:2016-08-09 17:07:52.062625845 +0000 UTC  old:2016-08-09 17:07:52.060625744 +0000 UTC\nI0809 17:35:05.658801       1 rate_calculator.go:51] Skipping rates for namespace:default/pod:kube-lego-2628658575-nbegw - different create time new:2016-08-09 17:25:09.245173745 +0000 UTC  old:2016-08-09 17:25:09.255174511 +0000 UTC\nI0809 17:35:05.658824       1 rate_calculator.go:51] Skipping rates for namespace:kube-system/pod:kube-controller-manager-ip-10-0-5-158.ec2.internal - different create time new:2016-08-09 17:07:53.482696741 +0000 UTC  old:2016-08-09 17:07:53.48469684 +0000 UTC\nI0809 17:35:05.658905       1 rate_calculator.go:51] Skipping rates for namespace:default/pod:kube-lego-2628658575-nbegw/container:kube-lego - different create time new:2016-08-09 17:27:40.598496929 +0000 UTC  old:2016-08-09 17:27:40.586496204 +0000 UTC\nI0809 17:35:05.658942       1 rate_calculator.go:51] Skipping rates for namespace:kube-system/pod:kube2iam-4ie1g/container:kube2iam - different create time new:2016-08-09 17:08:03.722201157 +0000 UTC  old:2016-08-09 17:08:03.732201629 +0000 UTC\nI0809 17:35:05.658968       1 rate_calculator.go:51] Skipping rates for namespace:default/pod:nginx-1537048879-eo10z - different create time new:2016-08-09 17:07:53.049675347 +0000 UTC  old:2016-08-09 17:07:53.047675248 +0000 UTC\nI0809 17:35:05.659031       1 rate_calculator.go:51] Skipping rates for namespace:kube-system/pod:heapster-1006675365-4psz8 - different create time new:2016-08-09 17:07:53.60270267 +0000 UTC  old:2016-08-09 17:07:53.623703708 +0000 UTC\nI0809 17:35:05.659088       1 rate_calculator.go:51] Skipping rates for namespace:kube-system/pod:influxdb-grafana-dnwdh/container:influxdb - different create time new:2016-08-09 17:07:51.708608106 +0000 UTC  old:2016-08-09 17:07:51.718608607 +0000 UTC\nI0809 17:35:05.659140       1 rate_calculator.go:51] Skipping rates for namespace:default/pod:gumco-223788322-yeys9 - different create time new:2016-08-09 17:07:51.76361086 +0000 UTC  old:2016-08-09 17:07:51.775611461 +0000 UTC\nI0809 17:35:05.659153       1 rate_calculator.go:51] Skipping rates for namespace:kube-system/pod:kube2iam-4ie1g - different create time new:2016-08-09 17:07:52.665656111 +0000 UTC  old:2016-08-09 17:07:52.675656613 +0000 UTC\nI0809 17:35:05.659221       1 rate_calculator.go:51] Skipping rates for namespace:kube-system/pod:kube-controller-manager-ip-10-0-5-158.ec2.internal/container:kube-controller-manager - different create time new:2016-08-09 17:07:53.829713887 +0000 UTC  old:2016-08-09 17:07:53.830713936 +0000 UTC\n. I've increased logging verbosity, and it appears to skip the metrics due to a small difference in container create times. I have no idea where to begin with this. :-/\nI0802 21:27:05.485307       1 rate_calculator.go:51] Skipping rates for namespace:default/pod:juicyfruit2015-3634674971-6y5pt/container:gum-company2015 - different create time new:2016-08-02 20:37:50.926259126 +0000 UTC  old:2016-08-02 20:37:50.921258978 +0000 UTC\n. Thanks\nKubernetes is 1.3.0, planning to upgrade to 1.3.4 soon.\nHeapster is v1.1.0.\nI'm in the kubernetes slack if you need me to poke and prod\n. @janwillies I had success with using the cgroupfs driver on the docker daemon\n. my linuxfoundation cla is now signed\n. This ends up filling the kubernetes HPA event log with these\n8m            17s             161     {horizontal-pod-autoscaler }                    Warning         FailedGetMetrics        failed to get CPU consumption and request: metrics obtained for 1/4 of pods (sample missing pod: staging/imposium-mysql-worker-2707898866-8arod)\n  8m            17s             164     {horizontal-pod-autoscaler }                    Warning         FailedComputeReplicas   failed to get CPU utilization: failed to get CPU consumption and request: metrics obtained for 1/4 of pods (sample missing pod: staging/imposium-mysql-worker-2707898866-8arod)\n  7m            17s             61      {horizontal-pod-autoscaler }                    Warning         FailedGetMetrics        failed to get CPU consumption and request: metrics obtained for 1/4 of pods (sample missing pod: staging/imposium-mysql-worker-2707898866-wkmxf)\n  7m            17s             59      {horizontal-pod-autoscaler }                    Warning         FailedComputeReplicas   failed to get CPU utilization: failed to get CPU consumption and request: metrics obtained for 1/4 of pods (sample missing pod: staging/imposium-mysql-worker-2707898866-wkmxf)\n  11m           1s              168     {horizontal-pod-autoscaler }                    Warning         FailedGetMetrics        failed to get CPU consumption and request: metrics obtained for 1/2 of pods (sample missing pod: staging/imposium-mysql-worker-2707898866-oll2v)\n  11m           1s              168     {horizontal-pod-autoscaler }                    Warning         FailedComputeReplicas   failed to get CPU utilization: failed to get CPU consumption and request: metrics obtained for 1/2 of pods (sample missing pod: staging/imposium-mysql-worker-2707898866-oll2v)\n  43m           0s              111     {horizontal-pod-autoscaler }                    Warning         FailedComputeReplicas   (events with common reason combined)\n  43m           0s              144     {horizontal-pod-autoscaler }                    Warning         FailedGetMetrics        (events with common reason combined)\n. Here's the output when stacktraces are enabled for listers.go:68\nW1007 14:16:03.358327       1 listers.go:68] can not retrieve list of objects using index : object has no meta: object does not implement the Object interfaces\ngoroutine 306 [running]:\nk8s.io/heapster/vendor/github.com/golang/glog.stacks(0x52fa700, 0x0, 0x0, 0x0)\n        /go/src/k8s.io/heapster/vendor/github.com/golang/glog/glog.go:766 +0xb8\nk8s.io/heapster/vendor/github.com/golang/glog.(*loggingT).output(0x52fa780, 0xc800000001, 0xc82084ee40, 0x518e695, 0xa, 0x44, 0x0)\n        /go/src/k8s.io/heapster/vendor/github.com/golang/glog/glog.go:675 +0xcb\nk8s.io/heapster/vendor/github.com/golang/glog.(*loggingT).printf(0x52fa780, 0x1, 0x3bfe5e0, 0x31, 0xc820d1f350, 0x1, 0x1)\n        /go/src/k8s.io/heapster/vendor/github.com/golang/glog/glog.go:655 +0x1d4\nk8s.io/heapster/vendor/github.com/golang/glog.Warningf(0x3bfe5e0, 0x31, 0xc820d1f350, 0x1, 0x1)\n        /go/src/k8s.io/heapster/vendor/github.com/golang/glog/glog.go:1094 +0x5d\nk8s.io/heapster/vendor/k8s.io/kubernetes/pkg/client/cache.ListAllByNamespace(0x7fcdceae1b08, 0xc820874a60, 0xc8208fec26, 0x7, 0x7fcdceb79140, 0xc821205060, 0xc820d1f4f8, 0x0, 0x0)\n        /go/src/k8s.io/heapster/vendor/k8s.io/kubernetes/pkg/client/cache/listers.go:68 +0x401\nk8s.io/heapster/vendor/k8s.io/kubernetes/pkg/client/cache.storePodsNamespacer.List(0x7fcdceae1b08, 0xc820874a60, 0xc8208fec26, 0x7, 0x7fcdceb79140, 0xc821205060, 0x0, 0x0, 0x0, 0x0, ...)\n        /go/src/k8s.io/heapster/vendor/k8s.io/kubernetes/pkg/client/cache/listers_core.go:62 +0xcb\nk8s.io/heapster/metrics/apis/metrics.podMetricsInNamespaceList(0xc820351800, 0xc820fcaae0, 0xc8208fed80, 0xc8208fec26, 0x7)\n        /go/src/k8s.io/heapster/metrics/apis/metrics/handlers.go:203 +0x464\nk8s.io/heapster/metrics/apis/metrics.(*Api).podMetricsList(0xc820351800, 0xc820fcaae0, 0xc8208fed80)\n        /go/src/k8s.io/heapster/metrics/apis/metrics/handlers.go:189 +0xb5\nk8s.io/heapster/metrics/apis/metrics.(*Api).(k8s.io/heapster/metrics/apis/metrics.podMetricsList)-fm(0xc820fcaae0, 0xc8208fed80)\n        /go/src/k8s.io/heapster/metrics/apis/metrics/handlers.go:78 +0x34\nk8s.io/heapster/vendor/github.com/emicklei/go-restful.(*Container).dispatch(0xc8204e25a0, 0x7fcdceb242d0, 0xc82022ab60, 0xc820e0e620)\n        /go/src/k8s.io/heapster/vendor/github.com/emicklei/go-restful/container.go:272 +0xf30\nk8s.io/heapster/vendor/github.com/emicklei/go-restful.(*Container).(k8s.io/heapster/vendor/github.com/emicklei/go-restful.dispatch)-fm(0x7fcdceb242d0, 0xc82022ab60, 0xc820e0e620)\n        /go/src/k8s.io/heapster/vendor/github.com/emicklei/go-restful/container.go:120 +0x3e\nnet/http.HandlerFunc.ServeHTTP(0xc820221c50, 0x7fcdceb242d0, 0xc82022ab60, 0xc820e0e620)\n        /usr/local/go/src/net/http/server.go:1618 +0x3a\nnet/http.(*ServeMux).ServeHTTP(0xc82020be30, 0x7fcdceb242d0, 0xc82022ab60, 0xc820e0e620)\n        /usr/local/go/src/net/http/server.go:1910 +0x17d\nk8s.io/heapster/vendor/github.com/emicklei/go-restful.(*Container).ServeHTTP(0xc8204e25a0, 0x7fcdceb242d0, 0xc82022ab60, 0xc820e0e620)\n        /go/src/k8s.io/heapster/vendor/github.com/emicklei/go-restful/container.go:287 +0x43\nnet/http.(*ServeMux).ServeHTTP(0xc82020b170, 0x7fcdceb242d0, 0xc82022ab60, 0xc820e0e620)\n        /usr/local/go/src/net/http/server.go:1910 +0x17d\nnet/http.serverHandler.ServeHTTP(0xc8201de900, 0x7fcdceb242d0, 0xc82022ab60, 0xc820e0e620)\n        /usr/local/go/src/net/http/server.go:2081 +0x19e\nnet/http.(*conn).serve(0xc82016aa00)\n        /usr/local/go/src/net/http/server.go:1472 +0xf2e\ncreated by net/http.(*Server).Serve\n        /usr/local/go/src/net/http/server.go:2137 +0x44e\n. @piosz FYI, @jbmcg will be following up on this with you, I believe he's also on the k8s Slack\n. ",
    "danielfm": "@DirectXMan12 I believe I'm running into a similar issue.\nHere's the /proc/self/cgroup from one of my pods:\n11:pids:/init.scope/system.slice/docker-beb8ae2859414c015b34d3e0670c9f59e7827abf50e3355accfda770381aac54.scope\n10:devices:/init.scope/system.slice/docker-beb8ae2859414c015b34d3e0670c9f59e7827abf50e3355accfda770381aac54.scope\n9:perf_event:/system.slice/docker-beb8ae2859414c015b34d3e0670c9f59e7827abf50e3355accfda770381aac54.scope\n8:hugetlb:/system.slice/docker-beb8ae2859414c015b34d3e0670c9f59e7827abf50e3355accfda770381aac54.scope\n7:cpu,cpuacct:/init.scope/system.slice/docker-beb8ae2859414c015b34d3e0670c9f59e7827abf50e3355accfda770381aac54.scope\n6:memory:/init.scope/system.slice/docker-beb8ae2859414c015b34d3e0670c9f59e7827abf50e3355accfda770381aac54.scope\n5:cpuset:/system.slice/docker-beb8ae2859414c015b34d3e0670c9f59e7827abf50e3355accfda770381aac54.scope\n4:net_cls,net_prio:/system.slice/docker-beb8ae2859414c015b34d3e0670c9f59e7827abf50e3355accfda770381aac54.scope\n3:freezer:/system.slice/docker-beb8ae2859414c015b34d3e0670c9f59e7827abf50e3355accfda770381aac54.scope\n2:blkio:/init.scope/system.slice/docker-beb8ae2859414c015b34d3e0670c9f59e7827abf50e3355accfda770381aac54.scope\n1:name=systemd:/system.slice/docker.service\nI'm running Kubernetes 1.3.4 (kube-aws 0.8.1) / Heapster 1.1.0 / Docker 1.10.3 / CoreOS 1068.8.0 (stable)\n. ",
    "gaddamidianil": "@DirectXMan12 \nI assume your analysis is right.\nwhen multiple pods running, its not capturing the metrics. When I tried with single pod, heapster doesnt show 'no metrics error' and hpa works fine. \nLooking forward for solution on multiple pods.\n. ",
    "Thermi": "Oh come on, at least write this somewhere in the manual or fix it in the code, god damnit.\nUpstream already had a PR because of that problem and it says why it breaks, too: olivere/elastic#193\n. visualizations.json.zip\n. @andrejvanderzee The availability of certain fields in the metrics category depends on the value of MetricsTags.type and other things. In our case, the fields you mentioned exist. What version of kubernetes are you using and what is the pod definition?\n. @andrejvanderzee \n\nWe do get the values 99,9% of the time for the same POD instances.\n\ncpu/request and cpu/limit?\n\nBut sometimes we get huge negative outliers which makes our graphs unreadable.\n\nAre you talking about cpu/limit_usage right now?\n\nUnfortunately we are on Rhel Atomic Host which now runs Kube 1.2.\n\nWe are on 1.3 right now. We'll see what comes up.\n. /remove-lifecycle stale. /lifecycle frozen. I encountered this issue as well. Please fix. The reported memory metrics are not useful in the current state.\nIn my scenario, I use the new elasticsearch sink that @almogbaku in #1313 implemented.\nWe previously found out that the memory usage that heapster reports not accurately represents the actual used memory as indicated by free on the corresponding host. It is important that heapster reports the memory usage MINUS the cache.\n. WorkingSet doesn't correspond to the actual \"used\" -+ buffers/cache output of free.\nFor comparison, here's one heapster metric and from free at the same time\n{\n  \"_index\": \"heapster-2016.11.04\",\n  \"_type\": \"memory\",\n  \"_id\": \"f3f9d8c2-a2ab-11e6-9941-2aa42b43b130\",\n  \"_score\": null,\n  \"_source\": {\n    \"MemoryMetricsTimestamp\": \"2016-11-04T16:30:00Z\",\n    \"Metrics\": {\n      \"memory/limit\": {\n        \"value\": 1472200704\n      },\n      \"memory/major_page_faults\": {\n        \"value\": 151\n      },\n      \"memory/major_page_faults_rate\": {\n        \"value\": 0\n      },\n      \"memory/node_allocatable\": {\n        \"value\": 3950506000\n      },\n      \"memory/node_capacity\": {\n        \"value\": 3950506000\n      },\n      \"memory/node_reservation\": {\n        \"value\": 0.31692135\n      },\n      \"memory/node_utilization\": {\n        \"value\": 0.81257236\n      },\n      \"memory/page_faults\": {\n        \"value\": 37131328\n      },\n      \"memory/page_faults_rate\": {\n        \"value\": 10.287301\n      },\n      \"memory/request\": {\n        \"value\": 1251999744\n      },\n      \"memory/usage\": {\n        \"value\": 3210072064\n      },\n      \"memory/working_set\": {\n        \"value\": 2282536960\n      }\n    },\n    \"MetricsTags\": {\n      \"host_id\": \"i-79c274f2\",\n      \"hostname\": \"ip-10-0-0-67.eu-west-1.compute.internal\",\n      \"labels\": \"beta.kubernetes.io/arch:amd64,beta.kubernetes.io/instance-type:m3.medium,beta.kubernetes.io/os:linux,failure-domain.beta.kubernetes.io/region:eu-west-1,failure-domain.beta.kubernetes.io/zone:eu-west-1a,kubernetes.io/hostname:ip-10-0-0-67.eu-west-1.compute.internal\",\n      \"nodename\": \"ip-10-0-0-67.eu-west-1.compute.internal\",\n      \"type\": \"node\"\n    }\n  },\n  \"fields\": {\n    \"MemoryMetricsTimestamp\": [\n      1478277000000\n    ]\n  },\n  \"highlight\": {\n    \"MetricsTags.hostname.raw\": [\n      \"@kibana-highlighted-field@ip-10-0-0-67.eu-west-1.compute.internal@/kibana-highlighted-field@\"\n    ],\n    \"MetricsTags.type\": [\n      \"@kibana-highlighted-field@node@/kibana-highlighted-field@\"\n    ],\n    \"MetricsTags.nodename.raw\": [\n      \"@kibana-highlighted-field@ip-10-0-0-67.eu-west-1.compute.internal@/kibana-highlighted-field@\"\n    ]\n  },\n  \"sort\": [\n    1478277000000\n  ]\n}\nip-10-0-0-67 core # free \n             total       used       free     shared    buffers     cached\nMem:       3857916    3751744     106172       1460     237904    1735816\n-/+ buffers/cache:    1778024    2079892\nSwap:            0          0          0\n. I opened an issue here: https://github.com/google/cadvisor/issues/1529\nFeel free to weigh in on it.\n. @AlmogBaku we can certainly export the complete dashboard and all visualizations.\n. I looked at our coreos setup and this is what we have:\nsystemctl cat docker.service\n[...]\nEnvironmentFile=-/run/flannel_docker_opts.env\nMountFlags=slave\nExecStart=/usr/lib/coreos/dockerd daemon --host=fd:// $DOCKER_OPTS $DOCKER_CGROUPS $DOCKER_OPT_BIP $DOCKER_OPT_MTU $DOCKER_OPT_IPMASQ\n[...]\n/run/flannel_docker_opts.env:\nDOCKER_OPT_BIP=\"--bip=10.2.53.1/24\"\nDOCKER_OPT_IPMASQ=\"--ip-masq=false\"\nDOCKER_OPT_MTU=\"--mtu=8951\"\n$ docker --version\nDocker version 1.11.2, build bac3bae\n@euank Which coreos version or docker version is recent enough and does adding --exec-opt native.cgroupdriver=systemd fix or cause the problem?\n. kubernetes 1.3, heapster 1.2 beta. cadvisor, as far as I am aware.. We can't test heapster 1.2, because we need to use @AlmogBaku's new mapping. \nheapster is running with \"--source=kubernetes:https://kubernetes.default\".. /remove-lifecycle stale\n/lifecycle frozen. ",
    "18612116089": "I had this problem yet. Anyone else can help ?\n. @daikaixian \nyes, use v1.1.0 can solve it.\nThanks very much.\n. ",
    "daikaixian": "@18612116089\nit may because you  are using  a older version of heapster.\ntry to use a newer version.\nsuch as 1.1.0\n. ",
    "sarahnovotny": "LGTM.  Thanks for taking this on.\n. ",
    "yanyanhu": "Sorry mis operated...\n. ",
    "zqfan": "+1, version v1.2.0, k8s v1.4.0\n. @hekike after read the source code, I think it is limited by k8sclient\n. ",
    "hekike": "Any idea why is it happening? Did you find the solution?\n. ",
    "wangweihong": "I met this error too. I am using heapster v1.2.0-beta.1 on kubernetes v1.4.0-alpha.2.\nThis is my heapster containers' logs:\n\nroot@ubuntu192:~/heapster/deploy/kube-config/influxdb# docker logs e603\nI0912 02:36:21.438528       1 heapster.go:69] /heapster --source=kubernetes:http://192.168.14.100:8080?inClusterConfig=false --sink=influxdb:http://monitoring-influxdb:8086\nI0912 02:36:21.438625       1 heapster.go:70] Heapster version 1.2.0-beta.1\nI0912 02:36:21.438652       1 configs.go:60] Using Kubernetes client with master \"http://192.168.14.100:8080\" and version v1\nI0912 02:36:21.438669       1 configs.go:61] Using kubelet port 10255\nE0912 02:38:28.777790       1 influxdb.go:217] issues while creating an InfluxDB sink: failed to ping InfluxDB server at \"monitoring-influxdb:8086\" - Get http://monitoring-influxdb:8086/ping: dial tcp 12.18.70.52:8086: getsockopt: connection timed out, will retry on use\nI0912 02:38:28.777838       1 influxdb.go:231] created influxdb sink with options: host:monitoring-influxdb:8086 user:root db:k8s\nI0912 02:38:28.777882       1 heapster.go:99] Starting with InfluxDB Sink\nI0912 02:38:28.777893       1 heapster.go:99] Starting with Metric Sink\nI0912 02:38:28.789627       1 heapster.go:189] Starting heapster on port 8082\nI0912 02:39:05.090404       1 influxdb.go:209] Created database \"k8s\" on influxDB server at \"monitoring-influxdb:8086\"\n\nI deploy my heapster by using yaml files under heapster/deploy/kube-config/influxdb.  and I change heapster-controller.yaml file like this\uff1a\n\nspec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes:http://192.168.14.100:8080?inClusterConfig=false\n        - --sink=influxdb:http://monitoring-influxdb:8086\nand expose all port used by service files.\n. @DirectXMan12 \n\nI set Hespater RC command like this:\n\ncommand:\n    - /heapster\n    - --source=kubernetes:http://192.168.14.100:8080?inClusterConfig=false\n    - --sink=influxdb:http://monitoring-influxdb:8086\n    - --v=5\n\nThis is logs I got.\nroot@ubuntu192:~/heapster/deploy/kube-config# docker logs d358\nI0913 00:38:07.718830       1 heapster.go:69] /heapster --source=kubernetes:http://192.168.14.100:8080?inClusterConfig=false --sink=influxdb:http://monitoring-influxdb:8086 --v=5\nI0913 00:38:07.718931       1 heapster.go:70] Heapster version 1.2.0-beta.1\nI0913 00:38:07.718958       1 configs.go:60] Using Kubernetes client with master \"http://192.168.14.100:8080\" and version v1\nI0913 00:38:07.718974       1 configs.go:61] Using kubelet port 10255\nI0913 00:38:07.724236       1 reflector.go:202] Starting reflector *api.Node (1h0m0s) from k8s.io/heapster/metrics/sources/kubelet/kubelet.go:339\nI0913 00:38:07.724486       1 reflector.go:253] Listing and watching *api.Node from k8s.io/heapster/metrics/sources/kubelet/kubelet.go:339\nE0913 00:40:15.017765       1 influxdb.go:217] issues while creating an InfluxDB sink: failed to ping InfluxDB server at \"monitoring-influxdb:8086\" - Get http://monitoring-influxdb:8086/ping: dial tcp 12.18.26.20:8086: getsockopt: connection timed out, will retry on use\nI0913 00:40:15.017811       1 influxdb.go:231] created influxdb sink with options: host:monitoring-influxdb:8086 user:root db:k8s\nI0913 00:40:15.017841       1 heapster.go:99] Starting with InfluxDB Sink\nI0913 00:40:15.017859       1 heapster.go:99] Starting with Metric Sink\nI0913 00:40:15.018263       1 reflector.go:202] Starting reflector *api.Pod (1h0m0s) from k8s.io/heapster/metrics/heapster.go:272\nI0913 00:40:15.018317       1 reflector.go:202] Starting reflector *api.Node (1h0m0s) from k8s.io/heapster/metrics/heapster.go:280\nI0913 00:40:15.018502       1 reflector.go:253] Listing and watching *api.Pod from k8s.io/heapster/metrics/heapster.go:272\nI0913 00:40:15.018597       1 reflector.go:253] Listing and watching *api.Node from k8s.io/heapster/metrics/heapster.go:280\nI0913 00:40:15.018566       1 reflector.go:202] Starting reflector *api.Namespace (1h0m0s) from k8s.io/heapster/metrics/processors/namespace_based_enricher.go:84\nI0913 00:40:15.018958       1 reflector.go:202] Starting reflector *api.Node (1h0m0s) from k8s.io/heapster/metrics/processors/node_autoscaling_enricher.go:96\nI0913 00:40:15.019078       1 reflector.go:253] Listing and watching *api.Namespace from k8s.io/heapster/metrics/processors/namespace_based_enricher.go:84\nI0913 00:40:15.019092       1 reflector.go:253] Listing and watching *api.Node from k8s.io/heapster/metrics/processors/node_autoscaling_enricher.go:96\nI0913 00:40:15.028949       1 heapster.go:189] Starting heapster on port 8082\nI0913 00:41:05.000324       1 manager.go:79] Scraping metrics start: 2016-09-13 00:40:00 +0000 UTC, end: 2016-09-13 00:41:00 +0000 UTC\nI0913 00:41:05.012757       1 manager.go:98] Querying source: kubelet:192.168.14.99:10255\nI0913 00:41:05.016719       1 manager.go:98] Querying source: kubelet:192.168.14.101:10255\nI0913 00:41:05.020291       1 kubelet.go:232] successfully obtained stats for 1 containers\nI0913 00:41:05.020796       1 manager.go:98] Querying source: kubelet:192.168.14.100:10255\nI0913 00:41:05.022521       1 kubelet.go:232] successfully obtained stats for 1 containers\nI0913 00:41:05.042467       1 kubelet.go:232] successfully obtained stats for 72 containers\n. When I select default namespace, It shows some pods (not all pods ) in kube-system namespace, kubernetes-dashboard-* and  kube-dns-*,  instead of real pods in default namespace.\nAnd I got this  When I select default namespace ,\n\nI0913 00:57:05.000339       1 manager.go:79] Scraping metrics start: 2016-09-13 00:56:00 +0000 UTC, end: 2016-09-13 00:57:00 +0000 UTC\nI0913 00:57:05.002737       1 manager.go:98] Querying source: kubelet:192.168.14.101:10255\nI0913 00:57:05.004853       1 kubelet.go:232] successfully obtained stats for 1 containers\nI0913 00:57:05.019721       1 manager.go:98] Querying source: kubelet:192.168.14.99:10255\nI0913 00:57:05.023690       1 manager.go:98] Querying source: kubelet:192.168.14.100:10255\nI0913 00:57:05.024931       1 kubelet.go:232] successfully obtained stats for 1 containers\nI0913 00:57:05.083551       1 kubelet.go:232] successfully obtained stats for 72 containers\nI0913 00:57:05.084075       1 manager.go:152] ScrapeMetrics: time: 83.58848ms size: 74\nI0913 00:57:05.084097       1 manager.go:154]    scrape  bucket 0: 3\nI0913 00:57:05.084102       1 manager.go:154]    scrape  bucket 1: 0\nI0913 00:57:05.084105       1 manager.go:154]    scrape  bucket 2: 0\nI0913 00:57:05.084108       1 manager.go:154]    scrape  bucket 3: 0\nI0913 00:57:05.084112       1 manager.go:154]    scrape  bucket 4: 0\nI0913 00:57:05.084115       1 manager.go:154]    scrape  bucket 5: 0\nI0913 00:57:05.084124       1 manager.go:154]    scrape  bucket 6: 0\nI0913 00:57:05.084127       1 manager.go:154]    scrape  bucket 7: 0\nI0913 00:57:05.084137       1 manager.go:154]    scrape  bucket 8: 0\nI0913 00:57:05.084140       1 manager.go:154]    scrape  bucket 9: 0\nI0913 00:57:05.084144       1 manager.go:154]    scrape  bucket 10: 0\nI0913 00:57:05.084555       1 manager.go:113] Pushing data to: Metric Sink\nI0913 00:57:05.084568       1 manager.go:113] Pushing data to: InfluxDB Sink\nI0913 00:57:05.084579       1 manager.go:116] Data push completed: Metric Sink\nI0913 00:57:05.084582       1 manager.go:116] Data push completed: InfluxDB Sink\nI0913 00:57:05.111846       1 influxdb.go:169] Exported 782 data to influxDB in 26.759744ms\nI0913 00:57:14.027026       1 reflector.go:407] k8s.io/heapster/metrics/heapster.go:280: Watch close - *api.Node total 177 items received\n. >  root@ubuntu192:~/heapster/deploy/kube-config# kubectl cluster-info \nKubernetes master is running at http://192.168.14.100:8080\nHeapster is running at http://192.168.14.100:8080/api/v1/proxy/namespaces/kube-system/services/heapster\nKubeDNS is running at http://192.168.14.100:8080/api/v1/proxy/namespaces/kube-system/services/kube-dns\nmonitoring-grafana is running at http://192.168.14.100:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana\n\nIf you mean <Heapster>/metrics/api/v1/model/namespaces or <Heapster>/metrics/api/v1/model/namespaces/default/pods,\nno, I can't get any infos. These urls both return 404 page not found. only http://192.168.14.100:8080/api/v1/proxy/namespaces/kube-system/services/heapster/metrics andhttp://192.168.14.100:8080/metrics  can get infos.\n. ",
    "nicolasbelanger": "+1 for this one.  \nI have a Kubernetes 1.3.7 cluster (using kops) on which I run heapster 1.1.0 and I have basically the same behaviour as @amalkasubasinghe  and @wangweihong ... Can't see my 'custom' namespaces...\n. That's what I did actually, here is my deployment manifest...\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: \"2\"\n  creationTimestamp: null\n  generation: 1\n  labels:\n    k8s-addon: monitoring-standalone.addons.k8s.io\n    k8s-app: heapster\n    version: v1.1.0\n  name: heapster\n  selfLink: /apis/extensions/v1beta1/namespaces//deployments/heapster\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: heapster\n      version: v1.1.0\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        k8s-app: heapster\n        version: v1.1.0\n    spec:\n      containers:\n      - command:\n        - /heapster\n        - --source=kubernetes.summary_api:''\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --v=5\n        image: gcr.io/google_containers/heapster:v1.1.0\n        imagePullPolicy: IfNotPresent\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 100m\n            memory: 300Mi\n        terminationMessagePath: /dev/termination-log\n      - command:\n        - /pod_nanny\n        - --cpu=80m\n        - --extra-cpu=0.5m\n        - --memory=140Mi\n        - --extra-memory=4Mi\n        - --threshold=5\n        - --deployment=heapster-v1.1.0\n        - --container=heapster\n        - --poll-period=300000\n        - --estimator=exponential\n        env:\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        image: gcr.io/google_containers/addon-resizer:1.3\n        imagePullPolicy: IfNotPresent\n        name: heapster-nanny\n        resources:\n          limits:\n            cpu: 50m\n            memory: 100Mi\n          requests:\n            cpu: 50m\n            memory: 100Mi\n        terminationMessagePath: /dev/termination-log\n      dnsPolicy: ClusterFirst\n      restartPolicy: Always\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\nstatus: {}\n. Guys, it looks like it's only a matter of populating the namespace drop-down in the UI... You can search the wanted namespace instead of the pre-defined \"default and kube-system\" and then search the pod and the data comes up... It would also be interesting to have the pod list filtered by namespace (worse, currently, the pod list shows them all, but no data will come up if the namespace does not fit the pod it is currently in...)\n. @Vernlium It's funny cuz I found out the fix/issue lately.  For some reason, the default datasource, in this case influxdb, is not selected in the variable definition.  As soon as I set it, boom, all the namespaces will be populated in the drop-down. \n\n. @Vernlium It's funny cuz I found out the fix/issue lately.  For some reason, the default datasource, in this case influxdb, is not selected in the variable definition.  As soon as I set it, boom, all the namespaces will be populated in the drop-down. \n\n. ",
    "guiocavalcanti": "I'm having the same problem using Kubernetes 1.8.7 on CoreOS. \n. ",
    "eviloop": "Hello,\nI noticed that the default dashboard has the refresh of the namespace dropbox query set to false/never, therefore it doesn't load the full namespace list or new namespaces.\nHave you checked if this is also the source of your problem ?\nThis can be solved by setting it to true via:\n- the dashboard UI itself - edit the dashboard template and then edit the namespace query\n- changing the dashboard file - search for the word \"refresh\" : \"false\" under the dashboard template\nHowever i do have a question, is the namespace dropbox technically necessary ? It doesn't help much since the user has to \"manually\" match the namespace with the Pod.\nIt would make more sense, in my opinion, to have a dashboard just for namespaces and an other for pods. We will be trying this custom dashboard for our cluster.\n. ",
    "Vernlium": "@nicolasbelanger  it's work\n\nin here put your namespace. ",
    "bg1szd": "@nicolasbelanger  Thanks for your workaround. I met the same issue. And with your work around, the non-default namespace could be listed.. ",
    "abonillasuse": "As @nicolasbelanger said. Edit the template, change the datasource to influxdb-datasource AND change the refresh option as well, else they won't show up. Then Save.\n. ",
    "rootsongjc": "@nicolasbelanger Thanks you, it works! Setting grafana dashboard template datasource with influxdb-datasource and refresh with on Dashboard Load, save the template. Refresh browser.  You will see the other namespaces.. ",
    "zreigz": "cc @luxas \n. ",
    "luxas": "I have scripts already that shows how to cross-compile heapster, grafana and influxdb for all arches here: https://github.com/luxas/kubernetes-on-arm/tree/master/addons/images\nping @kubernetes/heapster-maintainers is this something you'd be willing to merge in v1.2?\nI can probably make a PR for that probably, with full backwards compability.\nAfter all, it won't be any functional changes, just release infrastructure changes,\n. @brendandburns Well, kubernetesonarm/heapster:0.8.0 is v1.2.0-beta.2 of heapster. 0.8.0 is the release of Kubernetes on ARM it was shipped with. Also there are kubernetesonarm/grafana:0.8.0 which is 3.1.1 and kubernetesonarm/influxdb:0.8.0 which is at version v0.13.0.\nHowever, it's of course best if we have official images for them, so here we go: #1387 \nI planned to do it earlier, but fully focused on sig-cluster-lifecycle here in between.\nCheers, Lucas\n. I would not switch to Glide in any k8s official projects before we have done it for core (if we really want that, I'm not sure). Actually I'm in favor for committing the vendor dir into the project, because once you've downloaded the project with git, you don't have to fetch it with glide in scripts or manually.\ncc @thockin for thoughts on glide\nThis will not at least make the next release, and I don't think there is any argument for doing so either.. This is fixed with #1387 . Is this fixed?. Not specifically, just checked the status.\nMaybe the help-wanted label would make sense then?. I might have done something already for this, see: https://github.com/kubernetes/heapster/pull/1387. Please close this @piosz, it's already updated and fixed at HEAD. @vishh @kubernetes/heapster-maintainers This PR is now ready for review.\nI'd like to get it merged soon, as we're planning to add the heapster deployment by default to kubeadm.\nIt's easy for you to just merge this and push the influxdb and grafana containers.\nBut we can discuss what we should do with the heapster image.\nI think we have two options:\na) You checkout the v1.2.0 Go code, but use this releasing/deployment code when pushing the image\nb) After this PR is merged, I'll send a PR that targets the release-1.2 branch, and you can then release v1.2.1 with that commit.\nAlso, I'd like to know if you'd like that the amd64 images stay the same, or if they should be swapped to this \"new\" code (that's risky, but you choose)\nPlease take a look as soon as possible.. @piosz I think you and @vishh would be perfect :). ping @vishh @piosz \nI hope we'll be able to merge this PR this week so we can release a v1.3 version next week or so. I'll rebase when you say you have time to look at it. @vishh @piosz Rebased, but this is a WIP item, still testing locally\nI guess I'll have it ready tomorrow or on Thursday\nDo we want to have influxdb 1.1.1 in the next release? I think that would be great!\nIn that case, merge #1415 now, before this one, but do not push the image, since I would build on top of that PR.. @timstclair votes for using the new images that I wrote that are of minimal size, and I vote for that too.\n@vishh does it make sense to you to switch to those simpler images as well?\nThat would also reduce the amount of code that's required.. Updated this now. This works great on amd64!\nPlease take a look and merge today so we can test the new release soon.\n@vishh @piosz @DirectXMan12 . @piosz Please merge this now so you feel comfortable with it in time for the release.\nIt's ready to merge, and simplifies things a lot while adding the manifest list functionality, which means docker will pull the right layers for the right arch automatically.\nPing me if you have any comments, but I've worked on this for so long now so it's quite robust. @piosz Feel free to merge this.\nIt worked fine for @vishh \nA build-all or something make target that would build for all arches would be an improvement, but that's only a slight improvement.\nThe default build-only behaviour is still working with container or build.\nI'm excited to get this merged!. @piosz Just make sure you docker login to your GCR account before you push everything (i.e. you can't use gcloud docker only.. Yes, and for all architectures :)\nThat's just a matter of uploading binaries to Github, so that's easy. This is already fixed. I have cleaned all images up PR https://github.com/kubernetes/heapster/pull/1387 for the other architectures, and it works fine when using those images.\nI think we should be able to just use my updated/alternative images all the time; for amd64 as well.. I've heard @thockin pray earlier that there should always be a base image :)\nWe need busybox for grafana, but not really for the others.\nWhat are your main arguments for ripping the base image away? Space? Security?\nThe downside is that it gets harder to debug at least. This is fixed now in #1387\ncc @piosz please close. I'm in favor for updating as well, let's see if we can get it into the new release!\ncc @piosz @vishh @DirectXMan12 . @jackzampolin Do we need to update the influxdb Godeps as well?\nIf we say that heapster v1.3 supports influxdb v1.1 I think it should be fine.\nI guess it would be great to get onto the v1.x train, we want to avoid v0.x versions for sure if possible.. @jackzampolin Did you test it carefully as well?. Then it looks good to me :+1: \ncc @vishh @piosz @DirectXMan12 . Ah, hmm. I meant you should update the Godeps directory. @jackzampolin ^^\n@DirectXMan12 Race condition, commented at the same time :smile:. @piosz Can you make a v1.3 milestone so we see what's left to merge into v1.3?\nCan you release a beta.1 today as well? And while pushing the beta.1 images, please also push influxdb v1.1.1 and grafana v4.0.2 so we can test the influxdb deployment officially, it should \"just work\" :)\nLastly we also need to aggregate a changelog between v1.2 and v1.3.\ncc @Crassirostris . Cool, please add issues/PRs that you think should be in the next release to that milestone. Travis is green :+1: . No actually not, strange that Godeps worked for me. Or it failed on lots of other things that I'm fixing in PR #1454, but not this. @wxb0521 Please rebase before this can be merged. @piosz Can we please get https://github.com/kubernetes/heapster/pull/1415 in first?. I think that would be a good thing to do.\nIf it totally fails, we'll just revert and use ~0.13. Also, we should get my PR in for the beta. LGTM, but I do not have write access yet.\nI think we should a) update or b) remove the riemann and kafka images\nProbably the latter; since the kafka image has not been updated in more than a year and does likely not work.\ncc @huangyuqi\nEven worse, the riemann image has not been updated for more than one year and a half. cc @jfoy \nI propose to remove these images, these implementations seem to be a better fit outside of the main heapster repo, since the k8s team is not maintaining those images.\n@mwielgus @piosz I can send a such PR if we decide that. I strongly think we should remove them for the v1.3 release. If someone comes and updates everything in time for an eventual v1.4 that might be fine if he/she commits to maintaining them as well.. @ixdy Please update this to include only the heapster, influxdb and grafana change.\n@DirectXMan12 There were other changes as well, not only for the riemann image. Please reopen.. @piosz @Crassirostris Please wait one second with this until we've got #1454 merged\n@VAdamec @alirezaDavid @tobad357 These images were committed to the repo some days ago in #1387, and we will push the images soon. This all will be fixed with and will be a part of v1.3.. v0.13.0 is new, but will directly be updated to v1.1.1\nalso v2.6.0-2 is old, we should just push what we have tested with heapster v1.3, and I'm sure these guys are happy with the latest and greatest stuff that works with heapster v1.3 :)\nThe plan is: heapster v1.3, influxdb: v1.1.1 and grafana v4.0.2, and that will happen soon. I'll make the last changes needed to this, and then we can merge this.\nI'll ping you when ready. @piosz This is ready to merge now. LGTM. @piosz . Thank you @jgoclawski, this is great!\nLGTM (and extra plus for the gpg-signed commit! :smile:)\n@piosz Please merge into the release. @tonyjchong Thanks for the issue and sorry for the confusion.\nWe're planning to make this clear with the v1.3 release, see: https://github.com/kubernetes/heapster/issues/1453#issuecomment-272007293. @piosz Commits squashed. @Crassirostris Please push these images (grafana:v4.0.2 and influxdb:v1.1.1). LGTM. I've contacted some gcr.io folks, I'm waiting for an answer from them. \nIt works fine on docker hub and should work on gcr as well, but doesn't\nMeanwhile, heapster-amd64:v1.3.0-beta.1 is available. Sent a PR: #1502\n. @piosz This was fixed by inactivating the functionality temporarily, we can now close this. @vielmetti please send a PR with the small change to re-enable manifest lists :). @vielmetti Please send a PR to fix this or talk to @mkumatag . This has been LGTM since you fixed the nits.\nHaven't tried it out manually though, which I assume you have.... @rutsky That's right. Do you want to send a PR that updates it to v1.3.0-alpha.1?. @rutsky Sorry, that was a typo. v1.3.0-beta.1, yes please. I don't think that explicit canary Dockerfile is needed, instead we can use the normal Dockerfile for building a canary image just by a retagging. No objections from me. I'm fine with this :+1:. @adambkaplan Could you bump the grafana version to the latest at the same time?. Backwards compatibility must be kept as @andyxning pointed out.... also, please squash your commits :). @piosz PTAL and merge. flake or real error?. Should be fixed with changing back to arm.... PR made, PTAL. @andyxning Why not update the others too?\nSure, it can be done separately, but they should be updated anyway so why not? . Done. ping @piosz @DirectXMan12 @mwielgus Please merge and push new images. ping @piosz @DirectXMan12 @mwielgus . Yes, closing. https://github.com/kubernetes/heapster/blob/master/deploy/kube-config/influxdb/heapster.yaml. /lgtm\nping @piosz @DirectXMan12 @mwielgus . Thanks @miry for picking that up! I haven't just had a chance to fixup my earlier PR. Just noting that the docker push $(PREFIX)/heapster-influxdb-$(ARCH):$(VERSION) -> docker push $(PREFIX)/heapster-influxdb-$*:$(VERSION) change is required in the makefile though.. @piosz Please push all the new images created by @miry's other PR. I can't, @piosz or some other Googler can. We're just waiting for a Googler to push the image\ncc @caesarxuchao @vishh @piosz @nicksardo or someone else, could you please push the images?. If you have a problem, report a bug\n\nOn 11 Sep 2017, at 03:20, Joseph Heck notifications@github.com wrote:\n@luxas @piosz is it now functional for you? I just tried this evening, and only now found this bug. I've been debugging (and could use help debugging) from #1804. I'm not sure the images that were pushed are \"good\"...\nWould love to hear I'm off base and missing something obvious.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. This is now fixed as @piosz pushed the images, ref https://github.com/kubernetes/heapster/issues/1783. The maintainer statement is deprecated https://github.com/docker/docker/pull/25466, also we just did remove all MAINTAINER statements from core k8s dockerfiles. This is totally outdated and will look otherwise when he has rebased, but yes a new image has to be pushed.. Yes, please have this inside an if statement that checks if RetentionPolicy != \"0\". I did this for backwards compability since earlier versions are released as heapster:${version}\n\nGrafana and influxdb completely changed image names, so it's safe to require the -amd64 suffix there until gcr.io starts working as it should.. Is this known to work universally?. You can remove the above definitions of BASEIMAGE as well I think. Is there any harm done if it's applied/created when RBAC is not in use?. Ah, gotcha. Updated. unnecessary tab. I think using len(envParams[\"gf_security_admin_user\"]) != 0 is a much more common pattern for checking this.... @piosz Indeed an invalid change. I can't remember seeing that when I reviewed, probably came later in the review process.. yes, this was an oversight from earlier.\n$* resolves to the % parameter, which is the arch being built. $(ARCH) in this case resolves statically to amd64 every time . Basically the newer kube-cross image has only the armv7 variant (armhf) of gcc.\nEarlier versions of kube-cross came bundled with the armv5 (armel) variant of gcc. nit: https://github.com/influxdata/influxdb/releases/tag/v1.3.3 is out now. GOARCH=BUILDARCH?. s|BUILDARCH|$(ARCH)|g. ",
    "jmcarp": "Signed it!\n. ",
    "bprashanth": "if this is for the dns issue I believe it's fixed in later versions of alpine (>=3.4 from memory)\n. ",
    "clhodapp": "See https://github.com/rancher/rancher/issues/4451#issuecomment-245676039\n. The test I performed at the end of https://github.com/rancher/rancher/issues/4451#issuecomment-245698975 presents pretty direct evidence that there is still an issue with DNS in Alpine.\n. ",
    "Max-bazinga": "@DirectXMan12 Yes, the individual containers has values. But I want to filter the type equal to  \"node\" to get the host's value.\n. ",
    "Jaware": "it seems as if it connected the api server using the wrong host url,it should be masterUrl+/api/vi/proxy.....but in my situation,the host url is the grafana service url\nwhich is http://10...78:30304/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/public/app/app.ca0ab6f9.js,but my masterUrl is http://10...12:8080\n. i do not know how to sign it ,can anyone show me how?\n. can anyone merge this request?my google account can not login for a period of time,ps i am chinese\n. modify the  url in the yaml  from /api...... to /,ps i have sent a pull request,but they closed because of my lack of  google licence\n. @piosz  i think proxy way is not very straight,why not use mine ?the proxy url is too long \ud83d\ude04~\n. ",
    "valentin2105": "Same problem, unable to use Heapster & Grafana on Kubernetes v1.3.6.\nHere is grafana's logs :\nt=2016-09-30T11:35:25+0000 lvl=info msg=\"Request Completed\" logger=context userId=0 orgId=1 uname= method=GET path=/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/public/app/boot.1649cbaa.js status=404 remote_addr=10.240.0.30 time_ns=0 size=19\nTry as a NodePort and as a ClusterIP. \nAny idea? \n. kubectl get node -o yaml\n```\n- apiVersion: v1\n  kind: Node\n  metadata:\n    annotations:\n      node.alpha.kubernetes.io/ttl: \"0\"\n      volumes.kubernetes.io/controller-managed-attach-detach: \"true\"\n    creationTimestamp: 2017-10-16T04:58:50Z\n    labels:\n      beta.kubernetes.io/arch: amd64\n      beta.kubernetes.io/os: linux\n      kubernetes.io/hostname: texas.k8s.srv.example.com\n    name: texas.k8s.srv.example.com\n    namespace: \"\"\n  spec:\n    externalID: texas.k8s.srv.example.com\n  status:\n    addresses:\n    - address: 1204:f200:d:f:c8bc:23ff:ffee:f71f\n      type: InternalIP\n    - address: texas.k8s.srv.example.com\n      type: Hostname\n    allocatable:\n      cpu: \"8\"\n      memory: 8076356Ki\n      pods: \"110\"\n    capacity:\n      cpu: \"8\"\n      memory: 8178756Ki\n      pods: \"110\"\n```\n. Thanks, this PR seem a good way to make Heapster work on IPv6, \nAny idea when a release will be available ? \nI will do a feedback. \ud83d\ude04 . ",
    "haf": "Makes sense. What confuses me is whether there's a GUI anywhere which I as a human can use to peruse (pun intended) the metrics? The grafana folder begs the question.\nAlso, that your quickstart causes conflicts on GCE might be an issue?\n. I'm sorry, but even googling for 15 minutes doesn't make me find where I access the grafana dashboard. And what about the quickstart?\nI did find https://github.com/kubernetes/kubernetes/issues/4721 but there's nothing at /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana.\nHowever, the quickstart in part creates a deployment/service with that name.\n. I'm using Google Container Engine and would like to use this with it... Is there a guide for that?\n. ",
    "notsureifkevin": "This is definitely an issue. I ended up having to roll my own image to get this working. I'll put in a PR with a fix that allows for specifying ipv4 || v6.\nIt turns out that passing configuration into kubelet is more trouble than it's worth (imo). The fix I've come up with simply checks for a properly formed ipv4 address and uses that.\nhttps://github.com/kubernetes/heapster/pull/1381\n. The PR is waiting on me to look at a test failure. I'll look at this today.. Fixed the test failure, should be good to go now. Thank you for the reminder @mccormd . Did not realize I had made a change to a vendor library to support this. I'll be revisiting this and finding a way around it.\n. I signed it!\n. test failure appears unrelated, not familiar with test commands for retest\n. retest this please\n. @piosz test issue fixed, added ipv6 address to test entity which should confirm patch.. ",
    "mccormd": "Any chance of this going in?. ",
    "germansosa": "I signed it!\n. ",
    "nhlfr": "Do we want to use glide-vc as well? I don't think we have any libraries whose license would prevent us from using it.\n. I can add it to gitignore - it would require ensuring that vendor dir exists during build, but of course that's doable.\n. Just to clarify - Glide works well with Godep-based dependencies - it even detects conflicts in versions between main project and a dependency (regardless of whether that dep uses Glide or Godep). So I think that there is no need to worry about sequence in which k8s project are going to adopt Glide.\n. That's right. I'm going to remove the vendor dir today.\n. Looks like glide is using raw hg to fetch a repo from bitbucket...\n. Probably go get and godep don't use hg as an executed command - that's my assumption, I cannot imagine any other scenario.\nHowever, I'll try to think about and propose a change to k8s infra.\n. ",
    "helinbo2015": "@googlebot I signed it!\n. @huangyuqi thanks for your check, i have added bug explanation.\n. @thelinuxfoundation i signed it!\n. ",
    "aegixx": "Same issue here.\nFor the record, these configuration files are fairly out-of-date - though I'm not sure if that's the issue.\nI'm hooking up Chronograf instead of Grafana, but I'll let you know if I have any success.\n. I do have an update for grafana + a working configuration for chronograf - I'll post a PR shortly\n. I've updated the configurations in #1326, but this isn't going to fix the Grafana display problem - however, Chronograf works.  Though I haven't reviewed the actual metrics being collected, so #1320 may still be relevant.\n. I'm confused about the scenario where GF_SERVER_ROOT_URL needs to be set to /api/v1/proxy/namespaces/kube-system/services/grafana/ - For NodePort and LoadBalancer, the Grafana port is available at the root address via the Service.  \nI may need one of you guys to give me more specific detail on what you want changed to finish the PR up.  Setting the GF_SERVER_ROOT_URL to / is what allows me to fix this issue.\n. This change is included in PR #1334 \n. CLA signed\n. I believe everything has been handled and this should be ready for merging.  Looks like the CI test is failing due to Your test run exceeded 120 minutes - which I don't think I can do much about :-)\n. @piosz Neither Grafana or Chronograf are part of the project officially - so I think these integrations are primarily introduction points depending on which stack is consuming your metrics.  I'm fine removing Chronograf in favor of Grafana (based on my own usage, Chronograf is simpler, but inferior to what Grafana provides)  We just need to ensure Grafana functions as intended based on this PR (which I'll validate prior to my rebase)\n. Removed Chronograf and confirmed Grafana functions as intended.  This PR is now focused on getting Grafana up and running only.\n. With the removal of Chronograf, I can vastly simplify this PR, going to close and reopen a new one with a cleaner history.\n. Should resolve #1326.  Simplification & redo of PR #1332\n. This is just me being a newbie to Kubernetes :-) - I didn't realize these had any relevance beyond the deployment -- I will add both of these labels back.\n. nodePort is not required, so wouldn't it be more confusing to explicitly specify one?\n. Thank you - fixed!\n. @LaurentDumont  Do you think it is confusing to assume everyone is deploying this as a cluster add-on?  If they don't take that additional step - their service will be shutdown.  On the other hand, if they should be deployed this way, should grafana, chronograf, and influxdb also be flagged with cluster-service as well?\n. Made all services cluster-service and added notes on running them independently.\n. ",
    "LaurentDumont": "I was able to use the grafana service with a NodePort endpoint instead. I'm not quite sure what is the status of the Heapster project as a part of the Kubernetes ensemble of products though. Might be worth looking at another graphing solution.\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    kubernetes.io/name: monitoring-grafana\n  name: monitoring-grafana\n  namespace: kube-system\nspec:\n  type: NodePort\n  ports:\n  - port: 3000\n    name: http\n    targetPort: 3000\n    nodePort: 30005\n  selector:\n    name: influxGrafana\n. Out of curiosity, is it something that could be pushed to the Google grc repository? Would it make much simpler to set up a test grafana deployment with all the latest bells and whistles!\n. @cmachler As far as I know, those images are hosted by Google on their own public repository. Someone from Google would have to update that image as they only have 2.6.0 for now. I wouldn't update \ngcr.io/google_containers/heapster_grafana:v2.6.0-2\nas that image is missing from the Google image repository.\n. We could add a nodePort: XXXXX as a comment just to make the entire process a bit clearer for the average user.\n. Why remove the cluster label from the Grafana service? I know that it might cause issue for a service that isn't part of a Kubelet manifest but it could be indicated to the user.\n. Yeah, that makes more sense. I was used to specifying the nodePort so I dont have to look up which dynamic port was assigned.\n. @kris-nova As far as I know, if you manually start a service as \"cluster-wide\" and start it AND it is missing from /etc/kubernetes/addons, the addons.sh service is going to kill the service. I was using kube-deploy to start the cluster which means that I was using a fixed list of cluster-wide services.\nAdding it to /etc/kubernetes/addons should create everything you need automagically.\n. @aegixx Well, I guess that the \"best-practice\" approach would be to deploy it as a cluster-service. It handles update/restart/shutdown as opposed to a manual deployment. As long as it's clear in the docs (we can work on that part!) I feel like a cluster service is a best setup.\nIn the old configs, influxdb and grafana are marked as cluster services. Since Chronograf is essentially a Grafana replacement, it should also be marked as one.\n. ",
    "agsola": "I was having two problems in here. I think that closely related.\n1. I couldn't access the grafana UI using NodePort. I've found a solution.\n   From within the pod I can see this css:\n   wget http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/public/css/grafana.dark.min.a95b3754.css --> OK\n   but from my computer accessing to the NodePort URL I get this:\n   wget http://ONENODEIP:31574/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/public/css/grafana.dark.min.a95b3754.css\n   HTTP request sent, awaiting response... 404 Not Found\n   The content: {\"message\":\"Not found\"}\n   It seems that the server doesn't listen except for the main URL in every IP.\n   I tried with the stock version just adding the \"NodePort\". No luck.\n   Then I changed all the envs values except for the first one as said in here:\n   influxdb-grafana-controller.yaml\n   No luck either.\n   I was using apply after changing the yaml file.\n   After deleting and re-creating I got the UI working.\n   Some tests showed me that the key to get it working is removing this env variable:\n   GF_SERVER_ROOT_URL\n2. I cannot get any metric within kubernetes. It keeps like this after hours:\n   ~# kubectl get hpa\n   NAME       REFERENCE             TARGET    CURRENT     MINPODS   MAXPODS   AGE\n   my-nginx   Deployment/my-nginx   50%          1         10        18h\n   I've tried recreating everything (the service, the deployment, the hpa...).\n   However I haven't been able to get this working yet...\nAny ideas why the hpa doesn't get the metrics while the Grafana UI shows them to me without problems?\n. I've been able to get it working.\nBasically I've found that I needed to specify CPU request in order to get the metrics working.\nIt would be (I think) to add that to the \"mini guide\" to get heapster up and running.\n. Thank you.\nIn here it's specified: http://kubernetes.io/docs/user-guide/horizontal-pod-autoscaling/\nHowever it isn't in here: https://github.com/kubernetes/heapster/blob/master/docs/influxdb.md\nWhich may be confusing (IMHO). I've added a PR.\n. I reply in here because my problem may be related to this one.\nI have installed Heapster using this guide:\nhttps://github.com/kubernetes/heapster/blob/master/docs/influxdb.md\nHowever, after installing it I CAN see the metrics from my pods without any problem in the UI.\nI'm getting this error:\n~# kubectl describe hpa/my-nginx \nName:               my-nginx\nNamespace:          default\nLabels:             \nAnnotations:            \nCreationTimestamp:      Wed, 12 Oct 2016 10:21:12 +0200\nReference:          Deployment/my-nginx\nTarget CPU utilization:     50%\nCurrent CPU utilization:    \nMin replicas:           1\nMax replicas:           10\nEvents:\n  FirstSeen LastSeen    Count   From                SubobjectPath   Type        Reason          Message\n\n12m       11m     4   {horizontal-pod-autoscaler }            Warning     FailedGetMetrics    failed to get CPU consumption and request: some pods do not have request for cpu\n  12m       11m     4   {horizontal-pod-autoscaler }            Warning     FailedComputeReplicas   failed to get CPU utilization: failed to get CPU consumption and request: some pods do not have request for cpu\n  9m        3m      13  {horizontal-pod-autoscaler }            Warning     FailedGetMetrics    failed to get CPU consumption and request: some pods do not have request for cpu\n  9m        3m      13  {horizontal-pod-autoscaler }            Warning     FailedComputeReplicas   failed to get CPU utilization: failed to get CPU consumption and request: some pods do not have request for cpu\n  2m        9s      5   {horizontal-pod-autoscaler }            Warning     FailedGetMetrics    failed to get CPU consumption and request: some pods do not have request for cpu\n  2m        9s      5   {horizontal-pod-autoscaler }            Warning     FailedComputeReplicas   failed to get CPU utilization: failed to get CPU consumption and request: some pods do not have request for cpu\nThere is no way I can get the stats from the hpa, \nWith get, the same:\n~# kubectl get hpa/my-nginx \nNAME       REFERENCE             TARGET    CURRENT     MINPODS   MAXPODS   AGE\nmy-nginx   Deployment/my-nginx   50%          1         10        13m\nNo matter how much time I wait...\nIt is always at \"waiting\", though I can get the stats from the UI.\n. I've been able to get it working, just setting the CPU request.\n. I signed it!\n. I suggested that file as it includes the common troubleshooting guide, and would be the most common place to look for.\nMaybe it can be added to debugging and link it from docs/influxdb.md\n. ",
    "tamalsaha": "@piosz, is the intention to have the new architecture in place for Kube 1.5? \n. Closing this as we are pursuing Prometheus.. ",
    "cmachler": "Hi @LaurentDumont, I do not have a GCP account (expired free trial), is there anything else I can do to help with the PR?\n. Closing since updated in #1342 . @Crassirostris updated deployment and release notes, but Travis build fails as all have for PR's since 9 days ago.. ",
    "ashiquzzaman33": "@piosz  signed, thanks.\n. ",
    "jackzampolin": "@Crassirostris @lattwood Currently heapster is writing to the default retention policy. This was renamed in the 0.13 release of InfluxDB to autogen. There is some code around this that needs to be changed. The events/sinks/influxdb and metrics/sinks/influxdb need to be changed to reflect this. There have also been some configuration changes between the two versions and the default configuration will need to be updated. \nI am happy to help move this PR along and help move past any issues the InfluxDB version bump might cause.\nI could also make the modifications and put up a PR, but am having trouble building heapster locally.  Please advise.. This addresses #1143 and incorporates https://github.com/kubernetes/heapster/pull/1328. @andyxning Squashed!. @luxas Done! . Yup. Its ready to go.. I've gone ahead and done the rebase.. It looks like Heapster is writing to the database with rp=default This might be the cause of the 500s.  Can you please check and see if you can connect to the database with the influx cli and post the response from the following two queries?\n```\n$ influx -host 192.168.56.206 -port 8086\n\nSHOW DATABASES\nSHOW RETENTION POLICIES\n``. This issue should be resolved by [#1415](https://github.com/kubernetes/heapster/pull/1415) I just pulled down the latestmasterand all theCREATE DATABASEcalls for Influx ([here](https://github.com/kubernetes/heapster/blob/master/events/sinks/influxdb/influxdb.go#L191) and [here](https://github.com/kubernetes/heapster/blob/master/metrics/sinks/influxdb/influxdb.go#L202)) explicitly create thedefault` retention policy when creating the database. . @TinySong You need to submit the query like so:\n\nCREATE RETENTION POLICY \"default\" ON k8s DURATION 0d REPLICATION 1 DEFAULT. ",
    "jbmcg": "Heapster 1.2.0 and Kubernetes 1.4.0\n. As of right now, there's no longer anything in the heapster logs about Object interfaces or anything anymore, its just a hard \"No metrics for pod namespace/pod-name\" over and over again for every pod that is in a HPA (total of 3 HPAs, 1 using CPU usage, 2 using a custom metric which is a prometheus gauge between 0 and 1). Also, if I put load on one of the pods, I can usually get a \"No metrics for container container-name in pod namespace/pod-name\" message vs. the usual one.\nI'm able to see all metrics properly in cAdvisor (including the custom ones) and I can see graphs in Grafana for the pods that heapster is saying that it can't get metrics for, so it would appear the metrics are getting stored properly (although the graphs do look a little unusually flat)\nI switched the sink to \"log\" and can see that its definitely getting metrics for a number of things consistently, just not metrics for pods under HPAs it would seem.\n. The HPA that uses CPU usage is actually working properly now, its just the custom metrics (or the HPAs that use custom metrics) that are failing.\n3h      1s      13844   {horizontal-pod-autoscaler }            Warning     FailedGetCustomMetrics  metrics obtained for 1/2 of pods (sample missing pod: staging/imposium-sqs-worker-684296566-5l3a4)\n3h      1s      13844   {horizontal-pod-autoscaler }            Warning     FailedComputeCMReplicas failed to get custom metric value: metrics obtained for 1/2 of pods (sample missing pod: staging/imposium-sqs-worker-684296566-5l3a4)\nSwitched the source to kubernetes.summary-api as well, since we can see the custom metric updating in the /summary/stats API call of each node, but heapster still isn't picking it up.\n. FWIW I believe we actually have it working now. I think the main source of our woes was actually due to security group rules - we had to open up the ports we were using for InfluxDB and metrics collection on the Kubernetes worker nodes for when a pod is scheduled on a different instance than heapster is scheduled on.\n. ",
    "patagona-afriemann": "We are pretty much having the same Issue. We had deployments configured with resource limits+requests and did not get any metrics with the heapster logs in the OP.\nRemoving the limits actually got us metrics, though heapster is still logging No metrics for pod every few minutes. The HPA is working now, however removing the limits seems wrong to me.\nEDIT: in fact, no metrics are available in the dashboard at all for deployments with resource limits specified.. ",
    "jbdalido": "Sorry removed a not so polite debug line, ideally the produce would not happen on only one partition as pointed by @zllak, and it should not be sync. This version will work as expected, the same as metrics, but i can work on a better implementation for the two sinks asap. \n. ",
    "mcorbin": "Hi,\nI am currently working on the Riemann metric sink (to use https://github.com/riemann/riemann-go-client instead of Goryman). I will probably do the PR today. I will then work on the event sink.\nI don't think Riemann is fading away, and the next Riemann release will bring a lot of improvements. Ofc, it will never be as popular as elastic/beat ;). @shmish111 i changed several things on my PR on the metric sink (#1562), but I will take inspiration from your code ;)\nRegarding Riemann, the last release was the 3 month ago (changelog here). Since was added things like microseconds time resolution in the protocol (limited to the second currently), complete refactoring of the influxdb output (should support all influxdb functionalities), new outputs (Kafka, netuitive), various bugfixes... And more to come ;) If you have questions about Riemann (i can give you some use cases when i use Riemann in my company for example), you can also contact me directly ;). I can take care of the Riemann sink too (i just need some time to dig into the code).. @jamtur01 @piosz I did a PR.. I signed the 2 CLA.. @jamtur01 Yes i can migrate goryman to github.com/riemann. I will also try to look at the code. cc @rikatz . \nReview status: 12 of 17 files reviewed at latest revision, 4 unresolved discussions.\n\ncommon/riemann/riemann.go, line 17 at r1 (raw file):\nPreviously, jsoriano (Jaime Soriano Pastor) wrote\u2026\nPlease, organize imports in groups according to goimports (https://github.com/golang/go/wiki/CodeReviewComments#imports), and do the same in all edited files.\n\nHi, i used goimports on all files. Thanks  ;)\n\nevents/sinks/riemann/driver_test.go, line 134 at r1 (raw file):\nPreviously, jsoriano (Jaime Soriano Pastor) wrote\u2026\nSet these events here inline directly?\n\nDone, thanks\n\nmetrics/sinks/riemann/driver.go, line 27 at r1 (raw file):\nPreviously, jsoriano (Jaime Soriano Pastor) wrote\u2026\nCan we reuse the definition in common?\n\nI tried and failed. All sinks (influx, elasticsearch...) duplicates the sink struct in both events and metrics.\n\nvendor/github.com/riemann/riemann-go-client/.gitignore, line 3 at r1 (raw file):\nPreviously, jsoriano (Jaime Soriano Pastor) wrote\u2026\nI don't think we want to ignore vendor directory.\n\nI'm pretty new to Go, some projects consider pushing the vendor directory a bad practice, some projects dont. I don't really know what to do with the Riemann Go client.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: all files reviewed at latest revision, 1 unresolved discussion.\n\nvendor/github.com/riemann/riemann-go-client/.gitignore, line 3 at r1 (raw file):\nPreviously, jsoriano (Jaime Soriano Pastor) wrote\u2026\nIn heapster this directory is commited into the repository.\n\nYes, but here it's riemann-go-client vendor directory, so it should not impact Heapster (i can not find a Heapster dependency in vendor with it's own vendor directory). Furthermore, riemann-go-client depends only on protobuf.\n\nComments from Reviewable\n Sent from Reviewable.io \n. Fixed, thanks. Fixed, thanks. Fixed, thanks. Fixed, thanks. I have added a description, thanks. Fixed, thanks. Fixed, thanks. Fixed now ;) . ",
    "ghost": "@jsoriano \nWorks well for TCP, but for UDP I've got (after adding some debug):\n./heapster -v=5 --source=\"kubernetes:${ip}\" --sink=\"graphite:udp://127.0.0.1:2003\"\nThere were errors sending events to Graphite, reconecting:\nwrite udp 127.0.0.1:49188->127.0.0.1:2003: write: message too long\n. @jsoriano seems like problem here:\nhttps://github.com/marpaia/graphite-golang/pull/18\n. @jsoriano another little fix: https://github.com/marpaia/graphite-golang/pull/21. > We need someone familiarized with Graphite to review this. Do you know anyone?\n@piosz @jsoriano \nI test this patchset in my test environment, with Graphite (go-carbon).\nAnd I and my \u0441olleagues may review this.\nBTW:\nAt this moment there are no unexpected or buggy behavior, it works stable for me.\nIf my tests goes well - I planned use it in production.\nIf you have any questions etc. - you may asked me.. @piosz @jsoriano \nI review code and finish tests: everything works as expected.\nIf there are no any stop-factors, I think, it is ready to merge.\nI've run several tests, everything works as expected:\n- send to graphite via TCP/UDP;\n- set prefix;\n- change metric resolution interval;\nAlso, I ran some tests for evaluation of overhead, which can create heapster,\ndoing queries to kubelet; overhead, which create heapster vanishingly small: one percent or so\nTest stand:\n2 nodes:\nmaster node:\nCPU: 1 x i7-4770HQ CPU @ 2.20GHz;\nRAM: 4Gb\nNode running heapster with parameters:\n-v=10\n--metric-resolution=10s\n--source=kubernetes:$IP\n--sink=graphite:udp://$IP:2003?prefix=testcluster-02\nTest deployment includes one pod with one container.\nContainer consists of only one static build C program: while true sleep (1)\n(I think, it most lightweight process for running)\ndata node:\nCPU: 1 x i7-4770HQ CPU @ 2.20GHz;\nRAM: 8Gb\nTest results (sorry for PNG, I'm not too familiar with tables in Markdown):\n\nIf You have any questions - you may ask me!. @DirectXMan12 I can take care of Graphite Sink, jointly with @jsoriano of course)\nAlso, I can do some tests and reviews for others sinks, if needed.. @piosz @jsoriano \nLooks fine and works as expected, I think its ready to merge!. @jbehrends Hi, I'll see it!. @jbehrends @jsoriano \nYes, looks like that is not any sink (graphite, riemann etc.) bug,\nsink does not do any opertion with data, its only 'proxy' them...\nYou may create issue on heapster more generally, not 'sink-related'.\n(Anyway, for now I'll investigating this behavior). @jbehrends @jsoriano \nSeems like problem in cluster aggregator, watch PR: https://github.com/kubernetes/heapster/pull/1529\nI think, this issue may be closed. @jbehrends hmm, I'll investigate it!. This is very necessary. I need a retention policy set on deployment. . I did try Kubernetes 1.4 with Heapster 1.0, it did work but I was not able to get the /stats API. Has it been discontinued ?. and is there any place I can find more info about aggregation in sinks ?. Thanks, also I had one more question, I dont see the file system metrics (filesystem/usage,filesystem/limit,\nfilesystem/available)  in Heapster 1.3 data that I receive from the REST API call, all I see  for my kubernetes dashboard pod is \n0   \"memory/major_page_faults_rate\"\n1   \"network/tx_errors_rate\"\n2   \"network/rx_errors\"\n3   \"network/tx_errors\"\n4   \"memory/usage\"\n5   \"memory/working_set\"\n6   \"network/rx_errors_rate\"\n7   \"cpu/request\"\n8   \"uptime\"\n9   \"cpu/limit\"\n10  \"network/tx_rate\"\n11  \"network/tx\"\n12  \"cpu/usage_rate\"\n13  \"network/rx\"\n14  \"network/rx_rate\"\n15  \"memory/page_faults_rate\"\n16  \"memory/limit\"\n17  \"memory/request\"\nWas this removed or is there some other way to retrieve them ?\n. Which version of Heapster would you suggest running against Kubernetes 1.4 ?. I signed it!. Thanks @piosz :) . I have same problem.\nENV:\n  heapster-grafana version: 4.4.1\n  kubeadm: 1.7.0\n  OS: CentOS 7.2.1511\n. +1 InfluxDB and Grafana Images not found. \nError : \n```\ndocker pull gcr.io/google_containers/heapster-grafana-amd64:v4.4.3\nError response from daemon: manifest for gcr.io/google_containers/heapster-grafana-amd64:v4.4.3 not found\n```. @luxas Both images are not in gcr: InfluxDB and Grafana. Although the issue is created for InfluxDB. Please check Grafana too. . Thanks, @luxas . ",
    "noqcks": "I am waiting on this PR. Anything else need to be done before merge? \nThank you @jsoriano . I have tested this on a fork and I'm using it in a production perfectly fine. \nany reason not to merge? Looks like it received LGTM from @theairkit \n@piosz ?. thanks  \ud83d\udc4d . ",
    "adelbot": "Any news ?\nBR\n. Good news !\nHow do I check?. Good Job !\nThe Kafka is balanced over the nodes.\nBut i'm not received events with this error :\n0112 10:36:06.316789   11350 streamwatcher.go:109] Unable to decode an event from the watch stream: object to decode was longer than maximum allowed size\nLaunched commande :\n./heapster -v 3 --source='kubernetes:https://kubernetes.default.svc.cluster.local:443?useServiceAccount=true&kubeletHttps=true&kubeletPort=10250&inClusterConfig=true&insecure=true' --tls_cert=/secrets/heapster.cert --tls_key=/secrets/heapster.key --tls_client_ca=/secrets/heapster.client-ca --allowed_users=system:master-proxy --metric_resolution=60s --sink='kafka:?brokers=10.47.0.87:9092&brokers=10.47.0.87:9092&brokers=10.47.0.88:9092&timeseriestopic=sink-metrics&eventstopic=sink-events'\n. ",
    "madhurranjan": "This is resolved. It looks like in the new version , if you remove the parent node with \"dashboard\" and start directly with \"id\" everything works like a charm.\n. ",
    "farcaller": "Following the cla/linuxfoundation Details link says:\n\nContribute to CNCF as an employee\nYou are already authorized to contribute code to this project through your membership with Google.\n\nDo I need to do anything else?\n. ",
    "derekhound": "I met the same problem, but the above comment really resolve my problem. It works.\n. ",
    "TerraTech": "Sure, if it is confirmed to be the valid fix and I'm not overlooking something.\n. I signed it!\n. ",
    "hitman99": "It's in the kubrnetes dashboard, but the data comes from heapster as charts are not available until I deploy heapster add-on. \n. ",
    "eschwartz": "Any update on this?\nI'd really like to be able to collect k8s cache metrics via heapster,\nbut as it stands, the existing metrics are not useful.\nHas anyone else come up with a stop-gap alternative? I'm trying to post metrics to elasticsearch. ",
    "KristianLyng": "\"kubectl top node\" is also inconsistent with memory/node_utilization. \"kubectl top node\" seems to use working set to calculate its own utilization grade. Similarly, \"kubectl top node\" differ from typical memory usage in the dashboard (and anything else using \"memory/usage\") since it reports working set.\nThe result for me:\n\"kubectl top\":\nNAME                          CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%   \nmalxkube01                    277m         2%        4405Mi          6%        \nmalxkube02                    383m         3%        8746Mi          13%       \nmalxkube03                    238m         2%        7036Mi          10%       \nmalxkube04                    273m         2%        6810Mi          10%    \nmalxkube05                    255m         2%        11680Mi         18%       \ndrlxkube01                    340m         3%        3055Mi          4%        \ndrlxkube02                    626m         6%        8095Mi          12%       \ndrlxkube03                    402m         4%        8705Mi          13%       \ndrlxkube04                    372m         3%        7773Mi          12%       \ndrlxkube05                    2139m        21%       8848Mi          13%\nGrafana/influx: (memory/node_utilization)\nmin     max     avg     current\ndrlxkube01      12.69%  12.71%  12.70%  12.71%\ndrlxkube02      32.71%  32.78%  32.75%  32.76%\ndrlxkube03      35.65%  35.67%  35.66%  35.67%\ndrlxkube04      28.31%  28.63%  28.44%  28.46%\ndrlxkube05      26.92%  28.55%  27.63%  28.50%\nmalxkube01      18.28%  18.35%  18.33%  18.35%\nmalxkube02      21.50%  21.70%  21.60%  21.68%\nmalxkube03      16.31%  16.70%  16.46%  16.37%\nmalxkube04      20.26%  20.42%  20.31%  20.34%\nmalxkube05      37.67%  37.87%  37.81%  37.67%\nThese should probably be consistent... Not entirely sure if this should be a separate issue or not, as it seems to be more or less the same core problem.. ",
    "toddams": "Any update on this?. Same question). ",
    "gacopl": "really everyone is ok with this ?. ",
    "Ryan-Dmello": "Currently we faced some issue while building heapster for s390x. So after some investigation came across this bug which says -race detector flag is only supported for amd64\nhere is the link related to that bug https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=831724\n. Hi,\nwanted to know whether there is a concept of corporate CLA for cla/linuxfoundation. Any pointers to signing this cla will be very helpful.\nThanks,\nRyan D'Mello\n. ",
    "medhedifour": "When I access to the influxdb ui I didn't get any data.\nthe url : https://@IP/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb:8083/\n\nGrafana service is functionnal with influxdb. But I get only nodes and pods dashboards, I don't have dashboard for container. Can someone telle me why ?\n\n. ",
    "mrcrgl": "Hi @ezeev,\nSorry for my late response. Please let me know how can I help you. \n\nMarc . @piosz thank you for mention me. I could have a look at it. . @ezeev please, add some tests for your Sink, too.. I've finished my review. \n\n@piosz thank you for asking me being the sink owner. That sounds interesting, lets see how it goes with this PR.. Hi @ezeev \nThank you for applying my notes. It's fine for me if you would be the sink owner, or we both together. \nWe're actually using your container image of Heapster in our test-environment and looking forward to use it in production as well.\nBest,\nMarc. Hi @ezeev,\nIt'd be great to have some basic tests, what do you think?\nBest,\nMarc. Hi @ezeev, the PR looks fine to me but I'd like to have some unit tests added. \nIn my opinion it is easier and safer to maintain the code afterwards. \n/cc @DirectXMan12. @ezeev LGTM! Thank you for writing the unit tests. \nI guess this one could be merged, what do you think @piosz?\n/cc @DirectXMan12. It may be useful append information to the error like Flag name.. Like above. Struct wavefrontSink does not need to be exported.. The code of ms.LabeledMetrics and ms.MetricValues are looking similar. Maybe it can be moved to a function. Function body of sortedMetricSetKeys, sortedLabelKeys and sortedMetricValueKeys look similar. Maybe it can be moved to a function, too.. Function Connect() does not need to be exported.. Function Send() does not need to be exported.. Does not need to be exported.. Does not need to be exported. I see your point. ",
    "euank": "The problem in 30939 (linked above) was systemd >=226, docker using the systemd cgroup driver, and an older version of docker.\nUsing systemctl cat docker.service you can verify whether docker is configured to use the --exec-opt native.cgroupdriver=systemd option.\nNewer systemd versions don't fix this, but new enough docker versions do.\nOn CoreOS, we fixed this by moving to cgroupdriver=cgroupfs. I don't know what docker versions, if any, include the needed fix.\nIf this is that same issue, then it's not really a heapster bug but a docker bug.\n. 1137+ I believe. So long as DOCKER_CGROUPS isn't set in your docker.service file, it's defaulting to cgroupfs, so the issue in 30939 wouldn't be it.\nIf you're still seeing a similar problem, then it's a different bug to investigate.\nI suspect this issue could be a different bug than the one in that issue, but since it was referenced as a possible duplicate I wanted to give more information so we could be sure.\n. Thanks for the heads up, I removed the line about the influxdb UI as well as the commented out service port for it.. ",
    "miaoyq": "@piosz  I have opened a PR for this issue, please help to review, thanks!\n. @piosz  PTAL  Thanks so much!\n. @huangyuqi   Thanks for review, If this PR has any problems, please let me know.\n. @piosz  @huangyuqi  Could this PR be merged?  If the PR has any problems, please let me know. Thanks so much!. Thanks @huangyuqi  for review, I will fix the issue, and refactor the code based on your instructions.. I have refactored kafka sink and fixed #1344,  PTAL @huangyuqi . /cc  @huangyuqi @piosz . @piosz  Could this PR be merged?  PTAL, Thanks a lot!.  @piosz  The PR has been open for a long time, but is still not merged. If there is any problem still, I will be happy to modify.   PTAL, Thanks a lot!. @adelbot  Please refer to the contents kafka. @adelbot    Apologies for my mistaken, now heapster have been divided into two parts, namely heapster(for metrics) and eventer(for events). see here\nwe should use eventer to received events, look like:\n./eventer --sink=\"kafka:?brokers=localhost:9092&brokers=localhost:9093&eventstopic=testtopic\"  ... ...\nI wil update the docs.. @huangyuqi Could this PR be merged?   PTAL, Thanks!. ping @huangyuqi  @piosz  Could this PR be merged? . I'm sorry for delay,  have updated sink-owners.md, PTAL. @DirectXMan12 @andyxning . @juanluisvaladas  LGTM after address comments, but I'm not member.\nping @huangyuqi  @piosz. @DirectXMan12  I would like to be the maintainer of Kafka sink. :). @Kokan  Thanks :). @ryarnyah Thanks, I will review this PR.. /ok-to-test. /lgtm. Extra empty line. ditto.. Compression to compression, '%s' to %q.. compression to `compression`,  gzip and  none also.. I think we should use kafka.CompressionNone instead of -1 here.. Could you add a period after each line? :-). Make sense,  but we should avoid using a number directly.  :-). ",
    "DockerAsura": "Hi.,\ncomplied with golang 1.7.3. Working fine \nthanks Kumar\n. ",
    "aaron-prindle": "Got it working, closing\n. The problem was that I was using type: NodePort for the influxdb and heapster services (as those can be exposed via Minikube) which did not seem to work when attempting to use the services with grafana.   Changing those back to ClusterIP and exposing only grafana via NodePort did the trick.  The link in my original comment shows the logs/configuration I was using.  . ",
    "jakon89": "It was implemented in: https://github.com/kubernetes/heapster/pull/1462\nLooks like issue can be closed.. Good point, I am on it. I will send PR today.. As per https://github.com/OpenTSDB/opentsdb/issues/412 and current OpenTSDB logic it looks that empty tags are not allowed. I would mark tag where value is empty with defaultTagValue or delete this tag at all. What do you think? I will be glad to send quick PR if needed.. Hi @piosz!\nWell, test build failed but not sure if it's my fault.\nCould you please verify test results?\nThanks. @DirectXMan12 \nnot sure what you meant but PTAL. Thanks. Updated. Thanks!. Hi kelify!\nDo you use any http proxy in front of OpenTSDB? \nI did take a look at the OpenTSDB client source code and it doesn't check response code at all. Maybe it should check http status codes? As per docs - OpenTSDB return 204 HTTP in case of successful request. @bluebreezecf . ",
    "jeremyd": "+1\n. ",
    "keithballdotnet": "Where can I find the current schema?  I would be happy to do some monkey work and update.\n. I found an updated container.\ngcr.io/google_containers/heapster_grafana:v2.6.0-2\nUpdate the grafana to that.  All good.\n. @k8s-bot ok to test\n. I signed it!. ",
    "KarolKraskiewicz": "Hi @jonaz \nIt seems it is an old version of heapster, which is not compatible with you influxdb version.\nFor sure the solution would be to upgrade both influxdb to v1.1.1 and heapster to v1.3.0-beta.1\nAlternatively here it seems that verions of influxdb compatible with heapster 1.2 is influxdb v0.5. Have you tried that?\n. @jonaz I reviewed previous versions of heapser, and also double checked it with @piosz. It seems that for example cpu/request have always been of int64 type:\nv1.1.0: cpu/request definition, influxdb data sink\nv1.2.0: cpu/request definition, influxdb data sink\nv1.3.0: cpu/request definition, influxdb data sink\nSo it seems strange you ever got float type for this. Any idea where can it be comming from?\n. It could be done by post-processing text in pods.json during grafana startup. For example, it could be done, using sed from 'run.sh' which is starting up grafana in a container. \nSuch processing would add some extra complexity in the system, and may break pods.json if sed is not triggered or used in a wrong way. I was wondering if we would like to go ahead with such approach? Or maybe there are different solutions possible?\n@piosz any thoughts?\n. @piosz advised dashboard configuration could be injected into container using ConfigMap. It would externalize dashboard configuration and make it possible to configure \"refresh on load\" and any other variables from kubernetes level.. Configuration files from:\nhttps://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb\nhave been updated and I verified they are working now.\nNot sure how the hpa issue is related to heapster.  If hpa is still causing problems, it could be a better idea to open a separate ticket for that.\n@karunakar1122v - is there anything else missing here or the ticket can be closed?\n. Hi Everyone,\nIt seems the bug described here has been fixed and changes have been released within the new container: gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1\nI verified, that last version of heapster/influxdb/grafana configuration:\nhttps://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb\nwhich is using the new container, is working properly.\n@jmn - is there anything else missing here from your perspective?\n@lilnate22 - are you still experiencing issues described above? If yes, which version of heapster / influxdb / grafana images are you using?\n. @judexzhu It is an interesting problem, but I was not able to reproduce this problem in my environment. \nI can see you are using Heapster version v1.3.0-beta.1. Which versions of influxdb and Grafana are you using? Are they custom builds, or they are pre-builded containers like those below?\n grafana\ninfluxdb\nheapster\nHow many nodes do you have in your cluster?\n. It seems the error occurs when trying to export a graph in graphana as an image. I also agree it appears that missing fontconfig is the root cause.\nI tried to add fontconfig to busybox, but since busybox does not have any package manager, and fontconfig have some dependencies, it is not very practical.\nAn alternative would be to change the container form busybox to something with a package manager, for example apline, but it would be a bigger change, and I'm wondering if it is worth it just to provide the \"snapshot to image\" capability in grafana.\n@piosz any thoughts?\n. Hi @jlalouette \nI understand you tried those images:\ngcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\ngcr.io/google_containers/heapster-grafana-amd64:v4.0.2\nWhat is the compatibility issue you encountered? \nI tried the same images together with:\ngcr.io/google_containers/heapster-amd64:v1.3.0-beta.1\nAnd monitoring was working as expected.\n. I signed it! (The cla). I decided to push 1 updated to Recommender, consolidating info about all containers, instead of sending single request per container. It should save some time due to network latency and simplify processing on Recommender side. \nIt means however, we will be able to send information about only up to 20 000 containers:\n- info about single container = 512B\n- 512B x 20 000 = 10MB which is a limit of body size when handling http post request in go.\nThe 20k containers limit will be raised when sink/recommender communication protocol is changed from JSON to Protocol Buffer. It should get us to up to 30-40k and from here we can go further by switching to multipart post. . ",
    "fenggolang": "when I use es-2.3.5 and heapsterv1.2.0,it's ok\n. ",
    "matthughes": "Both I would think.  It would be a configuration flag of the ES sink.. ",
    "perfeyhe": "I found the reason, There is someting wrong with my cAdvisor,I can't see the subcontainers from cAdvisor panel, so I think this is the reason heapster didn't get pod info and sent it to influxdb. I'm still working on it to find what's wrong whith my cAdvisor.\n. I tried heapster:canary, but pod metric still can't be shown. Then I tried to downgrade docker to lxc-docker 1.9.1, the pod metric came out. It seems cAdvisor can not work well  with kubernetes 1.2.0 and docker-engine 1.2.1, since kube-push.sh is broken, I didn't try to upgrade kubernetes to 1.4.6 to see if there are same problems in this version.. ",
    "xwisen": "kubernetes version:\n\n[root@master1 ~]# kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"4\", GitVersion:\"v1.4.0\", GitCommit:\"a16c0a7f71a6f93c7e0f222d961f4675cd97a46b\", GitTreeState:\"clean\", BuildDate:\"2016-09-26T18:16:57Z\", GoVersion:\"go1.6.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"4\", GitVersion:\"v1.4.0\", GitCommit:\"a16c0a7f71a6f93c7e0f222d961f4675cd97a46b\", GitTreeState:\"clean\", BuildDate:\"2016-09-26T18:10:32Z\", GoVersion:\"go1.6.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n. Maybe a container named reg.5000 caused this problem.The cAdvisor disdinguish containers by ... \n",
    "GreatSUN": "Hi all,\nlooks like with Kubernetes 1.4.6 (tested version) the request to the kubelet is broken (at least with TLS/https). Looks like heapster is requesting /stats which is redirected via 503 to /stats/ from kubelet (works verified with curl -L) but heapster does not follow the redirect and fails.\nI would need this fixed as we have a dependency to kubernetes 1.4.6 here :-(\nThanks!. Just to add it, the parameters we are using:\n        - /heapster\n        - --source=kubernetes.summary_api:''?kubeletPort=10250&kubeletHttps=true&auth=true\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --tls_cert=/opt/kube/certs/localworker.pem\n        - --tls_key=/opt/kube/certs/localworker-key.pem\n        - --tls_client_ca=/opt/kube/certs/ca.pem\n\n. Hi all, as far as I am able to see, it looks working with heapster 1.1.0, but not with 1.2.0.\nAny idea?. ",
    "jmastr": "Checking.... ",
    "mashayev": "Hi, \nI tried to upgrade the Heapster to v1.2, but I get this error\nE1127 16:41:40.041374       1 reflector.go:203] k8s.io/heapster/metrics/sources/kubelet/kubelet.go:339: Failed to list *api.Node: Get https://kubernetes.default:/api/v1/nodes?resourceVersion=0: dial tcp 172.14.0.1:0: i/o timeout\nE1127 16:41:40.047957       1 reflector.go:203] k8s.io/heapster/metrics/heapster.go:313: Failed to list *api.Pod: Get https://kubernetes.default:/api/v1/pods?resourceVersion=0: dial tcp 172.14.0.1:0: i/o timeout\nE1127 16:41:40.047961       1 reflector.go:203] k8s.io/heapster/metrics/processors/namespace_based_enricher.go:84: Failed to list *api.Namespace: Get https://kubernetes.default:/api/v1/namespaces?resourceVersion=0: dial tcp 172.14.0.1:0: i/o timeout\nE1127 16:41:40.048130       1 reflector.go:203] k8s.io/heapster/metrics/heapster.go:321: Failed to list *api.Node: Get https://kubernetes.default:/api/v1/nodes?resourceVersion=0: dial tcp 172.14.0.1:0: i/o timeout\nE1127 16:41:40.048183       1 reflector.go:203] k8s.io/heapster/metrics/processors/node_autoscaling_enricher.go:100: Failed to list *api.Node: Get https://kubernetes.default:/api/v1/nodes?resourceVersion=0: dial tcp 172.14.0.1:0: i/o timeout\nE1127 16:42:05.000907       1 kubelet.go:270] No nodes received from APIserver.\nI used the latest yaml files from master branch\nMy K8S version is 1.4.1\n\u279c  $ kubectl get pods,svc,rc --namespace kube-system  \nNAME                                      READY     STATUS    RESTARTS   AGE\npo/heapster-959896820-1sgen               1/1       Running   0          8m\npo/kube-dns-v11-7t8n5                     4/4       Running   0          1h\npo/kube-dns-v11-vk4km                     4/4       Running   0          1h\npo/kubernetes-dashboard-413762064-rz9bl   1/1       Running   0          1h\npo/monitoring-grafana-2970430589-rhlgq    1/1       Running   0          8m\npo/monitoring-influxdb-3276295126-4k9qy   1/1       Running   0          8m\nNAME                       CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE\nsvc/heapster               172.14.26.16    <none>        80/TCP          8m\nsvc/kube-dns               172.14.0.2      <none>        53/UDP,53/TCP   1h\nsvc/kubernetes-dashboard   172.14.159.42   <nodes>       80/TCP          1h\nsvc/monitoring-grafana     172.14.52.166   <nodes>       80/TCP          8m\nsvc/monitoring-influxdb    172.14.208.78   <none>        8086/TCP        8m\nNAME              DESIRED   CURRENT   READY     AGE\nrc/kube-dns-v11   2         2         2         1h\nMy only change was in heapster-deployment.yaml:\ncommand:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default?inClusterConfig=false\n        - --sink=influxdb:http://monitoring-influxdb:8086\nI also tried to give direct link and not the kubernetes.default\nPlease assist.\n. Historical reason(this is how it worked for us in Heapster 1.1), thought when i'm checking the files in heapster-deployment.yaml, it configured without the summary api.\nWhen changing the source to summary api as you suggested:\ncommand:\n        - /heapster\n        - --source=kubernetes.summary_api:''\n        - --sink=influxdb:http://monitoring-influxdb:8086\nI'm getting this error:\n\u279c  influxdb git:(master) \u2717 kubectl logs heapster-3923365681-1xz46 --namespace kube-system \nI1128 05:56:50.992222       1 heapster.go:65] /heapster --source=kubernetes.summary_api:'' --sink=influxdb:http://monitoring-influxdb:8086\nI1128 05:56:50.992700       1 heapster.go:66] Heapster version 1.2.0\nF1128 05:56:50.993716       1 heapster.go:168] Failed to create source provide: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory. Hi,\nI've updated my K8S to 1.4.6 but it didn't fixed my problem, still seeing these errors:\nE1208 18:44:05.000248       1 kubelet.go:270] No nodes received from APIserver.\nE1208 18:44:16.025400       1 reflector.go:203] k8s.io/heapster/metrics/processors/node_autoscaling_enricher.go:100: Failed to list *api.Node: Get http://kubernetes.default/api/v1/nodes?resourceVersion=0: dial tcp 172.14.0.1:80: i/o timeout\nE1208 18:44:16.025400       1 reflector.go:203] k8s.io/heapster/metrics/processors/namespace_based_enricher.go:84: Failed to list *api.Namespace: Get http://kubernetes.default/api/v1/namespaces?resourceVersion=0: dial tcp 172.14.0.1:80: i/o timeout\nE1208 18:44:16.025434       1 reflector.go:203] k8s.io/heapster/metrics/sources/kubelet/kubelet.go:339: Failed to list *api.Node: Get http://kubernetes.default/api/v1/nodes?resourceVersion=0: dial tcp 172.14.0.1:80: i/o timeout\nE1208 18:44:16.025412       1 reflector.go:203] k8s.io/heapster/metrics/heapster.go:327: Failed to list *api.Node: Get http://kubernetes.default/api/v1/nodes?resourceVersion=0: dial tcp 172.14.0.1:80: i/o timeout\nE1208 18:44:16.025483       1 reflector.go:203] k8s.io/heapster/metrics/heapster.go:319: Failed to list *api.Pod: Get http://kubernetes.default/api/v1/pods?resourceVersion=0: dial tcp 172.14.0.1:80: i/o timeout\nPlease assist.. Hi,\nI don't use service proxy in my ENV.\nJust to be clear the heapster is from release-1.2\nMy other pods are working great, dashboard, kube-dns and etc... . Issue was fixed by changing the kubernetes.default to the full dns name we use in our ENV.\nThanks for the help.. @newcrane What do you mean?. I want to be able to change this parameter(Red circle). Not in the UI but when creating the POD.\n\nIn the code I can see: https://github.com/kubernetes/heapster/blob/96d9dbf9ee20c200bb7703a8dff0b5ea7c1d1ba1/grafana/dashboards/pods.json#L977\nFor example for my case, I need this var to be always true.. @DirectXMan12  Can you share the pull request?. Hii @johanneswuerbach @dieterdemeyer I got the same issue with Failed to create sink: Sink not recognized: librato\nI removed the quotes from sink and it's the same issue. Maybe heapster-amd64:v1.3.0-beta.1 image need to be updated?\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      containers:\n      - name: heapster\n        image: gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=librato:?username=xyz&token=secret&prefix=k8s&tags=cluster&tag_cluster=staging\n        volumeMounts: \n          - name: ssl-certs\n            mountPath: \"/etc/ssl/certs/ca-certificates.crt\"\n            readOnly: true\n      volumes: \n        - name: ssl-certs\n          hostPath: \n            path: \"/etc/pki/tls/certs/ca-bundle.crt\". @johanneswuerbach Thanks for the help, it works :). @johanneswuerbach One more question about \"The librato sink currently only works with accounts, which support tagged metrics.\"\nIf we are using source and not tags(We signed up on or before January 24th, 2017 12:00 PST), it means the sink wont work at all?\nAny plan to support both??. @johanneswuerbach Librato team have enabled the Tags for my account, the Sink is working now for me.\nI have another question: is there any option to change the metrics name in the chart?\nfor example I get this name in the chart:\ni-1q2w3edfrtt.ip-1-1-1-1.ec2.internal.node.ip-1-1-1-1.beta.kubernetes.io_arch:amd64_beta.kubernetes.io_instance-type:m3.large_beta.kubernetes.io_os:linux_failure-domain.beta.kubernetes.io_region:us-east-1_failure-domain.beta.kubernetes.io_zone:us-east-1c_kubernetes.io_hostname:ip-1-1-1-1\nI prefer to get the name of one of the tags or combination of them for example. something like:\n<CLUSTER_REGION>.<ENVIRONMENT>.<POD_NAME>.<POD_NAMESPACE>\nBelow is an example of how it works when I send the metrics with Collectd(I can control the way the name will be displayed), is it possible with Librato Sink?\n\nThanks for the help.. Hi @johanneswuerbach,\nI wonder if it is possible to filter which metrics will be sent to Librato and which not. From what I see there is no such an option. Can you confirm?. ",
    "newcrane": "@mashayev Can tell me what changed into?. @mashayev What did you change to for kubernetes.default in your system environment? Thank you very much.. I have same issue \n```\nE1211 09:36:32.417400       1 reflector.go:190] k8s.io/heapster/metrics/util/util.go:51: Failed to list v1.Node: Get https://kubernetes.default/api/v1/nodes?resourceVersion=0: dial tcp 10.96.0.1:443: i/o timeout\nE1211 09:36:32.418520       1 reflector.go:190] k8s.io/heapster/metrics/heapster.go:322: Failed to list v1.Pod: Get https://kubernetes.default/api/v1/pods?resourceVersion=0: dial tcp 10.96.0.1:443: i/o timeout\nE1211 09:36:32.421626       1 reflector.go:190] k8s.io/heapster/metrics/processors/namespace_based_enricher.go:84: Failed to list *v1.Namespace: Get https://kubernetes.default/api/v1/namespaces?resourceVersion=0: dial tcp 10.96.0.1:443: i/o timeout\n```. @zhangqx2010 Can you tell me how to solve?. ",
    "mingfang": "Yes! Please.. Any chance this can be done for the new release, v1.3.0?. This is not strictly necessary anymore since now I can do this in a Dockerfile\nCOPY --from=gcr.io/google_containers/heapster:v1.4.1 heapster /heapster/heapster\nFeel free to close this.. ",
    "kadamvandita": "How to copy obtain the binary other then then in dockerfile?\n. yes.. I tried it.But when I write a spec file for it and run it I get warning saying unstripped binary for s390x .Any idea on this?. spec file is a file written to generate rpm .Its for s390x SUSE architecture\n\nspec file for package heapster\n\nCopyright (c) 2017 SUSE LINUX Products GmbH, Nuernberg, Germany.\n\nAll modifications and additions to the file contributed by third parties\nremain the property of their copyright owners, unless otherwise agreed\nupon. The license for this file, and modifications and additions to the\nfile, is the same license as for the pristine package itself (unless the\nlicense for the pristine package is not an Open Source License, in which\ncase the license is the MIT License). An \"Open Source License\" is a\nlicense that conforms to the Open Source Definition (Version 1.9)\npublished by the Open Source Initiative.\nPlease submit bugfixes or comments via http://bugs.opensuse.org/\n\nName:           heapster\nVersion:        1.4.1\nRelease:        1\nLicense:        Apache-2.0\nSummary:        Enables Container Cluster Monitoring and Performance Analysis\nUrl:            https://github.com/kubernetes/heapster\nGroup:          Applications/Productivity\nSource:         https://github.com/kubernetes/heapster/archive/v1.4.1.tar.gz\nSource1:        https://github.com/tools/godep/archive/v79.tar.gz\nBuildRequires:   git make wget gcc tar go rpm gcc gcc-c++ libseccomp-devel flex  binutils-devel gmp-devel mpfr-devel mpc-devel ruby docker\nBuildRoot:      %{_tmppath}/%{name}-%{version}-build\nExclusiveArch:  s390x\n%description\nHeapster collects and interprets various signals like compute resource usage, lifecycle events, etc, and exports cluster metrics via REST endpoints.Heapster supports the pluggable storage backends.\n%global debug_package %{nil}\n%global __os_install_post %{nil}\n%prep\n%setup -q -n heapster-%{version}\n%setup -n heapster-%{version} -T -D -b 1\n%build\nPlatform=s390x\nMAINROOT=pwd\ncd ..\nexport GOPATH=pwd\nexport PATH=$PATH:$GOPATH/bin:$HOME/go/bin\ndocker copy --from=gcr.io/google_containers/heapster-s390x:v1.4.1 heapster /heapster/heapster\nmkdir -p $GOPATH/src/github.com/tools/godep\ncd $GOPATH/src/github.com/tools/godep\ncp -R $GOPATH/godep-79/* .\ngo install -x\ncd -\nmkdir -p $GOPATH/src/k8s.io/heapster\ncp -R heapster-%{version}/* $GOPATH/src/k8s.io/heapster\ncd $GOPATH/src/k8s.io/heapster\nmake ARCH=$Platform\nchmod 0755 %{_builddir}/src/k8s.io/heapster/heapster\n%install\nmkdir -p $RPM_BUILD_ROOT/usr/bin\ncp %{_builddir}/src/k8s.io/heapster/heapster $RPM_BUILD_ROOT/usr/bin/\n%files\n%defattr(-,root,root)\n%{_bindir}/heapster\n%doc  LICENSE CONTRIBUTING.md\n%changelog\n. I needed the binary so that I can include directly in the spec file\n. ",
    "huangfushun": "another one I would suggest is change \"sum\" to median/min/max for single node,pod stat. ",
    "k8s-oncall": " Reviewable:start \nThis change is\u2002\n Reviewable:end \n. ",
    "thaume": "I signed it, but it seems the bot is not picking it up :). I'm going to create a new PR. Should we let them stabilize it then ? I'm already using 4.0 on my cluster. Ok I'll update to 4.0.2 then. ",
    "craigwillis85": "Any 4.x.x is a welcome update from 3.1.1 \ud83d\ude04 . Sorry, but it would seem this isn't fixed.\ngcr.io/google_containers/heapster-grafana:v4.0.2 does not exist\nCan someone check that this image has been built and pushed?. ",
    "jgoclawski": "Grafana seems to have stabilized at v4.0.2 (released 11 days ago), after first quick patches for 4.0.0.. I can't comment on changes made in config.toml, but nevertheless we would love to see a release with  updated influxdb!. Thanks for a quick merge!. ",
    "aleksandra-malinowska": "I signed it!. @k8s-bot ok to test. @k8s-bot ok to test. Updated sink-owners document.. @k8s-bot test this. @k8s-bot test this\n. @k8s-bot test this . cc @crassirostris. @DirectXMan12 Thanks, I fixed description. Leaving the commit message as it is since it's already been merged (unless this is important enough to revert?). cc @piosz . @k8s-bot test this\n. @k8s-bot test this\n. @k8s-bot test this\n. @crassirostris can you please merge this?. cc @piosz . @k8s-bot test this\n. @crassirostris can you please merge this?. LGTM provided Stackdriver confirmed this metric will work with label. Is there a specific test for this endpoint, or is it needed in tests for another reason? If the former, we should probably enable this endpoint, if it's still supported at all. I'm not up to date on Heapster development, but IIRC this was being deprecated. . Done. This is as intended. There's a fullMetricName function as well (line 64). In Stackdriver v3, custom metric name and type are not the same (see https://cloud.google.com/monitoring/custom-metrics/creating-metrics, \"Custom Metric Names\").. Done. To be refactored in follow-up PR. To be refactored in follow-up PR. Will change in next PR (adding more metrics). Fixed. Done. Done. Done. To be refactored in follow-up PR. To be refactored in follow-up PR. Done. AFAIU it's impossible to declare const structs?. Not adding this field caused custom metrics to be rejected by Stackdriver. I did a test and writing GKE container metrics seems to work without it, removing this (although kubelet-to-gcm still uses it?). Removed. Fixed. Fixed. This causes 'make sanitize' to fail, so keeping 2015. fixed. Done. Done. Done. Done. ",
    "petergardfjall": "@DirectXMan12: I only submitted a small patch for a piece of code that wasn't working properly. I'm not sure who wrote the majority of the OpenTSDB sink, but I would assume that that person is a more suitable candidate.. ",
    "rikatz": "@AlmogBaku Why it will break support to ES2? I think it's only something like pointing the lib contained here: https://github.com/kubernetes/heapster/blob/master/common/elasticsearch/elasticsearch.go#L25\nTo the v5 library, right?\nI'll try to change the source code pointing to this new version, and see what happens. We did this earlier with telegraf and worked fine, the only real problem was with fields containing '.' character, because ES 2 does not support them.\n. Guys, it's pretty simple to support Elastic v5. I did some simple tests here and it worked. I'm going to open a PR late tomorrow, to change the lib version (and some minor changes needed to make this work) and post here.\n. @maxott Elasticsearch 5 is already supported in Heapster (as merged in #1607).\nCan you please close this issue?\nThanks!. I signed it. Now I really signed it :P. @DirectXMan12 Thanks for the answer! I'm going to close this, and open another one with the requested changes (separate commits for Godeps and for the main changes).\nThanks!. @AlmogBaku sure, I'm just testing stuff here and will open the new PR ASAP. @qianzhangxa I think that also that #1439 also 'solves' this, as the docs seems with some kind of problem.\nI'm working also to update the library #1443 so ES 5 also works with heapster :). @AlmogBaku Thanks! Can you take a look on the PR #1445 as this is stopping the possibility to use ES5 library because of an issue with Godeps? Thanks!!!. @DirectXMan12 @qianzhangxa I think this issue can be closed, as Elasticsearch 5 is supported in Heapster and is using the correct index rotation.\nThanks!. @DirectXMan12 @AlmogBaku Will wait for CI to run again (had some problems with godep here...).\nHope this PR is better :). @DirectXMan12 you're right. I spent the whole morning trying to use godep, but because of some strange reason (or my fault) I wasn't able to add the deps correctly.\nI've tried to godep save ./..., godep update ./..., and everything else and failed miserably! \nSometimes it truncates the Godeps.json, or it erases the directory. \nWith godep restore it tried to download missing libraries to my GOPATH (this is the right behaviour, I think) but broke with some dependencies that doesn't exists anymore (as the riemann library).\nSo I've made a new program, ran godep on it, copied the array of missing libraries to Godeps.json and the vendor/gopkg.in/olivere to the right place, inside the code.\nAs you have a lot more experience that I do, can you teach me the right way to do this? \nThanks!!. @DirectXMan12 this is one of the libs with problem:\ngithub.com/bigdatadev/goryman\n. @DirectXMan12 So this is exactly the problem, the referenced lib doesn't exists anymore on github.\nI've found the same library on other repo (sauravmndl/goryman), the checksums from the files on 'vendor' directory and this repo matches.\nForked this one, changed the in program imports from the old repo to my new one, and commited.\nThe new library is rikatz/goryman.\nNote that this lib seems deprecated (the original was deleted from github).\nI'm going to close this PR by now, and open a new one to solve the riemann library problem (the reference, not the code!)\nAnd when this is corrected, I'll open a new PR to this feature (ES5), and Godeps working fine!. @DirectXMan12 this PR is about the Godeps problem I've mentioned in #1443 . @jsoriano Don't know if I did right, but tried to squash them in one commit.\n. @jsoriano Made a real mess here. Thinking if it's better to erase everything and open a new PR.. OK, I give up! Will open a newer, fresher and cleaner one.. @jsoriano Tried again and failed in a miseable way :( New PR seems easier to me, a git dumb user :). (this is what happens when Ops try to be Devs :P ). @DirectXMan12 actually I don't care also about deprecating this sink (I'm a ES user anyway), but there is an option of changing the lib from this gorymann to this one: https://github.com/amir/raidman\nI don't know how hard is to do this (and if even does matter to anyone), but this is an option.. OK, so this is strange. The Build failed because a file I haven't changed. And 'make test-unit' passed here, in my environment.\nGonna try again tomorrow to see what happened.. ping @AlmogBaku @DirectXMan12 I think this is the final one :). @AlmogBaku I'm not a good golang developer (just trying to make this happen), so actually don't know how to implement this other way :)\nGonna study some ways to code this, but will need some help :). @AlmogBaku Thanks :) maybe I can learn some stuffs from your prototype!. @AlmogBaku I understand you... 2 days spent so I could discover that another dep was breaking the godep save. @AlmogBaku not the right thread, but have you seen this: https://github.com/golang/dep ? . Closing this in favor of the new PR created by @AlmogBaku . @mcorbin I've just forked the code before someone erases the other one. No modifications from the original. It would be great if this code becames official through riemann corp, as this could also be kept in heapster code :). @k8s-bot test this. @AlmogBaku sure. Will review tomorrow :). Sorry @AlmogBaku I've been pretty busy in work. Have seen that this worked now. Will try to test the code tomorrow and give you a feedback :). @AlmogBaku sorry. I'll be testing this today and will return you in the end of this day!. @AlmogBaku while running heapster with elasticsearch 5, it gaves me the following error:\nE0510 15:30:33.101740   12777 factory.go:84] Failed to create sink: Failed to create ElasticSearch client: Failed to an ElasticSearch Client: no Elasticsearch node available\nI'm using the following directive: --sink=\"elasticsearch:?nodes=http://ipaddress:9200&index=testEvent&ver=5\"\nThe Elasticsearch is available and answering:\ncurl http://ipaddress:9200\n{\n  \"name\" : \"Pz3qaoM\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"cluster_uuid\" : \"BUd5OBEGTpCIE6Xak5qglA\",\n  \"version\" : {\n    \"number\" : \"5.4.0\",\n    \"build_hash\" : \"780f8c4\",\n    \"build_date\" : \"2017-04-28T17:43:27.229Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"6.5.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\nAm I missing something?\n. @AlmogBaku some more info:\nI'm running a standalone Elasticsearch, and started it now with 'sniff=false' (as in docs). Running with ver, disabling health check, all have the same result.\nThe following is a dump from communication between Heapster and Elastic:\n```\nHEAD / HTTP/1.1\nHost: 10.X.X.X:9200\nUser-Agent: Go-http-client/1.1\nHTTP/1.1 200 OK\ncontent-type: application/json; charset=UTF-8\ncontent-length: 327\nGET /_nodes/http HTTP/1.1\nHost: 10.X.X.X:9200\nUser-Agent: elastic/5.0.36 (linux-amd64)\nAccept: application/json\nContent-Type: application/json\nAccept-Encoding: gzip\nHTTP/1.1 200 OK\ncontent-type: application/json; charset=UTF-8\ncontent-encoding: gzip\ntransfer-encoding: chunked\n```\n. Sorry!!! I've been pretty busy at work :( will test again on monday and will let you know!\nRicardo P. Katz\n\nEm 20 de mai de 2017, \u00e0s 14:22, Almog Baku notifications@github.com escreveu:\n@rikatz ?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @chancez Can you please quickly write how you started ES5 and Heapster?\n\nThanks!. OK. I'm testing this again now and will let you know. @AlmogBaku tested here in ES5 and it worked really fine :)\nLGTM!\nThanks and sorry about delaying this so much :(. I was just wondering here if it wouldn't be better to have a 'Generic Timestamp' (like a @timestamp) for each metric, as this breaks each Timestamp into CpuMetricsTimestamp, MemoryMetricsTimestamp and this, in kibana makes a lot difficult to create dashboards (as each metric have it's own timestamp and not a generic one).\nAlso, after pushing some metrics I've faced some 'Failed to push data to sink: ElasticSearch Sink' but don't know why. Heapster is pushing metrics anyway, so probably is something specific to some object inside my cluster.. @AlmogBaku other 'review'. While creating a dashboard in Grafana, I was trying to aggregate metrics per namespace and per pod-name.\nWhen using MetricsTags.pod_id it worked fine, but when using MetricsTags.pod_name I've faced the error related to fielddata\nDon't know if this is fine for now, but probably we need a better field mapping in this Sink, as those searches/aggregations could be used :)\nAnything I can help with?. @AlmogBaku I'm fine, for now if we just create a generic timestamp for all metrics :)\nIn the (near) future we can make a template mapping of the metrics that is created before the first insertion in the new index, but this is something that can be dealled in another PR.\n. @AlmogBaku thanks!! :). @AlmogBaku need your opinion in this :). OK, maybe I'm missing something or I don't understand well how Kibana works, but when you add a new 'index' pattern, you have to specify if the index contains 'time based events', and what's the Time-field base.\nIf we do add a common 'index' (like testevent-*) pointing only to 'CpuMetricsTimestamp', the different events (memory, network) are 'discarded' from Kibana search.\nIf you add a non timestamped index in Kibana, we can plot graphs but the behaviour is pretty strange. You have to add graphs with 'X axis' using a Histogram, but the plotting doesn't work well (and also selecting the Time range).\nDo you have any Kibana Dashboard example? \nThank you!. @AlmogBaku No problems at all. I'll make some tests here, including Kibana Dashboards and check if everything works fine.. @outcoldman about the issues:\nSeparating the indexes per metrics could be applicable, but I think would make harder to create Kibana and/or Grafana Dashboards. \nLet's take a look at how Elastic does this, as they have a Metricbeat Plugin for kubernetes/kubelet:\nRegardless of using kube-state-metrics (this is another subject) or kubelet (and the collected metrics), Metricbeat would everytime export the metrics to the same Elasticsearch cluster (or any other output) as defined here.\nThe most important thing here is that the metrics use the same Timestamp field, but different type mapping per metrics. \nWaiting for your considerations :)\n. Agreed, you can use wildcards in that. Maybe a new argument would help anyway.\n@AlmogBaku what do you think about this rebasing:\n\nUsing only one timestamp instead of per-metric timestamp. This could be made in a new PR without any impacts (I think), as I don't know, with ES5 if there was someone using previously the Elasticsearch sink. Maybe need to release a 'breaking change' with this.\nCreate a new dedicated_indexes argument to create a index per metric type.\n. @outcoldman let's take this by parts :D first of all, changing the timestamp schema to something more generic would help? For me would a lot.\n\nNeed more opinions before 'breaking' stuffs!\nThen let's see how to refactor the fields / mapping structures (in another PR) and break other stuffs :)\nLet's make a 'list' of things that we think need to be changed (fields to be removed, arguments to be inserted, things to be changed) and achieve this in a lot of PRs to improve the ES5 sink :)\n. @outcoldman Have changed the Issue description, take a look if I've missed something.\nAnyway, need some other people from community (instead or just 3 of us), as I think some changes would break existing production environments :). @AlmogBaku I think that, by the end of 'types' in ES6, it's a good idea to have a per-metric Index, probably the impact migrating from 5 to 6 would be less than expected.\nAbout the type, I think you can use it (the field _type) to filter which metrics you want. If this is not the case to Kibana < 5, than probably this needs to be a breaking change. Maybe, after all we need to decide wheter to support or not anymore ES 2x versions.\nLet's close those questions, and we can start working on that :D\n. ",
    "bavarianbidi": "I signed it!. I signed it!. ",
    "envintus": "We realized that eventer needs to run as a separate container and have done so. We are now getting event data in InfluxDB. So this can be closed.\nHowever, we only realized this after a lot of googling, because we could not find any mention of needing to do this in the heapster documentation. Relevant link that helped get the right manifest created for heapster: https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/cluster-monitoring/influxdb/heapster-controller.yaml#L55. ",
    "krzyzacy": "The change I'm making only affects if using new features from go 1.7.\nSeems it's pretty green here.. Seems the heapster test needs to be dockerized and use go 1.7 image. Currently it's using go in jenkins which is 1.6. I'll take a look later.. @k8s-bot test this. @k8s-bot test this. cc @DirectXMan12 @piosz . ref #1501 . I don't have permission here to assign reviewers, @piosz PTAL thanks :-). /reopen. /assign. /reopen. /unassign. @piosz PTAL. @k8s-bot test this. @DirectXMan12 thanks!. @k8s-bot test this. @k8s-bot test this. @k8s-bot test this. yup. I definitely did not change historical_handlers_test.go. cc @DirectXMan12 \nI have a green run when hard-code jenkins@ into the code, now add it properly as a flag. Waiting for the test result.. Aha Jenkins overloaded. Please try again later. everywhere... will retrigger it once we drain the queue.... @k8s-bot test this. greened, thanks Jenkins!. I think the issue is https://github.com/kubernetes/heapster/pull/1387 removed deploy/docker/canary/Dockerfile, while the canarypush job is still referencing it. Is the job still needed? @luxas . shall I delete the ci job?. @piosz sure, what's the context?. em I don't think the image has been changed though. some sort of conflicted resource names?. @k8s-bot test this. Oh the issue is only related to the version, I thought presubmit is broken for everything..\nNot super familiar with how the integration is written, something new/different in 1.5.3 package which requires a test update?. /reopen\nthis is heapster :-(. @DirectXMan12 assume the only requirement for make unit is go?. /test pull-heapster-e2e-prow. Updated property [core/project].\nActivated service account credentials for: [pr-kubekins@kubernetes-jenkins-pull.iam.gserviceaccount.com]\nmake: *** No rule to make target 'test-unit'.  Stop.\n@x13n ^^ this is the output. /test pull-heapster-e2e-prow\nthe log link is busted :-(. you can find pod logs from https://prow.k8s.io/?type=presubmit&job=pull-heapster-e2e-prow. /test pull-heapster-e2e-prow\n. heapster still pins against 1.7.0-beta2?. grrrr... \nWARNING: Flag --zones is deprecated. Use --filter=\"zone:( ZONE ... )\" instead.\nW0109 19:57:30.010] For example --filter=\"zone:( europe-west1-b europe-west1-c )\".\nW0109 19:57:30.010] WARNING: Flag --regexp is deprecated. Use --filter=\"name~'REGEXP'\" instead.\nW0109 19:57:30.011] ERROR: (gcloud.compute.instance-groups.managed.list-instances) wrong collection: expected [compute.regionInstanceGroupManagers,compute.instanceGroupManagers], got [compute.instanceGroups], for path [https://www.googleapis.com/compute/v1/projects/kubernetes-jenkins-pull/zones/us-central1-b/instanceGroups/kubernetes-minion-group]\nthis is the actual error, which I think because heapster does not work with latest gcloud\n(I'm saying this because https://storage.googleapis.com/kubernetes-jenkins/pr-logs/pull/heapster/1922/pull-heapster-e2e/666/build-log.txt is passing but also spills out \"error loading kubeConfig: open /root/.kube/config: no such file or directory\") . /retest. ah, right, https://github.com/kubernetes/heapster/pull/1849 is not merged yet. but pull-heapster-e2e passed, so assume 1.7 should still work -- let me try to ~~downgrade gcloud~~ fix the gcloud config. /test pull-heapster-e2e-prow. /retest\nhttps://github.com/kubernetes/test-infra/pull/6355. /retest. https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/heapster/1922/pull-heapster-e2e-prow/15/ passed :-) I'll kill the Jenkins entry. /test pull-heapster-e2e-prow\nrun again just in case. /close. /test pull-heapster-e2e-prow. /test pull-heapster-e2e-prow. /test pull-heapster-e2e-prow. /test pull-heapster-e2e-prow. /test pull-heapster-unit. /test pull-heapster-unit. /hold cancel\nprow PR is merged. /retest. which go version is heapster using? rerun gofmt I guess?. /retest\njust pushed the image. /retest. oh......whoops. how did e2e pass. puh I see. Should it be reference or not &_@ I wrote too much python and forgot how go works.... ",
    "chulkilee": "Please release new docker image as well - I do not want to use canary on non-dev clusters :). ",
    "et304383": "Can we pretty please get any merge conflicts fixed up and merge this?  We really need an easy way to configure the retention policy to something not infinite.. ",
    "wxb0521": "In the 22 row of docs/source-configuration.md , it is too long that no highlight.\n'a SSL' ->'an SSL' . I signed it!. In the 22 row of docs/source-configuration.md , it is too long that no highlight.\n'a SSL' ->'an SSL'. I rebase it.. ",
    "sjpotter": "In digging even deeper into heapster, I see that labels are handled by an embedded kubeclient wrapped in a caching layer of sorts, but dont have a good way of debugging it.  Really no clue what could be going wrong here besides the kubeclient not being setup correctly.. So as a followup, found this code\nhttps://github.com/kubernetes/heapster/blob/master/metrics/apis/metrics/handlers.go#L245-L277\nand the answer appears to be: this is due to heapster having a disconnect between its model and metrics api.\nthe model api will show pod level stats for any \"containers\" that kubelet's stats/summary api says belong to the pod.  However, the metrics api iterates over the actual containers returned from kubeclient in kube's api pod struct.  If those containers aren't represented in the data stored from stats/summary (say because only total pod data was returned in a single \"opaque\" container) then heapster will return no data via the metrics api. while it will will show usage in the model api.\nThis also makes it harder to include overhead related to a pod that is external to a container itself, kubelet's stat summary could include a \"container\" that captures that, but heapster as is will just ignore.\nanyways, this isn't a bug per say, unless one considers the disconnect between model and metric responses to be a bug.. I worked around it, but in a CRI environment, one doesn't necessarily have cadvisor running in a traditional way.  Ex would be in running the pod in its own VM where the hypervisor is running in the pod cgroup so can see its usage at the pod level, but the pod itself is opaque to the host.. ",
    "dongziming": "I signed it!. ",
    "qianzhangxa": "Thanks @rikatz !\nSo does that mean the next release (v1.3.0) will support ES 5 and it will create time-based indices in ES?. Thanks @andyxning \nYes, I am sure that my ES is always in green state. And I have made --v=4 and here is the full log:\nI0113 03:15:05.000444       1 manager.go:79] Scraping metrics start: 2017-01-13 03:14:30 +0000 UTC, end: 2017-01-13 03:15:00 +0000 UTC\nI0113 03:15:05.002841       1 manager.go:98] Querying source: kubelet:192.168.122.149:10255\nI0113 03:15:05.008094       1 manager.go:98] Querying source: kubelet:192.168.122.136:10255\nI0113 03:15:05.067992       1 kubelet.go:232] successfully obtained stats for 82 containers\nI0113 03:15:06.461351       1 kubelet.go:232] successfully obtained stats for 84 containers\nI0113 03:15:06.462169       1 manager.go:152] ScrapeMetrics: time: 1.461615011s size: 166\nI0113 03:15:06.462214       1 manager.go:154]    scrape  bucket 0: 2\nI0113 03:15:06.462254       1 manager.go:154]    scrape  bucket 1: 0\nI0113 03:15:06.462264       1 manager.go:154]    scrape  bucket 2: 0\nI0113 03:15:06.462270       1 manager.go:154]    scrape  bucket 3: 0\nI0113 03:15:06.462277       1 manager.go:154]    scrape  bucket 4: 0\nI0113 03:15:06.462283       1 manager.go:154]    scrape  bucket 5: 0\nI0113 03:15:06.462336       1 manager.go:154]    scrape  bucket 6: 0\nI0113 03:15:06.462343       1 manager.go:154]    scrape  bucket 7: 0\nI0113 03:15:06.462349       1 manager.go:154]    scrape  bucket 8: 0\nI0113 03:15:06.462355       1 manager.go:154]    scrape  bucket 9: 0\nI0113 03:15:06.462361       1 manager.go:154]    scrape  bucket 10: 0\nI0113 03:15:06.558299       1 manager.go:113] Pushing data to: Metric Sink\nI0113 03:15:06.558338       1 manager.go:116] Data push completed: Metric Sink\nI0113 03:15:06.558434       1 manager.go:113] Pushing data to: ElasticSearch Sink\nW0113 03:15:07.936498       1 listers.go:68] can not retrieve list of objects using index : object has no meta: object does not implement the Object interfaces\nI0113 03:15:20.498742       1 manager.go:116] Data push completed: ElasticSearch Sink\nI0113 03:15:35.000248       1 manager.go:79] Scraping metrics start: 2017-01-13 03:15:00 +0000 UTC, end: 2017-01-13 03:15:30 +0000 UTC\nI0113 03:15:35.000498       1 manager.go:98] Querying source: kubelet:192.168.122.149:10255\nI0113 03:15:35.008555       1 manager.go:98] Querying source: kubelet:192.168.122.136:10255\nI0113 03:15:35.058173       1 kubelet.go:232] successfully obtained stats for 82 containers\nI0113 03:15:36.058787       1 kubelet.go:232] successfully obtained stats for 84 containers\nI0113 03:15:36.059616       1 manager.go:152] ScrapeMetrics: time: 1.059261996s size: 166\nI0113 03:15:36.059648       1 manager.go:154]    scrape  bucket 0: 2\nI0113 03:15:36.059655       1 manager.go:154]    scrape  bucket 1: 0\nI0113 03:15:36.059660       1 manager.go:154]    scrape  bucket 2: 0\nI0113 03:15:36.059665       1 manager.go:154]    scrape  bucket 3: 0\nI0113 03:15:36.059670       1 manager.go:154]    scrape  bucket 4: 0\nI0113 03:15:36.059688       1 manager.go:154]    scrape  bucket 5: 0\nI0113 03:15:36.059694       1 manager.go:154]    scrape  bucket 6: 0\nI0113 03:15:36.059699       1 manager.go:154]    scrape  bucket 7: 0\nI0113 03:15:36.059704       1 manager.go:154]    scrape  bucket 8: 0\nI0113 03:15:36.059708       1 manager.go:154]    scrape  bucket 9: 0\nI0113 03:15:36.059713       1 manager.go:154]    scrape  bucket 10: 0\nI0113 03:15:36.061793       1 manager.go:113] Pushing data to: Metric Sink\nI0113 03:15:36.061859       1 manager.go:113] Pushing data to: ElasticSearch Sink\nI0113 03:15:36.061879       1 manager.go:116] Data push completed: Metric Sink\nW0113 03:15:37.941523       1 listers.go:68] can not retrieve list of objects using index : object has no meta: object does not implement the Object interfaces\nW0113 03:15:37.971198       1 listers.go:68] can not retrieve list of objects using index : object has no meta: object does not implement the Object interfaces\nW0113 03:15:56.062056       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\nI0113 03:16:05.000426       1 manager.go:79] Scraping metrics start: 2017-01-13 03:15:30 +0000 UTC, end: 2017-01-13 03:16:00 +0000 UTC\n.... It seems pushing data to ES took more than 20s, then a timeout (20s) was triggered which caused the following warning message to show:\nFailed to push data to sink: ElasticSearch Sink\n. @andyxning I think the network between Heapster and ES is good. In my K8s cluster, Heapster is running in a pod on a worker node, ES is running in another pod on master node, they are in the same physical subnet. Here is the result that I ping ES from Heapster pod:\n--- 192.168.122.143 ping statistics ---\n55 packets transmitted, 55 received, 0% packet loss, time 54255ms\nrtt min/avg/max/mdev = 0.387/0.743/1.145/0.166 ms\n. I do not have Kibana installed, but I can see the metrics when I call ES API. And when I switched back to Heapster v1.2.0, I do not see such warning message.. > We appear to try to connect with the same client certificates that we use to connect to Kubernetes.\n@DirectXMan12 Do you mean Heapster (assume it is running as a pod in Kubernetes cluster) will use the token in the default service account to connect to kubelet? If so, I think kubelet should be started with the flag --client-ca-file, and its value should be same with the flag --client-ca-file of kube-apiserver, right? And if kubelet is started in this way, will the communication from kube-apiserver to kubelet be affected? E.g., when I run \"kubectl logs ...\" to get logs of a pod, kube-apiserver needs to connect to kubelet to get the logs, will this function still work?. ",
    "mihivagyok": "Hello, do you know when v1.3.0 stable will be released?\nThanks!. ",
    "pineking": "@andyxning thanks for the correction. I will follow the cAdvisor issue.. ",
    "dnavre": "This PR will be very useful especially if one runs database shards in kubernetes. Please do merge it :). I just checked the Kublet API and it is returning that data. Here's an example:\n{\n    \"podRef\": {\n     \"name\": \"mongo-shard-one-0\",\n     \"namespace\": \"default\",\n     \"uid\": \"248d8673-6553-11e7-845b-0203d18125f4\"\n    },\n    \"startTime\": \"2017-07-10T10:10:51Z\",\n    \"containers\": [\n     {\n      \"name\": \"mongo\",\n      \"startTime\": \"2017-07-10T10:12:22Z\",\n      \"cpu\": {\n       \"time\": \"2017-07-14T19:39:12Z\",\n       \"usageNanoCores\": 24500208,\n       \"usageCoreNanoSeconds\": 17524856640381\n      },\n      \"memory\": {\n       \"time\": \"2017-07-14T19:39:12Z\",\n       \"availableBytes\": 669958144,\n       \"usageBytes\": 2916573184,\n       \"workingSetBytes\": 2475769856,\n       \"rssBytes\": 2034851840,\n       \"pageFaults\": 6895170,\n       \"majorPageFaults\": 1683\n      },\n      \"rootfs\": {\n       \"time\": \"2017-07-14T19:39:12Z\",\n       \"availableBytes\": 13413863424,\n       \"capacityBytes\": 19988041728,\n       \"usedBytes\": 330313728,\n       \"inodesFree\": 4950160,\n       \"inodes\": 5242880,\n       \"inodesUsed\": 45\n      },\n      \"logs\": {\n       \"time\": \"2017-07-14T19:39:12Z\",\n       \"availableBytes\": 13413863424,\n       \"capacityBytes\": 19988041728,\n       \"usedBytes\": 24576,\n       \"inodesFree\": 4950160,\n       \"inodes\": 5242880,\n       \"inodesUsed\": 292720\n      },\n      \"userDefinedMetrics\": null\n     },\n     {\n      \"name\": \"filebeat\",\n      \"startTime\": \"2017-07-10T10:12:28Z\",\n      \"cpu\": {\n       \"time\": \"2017-07-14T19:39:12Z\",\n       \"usageNanoCores\": 155419,\n       \"usageCoreNanoSeconds\": 104380152043\n      },\n      \"memory\": {\n       \"time\": \"2017-07-14T19:39:12Z\",\n       \"usageBytes\": 11821056,\n       \"workingSetBytes\": 11239424,\n       \"rssBytes\": 10301440,\n       \"pageFaults\": 29772,\n       \"majorPageFaults\": 537\n      },\n      \"rootfs\": {\n       \"time\": \"2017-07-14T19:39:12Z\",\n       \"availableBytes\": 13413863424,\n       \"capacityBytes\": 19988041728,\n       \"usedBytes\": 163926016,\n       \"inodesFree\": 4950160,\n       \"inodes\": 5242880,\n       \"inodesUsed\": 30\n      },\n      \"logs\": {\n       \"time\": \"2017-07-14T19:39:12Z\",\n       \"availableBytes\": 13413863424,\n       \"capacityBytes\": 19988041728,\n       \"usedBytes\": 20480,\n       \"inodesFree\": 4950160,\n       \"inodes\": 5242880,\n       \"inodesUsed\": 292720\n      },\n      \"userDefinedMetrics\": null\n     }\n    ],\n    \"network\": {\n     \"time\": \"2017-07-14T19:39:21Z\",\n     \"rxBytes\": 32309235058,\n     \"rxErrors\": 0,\n     \"txBytes\": 106828584460,\n     \"txErrors\": 0\n    },\n    \"volume\": [\n     {\n      \"time\": \"2017-07-14T19:37:43Z\",\n      \"availableBytes\": 13413883904,\n      \"capacityBytes\": 19988041728,\n      \"usedBytes\": 21782528,\n      \"inodesFree\": 4950160,\n      \"inodes\": 5242880,\n      \"inodesUsed\": 2,\n      \"name\": \"logs\"\n     },\n     {\n      \"time\": \"2017-07-10T10:10:43Z\",\n      \"availableBytes\": 4187172864,\n      \"capacityBytes\": 4187185152,\n      \"usedBytes\": 12288,\n      \"inodesFree\": 1022253,\n      \"inodes\": 1022262,\n      \"inodesUsed\": 9,\n      \"name\": \"default-token-w300s\"\n     },\n     {\n      \"time\": \"2017-07-14T19:37:43Z\",\n      \"availableBytes\": 140919939072,\n      \"capacityBytes\": 158399967232,\n      \"usedBytes\": 10569945088,\n      \"inodesFree\": 9830308,\n      \"inodes\": 9830400,\n      \"inodesUsed\": 92,\n      \"name\": \"data\"\n     },\n     {\n      \"time\": \"2017-07-10T10:10:43Z\",\n      \"availableBytes\": 4187181056,\n      \"capacityBytes\": 4187185152,\n      \"usedBytes\": 4096,\n      \"inodesFree\": 1022257,\n      \"inodes\": 1022262,\n      \"inodesUsed\": 5,\n      \"name\": \"mongo-keyfile\"\n     }\n    ]\n   }\nI can help you out with the implementation if needed. If you tell me approximately what should be done I'll try to do it :). I just checked the Kublet API and it is returning that data. Here's an example:\n{\n    \"podRef\": {\n     \"name\": \"mongo-shard-one-0\",\n     \"namespace\": \"default\",\n     \"uid\": \"248d8673-6553-11e7-845b-0203d18125f4\"\n    },\n    \"startTime\": \"2017-07-10T10:10:51Z\",\n    \"containers\": [\n     {\n      \"name\": \"mongo\",\n      \"startTime\": \"2017-07-10T10:12:22Z\",\n      \"cpu\": {\n       \"time\": \"2017-07-14T19:39:12Z\",\n       \"usageNanoCores\": 24500208,\n       \"usageCoreNanoSeconds\": 17524856640381\n      },\n      \"memory\": {\n       \"time\": \"2017-07-14T19:39:12Z\",\n       \"availableBytes\": 669958144,\n       \"usageBytes\": 2916573184,\n       \"workingSetBytes\": 2475769856,\n       \"rssBytes\": 2034851840,\n       \"pageFaults\": 6895170,\n       \"majorPageFaults\": 1683\n      },\n      \"rootfs\": {\n       \"time\": \"2017-07-14T19:39:12Z\",\n       \"availableBytes\": 13413863424,\n       \"capacityBytes\": 19988041728,\n       \"usedBytes\": 330313728,\n       \"inodesFree\": 4950160,\n       \"inodes\": 5242880,\n       \"inodesUsed\": 45\n      },\n      \"logs\": {\n       \"time\": \"2017-07-14T19:39:12Z\",\n       \"availableBytes\": 13413863424,\n       \"capacityBytes\": 19988041728,\n       \"usedBytes\": 24576,\n       \"inodesFree\": 4950160,\n       \"inodes\": 5242880,\n       \"inodesUsed\": 292720\n      },\n      \"userDefinedMetrics\": null\n     },\n     {\n      \"name\": \"filebeat\",\n      \"startTime\": \"2017-07-10T10:12:28Z\",\n      \"cpu\": {\n       \"time\": \"2017-07-14T19:39:12Z\",\n       \"usageNanoCores\": 155419,\n       \"usageCoreNanoSeconds\": 104380152043\n      },\n      \"memory\": {\n       \"time\": \"2017-07-14T19:39:12Z\",\n       \"usageBytes\": 11821056,\n       \"workingSetBytes\": 11239424,\n       \"rssBytes\": 10301440,\n       \"pageFaults\": 29772,\n       \"majorPageFaults\": 537\n      },\n      \"rootfs\": {\n       \"time\": \"2017-07-14T19:39:12Z\",\n       \"availableBytes\": 13413863424,\n       \"capacityBytes\": 19988041728,\n       \"usedBytes\": 163926016,\n       \"inodesFree\": 4950160,\n       \"inodes\": 5242880,\n       \"inodesUsed\": 30\n      },\n      \"logs\": {\n       \"time\": \"2017-07-14T19:39:12Z\",\n       \"availableBytes\": 13413863424,\n       \"capacityBytes\": 19988041728,\n       \"usedBytes\": 20480,\n       \"inodesFree\": 4950160,\n       \"inodes\": 5242880,\n       \"inodesUsed\": 292720\n      },\n      \"userDefinedMetrics\": null\n     }\n    ],\n    \"network\": {\n     \"time\": \"2017-07-14T19:39:21Z\",\n     \"rxBytes\": 32309235058,\n     \"rxErrors\": 0,\n     \"txBytes\": 106828584460,\n     \"txErrors\": 0\n    },\n    \"volume\": [\n     {\n      \"time\": \"2017-07-14T19:37:43Z\",\n      \"availableBytes\": 13413883904,\n      \"capacityBytes\": 19988041728,\n      \"usedBytes\": 21782528,\n      \"inodesFree\": 4950160,\n      \"inodes\": 5242880,\n      \"inodesUsed\": 2,\n      \"name\": \"logs\"\n     },\n     {\n      \"time\": \"2017-07-10T10:10:43Z\",\n      \"availableBytes\": 4187172864,\n      \"capacityBytes\": 4187185152,\n      \"usedBytes\": 12288,\n      \"inodesFree\": 1022253,\n      \"inodes\": 1022262,\n      \"inodesUsed\": 9,\n      \"name\": \"default-token-w300s\"\n     },\n     {\n      \"time\": \"2017-07-14T19:37:43Z\",\n      \"availableBytes\": 140919939072,\n      \"capacityBytes\": 158399967232,\n      \"usedBytes\": 10569945088,\n      \"inodesFree\": 9830308,\n      \"inodes\": 9830400,\n      \"inodesUsed\": 92,\n      \"name\": \"data\"\n     },\n     {\n      \"time\": \"2017-07-10T10:10:43Z\",\n      \"availableBytes\": 4187181056,\n      \"capacityBytes\": 4187185152,\n      \"usedBytes\": 4096,\n      \"inodesFree\": 1022257,\n      \"inodes\": 1022262,\n      \"inodesUsed\": 5,\n      \"name\": \"mongo-keyfile\"\n     }\n    ]\n   }\nI can help you out with the implementation if needed. If you tell me approximately what should be done I'll try to do it :). I did have a look at the code and indeed it looks that heapster is actually collecting the volume metrics and they should be present in my influxdb with resoruce_id = Volume:data label. but I don't have a metric like that. Instead, I have some with resource_id:/dev/xvda1 or something similar which isn't informative at all as the device name in /dev/* is more or less random.\nI guess a little bit more digging in the code is needed.. I did some more investigation and turns out that to enable the summary API one needs to change the protocol in the source from kubernetes: to kubernetes.summary_api:. This is not documented anywhere and nowhere is it written what's the difference is. Should I document it as comments in the example configuration files?. @DirectXMan12 , as requested I moved the test logic to the decoding test case. :). Squashed commits. Looks like there's nothing else to do, right? ). ",
    "weikinhuang": "Has there been any updates on this? When running a self hosted cluster with spindle disks this is very useful information to have.. How do I access this in v1.5.0 within influxdb/grafana? When I try to add a new dashboard graph, that field seems to be missing:\n\n. I don't see them in the recorded metrics. @andyxning Here is my deployment spec for heapster (mostly from the kubernetes/addons dir)\nyaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: heapster-v1.5.0\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n    version: v1.5.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: heapster\n      version: v1.5.0\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v1.5.0\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      containers:\n        - image: gcr.io/google_containers/heapster-amd64:v1.5.0\n          name: heapster\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: 8082\n              scheme: HTTP\n            initialDelaySeconds: 180\n            timeoutSeconds: 5\n          command:\n            - /heapster\n            - --source=kubernetes.summary_api:''\n            - --sink=influxdb:http://monitoring-influxdb:8086\n        - image: gcr.io/google_containers/heapster-amd64:v1.5.0\n          name: eventer\n          command:\n            - /eventer\n            - --source=kubernetes:''\n            - --sink=influxdb:http://monitoring-influxdb:8086\n        - image: gcr.io/google_containers/addon-resizer:1.7\n          name: heapster-nanny\n          resources:\n            limits:\n              cpu: 50m\n              memory: 90Mi\n            requests:\n              cpu: 50m\n              memory: 90Mi\n          env:\n            - name: MY_POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: MY_POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          command:\n            - /pod_nanny\n            - --cpu=80m\n            - --extra-cpu=0.5m\n            - --memory=140Mi\n            - --extra-memory=4Mi\n            - --deployment=heapster-v1.5.0\n            - --container=heapster\n            - --poll-period=300000\n        - image: gcr.io/google_containers/addon-resizer:1.7\n          name: eventer-nanny\n          resources:\n            limits:\n              cpu: 50m\n              memory: 200Mi\n            requests:\n              cpu: 50m\n              memory: 200Mi\n          env:\n            - name: MY_POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: MY_POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          command:\n            - /pod_nanny\n            - --cpu=100m\n            - --extra-cpu=0m\n            - --memory=190Mi\n            - --extra-memory=500Ki\n            - --deployment=heapster-v1.5.0\n            - --container=eventer\n            - --poll-period=300000\n      serviceAccountName: heapster\n      tolerations:\n        - key: \"CriticalAddonsOnly\"\n          operator: \"Exists\". Ah. It looks like I'm using the summary API. Which does not have these metrics. Is there any issue or pr I can subscribe to for disk metrics in the summary API? Thanks @andyxning . I would love to help, but my knowledge of golang's syntax is limited.. No problem, thanks @andyxning!. ",
    "jingxu97": "@andyxning, will you pick up this PR again?. /test pull-heapster-e2e. cc @piosz @kawych . @loburm @piosz @DirectXMan12 Could you please take a look and see whether this PR is ready to be merged? Thanks a lot!. @piosz @kawych This PR is adding pod-level metrics from summary API to heapster. PTAL. @kawych I think conflict is resolved. Thanks!. @kawych since this PR has lgtm, will it merged automatically? After it is merged, I will work on adding the metric to stackdriver sink. Thanks!. @andyxning, Thank you! It would save me lot of time if I knew your PR! . @piosz @kawych Could you please help review this PR? Thanks!. @kawych Thanks for reviewing the PR. I could not add a reviewer. Who you recommend to review the code for stackdriver change? Thanks!. cc @dashpole . /test pull-heapster-unit. fixed. Thanks!. added. Thanks!. ",
    "tallclair": "This only adds the metrics for the cAdvisor source, but we're moving towards the summary source instead (parsed here: https://github.com/kubernetes/heapster/blob/master/metrics/sources/summary/summary.go).\n@piosz What is the future plan for the cAdvisor source? We may remove the cAdvisor stats from the Kubelet at some point in the future.. ",
    "Gimi": "@andyxning just wondering that are you still working on adding disk metrics to summary api source?. I use \"kubernetes\" source and still am not seeing any disk metrics, any idea? I'm using gcr.io/google_containers/heapster:v1.5.0 running on kubernetes 1.8.6.. @andyxning no error nor warning message from heapster's log.. @andyxning sorry for the late response, I use statsd.. thanks @andyxning for checking. I at the beginning did use the summary api resource, but I switched back to the legacy one when I found this ticket. But still did not see any disk metrics. I'll check again and see if I missed anything. Thank you.. ",
    "tobad357": "We are also seeing the same issue, seems like they haven't been built and pushed yet. ",
    "alirezaDavid": "i have similar issue.. #1453 related.. @whereisaaron Thanks man, now it's work like a charm.. ",
    "VAdamec": "With heapster-1.3.0-beta it works (https://github.com/kubernetes/heapster/archive/v1.3.0-beta.0.zip), I'm not getting graphs in kubernetes-dashboard, but I can see them via Grafana now. With yaml in master I can see metrics in kubernetes-dashboard, but cannot start grafana. I'll put it together and we will see ;-). This should be already fixed see #1453 also you can test v1.3.0-beta.0 this works for me, but I'm on version 1.5.1 so need to be verified for older versions. See my small example how to create small demo cluster to test functionalities.. ",
    "merqurio": "AFAIK ES5 Is supported now but we are getting the same error. We got some logs, but most of them are failing.\n2017-09-15T17:04:09.342499158Z I0915 17:04:09.342205       1 heapster.go:72] /heapster --metric_resolution=60s --source=kubernetes.summary_api:''?inClusterConfig=false&kubeletPort=10250&kubeletHttps=true&auth=/etc/kubernetes/heapster-kubeconfig --sink=elasticsearch:?nodes=http://es-client:9200&maxRetries=5&pipeline=heapster\n2017-09-15T17:04:09.342579151Z I0915 17:04:09.342268       1 heapster.go:73] Heapster version v1.4.2\n2017-09-15T17:04:09.343280130Z I0915 17:04:09.343164       1 configs.go:61] Using Kubernetes client with master \"https://169.50.169.110:25155\" and version <nil>\n2017-09-15T17:04:09.343293864Z I0915 17:04:09.343192       1 configs.go:62] Using kubelet port 10250\n2017-09-15T17:04:09.542677207Z I0915 17:04:09.542415       1 heapster.go:196] Starting with ElasticSearch Sink\n2017-09-15T17:04:09.542711583Z I0915 17:04:09.542436       1 heapster.go:196] Starting with Metric Sink\n2017-09-15T17:04:10.242020705Z I0915 17:04:10.241765       1 heapster.go:106] Starting heapster on port 8082\n2017-09-15T17:06:25.193795909Z W0915 17:06:25.193406       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n2017-09-15T17:08:25.145027735Z W0915 17:08:25.144647       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n2017-09-15T17:09:25.118083250Z W0915 17:09:25.117728       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n2017-09-15T17:10:25.138367928Z W0915 17:10:25.137958       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n2017-09-15T17:12:25.153220868Z W0915 17:12:25.152926       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n2017-09-15T17:13:25.112317981Z W0915 17:13:25.111871       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n2017-09-15T17:15:25.158870684Z W0915 17:15:25.158498       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n2017-09-15T17:16:25.106594296Z W0915 17:16:25.106236       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n2017-09-15T17:17:25.145095592Z W0915 17:17:25.144678       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n2017-09-15T17:19:25.126816843Z W0915 17:19:25.126411       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n2017-09-15T17:20:25.112695665Z W0915 17:20:25.112295       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n2017-09-15T17:21:25.141472668Z W0915 17:21:25.141128       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n2017-09-15T17:23:25.190699834Z W0915 17:23:25.190306       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n2017-09-15T17:24:25.127376959Z W0915 17:24:25.127006       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n2017-09-15T17:25:25.130034247Z W0915 17:25:25.129665       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n2017-09-15T17:27:25.113790338Z W0915 17:27:25.113353       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n2017-09-15T17:28:25.104902897Z W0915 17:28:25.104521       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\n2017-09-15T17:29:25.140189218Z W0915 17:29:25.139858       1 manager.go:119] Failed to push data to sink: ElasticSearch Sink\nHow can we help? Other services are working correctly in the elasticsearch cluster\nEvery 10 minutes at least 2 pushes succeed:\n\n\n. Thanks for the suggestion @andyxning ! I just changed the transport.tcp.connect_timeout in the ingestion node but it keeps failing with a similar ratio. \nAny other idea ? Thanks again !. You were right @andyxning and thanks for the verbosity suggestion @AlmogBaku; the error is due to a null_pointer_exception during index mapping. I guess I need to modify my mapping. Do you have any available ? Thanks for your help !\nFor the curious: \n2017-09-22T23:49:03.228626935Z I0922 23:49:03.228324       1 esVersionManager.go:186] Failed to execute bulk operation to ElasticSearch on index: &{exception java.lang.IllegalArgumentException: ScriptException[runtime error]; nested: NullPointerException;     false map[caused_by:map[lang:painless caused_by:map[type:null_pointer_exception reason:<nil>] type:script_exception reason:runtime error script_stack:[] script:if (ctx._type == 'cpu') { ctx.Metrics['cpu/usage_rate'].cores = ctx.Metrics['cpu/usage_rate'].value * 0.001; ctx.Metrics['cpu/limit'].cores = ctx.Metrics['cpu/limit'].value * 0.001; ctx.Metrics['cpu/request'].cores = ctx.Metrics['cpu/request'].value * 0.001; } if (ctx.Metrics['cpu/node_allocatable'] !== null) { ctx.Metrics['cpu/node_allocatable'].cores = ctx.Metrics['cpu/node_allocatable'].value * 0.001; ctx.Metrics['cpu/node_capacity'].cores = ctx.Metrics['cpu/node_capacity'].value * 0.001; }] type:illegal_argument_exception reason:ScriptException[runtime error]; nested: NullPointerException;] [] []}\n. @andyxning we finally went with metricbeat, but we were using v1.4.2.\nSorry for the delay. We hit this issue too. Modifying the versions to the ones pointed out by @patricklucas solved the problem.. ",
    "eceglov": "@andyxning, I use the latest stable version (1.4.2) + Elasticsearch 5.5.1 and I have exactly the same exception as @merqurio if use custom ingest pipeline for heapster.\nyaml\n      - command:\n        - /heapster\n        - --source=kubernetes.summary_api:''\n        - --sink=elasticsearch:http://elasticsearch-logging:9200?sniff=false&maxRetries=3&pipeline=heapster\n        - -v=4\nErrors\nI0928 09:54:27.900978       7 esVersionManager.go:186] Failed to execute bulk operation to ElasticSearch on index: &{exception java.lang.IllegalArgumentException: ScriptException[runtime error]; nested: NullPoin\nterException;     false map[caused_by:map[script:if (ctx._type == 'cpu') { ctx.Metrics['cpu/usage_rate'].cores = ctx.Metrics['cpu/usage_rate'].value * 0.001; ctx.Metrics['cpu/limit'].cores = ctx.Metrics['cpu/lim\nit'].value * 0.001; ctx.Metrics['cpu/request'].cores = ctx.Metrics['cpu/request'].value * 0.001; } if (ctx.Metrics['cpu/node_allocatable'] != null) { ctx.Metrics['cpu/node_allocatable'].cores = ctx.Metrics['cpu/\nnode_allocatable'].value * 0.001; ctx.Metrics['cpu/node_capacity'].cores = ctx.Metrics['cpu/node_capacity'].value * 0.001; } lang:painless caused_by:map[reason:<nil> type:null_pointer_exception] type:script_exce\nption reason:runtime error script_stack:[]] type:illegal_argument_exception reason:ScriptException[runtime error]; nested: NullPointerException;] [] []}\nWithout using the pipeline, Heapster just logs\nW0928 10:06:05.907553       7 manager.go:119] Failed to push data to sink: ElasticSearch Sink\nThere is indeed 20 sec default timeout to sink the metrics to ES DefaultSinkExportDataTimeout. Otherwise, the sink fails every 20 secs.\nI increased the default timeout to 60 seconds to make sure that this is not something that ES couldn't process this amount of metrics on time.\nI0928 08:25:05.877147       7 manager.go:113] Pushing data to: Metric Sink\nI0928 08:25:05.877164       7 manager.go:116] Data push completed: Metric Sink\nI0928 08:25:05.877151       7 manager.go:113] Pushing data to: ElasticSearch Sink\nI0928 08:25:05.877176       7 manager.go:116] Data push completed: ElasticSearch Sink\nI0928 08:26:05.891823       7 manager.go:113] Pushing data to: Metric Sink\nI0928 08:26:05.891843       7 manager.go:116] Data push completed: Metric Sink\nI0928 08:26:05.891824       7 manager.go:113] Pushing data to: ElasticSearch Sink\nW0928 08:26:25.892036       7 manager.go:119] Failed to push data to sink: ElasticSearch Sink\nI0928 08:27:05.892343       7 manager.go:113] Pushing data to: Metric Sink\nI0928 08:27:05.892350       7 manager.go:113] Pushing data to: ElasticSearch Sink\nI0928 08:27:05.892358       7 manager.go:116] Data push completed: Metric Sink\nW0928 08:27:25.892627       7 manager.go:119] Failed to push data to sink: ElasticSearch Sink\nI0928 08:28:05.886834       7 manager.go:113] Pushing data to: Metric Sink\nI0928 08:28:05.886840       7 manager.go:113] Pushing data to: ElasticSearch Sink\nI0928 08:28:05.886845       7 manager.go:116] Data push completed: Metric Sink\nW0928 08:28:25.887110       7 manager.go:119] Failed to push data to sink: ElasticSearch Sink\nI0928 08:29:05.869862       7 manager.go:113] Pushing data to: Metric Sink\nI0928 08:29:05.869875       7 manager.go:116] Data push completed: Metric Sink\nI0928 08:29:05.869901       7 manager.go:113] Pushing data to: ElasticSearch Sink\nW0928 08:29:25.870099       7 manager.go:119] Failed to push data to sink: ElasticSearch Sink\nWeird thing is that some metrics are getting ingested to ES after all.\nAlso, the first data push to ES is always successful according to the logs.. Any updates?. ",
    "natb1": "To reiterate some of what is above: I ran into similar issues and it mostly came down to 1) my ES cluster being under provisioned which caused timeouts and 2) some errors in my pipeline (similar to the null pointer exceptions above) which were getting buried by the default logging. For me, this was mostly on the ES side though, it would be nice if the default error logging were just a bit more verbose and more explicit about timeouts.. adding this to my heapster index template also helped\n\"refresh_interval\" : \"10s\",\n\"translog\": {\n    \"durability\": \"async\"\n}. ",
    "CallMeFoxie": "I signed it!. ",
    "fate-grand-order": "@k8s-ci-robot I signed it!. I signed it! @googlebot. Thanks for your reply. @andyxning  CLA is ok now!. ",
    "ottoyiu": "same issue here with those version of images as well. Worked around this by using a external loadbalancer instead.. ",
    "jcooke89": "I'm experiencing the exact same issue, I'm speculating that it's related to #1453 . ",
    "judexzhu": "Excuse me , first all , thanks for all the contribution and your hard work , really great job,  and I'm stilling facing this issue, should I use the older version image and should I wait for awhile and it will be fixed really soon ? thanks in advance . I have the same issue with you , add the amd64 as #1469 said, either heapter timeout or Grafana time out with the Influxdb , looks like only one can connect to influxdb 8086 in the same time \n2017-01-23T03:58:22.399761000Z I0123 03:58:22.397433       1 influxdb.go:231] created influxdb sink with options: host:monitoring-influxdb:8086 user:root db:k8s\n2017-01-23T03:58:22.400195000Z I0123 03:58:22.397518       1 heapster.go:193] Starting with InfluxDB Sink\n2017-01-23T03:58:22.400636000Z I0123 03:58:22.397535       1 heapster.go:193] Starting with Metric Sink\n2017-01-23T03:58:22.417996000Z I0123 03:58:22.416795       1 heapster.go:105] Starting heapster on port 8082\n2017-01-23T04:00:25.285607000Z W0123 04:00:25.281553       1 manager.go:119] Failed to push data to sink: InfluxDB Sink\n2017-01-23T04:01:12.512830000Z E0123 04:01:12.507850       1 influxdb.go:150] Failed to create infuxdb: failed to ping InfluxDB server at \"monitoring-influxdb:8086\" - Get http://monitoring-influxdb:8086/ping: dial tcp 10.101.152.15:8086: getsockopt: connection timed out\nbut the grafana can connect no issue , but no graphic , for heapster didnt send any data to influxdb \nhope someome can help. I also check the #1482 and #1475 \nhere is my k8s retention polices\n\n. after using the new version , this issue can not be reproduced ,so I close the issue, thanks for the great job . ",
    "idcrook": "@judexzhu et al.\nI was about to file an issue, but I discovered this one.  Noticed that the latest images are there on gcr.io, but have just moved into per-ARCH id's now. Hope that helps!\nI'll submit a PR if that helps things along.\ndiff\nmodified   deploy/kube-config/influxdb/grafana-deployment.yaml\n@@ -13,7 +13,7 @@ spec:\n     spec:\n       containers:\n       - name: grafana\n-        image: gcr.io/google_containers/heapster-grafana:v4.0.2\n+        image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2\n         ports:\n           - containerPort: 3000\n             protocol: TCP\nmodified   deploy/kube-config/influxdb/influxdb-deployment.yaml\n@@ -13,7 +13,7 @@ spec:\n     spec:\n       containers:\n       - name: influxdb\n-        image: gcr.io/google_containers/heapster-influxdb:v1.1.1\n+        image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\n         volumeMounts:\n         - mountPath: /data\n           name: influxdb-storage. ",
    "sjezewski": "FYI this happened to me today. I could not pull either gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 or gcr.io/google_containers/heapster-grafana:v4.0.2 on my local machine.\nInstead, I used the grafana/grafana:4.0.2 image on dockerhub. Obvious in retrospect, but I'll have to update some of my scripts which clearly use an old deployment manifest.. So after some fiddling I was able to connect to the influxDB pod and run the sanity check suggested in the other thread, but not sure what to make of this. I'm connecting via kubectl port-forward and when I issue the queries that process outputs a new log line ... so as far as I can tell it really is connecting. However the db/policies don't seem to be setup at all?\n```\n\n\n\nclient = InfluxDBClient(\"localhost\", 8086, 'root', 'root', 'k8s')\nclient.query(\"SHOW RETENTION POLICIES\")\nResultSet({'(u'results', None)': [{u'duration': u'0s', u'default': True, u'replicaN': 1, u'name': u'default', u'shardGroupDuration': u'168h0m0s'}]})\nclient.query(\"SHOW DATABASES\")\nResultSet({'(u'databases', None)': [{u'name': u'_internal'}, {u'name': u'k8s'}]})\n```\n\n\n\nIt's possible I'm not connecting correctly (or perhaps w the wrong user?)\nI was also able to go grab the influx config file if thats helpful:\n```\n7-03-02[14:06:35]:~:1$kubectl --namespace=kube-system exec monitoring-influxdb-421024531-2pvmk -- cat /etc/config.toml\nreporting-disabled = true\nbind-address = \":8088\"\n[meta]\n  dir = \"/data/meta\"\n  retention-autocreate = true\n  logging-enabled = true\n[data]\n  dir = \"/data/data\"\n  wal-dir = \"/data/wal\"\n  query-log-enabled = true\n  cache-max-memory-size = 1073741824\n  cache-snapshot-memory-size = 26214400\n  cache-snapshot-write-cold-duration = \"10m0s\"\n  compact-full-write-cold-duration = \"4h0m0s\"\n  max-series-per-database = 1000000\n  max-values-per-tag = 100000\n  trace-logging-enabled = false\n[coordinator]\n  write-timeout = \"10s\"\n  max-concurrent-queries = 0\n  query-timeout = \"0s\"\n  log-queries-after = \"0s\"\n  max-select-point = 0\n  max-select-series = 0\n  max-select-buckets = 0\n[retention]\n  enabled = true\n  check-interval = \"30m0s\"\n[admin]\n  enabled = false\n  bind-address = \":8083\"\n  https-enabled = false\n  https-certificate = \"/etc/ssl/influxdb.pem\"\n[shard-precreation]\n  enabled = true\n  check-interval = \"10m0s\"\n  advance-period = \"30m0s\"\n[monitor]\n  store-enabled = true\n  store-database = \"_internal\"\n  store-interval = \"10s\"\n[subscriber]\n  enabled = true\n  http-timeout = \"30s\"\n  insecure-skip-verify = false\n  ca-certs = \"\"\n  write-concurrency = 40\n  write-buffer-size = 1000\n[http]\n  enabled = true\n  bind-address = \":8086\"\n  auth-enabled = false\n  log-enabled = true\n  write-tracing = false\n  pprof-enabled = false\n  https-enabled = false\n  https-certificate = \"/etc/ssl/influxdb.pem\"\n  https-private-key = \"\"\n  max-row-limit = 10000\n  max-connection-limit = 0\n  shared-secret = \"\"\n  realm = \"InfluxDB\"\n  unix-socket-enabled = false\n  bind-socket = \"/var/run/influxdb.sock\"\n[[graphite]]\n  enabled = false\n  bind-address = \":2003\"\n  database = \"graphite\"\n  retention-policy = \"\"\n  protocol = \"tcp\"\n  batch-size = 5000\n  batch-pending = 10\n  batch-timeout = \"1s\"\n  consistency-level = \"one\"\n  separator = \".\"\n  udp-read-buffer = 0\n[[collectd]]\n  enabled = false\n  bind-address = \":25826\"\n  database = \"collectd\"\n  retention-policy = \"\"\n  batch-size = 5000\n  batch-pending = 10\n  batch-timeout = \"10s\"\n  read-buffer = 0\n  typesdb = \"/usr/share/collectd/types.db\"\n[[opentsdb]]\n  enabled = false\n  bind-address = \":4242\"\n  database = \"opentsdb\"\n  retention-policy = \"\"\n  consistency-level = \"one\"\n  tls-enabled = false\n  certificate = \"/etc/ssl/influxdb.pem\"\n  batch-size = 1000\n  batch-pending = 5\n  batch-timeout = \"1s\"\n  log-point-errors = true\n[[udp]]\n  enabled = false\n  bind-address = \":8089\"\n  database = \"udp\"\n  retention-policy = \"\"\n  batch-size = 5000\n  batch-pending = 10\n  read-buffer = 0\n  batch-timeout = \"1s\"\n  precision = \"\"\n[continuous_queries]\n  log-enabled = true\n  enabled = true\n  run-interval = \"1s\"\n```\nI'm happy to jump on a slack somewhere if this would be more helpful to debug 'live'. Ok, the DB is there and the policy is set to the default policy. Just took me a moment to parse those results.\nIn this case ... I'm not sure where the issue lies.\nThe only thing I can think of that might be a 'non standard' k8s deployment is that in lieu of setting up DNS, when deploying I set the private DNS flag w kops and am using an /etc/hosts hack to allow kubectl to connect. All my other services work just fine. But that's the only other thing I can think of that's 'different'.\n. Ah!\nInstead of the one requested by XHR:\nhttp://localhost:8001/api/dashboards/home\nIf I hit this URL:\nhttp://localhost:8001/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/api/dashboards/home\nI see a valid response:\n{\"meta\":{\"isHome\":true,\"canSave\":false,\"canEdit\":true,\"canStar\":false,\"slug\":\"\",\"expires\":\"0001-01-01T00:00:00Z\",\"created\":\"0001-01-01T00:00:00Z\",\"updated\":\"0001-01-01T00:00:00Z\",\"updatedBy\":\"\",\"createdBy\":\"\",\"version\":0},\"dashboard\":{\"annotations\":{\"list\":[]},\"editable\":true,\"hideControls\":true,\"id\":null,\"links\":[],\"rows\":[{\"collapse\":false,\"editable\":true,\"height\":\"25px\",\"panels\":[{\"content\":\"\\u003cdiv class=\\\"text-center dashboard-header\\\"\\u003e\\n  \\u003cspan\\u003eHome Dashboard\\u003c/span\\u003e\\n\\u003c/div\\u003e\",\"editable\":true,\"id\":1,\"links\":[],\"mode\":\"html\",\"span\":12,\"style\":{},\"title\":\"\",\"transparent\":true,\"type\":\"text\"}],\"title\":\"New row\"},{\"collapse\":false,\"editable\":true,\"height\":\"510px\",\"panels\":[{\"headings\":true,\"id\":3,\"limit\":4,\"links\":[],\"query\":\"\",\"recent\":true,\"search\":false,\"span\":7,\"starred\":true,\"tags\":[],\"title\":\"\",\"transparent\":false,\"type\":\"dashlist\"},{\"editable\":true,\"error\":false,\"id\":4,\"isNew\":true,\"links\":[],\"span\":5,\"title\":\"\",\"transparent\":false,\"type\":\"pluginlist\"}],\"title\":\"Row\"}],\"schemaVersion\":12,\"sharedCrosshair\":false,\"style\":\"dark\",\"tags\":[],\"templating\":{\"list\":[]},\"time\":{\"from\":\"now-6h\",\"to\":\"now\"},\"timepicker\":{\"enable\":false,\"refresh_intervals\":[\"5s\",\"10s\",\"30s\",\"1m\",\"5m\",\"15m\",\"30m\",\"1h\",\"2h\",\"1d\"],\"time_options\":[\"5m\",\"15m\",\"1h\",\"6h\",\"12h\",\"24h\",\"2d\",\"7d\",\"30d\"],\"type\":\"timepicker\"},\"timezone\":\"browser\",\"title\":\"Home\",\"version\":2}}\nSo it seems like this is an issue w rewriting the paths properly. I think I saw a different issue about this somewhere.\nBut ... always happy for a point in the right direction.\n. Ok, got something working.\nAs in #657 \nI was getting the links improperly rewritten. My GF_SERVER_ROOT_URL is set to \"/\"\n... which is presumably why it didn't work when accessing it via the kubectl proxy.\nIt works just fine if I use kubectl portforwarding and access that way. E.g.\nkubectl --namespace=kube-system port-forward monitoring-grafana-3344903701-vtknq  3000:3000 &\nAnd then hitting localhost:3000\nHonestly I'm not sure which access pattern is the 'recommended way', but as is this setting doesn't work with the kubectl proxy access pattern. For my purposes, I have it working and have access, so I'm good.\n. ",
    "Dmitry1987": "Hi @r0bj ,\ndid you find a solution?  I'm stuck with same.. heapster same version, but on k8s 1.5.1 . I think it's because Rancher didn't installed cAdvisor on nodes... some post says they use \"docker stats\" directly for the dashboard, and 'removed cAdvisor' from installation since Rancher version 1.1 or something like that ...  will try tomorrow to setup cAdvisor sidecar on all nodes and see if it helps.. @DirectXMan12  seems like they don't, because my current setup is the \"out of the box\" latest Rancher 1.3 with latest Kubernetes from it's catalog (Rancher creates K8s infra containers from a template), and Heapster+grafana+influxDB  stack from catalog doesn't show/collect any pod stats. Grafana shows \"node\" stats though. so i'm trying to understand now how to make heapster receive it's needed data.\nHeapster log shows this non stop:\nlisters.go:68] can not retrieve list of objects using index : object has no meta: object does not implement the Object interfaces.\nhandlers.go:216] No metrics for pod xxxxxxxxxxx\nhandlers.go:265] No metrics for container   xxxxxxxxxx\nhandlers.go:216] No metrics for pod xxxxxxxxxxx\nhandlers.go:265] No metrics for container   xxxxxxxxxx\nand funny thing is, the pods it says \"no metric from\" are the 2 pods I applied HPA on their deployment. it doesn't seem to even try to fetch info about other pods, which is weird. Will be reading about how all this works...  \nusing  \"kubernetes/heapster:canary\" version.\nwill be glad for any advice or idea about what it might be  :) . For me this was solved with Rancher 1.4.0  which brings new \"stack\" of K8s with it... after I updated the K8s components, suddenly Kubelets started collecting data and heapster filled influxDB with all metrics, now I have everything in Grafana :+1:  as expected.  Seems like Rancher devs put back the cAdvisor (in previous release it was without it, I've read on their forum). \nbut still sometimes I see the  \"can not retrieve list of objects using index : object has no .....\"  errors logged, have no idea what causes this...   btw, what are \"stub pods\" @DirectXMan12  ?  I'm doing manual cleanup of any exited containers on hosts, and manual cleanup of unused images, can this cause such issue?. oh, i totally forgot I've read somewhere about kubelet cleaning the host (but only when gets full, to thresholds, right?).... i need to remove my manual cleaner cron job for sure. thanks :). Thanks @aeoluswing ,\nas I mentioned, in rancher 1.4 the data is being collected properly, and Heapster+Grafan+InfluxDB stack from catalog works well. Regarding those logs:\ncan not retrieve list of objects using index : object has no .....\nI always see them.  Do you mean a redeploy/cleanup of etcd helps get rid of those?  cleanup sounds a bit frightening, as all K8s data stored in there :D  , what did you do when cleaning up etcd?. Interesting... though I have no idea where cAdvisor hiding in those modified Rancher K8s images (they recompile some of the components, and put them all into custom special docker images), but it's worth to research. Thanks for the info @aeoluswing  :+1: . @jeremywu0127 ,  you should verify your Heapster instance can reach all nodes on needed ports and that other components can reach it back (500 and 9000 something, please check in heapster docs...)  verify the kubelet ports are open to access for Heapster. I had a very similar scenario, reason was Heapster not reaching all servers (as a test - open all traffic between server where Heapster hosted and few other nodes with pods, to see if you'll suddenly have HPA working for those pods). \nhope it helps.. mm.. no idea what else might cause this, especially on same host o_O really strange.. please share if you solve it :) . @jeremywu0127  can you try switching to different versions of heapster? maybe something with the schema changes... btw regarding RBAC, what if the policies are not configured properly and block 'something somewhere'?  (I'm getting lost in the rising complexity of k8s with each new version, still can't get my head around all aspects related to RBAC.. but worth to check this direction in troubleshooting... just a guess). Hey guys, thanks for reporting back, it's a useful info :+1:  . ",
    "r0bj": "@Dmitry1987 unfortunately no, I didn't find any solution for that.. @DirectXMan12 yes I can see skipping rate calculations in logs with --vmodule=*=4:\nI0209 12:54:35.152486       1 rate_calculator.go:51] Skipping rates for namespace:ops/pod:logstash-1576143268-3xjwl/container:logstash - different create time new:2017-02-08 18:58:30.485800088 +0000 UTC  old:0001-01-01 00:00:00 +0000 UTC\nMore logs with --vmodule=*=4:\nhttps://gist.github.com/r0bj/5b89014cd7dd3f37cb70fa89f1ab6e8a. I'm not using Rancher, just bare metal kubernetes installation.. @DirectXMan12 In my case I don't do any manual cleanup. All k8s components are in version 1.5.2, docker is 1.12.6.. heapster: 1.4.0-beta.0\nkubernetes: 1.6.4\ndocker: 1.12.6\nOS: ubuntu 14.04\nI0627 14:14:55.203314       1 rate_calculator.go:53] Skipping rates for namespace:ops/pod:logstash-1163116248-c1gnc - different create time new:2017-06-07 09:13:42.843665465 +0000 UTC  old:0001-01-01 00:00:00 +0000 UTC\nI0627 14:14:55.203350       1 rate_calculator.go:53] Skipping rates for namespace:prod/pod:helios-486069052-cpdfq/container:helios - different create time new:2017-06-20 13:40:54.279525385 +0000 UTC  old:0001-01-01 00:00:00 +0000 UTC\nI0627 14:14:55.203367       1 rate_calculator.go:53] Skipping rates for namespace:logs-ingress/pod:fluentd-listener-3760655665-x7b0k/container:fluentd-listener - different create time new:2017-05-18 09:49:33.437557364 +0000 UTC  old:0001-01-01 00:00:00 +0000 UTC\nI0627 14:14:55.203462       1 pod_based_enricher.go:129] Container namespace:ops/pod:celery-mediawiki-main-4064416888-zkwqz/container:mediawiki-main not found, creating a stub\nI0627 14:14:55.203545       1 pod_based_enricher.go:100] Pod namespace:kube-system/pod:calico-node-xv682 not found, creating a stub\nI0627 14:14:55.203579       1 pod_based_enricher.go:129] Container namespace:kube-system/pod:calico-node-xv682/container:install-cni not found, creating a stub\n[...]\nI0627 14:14:55.213923       1 node_aggregator.go:44] No metric for node node:k8s-worker-s11, cannot perform node level aggregation.\nI0627 14:14:55.213941       1 node_aggregator.go:44] No metric for node node:k8s-controller-s3, cannot perform node level aggregation.\nI0627 14:14:55.213953       1 node_aggregator.go:44] No metric for node node:k8s-worker-s12, cannot perform node level aggregation.\nMore logs:\nhttps://gist.github.com/r0bj/29cb420a761f0dd71d29887e4994d200\nHorizontal pod autoscaling fails with this issue.\nIs there anything I can do to debug it deeper?. @DirectXMan12 my heapster container command:\n- /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://metrics:8086?withfields=true&db=k8s_sjc\n        - --metric-resolution=10s. @DirectXMan12 thanks for the hint, so now messages are little different:\nI0629 09:06:25.582615       1 rate_calculator.go:49] Skipping rate calculations for namespace:kube-system/pod:kube-proxy-247jk - new batch (2017-06-29 09:06:14 +0000 UTC) was not scraped strictly after old batch (2017-06-29 09:06:14 +0000 UTC)\n[...]\nI0629 09:06:29.689475       1 handlers.go:215] No metrics for pod fandom/upstream-prod-1277166791-ghcjp\nFull log:\nhttps://gist.github.com/r0bj/07e4854728862e5cf1ef1c13b9102aa7\nSo instead of: different create time new:[...]  old:0001-01-01 00:00:00 +0000 UTC there is now: new batch ([...]) was not scraped strictly after old batch ([...])\nkubelet summary API endpoint seems working fine:\n```\ndocker ps -qf \"label=io.kubernetes.container.name=heapster\" | xargs docker inspect --format '{{.State.Pid}}'\n14691\nnsenter -t 14691 -n curl http://k8s-worker-s10:10255/stats/summary\n{\n  \"node\": {\n   \"nodeName\": \"k8s-worker-s10\",\n   \"systemContainers\": [\n    {\n     \"name\": \"kubelet\",\n     \"startTime\": \"2017-06-27T22:12:25Z\",\n     \"cpu\": {\n      \"time\": \"2017-06-29T09:22:24Z\",\n      \"usageNanoCores\": 3744712489,\n      \"usageCoreNanoSeconds\": 13508107484362565\n     },\n     \"memory\": {\n      \"time\": \"2017-06-29T09:22:24Z\",\n      \"availableBytes\": 88867352576,\n      \"usageBytes\": 41442099200,\n      \"workingSetBytes\": 12505178112,\n      \"rssBytes\": 502865920,\n      \"pageFaults\": 14161385628,\n      \"majorPageFaults\": 37805\n     },\n     \"userDefinedMetrics\": null\n    },\n[...]\n```\nFull stats summary:\nhttps://gist.github.com/r0bj/72049ac2ca2d2e02f62b2c44127b0143\nStats for other nodes are also available from heapster pod.. I've used hepaster with --metric-resolution=10s. Now I've switched to --metric-resolution=20s and it seems that there is no issue.\nI've done some digging and it seems that heapster polling interval should be > 2*\npolling duration of cadvisor:\nhttps://github.com/kubernetes/kubernetes/issues/18044#issuecomment-161435365\nAnd cadvisor polling interval is currently hardcoded to 10s:\nhttps://github.com/kubernetes/kubernetes/pull/19374\nSo the minimal heapster polling interval can be 20s.\n@DirectXMan12 Does it make any sense to you?. @piosz I can live with 30s interval of heapster polling, I just use 10s resolution in my metrics infrastructure so it was natural idea.\nAnyway, thanks for help!. /remove-lifecycle stale. Yes, kubelet is started with authorization-webhook true.\nAs a workaround I just created separate ClusterRole and use it for heapster:\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: heapster-custom-fix\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  - nodes\n  - namespaces\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - extensions\n  resources:\n  - deployments\n  verbs:\n  - get\n  - list\n  - update\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes/stats\n  verbs:\n  - get. ",
    "aeoluswing": "indeed,@Dmitry1987,i have been stuck this issue before and discover the 'broken metrics' matter about rancher-k8s(version1.1 or higher).The reason is just for the state collection method which via docker stat and container stat,but for heapster the more appropriate choice is cAdvisor(summary api for more efficient approach).after upgrade the rancher version deployed recently(v1.4 or v1.4.1),you can find the \"lost pod and container metrics\" in your back-end storage(e.g. influxdb) which will get the same response when invoke the API.\nwhat is worth mentioning,if i just upgrade the k8s components,\"no metrics\" errors arised.But cleaning the etcd cluster will vanish this.So i make a guess at the metrics mechanism,etcd maybe not renovate if you just update component.\ngood luck ^^. @Dmitry1987,etcd records the registry kv info for k8s,my step just trigger this procedure(rancher keeps the data lifecycle during component updating by setting the etcd data containers stat to 'started-once'),i suggest a data validation for cadvisor collecting metrics configurations,as we all known,cadvisor can collect any structured information(including name of metrics and regexps).Make sure cAdvisor application-metric labels(prefix is io.cadvisor.metric) contain the pod and container fields(i think these might be more easier than cleanup for prod). My pleasure.Capturing different structured metrics is a very helpful measure for cluster research,especially on cluster performance and schedule.It's a piece of cake for discussion on any topic about k8s,docker and rancher.haha^^. ",
    "mlmhl": "Thanks @DirectXMan12 \n\nI0209 12:54:35.152486       1 rate_calculator.go:51] Skipping rates for namespace:ops/pod:logstash-1576143268-3xjwl/container:logstash - different create time new:2017-02-08 18:58:30.485800088 +0000 UTC  old:0001-01-01 00:00:00 +0000 UTC\n\nAs you mentioned, the \"skipping rate\" message is caused by a version mismatch in the cluster, do you know witch version is mismated, the heapster vs. k8s or k8s vs. etcd? \nAnd in this message the old create time is \"0001-01-01 00:00:00 +0000 UTC\", looks like not set. \nAccording to my understanding, the metric data is retrieved from cadvisor, witch is embed in kubelet, are there any wrong with kubelet?. ",
    "jeremywu0127": "I have met this issue with k8s version 1.6.3, the kubectl get hpa shows like\n```\nkubectl get hpa\nNAME         REFERENCE               TARGETS    MINPODS   MAXPODS   REPLICAS   AGE\nphp-apache   Deployment/php-apache    / 50%   1         10        1          19h\nthe logs of heapster shows like\nI0526 05:52:59.974002       1 handlers.go:264] No metrics for container php-apache in pod default/php-apache-3933604115-ssq2r\nI0526 05:52:59.974789       1 handlers.go:215] No metrics for pod default/php-apache-3933604115-ssq2r\nI0526 05:53:29.980479       1 handlers.go:215] No metrics for pod default/php-apache-3933604115-ssq2r\nI0526 05:53:59.987180       1 handlers.go:215] No metrics for pod default/php-apache-3933604115-ssq2r\nthe php-apache  is\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: php-apache\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        run: php-apache\n    spec:\n      containers:\n      - name: php-apache\n        image: gcr.io/google_containers/hpa-example\n        imagePullPolicy: IfNotPresent\n        ports:\n          - containerPort: 80\nthe hpa logs like\nI0526 14:17:28.287910   28262 horizontal.go:515] Successfully updated status for php-apache\nE0526 14:17:28.288323   28262 horizontal.go:201] failed to compute desired number of replicas based on listed metrics for Deployment/default/php-apache: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from heapster\nI0526 14:17:58.282302   28262 horizontal.go:515] Successfully updated status for php-apache\nE0526 14:17:58.282334   28262 horizontal.go:201] failed to compute desired number of replicas based on listed metrics for Deployment/default/php-apache: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from heapster\nthen I add resource quta in php-apache by referring [#6293](https://github.com/openshift/origin/issues/6293), it works fine.\nnew php-apache looks like:\ncat php-apche.yml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: php-apache\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        run: php-apache\n    spec:\n      containers:\n      - name: php-apache\n        image: gcr.io/google_containers/hpa-example\n        imagePullPolicy: IfNotPresent\n        ports:\n          - containerPort: 80\n        resources:\n          limits:\n            cpu: 200m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\nI guess since k8s version `1.6`,  `hpa` support custom metrics more than cpu and memory,  k8s may not set default metric, so we must manually write resource quta in deployment . @Dmitry1987 sorry for late response. `heapster` and other components (`kubelet` and `kube-proxy`) run on same node, I manually run `curl`, it looks like\ncurl http://172.16.97.3:8082/apis/metrics/v1alpha1/namespaces/default/pods?labelSelector=run%3Dphp-apache\n{\n  \"metadata\": {},\n  \"items\": null\n }\nseems heapster can reach each other component, any ideas about the problem?. @Dmitry1987 I add resources request it works. manifests like the following\nbefore:\n      containers:\n        - name: netacuity\n          image: myhub.com/netacuity:latest\n          imagePullPolicy: Always\nnow:\n      containers:\n        - name: netacuity\n          image: myhub.com/netacuity:latest\n          imagePullPolicy: Always\n          resources:\n            requests:\n              cpu: 100m\n              memory: 50Mi\n``\nseems that it must specify the request resources of a container, thenhpaandheapster`  work. sorry for delay. ",
    "beinnova": "Hi, I have the same problem on GKE cluster. After updating to 1.6 version I can't see metrics.. ",
    "zjzhangkui": "@r0bj Does adding --housekeeping-interval=5s flag in kubelet work? I tried set this flag and metric-resolution set 15s, but still got no metrics returned from heapster error.  If changed metric-resolution to 20s, the issue is gone. Seams the flag housekeeping-interval set in kubelet not work. I restart the kubelet and had seen the flag with systemctl status kubelet.  . ",
    "stresler": "Is this what you mean:\n```\n./influx -host 192.168.56.206 -port 8086\nVisit https://enterprise.influxdata.com to register for updates, InfluxDB server management, and monitoring.\nConnected to http://192.168.56.206:8086 version unknown\nInfluxDB shell version: 1.1.1\n\nshow databases; \nname: databases\nname\n\n\n_internal\nk8s\n\nSHOW RETENTION POLICIES\nERR: database name required\nWarning: It is possible this error is due to not setting a database.\nPlease set a database with the command \"use \".\nuse k8s\nUsing database k8s\nSHOW RETENTION POLICIES\nname    duration    shardGroupDuration  replicaN    default\n----    --------    ------------------  --------    -------\nautogen 0s      168h0m0s        1       true\n```. I have done this (default needs to be quoted for anyone else finding this), and grafana has started displaying data. \n\nStill getting that 204 in logs, but I think I can maybe safely ignore that for now. . Thanks!. Don't spam me, I signed it. . Ok - did that. Lotta paperwork for two \"L\"s. Why close the duplicate issue that has a PR on it to fix this? Would it make more sense to close this one and re-open the one with the change on it? . ",
    "TinySong": "i get the same issue,  when execute \"CREATE RETENTION POLICY default ON k8s DURATION 0d REPLICATION 1 DEFAULT\", got another error: ERR: error parsing query: found DEFAULT, expected identifier at line 1, char 25, anyone help me . ",
    "drinktee": "similar problem. The network is ok. I started a busybox container and can curl the url and ping other containers, but dashboard print this.\n[2017-05-09T03:59:53Z] Outcoming response to 172.17.1.1:52422 with 200 status code\nSkipping Heapster metrics because of error: an error on the server (\"Error: 'dial tcp 172.17.1.7:8082: getsockopt: connection timed out'\\nTrying to reach: 'http://172.17.1.7:8082/api/v1/model/namespaces/default/pod-list/curl-3393124085-dcldc,curl2-2854614873-wgzkj/metrics/cpu/usage_rate'\") has prevented the request from succeeding (get services heapster)\n[2017-05-09T04:00:35Z] Outcoming response to 172.17.1.1:50410 with 200 status code\nAnd the heapster seems ok.\nI0509 03:53:57.371188       1 heapster.go:72] /heapster --source=kubernetes:https://kubernetes.default --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086\nI0509 03:53:57.371233       1 heapster.go:73] Heapster version v1.3.0\nI0509 03:53:57.371509       1 configs.go:61] Using Kubernetes client with master \"https://kubernetes.default\" and version v1\nI0509 03:53:57.371528       1 configs.go:62] Using kubelet port 10255\nI0509 03:53:57.417024       1 influxdb.go:252] created influxdb sink with options: host:monitoring-influxdb.kube-system.svc:8086 user:root db:k8s\nI0509 03:53:57.417058       1 heapster.go:196] Starting with InfluxDB Sink\nI0509 03:53:57.417067       1 heapster.go:196] Starting with Metric Sink\nI0509 03:53:57.425744       1 heapster.go:106] Starting heapster on port 8082\nI0509 03:54:05.025078       1 influxdb.go:215] Created database \"k8s\" on influxDB server at \"monitoring-influxdb.kube-system.svc:8086\". ",
    "jorgelon": "In my cases I found the solution some weeks ago, sorry. I simply needed flannel working in my master node to give connectivity to heapster pod. ",
    "taylorshaulis": "I would like to confirm the solution provided by @jorgelon \nSwitching from networking.kope.io to networking.flannel resolved my heapster getsockopt: connection timed out issue.. Same here.  4.2.0 generates /etc/grafana/grafana.ini error but 4.0.2 starts without issue. ",
    "nicolas-g": "also confirming @jorgelon  , it was fixed by simple running : \nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml. ",
    "vielmetti": "Can we reopen this? GCR now has multiarch support.. This should be revisited now that GCR supports fat manifests.. ",
    "karunakar1122v": "https://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb \nI installed using above comfigurations. Thanks for your reply...Will get back to you on this...Once I will try. Hi Team,\nNow I am able to see Graphs in Garfana.\nOnly issue is when I was working on autoscaling when I execute kubectl get hpa command always it showing below error.In below current always showing waiting.\nname                        CURRENT     MINPODS   MAXPODS   AGE\nstudent      50%           1         10        46m\n. Please anyone respond ASAP. ",
    "Demonsthere": "Hi there,\nI have the same problem, but from I can see here:\nhttps://console.cloud.google.com/kubernetes/images/tags/heapster-grafana-amd64?location=GLOBAL&project=google-containers\nThe name of the container changed to heapster-grafana-amd64, but in the yml file there is still gcr.io/google_containers/heapster-grafana:v4.0.2. Ok, sorry, i found out that the error was due to disabling read-only port of kubelet by security. \nClosing . ",
    "jmn": "I think I provided the wrong version number of the InfluxDB in the issue report here. I think this report is actually regarding gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1 But at this point I am not actually sure. I deployed image: kubernetes/heapster_influxdb:v0.6 and it works on my cluster for now.\n@whereisaaron Can you confirm a successful deployment of the examples as they are currently provided?. ",
    "nfons": "i am getting same issue (ish)\n: getsockopt: connection timed out\nW0129 15:59:25.064674       1 manager.go:119] Failed to push data to sink: InfluxDB Sink\nW0129 16:00:25.039653       1 manager.go:119] Failed to push data to sink: InfluxDB Sink\nE0129 16:00:26.729637       1 influxdb.go:150] Failed to create infuxdb: failed to ping InfluxDB server at \"monitoring-influxdb:8086\" - Get http://monitoring-influxdb:8086/ping: dial tcp 10.103.30.121:8086: getsockopt: connection timed out\nfrom my heapster... later on down the line:\n0129 16:29:30.957206       1 listers.go:68] can not retrieve list of objects using index : object has no meta: object does not implement the Object interfaces\nW0129 16:30:00.942962       1 listers.go:68] can not retrieve list of objects using index : object has no meta: object does not implement the Object interfaces\nW0129 16:30:30.949341       1 listers.go:68] can not retrieve list of objects using index : object has no meta: object does not implement the Object interfaces\nW0129 16:30:30.970448       1 listers.go:68] can not retrieve list of objects using index : object has no meta: object does not implement the Object interfaces\nW0129 16:30:31.017071       1 listers.go:68] can not retrieve list of objects using index : object has no meta: object does not implement the Object interfaces. @KarolKraskiewicz  after update to beta.1 no issues. @piosz  I was able to get it working via modifying deployments:\ngrafana-deployment.yaml:  image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2\nheapster-deploy: image: gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1\ninfluxdb-deploy:  gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\nthe charts seem to be loading now:\n\ni think we just need to PR to change the yaml files\n. @jlalouette  nothing. just those and it seemed to work.. ",
    "mortensteenrasmussen": "Related to #1496 . Related to PR #1496 . I signed it!. FWIW the images I changed in the YAML seems only to be released in a -$ARCH, meaning the image listed in the YAML doesn't even exist. \nThat coupled with the poor searchability of gcr.io images/tags makes for a rather unfortunate first impression (IMHO).. Related to #1496 (and others). ",
    "rarneson": "I signed it!. My commit was originally made with a different user.email value in my git config. I updated it and pushed the updates. It should now match my CLA signatures.. ",
    "diseku": "Any update on it? Using the image version 4.4.1, same issue:\nt=2017-08-07T13:08:30+0000 lvl=eror msg=\"Failed to upload alert panel image.\" logger=alerting.notifier error=\"fork/exec /usr/share/grafana/vendor/phantomjs/phantomjs: no such file or directory\". ",
    "gimlet2": "Guys, any plan to fix it somewhere soon?. ",
    "neith00": "I signed it!. I proposed to use monitoring-influxdb because it's reflecting whats used in the deployment.\nBut I agree http://INFLUXDB_HOST:INFLUXDB_PORT is better for understanding. . ",
    "jejer": "Seems we can't config cert and key for kubelet connection.\nI have tried to set --tls-cert-file / --tls-cert / --authorization-kubeconfig , all report \"Unauthorized\".\nHeapster version v1.5.0. Hi @SpComb ,\nI am afraid the auth in --source is for kube-apiserver , not for kubelet. \nMy kubelet has --client-ca-file but no --authorization-mode=Webhook.. ",
    "SpComb": "If the kubelet is configured with --authorization-mode=Webhook --client-ca-file=... (the case for kubeadm 1.9), then the kubelet API does not seem to accept a bearer token for authentication, but instead expects a TLS client cert signed by the kube CA: https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet-authentication\nA configuration like --source=kubernetes:?kubeletHttps=true&kubeletPort=10250 will use the in-cluster-config with the serviceaccount bearer token and the kube CA. This will first fail because the kubelet uses a self-signed cert for its bare hostname:\nE0320 10:59:05.041917       1 manager.go:101] Error in scraping containers from kubelet_summary:167.99.136.175:10250: Get https://...:10250/stats/summary/: x509: cannot validate certificate for ... because it doesn't contain any IP SANs\nWith ?insecure=true, this will fail because the kubelet expects a TLS client cert, and does not look at the serviceaccount bearer token:\nE0320 11:35:05.036030       1 manager.go:101] Error in scraping containers from kubelet_summary:...:10250: request failed - \"403 Forbidden\", response: \"Forbidden (user=system:anonymous, verb=get, resource=nodes, subresource=stats)\"\nIt seems to be possible to get this to work, but this requires provisioning heapster with a custom kubeconfig + client cert signed by the kube CA, containing a subject user bound to the heapster role.  The heapster configuration for that would then look something like --source=kubernetes.summary_api:?inClusterConfig=false&kubeletHttps=true&kubeletPort=10250&auth=.../heapster.conf. \nThe kubeconfig needs to have a users[*].user.client-certificate with a subject user/group bound to the heapster role for both kube and kubelet API access. The kubeconfig will presumably also need insecure-skip-tls-verify: true to talk to the kubelet API (unless using --feature-gates=RotateKubeletServerCertificate=true to provision the kubelet with a server cert signed by the kube CA?), which unfortunately also disables cert verification for the kube API.\nyaml\napiVersion: v1\nclusters:\n- cluster:\n    server: https://10.96.0.1\n    insecure-skip-tls-verify: true\n  name: default-cluster\ncontexts:\n- context:\n    cluster: default-cluster\n    namespace: default\n    user: default-auth\n  name: default-context\ncurrent-context: default-context\nkind: Config\npreferences: {}\nusers:\n- name: default-auth\n  user:\n    client-certificate: ../heapster-client.crt\n    client-key: .../heapster-client.key. ",
    "jlalouette": "hello,\ngrafana was not able to access influxdb database.\nbut i was using gcr.io/google_containers/heapster-amd64:v1.3.0-beta.0 instead of 1\n-------- Message original --------\nObjet : Re: [kubernetes/heapster] gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1 is not compatible with grafana-amd64:4.0.2 (#1503)\nDe : Karol notifications@github.com\n\u00c0 : kubernetes/heapster heapster@noreply.github.com\nCc : LALOUETTE Johan Johan.LALOUETTE@canal-plus.com,Mention mention@noreply.github.com\nHi @jlalouettehttps://github.com/jlalouette\nI understand you tried those images:\ngcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\ngcr.io/google_containers/heapster-grafana-amd64:v4.0.2\nWhat is the compatibility issue you encountered?\nI tried the same images together with:\ngcr.io/google_containers/heapster-amd64:v1.3.0-beta.1\nAnd monitoring was working as expected.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/kubernetes/heapster/issues/1503#issuecomment-280914451, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AYAotiIQs2ZVxwB3v6l84fdG5z3RJgrUks5reC56gaJpZM4L4UjL.\n. Hi Karol,\nI confirm that it works fine with gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1\nRegards,\nJohan\nDe : Karol [mailto:notifications@github.com]\nEnvoy\u00e9 : dimanche 19 f\u00e9vrier 2017 20:18\n\u00c0 : kubernetes/heapster heapster@noreply.github.com\nCc : LALOUETTE Johan Johan.LALOUETTE@canal-plus.com; Mention mention@noreply.github.com\nObjet : Re: [kubernetes/heapster] gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1 is not compatible with grafana-amd64:4.0.2 (#1503)\n@jlalouettehttps://github.com/jlalouette\nCan you verify if the error persist with gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1 and attach error log?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/kubernetes/heapster/issues/1503#issuecomment-280941253, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AYAotty1q_rTkeT4tQlVrwOBYjLAvKpeks5reJWDgaJpZM4L4UjL.\n. Hi,\nIt seems that grafana does not support the latest versions of InfluxDB (Earlier than 0.9).\nAfter a lot of test the only way I found to make heapster/grafana/Influxdb works with Kubernetes 1.4 is to use these versions : \n(Be careful with the undescore)\nINFLUXDB : gcr.io/google_containers/heapster_influxdb:v0.7\nGRAFANA : gcr.io/google_containers/heapster-grafana-amd64:v4.0.2\nHEAPSTER : gcr.io/google_containers/heapster:v1.3.0-beta.0\nNo specifics configurations required. I can upload the Kubernetes YAML associated to the deployments if you like.\n\n\n. I don't know if grafana 4.0.2 support InfluxDB 1.1 out of the box. The grafana datasource doc [http://docs.grafana.org/features/datasources/influxdb/] is refering to InfluxDB 0.8.x and 0.9.x but nothing else. \nWhat I do know is that heapster-grafana-amd64:v4.0.2 does not work well with heapster_influxdb:v1.1.1. @lilnate22  Cool, what did you change in addition to the heapster update (1.3.0.beta.0 --> 1.3.0.beta.1) ?. ",
    "frankgreco": "@piosz I will use the above tags.\nHowever, this is still an issue because the Kubernetes manifests are wrong. The manifests are not usable out of the box. . ",
    "bergquist": "@piosz Should work fine :) Just poke me if you have any questions regarding grafana. ",
    "tianshapjq": "@DirectXMan12 fix done :). squash done. @DirectXMan12. squash done. @DirectXMan12. @DirectXMan12 I just ran by the go-builded file for testing, and realized that may happen in a pod. I will check this in a running pod. BTW, where are the log files in a pod for Heapster? I couldn't find them in a pod :(. @DirectXMan12 yeah, it worked fine in a pod. I will close this pr, thanks.. @DirectXMan12 You mean that I should note that in the doc? If so, just noting that they are not supported in V1.2.0 only?. I found that only the /api/v1/model/namespaces/{namespace-name}/pods/{pod-name}/containers/ api is not supported, the others worked fine. Someone mistaked deleting the /containers api in the source code. I note this in the doc. \nDo we need to make a new release like V1.2.1 to fix this?. The situation is, I want to query more than 15 collections of data which can up to about 24 hours. Is it possible to obtain such a long time data with just configuring the Heapster start parameters? Maybe querying data from influxdb would work this out?. @jbehrends is there anything I can do to make Heapster querying data from influxdb but not memory cache? So that I can access the rest APIs of Heapster instead of influxdb.. @jbehrends thanks, that really helps a lot :). I found there are historical rest apis which can query data from the backend storages, like influxdb. The apis can just learn from the source code, seems no docs describing about this yet. I think this may be helpful for you @jbehrends. @DirectXMan12 @andyxning thx, I just thought if it's neccessary to note in the docs? Otherwise I will close this issue.. close since it's getting no response.. maybe you have to check the synchronization of the cluster time, can take a look at my issue #1543.\nWish it would help.. Did you take the right apiserver security port? IIRC, the security port will be 6443 usually.. @piosz could u please check this pr? thx~. ",
    "jorge07": "I signed it!. I've this issue with the alpha in the hap:\nunable to get metrics for resource cpu: no metrics returned from heapster\nChanged to gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1 fixed the issue. \nShould I close the issue or replace the images?. I finally updated with the \"stable\" image I'm using without issues.. ",
    "henriquetruta": "Reviewed 6 of 6 files at r2.\nReview status: all files reviewed at latest revision, all discussions resolved.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \n\nReview status: :shipit: all files reviewed at latest revision, all discussions resolved, all commit checks successful.\n\nComments from Reviewable\n Sent from Reviewable.io \n. ",
    "jbehrends": "UPDATE\nI just pulled down the latest build of this container and tested sine the referenced PR was merged:\ngcr.io/google-containers/heapster:v1.3.0\nThe cluster cpu_usage rate is no longer 2x the sum of the nodes cpu_usage rate, but it's now lower!  Not by a large margin but enough to not look correct.  See screenshot below:\n\nThe drop in cluster usage_rate is when I deployed the updated heapster version.. Someone can chime in if I'm wrong..  I think heapster was designed to aggregate metrics from all nodes in a kubernetes cluster and expose them via an API, and was not designed to store any data.  The only thing I'm aware of that leverages the API is the kubernetes dashboard. \nYour use case is where a \"sink\" comes into play.  If you have a sink configured such as influxdb, heapster will poll metrics from all nodes, and then send them off to influxdb, or whatever other backend you have configured.  Then you can query influxdb for metrics from the last X-hrs, days, etc.... No.  Heapster only pushes data to external destinations like influx, it can't query data back out.  In my environment I have heapster sending data to graphite, then I have dashboards setup to display the data.  The data is pretty much real-time and I have data as far back as I want.. ",
    "xrl": "Dropping the -amd64 worked for me.. ",
    "rutsky": "@luxas alpha.1? Why not beta.1? It easy to me to send PR, but I'm confused which version is correct?. ",
    "ChrisBuchholz": "I signed it!. ",
    "astropuffin": "Linux Foundation CLA is stuck in limbo for tech reasons. Can I get someone to review this in the meantime?. @DirectXMan12 this should be sink/elasticsearch, not influxdb.. hey. still working with my VP on the linux foundation signing. Jumping back in on this.. @andreykurilin re: comment\nempirically, when I set the config like so: --sink=elasticsearch:http://elasticsearch.example.com:9200?esUserName=$ES_USER&esUserSecret=$ES_PASS&sniff=false&maxRetries=1 then the opaque is not set. I was able to confirm this is the behavior of net/url's parse method.\nnot fully following what the question/action item is here. I hope that addressed it?. Hey, trying to get the CLA signed. Honestly, I have no idea what the problem is with the signing process. It seems to be stuck in limbo even though we've signed it.. ok there. finally figured it out. :/. Ah, I assumed that it would be squashed on merge. I'll add a few lines of documentation and rebase. :). ETA 30min. rebased and with docs. oooh yeah that could work. That way you know which are secrets, regardless of sink.. ",
    "chenjian158978": "Change smtp.skip_verify from false(defaults) to true can fix my problem.\nRead some details about adding alert notification:\n\n\nAlert Notifications\n\n\nSMTP settings\n\n\nHere is the new file of run.sh\n#!/bin/sh\n\n: \"${GF_PATHS_DATA:=/var/lib/grafana}\"\n: \"${GF_PATHS_LOGS:=/var/log/grafana}\"\n: \"${GF_SMTP_ENABLED:=true}\"\n: \"${GF_SMTP_HOST:=smtp.163.com:25}\"\n: \"${GF_SMTP_USER:=chenjian@163.com}\"\n: \"${GF_SMTP_PASSWORD:=dfdfdfdwer}\"\n: \"${GF_SMTP_SKIP_VERIFY:=true}\"\n: \"${GF_SMTP_FROM_ADDRESS:=chenjian@163.com}\"\n\n# Allow access to dashboards without having to log in\n# Export these variables so grafana picks them up\nexport GF_AUTH_ANONYMOUS_ENABLED=${GF_AUTH_ANONYMOUS_ENABLED:-true}\nexport GF_SERVER_HTTP_PORT=${GRAFANA_PORT}\nexport GF_SERVER_PROTOCOL=${GF_SERVER_PROTOCOL:-http}\n\necho \"Starting a utility program that will configure Grafana\"\nsetup_grafana >/dev/stdout 2>/dev/stderr &\n\necho \"Starting Grafana in foreground mode\"\nexec /usr/sbin/grafana-server \\\n  --homepath=/usr/share/grafana \\\n  --config=/etc/grafana/grafana.ini \\\n  cfg:default.paths.data=\"$GF_PATHS_DATA\"  \\\n  cfg:default.paths.logs=\"$GF_PATHS_LOGS\"   \\\n  cfg:default.smtp.enabled=\"$GF_SMTP_ENABLED\"    \\\n  cfg:default.smtp.host=\"$GF_SMTP_HOST\"     \\\n  cfg:default.smtp.user=\"$GF_SMTP_USER\"     \\\n  cfg:default.smtp.password=\"$GF_SMTP_PASSWORD\"    \\\n  cfg:default.smtp.skip_verify=\"$GF_SMTP_SKIP_VERIFY\" \\\n  cfg:default.smtp.from_address=\"$GF_SMTP_FROM_ADDRESS\"\n\n. @DirectXMan12 Can you close this issue please? thx a lot.. @orientzc \nCould you show me the full command of your docker run -p 3000:3000 -e \"GF_SMTP_ENABLED=true\" -e \"GF_SMTP_HOST=smtp.aaa.com:25\" -e \"GF_SMTP_FROM_ADDRESS=xxx\" -e \"GF_SMTP_USER=xxx\" -e \"GF_SMTP_PASSWORD=\"yyy\" -e \"GF_SMTP_SKIP_VERITY=true\"?\nThx a lot.. @orientzc \nI think you just add the ENV into your container. But this container do not start with those ENVs.\nCan you get my points?\nYou should use those ENV when your container start to RUN like below:\n exec /usr/sbin/grafana-server \\\n  --homepath=/usr/share/grafana \\\n  --config=/etc/grafana/grafana.ini \\\n  cfg:default.paths.data=\"$GF_PATHS_DATA\"  \\\n  cfg:default.paths.logs=\"$GF_PATHS_LOGS\"   \\\n  cfg:default.smtp.enabled=\"$GF_SMTP_ENABLED\"    \\\n  cfg:default.smtp.host=\"$GF_SMTP_HOST\"     \\\n  cfg:default.smtp.user=\"$GF_SMTP_USER\"     \\\n  cfg:default.smtp.password=\"$GF_SMTP_PASSWORD\"    \\\n  cfg:default.smtp.skip_verify=\"$GF_SMTP_SKIP_VERIFY\" \\\n  cfg:default.smtp.from_address=\"$GF_SMTP_FROM_ADDRESS\"\n\n. @orientzc \nI run it by myself, and read /etc/grafana/grafana.ini and run.sh.\nAnd now I get your point. \nIn fact, I DO NOT know how to run the env of GF_SECURITY_ADMIN_PASSWORD=secret without adding it into the script of run.sh and then the envs are be overriden like the Configuring your Grafana container part of https://hub.docker.com/r/grafana/grafana/.\n\nYou can read the grafana/grafana's Dockerfile, and maybe know how it works inside;\nYou can email the developers of grafana, they would help you more.\n\nThis is my suggestions.. ",
    "orientzc": "@chenjian158978 \nsorry. i use\ndocker run -p 3000:3000 -e \"GF_SMTP_ENABLED=true\" -e \"GF_SMTP_HOST=smtp.aaa.com:25\" -e \"GF_SMTP_FROM_ADDRESS=xxx\" -e \"GF_SMTP_USER=xxx\" -e \"GF_SMTP_PASSWORD=\"yyy\" -e \"GF_SMTP_SKIP_VERITY=true\". and i also met the same problem. i use your solution of edit the run.sh to solve it. Success. but i want to know that what's wrong with the way of ENV. what i missed. could anyone help me? Thanks.. @chenjian158978 \nThanks. like this?\nfirst, i edit /etc/hosts in machine. my smtp IP is 132.35.108.22. and i add\n132.35.108.22 hq.smtp.gra.cn\nsecond, i use --net=host instead of -p 3000:3000 to use the hosts in machine.\ndocker run --net=host -e \"GF_SMTP_ENABLED=true\" -e \"GF_SMTP_HOST=hq.smtp.gra.cn:25\" -e \"GF_SMTP_FROM_ADDRESS=orientzc@gra.cn\" -e \"GF_SMTP_USER=orientzc@gra.cn\" -e \"GF_SMTP_PASSWORD=\"\"\"#password#\"\"\"\" -e \"GF_SMTP_SKIP_VERIFY=true\" grafana/grafana:latest\nand i tried \ndocker run -p 3000:3000 -e \"GF_SMTP_ENABLED=true\" -e \"GF_SMTP_HOST=101.35.108.22:25\" -e \"GF_SMTP_FROM_ADDRESS=orientzc@gra.cn\" -e \"GF_SMTP_USER=orientzc@gra.cn\" -e \"GF_SMTP_PASSWORD=\"\"\"#password#\"\"\"\" -e \"GF_SMTP_SKIP_VERIFY=true\" grafana/grafana:latest\ndocker run -p 3000:3000 -e \"GF_SMTP_ENABLED=true\" -e \"GF_SMTP_HOST=101.35.108.22:25\" -e \"GF_SMTP_FROM_ADDRESS=orientzc@gra.cn\" -e \"GF_SMTP_USER=orientzc@gra.cn\" -e \"GF_SMTP_PASSWORD=#password# -e \"GF_SMTP_SKIP_VERIFY=true\" grafana/grafana:latest\ndocker run --net=hosts -e \"GF_SMTP_ENABLED=true\" -e \"GF_SMTP_HOST=hq.smtp.gra.cn:25\" -e \"GF_SMTP_FROM_ADDRESS=orientzc@gra.cn\" -e \"GF_SMTP_USER=orientzc@gra.cn\" -e \"GF_SMTP_PASSWORD=#password# -e \"GF_SMTP_SKIP_VERIFY=true\" grafana/grafana:latest. @chenjian158978 \nthanks very much. but i use \ndocker run -p 3000:3000 -e \"GF_SECURITY_ADMIN_PASSWORD=secret\" grafana/grafana:latest\neverything is ok. the run.sh is same. so i think Envs works.\nthe description of configuration container is https://hub.docker.com/r/grafana/grafana/. Thanks very much. @chenjian158978 . ",
    "XiLongZheng": "Is it planned to ship with 3/27 kubenretes 1.6 release?. Whether heapster-grafana-s390x will be released for 1.7?. ",
    "tvansteenburgh": "@piosz Any news on this? Really need an s390x image!. /remove-lifecycle stale. ",
    "samjeyam": "I signed it!. I signed it!. ",
    "nicolaiskogheim": "@piosz Sorry for the delay.\n\nDoes the following command work correctly\n\nYes.. @AlmogBaku Because then I cannot modify the files on the fly. Please read the first post in this thread.. @piosz Rebased now.\nI have only done this on files I use regularly, but those three dashes are a part of the YAML spec, and theres no reason not to include them. I could go through the other YAML files and add it if you want.. ",
    "dieterdemeyer": "@johanneswuerbach we could also use this sink if it gets merged.\nDo you happen to have a timeline on this ? If I can help in any way, please let me know.. @johanneswuerbach how exactly should I use that ? I'm just starting with Kubernetes, so bear with me... @johanneswuerbach Thanks, I'll give it a try... @johanneswuerbach I used the master branch of kubernetes/heapster and built a new image with the following command:\nOVERRIDE_IMAGE_NAME=myorg/heapster:1.3.0-SNAPSHOT make container push-override\nI'm trying to use the librato sink but I'm getting the following error message when the pod starts:\nFailed to create sink: Sink not recognized: \"librato\nI'm using the following configuration in my container command:\n--sink=\"librato:?username=<username>&token=<token>&prefix=k8s&tags=cluster\"\nCould you tell me what I'm doing wrong ?\nThanks in advance.. @johanneswuerbach Never mind, just found the cause.\nThe documentation found here describes quotes around the sink configuration but it actually shouldn't contain quotes.. @johanneswuerbach Sure, will do..\nDid you test the Librato sink with real live data ? I have some weird values in the metric k8s.cpu.usage...\nAre you available in other channels like the Kubernetes slack instance for a small talk about this ?. I signed it!. Not really sure why this PR is failing since it's just documentation...\nIs there an underlying problem here ?. Yes, I already signed it.. @k8s-bot /retest. Is it expected behaviour that when you change the retention policy in heapster, the retention policy won't be updated ?\nCurrently, it only sets the retention policy to the correct duration value when no k8s database exists.. My use case involves the Librato sink specifically.\nI was going through the code and one option would be to create a configurable option with an exclusion list of metrics which is taken into account in the following 2 files:\nhttps://github.com/kubernetes/heapster/blob/master/metrics/sinks/librato/librato.go\nhttps://github.com/kubernetes/heapster/blob/master/common/librato/librato.go\nWould that be the correct way ? Or can you show me which sink is currently using this ?. ",
    "johanneswuerbach": "@dieterdemeyer I haven't gotten any feedback from any k8s maintainer yet, so hard to say.\nWe are currently building a custom heapster from this branch using:\nOVERRIDE_IMAGE_NAME=org/custom-heapster:test-7 make container push-override. @dieterdemeyer you can use a config similar to the once you can find here https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/cluster-monitoring/influxdb/heapster-controller.yaml, but instead of the official image you can use your custom own and instead of influxdb, you can use the librato sink. Thanks @DirectXMan12 and no worries, I updated the sink owners file and left some comments. Let me know if anything else is required :-). Thanks @dieterdemeyer would you mind sending a PR removing the quotes? That would be great \ud83d\udc4d . Yes we are currently using it with 3 clusters, but I didn't check all the values yet. Not in the slack yet, but will join once again on a computer\n. @mashayev beta.1 doesn't contain the librato sink, but the recently released https://github.com/kubernetes/heapster/releases/tag/v1.3.0 does. Could you double check?. I'm only using tagged metrics and as far as I know source based metrics are also more or less deprecated. As far as I know you can just sent an email to librato support and your account will be whitelisted to use both.. Not sure, as far as I know the metric name is auto-generated from all received tags, but I guess you could ask librato support for clarification.\nThe sink is only forwarding each metric with all submitted tags to https://www.librato.com/docs/api/#create-a-measurement\n. Yes, that is correct.. \ud83d\udc4d thanks for doing this @dieterdemeyer lgtm. Thanks @DirectXMan12 \n@sgmiller I'm using a similar argument and it works for us.\n--sink=librato:?username=$LIBRATO_EMAIL&token=$LIBRATO_TOKEN&prefix=k8s&tags=cluster,environment&tag_cluster=xxx&tag_environment=production\nWhich version of heapster are you running?. This line never worked for me as https://docs.docker.com/engine/reference/commandline/tag/ has no -f flag. I can send a separate PR if you prefer that.. I was planning to, but https://www.librato.com/docs/api/#annotations isn't ready for tags yet. I'll contact librato to find out a timeframe.. ",
    "Resisty": "So I noticed when reviewing everything that the node creation times differed according to which nodes were showing up and which weren't; the two youngest nodes, which had been recreated several times since experimenting with heapster, were showing up in grafana but the older ones, which were created well before heapster was deployed, were not.\nTerminating the nodes and letting them spin back up suddenly brings them into heapster's purview for some reason:\nbrian.auron@LT-A8-120478 ~/github/heapster/deploy/kube-config/influxdb \ud83c\udf54  kubectl get nodes\nNAME                                         STATUS                     AGE\nip-10-127-20-19.us-west-2.compute.internal   Ready,SchedulingDisabled   21h\nip-10-127-20-27.us-west-2.compute.internal   Ready                      5m\nip-10-127-20-62.us-west-2.compute.internal   Ready                      23h\nip-10-127-20-84.us-west-2.compute.internal   Ready                      4m\nbrian.auron@LT-A8-120478 ~/github/heapster/deploy/kube-config/influxdb \ud83c\udf54  kubectl top nodes\nNAME                                         CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%\nip-10-127-20-19.us-west-2.compute.internal   114m         5%        1727Mi          43%\nip-10-127-20-27.us-west-2.compute.internal   69m          3%        836Mi           21%\nip-10-127-20-62.us-west-2.compute.internal   374m         18%       2101Mi          53%\nip-10-127-20-84.us-west-2.compute.internal   68m          3%        842Mi           21%\n\nI might be wrong but I don't think the documentation mentions needing to restart nodes after deploying heapster. Perhaps I'm still missing something?. ",
    "vjdhama": "I signed it!. @andyxning I already signed the linux foundation cla as well as google cla. Though the issue might have been that I signed google cla, then commented I signed it! and later signed linux foundation cla.. ",
    "yukiman76": "sjezewski That worked for me as well . Same problem here. \nlooks like the  grafana.ini  is not in the image . basic error \n/ # /usr/bin/setup_grafana\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp 127.0.0.1:3000: getsockopt: connection refused. Retrying after 5 seconds.... ",
    "hakamairi": "@sjezewski \nYou could have just changed in your graphana.yaml the following\nGF_SERVER_ROOT_URL: \n/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/\nThis will allow you to access via master_ip:api_port/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/\n. ",
    "caitong93": "\nmark tag where value is empty with defaultTagValue\n\nWould defaultTagValue be something like ____empty_tag_value ?. ",
    "BrianGallew": "I have this same problem with my cluster.   Launching the deployment will produce a heapster pod with logs indicating an inability to connect to kubernetes.default OR that it was able to connect but that it was requested to authenticate.  Killing the pod repeatedly will eventually get me a functioning heapster ... for a day or two.  Then it goes back to no connect/no auth.\nFWIW, the dashboard has the same issue.. I do have 3 endpoints.  Each has its own cert with both it's own IP as well as the service IP in the SAN list.  All three are signed by the same CA (verified).. ",
    "mayask": "Same issue. @DirectXMan12 \nNo, there's only one endpoint.\n$ kubectl describe svc kubernetes\nName:                   kubernetes\nNamespace:              default\nLabels:                 component=apiserver\n                        provider=kubernetes\nSelector:               <none>\nType:                   ClusterIP\nIP:                     10.0.0.1\nPort:                   https   443/TCP\nEndpoints:              10.0.2.15:6443\nSession Affinity:       ClientIP\nNo events.. ",
    "xuwang": "Same problem here:\nKubernetes v1.6.1\nContainer image \"gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1\" \nPlatform: AWS\nOn an nginx pod, I can reach api server by dong a curl call inside of nginx container, no timeout:\n```\ncurl -I -k https://kubernetes.default/api/v1/nodes?resourceVersion=0\nUser \"system:anonymous\" cannot list nodes at the cluster scope.\nHTTP/1.1 403 Forbidden\nContent-Type: text/plain\nX-Content-Type-Options: nosniff\nDate: Mon, 10 Apr 2017 19:44:02 GMT\nContent-Length: 63\n```\nInside of heapster container:\n$ # nc -w5 kubernetes.default 443\nnc: timed out\nIt does looks like an issue with heapster container network. We aren't get into authentication step yet.\nMore logs are here.. It turned out I have a staled endpoint.  It caused timeout. See issue https://github.com/kubernetes/kubernetes/issues/22609.\nI modified the --apiserver-count flag  to match the real number of apiservers and restarted  kube-apiserver. The  staled endpoint was cleaned up and timeout issue was resolved.. ",
    "johnko": "Thanks to @xuwang tip, one of our current workarounds is to run a cron job on all masters to check if a master is NotReady and remove it's apiserver endpoint from the svc/kubernetes.. ",
    "bruceauyeung": "@vishh @mwielgus \nhi, do you have a moment to check if this is a reasonable PR? thanks!. @DirectXMan12 would you please give a link about the bug of cAdvisor ? i'd like to know why. thanks!. @DirectXMan12 fixed as you sugguested. @DirectXMan12 fixed test case error. @DirectXMan12 i already have fixed as you requested, would you mind take an another look ?. @DirectXMan12 already done, and the test case error gone.. ",
    "vivkong": "I signed it!. ",
    "chengzhang1016": "@DirectXMan12 \nHi, Solly,\nToday, I tried starting heapster with --api-server or --api-server=1, both didn't open secure-port in heapster container.\n```\n[root@ip-172-18-5-84 heapster]# ps -ef|grep heapster \nroot     14690 14670  2 02:35 ?        00:00:00 /heapster --api-server --secure-port=8443 --bind-address=0.0.0.0 --source=kubernetes:https://kubernetes.default.svc --sink=influxdb:http://monitoring-influxdb:8086 --client-ca-file=/root/go/src/k8s.io/easy-rsa-master/ca.crt --tls-cert-file=/root/go/src/k8s.io/easy-rsa-master/server.crt --tls-private-key-file=/root/go/src/k8s.io/easy-rsa-master/server.key\n[root@ip-172-18-5-84 heapster]# kc get pod --namespace kube-system\nNAME                                   READY     STATUS    RESTARTS   AGE\nheapster-599163464-nvrfm               1/1       Running   0          12s\nkube-dns-806549836-2v0x0               2/3       Running   0          28s\nmonitoring-grafana-3975459543-2r36d    1/1       Running   0          12s\nmonitoring-influxdb-3480804314-831vq   1/1       Running   0          12s\n[root@ip-172-18-5-84 heapster]# kc exec -it heapster-599163464-nvrfm sh --namespace kube-system\n/ # netstat -anp|grep 8443\n/ # netstat -anp|grep 8082\ntcp        0      0 :::8082                 :::*                    LISTEN      1/heapster\n/ # netstat -anp|grep 6443\n[root@ip-172-18-5-84 heapster]# kc logs heapster-599163464-nvrfm --namespace kube-system | grep 8443\nI0310 07:47:19.833863       1 heapster.go:72] /heapster --api-server --secure-port=8443 --bind-address=0.0.0.0 --source=kubernetes:https://kubernetes.default.svc --sink=influxdb:http://monitoring-influxdb:8086 --client-ca-file=/root/go/src/k8s.io/easy-rsa-master/ca.crt --tls-cert-file=/root/go/src/k8s.io/easy-rsa-master/server.crt --tls-private-key-file=/root/go/src/k8s.io/easy-rsa-master/server.key\n```. This issue had been fixed. Thanks.. ",
    "cainelli": "I signed it!. So will the main endpoints extract metrics older than 15 min. or if we want to do that we'll have to query the backend sink?. ",
    "yannrouillard": "After having a look at the source code, I think the error lies here:\nif resp, err := sink.client.Query(q); err != nil {\n        if !(resp != nil && resp.Err != nil && strings.Contains(resp.Err.Error(), \"already exists\")) {\n            err := sink.createRetentionPolicy()\n            if err != nil {\n                return err\n            }\n        }\n    }\nIf I understand well, the createRetentionPolicy function will only be launched if there is an error in the Database creation query, so it will not happen at least the first time, and probably not the subsequent time as CREATE DATABASE doesn't return an error when database already exists from what I saw.\nI fixed that but it still doesn't work after as the 'default' retention policy already exists and so the creation fails with error ERR: retention policy already exists.\nI will propose a full fix, but can you confirm that there is not something wrong in my setup ?\n. Other issue: the retention configuration patch has only been applied on the metrics sink but not on the events sink.\nAs by default they are on the same database, it means that if the eventer create the database first the retention duration will be wrong.\n. Proposed pull request: #1581 . I signed it!. sure but influxdb doesn't return an error if the db already exists so we are already fine.\nWill find the doc reference to that fact (and I tested it, otherwise it would fail the second time it is launched). From https://docs.influxdata.com/influxdb/v1.2/query_language/database_management/#create-database \n\"A successful CREATE DATABASE query returns an empty result. If you attempt to create a database that already exists, InfluxDB does nothing and does not return an error.\"\nHowever this message is not present in doc of influxdb v0.9 and I've tested it with v1.2.\nI can do additional tests with older version of influxdb.\nWhat version of influxdb do we support?\n. ",
    "LLParse": "Any status on this? Would really like to see this feature soon. Thanks!. ",
    "arausuy": "Are there any updates on the status of this?. ",
    "woodlee": "This seems to still be an issue, but the robot's policy prevents me from re-opening it.. ",
    "adrianchifor": "Any update on this?. ",
    "caarlos0": "Hi, any news on this one?. @piosz thanks, can you guys release a new version of that image?\n. ping @piosz . @andyxning yah, pushing it go gcr.io would be great! :D. ",
    "manolama": "@kelify Any way you can post the JSON being sent to TSDB and returned from it? Maybe TCPDump it?. ",
    "hayatAmine": "hi, \n@jakon89 i have the same error , have you fin a solution ?\nThanks  in Advance. ",
    "buddyledungarees": "I ran into this during setup and the most likely problem is that your tsdb instance doesn't have chunking enabled and/or max_chunk_size isn't set large enough.\nAnother issue you might run into is opentsdb's default tag limit is 8, which is way too low for the prometheus style usage of tags/labels. I had to increase my limit to 12 to start accepting the default metrics from heapster.. @owain-je This was resolved for me by realizing that my kops cluster was missing the system:heapster role. I'm not sure why that role is missing but this was a cluster that was upgraded from 1.4 -> 1.5 -> 1.6 -> 1.7 so could've been missed as part of an upgrade.\nYou can create the role by applying the object here:\nhttps://github.com/kubernetes/kubernetes/pull/48357. ",
    "byxorna": "I can confirm this is a pain to debug. After tcpdumping and finding opentsdb reports a Chunked Encoding error even with requests are not chunked, I had to add the opentsdb settings:\ntsd.http.request.enable_chunked = true\ntsd.http.request.max_chunk = 111000\nthen debug with a captured request to replay against /api/put?details to figure out what the problem is. Would be dope to have better indications in heapster about errors experienced writing metrics.\nIt would be great to be able to filter tags from the outgoing metrics to stay below the tag limit, i.e. ?dropTags=nodename,namespace_id,host_id,pod_id. Basically, the opentsdb sink isnt functional for me :(. @DirectXMan12 is that documented in the docs? last i looked, i didnt see it there.. ",
    "willtrking": "I'm using influxdb, and was just testing it out with the default config from the influxdb deployment yaml files:\nhttps://github.com/kubernetes/heapster/blob/master/deploy/kube-config/influxdb/heapster-deployment.yaml\nIn case it changes:\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      containers:\n      - name: heapster\n        image: gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb:8086\nKubernetes version info:\njson\n{\n  \"major\": \"1\",\n  \"minor\": \"5\",\n  \"gitVersion\": \"v1.5.2\",\n  \"gitCommit\": \"08e099554f3c31f6e6f07b448ab3ed78d0520507\",\n  \"gitTreeState\": \"clean\",\n  \"buildDate\": \"2017-01-12T04:52:34Z\",\n  \"goVersion\": \"go1.7.4\",\n  \"compiler\": \"gc\",\n  \"platform\": \"linux/amd64\"\n}. @takama Thank you!! Fixed it for me as well. ",
    "kaijparo": "I got this too -- important to note that this is only happening in a cluster deployed with canal networking and in a private topology via kops in aws. \nSame setup in my public cluster does not have this problem . I'm having trouble getting an interactive terminal for the heapster pod, but I can get a busybox going and wget the url for the missing node without issue:\n\n/ # wget http://10.202.8.62:10255/stats/container/\nConnecting to 10.202.8.62:10255 (10.202.8.62:10255)\nindex.html           100% |***********************| 57822   0:00:00 ETA\n/ # ls\nbin         dev         etc         home        index.html  proc        root        sys         tmp         usr         var\n/ # cat index.html\n{\n  \"/\": {\n   \"name\": \"/\",\n   \"subcontainers\": [\n    {\n     \"name\": \"/docker\"\n. \n",
    "devent": "Hello, I have the same problem with my cluster. Everything else works fine.\nIn my tests I can connect to the master node running nc from heapster pod, but I can't connect to the worker node running the same nc from the heapster pod.\nFrom heapster pod\n/ # wget http://andrea-master-0.muellerpublic.de:39556\nConnecting to andrea-master-0.muellerpublic.de:39556 (185.24.220.41:39556)\nwget: error getting response: Connection reset by peer\nMaster node\nroot@andrea-master-0:~# nc -l 66 < kubectl-system.sh\ninvalid connection to [185.24.220.41] from (UNKNOWN) [37.252.124.149] 41476\nWorks\nFrom heapster pod\n/ # wget http://andrea-node-0.muellerpublic.de:52339\nConnecting to andrea-node-0.muellerpublic.de:52339 (37.252.124.149:52339)\nwget: can't connect to remote host (37.252.124.149): Connection timed out\nThe exact error is\n2017-04-21T09:21:12.184653499Z W0421 09:21:12.184455       1 manager.go:102] Failed to get kubelet_summary:37.252.124.149:10255 response in time\n2017-04-21T09:21:25.000604903Z W0421 09:21:25.000396       1 manager.go:147] Failed to get all responses in time (got 1/2)\n2017-04-21T09:22:12.216373835Z E0421 09:22:12.216138       1 summary.go:97] error while getting metrics summary from Kubelet andrea-node-0.muellerpublic.de(37.252.124.149:10255): Get http://37.252.124.149:10255/stats/summary/: dial tcp 37.252.124.149:10255: getsockopt: connection timed out\nVersions.\nheapster 1.3.0\nroot@andrea-master-0:~# ksys version\nClient Version: version.Info{Major:\"1\", Minor:\"5\", GitVersion:\"v1.5.4\", GitCommit:\"7243c69eb523aa4377bce883e7c0dd76b84709a1\", GitTreeState:\"clean\", BuildDate:\"2017-03-07T23:53:09Z\", GoVersion:\"go1.7.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"5\", GitVersion:\"v1.5.4+coreos.0\", GitCommit:\"97c11b097b1a2b194f1eddca8ce5468fcc83331c\", GitTreeState:\"clean\", BuildDate:\"2017-03-08T23:54:21Z\", GoVersion:\"go1.7.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}. @takama Thank you, my issue was resolved by applying FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT. I think you can close the issue now, because it looks like it's not an issue with Heapster.. ",
    "pastushenko": "Hello! I have the same problem.\nFailed to get kubelet:10.0.5.186:10255 response in time\nW0426 08:41:25.000504       1 manager.go:147] Failed to get all responses in time (got 4/5)\nKubernetes version: v1.6.1+coreos.0\nHeapster version: v1.3.0-beta.1\nI have 5 nodes in cluster.\nI use private network.\nAs network provider we use flannel.\nI found out that the heapster can't collect monitoring data from the node where his pod is placed.\nIn my case - the heapster pod is placed on node 4.\nFrom node 4 i can telnet self kubelet (port: 10255), but i can't telnet same kubelet from any container on node 4.\nFrom other nodes and other containers placed on other nodes - the problem don't appear (i can telnet kubelet on node 4)\nUPDATE:\nThere is no problem with heapster.\nIt is a network problem problem is with network.\nI followed tried this cases:\n- telnet self kubelet from node - OK!\n- telnet self kubelet from kubernetes system containers (apiserver, proxy) - OK!\n- telnet self kubelet from other container (created via kubernetes or manualy) - FAIL!\n. @takama - thanks. The problem was fixed by changing firewall rules.. ",
    "takama": "@willtrking  By default Calico blocks traffic from workload endpoints to the host itself with an iptables DROP action. If you want to allow traffic from endpoint to host, set this parameter to ACCEPT:\n- name: calico-node\n          image: quay.io/calico/node:v1.1.1\n          env:\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\". @pastushenko You can allow traffic from containers to the host itself using firewall rules especially to the port 10255. (Interfaces: flannel.1, docker0). @paalkr Please see my post at #1586 . ",
    "iamrandys": "Does anyone have any idea on how to configure FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT if Calico was used to create the Kubernetes cluster using Kops?\nkops create cluster --networking calico\n. ",
    "jurgenweber": "for those that kops create cluster --networking canal\nkubectl edit ds canal -n kube-system\nadd the environment variable as per the above with the other environment variables for the container named:\nspec:\n      containers:\n      - env:\n        - name: DATASTORE_TYPE\n          value: kubernetes\n ...\n        - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n          value: ACCEPT\n        image: calico/node:v1.0.2\n        imagePullPolicy: IfNotPresent\n        name: calico-node\nTo make it live you have two optinos, either delete the pods so they recreate or add this to your updatestratergy, replace 'DeleteOnly' with:\nupdateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\nand it will go and updated it all after saving the edit.\n. ",
    "chenjianjun19920702": "@pastushenko  k8s cluster already close firewalld, how to change firewall rules, Looking forward to your reply, thanks!!!. ",
    "zwlsuperman": "@pastushenko @willtrking @DirectXMan12 \n k8s cluster already close firewalld, how to change firewall rules\uff0c\nLooking forward to your reply, thanks!!!\nand I use flanal network,\nheapster pods error:\n_E0626 15:04:12.278239       1 manager.go:101] Error in scraping containers from kubelet_summary:39.122.99.100:10255: Get http://39.122.99.100:10255/stats/summary/: dial tcp 39.107.155.74:10255: getsockopt: connection timed out\nW0626 15:04:25.001017       1 manager.go:152] Failed to get all responses in time (got 4/5)\n[root@ao-docker01 ~]# kubectl describe hpa\nName:                                                  spider001\nNamespace:                                             default\nLabels:                                                \nAnnotations:                                           \nCreationTimestamp:                                     Tue, 26 Jun 2018 15:28:21 +0800\nReference:                                             Deployment/spider001\nMetrics:                                               ( current / target )\n  resource cpu on pods  (as a percentage of request):   / 50%\nMin replicas:                                          1\nMax replicas:                                          6\nConditions:\n  Type           Status  Reason                   Message\n  ----           ------  ------                   -------\n  AbleToScale    True    SucceededGetScale        the HPA controller was able to get the target's current scale\n  ScalingActive  False   FailedGetResourceMetric  the HPA was unable to compute the replica count: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)\nEvents:\n  Type     Reason                        Age                 From                       Message\n  ----     ------                        ----                ----                       -------\n  Warning  FailedComputeMetricsReplicas  15m (x481 over 4h)  horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)\n  Warning  FailedGetResourceMetric       3s (x511 over 4h)   horizontal-pod-autoscaler  unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)\n. ",
    "niyanchun": "Got it, thanks.. ",
    "derbeneviv": "@DirectXMan12 should i re-create an issue or you can re-open it?\nwhat i tried:\n1) built and pushed current master branch - i got the same problem as here https://github.com/kubernetes/heapster/issues/1583 \n2) pushed v1.2.0-2 from here: https://hub.docker.com/r/openshift/origin-metrics-heapster-base/tags/ ( that should be the correct version, i imagine)\nresult (for 1.2.0, it doesn't differ from 1.3.0-beta really):\n[root@development33-node-6 ~]# free -m\n total        used        free      shared  buff/cache   available\nMem:          32012        5558       12293        1767       14159       24024\nso memory/used whould be smth like 19 gig, right?\nbut it shows 18:\n```\nselect value/1024/1024 from \"memory/usage\" where  nodename='development33-node-6'  and type = 'node' and time > now() - 70s \n\n17944.68359375\n```\nsum(value) of type='pod' for a single host seems to be right, it equals stats which i got from docker(sum of total memory usage of all containers), but it differs from type = 'node':\n11435.2109375Mib(all pods) vs 17944.68359375Mib(node)\n\nsame for cluster again:\n```\nselect sum(value)/1024/1024 from \"memory/usage\" where   type ='node' and time > now() - 70s\n\n302423.734375\nselect value/1024/1024 from \"memory/usage\" where   type ='cluster' and time > now() - 70s\n589107.94140625\n```\nHow does heapster collect its metrics? Especially for node and cluster level, pods seems to be ok.\nIs there a way to run all its api calls manually? Maybe we have smth wrong with our kubernetes installation?. @DirectXMan12 you also mentioned some issues double-counting things at the node and cluster levels. \nCan you point it please, i couldn't find it.\nMaybe i can build an image from exact commit. \n",
    "darkprisco": "Sorry but I didn't get, I mean I am facing similar problem on my kubernetes cluster.\nkubectl top showing something different than kubectl describe and more over, if I kubectl top pods and I sum all memory used by pods running on the same agent I don't match value showed by kubectl top nodes.\nMy version of kubernetes is v1.6.6 and heapster is 1.3.0. ",
    "chengwang86": "@DirectXMan12 Hi I still observe that the sum of all worker nodes' memory usage > cluster mem usage. See https://github.com/kubernetes/heapster/issues/1897. I tried with heapster 1.3.0 and 1.4.2, the kubernetes version is 1.8.2. ",
    "michaelajr": "Good question. But it does. Did not work without it. Something to do with the Dashboard maybe? Noticed the errors in the Heapster logs when opening the Dashboard. . ",
    "c-knowles": "I was wondering if this is the same issue as https://github.com/kubernetes/heapster/issues/1589? I've been trying to tackle this for kube-aws in https://github.com/kubernetes-incubator/kube-aws/issues/685, I agree with the comments from @DirectXMan12 about not giving heapster the ability to modify its own configuration. Are there any working examples of heapster with nanny in the wild?. https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/cluster-monitoring/heapster-rbac.yaml seems to have the config needed sort out the pod nanny issue. I'm not always sure of the definitive setup as it seems to be distributed across repos and there's a lot of duplication, if anyone could help clarify for me it would be helpful to pulling in upstream changes into kube-aws.. ",
    "Ascendance": "YES, we don't, and we should. This fix should be implemented by default until metrics-server migration is complete. took me too long to figure out why heapster isn't working. . ",
    "dasher": "How about using an env var to pass a list of plugins to be installed at run time (as per the offical grafana way https://github.com/grafana/grafana-docker#installing-plugins-for-grafana-3 )?. ",
    "djschny": "Considering the official docker image for grafana supports GF_INSTALL_PLUGINS I was hoping the following would work:\n- name: GF_INSTALL_PLUGINS\n          value: raintank-kubernetes-app. ",
    "diegocn": "I can confirm it @djschny. Tried to use env, as suggested by official docker image for grafana, and doesn't work.\n```\n\nkubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.0\", GitCommit:\"fff5156092b56e6bd60fff75aad4dc9de6b6e\nf37\", GitTreeState:\"clean\", BuildDate:\"2017-03-28T16:36:33Z\", GoVersion:\"go1.7.5\", Compiler:\"gc\", Platform:\"linux/amd64\"\n}\nServer Version: version.Info{Major:\"1\", Minor:\"6+\", GitVersion:\"v1.6.6-rancher1\", GitCommit:\"973d2fa162b887a6f696f4574b8\n5e949d480a071\", GitTreeState:\"clean\", BuildDate:\"2017-06-17T03:54:44Z\", GoVersion:\"go1.7.6\", Compiler:\"gc\", Platform:\"li\nnux/amd64\"}\n```. \n",
    "kartoch": "I signed it!. May be completed by #1595. Forgot serviceAccount and serviceAccountName in deployment file. Has been fixed by @luxas in 1308dd71f0ba343895456b46d1bbf3238800b6f3, closing it as useless now.. ",
    "bowei": "Closing in favor of kubernetes/kubernetes#28407\n/close. ",
    "jackielii": "Running with -v=4\n$ kubectl logs heapster-614869780-76zmd -n kube-system -f\nI0419 08:31:54.708423       1 heapster.go:71] /heapster --source=kubernetes:https://kubernetes.default --sink=influxdb:http://monitoring-influxdb:8086 -v=4\nI0419 08:31:54.708556       1 heapster.go:72] Heapster version v1.3.0-beta.1\nI0419 08:31:54.708857       1 configs.go:61] Using Kubernetes client with master \"https://kubernetes.default\" and version v1\nI0419 08:31:54.708874       1 configs.go:62] Using kubelet port 10255\nI0419 08:31:54.766858       1 reflector.go:200] Starting reflector *api.Node (1h0m0s) from k8s.io/heapster/metrics/sources/kubelet/kubelet.go:342\nI0419 08:31:54.766965       1 reflector.go:249] Listing and watching *api.Node from k8s.io/heapster/metrics/sources/kubelet/kubelet.go:342\nI0419 08:31:54.768966       1 influxdb.go:252] created influxdb sink with options: host:monitoring-influxdb:8086 user:root db:k8s\nI0419 08:31:54.768985       1 heapster.go:193] Starting with InfluxDB Sink\nI0419 08:31:54.769002       1 heapster.go:193] Starting with Metric Sink\nI0419 08:31:54.769817       1 reflector.go:200] Starting reflector *api.Pod (1h0m0s) from k8s.io/heapster/metrics/heapster.go:319\nI0419 08:31:54.769842       1 reflector.go:200] Starting reflector *api.Node (1h0m0s) from k8s.io/heapster/metrics/heapster.go:327\nI0419 08:31:54.770575       1 reflector.go:200] Starting reflector *api.Namespace (1h0m0s) from k8s.io/heapster/metrics/processors/namespace_based_enricher.go:84\nI0419 08:31:54.771286       1 reflector.go:200] Starting reflector *api.Node (1h0m0s) from k8s.io/heapster/metrics/processors/node_autoscaling_enricher.go:100\nI0419 08:31:54.771912       1 reflector.go:249] Listing and watching *api.Node from k8s.io/heapster/metrics/processors/node_autoscaling_enricher.go:100\nI0419 08:31:54.771833       1 reflector.go:249] Listing and watching *api.Pod from k8s.io/heapster/metrics/heapster.go:319\nI0419 08:31:54.771844       1 reflector.go:249] Listing and watching *api.Node from k8s.io/heapster/metrics/heapster.go:327\nI0419 08:31:54.771797       1 reflector.go:249] Listing and watching *api.Namespace from k8s.io/heapster/metrics/processors/namespace_based_enricher.go:84\nI0419 08:31:54.782058       1 heapster.go:105] Starting heapster on port 8082\nI0419 08:32:05.000250       1 manager.go:79] Scraping metrics start: 2017-04-19 08:31:00 +0000 UTC, end: 2017-04-19 08:32:00 +0000 UTC\nI0419 08:32:05.002454       1 manager.go:98] Querying source: kubelet:10.10.10.202:10255\nI0419 08:32:05.015454       1 manager.go:98] Querying source: kubelet:10.10.10.205:10255\nI0419 08:32:05.017432       1 manager.go:98] Querying source: kubelet:10.10.10.200:10255\nI0419 08:32:05.024460       1 manager.go:98] Querying source: kubelet:10.10.10.201:10255\nI0419 08:32:05.027253       1 kubelet.go:233] successfully obtained stats for 79 containers\nI0419 08:32:05.093195       1 kubelet.go:233] successfully obtained stats for 90 containers\nI0419 08:32:05.102534       1 kubelet.go:233] successfully obtained stats for 90 containers\nI0419 08:32:05.122088       1 kubelet.go:233] successfully obtained stats for 82 containers\nI0419 08:32:05.122626       1 manager.go:152] ScrapeMetrics: time: 122.274079ms size: 341\nI0419 08:32:05.122639       1 manager.go:154]    scrape  bucket 0: 4\nI0419 08:32:05.122674       1 manager.go:154]    scrape  bucket 1: 0\nI0419 08:32:05.122693       1 manager.go:154]    scrape  bucket 2: 0\nI0419 08:32:05.122710       1 manager.go:154]    scrape  bucket 3: 0\nI0419 08:32:05.122726       1 manager.go:154]    scrape  bucket 4: 0\nI0419 08:32:05.122742       1 manager.go:154]    scrape  bucket 5: 0\nI0419 08:32:05.122759       1 manager.go:154]    scrape  bucket 6: 0\nI0419 08:32:05.122776       1 manager.go:154]    scrape  bucket 7: 0\nI0419 08:32:05.122792       1 manager.go:154]    scrape  bucket 8: 0\nI0419 08:32:05.122808       1 manager.go:154]    scrape  bucket 9: 0\nI0419 08:32:05.122818       1 manager.go:154]    scrape  bucket 10: 0\nI0419 08:32:05.124888       1 manager.go:113] Pushing data to: Metric Sink\nI0419 08:32:05.124888       1 manager.go:113] Pushing data to: InfluxDB Sink\nI0419 08:32:05.124926       1 manager.go:116] Data push completed: Metric Sink\nI0419 08:32:05.124931       1 manager.go:116] Data push completed: InfluxDB Sink\nI0419 08:32:05.145711       1 influxdb.go:215] Created database \"k8s\" on influxDB server at \"monitoring-influxdb:8086\"\nI0419 08:32:05.289849       1 influxdb.go:169] Exported 2607 data to influxDB in 144.115539ms\nW0419 08:32:14.566085       1 listers.go:68] can not retrieve list of objects using index : object has no meta: object does not implement the Object interfaces\nI0419 08:32:14.566133       1 handlers.go:216] No metrics for pod default/xstreamline-cluster-1605009199-nn379. needed to add resource limits to all containers in the pod deployment per documentation: \nhttps://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\nI do want to point our the the https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/ is not very straight forward on the request limit. I would add in bold font: \"request limit needs to be set\" before trying the hpa. ",
    "paalkr": "I now see that my report is a duplicate of https://github.com/kubernetes/heapster/issues/1586 \nBTW, I'm using Calico networking. Maybe I need to setup some special calico privileges? . Thanks!\nSendt fra E-post for Windows 10\nFra: Igor Dolzhikov\nSendt: onsdag 26. april 2017 kl. 20.59\nTil: kubernetes/heapster\nKopi: paalkr; Mention\nEmne: Re: [kubernetes/heapster] No statistics received from the node thatheapster pod is running (#1602)\n@paalkr Please see my post at #1586\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. ",
    "lcnsir": "?. ",
    "jaipradeesh": "Updating this value\n- name: GF_SERVER_ROOT_URL\n  value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/\n.. fixed it.. ",
    "SupreethKadalur": "Hello @AlmogBaku , @rikatz - Please let us know if ES5 PR is merged and ready for use. I have the same problem and unable to send data from heapster v 1.3.0 to ES5\n@rikatz - I totally agree that we should have  @timestamp for each metrics :-) this will solve many problems I have while creating dashboards in kibana. . @AlmogBaku, @DirectXMan12  - I'm using image \"docker pull kubernetes/heapster\" from docker hub. Merging and building this PR (heapster to Elastic 5) will push the image to docker hub ? Please let me know otherwise. Thank you. @DirectXMan12 , @AlmogBaku - I see that ES5 fix is merged and Heapster v1.4.0-beta.0 is created. I'm now using docker image \"gcr.io/google_containers/heapster-amd64:v1.4.0-beta.0\" but still see the same problem I had with heapster-amd64:v1.3.0 image. Am I missing something here? Please suggest. Thanks . Thank you all for the support and fix. Now ES5 issue is fixed with heapster v1.4.0. ",
    "bsafwen": "Hi, in my case, using your  commit  a68f4ac, heapster is able to find elasticsearch, I am able to retrieve most of the data but my problem is that  although I see a cpu/usage_rate in the mappings, I cannot see it in my queries which means the field has no data. On the other hand, cpu/usage is correct.. @AlmogBaku  Thank you for your reply. I changed the index from heapster-2016-06-09 to heapster* and it worked, still trying to figure why.. ",
    "alexandrst88": "@andyxning does it possible to log only critical or error level?. ",
    "dashpole": "@piosz is this the correct way to expose pod requests in gke?. @piosz ping. cc @dchen1107 . @piosz and I spoke offline.  Heapster will soon be replaced by the new Monitoring Server.  Since container requests and limits can be obtained using the kubernetes API, I plan to move container limits from querying Heapster, to using the API directly.  I also plan to add container requests during that process.  This will achieve my goals of adding container requests, and help facilitate the transition from heapster to the metrics server.  Closing this PR as it is no longer needed.. ",
    "munnerz": "They most likely are missing from the Docker image, which I (personally) see as best practice, as root CAs might be revoked or updated, meaning your image will eventually become incompatible with newer certs (or, conversely, a certificate that should no longer be trusted may be trusted)\nI think best practice here is to pass through /etc/ssl/certs from your host machine. Alternatively, an init container with a shared emptyDir at /etc/ssl/certs can be used to populate the directory.\n. ",
    "Shimi": "Thanks @munnerz, I used volumeMount. ",
    "NullVoxPopuli": "I tried this, and it didn't work (error persists)\n```yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: monitoring-grafana\n  namespace: kube-system\nspec:\n  replicas: 1\n  revisionHistoryLimit: 0\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /var\n          name: grafana-storage\n        - mountPath: /etc/ssl/certs\n          name: ca-certificates\n        env:\n        - name: INFLUXDB_HOST\n          value: monitoring-influxdb\n        - name: GRAFANA_PORT\n          value: \"3000\"\n      volumes:\n      - name: grafana-storage\n        emptyDir: {}\n      - name: ca-certificates\n        emptyDir: {}\n      - name: grafana-config\n        configMap:\n          name: grafana-config-ini\n          items:\n            - key: grafana.config.ini\n              path: grafana.ini\n\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)\n    # If you are NOT using this as an addon, you should comment out this line.\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: monitoring-grafana\n  name: monitoring-grafana\n  namespace: kube-system\nspec:\n  # In a production setup, we recommend accessing Grafana through an external Loadbalancer\n  # or through a public IP.\n  type: LoadBalancer\n  # You could also use NodePort to expose the service at a randomly-generated port\n  # type: NodePort\n  ports:\n  - port: 80\n    targetPort: 3000\n  selector:\n    k8s-app: grafana\n```. I had to change my volumes def from emptyDir to\nyaml\n      - name: ca-certificates\n        hostPath:\n          path: /etc/ssl/certs. any chance this'll get buttoned up soon?. Github will squash for you -- is there a reason to not use that feature?. ",
    "a4abhishek": "@NullVoxPopuli even above solution doesn't work for me.. ",
    "vniche": "I don't really know why, but volume naming change to ca-certs (instead of ca-certificates), in both volumes and volumeMounts, worked:\nvolumeMounts:\n        - mountPath: /etc/ssl/certs\n          name: ca-certs\n          readOnly: true\n      volumes:\n      - hostPath:\n          path: /etc/ssl/certs\n        name: ca-certs. ",
    "jeffery9": "can not find where defined the  clusterRole  system:heapster\n```\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  creationTimestamp: 2017-05-19T01:29:35Z\n  name: heapster\n  resourceVersion: \"48623\"\n  selfLink: /apis/rbac.authorization.k8s.io/v1beta1/clusterrolebindingsheapster\n  uid: 9dbf4447-3c32-11e7-8db8-0050562a8439\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:heapster\nsubjects:\n- kind: ServiceAccount\n  name: heapster\n  namespace: kube-system\n```\ncan't  found  clusterRole definition.\n```\nkubectl get clusterrole heapster\nError from server (NotFound): clusterroles.rbac.authorization.k8s.io \"heapster\" not found\n```. DNS works OK,\n```\nE0520 01:18:55.360650       1 reflector.go:190]\nk8s.io/heapster/metrics/util/util.go:51: Failed to list *v1.Node: Get\nhttps://kubernetes.default/api/v1/nodes?resourceVersion=0: dial tcp\n10.96.0.1:443: i/o timeout\n```\nOn 20 May 2017 at 02:08, jiahuang notifications@github.com wrote:\n\n@jeffery9 https://github.com/jeffery9 please check your cluster\u2018s dns\nis ok\uff1f\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/1648#issuecomment-302772869,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABVuLGV7RnZkP7S4hLF8wXvzqOqslPtRks5r7dqkgaJpZM4NgATJ\n.\n\n\n-- \nJeffery        -odoo expert form kunshan, suzhou, china\nIM Q913547235\n. I signed it!. I signed it!. thanks, maybe it have been delete by myself.\nOn 19 May 2017 at 23:46, Solly Ross notifications@github.com wrote:\n\nI just checked, and it looks like there is a role in the bootstrap policy\nfor Heapster (https://github.com/kubernetes/kubernetes/blob/\nmaster/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/policy.go#L201)\nwhich already contains these rules.\nIs your cluster set up with the bootstrap policy?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/pull/1649#issuecomment-302739220,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABVuLCh8tGFZm-Cthf63uo6kSH0g0c0Yks5r7blggaJpZM4NgCGl\n.\n\n\n-- \nJeffery        -odoo expert form kunshan, suzhou, china\nIM Q913547235\n. ",
    "arykalin": "Have same problem, any advises?. I've made a workaround of this by changing line 82 from setup_grafana.go to my grafana service address\n-       grafanaURL := fmt.Sprintf(\"%s://%s:%s@localhost:%s\", envParams[\"gf_server_protocol\"], envParams[\"grafana_user\"], envParams[\"grafana_passwd\"], envParams[\"grafana_port\"])\n+       grafanaURL := fmt.Sprintf(\"%s://%s:%s@192.168.1.1:%s\", envParams[\"gf_server_protocol\"], envParams[\"grafana_user\"], envParams[\"grafana_passwd\"], envParams[\"grafana_port\"])\nAnd running setup_grafana.go manualy after deploy.. ",
    "sotona-": "Same problem too.\nIn my case grafana_setup binary resolves localhost as 10.x.x.x(real ip from my network) and try to connect to it.\nIt is because my dns have A-record localhost.mycompany.local and resolv.conf in grafana's container have option \"search mycompany.local\".\nI solve this by placing nsswitch.conf file to grafana's container with string \"hosts:      files dns myhostname\".. ",
    "jpds": "I'm having this same issue on a kubeadm deployed cluster.. As a heads-up, this is still broken in the https://github.com/kubernetes/heapster/tree/release-1.4 branch. @AlexB138 1.4 works fine with the fix in the comment here: https://github.com/kubernetes/heapster/issues/1709#issuecomment-315979971. See #1783.. ",
    "Robbilie": "im running into this too and i have no idea how to fix it -.-. i know that i can mount a config file into it but i would rather not.\ni have no idea why the config file is missing and this was really annoying when i tried to upgrade to 1.4.\nany eta when a release with the fix is expected?. thats not the desired way. the official grafana and previous versions of heapsters grafana had a grafana.ini and i dont see why the 1.4.0 release shouldnt have one. lol theres literally no reason to not have one, 1.3 aswell as official grafana releases had one. @andyxning with one less slash yeah, thats the previous one which had the ini, https://hub.docker.com/r/grafana/grafana/ has the ini aswell. mount the file into the pod\u2026\nalthough this should imo be fixed ASAP in a 4.2.1 release bc this is just annoying and breaking\u2026. thats not a fix, i used to use a newer grafana image while waiting for heapster 1.4 because i need certain grafana notification channels. ",
    "emichaf": "This worked...deploying Grafana from the release-1.3 branch \nhttps://forums.manning.com/posts/list/41092.page. ",
    "dove-young": "Move back to gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 did not work for me. \nSame problem occurs\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp: i/o timeout. Retrying after 5 seconds...\nAnd actually the gcr.io/google_containers/heapster-grafana-amd64:v4.4.1 works well with K8S v1.6.1 . \nkubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.1\", GitCommit:\"b0b7a323cc5a4a2019b2e9520c21c7830b7f708e\", GitTreeState:\"clean\", BuildDate:\"2017-04-03T20:44:38Z\", GoVersion:\"go1.7.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"cfc-1.6.1.17+f04cb2d1cde1c7-dirty\", GitCommit:\"f04cb2d1cde1c7ae8edbb44289529b871e84d541\", GitTreeState:\"dirty\", BuildDate:\"2017-04-18T07:25:14Z\", GoVersion:\"go1.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nThe problem occurs is in K8S v1.5.5\nkubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"5\", GitVersion:\"v1.5.5\", GitCommit:\"894ff23729bbc0055907dd3a496afb725396adda\", GitTreeState:\"clean\", BuildDate:\"2017-03-22T00:23:49Z\", GoVersion:\"go1.7.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"5\", GitVersion:\"v1.5.5\", GitCommit:\"894ff23729bbc0055907dd3a496afb725396adda\", GitTreeState:\"clean\", BuildDate:\"2017-03-22T00:17:51Z\", GoVersion:\"go1.7.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}. ",
    "aysark": "Having same issue, with k8s v1.8.  Tried older image tags for heapster-grafana but no luck.. ",
    "lvicentesanchez": "Having this issue with grafana 4.4.3 and postgres as the database.. This are the log messages I get:\nt=2018-02-27T18:24:52+0000 lvl=info msg=\"Config overridden from Environment variable\" logger=settings var=\"GF_SERVER_PROTOCOL=http\"\nt=2018-02-27T18:24:52+0000 lvl=info msg=\"Config overridden from Environment variable\" logger=settings var=\"GF_SERVER_ROOT_URL=/\"\nt=2018-02-27T18:24:52+0000 lvl=info msg=\"Config overridden from Environment variable\" logger=settings var=\"GF_DATABASE_TYPE=postgres\"\nt=2018-02-27T18:24:52+0000 lvl=info msg=\"Config overridden from Environment variable\" logger=settings var=\"GF_DATABASE_HOST=XXXXXXXX\"\nt=2018-02-27T18:24:52+0000 lvl=info msg=\"Config overridden from Environment variable\" logger=settings var=\"GF_DATABASE_USER=grafana\"\nt=2018-02-27T18:24:52+0000 lvl=info msg=\"Config overridden from Environment variable\" logger=settings var=\"GF_DATABASE_PASSWORD=*********\"\nt=2018-02-27T18:24:52+0000 lvl=info msg=\"Config overridden from Environment variable\" logger=settings var=\"GF_AUTH_ANONYMOUS_ENABLED=true\"\nt=2018-02-27T18:24:52+0000 lvl=info msg=\"Config overridden from Environment variable\" logger=settings var=\"GF_AUTH_ANONYMOUS_ORG_ROLE=Admin\"\nt=2018-02-27T18:24:52+0000 lvl=info msg=\"Config overridden from Environment variable\" logger=settings var=\"GF_AUTH_BASIC_ENABLED=false\"\nt=2018-02-27T18:24:52+0000 lvl=info msg=\"Path Home\" logger=settings path=/usr/share/grafana\nt=2018-02-27T18:24:52+0000 lvl=info msg=\"Path Data\" logger=settings path=/var/lib/grafana\nt=2018-02-27T18:24:52+0000 lvl=info msg=\"Path Logs\" logger=settings path=/var/log/grafana\nt=2018-02-27T18:24:52+0000 lvl=info msg=\"Path Plugins\" logger=settings path=/usr/share/grafana/data/plugins\nt=2018-02-27T18:24:52+0000 lvl=info msg=\"Initializing DB\" logger=sqlstore dbtype=postgres\nt=2018-02-27T18:24:52+0000 lvl=info msg=\"Starting DB migration\" logger=migrator\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp [::1]:3000: getsockopt: connection refused. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp [::1]:3000: getsockopt: connection refused. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp [::1]:3000: getsockopt: connection refused. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp [::1]:3000: getsockopt: connection refused. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp [::1]:3000: getsockopt: connection refused. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp [::1]:3000: getsockopt: connection refused. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp [::1]:3000: getsockopt: connection refused. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp [::1]:3000: getsockopt: connection refused. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp [::1]:3000: getsockopt: connection refused. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp [::1]:3000: getsockopt: connection refused. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp [::1]:3000: getsockopt: connection refused. Retrying after 5 seconds.... I'm using a ClusterIP service as I want to access the grafana dashboard using our nginx ingress.. I can confirm that I'm having this issue with any 4.x version of Grafana when I'm trying to use postgres as the database. If I use sqlite3 it just works. I will try to replicate the issue with MySql. And same problem with mysql.... It seems that the migration for mysql and postgres is trying to connect to the dashboard... but grafana doesn't start listening in port 3000 until the migration is finished... or that's what happens with sqlite3:\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create annotation table v5\"\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index annotation 0 v3\"\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index annotation 1 v3\"\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index annotation 2 v3\"\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index annotation 3 v3\"\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index annotation 4 v3\"\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Update annotation table charset\"\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Add column region_id to annotation table\"\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create test_data table\"\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create dashboard_version table v1\"\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index dashboard_version.dashboard_id\"\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add unique index dashboard_version.dashboard_id and dashboard_version.version\"\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Set dashboard version to 1 where 0\"\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"save existing dashboard data in dashboard_version table v1\"\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"alter dashboard_version.data to mediumtext v1\"\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Created default admin user: admin\"\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Starting plugin search\" logger=plugins\nt=2018-02-28T11:27:00+0000 lvl=warn msg=\"Plugin dir does not exist\" logger=plugins dir=/usr/share/grafana/data/plugins\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Plugin dir created\" logger=plugins dir=/usr/share/grafana/data/plugins\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Initializing Alerting\" logger=alerting.engine\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Initializing CleanUpService\" logger=cleanup\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Initializing Stream Manager\"\nt=2018-02-28T11:27:00+0000 lvl=info msg=\"Initializing HTTP Server\" logger=http.server address=0.0.0.0:3000 protocol=http subUrl= socket=\nConnected to the Grafana dashboard.. /remove-lifecycle stale. I found the root of the issue... a connectivity issue with the DB... but there was no timeout error in the logs.. Grafana dashboards are not persistent across restarts of the pod; I had to use a database as my dashboard backend.. ",
    "zhang-jianwei": "I have the issues on kubernets v1.11.2 and heapster-influxdb v1.3.3 with kubeadm deploy.\nthe log as follow :\n Can't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp 104.238.186.14:3000: i/o timeout. Retrying after 5 `seconds...\nand from node pod telnet monitoring-influxdb.kube-system.svc 8086, not access. but nslookup is ok\nroot@k8s-2:~# sudo docker exec -it b1bc365a55ac nslookup monitoring-influxdb.kube-system.svc\nServer:    10.96.0.10\nAddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\nName:      monitoring-influxdb.kube-system.svc\nAddress 1: 10.103.167.134 monitoring-influxdb.kube-system.svc.cluster.local\n. ",
    "adambkaplan": "Similar issue reported in #1623 . I've signed the CLA for Google and Linux.. @luxas Updated grafana version. New image needs to be pushed out to GCR for the updated grafana.yaml configuration to work.. :squished:. What's the status on this? Rebased.. I'd make sure that the right version of Go is used to compile Grafana. The Grafana team seems to be using the latest version of Go with each successive build, and the base images may not come with those pre-installed.. @loburm No other problems. I believe I had a previous issue bumping from 4.0 to 4.2.. Updated grafana/run.sh so it doesn't export the GF_SERVER_HTTP_PORT environment variable. By default Grafana should bind to port 3000.. May be better off using the if val, ok := envParams[...] construct (one less map access call).. ",
    "kotarusv": "I'm using all latest versions\nInfluxDB shell version: 1.2.2\nGrafana : 4.2.0\nHeapster : v1.3.0. Can we support cluster name tag for InfluxDB sink as well? I think that is very important  in multi cluster environment rather depending on regular expressions. . ",
    "wedgeV": "I just updated to Grafana 4.2.0, but that image had different VOLUMEs compared to the heapster grafana image, that made it loose all the dashboards and settings. I solved this by taking the Grafana 4.2.0 image (https://github.com/grafana/grafana-docker), and removing the VOLUME directive in the Dockerfile and then build/deploy that image.\n@kotarusv Not sure if that's the issue you're running into, but this worked for me.. ",
    "DarkBlaez": "I will ask the dashboard repo since this issue seems to get punted along and no one can even spell out the exact instructions as referenced above. So here is how I have installed this:\n\nGit clone heapster project\n\nJust use the kube.sh in deploy folder to initiate. Suppose I could use kubectl directly as well but result is the same when I have done so\n\n\nI modify grafana.yaml in (/deploy/kube-config/influxdb) to use the api server proxy\n    value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/\n\n\nno other changes. Grafana can see the template, shows Cluster and Pod as the choices. The InfluxDB data source is connected with no issues, however, there is no data points showing in Grafana\n\nIn checking heapster it is running. I query it with curl via:\ncurl localhost:8001/api/v1/proxy/namespaces/kube-system/services/heapster/metrics\n\nThe result is a long list of metrics\n\nChecking further...\ncurl localhost:8001/api/v1/proxy/namespaces/kube-system/services/heapster/api/v1/model/debug/allkeys\n\nI get a return of:\n[\n  \"cluster\"\n ]\nWhat more can be done to check to see if heapster is feedin influxdb the data. This seems like perhaps the issue possibly.\nAlso is cAdvisor part of the equation ? Again this is a cluster installed using kubeadm. But as I recall with the manual or ansible (from contrib) install they install cAdvisor. What role does cAdvisor play in this as well?\nThanks\nDB\n. This is the same issue I reported on in a prior open issue #1619. I'm having the same issue. ",
    "remonlam": "@DirectXMan12 I'm running Kubernetes 1.5.2\nHere is a section of the log (begin/end)\n```\n2017-05-03T20:37:11.405901897Z I0503 20:37:11.405667       1 heapster.go:72] /heapster --source=kubernetes:https://kubernetes.default --v=8\n2017-05-03T20:37:11.405949524Z I0503 20:37:11.405703       1 heapster.go:73] Heapster version v1.3.0\n2017-05-03T20:37:11.406131491Z I0503 20:37:11.405920       1 configs.go:61] Using Kubernetes client with master \"https://kubernetes.default\" and version v1\n2017-05-03T20:37:11.406141522Z I0503 20:37:11.405937       1 configs.go:62] Using kubelet port 10255\n2017-05-03T20:37:11.406867567Z I0503 20:37:11.406666       1 round_trippers.go:395] GET https://kubernetes.default/api/v1/nodes\n2017-05-03T20:37:11.406888446Z I0503 20:37:11.406676       1 round_trippers.go:402] Request Headers:\n2017-05-03T20:37:11.406895182Z I0503 20:37:11.406681       1 round_trippers.go:405]     Accept: application/vnd.kubernetes.protobuf, /\n2017-05-03T20:37:11.406900232Z I0503 20:37:11.406686       1 round_trippers.go:405]     User-Agent: heapster/v0.0.0 (linux/amd64) kubernetes/$Format\n2017-05-03T20:37:11.406905812Z I0503 20:37:11.406692       1 round_trippers.go:405]     Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOi[xxxxxxxxxx]vpJN5JHXkDkN4pdXlXLgnKQBanVe3rYe1BXgqMddu1_0rZJ4PqJA6I7sSDIe3xCMGhlCN6RzysBxBkgeWLfGRq1DzlHOexF0qQvwhTUlCWz7UQ8Arm20uhEQS36B3oBUoaeqAKPwUtLj7G3Cef5k7osce24cWLsv6kQmY0tMnhDVYobdTq7PN_3RZ79fA-DJ2lXTu29NvUoJZ7TwTGobG3NvULb-10L-JPD0hO-5bAbtI40ApuJ__vQRL-6aGXuClBpqtfrOrl-3_wcqcgkMZK8Es1H0uvCfB3b10SblWK8yd20Qw\n2017-05-03T20:37:11.520084246Z I0503 20:37:11.519810       1 round_trippers.go:420] Response Status: 200 OK in 113 milliseconds\n2017-05-03T20:37:11.520107459Z I0503 20:37:11.519838       1 round_trippers.go:423] Response Headers:\n2017-05-03T20:37:11.520113502Z I0503 20:37:11.519846       1 round_trippers.go:426]     Content-Type: application/vnd.kubernetes.protobuf\n2017-05-03T20:37:11.520120668Z I0503 20:37:11.519852       1 round_trippers.go:426]     Date: Wed, 03 May 2017 20:37:11 GMT\n2017-05-03T20:37:11.536837874Z I0503 20:37:11.528967       1 request.go:989] Response Body:\n2017-05-03T20:37:11.536872652Z 00000000  6b 38 73 00 0a 0e 0a 02  76 31 12 08 4e 6f 64 65  |k8s.....v1..Node|\n2017-05-03T20:37:11.536879229Z 00000010  4c 69 73 74 12 84 e9 04  0a 19 0a 0d 2f 61 70 69  |List......../api|\n2017-05-03T20:37:11.536884866Z 00000020  2f 76 31 2f 6e 6f 64 65  73 12 08 32 31 32 37 37  |/v1/nodes..21277|\n2017-05-03T20:37:11.536890293Z 00000030  35 39 34 12 a1 2c 0a 86  03 0a 11 6e 75 63 2d 78  |594..,.....nuc-x|\n2017-05-03T20:37:11.536895731Z 00000040  36 34 2d 6e 30 2e 63 2d  73 2e 69 6f 12 00 1a 00  |64-n0.domain.local....|\n2017-05-03T20:37:11.536901153Z 00000050  22 1e 2f 61 70 69 2f 76  31 2f 6e 6f 64 65 73 6e  |\"./api/v1/nodesn|\n2017-05-03T20:37:11.536906725Z 00000060  75 63 2d 78 36 34 2d 6e  30 2e 63 2d 73 2e 69 6f  |uc-x64-n0.domain.local|\n2017-05-03T20:37:11.536923054Z 00000070  2a 24 61 30 63 65 37 37  66 39 2d 65 64 66 64 2d  |*$a0ce77f9-edfd-|\n2017-05-03T20:37:11.536928771Z 00000080  31 31 65 36 2d 39 65 31  63 2d 30 30 31 33 32 30  |11e6-9e1c-001320|\n2017-05-03T20:37:11.536934179Z 00000090  66 65 35 33 62 34 32 08  32 31 32 37 37 35 32 39  |fe53b42.21277529|\n2017-05-03T20:37:11.536939539Z 000000a0  38 00 42 08 08 da a9 ec  c4 05 10 00 5a 0e 0a 07  |8.B.........Z...|\n2017-05-03T20:37:11.536944851Z 000000b0  63 70 75 74 79 70 65 12  03 78 36 34 5a 2b 0a 16  |cputype..x64Z+..|\n2017-05-03T20:37:11.536950120Z 000000c0  6b 75 62 65 72 6e 65 74  65 73 2e 69 6f 2f 68 6f  |kubernetes.io/ho|\n2017-05-03T20:37:11.536955562Z 000000d0  73 74 6e 61 6d 65 12 11  6e 75 63 2d 78 36 34 2d  |stname..nuc-x64-|\n2017-05-03T20:37:11.536966077Z 000000e0  6e 30 2e 63 2d 73 2e 69  6f 5a 0f 0a 08 64 69 73  |n0.domain.localZ...dis|\n2017-05-03T20:37:11.536971429Z 000000f0  6b 74 79 70 65 12 03 73  73 64 5a 0d 0a 08 63 70  |ktype..ssdZ...cp|\n2017-05-03T20:37:11.536976825Z 00000100  75 63 6f 72 65 73 12 01  34 5a 1e 0a 15 62 65 74  |ucores..4Z...bet|\n2017-05-03T20:37:11.536982096Z 00000110  61 2e 6b 75 62 65 72 6e  65 74 65 73 2e 69 6f 2f  |a.kubernetes.io/|\n2017-05-03T20:37:11.536987348Z 00000120  6f 73 12 05 6c 69 6e 75  78 5a 0e 0a 06 6d 65 6d  |os..linuxZ...mem|\n2017-05-03T20:37:11.536992555Z 00000130  6f 72 79 12 04 31 36 47  42 5a 0a 0a 03 6e 66 73  |ory..16GBZ...nfs|\n2017-05-03T20:37:11.536997814Z 00000140  12 03 79 65 73 5a 20 0a  17 62 65 74 61 2e 6b 75  |..yesZ ..beta.ku|\n2017-05-03T20:37:11.537005886Z 00000150  62 65 72 6e 65 74 65 73  2e 69 6f 2f 61 72 63 68  |bernetes.io/arch|\n2017-05-03T20:37:11.537011092Z 00000160  12 05 61 6d 64 36 34 5a  14 0a 08 63 70 75 6d 6f  |..amd64Z...cpumo|\n2017-05-03T20:37:11.537016355Z 00000170  64 65 6c 12 08 69 35 2d  34 32 35 30 55 62 3e 0a  |del..i5-4250Ub>.|\n2017-05-03T20:37:11.537021968Z 00000180  36 76 6f 6c 75 6d 65 73  2e 6b 75 62 65 72 6e 65  |6volumes.kuberne|\n2017-05-03T20:37:11.537027212Z 00000190  74 65 73 2e 69 6f 2f 63  6f 6e 74 72 6f 6c 6c 65  |tes.io/controlle|\n2017-05-03T20:37:11.537032440Z 000001a0  72 2d 6d 61 6e 61 67 65  64 2d 61 74 74 61 63 68  |r-managed-attach|\n2017-05-03T20:37:11.537037759Z 000001b0  2d 64 65 74 61 63 68 12  04 74 72 75 65 7a 00 12  |-detach..truez..|\n2017-05-03T20:37:11.537043026Z 000001c0  26 0a 0d 31 30 2e 32 30  30 2e 35 2e 30 2f 32 34  |&..10.200.5.0/24|\n2017-05-03T20:37:11.537048589Z 000001d0  12 11 6e 75 63 2d 78 36  34 2d 6e 30 2e 63 2d 73  |..nuc-x64-n0.c-s|\n2017-05-03T20:37:11.537053868Z 000001e0  2e 69 6f 1a 00 20 00 1a  ed 28 0a 25 0a 1e 61 6c  |.io.. ...(.%..al|\n2017-05-03T20:37:11.537059212Z 000001f0  70 68 61 2e 6b 75 62 65  72 6e 65 74 65 73 2e 69  |pha.kubernetes.i|\n....\n2017-05-03T20:49:05.192051766Z I0503 20:49:05.191894       1 kubelet.go:92] Found system container /docker with labels: map[]\n2017-05-03T20:49:05.194572347Z I0503 20:49:05.194372       1 kubelet.go:235] successfully obtained stats for 5 containers\n2017-05-03T20:49:05.194584110Z I0503 20:49:05.194432       1 kubelet.go:92] Found system container /docker with labels: map[]\n2017-05-03T20:49:05.197007521Z I0503 20:49:05.196771       1 round_trippers.go:420] Response Status: 200 OK in 31 milliseconds\n2017-05-03T20:49:05.197037852Z I0503 20:49:05.196786       1 round_trippers.go:423] Response Headers:\n2017-05-03T20:49:05.197044871Z I0503 20:49:05.196793       1 round_trippers.go:426]     Date: Wed, 03 May 2017 20:49:05 GMT\n2017-05-03T20:49:05.197050211Z I0503 20:49:05.196799       1 round_trippers.go:426]     Content-Type: application/json\n2017-05-03T20:49:05.202482533Z I0503 20:49:05.202251       1 kubelet.go:235] successfully obtained stats for 5 containers\n2017-05-03T20:49:05.202512387Z I0503 20:49:05.202305       1 kubelet.go:92] Found system container /docker with labels: map[]\n2017-05-03T20:49:05.203031882Z I0503 20:49:05.202819       1 round_trippers.go:420] Response Status: 200 OK in 48 milliseconds\n2017-05-03T20:49:05.203049440Z I0503 20:49:05.202832       1 round_trippers.go:423] Response Headers:\n2017-05-03T20:49:05.203053117Z I0503 20:49:05.202836       1 round_trippers.go:426]     Content-Type: application/json\n2017-05-03T20:49:05.203055977Z I0503 20:49:05.202840       1 round_trippers.go:426]     Date: Wed, 03 May 2017 20:49:05 GMT\n2017-05-03T20:49:05.204795755Z I0503 20:49:05.204562       1 manager.go:98] Querying source: kubelet:192.168.100.118:10255\n2017-05-03T20:49:05.204821951Z I0503 20:49:05.204698       1 round_trippers.go:395] POST http://192.168.100.118:10255/stats/container/\n2017-05-03T20:49:05.204828502Z I0503 20:49:05.204713       1 round_trippers.go:402] Request Headers:\n2017-05-03T20:49:05.204834281Z I0503 20:49:05.204723       1 round_trippers.go:405]     Content-Type: application/json\n2017-05-03T20:49:05.204840276Z I0503 20:49:05.204734       1 round_trippers.go:405]     Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOi[xxxxxxxxxx]vpJN5JHXkDkN4pdXlXLgnKQBanVe3rYe1BXgqMddu1_0rZJ4PqJA6I7sSDIe3xCMGhlCN6RzysBxBkgeWLfGRq1DzlHOexF0qQvwhTUlCWz7UQ8Arm20uhEQS36B3oBUoaeqAKPwUtLj7G3Cef5k7osce24cWLsv6kQmY0tMnhDVYobdTq7PN_3RZ79fA-DJ2lXTu29NvUoJZ7TwTGobG3NvULb-10L-JPD0hO-5bAbtI40ApuJ__vQRL-6aGXuClBpqtfrOrl-3_wcqcgkMZK8Es1H0uvCfB3b10SblWK8yd20Qw\n2017-05-03T20:49:05.205070018Z I0503 20:49:05.204871       1 round_trippers.go:420] Response Status: 200 OK in 23 milliseconds\n2017-05-03T20:49:05.205081437Z I0503 20:49:05.204885       1 round_trippers.go:423] Response Headers:\n2017-05-03T20:49:05.205084587Z I0503 20:49:05.204893       1 round_trippers.go:426]     Content-Type: application/json\n2017-05-03T20:49:05.205089360Z I0503 20:49:05.204901       1 round_trippers.go:426]     Date: Wed, 03 May 2017 20:49:05 GMT\n2017-05-03T20:49:05.205094703Z I0503 20:49:05.204960       1 round_trippers.go:420] Response Status: 200 OK in 31 milliseconds\n2017-05-03T20:49:05.205099785Z I0503 20:49:05.204973       1 round_trippers.go:423] Response Headers:\n2017-05-03T20:49:05.205102709Z I0503 20:49:05.204980       1 round_trippers.go:426]     Content-Type: application/json\n2017-05-03T20:49:05.205105704Z I0503 20:49:05.204988       1 round_trippers.go:426]     Date: Wed, 03 May 2017 20:49:05 GMT\n2017-05-03T20:49:05.205711800Z I0503 20:49:05.205455       1 round_trippers.go:420] Response Status: 200 OK in 22 milliseconds\n2017-05-03T20:49:05.205763576Z I0503 20:49:05.205467       1 round_trippers.go:423] Response Headers:\n2017-05-03T20:49:05.205771458Z I0503 20:49:05.205473       1 round_trippers.go:426]     Content-Type: application/json\n2017-05-03T20:49:05.205774422Z I0503 20:49:05.205479       1 round_trippers.go:426]     Date: Wed, 03 May 2017 20:49:05 GMT\n2017-05-03T20:49:05.206009315Z I0503 20:49:05.205731       1 round_trippers.go:420] Response Status: 200 OK in 22 milliseconds\n2017-05-03T20:49:05.206020831Z I0503 20:49:05.205744       1 round_trippers.go:423] Response Headers:\n2017-05-03T20:49:05.206026314Z I0503 20:49:05.205750       1 round_trippers.go:426]     Content-Type: application/json\n2017-05-03T20:49:05.206030998Z I0503 20:49:05.205757       1 round_trippers.go:426]     Date: Wed, 03 May 2017 20:49:05 GMT\n2017-05-03T20:49:05.209087793Z I0503 20:49:05.208849       1 kubelet.go:235] successfully obtained stats for 5 containers\n2017-05-03T20:49:05.209119942Z I0503 20:49:05.208918       1 kubelet.go:92] Found system container /docker with labels: map[]\n2017-05-03T20:49:05.210372586Z I0503 20:49:05.210164       1 kubelet.go:235] successfully obtained stats for 5 containers\n2017-05-03T20:49:05.210389403Z I0503 20:49:05.210184       1 kubelet.go:92] Found system container /docker with labels: map[]\n2017-05-03T20:49:05.211008857Z I0503 20:49:05.210794       1 kubelet.go:235] successfully obtained stats for 5 containers\n2017-05-03T20:49:05.211025574Z I0503 20:49:05.210863       1 kubelet.go:92] Found system container /docker with labels: map[]\n2017-05-03T20:49:05.211036041Z I0503 20:49:05.210488       1 kubelet.go:235] successfully obtained stats for 5 containers\n2017-05-03T20:49:05.211041624Z I0503 20:49:05.210940       1 kubelet.go:92] Found system container /docker with labels: map[]\n2017-05-03T20:49:05.212968368Z I0503 20:49:05.212763       1 kubelet.go:235] successfully obtained stats for 5 containers\n2017-05-03T20:49:05.212988069Z I0503 20:49:05.212805       1 kubelet.go:92] Found system container /docker with labels: map[]\n2017-05-03T20:49:05.215811099Z I0503 20:49:05.215576       1 manager.go:98] Querying source: kubelet:192.168.100.116:10255\n2017-05-03T20:49:05.215839531Z I0503 20:49:05.215742       1 round_trippers.go:395] POST http://192.168.100.116:10255/stats/container/\n2017-05-03T20:49:05.215846027Z I0503 20:49:05.215754       1 round_trippers.go:402] Request Headers:\n2017-05-03T20:49:05.215850822Z I0503 20:49:05.215784       1 round_trippers.go:405]     Content-Type: application/json\n2017-05-03T20:49:05.215857926Z I0503 20:49:05.215794       1 round_trippers.go:405]     Authorization: Bearer \neyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOi[xxxxxxxxxx]vpJN5JHXkDkN4pdXlXLgnKQBanVe3rYe1BXgqMddu1_0rZJ4PqJA6I7sSDIe3xCMGhlCN6RzysBxBkgeWLfGRq1DzlHOexF0qQvwhTUlCWz7UQ8Arm20uhEQS36B3oBUoaeqAKPwUtLj7G3Cef5k7osce24cWLsv6kQmY0tMnhDVYobdTq7PN_3RZ79fA-DJ2lXTu29NvUoJZ7TwTGobG3NvULb-10L-JPD0hO-5bAbtI40ApuJ__vQRL-6aGXuClBpqtfrOrl-3_wcqcgkMZK8Es1H0uvCfB3b10SblWK8yd20Qw\n2017-05-03T20:49:05.218337156Z I0503 20:49:05.218104       1 round_trippers.go:420] Response Status: 200 OK in 33 milliseconds\n2017-05-03T20:49:05.218360510Z I0503 20:49:05.218149       1 round_trippers.go:423] Response Headers:\n2017-05-03T20:49:05.218364043Z I0503 20:49:05.218215       1 round_trippers.go:426]     Content-Type: application/json\n2017-05-03T20:49:05.218367100Z I0503 20:49:05.218239       1 round_trippers.go:426]     Date: Wed, 03 May 2017 20:49:05 GMT\n2017-05-03T20:49:05.222900425Z I0503 20:49:05.222660       1 kubelet.go:235] successfully obtained stats for 8 containers\n2017-05-03T20:49:05.222926597Z I0503 20:49:05.222701       1 kubelet.go:92] Found system container /init.scope with labels: map[]\n2017-05-03T20:49:05.222933696Z I0503 20:49:05.222715       1 kubelet.go:92] Found system container /system.slice with labels: map[]\n2017-05-03T20:49:05.222938923Z I0503 20:49:05.222745       1 kubelet.go:92] Found system container /user.slice with labels: map[]\n2017-05-03T20:49:05.222944193Z I0503 20:49:05.222774       1 kubelet.go:92] Found system container /docker with labels: map[]\n2017-05-03T20:49:05.233213925Z I0503 20:49:05.232946       1 round_trippers.go:420] Response Status: 200 OK in 28 milliseconds\n2017-05-03T20:49:05.233243924Z I0503 20:49:05.232975       1 round_trippers.go:423] Response Headers:\n2017-05-03T20:49:05.233265528Z I0503 20:49:05.232988       1 round_trippers.go:426]     Content-Type: application/json\n2017-05-03T20:49:05.233275639Z I0503 20:49:05.233000       1 round_trippers.go:426]     Date: Wed, 03 May 2017 20:49:05 GMT\n2017-05-03T20:49:05.239200886Z I0503 20:49:05.239000       1 kubelet.go:235] successfully obtained stats for 8 containers\n2017-05-03T20:49:05.239214623Z I0503 20:49:05.239051       1 kubelet.go:92] Found system container /init.scope with labels: map[]\n2017-05-03T20:49:05.239217687Z I0503 20:49:05.239064       1 kubelet.go:92] Found system container /system.slice with labels: map[]\n2017-05-03T20:49:05.239220414Z I0503 20:49:05.239076       1 kubelet.go:92] Found system container /user.slice with labels: map[]\n2017-05-03T20:49:05.239223169Z I0503 20:49:05.239102       1 kubelet.go:92] Found system container /docker with labels: map[]\n2017-05-03T20:49:05.271623590Z I0503 20:49:05.271285       1 round_trippers.go:420] Response Status: 200 OK in 55 milliseconds\n2017-05-03T20:49:05.271672625Z I0503 20:49:05.271311       1 round_trippers.go:423] Response Headers:\n2017-05-03T20:49:05.271684690Z I0503 20:49:05.271323       1 round_trippers.go:426]     Content-Type: application/json\n2017-05-03T20:49:05.271693780Z I0503 20:49:05.271335       1 round_trippers.go:426]     Date: Wed, 03 May 2017 20:49:05 GMT\n2017-05-03T20:49:05.278608469Z I0503 20:49:05.278435       1 kubelet.go:235] successfully obtained stats for 8 containers\n2017-05-03T20:49:05.278631324Z I0503 20:49:05.278478       1 kubelet.go:92] Found system container /init.scope with labels: map[]\n2017-05-03T20:49:05.278636592Z I0503 20:49:05.278492       1 kubelet.go:92] Found system container /system.slice with labels: map[]\n2017-05-03T20:49:05.278639474Z I0503 20:49:05.278499       1 kubelet.go:92] Found system container /user.slice with labels: map[]\n2017-05-03T20:49:05.278642359Z I0503 20:49:05.278518       1 kubelet.go:92] Found system container /docker with labels: map[]\n2017-05-03T20:49:05.278645134Z I0503 20:49:05.278556       1 manager.go:152] ScrapeMetrics: time: 278.149543ms size: 235\n2017-05-03T20:49:05.278647879Z I0503 20:49:05.278565       1 manager.go:154]    scrape  bucket 0: 29\n2017-05-03T20:49:05.278650571Z I0503 20:49:05.278570       1 manager.go:154]    scrape  bucket 1: 0\n2017-05-03T20:49:05.278653176Z I0503 20:49:05.278575       1 manager.go:154]    scrape  bucket 2: 0\n2017-05-03T20:49:05.278655676Z I0503 20:49:05.278579       1 manager.go:154]    scrape  bucket 3: 0\n2017-05-03T20:49:05.278658218Z I0503 20:49:05.278584       1 manager.go:154]    scrape  bucket 4: 0\n2017-05-03T20:49:05.278662835Z I0503 20:49:05.278588       1 manager.go:154]    scrape  bucket 5: 0\n2017-05-03T20:49:05.278665518Z I0503 20:49:05.278599       1 manager.go:154]    scrape  bucket 6: 0\n2017-05-03T20:49:05.278668105Z I0503 20:49:05.278607       1 manager.go:154]    scrape  bucket 7: 0\n2017-05-03T20:49:05.278670614Z I0503 20:49:05.278614       1 manager.go:154]    scrape  bucket 8: 0\n2017-05-03T20:49:05.278680206Z I0503 20:49:05.278623       1 manager.go:154]    scrape  bucket 9: 0\n2017-05-03T20:49:05.278683057Z I0503 20:49:05.278630       1 manager.go:154]    scrape  bucket 10: 0\n2017-05-03T20:49:05.281485590Z I0503 20:49:05.281321       1 manager.go:113] Pushing data to: Metric Sink\n2017-05-03T20:49:05.281499022Z I0503 20:49:05.281350       1 manager.go:116] Data push completed: Metric Sink\n```. Thanks, will do another test with this value, will let you know!. /close. ",
    "andyp1per": "I have been trying to track down heapster autoscale on minikube issues all day. Heapster as an add-on is not working because of https://github.com/kubernetes/minikube/pull/1513 . However the default configuration also does not work in minikube. It partially works in that I get graphs, but autoscaling is not working. The secret to getting it working for me was to replace\n--source=kubernetes:https://kubernetes.default\nwith\n--source=kubernetes\nI have no idea what this does, but I got the latter from the minikube configuration.. It's possible this is a red herring and I'm just not waiting long enough (it seems to take at least 3mins for the hpa to recoginze the current CPU usage). I also have the issue that the hpa is never able to recover current cpu usage for a statefulset.. https://github.com/kubernetes/kubernetes/issues/46005 :(. ",
    "watercraft": "I have tried with the latest Heapster.  We will be upgrading to whatever is recommended for k8s 1.6 in the next month or so.  If it is not fixed by that version, I'll be caring my patch forward.. Just signed the CLA; can you re-run the robot?. DirectXMan12, does my change 4436e46 address your feedback from this morning?\nI don't see any text with the requested changes after that changeset.. Forgive me if I've done this wrong. Following instructions from the web I created a new branch to squash the commits and created yet another pull request from that branch.  Can you approve that one?. Commit 4436e46 on watercraft/heapster is intended to address this feedback.\nIs there more I need to do?. ",
    "wangbaifeng": "Help me please\uff01\n@miaoyq @huangyuqi . Can't get the events data. \n. ",
    "YuPengZTE": "Thanks @DirectXMan12 .. ",
    "TwitchChen": "Thanks for your reply\uff0cThe problem has been settled\uff0cthanks\uff0c@DirectXMan12. ",
    "huangjiasingle": "@mintzhao i can't see any err from your heapster's log.... @jeffery9 please check your cluster\u2018s dns is ok\uff1f . @jeffery9 @k8s-reviewable   today's morning ,i use this to create  ,it's ok, but now ,i use this in another k8s's cluster, it's doesn't work fine, i also and the ClusterRoleBinding define in heapster.yaml ,it's work fine. i can't understand.. my k8s cluster env is v1.6.3,  role and rolebinding named kube-system:heapster doesn't exsit,so l have to create it by manual.. ",
    "mintzhao": "@huangjiasingle yes, so it's confused.. @DirectXMan12 tks, I will try tomorrow. I just tried --v=5. ",
    "nefuddos": "@mintzhao how about this issue, do you solve this problem? . ",
    "foxyriver": "may be the time of hosts is not synchronous. ",
    "Asher-Shoshan": "I have same issue with kuberenetes 1.8.  \nWhat I see in a container when running \"nslookup kubernetes.default\" is this:\n;; reply from unexpected source: 10.32.0.7#53, expected 10.96.0.10#53\nSeems trying to access the dns via entrypoint ip (10.32.0.7)  and not via the service ip (10.96.0.10)\nProbably the same occurs in the  \"heapster\" container, causing the time-out.\nAny resolution to the issue?\n. No cluster issues.   Changed the line in heapster.yaml to this:\n\"        - --source=kubernetes.summary_api:https://kubernetes.default\" \nAnd it's working fine now.  . ",
    "zioproto": "I would propose to reopen this issue. I had the same problem and the change suggested from @Asher-Shoshan actually fixes the problem. @DirectXMan12 any comment on this ? thanks. I figured out that the example pulles the tag v1.4.2. If I pull the image v1.5.2 I am not able to reproduce the bug anymore. I will propose another MR to refresh containers. Sorry about the noise. It looks like that upgrading to v1.5.2 here https://github.com/kubernetes/heapster/blob/master/deploy/kube-config/influxdb/heapster.yaml#L23\nfixes the problem of issue #1648 \nI will cose this PR then.. Sure, I will close this PR and I will create a new one of v1.5.3 where I upgrade all containers. ",
    "georgebuckerfield": "Thanks @DirectXMan12, I was wondering if I should have done that. I think I'll close this PR and open a new, tidier one.. I signed it!. Is there anything else I need to do to move this along? Or is it unlikely to be merged? We're using a build of my forked version without any issues for a little while now but it would be great to switch back to the upstream build. Thanks!. No problem, @crassirostris - I'll work on moving that code this weekend, I think I have most of it in a separate branch already. Let me know if there's anything else obvious that needs looking at.. No, you're right, consistency with the rest of the code is better. I'll squash the commits when I get home tonight. Thanks for all your help with this!. @igorpeshansky thanks for the feedback! I'll re-order where it looks for the project ID and add another option to provide it explicitly via an environment variable. So we'd check in this order:\n- environment variable \n- credentials file\n- GCE metadata\nDoes that sound right?. I think that if the credentials are supplied in PK12 format, then google.DefaultClient(oauth2.NoContext, gcm.MonitoringScope) will fail earlier in the process? At least from testing it locally, I get:\nerror getting credentials using GOOGLE_APPLICATION_CREDENTIALS environment variable: invalid character '\\u0082' after top-level value\nif I point GOOGLE_APPLICATION_CREDENTIALS at a downloaded p12 file. \nBut we could still have it fall back on the metadata server if there was a problem unmarshalling the json file for any other reason. \nI'm also thinking it would make sense to move some of this out into some functions in common/gce/gce.go to keep everything a bit tidier (GetProjectIdFromEnv(), GetProjectIdFromFile(), etc). \nThanks again for taking the time to review this.. My bad, forgot to remove them.. ",
    "jiripospisil": "Just bumped into it as well and solved it by doing [1]. Maybe the step is just missing in the guide? \n[1] kubectl create -f deploy/kube-config/rbac/heapster-rbac.yaml. ",
    "ffahri": "@jiripospisil thats solves myself too thanks. check this https://github.com/kubernetes/heapster/issues/1722\nv4.0.2 \nchange to --> image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2. you can build your own image with your own config\nhttps://github.com/kubernetes/heapster/tree/master/grafana\nbe careful for dont send public docker hub\nedit:\ni cant send alets before either but i get x509 cert error.\nthen i mount /etc/ssl/certs and alerting works now. ",
    "h34dless": "solved it with that, too - but after deploying the heapster-rbac.yaml I  had to delete the heapster pod for it to be effective. ",
    "espala": "Hello, I have same problem.. How can I do this?. Hello,\nI try, but it does not succeed. Can you help me ?\nmy grafana.yaml\n```\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: monitoring-grafana\n  namespace: kube-system\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /var\n          name: grafana-storage\n        - mountPath: /etc/ssl/certs\n          name: ca-certificates\n        env:\n        - name: INFLUXDB_HOST\n          value: monitoring-influxdb\n        - name: GF_SERVER_HTTP_PORT\n          value: \"3000\"\n        - name: GF_AUTH_BASIC_ENABLED\n          value: \"false\"\n        - name: GF_AUTH_ANONYMOUS_ENABLED\n          value: \"true\"\n        - name: GF_AUTH_ANONYMOUS_ORG_ROLE\n          value: Admin\n        - name: GF_SERVER_ROOT_URL\n          value: /\n      volumes:\n      - name: grafana-storage\n        emptyDir: {}\n      - name: ca-certificates\n        hostPath:\n          path: /etc/ssl/certs\n\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: monitoring-grafana\n  name: monitoring-grafana\n  namespace: kube-system\nspec:\n  type: NodePort\n  ports:\n  - port: 3000\n    nodePort: 30078\n  selector:\n    k8s-app: grafana\n```\nlogs;\n```\nt=2017-06-01T19:57:40+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index annotation 4 v3\"\nt=2017-06-01T19:57:40+0000 lvl=info msg=\"Created default admin user: [admin]\"\nt=2017-06-01T19:57:40+0000 lvl=info msg=\"Starting plugin search\" logger=plugins\nt=2017-06-01T19:57:40+0000 lvl=warn msg=\"Plugin dir does not exist\" logger=plugins dir=/usr/share/grafana/data/plugins\nt=2017-06-01T19:57:40+0000 lvl=info msg=\"Plugin dir created\" logger=plugins dir=/usr/share/grafana/data/plugins\nt=2017-06-01T19:57:40+0000 lvl=info msg=\"Initializing CleanUpService\" logger=cleanup\nt=2017-06-01T19:57:40+0000 lvl=info msg=\"Initializing Alerting\" logger=alerting.engine\nt=2017-06-01T19:57:40+0000 lvl=info msg=\"Initializing HTTP Server\" logger=server address=0.0.0.0:3000 protocol=http subUrl=\nConnected to the Grafana dashboard.\nThe datasource for the Grafana dashboard is now set.\nt=2017-06-01T19:58:02+0000 lvl=info msg=\"Sending notification\" logger=alerting.notifier type=slack id=0 isDefault=false\nt=2017-06-01T19:58:02+0000 lvl=info msg=\"Executing slack notification\" logger=alerting.notifier.slack ruleId=0 notification=test\nt=2017-06-01T19:58:02+0000 lvl=eror msg=\"Failed to send slack notification\" logger=alerting.notifier.slack error=\"Post https://hooks.slack.com/services/XXXXXXX/kslmdfDSFKLsdfFKDLSKNM: x509: failed to load system roots and no roots provided\" webhook=test\nt=2017-06-01T20:08:24+0000 lvl=info msg=\"Sending notification\" logger=alerting.notifier type=slack id=0 isDefault=false\nt=2017-06-01T20:08:24+0000 lvl=info msg=\"Executing slack notification\" logger=alerting.notifier.slack ruleId=0 notification=test\nt=2017-06-01T20:08:24+0000 lvl=eror msg=\"Failed to send slack notification\" logger=alerting.notifier.slack error=\"Post https://hooks.slack.com/services/XXXXXXX/kslmdfDSFKLsdfFKDLSKNM: x509: failed to load system roots and no roots provided\" webhook=test\n```\npod details;\n```\n/etc/ssl/certs # hostname\nmonitoring-grafana-1063424755-7qtjn\n/etc/ssl/certs # pwd\n/etc/ssl/certs\n/etc/ssl/certs # ls -lah\ntotal 624\ndrwxr-xr-x    2 root     root        4.0K Jun  1 20:07 .\ndrwxr-xr-x    3 root     root        4.0K Jun  1 19:57 ..\n-rw-r--r--    1 root     root        2.3K Jun  1 20:07 Makefile\n-rwxrwxrwx    1 root     root      258.0K Jun  1 20:03 ca-bundle.crt\n-rwxrwxrwx    1 root     root      340.7K Jun  1 20:06 ca-bundle.trust.crt\n-rwxr-xr-x    1 root     root         611 Jun  1 20:06 make-dummy-cert\n-rwxr-xr-x    1 root     root         830 Jun  1 20:07 renew-dummy-cert\n/etc/ssl/certs # stat -c '%A %a %n' *\n-rw-r--r-- 644 Makefile\n-rwxrwxrwx 777 ca-bundle.crt\n-rwxrwxrwx 777 ca-bundle.trust.crt\n-rwxr-xr-x 755 make-dummy-cert\n-rwxr-xr-x 755 renew-dummy-cert\n```\n. Yes, finally i'm succeeded. I missed the detail, thank you for your support.\nmy temporary grafana.yaml;\n```\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: monitoring-grafana\n  namespace: kube-system\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /var\n          name: grafana-storage\n        - mountPath: /etc/ssl\n          name: ssl-folder\n        - mountPath: /etc/pki\n          name: pki-folder\n        env:\n        - name: INFLUXDB_HOST\n          value: monitoring-influxdb\n        - name: GF_SERVER_HTTP_PORT\n          value: \"3000\"\n        - name: GF_AUTH_BASIC_ENABLED\n          value: \"false\"\n        - name: GF_AUTH_ANONYMOUS_ENABLED\n          value: \"true\"\n        - name: GF_AUTH_ANONYMOUS_ORG_ROLE\n          value: Admin\n        - name: GF_SERVER_ROOT_URL\n          value: /\n      volumes:\n      - name: grafana-storage\n        emptyDir: {}\n      - name: ssl-folder\n        hostPath:\n          path: /etc/ssl\n      - name: pki-folder\n        hostPath:\n          path: /etc/pki\n\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: monitoring-grafana\n  name: monitoring-grafana\n  namespace: kube-system\nspec:\n  type: NodePort\n  ports:\n  - port: 3000\n    nodePort: 30078\n  selector:\n    k8s-app: grafana\n```. ",
    "ringtail": "pod_id is very useful if you need to find something unique. namespace_name + pod_name is not unique in historical metrics. . Every time deployment release change it's ReplicaSet and Pod and then you need to update the current pod or ReplicaSet to deployment monitor job. so if you want to get the metrics before this release, you need to get the specific data with unique id otherwise you must query the metrics with much more complex statement to avoid data obfuscation.\n. Try to build heapster with the latest golang version.. @JiangGuangxing Yes\uff0cWe tested 1.11 with 1.4.1 . /assign @andyxning. I'll check the unit tests and recreate a pr.. /assign @andyxning fix unit tests and resubmit. /assign @andyxning . @andyxning any updates,buddy?. @andyxning Hey bro! we are looking forward to use the latest merged version for a long time, please.. @piosz Could you help me to merge this pr.. @andyxning Thank you. It really helps.. /retest. some of user developers has use this eventer sink,and I am also developing the slack sink to support some overseas  developers.. @x13n Is eventer deprecated as well as Heapster\uff1f. @x13n  Metric Server can replace Heapster. But there is not any optional choice to replace eventer.. @x13n Maybe we can move eventer to metrics server as well or simply create a new repo including some other monitoring tools in kubernetes.. @kawych That sounds bad.I think eventer will live for a long time although the heapster is deprecated.Our developers use promtheus,heapster or something else as their monitoring tool. But they all use eventer as an alarm tool.so in some ways,eventer is much more useful.We have developed DingTalkSink and AlicloudSink to support the scenarios.\ud83d\ude22 . /assign @andyxning. > @ringtail Sorry for the late. You need to also run make fmt because make sanitize is reporting error.\nyes,that's really strange.I have run make fmt and make sanitize locally. but there is nothing warning.\n\u279c  heapster git:(feature/fix-comments) make fmt\nfind . -type f -name \"*.go\" | grep -v \"./vendor*\" | xargs gofmt -s -w\n\u279c  heapster git:(feature/fix-comments) \u2717 make sanitize\nhooks/check_boilerplate.sh\nhooks/check_gofmt.sh\nhooks/run_vet.sh\n\u279c  heapster git:(feature/fix-comments) \u2717. if set dbExists to true, then the unittest will try to create a database and failed.. If create database with name \"default\",then you can never create a retention policy with name default.. Sorry bro.. ",
    "bluishpe": "when I try:\nwget http://monitoring-influxdb:8086/ping\n--2017-05-21 03:44:59--  http://monitoring-influxdb:8086/ping\nResolving web-proxy.XXX.net (web-proxy.XXX.net )... 16.XXX.XX.210\nConnecting to web-proxy.XXX.net (web-proxy.XXX.net)|16.XXX.XX.210|:8080... connected.\nProxy request sent, awaiting response... 404 Not Found\n2017-05-21 03:45:00 ERROR 404: Not Found.\n. You should check and confirm that you setting up an influxdb instance correctly.\n          Do you know what I should be checking exactly?\nThanks!. do you have the steps for me to verify the setup correctly?\nMy heapster has the following config:\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: heapster\n  namespace: kube-system\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes.summary_api:''\n        - --sink=influxdb:http://monitoring-influxdb:8086\n\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    task: monitoring\n    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)\n    # If you are NOT using this as an addon, you should comment out this line.\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: Heapster\n  name: heapster\n  namespace: kube-system\nspec:\n  ports:\n    - port: 80\n      targetPort: 8082\n      nodePort: 30310\n  selector:\n    k8s-app: heapster\n  type: NodePort\n```. Can you tell me the exact working version of below so that I can reconfigure the setup:\nKubernetes version\nheapster\ngrafana\ninfluxdb\nflannel\nkube-dns. Yes, have deployed in a kuberneter cluster, version 1.5.4\nwhat should I use instead of kube-dns? do we have another option?. even without port its the same error:\nwget http://monitoring-influxdb/ping\n--2017-05-21 08:07:46--  http://monitoring-influxdb/ping\nResolving web-proxy.XXX.net (web-proxy.XXX.net)... 16.XXXX.28.210\nConnecting to web-proxy.XXX.net (web-proxy.XXX.net)|16.XXX.210|:8080... connected.\nProxy request sent, awaiting response... 404 Not Found\n2017-05-21 08:07:47 ERROR 404: Not Found.\n. Can I edit /etc/hosts of the heapster to include ip and name of influxdb so that ping is successful between them?. influxDB yaml\n```yaml\nkind: Deployment\nmetadata:\n  name: monitoring-influxdb\n  namespace: kube-system\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: influxdb\n    spec:\n      containers:\n      - name: influxdb\n        image: gcr.io/google_containers/heapster_influxdb:v0.7\n        volumeMounts:\n        - mountPath: /data\n          name: influxdb-storage\n      volumes:\n      - name: influxdb-storage\n        emptyDir: {}\n\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    task: monitoring\n    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)\n    # If you are NOT using this as an addon, you should comment out this line.\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: monitoring-influxdb\n  name: monitoring-influxdb\n  namespace: kube-system\nspec:\n  ports:\n  - name: http\n    port: 8083\n    targetPort: 8083\n    nodePort: 30321\n    protocol: TCP\n  - name: api\n    port: 8086\n    targetPort: 8086\n    nodePort: 30322\n    protocol: TCP\n  selector:\n    k8s-app: influxdb\n  type: NodePort\n```. Same error\nport 30322 is for external connectivity. However here it seems like with pods there is no communication. Heapster should communicate to influxdb via 8086...please correct me if I am wrong.\nwget http://monitoring-influxdb:30322/ping\n--2017-05-21 08:25:31--  http://monitoring-influxdb:30322/ping\nResolving web-proxy.XXXX.net (web-proxy.XXXX.net)... 16.XX.XX.210\nConnecting to web-proxy.XXXX.net (web-proxy.XXXX.net)|16.XX.XX.210|:8080... connected.\nProxy request sent, awaiting response... 404 Not Found\n2017-05-21 08:25:32 ERROR 404: Not Found.. Is this the right way to execute within heapster pod?\nkubectl --namespace=kube-system exec heapster-1865471235-7sbfx -- wget http://monitoring-influxdb:8086/ping\nwget: bad address 'monitoring-influxdb:8086'\nkubectl --namespace=kube-system exec heapster-1865471235-7sbfx -- wget http://monitoring-influxdb:30322/ping\nwget: bad address 'monitoring-influxdb:30322'\n. 16.xx is the proxy within our company to access outside internet from that node.. Here is the output:\nkubectl --namespace=kube-system exec heapster-1865471235-7sbfx -- cat /etc/resolv.conf\nsearch kube-system.svc.cluster.local svc.cluster.local cluster.local hpeswlab.net\nnameserver 10.96.0.10\noptions ndots:5\n. I get this output:\nkubectl --namespace=kube-system exec -ti heapster-1865471235-7sbfx -- nslookup kubernetes.default\nServer:    (null)\nAddress 1: ::1 localhost\nAddress 2: 127.0.0.1 localhost\nnslookup: can't resolve 'kubernetes.default': Name does not resolve\nkubectl get pods --namespace=kube-system -l k8s-app=kube-dns\nNAME                        READY     STATUS    RESTARTS   AGE\nkube-dns-2924299975-gzhj9   4/4       Running   0          18h\nkubectl get ep kube-dns --namespace=kube-system\nNAME       ENDPOINTS                     AGE\nkube-dns   10.244.0.4:53,10.244.0.4:53   18h\n. Any suggestion to check kube-dns.\nIs there an alternative to kube-dns that I can use in my setup?. Yes it running:\nkubectl get services --all-namespaces\nNAMESPACE     NAME                  CLUSTER-IP       EXTERNAL-IP   PORT(S)                         AGE\ndefault       kubernetes            10.96.0.1                443/TCP                         19h\nkube-system   heapster              10.96.203.201           80:30310/TCP                    3h\nkube-system   kube-dns              10.96.0.10               53/UDP,53/TCP                   19h\nkube-system   monitoring-grafana    10.110.164.244          80:30320/TCP                    3h\nkube-system   monitoring-influxdb   10.104.155.62           8083:30321/TCP,8086:30322/TCP   3h\nI don't see any errors in the logs for dns pod.\n. ",
    "loburm": "Thanks for the comments, I have updated PR description and commit title.\ncc @timstclair . With #1708 merged, I'm closing this issue.. It's a duplicate of #1709 \nShould be already resolved by new release.. /assign @piosz. All comments have been resolved.. Thanks a lot!. @piosz do you have any other comments?. Fix #1697.. /assign @loburm. Sorry for the delay, I'm looking at this problem now. I will post update as soon as will find a root cause of issue.. This commit on the side of grafana is the root cause of the issue:\nhttps://github.com/grafana/grafana/commit/b191638f416b33451113bc4e8b95085cdf09cf64\nShortly: previously they have been always creating grafana.ini file but in January this part was moved to the postinst script which is executed only when we actually install package on the system (in our case we just extracting files, so script is not executed).\nI'll try to find workaround. Maybe we can use official grafana image for this.. Unfortunately, solution with docker image officially provided by the grafana is not going to work. This image is built only for amd64 (we support different arch) and image size almost 2 times bigger.\nSo I have prepared PR that just manually copies configuration files to /etc/grafana/: https://github.com/kubernetes/heapster/pull/1728. @ffahri this is not a fix of problem. Some users need features that have been released in 4.2.0 or later we just can't stuck with version 4.0.2 forever.. PR was merged. Tomorrow I'll push a new image to the gcr.. Image is pushed. In #1731 updates deploy file.. @discostur thanks for checking it.\nConsidering that last PR was merged, I'm closing this issue.. /assign @piosz . Fixes #1709 . Updated version of grafana to 4.4.1.. Hi @adambkaplan \nI have built this image. Everything looks fine (I haven't received any warnings from compiler). Do you know about any other problems that could affect the image?. /assign @piosz . /lgtm. /lgtm. This should be fixed by #1822. Soon we will cherrypick 1.4 release and in the week it's going to released (both heapster and new kubernetes version).. /test pull-heapster-e2e. /lgtm for last commit.. /lgtm. Please don't review it yet. It may require one more change.. /test pull-heapster-e2e. /lgtm. I'll try to patch and prepare a new release soon.. @piosz should we first apply your fix? I see that it's only on 1.4 branch not on the master: https://github.com/kubernetes/heapster/pull/1790. @mikebryant could you add to your fix #1790 commits? It should simplify your code.. actually now I don't see nil checks at all.. Sorry haven't noticed. Thanks for the tests.\n/lgtm. I also can't reopen this PR... \nLooks like some issue with k8s-ci-robot. It shouldn't close PR in one repo if they have been referenced from the other.. @andyxning don't you mind if I remove your comment with reference to other PR (just a random idea). Thanks for checking. I have created issue that describes this issue.. @andyxning is there any chance to split this PR into smaller chunks? Or all changes are dependent on each other?. OK, I see could you please resolve conflicts first and then I'll check what we can do.. @andyxning I think that we also need to change supported_kube_version in Makefile.. /lgtm. /lgtm\n@piosz could you have a final look?. Karol you can cherrypick just a first part of that PR, without tests. Then it should build.. /lgtm. /lgtm. /lgtm. What will happen if some users depend on this metrics?. Is there any way to warn users about removal of this metrics? I don't want to break people that depend on this metrics without giving them some time to migrate to other metrics.. Just not sure if it's not going to be better UX if we first explicitly deprecate it (in next release notes). And only after some short amount of time (1-2 month) remove it?. @DirectXMan12 do you propose to remove it in 1.10, or just mark as deprecated and remove in 1.11?. /lgtm. /lgtm\n@piosz could you also take a look?. /lgtm. @DirectXMan12 if you don't have any suggestions, can I merge this PR?. /lgtm. Before proceeding you need to sign CLA: https://github.com/kubernetes/community/blob/master/contributors/guide/README.md#sign-the-cla. /lgtm. Can anyone confirm that those images have been in the registry before?\nJust wonder if someone just forget to push them, or they have pushed and then removed from the registry.. @mbert thanks, I'll check why those images have been removed from k8s.gcr.io, and will try to revert it.. Unfortunately I was not able to find any logs, that would explain why those images have disappeared. To fix an issue, I'll build new images and push them.\nI'll post an update once they are available.. I have repushed images, they should be available now:\n$ docker pull k8s.gcr.io/heapster-grafana-amd64:v5.0.4\n$ docker pull k8s.gcr.io/heapster-influxdb-amd64:v1.5.2\nSorry for the inconvenience.. @kawych is working on releasing this image, it should be available soon.. /lgtm. /lgtm. /approve. /lgtm\n/approve. /lgtm. /approve. /lgtm. /lgtm\n/approve. /lgtm\n/approve. Comment was added.. Thanks, like that it's much better.. Thanks for the comment. I have tried to include all information in the log that could help to debug issue.\nCould you check it once again?. Thanks for the comment, it really makes sense. I think checking of CPU and Memory usage should be good enough heuristic.\nI have updated a code and test.. core.MetricMemoryWorkingSet is a metric that we fetch from kubelet. At the same time bytes_used (and minor_page_faults) are created manually in preprocessMemoryMetrics function.. this import should go into the first block. what about moving this part into separate function? It's repeated 3 times, and I can imagine that someone would be happy to reuse it in the future.. what about:\nrxErrors := uint64(0). let's make only one conversion below:\nIntValue: int64(rxErrors). I assume that sometimes it just not capable to figure out correct order and rules (in kubernetes you should split them in three categories: system, open source, internal).. I'm not sure if a lot of users actually using hyperkube, is it not enough information from kubectl version?. I think the same change has to be made to TranslateLabeledMetrics method, @kawych is it correct?. It would be great to add a bit more information about units of this metric, because I had to check cadvisor documentation to understand that it is actually multiplied by 1000.. Could you add comments here and below?. I think it's OK to keep it in this PR. Let's not try to overcomplicate it.. Error supposed to do the same. Is it not working?. why not to use 1.10.5, or even 1.11.0 (that is the last version for which we are going to support heapster)?. Makes, sense. Thanks.. could you write it as:\nvar clusterName string\nthis is more go native I would say. I would include error message in this warning.. Shouldn't we perform whole check inside gce.OnGCE block below?. The same, shouldn't it be a part of gce.OnGCE() block?. There is no sense in introducing additional variable.\nInstanceAttributeValue is going to return empty string in case of error. So this could be simplified to:\nclusterName, err = gce.InstanceAttributeValue(\"cluster-name\"). Is heapsterZone required in case of newResourceModel? Maybe we shouldn't return error in such case?. Same here, you don't need additional variable.. ",
    "andrey01": "@DirectXMan12 the other pods are working properly on our cluster, we are running many different ones.\nI have just tried the heapster v1.4.0 with K8s v1.7.5 and it has the same issue.\nI am nearly out of ideas, will have to dig deeper probably.\nMeanwhile, if you have some ideas what could I try, let me know please.\nUpdate 1\nThis is the way we run the heapster, kubectl apply the following manifests:\n1. https://github.com/kubernetes/heapster/blob/1308dd71f0ba343895456b46d1bbf3238800b6f3/deploy/kube-config/rbac/heapster-rbac.yaml\n2. https://github.com/kubernetes/heapster/blob/480a1ca21fe65882dd41f28cd9296a825fbbb602/deploy/kube-config/influxdb/heapster.yaml\n3. https://github.com/kubernetes/heapster/blob/480a1ca21fe65882dd41f28cd9296a825fbbb602/deploy/kube-config/influxdb/influxdb.yaml\n4. https://github.com/kubernetes/heapster/blob/69a4067c236032770c9145bba6634795454b88b1/deploy/kube-config/influxdb/grafana.yaml\nUpdate 2\nI have just double checked it in K8s 1.7.5, CentOS 7.3, docker 1.13.1 for all the available heapster docker images, so results are following:\nv1.3.0-alpha.1, v1.3.0-beta.1 are working:\n```\nkubectl -n kube-system logs --tail=1 -f heapster-1029728424-jnd84\nI0915 13:44:59.804446       5 heapster.go:105] Starting heapster on port 8082\nI0915 13:45:05.117265       5 influxdb.go:215] Created database \"k8s\" on influxDB server at \"monitoring-influxdb.kube-system.svc:8086\"\n```\nv1.3.0, v1.4.0-beta.0, v1.4.0, v1.4.1, v1.4.2, v1.5.0-beta.0 are not.\nmd5sum binary_path version commit\n26a76e1037c987766c480ae07a713a07  /mnt/docker/overlay/e6068f9abe9d3f6935d7555be29c75d6417f44131254bcd877c8d126ad747e26/root/heapster v1.3.0-alpha.1 da3aab91\nca883b67c48fcedb7d69165e78edf9f0  /mnt/docker/overlay/3f7ce0183025b48aa926caee676eebb5de6993e99c1064d9cf70139875e66d43/root/heapster v1.3.0-beta.1 4bf87de3\n--- below version are not working for me ---\n01e20c88c736070d57392ddcf0efc1ae  /mnt/docker/overlay/044ba8b80c29fea0c4c8afb88a272c1fa735dd0a1c41ed51fa53a36ae799053e/root/heapster v1.3.0 abbfe6e0\n3886e3f3b4f2b843e08067fd92a2bca1  /mnt/docker/overlay/a1b912c5ea9b6e63579e187f169fb336002bd9bcf9c0ffbc8b5e8fbf0a975afe/root/heapster v1.4.0-beta.0 a729275e\n038f85fd510a45dab4bbe4cfe547b0b8  /mnt/docker/overlay/92b867dd8f82beb48bbefda7e20557ffc4d5f85c7a2b5c99e4cf13b4e7c75865/root/heapster v1.4.0 0f6d09f4\n22a3cff26dfcfef5674c9d51336f9248  /mnt/docker/overlay/d52c27f15b0e053b8fbe9f069f66194fd1858be5d37dee6468e29951f1fbda4b/root/heapster v1.4.1 d7df1c0f\n53b67e2f14099c20520bfae80a087977  /mnt/docker/overlay/04bf13900b2a372be91f3e5af6069f18c8e0ab15ac88d766e8e01bac770977f1/root/heapster v1.4.2 ae12a18a\n191700a644713f0e33d1fc97f961a79c  /mnt/docker/overlay/37ea86525baca321fc1c118a3bf34d07be142b91ad5021e671ee3463fc262153/root/heapster v1.5.0-beta.0 859ccd72\nSo I guess either issue or its trigger lies somewhere between git log 4bf87de3...abbfe6e0 --stat -p.\nUpdate 3\nWould it make sense updating the vendor/k8s.io/apimachinery ? As on July 28 2017 IPv6 support for ChooseHostInterface was added (part 3 of 3) in https://github.com/kubernetes/apimachinery/commit/e97046a677c003086fbaac6222ae330b6cee12e8\n. @andyxning \n\nNo special settings, we are using weave-net 2.0.4 at the moment;\nas of /etc/resolv.conf please refer to the below outputs I have just prepared:\n\nAdding a CentOS container\n\nUseful for additional debugging, since the container will share the same namespace.\n\nSlighlty modified the https://github.com/kubernetes/heapster/blob/480a1ca21fe65882dd41f28cd9296a825fbbb602/deploy/kube-config/influxdb/heapster.yaml to include a centos container running in the same namespace as a heapster.\nspec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: gcr.io/google_containers/heapster-amd64:v1.3.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086\n      - name: centos\n        image: centos:7\n        imagePullPolicy: IfNotPresent\n        command:\n        - sleep\n        - infinity\nnamespace is really shared\nJust to make sure:\n```\n[root@myserver1:~] docker ps -a |grep -E \"centos|heapster\"\n488d43ce3e83        centos@sha256:eba772bac22c86d7d6e72421b4700c3f894ab6e35475a34014ff8de74c10872e                                                                          \"sleep infinity\"         8 minutes ago       Up 8 minutes                                       k8s_centos_heapster-107544395-wzx51_kube-system_976900bf-9a21-11e7-bc9b-fa163e533806_0\n4047d471bcc6        gcr.io/google_containers/heapster-amd64@sha256:3dff9b2425a196aa51df0cebde0f8b427388425ba84568721acf416fa003cd5c                                         \"/heapster --sourc...\"   8 minutes ago       Up 8 minutes                                       k8s_heapster_heapster-107544395-wzx51_kube-system_976900bf-9a21-11e7-bc9b-fa163e533806_0\n[root@myserver1:~] docker inspect k8s_heapster_heapster-107544395-wzx51_kube-system_976900bf-9a21-11e7-bc9b-fa163e533806_0 --format '{{.State.Pid}} - {{.HostConfig.PidMode}}'\n32341 - container:011aab52b74600416ac3cfba43e33c372b8c00b1e42a6f1ae892603cd5ce8f5c\n[root@myserver1:~] docker inspect k8s_centos_heapster-107544395-wzx51_kube-system_976900bf-9a21-11e7-bc9b-fa163e533806_0 --format '{{.State.Pid}} - {{.HostConfig.PidMode}}'\n32407 - container:011aab52b74600416ac3cfba43e33c372b8c00b1e42a6f1ae892603cd5ce8f5c\n[root@myserver1:~] ps -ef|grep -Ew \"32341|32407\"\nnfsnobo+ 32341 32324  0 16:24 ?        00:00:03 /heapster --source=kubernetes:https://kubernetes.default --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086\nroot     32407 32383  0 16:24 ?        00:00:00 sleep infinity\n```\nGetting into a container\n[root@myserver1:~] kubectl get pods --all-namespaces -o wide |grep -i heap\nkube-system     heapster-107544395-wzx51                            2/2       Running   0          1m        10.32.0.3      myserver1\n[root@myserver1:~] kubectl -n kube-system exec -ti -c centos heapster-107544395-wzx51 bash\n[root@heapster-107544395-wzx51 /]# ps -ef\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot         1     0  0 14:24 ?        00:00:00 /pause\n65534        5     0  0 14:24 ?        00:00:00 /heapster --source=kubernetes:https://kubernetes.default --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086\nroot        13     0  0 14:24 ?        00:00:00 sleep infinity\nroot        20     0  0 14:26 ?        00:00:00 bash\nroot        24     0  0 14:26 ?        00:00:00 bash\nroot        38    24  0 14:26 ?        00:00:00 ps -ef\nK8s API via IPv4 OK\n[root@heapster-107544395-wzx51 /]# TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\n[root@heapster-107544395-wzx51 /]# curl -sS -k -H \"Authorization: Bearer ${TOKEN}\"  https://kubernetes.default/api/v1/nodes?resourceVersion=0 |head\n{\n  \"kind\": \"NodeList\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"selfLink\": \"/api/v1/nodes\",\n...\nIPv6 disabled\nAs expected\n[root@heapster-107544395-wzx51 /]# ping6 kubernetes.default\nping: kubernetes.default: Name or service not known\nDNS OK\n```\n[root@heapster-107544395-wzx51 /]# yum install -y net-tools bind-utils\n[root@heapster-107544395-wzx51 /]# nslookup kubernetes.default\nServer:     10.96.0.10\nAddress:    10.96.0.10#53\nNon-authoritative answer:\nName:   kubernetes.default.svc.cluster.local\nAddress: 10.96.0.1\n```\nresolv.conf\n[root@heapster-107544395-wzx51 /]# cat /etc/resolv.conf \nnameserver 10.96.0.10\nsearch kube-system.svc.cluster.local svc.cluster.local cluster.local novalocal int.na.intgdc.com na.intgdc.com\noptions ndots:5\n. @andyxning \n\nNo special settings, we are using weave-net 2.0.4 at the moment;\nas of /etc/resolv.conf please refer to the below outputs I have just prepared:\n\nAdding a CentOS container\n\nUseful for additional debugging, since the container will share the same namespace.\n\nSlighlty modified the https://github.com/kubernetes/heapster/blob/480a1ca21fe65882dd41f28cd9296a825fbbb602/deploy/kube-config/influxdb/heapster.yaml to include a centos container running in the same namespace as a heapster.\nspec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: gcr.io/google_containers/heapster-amd64:v1.3.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086\n      - name: centos\n        image: centos:7\n        imagePullPolicy: IfNotPresent\n        command:\n        - sleep\n        - infinity\nnamespace is really shared\nJust to make sure:\n```\n[root@myserver1:~] docker ps -a |grep -E \"centos|heapster\"\n488d43ce3e83        centos@sha256:eba772bac22c86d7d6e72421b4700c3f894ab6e35475a34014ff8de74c10872e                                                                          \"sleep infinity\"         8 minutes ago       Up 8 minutes                                       k8s_centos_heapster-107544395-wzx51_kube-system_976900bf-9a21-11e7-bc9b-fa163e533806_0\n4047d471bcc6        gcr.io/google_containers/heapster-amd64@sha256:3dff9b2425a196aa51df0cebde0f8b427388425ba84568721acf416fa003cd5c                                         \"/heapster --sourc...\"   8 minutes ago       Up 8 minutes                                       k8s_heapster_heapster-107544395-wzx51_kube-system_976900bf-9a21-11e7-bc9b-fa163e533806_0\n[root@myserver1:~] docker inspect k8s_heapster_heapster-107544395-wzx51_kube-system_976900bf-9a21-11e7-bc9b-fa163e533806_0 --format '{{.State.Pid}} - {{.HostConfig.PidMode}}'\n32341 - container:011aab52b74600416ac3cfba43e33c372b8c00b1e42a6f1ae892603cd5ce8f5c\n[root@myserver1:~] docker inspect k8s_centos_heapster-107544395-wzx51_kube-system_976900bf-9a21-11e7-bc9b-fa163e533806_0 --format '{{.State.Pid}} - {{.HostConfig.PidMode}}'\n32407 - container:011aab52b74600416ac3cfba43e33c372b8c00b1e42a6f1ae892603cd5ce8f5c\n[root@myserver1:~] ps -ef|grep -Ew \"32341|32407\"\nnfsnobo+ 32341 32324  0 16:24 ?        00:00:03 /heapster --source=kubernetes:https://kubernetes.default --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086\nroot     32407 32383  0 16:24 ?        00:00:00 sleep infinity\n```\nGetting into a container\n[root@myserver1:~] kubectl get pods --all-namespaces -o wide |grep -i heap\nkube-system     heapster-107544395-wzx51                            2/2       Running   0          1m        10.32.0.3      myserver1\n[root@myserver1:~] kubectl -n kube-system exec -ti -c centos heapster-107544395-wzx51 bash\n[root@heapster-107544395-wzx51 /]# ps -ef\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot         1     0  0 14:24 ?        00:00:00 /pause\n65534        5     0  0 14:24 ?        00:00:00 /heapster --source=kubernetes:https://kubernetes.default --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086\nroot        13     0  0 14:24 ?        00:00:00 sleep infinity\nroot        20     0  0 14:26 ?        00:00:00 bash\nroot        24     0  0 14:26 ?        00:00:00 bash\nroot        38    24  0 14:26 ?        00:00:00 ps -ef\nK8s API via IPv4 OK\n[root@heapster-107544395-wzx51 /]# TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\n[root@heapster-107544395-wzx51 /]# curl -sS -k -H \"Authorization: Bearer ${TOKEN}\"  https://kubernetes.default/api/v1/nodes?resourceVersion=0 |head\n{\n  \"kind\": \"NodeList\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"selfLink\": \"/api/v1/nodes\",\n...\nIPv6 disabled\nAs expected\n[root@heapster-107544395-wzx51 /]# ping6 kubernetes.default\nping: kubernetes.default: Name or service not known\nDNS OK\n```\n[root@heapster-107544395-wzx51 /]# yum install -y net-tools bind-utils\n[root@heapster-107544395-wzx51 /]# nslookup kubernetes.default\nServer:     10.96.0.10\nAddress:    10.96.0.10#53\nNon-authoritative answer:\nName:   kubernetes.default.svc.cluster.local\nAddress: 10.96.0.1\n```\nresolv.conf\n[root@heapster-107544395-wzx51 /]# cat /etc/resolv.conf \nnameserver 10.96.0.10\nsearch kube-system.svc.cluster.local svc.cluster.local cluster.local novalocal int.na.intgdc.com na.intgdc.com\noptions ndots:5\n. #### To continue\ntcpdump\nApparently heapster is constantly trying to use the IPv6, despite the fact it is not working.\nI think hepaster should try then IPv4 instead, unless something is broken within the hepaster container inside.\n@andyxning is there a way to see what /etc/resolv.conf uses the heapster container itself? I've noticed that it is pretty slim and does not have sh / bash / cat tools inside.  Edit okay I found it, see below in this comment. :-)\n[root@heapster-107544395-wzx51 /]# yum install -y iproute tcpdump\n[root@heapster-107544395-wzx51 /]# tcpdump -qenn -i any \ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes\n14:45:35.489651  In 00:00:00:00:00:00 127.0.0.1.51835 > 127.0.0.1.53: UDP, length 36\n14:45:35.489672  In 00:00:00:00:00:00 127.0.0.1 > 127.0.0.1: ICMP 127.0.0.1 udp port 53 unreachable, length 72\n14:45:35.489788  In 00:00:00:00:00:00 ::1.49641 > ::1.53: UDP, length 36\n14:45:35.489800  In 00:00:00:00:00:00 ::1 > ::1: ICMP6, destination unreachable, unreachable port, ::1 udp port 53, length 92\n14:45:35.489902  In 00:00:00:00:00:00 127.0.0.1.43764 > 127.0.0.1.53: UDP, length 36\n14:45:35.489908  In 00:00:00:00:00:00 127.0.0.1 > 127.0.0.1: ICMP 127.0.0.1 udp port 53 unreachable, length 72\n14:45:35.489944  In 00:00:00:00:00:00 127.0.0.1.38169 > 127.0.0.1.53: UDP, length 36\n14:45:35.877095  In 00:00:00:00:00:00 127.0.0.1.46479 > 127.0.0.1.53: UDP, length 36\n14:45:35.877095  In 00:00:00:00:00:00 127.0.0.1.38827 > 127.0.0.1.53: UDP, length 36\n14:45:35.877110  In 00:00:00:00:00:00 127.0.0.1 > 127.0.0.1: ICMP 127.0.0.1 udp port 53 unreachable, length 72\n14:45:35.877110  In 00:00:00:00:00:00 127.0.0.1 > 127.0.0.1: ICMP 127.0.0.1 udp port 53 unreachable, length 72\nUpdate 1\nIPv6 isn't apparently fully disabled, from the container's PoV:\n[root@heapster-107544395-wzx51 /]# ping6 ::1\nPING ::1(::1) 56 data bytes\n64 bytes from ::1: icmp_seq=1 ttl=64 time=0.034 ms\n64 bytes from ::1: icmp_seq=2 ttl=64 time=0.033 ms\n^C\n--- ::1 ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 999ms\nrtt min/avg/max/mdev = 0.033/0.033/0.034/0.005 ms\n[root@heapster-107544395-wzx51 /]# ping6 localhost\nPING localhost(localhost (::1)) 56 data bytes\n64 bytes from localhost (::1): icmp_seq=1 ttl=64 time=0.032 ms\n64 bytes from localhost (::1): icmp_seq=2 ttl=64 time=0.459 ms\n^C\n--- localhost ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 1000ms\nrtt min/avg/max/mdev = 0.032/0.245/0.459/0.214 ms\nWhilst on the host it is:\n\ndue to a Host's /etc/hosts does not have the following line set:\n::1       localhost ip6-localhost ip6-loopback\n\nEdit but having it there and restarting a dockerd, recreating the heapster container, did not help.\n[root@myserver1:~] ping6 localhost\nping: localhost: Name or service not known\n\nThis is due net.ipv6.conf.all.disable_ipv6=1:\n\n[root@myserver1:~] ping6 ::1\nPING ::1(::1) 56 data bytes\nping: sendmsg: Network is unreachable\nping: sendmsg: Network is unreachable\n^C\nUpdate 2\nOkay here is the /etc/resolv.conf from the heapster container\n```\n[root@myserver1:~] docker inspect k8s_centos_heapster-107544395-wzx51_kube-system_976900bf-9a21-11e7-bc9b-fa163e533806_0 --format '{{.ResolvConfPath}}'\n/mnt/docker/containers/011aab52b74600416ac3cfba43e33c372b8c00b1e42a6f1ae892603cd5ce8f5c/resolv.conf\n[root@myserver1:~] cat /mnt/docker/containers/011aab52b74600416ac3cfba43e33c372b8c00b1e42a6f1ae892603cd5ce8f5c/resolv.conf\nnameserver 10.96.0.10\nsearch kube-system.svc.cluster.local svc.cluster.local cluster.local novalocal int.na.ourdomainXYZ.com na.ourdomainXYZ.com\noptions ndots:5\n```\nAnd the /etc/hosts\n[root@myserver1:~] cat /mnt/docker/containers/011aab52b74600416ac3cfba43e33c372b8c00b1e42a6f1ae892603cd5ce8f5c/hosts\n127.0.0.1   localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n\njust to double-ensure we are checking the right path\n\n[root@myserver1:~] cat /mnt/docker/containers/011aab52b74600416ac3cfba43e33c372b8c00b1e42a6f1ae892603cd5ce8f5c/hostname\nheapster-107544395-wzx51\nUpdate 3\n\ntcpdump'ing on the host\n\nThe failed IPv6 attempts ::1 > ::1: ICMP6, destination unreachable, unreachable port, ::1 udp port 53 seen inside a heapster POD (from a CentOS container), are never reaching outside to the host. So they are stopped within the heapster POD without going anywhere outside.\nUpdate 4\nTried stracing the hepaster, according to what it shows, it appears it is trying to ask a local DNS over a 53/UDP, first IPv4 (AF_INET) 127.0.0.1, then IPv6 (AF_INET6) ::1 every second \nhttps://gist.github.com/andrey01/936a5577ecf9aaa8af4a66570cc8f5cc\nAnd all attempts fail.\ntcpdump with more details -AA shows that it is asking for kubernetes.default\n[root@heapster-107544395-wzx51 /]# tcpdump -qenn -i any -AA\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes\n15:23:25.505865  In 00:00:00:00:00:00 127.0.0.1.51113 > 127.0.0.1.53: UDP, length 36\n................E..@\".@.@..............5.,.?M...........\nkubernetes.default.....................\n15:23:25.505877  In 00:00:00:00:00:00 127.0.0.1 > 127.0.0.1: ICMP 127.0.0.1 udp port 53 unreachable, length 72\n................E..\\>...@.=P...........\\....E..@\".@.@..............5.,.?M...........\nkubernetes.default.....................\n15:23:25.505964  In 00:00:00:00:00:00 ::1.59300 > ::1.53: UDP, length 36\n................`....,.@...................................5.,.?............\nkubernetes.default.....................\n15:23:25.505975  In 00:00:00:00:00:00 ::1 > ::1: ICMP6, destination unreachable, unreachable port, ::1 udp port 53, length 92\n................`....\\:@...................................Y....`....,.@...................................5.,.?............\nkubernetes.default.....................\n15:23:25.506049  In 00:00:00:00:00:00 127.0.0.1.49516 > 127.0.0.1.53: UDP, length 36\n................E..@\".@.@............l.5.,.?m...........\nkubernetes.default...................... We are running a dockerd with --iptables=false, I have been also trying to set it to true, restarting the dockerd but that did not do any difference. Neither have helped having the ::1 localhost ip6-localhost ip6-loopback in /etc/hosts  on the main host.. @andyxning here you go:\nnslookup\n```\n[root@heapster-487402730-frx1w /]# nslookup monitoring-influxdb.kube-system.svc\nServer:     10.96.0.10\nAddress:    10.96.0.10#53\nName:   monitoring-influxdb.kube-system.svc.cluster.local\nAddress: 10.102.222.93\n[root@heapster-487402730-frx1w /]# nslookup monitoring-influxdb.kube-system.svc.cluster.local.\nServer:     10.96.0.10\nAddress:    10.96.0.10#53\nNon-authoritative answer:\nName:   monitoring-influxdb.kube-system.svc.cluster.local\nAddress: 10.102.222.93\n```\nresolv.conf\nheapster POD\n```\n[root@myserver1:~] docker inspect k8s_heapster_heapster-487402730-frx1w_kube-system_1ad7d84a-9bc3-11e7-b167-fa163e533806_0 |grep -i resolv\n        \"ResolvConfPath\": \"/mnt/docker/containers/b66a55cc326bc07635fb71dde2138f3eddc95d672bd0c037b6b74926d5804709/resolv.conf\",\n[root@myserver1:~] cat /mnt/docker/containers/b66a55cc326bc07635fb71dde2138f3eddc95d672bd0c037b6b74926d5804709/resolv.conf\nnameserver 10.96.0.10\nsearch kube-system.svc.cluster.local svc.cluster.local cluster.local novalocal int.na.mydomainXYZ.com na.mydomainXYZ.com\noptions ndots:5\n```\nfrom centos container which is part of heapster POD\n[root@myserver1:~] kubectl -n kube-system exec -ti -c centos heapster-487402730-frx1w bash\n[root@heapster-487402730-frx1w /]# ps -ef\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot         1     0  0 16:13 ?        00:00:00 /pause\n65534        7     0  0 16:13 ?        00:00:00 /heapster --source=kubernetes:https://kubernetes.default --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086\nroot        15     0  0 16:13 ?        00:00:00 sleep infinity\nroot        21     0  0 16:14 ?        00:00:00 bash\nroot        37    21  0 16:14 ?        00:00:00 ps -ef\n[root@heapster-487402730-frx1w /]# cat /etc/resolv.conf \nnameserver 10.96.0.10\nsearch kube-system.svc.cluster.local svc.cluster.local cluster.local novalocal int.na.mydomainXYZ.com na.mydomainXYZ.com\noptions ndots:5. I think the problem is that the heapster is trying to perform a DNS query over 127.0.0.1 or ::1 instead of reading the nameserver 10.96.0.10 from the /etc/resolv.conf. tcpdump confirms this from both, the container network and the VM host network.\nUpdate 1\nI can see there were many changes starting with the heapster 1.3.0, not sure whether this could be related, probably not.. nevertheless, just to mention, ResolvConfDefault = \"/etc/resolv.conf\" was deleted in https://github.com/kubernetes/heapster/pull/1537/files#diff-c2a7e6ffb19bafecc85ea9765043d927L21\nUpdate 2\n@andyxning \n\nIIUC, when query an dns, client should not query 127.0.0.1:53, it should consult nameservers directly. Seems something is wrong.\n\nYes, absolutely. So far we have experienced this issue only with the heapster starting from v1.3.0 and higher.\nvia /etc/resolv.conf\nIt normally queries the nameserver found in /etc/resolv.conf as expected - 10.96.0.10:53 (kube-dns).\n[root@heapster-487402730-frx1w /]# dig A monitoring-influxdb.kube-system.svc.cluster.local. | grep -EA1 'ANSWER SECTION|SERVER'\n;; ANSWER SECTION:\nmonitoring-influxdb.kube-system.svc.cluster.local. 3 IN A 10.102.222.93\n--\n;; SERVER: 10.96.0.10#53(10.96.0.10)\n;; WHEN: Sun Sep 17 16:38:49 UTC 2017\nvia 127.0.0.1:53\nHere it fails as supposed to.\n[root@heapster-487402730-frx1w /]# dig A monitoring-influxdb.kube-system.svc.cluster.local. @127.0.0.1 | grep -EA1 'ANSWER SECTION|SERVER'\n[root@heapster-487402730-frx1w /]# echo $?\n1\nThe question is why does the heapster uses the 127.0.0.1 instead of the nameserver.. Okay, moving the heapster binary to the other container-base image such as CentOS 7 seem to have worked around the issue.\nDockerfile:\n```\nFROM centos:7\nCOPY heapster-1.3.0 /heapster\n```\ndocker build -t local/heapster-amd64:v1.3.0-centos .\n[root@myserver1:~] kubectl -n kube-system logs -c heapster -f heapster-3709176945-v26zc   \nI0917 16:52:03.751478       5 heapster.go:72] /heapster --source=kubernetes:https://kubernetes.default --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086\nI0917 16:52:03.751672       5 heapster.go:73] Heapster version v1.3.0\nI0917 16:52:03.752099       5 configs.go:61] Using Kubernetes client with master \"https://kubernetes.default\" and version v1\nI0917 16:52:03.752136       5 configs.go:62] Using kubelet port 10255\nI0917 16:52:03.810163       5 influxdb.go:252] created influxdb sink with options: host:monitoring-influxdb.kube-system.svc:8086 user:root db:k8s\nI0917 16:52:03.810186       5 heapster.go:196] Starting with InfluxDB Sink\nI0917 16:52:03.810190       5 heapster.go:196] Starting with Metric Sink\nI0917 16:52:03.833103       5 heapster.go:106] Starting heapster on port 8082\n@andyxning how is the heapster docker image getting created?\nUpdate 1\nYet another workaround was to start heapster with UID 0 (root):\nspec:\n      securityContext:\n        runAsUser: 0\nThen everything is working.\nI am starting to think that heapster is having some issues with accessing the /etc/resolv.conf file, running from the heapster 1.3.0+ docker images.\nI have checked the inner parts of the images, they look pretty simple. The major difference is that starting with heapster 1.3.0+ the directive USER 65534:65534 is used.\nHere are the permissions down to the /etc/resolv.conf:\n[root@myserver1:~] ls -lad /mnt/docker/containers/0496e74cb50f6b508917641a0e6f88020fb5c5a7b974531b02ff39c4d3bf9c16/resolv.conf /mnt/docker/containers/0496e74cb50f6b508917641a0e6f88020fb5c5a7b974531b02ff39c4d3bf9c16 /mnt/docker/containers /mnt/docker /mnt /\ndr-xr-xr-x. 18 root root  4096 Sep 15 14:50 /\ndrwxr-xr-x.  7 root root  4096 Sep 15 14:46 /mnt\ndrwx--x--x. 11 root root  4096 Sep 15 14:48 /mnt/docker\ndrwxr-x---+ 86 root root 24576 Sep 17 19:09 /mnt/docker/containers\ndrwxr-x---+  4 root root  4096 Sep 17 19:09 /mnt/docker/containers/0496e74cb50f6b508917641a0e6f88020fb5c5a7b974531b02ff39c4d3bf9c16\n-rw-r--r--+  1 root root   149 Sep 17 19:09 /mnt/docker/containers/0496e74cb50f6b508917641a0e6f88020fb5c5a7b974531b02ff39c4d3bf9c16/resolv.conf\nUpdate 2\nSo, when I am setting the USER 65534 in the Dockerfile (to simulate the difference observed in heapster 1.3.0+ docker images),\nthe /etc/resolv.conf becomes unreadable for that user.\n[root@myserver1:~] docker run --rm -ti --entrypoint bash local/heapster-amd64:v1.3.0-centos\nbash-4.2$ id\nuid=65534 gid=0(root) groups=0(root)\nbash-4.2$ ping google.com\nping: google.com: Name or service not known\nbash-4.2$ cat /etc/resolv.conf\ncat: /etc/resolv.conf: Permission denied\nbash-4.2$ ls -lad / /etc /etc/resolv.conf \ndrwxr-xr-x. 1 root root 4096 Sep 17 18:03 /\ndrwxr-xr-x. 1 root root 4096 Sep 17 18:03 /etc\n-rw-r--r--+ 1 root root  179 Sep 17 18:03 /etc/resolv.conf\nInterestingly, the same is not observed when setting the user to, say 99.\nEDIT the reason why it was not observed with 99 because 99 uid had a record in the image's /etc/passwd file, for more details please refer to https://github.com/kubernetes/heapster/issues/1658#issuecomment-331196813 or https://github.com/kubernetes/heapster/pull/1817\n```\n[root@myserver1:~] grep -Ew \"12345|65534|99\" /etc/passwd\nnobody:x:99:99:Nobody:/:/sbin/nologin\nnfsnobody:x:65534:65534:Anonymous NFS User:/var/lib/nfs:/sbin/nologin\n[root@myserver1:~] docker run --rm -ti -u 65534 --entrypoint bash local/heapster-amd64:v1.3.0-centos -c \"cat /etc/resolv.conf\"\ncat: /etc/resolv.conf: Permission denied\n[root@myserver1:~] docker run --rm -ti -u 12345 --entrypoint bash local/heapster-amd64:v1.3.0-centos -c \"cat /etc/resolv.conf\"\ncat: /etc/resolv.conf: Permission denied\n[root@myserver1:~] docker run --rm -ti -u 99 --entrypoint bash local/heapster-amd64:v1.3.0-centos -c \"cat /etc/resolv.conf\"\n; generated by /usr/sbin/dhclient-script\nsearch [...redacted]\nnameserver [...redacted]\n```. Looks like the issue is local and not heapster related.\n] docker run --rm -ti -u 65534 --entrypoint bash centos:7 -c \"cat /etc/resolv.conf\"\ncat: /etc/resolv.conf: Permission denied\nThe thing is that heapster is running under 65534 UID since v1.3.0+ which has triggered this issue.\nI will try to figure it out and will close this issue.. Yes, this was our local issue due to a over-complicated ACLs ...\nTo those who will wonder, when setting the Default ACLs (-d with setfacl), do not forget to explicitly set the root, e.g.:\nIncorrect\n/usr/bin/setfacl -Rdm group:splunk:rx,other:r /mnt/docker/containers\nCorrect\n/usr/bin/setfacl -Rdm group:root:r,group:splunk:rx,other:r /mnt/docker/containers\nEdit 1\nSo we must ensure group:root:r is additionally set when setting the Default ACL's (setfacl command with the -d flag).\nThis brings the default Docker behavior, when a docker container process is running under a non-root user is able to read the /etc/resolv.conf file.\nInterestingly, the user with UID 99 (nobody, in CentOS 7.3) does not experience the same issue.\n. > IIUC, you mean that the root reason for heapster to perform a DNS query against 127.0.0.1 is that heapster can not read the /etc/resolv.conf file?\n\n... it is expected that 65534 is the UID of nobody. Maybe this is not true across all linux distributions.\n\n@andyxning you are right:\nUbuntu xenial\n$ id nobody\nuid=65534(nobody) gid=65534(nogroup) groups=65534(nogroup)\nCentOS 7\n```\nid nobody\nuid=99(nobody) gid=99(nobody) groups=99(nobody)\nid nfsnobody\nuid=65534(nfsnobody) gid=65534(nfsnobody) groups=65534(nfsnobody)\n```\nBut the real issue lies somewhere else... where /etc/resolv.conf is unreadable to others. I have tried to completely remove /mnt/docker/containers directory, restarting the dockerd, applying our old ACL setting which has other:r, and I cannot reproduce this issue since /etc/resolv.conf is readable with 12345:12345 (non-existing user/group) and with 65534:65534 (nfsnobody:nfsnobody in CentOS 7).\nEdit 1\nI will try to put back our old deployment, in order to reproduce it one more time.\n. Ok, I am able to reproduce the issue:\nsetfacl --recursive --remove-all -- /mnt/docker/containers\nsetfacl -Rdm group:splunk:rx,other:r -- /mnt/docker/containers\nsetfacl -Rm group:splunk:rx -- /mnt/docker/containers\nAfter that, no matter if one restarts the dockerd or reloads the overlay FS, the issue occurs again:\n~] kubectl -n kube-system logs -f heapster-1428236806-xs2kj\nI0918 07:54:11.813694       5 heapster.go:72] /heapster --source=kubernetes:https://kubernetes.default --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086\nI0918 07:54:11.813786       5 heapster.go:73] Heapster version v1.4.0\nI0918 07:54:11.814064       5 configs.go:61] Using Kubernetes client with master \"https://kubernetes.default\" and version v1\nI0918 07:54:11.814079       5 configs.go:62] Using kubelet port 10255\nE0918 07:54:11.816125       5 kubelet.go:334] Failed to load nodes: Get https://kubernetes.default/api/v1/nodes: dial tcp: lookup kubernetes.default on [::1]:53: read udp [::1]:51759->[::1]:53: read: connection refused\nE0918 07:54:11.817474       5 influxdb.go:264] issues while creating an InfluxDB sink: failed to ping InfluxDB server at \"monitoring-influxdb.kube-system.svc:8086\" - Get http://monitoring-influxdb.kube-system.svc:8086/ping: dial tcp: lookup monitoring-influxdb.kube-system.svc on [::1]:53: read udp [::1]:49628->[::1]:53: read: connection refused, will retry on use\nI0918 07:54:11.817486       5 influxdb.go:278] created influxdb sink with options: host:monitoring-influxdb.kube-system.svc:8086 user:root db:k8s\nI0918 07:54:11.817540       5 heapster.go:196] Starting with InfluxDB Sink\nI0918 07:54:11.817549       5 heapster.go:196] Starting with Metric Sink\nE0918 07:54:11.819666       5 reflector.go:190] k8s.io/heapster/metrics/heapster.go:322: Failed to list *v1.Pod: Get https://kubernetes.default/api/v1/pods?resourceVersion=0: dial tcp: lookup kubernetes.default on [::1]:53: read udp [::1]:54248->[::1]:53: read: connection refused\nE0918 07:54:11.819755       5 reflector.go:190] k8s.io/heapster/metrics/util/util.go:51: Failed to list *v1.Node: Get https://kubernetes.default/api/v1/nodes?resourceVersion=0: dial tcp: lookup kubernetes.default on [::1]:53: read udp [::1]:54248->[::1]:53: read: connection refused\nE0918 07:54:11.819839       5 reflector.go:190] k8s.io/heapster/metrics/util/util.go:51: Failed to list *v1.Node: Get https://kubernetes.default/api/v1/nodes?resourceVersion=0: dial tcp: lookup kubernetes.default on [::1]:53: read udp [::1]:54248->[::1]:53: read: connection refused\nE0918 07:54:11.821335       5 reflector.go:190] k8s.io/heapster/metrics/processors/namespace_based_enricher.go:84: Failed to list *v1.Namespace: Get https://kubernetes.default/api/v1/namespaces?resourceVersion=0: dial tcp: lookup kubernetes.default on [::1]:53: read udp [::1]:48313->[::1]:53: read: connection refused\nE0918 07:54:11.823103       5 reflector.go:190] k8s.io/heapster/metrics/util/util.go:51: Failed to list *v1.Node: Get https://kubernetes.default/api/v1/nodes?resourceVersion=0: dial tcp: lookup kubernetes.default on [::1]:53: read udp [::1]:51373->[::1]:53: read: connection refused\nI0918 07:54:11.909308       5 heapster.go:106] Starting heapster on port 8082\nE0918 07:54:12.820936       5 reflector.go:190] k8s.io/heapster/metrics/heapster.go:322: Failed to list *v1.Pod: Get https://kubernetes.default/api/v1/pods?resourceVersion=0: dial tcp: lookup kubernetes.default on [::1]:53: read udp [::1]:56828->[::1]:53: read: connection refused\nE0918 07:54:12.824619       5 reflector.go:190] k8s.io/heapster/metrics/util/util.go:51: Failed to list *v1.Node: Get https://kubernetes.default/api/v1/nodes?resourceVersion=0: dial tcp: lookup kubernetes.default on [::1]:53: read udp [::1]:57378->[::1]:53: read: connection refused\nE0918 07:54:12.824669       5 reflector.go:190] k8s.io/heapster/metrics/util/util.go:51: Failed to list *v1.Node: Get https://kubernetes.default/api/v1/nodes?resourceVersion=0: dial tcp: lookup kubernetes.default on [::1]:53: read udp [::1]:57378->[::1]:53: read: connection refused\nE0918 07:54:12.904243       5 reflector.go:190] k8s.io/heapster/metrics/processors/namespace_based_enricher.go:84: Failed to list *v1.Namespace: Get https://kubernetes.default/api/v1/namespaces?resourceVersion=0: dial tcp: lookup kubernetes.default on [::1]:53: read udp [::1]:42402->[::1]:53: read: connection refused\nAs a remedy, one would need to explicitly set the group:root:r for the 2nd setfacl command:\nsetfacl --recursive --remove-all -- /mnt/docker/containers\nsetfacl -Rdm group:root:r,group:splunk:rx,other:r -- /mnt/docker/containers\nsetfacl -Rm group:splunk:rx -- /mnt/docker/containers\nStrangely, this affects only the heapster 1.3.0+ containers, nevertheless I was removing the image right before removing the heapster deployment, restarting the dockerd, reloading the overlay FS.\nRunning some other docker image, e.g. centos:7 or alpine:3.6 did not indicate this problem at all:\ncentos7] docker run --rm -ti -u 65534:65534 --entrypoint bash centos:7 -c \"id; ls -la /etc/resolv.conf /etc/shells /etc/libaudit.conf; wc -l /etc/resolv.conf /etc/shells /etc/libaudit.conf;\"\nuid=65534 gid=65534 groups=65534\n-rw-r-----. 1 root root 191 Apr 19 12:56 /etc/libaudit.conf\n-rw-r--r--+ 1 root root 179 Sep 18 07:57 /etc/resolv.conf\n-rw-r--r--. 1 root root  76 Jun  7  2013 /etc/shells\n  6 /etc/resolv.conf     <<<<< OK, readable as expected\n  6 /etc/shells\nwc: /etc/libaudit.conf: Permission denied   <<<<<< OK as expected\n 12 total\nUpdate 1\nI have checked the differences between the working and non-working configuration for contents permissions under overlay GraphDriver LowerDir,MergedDir,UpperDir,WorkDir  and the ResolvConfPath (its upper level contents) and have found one single difference which was under the latter one (top-level for ResolvConfPath):\n(not working) without group:root:r in the beforementioned ACLs:\n4196597    8 drwx------   4 root     root         4096 Sep 18 10:16 /mnt/docker/containers/9494f5755bc1045b4f98c69dc05ae16d6a1c162299d1e40e4c246079b3ab9e1b/\n(working) with group:root:r:\n4196505    8 drwxr-x---   4 root     root         4096 Sep 18 10:11 /mnt/docker/containers/d8f29a5c7efdea458ac988d1e99e0976b97e760cc473927f6e1d23c44755ab0c/\nI still cannot get why the issue occurs only to the heapster 1.3.0+ container, but not to centos:7 or alpine:3.6 when running with -u 65534:65534 (or any other random UID/GID). I have even tried to build a new image based on a simple Dockerfile, with a USER 65534:65534 set, though have not tried with the FROM scratch image yet.. @andyxning okay, I think I have got really close to the actual issue.\nIt appears that GID is not actually 65534 (nobody), but is 0 (root).\nThis is exactly why our Default ACL \"fix\" works while what it does, is explicitly allowing the group:root:r.\nSo as I have finally managed to access the heapster container and see what is going on from inside.\naccessing the scratch images\nThe images built FROM scratch can be pain to debug, so here are steps I have prepared to ease the pain:\nwget https://busybox.net/downloads/binaries/1.27.1-i686/busybox\nchmod +x busybox\ncp busybox /proc/$(pidof heapster)/cwd/busybox\nmkdir -p /proc/$(pidof heapster)/cwd/{bin,sbin,usr/bin,usr/sbin}\nnsenter --mount=/proc/$(pidof heapster)/task/$(pidof heapster)/ns/mnt -- /busybox --install\n] kubectl -n kube-system exec -ti heapster-1428236806-lt5wd sh\n/ $ id\nuid=65534 gid=0\n/ $ ps -ef\nPID   USER     TIME   COMMAND\n    1 0          0:00 /pause\n    5 65534      0:09 /heapster --source=kubernetes:https://kubernetes.default --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086\n   29 65534      0:00 /busybox sh\n   47 65534      0:00 sh\n   52 65534      0:00 ps -ef\n/ $ stat / /etc /etc/resolv.conf |grep -E 'File:|^Access: \\('\n  File: /\nAccess: (0755/drwxr-xr-x)  Uid: (    0/ UNKNOWN)   Gid: (    0/ UNKNOWN)\n  File: /etc\nAccess: (0755/drwxr-xr-x)  Uid: (    0/ UNKNOWN)   Gid: (    0/ UNKNOWN)\n  File: /etc/resolv.conf\nAccess: (0654/-rw-r-xr--)  Uid: (    0/ UNKNOWN)   Gid: (    0/ UNKNOWN)\n/ $ id\nuid=65534 gid=0\n/ $ ls -lad / /etc /etc/resolv.conf\ndrwxr-xr-x    1 0        0             4096 Sep 18 09:20 /\ndrwxr-xr-x    1 0        0             4096 Sep 18 09:19 /etc\n-rw-r--r--    1 0        0              149 Sep 18 09:19 /etc/resolv.conf\n/ $ cat /etc/resolv.conf\ncat: can't open '/etc/resolv.conf': Permission denied\nSo you can see that the root group and others can read the /, /etc, /etc/resolv.conf, but it cannot really read the resolv.conf.\nSo our Default ACL (if without a \"fix\" group:root:r) are not permitting the root to read the resolv.conf.\nbefore\n```\n[root@myserver1:~] kubectl -n kube-system exec -ti heapster-1428236806-p59ff -- sh -c \"cat /etc/resolv.conf\"\ncat: can't open '/etc/resolv.conf': Permission denied\n[root@myserver1:~] getfacl /mnt/docker/containers/0273110ec9b94f1d7ece40be481b67170d9739e21aa9be7f0e4ccdb9d8ff4928/resolv.conf\ngetfacl: Removing leading '/' from absolute path names\nfile: mnt/docker/containers/0273110ec9b94f1d7ece40be481b67170d9739e21aa9be7f0e4ccdb9d8ff4928/resolv.conf\nowner: root\ngroup: root\nuser::rw-\ngroup::---\ngroup:splunk:r-x\nmask::r-x\nother::r--\n```\nafter\n```\n[root@myserver1:~] setfacl -m group:root:r,group:splunk:rx -- /mnt/docker/containers/0273110ec9b94f1d7ece40be481b67170d9739e21aa9be7f0e4ccdb9d8ff4928/resolv.conf\n[root@myserver1:~] kubectl -n kube-system exec -ti heapster-1428236806-p59ff -- sh -c \"cat /etc/resolv.conf\"\nnameserver 10.96.0.10\nsearch kube-system.svc.cluster.local svc.cluster.local cluster.local novalocal int.na.mydomainXYZ.com na.mydomainXYZ.com\noptions ndots:5\n[root@myserver1:~] getfacl /mnt/docker/containers/0273110ec9b94f1d7ece40be481b67170d9739e21aa9be7f0e4ccdb9d8ff4928/resolv.conf\ngetfacl: Removing leading '/' from absolute path names\nfile: mnt/docker/containers/0273110ec9b94f1d7ece40be481b67170d9739e21aa9be7f0e4ccdb9d8ff4928/resolv.conf\nowner: root\ngroup: root\nuser::rw-\ngroup::---\ngroup:root:r--\ngroup:splunk:r-x\nmask::r-x\nother::r--\n```\nThe real question now is why are we getting gid=0, whilst it should be 65534 as per Dockerfile. Maybe this is some Kuberntes specific? https://github.com/kubernetes/kubernetes/issues/22179 . ",
    "arno01": "@andyxning that is interesting, I have created the PR https://github.com/kubernetes/heapster/pull/1817 \nwhich  intends to bring the default Docker behavior in order to preserve the relevant GID set to 65534 (nobody) since by default Kubernetes would not respect the image's USER uid:gid set in the Dockerfile. Not sure whether there is an opened issue/pr for the Kubernetes to address this.\n\nPrepare a Dockerfile with the user 65534:65534 (nobody:nobody)\n```\nFROM scratch\nwget https://busybox.net/downloads/binaries/1.27.1-i686/busybox\nchmod +x busybox\nCOPY busybox /busybox\nUSER 65534:65534\nENTRYPOINT [\"/busybox\"]\nCMD [\"id\"]\n```\nBuild the image\ndocker build -t scratch .\nRunning the image shows uid:gid as expected - 65534:65534.\ndocker run --rm -ti scratch\nuid=65534 gid=65534\nWhen uid is explicitly set to 65534, then gid is not getting picked, because scratch image does not have /etc/passwd file.\ndocker run --rm -u 65534 -ti scratch\nuid=65534 gid=0\ndocker run --rm -u nobody  -ti scratch\ndocker: Error response from daemon: linux spec user: unable to find user nobody: no matching entries in passwd file.\ndocker run --rm -ti scratch cat /etc/passwd\ncat: can't open '/etc/passwd': No such file or directory\nAs a remedy, let's add the /etc/passwd file to the scratch image\necho 'nobody:x:65534:65534:nobody:/home:/bin/false' > passwd\nDockerfile:\n```\nFROM scratch\nwget https://busybox.net/downloads/binaries/1.27.1-i686/busybox\nchmod +x busybox\nCOPY busybox /busybox\nCOPY passwd /etc/passwd\nUSER 65534:65534\nENTRYPOINT [\"/busybox\"]\nCMD [\"id\"]\n```\nThe gid is getting picked now:\ndocker run --rm -ti -u nobody scratch \nuid=65534(nobody) gid=65534\ndocker run --rm -ti -u 65534 scratch \nuid=65534(nobody) gid=65534\nRunning in Kubernetes\nbuilding and pushing the images\ndocker build -t andrey01/scratch:passwd .\ndocker build -t andrey01/scratch:nopasswd .\ndocker push andrey01/scratch:passwd\ndocker push andrey01/scratch:nopasswd\nPreparing the jobs\nk8s-job-scratch-passwd.yml:\nkind: Job\napiVersion: batch/v1\nmetadata:\n  name: scratch-passwd\nspec:\n  template:\n    metadata:\n      labels:\n        app: scratch-passwd\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: scratch-passwd\n        image: andrey01/scratch:passwd\n        imagePullPolicy: Always\nk8s-job-scratch-nopasswd.yml:\nkind: Job\napiVersion: batch/v1\nmetadata:\n  name: scratch-nopasswd\nspec:\n  template:\n    metadata:\n      labels:\n        app: scratch-nopasswd\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: scratch-nopasswd\n        image: andrey01/scratch:nopasswd\n        imagePullPolicy: Always\nRunning the jobs\nkubectl create -f k8s-job-scratch-nopasswd.yml \nkubectl create -f k8s-job-scratch-passwd.yml\nChecking the output\n$ kubectl get jobs\nNAME               DESIRED   SUCCESSFUL   AGE\nscratch-nopasswd   1         1            19s\nscratch-passwd     1         1            16s\n$ kubectl logs $(kubectl get pods --show-all -l app=scratch-passwd --output jsonpath='{.items[0].metadata.name}')\nuid=65534(nobody) gid=65534\n$ kubectl logs $(kubectl get pods --show-all -l app=scratch-nopasswd --output jsonpath='{.items[0].metadata.name}')\nuid=65534 gid=0\nAs you can see gid is 0 for the scratch container which does not have /etc/passwd file containing the relevant uid:gid pair.\nCleanup\nkubectl delete job scratch-nopasswd scratch-passwd. Humm, Google wants me to sign the Contributor License Agreement (CLA), but I do not have the account at Google. Is there any workaround how I can sign the CLA ?. I signed it!. @andyxning alpine:3.6 sounds reasonable to me as well.. @andyxning done.\nHow do you think, should we add some sort of a test which would be always ensuring the expected UID:GID pair is present in the /etc/passwd file?\nE.g. something like:\nRUN grep -q \"65534:65534\" /etc/passwd\nEdit\nAnd how about doing RUN apk --update add ca-certificates instead of doing https://github.com/kubernetes/heapster/blob/c0f5ab124765c09c0d7a679e843d8ffd1aaca016/deploy/docker/Dockerfile#L4 and https://github.com/kubernetes/heapster/blob/b5fda35d0be85ffc5ed022eaa68f0e7c12138b01/Makefile#L71 ?\n. /retest. ",
    "alexef": "@DirectXMan12 I'm failing to find in the documentation, what certificates should I mount?. ",
    "bskiba": "Bump to verify cla.. ",
    "hzfeng87": "my cni0 has the IP 10.244.3.1 and it was assigned after I created flannel. I am confused that why 10.244.2.1/24 is required force.. ",
    "grailsweb": "image version is 4.0.2. update the yaml file and create the pod.\nhttps://github.com/kubernetes/heapster/issues/1469#issuecomment-274218852\n. ",
    "alvaroaleman": "Well, in that case the yamls in the repo should use 4.0.2 instead of referencing a non-existing tag.. ",
    "saschagrunert": "No, I guess the only thing is missing here is the image on google contianers.. Same here. . ",
    "raghu999": "Seeing same issue here\n**[root@kubemaster-hob-01s influxdb]# docker pull gcr.io/google_containers/heapster-grafana-amd64:v4.2.0\nTrying to pull repository gcr.io/google_containers/heapster-grafana-amd64 ...\nPulling repository gcr.io/google_containers/heapster-grafana-amd64\nTag v4.2.0 not found in repository gcr.io/google_containers/heapster-grafana-amd64\n[root@kubemaster-hob-01s influxdb]# docker pull gcr.io/google_containers/heapster-grafana-amd64:v2.6.0-2\nTrying to pull repository gcr.io/google_containers/heapster-grafana-amd64 ...\nPulling repository gcr.io/google_containers/heapster-grafana-amd64\nTag v2.6.0-2 not found in repository gcr.io/google_containers/heapster-grafana-amd64**\nNone of the grafana images are working . ",
    "allen12921": "Hi Piosz,\nthis gcr.io/google-containers/heapster-grafana-amd64:v4.2.0 can't start up\nit's giving below error\nkubectl logs -p  monitoring-grafana-2108077385-z2178  --namespace=kube-system \nStarting a utility program that will configure Grafana\nStarting Grafana in foreground mode\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp [::1]:3000: getsockopt: connection refused. Retrying after 5 seconds...\nt=2017-06-28T01:21:22+0000 lvl=crit msg=\"Failed to parse /etc/grafana/grafana.ini, open /etc/grafana/grafana.ini: no such file or directory%!(EXTRA []interface {}=[])\". ",
    "yaacov": "@DirectXMan12 hi, please take a look.\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1457933#c3. > Firstly, you should sign the CLA. :)\n@simon3z @DirectXMan12 I need help here ... how do I sign this ?. @andyxning thanks,\n\nWhy would you change this vendor file without updating godeps.\n\nDo you know who can verify the meaning of rss here ?\nIf I'm wrong in interpreting what rss mean ( if it include the cache ) than this pull request is wrong, and will not scratch my itch / BZ  :-)\n\nyou should sign the CLA. :)\n\n@simon3z @DirectXMan12 am I allowed to sign CLA ?\n. > contains memory excluding cached \nSo we need to fix this vendor file, right ?\n\nWhy would you change this vendor file without updating godeps.\n\nHow / Where do I update the godeps ? sorry I'm new to this.. > You should not and need not do this\nThanks, done :-). ",
    "marcinkubica": "@andyxning @piosz  4.0.2 in repo\n\n. I signed it!. Not needed. Covered in #1671\n. ",
    "ompraash": "@DirectXMan12 shouldn't the system add a tag to the podname to indicate it is deleted. This would be a feature.. @andyxning @DirectXMan12 that is not true. Have the pods deleted 24 hrs ago, and yet i see them in the drop-down. thou do not show any stats, but yes the drop down list yet appears.. @andyxning i tried as suggested but did not work. let me delete and recreate my env again. . ",
    "kachkaev": "Also facing this. Temporary had to \n```bash\nrun this if you've already created grafana, but it does not work\nkubectl delete -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yaml\nstart grafana v4.0.2\nhttp https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yaml > grafana.yaml\nsed -i 's/4.2.0/4.0.2/' grafana.yaml\nkubectl create -f grafana.yaml\nrm grafana.yaml\n```\nhttp command is from httpie package. You can use curl, wget, etc.. How can a PR be a duplicate of an issue? :\u2013). ",
    "hoangphuocbk": "I'm sorry because there are one person before me pointing it out.. ",
    "astef": "I signed it!\n. ",
    "vnandha": "@andyxning thanks, I realized lately. closing this.. ",
    "cdxOo": "Thank you for pointing me to event-exporter, I wil definetly try it out.. ",
    "gytisgreitai": "Is event-exporter the preffered way to use for now? (I was not able to get GCL working at all): \nI0710 06:50:29.886582       1 heapster.go:72] /heapster --source=kubernetes:https://kubernetes.default --sink=gcl\nI0710 06:50:29.886675       1 heapster.go:73] Heapster version v1.4.0\nI0710 06:50:29.886867       1 configs.go:61] Using Kubernetes client with master \"https://kubernetes.default\" and version v1\nI0710 06:50:29.886888       1 configs.go:62] Using kubelet port 10255\nE0710 06:50:29.920247       1 factory.go:84] Failed to create sink: Sink not recognized: gcl\nF0710 06:50:29.920271       1 factory.go:101] No available sink to use. ok, i got confused by this yaml https://github.com/kubernetes/heapster/blob/master/deploy/kube-config/google/heapster.yaml. ",
    "prat0318": "A dupe of #1671 . ",
    "SankarMittapally": "Please find the details below. Actually I am able to access grafana dashboard but there is no pods and cluster dashboards and tried to establish connection between grafana and influxdb datasource to get the data and got the above error. Please guide me how to fix this issue.\n`root@kubernetes:/home/smittapally# kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.4\", GitCommit:\"d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae\", GitTreeState:\"clean\", BuildDate:\"2017-05-19T18:44:27Z\", GoVersion:\"go1.7.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.4\", GitCommit:\"d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae\", GitTreeState:\"clean\", BuildDate:\"2017-05-19T18:33:17Z\", GoVersion:\"go1.7.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nroot@kubernetes:/home/smittapally#\n`\nI pulled the heapster repository and deployed the grafana, influxdb and heapster.\n`root@kubernetes:/home/smittapally/heapster# git log -1\ncommit 7a7c2c2c2c31b2d953da407a38fb4fe5d71ba2a1\nMerge: 89c4ab5 61e7497\nAuthor: Solly Ross directxman12+github@gmail.com\nDate:   Tue Jun 6 11:30:11 2017 -0400\nMerge pull request #1670 from yaacov/source-summary-rss-memory\n\nExport rss memory in summary source\n\nroot@kubernetes:/home/smittapally/heapster# git branch\n* master\nroot@kubernetes:/home/smittapally/heapster#\n`\nheapster-amd64:v1.3.0\nheapster-influxdb-amd64:v1.1.1\nheapster-grafana-amd64:v4.0.2. yes, all three services are up and running but not connected to Kubernetes cluster to get the data.. I am unable to access any API, it is saying system:anonymous. I used to NodePort for Grafana and came to know that graphs are not there.. ",
    "jasonkenneth": "I'm running into this issue. The only change I am making is toggling the anonymous enabled environment variable:\n        - name: GF_AUTH_ANONYMOUS_ENABLED\n          value: \"false\"\nWhen it's true, both the anonymous user and logging in as admin have the datasource set and the dashboards available.\nWhen it's false, anonymous access to grafana is disabled; however, when logging in as admin, there is no datasource configured and (logically) there are no dashboards to see.. ",
    "geekofalltrades": "@jasonkenneth , I was running into the same issue, and I just solved it.\nWhen you disable anonymous auth, you have to also enable basic auth:\n- name: GF_AUTH_BASIC_ENABLED\n  value: \"true\"\n\nIf we look at the source code of the Grafana setup script, we see that it constructs a URL to communicate with the Grafana API which includes basic auth credentials for the default admin user. So, if anonymous auth isn't enabled, we have to turn on basic auth, instead, otherwise the script can't communicate with the Grafana API to set up the data source and the dashboards.\nThe other issue that I discovered during this troubleshooting process was that the grafana run.sh script defaults anonymous auth to on. This is the opposite of the Grafana default, and results in unexpected behavior. In order to turn off anonymous auth, you must explicitly set\n- name: GF_AUTH_ANONYMOUS_ENABLED\n  value: \"false\"\n\nin the grafana Deployment manifest. I think this behavior should be documented, because I spent a long time troubleshooting why my Grafana interface was still visible without login, even after I removed the anonymous auth options from the config. I was expecting removing the configuration values to revert me to the Grafana defaults.. ",
    "gcstr": "Hi, I'm running the same issue and I can't get any other from Grafana than one pod on kube-system namespace.\n@DirectXMan12, running curl localhost:8001/api/v1/namespaces/kube-system/services/proxy/apis/metrics/v1alpha1/namespaces/kube-system/pods\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"the server could not find the requested resource\",\n  \"reason\": \"NotFound\",\n  \"details\": {},\n  \"code\": 404\n} .\n",
    "doubaokun": "I signed it!\nLinux and Google CLA signed.. Change the image to be gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 will fix this issue.. ",
    "johnharris85": "LGTM. ",
    "pokoli": "This PR fixes an error unable to deploy heapster due to an unexisting image error. So LGTM. ",
    "frankruizhi": "I've signed it!. ",
    "matlockx": "updated CLA. \ud83d\udc4d . ",
    "miry": "@luxas in the merged PR we have grafana v4.4.1, I think you can use the latest versions:\nGrafana https://github.com/grafana/grafana/releases/tag/v4.4.3\ninfluxbd v1.3.3 https://github.com/influxdata/influxdb/releases/tag/v1.3.3. @DirectXMan12 updated. @luxas @DirectXMan12 Updated versions to recent one. Updated description.. @DirectXMan12 updated description.. already merged fixes in separate pr.. it works for me.. Maybe better to change documentation to use stable versions:\n```shell\n$ git clone https://github.com/kubernetes/heapster.git\n$ cd heapster/deploy/kube-config/\nSwitch to current stable  version\n$ git checkout release-1.4 # or git checkout v1.4.1\nDeploy heapster\n$ kubectl apply -f rbac/\n$ kubectl apply -f influxdb/\n$ kubectl get pods --all-namespaces\n``. @khoing1111 I would suggest to use latest version of heapster 1.4.1 instead of heapster/v1.3.0`. Also for testing I use minikube. You can try first on it with lastest deployment spec files.. @khoing1111 to check influxdb data metrics you can use next approach:\nConnect to inflxudb:\nshell\n$ kubectl run --rm -it influxdbcli --image=influxdb -- influx -host monitoring-influxdb.kube-system.svc\nand get metrics via:\nsql\nSELECT * FROM \"k8s\".\"default\".\"cpu/usage_rate\"  limit 10\nor check latest metrics:\nsql\nSELECT * FROM \"k8s\".\"default\".\"cpu/usage_rate\"  where time > now() - 5m  limit 10\n. This version is already old. Updated to 1.4.2 in https://github.com/kubernetes/heapster/pull/1795. ",
    "outcoldman": "@AlmogBaku I am confused, how is having multiple timestamp fields helps kibana?\nI actually see several issues with current sink implementation of ElasticSearch:\n\n\nI would suggest to change the mapping to keep events with different set of fields in separate indexes. See for details https://www.elastic.co/blog/index-vs-type. Having multiple types in the same index, which do not have any common fields, just hurts lucene. Certainly does not save the space.\n\n\nCurrent mapping has a lot of fields analyzed by default. Why would you analyze MetricsTags.namespace_name? Certainly you don't want to use full text search on these fields. The only analyzed fields should be probably Message from events (maybe something else).\n\n\nEvery index should have only one timestamp field. That will work best with kibana.\n\n\nEvents currently aren't searchable as Time-based events, considering that we probably don't have a timebased field for it... And some alias.. @rikatz yes, you can send all the metrics to just one index, but as Do your documents have similar mappings? If no, use different indices.\n\n\nWith metricsbeat you can:\na) you can specify dedicated index for every type with elasticsearch output. \nb) use logstash output and forward events based on type to specific index\nI agree that having multiple indexes per every type can be hard to manage if you are just playing with that setup. But in case of real production use - that will pay off. Considering that sinks are configured only on command arguments level - maybe having a dedicated_indexes=[true|false] can target both scenarios?\nFor consideration - we can implement another sink - generic HTTP/JSON one, so it will be possible to use for example logstash http input and so folks will be able to do anything they want on pipeline.\nBtw, considering that ES5 supports ingest pipelines - I guess I can use that now as a workaround for separating types and putting stuff in right indexes. . > but I think would make harder to create Kibana and/or Grafana Dashboards.\nbtw, regarding this one. If you will have indexes with names like heapster-metrics-cpu-YYYY.MM.dd, heapster-metrics-network-YYYY.MM.dd, ... and heapster-events-YYYY.MM.dd you can easily build dashboards in Kibana and Grafana by just using names like heapster-metrics-* . @rikatz also I would suggest to update the mappings:\n\nremove all {\"type\": \"text\", \"fields\": {\"raw\": {\"type\": \"keyword\"} } } types and replace them with just {\"type\": \"keyword\"}, nobody will use full text search over metrics fields, it just does not make any sense. That will save the storage.\nsplit labels with , and send it as an array of [\"app=foo\",\"label2=my_label\"] and keep them as {\"type\": \"keyword\"} as well, so you will be able to search with MetricsTags.labels=app=foo\nif that is possible - would suggest to rename the fields memory/cache to more like JSON friendly memory_cache and maybe remove unnecessary nesting with Metrics. memory_cache, just keep memory_cache\nkeep only one field with type text, which is Message on events. . @rikatz sure, just one common timestamp field will help a lot, that will be good start.\n\nAs for breaking changes - I would suggest to break just once. If with every release this sink will have breaking changes - folks will be very unhappy. I would suggest to actually make all of them at once and standartize the schema with best practices from elasticsearch.\nBtw as for mappings - you can take a look on which mappings ElasticSearch is using https://www.elastic.co/guide/en/beats/metricbeat/master/exported-fields-kubernetes.html#_kubernetes_event_message - they do not have any text fields at all, Message also is a keyword. Not sure if that is desirable, but again - possible nobody will use full text search on this field as well. Plus if you will really need to use analyzers and full text search on some fields - you can use ingest pipeline.\nBtw on the list 3rd and 4th is the same.\nCurious if maybe metricbeats/kubernetes authors can help with advise as well. cc @exekias @monicasarbu - the community will really apprieate if you will help heapster be much nicer with ElasticSearch, we understand that you want to make your beats experience as simple as possible. But heapster is the native metrics collector from kubernetes, at least some advice and guidence will be appreciated. Your choise of pulling metrics from kubernetes directly is certainly ok, but my suggestion - API in kubernetes are getting breaking changes times to time - for example prometheus/kubernetes plugin feels like is getting broken all the time, and they actually using the same path as you do by pulling metrics directly. I would assume that heapster will have less of the breaking changes, considering closer sitting with kubernetes. This is why I am asking you to help with that, as this is what most people will rely on. The best case can be that heapster and metricsbeats/kubernetes will produce the same mappings (with the same names) so all the dashboards built for one can be used with another.\n. About the types and indices, https://www.elastic.co/blog/index-type-parent-child-join-now-future-in-elasticsearch - in ES 6 - there will be no types, only one type per index. @AlmogBaku \n2. that is the main confusion about the elasticsearch indexes and types. There are not similar to databases and tables. Both articles has really good explanation why having one type per index makes sense  https://www.elastic.co/blog/index-type-parent-child-join-now-future-in-elasticsearch and https://www.elastic.co/blog/index-vs-type\n3. Not sure what you are referencing to. If you will have type per index - you will be able to do that. Even if not - you will have just one kibana type for this index - which will have fields from all the different types in the index. How different timestamps help? At minimum you can always search for type:cpu at search. ",
    "exekias": "Hi there!\nI would say keyword is right in most cases, normally you do metric lookups by their full value. If you expect to have a random text probably text is still preferable.\nAs for the labels, we use nesting to store them, so they would be something like \"labels.app\": \"foo\".\n@outcoldman pointed out to our exported fields list, it's a good reference if you want to match our names.. ",
    "wangwu50": "I have the same question. ",
    "evhfla": "Same problem here. ",
    "narun4sk": "I second that, same problem.\nkubectl logs --namespace kube-system monitoring-grafana-2108077385-kxz29\nStarting a utility program that will configure Grafana\nStarting Grafana in foreground mode\nt=2017-06-27T14:45:30+0000 lvl=crit msg=\"Failed to parse /etc/grafana/grafana.ini, open /etc/grafana/grafana.ini: no such file or directory%!(EXTRA []interface {}=[])\". Unfortunately for me the older version doesn't work either...\n$ kubectl -n kube-system logs monitoring-grafana-2107815241-zd7p5\nStarting a utility program that will configure Grafana\nStarting Grafana in foreground mode\nt=2017-06-28T09:36:01+0000 lvl=info msg=\"Starting Grafana\" logger=main version=v4.0.2 commit=unknown-dev compiled=2017-06-28T09:36:01+0000\nt=2017-06-28T09:36:01+0000 lvl=info msg=\"Config loaded from\" logger=settings file=/usr/share/grafana/conf/defaults.ini\nt=2017-06-28T09:36:01+0000 lvl=info msg=\"Config loaded from\" logger=settings file=/etc/grafana/grafana.ini\nt=2017-06-28T09:36:01+0000 lvl=info msg=\"Config overriden from command line\" logger=settings arg=\"default.paths.data=/var/lib/grafana\"\nt=2017-06-28T09:36:01+0000 lvl=info msg=\"Config overriden from command line\" logger=settings arg=\"default.paths.logs=/var/log/grafana\"\nt=2017-06-28T09:36:01+0000 lvl=info msg=\"Config overriden from Environment variable\" logger=settings var=\"GF_SERVER_PROTOCOL=http\"\nt=2017-06-28T09:36:01+0000 lvl=info msg=\"Config overriden from Environment variable\" logger=settings var=\"GF_SERVER_ROOT_URL=/\"\nt=2017-06-28T09:36:01+0000 lvl=info msg=\"Config overriden from Environment variable\" logger=settings var=\"GF_AUTH_ANONYMOUS_ENABLED=true\"\nt=2017-06-28T09:36:01+0000 lvl=info msg=\"Config overriden from Environment variable\" logger=settings var=\"GF_AUTH_ANONYMOUS_ORG_ROLE=Admin\"\nt=2017-06-28T09:36:01+0000 lvl=info msg=\"Config overriden from Environment variable\" logger=settings var=\"GF_AUTH_BASIC_ENABLED=false\"\nt=2017-06-28T09:36:01+0000 lvl=info msg=\"Path Home\" logger=settings path=/usr/share/grafana\nt=2017-06-28T09:36:01+0000 lvl=info msg=\"Path Data\" logger=settings path=/var/lib/grafana\nt=2017-06-28T09:36:01+0000 lvl=info msg=\"Path Logs\" logger=settings path=/var/log/grafana\nt=2017-06-28T09:36:01+0000 lvl=info msg=\"Path Plugins\" logger=settings path=/usr/share/grafana/data/plugins\nt=2017-06-28T09:36:01+0000 lvl=info msg=\"Initializing DB\" logger=sqlstore dbtype=sqlite3\nt=2017-06-28T09:36:01+0000 lvl=info msg=\"Starting DB migration\" logger=migrator\nt=2017-06-28T09:36:01+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create migration_log table\"\nt=2017-06-28T09:36:02+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create user table\"\nt=2017-06-28T09:36:02+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add unique index user.login\"\nt=2017-06-28T09:36:04+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add unique index user.email\"\nt=2017-06-28T09:36:05+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop index UQE_user_login - v1\"\nt=2017-06-28T09:36:05+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop index UQE_user_email - v1\"\nt=2017-06-28T09:36:05+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Rename table user to user_v1 - v1\"\nt=2017-06-28T09:36:05+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create user table v2\"\nt=2017-06-28T09:36:05+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_user_login - v2\"\nt=2017-06-28T09:36:05+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_user_email - v2\"\nt=2017-06-28T09:36:05+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"copy data_source v1 to v2\"\nt=2017-06-28T09:36:06+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Drop old table user_v1\"\nt=2017-06-28T09:36:06+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create temp user table v1-7\"\nt=2017-06-28T09:36:06+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index IDX_temp_user_email - v1-7\"\nt=2017-06-28T09:36:06+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index IDX_temp_user_org_id - v1-7\"\nt=2017-06-28T09:36:06+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index IDX_temp_user_code - v1-7\"\nt=2017-06-28T09:36:06+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index IDX_temp_user_status - v1-7\"\nt=2017-06-28T09:36:06+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create star table\"\nt=2017-06-28T09:36:06+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add unique index star.user_id_dashboard_id\"\nt=2017-06-28T09:36:06+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create org table v1\"\nt=2017-06-28T09:36:06+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_org_name - v1\"\nt=2017-06-28T09:36:07+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create org_user table v1\"\nt=2017-06-28T09:36:07+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index IDX_org_user_org_id - v1\"\nt=2017-06-28T09:36:07+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_org_user_org_id_user_id - v1\"\nt=2017-06-28T09:36:07+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"copy data account to org\"\nt=2017-06-28T09:36:07+0000 lvl=info msg=\"Skipping migration condition not fulfilled\" logger=migrator id=\"copy data account to org\"\nt=2017-06-28T09:36:07+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"copy data account_user to org_user\"\nt=2017-06-28T09:36:07+0000 lvl=info msg=\"Skipping migration condition not fulfilled\" logger=migrator id=\"copy data account_user to org_user\"\nt=2017-06-28T09:36:07+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Drop old table account\"\nt=2017-06-28T09:36:07+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Drop old table account_user\"\nt=2017-06-28T09:36:07+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create dashboard table\"\nt=2017-06-28T09:36:07+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index dashboard.account_id\"\nt=2017-06-28T09:36:07+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add unique index dashboard_account_id_slug\"\nt=2017-06-28T09:36:07+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create dashboard_tag table\"\nt=2017-06-28T09:36:08+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add unique index dashboard_tag.dasboard_id_term\"\nt=2017-06-28T09:36:08+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop index UQE_dashboard_tag_dashboard_id_term - v1\"\nt=2017-06-28T09:36:08+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Rename table dashboard to dashboard_v1 - v1\"\nt=2017-06-28T09:36:08+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create dashboard v2\"\nt=2017-06-28T09:36:08+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index IDX_dashboard_org_id - v2\"\nt=2017-06-28T09:36:08+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_dashboard_org_id_slug - v2\"\nt=2017-06-28T09:36:08+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"copy dashboard v1 to v2\"\nt=2017-06-28T09:36:08+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop table dashboard_v1\"\nt=2017-06-28T09:36:08+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"alter dashboard.data to mediumtext v1\"\nt=2017-06-28T09:36:08+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Add column updated_by in dashboard - v2\"\nt=2017-06-28T09:36:08+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Add column created_by in dashboard - v2\"\nt=2017-06-28T09:36:08+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Add column gnetId in dashboard\"\nt=2017-06-28T09:36:09+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Add index for gnetId in dashboard\"\nt=2017-06-28T09:36:09+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Add column plugin_id in dashboard\"\nt=2017-06-28T09:36:09+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Add index for plugin_id in dashboard\"\nt=2017-06-28T09:36:09+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Add index for dashboard_id in dashboard_tag\"\nt=2017-06-28T09:36:09+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create data_source table\"\nt=2017-06-28T09:36:09+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index data_source.account_id\"\nt=2017-06-28T09:36:10+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add unique index data_source.account_id_name\"\nt=2017-06-28T09:36:10+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop index IDX_data_source_account_id - v1\"\nt=2017-06-28T09:36:10+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop index UQE_data_source_account_id_name - v1\"\nt=2017-06-28T09:36:10+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Rename table data_source to data_source_v1 - v1\"\nt=2017-06-28T09:36:10+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create data_source table v2\"\nt=2017-06-28T09:36:10+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index IDX_data_source_org_id - v2\"\nt=2017-06-28T09:36:10+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_data_source_org_id_name - v2\"\nt=2017-06-28T09:36:10+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"copy data_source v1 to v2\"\nt=2017-06-28T09:36:10+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Drop old table data_source_v1 #2\"\nt=2017-06-28T09:36:10+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Add column with_credentials\"\nt=2017-06-28T09:36:10+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Add secure json data column\"\nt=2017-06-28T09:36:10+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create api_key table\"\nt=2017-06-28T09:36:10+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index api_key.account_id\"\nt=2017-06-28T09:36:10+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index api_key.key\"\nt=2017-06-28T09:36:11+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index api_key.account_id_name\"\nt=2017-06-28T09:36:11+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop index IDX_api_key_account_id - v1\"\nt=2017-06-28T09:36:11+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop index UQE_api_key_key - v1\"\nt=2017-06-28T09:36:11+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop index UQE_api_key_account_id_name - v1\"\nt=2017-06-28T09:36:11+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Rename table api_key to api_key_v1 - v1\"\nt=2017-06-28T09:36:11+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create api_key table v2\"\nt=2017-06-28T09:36:11+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index IDX_api_key_org_id - v2\"\nt=2017-06-28T09:36:11+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_api_key_key - v2\"\nt=2017-06-28T09:36:11+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_api_key_org_id_name - v2\"\nt=2017-06-28T09:36:11+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"copy api_key v1 to v2\"\nt=2017-06-28T09:36:12+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Drop old table api_key_v1\"\nt=2017-06-28T09:36:12+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create dashboard_snapshot table v4\"\nt=2017-06-28T09:36:12+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop table dashboard_snapshot_v4 #1\"\nt=2017-06-28T09:36:12+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create dashboard_snapshot table v5 #2\"\nt=2017-06-28T09:36:12+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_dashboard_snapshot_key - v5\"\nt=2017-06-28T09:36:12+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_dashboard_snapshot_delete_key - v5\"\nt=2017-06-28T09:36:12+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index IDX_dashboard_snapshot_user_id - v5\"\nt=2017-06-28T09:36:12+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"alter dashboard_snapshot to mediumtext v2\"\nt=2017-06-28T09:36:12+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create quota table v1\"\nt=2017-06-28T09:36:12+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_quota_org_id_user_id_target - v1\"\nt=2017-06-28T09:36:12+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create plugin_setting table\"\nt=2017-06-28T09:36:13+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create index UQE_plugin_setting_org_id_plugin_id - v1\"\nt=2017-06-28T09:36:13+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Add column plugin_version to plugin_settings\"\nt=2017-06-28T09:36:13+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create session table\"\nt=2017-06-28T09:36:13+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Drop old table playlist table\"\nt=2017-06-28T09:36:13+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Drop old table playlist_item table\"\nt=2017-06-28T09:36:13+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create playlist table v2\"\nt=2017-06-28T09:36:13+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create playlist item table v2\"\nt=2017-06-28T09:36:13+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop preferences table v2\"\nt=2017-06-28T09:36:14+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"drop preferences table v3\"\nt=2017-06-28T09:36:14+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create preferences table v3\"\nt=2017-06-28T09:36:14+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create alert table v1\"\nt=2017-06-28T09:36:14+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index alert org_id & id \"\nt=2017-06-28T09:36:14+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index alert state\"\nt=2017-06-28T09:36:14+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index alert dashboard_id\"\nt=2017-06-28T09:36:14+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create alert_notification table v1\"\nt=2017-06-28T09:36:14+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Add column is_default\"\nt=2017-06-28T09:36:15+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index alert_notification org_id & name\"\nt=2017-06-28T09:36:15+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"Drop old annotation table v4\"\nt=2017-06-28T09:36:15+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"create annotation table v5\"\nt=2017-06-28T09:36:15+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index annotation 0 v3\"\nt=2017-06-28T09:36:15+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index annotation 1 v3\"\nt=2017-06-28T09:36:15+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index annotation 2 v3\"\nt=2017-06-28T09:36:15+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index annotation 3 v3\"\nt=2017-06-28T09:36:15+0000 lvl=info msg=\"Executing migration\" logger=migrator id=\"add index annotation 4 v3\"\nt=2017-06-28T09:36:16+0000 lvl=info msg=\"Created default admin user: [admin]\"\nt=2017-06-28T09:36:16+0000 lvl=info msg=\"Starting plugin search\" logger=plugins\nt=2017-06-28T09:36:16+0000 lvl=warn msg=\"Plugin dir does not exist\" logger=plugins dir=/usr/share/grafana/data/plugins\nt=2017-06-28T09:36:16+0000 lvl=info msg=\"Plugin dir created\" logger=plugins dir=/usr/share/grafana/data/plugins\nt=2017-06-28T09:36:16+0000 lvl=info msg=\"Initializing Alerting\" logger=alerting.engine\nt=2017-06-28T09:36:16+0000 lvl=info msg=\"Initializing CleanUpService\" logger=cleanup\nt=2017-06-28T09:36:16+0000 lvl=info msg=\"Initializing HTTP Server\" logger=server address=0.0.0.0:3000 protocol=http subUrl=\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp: i/o timeout. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp: i/o timeout. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp: i/o timeout. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp: i/o timeout. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp: i/o timeout. Retrying after 5 seconds...\n$ kubectl -n kube-system exec monitoring-grafana-2107815241-zd7p5 -- netstat -ntlp\nActive Internet connections (only servers)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    \ntcp        0      0 :::3000                 :::*                    LISTEN      1/grafana-server. That's not a typo, new version has been added with the 05f739ef6e7fddb265b9b73b2c65b58ef5e1b500. ",
    "mjsilva": "Any solution while this is not fixed, use a previous version?. Confirm that if you use gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 it works. ",
    "yuananf": "Same problem here. ",
    "AlterEgo7": "Latest release has 4.2.0 grafana image version. Changing it to 4.0.2 fixes the issue. Not sure if this is a typo in the container image version or not. . ",
    "christiaan-janssen": "I'm trying with gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 but then I get:\nBack-off restarting failed container\nError syncing pod, skipping: failed to \"StartContainer\" for \"grafana\" with CrashLoopBackOff: \"Back-off 5m0s restarting failed container=grafana pod=monitoring-grafana-2108077385-736m7_kube-system(e382bc28-5cb4-11e7-b2c7-42010a8400a0)\"\nFailed to pull image \"gcr.io/google_containers/heapster-grafana-amd64\": rpc error: code = 2 desc = Error: Status 405 trying to pull repository google_containers/heapster-grafana-amd64: \"v1 Registry API is disabled. If you are not explicitly using the v1 Registry API, it is possible your v2 image could not be found. Verify that your image is available, or retry with `dockerd --disable-legacy-registry`. See https://cloud.google.com/container-registry/docs/support/deprecation-notices\". ",
    "bishenghua": "it doesn't work:\nStarting a utility program that will configure Grafana\nStarting Grafana in foreground mode\nt=2017-06-30T13:39:23+0000 lvl=crit msg=\"Failed to parse /etc/grafana/grafana.ini, open /etc/grafana/grafana.ini: no such file or directory%!(EXTRA []interface {}=[])\"\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp 127.0.0.1:3000: getsockopt: connection refused. Retrying after 5 seconds...\n. i have soved it! Becaues it has not this file:/etc/grafana/grafana.ini in gcr.io/google_containers/heapster-grafana-amd64:v4.2.0\n\ndocker pull gcr.io/google_containers/heapster-grafana-amd64:v4.0.2\ndocker run -d --name test -v /tmp/ttt:/tmp/ttt gcr.io/google_containers/heapster-grafana-amd64:v4.0.2\ndocker exec -it test -- cp -rp /etc/grafana /tmp/ttt && docker rm -f test\nvim Dockerfile:\nFROM      gcr.io/google_containers/heapster-grafana-amd64:v4.2.0\nMAINTAINER shenghua bi net.bsh@gmail.com\nCOPY /tmp/ttt/grafana /etc/grafana\ndocker build -t=\"gcr.io/google_containers/heapster-grafana-amd64:v4.2.1\" .\nyon can use your heapster-grafana-amd64:v4.2.1 instead of heapster-grafana-amd64:v4.2.0\n\nit works fine.\nGood luck!\n . ",
    "4admin2root": "It works with grafana 4.0.2 , but there are no datapoints.. ",
    "tanmaykm": "Ran into this. As a temporary workaround, I am using image luxas/heapster-grafana-amd64:v4.3.2 instead of gcr.io/google_containers/heapster-grafana-amd64:v4.2.0.\nRef: https://github.com/luxas/kubeadm-workshop/blob/master/demos/monitoring/influx-grafana.yaml.\nHaven't had time to try out, but maybe copying sample.ini (as grafana.ini) and ldap.toml from /usr/share/grafana/conf to /etc/grafana would have the same effect.. ",
    "kawych": "I've signed the cla.. /lgtm. /ok-to-test. /lgtm. @jingxu97 @andyxning \nI've debugged the integration test for a bit, it's an issue in your deployment files that causes heapster pod not to be created. I've left some comments, after applying these changes, heapster pod is created and tests are running, so you should be fine.. @piosz @DirectXMan12 - Please take a look and re-apply LGTM label if it looks fine\n@andyxning - Note #1980 Consider bumping it one more version - to 1.9.4. Alternatively we can get this merged and bump it again later.. /retest. This cherry-pick doesn't build, closing.. /lgtm. This PR looks really good, I have no comments. LGTM once it's tested.. /lgtm. /lgtm. Please resolve the conflicts.. Heapster deployment files are here, choose the ones according to your backend. You shouldn't have to run cadvisor.. Closing as duplicate of #1903.. /lgtm. @pongad\nCan you take a look?. /retest. This is one of changes, the other is #1906. This alone doesn't impact performance significantly - the test with 100 nodes and 3k user pods shows no reduced memory usage and CPU usage reduced only slightly (20m vs 18m). This was on standalone instance of heapster.. /retest. /retest. Please note that this documentation is outdated, see #1715. /ok-to-test\n/lgtm. @wrp \nAs @andyxning mentioned, can you change this PR to remove google.md file instead of changing this link, or just close this PR (then I'll file a PR to remove the outdated docs). /lgtm\nThanks! Please squash the commits.. /retest. As discussed in Stackdriver Team, node is now removed from resource labels for k8s_pod and k8s_containers. It is not required to identify a pod, and there cases where we may want to export metrics for pods without a node assigned, i.e. unscheduled pods.. /lgtm\n/ok-to-test. Similar issue #1988 has been noticed for Stackdriver sink, it may have the same root cause. Waiting for more input there.. @x13n No, this was noticed by someone from Stackdriver when he verified exported metric values. I didn't file an issue since I have this fix already, do you want me to file it?. Sorry, I missed it - issued #1979. Thanks!. Probably we should release 1.9.4, #1849 is  bumps it to 1.9.3 and is almost ready.. /lgtm. /ok-to-test. /lgtm. /ok-to-test. /ok-to-test. @x13n \nDaniel, IIRC you have been working on this bug some time ago - can you summarize here what was the outcome?. Thanks Daniel. Container name is empty for pod metrics, so if container metrics are reported with empty container name, this can lead to similar error even for one-container pods (not sure if the error message would be the same though).. I'm not 100% sure. This component: https://github.com/kubernetes/heapster/blob/master/metrics/processors/pod_aggregator.go is responsible for aggregating all container metrics on pod level. It looks like it blacklists all Delta and Cumulative metrics. CPU usage is cumulative, so it should be blacklisted. Memory usage is gauge, so I think it's exposed also as pod metric.. From my observations so far, Heapster runs and exports metrics, but fails at verification of cpu utilization: https://github.com/kubernetes/kubernetes/blob/master/test/e2e/instrumentation/monitoring/stackdriver.go#L155. LGTM once the metric is added to Stackdriver. Please take a look at presubmits and consider adding a corresponding metrics for legacy resource model. Thanks.. /lgtm. Lgtm for the core changes. For stackdriver sink, we need to define Stackdriver metrics first (I'm happy to review, please include there also a reviewer from Stackdriver).. @jingxu97 \n\"metrics/sinks/riemann/driver_test.go:200: k8s.io/heapster/metrics/core.LabeledMetric composite literal uses unkeyed fields\"\nNot sure why is it failing now, but adding explicit key names (\"Name\", \"Labels\", \"MetricValue\") to the struct in a file mentioned above should fix it.. Please fix the presubmit checks.\n/ok-to-test. /retest. @jcharlytown The other PR approved. I'm fine with both ways, but since the review for #2009 is finished, let's submit it first.\n/lgtm\n/hold\nHolding to give @andyxning a chance to comment.. #2008 approved and waiting to be merged, closing this PR.. /lgtm. /retest. /lgtm. /ok-to-test\n/lgtm. /ok-to-test\nPlease sign the CLA as described above.. /lgtm\nPlease sign the cla. /cc @piosz \nI don't think we should fully deprecate Heapster, at least not at this point. For instance, Heapster is currently a go-to monitoring agent for Stackdriver.\nI suggest to deprecate only Heapster API and work on replacing it with metrics API served by Metrics Server. My understanding of current monitoring architecture for k8s: https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/monitoring_architecture.md is that Heapster is going to become just another third-party monitoring agent.. @DirectXMan12 @brancz \nThank you for detailed explanations. I've also discussed with @piosz the Stackdriver case and once he approves, I have no objections. Removing myself from reviewers.. /ok-to-test\n/lgtm. /ok-to-test\n/lgtm. /lgtm. /ok-to-test\n/lgtm. /lgtm. My understanding of this PR was that it's not adding a new feature, but a missing piece of #1891. @mindprince can add more context.. /ok-to-test. /lgtm. /ok-to-test\n/lgtm. /ok-to-test\n/lgtm. /ok-to-test\nLGTM once comments from Marian are resolved. This causes Heapster to label metrics incorrectly with empty host id. For Stackdriver sink, this causes a majority of metrics to not be exported.. I don't think this is relevant to any other sink, but it could be. I would argue that this is a bug in Heapster sources, because the metric labels are set there. One of them is HostID, and the summary source assigns to it an incorrect value.\nI have to agree that using a node annotation for this is hacky, the problem is that there is no environment-agnostic source of this information now. I identified this way to be least disruptive, as it's optional to annotate nodes and it doesn't change the default Heapster behavior. The Summary API source doesn't propagate information about node annotations to the sinks, therefore fixing this in Stackdriver sink would require either\n1) Propagating all node annotations to sinks\n2) Calling API Server again just to fill the missing annotations for the nodes\nFrom my discussions about removing this field, it is key that it was a part of node spec, which is immutable. Therefore on a cloud providers where this value changed during the node update, it forced a node recreation. The optional annotation doesn't have the same problem.. Sorry for that, I considered it a small change with no effect on performance and transparent to anyone that doesn't depend on HostID label. I'll make sure to give more time for others to comment for any other PRs, for this one we can continue a discussion here whether to accept this fix or replace with something else.\nWrt. to removing HostID label completely, I wouldn't consider it as an option because it's already used by some (Stackdriver) sink. It does cause significant breakage for Stackdriver integration, i.e. all node metrics have now indistinguishable labels, Stackdriver API doesn't accept duplicate timeseries for the same period, so it rejects them. Because Heapster sends metrics in batches, this causes a rejection of the whole batch. Same thing for kubelet and docker-daemon metrics - they are identified by the instance.. Thanks for your suggestion. We considered that path, but turns out that Stackdriver relies on the instance_id matching the actual ID and there are several things that can break with such Heapster upgrade, as Stackdriver has other channels that export data as well and they need to match labels for metrics. Other problem is that the upgrade will cause too much alerts to fire due to not matching labels.. /retest. Answered in #2057, I can create an issue for this if needed. The issue starts with K8s 1.11, which uses Heapster 1.5, this is why I want to backport it. The other approach I'm considering is to release next Heapster version and cherry-pick it to Kubernetes 1.11.. /retest. Closing until the presubmits are fixed.. Looks like the OWNERS file is missing on the release 1.5 branch, manually adding \"approved\" label.. /retest. /retest. /retest. cc @DirectXMan12, as you were involved in discussion about this change.. /retest. /ok-to-test\n/lgtm. /approve. I believe the deprecation plan still applies to eventer - as documentations says, it is still a part of heapster: https://github.com/kubernetes/heapster/blob/master/docs/overview.md it's also distributed as one image.\nI don't know what's the replacement. The purpose was to shift maintenance of integrations from Kubernetes community to logging/monitoring system maintainers, i.e. we rely on third-party tooling to export metrics and events to each logging/monitoring system. Note that Metrics Server has much narrower scope than Heapster and does not cover Eventer functionality.. Please see our discussion from the PR that deprecated heapster: #2022. The solution suggested by @brancz and @DirectXMan12 in similar case was to create a special-purpose Heapster fork. Maybe you can do similar thing with Eventer? Also, there is a short comment about potential alternatives: #2022#issuecomment-413022143. FYI @bmoyles0117 . This is a descriptor for metric labels. metricLabels descriptor specifies a single label as defined here: https://github.com/kubernetes/heapster/blob/master/metrics/core/labels.go#L120. Fixed. Done. Done. Done. Moved back to stackdriver.go. This is overriden by a flag if set. Done. Done. Done. I prefer not doing the refactoring in this PR. After talking to @fgrzadkowski I think it's not that obvious how the code should look like. I've written some example code refactored to a map, which still doesn't look ideal - feel free to talk to me offline and discuss which form is better. In general, I think this is a separate discussion and doesn't belong in this PR.. Added checks, PTAL. I think we should just skip the metric and dependent derived metrics.. This is actually a mistake, removed this if.. For new resources we have different set of metric. I don't think this PR can be sensibly split into \"using new resource model\" and \"using new set of metrics\".. Done. By mistake, when removing and reintroducing code from below.. Moved there. Done. Done. Clarified the comment a little bit, PTAL. Done. Done. Done. Done. Done.. Specified this more precisely in a PR description. done. done. Done.. This will be exported in follow up PR.. Did you investigate a possibility of retrieving PVCs not used by any pod? Otherwise, what value do these metrics add? Heapster already collects (and exports) volume metrics on pod level.. I think Heapster can retrieve PVCs from api server, using core v1 lister: https://github.com/kubernetes/client-go/blob/master/listers/core/v1/persistentvolumeclaim.go. Similar examples are present in processors, for example pod_based_enricher lists all pods and retrieves some information for them: https://github.com/kubernetes/heapster/blob/master/metrics/processors/pod_based_enricher.go\nI prefer this over adding pvc label to existing volume metrics, because this label would be applicable only to particular type of the volume. This would leave us without the association between PVCs and pods, but do we need it?. Actually, I missed a part of it (api server don't have most data about the volumes that we need). So this approach LGTM, this comment is resolved.. Do you mean \"Error creating GCL service\"?. Does this work on AWS? What does it return? Can you explain it somewhere, preferably in a comment for GetProjectId() function?. Done.. Done. Done. Done. Done. I think it actually returns different type.. Done. Please point to the specific section with reference to API documentation: https://cloud.google.com/monitoring/custom-metrics/reading-metrics. The legacy client is used by HPA and kubectl top. The latter case was removed ~1 week ago in #56206. I agree with @DirectXMan12 that we shouldn't change the API version, instead we should remove this endpoint entirely from Heapster at some point (I'd say it's definetely too early for that).. Please use a namespace \"heapster-e2e-tests\" here. Please use different rbac file, for your testing service account. The current one binds cluster role system:heapster to the service account from kube-system namespace.. You will have to call setupHeapsterServiceAccount and setupHeapsterRBAC after the test namespace is created.. We should probably bump the image version to v1.5.1. Yes, if we add a new metric, we should have the same metric for new resource model. Can you provide more context behind adding this metric? We should decide what metric do we need and what monitored resource to use.\nAlso, this metric needs to be defined in Stackdriver first, doesn't it?. Thanks for the context, I agree we should add the metric. I've asked folks from Stackdriver to take a look, when the Stackdriver change is in place we can merge this PR.. Can you put all resource request metrics here, both primary and extended? You can initialize a map with existing primary resource metric. I don't think the logic should be different for extended resources.\nWhat about limits?. Note that we have kube_api.ResourceEphemeralStorage.. Keep in mind that this exposes the metrics for customers using new resource model. New resource model is going to be opt-in in GKE for Kubernetes 1.10 and legacy resource modes is still going to be a default. If you want to expose this metric also to customers with new resource model, please add a second (legacy metric). The appropriate name would be \"container.googleapis.com/container/nvidia_gpu/request\" and it should be reported against legacy \"gke_container\" resource type.. Reverted. I chose this version because the same version is used on master. Bumped to 1.10.5 - I agree that it's a good idea. Prefer not to update to 1.11 yet since the issue with Stackdriver and instance_id hasn't been fully resolved yet.. ",
    "SleepyBrett": "I just found this after seeing the documentation mentioning that kafka doesn't support a sink for events. It certainly used to (back in like 1.2-1.3 if my memory serves). Is there a reason this was removed? . ",
    "dvdklnr": "Thank you for pointing it out, it's not clear from the examples. . yes, please. ",
    "trunet": "any workaround? trying to deploy a brand new heapster and having problems. any news?. ",
    "emansom": "It works again. Thanks guys. \ud83d\udc4d \n\n. ",
    "BruceShenC": "@andyxning I have tried but it still fail. ",
    "juixtu": "@DirectXMan12 thanks! . ",
    "kbroughton": "Regarding the \"Some checks were not successful\" below\nI ran on ubuntu 14.04 with latest go1.8.3 and had no issues.\nNote that i get the Error [sanitize] in the Details link if i have GOPATH set wrong.\nubuntu@jenkins-dev:~/go/src/k8s.io/heapster$ make test-unit\nrm -f heapster\nrm -f eventer\nhooks/check_boilerplate.sh\nhooks/check_gofmt.sh\nhooks/run_vet.sh\nfind . -type f -name \".go\" | grep -v \"./vendor\" | xargs gofmt -s -w\nGOARCH=amd64 CGO_ENABLED=0 go build -ldflags \"-w -X k8s.io/heapster/version.HeapsterVersion=v1.4.0 -X k8s.io/heapster/version.GitCommit=d877a1d\" -o heapster k8s.io/heapster/metrics\nGOARCH=amd64 CGO_ENABLED=0 go build -ldflags \"-w -X k8s.io/heapster/version.HeapsterVersion=v1.4.0 -X k8s.io/heapster/version.GitCommit=d877a1d\" -o eventer k8s.io/heapster/events\nGOARCH=amd64 go test --test.short -race ./...\nok      k8s.io/heapster/common/elasticsearch    1.021s\nok      k8s.io/heapster/common/flags    1.019s\n?       k8s.io/heapster/common/gce  [no test files]\n?       k8s.io/heapster/common/influxdb [no test files]\n?       k8s.io/heapster/common/kafka    [no test files]\n... skipping a few lines\n?       k8s.io/heapster/vendor/k8s.io/metrics/pkg/apis/metrics/v1alpha1 [no test files]\n?       k8s.io/heapster/version [no test files]\nubuntu@jenkins-dev:~/go/src/k8s.io/heapster$. ",
    "camsjams": "I was fooled by this doc for about 5 minutes before I realized how outdated it was.. ",
    "Tenzer": "This sounds very similar to what is talked about here: https://github.com/kubernetes/kubernetes/issues/45790. I'm seeing the same problem and a workaround is to configure Grafana to contact InfluxDB via '127.0.0.1' instead of the service hostname.\nI have not been able to find a way to solve it properly for now.. ",
    "mtanski": "Sorry, did not mean to open this in this repository.. ",
    "MaXinjian": "I have meet the exact same problem.@liyulai2017  Have you solve this problem?. ",
    "fanhongli": "vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\nchange to like this \nEnvironment=\"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml --read-only-port=10255 \"\nthen \nsystemctl daemon-reload\nsystemctl restart kubelet\n. ",
    "allencloud": "I signed it!. Thanks for your feedback and help. @DirectXMan12 \nAnd I am wondering could we make this move on? \ud83c\udf7a . any update ? \u2600\ufe0f . ping @mwielgus @piosz @crassirostris\nCould you help to review this? And feel free to let me know if I missed something.. any update? \u2600\ufe0f . Thanks for your review. @DirectXMan12  I have updated the PR. Actually double review or more is necessary and first we should absolutely make the logic consistent.. Hi, @DirectXMan12 \nIs it OK to attach a ok-to-test comment to this PR?\nSince I think it will trigger another important test.  \ud83d\ude04 \u2600\ufe0f . Could you help to review this PR? @mwielgus @piosz @crassirostris\nAnd please feel free to tell me if I missed something.\nThanks a lot. \ud83d\ude03 \ud83c\udf7b . I have updated the PR. PTAL @andyxning \nThanks . Thanks for your feedback. @DirectXMan12 Totally agree with you.\nNow I have updated this with should not, PATL.. ",
    "gdecroux": "i signed the CLA agreement :-). Actually you are right, the 4.2.0 does exist here (https://console.cloud.google.com/gcr/images/google-containers/GLOBAL/heapster-grafana-amd64) . ",
    "cienijr": "I signed it!. ",
    "davidstack": "I signed it!. oh,i did not use this file.thanks.i closed this pull request. ",
    "justlooks": "i use flannel network solution. i use k8s v1.7 ,strange thing is i download all yaml ,and use \"kubectl apply -f \" ,all work fine. ",
    "jenningsloy318": "just filesystem/usage for pod are missing. \nHeapster: gcr.io/google_containers/heapster-amd64:v1.4.0\nno errors found in heapster\nI0724 07:58:21.379112       1 heapster.go:72] /heapster --source=kubernetes:https://kubernetes.default --sink=influxdb:http://monitoring-influxdb.monitoring.svc:8086\nI0724 07:58:21.379159       1 heapster.go:73] Heapster version v1.4.0\nI0724 07:58:21.379498       1 configs.go:61] Using Kubernetes client with master \"https://kubernetes.default\" and version v1\nI0724 07:58:21.379522       1 configs.go:62] Using kubelet port 10255\nI0724 07:58:26.487975       1 influxdb.go:278] created influxdb sink with options: host:monitoring-influxdb.monitoring.svc:8086 user:root db:k8s\nI0724 07:58:26.488004       1 heapster.go:196] Starting with InfluxDB Sink\nI0724 07:58:26.488009       1 heapster.go:196] Starting with Metric Sink\nI0724 07:58:26.782982       1 heapster.go:106] Starting heapster on port 8082\nI0724 07:59:05.587735       1 influxdb.go:241] Created database \"k8s\" on influxDB server at \"monitoring-influxdb.monitoring.svc:8086\". @DirectXMan12 \nI got some update, now we can see the explicit error message.\nit can't access /var/lib/kubelet.\nI use btrfs as the docker storage backend, \n1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588418(10.58.137.244:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 41 in cached partitions map\"\nE0726 06:18:05.081212       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588417(10.58.137.243:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 44 in cached partitions map\"\nE0726 06:19:05.085131       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588417(10.58.137.243:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 44 in cached partitions map\"\nE0726 06:19:05.085181       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588418(10.58.137.244:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 41 in cached partitions map\"\nE0726 06:20:05.097212       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588417(10.58.137.243:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 44 in cached partitions map\"\nE0726 06:20:05.146152       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588418(10.58.137.244:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 41 in cached partitions map\"\nE0726 06:21:05.082110       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588417(10.58.137.243:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 44 in cached partitions map\"\nE0726 06:21:05.141125       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588418(10.58.137.244:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 41 in cached partitions map\"\nE0726 06:22:05.069680       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588418(10.58.137.244:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 41 in cached partitions map\"\nE0726 06:22:05.083300       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588417(10.58.137.243:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 44 in cached partitions map\"\nE0726 06:23:05.064718       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588418(10.58.137.244:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 41 in cached partitions map\"\nE0726 06:23:05.083943       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588417(10.58.137.243:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 44 in cached partitions map\"\nE0726 06:24:05.057546       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588418(10.58.137.244:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 41 in cached partitions map\"\nE0726 06:24:05.094787       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588417(10.58.137.243:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 44 in cached partitions map\"\nE0726 06:25:05.062773       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588418(10.58.137.244:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 41 in cached partitions map\"\nE0726 06:25:05.084151       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588417(10.58.137.243:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 44 in cached partitions map\"\nE0726 06:26:05.052515       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588418(10.58.137.244:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 41 in cached partitions map\"\nE0726 06:26:05.069117       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588417(10.58.137.243:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 44 in cached partitions map\"\nE0726 06:27:05.065989       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588418(10.58.137.244:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 41 in cached partitions map\"\nE0726 06:27:05.072442       1 summary.go:97] error while getting metrics summary from Kubelet cnpvgl56588417(10.58.137.243:10255): request failed - \"500 Internal Server Error\", response: \"Internal Error: failed RootFsInfo: error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 44 in cached partitions map\". Found this is related with kubelet issue when the storage is btrfs. ",
    "ZhiqinYang": "@DirectXMan12 \nexcuse me? How can I get filesystem/usage with heapster API? I had build an image with the master branch, but I cannot get this metrics? how can I get it?. @DirectXMan12 \nI config use summary_api, i can get filesystem/usage  with /api/vi/model/node/node1/metrics/filesystem/usage?labels=resource_id:/  , but i can't get network/tx , network/rx ;  without summary_api, i can get network/tx, network/rx, but i can't get filesystem/usage . why how can i solve this ? any help?. ",
    "matt-deboer": "Just found that as a workaround, one can extract the correct node list by hitting /api/v1/model/debug/allkeys and doing some string manipulation/deduping.... ",
    "i7clock": "@yin32167 just type in the name of your namespace and it will show your pods.. at least thats what helped me. ",
    "DanyalITMO": "Hi everybody,\nI got the same error as @yin32167. I also have some namespaces like default, kube-public, kube-system, nginx-ingress, test1, test2 and etc. I installed Heapster stack from\ndeploy/kube-config/influxdb/ and\ndeploy/kube-config/rbac/heapster-rbac.yaml\nI install latest version of container:\nheapster-grafana-amd64:v4.4.1\nheapster-amd64:v1.4.1\nheapster-influxdb-amd64:v1.1.1\nBut, when I open URL heapster I see only two namespaces - \"kube-system\" and \"nginx-ingress\".\ncurl http://10.110.178.213/api/v1/model/namespaces/\n[\n  \"nginx-ingress\",\n  \"kube-system\"\nCould you please help me to install Heapster correctly?\nI have the following configuration on all my nodes:\nUbuntu 16.04.2 LTS 4.4.0-79-generic\nKubernetes v1.7.4\nDocker 1.12.6. I solved my problem with heapster. If you want to have actual information in Grafana from database you need to configure Template:\n1. go to YOUR_DASHBOARD (for example Pods) - Templating\n\n2. press \"Edit\" in tab Variables\n3. set fields \"Data source =  influxdb-datasource\" and \"Refresh = On Dashboard Load\"\n\nAfter that you will have actual namespaces and pods list.\n@DanyalITMO\n\nBut when I open URL heapster, I see only two namespaces - \"kube-system\" and \"nginx-ingress\".\n\nJust restart kubelet service on all the nodes. For my Ubuntu 16.04:\n# systemctl restart kubelet\nNow I have all my namespaces and pods in my InfluxDB.. ",
    "yin32167": "@DanyalITMO   thanks very much, i try it and it works. and i find that you needn't restart your kubelet, just press the 'Save & Test' button in 'Data Sources', it will reload the namespaces  . ",
    "zhangqx2010": "Finally I find  cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n[Service]\nEnvironment=\"KUBELET_KUBECONFIG_ARGS=--kubeconfig=/etc/kubernetes/kubelet.conf --require-kubeconfig=true\"\nEnvironment=\"KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true\"\nEnvironment=\"KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin\"\nEnvironment=\"KUBELET_DNS_ARGS=--cluster-dns= 10.96.0.10 --cluster-domain=cluster.local\"\nEnvironment=\"KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt\"\nEnvironment=\"KUBELET_CADVISOR_ARGS=--cadvisor-port=0\"\nEnvironment=\"KUBELET_CGROUP_ARGS=--cgroup-driver=systemd\"\nExecStart=\nExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_CGROUP_ARGS $KUBELET_EXTRA_ARGS\nThe KUBELET_DNS_ARGS caused this issue.. Just do as the comments above.\nYou can see the KUBELET_DNS_ARGS parameter in 10-kubeadm.conf. Change it whatever you like for cluster dns.. Not to delete. Just modify the line in the config\nEnvironment=\"KUBELET_DNS_ARGS=--cluster-dns= 10.96.0.10 --cluster-domain=cluster.local\"\nAssign the IP of your customized DNS IP addr to that parameter.. ",
    "BigbigY": "@ zhangqx2010 Is that delete KUBELET_DNS_ARGS\uff0cCould you please elaborate on how to do it?. ",
    "asifdxtreme": "I signed it!. I signed it!. @andyxning Thanks, I have signed the CLA now.. @DirectXMan12 , you can check the detailed status of the GoReport at https://goreportcard.com/report/github.com/kubernetes/heapster. @andyxning  can you help me to merge this PR. ",
    "fisherxu": "/assign @DirectXMan12. @DirectXMan12 I have already added them, PTAL. Thanks! . @andyxning Can you please help remove the label needs-ok-to-test? Thanks!. @andyxning @DirectXMan12  Can you help merge this pr? thanks!. ping @DirectXMan12 @piosz for review this pr!  Thanks @andyxning. Thanks :) @DirectXMan12. ",
    "Gianman": "Hello,\nAny update/clarification regarding this issue?\nThanks in advance.. ",
    "kinsu": "from the pod_aggregator it appears that this was done on purpose\nhttps://github.com/kubernetes/heapster/blob/master/metrics/processors/pod_aggregator.go#L119\nbut this (at least to me) is kind-of contradictory since heapster is suppose to aggregate the data in pod level.\nsome insight on this would be great!. we tried to change/remove this constrain (i.e. the check for non-cummulative type of metric in pod_aggregator.go) but faced a strange outcome. \nThat is, negative values were visible in usage_rate. \nfrom what I can see, in case we have 1 container within the pod (+ pause container) \n```\npod_aggregator_values#   MetricName:  cpu/usage     Podname:  test-server    OldAggrValueBefore:  159812027  NewMetricValue:  24757079714362 NewAggregatedValue 24757239526389\nrate_calculator#    Pod:  dummypod    metricValNew:  159954312  metricValOld:  24757239526389    NewVal:  -691752\nrate_calculator#    Pod:  dummypod    metricValNew:  24765917979065  metricValOld:  24757079714362    NewVal:  311\n```\nBut also if more than one container exists (apart from the pause container) in the same pod:\nrate_calculator#    Pod:  test-server    metricValNew:  20120578914  metricValOld:  20113753416    NewVal:  0\nrate_calculator#    Pod:  test-server    metricValNew:  19645284455  metricValOld:  19638330290    NewVal:  0\nrate_calculator#    Pod:  test-server    metricValNew:  142284977  metricValOld:  514349464276    NewVal:  -16050\nrate_calculator#    Pod:  test-server    metricValNew:  335822999773  metricValOld:  335774391520    NewVal:  2\nrate_calculator#    Pod:  test-server    metricValNew:  20562033995  metricValOld:  20555225125    NewVal:  0\nrate_calculator#    Pod:  test-server    metricValNew:  41405300433  metricValOld:  41392672637    NewVal:  0\nrate_calculator#    Pod:  test-server    metricValNew:  15943527234  metricValOld:  15939907389    NewVal:  0\nrate_calculator#    Pod:  test-server    metricValNew:  60800262779  metricValOld:  60792898922    NewVal:  0\nwhere the -16050 coming from the subtraction of the current value (142284977) minus this 335774391520 which actually is the aggregated value that produced from the previous (!) run (previous sampling) of the pod_aggregator processor. \n```\npod_aggregator_values#   MetricName:  cpu/usage     Podname:  test-server     OldAggrValueBefore:  498409556887  NewMetricValue:  15939907389 NewAggregatedValue 514349464276\nthe negative value always comes from pause container\n    \"app\": \"myapplication\",\n    \"io.kubernetes.container.name\": \"POD\",\n    \"io.kubernetes.docker.type\": \"podsandbox\",\n    \"io.kubernetes.pod.name\": \"dummypod\",\n    \"io.kubernetes.pod.namespace\": \"default\",\n```\nIf we skip the POD types in rate calculator then the pod aggregated values as well as the values for the pod_containers for cpu usage & cpu usage_rate are OK! (even when pod contains more than one container).\nthe aggregated values are stored in the metricset with a key derived from the ns,podname, while in RateCalculator the \"for loop\" is using a key that is again derived from ns,podname and the key for the pause container appears to be exactly the same as the key for the pod level statistics.\nQuestion is: \nEven though the pause container is type \"pod\" shouldn't heapster mark it differently i.e. following the pattern for containers (namespace:%s/pod:%s/container:%s ) or whatever, \nin order to avoid confusion with pod level aggregated values ?\n. my point was to treat the pause container as other containers. That is:\n```\ndiff --git a/metrics/sources/kubelet/kubelet.go b/metrics/sources/kubelet/kubelet.go\nindex f2d78d0..23f6a2e 100644\n--- a/metrics/sources/kubelet/kubelet.go\n+++ b/metrics/sources/kubelet/kubelet.go\n@@ -98,15 +98,16 @@ func (this kubeletMetricsSource) handleSystemContainer(c cadvisor.ContainerInf\nfunc (this kubeletMetricsSource) handleKubernetesContainer(cName, ns, podName string, c cadvisor.ContainerInfo, cMetrics *MetricSet) string {\n        var metricSetKey string\n-       if cName == infraContainerName {\n-               metricSetKey = PodKey(ns, podName)\n-               cMetrics.Labels[LabelMetricSetType.Key] = MetricSetTypePod\n-       } else {\n-               metricSetKey = PodContainerKey(ns, podName, cName)\n-               cMetrics.Labels[LabelMetricSetType.Key] = MetricSetTypePodContainer\n-               cMetrics.Labels[LabelContainerName.Key] = cName\n-               cMetrics.Labels[LabelContainerBaseImage.Key] = c.Spec.Image\n-       }\n+       // if cName == infraContainerName {\n+       //      fmt.Println(\"kubelet#    podName:\", podName, \"    cName:\", cName)\n+       //      metricSetKey = PodKey(ns, podName)\n+       //      cMetrics.Labels[LabelMetricSetType.Key] = MetricSetTypePod\n+       // } else {\n+       metricSetKey = PodContainerKey(ns, podName, cName)\n+       cMetrics.Labels[LabelMetricSetType.Key] = MetricSetTypePodContainer\n+       cMetrics.Labels[LabelContainerName.Key] = cName\n+       cMetrics.Labels[LabelContainerBaseImage.Key] = c.Spec.Image\n+       //}\n        cMetrics.Labels[LabelPodId.Key] = c.Spec.Labels[kubernetesPodUID]\n        cMetrics.Labels[LabelPodName.Key] = podName\n        cMetrics.Labels[LabelNamespaceName.Key] = ns\n```\nUsing the above, pod levels cummulative counters are working now (correctly aggregated).\nDo you see any problem with the above approach ?\nthank you in advance!. for what i can say this appears that it is not related with influx sink only.\nat least filesystem/available is missing from Kafka sink as well as ElasticSearch sink. are there any news on this? \nAs mentioned above this appear to happen in ES sink too. Shouldn't the title change too ?\n. ",
    "CsatariGergely": "https://github.com/kubernetes/heapster/pull/1754 corrects this. . ",
    "ktsakalozos": "As @luxas suggested, @ixdy and @mkumatag might be able to help. Thanks\ufffc\ufffc\n. ",
    "kerneljack": "@DirectXMan12 thanks, but what is the right method to do that? I was thinking of using ConfigMaps but the Heapster docs don't mention if that's possible or supported. I tried editing the deployment where the addon is defined as I mentioned, but that gets reverted by the cluster pretty quickly.. ",
    "emfree": "Thanks @DirectXMan12! I've updated sink-owners.md in this PR. (I'm responsible for Kubernetes integrations at honeycomb, and worked on this patch with @systemizer.). Wonderful, thank you!! :tada: :tada: . ",
    "alok87": "@DirectXMan12 i agree but this is the version mentioned in the example heapster doc. . also we are facing memory leak which the influx db team says is fixed in newer version - https://github.com/influxdata/influxdb/issues/8718#issuecomment-323360920\nWhen can we upgrade to latest version of influx?. @luxas @miry gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3 is not available. Please some one push it.. @piosz @luxas by when will you be able to push these images?. @piosz can u please push these images?. thanks @piosz works for me as well.. ",
    "swade1987": "@alok87 we are seeing the same issue running version 1.1.1 on Kubernetes 1.7.4.\nOver the past two days, it spiked at 21.41GB memory usage and is now running at 14.30GB.. ",
    "wjiangjay": "@DirectXMan12 Tried with kube-sample-server and can not reproduce this issue.. ",
    "fejta": "We don't use travis\n/assign @krzyzacy . These jobs will start failing in the extremely near future. Please migrate them out of jenkins and into their own GCP project (if this hasn't happened already). ",
    "BenTheElder": "\nI need more context here. What jobs, where is this configured?\n\n\nAll the heapster jobs running in test-infra via jenkins\nhttps://github.com/kubernetes/test-infra, the triggers are in prow/config.yaml and the rest of the config is in jobs/config.json and jenkins/job-configs. \nAny jobs in jenkins/job-configs need to be moved to their own jenkins project or ideally migrated off of jenkins to prow. We are going to shut down the kubernetes jenkins and heapster/ cadvisor tests have been running on these clusters.\n\n/cc @dashpole we discussed the cadvisor jobs and may look at heaspter as well if we have time.. /retest. /test pull-heapster-e2e-prow. /retest. /retest. /test pull-heapster-e2e. /retest. @x13n #1927 is in but this still failed with 1.7.... /retest. /test pull-heapster-e2e-prow. /test pull-heapster-e2e\nThanks @krzyzacy @x13n !\nWe should still port this off of docker-in-docker probably someday but getting it on Prow is good. :smile: . /test pull-heapster-e2e. /ok-to-test. ",
    "acondrat": "It's v1.3.0\nImage:     eu.gcr.io/google_containers/heapster-amd64:v1.3.0. ",
    "Nemesisesq": "I am trying to restart grafana but service is not installed and I have a hard time finding how to restart the grafana-server on this installation. ",
    "glitchcrab": "I also saw the same issues a few hours ago on a newly-provisioned cluster with the latest checkout of head.. @miramar-labs \n\nso who is updating the versions in the yaml files without checking that the images are actually up in the google repo?\n\nI couldn't agree more - such a basic and obvious thing to do.. paging @piosz . Yup, 1.4 was broken so using head is necessary, but broken. . @hbokh I've also been seeing this issue since my initial comment - I tried the workaround before it was suggested but none of the Heapster-provided metrics were available. Ended up deciding to wait for the updated images to be pushed to GCR, however as you note this does seem to be a massive SPOF - its taking a very long time to get two images built/pushed. . ",
    "miramar-labs": "latest images are not in the google repo: \nhttps://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml\nspec:\n      containers:\n      - name: influxdb\n        image: gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3\nI only see v1.1.1 in the google repo:\nhttps://console.cloud.google.com/gcr/images/google-containers/GLOBAL/heapster-influxdb-amd64\nso who is updating the versions in the yaml files without checking that the images are actually up in the google repo? . same thing with : https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yaml. the two URLs are still broken... any idea when this will be fixed?. those Googler's must be super busy ;). no real sense of urgency ... . even if you revert back to the latest images up in the repo, the graphs don't show up anymore... this is because the web app is doing some health check with heapster right at the start and its a one-shot thing .. if it fails you never see the graphs for the lifetime of that pod. The healthcheck used to work but is now failing due to some change in the heapster api whereby it now returns failure until enough data has been scraped .. which takes a couple of minutes. So the trick appears to be - kill the dashboard pod a couple of minutes after your cluster has started up .. this will cause it to reload and this time the initial healthcheck will pass (as you've now got data) .. and you will see the graphs again.. ",
    "igormilovanovic": "same here. ",
    "patricklucas": "To summarize, the latest available influxdb image is v1.1.1 (v1.3.3 referenced in influxdb.yaml) and the latest available grafana image is v4.4.1 (v4.4.3 referenced in grafana.yaml).\nBoth versions were bumped by @luxas on 2017-06-26 in 480a1ca21fe65882dd41f28cd9296a825fbbb602. @luxas could you explain and/or rollback this change?. ",
    "liangrog": "Even release-1.4 can't work.\nIssue with grafana:\nkube-system   monitoring-grafana-762361155-s36lx                                          0/1       CrashLoopBackOff   3          1m\nHere is the log:\nStarting a utility program that will configure Grafana\nStarting Grafana in foreground mode t=2017-08-28T04:37:09+0000 lvl=crit msg=\"Failed to parse /etc/grafana/grafana.ini, open /etc/grafana/grafana.ini: no such file or directory%!(EXTRA []interface {}=[])\"\n. ",
    "AlexB138": "Just to clarify, while waiting for Googlers to push those images, what is the current working config? 1.4.1 doesn't work, and head doesn't work. . ",
    "miguellgt": "I used these images and it works.\ngcr.io/google-containers/heapster-influxdb-amd64:v1.1.1\ngcr.io/google-containers/heapster-grafana-amd64:v4.4.1\n\n. ",
    "hbokh": "Even with the above mentioned workaround, the dashboard is nowhere looking as it used to be.\nAt this moment I can't get it to look like this anymore - https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ There's only a list of pods...\nIs this missing behavior related to these workaround?\nBesides, it seems like there is a SPOF when it comes to releasing and image-pushing in this project.. @piosz Would you be so kind and (make somebody) push the right image-versions before you resume working on https://github.com/kubernetes-incubator/metrics-server? \ud83d\ude07\nMuch appreciated, dzi\u0119kuj\u0119! . ",
    "thenux": "So guys...just to summarize, I have a cluster of 3 nodes in Google Cloud Platform, so I deployed my guestbook and now I want to use Heapster to monitor my metrics.\nI followed the guide with InfluxDB and Grafana in https://github.com/kubernetes/heapster/blob/master/docs/influxdb.md\nBut when I do kubectl create -f deploy/kube-config/influxdb/ I just get ImagePullBackOff from the grafana and influxdb pods...what should I do?. Yep! It seems fine now with the last edit for the new images :). ",
    "heckj": "@luxas @piosz is it now functional for you? I just tried this evening, and only now found this bug. I've been debugging (and could use help debugging) from https://github.com/kubernetes/heapster/issues/1804. I'm not sure the images that were pushed are \"good\"...\nWould love to hear I'm off base and missing something obvious.. Thanks @luxas - I did, before I found this thread: #1804 \n. I'm really not sure where the IP address 198.105.244.23:8088 is coming from. I did a docker pull gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3 to get the image locally, and then looked at the contents in the image itself. When I run it locally on docker (i.e. docker run -it --rm gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3) I get the identical error message:\nconsole\nrun: open server: listen: listen tcp 198.105.244.23:8088: bind: cannot assign requested address\nWhich makes me think the 198.105... address is built in somewhere, but I'm not spotting it. The entrypoint for the container is:\nconsole\n\"Entrypoint\": [\n                \"influxd\",\n                \"--config\",\n                \"/etc/config.toml\"\n            ],\nSo I took at look at the contents in /etc/config.toml, but nothing matched that IP address. The contents of that /etc/config.toml I went ahead and placed into a gist for anyone else looking - https://gist.github.com/heckj/b0aa1f243c1eff766a8bf60a00137fcb\nIf I invoke influxDB directly within the container, not using the configuration, it appears to bind to localhost and start right up (at the default port 8088) - so I'm guessing that perhaps I need to inquire with the folks who make influxDB and see if they have any suggestions.\ninvoking the binary directly, I used docker run -it --rm --entrypoint \"/bin/sh\" gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3 and then just ran\ninfluxd without the --config /etc/config.toml\nand the results:\n```console\n 8888888           .d888 888                   8888888b.  888888b.\n   888            d88P\"  888                   888  \"Y88b 888  \"88b\n   888            888    888                   888    888 888  .88P\n   888   88888b.  888888 888 888  888 888  888 888    888 8888888K.\n   888   888 \"88b 888    888 888  888  Y8bd8P' 888    888 888  \"Y88b\n   888   888  888 888    888 888  888   X88K   888    888 888    888\n   888   888  888 888    888 Y88b 888 .d8\"\"8b. 888  .d88P 888   d88P\n 8888888 888  888 888    888  \"Y88888 888  888 8888888P\"  8888888P\"\n[I] 2017-09-11T00:08:17Z InfluxDB starting, version unknown, branch unknown, commit unknown\n[I] 2017-09-11T00:08:17Z Go version go1.8.3, GOMAXPROCS set to 2\n[I] 2017-09-11T00:08:17Z no configuration provided, using default settings\n[I] 2017-09-11T00:08:17Z Using data dir: /root/.influxdb/data service=store\n[I] 2017-09-11T00:08:17Z opened service service=subscriber\n[I] 2017-09-11T00:08:17Z Starting monitor system service=monitor\n[I] 2017-09-11T00:08:17Z 'build' registered for diagnostics monitoring service=monitor\n[I] 2017-09-11T00:08:17Z 'runtime' registered for diagnostics monitoring service=monitor\n[I] 2017-09-11T00:08:17Z 'network' registered for diagnostics monitoring service=monitor\n[I] 2017-09-11T00:08:17Z 'system' registered for diagnostics monitoring service=monitor\n[I] 2017-09-11T00:08:17Z Starting precreation service with check interval of 10m0s, advance period of 30m0s service=shard-precreation\n[I] 2017-09-11T00:08:17Z Starting snapshot service service=snapshot\n[I] 2017-09-11T00:08:17Z Starting continuous query service service=continuous_querier\n[I] 2017-09-11T00:08:17Z Starting HTTP service service=httpd\n[I] 2017-09-11T00:08:17Z Authentication enabled:false service=httpd\n[I] 2017-09-11T00:08:17Z Listening on HTTP:[::]:8086 service=httpd\n[I] 2017-09-11T00:08:17Z Starting retention policy enforcement service with check interval of 30m0s service=retention\n[I] 2017-09-11T00:08:17Z Listening for signals\n[I] 2017-09-11T00:08:17Z Storing statistics in database '_internal' retention policy 'monitor', at interval 10s service=monitor\n[I] 2017-09-11T00:08:17Z Sending usage statistics to usage.influxdata.com\n```. I made a local change to revert back to influxdb version v1.1.1 per #1783 to see if that would resolve this issue. InfluxDB definitely starts up just fine after that - everything appears to be running, but in the while I was looking at the grafana logs I saw \nconsole\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp 198.105.244.23:3000: i/o timeout. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp 198.105.244.23:3000: i/o timeout. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp 198.105.244.23:3000: i/o timeout. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp 198.105.244.23:3000: i/o timeout. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp 198.105.244.23:3000: i/o timeout. Retrying after 5 seconds...\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp 198.105.244.23:3000: i/o timeout. Retrying after 5 seconds...\nWhich includes that exact same \"from nowhere\" address of 198.105.244.23. I just reverted to Grafana image v4.1.1 and the IP address disappears:\nconsole\nt=2017-09-11T00:40:34+0000 lvl=info msg=\"Created default admin user: admin\"\nt=2017-09-11T00:40:34+0000 lvl=info msg=\"Starting plugin search\" logger=plugins\nt=2017-09-11T00:40:34+0000 lvl=warn msg=\"Plugin dir does not exist\" logger=plugins dir=/usr/share/grafana/data/plugins\nt=2017-09-11T00:40:34+0000 lvl=info msg=\"Plugin dir created\" logger=plugins dir=/usr/share/grafana/data/plugins\nt=2017-09-11T00:40:34+0000 lvl=info msg=\"Initializing CleanUpService\" logger=cleanup\nt=2017-09-11T00:40:34+0000 lvl=info msg=\"Initializing Alerting\" logger=alerting.engine\nt=2017-09-11T00:40:34+0000 lvl=info msg=\"Initializing Stream Manager\"\nt=2017-09-11T00:40:34+0000 lvl=info msg=\"Initializing HTTP Server\" logger=http.server address=0.0.0.0:3000 protocol=http subUrl= socket=\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp 198.105.244.23:3000: i/o timeout. Retrying after 5 seconds.... @andyxning I don't believe so. The address to bind specified in config.toml is bind-address = \"localhost:8088\" (whole file from the 1.3.3 image at https://gist.github.com/heckj/b0aa1f243c1eff766a8bf60a00137fcb).\nThe address that it attempts to bind to (198.105.244.23) appears to be unrelated to the cluster at all - and the same address attempts to be used when I use a docker run command on the same container image on my laptop without any kubernetes infrastructure around it.. Also of note, there's no \"git branch\" or release information that's output from the the binary as there had been in previous versions. \nThe output from the v1.3.3 image reports:\nconsole\n2017-09-11T00:08:17Z InfluxDB starting, version unknown, branch unknown, commit unknown\nThis pattern matches the output in the v1.1.1 image (gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1)\nconsole\n[run] 2017/09/12 21:39:31 InfluxDB starting, version unknown, branch unknown, commit unknown\n[run] 2017/09/12 21:39:31 Go version go1.7.4, GOMAXPROCS set to 2\nI'm not sure if there should be, but it appears to be the expectation of binary that it might have that detail. . AHH! Yeah, that's where the IP address is coming from. I ran \"nslookup localhost\" on my local machine and:\n```console\nnslookup localhost\nServer:     192.168.0.1\nAddress:    192.168.0.1#53\nNon-authoritative answer:\nName:   localhost.Home\nAddress: 198.105.244.23\nName:   localhost.Home\nAddress: 198.105.254.23\n```\nI suppose a relevant question - should \"localhost\" have been set to reference something - is that a step that I missed somewhere in setting up my bare-node kubeadm cluster, or should be added?\nI'll close my PR on reverting these changes, and I think using the ip:name form (or just using the influxdb default listen) makes the most sense if that container doesn't have (or doesn't want to have) an override in /etc/hosts that seems otherwise expected/common.\n. @DirectXMan12 Thank you - the local machines onto which I installed Kubernetes do have localhost referencing their own respective loopbacks, and the upstream DNS from my network is what is providing the response to 'localhost' at the IP address I ran into.\nSince this was happening within a container, would it normally be consider the responsibility of a container to make sure any localhost references were in place, or is this something I should be configuring to make explicit on my base hosts?\nI certainly wouldn't have expected the existing behavior. The machines were set up and configured with leveraging kubeadm, which did most of the configuration and setup. I'm not certain where I could override the \"upstream DNS\" to explicitly make a non-response or local-to-the-container response to localhost in the event it was requested like this image does. Where would I set that, or what system should I add configuration to in order to prevent my upstream DNS from providing that crazy response?. For anyone else following down this same track, or seeing the same results, the upstream provider CenturyLink was providing a response through my home DSL link. While I had made DHCP reservations for the machines that made up my cluster, primary DNS was still be passed down from my service provider.\nI updated /etc/network/interfaces on my ubuntu hosts to explicitly assert DNS servers 8.8.8.8 and 8.8.4.4 with the following lines:\nconsole\ndns-nameservers 8.8.8.8\ndns-nameservers 8.8.4.4\nAnd rebooted the systems. By removing the offending nameserver (which was 205.171.3.25 in my case) that was providing an erroneous answer for localhost, the images and deployment provided worked correctly and started without issue.. I could definitely use some verification, but it appears to be an issue with the binaries that were loaded as \"influxd\" on the images hosted by google. I definitely agree I'd prefer to resolve the issues with the images and not roll back if possible, but frankly I don't know how to do anything about the images themselves. I'm not even sure who to contact, other than the bug I filed (#1804) and put up this PR.. closing. ",
    "ofirmakmal": "I've found an invalid charcter in the configuration string.\nThanks anyway.. ",
    "juanluisvaladas": "I signed it!. /ok-to-test. /ok-to-test. ~@burmanm  @mwringe : Can you review this? Thanks~\nWrong maintainers. Sorry, wrong maintainers quoted.\n@huangyuqi Can you please review this?. @miaoyq could you perhaps check the PR? I see you're a big contributor to this sink. Updated addressing comments.. I've been running this in production for over a month without any issues so far, I'd say it's pretty safe.. Please? :'-(\nAnyone? :'-(\n@huangyuqi  you alive mate???. Thanks, but unfortunately I don't know nearly enough about kafka. I don't think I can take this responsibility in good faith.. ",
    "jtudelag": "LGTM +1. ",
    "rsevilla87": "+1. ",
    "yguo0905": "/assign @piosz \n/assign @loburm \nCould you please take a look and confirm this?. /lgtm. ",
    "JakimLi": "duplicate with #1783 , close the issue. ",
    "thedebugger": "FYI, When importing in Grafana v4.3, dashboard used to be blank. @andyxning it is compatible but since the committed json has a dashboard element, it used to look like this when you import it -- https://gist.github.com/thedebugger/45054f1a009b14d0ea47b1c980950924\nSo to grafana it seems the dashboard is empty - no panels/rows comes up. If you remove the dashboard element, then import works. \nLet me know if you need more information. @andrewsykim my hunch is that something went wrong when we exported the dashboard and committed here? And yes agreed grafana should handle if this is a breaking change. I tried finding the grafana export schema but documentation is very limited. Can you try if you can dig something up?. ",
    "jicki": "@luxas . ",
    "jeroenjacobs1205": "You might take a look here: https://github.com/kubernetes/heapster/releases\nAt the 1.4.0 notes, you can see they fixed some ElasticSearch stuff.. ",
    "gswallow": "Thanks!\nOn Tue, Sep 12, 2017 at 11:13 AM, Jeroen Jacobs notifications@github.com\nwrote:\n\nYou might take a look here: https://github.com/kubernetes/\nheapster/releases\nAt the 1.4.0 notes, you can see they fixed some ElasticSearch stuff.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/1802#issuecomment-328858381,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AEB7qCftDBzdoKhESnq7QYCM0hMHbrf7ks5shouGgaJpZM4PRcln\n.\n. \n",
    "yogeswaran": "I signed it!. @DirectXMan12 - agreed and I have updated sink-owners.md file. @DirectXMan12 - Thanks for reviewing, I have fixed all per your comments.. @DirectXMan12  - did you get a chance to review my changes again? is there anything else need to be done?. ok, I have squashed 7 commits into 2 commits.. @DirectXMan12  - I removed  \"rebased and resolved conflicts\" commit but there is one line conflict in docs/sink-owners.md while adding an entry for statsd. If I resolve this conflict, I will end up with 2 commits.. will enter another clean PR, closing this out.. This PR is replaced by #1848 . @DirectXMan12 this PR is same as #1803 which you have already reviewed, can you please review/approve?. @DirectXMan12 - looks like test is complete, can you merge please?. @bangau1 - can you try removing double-quotes?\nie, try \n--sink=statsd:udp://dd-agent-service.default:8125\ninstead of \n--sink=\"statsd:udp://dd-agent-service.default:8125\"\n. you are right @andyxning , tcp is not part of statsd protocol and udp is the default for statsd.. ",
    "Itfly": "Also got this issue because of our company's internal newwork setting.\nSince influxdb is wrote by go, it can override config with environment variable\nso i resolved it by add a env to influxdb.yaml:\nenv:\n        - name: INFLUXDB_BIND_ADDRESS\n          value: 127.0.0.1:8088. @qiyongxiao \nYou should set the env in your deployment's pod template spec, like this:\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: monitoring-influxdb\n  namespace: kube-system\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: influxdb\n    spec:\n      containers:\n      - name: influxdb\n        image: k8s.gcr.io/heapster-influxdb-amd64:v1.3.3\n        env:\n        - name: INFLUXDB_BIND_ADDRESS\n          value: 127.0.0.1:8088\n        volumeMounts:\n        - mountPath: /data\n          name: influxdb-storage\n      volumes:\n      - name: influxdb-storage\n        emptyDir: {}. ",
    "qiyongxiao": "@Itfly Can u tell me where the env to added? It's in Service.spec or Deployment.spec? I added it to this, but got an error \nerror validating \"influxdb.yaml\": error validating data: ValidationError(Service.spec): unknown field \"env\" in io.k8s.api.core.v1.ServiceSpec; if you choose to ignore these errors, turn validation off with --validate=false\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    task: monitoring\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: monitoring-influxdb\n  name: monitoring-influxdb\n  namespace: kube-system\nspec:\n  ports:\n  - port: 8086\n    targetPort: 8086\n  selector:\n    k8s-app: influxdb\n  env:\n  - name: INFLUXDB_BIND_ADDRESS\n    value: 127.0.0.1:8088. ",
    "jmgao1983": "@DirectXMan12 \nMy grafana image is: gcr.io/google_containers/heapster-grafana-amd64:v4.2.0\nlogs\nbash\nStarting a utility program that will configure Grafana\nStarting Grafana in foreground mode\nCan't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp 127.0.0.1:3000: getsockopt: connection refused. Retrying after 5 seconds...\nt=2017-09-15T01:37:58+0000 lvl=crit msg=\"Failed to parse /etc/grafana/grafana.ini, open /etc/grafana/grafana.ini: no such file or directory%!(EXTRA []interface {}=[])\"\nthen I tried another image:gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 , and it ran up fine, could it be the image's fault?. ",
    "JuneZhao": "@DirectXMan12  let me first attach the pods info:\nzjune@k8s-master-ECC3CC6B-0:~$ kubectl get pods --all-namespaces\nNAMESPACE     NAME                                                READY     STATUS        RESTARTS   AGE\ndefault       discuz-1878213324-8jr94                             1/1       Running       12         7d\ndefault       keystone-3099452964-f33bm                           1/1       Running       45         7d\ndefault       mongo-0                                             2/2       Running       14         7d\ndefault       myk8s--release-datadog-jgvfg                        1/1       Running       36         34d\ndefault       myk8s--release-datadog-qmq55                        1/1       Running       36         34d\ndefault       myk8s--release-kube-state-metrics-694891158-pv6kf   1/1       Running       8          7d\ndefault       splunk-1114074158-n3n5z                             1/1       Running       8          7d\ndefault       ssr-935902835-34pgr                                 1/1       Running       8          7d\ndefault       wordpress-4094249903-kp6jv                          1/1       Running       7          7d\ndefault       wordpress-mysql-3536461403-03z3n                    1/1       Running       8          7d\n**kube-system   heapster-2708163903-4dvdv                           0/2       Terminating   0          1m\nkube-system   heapster-2708163903-zxrq7                           2/2       Running       0          4s\nkube-system   heapster-867061013-bpc6l                            0/2       Terminating   0          9s**\nkube-system   kube-addon-manager-k8s-master-ecc3cc6b-0            1/1       Running       45         20d\nkube-system   kube-apiserver-k8s-master-ecc3cc6b-0                1/1       Running       45         20d\nkube-system   kube-controller-manager-k8s-master-ecc3cc6b-0       1/1       Running       121        20d\nkube-system   kube-dns-v20-5nxlr                                  3/3       Running       24         7d\nkube-system   kube-dns-v20-nb5k7                                  3/3       Running       24         7d\nkube-system   kube-proxy-8bcw4                                    1/1       Running       44         41d\nkube-system   kube-proxy-ff6c6                                    1/1       Running       45         41d\nkube-system   kube-scheduler-k8s-master-ecc3cc6b-0                1/1       Running       125        20d\nkube-system   kubernetes-dashboard-3995387264-4ht1g               1/1       Running       7          7d\nkube-system   tiller-deploy-3019006398-b83zr                      1/1       Running       9          7d\nThe whole log while I am logging\nzjune@k8s-master-ECC3CC6B-0:~$ kubectl get pods --all-namespaces\nNAMESPACE     NAME                                                READY     STATUS        RESTARTS   AGE\ndefault       discuz-1878213324-8jr94                             1/1       Running       12         7d\ndefault       keystone-3099452964-f33bm                           1/1       Running       45         7d\ndefault       mongo-0                                             2/2       Running       14         7d\ndefault       myk8s--release-datadog-jgvfg                        1/1       Running       36         34d\ndefault       myk8s--release-datadog-qmq55                        1/1       Running       36         34d\ndefault       myk8s--release-kube-state-metrics-694891158-pv6kf   1/1       Running       8          7d\ndefault       splunk-1114074158-n3n5z                             1/1       Running       8          7d\ndefault       ssr-935902835-34pgr                                 1/1       Running       8          7d\ndefault       wordpress-4094249903-kp6jv                          1/1       Running       7          7d\ndefault       wordpress-mysql-3536461403-03z3n                    1/1       Running       8          7d\nkube-system   heapster-2708163903-4dvdv                           0/2       Terminating   0          1m\nkube-system   heapster-2708163903-zxrq7                           2/2       Running       0          4s\nkube-system   heapster-867061013-bpc6l                            0/2       Terminating   0          9s\nkube-system   kube-addon-manager-k8s-master-ecc3cc6b-0            1/1       Running       45         20d\nkube-system   kube-apiserver-k8s-master-ecc3cc6b-0                1/1       Running       45         20d\nkube-system   kube-controller-manager-k8s-master-ecc3cc6b-0       1/1       Running       121        20d\nkube-system   kube-dns-v20-5nxlr                                  3/3       Running       24         7d\nkube-system   kube-dns-v20-nb5k7                                  3/3       Running       24         7d\nkube-system   kube-proxy-8bcw4                                    1/1       Running       44         41d\nkube-system   kube-proxy-ff6c6                                    1/1       Running       45         41d\nkube-system   kube-scheduler-k8s-master-ecc3cc6b-0                1/1       Running       125        20d\nkube-system   kubernetes-dashboard-3995387264-4ht1g               1/1       Running       7          7d\nkube-system   tiller-deploy-3019006398-b83zr                      1/1       Running       9          7d\nzjune@k8s-master-ECC3CC6B-0:~$ kubectl logs heapster-2708163903-zxrq7  -n kube-system\nError from server (NotFound): pods \"heapster-2708163903-zxrq7\" not found\nzjune@k8s-master-ECC3CC6B-0:~$ kubectl get pods --all-namespaces\nNAMESPACE     NAME                                                READY     STATUS    RESTARTS   AGE\ndefault       discuz-1878213324-8jr94                             1/1       Running   12         7d\ndefault       keystone-3099452964-f33bm                           1/1       Running   45         7d\ndefault       mongo-0                                             2/2       Running   14         7d\ndefault       myk8s--release-datadog-jgvfg                        1/1       Running   36         34d\ndefault       myk8s--release-datadog-qmq55                        1/1       Running   36         34d\ndefault       myk8s--release-kube-state-metrics-694891158-pv6kf   1/1       Running   8          7d\ndefault       splunk-1114074158-n3n5z                             1/1       Running   8          7d\ndefault       ssr-935902835-34pgr                                 1/1       Running   8          7d\ndefault       wordpress-4094249903-kp6jv                          1/1       Running   7          7d\ndefault       wordpress-mysql-3536461403-03z3n                    1/1       Running   8          7d\nkube-system   heapster-2708163903-p4rgj                           2/2       Running   0          40s\nkube-system   kube-addon-manager-k8s-master-ecc3cc6b-0            1/1       Running   45         20d\nkube-system   kube-apiserver-k8s-master-ecc3cc6b-0                1/1       Running   45         20d\nkube-system   kube-controller-manager-k8s-master-ecc3cc6b-0       1/1       Running   121        20d\nkube-system   kube-dns-v20-5nxlr                                  3/3       Running   24         7d\nkube-system   kube-dns-v20-nb5k7                                  3/3       Running   24         7d\nkube-system   kube-proxy-8bcw4                                    1/1       Running   44         41d\nkube-system   kube-proxy-ff6c6                                    1/1       Running   45         41d\nkube-system   kube-scheduler-k8s-master-ecc3cc6b-0                1/1       Running   125        20d\nkube-system   kubernetes-dashboard-3995387264-4ht1g               1/1       Running   7          7d\nkube-system   tiller-deploy-3019006398-b83zr                      1/1       Running   9          7d\nzjune@k8s-master-ECC3CC6B-0:~$ kubectl logs heapster-2708163903-p4rgj  -n kube-system\nError from server (NotFound): pods \"heapster-2708163903-p4rgj\" not found\nzjune@k8s-master-ECC3CC6B-0:~$ kubectl logs heapster-2708163903-p4rgj  -n kube-system\nError from server (NotFound): pods \"heapster-2708163903-p4rgj\" not found\nzjune@k8s-master-ECC3CC6B-0:~$ kubectl get pods --all-namespaces\nNAMESPACE     NAME                                                READY     STATUS    RESTARTS   AGE\ndefault       discuz-1878213324-8jr94                             1/1       Running   12         7d\ndefault       keystone-3099452964-f33bm                           1/1       Running   45         7d\ndefault       mongo-0                                             2/2       Running   14         7d\ndefault       myk8s--release-datadog-jgvfg                        1/1       Running   36         34d\ndefault       myk8s--release-datadog-qmq55                        1/1       Running   36         34d\ndefault       myk8s--release-kube-state-metrics-694891158-pv6kf   1/1       Running   8          7d\ndefault       splunk-1114074158-n3n5z                             1/1       Running   8          7d\ndefault       ssr-935902835-34pgr                                 1/1       Running   8          7d\ndefault       wordpress-4094249903-kp6jv                          1/1       Running   7          7d\ndefault       wordpress-mysql-3536461403-03z3n                    1/1       Running   8          7d\nkube-system   heapster-2708163903-4p4bs                           2/2       Running   0          12s\nkube-system   kube-addon-manager-k8s-master-ecc3cc6b-0            1/1       Running   45         20d\nkube-system   kube-apiserver-k8s-master-ecc3cc6b-0                1/1       Running   45         20d\nkube-system   kube-controller-manager-k8s-master-ecc3cc6b-0       1/1       Running   121        20d\nkube-system   kube-dns-v20-5nxlr                                  3/3       Running   24         7d\nkube-system   kube-dns-v20-nb5k7                                  3/3       Running   24         7d\nkube-system   kube-proxy-8bcw4                                    1/1       Running   44         41d\nkube-system   kube-proxy-ff6c6                                    1/1       Running   45         41d\nkube-system   kube-scheduler-k8s-master-ecc3cc6b-0                1/1       Running   125        20d\nkube-system   kubernetes-dashboard-3995387264-4ht1g               1/1       Running   7          7d\nkube-system   tiller-deploy-3019006398-b83zr                      1/1       Running   9          7d\nzjune@k8s-master-ECC3CC6B-0:~$ kubectl logs heapster-2708163903-4p4bs  -n kube-system\nError from server (BadRequest): a container name must be specified for pod heapster-2708163903-4p4bs, choose one of: [heapster heapster-nanny]\nzjune@k8s-master-ECC3CC6B-0:~$ kubectl logs heapster-2708163903-4p4bs  -n kube-system heapster\nI0919 02:23:55.459423       1 heapster.go:72] /heapster --source=kubernetes.summary_api:\"\"\nI0919 02:23:55.459602       1 heapster.go:73] Heapster version v1.3.0\nI0919 02:23:55.460160       1 configs.go:61] Using Kubernetes client with master \"https://10.0.0.1:443\" and version v1\nI0919 02:23:55.460337       1 configs.go:62] Using kubelet port 10255\nI0919 02:23:55.462527       1 heapster.go:196] Starting with Metric Sink\nI0919 02:23:55.565644       1 heapster.go:106] Starting heapster on port 8082\nzjune@k8s-master-ECC3CC6B-0:~$ kubectl logs heapster-2708163903-4p4bs  -n kube-system heapster-nanny\nI0919 02:23:54.463166       1 pod_nanny.go:56] Invoked by [/pod_nanny --cpu=80m --extra-cpu=0.5m --memory=140Mi --extra-memory=4Mi --threshold=5 --deployment=heapster --container=heapster --poll-period=300000 --estimator=exponential]\nI0919 02:23:54.463522       1 pod_nanny.go:68] Watching namespace: kube-system, pod: heapster-2708163903-4p4bs, container: heapster.\nI0919 02:23:54.463584       1 pod_nanny.go:69] cpu: 80m, extra_cpu: 0.5m, memory: 140Mi, extra_memory: 4Mi, storage: MISSING, extra_storage: 0Gi\nI0919 02:23:54.468723       1 pod_nanny.go:110] Resources: [{Base:{i:{value:80 scale:-3} d:{Dec:<nil>} s:80m Format:DecimalSI} ExtraPerNode:{i:{value:5 scale:-4} d:{Dec:<nil>} s: Format:DecimalSI} Name:cpu} {Base:{i:{value:146800640 scale:0} d:{Dec:<nil>} s:140Mi Format:BinarySI} ExtraPerNode:{i:{value:4194304 scale:0} d:{Dec:<nil>} s:4Mi Format:BinarySI} Name:memory}]\nYou will notice the heapster deployment fail several times I would recommend to read the output to check heapster fail reason. /close. @kubernetes/sig-azure-bugs. @kubernetes/sig-scalability-bugs. @DirectXMan12  how can we check that, it is managed  by azure, i dont know where to check. ",
    "arindam00": "I am also facing the same issue with Heapster version 1.3.0. Is there a bug with heapster version 1.3.0 ?. ",
    "jackyycheng": "I'm also seeing this issue as well.... ",
    "feiskyer": "I think it's not a heapster problem, we encounter same problem with v1.4.x images.\nA quick fix for the problem is removing the label from heapster deployment: kubernetes.io/cluster-service: \"true\".\n. ",
    "aaryabhatt": "or do I need to follow this to redeploy node certificates\nhttps://docs.openshift.com/container-platform/3.3/install_config/redeploying_certificates.html#redeploying-node-certificates. Thanks DirectXMan12, I asked it here because did not get any response on openshift origin for this same issue and found something similar issue here and openshift origin based on kubernetes. . ",
    "felixPG": "@DirectXMan12 read again my post please! READ PLEASE!. @DirectXMan12 was problem of ingress controller due to bug of kubernetes. Thanks. Ok, I make the same work and all ok, but the option of values with plugins to install is a good way.. ",
    "atzannes": "I was invited and joined the LFID group of AltSchool contributors. I also associated my github account with the LFID. @DirectXMan12 thanks for the review and bare with me as I am a novice go programmer. I am using the summary source and didn't have to modify anything else to get the filesystem/available metric. . Apologies for being MIA, I've been relocating across the Atlantic. Was the retest needed for flaky tests? Is the PR ready to merge now?. @DirectXMan12 rebased and switched to using host.String() as you said. Good catch, thanks!. Anything else we need to wrap this up?. it's my pleasure. Apologies for being MIA. Is this ready PR to merge?. I'm wondering if it is preferable to return the URL object instead of the stirng here. All uses of this function at this point just use the String, so that's why I opted for returning the string instead of the object, but I'm on the fence.. Switched to using host.String() after rebasing.. ",
    "sgmiller": "v1.4.1. ",
    "mikebryant": "Bumping the log level up to 8:\n```\nI1009 11:28:05.216627       1 round_trippers.go:420] Response Status: 200 OK in 78 milliseconds\nI1009 11:28:05.216651       1 round_trippers.go:423] Response Headers:\nI1009 11:28:05.216656       1 round_trippers.go:426]     Content-Type: application/json\nI1009 11:28:05.216659       1 round_trippers.go:426]     Date: Mon, 09 Oct 2017 11:28:05 GMT\nI1009 11:28:05.222644       1 round_trippers.go:420] Response Status: 200 OK in 100 milliseconds\nI1009 11:28:05.222661       1 round_trippers.go:423] Response Headers:\nI1009 11:28:05.222665       1 round_trippers.go:426]     Content-Type: application/json\nI1009 11:28:05.222667       1 round_trippers.go:426]     Date: Mon, 09 Oct 2017 11:28:05 GMT\nI1009 11:28:05.225100       1 round_trippers.go:420] Response Status: 200 OK in 146 milliseconds\nI1009 11:28:05.225108       1 round_trippers.go:423] Response Headers:\nI1009 11:28:05.225111       1 round_trippers.go:426]     Content-Type: application/json\nI1009 11:28:05.225114       1 round_trippers.go:426]     Date: Mon, 09 Oct 2017 11:28:05 GMT\nI1009 11:28:05.229749       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-system/pod:weave-net-pq7f1/container:weave. Create time of containers are 2017-10-05 03:13:21 +0000 UTC and 2017-10-05 03:18:24 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:05.254827       1 round_trippers.go:420] Response Status: 200 OK in 113 milliseconds\nI1009 11:28:05.254850       1 round_trippers.go:423] Response Headers:\nI1009 11:28:05.254854       1 round_trippers.go:426]     Content-Type: application/json\nI1009 11:28:05.254858       1 round_trippers.go:426]     Date: Mon, 09 Oct 2017 11:28:05 GMT\nI1009 11:28:06.243604       1 round_trippers.go:420] Response Status: 200 OK in 1163 milliseconds\nI1009 11:28:06.243631       1 round_trippers.go:423] Response Headers:\nI1009 11:28:06.243635       1 round_trippers.go:426]     Date: Mon, 09 Oct 2017 11:28:06 GMT\nI1009 11:28:06.243638       1 round_trippers.go:426]     Content-Type: application/json\nI1009 11:28:06.363634       1 round_trippers.go:420] Response Status: 200 OK in 1280 milliseconds\nI1009 11:28:06.363657       1 round_trippers.go:423] Response Headers:\nI1009 11:28:06.363661       1 round_trippers.go:426]     Content-Type: application/json\nI1009 11:28:06.363664       1 round_trippers.go:426]     Date: Mon, 09 Oct 2017 11:28:06 GMT\nI1009 11:28:07.273312       1 round_trippers.go:420] Response Status: 200 OK in 2064 milliseconds\nI1009 11:28:07.273339       1 round_trippers.go:423] Response Headers:\nI1009 11:28:07.273344       1 round_trippers.go:426]     Date: Mon, 09 Oct 2017 11:28:07 GMT\nI1009 11:28:07.273347       1 round_trippers.go:426]     Content-Type: application/json\nI1009 11:28:07.276144       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-monitoring/pod:weave-scope-agent-826w5/container:agent. Create time of containers are 2017-10-01 01:43:20 +0000 UTC and 2017-10-01 01:41:55 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.284488       1 round_trippers.go:420] Response Status: 200 OK in 18129 milliseconds\nI1009 11:28:23.284570       1 round_trippers.go:423] Response Headers:\nI1009 11:28:23.284575       1 round_trippers.go:426]     Content-Type: application/json\nI1009 11:28:23.284578       1 round_trippers.go:426]     Date: Mon, 09 Oct 2017 11:28:22 GMT\nI1009 11:28:23.328078       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-extra/pod:container-log-ingest-bf032/container:ingest. Create time of containers are 2017-10-04 15:53:48 +0000 UTC and 2017-10-09 09:34:52 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.328150       1 summary.go:202] Metrics reported from two containers with the same key: namespace:eventrouterkafka/pod:zookeeper-server-2/container:zookeeper-server. Create time of containers are 2017-10-05 00:59:50 +0000 UTC and 2017-10-04 15:53:46 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.328189       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-extra/pod:registry-mirror-gcr-d5pnn/container:mirror-hostess. Create time of containers are 2017-10-09 11:02:08 +0000 UTC and 2017-10-05 00:35:19 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.328202       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-extra/pod:registry-mirror-gcr-d5pnn/container:certificate-installation. Create time of containers are 2017-10-05 01:00:06 +0000 UTC and 2017-10-04 15:53:36 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.328234       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-system/pod:kube-apiserver-1890097792-kkhm8/container:kube-apiserver. Create time of containers are 2017-10-05 00:59:50 +0000 UTC and 2017-10-04 15:53:06 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.328269       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-monitoring/pod:weave-scope-agent-wbvxr/container:agent. Create time of containers are 2017-10-04 15:53:58 +0000 UTC and 2017-10-04 15:52:44 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.328283       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-monitoring/pod:weave-scope-agent-wbvxr/container:agent. Create time of containers are 2017-09-29 23:38:01 +0000 UTC and 2017-10-04 15:53:58 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.328291       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-monitoring/pod:weave-scope-agent-wbvxr/container:agent. Create time of containers are 2017-10-05 01:01:01 +0000 UTC and 2017-10-04 15:53:58 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.328301       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-monitoring/pod:weave-scope-agent-wbvxr/container:agent. Create time of containers are 2017-09-30 19:23:05 +0000 UTC and 2017-10-05 01:01:01 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.328308       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-monitoring/pod:weave-scope-agent-wbvxr/container:agent. Create time of containers are 2017-10-02 05:08:25 +0000 UTC and 2017-10-05 01:01:01 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.328332       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-extra/pod:registry-mirror-quay-jzlh9/container:certificate-installation. Create time of containers are 2017-10-05 01:00:06 +0000 UTC and 2017-10-04 15:53:36 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.412037       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-extra/pod:registry-mirror-quay-jzlh9/container:mirror-hostess. Create time of containers are 2017-10-05 00:04:53 +0000 UTC and 2017-10-09 11:24:58 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.412129       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-extra/pod:default-http-backend-1833742259-k010s/container:default-http-backend. Create time of containers are 2017-10-05 10:32:16 +0000 UTC and 2017-10-04 15:53:46 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.412159       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-system/pod:pod-checkpointer-kubernetes-cit-kubernetes-cr0-2-1506506680/container:checkpoint. Create time of containers are 2017-10-05 00:59:50 +0000 UTC and 2017-10-04 15:53:46 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.412183       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-e2etests-http-update/pod:e2etestapp-3929767995-5k1rb/container:testapp. Create time of containers are 2017-10-05 01:01:01 +0000 UTC and 2017-10-04 22:53:29 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.412216       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-e2etests-deployment-service/pod:e2etestapp-395387445-h5ntp/container:testapp. Create time of containers are 2017-10-05 01:01:01 +0000 UTC and 2017-10-04 22:53:17 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.412253       1 summary.go:202] Metrics reported from two containers with the same key: namespace:kube-system/pod:kube-controller-manager-3400949544-fmrrt/container:kube-controller-manager. Create time of containers are 2017-10-04 22:59:51 +0000 UTC and 2017-10-05 01:00:06 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.412271       1 summary.go:202] Metrics reported from two containers with the same key: namespace:eventrouterkafka/pod:kafka-server-2/container:kafka-server. Create time of containers are 2017-10-05 05:02:41 +0000 UTC and 2017-10-05 11:54:47 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.412282       1 summary.go:202] Metrics reported from two containers with the same key: namespace:eventrouterkafka/pod:kafka-server-2/container:kafka-server. Create time of containers are 2017-10-05 07:42:47 +0000 UTC and 2017-10-05 11:54:47 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.412289       1 summary.go:202] Metrics reported from two containers with the same key: namespace:eventrouterkafka/pod:kafka-server-2/container:kafka-server. Create time of containers are 2017-10-05 01:53:27 +0000 UTC and 2017-10-05 11:54:47 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.412295       1 summary.go:202] Metrics reported from two containers with the same key: namespace:eventrouterkafka/pod:kafka-server-2/container:kafka-server. Create time of containers are 2017-10-05 05:46:23 +0000 UTC and 2017-10-05 11:54:47 +0000 UTC. Metrics from the older container are going to be dropped.\nI1009 11:28:23.412303       1 summary.go:202] Metrics reported from two containers with the same key: namespace:eventrouterkafka/pod:kafka-server-2/container:kafka-server. Create time of containers are 0001-01-01 00:00:00 +0000 UTC and 2017-10-05 11:54:47 +0000 UTC. Metrics from the older container are going to be dropped.\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x18 pc=0x1591838]\ngoroutine 257 [running]:\nk8s.io/heapster/metrics/sources/summary.containerIsTerminated(0xc42104f2c0, 0xed1681487, 0xc400000000, 0x25816e0, 0xc42102d338)\n    /go/src/k8s.io/heapster/metrics/sources/summary/summary.go:213 +0x48\nk8s.io/heapster/metrics/sources/summary.(summaryMetricsSource).decodePodStats(0xc421c0c000, 0xc42164bbf0, 0xc42164bc20, 0xc421d79be8)\n    /go/src/k8s.io/heapster/metrics/sources/summary/summary.go:203 +0xaf1\nk8s.io/heapster/metrics/sources/summary.(summaryMetricsSource).decodeSummary(0xc421c0c000, 0xc4211a9f80, 0x0)\n    /go/src/k8s.io/heapster/metrics/sources/summary/summary.go:131 +0x336\nk8s.io/heapster/metrics/sources/summary.(summaryMetricsSource).ScrapeMetrics(0xc421c0c000, 0xed16d5404, 0xc400000000, 0x25816e0, 0xed16d5440, 0xc400000000, 0x25816e0, 0xa)\n    /go/src/k8s.io/heapster/metrics/sources/summary/summary.go:101 +0x262\nk8s.io/heapster/metrics/sources.scrape(0x23f6f00, 0xc421c0c000, 0xed16d5404, 0xc400000000, 0x25816e0, 0xed16d5440, 0x0, 0x25816e0, 0x0)\n    /go/src/k8s.io/heapster/metrics/sources/manager.go:169 +0x2a0\nk8s.io/heapster/metrics/sources.(sourceManager).ScrapeMetrics.func1(0xc42108b130, 0x23f6f00, 0xc421c0c000, 0xc421b04a20, 0xed16d5404, 0x0, 0x25816e0, 0xed16d5440, 0x0, 0x25816e0, ...)\n    /go/src/k8s.io/heapster/metrics/sources/manager.go:99 +0x155\ncreated by k8s.io/heapster/metrics/sources.(*sourceManager).ScrapeMetrics\n    /go/src/k8s.io/heapster/metrics/sources/manager.go:115 +0x387\n```. @loburm Sure, How's that?. The commit you linked to removed checking those variables entirely.\nI've kept the tests though, in case someone references them again in future.. ",
    "Random-Liu": "@piosz Can we update heapster to fix this? https://github.com/kubernetes/heapster/issues/1838\nI also hit this issue.. Thanks a lot!\nOn Nov 2, 2017 12:25 AM, \"Marian Lobur\" notifications@github.com wrote:\n\nI'll try to patch and prepare a new release soon.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/kubernetes/heapster/issues/1838#issuecomment-341338156,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AFjVu5YAb_wNTBD839Q5lBVON3zDH-Ayks5syW5bgaJpZM4PySHX\n.\n. @loburm Thanks a lot! We really need this. If there is anything I could help, feel free to tell me!. \n",
    "Joseph-Irving": "Ah right thanks, I got a bit confused as it mentioned using the metrics-server but in the before upgrading section:\nThe metrics APIs, custom-metrics.metrics.k8s.io and metrics, were moved from v1alpha1 to v1beta1, and renamed to custom.metrics.k8s.io and metrics.k8s.io, respectively. If you have deployed a custom metrics adapter, ensure that it supports the new API version. If you have deployed Heapster in aggregated API server mode, upgrade Heapster to support the latest API version.\nWhich I assumed meant that heapster would be still be supporting the metrics api.. So I did a bit of digging and it seems that the changes to vendor/k8s.io/client-go/tools/cache/reflector.go is causing the server to never start. It used to start a new goroutine but that no longer happens. By having the reflector run in its own routine go reflector.Run(wait.NeverStop) I managed to get it to start however I think it's now starting before populating nodes, pods etc and causing other issues.. ",
    "owain-je": "So this has been running for a few days and I still see the same. \nI1019 13:45:08.998050       1 handlers.go:264] No metrics for container hyperkube in pod kube-system/promop-prometheus-operator-get-crd-fm9jz\nI1019 13:45:08.998068       1 handlers.go:215] No metrics for pod kube-system/promop-prometheus-operator-get-crd-fm9jz\nI1019 13:45:08.998103       1 handlers.go:264] No metrics for container hyperkube in pod infra/p-prom-op-prometheus-operator-get-crd-0cq19\nI1019 13:45:08.998109       1 handlers.go:215] No metrics for pod infra/p-prom-op-prometheus-operator-get-crd-0cq19\nI1019 13:45:08.998134       1 handlers.go:264] No metrics for container hyperkube in pod kube-system/promop1-prometheus-operator-get-crd-nl47c\nI1019 13:45:08.998138       1 handlers.go:215] No metrics for pod kube-system/promop1-prometheus-operator-get-crd-nl47c\nI1019 13:45:08.998156       1 handlers.go:264] No metrics for container hyperkube in pod default/tailored-hound-prometheus-operator-get-tprs-vs7d7\nI1019 13:45:08.998159       1 handlers.go:215] No metrics for pod default/tailored-hound-prometheus-operator-get-tprs-vs7d7\nI1019 13:45:08.998212       1 handlers.go:264] No metrics for container hyperkube in pod infra/p-prom-op1-prometheus-operator-get-crd-h8wlp\nI1019 13:45:08.998215       1 handlers.go:215] No metrics for pod infra/p-prom-op1-prometheus-operator-get-crd-h8wlp\nI1019 13:45:08.998267       1 handlers.go:264] No metrics for container hyperkube in pod infra/promop-prometheus-operator-get-crd-c3bj3\nI1019 13:45:08.998270       1 handlers.go:215] No metrics for pod infra/promop-prometheus-operator-get-crd-c3bj3\nI1019 13:45:08.998277       1 handlers.go:264] No metrics for container hyperkube in pod infra/p-prom-op1-prometheus-operator-create-sm-job-3lq8v\nI1019 13:45:08.998279       1 handlers.go:215] No metrics for pod infra/p-prom-op1-prometheus-operator-create-sm-job-3lq8v\nI1019 13:45:08.998310       1 handlers.go:264] No metrics for container hyperkube in pod kube-system/promop1-prometheus-operator-create-sm-job-ck2ms\nI1019 13:45:08.998313       1 handlers.go:215] No metrics for pod kube-system/promop1-prometheus-operator-create-sm-job-ck2ms\nI1019 13:45:08.998325       1 handlers.go:264] No metrics for container hyperkube in pod kube-system/promop-prometheus-operator-create-sm-job-0nhp0\nI1019 13:45:08.998328       1 handlers.go:215] No metrics for pod kube-system/promop-prometheus-operator-create-sm-job-0nhp0\nI1019 13:45:08.998332       1 handlers.go:264] No metrics for container hyperkube in pod infra/p-prom-op-prometheus-operator-create-sm-job-7g88t\nI1019 13:45:08.998335       1 handlers.go:215] No metrics for pod infra/p-prom-op-prometheus-operator-create-sm-job-7g88t\nI1019 13:45:08.998356       1 handlers.go:264] No metrics for container hyperkube in pod infra/promop-prometheus-operator-create-sm-job-5xn34\nI1019 13:45:08.998359       1 handlers.go:215] No metrics for pod infra/promop-prometheus-operator-create-sm-job-5xn34\nI1019 13:45:08.998477       1 handlers.go:264] No metrics for container hyperkube in pod infra/p-prom-op-prometheus-operator-get-tprs-h8fnd\nI1019 13:45:08.998481       1 handlers.go:215] No metrics for pod infra/p-prom-op-prometheus-operator-get-tprs-h8fnd\nI1019 13:45:14.197903       1 handlers.go:264] No metrics for container hyperkube in pod infra/promop-prometheus-operator-create-sm-job-5xn34\nI1019 13:45:14.197921       1 handlers.go:215] No metrics for pod infra/promop-prometheus-operator-create-sm-job-5xn34\nI1019 13:45:14.198052       1 handlers.go:264] No metrics for container hyperkube in pod infra/p-prom-op-prometheus-operator-get-tprs-h8fnd\nI1019 13:45:14.198060       1 handlers.go:215] No metrics for pod infra/p-prom-op-prometheus-operator-get-tprs-h8fnd\nI1019 13:45:14.198108       1 handlers.go:264] No metrics for container hyperkube in pod kube-system/promop-prometheus-operator-get-crd-fm9jz\nI1019 13:45:14.198111       1 handlers.go:215] No metrics for pod kube-system/promop-prometheus-operator-get-crd-fm9jz\nI1019 13:45:14.198152       1 handlers.go:264] No metrics for container hyperkube in pod infra/p-prom-op-prometheus-operator-get-crd-0cq19\nI1019 13:45:14.198168       1 handlers.go:215] No metrics for pod infra/p-prom-op-prometheus-operator-get-crd-0cq19\nI1019 13:45:14.198181       1 handlers.go:264] No metrics for container hyperkube in pod kube-system/promop1-prometheus-operator-get-crd-nl47c\nI1019 13:45:14.198184       1 handlers.go:215] No metrics for pod kube-system/promop1-prometheus-operator-get-crd-nl47c\nI1019 13:45:14.198202       1 handlers.go:264] No metrics for container hyperkube in pod default/tailored-hound-prometheus-operator-get-tprs-vs7d7\nI1019 13:45:14.198205       1 handlers.go:215] No metrics for pod default/tailored-hound-prometheus-operator-get-tprs-vs7d7\nI1019 13:45:14.198256       1 handlers.go:264] No metrics for container hyperkube in pod infra/p-prom-op1-prometheus-operator-get-crd-h8wlp\nI1019 13:45:14.198259       1 handlers.go:215] No metrics for pod infra/p-prom-op1-prometheus-operator-get-crd-h8wlp\nI1019 13:45:14.198302       1 handlers.go:264] No metrics for container hyperkube in pod infra/promop-prometheus-operator-get-crd-c3bj3\nI1019 13:45:14.198305       1 handlers.go:215] No metrics for pod infra/promop-prometheus-operator-get-crd-c3bj3\nI1019 13:45:14.198342       1 handlers.go:264] No metrics for container hyperkube in pod infra/p-prom-op1-prometheus-operator-create-sm-job-3lq8v\nI1019 13:45:14.198346       1 handlers.go:215] No metrics for pod infra/p-prom-op1-prometheus-operator-create-sm-job-3lq8v\nI1019 13:45:14.198363       1 handlers.go:264] No metrics for container hyperkube in pod kube-system/promop1-prometheus-operator-create-sm-job-ck2ms\nI1019 13:45:14.198366       1 handlers.go:215] No metrics for pod kube-system/promop1-prometheus-operator-create-sm-job-ck2ms\nI1019 13:45:14.198377       1 handlers.go:264] No metrics for container hyperkube in pod kube-system/promop-prometheus-operator-create-sm-job-0nhp0\nI1019 13:45:14.198380       1 handlers.go:215] No metrics for pod kube-system/promop-prometheus-operator-create-sm-job-0nhp0\nI1019 13:45:14.198384       1 handlers.go:264] No metrics for container hyperkube in pod infra/p-prom-op-prometheus-operator-create-sm-job-7g88t\nI1019 13:45:14.198387       1 handlers.go:215] No metrics for pod infra/p-prom-op-prometheus-operator-create-sm-job-7g88t. I will give this a try when I am back in the office. . ",
    "BlueMonday": "This is a duplicate of: https://github.com/kubernetes/heapster/issues/1579. /retest. /retest. Done. ",
    "tokiwinter": "Also worth noting, I've tried the summary api i.e. --source=kubernetes.summary_api:https://kubernetes.default and it didn't help.. I've tried running heapster standalone, and it seems considerably worse:\nI1012 17:58:05.129966       1 rate_calculator.go:53] Skipping rates for namespace:default/pod:busybox-56db8bd9d7-lwzzn/container:busybox - different create time new:2017-10-12 17:57:53.918666406 +0000 UTC  old:2017-10-12 17:49:33.262530678 +0000 UTC\nThere is an ~eight minute difference there. Can some light be shed on what is going on? . Yeah, ntp/time sync across all nodes is good (was the first thing I checked), as for logs, I'll check when I'm working on this again (context switching is a great thing), probably within the next day or so.. ",
    "TransactCharlie": "HI. I'd love this PR to be in the mainline! (not a contributor / reviewer sadly)\n. thanks.. ",
    "jiayingz": "Co-ping. Given that there are other PRs queued after this one, can we speed up the review on this PR?. /assign @kawych. /test pull-heapster-e2e. /test pull-heapster-unit. @kawych I think this PR is ready for review now. Thanks!. /cc @jingxu97 @andyxning \nping @kawych . /lgtm. @jingxu97 once you add the metric for local storage, I think you want to add it here as well.. Done.\nFor extended resources, we currently require requests to always equal to limits. That is why I leave resource limits out for now.. Looks like that requires kube_api upgrade. Leave that out for now.. ",
    "githubvick": "hi @DirectXMan12 Thx a lot, it worked, I'll close the issue in another few minutes to make sure it doesn't restart. ",
    "zatricky": "WFM\nResult with the advice in this PR is a working grafana dashboard. Was getting 404s on all resources when following the original advice.. ",
    "4406arthur": "/retest. ",
    "NickrenREN": "@DirectXMan12  thanks for your reply.\nHeapster: 1.4.3,  \nkubernetes: latest (compiled using master branch), \nHeapster configuration:  using yaml files from the dir cluster/addons/cluster-monitoring of kubernetes repo\nI get the ip and port from the heapster service.  and then get the metrics from browser, results are shown above.. @jcharlytown  thanks for your comment. How can i get the filesystem metrics through API ? How to get labels ?\nI enabled the log sink but can not find related logs\nnickren@nickren-14:~/test/test/heapster$ docker logs 9c09b1ff1605\nI1106 03:56:43.248361       1 pod_nanny.go:63] Invoked by [/pod_nanny --cpu=100m --extra-cpu=0m --memory=190Mi --extra-memory=500Ki --deployment=heapster-v1.4.3 --container=eventer --poll-period=300000]\nI1106 03:56:43.248493       1 pod_nanny.go:77] Poll period: 5m0s\nI1106 03:56:43.248510       1 pod_nanny.go:78] Watching namespace: kube-system, pod: heapster-v1.4.3-85b8dbbcbb-h6spv, container: eventer.\nI1106 03:56:43.248520       1 pod_nanny.go:79] cpu: 100m, extra_cpu: 0m, memory: 190Mi, extra_memory: 500Ki, storage: MISSING, extra_storage: 0Gi\nI1106 03:56:43.248529       1 pod_nanny.go:80] Accepted range +/-20%\nI1106 03:56:43.248539       1 pod_nanny.go:81] Recommended range +/-10%\nI1106 03:56:43.248918       1 pod_nanny.go:122] Resources: [{Base:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} ExtraPerNode:{i:{value:0 scale:-3} d:{Dec:<nil>} s: Format:DecimalSI} Name:cpu} {Base:{i:{value:199229440 scale:0} d:{Dec:<nil>} s:190Mi Format:BinarySI} ExtraPerNode:{i:{value:512000 scale:0} d:{Dec:<nil>} s:500Ki Format:BinarySI} Name:memory}]\nnickren@nickren-14:~/test/test/heapster$ docker logs 9429542ab5f6\nI1106 03:23:39.955775       1 eventer.go:68] /eventer --source=kubernetes:'' --sink=influxdb:http://monitoring-influxdb:8086 --sink=log\nI1106 03:23:39.955805       1 eventer.go:69] Eventer version v1.4.3\nI1106 03:23:49.010489       1 influxdb.go:247] created influxdb sink with options: host:monitoring-influxdb:8086 user:root db:k8s\nI1106 03:23:49.010712       1 eventer.go:95] Starting with InfluxDB Sink sink\nI1106 03:23:49.010731       1 eventer.go:95] Starting with LogSink sink\nI1106 03:23:49.010748       1 eventer.go:109] Starting eventer\nI1106 03:23:49.010759       1 eventer.go:117] Starting eventer http service\nI1106 03:24:00.001798       1 manager.go:100] Exporting 10 events\nI1106 03:24:00.001949       1 log_sink.go:46] EventBatch     Timestamp: 2017-11-06 03:24:00.001785357 +0000 UTC\n   2017-11-06 03:23:39 +0000 UTC (cnt:1): Created container\n   2017-11-06 03:23:39 +0000 UTC (cnt:1): Started container\n   2017-11-06 03:23:39 +0000 UTC (cnt:1): Container image \"gcr.io/google_containers/addon-resizer:2.0\" already present on machine\n   2017-11-06 03:23:39 +0000 UTC (cnt:1): Created container\n   2017-11-06 03:23:40 +0000 UTC (cnt:1): Started container\n   2017-11-06 03:23:40 +0000 UTC (cnt:1): Killing container with id docker://eventer:Need to kill Pod\n   2017-11-06 03:23:40 +0000 UTC (cnt:1): Killing container with id docker://heapster:Need to kill Pod\n   2017-11-06 03:23:40 +0000 UTC (cnt:1): Killing container with id docker://heapster-nanny:Need to kill Pod\n   2017-11-06 03:23:40 +0000 UTC (cnt:1): Killing container with id docker://eventer-nanny:Need to kill Pod\n   2017-11-06 03:23:42 +0000 UTC (cnt:21): Liveness probe failed: HTTP probe failed with statuscode: 503\nI1106 03:24:00.050559       1 influxdb.go:209] Created database \"k8s\" on influxDB server at \"monitoring-influxdb:8086\"\nI1106 03:24:30.001946       1 manager.go:100] Exporting 0 events\nI1106 03:24:30.002057       1 log_sink.go:46] EventBatch     Timestamp: 2017-11-06 03:24:30.001922052 +0000 UTC\nI1106 03:25:00.000132       1 manager.go:100] Exporting 1 events\nI1106 03:25:00.000195       1 log_sink.go:46] EventBatch     Timestamp: 2017-11-06 03:25:00.000122627 +0000 UTC\n. I can get filesystem metrics from container api\nnickren@nickren-14:~/test/test/heapster$ curl http://10.0.0.198:80/api/v1/model/namespaces/default/pods/default-mem-demo-2/containers/defalt-mem-demo-2-ctr/metrics/filesystem/usage?labels=resource_id:/\n{\n  \"metrics\": [\n   {\n    \"timestamp\": \"2017-11-06T06:03:00Z\",\n    \"value\": 65536\n   },\n   {\n    \"timestamp\": \"2017-11-06T06:04:00Z\",\n    \"value\": 65536\n   },\n   {\n    \"timestamp\": \"2017-11-06T06:05:00Z\",\n    \"value\": 65536\n   }\n  ],\n  \"latestTimestamp\": \"2017-11-06T06:05:00Z\"\n }\nbut  can not get them from pod api\nnickren@nickren-14:~/test/test/heapster$ curl http://10.0.0.198:80/api/v1/model/namespaces/default/pods/default-mem-demo-2/metrics/filesystem/usage?labels=resource_id:/\n{\n  \"metrics\": [],\n  \"latestTimestamp\": \"0001-01-01T00:00:00Z\"\n }. @jcharlytown  yeah, i ran the heapster with summary api source.\ncontainers:\n        - image: gcr.io/google_containers/heapster-amd64:v1.4.3\n          name: heapster\n          livenessProbe:\n            httpGet:\n              path: /healthz\n              port: 8082\n              scheme: HTTP\n            initialDelaySeconds: 180\n            timeoutSeconds: 5\n          command:\n            - /heapster\n            - --source=kubernetes.summary_api:''\n            - --sink=influxdb:http://monitoring-influxdb:8086. ",
    "jcharlytown": "@NickrenREN, the filesystem metrics seem to be handled differently due to the fact that they're \"labeled\" metrics. They seem not to show up on the /pods/NAME/metrics endpoint like the other metrics. I ran into this issue myself. See this thread here: https://groups.google.com/forum/#!topic/kubernetes-dev/q8wdi6hJDkg\nI confirmed that my setup in fact does produce those metrics (also for PVs and PVCs) by running heapster with log sink enabled for a couple of minutes to check what metrics show up there. . @NickrenREN, heapster supports multiple sources. If I am not mistaken the volume metrics are only available via the summary api source. See this documentation, at the very bottom: https://github.com/kubernetes/heapster/blob/master/docs/source-configuration.md. In your logs above, I see the following line:\nI1106 03:23:39.955775       1 eventer.go:68] /eventer --source=kubernetes:'' --sink=influxdb:http://monitoring-influxdb:8086 --sink=log\nI don't know why it says eventer for you, but that looks like the entrypoint into heapster. Are you sure you were using the summary api at this point in time?\nStarting kubernetes 1.8 you can also find the metrics via the kubelet prometheus endpoint, at $node:10255/metrics - the metrics are prefixed with volume_stats_.. @andyxning , as I mentioned in my first comment, this PR is intended as a follow-up to PR #1947. I took the changes from there and incorporated your comments. I think the only change missing is the comment line. I will gladly add the comment to this PR as well. Is there anything else?\nIf you think that there is a chance that PR #1947 starts making progress again, I am happy to close this PR. I simply need the fix, and prefer to use the \"official\" build over a custom one. . @googlebot  I signed it!. @andyxning, to be honest, I don't fully understand why the e2e test failed. Can you help?. Btw, I added a second commit to fix the vetting issues. There is a separate PR for this already, #2009. Is this PR ok to go as is, or do we need to split it up and wait for the other PR to be merged?. ",
    "gertjan-carbon": "Fixed it by adding INFLUXDB_DATA_MAX_SERIES_PER_DATABASE = 0 to the environmental settings of the pod.\nAs suggested in https://github.com/influxdata/docs.influxdata.com/issues/710. Looks like they have changed the settings a bit:\nhttp://docs.influxdata.com/influxdb/v1.6/administration/config/ http://docs.influxdata.com/influxdb/v1.6/administration/config/\nAnd it can also be this setting: INFLUXDB_DATA_MAX_VALUES_PER_TAG\nIt seems your setting is correct although I have used single instead of double quotes.\n\nOn 5 Oct 2018, at 11:50, xorpierre notifications@github.com wrote:\n@gertjan-carbon https://github.com/gertjan-carbon Can you show me how this is suppose to look like?\nI added the env var to the deployment and I see it in the running pod as this:\n`spec:\ncontainers:\ncommand:\n/heapster\n--source=kubernetes:https://kubernetes.default https://kubernetes.default/\n--sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086 http://monitoring-influxdb.kube-system.svc:8086/\nenv:\nname: INFLUXDB_DATA_MAX_SERIES_PER_DATABASE\nvalue: \"0\"\n`\nBut I still run into the error, it seems influxdb is ignoring the variable\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/kubernetes/heapster/issues/1862#issuecomment-427323336, or mute the thread https://github.com/notifications/unsubscribe-auth/AdHJ00-60tD4TZiQiUw29DkBiPCTfuAAks5uhzlfgaJpZM4QMxtT.\n\n\n. ",
    "JHK": "I signed it!. ",
    "khaled-nabil": "@DirectXMan12 Thanks for the clarification, it seems I have missed that part.\nI haven't read anywhere that model API is deprecated, perhaps an update of model.md is due. Is heapster to be replaced by kube-state-metrics?. If you're running your cluster on GKE, my guess is that you're running a second Deployment of Heapster in order to dump metrics into InfluxDB sink. If that's the case, do both Heapster metrics match?\nIt might be useful to look into CPU and Memory Utilization at the the Node.. ",
    "de1m": "I've solved my problem\nI've added the rbac config\n```\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: heapster\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:heapster\nsubjects:\n- kind: ServiceAccount\n  name: heapster\n  namespace: kube-system\n\n```\nMaybe the role \"cluster-admin\" is too much. ",
    "leezout": "Same issue here, it didn't create the K8 database.\nI fixed it when I've updated heapster.yaml to use a new image version : \nimage: gcr.io/google_containers/heapster-amd64:v1.4.3. ",
    "Guillaume-Mayer": "I was able to fix my issue manually adding \u2013sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086 in the heapster deploy.\nSo it was an Azure issue I guess. . ",
    "mindprince": "/cc @piosz @vishh @loburm @andyxning. Why use the cAdvisor endpoint instead of the summary API?\nAlso, when we expose these metrics, we would probably expose them as\naccelerator/.... @piosz I took a quick look and left some comments.\nI also have a question if we even want to do this or just want to do through the summary API? Can both co-exist?. Agreed. And these were added to the summary API in #1891. /cc @piosz @kawych @vishh \nI haven't tested this end to end yet. Sending for an early review to make sure I am in the right direction.. Tested it and it works as expected. Good to get merged.. /assign @kawych @piosz . /assign @kawych @piosz . Couple of points:\n1. In my mind, this was a missing piece of #1891 where these metrics were added in the old resource model but don't work in the new resource model. But I see your point as well.\n2. The heapster deprecation timeline says Heapster is deprecated from 1.11 which is not released yet. The heapster deprecation was announced after the 1.11 cycle had already begun and there were already new features (#1989, #1999) in the master branch.\nSo, I think we need to make one final release of Heapster from the master branch that will go in 1.11. And we shouldn't merge any new PRs to heapster that are not clear-cut bug fixes after 1.11 is released.. > Heapster was deprecated officially as soon as we merged the deprecation PR. The 1.11 on the timeline was intended to indicate that it was deprecated \"as part of the 1.11 development cycle\". Apologies if that wasn't clear.\n\nOur existing policy so far has been that no new features will merge into the 1.11 cycle releases of Heapster that weren't already in place by the time the deprecation PR merged. There were a few features merged before the deprecation PR was in place, so those'll be in the next release.\n\nYes, it wasn't clear. Specially because the expectation is that deprecations are aligned with the Kubernetes release schedule. Doing this at an arbitrary point in the cycle doesn't make a lot of sense to me. I also didn't see any announcement in any release notes or in kubenetes-dev@.\nIn any case, I don't think there are any plans to add more code like this in the 1.11 cycle. So, we should be good.. Use the names that are used in the upstream API.\naccelerator/memory_total\naccelerator/memory_used\naccelerator/duty_cycle. Let's not use nvidia in the metric name and instead use labels if possible.. Instead of Utility use DutyCycle. They are different things.. MemoryTotal. ResourceNvidiaGPU is a deprecated resource and is going away. The new resources are exposed through device plugins. I would not add this metric at all for now.. Am I reading this right? This is just one label with key=LabelResourceID.Key and value=concatenation of Make, Model and ID? If so, I don't think we want this.\nAll these metrics should have three extra labels.\nmake, model and accelerator_id.. Get rid of this diff. It's changing the labels from what I added in the other PR.. Adding these labels here adds these labels to a lot of metrics where they don't make sense at all. You should use acceleratorLabels instead.. All the diff from this file can go away.. Keep this.. As I said before, this is deprecated and the resource management WG doesn't want to add more code that depends on this. Please get rid of this metric completely.. These labels only show up for accelerator metrics (because we use acceleratorLabels only in accelerator metrics). Because of this, I think it is better to not repeat accelerator in the label name. accelerator_id is a special case because id is usually used for other things.\nAlso, this will keep things consistent with Stackdriver and prometheus.. Minor nit: if you move this field from last to here, the diff would be shorter.. ",
    "zouhuigang": "Solved, restart the dashboard panel you can see the graphics. ",
    "pcm32": "I think that the incorrect part lies in the Grafana query for influxdb being used. Currently it is doing a sum over the time interval used in the group by, \nSELECT sum(\"value\") \nFROM \"memory/usage\" \nWHERE \"type\" = 'node' AND \"nodename\" =~ /$nodename$/ \n              AND $timeFilter \nGROUP BY time($interval), \"nodename\" fill(null)\nwhich is probably aggregating things differently on larger or smaller time intervals. Using the mean or the median as aggregator function in the select, you don't see the compounded effect over different time lapses (value levels stay the same, regardless of the time window chosen).. I can confirm that the same is currently happening with the CPU usage graphs, if you change the time window, values change for the same time points. I know what to change now, and if you agree with the change (I would change the aggregator to mean in all the grafana queries on the json files, in lines like this), I'm happy to create a PR with it.. Possibly max is an even better aggregator as well, as mean will soften peaks, giving you false impressions when it comes to discovering whether you use to much of the resources.... I have updated the PR to use mean instead of max @abelsromero, now it depends on project owners.. I have assigned the PR a project owner, so that it can be merged and we can close this.. /reopen. well I started it initially because it was set to sum, which had horrible results. I first used mean, but then it made more sense for most of my observations to use max. But, as long as it moves away from sum to either mean or max, I think is fine. I do think though that mean has an undesired softening effect, but I agree that max has also a bias.. Maybe if some admin or project owner can let me know what they decide, I'm happy to change the branch to reflect it.. I have switched all max to mean in the dashboards. Hopefully this can now be merged.. /assign @andyxning. I don't know.... I guess we need to ask another owner to approve it.... Maybe @loburm could help us being an owner? he approved a recent PR. Sorry for the ping @loburm, I hope this is fine.. Thanks @DirectXMan12! The tide robot apparently requires an lgtm label from what I read below, are you able to set that up?. ",
    "abelsromero": "What's the status on this? I understand the PR is waiting for validarion.\nHonestly, we are seeing the same problem and fixing the dashboards, but it would be nice to have it merged in upstream.. Thanks for keeping an eye on this.. Sorry to ask, but what's the reason not to merge this fix? The issue got closed by rotten but this is still pending. Thanks a lot !!. ",
    "verult": "/cc @piosz @kawych @jingxu97 . /test continuous-integration/travis-ci/pr\nLooks like the event manager test is flaky. It happened in this PR as well: https://github.com/kubernetes/heapster/pull/1820. Is it necessary to keep PVC stats as LabeledMetrics? It's convenient, since I can reuse decodeFsStats, but AFAIU the labels are used to distinguish between volumes, and that's not necessary here.. @DirectXMan12 one benefit of pre-aggregating PVC stats and treating it as a separate resource is we can now include PVC-specific information, such as storage classes and volume mode (filesystem or block). These shouldn't be included as pod-level volume metric labels because not all volumes are PVCs.. /remove-lifecycle stale. Is the kubelet source still used? If not, I'll add this in another PR.. I think it'd be a lot more difficult to obtain filesystem stats of volumes not mounted on any pod. The main value is to allow users to query PVC stats by PVC name, without having to scan through pods themselves. But maybe adding a pvc_label to existing volume metrics is sufficient for this purpose?. ",
    "integrii": "How do I make the cla/linuxfoundation check again?  I definitely have signed it with a matching email address.... Well, I guess asking worked.. @jsoriano . @jsoriano it looks like this heapster e2e Jenkins tests are broken.  It threw errors about a missing kubeconfig.  Can you authorize this merge?. /retest\n. Tests worked this go.. @jsoriano Please authorize merge so this can finally be done!. I explicitly take issue with this line on factory.go line 44:\nfunc (this *SinkFactory) Build(uri flags.Uri) (core.DataSink, error) {\nIn go, its best to return concrete types.  Accept interfaces, but return concrete types.  Because a interface was returned here, it became hard to return \"nothing\" in the case of an error.  Because of that, the developer chose to return a nil.  Because a nil was returned, the code blew up somewhere.\nLooking at the codebase now, it looks like this case is handled.  I am assuming this was fixed in master but is not in the image for Heapster 1.5.2.... ",
    "mrblackus": "After investigation, it seems that uptime resets happen when there is some load on the pod, but it's not always true...\nHere is a graph with uptime in yellow, and CPU usage in millicores in green.\n\n. You're right, I have a second heapster deployment that sinks data on InfluxDB, and the default kubernetes one that sinks them to StackDriver.\nI couldn't find the CPU usage per pod on StackDriver, but memory reporting seems to be the same on InfluxDB and StackDriver.\nAfter digging more, I found this issue on cAdvisor that looks interesting. Not sure if it's linked or not, but after reading heapster code, it's unlinkely that this code is doing something wrong, so kubelet or cAdvisor might be responsible.. /remove-lifecycle stale. /remove-lifecycle rotten. ",
    "DanielMSchmidt": "You are right, forcing a new docker version on everyone is probably a bad idea \ud83e\udd14 . ",
    "charlesakalugwu": "/sig autoscaling. ",
    "shadycuz": "Also seeing this, but we are running OC? Might not make a difference.  We see the same events describing the pods.. @MaciekPytel Different errors in the event log?\nunable to get metrics for resource cpu: unable to fetch metrics from API: the server could not find the requested resource (get pods.metrics.k8s.io)\nbut  I will check into this.. ",
    "MaciekPytel": "It's likely the same issue as https://github.com/kubernetes/kubernetes/issues/57673. @shadycuz Yeah, good point. I've just seen that issue so much I automatically assumed this to be the case. This looks more like some network or RBAC related issue.. ",
    "blakebarnett": "I think https://github.com/kubernetes/kubernetes/issues/57673 only applies to 1.9, but it does seem like the system:heapster cluster role is lacking some of the apiGroup/resources (or perhaps it's expecting the 1.9 configuration?). confirmed adding\n- apiGroups:\n  - apps\n  resources:\n  - deployments\n  verbs:\n  - get\n  - list\n  - watch\nfixed it for me.. I may have confused the issue, I thought the original post was running 1.5.0 which we are, on k8s 1.8.x, but it looks like it's 1.4.x and something else is going on.  Adding this fixed it for us.. In 1.9, the apiGroup for Deployment moved from \"extensions\" to \"apps\", the service account that heapster uses only gives access to extensions.deployments, so I changed that to add the same permissions to apps.deployments in the system:heapster cluster role. \n@DirectXMan12 said this shouldn't be required but it's working for me now... \u00af_(\u30c4)_/\u00af. They are here: https://github.com/kubernetes/heapster/issues/1908#issuecomment-363218006. To be clear, the proper fix is here for 1.9, the cluster role change only applies to running the newer heapster on < 1.9: https://github.com/kubernetes/kubernetes/issues/57673#issuecomment-354270925. ",
    "amitman1810": "Can you please explain how exactly it was fixed ?. ",
    "avish1990": "@blakebarnett Could you please paste your yaml changes here?. ",
    "tstoermer": "ES 6 does no longer support multiple types in mappings per index. The metrics must be merged to one type or written to different indices.\nhttps://www.elastic.co/guide/en/elasticsearch/reference/6.0/breaking-changes-6.0.html\nThere is also a new version of the used ES client \nhttps://github.com/olivere/elastic. @Freyert \nRight, just wanted to point the official breaking changes - linking to the details you mentioned.. ",
    "Adriien-M": "We are looking for this update, what is the status of this issue?. ",
    "f0": "any progress to support elasticsearch 6?. ",
    "Freyert": "@AlmogBaku I'm postulating that these tests represent the original schema?\n@tstoermer is this the documentation you were referencing?\nIndices created here.\nSink Code is Here.\n. ",
    "ihardzeenka": "Hi guys. About schema for ES 6. \nNow you have single index with mapping types like cpu, memory, events and etc.\nAccording to ES documentation there is 2 options here (https://www.elastic.co/guide/en/elasticsearch/reference/6.x/removal-of-types.html):\n1) Simply split single index into multiple indexes like: heapster-cpu , heapster-memory and etc.\n2) Merge types into single type (I don't really like that option)\nI don't know go language but looking into your code I believe it is not a big change to simply add existing type name into index name.. ",
    "MnrGreg": "Would be great to get something going here!. ",
    "jimk-osu": "+1\nWould having a temporary countermeasure of single typing be OK with the goal of having a better solution as a permanent countermeasure? \nI agree with @ihardzeenka  that single typing may not be the right way, but it in the mean time a working solution would be best.. ",
    "jhorwit2": "@DirectXMan12 what's the deprecation plan for eventer since that's not included functionality in the metrics-server?. ",
    "smelchior": "we do currently have the same issue, maybe https://www.elastic.co/guide/en/beats/metricbeat/6.x/metricbeat-metricset-kubernetes-event.html can be used as an alternative?. ",
    "tirumaraiselvan": "I don't see any access request by heapster in influxdb logs. \nThis is what it looks like:\n[I] 2017-12-14T10:43:19Z InfluxDB starting, version unknown, branch unknown, commit unknown\n[I] 2017-12-14T10:43:19Z Go version go1.8.3, GOMAXPROCS set to 1\n[I] 2017-12-14T10:43:19Z Using configuration at: /etc/config.toml\n[I] 2017-12-14T10:43:19Z Using data dir: /data/data service=store\n[I] 2017-12-14T10:43:19Z opened service service=subscriber\n[I] 2017-12-14T10:43:19Z Starting monitor system service=monitor\n[I] 2017-12-14T10:43:19Z 'build' registered for diagnostics monitoring service=monitor\n[I] 2017-12-14T10:43:19Z 'runtime' registered for diagnostics monitoring service=monitor\n[I] 2017-12-14T10:43:19Z 'network' registered for diagnostics monitoring service=monitor\n[I] 2017-12-14T10:43:19Z 'system' registered for diagnostics monitoring service=monitor\n[I] 2017-12-14T10:43:19Z Starting precreation service with check interval of 10m0s, advance period of 30m0s service=shard-precreation\n[I] 2017-12-14T10:43:19Z Starting snapshot service service=snapshot\n[I] 2017-12-14T10:43:19Z Starting continuous query service service=continuous_querier\n[I] 2017-12-14T10:43:19Z Starting HTTP service service=httpd\n[I] 2017-12-14T10:43:19Z Authentication enabled:false service=httpd\n[I] 2017-12-14T10:43:19Z Listening on HTTP:[::]:8086 service=httpd\n[I] 2017-12-14T10:43:19Z Starting retention policy enforcement service with check interval of 30m0s service=retention\n[I] 2017-12-14T10:43:19Z Listening for signals\n[I] 2017-12-14T10:43:19Z Storing statistics in database '_internal' retention policy 'monitor', at interval 10s service=monitor\n[httpd] 10.140.0.17,10.60.2.68 - - [14/Dec/2017:10:44:52 +0000] \"GET /ping HTTP/1.0\" 204 0 \"-\" \"curl/7.47.0\" d0cd680d-e0bb-11e7-8001-000000000000 202\n[I] 2017-12-14T11:13:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T11:43:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T12:13:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T12:43:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T13:13:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T13:43:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T14:13:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T14:43:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T15:13:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T15:43:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T16:13:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T16:43:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T17:13:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T17:43:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T18:13:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T18:43:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T19:13:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T19:43:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T20:13:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T20:43:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T21:13:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T21:43:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T22:13:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T22:43:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T23:13:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-14T23:33:19Z new shard group 2 successfully precreated for database _internal, retention policy monitor service=metaclient\n[I] 2017-12-14T23:43:19Z retention policy shard deletion check commencing service=retention\n[I] 2017-12-15T00:09:50Z Snapshot for path /data/data/_internal/monitor/1 written in 251.384356ms engine=tsm1\n. ",
    "rohansahai": "ok sorry about that, thanks. ",
    "wrp": "Squashed and rebased. ",
    "idanshahar": "I signed it!\n. ",
    "jdumars": "/ok-to-test\n. /ok-to-test\n. /ok-to-test\n. /ok-to-test\n. /ok-to-test. ",
    "maartenvermeyen": "I moved the question to stackoverflow: https://stackoverflow.com/questions/48172151/kubernetes-pod-cpu-usage-calculation-method-for-hpa. ",
    "serathius": "/assign @piosz . done. @kawych I added separate commit to fix static analysis failure. I was mainly motivated by https://github.com/kubernetes/heapster/pull/1643/files. I wanted it to be consistent with other last type metrics. Yep. @crassirostris Should last event timestamp come from before or after execution?\n. done. ",
    "ryarnyah": "I signed it!. @Kokan It was merged before i can rename usageKafkaclient. Do i need to make another PR to rename  usageKafkaclient?. Done :D. Is there anything i need to do to make this PR evolve? Did i need to assign someone from base OWNERS file?. I rebased my PR and add some cleanup on riemann metric sink tests https://github.com/kubernetes/heapster/pull/1951/commits/115411f2604b2dab7db4aa49563d735d7db0f572 (unrelated to this PR subject but cause fail on go vet)\nEdit: Remove commit on riemann sink, clean up was done with https://github.com/kubernetes/heapster/pull/2009\n/assign @piosz . It seem that it can't resolve \"kubernetes.default\" on your DNS \"10.254.0.2:53\".. Duplicate https://github.com/kubernetes/heapster/pull/1940. It seem that it was related to this fix https://github.com/kubernetes/heapster/pull/1940\nYou may have an error in sink initialisation. Can you try an older version before 1.5.0 to show you the error message?(see https://github.com/kubernetes/heapster/commit/0feaf73df74f3eefa28dfe7f111b87c1653fbdec#diff-1576fd41228e6cabea46c1ffba259246). Can your build master branch? (docker must be installed)\nbash\ngit clone https://github.com/kubernetes/heapster.git\ncd heapster && make container\ndocker tag staging-k8s.gcr.io/heapster-amd64:v1.5.0-beta.3 {}/heapster-amd64:test\nYou can use mine \"ryarnyah/heapster-amd64:test\" (last commit 8320b3c974af112efe468a20f0605a35a428500d), i build it 2 min ago. But i recommend you to build your own (don't install any image you don't know). Changed to SyncProducer.. This error is back!. This function is removed and the code is back to NewKafkaClient.. I changed kafka.CompressionNone to -1, is that better? (can't return nil, because, kafka.CompressionCodec is a int8). Done, sorry it was from my first tests..... Done.. Done.. Done.. I add some function to hide password from console log(replace like you suggest with ***).. Is that better?. I personally prefer kafka.CompressionNone to -1. So i revert to it.. ",
    "cookeem": "@DirectXMan12 Sorry for reply late, finally I use calico on flannel, everything works fine, close this issue.. ",
    "JooyoungJeong": "Thanks @DirectXMan12 \nI am honored to have ownership!! . I've updated sink-owners.md. . ",
    "OrlinVasilev": "@r0bj - have started the kubelet with authorization-webhook true ? currently I'm facing the same issue which I cannot solve. thanks worked for me  just starting the kubelet with --authentication-token-webhook=true and later on adding it to the kops config files.\nthanks a lot hope will not hit the same on metrics server.. ",
    "ljfranklin": "With K8S v1.12.0 I was able to get the Dashboard + Heapster working with this suggestion plus adding the verb create to the nodes/stats section to avoid a 403 error.. ",
    "whites11": "Thank you very much for getting back to me @DirectXMan12 \nYour hypothesis make a lot of sense to me, I'll try increasing verbosity and post the logs.\n\nI do try not to bite, I swear :-P\n\nPhewwww\n. So, a few updates on this case.\nI raised verbosity to 10. It is so verbose that kubectl logs hangs up at a certaing point.\nSo i lowered verbosity to 5 and I discovered a difference in the log between a load situation vs not load situation. When CPU data is missing this is what I get in the log:\nSkipping rates for namespace:my-namespace/pod:my-pod-name-59587c855f-s9t99/container:app - different collection start time new:2018-01-25 09:52:13.317097259 +0000 UTC  old:2018-01-25 09:45:30.166309949 +0000 UTC\nThis must be the reason why I get no CPU usage rate, but I have no clue on the cause.. Hi,\nFirst of all, sorry for late response.\nI extracted a 1 minute log from the heapster container while the problem was occouring.\nI tried hard to extract a smaller part of the log but I am having a very hard time understanding where a \"scrape period\" starts and where it ends. The problem is that the log file is 23MB.\nI attached a gzipped version, hope it helps.\nall.log.gz\n. ",
    "srm09": "I have signed the CLA.. ",
    "szaharici": "I dug a bit around and found the cause, we do have a localhost.default-domain entry on our DNS server that is always resolved when using localhost instead of reading the entry from the /hosts file. This is triggered by the absence of /etc/nsswitch.conf speicifying the host file priority in the docker image. ",
    "qhyou11": "I signed it!. I signed it!. ",
    "acobaugh": "I can likely help with this. May take me a little bit to learn the code, but most of the bugs look pretty straight forward. Was going to work on #1579 (was actually going to file that bug and one about allowing the RP to be selected instead of using default always when I saw the bug already existed).. I wanted to run a quick test of my code in #1959. . This is likely more of an influxdb problem than a heapster problem per se. On your influxdb system influx -execute 'show series on k8s'  | wc -l. Memory consumption is roughly a function of series cardinality. When you approach 1M series, the older inmem index format starts to show its limitations: https://www.influxdata.com/blog/path-1-billion-time-series-influxdb-high-cardinality-indexing-ready-testing/\nThis could also be caused by inadequate shard group duration tuning. I think in general you want the shard duration to be short enough that the indexes don't grow to be huge (>~5GB in size).\nI do wonder if we need to add options to omit certain tags, particularly the ones that are always going to be changing. So leave out at least pod_name and pod_id, and instead rely on labels in our queries.. After some casual observation of my own cluster with hardly anything running in it, I have 2613 series over 36 measurements. 34 of those measurements have / in the name, and they all seem to have a single field named value. If we collapsed the measurements with / in them into a single measurement by using multiple fields per measurement (ie, all memory values in a single measurement called memory), then we should be able to reduce cardinality by almost an order of magnitude. We may also be able to do away with the *_rate measurements, because we can get that with the derivative() influxql  function.. The retention policy that heapster writes to is hardcoded to default: https://github.com/kubernetes/heapster/blob/d25a176baee4554dc59052785c4ee940fc94d305/metrics/sinks/influxdb/influxdb.go#L214\nIf you're manually creating your RP, you'll have to name it default. Note that this is not the same as the default RP, which is the RP written to when the client does not specify one. . I signed it!. I'm not sure I agree with the usage of max() instead of mean() here. If I want higher resolution, then I'm going to reduce my $interval. Using max() would actually give a skewed view of reality. Most of the grafana dashboards I see in the wild use mean(). I could maybe see an argument for separate set of metrics showing max over the selected interval, but that would make the graphs pretty crowded.. My personal vote is for mean(), but I'm still new to the project. Pretty much every single dashboard I've seen over the last few years of using grafana has used mean(). . /approve. As the very short-term co-maintainer of the influxdb sink, I fully support this effort. For the influxdb case, I believe telegraf with its prometheus input plugin ought to be able to replace the functionality of heapster by pulling metrics straight from metrics-server. I haven't dug into this yet, but I wouldn't be surprised if someone in the TICK community is already doing just that, and we just need to document those methods and configurations within the kubernetes project as an example to get metrics into influxdb, and as a migration path for the heapster+influxdb users out there.. There wouldn't be anything you could do with influxdb. You could probably write a continuous query to drop points that match a certain pattern, or just throw something into cron.\nA properly tuned influxdb system should be able to handle at least a million series, likely 10's of millions. Are the additional series causing an issue for you?\nMake sure you have an RP configured to bound the series growth in time. If you have a k8s cluster with a lot of churn, you may need to make this very short.. ",
    "devopsprosiva": "@acobaugh Any idea when the update for selecting rp will be available?. ",
    "stanxing": "@acobaugh \n- I have carefully considered the possibility of setting pod_name as field key ,but I found it hid a bigger problem.If pod_name is a filed key,  it will not have a unique tag key (pod_id have be removed in heapster v1.5) to identify the point data and cause the data written to be overwritten. For example, influxdb wrote data in the timestamp (1519455048000000): \"cpu/request namespace=default, value=100 pod_name=k8s-nginx-f278\" and the next point data was also written in the same timestamp (1519455048000000): \"cpu/request namespace=default, value=100 pod_name=k8s-nginx-1234\", it will overwrite the previous point data. So pod_name is not be used as field key.\n- Also, field key should be a metric in influxdb( it is my understanding), just like request, limit as field key in cpu measurement, so it not suitable for influxdb to regard pod_name as a field key.\n- May be the method you said that collapsed the measurements with / in them into a single measurement is a better way to reduce the usage of memory. According to my test ,  memory was reduced by 40%, (series reduced from 158696 to 45101). But I read the article(https://www.influxdata.com/blog/path-1-billion-time-series-influxdb-high-cardinality-indexing-ready-testing/) and found maybe the best way is TSI index based disk space. As you said , this is likely more of an influxdb problem than a heapster problem per se.\n. ",
    "kevinschoon": "@DirectXMan12 Thank you for the follow up, escaping the password resolves the error. I opened the issue because it wasn't immediately clear to me how the parameters were being parsed (although it is indeed a URL) or how it was being validated.. ",
    "lainekendall": "\ud83d\udc4d . ",
    "PheonixS": "We solve the issue by adding\n--authentication-token-webhook=true \\\nin file/etc/kubernetes/kubelet.env And after this restarting all kubelets in cluster.\n. ",
    "cblecker": "@DirectXMan12 Do you want reviewers assigned? I don't see any reviewer sections, only approvers.. Yes, you do. blunderbuss (the plugin that assigns reviewers) doesn't look at the approvers section. Next steps:\n- Add the repo to: https://github.com/kubernetes/test-infra/blob/c6139cc1f5d5dc31715d851d96c4e7594557e853/prow/plugins.yaml#L36-L55 to configure the approve plugin\n- Add \"approve\" and \"blunderbuss\" to https://github.com/kubernetes/test-infra/blob/c6139cc1f5d5dc31715d851d96c4e7594557e853/prow/plugins.yaml#L201-L202 to enable the approve plugin\n- Add repo to https://github.com/kubernetes/test-infra/blob/c6139cc1f5d5dc31715d851d96c4e7594557e853/prow/config.yaml#L44-L61 to enable auto-merging from tide when all tests pass and when the right labels are applied\nYou should be able to open a PR against k/test-infra with all of these in the same PR. No problem! Feel free to ping me on the PR to k/test-infra and I can help review :). ",
    "javier-b-perez": "/assign @huangyuqi. going with a different approach.. ",
    "yujmo": "root@node1:~# kubectl logs heapster-5c448886d-dpzjc -n kube-system\n168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: dial tcp 192.168.125.223:10255: getsockopt: connection refused\nE0227 04:42:05.004917       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.224:10255/stats/container/\": Post http://192.168.125.224:10255/stats/container/: dial tcp 192.168.125.224:10255: getsockopt: connection refused\nE0227 04:42:05.006337       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.225:10255/stats/container/\": Post http://192.168.125.225:10255/stats/container/: dial tcp 192.168.125.225:10255: getsockopt: connection refused\nI0227 04:42:05.020566       1 influxdb.go:241] Created database \"k8s\" on influxDB server at \"monitoring-influxdb.kube-system.svc:8086\"\nE0227 04:43:05.001748       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.224:10255/stats/container/\": Post http://192.168.125.224:10255/stats/container/: dial tcp 192.168.125.224:10255: getsockopt: connection refused\nE0227 04:43:05.012134       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.225:10255/stats/container/\": Post http://192.168.125.225:10255/stats/container/: dial tcp 192.168.125.225:10255: getsockopt: connection refused\nE0227 04:43:05.014361       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: dial tcp 192.168.125.223:10255: getsockopt: connection refused\nE0227 04:44:05.021694       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.224:10255/stats/container/\": Post http://192.168.125.224:10255/stats/container/: dial tcp 192.168.125.224:10255: getsockopt: connection refused\nE0227 04:44:05.021997       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.225:10255/stats/container/\": Post http://192.168.125.225:10255/stats/container/: dial tcp 192.168.125.225:10255: getsockopt: connection refused\nE0227 04:44:05.024203       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: dial tcp 192.168.125.223:10255: getsockopt: connection refused\nE0227 04:45:05.008247       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.225:10255/stats/container/\": Post http://192.168.125.225:10255/stats/container/: dial tcp 192.168.125.225:10255: getsockopt: connection refused\nE0227 04:45:05.010556       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: dial tcp 192.168.125.223:10255: getsockopt: connection refused\nE0227 04:45:05.022405       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.224:10255/stats/container/\": Post http://192.168.125.224:10255/stats/container/: dial tcp 192.168.125.224:10255: getsockopt: connection refused\nE0227 04:46:05.011317       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.225:10255/stats/container/\": Post http://192.168.125.225:10255/stats/container/: dial tcp 192.168.125.225:10255: getsockopt: connection refused\nE0227 04:46:05.014376       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.224:10255/stats/container/\": Post http://192.168.125.224:10255/stats/container/: dial tcp 192.168.125.224:10255: getsockopt: connection refused\nE0227 04:46:05.019240       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: dial tcp 192.168.125.223:10255: getsockopt: connection refused\nE0227 04:47:05.002541       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.224:10255/stats/container/\": Post http://192.168.125.224:10255/stats/container/: dial tcp 192.168.125.224:10255: getsockopt: connection refused\nE0227 04:47:05.003915       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.225:10255/stats/container/\": Post http://192.168.125.225:10255/stats/container/: dial tcp 192.168.125.225:10255: getsockopt: connection refused\nE0227 04:47:05.011263       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: dial tcp 192.168.125.223:10255: getsockopt: connection refused\nE0227 04:48:05.005722       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: dial tcp 192.168.125.223:10255: getsockopt: connection refused\nE0227 04:48:05.008550       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.224:10255/stats/container/\": Post http://192.168.125.224:10255/stats/container/: dial tcp 192.168.125.224:10255: getsockopt: connection refused\nE0227 04:48:05.010068       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.225:10255/stats/container/\": Post http://192.168.125.225:10255/stats/container/: dial tcp 192.168.125.225:10255: getsockopt: connection refused\nE0227 04:49:05.016274       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.224:10255/stats/container/\": Post http://192.168.125.224:10255/stats/container/: dial tcp 192.168.125.224:10255: getsockopt: connection refused\nE0227 04:49:05.017228       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: dial tcp 192.168.125.223:10255: getsockopt: connection refused\nE0227 04:49:05.021759       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.225:10255/stats/container/\": Post http://192.168.125.225:10255/stats/container/: dial tcp 192.168.125.225:10255: getsockopt: connection refused\nE0227 04:50:05.001998       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.225:10255/stats/container/\": Post http://192.168.125.225:10255/stats/container/: dial tcp 192.168.125.225:10255: getsockopt: connection refused\nE0227 04:50:05.009307       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.224:10255/stats/container/\": Post http://192.168.125.224:10255/stats/container/: dial tcp 192.168.125.224:10255: getsockopt: connection refused\nE0227 04:50:05.019225       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: dial tcp 192.168.125.223:10255: getsockopt: connection refused\nE0227 04:51:05.007700       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.224:10255/stats/container/\": Post http://192.168.125.224:10255/stats/container/: dial tcp 192.168.125.224:10255: getsockopt: connection refused\nE0227 04:51:05.012015       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.225:10255/stats/container/\": Post http://192.168.125.225:10255/stats/container/: dial tcp 192.168.125.225:10255: getsockopt: connection refused\nE0227 04:51:05.012498       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: dial tcp 192.168.125.223:10255: getsockopt: connection refused\nE0227 04:52:05.011015       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.225:10255/stats/container/\": Post http://192.168.125.225:10255/stats/container/: dial tcp 192.168.125.225:10255: getsockopt: connection refused\nE0227 04:52:05.013206       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.224:10255/stats/container/\": Post http://192.168.125.224:10255/stats/container/: dial tcp 192.168.125.224:10255: getsockopt: connection refused\nE0227 04:52:05.022190       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: dial tcp 192.168.125.223:10255: getsockopt: connection refused\nE0227 04:53:05.003211       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.225:10255/stats/container/\": Post http://192.168.125.225:10255/stats/container/: dial tcp 192.168.125.225:10255: getsockopt: connection refused\nE0227 04:53:05.018451       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.224:10255/stats/container/\": Post http://192.168.125.224:10255/stats/container/: dial tcp 192.168.125.224:10255: getsockopt: connection refused\nE0227 04:53:05.021221       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: dial tcp 192.168.125.223:10255: getsockopt: connection refused\nE0227 04:54:05.003500       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.224:10255/stats/container/\": Post http://192.168.125.224:10255/stats/container/: dial tcp 192.168.125.224:10255: getsockopt: connection refused\nE0227 04:54:05.020013       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.225:10255/stats/container/\": Post http://192.168.125.225:10255/stats/container/: dial tcp 192.168.125.225:10255: getsockopt: connection refused\nE0227 04:54:05.024419       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: dial tcp 192.168.125.223:10255: getsockopt: connection refused\nE0227 04:55:05.007243       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.225:10255/stats/container/\": Post http://192.168.125.225:10255/stats/container/: dial tcp 192.168.125.225:10255: getsockopt: connection refused\nE0227 04:55:05.015208       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: dial tcp 192.168.125.223:10255: getsockopt: connection refused\nE0227 04:55:05.016165       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.224:10255/stats/container/\": Post http://192.168.125.224:10255/stats/container/: dial tcp 192.168.125.224:10255: getsockopt: connection refused\nE0227 04:56:05.005913       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.224:10255/stats/container/\": Post http://192.168.125.224:10255/stats/container/: dial tcp 192.168.125.224:10255: getsockopt: connection refused\nE0227 04:56:05.007933       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.225:10255/stats/container/\": Post http://192.168.125.225:10255/stats/container/: dial tcp 192.168.125.225:10255: getsockopt: connection refused\nE0227 04:56:05.021608       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: dial tcp 192.168.125.223:10255: getsockopt: connection refused\nthis is my  heapster's logs . (^_^). while I change env KUBELET_PORT=\"--port=10255\".\nE0227 05:01:05.014232       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: malformed HTTP response \"\\x15\\x03\\x01\\x00\\x02\\x02\\x16\"\nE0227 05:02:05.011760       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.224:10255/stats/container/\": Post http://192.168.125.224:10255/stats/container/: malformed HTTP response \"\\x15\\x03\\x01\\x00\\x02\\x02\\x16\"\nE0227 05:02:05.015159       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: malformed HTTP response \"\\x15\\x03\\x01\\x00\\x02\\x02\\x16\"\nE0227 05:02:05.015918       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.225:10255/stats/container/\": Post http://192.168.125.225:10255/stats/container/: malformed HTTP response \"\\x15\\x03\\x01\\x00\\x02\\x02\\x16\"\nE0227 05:03:05.005381       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.224:10255/stats/container/\": Post http://192.168.125.224:10255/stats/container/: malformed HTTP response \"\\x15\\x03\\x01\\x00\\x02\\x02\\x16\"\nE0227 05:03:05.007570       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.225:10255/stats/container/\": Post http://192.168.125.225:10255/stats/container/: malformed HTTP response \"\\x15\\x03\\x01\\x00\\x02\\x02\\x16\"\nE0227 05:03:05.009288       1 kubelet.go:231] error while getting containers from Kubelet: failed to get all container stats from Kubelet URL \"http://192.168.125.223:10255/stats/container/\": Post http://192.168.125.223:10255/stats/container/: malformed HTTP response \"\\x15\\x03\\x01\\x00\\x02\\x02\\x16\". root@node1:/etc/kubernetes# curl https://192.168.125.223:10255/stats/container/ -k --cert ssl/node-node1.pem --key ssl/node-node1-key.pem\n\"working_set\": 1998614528,\n      \"failcnt\": 0,\n      \"container_data\": {\n       \"pgfault\": 290569,\n       \"pgmajfault\": 716\n      },\n      \"hierarchical_data\": {\n       \"pgfault\": 290569,\n       \"pgmajfault\": 716\n      }\n     },\n     \"network\": {\n      \"name\": \"calid5b47182ebd\",\n      \"rx_bytes\": 1133962,\n      \"rx_packets\": 12512,\n      \"rx_errors\": 0,\n      \"rx_dropped\": 2,\n      \"tx_bytes\": 1991637,\n      \"tx_packets\": 12167,\n      \"tx_errors\": 0,\n      \"tx_dropped\": 0,\n      \"interfaces\": [\n       {\n        \"name\": \"calid5b47182ebd\",\n        \"rx_bytes\": 1133962,\n        \"rx_packets\": 12512,\n        \"rx_errors\": 0,\n        \"rx_dropped\": 2,\n        \"tx_bytes\": 1991637,\n        \"tx_packets\": 12167,\n        \"tx_errors\": 0,\n        \"tx_dropped\": 0\n       },\n       {\n        \"name\": \"caliee096b9f5c1\",\n        \"rx_bytes\": 147690,\n        \"rx_packets\": 1188,\n        \"rx_errors\": 0,\n        \"rx_dropped\": 2,\n        \"tx_bytes\": 270520,\n        \"tx_packets\": 1305,\n        \"tx_errors\": 0,\n        \"tx_dropped\": 0\n       },\n       {\n        \"name\": \"cali898bec5c12e\",\n        \"rx_bytes\": 1978859,\n        \"rx_packets\": 10876,\n        \"rx_errors\": 0,\n        \"rx_dropped\": 2,\n        \"tx_bytes\": 1381159,\n        \"tx_packets\": 12205,\n        \"tx_errors\": 0,\n        \"tx_dropped\": 0\n       },\n       {\n        \"name\": \"cali6d9b6ffb01b\",\n        \"rx_bytes\": 402790,\n        \"rx_packets\": 3462,\n        \"rx_errors\": 0,\n        \"rx_dropped\": 0,\n        \"tx_bytes\": 7151304,\n        \"tx_packets\": 4790,\n        \"tx_errors\": 0,\n        \"tx_dropped\": 0\n       },\n       {\n        \"name\": \"calibf7a888fa87\",\n        \"rx_bytes\": 3308817,\n        \"rx_packets\": 3796,\n        \"rx_errors\": 0,\n        \"rx_dropped\": 2,\n        \"tx_bytes\": 663839,\n        \"tx_packets\": 3479,\n        \"tx_errors\": 0,\n        \"tx_dropped\": 0\n       },\n       {\n        \"name\": \"cali5ed7e9a2744\",\n        \"rx_bytes\": 650615,\n        \"rx_packets\": 7018,\n        \"rx_errors\": 0,\n        \"rx_dropped\": 2,\n        \"tx_bytes\": 12437019,\n        \"tx_packets\": 8249,\n        \"tx_errors\": 0,\n        \"tx_dropped\": 0\n       },\n       {\n        \"name\": \"tunl0\",\n        \"rx_bytes\": 1128901543,\n        \"rx_packets\": 373879,\n        \"rx_errors\": 0,\n        \"rx_dropped\": 0,\n        \"tx_bytes\": 794618068,\n        \"tx_packets\": 248147,\n        \"tx_errors\": 0,\n        \"tx_dropped\": 0\n       },\n       {\n        \"name\": \"eth0\",\n        \"rx_bytes\": 3684904786,\n        \"rx_packets\": 3339772,\n        \"rx_errors\": 0,\n        \"rx_dropped\": 0,\n        \"tx_bytes\": 977187888,\n        \"tx_packets\": 1758004,\n        \"tx_errors\": 0,\n        \"tx_dropped\": 0\n       }\n      ],\n      \"tcp\": {.....\nso default heapster is need https\n . \nI'm sorry. ",
    "lwanqiang": "I have the same problem as you, How did you solve it?. ",
    "drewboswell": "I'm having the same problem, how did you solve this? Did you \"just\" enable kubeletHttps?. ",
    "bangau1": "Perhaps @yogeswaran can help this issue? Sorry if mentioning you in this issue violates the repository guideline.. Hi @ryarnyah thanks for pointing that out. Unfortunately, version before 1.5.0 (which is 1.4.3) doesn't have statsd sink.\nHere is the error message when I deployed the v1.4.3 version:\nheapster | Mar 2, 2018, 2:54:08 PM | F0302 07:54:08.297823       1 factory.go:101] No available sink to use\n-- | -- | --\nheapster | Mar 2, 2018, 2:54:08 PM | E0302 07:54:08.297799       1 factory.go:84] Failed to create sink: Sink not recognized: \"statsd\nheapster | Mar 2, 2018, 2:54:08 PM | I0302 07:54:08.284421       1 configs.go:62] Using kubelet port 10255\nheapster | Mar 2, 2018, 2:54:08 PM | I0302 07:54:08.284403       1 configs.go:61] Using Kubernetes client with master \"https://kubernetes.default\" and version v1\nheapster | Mar 2, 2018, 2:54:08 PM | I0302 07:54:08.284228       1 heapster.go:73] Heapster version v1.4.3\nheapster | Mar 2, 2018, 2:54:08 PM | I0302 07:54:08.284130       1 heapster.go:72]  /heapster --source=kubernetes:https://kubernetes.default  --sink=\"statsd:udp://dd-agent-service.default:8125\". Hi @yogeswaran @stash1001 many thanks for the solution, it solved my problem :+1: :smile: \nI guess I can close this issue then.. ",
    "stash1001": "I had the same issue.    Removing the quotes fixed it for me.   In the docs it shows quotes. /assign @loburm . I signed it!. ",
    "RouR": "This was done before open this issue.\nMaybe i did it wrong\nWhat do you advise?. ",
    "AdamDang": "/assign @crassirostris. I signed it!. recreate this PR. /assign @piosz. /assign @directxman12. /check-cla. /check-cla. /assign @huangyuqi. /assign @directxman12. /assign @loburm. /assign @x13n. /assign @loburm. /assign @almogbaku. ",
    "Nodraak": "Hello :)\nWhat's the status on this PR? Do you need help?\nBTW, Grafana v5.0.4 and influxdb v1.5.2 are out\n. The \"dlopen in static linked app\" issue did not get many answers in the go-sqlite3 repo ...\n\nI don't know golang but the issue might be related to https://github.com/golang/go/issues/21421#issuecomment-322099345 and https://github.com/confluentinc/confluent-kafka-go/issues/59#issuecomment-379296764 ...\nTL;DR: try adding the flag -static-all (https://github.com/007/heapster/blob/3cce3e41b8484791a70e0c60b8c9269d81024ba4/grafana/Makefile#L28). I'll try that later if I find the time\n. So, I've cloned your repo and checkout-ed your branch. I have ran make build in both grafana and influx folders, and I've updated by k8s test cluster with the newly created images staging-k8s.gcr.io/heapster-grafana-amd64:v5.0.4 and staging-k8s.gcr.io/heapster-influxdb-amd64:v1.5.2.\nEverything is running fine without errors.\nI'm not sure what you meant by I'm stuck on how to test the \"static\" linking in sqlite, it looks good on my side!\n. ping @andyxning. ",
    "zhuchenjie": "I have same problem so.... ",
    "hzxuzhonghu": "ref https://github.com/kubernetes/kubernetes/issues/61807. /assign @kawych. /assign @andyxning. @andyxning Thanks.. ",
    "okian": "@Demonsthere I'm getting the same error, can you please guide me how to solve it?. I just removed --read-only-port flag from /etc/kubernetes/kubelet.env in all nodes and it's fixed, however i'm not sure from security standpoint is it safe or not!!. ",
    "GregoryKhakov": "Stucked on the same problem. Removing --read-only-port flag from /etc/kubernetes/kubelet.env didn't help. But setting --read-only-port=10255 resolved the issue.. ",
    "nelsonfassis": "Updated Heapster image to k8s.gcr.io/heapster-amd64:v1.5.2 ( Was using k8s.gcr.io/heapster-amd64:v1.4.2) and changed source from (--source=kubernetes:https://kubernetes.default) to --source=kubernetes.summary_api:''\nFixed the issue.. ",
    "towolf": "Had the problem as well, until I found this:\nYou can add --authentication-token-webhook to the kubelet startup flags, such that the Bearer token of Heapster is used for auth and not only client certs.\n. Ah, yes. Sorry. I just found this in my bash history. I had forgotten about it:\n[2018-04-14 15:05:11] kubectl -n kube-system edit clusterrole system:heapster. ",
    "paphillon": "@towolf  - Thank you! I will try this out and update this thread\nUpdate: - Yes it works! However, in addition to above, I had to update the system:heapster clusterrole as per https://github.com/kubernetes/heapster/issues/1936. ",
    "wenzuowei110": "same problem. ",
    "ttarczynski": "I've also seen a very similar issue with newer grafana image: gcr.io/google_containers/heapster-grafana-amd64:v5.0.4\n grafana starts properly\n there are no Pods and Clusters dashboards\n Importing the cluster.json results with and empty dashboard (without any panels or graphs)\n I've managed to get importing work  by applying this patch: #1796 (which was reverted this year with commit b67384)\nI've also found in grafana logs what happens when the setup_grafana script posts these 2 dashboards:\n* both requests end with HTTP 422 responses:\nt=2018-09-06T10:48:52+0000 lvl=info msg=\"Request Completed\" logger=context userId=0 orgId=1 uname= method=POST path=/api/dashboards/db status=422 remote_addr=[::1] time_ms=71 size=84 referer=\nt=2018-09-06T10:48:52+0000 lvl=info msg=\"Request Completed\" logger=context userId=0 orgId=1 uname= method=POST path=/api/dashboards/db status=422 remote_addr=[::1] time_ms=4 size=84 referer=. Is it maybe the same issue as #2013 ?. ",
    "lixianyang": "@ttarczynski I have the same problem\nt=2018-09-10T15:57:05+0000 lvl=info msg=\"Initializing HTTP Server\" logger=http.server address=0.0.0.0:3000 protocol=http subUrl= socket=\nConnected to the Grafana dashboard.\nThe datasource for the Grafana dashboard is now set.\nt=2018-09-10T15:57:07+0000 lvl=info msg=\"Request Completed\" logger=context userId=0 orgId=1 uname= method=POST path=/api/dashboards/db status=422 remote_addr=[::1] time_ms=4 size=84 referer=\nt=2018-09-10T15:57:07+0000 lvl=info msg=\"Request Completed\" logger=context userId=0 orgId=1 uname= method=POST path=/api/dashboards/db status=422 remote_addr=[::1] time_ms=4 size=84 referer=. Use heapster-grafana-amd64:v4.4.3 can show dashboard. I have the same issue\nimage version:\nheapster-grafana-amd64:v5.0.4\nheapster-influxdb-amd64:v1.5.2\nheapster-amd64:v1.5.4. ",
    "marcin-je": "I've just discovered that Eventer reports one out of two types of measurements. One will create events another will create log/events. If you want to use events on your influxdb sink, you need to pass withfields=true like:\n- --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086?withfields=true. ",
    "mikenorgate": "I am using cluster DNS so it could well be a DNS cache issue that I am seeing. ",
    "brancz": "@kawych can we find a way to have heapster transition from this org to a stackdriver organization? From the Kubernetes (sig-instrumentation) project point of view we agreed that Heapster is to be depcrecated. That doesn't mean stackdriver can't continue to support it, but as the Kubernetes project as a whole it's not a project we want to continue supporting. Can we agree on deprecating it here and having a stackdriver fork and support as necessary for your organization?. Thanks for taking the time to express yourself @DirectXMan12! You have clearly laid out, what my thoughts are exactly (but expressed them poorly myself).. Looks good to me from my side.. /lgtm. ",
    "m1093782566": "It's really a very sudden news to me! I was thinking contributing a new sink(poseidon, which is leaded by sig-scheduling) to heapster.. ",
    "deepak-vij": "Yeah, our current ongoing Poseidon/Firmament scheduler effort leverages Heapster real-time metrics for workload scheduling. We created a new sink for pushing metrics information out of heapster to Firmament scheduler.\nAs Dujun mentioned, we were about to initiate new Poseidon sink upstreaming process in the next couple of days.  . ",
    "eherot": "Hey guys, were any decisions ever made W/R/T the replacement for eventer? I see a bunch of discussion of linking to a replacement but with no follow-up.... ",
    "SFHfeihong": "I have same problem. The version of  grafana  is v5.0.4.. Based on https://github.com/kubernetes/heapster/issues/1179, I have deal with it successfully. Thank you!!. ",
    "wu105": "For new clusters installations getting grafana v5.0.4, the grafana does not provide any pre-loaded dashboards.   With grafana v4.4.3 we used previuosly, dashboards cluster and pod are pre-loaded.  \nOur work around is to save the dashboards from clusters that do have the cluster and pod dashboards as json files, and import them into the new cluster's grafana.    \nNew heapster versions should continue to include clulster and pod as built in dashboard as earlier versions.  If someone knows good  dashboards for kubernetes on grafana.com, please share with us.  I tried quite a few and still prefer the cluster and pod dashboards.. /reopen\n /remove-lifecycle rotten. compared with  #2013, this thread points an issue showed up between v4.4.3 and v5.0.4 and contains a workaround by saving the dashboards from clusters that do have the cluster and pod dashboards as json files, and import them into the new cluster's grafana.. grafana doc on configuring reverse proxy:  http://docs.grafana.org/installation/behind_proxy/. ",
    "zedtux": "Thank you @DirectXMan12 for closing my issue .. but as I stated in my issue, I followed your wiki and got this issue.\nUnless I missed something in the wiki (in this case, please point me to it) otherwise the wiki should be updated.\nI'm new to k8s and your comment is not helping me (I mean I was able to understand that I have an RBAC issue with unauthenticated users).\nThis issue is more like to improve the project's doc in order to help new comers like me.. Thank you @DirectXMan12 for your comment \ud83d\udc4d \nAm I not able to re-open the issue. There's no reopen button.. @andyxning and @acobaugh any news on this please ?. I hate those stupid bots !\n/remove-lifecycle stale. Damned ... /remove-lifecycle stale. ",
    "46053710": "\"--source=kubernetes\",\n              \"--sink=elasticsearch:http://10.12.213.1:9200?sniff=false\",\n              \"--sink=kafka:?brokers=10.12.22.40:9092&brokers=10.11.37.15:9092&brokers=10.12.22.57:9092&timeseriestopic=topic-monitor\",  \nThis configuration reported almost the same error\u3002. ",
    "nathanleclaire": "cc @emfree . @x13n @loburm Could we get this reviewed and/or merged please? Thanks!. thanks @DirectXMan12 !. ",
    "damianoneill": "$ curl -s http://xxx:10255/stats/ | jq '.stats[0].network'\n{\n  \"name\": \"eno1\",\n  \"rx_bytes\": 24109667713,\n  \"rx_packets\": 78524878,\n  \"rx_errors\": 0,\n  \"rx_dropped\": 12,\n  \"tx_bytes\": 3019947673,\n  \"tx_packets\": 25560068,\n  \"tx_errors\": 0,\n  \"tx_dropped\": 0,\n  \"interfaces\": [\n    {\n      \"name\": \"eno1\",\n      \"rx_bytes\": 24109667713,\n      \"rx_packets\": 78524878,\n      \"rx_errors\": 0,\n      \"rx_dropped\": 12,\n      \"tx_bytes\": 3019947673,\n      \"tx_packets\": 25560068,\n      \"tx_errors\": 0,\n      \"tx_dropped\": 0\n    },\n    {\n      \"name\": \"flannel.1\",\n      \"rx_bytes\": 0,\n      \"rx_packets\": 0,\n      \"rx_errors\": 0,\n      \"rx_dropped\": 0,\n      \"tx_bytes\": 0,\n      \"tx_packets\": 0,\n      \"tx_errors\": 0,\n      \"tx_dropped\": 8\n    }\n  ],\n  \"tcp\": {\n    \"Established\": 0,\n    \"SynSent\": 0,\n    \"SynRecv\": 0,\n    \"FinWait1\": 0,\n    \"FinWait2\": 0,\n    \"TimeWait\": 0,\n    \"Close\": 0,\n    \"CloseWait\": 0,\n    \"LastAck\": 0,\n    \"Listen\": 0,\n    \"Closing\": 0\n  },\n  \"tcp6\": {\n    \"Established\": 0,\n    \"SynSent\": 0,\n    \"SynRecv\": 0,\n    \"FinWait1\": 0,\n    \"FinWait2\": 0,\n    \"TimeWait\": 0,\n    \"Close\": 0,\n    \"CloseWait\": 0,\n    \"LastAck\": 0,\n    \"Listen\": 0,\n    \"Closing\": 0\n  }\n}. Hi @cezarygerard the challenge is we are using redhat atomic and therefore the version of Kubernetes is managed by this container operating system.  . ",
    "cezarygerard": "Hi @damianoneill \nThank you for submitting the issue.\nDo you see the same behaviour on clusters with newer versions of Kubernetes?. ",
    "bmoyles0117": "@DirectXMan12 this change is a stability fix for the Stackdriver sink, when we run Heapster in any environment that is not GCE, Heapster crashes. Having this change allows us to mitigate this crash for our users when the GCE Metadata Server is unavailable. Is there any way I can get your help in approving this change, and validating the case? I really appreciate any input you have on this, and any alternatives you have to offer to help us release this critical stability fix to our users.. Just clarifying a bit, by \"our users\", I'm totally being a bit selfish when I say that means \"Stackdriver Users\", or \"Users that have enabled the Stackdriver sink\", I didn't mean to conflate that this somehow affects heapster users not using this sink.. Updated. Updated. Updated. Updated. Done. Updated, PTAL. Updated. Updated. ",
    "LynnChen1989": "this is my grafana.ini config file.\n```\n############## Grafana Configuration Example\n\nEverything has defaults so you only need to uncomment things you want to\nchange\npossible values : production, development\napp_mode = production\ninstance name, defaults to HOSTNAME environment variable value or hostname if HOSTNAME var is empty\n; instance_name = ${HOSTNAME}\n############################## Paths\n[paths]\nPath to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n\n;data = /var/lib/grafana\n\nDirectory where grafana can store logs\n\n;logs = /var/log/grafana\n\nDirectory where grafana will automatically scan and look for plugins\n\n;plugins = /var/lib/grafana/plugins\n\n############################## Server\n[server]\nProtocol (http, https, socket)\n;protocol = http\nThe ip address to bind to, empty will bind to all interfaces\n;http_addr =\nThe http port  to use\n;http_port = 3000\nThe public facing domain name used to access grafana from a browser\n;domain = localhost\nRedirect to correct domain if host header does not match domain\nPrevents DNS rebinding attacks\n;enforce_domain = false\nThe full public facing url you use in browser, used for redirects and emails\nIf you use reverse proxy and sub path specify full url (with sub path)\n;root_url = http://localhost:3000\nLog web requests\n;router_logging = false\nthe path relative working path\n;static_root_path = public\nenable gzip\n;enable_gzip = false\nhttps certs & key file\n;cert_file =\n;cert_key =\nUnix socket path\n;socket =\n############################## Database\n[database]\nYou can configure the database connection by specifying type, host, name, user and password\nas seperate properties or as on string using the url propertie.\nEither \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\nIf the password contains # or ; you have to wrap it with trippel quotes. Ex \"\"\"#password;\"\"\"\n;password =\nUse either URL or the previous fields to configure the database\nExample: mysql://user:secret@host:port/database\n;url =\nFor \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\nFor \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\nMax conn setting default is 0 (mean not set)\n;max_idle_conn =\n;max_open_conn =\n############################## Session\n[session]\nEither \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\nProvider config options\nmemory: not have any config yet\nfile: session dir path, is relative to grafana data_path\nredis: config like redis server e.g. addr=127.0.0.1:6379,pool_size=100,db=grafana\nmysql: go-sql-driver/mysql dsn config string, e.g. user:password@tcp(127.0.0.1:3306)/database_name\npostgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\nSession cookie name\n;cookie_name = grafana_sess\nIf you use session in https only, default is false\n;cookie_secure = false\nSession life time, default is 86400\n;session_life_time = 86400\n############################## Data proxy\n[dataproxy]\nThis enables data proxy logging, default is false\n;logging = false\n############################## Analytics\n[analytics]\nServer reporting, sends usage counters to stats.grafana.org every 24 hours.\nNo ip addresses are being tracked, only simple counters to track\nrunning instances, dashboard and error counts. It is very helpful to us.\nChange this option to false to disable reporting.\n;reporting_enabled = true\nSet to false to disable all checks to https://grafana.net\nfor new vesions (grafana itself and plugins), check is used\nin some UI views to notify that grafana or plugin update exists\nThis option does not cause any auto updates, nor send any information\nonly a GET request to http://grafana.com to get latest versions\n;check_for_updates = true\nGoogle Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n############################## Security\n[security]\ndefault admin user, created on startup\n;admin_user = admin\ndefault admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password = admin\nused for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\nAuto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\ndisable gravatar profile images\n;disable_gravatar = false\ndata source proxy whitelist (ip_or_domain:port separated by spaces)\n;data_source_proxy_whitelist =\n[snapshots]\nsnapshot sharing options\n;external_enabled = true\n;external_snapshot_url = https://snapshots-origin.raintank.io\n;external_snapshot_name = Publish to snapshot.raintank.io\nremove expired snapshot\n;snapshot_remove_expired = true\nremove snapshots after 90 days\n;snapshot_TTL_days = 90\n############################## Users\n[users]\ndisable user signup / registration\n;allow_sign_up = true\nAllow non admin users to create organizations\n;allow_org_create = true\nSet to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\nDefault role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\nBackground text for the user field on the login page\n;login_hint = email or username\nDefault UI theme (\"dark\" or \"light\")\n;default_theme = dark\nExternal user management, these options affect the organization users view\n;external_manage_link_url =\n;external_manage_link_name =\n;external_manage_info =\n[auth]\nSet to true to disable (hide) the login form, useful if you use OAuth, defaults to false\n;disable_login_form = false\nSet to true to disable the signout link in the side menu. useful if you use auth.proxy, defaults to false\n;disable_signout_menu = false\n############################## Anonymous Auth\n[auth.anonymous]\nenable anonymous access\n;enabled = false\nspecify organization name that should be used for unauthenticated users\n;org_name = Main Org.\nspecify role for unauthenticated users\n;org_role = Viewer\n############################## Github Auth\n[auth.github]\n;enabled = false\n;allow_sign_up = true\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n############################## Google Auth\n[auth.google]\n;enabled = false\n;allow_sign_up = true\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n############################## Generic OAuth\n[auth.generic_oauth]\n;enabled = false\n;name = OAuth\n;allow_sign_up = true\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://foo.bar/login/oauth/authorize\n;token_url = https://foo.bar/login/oauth/access_token\n;api_url = https://foo.bar/user\n;team_ids =\n;allowed_organizations =\n############################## Grafana.com Auth\n[auth.grafana_com]\n;enabled = false\n;allow_sign_up = true\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email\n;allowed_organizations =\n############################## Auth Proxy\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n;ldap_sync_ttl = 60\n;whitelist = 192.168.1.1, 192.168.2.1\n############################## Basic Auth\n[auth.basic]\n;enabled = true\n############################## Auth LDAP\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n;allow_sign_up = true\n############################## SMTP / Emailing\n[smtp]\nenabled = true\nhost = smtp.xxx.com:25\nuser = ops@xxxx.com\nIf the password contains # or ; you have to wrap it with trippel quotes. Ex \"\"\"#password;\"\"\"\npassword = xxxxx\n;cert_file =\n;key_file =\nskip_verify = true\nfrom_address = xxxx\nfrom_name = K8s-Grafana\n[emails]\n;welcome_email_on_sign_up = false\n############################## Logging\n[log]\nEither \"console\", \"file\", \"syslog\". Default is console and  file\nUse space to separate multiple modes, e.g. \"console file\"\n;mode = console file\nEither \"debug\", \"info\", \"warn\", \"error\", \"critical\", default is \"info\"\nlevel = info\noptional settings to set different levels for specific loggers. Ex filters = sqlstore:debug\n;filters =\nFor \"console\" mode only\n[log.console]\n;level =\nlog line format, valid options are text, console and json\n;format = console\nFor \"file\" mode only\n[log.file]\n;level =\nlog line format, valid options are text, console and json\n;format = text\nThis enables automated log rotate(switch of following options), default is true\n;log_rotate = true\nMax line number of single file, default is 1000000\n;max_lines = 1000000\nMax size shift of single file, default is 28 means 1 << 28, 256MB\n;max_size_shift = 28\nSegment log daily, default is true\n;daily_rotate = true\nExpired days of log file(delete after max days), default is 7\n;max_days = 7\n[log.syslog]\n;level =\nlog line format, valid options are text, console and json\n;format = text\nSyslog network type and address. This can be udp, tcp, or unix. If left blank, the default unix endpoints will be used.\n;network =\n;address =\nSyslog facility. user, daemon and local0 through local7 are valid.\n;facility =\nSyslog tag. By default, the process' argv[0] is used.\n;tag =\n############################## AMQP Event Publisher\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\n############################## Alerting\n[alerting]\nDisable alerting engine & UI features\n;enabled = true\nMakes it possible to turn off alert rule execution but alerting UI is visible\n;execute_alerts = true\n############################## Internal Grafana Metrics\nMetrics available at HTTP API Url /api/metrics\n[metrics]\nDisable / Enable internal metrics\n;enabled           = true\nPublish interval\n;interval_seconds  = 10\nSend internal metrics to Graphite\n[metrics.graphite]\nEnable by setting the address setting (ex localhost:2003)\n;address =\n;prefix = prod.grafana.%(instance_name)s.\n############################## Grafana.com integration\nUrl used to to import dashboards directly from Grafana.com\n[grafana_com]\n;url = https://grafana.com\n############################## External image storage\n[external_image_storage]\nUsed for uploading images to public servers so they can be included in slack/email messages.\nyou can choose between (s3, webdav)\n;provider = local\n[external_image_storage.s3]\nbucket_url = https://s3.cn-north-1.amazonaws.com.cn/alert-image\naccess_key = xxxx\nsecret_key = xxx\n[external_image_storage.webdav]\n;url =\n;public_url =\n;username =\n;password =\n```\nWhen some alarm occur, I got the alarm email, but images about  tendency chart was missed.\nI got the error messages from log like: \n```\nlvl=eror msg=\"Failed to upload alert panel image.\" logger=alerting.notifier error=\"fork/exec /usr/share/grafana/vendor/phantomjs/phantomjs: no such file or directory\"\n```\nabout config file, you can just focus on section External image storage.\nthks.\n. @andyxning phantomjs is existed, I'm sure. when I use command \"ls -l\", I can got it.. ",
    "rpardini": "Same problem here. Also k8s.gcr.io/heapster-influxdb-amd64:v1.5.2 is missing too.. ",
    "MarcDufresne": "Calling the registry API I can see only 2 tags for heapster-influxdb-amd64.\ncurl -X GET https://k8s.gcr.io/v2/heapster-influxdb-amd64/tags/list\njson\n{\n    \"child\": [],\n    \"manifest\": {\n        \"sha256:15d48e8a864d73e763f6eb7a010e93902cf625967062bdf25a055098a6aac634\": {\n            \"imageSizeBytes\": \"4368258\",\n            \"layerId\": \"\",\n            \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n            \"tag\": [\n                \"v1.1.1\"\n            ],\n            \"timeCreatedMs\": \"1484908581555\",\n            \"timeUploadedMs\": \"1515015410374\"\n        },\n        \"sha256:f433e331c1865ad87bc5387589965528b78cd6b1b2f61697e589584d690c1edd\": {\n            \"imageSizeBytes\": \"4744881\",\n            \"layerId\": \"\",\n            \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n            \"tag\": [\n                \"v1.3.3\"\n            ],\n            \"timeCreatedMs\": \"1504622163891\",\n            \"timeUploadedMs\": \"1515015426684\"\n        }\n    },\n    \"name\": \"heapster-influxdb-amd64\",\n    \"tags\": [\n        \"v1.1.1\",\n        \"v1.3.3\"\n    ]\n}\nAnd 4 tags for heapster-grafana-amd64\njson\n{\n    \"child\": [],\n    \"manifest\": {\n        \"sha256:4a472eb4df03f4f557d80e7c6b903d9c8fe31493108b99fbd6da6540b5448d70\": {\n            \"imageSizeBytes\": \"51562322\",\n            \"layerId\": \"\",\n            \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n            \"tag\": [\n                \"v4.4.3\"\n            ],\n            \"timeCreatedMs\": \"1504621342731\",\n            \"timeUploadedMs\": \"1515015380080\"\n        },\n        \"sha256:9bf6d8edf8884f19209ca424308e28aa0cbaaeb9f02b4ac237effb6011a09c1f\": {\n            \"imageSizeBytes\": \"49999898\",\n            \"layerId\": \"\",\n            \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n            \"tag\": [\n                \"v4.2.0\"\n            ],\n            \"timeCreatedMs\": \"1498217420895\",\n            \"timeUploadedMs\": \"1515015317303\"\n        },\n        \"sha256:ab7927a702b41e4ffaf1e54fc9647db32a71ea971f441e75906ee83daa3c898d\": {\n            \"imageSizeBytes\": \"45207279\",\n            \"layerId\": \"\",\n            \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n            \"tag\": [\n                \"v4.0.2\"\n            ],\n            \"timeCreatedMs\": \"1484909265808\",\n            \"timeUploadedMs\": \"1515015285755\"\n        },\n        \"sha256:d03589619f88f6403ec2d0fccef1483fd900741a0bc79f4064b233f1051fd1b8\": {\n            \"imageSizeBytes\": \"51538655\",\n            \"layerId\": \"\",\n            \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n            \"tag\": [\n                \"v4.4.1\"\n            ],\n            \"timeCreatedMs\": \"1500360925827\",\n            \"timeUploadedMs\": \"1515015348309\"\n        }\n    },\n    \"name\": \"heapster-grafana-amd64\",\n    \"tags\": [\n        \"v4.0.2\",\n        \"v4.2.0\",\n        \"v4.4.1\",\n        \"v4.4.3\"\n    ]\n}. ",
    "stan-grin": "\u279c  ~ docker pull k8s.gcr.io/heapster-grafana-amd64:v5.0.4\nPulling repository k8s.gcr.io/heapster-grafana-amd64\nunauthorized: authentication required. ",
    "hbceylan": "The problem still continue;\n$ docker pull k8s.gcr.io/heapster-influxdb-amd64:v1.5.2\nError response from daemon: manifest for k8s.gcr.io/heapster-influxdb-amd64:v1.5.2 not found\n\n. ",
    "nhwuxiaojun": "The problem still continue;\n$ docker pull k8s.gcr.io/heapster-grafana-amd64:v5.0.4\nPulling repository k8s.gcr.io/heapster-grafana-amd64\nunauthorized: authentication required\n$ docker pull k8s.gcr.io/heapster-influxdb-amd64:v1.5.2\nPulling repository k8s.gcr.io/heapster-influxdb-amd64\nunauthorized: authentication required\nOnly heapster can be pulled\n$docker pull  k8s.gcr.io/heapster-amd64:v1.5.3\nv1.5.3: Pulling from heapster-amd64\nDigest: sha256:fc33c690a3a446de5abc24b048b88050810a58b9e4477fa763a43d7df029301a\nStatus: Image is up to date for k8s.gcr.io/heapster-amd64:v1.5.3. @MarcDufresne Thanks . I use heapster-grafana-amd64:v4.4.3 instead of  heapster-grafana-amd64:v5.0.4 and use  heapster-influxdb-amd64:v1.3.3 instead of v1.5.2. It works. ",
    "mgaruccio": "I'm seeing the same problem as well.  The odd thing is that the code updating those versions was pushed 3 months ago and it looks like both image pull errors just started in the last 11 or 12 days, possible the newer versions got pulled from the registry?. ",
    "mbert": "Looks so. I would not mind as long as the old versions work just as well, but since the newer versions are referenced in the yamls for setting up the dashboard, this is an actual problem. . Confirmed. I had been using them for several months. . I've tried this out. It's almost all fine, however image k8s.gcr.io/heapster-amd64:v1.6.0-beta.1 which is referenced in heapster/deploy/kube-config/influxdb/heapster.yaml of the dashboard repo is not available. . ",
    "wynn5a": "Yes, same problem, no answer here.. ",
    "isabellyrocha": "@loburm Any update on this? I have also been using them for several months, but have the same problem now.. Same thing here:\nError response from daemon: manifest for k8s.gcr.io/heapster-amd64:v1.6.0-beta.1 not found. ",
    "tomfotherby": "It's not available yet:\n$ docker pull k8s.gcr.io/heapster-amd64:v1.6.0-beta.1\nError response from daemon: manifest for k8s.gcr.io/heapster-amd64:v1.6.0-beta.1 not found\n\n(I found this issue while following the AWS Dashboard tutorial: https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html and trying to debug why it doesn't work). ",
    "carlosmkb": "here:\nFailed to pull image \"k8s.gcr.io/heapster-amd64:v1.6.0-beta.1\": rpc error: code = Unknown desc = unauthorized: authentication required\n. ",
    "jhelbling": "docker pull k8s.gcr.io/heapster-amd64:v1.6.0-beta.1\nv1.6.0-beta.1: Pulling from heapster-amd64\n44f3029d6016: Downloading [=====>                                             ]  2.308MB/20.38MB. ",
    "amihura": "Failed to pull image \"k8s.gcr.io/heapster-influxdb-amd64:v1.5.2\": rpc error: code = Unknown desc = Error response from daemon: manifest for k8s.gcr.io/heapster-influxdb-amd64:v1.5.2 not found. ",
    "tubit": "It's still the same, image not available.. ",
    "dionmes": "Same here.. ",
    "boukandouramhamed": "Same here. . ",
    "0xDing": "Same here.\n. ",
    "nanno77": "Same here.. ",
    "Joshscorp": "same here. ",
    "Mechtecs": "Same here.. ",
    "mingweiLIU": "Same here.. ",
    "kithral": "Same - Tags Available\nv1.1.1 and v1.3.3. I just the yaml to load version 1.3.3 and it loaded fine.. ",
    "modernix": "Same here.. ",
    "Tizeen": "Same here.. ",
    "divyangjp": "v1.5.2 is not available on Google container registry. Using v1.3.3 made it work.. ",
    "hwdevops": "The heapster image is not found, and other versions and platforms do not exist.. ",
    "kremerol": "For me, 1.5.3 is NOK, but 1.5.2 is OK.\nSo, as a workaround, you can stick to 1.5.2:\n```\ncurl -sSL \"https://raw.githubusercontent.com/kubernetes/heapster/v1.5.2/deploy/kube-config/influxdb/influxdb.yaml\" | sed \"s/amd64/arm64/g\" | kubectl apply -f -\ncurl -sSL \"https://raw.githubusercontent.com/kubernetes/heapster/v1.5.2/deploy/kube-config/influxdb/heapster.yaml\" | sed \"s/amd64/arm64/g\" | kubectl apply -f -\ncurl -sSL \"https://raw.githubusercontent.com/kubernetes/heapster/v1.5.2/deploy/kube-config/influxdb/grafana.yaml\" | sed \"s/amd64/arm64/g\" |  sed \"s/# type: NodePort/type: NodePort/g\" | kubectl apply -f -\nkubectl apply -f \"https://raw.githubusercontent.com/kubernetes/heapster/v1.5.2/deploy/kube-config/rbac/heapster-rbac.yaml\"\n```. ",
    "avgKol": "Here is an explanation for \"Cumulative CPU usage on all cores\" \nhttps://github.com/kubernetes/dashboard/issues/1010#issuecomment-324632866. ",
    "jakubbujny": "I'm sorry but with that description I still don't know how to interpret such big numbers which I'm observing. Should I report bug?. Now it makes perfect sense - thank you! I looked on graphs, calculated non negative derivative and compared to usage_rate and it's totally fine. Please consider placing that information in documentation/readme - especially that it's actually CPU time metric like those from Linux kernel from /proc/stat in [nanoseconds]\n. @DirectXMan12 - ready to merge :). ",
    "SamolazovRoman": "\u279c  heapster git:(master) \u2717 kubectl -n kube-system describe pod/monitoring-grafana-68b57d754-zvpzx\nName:           monitoring-grafana-68b57d754-zvpzx\nNamespace:      kube-system\nNode:           minikube/10.0.2.15\nStart Time:     Thu, 12 Jul 2018 09:46:56 +0300\nLabels:         k8s-app=grafana\n                pod-template-hash=246138310\n                task=monitoring\nAnnotations:    <none>\nStatus:         Pending\nIP:             172.17.0.4\nControlled By:  ReplicaSet/monitoring-grafana-68b57d754\nContainers:\n  grafana:\n    Container ID:\n    Image:          k8s.gcr.io/heapster-grafana-amd64:v5.0.4\n    Image ID:\n    Port:           3000/TCP\n    Host Port:      0/TCP\n    State:          Waiting\n      Reason:       ImagePullBackOff\n    Ready:          False\n    Restart Count:  0\n    Environment:\n      INFLUXDB_HOST:               monitoring-influxdb\n      GF_SERVER_HTTP_PORT:         3000\n      GF_AUTH_BASIC_ENABLED:       false\n      GF_AUTH_ANONYMOUS_ENABLED:   true\n      GF_AUTH_ANONYMOUS_ORG_ROLE:  Admin\n      GF_SERVER_ROOT_URL:          /\n    Mounts:\n      /etc/ssl/certs from ca-certificates (ro)\n      /var from grafana-storage (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqnwq (ro)\nConditions:\n  Type           Status\n  Initialized    True\n  Ready          False\n  PodScheduled   True\nVolumes:\n  ca-certificates:\n    Type:          HostPath (bare host directory volume)\n    Path:          /etc/ssl/certs\n    HostPathType:\n  grafana-storage:\n    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:\n  default-token-qqnwq:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-qqnwq\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type     Reason                 Age                 From               Message\n  ----     ------                 ----                ----               -------\n  Normal   Scheduled              38m                 default-scheduler  Successfully assigned monitoring-grafana-68b57d754-zvpzx to minikube\n  Normal   SuccessfulMountVolume  38m                 kubelet, minikube  MountVolume.SetUp succeeded for volume \"ca-certificates\"\n  Normal   SuccessfulMountVolume  38m                 kubelet, minikube  MountVolume.SetUp succeeded for volume \"grafana-storage\"\n  Normal   SuccessfulMountVolume  38m                 kubelet, minikube  MountVolume.SetUp succeeded for volume \"default-token-qqnwq\"\n  Warning  Failed                 37m (x3 over 38m)   kubelet, minikube  Failed to pull image \"k8s.gcr.io/heapster-grafana-amd64:v5.0.4\": rpc error: code = Unknown desc = Error response from daemon: manifest for k8s.gcr.io/heapster-grafana-amd64:v5.0.4 not found\n  Warning  Failed                 37m (x3 over 38m)   kubelet, minikube  Error: ErrImagePull\n  Normal   Pulling                36m (x4 over 38m)   kubelet, minikube  pulling image \"k8s.gcr.io/heapster-grafana-amd64:v5.0.4\"\n  Normal   BackOff                32m (x21 over 38m)  kubelet, minikube  Back-off pulling image \"k8s.gcr.io/heapster-grafana-amd64:v5.0.4\"\n  Warning  Failed                 2m (x149 over 38m)  kubelet, minikube  Error: ImagePullBackOff\n\u279c  heapster git:(master) \u2717. \u279c  heapster git:(master) \u2717 kubectl -n kube-system describe pod/monitoring-influxdb-cc95575b9-5xqz8\nName:           monitoring-influxdb-cc95575b9-5xqz8\nNamespace:      kube-system\nNode:           minikube/10.0.2.15\nStart Time:     Thu, 12 Jul 2018 09:46:56 +0300\nLabels:         k8s-app=influxdb\n                pod-template-hash=775113165\n                task=monitoring\nAnnotations:    <none>\nStatus:         Pending\nIP:             172.17.0.5\nControlled By:  ReplicaSet/monitoring-influxdb-cc95575b9\nContainers:\n  influxdb:\n    Container ID:\n    Image:          k8s.gcr.io/heapster-influxdb-amd64:v1.5.2\n    Image ID:\n    Port:           <none>\n    Host Port:      <none>\n    State:          Waiting\n      Reason:       ImagePullBackOff\n    Ready:          False\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /data from influxdb-storage (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqnwq (ro)\nConditions:\n  Type           Status\n  Initialized    True\n  Ready          False\n  PodScheduled   True\nVolumes:\n  influxdb-storage:\n    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:\n  default-token-qqnwq:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-qqnwq\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type     Reason                 Age                  From               Message\n  ----     ------                 ----                 ----               -------\n  Normal   Scheduled              39m                  default-scheduler  Successfully assigned monitoring-influxdb-cc95575b9-5xqz8 to minikube\n  Normal   SuccessfulMountVolume  39m                  kubelet, minikube  MountVolume.SetUp succeeded for volume \"influxdb-storage\"\n  Normal   SuccessfulMountVolume  39m                  kubelet, minikube  MountVolume.SetUp succeeded for volume \"default-token-qqnwq\"\n  Normal   Pulling                37m (x4 over 39m)    kubelet, minikube  pulling image \"k8s.gcr.io/heapster-influxdb-amd64:v1.5.2\"\n  Warning  Failed                 37m (x4 over 39m)    kubelet, minikube  Failed to pull image \"k8s.gcr.io/heapster-influxdb-amd64:v1.5.2\": rpc error: code = Unknown desc = Error response from daemon: manifest for k8s.gcr.io/heapster-influxdb-amd64:v1.5.2 not found\n  Warning  Failed                 37m (x4 over 39m)    kubelet, minikube  Error: ErrImagePull\n  Normal   BackOff                13m (x106 over 39m)  kubelet, minikube  Back-off pulling image \"k8s.gcr.io/heapster-influxdb-amd64:v1.5.2\"\n  Warning  Failed                 3m (x149 over 39m)   kubelet, minikube  Error: ImagePullBackOff\n\u279c  heapster git:(master) \u2717. ",
    "mtsgrd": "Any help would be appreciated. It was 4 days since I last checked, and in this time the number of series has grown by 60k(!). ",
    "Queuecumber": "Sorry I'm not sure what you mean by networking vendor, I'm not doing anything non-standard here it's a bare metal install on a VM running on my laptop for evaluation purposes. . I should also point out that its a single-node cluster in theory packets shouldnt even be leaving the machine, I'm not sure how a network vendor would be relevant . So there were actually a couple of things going wrong but it is worth pointing out that heapster requires kubelet to have its readOnlyPort set to 10255. Kubeadm does not provide an easy way to do this with its config file, but you can instead update /etc/default/kubelet to set KUBELET_EXTRA_ARGS=--read-only-port 10255. Yeah that looks perfect, it's too bad it requires a source change to work. Probably the default should be to use the secure port for everything right?. ",
    "chakravarthy392": "@007 They have been mentioning that Heapster is depreciated from last 3 months but released a latest version to fix some issues. The latest version is not even updated in Makefile. So I just want to know if 1.5.4 is only getting installed even though it shows 1.5.3 as version !!. ",
    "fqsghostcloud": "/assign @x13n . ",
    "Hokwang": "I tried to use 1.10.7 and 1.9.10, but there's problem still.\nand I tried to do not use ingress, so use NodePort, but problem still exists.\nI want to know how grafana create dashboard automatically \nand can anybody tell me a normal log for that?. @DirectXMan12 can you comment anything? I need some help.. ",
    "arenstar": "Same Same - No default dashboards either. ",
    "guocaifeng": "Translate the following into English\nHello:\nHow should I configure it to display it?I don't know about your disabled cluster configuration, so I hope you can help me figure out how to configure it.\nHere is my full configuration.Thank you very much\n\u60a8\u597d\uff1a\n      \u8bf7\u95ee\u6211\u5e94\u8be5\u600e\u6837\u914d\u7f6e\uff0c\u624d\u53ef\u4ee5\u5c06\u4ed6\u663e\u793a\u51fa\u6765\u5462\uff1f\u60a8\u6240\u7684\u7981\u7528\u96c6\u7fa4\u914d\u7f6e\uff0c\u6211\u81ea\u5df1\u4e0d\u6e05\u695a\uff0c\u5e0c\u671b\u60a8\u53ef\u4ee5\u5e2e\u6211\u6307\u51fa\u6765\u9700\u8981\u600e\u6837\u914d\u7f6e\u3002\n      \u4ee5\u4e0b\u662f\u6211\u7684\u5168\u90e8\u914d\u7f6e\u3002\u975e\u5e38\u611f\u8c22\u3002\n```\n1. kubernetes-dashboard.yaml\nCopyright 2017 The Kubernetes Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\nConfiguration to deploy release version of the Dashboard UI compatible with\nKubernetes 1.8.\n\nExample usage: kubectl create -f \n------------------- Dashboard Secret -------------------\napiVersion: v1\nkind: Secret\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard-certs\n  namespace: kube-system\ntype: Opaque\n\n------------------- Dashboard Service Account -------------------\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\n\n------------------- Dashboard Role & Role Binding -------------------\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: kubernetes-dashboard-minimal\n  namespace: kube-system\nrules:\n  # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret.\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"create\"]\n  # Allow Dashboard to create 'kubernetes-dashboard-settings' config map.\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"create\"]\n  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  resourceNames: [\"kubernetes-dashboard-key-holder\", \"kubernetes-dashboard-certs\"]\n  verbs: [\"get\", \"update\", \"delete\"]\n  # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  resourceNames: [\"kubernetes-dashboard-settings\"]\n  verbs: [\"get\", \"update\"]\n  # Allow Dashboard to get metrics from heapster.\n- apiGroups: [\"\"]\n  resources: [\"services\"]\n  resourceNames: [\"heapster\"]\n  verbs: [\"proxy\"]\n- apiGroups: [\"\"]\n  resources: [\"services/proxy\"]\n  resourceNames: [\"heapster\", \"http:heapster:\", \"https:heapster:\"]\n  verbs: [\"get\"]\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: kubernetes-dashboard-minimal\n  namespace: kube-system\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: kubernetes-dashboard-minimal\nsubjects:\n- kind: ServiceAccount\n  name: kubernetes-dashboard\n  namespace: kube-system\n\n------------------- Dashboard Deployment -------------------\nkind: Deployment\napiVersion: apps/v1beta2\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      k8s-app: kubernetes-dashboard\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard\n    spec:\n      serviceAccountName: kubernetes-dashboard-admin\n      containers:\n      - name: kubernetes-dashboard\n        image: www.bmyt.com/k8s/kubernetes-dashboard-amd64:v1.10.0\n        ports:\n        - containerPort: 9090\n          protocol: TCP\n        args:\n          #- --auto-generate-certificates\n          # Uncomment the following line to manually specify Kubernetes API server Host\n          # If not specified, Dashboard will attempt to auto discover the API server and connect\n          # to it. Uncomment only if the default does not work.\n          #- --apiserver-host=http://10.0.1.168:8080\n        volumeMounts:\n        - name: kubernetes-dashboard-certs\n          mountPath: /certs\n          # Create on-disk volume to store exec logs\n        - mountPath: /tmp\n          name: tmp-volume\n        livenessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /\n            port: 9090\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      volumes:\n      - name: kubernetes-dashboard-certs\n        secret:\n          secretName: kubernetes-dashboard-certs\n      - name: tmp-volume\n        emptyDir: {}\n      serviceAccountName: kubernetes-dashboard-admin\n      # Comment the following tolerations if Dashboard must not be deployed on master\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n\n------------------- Dashboard Service -------------------\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\nspec:\n  ports:\n    - port: 9090\n      targetPort: 9090\n  selector:\n    k8s-app: kubernetes-dashboard\n------------------------------------------------------------\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard-external\n  namespace: kube-system\nspec:\n  ports:\n    - port: 9090\n      targetPort: 9090\n      nodePort: 32017\n  type: NodePort\n  selector:\n    k8s-app: kubernetes-dashboard\n2. kubernetes-dashboard-admin.rbac.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard-admin\n  namespace: kube-system\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: kubernetes-dashboard-admin\n  labels:\n    k8s-app: kubernetes-dashboard\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: kubernetes-dashboard-admin\n  namespace: kube-system\n\nheapster.yaml\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: heapster\n  namespace: kube-system\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: www.bmyt.com/k8s/heapster-amd64:v1.5.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086\n\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    task: monitoring\n    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)\n    # If you are NOT using this as an addon, you should comment out this line.\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: Heapster\n  name: heapster\n  namespace: kube-system\nspec:\n  ports:\n  - port: 80\n    targetPort: 8082\n  selector:\n    k8s-app: heapster\n\nheapster-rbac.yaml\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: heapster\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:heapster\nsubjects:\nkind: ServiceAccount\n  name: heapster\n  namespace: kube-system\n```. Thank you very much. I'll try. \n",
    "ILMostro": "Actually, this only happens if I try to specify it as a NodePort.  The issue can be closed, as it's a problem introduced by my customization.\n. @aschmied Thank you for following up with this.  I suspect that's correct, as I now see the difference side by side. :). ",
    "aschmied": "@ILMostro you've probably solved this by now, but just in case: you're running into the issue described here. When you split a deployment YAML into multiple sections and one of them has an error the reported line number is the offset from the start of the section not the start of the file. I think the root problem is that the indenting in your NodePort spec should be\nspec:\n  type: NodePort\n  ports:\n  - port: 8086\n    targetPort: 8086. ",
    "mirake": "@andyxning  But I didn't modify any go source code file, should I do that ?. ",
    "danopia": "I tried out v1.6.0-beta.1 in a Kubernetes v1.12.2 cluster and the pod memory metrics were all extremely underreported in dashboard (left). Downgrading to v1.5.4 brought believable numbers back (right)\n\n. ",
    "xichengliudui": "/assign @piosz. /ok-to-test. assign @x13n. /assign @andyxning. /retest. /retest. @andyxning\uff0cThank you, but still failed, I'm going to shut it down. ",
    "xiezongzhe": "/assign @andyxning. ",
    "jesseshieh": "I signed it!. ",
    "SataQiu": "/assign @andyxning. /test pull-heapster-unit. @andyxning thank you!\n. /retest. ",
    "nikopen": "@DirectXMan12 what are the implications for k/dashboard? It's widely used by end users.\nWe're very near the release of 1.13 (marked for next Monday), and there's an outstanding change to make the dashboard work without heapster: https://github.com/kubernetes/dashboard/issues/2986\nIs there an owner for the above integration? Shouldn't it be first available before retiring Heapster?\npinging release leads\ncc @spiffxp @AishSundar . ",
    "AishSundar": "@DirectXMan12 what does \"retiring\" mean? If I read the deprecation.md correctly, we only stop any maintenance / bug fixes to Heapster but it will continue to work for end users until they migration to other solutions called out in the README. is that right?. @DirectXMan12 Thanks for clarifying the expectations here (FWIW its inline with what thought wll be the state of the world post 1.13) I will followup on kubernetes/dashboard#2986 regarding dashboard support for metricsAPI. ",
    "caesarxuchao": "I'd say you should set kubeConfig.GroupVersion at line 111, that's the only path that could end up with a nil GroupVersion and cause the panic. I don't know why this problem is exposed now.\n. Sorry I don't have time to investigate why NewNonInteractiveDeferredLoadingClientConfig() returns empty version. On the other hand, the version in the kubeconfig isn't useful currently, it will be given a default value anyway, so I can LGTM the PR. It's just that you might get surprises when we have more groups that have multiple versions and the default version isn't the one you want (currently only batch have two versions). \n. ",
    "nickschuch": "No worries, I will make that change today.\n. ",
    "kris-nova": "@LaurentDumont - wondering if we can get more information about the issue this might cause? We are noticing some unexpected behavior when having this label defined.\n. ",
    "mssola": "It should be kubectl --namespace kube-system get services, no ?\n. ",
    "Mogztter": "typo: creates an event. typo: creates an event. dataEvents was renamed to events. do nothing \"if\" ? (instead of \"of\" ?). ",
    "igorpeshansky": "There is another option -- the credentials file actually has the project id embedded in it. If the user didn't supply one, why not try the one from the credentials file, and then the one from the metadata server, in that order?. This is actually already done by google.DefaultClient when the credentials file isn't present. Looks like a bunch of that code can be reused.. Two comments here:\n(a) You want the project id from the credentials to override the one from the environment,\n(b) You probably still want a way to supply the project id explicitly, in case the credentials aren't owned by the target project but simply have access to it.. Note: older credentials files may not have an explicit project id field, but have it embedded in the email address. Not sure it it's worth handling those cases here.. Yes, this is the order that makes the most sense from the Stackdriver perspective.. gcpProjectIdEnv?. [Optional] gcpCredentialsEnv?. [Optional] Some credentials are supplied in the PK12 format, rather than JSON. I'll let @crassirostris and @piosz decide whether this error is the right behavior on those cases or whether this code should fall back on the metadata server.. (a) Nit: if there's a fallback, the variable is not required,\n(b) Typo: \"nvironment\".. Would it make sense to keep this comment?. Nit: \"...the environment variable...\".. [Optional] The Google OAuth2 libraries also look for this file in the default location /etc/google/auth/application_default_credentials.json. @crassirostris and @piosz should decide if it makes sense to match this behavior.. [Optional] Adding a trailing \".\" would be consistent with the above comments, but not the ones below... I'll let you pick the style you want to follow.. That's fair -- the P12 format is listed as deprecated on the key download page.\nI'll let the Kubernetes team comment on the refactoring, but keep in mind that extracting the project id from credentials is actually not GCE-specific (though it might be GCP-specific).. ?. Should we have a comment here as well, e.g., // Get project ID from the environment?. ",
    "pongad": "Maybe rename these imports?. var timeseries []*monitoringpb.TimeSeries? That way empty arrays don't allocate.. Isn't this loop just\ngo\nfor req := range reqQueue {sink.sendOneRequest(req)}\n?. No need to write gax.WithGRPCOptions() if you don't pass options.\nThis function is used for passing more options to gRPC layer.. nil?. Consider using this. ",
    "neoseele": "CLA is signed, not sure if the bot will pick it up automatically or not. :)\nThe reason for this FR is discussed in b/73268296. Since cadvisor's summary api provides inodes metrics already, I see no reason why we shouldn't sink them to SD. \n. "
}
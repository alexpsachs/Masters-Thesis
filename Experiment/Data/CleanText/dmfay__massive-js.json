{
    "robconery": "Thank you :)\n. I need to work on this a bit more - Karl and I sort of left it in Limbo :). I have a need for it so I'll get working.\n. Just did...\n. Thanks! This has been fixed :)\n. This sounds like a great idea - I can't read the diff though :/\n. Happy to have your patch/help/whatnot\n. A couple of thoughts - first is you're asking me to rewrite the entire codebase for something that I don't see as a problem though I do agree it's silly (installing an extra module). The second is that I can remove the MySQL/PG mods from the dev dependencies and also solve this problem.\nFinally, if you have a look here:\nhttps://github.com/grncdr/node-any-db/blob/master/package.json#L25\nYou can see that you'll be installing those same modules anyway :)\n. Thanks John - something's wrong with my notifications and I didn't see this!\n. Yep done!\n. Not yet, no but that's a good idea - I'll push in the next rev\n. No joins - I'm doing transactions in the next rev...\n. Which provider are you using? Do you have a debugger? Also - is there something down the stack trace that would identify the line? Yeah ideally this is passed back up in the callback - looks like we're not handling that somehow. I'll have a look.\n. This I might have found it - line 92 of lib/query.js this is the execute function:\njavascript\n  //execution uses the Client\n  self.execute = function(callback) {\n    self.db.execute(self.sql, self.params, function(err,result) {\n      if(callback) { callback(err, result); }\n      self.emit(\"executed\");\n    });\n  };\nNormally the callback would be sent in to execute, but in this case I need to emit \"executed\" which is likely the thing failing given that the error is happening in events.js (a Node module).\nSoooo I'm thinking that there should be an if statement here that evals the error - emitting a failure once executed. Do you want to change the code locally and see if it works? If it does - PR me so you get credit :).\n. This was fixed in 0.0.4...\n. Shoved MySQL out; no longer supporting in v2. Although I think this problem might come from a local implementation bug or from improper error catching which was fixed in 0.0.4.\n. Odd - I'll have a look. The package.json is correct; not sure how else to make this right.\n. I'm actually thinking of ripping MySQL out completely. Horrible database.\n\nOn Dec 16, 2014, at 00:54, magicbrighter notifications@github.com wrote:\nmysql package version is 0.95 \uff0cbut mainline of mysql is 2.1.x. 2.1.x fix more bugs like auto reconnect etc. Do you have plan update mysql package\n\u2014\nReply to this email directly or view it on GitHub.\n. Ripped out in v2\n. There was something wrong in the load up - it was throwing and wouldn\u2019t start. I\u2019ll take another shot at this in a bit - had an idea last night at 2am :(\n\nOn March 5, 2015 at 7:44:44 AM, John Atten (notifications@github.com) wrote:\nThis works...I think. Could stand additional exercising + tests. I'll get to that after work tomorrow.\n\u2014\nReply to this email directly or view it on GitHub.\n. Couldn't help myself... \n. Lol... Btw I screwed up; the function assignment needs to happen in the walker file. Try and execute the deep query, it won't work. Wanna fix?\nJust move the function bind from index into Luke \n\nOn Mar 5, 2015, at 14:35, John Atten notifications@github.com wrote:\nDid you struggle with the choice between \"luke_file_walker\" and \"file_walk_with_me\"?\n\u2014\nReply to this email directly or view it on GitHub.\n. I blew it... fixing again\n. OK fixed it... too late!\n. first and last are repetitive. count is implemented. I don't case my comments.\n. Cleaned...\n. Both really - the REPL could be a huge tool for helping people out with SQL etc. I\u2019m going all over the place with ideas - let\u2019s hash a few out :).\n\nFirst idea is having Massive help you develop your DB. So running something like \u201cinitdb\u201d would create a db directory and an initialization script (maybe in /schema/tables or something). It could then stub out a nice users table for you - something you could use as a guide to learn how to run CREATE TABLE scripts and use tsvector, etc.\nThere could be a number of commands in there - like setting up a table for searchability (adding a search(tsvector) column, indexing it with GIN, creating a trigger). Outputting common operations (JSONb searches) etc.\nIn a way - like Rails generators except for Postgres queries :).\nWe could probably use Underscores templating to get started if we wanted - but figuring out a nice mechanism that underlies generation would be neat :).\nOn March 10, 2015 at 1:42:18 AM, John Atten (notifications@github.com) wrote:\nDo you mean stub out sql-ish templates + suggestions on-screen in the REPL, or take some guided inputs and generate scripts/save in a queries folder (or both)?\nGot an example of the REPL input you envision, and idea for output?\n\u2014\nReply to this email directly or view it on GitHub.\n. Yep - there are better tools out there and, frankly, I hate migrations in general :).\n. Yeah this is a really tough one. Postgres naming schemes go beyond \"guidelines\" and actually are enforced at a few levels. It simply does not like weird casing.\nLet's push a test case and see what we can do. I do not want to slow things down by snooping column info (it was in there and I took it out as it was just ... ugh). I do think we can delimit using \"TableName\" so that should be OK.\nAlso the REPL now supports invoking a find() or scripted query without a callback - the output is sent right to the screen HUZZAH.\n. Done sir :)\n. I think there's only one more place we need to add quotes to yes? Let's just add that and we'll stop there :). I think casing in the Node world isn't as bad as the .NET world.\n. Help me understand the problem? We have quotes going out, now we need quotes going in (which makes perfect sense). This is a formatting issue which should be a matter of adding quotes. Is there more to this?\n. RAD! I'll review in 40 mins THANKS!\n\nOn Mar 5, 2015, at 20:18, Owen Morgan-Jones notifications@github.com wrote:\nThe fix for #26 doesn't account for the PK column having non-standard casing. As such using the findOne function can throw errors. This PR fixes that by escaping the PK in \"s.\nI've also added a bunch of specs around how the custom sets of columns and compound order bys can be used when your columns aren't in the pg idiom. I made a few further changes to the Users table to allow for testing this.\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/robconery/massive-js/pull/27\nCommit Summary\nFixing issues surrounding non double quote delimited object names and come accompanying tests\nMerging upstream changes\nAdded specs for use of CamelCased object names in Columns and Order BYs and handled default Order By case when PK column name is non-standard cased.\nAdded specs for use of CamelCased object names in Columns and Order BYs and handled default Order By case when PK column name is non-standard cased.\nMerge branch 'v2' of https://github.com/Sharkwald/massive-js into v2\nRemoving reference to new table I added\nFile Changes\nM lib/table.js (4)\nM test/db/schema.sql (12)\nM test/table_spec.js (32)\nPatch Links:\nhttps://github.com/robconery/massive-js/pull/27.patch\nhttps://github.com/robconery/massive-js/pull/27.diff\n\u2014\nReply to this email directly or view it on GitHub.\n. Thanks so much - outstanding! Love the tests :) much appreciated.\n. Thanks! First: have you seen this fail? As I understand it (and have tested) __dirname is set based on the executing file. From the docs:\nThe name of the directory that the currently executing script resides in.\n\nSo, if you reference massive in /projects/myapp/app.js:\njavascript\nvar massive = require(\"massive\");\nmassive.connect({db : \"mydb\"}, function(err,db){//...});\nThe executing directory will be /projects/myapp/ because app.js is the executing file. If it were any other way, our tests wouldn't work :).\nI tested this by referencing massive in an external directory:\nnpm install ~/projects/massive\nIn that directory I have a /db subdirectory which was read fine. I then removed that directory to be sure that readdirSync would return an empty array instead of throwing (readFile throws) - and it does.\nThus my question - all of these things could be strange curiosities, but if you saw it fail - I'd love to know! \n. Weird. I'll have a play here - but as I said I tested with node_modules on my Mac and it worked as expected. I'll kick this up in my VM and see what happens.\nAlso - which version of Node? And thanks for the detail :)\n. Confirmed... wow this is really strange behavior. I see what you mean about the helpers - I'll just assume you're right as having __dirname be two separate things on Windows/*nix would be ridiculous.\nWondering how/why my tests were working before... I'l have to sleuth this...\n. Seems to work well - thanks again!\n. Yeah I\u2019m trippin out on how I missed this. I did have a db directory in the root when I was goofing around\u2026 that must have been what tripped me up when I did an install for node_modules \u2026 I hadn\u2019t done up the npmignore file yet.\nDamn.\nOn March 7, 2015 at 6:11:39 PM, dpen2000 (notifications@github.com) wrote:\nThanks! The tests would work fine I think. The scripts path for the tests is set here:\nvar scriptsDir = path.join(__dirname, \"..\", \"db\");\n(https://github.com/robconery/massive-js/blob/master/test/helpers/index.js#L5)\n__dirname here I would expect to give [locationOfMassiveSource]\\test\\helpers\\ (because the currently executing file is in helpers folder), the .. takes you down to test\\ and then the db folder is at that level so it finds the db folder correctly there.\nFor what it's worth, my node version is 0.10.33. Great project by the way: really like the jsonb functionality in particular.\n\u2014\nReply to this email directly or view it on GitHub.\n. Looks good - did you pull out the try/catch for any reason? It'll throw if the directory doesn't exist (which I was oblivious to for some reason) so we should allow for that.\n. Cheers - merge it in when you\u2019re done and thanks!\nOn March 8, 2015 at 7:51:33 AM, John Atten (notifications@github.com) wrote:\nPull out the function assignment from the file walking bits.\n@robconery Earlier on, you had thought you wanted to fie walk/Function assignment pulled out of index.js. You have since moved it back. Did you want to keep it there now, or did you still want to pull it out?\nIt's two functions now - easier to read/understand, but potentially dirtying up index.js with noise.\nWhat say you?\n\u2014\nReply to this email directly or view it on GitHub.\n. Hmmm - good question. The only schemas we create are for document tables. Can you see if using \u201cshema.doctable\u201d works for db.saveDoc?\nI\u2019d like it if people could create their own schemas and we just respond - in the future we can do more :).\nOn March 8, 2015 at 4:54:59 PM, John Atten (notifications@github.com) wrote:\nDo we want this to be bi-directional? In other words, do we want to be able to push schemas into the back end in some fashion when creating tables, by simply bolting the namespace on (somehow)? Or do we want to discourage willy-nilly schema creation by forcing folks to do this explicitly (even if it is a discrete/explicit massive api call).\n\u2014\nReply to this email directly or view it on GitHub.\n. Sorry - you lost me. For schemas I want people to work directly with the DB, I don\u2019t think Massive needs to be involved. The only place we care about schemas is when we create document tables with saveDoc - other than that I want to namespace the tables if the schema exists (db.myschema.mytable.find()).\nMake sense? So we shouldn\u2019t need addSchema\u2026 unless I missed something?\nOn March 8, 2015 at 5:09:17 PM, John Atten (notifications@github.com) wrote:\nThat's where I was wondering about for the moment. I kind of like the notion that if you want to add a schema, you need to be explicit instead of just bundling it on with a new table (doc or otherwise). People who know better will probably not do it that way, and people who don't know better will end up with a holy mess on their hands in the back end.\nI'll play with it though. We can make both something like you describe, and something like db.AddSchema(\"schemaName\") and see how you feel about them.\n\u2014\nReply to this email directly or view it on GitHub.\n. I just want a lot of code that\u2019s organized and easy to find/read. I was just looking for something simple - MkDocs looks easy; surprised Jekyll doesn\u2019t have something like it.\nThe one really nice thing about using gh-pages is flexing Github for community/doc support (as if).\u00a0\nOn March 8, 2015 at 4:51:02 PM, John Atten (notifications@github.com) wrote:\nI think you cover the essentials pretty well in the read me. However, I agree that some examples and a \"how to use this if you're brand-new\" might be good. Basic and well documented example project?\nI really like your style with the narrative bits like the read me. We should probably stick to that there, but I am happy to take on a more terse and simple set of examples and/or reference if you decide where the best place for it to live should be.\n\u2014\nReply to this email directly or view it on GitHub.\n. Yeah we can host in gh-pages. I keep feeling like we should be able to flex Jekyll somehow; tell ya what lemme take a stab at it over the next week or so. I have a template I really like - maybe I can get it to work with Jekyll (which I know pretty well).\nOn March 10, 2015 at 3:11:38 AM, John Atten (notifications@github.com) wrote:\nIf we used MkDocs or something like it, do you wanna host in gh project pages (with the attendant orphan branch in repo), separate repo of it's own (maybe on the massivejs org site you set up? Or a new repo Massive Docs?) or somewhere else?\nI'm happy to jump in and start messing with it, but the hosting decision is really in your court. On quick read, looks like we could use MkDocs, drop it in gh project pages, and easily move elsewhere if need be later?\n\u2014\nReply to this email directly or view it on GitHub.\n. Excellent - thank you!\n. Not a fan of promises TBH - lots of noise for very little return. \n. I know this will cause some problems but I don't want to add promises at this time.\n. \n. LOL I have a spec file here that I didn\u2019t git add for some reason! I pulled your changes down and you had almost the exact same file :).\nOn March 15, 2015 at 6:20:33 AM, John Atten (notifications@github.com) wrote:\nClosed #39 via 1e8628a.\n\u2014\nReply to this email directly or view it on GitHub.\n. Excellent thank you!\n. That's a lot of tables, and a whole lot of functions. My first reaction is that we should probably test to see which functions are from PostGis. My second reaction is that we should enable some kind of filter for table reads - meaning \"Only load these\" or \"this pattern\".\n@vintagexav when you run the function sql do you see anything that would mark the function as Postgis? Also, would killing the function read help you in this case? \nFinally - a schema or two on your table would help as well - @xivsolutions we should have the ability to load only a particular schema.\n. For some reason I didn't catch the nuance of the connection use here. That's a whole other issue:\n\nmassive.connect is being called before each request\n\nThat shouldn't be the case, and it's not how Massive was designed (at least not how I use it). The idea is that you \"bring Massive online\" when the app starts. You capture the instance somewhere (maybe in your own module which is cached) and use it from there.\n@vintagexav would this help your situation? @xivSolutions maybe we should make this use case clear in the README?\n. @vintagexav I'm happy to offer ideas - if you want to share some code I can try to help out. One way I've seen people do this is app.set(\"db\") with Express (if that's what you're using) or, as mentioned, having a dedicated module that creates an instance and keeps it alive (though Massive does this already).\n. Let's keep this thread on track. @vintagexav the first thing that we need to establish is that you see this slow down (10-20s) for your situation (100+ tables, 1200+ functions). This is not our targeted Use Case. \nThat said, it's something I very much want to accomodate. The queries as you see them, on their own, are not necessarily slow. What makes this whole thing slow is that Massive is trying to wrap close to 2000 objects and callbacks which is utterly ridiculous.\nTo keep us on track: you need to get this spin up time to come down, I need to see if we can help you with the way you're instantiating Massive. Both of the these things are doable and simple. There are 3 things to consider:\n- can you use a schema to namespace the things you want Massive to work with\n- can we come up with a filter strategy so that you can decide, on your own, which tables/functions you want spun up (if any)\n- is Massive the right tool here\nThe latter is a question we haven't brought up yet, but I'll be honest in saying that I didn't design it for your scenario (as valid as it is) and I'm not particularly inclined to add a bunch of code to accomodate for this scenario as I don't find it to be typical.\nBefore you get mad at me :):):) I think there's a good chance we can get some good middle ground that helps us all.\nSo - action items here are 1) help us come up with a filter strategy for you. We need to load tables/functions - what would work for you? and 2) would using a schema mitigate some of your issues?\nI really mean it when I say thank you for your time. Please don't take any comment above as dismissive.\n. As long as we're sure the connection is freed up - yay!\n. Radical - great catch John :).\n. Thanks @tracker1 - but the issue isn't connections (as far as I can tell) and pooling is handled by the driver, which apparently I had backwards (oops!).\nClosing this issue for now; I think I'll open a few follow-ons to address large table/function DBs.\n. Thanks :). Your question is a bit confusing, but I'll see if I can answer with I think you mean.\nSQL Injection is handled by the driver. All you need to do is be sure to use a parameter, like so:\nselect * from table where name=$1;\nThat $1 marks the statement for inclusion of a parameter. Massive is built on top of this idea. If you use SQL in files, you'll need to be sure to write you SQL like this. Then you can call it (assuming your file is called myQuery.sql) like this:\ndb.myQuery('joe', ...)\nThe value \"joe\" will be added to the query as a parameter, which is sanitized by the driver.\nYour question, the way it's phrased, makes me wonder what you mean by injections into existing scripts in the db folder. If someone had access to your box and dumped a SQL file in the /db folder and restarted your server - yes that's a problem. I don't think that's what you mean, however, because that would be a very silly way of attacking your server.\nThe next thing that confused me is the web api layer can handle parameter checking. No, it can't. You really shouldn't try and sanitize input yourself, otherwise it ends up going in the database. Let the driver do it, almost as a rule (which is what we do). The driver knows the right escapes, don't do this yourself.\nIn terms of plans for safety checks, I don't build unsafe stuff as a rule - which is not to say that a vulnerability can't be found. Merely that I did my very best to be sure I did the bare minimum and SQL Injection is part of that.\n. The issues list is not the place for questions/examples; better if you send an email or (preferably) read through the test suite/README and see what you can find.\n. Thanks Jon - I'm in Greece currently and have had no internet until now!\n. Good question. Hmmm - this is getting complicated I spose :). For now let's ignore schema...\n. Thanks Matt - I don't know if I follow what the issue is here. Postgres does indeed have a number of ways of storing dates and time (timestamptz is what we used for our columns, we could have used UTC).\nPeople have different ways of writing code and handling dates with Javascript.\nMassive just hands one thing to the other - your code to Postgres. I don't control how you store your date values, nor how you represent that in code.\nWhat, specifically, is your issue with Massive? Yes, dates are difficult to deal with. At the end of the day we're simply handing Postgres a parameterized value to compare against your data. I don't see anything else we need to do here.\n. Yes exactly. The PG driver handles that and Postgres coerces as it needs. For instance you can send in a date in a string, like \"Feb 10, 2011\" ang PG will parse it with GMT, though I would need to confirm this. I'll add a test when I can, but if you could wrap your thoughts in PR I would really appreciate it.\n\nOn Apr 25, 2015, at 00:17, Matt Johnson notifications@github.com wrote:\nI'll see if I can give you a concrete example shortly. But yes - I understand that Massive is just handing things from one to another. In doing so, I assume it's responsible for mapping types from one system to the other - and that's where the trouble usually lies.\nUnless the mapping is already being handled by a dependency?\n\u2014\nReply to this email directly or view it on GitHub.\n. I think there is some confusion about dates here. First, @xivSolutions is correct. Massive doesn't translate anything - it just hands you your data.\n\nWhen you get a Date from Postgres, a new Javascript Date is created by the driver and in Javascript a Date represents a moment in time. So if you don't specify which moment that is (by using a simple Date type in Postgres) then you will end up with Midnight UTC.\nMy suggestion for you would be to do whatever translation you need with Postgres, not with Massive or Javascript. Postgres is pretty good with dates and if you need a certain representation, you can do that with a simple query or date function.\n. Interesting issue - I would suggest there's nothing wrong at all with the way Massive/Postgres is behaving. Your have a naked date in there with no timezone - the system has no idea what this date represents so it defaults to UTC rather than the server timezone. Yes, you can override this interpretation, but that solves the problem with Massive and Node, not with a reporting tool or some other data access tool you decide to use because Massive is buggy :).\nThe problem isn't the driver and it's not Massive - you have incorrect data. Honestly mate if the timezone is important to you (and I think you're finding out that it is) - it should be recorded properly (or adjust your input to UTC).\n. Such civility - how nice :). Disagreements are lovely and always welcome! I just wrote a really long response to this with some nice citations to the docs and I then went back to grab some code from @swissspidy's example when I realized I completely misunderstood the question he was asking. \nDuh.\nThis date (from his question):  2016-03-20 14:10:01.000+00 is a perfectly valid GMT date and his assumption that the return Sun Mar 20 2016 14:10:01 GMT+0100 (CET) was incorrect is understandable. It is incorrect.\nWhen I read the question at first I thought it was \"I'm storing a date without a timezone and I'm getting back a date with a timezone what's happening\" - completely my fault. Sorry.\nYes - this is the pg module overriding the timestamp, which is bad. Sorry for being thick-headed... I agree we should provide the override in #240. \n. BTW @rasantiago it's worth noting that....\n\nDATE simply holds a date with no care for time or timezone. TIME by default holds a time with no concern for date or timezone. TIMESTAMP data type by default only cares about date and time with no concern for timezone. DATE, TIME and TIMESTAMP can optionally support timezone but it is not required.\n\nDates and times are meaningless without some notion of a timezone. Postgres defaults all of these to the local server's timezone (or the TimeZone conf setting). A timezone is required for the storage of the data, but yes you can get away with just using now() and Postgres will figure one out for you.\n. If you're doing any calculations/rollups by date, then timezone is never meaningless. You might think it's meaningless now, but in a few years time I would offer that you'll kick yourself.\nI actually spent some time yesterday thinking about when I would just want a date without a timezone. Even if I just wanted YEAR, MONTH or DAY - I'd still calculate that from a timestamp with a timezone.\nI'll go so far as to say recording a date without a timezone in all cases is invalid. As I write this it's tomorrow in Australia - so on which date did I reply to you? Ask @shiftkey and he'll say \"Tuesday the 26th\" whereas it's Monday morning the 25th to me.\nWhen would you suggest a date is OK without a timezone?\n. @rasantiago OK let's go with that :) - the concern is local. Two years from now the union that does the municipal work files a class action suit against your local utility. They hire lawyers from New York to represent them, and these lawyers subpoena the county for all of the scheduling files so they reconcile against payment.\nThey send the data they receive to their analysis house in Bangalore. It's raw data because no lawyer would ever trust simple reports, and they've directed their analysis team to crunch the data for discrepancies.\nYou and I know the data is local, but do we know when it was recorded. Your phone rings:\n\nSo Roberto, remember that app you wrote for us some years back? Was the server always at the same facility? Did it ever get moved? Can we prove the scheduling dates are accurate?.\n\nYour answer: Nope.\nHere's the problem with your supposition: you're basing it on an assumption and it's likely that you'll remain correct in your assumption, until you're not correct and then you're in trouble.\nI've been burned by this 3 times in my life - the first was when I did this exact thing with lab samples (back when I was a geologist that was also a DBA). The sample results came to us in a CSV and I recorded the data in MS Access. I did this for a number of years. And then our clients got sued for contaminating the water under a neighboring property.\nThe first thing we needed to do was to establish When Where and How with respect to all of the readings we took. We had to gather every bit of information including rainfall records, tidal movements etc to show a reasonable gradient map (and subsequent flow) of contaminant that did not intersect the neighboring property (they also managed to contaminate the ground on on their own, but were trying to offset costs by suing our clients).\nOur data was attacked mercilessly and they tried to show our recordings were inaccurate because the lab we used was local but they sent the samples cross country for verification - which meant the dates could possibly be confused in our readings and were therefore inadmissable because the data would be suspect. Seriously - this happened. We got out of this because we were able to confirm the labs readings with our samples - so it was OK.\nIn my sale to Pluralsight I had to reconcile some discrepancies in the data - specifically in December of 2011 when I released a new video and also dropped the price of annual subs. I was using MySQL and didn't think about using :date in my Rails migration - which didn't timestamp the data.  The sale ran for 10 days, right over the New Year.\nWhen Pluralsight ran the numbers, they were off from mine but $2200 or so (I sold a lot that month). They're in Mountain time, I was in HST (Hawaiian Standard), my servers were in New York. It took me quite a few days to sort that one and a call to my CPA so I could adjust my tax return that year (thankfully I got him just in time).\nThat's not a local concern I suppose - but when I setup Tekpub I didn't think much about using a simple date column; I thought it was obvious.\n\nAll the users of that system are local and always will be. Thus the date and time of camps are not recorded with timezone.\n\nIn this case I think you're reasoning is sound - but I'd like you to consider something as a fellow coder: you've built your solution based on an assumption. I maintain the date is only valid if your assumption holds - that no lawsuits will ever need it, no event in the life of this day camp will ever require that information for anything other than people in the local area.\nYou could be right, or you could be very wrong. You have a lot of experience - what do you tell your developers who don't test their code or take some kind of shortcut? Why would you ever choose to leave a hole in your app?.\nTo me, that's just plain lazy :).\n. Your birthday is an interesting example. Yes, your birthday is a timestamp (look at your birth record) but I will agree that recording someone's birthday is more about their age, not their time of birth - this doesn't refute my point.\nDaylight savings is part of the time zone equation (PST vs PDT, for instance). I had to live with this for far too many years in Hawaii and yeah, business hours (which are intervals) have everything to do with time zones - I don't get your point on this.\nThere's nothing \"timezoneless\" about NBC's schedule and there's nothing varying about it - a broadcast happens at a point in time... again I don't get your point.\n\nEven if I get on a plane and travel to another time zone, it will still go off at that time in whatever local time zone I'm in\n\nRight. That's why you record dates with timezones.\n\nthe decision of what context should be in a date/time value is something the app developer should decide, on a case-by-case basis. The dev stack should enable all possibilities.\n\nNope. Never. Developers are horrible when it comes to understanding (and recording) data properly and the last damn thing I want is a developer pondering how to properly record a date and deciding that my business doesn't need to know the timezone. No thanks.\nYou go ahead and write your apps for your business the way you want - I've already learned the hard way.\n. This memory just hit me as I was on my way to bed. I was in Hawaii a few years back and my friend @samsaffron had a birthday - I knew it because Skype told me. I sent him a note saying \"Happy Birthday\" and he said \"thanks mate - that was yesterday\".\nI woke up in Poland in September and freaked out - my iPad told me it was my anniversary and I had completely forgotten to call my wife! But then... it was the day before back in Seattle... \nGreat examples to dive into. If I was on the product team at Skype I think I'd very much want to know what timezone a person was in so I could relate it to birthday/anniversary notifications.\n. I hate to do this - but after reading this thread I'm sensing that it's a bit more about personal PR/branding than it is about Massive. The thread has gotten way off topic and I should have locked this off a while back as the OP is way too vague and theoretical.\n. I believe it means that users can\u2019t use this functionality unless they\u2019ve upgraded, which, in your case, appears to be the problem. So my choice is to change the codebase, impacting our current users, publish a change and make sure nothing breaks\u2026 or you just upgrade your Node install.\nAs a software developer, which would you choose?\nOn April 28, 2015 at 2:46:44 PM, Paulo Vieira (notifications@github.com) wrote:\nMassive uses path.parse in the walkSqlFiles helper function. However this method is only available in node 0.12 (and not in 0.10).\nThis means that the users can't use the \"SQL Files as Functions\" functionality. If I place a .sql file in the \"db\" directory, massive will crash immediately with:\nTypeError: Object # has no method 'parse'\n\u2014\nReply to this email directly or view it on GitHub.\n. Thank you :) and yes, that would be lovely (a PR)\n\nOn Apr 29, 2015, at 03:42, Paulo Vieira notifications@github.com wrote:\nI note a bit of irony in your reply, which is probably justified given the lack of clarity in my message.\nObviously this is only a \"problem\" for those who don't want (or can't) upgrade from 0.10 to 0.12. But given that 0.12 is only a few weeks old and people have had apps in production using 0.10 for a long time, I would say the percentage of developers who would like to try massive and are using 0.10 is quite large.\nI would suggest at least adding the \"engines\" property to package.json (or simply adding the 0.12 requirement to the readme). If that information is not present, I would assume the module works with 0.10.\nBy the way, the parse-filepath module seems to be a good substitute for path.parse:\nhttps://www.npmjs.com/package/parse-filepath\nIf there's interest, I'm happy to submit a PR with this change, thus making massive compatible with 0.10 (hopefully).\n\u2014\nReply to this email directly or view it on GitHub.\n. Solved with PR\n. Nicely done - good job :).\n. Honestly wasn't being sarcastic; sorry mate. I've been writing a tutorial this past week and I'm fairly certain that tone bled into my response :). Glad you figured it and got it working - if you'd like to PR the README for some credit go for it.\n. First: which version are you using? It looks like 2.0, but I'm not entirely sure.\n\nSecond: how can you tell it's \"failing silently\"? From the code you've offered, you're not examining the err callback (the first argument). I assume you're doing this, but all I have to go on is your example here.\nFinally: It looks like you're trying to run an async routine in a loop, which is synchronous. Would you mind showing me the exact routine that's failing? As you've been able to tell - the insert is working, but not as you're expecting, which leads me to think it's an async issue. \n. The problem is async, I'm 99% sure. You can't run async code in a loop and expect it to work all the time. In some cases (like with a console app that stays open and doesn't exit early) it might look like it works, but that's only because the Node process is staying alive until you Ctrl-c it.\nThe best way to handle this is to build your array of inserts first, then push them in. Preferably in a transaction.\nThink about the nature of callbacks and the Event Loop. You're invoking something that does some IO and is expecting a callback to notify you when it's completed. The for loop executes and each one of those steps gets pushed onto the Event Loop - your callback is the only way for your app to know the call was executed and has completed. If the Node process stops, then yeah everything grinds to a halt.\nIn short: I am 99.999999% sure this is an async problem though I can't prove it, unfortunately. You can fix this by passing a built array.\n. @stephensong Can't say why, unfortunately. I do think it's interesting and yes @xivSolutions if you want to dig in by all means do! In general, though, my thinking on this (for now) is 1) executing a DB in an unbound loop is almost always a bad idea and 2) executing async calls in a loop will almost always cause pain.\nLooking over your code - is prepping an array an option for you? That would solve your problems and also (most likely) make things a bit more performant. In addition, the batch gets executed within a transaction so if there's a failure you don't have partial inserts.\n. Wow thanks for the good work here! I think I now have a firm grasp on this :). The reason you only see 10 records is this:\nhttps://github.com/brianc/node-postgres/blob/master/lib/defaults.js#L27\nYou're using up all the connections and maxing out the pool. Once that happens (I'm guessing) the writes stop because there are no more connections to be had. When you pass the client instance back through done() as we're doing, it's supposed to release it back to the pool (meaning we don't need it anymore). Here's an example:\nhttps://github.com/brianc/node-postgres/wiki/Example\nI think this might be a bug in pg. We could also just use db.end() but that means we won't have pooling. Which I think might be OK because we're working in a single-threaded environment and I can't see any way in which we could have two connections open at once.\nOriginally we had just done() in there but it was slowing our tests down (for some reason). @xivSolutions will know more on this - it was something he spotted.\nYou're the only one seeing this (I think) because executing in a loop as you're doing is not a typical operation... and I think it's an edge case to be honest but then again, that's where the bugs are yes?\n@xivSolutions would you mind switching over to db.end() and rerunning the tests? I'm on horrible internet here again...\nThanks again @stephensong for your investigation!\n. Pushed just now - thanks John I think I spaced on that :). @stephensong if you pull 2.0.4 you should see your error go away :).\n. No - you only connect once to get your DB instance. You'll want to keep that instance alive with your application - each connection to the DB is managed by the pg connection pool so you're safe there.\n. Let me see if I understand the problem(s): the first is that you overrode a generated routine and want Massive to stop you from doing that. The problem is Javascript doesn't have a facility for that - there's no way for Massive to know that you've done such a thing.\nThe second seems to be that you encountered a problem running your query - meaning that your SQL string has two parameters ($1 and $2) but you only passed one parameter, so it didn't work. \nDid I get this right?\nIf so, what would you expect Massive to do in each case?\n. Yay! And you're welcome :). Here's what I do when building up a schema:\nhttps://github.com/robconery/pg-auth\nThe idea is to use tests to drive the build, when you're done you have a set of nice SQL files. When you need to migrate to a new schema my preference is to use a tool like Navicat to create a diff script. Or just build one as you go... but yes generally you have to keep track of this stuff.\nOR you can use a node-based migration framework, of which there are many :).\n. Closing this for now.\n. It's removing the PK so you don't update it (which can cause errors) - the position doesn't matter. What's interesting is that the SQL is using $1 where it *should be using * the actual integer value... unless it's not an integer? What type is it?\n. Hmm - I swear I thought that it was handled. @stephensong would you mind opening another issue for correct handling of UUIDs? Want to be sure you get the credit here - you're helping us quite a lot - thank you!\nI could swear we did that but apparently we didn't. @xivSolutions if you get a chance to take a swing at this yay - I'm dealing with 10 hour jetlag for the next few days so I might be a bit muddled. I'll take a crack at it tonight as well.\nI'm thinking we need to be sure to cover:\n- uuids, generated or not\n- string keys, generated or not (could be defaulted with a func)\nCheers gents :)\n. Oh and @stephensong to answer your question - Massive shouldn't be assuming your key is any data type - however (as you can tell) when we were putting this thing together that was our working set :). \n. I'll assume you're talking about the bcc column and _.flatten?\nI've run into this before with Postgres - it doesn't want a naked array as a jsonb value for some reason (I've had it strip the array even from a single JSON object once, annoying). I've tried to run your query and I get:\ninvalid input syntax for type json\nWhich it clearly is valid JSON, but it's not valid in Postgres for the jsonb type.\nYou have two choices - you can JSON.stringify() the array value and it will work just fine (and be stored as valid JSON) or you can use what Postgres wants you to use, something like:\njs\nbcc : {\n  addresses : [...]\n}\nThe latter solution is a bit clearer to me, but it's up to you. This isn't something Massive can fix (that I can see).\n. Are you referring to the array PG data type? Yes, we use flatten which is an issue and thank you for the PR! I just want to be sure it doesn't Bork other things so tests are appreciated :)\n\nOn Jun 18, 2015, at 12:24, Dian Fay notifications@github.com wrote:\nTable.insert calls db.query after recursively flattening the parameters, so elements in array fields are instead treated as independent parameters. It also only ever returns one row even if multiple rows were created. Pull request incoming.\n\u2014\nReply to this email directly or view it on GitHub.\n. The first thing I'm wondering is why insert or update can't be called directly? I believe they can; if you're generating a key in your code it seems reasonable that you can identify which one to use.\n\nAgree with you @AdrianRossouw on sharding etc. In situations like that I usually opt for a more comprehensive solution that involves functions for inserting data. Not always convenient, but neither is sharding. This, also, is another reason for those SQL files.\nI believe save is the bubble-gum simple method designed for the 80% scenario. I'd prefer not to change it up but I'm open to options that don't add bloat. And right now all I can see coming out of this is a fairly detailed effort at trying to figure out inserts and updates and generated vs. non-generated keys, which will produce bugs :).\n. Closing this for now. I think calling insert or update will work fine. Otherwise use a SQL file. \n. Help me understand this?\n. What is it about promises that gives you a better understanding/working-feel for Node and Javascript in general? Is it the \"feeling\" that things are synchronous? Honestly I'm not trying to be an ass here - I really want to understand what it is about promises that make people feel better.\nBecause at this point, that's what it is: a feeling. I'm all about a groovy API to make people feel happy - but promises don't do that in my opinion.\n. As you can tell, I have a pretty strong opinion on promises - maybe it's because I used previous versions of Node.\n\nWell, it is 2015. \n\nThat's not going to help this discussion. You're talking to a person who spends his days pushing people outside their comfort zones. I'm not scared of promises, I simply don't like them.\nTo be constructive - I'm OK with exporting the class in general. Would appreciate a PR so I can focus on an implementation rather than a philosophical debate.\n. @shawn-simon Yes you should forego any benefit because I dislike promises. That's how you should make a technical decision - entirely based on the project lead's opinions. If you have a differing opinion you're welcome to share it, but don't drop in and troll, that's bullshit.\n. LOL nice swipe couched in a seemingly thoughtful reply. Well done :).\nIs this what you mean by making my code look better?\nhttps://github.com/TryGhost/Ghost/blob/master/core/server/index.js#L125\nPromises are horrible mate, an abstraction for a hobbled language that looks neat at first and then you end up with an awful mess when the shit hits the fan. Javascript is an evented language - get used to it. Promises are syntactic bronzing. They make otherwise reasonable people do dumb things - witness the link above, and your comments.\n. Sort of - I was just thinking about this myself. The simplest thing to do is just use a SQL file (which is encapsulated in a tx) and it also means we don\u2019t need to create abstraction noise around it.\nThoughts?\n\nOn Jul 1, 2015, at 6:29 AM, Dian Fay notifications@github.com wrote:\nI saw an old closed issue where transaction support was mentioned as a future consideration ( #14 https://github.com/robconery/massive-js/issues/14 ). Is that still planned?\n\u2014\nReply to this email directly or view it on GitHub https://github.com/robconery/massive-js/issues/81.\n. I\u2019ve been thinking about it quite a bit. The simplest is a promise chain or to wrap in Async somehow - the hard part is making the query \u201caware\u201d that\u2019s in a tx so it only returns some type of command.\n\nMy gut feeling is that this is more work then we think\u2026\nOne idea would be to wrap up a \u201ccontext\u201d - something that clones the current DB and allows you to build on top of it:\nvar tx = db.createTransaction() //clones the current DB with tables\ntx.myTable.insert({..})\ntx.execute()\nThoughts?\u00a0\nOn July 1, 2015 at 9:12:40 AM, John Atten (notifications@github.com) wrote:\n@dmfay and @robconery - have you messed about with how this might be accomplished yet? I have been travelling extensively this month, and not able to do any actual code work at the moment (grrr). Do we think it can be done without making a mess of the existing code or adding excessive complexity?\nI wonder if an independent module could be used for transactions?\n\u2014\nReply to this email directly or view it on GitHub.\n. I don\u2019t think we need a full cloning operation necessarily - maybe just a table load - which is actually just an array copy since we already have a tables array.\nIt\u2019s not the fastest thing in the world - but the nice thing is that we can handle connections explicitly with the tx object. Given that we have only one thread running per process, we could also just have a transaction object ready to go from spin up, so there\u2019s no hit when wanting to execute one (we could set the props on the prototype and new up a TX object as needed).\nI really want to avoid syntax/abstraction wonkiness and promises. I hate them.\n. > I can tell you from long experience that when working with databases and NodeJS, promises offer absolutely the best syntax for writing transactions\nYes, please tell me about my narrow view of things :p.\nYou want to know why I dislike promises? This:\nhttps://github.com/TryGhost/Ghost/blob/master/core/server/index.js#L142\nI understand other people like them, I understand you like them and have dropped by to pimp your library and diss me at the same time, which is great. It's also pretty bad form.\nI'm happy you like your library; use yours, don't use mine.\n. I don't know if this is an issue for Massive. Perhaps try StackOverflow?\n. Is \"love\" too strong a word to use here? Thanks again :)\n. Hope you don't mind - I just added you to commit so it's easier for you :). Thanks again for all your work - much appreciated!\n. I suppose we could try that - I'm a bit hesitant because failure to connect should be handled by you, not by Massive :)... however I'm all for convenience. \nIf you want to PR this - go for it. Have a look in runner.js - that's where we connect. You can catch the connection error and be sure to console.log that an error connecting occurred, mentioning a single retry (and only a single retry) in 2 seconds or so.\nSound good?\n. Hmmm - I\u2019m not  docker fan and I don\u2019t want people to have to figure it out just to run our tests\u2026 so I\u2019d rather not...\n\nOn Jul 14, 2015, at 10:56 AM, Alexander Zeitler notifications@github.com wrote:\nIs it ok to add Docker for testing purposes (spin up a container, create the rob:password@localhost/massive database, run the tests)\n\u2014\nReply to this email directly or view it on GitHub https://github.com/robconery/massive-js/issues/93#issuecomment-121323108.\n. Hmm - I don't... sorry...\n. Sorry about that - have been traveling (again!). I'm tossing this around in my mind - I strongly dislike libraries throwing and halting the thread - but then again if a connection can't be made, that's a very haltable kind of thing. By bubbling the connection error you're handing the \"can we go on\" decision off... which I guess is precisely what you want...\n\nHmmm  - OK go for it :). Let's see what happens...\n. I've been traveling again; I'll tackle this next week...\n\nOn Aug 30, 2015, at 01:27, Alexander Zeitler notifications@github.com wrote:\n@ldesplat Thanks, using the domains approach catching the error works. \nI have already created a PR for bubbling: #104\n\u2014\nReply to this email directly or view it on GitHub.\n. OK I'm bubbling the connection error so you should be able to handle (hopefully). Published 2.0.8 just now...\n. I've been doing this (loading schema after connect) and I just rerun connect which loads up the changes. Would that help?\n. I'll get a push out next week...\n. I think I agree - result[0] is a pain (I'm living it right now). I like the idea of a doing this for an update (single) as well as an insert (single) - I say... make it so\n. Massive shouldn't return nulls - the first thing I want to be sure of is that you're checking both values of the callback. It should look like:\n\njs\ndb.users.search({columns: [\"first_name\", \"last_name\", \"username\"], term: \"harry\"}, function(err,results){\n  //err should be null, results has the data\n});\nIf no data is returned the query result will be []. If that doesn't work, check to be sure you're getting results from Postgres:\nsql\nselect * from users where\nto_tsvector(concat(first_name, ' ', last_name, ' ', username)) @@ to_tsquery('harry');\nIf that works I'll need to see your exact code.\n. I need to think about it a bit more. Not being able to connect should throw... and I just want a little time to think this through as it doesn't directly address the problem, it's essentially a hack to allow your situation to work (and I'm sure others might have this situation as well).\nI want to address this, but I'm wavering a bit on whether bubbling a connection error is the right way.\n. We merged #257, you can click it right there above your comment. We're now passing defaults and bubbling errors...\n. At this point I'll take_whatever_ and I'm just extremely grateful you're doing it :). Carry on good sir!\n. Yes! \n\nOn Feb 7, 2016, at 12:48, Robin Murphy notifications@github.com wrote:\nI've made an update to the format. I've included an example at the top of each function. I've started to add headings for each method as well. Let me know what you think. If you're happy I'll move the branch out of my fork and into the main repo and we can contribute there.\nmaster...robinjmurphy:api-docs\n\u2014\nReply to this email directly or view it on GitHub.\n. A growing use case for Node is as a utility tool - not just as the main executor for a web site/API. In some cases people use it for simple scripting of \"things\" - like data analysis and loading/unloading of sample/analytical data. This used to be the sole domain of Python - but more people are turning to Node because it's so simple to use.\n\nSo, Massive has sync abilities in there to accommodate. Given that I made this thing for me, mostly, I put it in there :). I hear you about it being \"foreign\" to Node - and I'm a very big advocate against doing things synchronously - but there are times when it just needs to be done (like when your app starts and you want a Massive instance).\nI'm not going to implement changes for a future version of something - if Node bumps and deasync breaks, then we'll deal with it when it happens.\n. Done...\n. I do appreciate the PR, thank you :). My issue is breaking things, and currently things work just fine with our dependency graph. This isn't solving any problem as far as I can see, and your comment is making me more than a little scared.\n. Ugh I'll take care of this in 1.5 hours. Thanks all for the convo.\n\nOn Sep 11, 2015, at 13:31, belfour notifications@github.com wrote:\nVery new to node-js, postgres & github so please direct me to the proper place to ask this question if not here.\nWhen I use NPM to install massive I get the error below. Can I correct that? My assumption is that I need to manually pull and install each dependency from Github. Not a big deal but is there an easier way?\nBuild failed\nnpm ERR! Windows_NT 6.1.7601\nnpm ERR! argv \"C:\\nodejs\\node.exe\" \"C:\\nodejs\\node_modules\\npm\\bin\\npm-cli.js\" \"install\" \"massive\" \"--save\"\nnpm ERR! node v4.0.0\nnpm ERR! npm v2.14.2\nnpm ERR! code ELIFECYCLE\nnpm ERR! deasync@0.0.10 install: node ./build.js\nnpm ERR! Exit status 1\nnpm ERR!\nnpm ERR! Failed at the deasync@0.0.10 install script 'node ./build.js'.\n\u2014\nReply to this email directly or view it on GitHub.\n. I've upgraded to 0.1.1 but I'm not sure this will solve the issue as I'm not sure which is the most current version. Can you please check (I don't have Node v4 installed). I just pushed 2.0.9 so you can pull directly.\n. @keithlayne your email is spamming the list :).\n. I've upgraded to deasync 0.1.1 - please let me know if this works for you.\n. Appreciate that - the use case for the sync stuff is for convenience only; not intended to be used routinely. Appreciate you chiming in...\nOn Sep 12, 2015, at 6:56 AM, Vladimir Kurchatkin notifications@github.com wrote:\nJust to give you some backstory: I've originally came up with an idea of deasync, but quickly realised that it's terrible and broken by design, so I never actually publsihed it on npm. I strongly recommend not to use it for anything. It has weird semantics and seriously messes with event loop. It goes without saying that deasync approach is not supported by node.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/robconery/massive-js/pull/119#issuecomment-139772595.\n. The core of the project is making database work fun; sounds a bit goofy but that is, truly, at the core of it. For you, that might mean the SQL file thing, for others it's the document stuff. All three approaches together (the simple table stuff, the sql file/function bits, the document stuff) really capture the essence of working with Postgres.\n\nThat's part 1 :).\nThe second part is that I built this for a project I was working on, and these are the things I used (and still do) routinely. I think philosophically you have a point and it's something I've thought of doing later on as necessity demands (massive-docs, massive-files, etc). Right now the project is rather small and, in essence, I might even say it's \"finished\" in terms of things going in. \nThis projects progenitor is a .NET library by the same name that was actually quite popular. I wrote it dedicated to SQL Server, but soon people came and offered a Postgres alternative, MySQL, MongoDB, Oracle... and that's when life got harder. At least for me.\nI'm a huge fan of simplicity and direction - least possible action and all that. The project, at least for me, really couldn't be any simpler. Contributions have been pretty strong and have mostly focused on fine-tuning things with the odd improvement here and there. And there's always a chance to contribute.\nSo, in summing: I kind of like where we're at :).\n. I'm a little confused. Are you creating an issue for us to fix or just reporting that Massive was slow for you on start and that you've moved on. If the latter - sorry to hear it but yeah startup time is important.\nIf the former - we have filters in the start routine so you can limit what gets popped onto the db instance.\nFor help with that, I would need to know your setup. It looks like you either...\n- Have a ton of tables\n- Are accessing servers across a network partition somewhere\nIt sounds like there's something else going on. For Massive to \"peg the CPU\" it would mean it's doing some kind of logical processing within the Node process which it is not - it's a query call to Postgres which is non-blocking. If Postgres is pegging the CPU than it sounds to me like there's a config thing happening (Postgres shouldn't peg your CPU if you're simply running a query).\nWe can sort this for you - just need some detail.\n. Interesting - yeah I don't think we've dealt with foreign tables but... not sure if that should matter. So you can see the slow down when you run the info-schema query in PSQL? Or is it only with Massive?\n. We are compliant with Node 5.x stable currently.\n. It's to the point now where I just trust you :p\n. That error is from the database, not Massive. My guess is there's a config issue with UTF8 and Postgres here, somehow. What's the query that you're running?\nMassive's job is to read the file, pull the contents then hand off to Postgres awaiting the return. In this case an error was returned.\n. OK so the issue isn't that the file name starts with a BOM, it's that the file contents start with a BOM... I think I understand now.\nThe fix for this would be to run a replace when the contents are scraped (at the start) - if you want to PR please do :).\n. Lovely - thank you! Just want to be sure all tests are passing? I haven't hooked up Travis yet... which I keep meaning too... \n. Make sure you have mocha installed: npm install mocha -g and then have Postgres installed locally. Create a login that matches our test login (or change it to suit) and then run mocha . - don't forget the dot :).\n. Are you sure that's the exact contents of your file? Is it UTF-8 by chance? We just had a report of a BOM problem with UTF8 encoded files, caused by an IDE.\nThis error is from your database, showing the code above is interesting but what would be more helpful is to see the exact file contents.\n. You can pull from recent - we just fixed this :)\n. What you're asking for won't work with Node, at least as far as I understand what it is you want. There is no \"zero-downtime hot-swap\" as far as I know because of the way Node works (single thread/process) and the way modules work with NPM (static instances). It has to work this way, otherwise you get deadlocks and thread snarls.\nAs far as clustering goes (I assume you mean the Node kind) - Massive will work just fine in that case. PM2 is a great example of multi-process Node production support. But this only works if its governed by PM2.\nSo, to achieve what you want you would need what @dmfay is suggesting: a watcher process that restarts PM2 when SQL files change. Which could work, but this could/should also be governed by a deployment process (with shipit or something) and then... well it's literally just deployment.\nAnyway, if you want to do this you can use something like requre-reload which, frankly, sounds like a disaster waiting to happen.\nNow, to your problem: how do you analyze and change a SQL command on the fly? Use a function or a view and lint/push to Postgres as you need. That's what I would do.\n. Ah yes, cluster. Forgot about that :). No, using it is not something I'm interested in right now. Sounds like a horror show waiting to happen :).\n. I agree with you on distributed programming techniques helping with scaling, but I'd never try to re-implement Erlang in Javascript. That would be utter hell. A simpler answer is just to use messaging - something like Kue works really, really well and you don't write the code yourself.\nI dig that you like what you see and where it's leading you, but I think you might be better served using a platform built for distributed programming. At least try out a queue like Rabbit or something.\n. I'm not too sure what your question is - are you asking  how you catch the error? What to do with it?\n. I believe if you're using the latest you can just eval the error when you call massive.connect. If it can't connect, the db will be null and the error will tell you. What have you tried so far?\n. Can you give me a code example with as much detail as you can...\n. None of these methods should throw - can you send a stack trace?\n. Hmm - that's the driver code; honestly the only thing I can tell you at this point is to use a try/catch. Hard to diagnose the issue.\n. I can almost guarantee this is a permissions problem. Can you try to connect as a super user just to be sure?\n. Give yourself ownership:\nalter database test owner to aderbas;\n. Did this resolve? Also, @aderbas notice the owner in the table there is postgres, not your app user. Also, we use node-postures to hit the DB; it's the query (asking for schema info) that is failing. I was thinking this was permissions but it seems you didn't have keys set - just want to be sure this is solved.\n. I'll throw this back at you: can you explain why a primary key is not necessary? I do know that legacy systems and other completely wrong ideas lead to tables without primary keys... in that situation you're welcome to use straight up SQL to get around the issue. Or, perhaps, use a different DB tool that allows you to shoot yourself in the face :).. I suspect that Unexpected token = is the explanation you need. You just need `exports=.. Ah - cheers thanks. I think this is the key para that I read:\n\nTime-series data is largely immutable. Writes primarily occur as new appends to recent time intervals, not as updates to existing rows. Both read and write workloads have a natural partitioning across both time and space.\n\nI can see why they went with this approach... sort of. No need to find individual rows by a key and since you're not updating/deleting the whole idea of a key is moot. I might suggest, however, just using a set of views or functions? It seems the value here is the SQL extension - so dig in!. Is Massive throwing an error?\n. I'm thinking this is a Postgres error...\n. I'm sorry I just don't understand what it is you want here. If you look through the stack trace Massive is simply trying to connect. The pg library does it's thing and and boom - Postgres doesn't like it.\nI don't think Massive is throwing this error - Postgres is.\n. Good question :) I don't think it's built in yet for Full Text search. I might suggest using a SQL file for this - create the query you need then call it directly; better then muscling though an abstraction.\n. I'll add to what @xivSolutions said and suggest you use timestamptz as a type in your database and let Postgres handle the timezones; Javascript is horrible at it.\n. No worries - happy to reopen if there's cause :).\nYour server writes times for a given timezone, so if you need to translate to your users timezone you need to know what timezone they're in. You can do this from the client and you can send in that timezone, or you can just UTC everything.\nEither way this is not the purview of a data access library. \n. It's been brought up in #37 and I know people like them... I'm not one of them. A lot of code to get around callbacks.\n. Good idea! PRs always welcome :)\n. Thanks!@\n. it's mocha . and make sure you have a db that we can run the tests against called \"massive\":\ncreatedb massive\n. Or just replace that with valid connection info. Also, use mocha without the dot - sorry my bad :).\n. I don't know if you're asking a question or submitting an issue. This is the Issue list - it's OK to ask questions about missing features etc but I don't have an immediate answer for this. Feel free to reopen if you have a problem.\n. As @dmfay said, you don't have to do the if statement here - db.save does this for you:\ndb.parts.save({\n    id: part.id,\n    name: data[i].name,\n    status: data[i].status,\n    fake: 'duper'\n}, function(err, res) {\n    if(err) return console.log(err);\n});\nBecause you added the id field Massive will know to run update.\nI think you're getting a duplicate error because 1) Massive doesn't know what your PK is or 2) You don't have a PK.\n. I didn't even notice the for loop - yeah that can cause all kinds of crazy. If you must do this there's a saveSync method you can use; but it's not meant for everyday kind of use. The async library is pretty good too but they have a big warning about executing code in a loop like that.\n. This is a Postgres error, Massive doesn't throw this. If I had to guess, I would say that Postgres can probably tell those two values apart and your test data isn't being cleared properly.\nTriggers perhaps? Do you have a callback you're using?\n. Would you mind telling me what this function is supposed to do? In general terms? It's sort of hard to understand what's going on and what you're trying to do etc.\n. No worries! It's usually the culprit when head-scratching stuff happens :).\n. The columns need to be named the same as the columns in your table:\njs\ndb.table.search({columns: ['name', 'status'], term; 'foo'}, function(err, data) {\n    console.log(data);\n});\n. What is the err. Also, can you show me the real code; it's impossible for me to guess.\n. I don't think this is an issue with Massive, you're trying to write SQL that's not working. If you want to know the problem make sure you console.log the error as well as the result.\n. We started off with pooling, but when we load tested things we realized the pool wasn't acting in the way we thought it would.  I can't remember completely, but I do recall in testing we would routinely error out - the release connections wouldn't be recycled properly.\nAnyway all those problems went away when we just went with the client settings; TBH I think pooling might be a tad buggy in the driver though the only reason I say that is we had errors when using it and they went away when we stopped.\nThe final bit of advice I got on this was from @datachomp who told me one day:\n\nif there's a chance we'll need pooling I'm not going to trust your shitty little library, I'll use pg_bouncer\n\nSo there's that :).\n. It would be helpful to actually see your code...\n. That came out a little shorter than I intended - sorry in the middle of a few things :).\nSo, if I understand the issue here: you're doing a find without limit, so pulling 200,000 records into memory and you're running out of memory. You didn't say explicitly but I'm guessing Sequelize didn't cause the error?\nI haven't looked at their code but I'll guess it's because they utilize streaming for larger queries - which is a good idea always for anything over a few hundred results. You can have Massive stream things by adding {stream: true} to your query options:\n``` js\ndb.users.find({company_id : 12}, {stream:true}, function(err,stream){\nstream.on('readable', function(){\n    var user = stream.read();\n    //do your thing\n  });\nstream.on('end', function(){\n    //deal with results here\n  });\n});\n```\nThis will keep your connection open longer and could use up the pool (depending on what you're doing). Give this a try and see what happens.\nFinally, if there's a memory issue it's likely at the driver level. We close off every connection after execution:\nhttps://github.com/robconery/massive-js/blob/master/lib/runner.js#L40\nWe used to have pooling enabled here but it seemed... buggy - the release never happened as we expected so we just went back to straight querying and it hasn't really caused issues.\n. Well, hard to say really. Pulling 200K records repeatedly and seeing disastrous results really isn't surprising - as you can imagine Massive can't handle memory for you (short of making sure the connection is killed).\nGive stream a try.\n. There are a number of things that won't work without the way we handle primary keys; it's not something I'm wanting to support with Massive. In addition, there are no tests in any of the commits you've offered here.\nCode aside, as a database aficionado I can see 0 reasons why a composite key is preferable outside of a many to many, and I see absolute NO case where omitting a PK is acceptable.\n. Decided to not do transaction support at this time - the abstraction is just too ridiculous. Best to use a single SQL file.\n. Read our README - you can pop a SQL file into a directory and execute it as a method with Massive. This is done within a transaction.\n. A PR would be great - thanks :). \n. I've done this exact thing (update X where 1=1) to trigger a ... trigger :). Timestamps, search triggers - that said I hate the notion of throwing anywhere for anything.\nI don't think this is an error condition - there's nothing preventing us from running the query. I think I'm OK returning the records directly.\nPerhaps for 3.0 we'll wrap these operations in a message of some kind :). @robinjmurphy thanks again for all your work!\n. Can you try....\ndb.run(\"select * from parts where name like $1\", [\"%rob%\"], function(err, parts) { \n        if(err) {\n            console.log('ERROR: ', err);\n            res.send(err);\n        } else {\n            res.send(parts);\n        }\n    });\nYou have to pop the percent bits into the arg....\n. Thanks :) - I think I understand what the problem is but it always helps to see the code causing an error so I can repro it. I'm having a hard time following the problem.\n. Interesting. So your file is called findAppReleaseEnv and you're invoking it and passing in the array, correct?  Yes - that works and is tested and yes, we're using Node proper.\nWhat is nw.js? Do you have access to Node?\n. Ahh - OK. I'll  leave it open for now - it could be something else that's tripping things up. If you want to pop your code in here I'll see if something catches my eye. Cheers...\n. Ahhh - yeah just went through this with Electron :). Confusing to have Node basically running in both places... and modules available separately. Crazy! Glad you solved :).\n. Yeah I need to move it out, but I can't do that until we rev to 3.0 as it's a breaking change.\n. This will be removed in v3.\n. I was thinking of issuing a NOTIFY when a document is saved - I'll have a peak at this in v3.\n. Some detail here would help.\n. No prob :) - I could swear @dmfay added support for views...\n. Confirmed - quite a few failing tests with this PR. Please revert.\n. Done :)\n\nOn Jan 26, 2016, at 9:25 AM, Dian Fay notifications@github.com wrote:\nLooks like it's polluting the schema. There's another ES6-only bit I inadvertently added a little while back so I'll see if I can fix everything up quickly. @robconery https://github.com/robconery can you take a look at #165 https://github.com/robconery/massive-js/pull/165 when you have a minute? I didn't want to take over management stuff like that...\n\u2014\nReply to this email directly or view it on GitHub https://github.com/robconery/massive-js/pull/182#issuecomment-175128549.\n. Look OK to me - if you're up for it.. have at it!\n. I'm just going to make life a bit easier for you and add you on commit. Can't thank you enough for all this!\n. Wow this is an interesting question! First - the driver supports connection pools but I disconnected it because I couldn't get it to work reliably. If you felt like dabbling... I'd love a PR!\n\nSecond - concurrency in Node is a fascinating thing. I don't believe you can expect concurrency because you're using a pool - you're still relying on the event loop and queries will still happen serialy.\nWhat I might recommend is checking out Kue from @tj - it uses Redis and can do parallel execution.\nAnyway - sorry for the drably reply - I've just been through the gauntlet on this one :)\n. Looks like I'm wrong - it's back in there:\nhttps://github.com/robconery/massive-js/blob/master/lib/runner.js#L40\nSorry about that. As Jon mentions - we had a problem (from yours truly) because I read an old README that ... well let's say it maxed the pool out in our testing and I thought it was broken :).\nThe \"been through the gauntlet\" thing was not about Massive - it's about a pooling discussion I'm having with Moebius (an Elixir port of Massive). Essentially it was about pooling and the need for it, given Erlang.\nIn addition - we had a rather lengthy discussion about \"parallel\" vs. \"async\" vs. \"concurrent\" when it comes to connections and pooling. A fascinating discussion - broke my brain. Pooling and concurrency have nothing to do with each other - it just makes it a bit faster. In fact (as was drilled into my head) - having a pool while trying to do concurrent queries involving transactions spells massive disaster in systems that aren't single-threaded like Node.\nAnyway - hopefully that clears up my thoughts on this :) and I hope I didn't scare off @rdegges :).\n. No, not correct :). We use connection pooling as provided by node-pg. Let's move on to the other things now...\n\nWhile Node is waiting for a response, it will simply continue executing other instructions -- these other instructions may be other queries that are finished and can now be used via callbacks, or they may be simple instructions: add / subtract / create a variable / whatever.\n\nNode is single-threaded, and IO happens using the Event Loop - executing one thing at a time. There is no concurrency here, no parallel execution. For that you need some kind of messaging that operates outside of Node's single thread.\nSo, in short - no. You will not ever have parallel execution. You can't unless you have multiple Node processes.\n\nIf you've already got 20 queries in flight, waiting for Postgres to finish generating a response, and attempt to do ANOTHER query, then Massive will open a single additional NEW connection for that query alone, execute it, then close that additional connection once that query has been completed, while NOT affecting the other 20 connections in the database pool.\n\nNo. For all the reasons stated above. You cannot execute parallel queries in Node unless you have more than one Node process.\n\nThis means I have a total of 10 * 4 * 10 = 400 open DB connections at any given moment.\nAssuming my application load is consistent and queries are identical, I'd have fairly consistent throughput without a lot of network overhead from opening / closing DB connections all the time.\n\nYour math is correct, but I think there's a lot more to consider here. Before I get to that - Massive is a query tool; it uses node-pg (and it's connection pools) to execute queries.\nConnection pools have nothing to do with concurrency. It's all about speed and, unfortunately, is basically (at the end of the day) caching. Caching is hard. I would never trust a Node module to effectively pool 400 connections to my database.\nIf this is truly what you need, have a look at pg_bouncer. It's much better at pooling connections for high horsepower stuff.\nBetter yet, have a look at TJ's Kue project. This will do what you want (spreading the job load concurrently) and yes, it can use Massive in exactly the way you want without all the extra Node clusters. This is by far the better choice.\n. Yeah - no. Node will lob the calls onto the Event Loop - this is a CPU-bound process so Node is free to do that as fast as it likes and it won't wait for a response. It's the queuing that is asynchronous. \nNow, imagine that your first query, for some reason, takes longer than your second. Would you expect that result to come back first? If so - how would you pick that result up?\nThis is called a race condition and Node cleanly (and happily) avoids all the problems of parallel/concurrent programming by being single threaded. This means everything happens in order - first in, first out.\nIt's confusing - Node is indeed asynchronous but that is not the same as parallel. For that to happen, by definition, you need more than one thread/process executing. \nSo no. You can't do parallel processing with Node.\n. Seriously - have a look at TJ's Kue project. It will do what you need with a lot less pain, I promise :). It even does retries and allows you to batch things...\n. Hey no prob - this is something I just went through with Elixir (and what started the confusion above); it's really really confusing stuff (for me, anyway). Let me know if you find out something different of if I didn't grasp what you were trying to say :). This is a great topic.\n. @tdzienniak What am I not right about exactly? Node is single-threaded. Yes, libuv has a thread pool - but that still won't make Node process things in parallel. Given this single thread, how would you tell the EventLoop \"make it parallel\"? Even if you did make it parallel (which I'll bet there's a way to do it with multiple Node processes) - what would that gain you?\nI appreciate where you're going with your response - but the entire communication chain with Node/EventLoop is asynchronous, yes, but not parallel. In any processing loop - if one element is synchronous, it's all synchronous - it has to be.\nThe only way to effectively parallel process a set of queries is to have parallel processes. A message queue is very good for this, and works well. \n. @timruffles In the beginning we had implemented pooling the way the docs said to do it (by calling done(true) but that, for some reason, ended up with us soaking up the connection pool - which seemed odd. We dug in a bit and couldn't figure out the problem, so I figured it was a problem with the way pooling was handled.\nOne of our contributors figured out that the passing in of the boolean was causing the issue - which seemed a weird solution but, indeed, that was it. So we put it back in.\n. Oh - @timruffles that's an excellent point (and example) of queries overlapping in Postgres. I do agree semantics are an issue :). Your example illustrates nicely how operations can overlap - yet each result is queued in the order received, and each result is returned when completed in a serial way.\nThe net result here is, essentially, a set of concurrent queries. I will still maintain that Node doesn't do parallel execution - but I'll capitulate that your example is pretty damn close :). \n. Here's the thing I'd like to avoid: an issue report about Massive being \"slow\" because of 10,000 rows being returned. Sorry I missed the discussion on the issue - but if you want more values back you can 1) stream or 2) explicitly ask for more. The 1000 is in there for people that accidentally ask for all rows.\nCan you elaborate a bit on this?\n. Thanks for the nudge - I thought I brought this in already!\n. Thanks! The search index is GIN as that's what you use for JSONB. We create the index on the fly only in the case of a jsonb document - not for regular tables. The one thing you would want to change, most likely, is the case of reducing the index size by deciding which fields in your document you would want to index. Right now we do all of it using jsonb_path_ops - which says \"traverse my document and index every key\".\nMost likely you won't want that - so you can just create the table yourself and you can still use Massive to query it. We do expect the jsonb to be called body and the search field to be called search however.\nWe use node-pg and we have a myriad of workarounds - Massive is just a query tool that's designed to help, not impose a system. For instance one thing I do regularly is popping SQL in a file so I can execute it as a function (JS function). You can also just run a query using run.\n. > Regarding \"We use node-pg [...]\", I understand this to be \"brianc/node-postgres\" - do we need to \"npm install pg\" ? and will I get the native bindings?\nnpm does this for you - you don't need to worry about the driver nor how to access it.\n\nwhat do you recommend to combine massive with accessing via node-pg directly (how do I :) )\n\nYou don't need to, but if you want to you can require(\"pg\") whenever and wherever you want. But I would advise not doing that. Massive is using the connection pool that pg provides - it's all managed for you. If you want to run SQL directly, we have a method for that: run or, like I said, just pop a SQL file into your /db directory and we'll read it in and execute it.\n\nI guess I thought we only need the body, and then the index would transform the body into the right form\n\nIf you're going to use jsonb you can't set a trigger to pull the tsvector values out. It's kind of complicated - what we do is populate the search field on save (which is separate from the body field, which is jsonb). If you create your own document table, just be sure it looks like ours is I guess what I'm saying :).\n\n. I am wondering if there is another way I should be looking at this - this might just be a roll-your-own situation, but any thoughts welcome :)\n\nIf you need joins then documents are likely not a good way to go. I tend to view a document in the scope of a single transaction - in fact I'm doing through this design exercise right now. Just put all the information together and then learn the JSONB functions so you can effectively query it later on.\n. @pkwnz I think you misunderstand me :). Mongo, Cassandra, Riak - they don't fall down. In fact they power some of the biggest data stores you can find - Cassandra in particular (Facebook). These are highly-tuned K/V stores with purpose-driven schema, relational theory doesn't come into it at all. There is absolutely nothing immature about these systems.\nI'm not knocking joins - one of my favorite document stores (RethinkDB) uses joins to keep the indexes and tables trimmed down. It works and it's lovely. But it distracts from the possibilities of using document storage.\nFor instance: if you use DDD (since you brought it up) - you can easily conceive of a document/entity. Saving that entity transactionally as a document make perfect sense. I tend to go one step more than that, though. \nIf you are into \"Event Sources\" (in the Fowler sense) - you can simply store the results of a given event as a single document. For instance a Checkout - store all the information gathered during the checkout. When Stripe calls back, save that as a PaymentResult or Transaction.\nQuerying documents is incredibly easy - there's absolutely no reason to use relational systems outside of a transactional (OLTP) environment. We don't need to worry about disk space. Data rules can live in your app instead of enforcing integrity, and the transactional guarantees are provided by the documents themselves.\nBefore you think I'm being a NoSQL fanatic - please know I started life as a DBA :). I don't ever take data lightly and I love relational systems as much as I love docs. I've found they both work exceedingly well :).\n. Cheers :)\n. I think the one problem would be an error (runtime) of some kind popping up before commit/rollback. If the runtime exits when the transaction is open, and the connection returned to the pool, the transaction will stay open and the connection will be unusable (transactions are per connection with PG). This would mean we would have a roaming zombie connection which would be rather bad.\nIt's one of the main reasons I've shied away from transactions with Massive - it's so much simpler to just use a SQL file with BEGIN and COMMIT. That said I'm open to making things simpler.\n. I added a comment on the PR but I'll add it here as well. Imagine this scenario:\njs\ndb.tx.begin(function (err) {\n    db.tx.mytable.insert(mydata, function (err) {\n       //User code goes BANG\n        if (err) { db.tx.rollback(); }\n        else { db.tx.commit() };\n    });\n});\nIf the thread exits (which it would) the connection pool would be gone as well and, ideally, the transaction voided in PG (which is per connection). However, to get around this, Joe Coder would put this is a try/catch - you don't want a tx to die on you yes?\nThe problem is that the catch block would need to release/rollback or else we end up with a pending transaction stuck on a connection that's put back into the pool, which would be a roving chaos monkey.\nIt is possible to do this correctly - but at the end of the day it's just simpler to add \"BEGIN\" and \"COMMIT\" to your query chain by hand - or better yet to just use a SQL file.\n. >  But it is unavailable for others, so we'll avoid the chaos monkey :)\n@n321203 No, that's not the point. The transaction is bound to the connection and, given that it's part of a pool, the connection will go back into the pool if Joe Coder calls done() - it's at that point that we have some big trouble. If/when that connection is handed back out you'll get a \"cannot perform query with an open transaction. Please commit or rollback\". This is a gruesome runtime error that yours truly has run into before.\n\nMy use case is that I'd like to be able to quickly update multiple tables atomically without having to write a separate sql file. I don't understand what you mean with the query chain \u2013 surely writing\n\nIt's code in a file no matter where you put it; abstracting something like this is not necessary as SQL has the best facility for \"updating multiple tables atomically\". Just create a SQL file and call it from JS - simple stuff.\n. All modules in Node are singletons - it's probably better if you just require your db module directly.\n. Yes - you have to (for now) update the whole thing, Postgres doesn't allow partial updates.\n. If I can condense this: 1) you want to refactor the file-walker using glob and 2) you want to restructure the way we track the table info. Let's break this out to separate issues if possible.\nIf you want to try and refactor the file walker I suppose go for it, however it works and I see no reason right now to mess with something that works. I do know there's that comment in there but I'm feeling a bit shy about this now.\nRE the meta stuff - I'm also pretty shy about restructuring the project to fit your project's needs. I'm all ears on recommended changes based on individual, specific issues you're having, however.\n. Let's close the glob discussion as supporting dots in directories is not something I feel like thinking about.\nI'm having a hard time dissecting what issue we're focusing on with the file sql. I don't mean to be too direct RE my quip about restructuring to fit your project - but that's exactly the scenario I read after your initial pushback. \nIf I'm reading you correctly: you want to know where the methods came from (SQL File, Table, View, etc) - yes? You need this because you're creating a library that does some stuff and you need to know the provenance (to use a fancy word) of each method. I think this is an interesting problem to solve - but I don't see how it's a structural concern just yet.\nMy first question would be: can you interrogate Views, Tables and Functions to see if the method exists? Would having a is_file function help you out? \n. This was changed (I think) in a PR I just pulled in - try master? If not happy to have a PR for it :).\n. Massive does do this for document stuff, but no we don't do it for regular tables. Make sure you use timestamptz if you need timezone correlations - and yes a trigger is a good idea :).\n. Some more information would be useful, and I'm always open to PRs :)\n. Think I'd rather have a dedicated logging package that you can configure. @dmfay do you have any experience with this (logging)? I liked the way ActiveRecord does it for development - we could implement something like that for execution (outputting params, etc with SQL). Thoughts?\n. I'm going to put this against 3.0; it's a good idea, as is better logging.\n. Interesting problem - invalid JSON is... invalid JavaScript... we don't do any serialization or validation of the object. Seems like deasync is the culprit - we return all errors in the callback. An abort like this is altogether... not good :).\nDo you have to use sync? \n. This should be dealt with in v3. closing for now.\n. The ->> operator turns a value into text - so your ordering won't be what you expect here. Give this a try:\nsql\nSELECT id, body->>'title' AS title, body->>'updatedAt' AS updated_at\nFROM projects\nORDER BY (body->>'updatedAt')::date DESC\nLIMIT 10\nGlad you saw the benefit of using SQL in this case :). Also - if you use this query a lot you can stick it a SQL file and call it directly.\n@dmfay there are a number of reasons for partial documents - the biggest being the use of an \"Event Source\" if you will. I use this concept a lot (storing a TON of information about a given event in one place). But I do agree with you that if you're querying it often, it's probably a good idea to peal it apart.\n. https://twitter.com/robconery/status/725160602419650560\n. This usually happens if you don't send in your parameters in an array - can you add your calling code for this?\n. Ahh - yes you're right sorry :) missed that. I'm wondering - can you do this with just plain SQL and a BEGIN and END? I hate to engineer your solution - but it looks like there's something here we're not accounting for. Which is a bummer - @dmfay do you recognize the issue here?\n. I'm actually open to moving to promises in v3 - just need some help.\n. Check the bluebird branch - we're thick into this currently :).\n. I'm a bit torn. On the one hand I can patch in a number of things using a promise lib. On the other I can wait until ES6 hits with full promise support (and other things) so I just need to refactor/upheave once. I like the idea of doing big changes just once - but I guess I'm open....\n. Yeah I've gone through all that already - it's part of the problem :). ES6 will allow us to streamline a ton of shit - and promises will actually help with testing in a big way (making them easier to write). \nSpeed is a large concern, but it's not my primary metric TBH. If I can slim down the lib and make it easier to maintain I will do it. Moreover if it generates community involvement, that's even better.\nI think we're beyond the promise debate now. I'm happy to give in and move on; it's where things seem to be going.\nI don't want to be harsh about this - but I'd like to keep this thread technical and not flame out into a large opinion piece. I've pushed back on promises many times using the same exact logic... but it just so happens that ES6 and Node vWhatever will really bring some nice changes - the question is \"when do we jump\".\n. Yeah I'm ready - just need the time!\n. The state is that it's pretty popular so far and I just need the time to work on it, which (once I get this book finished) should be in the next month.\n. Not really - the whole idea with saveDoc etc is that you don't think about the abstraction. If you need something else, perhaps roll your own and query as normal?\n. Love it - thank you. Can you do me a favor and add a test for this? I would very much appreciate it :). If you need some help, let me know.\n. They look nice - thanks!\n. Hmmm I see to remember having this problem and it was weird because we're dealing with documents within a table and the options would need to be handled in a special way. For instance, the order statement you have here should work because id is part of the table itself (all the JSONB stuff comes from body), however if you wanted to order on something else you would need to do it thus:\nsql\norder (body ->> 'some_key')::integer desc\nJSONB won't necessarily be ordered the way you like (for example with dates) so casting it is necessary. Anyway - what I'd like to know is: is this failing for you (as in an error) or is it just not working at all? You mention its ignored, but we are sending the options to the query:\nhttps://github.com/robconery/massive-js/blob/master/lib/document_table.js#L127\nSo I'm a bit confused. @dmfay we don't have any tests for options and docs, but it would be nice to build some in if you have any time this weekend. I'm goofing around with v3 and can try as well. \n. Thanks - I'll have a look this weekend!\n. I do believe the options work on this one if you pass {single: true}\n. Hmmm - the idea was to prevent some duplication; I suppose we can have it in both places - looks like the tests still work so I guess it's OK :).\n. Great idea on this! Thanks for rolling it together :). Agree with @dmfay - if we could have options work it would be lovely.\n. I think @dmfay we could accommodate this on startup - maybe tacking on pgDefaults option or something in the connection? Changing the poolsize is particularly interesting although the date issue referenced in #51 will cause more issues but... whatever :).\nThoughts? @xivSolutions?\n. Yep, done - 2.3 is up\n. Makes sense :). If you wanted credit for the PR that would be great!\n. Thanks! I really do appreciate it :).\n. This one's been nagging me for a while - nice work and thanks!\n. Yep that should work. If not I think you can just...\ndb.contacts.where(\"email like $1\", [\"rob\"], (e,r) => {});\n. Woohoo!\n\nOn May 16, 2016, at 05:00, John Atten notifications@github.com wrote:\n\ud83d\udc4d\nOn Mon, May 16, 2016 at 5:42 AM, Dian Fay notifications@github.com wrote:\n\nIt's kind of a glaring omission imo, so I spent some time over the weekend\nseeing what I could do. Postgres has a bunch of neat expanded\npattern-matching operators above and beyond just LIKE and NOT LIKE\n(case-insensitive ILIKE, regexes, weird mishmash of SQL pattern-matching\nand regex) and it's a shame to leave those out of the criteria finders.\nIt's not quite ready yet but I'll probably finish it up and roll it in\nsometime this week.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/robconery/massive-js/issues/251#issuecomment-219406483\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\n. We've talked a lot about this - yeah the only reason to rewrite anything would be to take advantage of ES2015. As @xivSolutions mentioned, we're waiting a bit to see what happens with Node adoption. \n\n\nAlso, we can change the API significantly with some of the new capabilities. I do appreciate your offer, but I want to wait a bit until we know a bit more.\n. Sounds good to me :)\n. Can you show me the JS instead of CoffeeScript?\n. OK, now you show me the code that's failing with the exception etc... (and stack trace)\n. Weird. Looks like the serializer is turning this function into a property. Not sure what to make of it.\nAlso - you're using Massive like an ORM, I might suggest being explicit with what you want to save. We do partial updates, so if you only want to update a few columns, perhaps specify them.\n. Sure - we just need to be sure we omit the PK as an update field.\n. Muchas gracias!\n. My feeling on this has always been to use SQL: it's the best DSL for working with the database. Given that Massive is PG-centric, there's no reason to hide SQL behind a weird abstraction that may or may not capture the nuance you're going for.\nsql\nconst sql = `select field.team_id, team.title\n                     from field inner join team on team.id = field.team_id\n                     where team_id=52`\nreturn db.run(sql);\nAdding joins is Dian's choice, of course. One reason you don't see joins in Massive is because I didn't create it to abstract SQL. I created it (among other reasons) to help when querying things, embracing SQL in PostgreSQL as a first class citizen. . Just now! We're revved to 2.4...\n. Are you talking about the JSONB stuff? A little more context here would help, with any errors you're seeing.\n. Have you tried db[\"myquery\"](function...\"\n. You need to pass your argument as an array, I believe. I'm in the airport now but will look when I get home\n\nOn Jun 13, 2016, at 08:50, Marius Skaar Ludvigsen notifications@github.com wrote:\nHi,\nI have the following sql function:\nCREATE OR REPLACE FUNCTION my_test (int)\nRETURNS int AS $$\nBEGIN\nIF $1 = 1 THEN\n  RETURN 10;\nELSE\n  RETURN 20;\nEND IF;\nEND;\n$$ LANGUAGE plpgsql;\nWhich i expect to be able to run like this:\ndb.my_test(1,(err, res)=>{\n  // Do something\n});\nBut this returns the following error:\nError: bind message supplies 1 parameters, but prepared statement \"\" requires 0\n    at DB.query (/home/marius/vimond/vcc-curation/node_modules/massive/lib/runner.js:22:11)\n    at Executable.invoke (/home/marius/vimond/vcc-curation/node_modules/massive/lib/executable.js:54:13)\n    at rootObject.(anonymous function) as my_test\n    at Object. (/home/marius/vimond/vcc-curation/build/server/app.js:17:17)\n    at Module._compile (module.js:541:32)\n    at Object.Module._extensions..js (module.js:550:10)\n    at Module.load (module.js:458:32)\n    at tryModuleLoad (module.js:417:12)\n    at Function.Module._load (module.js:409:3)\n    at Function.Module.runMain (module.js:575:10)\n    at startup (node.js:160:18)\n    at node.js:449:3\nCurrently I do this as a workaround:\nmassiveInstance.my_test((errr res)=>{\n  massiveInstance.loadFunctions((err, res)=>{\n    db.my_test(1,(err, res)=>{\n       // Success\n    });\n  }); \n});\nI guess I'm doing something wrong or this is not supported. So I would be very happy to get some feedback on what I might be doing wrong.\nMarius\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Two things pop to mind - the first is schemas. Does this function belong to a schema? If so you'll need to reference that.\n\nThe next is that (I think) we don't do scalar functions. I seem to remember taking that out and I don't have the reason off the top of my head. If you returned SETOF(int) or just RECORD that might work. \n. OK - do me a favor and return a SETOF int. Then, at the end there, you should be able to:\nsql\nreturn query\nselect * from result;\n. Wait - I just reread the title of the issue - function in SQL file - explain that a bit?\n. Oh duh. Of course... sorry never mind me...\n. I think the problem is VARIADIC - the driver isn't sending the arguments in a way that the engine is interpreting as a variadic argument. There is a simpler way to get around this, I hate to say, which is just a straight SQL call. \n. It really should support query options :). Thank you for this report ... if you're open to doing a PR that would be ultra helpful, other than that I'll dig in when I can.\n. I'm not familiar with Typescript, but if you wanted to add a companion project that did this kind of thing I'd be happy to link in README.\n. I would guess it's hanging on db.end() because when you call findSync we close the current connection. Thanks for letting me know - not sure what we can do about it. We will be booting the sync stuff in the next rev.\n. Hmmm - OK. It is an interesting thing to try and figure out. The hang could be coming from a callback weirdness or from PG itself. If a transaction is open, for instance, and we try to write outside of it with another transaction, we'll get a block.\nBut these are read only... which tells me that it's a callback notting getting fired. Yuck - the sync stuff is a mess. I'll bet we're trying to reuse a connection (from node-pg) that already has a pending operation.\nDo me a favor? Move db.end() up a line?\n. I have a feeling this might be at the driver level - or at least how we're using the driver. The sync thing is so useful, but it's such a bad idea.\n. You can use db.end() to release the connection, but we use a connection pool, as does Knex (probably) so it's not really being released. \nCan't really help you otherwise.\n. While this will work, I need to throw some caution here. psql is very capable and if you need to execute it from JS I would push you to experiment with System.cmd and pipe a command out to the shell.\n100Mb is quite large and will probably slow your entire app down as Massive will read that file and load it into memory... and that's your app memory... which could cause some problems depending on your environment.\n. We already wrap the file in a transaction so you don't need to add your own BEGIN and END. Also - the driver is not terribly sophisticated and it isn't designed to handle multiple calls to the DB. We can fool it a bit if we structure the query in a simple way (which is what we do on test spin up).\nSo: just roll in the INSERTS inline, separated with a ; and you should be OK.\n. I've gotten around this and I'm trying to remember how we did it - @xivSolutions do you remember how we run the batch insert? Isn't it building a big fat query on the fly? I've had this problem pop up before and somehow gotten around it.\n. Yeah that's the reason - it's a driver issue. One way around this is to use a function - it would be transactional too.. Weird. Would you mind sharing the queries? Are you using caching of any kind?\nWe don't have logging just yet - but are looking at dropping it in (we've had multiple requests).\nAlso - can you show me how you're using Massive?\n. Cheers :) no worries. Checkout using a GIST indexed tsvector field for some mind-blowing speed on text queries :)\n. No, it can't and no, it's not a goal. What you're describing is what an ORM does, and this isn't an ORM.\nThat said - the document stuff (JSONB) is exactly what you need. Or you could write your own SQL file and use a function to serialize results to JSON, giving you back what you want. I think I did something like that and I can try to find it if you're up for some SQL fun.\n. Fair warning: this query is a time-bomb and I'd strongly urge you to consider using the full text search stuff we have for documents. In order to run this query, Postgres needs to run a full table scan to materialize and then compare the fields.\n. Interesting problem here. If you need to run partial matches on your document it might not be the best structure for you using Postgres - depending on how you think your table will grow. The index can't be used in this case (although I'm not too sure about that to be honest - I'm 99% sure but since you're changing it to text I think it won't work).\nIf you have 1000 records, no big deal. 100,000 and the query will slow everything down since it will have to materialize each document and then run the compare.\nI might suggest denormalizing in this case - possibly using a trigger or something - to put whatever you want to partial match against into it's own column. Then you can index that column and not worry about the future.\n. My first instinct would be \"don't\" - but that's just me :). I let my tests hit the DB. The other thing is that JS is pretty wide open RE mocks - it's pretty easy to create your own db.login_sessions.save function that returns what you want.\n. The first try is close! The save function is private so you need to explicitly pop it on Db.login_sessions object (Db.login_sessions.save= function...)\n\nOn Jul 12, 2016, at 13:29, Scott Rippee notifications@github.com wrote:\nI have some tests that hit a DB but for the most part I'm looking to test independently on my CI system. I think my question is due to my lack of javascript experience. How would I declare the mock save? I've tried:\ndb.login_sessions = function() {\n    function save(options, callback) {\n        console.log(\"In login_sessions mock\");\n            return callback(null, {});\n        }\n});\nwhere db is the result of the connection. With this approach it reports an error that login_sessions.save() is no longer a function.\nIf I do:\nmassive.login_sessions = function() {\n    function save(options, callback) {\n        console.log(\"In login_sessions mock\");\n            return callback(null, {});\n        }\n});\nIt doesn't properly overwrite the method and I tried:\ndb/massive.prototype.login_sessions = function() {\n    function save(options, callback) {\n        console.log(\"In login_sessions mock\");\n            return callback(null, {});\n        }\n});\nand for both of those it reports can't set property 'login_sessions' of undefined.\nThanks for any help!!\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. If I understand what you're asking (splitting out fx from Massive) I don't really have an urge to support two libraries if I'm honest.\n. Do you have ownership privileges on the DB? Or the ability to run information_schema queries? If the casing is not the issue, then the startup query is not returning any table information is my guess.\n. Thanks for this - would you mind adding a test or two to be sure it works as intended?\n. There is a lot going on with this and it makes me nervous. I'm wondering if this can be pushed to v3? I need to get that rolling (hopefully in the next month or so) and the main thing I want to avoid is a major change to v2 that will break things that we need to support.\n\nJust to be clear: the driver for this change is multiple connections?\n. Cheers - thanks so much for the lengthy reply :). Let me download and stare at it for a bit. The biggest thing for me is losing touch with the codebase - which I suppose is already happening anyway.\nI can get to this over next weekend.\n. Yeah, this could be clearer :). If the parameter you send in (which in this case is id) corresponds to the primary key and the value is an integer (or a UUID), then result will be a single record. \nOtherwise it's treated as a simple where clause which returns multiple records.\n. I hate to say I have no idea what you're talking about :/ can you show me your failing code and the error you're getting? I'll do my best to help.\n. I do appreciate you troubleshooting this for me, however I'm still very unclear on what the problem is. I can't tell from the code you've provided what is actually triggering this error.\nLet's try this again.\nYou're passing Massive something and I need to know what that is. Specifically: what is the API call you're making. I can tell that tasks is your document table and you're trying to find something, but without knowing the criteria (and options) it's impossible to troubleshoot this.\n. Wild. I'll be ditching the Args library in v3... man I just need to finish this project I'm on and we're off...\n. Hmm - no not at the moment but it would be a damn nice feature to have. You can do this fairly easily (I think) if you sniff out an array in saveDoc. You would have to loop over it with a transaction, adding the documents in a bulk insert. @xivSolutions is pretty good at this stuff... maybe he'll weigh in.\nAny chance you'd want to give it a try?\n. I guess we just spin up some milestones and start lobbing issues in that we can work towards. I can also open a v3 branch that we can work against.\nAnd yeah - if we go the ES6 route entirely, our codebase will decrease \"massively\". LOL.\n. I just need to finish this book (this week, I think) and then I'm all over this.\n. Happy to do promises - I'll have to see if a transaction abstraction makes any sense. I'm closing the for now.\n. I suppose calling db.loadTables would do the trick. Have you tried that?\n. Well here's my conundrum - I think putting scripts/code files in the root of a project isn't generally a good thing to do because it makes things messy in the future. This is your business - but it's kind of one of those things :). The other thing is that you can do exactly the same thing (I think) within your scripts directory - just create sub directories for each table... which I think is a bit weird.\nMy problem is any code I add I need to support and, generally, when it's accommodating a specific project need I tend to reject it. Does that make sense? In other words I'm not sure I see how this will help the project in general.\n. Will happily await your PR :).\n. Oops - didn't mean to close...\n. I'll do a push right after this goes in. I just looked over the code - ideally it should be a quick matter of:\n- updating the script for creating the Doc table to include updated_at. I'd like to keep that as the name just for parity's sake\n- tweaking the update routine to include a timestamp (now()) for updated at. No need for a parameter here - just set updated_at=now()\nThat should be it!\n. This doesn't make sense. Do you mean \"return created_at as a property on the document\"? If so - I'm a bit hesitant to do this because it hijacks the model structure. I'm wondering if we can do something a little more elegant here? Maybe an opt-in somehow? Or checking to see if created_at exists on the document and setting it if so? @dmfay what do you think?\n. I wonder if we can do an option for this in findDoc, something like meta:true or something? Then we put the meta bits on?\n\nOn Sep 6, 2016, at 10:05, jeremiah adams notifications@github.com wrote:\nRoger, we can do that. Performance is a concern in the docs: http://massive-js.readthedocs.io/en/latest/document_queries/#why-finddoc-is-preferred\nThis runs a full table scan (or a \"Sequential Scan\") of the data and is not very performant as it does not use the GIN index we built for. If you use findDoc() however, we'll use the containment operator @>:\nThis query will take full advantage of our index.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Yeah this gets weird fast. I was thinking we could strip it off, just like we do with id... but that can be a slippery slope. Although we do control the project so... I'm leaning on the idea that meta dates are probably OK. \n. I would strongly, urgently suggest you do not do this. It is a nightmare waiting to happen! Casing is a big deal and PG has a very strong opinion about it. As @dmfay suggested: Massive is wrapper and if you need to cast it to a something else then do it explicitly and make sure the code you write is justified in terms of existence. It might not seem like it now, but the code you're presenting here will likely be the #1 thing you support in the life time of the app.\n\nIf this is your preference (the casing): get over it. If this is the preference of your boss or organization, then make sure you have air cover.\n. I'm talking about a generalized conversion routine. Do as you will, of course, but in all the data access tools I've made, the naming of things (as well as type resolution) has always been a pain point. If casing is a big deal to you, I would suggest you deal with it on a \"case by case\" basis. Send the data into an object and have it load/resolve as you intend. Just my opinion...\n. @jnystrom please don't take this the wrong way, but writing code based on annoyances isn't something I want to do :). I hate to put it this way, but maybe just get over it :). Honestly, I don't mean to be shit about it ... it's just an underscore. Code == work I have to do by way of support. The less the better :).\n. @jnystrom Oops - didn't answer your question sorry :). Yes, I just let the underscores in. I quite like it because I know which table I'm using, or which function/view. \n. I might recommend you look into EventEmitter and pub/sub your own events maybe? We used to have events in there but they didn't make much sense.\n. No apologies needed :). You can do LISTEN/NOTIFY with an EventEmitter but... that's a long way round. Are you thinking of using socket.io or something?\nAnother idea: you could use Firebase and keep things ultra simple :).\nAnyway: yeah you might need to use node-pg directly for this.\n. The JS code would be helpful here as well :) - thanks for a nice issue too!\n. Oh - also I should add that your view needs to be readable from the INFORMATION_SCHEMA query we do here: https://github.com/robconery/massive-js/blob/master/lib/scripts/views.sql\nThis means that whatever account you use to instance Massive needs to be able to read from this query.\n. LOL no problem! Glad its solved :).\n. \ud83d\udc4d \n@dmfay what say you?\n. Oh dear - yeah that won't work. First: we're getting rid of deasync in v3, which we're still planning. \nThe problem is that deasync needs native bindings, which means every client that uses your Node app will need to have the ability to compile these native bindings. That won't work.\nIf you need data storage on the client I'd highly recommend NeDB, which is a document store that works like SQLite. If you need to reach out over the network to connect to PostgreSQL, Massive isn't going to work for you.\n. I don't see why not... open to a PR if you're willing. The reason we have the override (non namespace) is a convenience for building the table on the fly. The only reason you would need/do that is if you're saving - it doesn't make sense otherwise.\nAlso - help me understand the this issue some more?\n. There isn't any middle where. You would need to have a custom delete function that updates deleted_at.\n. Hi @vitaly-t - thanks for dropping in. Love your pg-promise project, I think we might use it in vNext of Massive :). Given that, how would you do this with pg-promise? I guess with the SQL example you mention?. No problem at all :) we're all friends here. As long as it's constructive. Really like that example - nicely done. I haven't played with the helpers much at all - will have to take a look.. We're working on vNext right now so... yes I think so. At least I queued it up for @xivSolutions to take a swing at :).. I should probably not get into this, but ... it's worth it...\nIf you're updating in a loop, constantly, forever - that will not \"bottleneck\" your database. Connection pooling (which we use) mitigates the opening/closing cost and the MVCC will prevent locking, assuming you aren't trying to write twice.\nWhat will \"bottleneck\" your database is bad indexing and non-sargeable queries that use full table scans. Writes are cheap, especially single-record writes. If you write in blocks, unbound, you run the possibility of hanging an MVCC transaction over multiple records. That could be bad for your system depending on your need for \"clean\" data.\n. Yeah I read through that and I'm having a difficult time understanding what you mean by \"performance\". If you're talking about pure speed in a single operation, yes, bulk inserts make sense. If you're talking about scaling an application that might do 100,000 writes per minute, no, this doesn't make sense at all.\nNone of this matters anyway: writes aren't a bottleneck, especially with PostgreSQL and the MVCC. Bad reads and non-sargeable queries are.. @Aedaeum by \"write twice\" I mean updating the same record in separate transactions. This will cause a lock and keep the connection open, removing it from the pool, which is bad. It still won't slow your application down.. @vitaly-t  To expand on what I mean RE scaling: small updates via primary key are incredibly cheap. Inserts too as long as the indexing is happening in the background. You can pound on PostgreSQL pretty heavily before anything bad happens, and even then Node, with it's single thread, would have to do a lot to put a dent in it.\nIf you're running multiple instances of Node then yes, you could firehose PostgreSQL pretty good and possibly chew up all 100 connections. But I would put money down that it's not going to be due to individual operations - it would be due to updating 100 rows and causing locks, which soak up a connection. In that case you want quick in, quick out.. Right - the original quote was about the MVCC:\n\nConnection pooling (which we use) mitigates the opening/closing cost and the MVCC will prevent locking, assuming you aren't trying to write twice.\n\nIf you try to update a record already in a transaction, it will be locked. Yes, if you use smaller operations, this is less likely to happen. If you use bulk update operations, you have to ensure at the application level that you won't be trying to write to the same record twice. More updates == more risk.\nJust to be clear: a single DB write operation via primary key is very very fast. It won't bottleneck your app. The thing that will bottleneck it (possibly) is locking connections which will cause a queue to form. This is my point.\n. One thing I always recommend to people who are worried about hurting their database is to actively try and do it. PostgreSQL is amazingly resilient. In fact I'll go so far as to say that I would be surprised if you could even dent it.\nI was able to shut PostgreSQL down once, and that was using Elixir when I spawned 100 processes that tried to execute a select * over 10,000 rows. Even then, the only problem I had was a disappearing connection pool, so I had some shut downs. I was able to get a throughput of ~8500 queries/second, which is pretty damn slow compared to other systems.\nYou should be able to get up to the 10s of 1000s easily. Even then, to truly know what your bottleneck is you would have to run some profiles with pg_stat_io (see Craig Kerstiens stuff) to know what's slowing things down in your DB. 99% of the time it's a matter of an additional index - and 99.99999% of the time the slowdowns involve reads.\nFinally: all of this is kind of moot anyway. If you have millions of writes that need to go in, I'd strongly suggest using a job system like kue. It can batch writes as you need and also execute them concurrently, which is nice.\n. Well I ran a data warehouse for 3 years so I'm quite familiar with importing large chunks of data. I think (as with most discussions surrounding perf) you and I are discussing two different things: speed vs. scaling. They are not the same.\nIf you had to bulk import millions of rows into a production machine I think you might hear me scream all the way across the world. That needs to be batched into smaller writes so you don't drop your entire production system as the write goes in.\nAs I said also: yes, in a single op a bulk COPY is going to outperform looped writes. That's obvious. That also will kill your scaling if you have a high-write environment.\nAll of this is beside the point, which is the database being the \"bottleneck\" on looped writes. It's not.. LOL yeah that was something that was on the radar. This is a driver-level problem, we don't parse arguments in SQL strings - which I'm sure @dmfay will reply. At least I hope so!. From what I understand, PostgreSQL does not consider an array to be valid JSONB. I've run into this before and was able to get around it by essentially wrapping the array in an object: {\"thing\": [...]}. Not ideal, I suppose.. Strike my comment - I was totally wrong about this. In my defense it might have changed since v9.3 but... arrays can be saved just fine sorry!. ",
    "bteller": "I'm apparently making a mess of this thing. Initially it only shows the single line change, but after I commit it removes all 139 lines just to add a single line. This line below needs to be added under line 24. I'll keep trying.\nand tc.constraint_type = 'PRIMARY KEY'    \\\n. Okay, I had to create a new pull request for this, but this time the diff is much cleaner. \n. ",
    "rypan": "Hi Rob & John -  I was tinkering with this library and had a few questions on .run() and .limit(). Does .run() work?\ndb.run(\"SELECT * FROM raw_data LIMIT 10\", function(err,results){\n  console.log(results);\n});\nIt seems the SQL translation works console.log(db.run(\"SELECT * FROM raw_data LIMIT 10\").sql); -  It just doesn't seem to be executing anything within the function.\nAlso, I was tinkering with .limit() - I noticed within query.js that you support limits & offset. However, this doesn't seem to do the trick: .limit(10,100).  I get an error of error: argument of LIMIT must be type bigint, not type record Am I triggering offset wrong?\n. Sweet! That worked. I'm using Postgres. \nBoth .limit() and .run() are working as expected!\n. You may want to update package.json too - I think that'll ensure when npm install is run it will pull the latest.\n. ",
    "xivSolutions": "Hi Rypan - \nI am jumping on this because if anything broke in Rob's code, I am the likely culprit  :-)\nIn my last set of changes I set up the run() method incorrectly. It likely would have worked if you had appended .execute(err, result), but that is not in keeping with the docs, and I don't think it was how Rob originally had it set up. \nI went ahead and fixed that. \nSo far as the .limit() thing goes, how are you calling that?\nI went ahead and modified the code in the example file like so, and it worked properly:\nvar showProducts = function(callback){\n  var query = db.products.find().limit(1, 1);\n  query.on(\"row\", function(p){\n    console.log(p);\n  });\n  query.on(\"end\", callback)\n}\nOn a final note, are you using postgres, or mysql? It appears the postgres implementation has received a little more attention, although I tried to bring them back into sync with my previous changes. It is entirely possible I or a previous committer missed something. \nYou can modify the callbacks.js example file as above, and it should work. Please do let me know if it doesn't. \n. This does not appear to be a Massive-related issue, but more one of using the PG-Node Library. \nThat said, when you make the call to employeeClient.query(queryText); You can (SHOULD!) either:\n1. Add a callback which will include any error result:\njs\nemployeeClient.query(queryText, function(err, res) { \n  if(err) {\n      // do some error handling\n  } else { \n      // do some other stuff...\n  }\n});\nOr, since you are using the evented model, you can try adding something like so:\njs\nquery.on('error', function(error) {\n  //handle the error\n});\nSee documentation for this on brianc's node-postgres wiki:\nhttps://github.com/brianc/node-postgres/wiki/Query\n. Needed to parse the incoming arguments for .first() when passing the call on to .find()\n. I think the quoted column name is a bug. I'm out in the wilderness at the moment, but when I get back, if Rob hasn't weighed in I'll check into it.\nI don't think the MySQL portion if the code has received the same attention as the PG.\nIt may be something I introduced inadvertently when fixing something else.\nI'll be back in civilization later today...\nSent from my iPhone\n\nOn Aug 16, 2014, at 10:13 PM, yiddler notifications@github.com wrote:\nIt appears wrapping a column name in single or double quotes is not allowed for MySql, it treats the column as a string. Instead you can use back ticks (`), I tested the query several different ways in the query window Navicat:\nThese two return results:\nSELECT * FROM Users WHERE id = 1\nSELECT * FROM Users WHERE id = 1\nThese two do not return results:\nSELECT * FROM Users WHERE \"id\" = 1\nSELECT * FROM Users WHERE \"id\" = 1\nmassive/lib/query.js self.where function wraps the column name in double quotes:\n... util.format('\"%s\" %s %s', ...\nself.where = function(conditions) {\nif (_.isUndefined(conditions)) { return self; }\nif(.isNumber(conditions)) {\n  return self._append(\" \\nWHERE \\\"%s\\\" = %d\", self.table.pk, conditions);\n}\nif (.isString(conditions)) {\n  self.params.push(conditions);\n  return self._append(\" \\nWHERE \\\"%s\\\" = %s\", self.table.pk, self.db.placeholder(self.params.length));\n}\nvar conditions = [];\n.each(conditions, function(value, key) {\n  var parts = key.trim().split(/ +/);\n  var property = parts[0];\n  var operation = operationsMap[parts[1]] || '=';\nif (.isBoolean(value) || .isNumber(value)) {\n    return _conditions.push(util.format('\"%s\" %s %d', property, operation, value));\n  }\nif (!_.isArray(value)) {\n    self.params.push(value);\n    return _conditions.push(util.format('\"%s\" %s %s', property, operation, self.db.placeholder(self.params.length)));\n  }\nvar arrayConditions = [];\n  _.each(value, function(v) {\n    self.params.push(v);\n    arrayConditions.push(self.db.placeholder(self.params.length));\n  });\n  _conditions.push(util.format('\"%s\" %s (%s)', property, operation == '!=' || operation == '<>' ? 'NOT IN' : 'IN', arrayConditions.join(', ')));\n});\nreturn self._append(' \\nWHERE ' + _conditions.join(' \\nAND '));\n};\nIs this the expected behavior?\nIf so, how to I get MySql to allow the double quotes?\nThanks!\n\u2014\nReply to this email directly or view it on GitHub.\n. Yeah...\n\nLooks like the where method (used when calling .find() with arguments) wraps the column name in double quotes, probably as a PG-centric thing (it looks like it's been that way for a while). PG uses lower-case database object names by default, and quotes are used to explicitly delimite an object name (if, for example, one were to proper-case a column name, you would need to wrap it in double quotes within the SQL statement. \nLike I said, it appears the MySql stuff has not had the same attention as the PG stuff. I haven't looked at this in a while, so I'll see what I can see about fixing it. \nAs it stands, the where method is defined as part of the Query.js file, which is used by both PG and MySql, so we cant just rip out the delimiters (PG automatically down-cases incoming SQL object names by default). \nI'll mess with it in the next few days. Also interested to hear what Rob has to say. I recall him expressing that this project could use some refactoring/cleaning up. \nThanks for pointing this out. \n. This works...I think. Could stand additional exercising + tests. I'll get to that after work tomorrow. \n. Lol love it. \n. Did you struggle with the choice between \"luke_file_walker\" and \"file_walk_with_me\"? :-)\n. I can after work. Grrr. :-(\n. I figured that would happen :)\nIs it Friday yet?\n. \n. Do you mean stub out sql-ish templates + suggestions on-screen in the REPL, or take some guided inputs and generate scripts/save in a queries folder (or both)?\nGot an example of the REPL input you envision, and idea for output?\n. This is an interesting one. Hard to find the sweet spot between making helpful utility functions that get your SQL made mo 'easier, and making something where you're typing some convoluted bastard-hybrid-mongrel SQL/JS into a terminal instead of just...writing some SQL. \nPushing out helpful example/template SQL to the terminal is easy enough (\"reminders\" - like you said \"how do I create a table again?\") but on the other hand that type of thing is all over the web. And, if someone is savvy enough to be choosing PG as a database, they probably have SOME clue . . .\nDefinitely like the helpers for adding search and JSONB stuff though. I still suck at all that, so that's a good sign we may be onto something. \n. Ahhh, the naming fun begins :-)\n@robconery - This is where, in Biggy, we hit information schema on load and snooped/cached a bunch of mappings for db objects (tables/columns). I am interested to see @Sharkwald 's solution. However, do we want to discuss whether we want massive to try to snoop/cache/solve naming discrepancies, or do we want to encourage folks to use node and  pg-idiomatic naming, and to match object names to db object names in code?\nIn Biggy, I felt it was handy to do the snoop/caching, since C# devs most commonly proper-case object names, and don't tend to use underscores in them (i.e. UserGroups vs. user_groups). In a node/pg environment, do we want to build this sort of snoop/cache thing in out of the box?\nThe fix here is less of a chore than it was in the static type system of C#, but as you know, introduces additional complexity.\nWhat say you, sir?\n. @Sharkwald btw - Welcome! And, thanks for jumping in! Eager to see your PR :-)\n. Don't be nervous, this is how we learn, and it's exactly how I got started here, not that long ago. :-) \nIt's ultimately up to Rob, but there is a case to be made either way. It's a valid discussion to have, and potentially a valid fix. :-)\nShoot your PR, and let's see what Rob thinks. The danger in this is that many times solutions for naming bugs solve the easy to see cases, but introduce more subtle, \"edge case\" naming bugs. On the other hand, a well-crafted solution that tests for all cases solves a common problem when mapping pg objects to code. \n. I think we need to make a decision here. We can decide we support camel cased and other non-pg style db object names (spaces, casing, etc) and delimit everything, or that we don't. As it sits right now, we have some code which delimits the table name, and some which doesn't. Unfortunately, it doesn;t stop with table names. Databases which have cased table names are very likely to have cased column names as well. \nIt is pretty easy to wrap object names in delimiters everywhere (I would just add a delimitedTableName to the table object in massive), but this leads to two issues:\n- If you need to output that SQL anywhere (say, a script) that sh$%T is UGLY.\n- Where do we stop? If we allow cased table names, what about column names? etc. \nFWIW, this came up as I am working on implementing the schema stuff. Thoughts?\n. The issue @Sharkwald was describing is when you have a backend with cased database objects. This does create a problem. \nProper casing on js objects, I agree isn't much of a problem. Looks to me like he pulled down the default chinook pg db, which is all proper-cased in the db itself. The version of Chinook you were using in here previously (and in Biggy) previously I had modified for pg to use compatible (lower-cased/underscored) names. \nMy question was whether we want to support databases with non-standard casing in the back end. \nI'm inclined to say we shouldn't, and encourage folks to adhere to pg standards. \n. The challenge will be with inserts and updates, in cases where column names are proper cased on the back-end. We'll need to delimit them in the insert and update sql. Not a huge problem, but it does add some complexity (not to mention some butt-ugly generated sql ;-)\nIn my recent push, I bolted a delimitedTableName property onto the table prototype, some that things would work with the schema stuff I was working on. then I accessed that instead of putting delimiters in all the sql statements. See HERE. \nIt's a little funky, but makes it easier to manage the quoted SQL code. \n. Pretty sure this is the same thing - escaping the  loadFunctions() method , but if you could make it its own ticket would be great. I can push a fix in the next day or so (I'm travelling atm), or if you want to take a shot at it and shoot a PR is cool also. Basically, need to apply escapes to the loadFunctions() method. \nNice catch. \n. No try catch there when I got to it (unless I'm missing somethinig). I mainly just pulled it straight out, then diddled about with the closure aspects of it...but if that's the case, we should put it back!\n. Oooh. That's weird. And concerning. I see where you mean now, and I know how it got missed. Was working in a stale copy at first, then pasted. Fixing. \n. Was thinking this very thing as I was working on the query file walker...\n. Do we want this to be bi-directional? In other words, do we want to be able to push schemas into the back end in some fashion when creating tables, by simply bolting the namespace on (somehow)? Or do we want to discourage willy-nilly schema creation by forcing folks to do this explicitly (even if it is a discrete/explicit massive api call). \n. That's where I was wondering about for the moment. I kind of like the notion that if you want to add a schema, you need to be explicit instead of just bundling it on with a new table (doc or otherwise). People who know better will probably not do it that way, and people who don't know better will end up with a holy mess on their hands in the back end. \nI'll play with it though. We can make both something like you describe, and something like db.AddSchema(\"schemaName\") and see how you feel about them.\n. Nope. That was my original question, whether we wanted massive to be able to create schema's at all. Understood on the document tables thing. \n. Pushed up the initial bits, which at this point only really load the tables in a schema onto a bolted-on namespace. Going to mess with execution next. This is not ready for merge yet. \nI'm thinking though. It's not critical now probably, but if we're loading schema, we may want to separate the schema load from the table load. Ultimately, it looks like we are expecting to load pg functions (is there a reason to load views, given our approach here??) and other potentially schema-bound items. \nWe can always change things up when we run into this, but right now the schema load is intrinsic to the table load. We could duplicate that for whatever else we want, but...\nOn the other hand, I THINK loading this way ensures only schema to which the user has permissions are loaded. \n. This works with the basic relational stuff. Needs to be played with. I brute-forced it for now, to see if the idea is something you want to pursue. Can be refined, I'm sure.  Gonna test/confirm for docs next.\n. Works with docs for the basic load/save. Need to test doc queries. The code is atrocious in spots, but I got it working. I can pretty it up tomorrow after work, and test the hell out of it all. I'm sure I've missed more than one stupid thing here, but the concept works. \n. Closed for now. Pushed to master. This stuff could use some exercising to make sure we haven;t missed something. \n. I think you cover the essentials pretty well in the read me. However, I agree that some examples and a \"how to use this if you're brand-new\" might be good. Basic and well documented example project?\nI really like your style with the narrative bits like the read me. We should probably stick to that there, but I am happy to take on a more terse and simple set of examples and/or reference if you decide where the best place for it to live should be. \n. If we used MkDocs or something like it, do you wanna host in gh project pages (with the attendant orphan branch in repo), separate repo of it's own (maybe on the massivejs org site you set up? Or a new repo Massive Docs?) or somewhere else?\nI'm happy to jump in and start messing with it, but the hosting decision is really in your court. On quick read, looks like we could use MkDocs, drop it in gh project pages, and easily move elsewhere if need be later?\n. This is ready to merge into master. UPDATE: No it isn't. I boffed this. I thought I had the new tests separated from the schema stuff. Don't merge this. Things will break. \n. @jbogard- you DID see we closed out the SQL Server issue no? Rob thought the SQL Server support should be the priority, you can tell from his closed issue. To get the Oracle support you will need to upgrade to Massive-js Enterprise Edition. ;)\n. That's funny. Yeah, I often find that I'm thinking/working on something,\nand you push up that very thing (was thinking to myself yesterday \"I wonder\nif it might make sense to build out an auth/membership package for this.\"\nGot back to the hotel, and voila - there it is in your twit-feed). I am not\nas proficient with Node/js as I am with .net (yet), so it goes a little\nslower.\nThis is a cool project man.\nOn Sun, Mar 15, 2015 at 1:48 AM, Rob Conery notifications@github.com\nwrote:\n\nLOL I have a spec file here that I didn\u2019t git add for some reason! I\npulled your changes down and you had almost the exact same file :).\nOn March 15, 2015 at 6:20:33 AM, John Atten (notifications@github.com)\nwrote:\nClosed #39 via 1e8628a.\n\u2014\nReply to this email directly or view it on GitHub.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/robconery/massive-js/issues/39#issuecomment-80923990.\n. This SHOULD have produced no functional changes, just pulled out some repetitive stuff. Do please let me know if I missed something stupid. \n. Interesting. Possibly we should be checking to see if these items are already loaded during the call to connect. Seems like since tables, queries (stored in the local directory as .sql files), and functions all represent structural items unlikely to be modified during production use by an application, this should be safe. \n\nThen just make sure that any actions which DO cause structural changes to any of these things (and I don't think there are any at the moment) refresh the Db namespace when complete.\nCould probably use some additional investigation as to the primary cause of the lag though. seems awfully slow for executing a couple information_schema queries, and iterating the result (even with some of the processing we have going on). Maybe the recursive walk through the file structure in LoadQueries()?\nThat's a lot of functions to load, but again, all that's really happening there is reading the function name, they aren't being executed or anything. \nInteresting to see what Rob thinks, but I will do some investigating tomorrow too. Meantime, any additional info you find will be most helpful. \n@robconery  - Time maybe to set up a larger-scale perf test/demo against a production-size dataset, with a meaningful number of tables/functions, and a healthy dose of query files in the project directory?\n. I think the time you are seeing for loadFunctions() is also including the call to LoadQueries(). Are you loading any query (.sql) files from the project directory as well? \n. @robconery - love that idea (filterable on schema). I am also wondering what you think of doing the table/query/function load the first time you fire up the application, and from there checking to see if that stuff is already loaded before calling three backend queries every time a connection is made? \nAs stated previously, this does not seem like the type of stuff that is likely to change often in a running production application. \nI would still want to see some better load times on some of this, and check to see if this is specific to the db/machine/configuration/whatever @vintagexav is working with, or in general is there some perf issue we should address. Gonna try playing with all of that today/tonight. \n. Agreed on the connection usage, and example for the README. \nHowever, I am concerned about the 300-500 ms query times he indicates when running the tables query in Navicat - I can't replicate those results here on my machine with other sample data (using pgAdmin, and granted his database/table/function structure sounds like a unique snowflake). I get a more traditional 20-30 ms execution time. And the super-long 'load_functions' time, 1000-ish rows or no, seems suspicious as well. May need to pull down postgis and mess with it. \nI think we might want to set up a full-size/standard perf/demo db, and configure some samples. Maybe in a separate repo or something. Would be good, though, to be able to have some points of comparison when these things pop up. \n. @vintagexav RE: Conncetion usage aside, I am still concerned with your query execution times in Navicat:\n```\nIn Navicat:\ntables.sql:\nselect tc.table_schema as schema, tc.table_name as name, kc.column_name as pk\nfrom \n    information_schema.table_constraints tc\n    join information_schema.key_column_usage kc \n        on kc.table_name = tc.table_name\nwhere \n    tc.constraint_type = 'PRIMARY KEY'\norder by tc.table_schema,\n         tc.table_name,\n         kc.position_in_unique_constraint;\nOutputs 47 records\n```\nYou say this is taking 250-300 ms? To return 47 rows?\nIf I run this very same query against the chinook database (30 records) I see execution time of ~30 ms using pgadmin. Not sure what could be different about the same query in your db, since it seems this should be unaffected (or marginally affected at best) by having postgis present (just a small handful of additional tables). Maybe a ton of key weird usage stuff to process in the join becuase of postgis? You are reporting an order of magnitude difference here, outside of the Massive context. \nIf you are in fact getting execution times of 250-300 ms for this query in Navicat, then something is different, either in Navicat (which I doubt) or about your db. ??\n. @vintagexav - Can you be bothereed to pull down this (slightly modified) implementation of the Chinook database, and running the tables query against it in Navicat and pgadmin? I am interested in comparing apples to apples here, and see if, in addition to the connection usage thing, there is something else going on?\n. I'm trying to come up with a very generalized example to add to the README. The challenge for me is, there are many different ways to approach the problem, depending on your needs and somewhat, how well you know Node - I'm still working on that second item ;-)\nIn the meantime @vintagexav, not knowing if you are using express, something else, or your own hoand-rolled routing, consider this brute-force/dumb-ish example to get started:\npretend you have a module you wrote that magically processed and routes requests:\n``` javascript\nvar assert = require(\"assert\");\nvar Route = function(req, res, db) { \n  this.db = db;\n  //...Magic happens....\n  // ... and the request is properly routed to:\n  this.get(req, res);\n}\nRoute.prototype.get = function (req, res) { \n  // return all the artist records:\n  this.db.artist.find(function (err, artists) { \n    assert(!err, err);\n    res.writeHead(200, { 'Content-Type': 'application/json', \"Access-Control-Allow-Origin\":\"*\" });\n    res.write(JSON.stringify(artists));\n    res.end();\n  });\n};\nmodule.exports = Route;\n```\nThen, consider an index.js file for your project like this:\n``` javascript\nvar massive = require(\"massive\");\nvar path = require(\"path\");\nvar Route = require(\"./lib/route\");\nvar connectionString = \"postgres://massive:password@localhost/chinook\";\nvar scriptsDir = path.join(__dirname, \"..\", \"db\");\n// stash the db here:\nvar db = {};\nvar configure = function(next) { \n  massive.connect({\n  connectionString : connectionString,\n  scripts : scriptsDir}, function(err, res) { \n    db = res;\n  });\n  // other configuration crap...\n  next();\n};\nvar handleRequests = function() { \n  var http = require('http');\n  var port = 1337;\n  http.createServer(function(req, res) {\n    var route = new Route(req, res, db);\n  }).listen(port);\n};\n// Start things up...\nconfigure(handleRequests);\n```\nIt's crude, and you wouldn't necessarily do things this way in a real application, but the concept is there anyway. @robconery if you see anything too dumb in the above, feel free to tune it up. \nI will say, coming up with an example that is clear is challenging, given the many ways one could approach this. I'm going to add something to the README that hopefully clearly illustrates the idea, and is equally clear that there are many ways to skin the cat. \n. Ah.. Ok. Didn't want to assume you were using Express, and should have caught the network host in your previous comment. \nthat IS some odd perf though...when you consider that with run() all that's really happening is Massive plucks out the args needed, and passes things on to pg query() anyway, then returns the result. All that expensive initialization is already done, so hard to see where the extra 400 + ms is coming from...\n. I wonder if this is related to pg client or connection pooling? Some where I think we're missing something silly here. \n. I wonder if because we are passing the db instance in the call to done() (making it a \"truthy\" value) we are destroying the client with each call (per this section of briancc's wiki entry), and failing to take advantage of the client pooling?\n. I think that might have been it (or, part of the issue, anyway). Remove the db passed to the call to done() in runner.js and all the tests run over 2X faster (when using the List reporter, which gives individual test execution times). @robconery did I miss something here? I did a fix, and pushed before I saw your comment above, but the result was immediately apparent in test execution times. \n. According to brianc's wiki, calling done() returns the client to the pool, ready for another connection, but calling done(db) (or passing any \"truthy\" value) actually causes the client to be removed from the pool and and destroyed. \n@vintagexav - can you dig into node_modules (if you pulled massive down from npm - this fix isn't in there yet), do the same fix, and see if it at least corrects the consistency issues between pg and massive execution times for your project? there will likely still be long-ish load times for your specific case, but interested to see the impact of the change here too. Then we can look at some of the things Rob suggests for filtering and namespaces. \nI like the filtering idea in general anyway...\n. Commit b6ef554 closes #45, not #44. ooops. \n. Closed by b6ef554\n. @robconery - See the branch filters_review. Would like your eyes on this. \nI took this first stab sticking to the \"sql in a file\" model. I am not yet happy with the tests - they all pass, and they show it all appears to be working. However, they are fragile, and I think could be improved. I'll work on that while I'm traveling this week. \nIf you think it's headed in the right direction, I'll delete the remote branch, tidy up where needed, and push to master. \nIn theory, we should have:\nFilter by schema and/or blacklist:\nInclude only tables from specified schemas:\n``` javascript\nmassive.connect({connectionString: connString, schema: 'myschema'}, function (err, res) {}\n// should return db with only tables from myschema loaded\n```\nMultiple schemas:\n``` javascript\nmassive.connect({connectionString: connString, schema: ['myschema', 'secrets']}, function (err, res) {}\n// should return db with only tables from myschema and secrets loaded\n```\nrestrict schemas, but allow explicit exceptions\n``` javascript\nmassive.connect({connectionString: connString, schema: 'myschema', exceptions: 'secrets.__secret_table'}, function (err, res) {}\n// should return db with tables from myschema and secrets loaded, and any tables listed in exceptions (must ref by schema name unless public)\n```\nblacklist using a pattern:\n``` javascript\nmassive.connect({connectionString: connString, schema: 'myschema', blacklist: 'myschema.a%'}, function (err, res) {}\n// should return db with tables from myschema but excludes tables in myschema matching the pattern mychema.a% (must ref by schema name unless public)\n```\nblacklist using a pattern, with exceptions:\n``` javascript\nmassive.connect({connectionString: connString, schema: 'myschema', blacklist: 'myschema.a%', exceptions: myschema.artists}, function (err, res) {}\n// should return db with tables from myschema but excludes myschema.albums, includes myschema.artists\n```\nwhitelist tables:\n``` javascript\nmassive.connect({connectionString: connString, whitelist: ['products, myschema.artists']}, function (err, res) {}\n// should return db with only the products and myschema.artists tables\n```\nWhitelist trumps everything else. \nEtc. If you approve of the general direction, I will make some improvements, and push to master. \nThoughts?\n. Should function  loading be affected by the schema, blacklist/whitelist filters, or should it be independent?\nFor example, if we do this (and apply no function filter as per the issue topic here):\n``` javascript\nmassive.connect({connectionString: connString, schema: 'myschema'}, function (err, res) {}\n// should return db with only tables from myschema loaded\n```\nShould functions not in myschema still be loaded, or should this limit to loading only functions in the specified schema(s)?\n. Appears to work correctly. Included schema as part of filter though, otherwise we are asking for an issue the first time someone has a name collision. Also, to maintain consistency with the way tables are filtered (see the tests). \n. I believe his is a JS problem, and not a Massive-specific issue. I think you need to EITHER perist the data/time with a timezone/offset, OR make sure to save all dates/times as UTC, and then allow JS to apply the local offset. \nWe had a similar issue at work recently, where we were pushing jsut a date into the backing DB ( in this case, SQL Server) wof course was represented as '2015-11-19 00:00.000'. When the data was retreived via AJAX, and then coerced using new Date(data.date) JS was assuming UTC because there was no TZ/Offset. Therefore, subtracting 6 hours from UTC< and giving us the day prior to the actual date. \nSee http://stackoverflow.com/questions/7556591/javascript-date-object-always-one-day-off\nNot sure if there is a one-size-fits-all solution here that should be implemented within Massive itself - I think no matter which way we were to go, certain use cases would run into some version of this issue. \nI could be missing something here, but will dig deeper myself over the weekend....\n. @rasantiago - IMHO you are correct in stating that \"is that Postgres has it correct and the PG module has it wrong (because in some ways JS has it wrong) and this error is simply passed along through Massive.\"\nBeyond this, I disagree. Postgres has it right, in the context of it's own type system. JS most definitely has a funky way of dealing with dates/times/timezones. And, at the end of the day there is a gross impedance mis-match there. The PG driver doesn't \"have it wrong\" - there was clearly a design decision made which recognizes that there is no one good way to reconcile all this between Postgres and JS (well, that one good way is a whole 'nonther lib, but I digress). So the driver gets out of the way, and hands the data to the client to deal with it. \nThis is also the approach Massive takes. \n\"Pushing everything into UTC is one approach to dealing with this issue and it would be nice to set this through Massive.\"\nAgreed, to the extent that we will likely be adding a pgDefaults option to Massive, so that things like this (and connection pooling!) can be set at load. \nNote, however, the JS date/time BS and attendant issues are not unique to the PG driver, or Massive. It is my experience that this type of thing rears its ugly head between JS and just about any other back-end which has more advanced/complex date time handling. I've had to deal with it in .NET/SQL Server, Elixir + Postgres, Node + Postgres, etc. Managing Dates/Times/Timezones is already hard, and JS makes it even harder. \nLook for an optional pgDefaults object at load coming soon (I think...).\nThanks for the feedback, nice discussion all. :-)\n. @mj1856  - You make some interesting points, and they are (mostly) very applicable at an application/domain level. In my opinion, the original issue here, and most of your points, fall outside the scope of the database driver level, or the data access library level. \nA. As Rob states, your \"birthday\" in data terms is indeed a moment in time, correctly represented as a datetime (Jimmy was born on 2/12/2016 at 2:08 AM US Central Time in Chicago for example). When most of us refer to our birthday as a simple date without a time component, this is a date time formatting issue, best left up to the application and/or developer. If someone really wants to store dates without time or timezone components, then datetime is the wrong data type in JS. \nB. All of your points, and also Rob's, would be well-considered at the application level. With respect to Postgres' type system and the different flavors of date/time representations available, there is, ultimately, an impedance mis-match here. JS has very limited date/time support internally, and some annoying quirks with respect to how it handles interpretation of dates and times incoming from the backing store. It's not the driver's job, or Massive's job, to make assumptions about how the user want's to handle these quirks. \nC. Apparently, Postgres makes available a type which represents a date, with a resolution of one day. I get that this is a convenient way to represent things like \"my birthday' or \"my anniversary\", and would be, in fact, what we are often after when we retrieve a date from the database (time zone issues aside, per Rob's comment above). \nHowever:\n1. JS does not have a corresponding type. So, what is the correct way to handle this? Do we assume that such dates should be returned as a JS Date? If so, what time zone, what offset? Just UTC? Do we force it to be 12:00 AM on 12/31/2016, or 11:59:59 PM on 12/31/2016? Or, maybe just a string? \"12/31/2016\" Seems like no matter which way we go, we would be introducing a behavior which would work for some cases out of the box, and be a source of endless annoyance for the rest. SO we just get out of the way. \n2. Given point 1, I submit that in the target language (JS), the notion of a \"date\" is represented in one way. Quirks or not, if you want something to be a \"date\" in JS, then you get to work with the idiosyncracies that brings. In JS, there is no notion of 12/31/2016 without a time component, If this is what you want, then you get to either format the date and return a string, or persist your date as a string, and live with the consequences. Or, deal with the time and timezone issues the target lang brings. Period.\n3. The minute you do new Date(myDate) in JS, JS itself will do it's annoying thing. If there is no timezone, it will \"help\" you figure that out by assuming UTC, and applying the local offset. Without introducing a custom type, or some non-standard JS behavior, this is what you get to deal with, working with JS. \nAs to the original issue at hand here - Massive undertakes to make it easier to query your data, providing some of the common query-type functions out of the box, and allowing you to add specific SQL queries as functions when needed. \nMassive does not try to provide custom JS type implementations, nor any date/time library functions. It returns your data in the appropriate (approximate?) JS format. It is the job of the developer and the client application to deal with how dates/times are presented and manipulated. \nThat Postgres (the coolest relational Db in the world, in my opinion) offers up some types for which there is no direct counterpart in the target language, is simply one of those things to which we, as programmers, get to apply our skillz and problem-solve. \nAt the application level. Not at the Db driver lever, and not at the query tool level. If your Db thinks something is a \"date\" then that is what we're going to return. \nAs stated in another issue, though, we do plan to add a config object which will allow the client to specify parsing JS dates to UTC. But that will be an option, not standard behavior. \n. Is there a sequence created for your user_code column? It looks like you have specified an integer type, and set up a constraint, but I think without explicitly creating a sequence or declaring the user_code column as a serial type (which causes the sequence to be created), your default won't work?\nNot sure if that's related, and I'm on the road at the moment. But if you are not explicitly creating a sequence for the user_code column, I'm wondering if this is a contributing factor?\n. Unless I am totally missing something, the sequence will not be created using the DDL you have provided, unless you are creating it somewhere else using CREATE SEQUENCE. In fact, the DDL you reference fails in pgAdmin3 because the sequence doesn't exist. \nStill seems like Massive should be trying (and, worst-case, failing) to execute INSERT and not UPDATE, tho...\nJust as a sanity check for me, can you drop/create the table, and try using:\n```\nCREATE TABLE users_test\n(\n  -- change this:\n  user_code serial primary key not null,\n  username character varying(15) NOT NULL,\n  firstname character(15) NOT NULL,\n  lastname character(15) NOT NULL,\n  email character varying(100) NOT NULL,\n  password character varying(100) NOT NULL,\n  birthday date NOT NULL,\n  provider character varying(15) NOT NULL,\n  salt character varying(100) NOT NULL,\n  created timestamp without time zone NOT NULL DEFAULT ('now'::text)::timestamp without time zone,\n  -- ditch this:\n  -- CONSTRAINT user_code_pk PRIMARY KEY (user_code),\n  CONSTRAINT email_uix UNIQUE (email),\n  CONSTRAINT username_uix UNIQUE (username),\n  CONSTRAINT email_blank CHECK (email::text <> ''::text),\n  CONSTRAINT firstname_blank CHECK (firstname <> ''::bpchar),\n  CONSTRAINT lastname_blank CHECK (lastname <> ''::bpchar),\n  CONSTRAINT users_password_check CHECK (password::text <> ''::text AND char_length(password::text) > 5),\n  CONSTRAINT users_provider_check CHECK (provider::text <> ''::text)\n)\nWITH (\n  OIDS=FALSE\n);\nALTER TABLE users_test\n  OWNER TO postgres;\n```\nAnd then try running your Massive.js code?\nIn the tests, we use save with and without the PK, and it works as expected. I'm not saying you're wrong here (at all!) just trying to trouble shoot. I'll try to fire up something similar here when I get back to my hotel, and see what there is to see. \n. @cwgabel - Think I see the issue. \nIf you pulled Massive down from NPM, that version is missing some recent fixes. Particular to your issue, PR #42 Correctly get Primary Key when loading tables fixed a situation where more than one row was being returned as a table primary key. \nThis was further addressed recently by Issue #53 Correct Join on Tables.sql Query to Return Single Row per Table-tests\u2026 and fixed with THIS\nThe issue was simply there has not been a bump in the NPM version since a even earlier fix was added for this very issue. Your DDL defines three columns with UNIQUE Constraints (the PK, email, and user name). The unrepaired tables.sql query (which joined tokey_column_usage only on table_name) was returning all three columns as rows, and Massive was assigning the last of the three (email) as the pk for your table, instead of user_code. So, since Massive checks to see if the table pk is provided and has a value, it found both and assumed you are trying to UPDATE. \nUntil we push an update to NPM, if you go into the node-modules/massive/lib/scripts directory, and change the tables.sql to THIS:\nselect tc.table_schema as schema, tc.table_name as name, kc.column_name as pk\nfrom \n    information_schema.table_constraints tc\n    join information_schema.key_column_usage kc \n        on kc.table_name = tc.table_name and\n           kc.constraint_schema = tc.table_schema and\n           kc.constraint_name = tc.constraint_name \nwhere \n    tc.constraint_type = 'PRIMARY KEY'\norder by tc.table_schema,\n         tc.table_name,\n         kc.position_in_unique_constraint;\nSHOULD fix the problem. \nI am curious about the UNIQUE constraints doubling up in the PK, though. Gotta go check the Postgres docs on this. \nHey @robconery This issue actaully probably warrants a bump in the NPM version, no?\n. I can't repro that behavior so far. Running that within .forEach() is an odd choice, given that you can pass the array directly to the insert method (or even better, the .save() method...). However, whether or not it is a \"good\" thing to do or not, when I run the following against the test database I get all 225 records inserted (although the order in which the inserts are written to the console is . . . odd):\n``` javascript\nfunction getTestData() { \n  var testArtists = [];\n  for(var i = 0; i< 225; i++) { \n    testArtists.push({name: \"Artist \" + i});\n  }\n  return testArtists;\n};\nMassive.connect({connectionString: constr}, function ( err, res) { \n  var newArtists = getTestData();\nnewArtists.forEach(function (r) { \n    res.myschema.artists.insert(r, function (err, saved) { \n      console.log(saved);\n      // outputs each new record as it is created, with newly inserted serial id\n    });\n  });\n});\n```\nMaybe I'm missing something here?\n. The massive.loadSync() is an experimental thing (although I see no reason it shouldn't work, given what you've supplied here). On the other hand, bear in mind all it does is load the db synchronously - it does not allow you to use Massive itself synchronously beyond that (and I should document it as such!). \njust for kicks, can you try (if you haven't already) ditching the call to .loadsync() and do something like this:\n``` javascript\nMassive.connect({connectionString: constr}, function ( err, res) { \n// Is there a callback that can happen here when your load is complete? This is called synchronously, but if it is an async process, might cause issues:\n  let solns = // an array of, in my case, 225 rows loaded from elsewhere\nsolns.forEach(function (r) {\n    db.solutions.insert(r, function (err, saved) { \n      console.log(saved);\n      // outputs each new record as it is created, with newly inserted serial id\n    });\n  });\n});\n```\nFrom here it still looks like there is an odd mix of sync and async going on here. depending on how you are loading the records into solns \"from elsewhere\", I'm wondering if you aren't cooking right through that, and beginning loop iteration before the load is complete. On the other hand, it does seem your call to insert_solution() is actually executing 225 times. \nNo matter how you slice it though, calling insert() within the forEach loop is an odd, less-efficient approach, and also requires fetching a new connection from the pool for each insert. Is there a reason you are taking this approach?\n. @stephensong - I can't speak for Rob, but I intend to try this on my Windows machine in the near future (I was working on the MBP when I tested). While I agree with Rob in that it seems like a funny mixture of sync/async (and an edge case), I very much want to see if it can be repro'd. \nI caught that in your comments, but wanted to make sure you were running the precise same code I was, and not using the deasync stuff. \nI think closing the issue for now is probably appropriate, but that doesn't mean investigation stops (for me, anyway). I think there is value to be had in understanding what's going on, even if that is to determine for certain that it is, in fact, a sync issue. \n. Ahhh...\nThe call to done(db) has, in fact, been fixed in master (see #43 )but not pushed to NPM. When you pass a \"truthy\" value to done() it destroys the client and removes it from the pool, causing both performance issues, and the issue you are seeing here. I assume your code is depleting the available PG clients.\nThis is probably why I couldn't repro the issue - I'm using the Github code, not the NPM version. \nFrom briancs' wiki: https://github.com/brianc/node-postgres/wiki/pg#parameters\nFrom the previous issue:\nAccording to brianc's wiki, calling done() returns the client to the pool, ready for another connection, but calling done(db) (or passing any \"truthy\" value) actually causes the client to be removed from the pool and and destroyed. \n@robconery - This has been fixed but not pushed to NPM. However, Interesting idea to change it to end(). I'll try that, and see what happens. Meantime, though, we have a number of issues fixed that probably warrant a new NPM version as soon as possible, when you're able. \nSpecifically #43 (directly related to this one (the change from done(db) to done())  and #55 (tables.sql returning multiple PK columns)  are fairly important. \nI'll do some messing about with end() instead of done() and update this thread this evvening. I'm at a work conference in lovely Bend, OR right now ... \n. @mzedeler  - It's hard to know exactly what's happening beyond the obvious here. The obvious is that your override of the generated insert() method attempts to perform an insert with a mismatch between expected parameter values, and actual parameter values. \nOne of the handy things about JavaScript is that is allows you to bolt on new methods/properties (\"overrides\") etc.. to your heart's content, and will try to do what you tell it to. \nOne of the challenges of Javascript is that it will allow you to bolt on new methods/properties (\"overrides\") etc. and will try to do what you tell it to without complaining.\nIn this case, it is doing almost exactly what you are asking - you added an override (intentionally or no) for an existing method which appears to build either a flawed SQL statement, or incorrectly matches parameter values to parameters specified. Massive attempts to execute, and Postgres itself complains to the PG library, which passes the error on up to Massive.\nIn any other language, (say, a strongly typed language such as C#) it is possible to create an override method which compiles, and executes without complaint from the compiler or runtime, and which still fails when it attempts to execute improperly constructed SQL against PG or any other database. \nIt doesn't matter if the method is an override or not, it still needs to pass to the backing store a SQL statement with a matching set of params and values. If the parameter values don't match the parameters specified, you will get a similar error. \nUnless I am missing something, the problem is not that Massive allowed an \"override\" but that there is a bug in the code in the \"override\" method itself? There is no way Massive, or any other library, can protect you from attempting to execute SQL with a mis-match between parameter placeholders and parameter values beyond doing precisely what happened here. \nIn other words, are you asking massive to warn you if a method in your own code attempts to override an existing method? Or are you asking massive to check what your override method does to make sure it is going to work correctly? The former is probably doable (though not practical, nor really necessary), the latter is not. If you choose to override a method, you do then need to make the code work properly.\nHave we missed something?\n. This is fixable. I can push a fix in the next day or two. Traveling atm.\n. Do the tests pass when you run them with this in place?\n. Uh...zero passing?\n. If you have mocha installed you should be able to just do 'mocha' from the project root\n. Is there a database named \"massive\" defined in your Postgres cluster?\n. This fix works for me, and all tests still pass. I'm going to merge this, but then add some tests for this case. \n. @paulxtiseo - not certain what's going on with your failing tests for findDoc and searchDoc. I merged your changes, all tests pass, and I added a couple tests to test the escape stuff you put in. All tests pass. :-)\n. Yeah, the idea was to enable an application to specify one or more configure files by passing in a path/paths as described in the original comment. Meaning, a sensible default, easily overridden by the application if needed.\nPoint well taken, though. We appreciate all of this type of discussion :)\n. Not sure, but you might need to namespace your functions. While Postgres may support function overloading, I'm pretty sure JS will simply \"hoist\" the last one it finds.\nThere may be a solution of some sort for this in the way we load functions during massive.connect(). I'll investigate.\nMeantime, trying to load two functions of the same name into the massive instance is simply replacing the first with the second.\n. @warmuuh - It's an interesting problem. Solving it may require more complexity than we want to introduce, but it is worth exploring. \nFYI, your hack didn't come through in your comment. \n. When you remove the line you indicated, I believe Massive IS updating the PK - just with the same value. \n@robconery  - Do we want to handle cases where the PK is other than a sequence? It's not TOOO difficult, but it does add some complexity. \n. @robconery On this. I've got a pretty good start here, but now it's past my bedtime on a work night. Should have some cool stuff to push tomorrow night. \n. The idea here is that you only ever call massive.connect() once. There is a load penaly associated with the load process, so once you have a massive instance, you're done with connect(), and you want to avoid calling it again. You are correct, the docs really only show a few snippets at this point. \nI am traveling at the moment, and can't actually test the following, so beware I may sprinkle in a few mistakes. The concepts are basically correct though...\nOnce you use app.set('db', massiveInstance) the massive instance is then available as a property on the app object:\n``` javascript\nvar configure = function(done) { \n  var connectionString = \"postgres://massive:password@localhost/chinook\";\n  var scriptsDir = path.join(__dirname, \".\", \"db\");\n// connect to Massive and get the db instance:\n  massive.connect({\n  connectionString : connectionString}, function(err, massiveInstance) { \n// Set a reference to the massive instance on Express' app:\napp.set('db', massiveInstance);\n\n// Let the called know the db is ready...\ndone();\n\n});\n};\nconfigure(function() { \n  // Run it...\n  http.createServer(app).listen(port);\n});\n```\nFrom there (for example) you could define another module (say, named 'routes.js') like so:\n``` javascript\nvar Routes = function(app) { \napp.get('/artists', function (req, res) {\n    var db = app.get('db');\n    db.artist.find(function (err, artists) { \n      res.send(artists);\n    });\n  });\napp.get('/artists/:id', function (req, res) { \n    var db = app.get('db');\n    var id = Number(req.params.id);\n    db.artist.find(id, function (err, artist) { \n      res.send(artist);\n    });\n  });\n};\nmodule.exports = Routes;\n```\nThere are a number of ways to work this into an Express application (and note, I am not an expert on Express at this point). But the over-arching idea is to call connect() once at application load, cache the instance, and use that cached instance to work with your database. \n@robconery may have something to add, and/or set me straight in the above, as I think he has worked with Express more extensively than I have. \n. @tigernassau - agreed on the examples - problem is I am on the road at a tradeshow, and could only do a quick/dirty attempt for the moment. I had to swipe some quick snippets from a file I had available. I wanted to do my best to illustrate the idea with code I know I had running at some point :-)\nWe will likely update the docs (I will as soon as I return from this trip), and/or do up a blog post, when I can make a better, more consistent example. Rob may also have something to add, but I think he, too, is travelling at the moment. \n. This works nicely so far! Merged. \n. This looks good, nicely done!\n. @dmfay Totally agree on. Form my perspective there are few cases to use anything other than a generated key, but it does happen. \nThe boolean flag thing would work, but it's pretty darn kludgy. \nI think for now, maybe just state explicitly in the docs that tables without generated keys require explicit use of insert or update and then throw when someone tries to use save against such a table (or, alternately, assume save in this case means to update and let things fail if no matching key exists. Also kuldgy). \nSeems there are a few ways to maybe achieve this using CTE's, or trial inserts + return codes, but all seem awfully messy for what should be an edge case. \n. Agreed. We need to be able to handle non-generated PKs. \n. Ahhh.. The compound key issue. I knew this would come up eventually.\nThis is solveable, but it makes things complicated. I'm wondering if this is a case where we leave it to the user to implement the necessary logic as an SQL query or function for this edge-case, or try to put something in Massive itself to handle compound keys. \nIf I can ever get caught up from moving, I'd love to look into it either way. \nThoughts?\n. Yeah, I had been messing about with something like that (also detected auto-generated keys and type) before The Big Move. May break it all back out, see what I can get from it. Complexity came from changing things like getPkName() and code which depends on a PK ref  to return an array (and callers to expect an array).\nNot horrible, but definitely reduced the simplicity. \n. Seems like it would be handy, if the total transaction was a series of simple inserts/updates etc, to be able to go .begin/.end somehow. Anything beyond that though would make more sense as a SQL file (which was my initial reaction as well). \nI think only if it could be done without adding too much noise/complexity. \n. @dmfay and @robconery  - have you messed about with how this might be accomplished yet? I have been travelling extensively this month, and not able to do any actual code work at the moment (grrr). Do we think it can be done without making a mess of the existing code or adding excessive complexity? \nI wonder if an independent module could be used for transactions?\n. With the run function, looks like you are passing an int as an argument when no corresponding param is indicated in the SQL.\nThe function is not finding your call back.\n. Nice! :-)\n. Ahh... You can do this, just haven't updated the README yet. My bad. \nYou can actually filter functions, schemas, and tables (blacklist, whitelist, etc) during the call to connect. For some trivial examples, see the tests in the file load_filter_spec.js. \nYou can exclude all functions, much as you suggest, like so (from the tests):\njavascript\n+  it('excludes functions at load whenever it is told...', function (done) { \n+    massive.connect({connectionString: constr, excludeFunctions: true}, function (err, db) { \n+      assert(db.functions.length == 0);\n+      done();\n+    });\n+  });\nOr you can us a function blacklist filter (in much the same way you are above) by passing either a comma-delimited string, or an array of strings, and you can use the same \"wildcard\" syntax as in your code above (here, again, some simple examples from the tests, but you get the idea):\njavascript\n+  it('excludes functions with name and schema matching multiiple blacklist patterns as a comma-delimited string argument', function (done) { \n+    massive.connect({connectionString: constr, functionBlacklist: \"myschema.artist_%, all_%\"}, function (err, db) { \n+      assert(!db.myschema.artist_by_name && !db.all_products && db.myschema.all_albums && db.AllMyProducts);\n+      done();\n+    });\n+  });\n+  it('excludes functions with name and schema matching multiiple blacklist patterns as a string array argument', function (done) { \n+    massive.connect({connectionString: constr, functionBlacklist: [\"myschema.artist_%\", \"all_%\"]}, function (err, db) { \n+      assert(!db.myschema.artist_by_name && !db.all_products && db.myschema.all_albums && db.AllMyProducts);\n+      done();\n+    });\n+  });\nLooks like you could take all of the pattern matches from your SQL above, and set them up in an array to pass to the connect() method similar to the test examples. \nI think you may be right though, I may \"steal\" your pattern matches, and set up another simple switch which enables opt-in for including this specific set of functions for PostGis. \nI'll also update the README about the various filtering options available for connect().\nIn the meantime, take a look at the test file and see how it works for you. Please let us know if you run into problems. The filtering use-cases have been tested in basic form, but have not been exercises extensively in a \"real-world\" setting. \n. I was leaving it open as a reminder. Glad it worked for you!\n. Yeah, Postgres down-cases your incoming SQL. If you REALLY wanted to, you can modify your SQL like so to maintain casing:\nCREATE TABLE CUSTOMER(\"CompanyName1\" VARCHAR(20));\nI would avoid this, though. It makes life harder than it needs to be. Strongly recommend sticking to Postgres conventions unless there is a compelling reason not to. \nMassive accommodates different casing scenarios for the case where you have inherited a database with non-conventional casing, but I recommend staying away from it if you are starting from scratch. \n. Have you run npm install recently? Some of the recent changes you may have pulled down involved new libraries which may cause code to break if they are not in place. \nAlso note - in the test/db/schema.sql file is where you will find the \"setup\" script, such as it is. the test/helpers/index.js file performs the basic setup in advance of each test module, including executing the SQL in the schema.sql script.\nAlso, review the actual error report, and see what may be going on. For example, the first case, it appears you have more functions loading than there are in the existing code. The assert shows you that there are 30 functions found, against a test condition of 20 (see the loader_spec.js test file - the test expects 20 functions, and according to your mocha output, 30 are being reported). \nIn the current incarnation of massive, as pulled down directly from the master repo, all tests should be passing. \n. in a sample app I was putting together, I made just added an configure.js file which I used to do this type of thing, left it at the project root next to index.js.\n. Massive makes use of the client pool afforded by brianc's node-postgres library. \nThe client pool handles the disconnect for you, and manages available pg client pooling. \n. @jfbaquerocelis - The short version is, you don't need to worry about closing the connection in your code. The underlying pg lib worries about it for you.\n. My only comment would be that if a method can possibly return more than one result, it should do so consistently. I recognize that the current API is a little inconsistent this way already. I think Rob may feel differently, but my two cents is that if there is a possibility of multiple results, the calling code should be able to assume the return type will be an array (and I actually think the rest of the API might benefit from adherence to this). \nThat's just me though. I get the convenience of not having to deal with result[0] but it does leave a hole for unexpected results. \nAlternatively, if we decide that an action involving a single, PK-referenced record should return an object, we should make sure that is the case across the API (I think we are inconsistent about this at the moment, but I've been away from the code for a while, dealing with a job transition).\nRob? What say you?\n. Done! :-)\n. There was an issue with the way the ts_vector terms were being concatenated. Should be fixed, added a couple tests. Now returns empty array when search term not found, and works correctly.\n. Eek. I'll investigate more... re-opening. \n. Closing again. See #154 and associated commit....\n. It's not 100% clear what you're trying to do here. Are you trying to write 'done' to the console if an error occurs? If so, you need to wrap the console.log('done') in curly braces as part of the if() block. \nOtherwise, if you are trying to write 'done' to the console as part of the 'happy path', and then exit, you will also need to exit the process as part of your happy path (generally using the \"success\" code of 0):\n``` javascript\nif(err) { \n  process.exit(1);\n}\n// do some stuff with your data, call other functions - the work your application does...\nconsole.log('done');\nprocess.exit(0);\n```\nNote that generally, you are going to load up a reference to db and cache it somehow for use during the entire application lifecycle (also see the express example in the README as one example, if you are using express):\n``` javascript\nvar massive = require('massive');\nvar myDbInstance;\nmassive.connect({ connectionString: 'postgresql://pean:pean@localhost/pean' }, function (err, db) {\n  if (err) {\n    process.exit(1);\n  }\n  myDbInstance = db;\n// do moar work here with the db...\nconsole.log('done');\n});\n```\n. Node, and (to a certain extent, by association) Massive, are primarily expected to be used in a web server context. Meaning, more as a continually running service, rather than a process which runs, does some work, and exits. Therefore, the general use case will see Massive spun up, and the instance \"kept alive\" to service incoming requests. \nIt's not the only use case for either Node, or Massive, but it is the common one. Therefore, in your example, Node is waiting for the next call to be made, and eventually times out (this is a non-technical explanation, possibly short on detail, but correct in spirit). \n. Love it...I'll make some time this week. Been getting up to speed on the new job, so time has been tight. \n. Just fork the project - the docs all live in the docs directory\n. I like it! :-) I think API docs like the above are especially helpful with JS. \nI think a more notes about the (possible) return type for a given function is in order here. For example, the find method, and many others, will return either an array of row objects, or a single object, depending. This is handy, but can be confusing if it's not clear. \nOn the one hand I like the idea of generated markdown, as it is easy to maintain. On the other I don't, as it clutters up the code files. \nI say proceed, and shoot a PR. If it were me, I would probably skip the JSDoc, as to do it right REALLY clutters/dirties up the code. @robconery may feel differently. \n. @robinjmurphy - I was thinking in the section where you describe the function, right under the syntax example. Instead of \"Returns rows that match the given constraints\" maybe just expan on that, with a super-simple example of usage and outputs (or maybe a link to the narrative part of the docs where examples live). \nOnce you get things rolling format-wise, I  can try to help some too. I've been swamped recently, but been meaning to get back to some of this myself. \n. deasync provided the needed sync capabilities better for our purposes than other libraries. I can;t speak for Rob, but Massive is, in fact, a Node-based library. Without stepping into the \"schism\" around all that, I think you're welcome to see if you can find something which works as well. If you do, let us know (or shoot us a PR!). \nAll that said, deasync is in a rather \"bleeding edge\" state as I recall, and may evolve, or be abaondoned (or be made compatible with io.js). \nI think we are open to feedback, but when I was digging around trying to find a clean way to wrap some things up, deasync was by far the easiest to use. \n. I originally wanted a Sync \"load\" method, so one would have the option of letting a host application bootstrap without diving immediately into nested callbacks. When Massive spins up, it loads a bunch of stuff and caches it. It seemed like there were cases where this might be attractive. \nRob added a bunch of additional sync methods, I am guessing for use with his Meteor.JS stuff (but I could be wrong). It is a node-based application, but there are cases where some sync love is not a bad thing. So long as you are intentional about it. \nWe'd love the help! :-)\n. Sorry guys - we're working on some additional docs as we speak here - we'll make sure to mention the testing set-up (though it IS pretty simple/bare-bones - npm install -g mocha and you're done. You DO need to have a db created in Postgres, named to match the test db, and suitable creds in your conn string (see the tests/helpers/index.js file). \nAlso, I'll take another look at the deasync compatibility stuff. I know Rob was not concerned with ensuring compatibility specifically with io.js, but we may need to look at how to best meet everyone's needs as Node itself progresses, and/or merges with io.js. \nWe appreciate your taking the time to note the issues, and the PR's, even if we're not taking immediate action. I've had a lot going on recently (new job, moving cross-country), and I believe Rob has as well. \n. I think the thing to bear in mind is that not everyone will be jumping to Node 4.0.0 immediately. We want to make sure not to break anything which relies on previous (and probably more widely installed) versions of Node. Ultimately it will be Rob's call, but I'll certainly look into the ramifications. \n. What are you getting back as the request body that you are then trying to save after stringify? Maybe output to the console and share? Do the object properties match the table columns?\n. Hmm. No, you shouldn't have to match all the fields. Does your table have a serial int id column? \n. Can you take a look at the functions in your Postgres instance? Sounds like possibly you are loading up a metric ton of functions. The query in question pulls all the functions (if you haven't filtered them) and bolts them on to the Massive instance. If there are a ton of them (as may be the case with some PG libraries) this may be a contributing factor. \n. Yeah, in this case Massive is iterating over all those and adding them onto the instance. You might want to look into using the blacklist or Whitelist filter, or pass false to using functions at all if you don;t need them from Massive. \nI'm trying to finish some docs up for the filtering stuff, but I'll try to follow up when I get home from work if you'd like. That's a lot of functions. \n. Similar to a previous issue, Date/Timezone issues are application/Javascript issues. Postgres stores what you give it, and hands it back when you ask fo9r it. If you store a Date without time/time zone/offset info, you will be handed back a date with 0:00.000 as the time - midnight. When your application, or the database driver, does new Date(yourDateFromTheDb) on a date in this format, the JS will apply the appropriate offset for local time. If you don;t want this behavior, you need to provide TZ info when you save the date. \nSee the previous issue here:\nhttps://github.com/robconery/massive-js/issues/51\nAnd this Stack Overflow question and answers here:\nhttp://stackoverflow.com/questions/7556591/javascript-date-object-always-one-day-off\nIf you want full control over timezone stuff in your app, store all dates/times in the Db as UTC, and then let JS handle the adjustment to local. Or similar strategy. \nThis is not an issue for massive, but instead something you need to deal with in any JS application using dates.\nWe're happy to assist with solving your problem, but again, this is not an issue for Massive per se. \n. I agree wit h Rob, this sounds like it may not be directly a Massive issue, although it may be that the underlying PG driver doesn't know how to work with the geo data type. If I can, I'll experiment this weekend and see what there is to see. What happens of you include a hard-coded lat/long string value in the `POINT()' function in your SQL?\n. Ahhh.. nicely done. :-)\n. @stevenmwade @robconery -\n Ok. There was a weird issue here. Fixed. The test was passing previously (and still did prior to this patch), and I'm not sure what was going on, but the code pushed addresses things (so far...). \nbefore applying the fix referenced, the 'search()' function as follows...\njs\n      db.products.search({columns : [\"Name\", \"description\"], term: \"description\"},function(err,res){\n        assert.equal(res.length,4);\n        done();\n      });\n...would build sql like so:\nsql\nselect * from \"products\" where to_tsvector(concat('Name, ', 'description')) @@ to_tsquery($1);\nwhich if copied to pgAdmin3 (replacing the $1 param with a hard string 'description', which appears in the description field for all four records in the table test data), would execute as expected, and return 4 product records. However, if we create a table per the OP like so:\n``` sql\ncreate table \"table\"\n(id serial primary key not null,\nname varchar(20),\nstatus varchar(20),\nbody jsonb)\n-- add some sample searchable data...\ninsert into \"table\" \n    (name, status) \nvalues \n    ('john', 'good'), \n    ('tom', 'bad'), \n    ('jack', 'mediocre'), \n    ('jill', 'mediocre'), \n    ('tim', 'tim')\n```\nand then run this (using any of the name or status values inserted - in this case we'll use 'tim'):\njs\nMassive.connect({connectionString: constr}, function ( err, res) { \n    res.table.search({columns: [\"name\", \"status\"], term: 'tim'}, function(err, data) { \n      console.log(err);\n    });\n});\nthe resulting sql, same as previous, looks like this (some format as previously):\nsql\nselect * from \"table\" where to_tsvector(concat('name, ', 'status')) @@ to_tsquery($1);\nBUUUUT, popping THAT into pgAdmin3 (with 'bad' replacing the $1 param) yields an empty result set.\nI have NO IDEA why these two perform differently, but this is why the test passed before the fix. \nThe fix just pushed up fixes this, and produces sql like so:\nsql\nselect * from \"table\" where to_tsvector(concat(name, ' ', status)) @@ to_tsquery($1);\nselect * from \"products\" where to_tsvector(concat(Name, ' ', description)) @@ to_tsquery($1);\nWhich appears to work in all multi-column cases. \n. Closing for now. Sure wish I know why the statement against products worked, but the other did not. Possibly due to the use of keywords in the table/column names in the other case?\n. @eymengunay @dmfay it looks like we have a few things that should probably be bundled up into a release in the near future. i know @robconery is travelling at the moment, and I also think we should review what's pending a release, and if there is anything else outstanding that should be. \nUltimately that's all up to Rob, but it shoujld be coming, soon\n@dmfay Thanks for staying on top of this stuff :-)\n. @kzar79 - Cool that you solved the problem. This issue pops up from time to time. As I'm sure you know, designing any software involves a series of trade-offs. In our case we decided to prioritize simplicity over adding a ton of features that serve primarily edge-cases (the 80/20 rule, sort of). \nWe recognize that Db's exist in the wild with composite keys, and with tables with no defined key. While composite keys are not necessarily a true \"edge case\" (particularly when it comes to reference tables and the like), we felt they do fall into that 20% area. \nTables with no key are, to us, a true edge case, and often a Db design \"smell\" (though not always! I get it, and have done so myself :-)). \nOne potential work-around may be to use a SQL file for either of those cases. Massive is actually predicated on the notion that the lib should really only do so much - or, as a wise man once put it, \"SQL is the best DSL for talking to a database.\" Bolting SQL files on as functions is an encouraged design choice within Massive. \nMassive attempts to put an abstraction layer of the most common, repetitive stuff you might otherwise find yourself coding ad nauseum. It does not purport to be a full-fledged ORM (in fact, it goes out of its way NOT to be). \nGlad you were able to implement a solution that works for your project, and glad you are finding the lib useful :-)\n. In my opinion, I think trying to do an update with no columns specified is unusual enough I would want this to return an error. I can imagine cases where it might happen (particularly if the update is being assembled as part of some larger programmatic structure), but I would want to explicitly handle those in client code, and any other time (\"by accident\"), I would want the error. \nI suspect there's a good chance @robconery  may come down on the other side of this equation, and direct you towards the path of simply returning matching records. \nBut there's my two cents. \n. I'm not convinced returning the matching records is the right thing to do here - seems to me in most cases you would want to return an error, and if client code were calling update and might pass in an empty array of fields, that case should be explicitly handled on the client side. Most of the time, passing empty fields is something I would not want to silently fail. \n@robconery do you have any thoughts on this?\n. @robinjmurphy You heard the man...flex your commit powers and merge when you feel good about it :-)\n. Yeah. You need to use the same concatenation style, but in a manner Postgres understands. In other words, when you wrap your parameter placeholder in the wildcard symbols you need to use the Postgres concatenation symbol:\njs\ndb.run(\"select * from parts where name like '%' || $1 || '%'\", [name], function(err, parts) {\n        if(err) {\n            console.log('ERROR: ', err);\n            res.send(err);\n        } else {\n            res.send(parts);\n        }\n    });\nThat SHOULD work. \n. As I recall, the destroy() method is not implemented currently in a manner allowing criteria against documents. The current implementation only works against column-based criteria. \nI'm thinking this is a candidate for an enhancement. \n@robconery @dmfay  thoughts?\n. Nice!\n. I'm of the mind that pub/sub is a different concern. While it may be handy, it seems like an application concern to me (and I suspect Rob would agree, but you never know...). On the other hand, a complimentary lib, or a pluggable thingy, might be handy...\n. Ahhh...\nI thought we had those events in there...\nThis may have some traction. If they are not there anymore possibly we should add those back in.\n. @stevenmwade - Does the card with id = 159 already exist int he database? If not, the problem is that save() is assuming it does, and trying to update an existing record which is not there. \nIf you are not using auto-incrementing Id's, you will need to explicitly call insert() instead of 'save(). Did you set the conn string in thetest/helpers/index.js` file?\nhttps://github.com/robconery/massive-js/blob/master/test/helpers/index.js\nAnd, did you add rob as a user, with the password password? And grant admin permissions?\n. @robconery may have some specifics to add, but I think we may need to find a way to support both. \nI also think there are some features in 9.5 worth taking advantage of (\"upsert\" being one of them). \nI hope to dig in and see what's what with 9.5 this weekend. \nThat second test looks like it's simply not finding all of the functions to load. May or may not be related to the version change. \n. @robinjmurphy My thoughts precisely, on all points. \n. Why are you using the database synchronously inside an async architecture?\n. ``` js\nvar database;\nmassive.connect({connectionString: \"connString\"}, function(err,db){\n  database = db;\n// Your code....\n\n});\nTake a lookk at the tests and the readme for some more usage examples.\n```\n. @rcyrus - It is my understanding that each client in the pg pool represents an open connection, and the driver maintains a pool of clients. \nEarlier on, we had an issue where we were passing a \"truthy\" value to the done() callback which was improperly disposing of clients. See this discussion, near the end (it's long...), and this commit. \nSo far as I understand things, Massive maintains a client pool (representing open connections) just fine...we might want to investigate if the configs for these can be passed as configuration items in Massive, but according to the first para on the PG driver README, they are configurable. \nIf you want to look into the configuration for these and shoot a PR, awesome. I can also try to look into it. In any case, they should be options, but the defaults should stay if no explicit config params are passed. \n. @rcyrus - Looks like some decent info HERE on configuring the pool size and such. I think it's just a matter of mapping some configs onto the Massive config object, and doing so cleanly. @robconery likes sensible defaults (the 80/20 rule applies here fer sher), and becomes angry when the API gets dirty or complicated ;-)\n. I think the skipBelow95 is handy, but I think the loads up functions test doesn't need it. The test should probably just be written so that the number of functions is evaluated to be >= 20, since in itself, that demonstrated the functions were loaded. \nI see the skipBelow95 function becoming useful if we add in other 9.5-specific functionality, such as upsert. \n. @robinjmurphy - Any idea why the Travis CI build is failing on this branch?\n. Ahh...got it. \n. @robinjmurphy FYI - Rob added you to commit, so when you feel comfortable, you can do your own merges and such. Personally, I have no qualms just merging stuff like test fixes, and the logo fix. When it comes to architectural decisions or significant new features, I tend to wait for Rob to at least weigh in. \nNice work - really appreciate your additions. Are you using this in prod somewhere?\n. Depending on your needs, you can also use Postgres' array_to_json function (See THIS link), as well as all the other array functions. This is a perfect use-case for SQL files in Massive. \nMight be worth looking into adding some array-to-and-from-json to the API here at some point. \n. Interesting! Definitely worth looking into...so long as it doesn't mess up any of the internal plumbing in Massive. \nOn the other hand, taking on another external dependency is something to consider carefully as well. \n. Ha ha - Yep. Stealing, with some attribution (\"code re-use\" ;-)), was my thought. That way we can make it available through the API where it makes sense, but retain the indexed params if/where that makes sense. \n. You haven't specified a username in your connection string. In most cases (and depending on how you configured your Postgres install and users, you should be able to:\njs\nvar connectionString = \"postgres://postgres@localhost/phoenix\"\nYou may need to specify an actual user + password:\njs\nvar connectionString = \"postgres://someuser:somepassword@localhost/phoenix\"\n. Hmmm....platform? Postgres version?\nAlso, maybe try it using the more standard async call? There have been some issues with the deasync lib. We will likely be pulling that out into a separate lib in the future.\n. Params are parsed and submitted per [brianc's Node-Postgres driver]https://github.com/brianc/node-postgres), which, as far as I understand, adheres to Postgres query parameter requirements. See specifically the section in the documentation on Parameterized Queries\n. I think we tried using glob for this early on, but moved back to a more traditional function instead. I don't recall what the issue was, but you're welcome to try it on for size and shoot a PR. \nThe meta property doesn't sound unreasonable either. However, @robconery generally decides what should go in, and what should be left to the user for this project. My recommendation is, put it together, with appropriate tests, and see what happens. \n. Assistance with building SQL is something we talked about adding a ways back. I like the idea. Note, however, that there may be challenges with managing parameters across multiple statements (among other things). It seems like there should be an easy way to do this, but I suspect it may be a little more disruptive than it appears at first. \n. The deasync library allows synchronous use of Massive. But for the common application development use, you should probably be using standard async patterns, as Node is an async platform. \nSync is handy for some limited use-case, but the deasync library itself is a little wobbly and experimental. We may be pulling the sync stuff out in a coming rev, or at least into its own library. \n. Are you using any of the sync methods? \n. Interesting. \n. Try connecting the standard way, and see if the problem persists. As mentioned, deasync is an interesting library, but presents some issues,a nd I believe we will be removing it next rev for these reasons. \n. This is a limitation of PL/pgSQL.\nThe PL/pgSql DO statement causes an anonymous function to be executed, but will not accept parameters, nor return a value:\nhttp://www.postgresql.org/docs/9.5/static/sql-do.html\nThere is an interesting discussion HERE:\nhttp://postgresql.nabble.com/Anonymous-code-block-with-parameters-td5819155.html\nand someone in that thread suggests simply creating a temp function like so:\ncreate function pg_temp.tempfunc(i int4) returns int4 as $$ begin end; \n$$ language plpgsql;\nBut, for what it looks like you're trying to do, that might be less than ideal. \n. @aray12 - agreed on the hackiniess. You did exactly what I would have done.\n. This has been asked about a few times - and at this point, we will not be adding promises directly. \nSee the discussion below:\nhttps://github.com/robconery/massive-js/issues/37\nhttps://github.com/robconery/massive-js/issues/81\nhttps://github.com/robconery/massive-js/issues/142\n. The Bluebird branch is sort of trying things on - we are exploring here a bit. Ultimately what/when we release, and what direction it takes is up to Rob, but I can see an interim pre-release with something like the structure you see in the Bluebird branch, and meanwhile we are also experimenting with a fairly major change to go all-in with promises, possibly also all-in with ES6.\nIn the meantime, if one were to pull down what we have and experiment/give feedback (what works, what doesn;t, etc) would be most helpful. :-)\n. For myslef, I'm still investigating - I think Rob has been a little bust of late. So far, it seems like transpiling might be a better way to go while the dust is settling out witht he various versions of Node/V8, but I'm still looking into it all. \nIn particular, so far as I can tell at the moment, Mocha is not yet ES6 compatible. Also, we can always code using the ES6 stuff, and pull the transpile steps out when the features we want are fully supported in the wild. \nBut I hstill have more research/experimentation to do. Transpiling does require some additional dev env setup, and feels a little flaky at times. But I'm still new to the ES6/Babylon stuff. \n. @Sinewyk - Please do jump in. I was able to do some preliminary investigatin' on the ES6 front, and then had to move. I haven;t had a chance to jump back to it yet, but I had read somewhere (and had not had a chance to dig into it) that Mocha had some challenges with ES6. \nHowever, it also looks like the Node v6 release moves things closer in terms of Node compatibility, so we'll see. Hoping to get back to this soon. Moving sucks. :-(\n. Likewise, I am eager to jump in on this V3, but have been bogged down huge with work. \n. Ok. As you mentioned in the other comment thread (thanks for opening a proper issue!) https://github.com/robconery/massive-js/commit/d939f7a18d82c31d24a59322ef00de89bc0f65f1#commitcomment-16943433, you have a database with schema. \nYou really don't need set search_path here, because Massive (and, fortunately, sql) understand the notion of schema. So, to start, you can re-write your sql like so:\nSELECT * FROM moviedb.topics where DESCRIPTION=$1;\nFrom there, note the following - once you place params into your sql statement, the underlying node-postgres driver considers it to be a prepared statement, and it will not run, since the driver will not allow multiple sql commands within a single prepared statement. \nWhich is the root of your problem here.\nThe fix above (explicitly specifying the schema using \"dot\" notation in your sql) should fix this up straight away. \nOn a final note, though, I mentioned that Massive itself also has the notion of schema. So you could skip the sql file if needed (in a case like this), and simply do this:\njs\ndb.moviedb.topics.where('DESCRIPTION = $1', 'Action', function(err, res) { \n    console.log(res);\n});\nThe README may be in need of a little updating, but most of the basics are there. \n. Nice :-)\nYou can likely look at this test file for how I did the other tests:\nhttps://github.com/robconery/massive-js/blob/master/test/load_filter_spec.js#L281\n. Agree about adding a pgDefaults _option _on start-up. There are a number of reasons that would be handy, including (for those who choose to go this route) setting the option parseInputDatesAsUTC mentioned by the OP in #51. \nPool size management is the big win here I think. As far as the date stuff, to me that's just JS quirkiness, and Massive already does the correct thing by staying out of the way and allowing the irritating, but \"normal\" JS behavior to remain. \n. Nice! Thanks :-)\n. No worries. I'll go ahead and close it :-)\n. @dmfay  \ud83d\udc4d\nOn Mon, May 16, 2016 at 5:42 AM, Dian Fay notifications@github.com wrote:\n\nIt's kind of a glaring omission imo, so I spent some time over the weekend\nseeing what I could do. Postgres has a bunch of neat expanded\npattern-matching operators above and beyond just LIKE and NOT LIKE\n(case-insensitive ILIKE, regexes, weird mishmash of SQL pattern-matching\nand regex) and it's a shame to leave those out of the criteria finders.\nIt's not quite ready yet but I'll probably finish it up and roll it in\nsometime this week.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/robconery/massive-js/issues/251#issuecomment-219406483\n. I believe you can call update directly (instead of using the save function, which relies on the presence of a primary key to determine whether to insert of update), and set the criteria as needed, and pass the values you wish to set the fields to. \n\nLet's think about the notion of a \"bulk update\" for a minute. To me, this would mean (in SQL terms) something like the following:\nSQL\nUPDATE presidential_candidates SET is_viable  = FALSE WHERE is_valid = TRUE;\nThat SQL will set the is_viable field for all the records in the presidential_candidates table where is_valid is currently true,  to FALSE\nLikewise, from Massive, you can do this:\njs\ndb.presidential_candidates.update({is_valid: true}, {is_valid: false}, function(err, res) {\n  // Do something after updating\n});\nWhat you cannot do with Massive (at least, at the moment) is pass an array of objecets, which each need to be updated independently. A couple of things to note here:\n- The notion of \"iterating over an array and updating things\" is a very OO concept - it is not a SQL concept. Whether or not the update method in Massive accepts an array, and then does some iteratin', or your client code does the iteratin' and then calls update for each, there is still some iteratin' goin' on. \n- Unlike the bulk insert method, in which Massive pretty much takes a batch of data from an array, and builds an SQL statement that doesn't require criteria, update needs to know which specific record(s) to update, and which fields to apply which values to. Building an SQL batch to do this is a whole different deal, and is not currently implemented in Massive. \n- Massive is not an ORM in the same sense as Entity Framework, NHibernate, or ActiveRecord - it doesn't do the change-tracking bits that make the sort of update(someArrayOfObjects) stuff possible. Massive does it's best to provide some essential query tools, and stay out of the way. Thus, it stays simple, but occasionally, you may find yourself thinking you should be able to do certain ORM-ey things with it that are not part of the design scope here. \nLong story short, if you're looking to do a \"bulk update\" in the SQL context (\"update all the records that meet this criteria, and change these fields on each to these new values\") then you're in luck - YOU CAN!!\nIf you are looking to do the OO version (\"Here's a bunch of objects from this table where I changed some values on each one - store the changes\") you can't, at least for now, not in any way besides looping your objects and setting up a call to update for each one. \nHope that helped. Let me know if you have any questions, and also, skim through the test files to get a basic sense of how some of these calls work (if you haven't already) :-)\n. @andrerpena - We're actually looking at implementing an ES2015 version of the lib (a re-write, basically). My understanding is that, with Node 6.0 there is 96% ES2015 compatibility with Node and V8. It's taking a while because of time constraints. \nWould be happy to hear your thoughts on the matter, and always happy to have contributions. Just may be a bit before the initial MVP/\"spike\" code is out here for work. \nOur thinking was to leave v2 in its current form until then though, and allow v3 to be the new shiney thing. \n@robconery may feel differently on these points, but to me, it makes more sense to re-write the thing to specifically take advantage of ES2015 features from the Massive API all the way down, instead of simply porting the existing code via Babel. \n. @swinston100 - I don't think you want to terminate the connection that way. I you really need to reload the schema, you should look into ways, in the context of your application, to just re-initialize the db variable by calling connect again, which is what causes the database schema stuff to be loaded. \nI am wondering why you need to create a table such as Account at runtime? Are you certain this issue is not related to a need to re-examine your database architecture? I strongly suspect you might want to consider how it is you are persisting Account records. Can you provide some detail as to why you want to do this?\n. You might take a look at the /test/helpers/index.js file for an example of re-setting the Db. \nNote, you will need to watch mixing your sync/async calls here - if you make an async call to do your table creates, you will need to wrap your call to reset the Db in a callback that gets executed when the table creates are done. \nIn this case, you might want to factor out your loading of the Db and setting of the Express variable into a discrete method. Also, I would probably go all-in with async calls/callbacks here. First off, we will be deprecating the deasync stuff in the near future, and also it will save you from possible situations where your code execution cooks right on past an async call to create tables, and executes the next (sync) call to load the Db, when in fact the table create bits have likely not even executed yet. \n. Great - glad you solved it :-)\n. There is not currently a way to do joins in Massive, but I personally think we may want to add that capability in v3. Up to @robconery ultimately. \nIn the short-term, doing a SQL script and executing that per Massive's SQL-files-as-functions is the one way to do it, or per dmfay's comment, an ad-hoc query.\nNote that part of the reasoning behind Massive is to encourage the use of SQL where it is more appropriate. Since Massive is not really an ORM in the typical sense, the notion of a join falls more into the SQL realm. \n. I suspect you experiencing the weirdness with the deasync library. \nyou are using synchronous method calls, which rely on deasync. Unless you are doign something specific which requires the sync call, I strongly recommend you ditch the synchronous stuff. \nIn fact, we will be removing all that in an upcoming rev, as there are too many issues with deasync, and it seems to be encouraging people to try to use Node in a synchronous manner where it really, really shouldn't be. \n. The read me gives you the basics, and there's a link to some docs as well. Fortunately, Massive keeps it pretty simple. For the moment it is callback-based, not promises, although we will be moving to promises and a 100% ES2015 story in the next version. \nDon't be afraid to ask questions. I can see how the sync stuff might seem tempting, but really, when in Node, use Callbacks (or promises as the case may be, but Massive doesn't directly support that yet). \n. Ah, ok. Hang on.\n. Right. Do you have your code in a repo you can link to? Looks like there's something going on, but I have no way to know without seeing a larger code context. \n. Ok, can you point me to the code file where the issue is occuring? Where is the call where the problem arises?\n. Shit, I guess that was already stated in the output above. Mybad. Hang on then...\n. Ah. ok. \nYou are doing THIS:\njs\n    var checkin =\n      new Checkin({\n        email: email,\n        user_id: user_id,\n        checked_in_at: checked_in_at,\n        duration: duration,\n        action_code_id: action_code_id\n      });\n    checkin.save();\n});\nBut you need to pass the object to be saved into db.save(yourObject) (db or whatever you are using to hold your massive variable. \nSee the readme file, where the example is:\njs\n//no PK does an INSERT\ndb.users.save({email : \"new@example.com\"}, function(err,inserted){\n  //the new record with the ID\n});\nSo in your case, you would want to do something like:\njs\n    var checkin =\n      new Checkin({\n        email: email,\n        user_id: user_id,\n        checked_in_at: checked_in_at,\n        duration: duration,\n        action_code_id: action_code_id\n      });\n    yourMassiveInstanceVariable.save(checkin, function(err,inserted) {\n         // do some stuff here...inserted is the returned new record, with your new Id\n    });\nBUT, you need to have your massive instance available first. \nSee the test files, the readme, and maybe experiment a little. Massive is not an ORM in the familiar sense. \nIn particular, for CRUD ops, see the table_spec.js file:\nhttps://github.com/robconery/massive-js/blob/master/test/table_spec.js\n. Ah. Missed that bit. Ok. Seems like we've seen this recently as well. \nYou have properties on your object which Massive is then trying to map to database columns. Specifically, your Checkin.prototype.load property (which is, of course, a function). \nThis is the donwside of the way you are doing it, although seems like recently ~~@dmfay~~ @robconery  was assisting someone with a similar issue. \n. See this issue and specifically the resolution HERE\n. Probably a few months anyway. But you never know!\n. You likely need to also thunkify the find function, and others within Massive. \nI strongly recommend you lean into the callback style of programming, or MAYBE try the using bluebird w/PromisifyAll if you are dead set on trying to avoid callbacks. \nBut seriously, callbacks are an innate part of how JS works. \n. Yeah, I saw that, and changed my response :)\n. @hypexr - Yeah, if the files are not present when Massive loads at application start, they won;t be available as functions (at least, in the current incarnation of Massive). \nFor now you would need to do as you suggest, and read the text, and execute using run.\n. When I run the following (with mocha) on my local machine, with Node 4.5.5, everything works as expected, and the test below passes:\njs\n      db.my_test([1],(err, res)=>{  \n        assert.equal(res[0].my_test, 10);  \n      });\nThis is after adding your my_test Posgres function as defined in your original comment to the test/db/schema.sql file. Is it possible you have a function named my_test defined more than once in your SQL file, one with input params, and once without? \nWhat Node version are you using? On the primary machine I use when working with Massive (MBP) I have upgraded to Node 6.2, and I can't make the master branch of Massive run in its current state (largely due to deasync...I think...which we're removing at some point). So I had to quickly use Node 4.5.5 on my Windows box instead, before work this morning.\nIf you or @robconery hasn't solved this by the time I get home, I'll re-install the node version you have on the MBP and see what there is to see. \n. @ludvigsen A function needs to exist in the database before you can invoke it from Massive - much like a table. \nYour script exists as a function in Massive, which, when you call it, executes the SQL in the script - namely, it creates the function in the database. It does not also load the new function into Massive. The first time you make the call to my_test massive executes that SQL, and a function is created (or recreated) in the back end. Since Massive loads up database-defined functions at load, it is not loaded as a property of Massive this way.\nWhen you do your second example, you are executing the SQL, which creates the function, and subsequently loading it onto massive, and you can then execute it.\nIf you look at the tests, you will see that there is a schema file that gets executed (because test/db/schema.sql gets bolted onto Massive, during the call to loadFunctions and is then available to be invoked). See the test/helpers/index.js file for an example of how we set this up.\nSo - in short - your function needs to exists as a database object before you can invoke it. You can acheive this by adding a script to create this and any other schema objects at load. But when you call a databse function loaded by Massive, it attempts to execute an existing function. \nYou are confusing the execution of a SQL script with the execution of a database function.\nDid that make sense?\n. Looks to me like the function is not even getting loaded onto your db object at all. From teh error message you indicated:\ndb.editArtist is not a function\nTells me that the problem is less likely related to the function implementation, but that however you are trying to invoke it, it's not there. \nCan you show us how you are initializing Massive, and how you are setting it up to be callable within your app? If that function exists in your database when Massive initializes, I should expect it to load, and be callable the way you are in your original post.\nIf your code is in a repo somewhere, so much the better. We can try to see what's going on. \n. The original issue was posted by someone else, who was trying to add a function to a SQL file, and then was not understanding that this would not cause the function to be loaded in massive. I THINK I resolved his issue. \nThe new poster MAY be doing the same thing, but it's not clear from his example code. Also, given that he is executing the function from the terminal, I think he has run all the scripts, but could be wrong. \nIt's very possible he too is trying to use the SQL file containing the function as a callable file, in which case, he will have the same issue as the previous poster. \n. Given the error output from @naprirfan I suspect there is an issue with how he is loading massive and/or consuming it in his application. \n. @naprirfan - \nPostgres, and JS, are both case-sensitive. \nWhen you run your script with mixed-case names, postgres down-cases those names into all lower-case, unless you delimit them with double-quotes. \nSo, when your function is created, it's name in Postgres is editartist, not editArtist.\nThis same applies to all of your tables, and any other database objects you create with scripts. \nWhen Massive interrogates the database and loads up your tables and scripts, they are bolted onto the Massive instance just as Massive found it - in the case of your function, as a JS function named editartists. Since JS is case-sensitive, you can't refer to it as editArtist, you need to use editartist\nI personally recommend moving away from mixed-case database object names in Postgres, and instead would use under-score separators, as this is more idiomatic in Postgres (and most databases other than SQL Server). So... the table artists instead of Artists (which will become artists anyway, unless you delimit with double quotes). Function names edit_artist instead of editArtist, etc. \nIf you really, really need to use mixed-case names for your database objects, you need to double-quote them on creation, i.e:\nsql\nCREATE TABLE \"ArtistsSkills\"(\n  \"ArtistId\"  BIGINT REFERENCES \"Artists\" (\"ArtistId\"),\n  \"SkillId\" INTEGER REFERENCES Skills (\"SkillId\"),\n  PRIMARY KEY (\"ArtistId\", \"SkillId\")\n);\nSame applies to your function script. Note, in the above, you must always refer to the mixed-case object name using the double-quotes. It's a pain. \nLastly, your root-level route/function works, because (and only because) Postgres down-cases all incoming SQL before execution, unless, as we've seen, object names are delimited with double quotes. \nIf you tried to use any of Massive's in-built functions with your current db script, such as db.Artists.find(1), it would fail, unless you used db.artists.find(1). To Postgres, all of your db object names (tables, columns, constraints, etc) are lower-case. And all incoming SQL will be lower-case. \nAnd those object names will be represented on Massive the way they are found on the database. \nHope all that helped :-)\n. @aderbas - Can you provide the actual procedure (function) definition here? It would help figure out what's going on. Also, when you say \"the procedure already exists,\" when, specifically are you creating it?\nAre you sure the application login role has execution rights on the procedure?\n. Yes. if you mean \"can I execute the output from a pg_dump as a SQL script, certainly.  In fact, if you take a look in the test/helpers/index file HERE you will see that our tests load up a file named schema in the scriptsDir and then execute it as a JS function. While the schema file is not output from pg_dump, you will see an example of using a script file to create tables and load data.\nNote, however, that you need to engage in some trickery if you want to execute the actual database create bits. To do that, you have to connect initially to another database on the Postgres instance (the postgres database works), then execute the database creation stuff, and then connect to the newly created db and run your table/object creates, and populate with data. \nI had something like that put together at some point, but the details are lost on my at the moment. Wasn't horribly difficult to do, but did have to experiment a little. \nIf you are asking whether you can execute psql commands, not directly, but you could experiment with adding some code to your application to do so - again, I don't think that's overly difficult, it's just not part of MassiveJs at the moment (although I think it might be a handy addition to v3!)\nSee HERE for a quick example. I have not tried this myself, but psql is, in fact, a command-line binary, so...\n. @robconery - yep - dynamically building one big singe insert with multiple value groups. We don't have that here in Massive within a transaction (yet). But I think it's worth adding.\nYou should be able to build one insert which handles all your records, just like he is doing. it is not only more performant, but simpler to write in SQL:\nsql\nINSERT INTO myTable (column1, column2, column3) values ($1, $2, $3), ($4, $5, $6) ...($n, $n+1, $n+2);\nSadly, this doesn't lend itself to the OP's use-case (using a query from a file), I don't think, because he would need to know in advance how many records he plans to insert. \nBUT, I think we should add an insertRange(objects) which will do this in a transaction, like we did in Moebius.\nI'll see if I can break away from work long enough to take a swing at that. \n. Like Rob said - Massive doesn't do that type of mapping directly, but you can definitely use Postgres' JSON type and Array/aggregate functions to impose a document structure on relational data, and then return the JSON object the way you describe. \nYou can find the really basic details in a blog post I did a ways back, and I know Rob has also written about this approach.\nYou can definitely create this type of query as a SQL file as Rob says, and then just call it as a function in Massive. \n. It can be done using the multiple insert statement as Dian describes, but you have to keep track of how many rows and how many params you are using per command. There are limits, usually imposed by the driver. Also, the COPY would work, although that way ignores malformed or invalid inputs, and your insert can become a mess if your SQL gets assembled wrong or someone pushes malformed records thru.\nI've been thinking on proposing/adding some bulk insert stuff, maybe as a V3 thing.\n@robconery I think it's time, if we can ever make the time. V3 all the things. \n. We've been spitballing a little. Yeah, I personally feel that Node/v8 has sufficient support for ES6 + in-build promises that there is no need to clutter things up with a third-party Promise lib. So, yeah, I think the 'bluebird' branch is more of an experiment, and an abortive one. \nWe've discussed going full-tilt with ES6, and also trying to slim things down a bit. The current code base is getting a little scattered (I think, anyway). \nI can;t speak for Rob, but to me, the only thing preventing a start here is available time (which has been scarce for me of late) and some roadmapping/api discussions/decisions. \n. Do the existing tests run? There is a test in the tests/table_spec.js file that does this very thing. Would be helpful to know if that test file runs correctly, or if the test fails as well. From the looks of things, your code seems like it should be working. \nAlso, could you show us the exact create script for your table?\n. Hmmm...this is an interesting one. It appears you are correct sir. Looks like we may need to do some re-organizing of the code there so that param values for such statements aren't pushed into the array. I can look at this after work, unless you want to take a stab at it, or unless @dmfay gets to it in typical lightening-fast fashion ;-)\nNice catch :-)\n. @dmfay  - Yeah, I know Rob's been pretty busy with his book, and I've been really tied up with a feature at work. Sigh. Really want to get rolling on that. \n. @dmfay - You bad ass. \ud83d\udc4d :-)\n. Can you check to see what happens of you remove the public like so:\njs\n    db.crap.insert(userObject, function(err, res) {\n        console.log(err);\n        return callback(null, res);\n    });\nWhen Massive loads the tables, items in the public schema are bolted directly onto the Massive instance itself, without the public qualifier. I suspect that is the issue, but if not, let us know. \n. But...who would define a database table without a primary key? :trollface: \nKidding :-) Glad you got it resolved!\n. There's no way to do this in Massive directly - any way we would implement this would require iterating a collection of records to update, and while I can't speak for @robconery or @dmfay the complexity involved would outweigh any percieved gain, when it is just as simple (simpler, actually, IMHO), for the client code to perform the iteration like so:\n```js\n// some collection of users needing to be updated, however you arrived here:\nvar usersToUpdate =[\n  {1, 'hollis@weimann.biz', 'Hollis', 'O\\'Connell'},\n  {2, 'robert@duncan.info', 'Robert', 'Duncan'}\n];\nusersToUpdate.forEach(function(user) { // ES6/2015 array.prototype syntax\n  //include the PK in the criteria for an update\n  db.users.save({id : 1, email : \"test@example.com\"}, function(err, updated){\n    //the updated record for the new user\n  });\n});\n```\nMassive does allow updating of multiple records which meet a common criteria with the same values like so (example from the table_spec.js test file):\njs\n    it('updates multiple products', function (done) {\n      db.products.update({in_stock: true}, {in_stock: false}, function(err, res) {\n        assert.ifError(err);\n        assert.equal(res.length, 2);\n        assert.equal(res[0].id, 1);\n        assert.equal(res[0].in_stock, false);\n        assert.equal(res[1].id, 2);\n        assert.equal(res[1].in_stock, false);\n        done();\n      });\n    });\nAlso, Massive does allow for insertion of multiple new records (a far, far more common scenario then updating multiple existing records with different data in each field set) like so (again, from table_spec.js):\njs\n    it('inserts multiple products', function (done) {\n      // you could also call save() here with the same result):\n      db.products.insert([{name: \"A Product\"}, {name: \"Another Product\"}], function (err, res) {\n        assert.ifError(err);\n        assert.equal(res.length, 2);\n        assert.equal(res[0].name, \"A Product\");\n        assert.equal(res[1].name, \"Another Product\");\n        done();\n      });\n    });\nUnless I have missed the point of what you are trying to do in your example completely, I would recommend walking the collection client side. If it is a one-off, and/or a particularly large number of records, then yes, you will likely be better off writing some raw sql. \nCan you elaborate more on the problem you are trying to solve?\n. @Aedaeum - Yes, that does sound like a special case, where iterating is not necessarily the best way to go. And, let's be clear, I did not say iterating one by one was the solution to your problem, just that any solution we would need to implement behind the scenes would also need to iterate the items to be inserted as well. \nIt sounds like you have a large number of records that need to be updated. No matter how you slice it, there is a lot of database writing that needs to be done here, but possibly there is a better way to manage it. I'll have to think on this, and possibly @robconery might have some insight as well. \nI can say that this is an edge case which Massive does not try to address directly. . @jnystrom @dmfay - Getting transaction support into Massive is something we would all like to see, and if it is doable in a reasonable way (emphasis on \"reasonable\") I expect it will appear in the long, long, awaited v3. Sigh. Time. . Unless I'm missing something (always a possibility), in your example, it looks like you are basically excluding any records where the start date is not equal to 2017-02-28T17:35:00.000Z\nIn other words, this statement:\njs\n.find({start_date >=:'2017-02-28T17:35:00.000Z', end_date <=: ''2017-02-28T17:35:00.000Z'})\nHas the exact same Date/Time for both start and end date. Is this what you meant?\nAlso, in the case of this type of comparison, you will need to wrap the property names and comparison operators in quotes so that Massive can identify the entire property name (for the purpose here, Massive treats the column name and the comparison operator together as a property name, as you can see by the way you are assigning the value to the right of the colon):\njs\n.find({ 'start_date >=': '2017-02-28T17:35:00.000Z', 'end_date <=': '2017-02-28T17:35:00.000Z' })\nJS does not allow property names like { my property name: 'some name' } but it will allow:\n{ 'my property name': 'some name' } or...wait for it... { 'start_date >=': '2017-02-28T17:35:00.000Z' }\n(It is one of the slightly annoying things I wish we could find a way to deal with more elegantly...)\n. @nurugger07 You're doing it wrong. \nIf you want to use Massive against Oracle, start with this:\nDROP DATABASE INCLUDING BACKUPS NOPROMPT;\nThen just pop open a terminal and do this:\nsudo apt-get install postgresql\n. ",
    "maca": "hummm... lets see.. this is my first node app...\nhere's the stack trace:\nevents.js:72\n        throw er; // Unhandled 'error' event\n              ^\nerror: duplicate key value violates unique constraint \"users_email_key\"\n    at p.parseE (/home/maca/Web/--/backend-app/node_modules/massive/node_modules/pg/lib/connection.js:412:11)\n    at p.parseMessage (/home/maca/Web/--/backend-app/node_modules/massive/node_modules/pg/lib/connection.js:287:17)\n    at Socket. (/home/maca/Web/--/backend-app/node_modules/massive/node_modules/pg/lib/connection.js:45:22)\n    at Socket.EventEmitter.emit (events.js:95:17)\n    at Socket. (stream_readable.js:746:14)\n    at Socket.EventEmitter.emit (events.js:92:17)\n    at emitReadable (_stream_readable.js:408:10)\n    at emitReadable (_stream_readable.js:404:5)\n    at readableAddChunk (_stream_readable.js:165:9)\n    at Socket.Readable.push (_stream_readable.js:127:10)\n. heres my migration, I love sequel too:\n```\nSequel.migration do\n  up do\n    create_table :users do\n      primary_key :id\n      String     :email, null: false, unique: true\n      String     :crypted_password, null: false\n      String     :secret, null: false\n      constraint :valid_email, Sequel.like(:email, /^[A-Za-z0-9._%-]+@[A-Za-z0-9.-]+[.][A-Za-z]+$/)\n      index :email\n    end\n  end\ndown do\n    drop_table :users\n  end\nend\n```\n. Workaround:\n//execution uses the Client\nself.execute = function(callback) {\n  this.db.on('error', function(){});\n  self.db.execute(self.sql, self.params, function(err,result) {\n    if(callback) { callback(err, result); }\n  });\n};\n. ",
    "riginoommen": "how can we implement error handling in such a situation in node js.\nmy code is\n```\npostEmployee: function(req, res) {  \n    var pg = require('pg');  \nfunction createNewEmployee (cols) {\n  // Setup static beginning of query\n  var query = [\"INSERT into employee \"];\n  query.push(\"(\");\n\n  // Create another array storing each set command\n  // and assigning a number value for parameterized query\n  var set = [];\n  Object.keys(cols).forEach(function (key, i) {\n    set.push(key); \n  });\n\n  query.push(set.join(\", \"));\n\n  query.push(\") VALUES (\");\n\n  var set = [];\n  Object.keys(cols).forEach(function (key, i) {\n    set.push(\"'\" + cols[key] + \"'\"); \n  });\n\n  query.push(set.join(\", \"));\n\n  // Add the WHERE statement to look up by id\n  query.push(\");\" );\n\n  // Return a complete query string\n  return query.join(\" \");\n}\n\n// Setup the query\nvar queryText = createNewEmployee(req.body);\n\n\nvar employeeConnectionString = \"postgres://postgres:skedge@localhost:5432/skedge\";\nvar employeeClient = new pg.Client(employeeConnectionString);\n\nemployeeClient.connect();\n\nvar query = employeeClient.query(queryText);\n\nquery.on(\"row\", function (row, result) { \n    result.addRow(row); \n});\n\nquery.on(\"end\", function (result) {   \n    employeeClient.end();\n    res.writeHead(200, {'Content-Type': 'text/plain'});\n    res.write(JSON.stringify(result.rows, null, \"\") + \"\\n\");\n    res.end();  \n});\n\n},\n```\nand error log is\nevents.js:72\n        throw er; // Unhandled 'error' event\n              ^\nerror: duplicate key value violates unique constraint \"employee_user_id_key\"\n    at Connection.parseE (/home/rigin/Work/Ops Portal/operations-portal-server/node_modules/pg/lib/connection.js:534:11)\n    at Connection.parseMessage (/home/rigin/Work/Ops Portal/operations-portal-server/node_modules/pg/lib/connection.js:361:17)\n    at Socket. (/home/rigin/Work/Ops Portal/operations-portal-server/node_modules/pg/lib/connection.js:105:22)\n    at Socket.emit (events.js:95:17)\n    at Socket. (stream_readable.js:765:14)\n    at Socket.emit (events.js:92:17)\n    at emitReadable (_stream_readable.js:427:10)\n    at emitReadable (_stream_readable.js:423:5)\n    at readableAddChunk (_stream_readable.js:166:9)\n    at Socket.Readable.push (_stream_readable.js:128:10)\n    at TCP.onread (net.js:529:21)\nnpm ERR! weird error 8\nnpm ERR! not ok code 0\n. ",
    "ymitis": "It appears wrapping a column name in single or double quotes is not allowed for MySql, it treats the column as a string.  Instead you can use back ticks (`),  I tested the query several different ways in the query window Navicat:\nThese two return results:\nSELECT * FROM Users WHERE id = 1\nSELECT * FROM Users WHERE id = 1\nThese two do not return results:\nSELECT * FROM Users WHERE \"id\" = 1\nSELECT * FROM Users WHERE \"id\" = 1\nmassive/lib/query.js self.where function wraps the column name in double quotes:\n... util.format('\"%s\" %s %s', ...\nself.where = function(conditions) {\n    if (_.isUndefined(conditions)) { return self; }\n```\nif(.isNumber(conditions)) {\n  return self._append(\" \\nWHERE \\\"%s\\\" = %d\", self.table.pk, conditions);\n}\nif (.isString(conditions)) {\n  self.params.push(conditions);\n  return self._append(\" \\nWHERE \\\"%s\\\" = %s\", self.table.pk, self.db.placeholder(self.params.length));\n}\nvar conditions = [];\n.each(conditions, function(value, key) {\n  var parts = key.trim().split(/ +/);\n  var property = parts[0];\n  var operation = operationsMap[parts[1]] || '=';\nif (.isBoolean(value) || .isNumber(value)) {\n    return _conditions.push(util.format('\"%s\" %s %d', property, operation, value));\n  }\nif (!_.isArray(value)) {\n    self.params.push(value);\n    return _conditions.push(util.format('\"%s\" %s %s', property, operation, self.db.placeholder(self.params.length)));\n  }\nvar arrayConditions = [];\n  _.each(value, function(v) {\n    self.params.push(v);\n    arrayConditions.push(self.db.placeholder(self.params.length));\n  });\n  _conditions.push(util.format('\"%s\" %s (%s)', property, operation == '!=' || operation == '<>' ? 'NOT IN' : 'IN', arrayConditions.join(', ')));\n});\nreturn self._append(' \\nWHERE ' + _conditions.join(' \\nAND '));\n```\n};\nIs this the expected behavior?\nIf so, how to I get MySql to allow the double quotes?\nThanks!\n. Thanks for the quick response!\n. ",
    "trentrichardson": "An idea for enhancement might be a super light weight migration utility. Since massive already likes sql files it seems like it would be an easy routine to do up and down migrations. The user creates the actual sql files (not generated like big ORMs). \nMassive would just need to keep up with the current migration level and just migrate latest or migrate 2 and run each sql file to migrate the db to that level:\n1_down.sql\n1_up.sql\n2_down.sql\n2_up.sql\n3_down.sql\nEtc...\nThis approach isn't completely thought through, but after using the library a bit this came to mind and seemed to be related to this.\n. @jfbaquerocelis I had a similar error after updating to node 4.1.1.  I simply deleted my node_modules directory and re-ran npm install\n. @jfbaquerocelis Ok.  It could be the case I'm just not using all the features of massivejs, or not using the features you are in your project.\nFor anyone else wondering, I had an existing project I have been developing on node 0.12.  I upgraded to node 4.1.1 and got an error about deasync (I don't recall the exact error).  I simply deleted the node_modules directory and ran npm install to get a fresh set of dependencies.  After that massivejs has been working flawlessly.\n. ",
    "dmfay": "Migrations are the sort of thing I'd much prefer to handle with a dedicated utility, to be honest. There's already node-pg-migrate and node-db-migrate, among others, and even if they might lack certain features or be a bad fit for some use cases, it's highly unlikely a quick&dirty effort to add a migration module to Massive would produce anything more generally useful. I'm not a fan of the Swiss army knife approach -- better to do one thing and do it well imo.\n. Yes it is the array type, and I did add some quick tests for insert -- not the most thorough set I've ever written, but it at least validates that insert works with and without arrays. Is there anything else you want covered for this?\n. Admittedly this is kind of a moot point for me since I prefer to generate keys anyway; I do like the idea of using the new upsert functionality, but 9.5 isn't dropping until sometime in Q3 at the earliest so I'd be wary of being a little too bleeding-edge there.\nOne potential option C would be to allow something along the lines of db.stuff.save({id: 1, name: 'hi'}, {isNew: true}, function (err, res) {...}); and decide whether to insert or update based on isNew (if present). I'm not too keen on that idea though, because it doesn't really make anyone's life easier -- the level of effort is about as much as required to just call insert or update directly, so I don't see a reason to implement that over option B.\n. There's a third assumption missing from the op -- 'all pks are unary'. I don't know if I'll have time to fix this properly so I might just have to hack around it on the application side for now, but an update on a table with a compound primary key is going spectacularly wrong:\n```\nCREATE TABLE page_checkpoint (\n    test_run_id UUID NOT NULL,\n    page_number int NOT NULL,\n    checkpoint_id UUID NOT NULL,\n    status character varying(50) NOT NULL DEFAULT 'unknown',\n    reason text\n);\nALTER TABLE ONLY page_checkpoint\n    ADD CONSTRAINT page_checkpoint_pkey PRIMARY KEY (test_run_id, page_number, checkpoint_id);\n```\ndb.page_checkpoint.update({\n    test_run_id: chk.test_run_id,\n    page_number: chk.page_number,\n    checkpoint_id: chk.id,\n    status: 'stuff',\n    reason: 'things'\n}, cb);\nUPDATE \"page_checkpoint\" \nSET \"test_run_id\" = $1, \"checkpoint_id\" = $2, \"status\" = $3, \"reason\" = $4\nWHERE \"page_number\" = $5 RETURNING *\nThe query in tables.sql doesn't need any adjustment, but loadTables needs to group the resulting rows by schema and name -- right now it processes page_checkpoint three times and the third row declaring page_number to be the primary key wins out. I actually wrote a collapser that builds result trees based on column aliasing (to an arbitrary depth, even!), although that's probably overkill for this.\n. On looking, I'm actually not sure it's too complicated. It's easy enough to detect if you reduce the schema/name/pk rowset before processing the tables; the storage format changes; and then the only place it seems to matter is in the updates, since find() on a compound key looks identical to a find() on multiple fields.\n. Fixed in #78 \n. Yeah, I'm thinking more along the lines of what John mentioned -- I'm not at all averse to procedures or executable scripts as a matter of principle, but in many cases I'd prefer to leverage the generated read/write methods for consistency's sake (or, alternatively, I have something that would be hellish spaghetti if expressed in SQL).\nOne other thing I'd like to be able to do is run a test in a transaction that gets rolled back in afterEach().\n. I played around with it a bit yesterday, and yeah it's not a trivial addition. Opening and closing a transaction are both easy enough, but you have to ensure that all your transactional queries run on the same pg.Client (and that nothing else uses your client either!).\nCloning is definitely one way of doing it, and dirac (https://github.com/jrf0110/dirac) has what looks to be a good, clean implementation of it for what they do, but they're more of a pure lightweight O/RM than a MyBatis-style data mapper.\nI think it might be possible to avoid the overhead of cloning though, if somehow we could force runner.query to use a particular client -- if beginning a transaction were to reserve a client and return a token by which it could be retrieved and run against, maybe? Although I don't think that'd work out very well for backwards-compatible APIs....\nAs for third-party modules, there's https://github.com/goodybag/node-pg-transaction but I think we'd still have the problem of keeping the dedicated client reserved.\n. oh cool, thanks! I will endeavor not to break things horribly \n\n\"Rob Conery\" notifications@github.com wrote:\n\nHope you don't mind - I just added you to commit so it's easier for you :). \nThanks again for all your work - much appreciated!\n\u2014 \nReply to this email directly or view it on \nGitHub.\n. Might be possible to stub it with sinon to simulate the fault and then use stub.restore() to put the method back?\n. There's no setup script but it's pretty simple -- make sure you have a user rob/password (you may need to change your postgres config to require password authentication locally, I can't remember offhand) with full rights on a database named 'massive'. The connection string is in test/helpers/index.js. The tests bootstrap all the schema & data they need, and I'm not seeing any failures at the moment.\n\nYou are getting enough successes that I don't think it's the connection, though -- unless your server is just randomly going incommunicado!\n. Yeah, I ran into a similar thing -- Massive loads up your tables on connect, so when you create the tables after connecting they don't have the methods attached. The second time, since you already have the schema leftover, it works fine.\nWhat I ended up doing is connecting once to make schema changes and then reconnecting. It worked out fine for me since I'm doing it in a grunt task and can muck around with schema as a superuser before reconnecting as the app user to seed the data, although I can see where a 'reload configuration' option might be useful in other situations.\n. That's up to @robconery but if you're willing to put up with potential instability* in the mean time you should be able to track the master branch in your package.json dependency declaration like so:\n\"massive\": \"https://github.com/robconery/massive-js/tarball/master\",\n- in other words, don't do this on production!\n. @xivSolutions John, could you merge #99 if you're okay with this? I'm staring at a case where I'm upserting an object and getting varying types back right now :)\n. thank you!\n. For whatever it's worth I am pro-bubbling in general and would prefer to be able to decide what to do with a connection error without any extra try/catch rigmarole on top of the existing error checks in my callbacks.\n. We've got a deadline coming up next month so I don't know how much I'll be able to contribute until that's over but I'll see what I can do!\n. Moving to github pages for v3, and aside from a couple points which are still in flux the documentation is basically complete on the promises branch.. Deprecating deasync.. Last week I upgraded our app to 4.1.1 and didn't notice any problems in my testing (the change just got merged down to the development branch though, so caveat emptor). We are tracking Massive's master branch directly for now rather than specifying a version, but right now master and 2.0.9 are identical anyway.\n. Can you post the output?\n. Re: watching files, we're using Grunt in our dev environments and have a task set up via grunt-contrib-watch that restarts the server if anything changes, including in /db. Your scripts are really only going to change during active development -- I hope! -- so that feature would just be dead weight once deployed.\n. Massive is Rob's project so it's his call in the end, I was just pointing out that there're already external solutions for most file-watching use cases. If you need it in production (which it must be said, albeit without knowing the particulars of your situation, does seem like the sort of thing that could go very wrong very easily) there are libraries such as https://github.com/fgnass/filewatcher or https://github.com/bevry/watchr that can do the same thing, so I wouldn't bet on the feature making it into Massive itself when it'd only take a little custom code to wrap it in something that kicks it over when a change occurs. That's the standard use case for promisification for example: Massive doesn't return promises itself, but multiple projects including ours wrap it with a converter.\nSyntax checking on load does sound like it could be useful if it doesn't slow down startup too much, but I'd have to check it out for myself.\n. Check your schema permissions and make sure the user you're connecting as has access to it. I just spent way more time than I want to admit the other day figuring that out on our build server.\n. If those don't work, try going through this and see if that's the problem: http://stackoverflow.com/a/7758860\n. What happens when you connect as the app user and \\dt?\nAlso -- do your tables have primary keys defined?\n. If your permissions are all in order and you're not using the whitelist/blacklist functionality about the only thing I can think of that'd prevent Massive from seeing a table is if it doesn't have a primary key column.\n. no problem :)\n. Additionally: because one of the principal reasons to use table objects in the first place is that it lets you quickly and easily manage and save individual records, which are best tracked by a primary key.. They raised #406 -- it's a Node version issue.. Views would help with reads, but having to write through a function instead of the table API is a bit ugly. There is already an exception for foreign tables since those can't have primary keys, so @bjnortier if you're interested in expanding that for time series stuff feel free to open another issue to discuss what needs to happen there!. If you didn't create the user rob with password password and superuser permissions on the massive db, that would definitely be a contributing factor.\n. db.table.save emits an update if the primary key column is present in the data or an insert if not. You still need to have the id, of course. Not sure what you mean by it only working 'if your values don't equal either' -- can you clarify?\nUpdating jsonb data is an all-or-nothing proposition (no tweaking internal values) and it can sometimes be a little tricky, but I find that stringifying them before I pass them on to Massive usually takes care of it.\n. I threw together a quick test and Massive isn't having any problems explicitly setting a unique field to its current value. You might check to see if you've got leftover data polluting your test environment, but the biggest problem I can see with that code is that you're using asynchronous functionality inside a for loop. Look into using something like each from https://github.com/caolan/async instead.\nI'd write the inner part of your code as follows, though:\n``` javascript\ndb.parts.findOne({name: data[i].name}, function(err, part) {\n    if (part) {\n      part.name = data[i].name;\n      part.status = data[i].status;\n      part.fake = 'duper';\n    } else {\n      part = {\n        name: data[i].name,\n        status: data[i].status,\n        fake: 'super'\n      };\n    }\ndb.parts.save(part, function (err, parts) {\n  if (err) { return console.log(err); }\n});\n\n});\n```\nwill insert any parts that it can't find by name, and update those it can. & yes, if your incoming data does not include the ids you'll have to query the database for them, although finding each part name by name may prove to perform poorly at scale.\n. nice catch!\n. Also, if you need to get by in the mean time and are willing to risk a little jankiness, you can track master or even a specific commit in your package.json (link). But be sure to update once the release drops!\n@xivSolutions it's no problem, without Massive I'd have had to write my own data layer from the ground up -- this is way better!\n. Junction tables sometimes outgrow their original remits -- I've got one that's tracking a status field in addition to the relationship, for example. Fortunately I only need to insert via Massive so the composite key issue doesn't come up, but similar situations might not work out so cleanly.\n. That changed a little while back when I added support for JSON traversal in the find-style args. This unit test passes:\n``` javascript\nit('deletes by matching json', function (done) {\n  db.docs.destroy({'body->>title': 'Starsky and Hutch'}, function(err, deleted) {\n    assert.equal(deleted.length, 1);\n\n    db.docs.find({id: deleted[0].id}, function (err, found) {\n      assert.equal(err, null);\n      assert.equal(found.length, 0);\n      done();\n    });\n  });\n});\n\n```\nNote the omission of both the whitespace & the single-quotes around the JSON field name (\"body->>title\" vs \"body ->> 'title'\"); Massive adds the quotes itself after splitting on the possible operators. Vincent, that's probably what's messing your call up -- give it a shot without those.\n. Looks like the docs are incorrect -- sorry about that! I'll see if I can make it a little less sensitive about whitespace at the very least though.\n. #171 allows pretty much any legal combination of whitespace and quoting so the docs shouldn't need to be updated. I don't know when Rob's planning to release the next version though.\n. Yes, that's by design. There's only one db namespace and tables are loaded in before folders/scripts, so if there are naming collisions the latter will win out. If that's how you prefer to organize your scripts, the easiest thing to do is to prefix your folder names -- so db.tablename refers to the table itself, and db._tablename or some such is a collection of scripts that operate on the table.\n. It works in code but fails as you described using the REPL. Looks like a bug in Args-JS -- it's using instanceof to check whether parameters are arrays and this seems to be one of the weird edge cases where that returns a false negative.\nI submitted a fix to Args-JS here: https://github.com/joebain/args.js/issues/22\n. Args.js integrated the fix and posted a new patch release. Update your dependencies recursively (or just remove node_modules/massive and npm install again) and the problem should be gone.\n. Ideally you should be using the varchar(n) type if you want to store and retrieve variable-length strings -- by using char(n) you're telling it \"data in this column is always 30 characters long no matter what, pad it out if necessary\", so it's just giving you what you've asked for.\nIf you can't change the column type for whatever reason, you can use Postgres' own trim() in a SQL script or function, or map your results through something that calls JavaScript's string.trim() on the receiving side, but there's nothing in Massive that will do it for you.\n. Well, I'm not Rob, but looking at it I have to ask: what about this technology is so generally compelling that makes it suitable for bundling into a data access toolkit with a distinctly minimalist, \"here's your database and what's in it, everything else is on you, go nuts\", mindset? Granted, triggers and pub/sub aren't always sufficient for peoples' needs (or you wouldn't have written this), but: we don't do anything with triggers or pub/sub in the first place. Massive reads and writes data; monitoring is a separate concern.\n. I thought about this some more and I'm keeping Massive's focus on data mapping; an event system necessitating extra setup is more than I want to get into.. Materialized views aren't part of the ANSI standard so information_schema.views doesn't cover them, unfortunately. Should be possible to make lib/scripts/views.sql query the system catalogs (pg_views and pg_matviews) instead, since we don't care about key information, but I don't know if I'll be able to get to it this week.\n. Thought I'd give it a shot while waiting on code from someone else, and that turned out to be pretty much all that was needed. Let me know if you run into any issues!\n. I'm all for the feature itself, and the problems don't sound that bad -- updateDoc is a new function so anyone running < 9.5 can just keep going with saveDoc, and jsonb is better for storage anyway. Can you add some tests and submit a pull request?\nAlso you're pulling args[pkName] out into plVal and not doing anything with it, so you might want to check that out.\n. oh cool! What about setAttribute for the name? It's more concise but still gets the point across.\n. Looks like it's polluting the schema. There's another ES6-only bit I inadvertently added a little while back so I'll see if I can fix everything up quickly. @robconery can you take a look at #165 when you have a minute? I didn't want to take over management stuff like that...\n. Not at present, but it wouldn't be too difficult to implement -- check for an override in args.options, probably. Feel free to submit a pull request!\n. Thought I'd let Travis take it but it's not -- @robconery did you enable the build from that side? Step 2 in https://docs.travis-ci.com/user/getting-started/#To-get-started-with-Travis-CI%3A\n. This definitely looks helpful!\n. I think the idea is it's not always practical to guess at an arbitrary higher limit, which is perfectly reasonable, but the approach I'd go for is a flag in the options object which turns the arbitrary lower limit off rather than removing it entirely.\n. Storage and retrieval work pretty much how you'd expect (look for tests in table_spec and queryable_spec dealing with product.tags), and find etc can use the contains/contained by/overlap operations like you'd use != or any other operation. Anything else you're going to need to script.\n. We're using jsonb fields mixed in with relational structures and it's going fine, although in our case the jsonb is basically pulled and pushed as a single unit so we're not doing any heavy-duty searching. But if you create your document table with the structure Massive expects and then add your FKs and other columns on top of that, you should be able to use Massive's document functions for JSON manipulation and searching and the standard table functions for everything else.\n. This would definitely be cool for scripts & functions (table crud should stick with indexed parameters imo), but it's going to be difficult to implement or integrate there because of how Executable.invoke() sorts out its arguments.\nYou can pass up to three args: parameters (primitive or array), options (object; currently just used to toggle results streaming), and callback. Since both parameters and options are optional, it checks the types of what you pass it and assumes an object is the options and anything else is the parameters. When using this, the parameters could also be an object, and I don't know if it's possible to resolve that ambiguity while remaining backwards-compatible.\n. It's happening! At least for run and script files!. I think I see a problem, unfortunately -- haven't pulled the code to test myself but it looks as if once you've opened a transaction you can't do anything outside it until it's finished because it makes the runner stateful.\nI haven't looked at adding transaction support in a while (priorities changed) but at this point I think either actually cloning or simulating cloning db is probably the way to go. End result would look something like this:\njavascript\ndb.tx.begin(function (err) {\n    db.tx.mytable.insert(mydata, function (err) {\n        if (err) { db.tx.rollback(); }\n        else { db.tx.commit() };\n    });\n});\n. Thanks to Node's module caching, the runner does have singleton-like characteristics even if it doesn't meet the complete technical definition. Try changing the before() hook in one of the testcases to open a transaction and log whether you're in one or not from the runner, then run mocha test -- you'll see that everything from that point onward is executed within the transaction even as the succeeding testcase modules re-require and reconnect Massive.\n. I've corrected the deasynced function name in another pull request, and there's not much call to do more there since we're getting rid of DA entirely.\n. You can parameterize an IN list of a fixed length if you write your statement as ...WHERE table.table_id IN ($1, $2, $3);, but you probably want to be able to pass in as few or as many items as you want at any given time. I don't think the parameterization engine allows for that, but fortunately there's an only-slightly-hacky workaround using array parameters:\nsql\nSELECT * FROM products WHERE id = ANY($1);\njavascript\ndb.someProducts([[1, 2]], function(err,products){\n    assert.ifError(err);\n    var p1 = products[0];\n    var p2 = products[1];\n    assert.equal(\"Product 1\", p1.name);\n    assert.equal(\"Product 2\", p2.name);\n    done();\n});\nNote the nested array -- functions and query files take in arguments as an array already, so the payload has to account for that.\n. I called it hacky because it's achieving the same result as the standard form x IN (y, z) in a non-standard way -- which, however effective it might be (and it is!), is imo definitionally a hack. You're right there are no major differences, but here's one example where the finer points might trip someone up.\n. Only with Java, I'm afraid. I do think a dedicated module would be appropriate here -- something that offers discrete log levels would allow us to have log statements seamlessly interspersed through the code while only ever capturing messages meeting a level threshold set on startup. Looks like winston supports that at a glance, but I don't know much else about it.\n. There's also log4js which is close to what I'm used to and seems relatively active, but I don't know how mature it is.\nBoth that and winston at least have the concept of appenders ('transports' in the latter), which we could consume as part of that extra startup configuration. For example if in addition to the log threshold you specified a file appender when initializing Massive, we could use that to write out to the same file you're logging your other stuff to -- assuming there aren't any wacky permissions issues, of course. Otherwise we'd default to logging errors and only errors out to the console, or something along those lines.\n. With pg-promise, Massive is even thinner than it used to be and pg-monitor is easy enough to set up.. That's going to depend on your framework. With Express, you can app.set the db instance in the asynchronous connect callback.. What do you mean by \"without luck\"? At a glance, db.find is incorrect -- finders and other table crud functions should be invoked via db.mytable.find(...). saveDoc is the exceptional case here since you don't have to have the table already created.\nReturning partial documents isn't supported, and honestly if I found myself considering that my first instinct would be to go back and take a good long look at my data model to see if the document needs to be broken up or if certain fields should be columns instead. I don't know your specific needs, of course, but it's a bit of a red flag.\n. aw :)\n. Yes, it's promises. The other two were experiments that didn't pan out.\nCurrently, the promises implementation is baseline complete; all tests pass, but I haven't tried integrating it into an application yet. There are a couple new changes on the master/2.x branch which haven't made it across (eg loading foreign tables), and there are some other things I want to at least look at like transactions before dropping 3.0. But the big thing right now is that all the documentation needs to be rewritten. I'm planning to put the v3 docs in GitHub Pages instead of RTD, and I've gotten at least basic API documentation done with JSDoc, but there's still just a lot of writing and template wrangling to do.. oh, cool! I've tried to keep the API as close as possible to 2.x so following the old docs and converting to promises should be what you need for most of it. If you can raise new issues for stuff like that you run into that would be awesome, and if you feel like submitting pull requests against that branch I'll happily review them.\nI do want to keep the error in this case (since if you try to create a document table but actually get an existing table with a completely different structure that leads to unexpected behavior down the line), but I'm on board with adding hasTable to the Database object. some is shorter than doing a comparison on findIndex but it looks good otherwise.\nI'm not sure what you'd be using promisifyAll for against this branch, since everything already uses promises...?. If it's on the Database prototype you can just pass the name, but otherwise yeah.. The biggest user-facing concern addressed by moving to promises internally is transaction support; it's much more difficult to manage a single connection in a pool with a callback architecture. But it's also just much cleaner and easier to work with in general.. test/queryable/find.js has an example implementation under streaming results.. Also -- document tables are still tables, and you can use the standard find, insert, update, and other functions with a document table just fine if you want to build on top of it.\n. @robconery yeah, I've only started to use the document functionality at all pretty recently. I'll take a stab at adding some tests if I have time soon.\ne: cripes, I meant options (spent the last 2 days pushing coverage, I've got tests on the brain...). Offset and limit with an enforced order by id at the very least should be easy without getting into questions of looking at the document body vs other fields in the table.\n. As it stands now, you can use order with findDoc but have to call out the field the way Rob described, using the JSON traversal operators from the body field. That might not be ideal, but it gets the job done.\nPersonally, I'm fine with that being the state of things: I've got some, for lack of a better term, hybrid document tables with a body but also various other fields, and it's handy to be able to look at those either way as the situation requires. If we tried to parse and process the order string to turn myfield desc into body->>'myfield' desc, there isn't a good way to make sure myfield is really on body.\n. Just added documentation and a testcase proving it, but yes, single works in 2.2.0.\n. oh, that part works exactly as it used to -- the body saved to the database doesn't have the pk value (_.omit(args, pkName) returns an altered copy). I needed it to stop modifying the original document I passed in since my process can call saveDoc with the same object multiple times.\n. It would be better to use a separate options object, as in findDoc etc -- it's not inconceivable that someone could have an actual field in their document named offset or limit and want to search on it.\n. aha, I didn't even think of adding another option to allow switching the order by between the document and the outside table when I was working on the QueryOptions the other day, nice work! The offset/limit parts look good, but the string parsing for sorting does seem kind of brittle: it doesn't support multiple expressions (salary desc, name asc), arbitrary whitespace between field and direction, or expressions that contain spaces (order by a + b can happen!). There's also the casting thing Rob mentioned in the original issue #233 , so this is unfortunately a bit of a can of worms!\nFor right now, passing args.options.order through unaltered and including the body ->> part of the definition from the application end is the safest way to go imo, but it would be cool to build out order by clauses eventually -- maybe from an array of name/direction/type (optional, to solve the casting problem) objects in conjunction with the external flag instead of tearing down and rebuilding a string?\n. Nope, it looks good!\n. It'd be pretty great to be able to turn on parseInt8 and get counts back as proper numbers too! Definitely agree with John that it needs to be something the user has to explicitly enable themselves though.\n. Good stuff here, thanks! I left a few comments on some implementation details :)\n. Great! Can you update the name of drop_document_table.sql so it's consistent with the new function name, and then I think it's good to go :)\n. PR made it in a while back.\n. It's kind of a glaring omission imo, so I spent some time over the weekend seeing what I could do. Postgres has a bunch of neat expanded pattern-matching operators above and beyond just LIKE and NOT LIKE (case-insensitive ILIKE, regexes, weird mishmash of SQL pattern-matching and regex) and it's a shame to leave those out of the criteria finders. It's not quite ready yet but I'll probably finish it up and roll it in sometime this week.\n. v3 (in progress!) will be using ES6 features.. I think the signature would be a bit nicer as db.docs.searchDoc({ keys: ['title'], term: 'Document', where: { is_good: true } }, { order: 'id desc' }, function (err, res) {...}); since that would let you omit the options entirely if you don't need them -- in your current version you have to include an options object if you have criteria no matter what.\nAs for the replacing, take a look at the use of offset in where.js -- if you supply an offset arg to getWhereForDoc you can set the placeholder ordinals properly the first time. If you refactor getWhereForDoc to use Where.generate instead of Where.forTable you can avoid replacing that first WHERE too.\n. oh that's a good call, nice work!\n. Is this about the 'relation already exists' error message in your comment on the other issue or is there something else you're seeing? If the former, make sure createDocumentTable is only being invoked once.\n. With the single exception of generating document tables (which have a fixed specification), Massive's design doesn't include any special functionality for modifying the structure of your database. You can emit arbitrary SQL, including CREATE and DROP statements, via db.run but that's the extent of it.\nFor basic questions, please see the documentation.\nThe specific problem you're having is likely due to saveDoc being in that for loop -- the function does create the table if it doesn't exist, but the fact of the table's existence is only recorded once it calls back; so if you're just iterating, the callbacks are going nowhere and you're working with the outdated db definition and it tries to create the table again each time. You have a couple of options here:\n- Create the table beforehand, and bulk insert the array:\n``` js\nvar items = excel.map(function (item) { return {body: item}; });\ndb.test.insert(items, function (err, rows) {...});\n``\n- Use a library like [async](https://github.com/caolan/async) to insert row-by-row and handle the asynchronous callbacks.\n. Massive only scans for db objects on startup, so if you alter your schema while it's running you'll need to reconnect to your database (with the exception of document tables as mentioned in #259 ). Doing that on command shouldn't cause any major problems since Node is single-threaded, but unless you have some scenario that absolutely requires modifying your schema on the fly it's better to have it all ready before you connect Massive in the first place so you're only incurring the startup cost once.\n. Massive's finders don't support looking at any relations other than the origin as yet, but as long as you have a standardized rather than an ad-hoc query you can put it in a view and use the finders on that just like you would a table.\n. @xivSolutions a while back I built something to decompose results of a join query into an arbitrarily-deep nested structure. I haven't merged it in because it requires really clunky aliasing with double-underscores to separate path elements (table1__table2__fieldname` etc) in the SQL script and hand-tuned definition for how to decompose the resultset in some cases, so it's not really in a publicly-releasable state imo.\nIt also needs Lodash, although I'm not 100% sure if I used anything not in Underscore (we might want to consider moving to that anyway, there's no reason to prefer the smaller library on the server side). But if there's interest in working out those other rough patches, that's one of the really complicated bits already done.\n. @quinn https://gist.github.com/dmfay/c4017b8768519679e7948706fc09532e\nThoroughly unofficial, you understand :)\n. It's a pretty long-term consideration for my part, although if someone wants to take it on I'd be happy to discuss how to fit it into the project.. Having given it a little more thought: this really isn't about providing a mechanism to emit JOIN somethingelse ON somethingelse.x = something.y as part of db.something.find(...). That's a huge departure from what Massive does; like Rob said, it's abstracting SQL instead of facilitating it. The \"Massive way\", if that can be said to exist, is to turn those multi-table queries into views (or functions/scripts, if you need to apply parameters directly -- for example with pivots you want to restrict the number of rows as early as possible, so a view is unsuitable).\nWhat I am interested in with this feature is in making the results of multi-table queries immediately useful without manual post-processing. That's what the code in that gist does: given a row set, it decomposes it into an object tree based on matching named sub-rows. The problem is that in order to use it as written, you have to define those sub-rows in your select list, and that's... probably not the worst feature I could come up with, but it's pretty bad.\nSomething that could work would be to skip the field name to schema translation from the gist implementation and pass the desired output schema in the find options. Given a table parent with a 1:many relationship to children, which in turn has a 1:1 relationship to grandchildren:\nCREATE VIEW multitableview AS\nSELECT\n  parent.parentid, parent.parent1, parent.parent2,\n  children.childid, children.child1, children.child2,\n  grandchild.grandchildid, grandchild.grandchild1, grandchild.grandchild2\nFROM parent\nJOIN children ON children.parentid = parent.parentid\nJOIN grandchildren ON grandchildren.childid = children.childid;\ndb.multitableview.find({}, {\n  decompose: {\n    pk: 'parentid',\n    columns: ['parent1', 'parent2'],\n    children: [{\n      pk: 'childid',\n      columns: ['child1', 'child2'],\n      grandchild: {\n        pk: 'grandchildid',\n        columns: ['grandchild1', 'grandchild2']\n      }\n    }]\n  }\n}).then(...);\nSample output:\n{\n  parentid: 1,\n  parent1: 'abc',\n  parent2: 'def',\n  children: [{\n    childid: 11,\n    child1: 'ghi',\n    child2: 'jkl',\n    grandchild: {\n      grandchildid: 111,\n      grandchild1: 'mno',\n      grandchild2: 'pqr'\n    }\n  }]\n}\nMy immediate critiques of this approach are:\n\nit assumes that nobody's going to want to do this with a field named pk or columns, or with a field named the same as a nested object/collection. This seems fair enough as long as it's documented.\nit's perhaps dangerously close to introducing models, and I don't like models at all :) That said, the decomposition schema is ad hoc and definitely easier than changing field names in a view, so I don't think that's worth worrying about.\n\nI'm open to thoughts on this & alternate suggestions, if anyone has them!. That's a bug!\n. This issue is closed. If one of the posted solutions for the problems it's covered doesn't fix what you've got going on and your function doesn't have variadic arguments (since there's nothing we can do about that), please file another issue and include the function definition.. whoops. Everything's good now though.\n. Sync methods are out in v3.. Not through the table API, no. With 9.5 it should be possible to have save and saveDoc generate INSERT... ON CONFLICT ($pk) DO UPDATE instead of testing the primary key up front, but since that's an existing API call we'd have to implement some sort of option to turn it on so as not to break those functions for previous versions of Postgres.\nIf you're on 9.5 and you need to upsert something with a (potential) primary key, you should be able to cobble something together using the ON CONFLICT clause in a script file. That may or may not be more trouble than it's worth, though.\n. CTEs are fine; I'm using them in several scripts. I'd bet it's the extra select * from addedRemedy that's throwing you (you also appear to be missing $2 in your values statement in the CTE). Try using a RETURNING clause instead, or wrapping the insert in another CTE with the main statement being the select from addedRemedy.. @lfreneda you may find the code I posted in the linked issue useful: https://gist.github.com/dmfay/c4017b8768519679e7948706fc09532e. Massive doesn't build paths out of nested objects (although that would be cool!). You need to use the JSON traversal operators directly eg \ndb.foobar.findDoc({\n    'body#>>{person,firstName} ilike': 'a%'\n}, ...)\nOnly the 'as-text' operations ->> (key/element) and #>> (path) are currently supported, which could make matching the array a bit tricky, but try the text field matching out first and see what you get!\n. Entities are represented as objects. You're replacing the login_sessions object with a function which returns nothing, effectively wiping it out -- the save() defined in your outer anonymous function isn't attached to anything and disappears as soon as you exit the scope of the outer function. Try this:\ndb.login_sessions = {\n    save: function () { /* your code */ },\n    find: function () { /* etc */ }\n};\n. Can't the module be included via require('massive/lib/where') already? Maybe not an ideal solution since you might not want all of Massive coming along with, but space is cheap on the server side...\n. Sure! Note that I'm currently working on a rewrite of that module though so it's going to change substantially in the near future.. Postgres uses 64-bit ints by default including for count, but JavaScript only supports 32-bit. Since we (and the node-pg team) can't guarantee that nobody's database will need the extra int space, int8s get converted to strings and the job of parsing it is on the user.\nIf you're running 2.4 you can pass the parseInt8 connect option like so:\njavascript\nmassive.connect({\n      connectionString: helpers.connectionString,\n      defaults: {\n        parseInt8: true\n      }\n    }, ...);\nOtherwise use the driver-level option detailed here.\nAlso, the sync functions are effectively deprecated at this point and will be gone in 3.0, so unless you're just playing with the REPL it'd be a good idea to move to the standard versions.\n. You're using users and Users interchangeably -- are you sure all your references are correct?\n. Closing since sync methods will be deprecated in v3.. Very cool! Couple of comments:\n- invoke should bubble the result errors instead of throwing. The throw for trying to stream is one thing since it speaks to a problem with how the function is being invoked in the first place, but once it's actually executed we defer to the client's error handling.\n- As far as I know there's nothing in the way of supporting domain, enum, or range types (d, e, & r) in addition to base types.\n. Code looks good, can you add some documentation to functions.md under 'Function Invocation'?\n. that works, thanks!\n. Looks like I gave you a bunch of conflicts from revising test setup and teardown -- at least now you can just define a testing schema for your stuff and not have to worry about editing other testcases!\nSome of this is a bit primitive, yeah. It'd be cool to build wrappers instead of requiring users to mess around with pgClient themselves, but that's fine for a non-public API. The big sticking point for me is spinning up a whole new connection pool: I'm not sure there's a better way until promises come in (which afaik is still more or less in limbo), and you've lightened up the connection load as much as possible by passing in the existing resources, but I'm just generally a little wary of the approach. @robconery or @xivSolutions probably know more about node-pg's connection mechanics than I do so could say better whether it'll hold up.\n. Fix the conflicts first up; functionality-wise, the only critique I really have is that I think it should throw in newDb.connectionWrapper instead of falling back to oldHandler. If initializing the wrapper failed I'd want to know about it, instead of it quietly running whatever I wanted it to run with alternative settings. Especially given the intended first use for RLS -- when those settings mean you're running a query with different privileges, that could be a big problem.\nI'll merge with those taken care of. Pull requests against the promises branch are welcome too! All my free time for the next little bit has gotten sucked into preparing for a talk, but I've made a ton of progress towards 3.0 there. It's strictly in \"tests pass\" rather than \"dropped into a moving system\" shape, but once I figure out how I want to generate API docs and rework the handwritten documentation it should be basically at parity with 2.6, and then the fun really starts.. @benjie @jmealo I've just switched master over to v3 development. I'll still merge this if either of you would like to continue it, but you'll need to change the base branch to v2.. @sammoore worth noting that pg-promise added support for multiple connection pools which could be helpful doing something like this. I did a little experimenting with proxies to try to invoke the same attached objects using different pools in the pgp-beta branch, but haven't gotten too far with it as yet.. Closing since this has been inactive for ages, but anyone interested in getting this functionality into v2 should feel free to open a new one.. You add var self = this; at a few points and don't seem to use it, and I think you can take away the 'would love to be refactored' comment on walkSqlFiles! Other than that I'm good, but since the behavior when loading files is different I'll defer to Rob on pulling it in.\n. @benjie I like where you're going with this, but my first priority right now is wrapping up v3 so I'd rather not pull anything big into master in the mean time. If you have the time/inclination do check out the promises branch, though!. Closing out legacy PRs.. Not sure you'd need an explicit transaction since you can pass multiple items in the VALUES list -- but concatenating thousands of strings might drag. That sounds like it'd be a problem either way though, maybe something with COPY from stdin? ref https://github.com/brianc/node-postgres/pull/232\n. I'm interested in helping out on v3 too, and I do have some time to work on Massive. Do you guys have a roadmap anywhere or is it all ideas and the (abortive?) bluebird branch at the moment?\n. How/where do you all want to have those discussions? There're a couple of items I have in mind but I don't know if github issue threads really lend themselves to mapping a variety of topics out like that.\nWe've been using ES6 promises for a while now (with a custom wrapper to promisify Massive methods on demand) and it's been working pretty well so I definitely agree there's not much call for a thirdparty library there.\n. All done as of #428!. You can exploit Node's module caching by installing and requiring the proper version of pg directly before you require Massive (or any other module that itself relies on pg). That way, when you do require('massive');, it picks up the already-instantiated pg object from the cache. I (ab)use this to profile queries in dev with pachelbel.\nFormalizing this by letting you pass an already-initialized driver in would definitely be cleaner, though!. v3 will be using pg-promise and exposing the pg object.. What does your update statement look like? With a compound key you might need to use the longer signature -- db.foo.update(criteria, changes, callback); -- instead of the shorter version.\n. No further activity.. No activity; also, I'd want to do something like this with a trigger function to ensure all modifications are accounted for and that's borderline in terms of scope. Might be worth revisiting \"extended\" document tables later.. The point of document queries is that you just get the body as a single object without metadata (save the id field, which isn't really negotiable). I can definitely see the metadata being useful, though, but we wouldn't want to risk clobbering a created_at field that already exists on the document body.\nfindDoc and searchDoc can both already leverage the query options object, so that's the obvious place to put a flag to pull in metadata, but the big problem then is getting rid of it when you go to save since we don't want to persist it both in the table and the document body. It might work better as a startup option alongside the table/schema filters so that metadata fields are always added and removed consistently.\n. It sounds like you should be using the normal table find methods, then. Those still work on document tables -- the purpose of findDoc etc is to streamline things when you only want the document body.\n. Saving unwanted metadata in the doc body is the problem there -- it'd have to be something set on the table itself so it's always added when you use a doc finder and removed when you persist.\n. Closing due to inactivity; it does bear mentioning that find can hit the GIN index eg: db.doctable.find({'body @>': {field: 'value'}}, ...); to pull back the entire row, which avoids the question of managing metadata in and outside the document.. When you're connecting and inserting a row with the trigger firing successfully, is it as the same user you're connecting Massive with? Nothing looks wrong to me in what you've posted (and I'm using BEFORE UPDATE triggers with Massive myself) so the first thing I'd check is permissions & ownership.\n. oh wow, thanks for this! It looks good functionally, but the naming 'inherits tables' or 'tables inherits' probably parses better in French than in English -- could you change the names of your vars & load function + script after 'descendant tables' instead?\n. The documentation is out of date -- there's a setAttribute (id, key, val, next) function which uses jsonb_set to update.\n. I added some more information to http://massive-js.readthedocs.io/en/latest/document_queries/ but feel free to update the API docs too!\n. yes, please do. In the mean time you could use a script or function, if you don't need to be flexible with what you're setting.\n. I'm sticking a pin in the document stuff until promises are done but I'd like to keep the number of functions down. I'm not sure it'll be too bad after that though since you could Promise.all() and map your array.. The short answer is 'no'. If you supply criteria without the primary key there's no guarantee they match at most one row (barring a unique constraint, but that's its own special case). It's why the existing upsert functionality for regular tables in Massive is predicated on the presence or absence of the primary key in the criteria, and why Postgres' native ON CONFLICT clause has similar restrictions.\nThis principle can be worked around in specific situations, but there's no general solution -- or rather, the general solution is \"know your primary key, if it exists\". If the document you may have created was inserted through Massive, the document returned in the callback will have the primary key.\n. I've been holding off on doing much until the Promises implementation starts to take shape -- not a lot of point in getting into something big when a change on that scale is imminent. But this, yeah, this is a problem.\n. crap also needs to have an explicit primary key (with the PRIMARY KEY specified in the create statement) and be visible to the user you're connecting with.\n. That's not Massive, it's Postgres. Look up the table in psql or whatever tool you use -- your field's actually firstname rather than firstName, since Postgres automatically fixes cases and then is case sensitive when you try to query it.\nYou can \"quote\" the field names in your create statement if you really want them to be cased like that in the database, but standard practice for Postgres is snake_case and given the sensitivity it's generally a good idea to go along with it. There are conversion modules like humps which would take some of the sting out of converting objects but it'll still be a bit slower to do that, of course.\n. Massive is just a data mapper and uses pg to actually talk to the database; there's a pg-camelcase module that hooks into the driver to convert casing, but I don't think there's an easy way to use that with Massive since the pg dependency isn't exposed anywhere.\nI just turn off camelcase in my jshintrc to shut it up and deal with it looking a little funny :)\n. I was just waiting for Travis to finish before merging it :)\n. There we go!. Couple questions:\n\n\nWhy are you generating one stats table per user? If the idea is that it's got a \"key\" and a \"value\" column so you can change which statistics you're collecting on the fly without a schema update, consider using a JSON field in your users table instead. If you're aggregating historical data, look into a materialized view on a single stats table with a version column. It also bears thinking about how you'd calculate systemwide stats (eg total unread messages, or something): it's very difficult with your data spread across an arbitrary number of tables, very easy if it's all in one place. There may be reasons to have one table for each row in another table, but it's worth looking into alternatives when possible.\n\n\nAre you actually looking at a level of throughput that requires deferring writes? Postgres isn't designed for superhigh write volume on the order of something like Cassandra, but it can keep up with quite a bit. If realtime writes to the database are in fact a bottleneck... things get weird. COPY FROM file/stdin into a temporary table, then UPDATE FROM into your actual table? You'd have to have a single statistics table (or a single field on your users table) to get away with that though; even if you got something working with your multi-table schema, you'd still have to iterate the incoming data to figure out which ones to update, so you wouldn't be saving anything.. In short: you don't. Even the raw query you posted is iterating over the temporary object created by VALUES; it's just performing the iteration as close as possible to the data to minimize roundtrip time. This can't be abstracted out without either increasing write time or doing redundant work to batch the data. The work must happen in the database engine in order to realize best performance.\n\n\nUsing COPY FROM to stream your logfile into temporary storage and updating your actual table from there like I suggested would keep the iteration in the db engine and leverage your PK/FK indexing. You should be able to do that with a script or database function, since Massive lets you invoke those directly. But as John said, any solution that involves using a data mapper or an O/R mapper to build and emit an UPDATE statement or statements will be suboptimal: mappers are a 99% solution, and this is a 1% problem.. You're absolutely not the first person to run up against something like this, but that doesn't change the fact that mappers simply aren't the tool for this particular job :) For COPY, look at the (extensive!) Postgres documentation for whichever version you're using.\nNot knowing the particulars of your situation, I would update on message unless load testing at the scale you're planning for shows noticeable performance degradation compared to not updating on message; there's little point in solving problems you don't have. But I'd also think about where and when the stats you're collecting are used, and just how current they need to be, given you were already willing to sacrifice realtime calculation for throughput. Could you get away with simply refreshing a materialized view from an independent daemon or scheduled task? If they're only visible on request, are they simple enough to calculate on the fly without storing anything at all?. A materialized view is a view (in other words, a prewritten SELECT query) whose results are physically stored and must be manually refreshed. So it's not realtime, but it's faster to query than a comparable standard-issue view. Won't help you if you're not storing message history, though.\nIt's honestly sounding like you should look into Redis or something along those lines as a transient store for messages (especially with the ability to set a time-to-live on insert), and update stats in Postgres as necessary from messaging event handlers and so on. If you know the primary key, updates are dirt cheap. I wouldn't worry about that until you're in real danger of becoming the next Twitch.\nFuture-proofing is great but it's necessarily an unattainable goal. What you should be trying to design is something that meets your current needs as simply as possible, scales up to a reasonable degree, and is then easy to expand or replace once it reaches its limit. What you've got sounds like a complex system with a lot of moving parts, intended to handle far more than you're going to be throwing at it for the foreseeable future -- overengineered rather than future-proofed. Build something simple and sufficient; if it does exceed your designed capacity and you need to go back to it, that's the best possible problem to have.. Nothing complicated: when someone enters a message, the body goes to Redis and you update their row in the users table in Postgres. A script or function would be best for this since that way you don't have to retrieve the original count first just to increment it.. No -- Redis would contain your messages, replacing your logfile (transient data). Postgres has your users and statistics (permanent data). Adding a new message becomes a two-part process where, instead of appending to the log, you insert the new message into Redis and update the statistics in Postgres.. The point of using a function or script to update the sent count is that when a message is sent, all you have is the user id. You could issue a SELECT query to retrieve the statistics, add 1 to sent, and issue an UPDATE to persist it; or, you could invoke a function which emits UPDATE statistics SET sent = sent + 1 WHERE user_id = $1 and thus increment the count with one roundtrip to Postgres instead of two.\nThe statistics stay in Postgres; Redis in this scenario only exists to store messages.. It's here: http://massive-js.readthedocs.io/en/latest/functions/. Caching is a problem you can address later, this is about storage. Put your messages in Postgres if you want to keep them permanently (you could clean them up on a schedule or something, but that's extra work); put them in Redis with a TTL if you want them to expire automatically. The latter scenario seems more like what you want. And either way you'll update the stats in Postgres as you add a message.\nYou shouldn't use the log file as storage at all -- the reason we have databases in the first place is that flat files aren't very good at it!. Multi-row updates can't apply different data to each row.. You can effectively join a table in SQL and update by key, sure, but this was about going through the table API in Massive, which doesn't do that. The original problem was settled by reframing the question in a way that obviated the necessity of the multi-row update entirely, so I don't really see what's to be gained by dropping in a month later to quibble over minutiae.. That all said (& it is a cool feature!) @Aedaeum I would still caution against using it as you originally intended to -- the deferred-write architecture you came up with is much more complicated than you have a demonstrated need for, and that's all too frequently the kiss of death for software projects.. Yeah, prepared statements don't work that way. The query engine just takes the parameters you give it and binds them in the appropriate places, it can't expand one parameter into several. With Postgres you can get around this by using the ANY array comparison operator instead of IN.. Like you'd expect: write your statement ...WHERE field = ANY($n), and make sure $n is an array of primitives. It's not a special case except in that it's a Postgres extension on the SQL standard.. Not now, and, I'm afraid, not ever. Prepared statements bind parameters rather than interpolate them (and with good reason -- the latter is how you open yourself up to SQL injection!). The only halfway realistic implementation involves sorting the final result set outside the database, which would be terrible for a whole host of reasons.\nThink about using a view instead, if possible. Massive treats those identically to tables, save of course that they're read-only, so you'd be able to pass sort options to db.myview.find().. You're not missing anything: creating a camelcased document table is not currently supported, and the document table functions do not delimit the table/schema names to enforce casing. We should be doing that as a matter of principle, but in the mean time given the headaches associated with cased names in Postgres I'd recommend using the standard snake_case if at all possible.. You can use a script file with the || concatenation, or if you have already retrieved records with find or findOne you can append the string in JavaScript and save/update.. I wouldn't expect it to, since the lock from FOR UPDATE expires when the transaction that creates it ends, and your script will be executed as a self-contained transaction.. Yes, right now you'd need a Postgres function in order to use FOR UPDATE effectively. Moving to promises with v3 will make transaction management much more doable, but that's slow going right now.. I can't commit (if you will) to having it in the release, but transaction support has been on my wishlist for a while and I do intend to tackle it at some point.. It looks like the documentation is in error -- searchDoc doesn't apply offset or limit. It probably should at some point!. This is functioning now. Added tests to verify.. JSONB array handling gets tricky, unfortunately. As far as I know for this you'd need to expand the array into a rowset with jsonb_each which is more than the query builder is capable of.. Criteria queries allow you to specify an array of sub-criteria under an or field: {or: [{flag: false}, {flag: null}]}.. We don't use Buffer anywhere, the warning is generated from one or more dependencies so there's not a lot for us to do other than keep those up to date.. At least one such usage is in the pg driver's buffer-writer dependency, but there may be others if you grep node_modules.. Massive supports integer and UUID primary key types, and right now I don't have plans to expand that. I'd caution against using something like a username as a primary key -- if it changes, you have to update all foreign keys referencing your users table, and if you miss one, you \"lose\" data until you go in and fix it. Synthetic keys make life a lot easier.. There is nothing that generates a condition with the BETWEEN keyword (yet! although it'll probably take some doing since so far it only supports binary operators). However, plain date comparisons do work. Yours look backwards -- you want to find start_dates before the timestamp and end_dates after.. Your find criteria are looking for a record with start_date >= x and end_date <= x, so the only matching records will have a start_date after/equal to x and an end_date before/equal to it, therefore a start_date after or equal to the end_date.. good catch, thanks!. Yes, but not quite in that form:\ndb.users.search({columns: ['email', 'name'], term: 'rob', where: {companyId: 123456}, (err, users) => {...});\nThe where is a standard criteria object and you can use operators, or arrays, and so on as usual.. Clever! Unfortunately, it only works well as long as there's at most one foreign row per source row; applied for instance from dealers joining cars, you wind up with a huge mess you have to process yourself. I don't want to support something like this halfway -- if Massive can do joins, it needs to be able to decompose the resultset into an array of object trees in order to be really useful.\nI already have a decomposer mostly working against specially-aliased script queries here, but I haven't pulled it into Massive yet since my priority right now is the conversion to promises. If you're interested, you're more than welcome to pick that up and run with it!. Unless you specify scripts: path/to/directory in the init block, the /db directory is assumed to be located in the current working directory. Are some students running the server from the project root and others from /server?. path.join should give you better results than concatenation if you decide to go that route, but since you're presumably not doing anything out of the ordinary with script sourcing I'd try to get a consistent layout and startup process working first. Especially important is having everyone start the application from the same location every time, since the scripts directory is located relative to the process working directory.\nThe project structure I use with Massive looks like this:\nproject/\n  |-db/\n  |-lib/\n  |-node_modules/\n  |-app.js\nwhere app.js is the module entrypoint which initializes Massive, analogous to your server/index.js. Startup is node app.js from the project directory.. Yes, with ilike or ~~* as the operation. The full list is here.. Are you still having problems? Make sure your parent table has a primary key defined; Massive ignores anything without one.. Could you create a second issue for that and include sample code?. You have to call connect again. If your schema is static, it's better to manage it separately from your application so it's ready when you start up.. Good to know, thanks!. It's in the docs.\nIf your query is static it's better for a variety of reasons to use scripts in the db directory.. Few things!\n\nI'm not sure why you changed the name -- there's nothing specific to document tables, so the original hasTable is more appropriate\nThe documentation comment mentions creating the table instead of searching\nThere's nothing async so it doesn't need to be a co-wrapped generator (the build is failing because it's a generator without a yield)\nPlease add a testcase in test/database.. 1. It's just as much a companion to dropTable as it is to createDocumentTable imo.\nAre you familiar with mocha? I've offloaded the bulk of the setup/teardown stuff into test/helpers/index.js so each test file is pretty lightweight. What I'd suggest is copying test/database/dropTable.js to test/database/hasTable.js and editing it to test your function instead. You want to assert that it comes back with true when the name is found and false when it isn't, both with a schema and without.. You'll need PostgreSQL installed, and to create a database named massive. The built-in postgres user should be able to connect through peer authentication rather than a password (or you can temporarily change the connection string it uses in test/helpers/index.js, just be sure not to check it in!). Then npm install to download the dependencies, and npm test to run all the tests and linter. You can run just your test by mocha path/to/file.\n\nMocha is fairly simple, especially since most of the boilerplate is taken care of; describe methods group the it('should...') test methods, which in turn should throw AssertionErrors on failing. Async tests should return a promise or have a it('should...', function (done) {...}) signature and invoke done on completion. Massive's tests include the chai assertion library so there are a lot of ways to express what should happen. Your tests could take advantage of assert.isTrue and assert.isFalse for example.. Looks good, thanks!. this[name] could just as easily be a view or function. saveDoc uses it because it has to invoke a function on the object it finds, but you're just looking to make sure it 1) exists and 2) is a table. You could have used instanceof to check its type, but it's kind of six of one, half a dozen of the other imo.. If you want to refactor that, sure!. You need to add this repo as another remote and pull from it:\ngit remote add dmfay git@github.com:dmfay/massive-js\ngit pull dmfay promises. Thanks for going to the trouble on this! yield is nonstandard, unfortunately; tons of people use co but it's not something I want to assume. Ideally await would have dropped earlier, but as it is substituting that would also leave a ton of users behind. .then(...) is annoying but it's the formulation most generally compatible.\nIn your connection examples it looks like you're storing the connection promise rather than the connected database, which you get by yielding whenever you query. That's suboptimal; better to massive(...).then(db => { app.set('db', db); ... }).. I meant 'suboptimal' in terms of it being an extra step to go through; as far as I know, you're correct that the performance implications are negligible. Since you're already storing something for later retrieval, there's no reason for that to be anything other than the database driver itself instead of the promise. You just have to make sure you listen or what have you in the promise callback or after yielding/awaiting.. There's nothing to app.get before the server starts listening. You can take advantage of this by listening at the end of an asynchronous process:\n```js\nconst app = express();\nconst server = http.createServer(app);\nmassive('').then(db => {\n  app.set('db', db);\nserver.listen(3000);\n});\n```\nYou can of course connect in another function or another module so long as you return a promise from it and listen in the promise callback, but you shouldn't need to import it anywhere else in most cases -- that's why you app.set and app.get it in the first place.. That's exactly the kind of special case I was thinking about :) There's ways to go about that (personally, I'd question whether it belongs in the webapp module or should be broken out into a separate process) but the readme needs to target a simple, general case.. Validation is a huge, huge can of worms in terms of project scope. Fundamentally it's not what I envision for Massive: its purpose is to move data around, not to define what that data should be. That's why it introspects your tables instead of making you write models, for example. Protecting the user from her- or himself is a job for a different tool, and there are plenty of feature-rich validation libraries to choose from (shameless plug -- I even wrote my own at one point!).. Yeah, that's also something you should be doing in client code if you need it imo.. Thanks for contributing!. There have been some other fixes and stuff waiting in the wings for a while so I went ahead and released it as v2.6.1 just now.. Thanks!. Thanks! I've been focusing more on getting things working with a similar API than really diving into pg-promise (or, apparently, going back to do path concatenation right) so I'm definitely open to suggestions on how to do more with the driver.. Yeah, there's definitely some stuff left to clean up from experimenting with things, which is part of why I haven't actually cut the 3.0 release just yet. I did get into the query formatting a bit -- you might notice some tests with run and functions/scripts using named parameters, although that's really just scratching the surface.. I wanted another pair of eyes on the docs before I cut a new tag, just to make sure everything in there made sense. Someone was gracious enough to look them over this afternoon, so I'll be doing that in a moment here. Thanks for checking in, and please don't hesitate to reach out if you run into anything!. It's alive!. Short of evicting the module from Node's cache you don't, really. But if you're just trying to run the introspection to load tables/views/etc again, there's a function for that now: db.reload().. You're welcome!. Synchronous functions are no longer supported, and the connect function itself no longer exists -- it's all promises all the way. Connection docs are here.\nIf you can use async/await that's an excellent way to manage promises, or there are libraries like co which let you yield them.. That's for version 2.x, yeah. I'll see about adding a disclaimer on that side of things. Documentation for v3 is on GitHub Pages.. npm i [-g] massive does actually install 3.0.0-rc1; any particular reason behind this?. Thanks!. oh, very cool! That's probably worth including in the release too. I've been trying to do something weird with proxies for the connection pools -- mixed results so far :). I've got a pgp-beta branch where I'm playing around with 6.x, yeah. I thought the least obtrusive way to handle multiple pools would be to treat them similarly to schemas: db.highPriority.myfunction(...), db.lowPriority.mytable.update(...), etc. Since that makes managing attached objects messier, I've been looking into using es6 proxies to forward invocations off the pool object back to db instead.. Just Twitter, when I dropped the release. Looks like you had the same thought there!. Good question! I don't know either :). node-pg's promise support was cumbersome at best up until about four days ago, which would have made integrating it a hassle for no good reason. And pg-promise does more than just facilitating communication with the database: it handles query file management, named parameters, tasks, transactions, and so on and on. These are all things I could write myself -- eventually -- but Vitaly's implementation is at least as good as anything I could come up with. If I don't waste time reinventing the wheel, I can focus more on improving Massive itself.. yeah, multiple databases = multiple instances of Massive for now. I need to get back into that proxy stuff I was trying out for juggling connection pools.. I've barely had time to think the last couple months let alone try to keep pace with you. Busy busy :grimacing:. congrats, have fun at the new place!. Thanks! As far as upgrading: the pg-query-stream reversion to the classic style doesn't look like it's going to be changed anytime soon, and I don't want to bump to v5 over streams. Other option is to convert it to a modern stream in Massive, I guess, but that makes me itchy. I'll have to think about it.. Thanks! :). yeah, I'm going to have to think about that -- inattention to semver has also been an issue :/. Massive loads query files in the /db directory, but right now it's not using pg-promise's QueryFiles to do that -- just plain old fs.readFile. That can change though, and it sounds like minify should take care of it!. Thanks, I'll take a look at that in a little while.. Overloading tables with scripts like that -- having both a company table and a db/company scripts directory -- isn't supported, although it looks like I missed bringing that note over from the v2 docs. So the short answer is \"name your company scripts directory something else\" :)\nThat said, your example seems pretty reasonable and it should be possible to handle overloading for attach/detach to compose and decompose objects (what's happening in your 'failing case' is that the company script namespace resolves first, but then the company table is constructed and replaces it). If you're interested in implementing it yourself, I welcome pull requests; otherwise, I'll put it on the docket.. The relevant code is in lib/database.js, specifically the functions attach and detach: https://github.com/dmfay/massive-js/blob/master/lib/database.js#L55\nattach should test whether something already exists on the instance at spec.path and behave accordingly, adding scripts to an existing Queryable/Table or replacing script namespaces with a new Queryable/Table and adding the function executors back depending on what the 'something' is. Since reload introspects tables, views, functions, and scripts in parallel either situation may occur.\ndetach might not even need modification; it's not really intended for public use so I don't think we need to be super-strict about decomposing entities piece by piece. Definitely test that out rather than just taking my word for it, though!. I'm happy to hear it's doing well by you! I've thought about expanding that part of the docs into a sort of \"cookbook\" demonstrating common usage patterns; I still might, if I collect a few more. However, I don't want to introduce framework-specific functionality into the core library: if I start explicitly supporting frameworks, each of those modules is not only dead weight if another (or no!) framework is in use, it's another dependency to keep up with and test.\nYou've probably got more going on than just Massive -- maybe look into extracting all your app.uses and app.sets out into a directory of middleware modules. From there, you can use something like require-all to pull those modules into an array (you'll want your modules to export a priority to sort the array on too) and apply everything to the application object during startup. As an added bonus, this makes testing custom middleware much, much easier.. @dotob wow, which version of Massive did you upgrade from? Materialized views have only been present in Postgres since 9.3 so it makes sense that the query which looks in pg_matviews wouldn't work on an older server version, but that query was introduced in v2.2.0 almost two years ago.\nAdding the option to disable it should be pretty straightforward, at least.. @dotob massive(connection, {excludeMatViews: true}) should take care of that if you pull from master. I don't have easy access to an 8.3 instance, so let me know how it goes! If there aren't any other legacy issues I'll cut a release.. v4.4.0 is out, I was just waiting to see if anyone ran into other issues with legacy Postgres instances.. Materialized views are really useful. Unlike normal views which run their stored query every time you select from them, materialized views cache their results so you can get near-instant reads from queries that usually take a long time to run. Of course there's a catch: you have to refresh the data asynchronously, so you have to be willing to read potentially stale data.\nIf you don't have or need any materialized views, you can turn the feature off, but it's not really hurting anything. The only real reason for the toggle is because people are still using older versions of Postgres which don't have materialized views, therefore no pg_matviews table, therefore the query that tries to pull information from pg_matviews throws and they can't connect.. thanks!. Yes: db.mytable.insert. save is predicated on the assumption that you autogenerate primary keys.. I exposed that property and documented the pg-monitor setup after the 3.0.0-rc1 release -- if you pull master instead of the RC release, it'll work, or if you'd rather wait a bit I should have v3 proper out within the next couple days. I'll look into separating the doc build process so they'll track the latest release instead of the repo itself.. That's all taken care of.. I left those alone because they didn't face client code, but I suppose I might as well go for it. Thanks for adding .all!. no, that was kind of clunky -- thanks!. save is a very (very!) thin wrapper on update and insert, which is also affected -- given an object it returns an object, given an array it returns an array. This is how it worked in v2, and from a convenience perspective it's really annoying to have to unwrap an array after giving Massive a single object to write. So while I'm sympathetic to the consistency argument I'm inclined to call this a documentation error.\nI've never worked with TypeScript, but it looks like it's possible to declare multiple return types for a function; would something along the lines of insert(data: any[] | object): Promise<T> | Promise<T[]>; be appropriate?. I've corrected the documentation and raised an issue with DefinitelyTyped.. If it were just using default parameters I could see expanding backward compatibility, but I'm looking at doing stuff with proxies and there's no way to make that work with anything older than 6.x. If that doesn't pan out I'll take another look at it, but I don't want to add 4.x compatibility only to take it out again shortly afterward. In the mean time, I've updated the readme.\nNode does move quickly but 6.x is over a year old and two major versions behind current at this point, so I don't think it's that big a problem.. Make sure the postgres user has a password (it is locked down and doesn't have one by default) and that you're providing the right one to Massive, or create another user in Postgres to connect as and use its credentials instead. The latter course of action is much safer -- connecting as postgres is like doing everything as root.. Good catch, thanks! I'm going to remove the 1 to be as clear as possible -- the id in this example is never specified.. No. If you're using Node 7 or 8 you can use async/await, or otherwise a flow control library like co to manage promises with a synchronous idiom, but it's gone for good from Massive itself.. You'll want to pass a connected instance in, add an asynchronous init function to your repository, or since it's just the connection, deasync it yourself.. No need for an issue, and the changes look fine to me. I'm a little curious why the distinction matters, though -- shouldn't the jsonb_path_ops index cover nulls? Or are you using this for json fields outside a document table with that index?. I see, thanks for the explanation! Do you have any interest in porting it to v3 too or should I put that up on the todo board?. I have to admit that issuing DDL on init was not a use case I'd anticipated! reload is indeed meant to pick up structural changes without a full teardown, so you're doing that right and Massive should use the QueryFile autoreload instead of reinitializing everything like it does right now. But it does sound like you might want to investigate a more formal db setup/migration process if you're currently unsure whether your structure will be there or not when you start the app.\nQueryFiles persist even after a connection close so disconnecting wouldn't help with the warning. It isn't really hurting anything right now, but the instance passed to the loader has a collection of executables so that should help with reloading existing files in place.. You're doing it right (this is the relevant page in the docs). Two questions:\n\nWhat is \"doesn't work\": no results, an error? Is there a stack trace?\nDoes your table have a primary key defined?. Looks like that was the only one, & that's taken care of.. My first guess was something all the way up the chain with node-pg, but no, this works:\n\n```\nconst { Client } = require('pg');\nconst client = new Client();\n(async () => {\n  await client.connect();\nconst res = await client.query('insert into bigints (one, many) values ($1, $2)', ['111', ['222', '333']]);\n  console.log(res);\n  await client.end();\n})();\n\nLOG:  execute : insert into bigints (one, many) values ($1, $2)\nDETAIL:  parameters: $1 = '111', $2 = '{222,333}'\n```\nA prepared statement from psql works too:\n[local] dian#dian= prepare ins (bigint, bigint[]) as insert into bigints (one, many) values ($1, $2);\n[local] dian#dian= execute ins('111', '{\"222\",\"333\"}');\nLOG:  statement: execute ins('111', '{\"222\",\"333\"}');\nDETAIL:  prepare: prepare ins (bigint, bigint[]) as insert into bigints (one, many) values ($1, $2);\nwhereas from the Massive REPL, going directly to pg-promise doesn't:\ndb.instance.query('insert into bigints (one, many) values ($1, $2)', ['111', ['222', '333']]);\nLOG:  statement: insert into bigints (one, many) values ('111', array['222','333'])\nERROR:  column \"many\" is of type bigint[] but expression is of type text[] at character 48\nHINT:  You will need to rewrite or cast the expression.\nSTATEMENT:  insert into bigints (one, many) values ('111', array['222','333'])\nIt looks like pg-promise is formatting array values for prepared statements here, in a way that Postgres can't automatically coerce to the correct type.. @tamlyn I missed your comment about using options to allow casting per-field -- that'd be neat!. Yes I would! Boolean options are preferable; onConflictIgnore and onConflictUpdate, maybe?\nI am actually in the middle of some reorganizing around statement generation so you may want to hold off for a day or so to avoid a different kind of conflict :). @tamlyn I just merged the big statement refactor down, so you're good to go on this!. Massive doesn't have a \"roadmap\" as such. The goal has always been to cover ~90% of day-to-day database usage and make doing the remaining 10% with SQL as easy as possible; there's not much to get all project management over since that goal's been comfortably met for some time, and everything else is, functionally speaking, gravy. If you'd like to take a crack at generating a proper ON CONFLICT DO UPDATE clause, feel free -- I'm happy to talk over ideas and pull requests! Otherwise it'll happen when it happens.\nCan you open another issue for your custom id problem and clarify it a little more?. Your payload is fine; that's a bug. Easy enough to test for; I'll take your ideas for fixing it under consideration :). I've just published v2.7.1 with a revision of the criteria key parsing logic. Give it a whirl and let me know if you run into any problems. I'll be porting it to master shortly (thinking about simplifying JSON traversal to column.path.to.json.field instead of making users deal with ->> and #>>), since a similar logic change happened in v3 as well but hasn't been publicly released yet.. oh, good thought! If you want to go through some of it and put up a pull request that'd be great, or I'll get to it in a little while.. I've actually been thinking about trimming the readme down to remove most of the detailed documentation, since having that in two places is more work than it's really worth.. Something like that, yeah. I'm not just going to truncate it with a link to the main docs :). Variadic functions aren't supported, unfortunately. It looks like it's quite possible by detecting pg_proc.provariadic on function load and accommodating it when executing, it just hasn't been done yet.\nYou can achieve a similar effect in the mean time by accepting a text array in your function instead: CREATE OR REPLACE FUNCTION public.get_car_tags(car_ids text[])\nand invoking the function from Massive with a doubly-nested argument array: db.get_car_tags([['1', '3']]);. I'm going to keep this open, actually -- I use issues for a feature todo list, or if someone wants to jump in and implement it themselves I'll be happy to look over a pull request.. Completed in #444. It can use a connection string but you do lose the ability to specify pool size.. bigint/int8s I'm guessing? Yes. JavaScript Numbers only go up to 53 bits, so the driver returns 64-bit integers from Postgres as strings. You can use pg-types to override this behavior if you know you're never going to need numbers that big.. Bigints turning into strings was the undesired behavior here and the fix involved using custom parsers to ensure they stayed numeric. Could you please clarify?. That is weird! However, this testcase works fine against v4.2.0:\n```\n  it('changes types', function () {\n    const stringCount = yield db.run('select count() from products');\nassert.typeOf(stringCount[0].count, 'string');\n\ntypes.setTypeParser(20, parseInt);\n\nconst intCount = yield db.run('select count(*) from products');\n\nassert.typeOf(intCount[0].count, 'number');\n\n});\n```\nso I'm guessing it's something in your code. Are you perhaps changing the type parser before you initialize Massive? I have no idea why that would have worked prior to v4.1.0 but I think it'd explain it not working.. Have you tried rm -rfing your node_modules and package-lock.json (if you have one) and reinstalling your dependencies? I'll occasionally edit something in there and forget to change it back.\nIf that didn't or doesn't change anything, you could try cloning the repository and seeing if you can run that testcase on your local system just to rule out anything exceptionally bizarre.. oh wow, that's a new one :rofl: At least it's working!. docs/persistence.md under insert, and please add an entry to the table in docs/options.md. Don't worry about the README, I've been trying to trim that down and it really doesn't need to be exhaustive. The change looks good other than the documentation!. oh nice work, thanks!. Thanks for adding this! Can you create a test or two in test/queryable/where.js to validate the new functionality?. Closing in favor of #455.. I'm not sure I'm following your process completely. I know pg_temp is per connection; are you connecting Massive without a pool, creating structure through scripts, and using reload to scan it all in?\nI'd be open to a config option to set the default search path and override public, but I don't know if that's what you intend.. ah, I haven't had to do much with search paths, I didn't realize it could be set more permanently. We should respect the db/user setting, so that sounds good!. Thanks!. This turned out to be a documentation issue.. Yes to more tests, but it's looking good so far!. I'd prefer not to touch the built-in role (if a test blows up and search_path isn't unset things could get weird) and setting poolSize is doable. You can connect in a testcase with whatever parameters you want, as long as the test and the before kill the pool to avoid duplication. Check out the connect tests for examples.. That's a Postgres error. You can try preparing the statement yourself in psql: prepare mystatement as select... will raise the message you mention.\nIf you switch the order around to where c.hostname = $1 and (ag.id = $2 or $2 is null), it's able to infer the type.. whoops! I'd been watching that issue in pg-promise and didn't see this before I bumped the version myself :). I published 3.2.1 two days ago :) The version bump & release don't appear in master because that's got breaking changes toward v4, it's off a v3 branch if you look.. awesome, thanks! & yeah I should go clean up the ^s. Theoretically breaking changes shouldn't happen with minor versions but I can't exactly cast the first stone with regard to rigorously adhering to semver either ;). 3.2.2 is out!. Looks good to me, thanks!. This looks fine, thanks for contributing! Re documentation, I've basically automated it with npm scripts. build/apidocs.sh runs before every version update. If you just want to see what the JSDoc you write will look like, you can do this:\n```\nrm -rf ./docs/api\njsdoc -d ./docs/api -c ./.jsdoc.json -r\n```\nRemoving that extra check sounds good too, I didn't realize node-pg had made it redundant.. I don't want to leave you hanging here but things have gotten super hectic for me. With any luck I'll be able to take a look at this over the weekend and figure out what's going on with the duplicate QueryFile warnings in the test output; they're not really worrying, but I'd definitely prefer not to have them. Regarding the docblock, have you tried using the @module directive?. I figured out what was going on with jsdoc -- helps if your class is named the same thing as the file it's in! I didn't have much luck with the warnings but that's no big deal.. I want to get #440 done first so all the breaking changes come in at once, but that shouldn't take me long at all. Look for it in the next day or so. If you want to get a head start, you can track master in your package.json.. This one is, but I've been working on stuff that isn't -- right now master contains major changes to traversing JSON fields that aren't compatible with v3.. v4 is taking a little longer than I anticipated, so I cherry-picked a bunch of stuff including this and released v3.3.0.. Good catch, thanks! One small tweak: please use === in the test for null instead of ==. I should probably have that in the linter config.... ah, gotcha. Early in the morning here. Thanks for the tip about the rule config, I didn't know about that.. I should say I'm not by any means dead set against using ad-hoc schemas as in my example for #263. I'd prefer to avoid it if possible, of course, but easy decomposition is so much more useful than strict adherence to no-independent-schemas-not-ever that if it's a choice between the two the first one wins hands down. What I don't want is a solution that requires handwritten SQL; I think that really limits how much use people will get out of it. Something like this needs to be accessible when eg using db.find against a multi-table view in order to be most effective.. I wound up doing more or less what I was talking about in #263. options.decompose takes a schema and will cut your resultset to fit.. Here's why it's an issue:\n\nQueryFiles are cached at the driver level, and it warns if you reload the same file unnecessarily. You can ignore warnings if you prefer by passing a driverConfig as the third argument to the Massive constructor.\nYou're connecting with a connection string; in v2, I can't remember whether that created a pool for you. If it didn't, then your connection would have been slower and prone to blocking but you wouldn't have run up against the limit so quickly. Or possibly you just have more repositories now. Either way, this is a Postgres limitation (look into the max_connections setting) and a Postgres error message.\n\nBottom line, Massive is meant to be connected once and for everything to route through a single instance. You can still use the repository pattern, but all your repositories should be using the same Massive object instead of maintaining their own. I don't know if you can create a static property on the parent class with TypeScript, but I'd look into that.. When you're dealing with fixed points in time, you should always use timestamp with time zone aka timestamptz. This StackOverflow answer goes into why.. ah, that's a bit of a tricky one. save and saveDoc are not true upserts in that they insert the record if the id value is not found and update it otherwise. Rather, what happens is that these functions check the data you provide for a primary key value to decide whether they should emit an INSERT or an UPDATE.\nSince you're passing a document with id 45, saveDoc recognizes that the primary key is set and emits an UPDATE; but since the corresponding record # 45 does not exist in the database, obviously no rows are affected. How are you generating your id? Stock document tables have a serial primary key so it will be autogenerated.. Looks good, thanks!. yeah, that's too fragile. Thanks for the report!. That actually isn't supposed to work! As you've noticed it leads to inconsistent behavior, which is why if you try to INSERT INTO mytable (id, name) VALUES (1, 'alice'), (2) in psql, Postgres will abort with an error telling you the VALUES lists must all be the same length. You'd have gotten that error from Massive before too, since it just concatenated the object values arrays. With the fix for #465 it's looking up the keys instead and generating undefined for any missing, which satisfies the length test for Postgres but will happily insert your malformed data so long as it doesn't violate not null constraints.\nIt seems like it should be possible to support merging keysets so cases like db.mytable.insert({field1: 'one'}, {field2: 'two'}) would generate ('one', null), (null, 'two') VALUES lists. I'll have to think about that one a bit.. Yeah, that's going to need fixing. I think strictly speaking the Right Way would be to record column types on init and cast, but the possibility of custom types makes me a little nervous about that approach.\nThere is also a simpler workaround than what @vitaly-t suggested in the other ticket, btw: just insert {array_field: '{}'} and Postgres will parse it :smile:. I mean, you could add a fallback to the legacy {} yourself and I could take it out of Massive, but it's kind of six of one half a dozen of the other :wink: An introspection-based solution would have to be Massive anyway, but right now I think that's more trouble than it's worth.. I was able to run the tests locally when I pulled it down, let's see what happens here.... go figure! If I had to guess I'd pin this one on Greenkeeper -- it just tweaks the version in package.json and leaves the lockfile alone, so the first failed build would have been trying with pg@6.4.2.. I think it's because I'm using pg-query-stream directly since there wasn't any streaming support in pg-promise until now :wink: I'll check it out later.. For certain definitions of \"luck\", yes :smile: Obviously if I test for classic streams I get data out the way I expect it, but that's a pretty huge step backwards for the API.\nOther than waiting my options are to take the hit and go to v5 already (semver!), to convert the classic stream to a modern readable stream in Massive, or to try to fix pg-query-stream myself and submit a PR there. I'm thinking about the last, it's the only one that makes any sense at all.. I wound up hacking around it :grimacing:. Yeah, the idiomatic JSON path processing is only in the WHERE clause generation right now. It'll be a bit messy to break that out and apply it to ORDER BY, but I've been meaning to revisit sorting for a while now. Consider it a bug at the moment, but at least the direct way still works there.. Reopening til it's fixed. I should put a note about that in the contributing guide.... Your join tables probably don't have primary key constraints defined. Some table functionality depends on primary keys so Massive ignores tables without them.. Adding a primary key is the quick fix, yes. I've found an effective approach for pure junction tables (consisting only of two foreign keys) is to use a script file with a CTE to add the related entity and the junction in a single transaction.\nThe principal use for primary keys is a tableEntity.save() function which will emit an insert or update depending on whether the object passed includes a value for the primary key. It's also used to streamline query generation when searching by primary key alone.. What does it do when you try? \"Cannot connect\" covers a multitude of sins.\nPast that, this is either an issue with the pg-promise library or with the node-pg driver. Massive just passes your connection information, string or object, up to the former, which uses it to initialize the latter.. Does the password contain special characters? That sounds like an encoding issue -- you might try running it through encodeURIComponent.. No you can't; it does create a 10-connection pool by default.. Have you tried encoding the username instead if it's what has the special characters?\nedit: oh you mentioned escaping, sorry -- busy weekend! But running that through encodeURIComponent itself is the only idea I have at this point.. closing due to inactivity. It looks like you're using Koa? The corresponding technique to using the Express app is to put it on the context:\napp.context.db = await massive(config.connection);\nThen you can have middleware/routes which await ctx.db.saveDoc and pass ctx to shared functionality.\nYou could do what you're suggesting and have a separate module which initializes and returns an instance of Massive act like a singleton a few different ways. That's only something I'd consider if I needed to access the database independent from the server application, but createTask sounds like it could be that.. @jaimesangcap at the simplest level you put your database init stuff in a separate module that looks like this:\n```\nconst massive = require('massive');\nlet db;\nexports = module.exports = () => {\n  if (!db) {\n    db = massive(...);\nreturn db;\n\n}\nreturn Promise.resolve(db);\n};\n```\nThen you get an instance in your async function like this:\n```\nconst getInstance = require('./getInstance');\n(async () => {\n  const db = await getInstance();\nconsole.log(db.tables.map(t => t.name));\n})();\n```\nStrictly speaking this isn't a capital-S Singleton as handed down from on high by the GoF but it'll do.. I would love to have runtime-configurable document tables, and a uuid key option and updated_at are only the start of where that could go. It's pretty far down my priority list at the moment so if you want to take a stab at it, be my guest!. Right now I'm adding the missing idiomatic JSON traversal to find functions' options.columns -- actually deprecating that in favor of combining new options.fields & options.exprs to avoid having to cut v5 already, since the field parser adds quotes which break scalar operations like arithmetic or string concatenation.\nNext up is revising how tables views functions and scripts are structured on the Database object to use a more compositional approach. This will allow (for example) having a company table, a company schema, and a company subdirectory in your script files all at the same time (#387) and also hopefully make some further-in-the-future things easier, like juggling multiple connection pools.. I got five people who'd never touched open source before to make their first commits breaking apart some of Massive's test schemas at a workshop just last month so no worries there :laughing: This is a little more involved so it might take a bit of back&forth to get this done but I'm game if you are.\nSetting the defaults in the database options sounds good. Overrides should happen as close to the point of creating document tables as possible, though: if you're generating document tables dynamically, the name may not be consistent or predictable. So I'd do that as another argument to createDocumentTable itself instead of in the options.. Connect and reload tests are kind of screwy because you have to have infrastructure in place to support alternate connection parameters -- just ignore those for now.. Those connect/reload tests should pass now if you pull the master branch.. We've covered at least the obvious bases here.. aw thanks :grin: It was Rob's originally so credit where it's due there, I just took over for v3 and onward :wink:. yeah, none of those are really necessary here, but thanks for the heads up!. Rolled this into the driver upgrade commit, thanks!. db.query will already do this. I need to document that then probably deprecate db.run.. all done here.. You mean for document tables? That's the only place we're creating ids that I can think of...\nIt's not directly possible at the moment, although you can certainly pre-create document tables with SQL and change the id type to UUID. If you need to do it dynamically, you could db.createDocumentTable, db.query with DDL to change the type, then db.reload. @jaimesangcap was looking at more friendly configuration including setting UUID primary keys in #501.. hi, thanks for contributing! Aliasing fields in generated queries has been a bit of a weak point so I'm glad it's getting some love. I do have concerns about the string-parsing approach, though: this code looks like it doesn't account for edge cases where field names or aliases contain whitespace or punctuation. You've also reproduced some of the parseKey logic to generate the #>> traverser arguments; I don't think this will execute the way you intended since the Select constructor passes fields through parseKey, which replaces the JSON punctuation (including for array elements) with proper traversers before anything reaches format().\nI think a better way to do this would be to allow options.fields to be an object {alias: 'mycolumn', theAttr3: 'jsonbField3.attr2[0].attr3'}. This could be processed in the Select constructor just like exprs is, except the values go through parseKey to turn them into valid column names or JSON elements.\nAlso, I do require tests for new features. Enough people depend on Massive to function reliably that \"it works on my machine\" isn't sufficient.. Closing due to inactivity. If you (or anyone else) are still interested in aliasing options.fields feel free to open a new pull request.. If you're accepting textual input directly into the order option as a string, that's an acknowledged vulnerability (also if you can upgrade definitely do so! If you pass order objects they are escaped and you can use normal JSON syntax). I've put off doing anything about it for a while now because there have been other features I've wanted more immediately and the applications I have using Massive don't allow direct input into order by design. I am open to suggestions and pull requests though!. Nothing to do here.. Published as 2.7.3 (npm i massive@legacy or npm i massive@2), thanks for catching this!. You're using flat decomposition schemas to rename resultset columns? That's creative!\nDecomposition is supposed to work for all statements, but it looks like inserts at least don't preserve that key in the options object. I won't be able to get to it until this evening at the earliest but if you want to take a shot at it before then, check out the first few lines of the constructor in lib/statement/insert.js. It should be a one-line fix to preserve options.decompose and then it's just a matter of validating that with a testcase and making sure update and delete (destroy) statements do the same.. I took a second look and it turns out it's a little more involved than I was thinking :grimacing: Try pulling the decompose branch and see how that works for you. You'll probably want to include the id in your columns since that also acts as a filter ;). Awesome! I've released 4.6.3 so you should be able to get the update from npm.\nContributions are welcome if you're so inclined. I try to keep Massive relatively minimal but the goal is for it to be useful first and foremost. I have actually wanted to pick users' brains about the documentation for a while, though: too much? Too little? Does the organization make sense? Were any areas in particular difficult?. Yeah, I'm kind of conflicted about the split. I wrote the readme to provide a more gradual introduction building on the basics but it still more or less doubles the amount of effort that has to go into documentation.. Easy answer on the last question -- Massive doesn't have updatable views because I've never needed updatable views, and this is the first time someone else has brought them up! :smile:\nI'd absolutely be interested in that feature. My first thought is to simply make them Tables (probably renaming that class to Writable or some such), or if there's some functionality that doesn't work for views, to split the class. But before we get too far into that, you may want to evaluate PostGraphile as well since you're already building queries with GraphQL.. I don't think it'd be too bad. The main issue is that reload in lib/database.js couples the object type with the loader (starting around line 226), so anything coming out of the \"views\" loader is a Queryable. Assuming all Table functionality is more or less meaningful for updatable views, the \"views\" loader SQL scripts (lib/scripts) could simply ignore them in the WHERE clause and then a new loader (lib/loaders) could bind them to the Table constructor. Once it's working and tested, then think about renaming Table; and the listViews and listTables functions should be updated, of course.\nI am a little busy at the moment, but if you'd like to take point on this I'd be happy to answer questions and review pull requests!. Looks good, I've released 4.6.4. Thanks!. And here I was trying to think of how to preserve ordering and coming up blank. That's creative!\nI think the build failure is a fluke -- one setup hook had issues. I've seen it happen once or twice before but haven't figured out exactly what's going on. I'll rerun it just to make sure.. I'm a little perplexed by the shifting of tests (looks like that caught you by surprise too), but I suppose the ordering of internal arrays was never guaranteed in the first place. Merged, thanks for contributing!. I don't run into compound keys that often so Massive may not have them covered as thoroughly; however, I have been working at it off and on and I know for a fact that save works:\nit('saves over an existing record if passed both keys', function () {\n    return db.compoundpk.save({\n      key_one: 123,\n      key_two: 456,\n      value: 'yet again'\n    }).then(res => {\n      assert.isOk(res);\n      assert.equal(res.key_one, 123);\n      assert.equal(res.key_two, 456);\n      assert.equal(res.value, 'yet again');\n    });\n  });\nCheck out test/table/compound-pk.js for more. If there is a gap in coverage then please do feel free to submit a pull request!. save really only works if you autogenerate keys on the database side, since the behavior depends on whether the record object you pass it contains the key(s) or not. You'll need to fall back to calling insert and update yourself, but you could streamline things a little bit: use the onConflictIgnore option with insert. If the new record does conflict, it won't throw, but you won't get a result back from the insert since it didn't insert anything. When that happens, you know the row already exists and needs to be updated instead. So it's one or two roundtrips instead of two guaranteed -- small savings, but they add up.. I should have mentioned, there's also ON CONFLICT DO UPDATE if you want a true upsert, but you'd have to do that in a script file or database function since Massive doesn't offer that yet.. Massive isn't a migration framework and doesn't create tables (other than well-defined document tables... which could probably use column comments, to be fair). Did you mean to file this against node-pg-migrate?. I notice you're inserting data[0] but saving data -- is that correct? Can you provide sample data?. The error message you received is consistent with passing an array or thing-other-than-an-object to save, and that check goes back past 3.1.0. Please provide sample data which triggers the error on save.. You can only save one object at a time -- it doesn't handle arrays.. From what I know of partitioning tables, the child tables still have to be named independently so I'm not quite following what's happening here. Can you provide a script with CREATE TABLE statements I can use to replicate the issue?. I couldn't run this script as-is. Here's what I got working:\n```\ncreate table reservation (\n    property uuid not null default uuid_generate_v1mc(),\n    reservationid text not null,\n    confnum text,\n    creationdt timestamp with time zone,\n    updatedt timestamp with time zone,\n    status text,\n    arrivaldt date,\n    departuredt date,\n    constraint reservation_pkey\n        primary key (property, reservationid)\n);\ncreate table reservation_hhh (\n    property uuid not null default uuid_generate_v1mc(),\n    reservationid text not null,\n    confnum text,\n    creationdt timestamp with time zone,\n    updatedt timestamp with time zone,\n    status text,\n    arrivaldt date,\n    departuredt date\n) inherits (reservation);\n``\nI should note the parentreservationand childreservation_hhhis consistent with how I've used table inheritance previously: two tables, two names, no collision. Massive 4.6.4 loads the database I ran my version of the script against just fine. Is this missing some other piece -- views, functions, scripts?. ah, found it. I learned more about table inheritance today.. You should be good if you update to v4.6.5.. That's an undocumented legacy holdover; functions use [their own whitelist+blacklist](https://dmfay.github.io/massive-js/connecting.html#loader-configuration-and-filtering) and ignoreallowedSchemas`. I'll leave this up to track it but that's going to be a compatibility break that can't entirely be excused as a bug, so it'll be a new major version.. Closing this now that it's tracked.. good catch, thanks!. pg-promise's transaction capability was one of the big reasons I made sure it's possible to use the driver directly. I wanted it to be possible to run transactions even if having to use pgp's idioms instead of Massive's makes for a bit of a mechanical disconnect.\nA while back I was looking at doing some stuff with proxies to support multiple connection pools by switching out what happened under the hood when Massive invoked the driver's query functionality. I never quite got it off the ground and I've been focusing on other stuff since, but if it worked out I think it could apply to transactions as well. Or if you have other ideas, I'm definitely open to discussing them!. What I was trying was to maintain multiple pools in the Database object and build the API tree out of proxies so I could attach different pools at the root of the same tree -- you'd be able to db.mytable.find(...) or db.poolname.mytable.find(...) as necessary, and in the latter case you'd be navigating a tree of proxies to the former which would substitute the appropriate pool into db.query. The usage seemed a little less arcane than adding a poolName option. I didn't get it off the ground and it's been on ice for a good while though :grimacing:\nBut it seems like the same principle could apply for transactions, a db.tx returning a similar proxy tree representing the API that tracks and uses a handle to the connection maintaining the transaction.. You can already run raw SQL in tasks & transactions through db.instance. What's missing is the ability to work with the API (db.mytable.insert(...)) in a transaction. Like I mentioned above, I've got ideas in that direction but haven't been able to follow through yet. If you're interested in hacking on it I'm happy to talk ideas & review pull requests :). This is more or less what I envision as far as the syntax (with async/await since that's a lot more readable):\n```\ndb.transaction(async function (tx) {\n  const one = await tx.mytable.insert(...);\n  const two = await tx.anothertable.insert(...);\ntry {\n    tx.aScriptWhichCouldFail();\n  } catch (e) {\n    return tx.rollback(e);\n  }\nreturn tx.commit();\n});\n```\nSo tx has a copy of the API Massive builds and attaches to db. This is where it gets sticky: each Queryable, Table, or Executable maintains an encapsulated dependency on db so it can query Postgres through the db.query method. In order to build something like this, that circle has to be broken somehow.\nThe proxy stuff I talked about before was one idea: if a Queryable could hot-swap what db meant, the same structure could be used with different connection pools or transactions.\nAnother possibility is to brute-force copy entities off the Database object and attach them to the Transaction object, but I'm a little worried about performance in that case.. I had a little time to play around with it and this is suspiciously light but tailing the log shows it behaving as expected -- check out the transactions branch. I'm just brute-forcing the API with _.cloneDeep and overriding instance; I'd like to do that more elegantly but this may turn out good enough? At least it's not incurring the full introspection hit.. @ferdinandsalis @flux627 I'm interested in hearing how it's working out for you if you've had a chance to take it for a spin!. @ferdinandsalis I'm not familiar with lambda but you could require/import the individual massive/lib/statement/{select,insert,update,delete} files and build your own queries by mocking the source spec (see lib/queryable and lib/table constructors for what those need to be) and passing in criteria and options. It'll be a little gnarly and you won't have the API tree but you shouldn't need to intitialize to call format.. Transactions are available starting with v5 now :tada:. The variadic argument is an array inside the function body; outside, it's just more arguments and shouldn't be enclosed in a JavaScript array. Invoke your function like this:\ndb.teststrings('one', 'two', 3, 'four', 'five').then(res => {...});. @Zolmeister suggests turning deep insert off by default. Given I've seen more issues come up from that feature than anything else in recent memory I'm at least going to think about it -- comment welcome!. I dropped a release candidate last night: npm i massive@next. It should be pretty stable from here; there are one or two more things I have in mind but I don't intend any more major disruptions.\nDocumentation in the package or the v5 branch is up to date. If you check out the branch and navigate to the docs/ directory, you can run your own instance of the documentation site with bundle exec jekyll serve (requires Ruby). I'll be updating the gh-pages branch once this makes it to master.\nPlease report any issues you run into!. all done!. That sounds right, feel free!. There's no support for it at present; it's very much in the \"nice to have eventually\" category.\nDepending on what you're trying to do, I'd recommend a view or query file over inlining SQL with db.query. The latter is convenient for experimentation but isolating query logic makes it much easier to test completely.. awesome, thanks!. released 4.6.6.. It is not, but adding an ignoreJunctions option for inserts would be pretty straightforward if you'd like to submit a pull request.. Released as 4.7.0, thanks!. There are global options but that is not one of them. I've been thinking about changing how that's enabled somehow for v5, but I've also been pretty busy. I've just thrown a v5 branch up if you or anyone else wants to make a pull request against it.. Can you post the table definition?. Yeah, Massive doesn't impose any limit -- it's either Postgres or your RAM. Out of curiosity, what are you trying to do with a batch insert of 50k records at a time?!. Depending on how much processing you need to do, I'd look into streaming records in and collecting smaller batches to fire at PG, or just using COPY.. There's no API, but it'd be worth a shot with a script file.. I assume you got whatever this was working :). That's that fixed, thanks for reporting it!\nTell me more about GitBook? What's it got that would make you more likely to contribute to docs there than opening a pull request here?. Yeah, I was looking at it and thinking WYSIWYG might be nice for some people but it's all Markdown in the end and the toolchain I've got is nice and simple already. Search is a good point but I don't think Massive's at the level of complexity where we really need that, and I hope to keep it that way! :grin:. Your table only has an id column but you're inserting a record with a username field? Am I understanding this correctly?. Well, that explains the what, but not the why: why are you inserting a record with a field that doesn't exist in the table? Do you have something set up with rules or triggers?. That makes more sense, thanks. Yeah, the error messaging around it needs some work.. Closing, see #546. Appreciated, but I have to close it since it isn't really actionable.. Sell me on it, then. What's the advantage of lazy init over just taking the introspection hit at startup? What makes it worth the extra complexity?. ah. With ES2017 you can db = await massive(...); which more or less obviates the issue for Node 7+. That leaves 6, and it's a lot less compelling to add and support new features that only matter for the oldest version Massive supports, especially when co exists.. It's a CTE instead of a subquery if that matters, but there's deep insert. Nothing for update as yet though, you'd have to do that with a script or function.. oh I didn't mean like that -- what you have is a subquery (using a SELECT inside another statement), deep insert uses CTEs (WITH (...) AS x ...). And yes, it only works with single records because all the child records get the parent record's primary key so you can only do one per query. You could map individual accounts with their associated transactions through it if you wanted to avoid writing it out yourself, but if you want to get it all done with one operation I'm afraid you're going to have to write some SQL.\ndb.raw is an interesting thought, but I'm a little nervous about adding more ways to potentially SQL inject yourself in the foot.. Connections are pooled so I expect the table would not be reliably retrievable if you try to use it outside the SQL script.. Closing due to inactivity.. Yeah, a timestamp is appropriate for certain situations but I'm comfortable calling that a bug since almost everyone will expect a timestamptz. Go ahead and make the change (add a test too please!) and I'll cut 2.7.4.. I've published 2.7.4 on the legacy tag and 4.7.1 on latest. Thanks!. oh awesome, thank you!. SELECT * from a self-join will return duplicate columns: you'll get an id for the user and an id for the supervisor, and so on. Massive has no way to tell which is which. Use aliasing to ensure your column names are unique, and update your decomposition schema accordingly. That ought to work better.. I'm honestly surprised it seems to be working that well with that SELECT statement. The field consolidation happens in the driver -- while the SELECT itself returns multiple id columns as appropriate, the results Massive gets back are condensed into objects with keys mapped to column names. If you check the results array returned from the query without decomposition, you should see objects containing one id, one username, etc since duplicate keys overwrite the original.\nIn fact, what's causing the issue is probably exactly this: your LEFT JOIN returns duplicate fields nulled out, which the driver condenses into a \"user\" {id: null, username: null}, which causes problems for decompose trying to apply the schema.. No, you can totally still do it in one query! Just alias your columns: \nSELECT u.id, u.username, s.id AS supervisor_id, s.username AS supervisor_username\nFROM users u\nLEFT OUTER JOIN users s ON s.id = u.supervisor_id\nThen define your columns appropriately in your decomposition schema. The nested supervisor's pk is supervisor_id, and its columns include both supervisor_id and supervisor_username.. You can de-alias them in the schema:\n{\n  pk: 'id',\n  columns: { id: 'id', username: 'username' },\n  supervisor: {\n    pk: 'supervisor_id',\n    columns: {supervisor_id: 'id', supervisor_username: 'username'}\n  }\n}. If you post some code that exhibits the behavior I can look at it but that's a new one to me. I'm using async+await with script files successfully and can't think of anything on the Massive side that would be causing that.. There has not. I wouldn't say it's never going to happen but it's something I'd be extremely careful about and right now it's not remotely a priority.. You can!. Primary key constraints aren't inherited, and Massive looks for a primary key when it loads tables. If you add one to your instances table it should start showing up.. That's going to change in v5.. That doesn't sound right -- connecting every time adds a ton of overhead compared to even a shared active connection, let alone a proper connection pool. And the examples in the official node-pg docs have a module (db/index.js) caching a connection pool and exposing a query function for the routes.\nApplication routes can be hit by many users simultaneously. Generally you want them to be as light as possible, which means doing your init outside as much as possible.\nMassive's docs have an Express example too.. It does for ordinary predicates, but you've got an array operation here and parameters get processed through the literalizeArray mutator which evidently doesn't handle date casts.\nIf you'd like to dig in and submit a pull request, feel free!. This appears to be a driver issue; node-pg casts Dates appropriately for most queries, but ranges other than numrange aren't currently supported. The casts in docGenerator are there to ensure fields in JSON documents are tested against the appropriate types, since they otherwise come out as text.. Can you explain how this came up? Your workaround seems fine if there's a common or compelling reason criteria and options objects might not have standard prototype methods but I'd like to know why this could happen first.. oh, it's GraphQL. @eduardomourar feel free -- I'm traveling the next few days but I should be able to review and release this weekend/early next week.. It\u2019s not intentional, but since there\u2019s a clear default behavior when the option isn\u2019t specified at all I\u2019m not especially put out by it falling back to that. If you feel strongly enough about it to write up a test and submit a pull request, go right ahead :). There\u2019s another test in the statement suite but that\u2019s still pretty light coverage. The thing is you can\u2019t select nothing so the options are either error (either before or at the SQL syntax error of \u2018SELECT FROM tbl\u2019) or select everything. The original design went with the \nlatter, but since passing an empty fields list is likely unintended I\u2019m good with the error.\nPlease make your PR against the v5 branch rather than master since it\u2019s technically a compatibility break.. The test \u2018before\u2019 methods call a function named resetDb in test/helpers/index.js which loads a schema by the passed name from test/helpers/scripts/$name/schema.sql. This cuts down on fixture boilerplate since it\u2019s always doing the same thing with the schema script no matter what\u2019s actually in it. If you need to modify that script to add more inserts or a new table just make sure it doesn\u2019t break any other tests which use it :). All the SQL methods revolve around Database.query... which doesn't have tests for the error case either, so that's a coverage gap. But find, count, etc don't need it nearly as urgently since any rejected promises would be coming from query.\nI've more or less settled on return promiseyFunction(...).then(() => { assert.fail(); }).catch(/* some other assertions */); as in test/table/deep-insert.js but there are a handful of tests in other styles. Massive is compatible back to Node 6 so try/catch on await isn't happening yet.. Your lib/statement/select.js still sets fields to ['*'] if it's empty. You need to throw in the constructor if fields is empty and catch it in the Queryable methods so you can safely return a rejected promise.\nThere is a reject test for find but it's catching a \"natural\" error (trying to use a primary key criterion with a view) rather than testing a specially generated rejection.. There are three cases:\n\noptions.fields is undefined, in which case the Select's fields needs to be ['*'] for interpolation into the emitted SQL\noptions.fields is defined as an empty array, which is where you're trying to raise an error\noptions.fields is defined as anything else, in which case the existing logic takes over\n\nThe error check needs to happen in the Select, since otherwise find starts behaving differently from findOne, findDoc, etc. I think what you ran into was that you were testing this.fields to see if it was an empty array, instead of testing the options.fields passed into the constructor. Try that instead.. It's a common mistake with equivalence in JavaScript :slightly_smiling_face: Arrays are objects, and object equality tests by reference. So this works:\n```\nconst a1 = [1, 2, 3];\nconst a2 = a1;\na1 === a2; // true\n```\nBut this doesn't:\n```\nconst a1 = [1, 2, 3];\nconst a2 = [1, 2, 3];\na1 === a2; // false\n```\nTesting if (options.fields && options.fields.length === 0) throws as expected.. My turn to be out of practice -- of course if you throw an exception that isn't caught then it won't reject the Promise :woman_facepalming: Best place to catch is in Database.query() so errors have to be thrown from format() instead of in the constructor:\n```\nFrom 9a832f0bb612e6e5f47a7206a60f7761319907e1 Mon Sep 17 00:00:00 2001\nFrom: Dian Fay dian.m.fay@gmail.com\nDate: Tue, 24 Apr 2018 22:22:13 -0400\nSubject: [PATCH] catch errors from statement format() in db.query()\n\nlib/database.js         |  7 ++++++-\n lib/statement/select.js | 12 ++++++++----\n 2 files changed, 14 insertions(+), 5 deletions(-)\ndiff --git a/lib/database.js b/lib/database.js\nindex b95353f..b1d516b 100644\n--- a/lib/database.js\n+++ b/lib/database.js\n@@ -333,7 +333,12 @@ Database.prototype.query = function (query, params = [], options = {}) {\n   if (query instanceof this.pgp.QueryFile || _.isString(query)) {\n     sql = query;\n   } else {\n-    sql = query.format();\n+    try {\n+      sql = query.format();\n+    } catch (e) {\n+      return Promise.reject(e);\n+    }\n+\n     params = query.params;\n     options = query;\n   }\ndiff --git a/lib/statement/select.js b/lib/statement/select.js\nindex 8052db7..1807063 100644\n--- a/lib/statement/select.js\n+++ b/lib/statement/select.js\n@@ -48,10 +48,6 @@ const Select = function (source, criteria = {}, options = {}) {\n       break;\n   }\n\nif (options.fields && options.fields.length === 0) {\nthrow new Error('The fields array cannot be empty');\n\n}\n   this.fields = [];\n\n_.castArray(options.fields || []).forEach((f) => {\n@@ -66,6 +62,10 @@ const Select = function (source, criteria = {}, options = {}) {\n     this.fields.push(${expr} AS ${name});\n   });\n\nif (options.fields && options.fields.length === 0) {\nthis.error = 'The fields array cannot be empty';\n}\n+\n   if (this.fields.length === 0) {\n     this.fields = ['*'];\n   }\n@@ -96,6 +96,10 @@ const Select = function (source, criteria = {}, options = {}) {\n@return {String} A SQL SELECT statement.\n  */\n Select.prototype.format = function () {\nif (this.error) {\nthrow new Error(this.error);\n}\n+\n   let sql = SELECT ${this.fields.join(',')} FROM;\n\nif (this.only) { sql += 'ONLY '; }\n2.17.0\n``. There's a difference between throwing an exception and returning a rejected promise. If you look at the patch above I modifiedDatabase.queryto catch exceptions coming from the statement'sformat` method, and threw from in there instead of the statement constructor.\nThe test is passing in your second commit because fields is never empty by the time it makes it to Database.query -- the constructor adds * to build the SQL statement. That's why my patch has to cache the error instead of just testing it in format. Try applying it and see how that works for you! There's probably some room for improvement but it should at least get you going down the right path.. I didn\u2019t realize destructuring like that worked in Node 6! You were targeting the wrong branch though \ud83d\ude04 . First off, which version of Massive and which version of Node are you using? The only way I can think of to reproduce this issue with the current codebase involves hand-editing your node_modules/massive.. Closing due to inactivity.. Probably. The wording in the docs is a bit strong (the tests run on schemas generated with multi-statement files) but for production usage you want to avoid One Big Transaction like the plague. If I had to guess, you're probably running into a lock partway through.. Your db module will need to wrap the process of connecting to Postgres and export a function which returns the connected instance. Then when you require it, you'll have to await or otherwise resolve that function to get the instance out.. Node 10, not Postgres 10 :) I'm waiting for https://github.com/nodejs/node/issues/20503 to release, until then streams are broken.. It was! It also exposed another bug in my testcases that took entirely too long to work out :woman_facepalming: . Pass {order: null} in your options to disable it entirely.. Inertia -- the original implementation deferred to find and just picked the first result off of that. Nobody's noticed until now. Feel free to drop a pull request!. Thanks for the reports! I didn't know if this was going to be possible at first with all the special handling document criteria already get but it seems to have worked out okay. Update to 4.8.2 next chance you get.. Massive exposes pg-promise as db.pgp, then pg-promise exposes node-pg so you have access to that as db.pgp.pg. You're already connected by the time you can do anything with it so I don't know if that's all it'll take though.. Closing due to inactivity.. The easy way around it is to stringify bar. Type management is mostly deferred to the driver so a real fix might be a bit involved.. The correct way to use pg-promise formatting for this case:\njs\n  it('inserts json/jsonb arrays using custom type formatting', function () {\n    return db.normal_pk.insert({\n      field1: 'nu',\n      json_field: {\n        data: ['one', 'two', 'three'],\n        rawType: true,\n        toPostgres: (p) => {\n          return db.pgp.as.format('$1::jsonb', [JSON.stringify(p.data)]);\n        }\n      }\n    }).then(res => {\n      assert.equal(res.field1, 'nu');\n      assert.deepEqual(res.json_field, ['one', 'two', 'three']);\n    });\n  });. The generated SQL looks like it's coming from a different statement -- you're inserting into signature and then into test trying to pass the same user_id. Does this signature table have user_id as a combination primary key + foreign key to user?\nDeep insert is (so far) meant to work with entirely new object graphs -- new user, new signature, new test, with the foreign keys in the latter two explicitly undefined to indicate they're placeholders for whatever pk is generated for the user. With the discrepancy between your app code and SQL, I can't tell if you already have a user or if you're trying to pregenerate the primary key, but I think either could have issues.\nAs for having to specify the schema, yeah, that's required (although it looks like it needs escaping, whoops) because you can have foreign key relationships across schemas -- test might be in public for all Massive knows.. Try this:\ndb.private.user.insert({ user_id: 'b146c47f-3380-49b4-8022-58a7d47f56d7',\n  avatar: 'string',\n  fullName: 'string',\n  color: 'string',\n  'private.test':\n   [ { user_id: undefined,\n       icon_id: 'twitter',\n       url: 'http://plokok' } ] });. Closing due to inactivity.. Similar questions have come up before, most recently in #590; see if my answer there works for you. I don't think there's really any good way to implement around introspection being \"hard-async\", if you will; there are various kludges, but all of them come with a price. Proxies are slow, cached results may not be valid when it actually starts up for real, with your \"hybrid\" you've (as you said) still got to await it anyway to be sure it's usable, deasync is a whole other dependency with its own set of issues and does a lot of mucking about in the Node engine.\nSo far I honestly haven't thought the init step is that big a deal, since it's a one-time cost, fairly easily abstracted, and init functions in the frameworks I've used have dealt with asynchronicity well enough. Async is Node's bread and butter and it's been getting easier and easier to work with.. The added complexity (and supporting/maintaining that complexity) is the other aspect that gives me pause about proxies. But let's back up a bit -- I'm still not getting why awaiting a promise on initialization is a significant annoyance. The model I've used with koa and Express is in the docs. What does yours look like now and what are you shooting for?. Taking advantage of Node's module caching is the course I've recommended before & it still seems like it'd suffice. Probably worth adding this to the docs though:\n```js\nconst massive = require('massive');\nlet db;\nexports = module.exports = function () {\n  if (db) {\n    return db;\n  }\nreturn massive({\n    host: 'localhost',\n    port: 5432,\n    database: 'massive'\n  }).then(instance => {\n    db = instance;\nreturn Promise.resolve(db);\n\n});\n};\n```\nYou only have to await the first invocation and after that it's synchronous, so you can invoke it from anywhere:\n```js\nconst getDb = require('./db');\ngetDb().then(db => {\n  console.log(db.listTables());\n// don't pass the instance\n  return Promise.resolve();\n}).then(() => {\n  // retrieve the already-connected instance synchronously\n  const db = getDb();\nconsole.log(db.listTables());\nprocess.exit(0);\n});\n``. That's more strictly correct, but since it always returns a promise you always have to unwrap it, which is what Tamlyn wanted to avoid after initialization.. I don't think it was about performance, it's that it can add complexity when you have to acquire the instance inside your promise handling instead of outside it..updatetakes a minimum of two arguments now: criteria to find rows you want to change, and the column:value changes to make. The single-object version with a primary key and fields to update duplicated the functionality ofsaveand made options object usage ambiguous (is this attempting to modify a column namedsingleor is it an options object with a fieldsingle? that kind of thing). Your newupdate` call is not modifying any fields at all since the changes object is empty.\nI'm not following your findOne case; that error message is specific to the update method.\nThe rest:\n\nIf your options object specifies fields: [...], you have to actually pass fields; if the array is empty, it's taken to be an error. If you don't specify fields in your options object or don't add options at all, no harm no foul.\nYou can use the binary update for that, just pass a primary key or a criteria object like {id: 1234} as the first argument. It should emit (almost) exactly the same SQL and there's no chance of the ambiguity I mentioned above. The record will be returned in an array unless you pass options.single.\nThis only matters if you're using updateDoc with a JSON field that isn't named body. You used to have to specify it as an additional argument, but that's been moved into the options object. The documentation reflects this but it came out a bit awkwardly in the changelog.\n\nIn general I'd recommend giving the docs a once-over if something throws you -- there's quite a bit that's different, and the changelog is necessarily a summary rather than a full accounting.. nice catch, thanks!. I followed your steps:\n```sql\ncreate user mygroup;\ncreate user myuser;\ngrant mygroup to myuser;\ncreate table mytable (id serial primary key);\nalter table mytable owner to mygroup;\n```\nand I found that Massive did in fact find and load mytable when connecting as myuser, although it appeared as \"public.mytable\" instead of \"mytable\" as myuser's current_schema was null. You might check this first, but the real issue was that neither mygroup nor myuser had the correct permissions for the public schema. grant usage on schema public to mygroup fixed that and mytable loaded as expected without the explicit path.. You might check your pg_hba.conf, but I think that'd just determine whether you could log in or not as opposed to what you have access to. Past that all I've got is the basic list: am I really connecting to the same database, as the same user, etc.. This would definitely be cool, but I don't think it's practical. This isn't a simple predicate operation as with the simple array &&. You have to join and process the array elements themselves in order to detect an intersection, and right now the query builder doesn't account for joins at all -- the idea being it's better to formalize a complex query with a view, script, or function, and decompose the resultset into something friendlier. And that's without getting into wrapping it back up to return the original matching record, behavior with or, and so forth.\nIf you don't need absolute maximum efficiency, you can check containment with a criteria object like {studios: [{name: 'Warner'}]}, and use the or key to do multiple tests. Or just use a script file! The query builder can't cover every eventuality, and this one's a lot more complicated than it might seem at first blush.. That should still work in v3, but I wrote a proper JSON path parser for v4 and up. So you need to issue db.curation[table].find({ 'body.listId': 'test2', 'active': true }); instead of using the Postgres pathing operators. Array indices work how you'd expect coming from JS too.. Massive does this very well! The fundamental difference with all these shiny new models is that instead of doing CRUD on each table where applicable, you only write to some areas of your database, and you only read from others. For event sourcing, you have lots of db.events.insert, use triggers or materialized views to cache the current data-state, and then everything else is just reading those state tables and/or views. Nothing actually gets updated or deleted from the API layer, only in the event functions themselves. This would allow you to clear out everything except the events table and replay those to regenerate the state of your database at any point in time.\nIn fact, I'm currently working on a specialized manufacturing/RMA pipeline app using Massive to talk to an event-sourced database \u00e0 la Toby Hede's model, and it's not the first time I've taken this approach. . Absolutely, it'd probably fit best as a new page somewhere around documents/transactions in the left table of contents.. It would be cool! I'm going to punt on it though -- most of the work to be done here would fit in better at the driver level so there's a single consistent implementation whether you're using Massive or not. So if you're interested in working toward it I'd suggest pulling pg-promise (hi Vitaly!) instead.. Afraid I haven't gotten into pgp's sequences much at all, so I can't really help you there. If it makes it in upstream though it shouldn't be too much work to add proper support for it through Massive though!. Yeah, you can't run PLPGSQL flow control statements outside the context of a function. It should be possible to avoid duplicating the script using pg-promise's raw text interpolation though! I like the overall approach here but there are a few other fairly quick changes:\n\nv1mc UUIDs are more efficiently indexed than v4 UUIDs so the default should be uuid_generate_v1mc() for best performance\nthe option name is a bit verbose: documentPkType gets the point across imo. Just default that to serial and make sure to set the default when it's uuid\ninstead of changing all document tables created during testing and dealing with all the fallout from that, just add a single new test to test/connect.js that sets documentPkType, creates a document table, and verifies that a new record has a UUID primary key\n\n& thanks! It's good to have some movement on this one :). Okay, quite a bit going on here, let's dig in :)\nExtension\nIn testing, resetDb drops all non-system schemas and the extension goes with them. In production, you should have any extensions you need installed before you spin up your application; like you noticed, if you've got your app user permissions set up appropriately it won't be able to create an extension at runtime. And of course it's less than ideal to create the extension in the script if you aren't using UUID keys anywhere.\nBottom line I don't see db.createExtension getting a lot of production use but it's not hurting anything. You don't need to then off this.query though, since that already returns a promise. Otherwise :+1: \nUUID versions\nverUUID is on the obtuse side; uuidVersion would be clearer.\ngetDefaultSQLForUUID expects users to read the uuid-ossp docs to find out which particular function name they want; I'd rather hide that and offer v1, v1mc, and so on to make that more straightforward. The documentation (docs/documents.md) needs to explain how to use UUID keys and list the available types too, and a note about using v4 for randomness vs v1mc for performance would also be good to have.\nThe constant-returning functions like uuid_nil and the uuid_ns_* set are essentially useless for primary keys so there's no need to include those. Nil is sometimes useful in testing but in my experience it's been used to pregenerate a single entity's PK; there's no point in hardcoding a table-level default to straight zeroes.\nTests\nsaveDoc is an extra degree out from where the action's happening. What this needs is a new test in test/database/createDocumentTable.js that ensures invoking createDocumentTable with the UUID key configuration really does create a table that sets UUID keys. Create a table, save a document, check the key the way you've already got.\nThis way you also avoid modifying the global loader which could potentially result in a lot of extra noise if something goes horribly wrong later on ;). re getDefaultSQLForUUID: that's exactly what I meant -- there's enough research involved in knowing whether you want v1 or v4 or whatever without needing to track down the exact name of the function that generates that type of UUID in Postgres!\nThere are good arguments for both v4 and v1mc as the default so I'm fine with v4 winning that one. I did just have the thought that it could potentially be reduced to a single configuration parameter -- so if you set (spitballing) documentPkType = v4 it's a v4 UUID, with the default still being SERIAL. You've almost certainly put more thought into this than I have, so what's your take on that idea vs a UUID flag + separate version specification?\nuuid_nil just isn't useful for testing: if you set that as the table-level default, you can only ever insert one record without having to specify a pk value yourself, which defeats the purpose of defaults. So let's pull that one out too.\nAs far as the tests: what you've got in the createDocumentTable tests now confirms that the table exists, but doesn't actually test the PKs of a stored record. If you save a document and test for a UUID key there, I don't think you need to touch the saveDoc tests at all -- the createDocumentTable tests will have proved it out sufficiently.. Looks great! Just add some user information to docs/documents.md and this is good to go :). I've just published 5.1.0 -- thanks for contributing!. I've usually just tailed the Postgres log when I've needed to track down SQL errors -- you do have to restart PG if you don't have it turned on, but if you're already logging then it's quite transparent.\nI'm open to improving error messaging on the Massive side (clearer is always better) but just showing SQL seems like a half measure as the developer is still required to work out which call would have generated it. I haven't really gone past the surface level of dealing with JavaScript errors & stack traces & so on but it seems like it should be possible somehow, I just want to make sure it's done elegantly without boilerplate error traps on every API method.. This is strange. I don't have a ton of time to dig into it at the moment, but I modified the existing tx rollback test to use async and await:\n```js\n    it('rolls back if anything rejects (async/await)', async function () {\n      const total = await db.products.count();\n  try {\n    await db.withTransaction(async tx => {\n      const record = await tx.products.insert({string: 'beta'});\n\n      assert.isOk(record);\n      assert.isTrue(record.id > 0);\n      assert.equal(record.string, 'beta');\n\n      await tx.products.save({id: 'not an int', description: 'test'});\n    });\n\n    assert.fail();\n  } catch (err) {\n    assert.isOk(err);\n    assert.equal(err.code, '22P02');\n\n    const newTotal = await db.products.count();\n    assert.equal(total, newTotal);\n  }\n});\n\n```\nThat catches the error correctly and ensures the transaction rolled back. And it's about the same structure you have: outer async function, try, await db.withTransaction's inner async function, insert a new record, do something wrong and abort.\nI suppose it might be possible that some other piece of code is also inserting into store_charge if you're hooking this up as an event? It might be worth logging out the new record id when you insert into store_charge and confirming that it's the same as the one that shouldn't be there afterwards.. congrats, you found a bug! Script and function executors were retaining the original instance handle and weren't getting routed through the transaction. Pull down v5.1.1 and you should be all set.. I forgot I had my npmrc set up to publish to a private registry \ud83e\udd26\u200d\u2640\ufe0f It's up on the public one now.. Does the enrollment.existing_enrollment function exist on the production db server, and does the user Massive is connecting as in prod have permissions set to be able to see it?. oh it's a script, okay. Is the output of db.listFunctions() the same in both environments?. Massive treats database functions and script files more or less identically (for our current purpose at least).\nIf listFunctions isn't returning anything in prod, that's your problem -- something is preventing Massive from loading scripts. Directory ownership & permissions seem like the most likely culprit.. There's always adding console.log statements in node_modules/massive -- check out lib/loaders/scripts and just throw a bunch of stuff in the output so you can see exactly what it does and doesn't find.. I hadn't heard of Forever before, so that's a no :). It says db.get_bins is not a function; is there anything there or is it just undefined?\nAlso, you'll need to await that invocation but that's a separate issue.. Obviously the solution is to drink more and get on that Ballmer peak :wink: . Document tables are still tables and you can use both the regular table and the document API methods with them. The document body is stored in a column named body. For example if you need to add extra fields on creating a new document:\njs\nconst recordWithBodyAndOtherStuff = await db.extendedDocumentTable.insert({\n  some_fk: 123,\n  another_field: 'something goes here',\n  body: {\n    foo: 'bar'\n  }\n});\nWhen you don't need the extended information, you can continue to use the document API:\njs\nconst justTheBody = await db.extendedDocumentTable.findDoc({foo: 'bar'});. It took me a bit to realize you were talking about v2 -- this definitely works in trunk! :laughing: Out of curiosity, what's standing in the way of upgrading?\nOutside security updates, I'm not prioritizing work on older versions, but if you'd like to put true/false detection and a testcase proving it out into a pull request against the v2 branch I'd be happy to merge and cut a patch release.. I'm still running 10.1 in RDS for now, luckily I found that bug with a dev database so I could just throw it out and spin up a new one :joy: \nI think the new join condition could be a little more efficient if it went through pg_class.oid = pg_attribute.attrelid instead of concatenating and casting to regclass (the view scripts even already join pg_class in), but that sounds good otherwise! As much as I'd prefer to use the standard information schema for introspection where possible I think that ship's long since sailed.. looks great, thanks! Pull down 5.2.1 and you should be all set :). Yeah it's a bit clunky, but I didn't want to bump the major version over something with this limited scope. I'll reevaluate it later on.. There have not been any efforts in that direction on Massive, or, as far as I'm aware, the lower-level driver chain with pg-promise and node-pg. I'd be thrilled to have the option available though!\nIt sounds like there are two parts: connection configuration needs to include the replicas and the connection process needs to set up the appropriate pools (there's some relevant discussion in #381 you might want to look at, although my proxy experiments didn't wind up going anywhere), and then queries need to be routed appropriately. I think both your ideas sound solid: send SELECTs to replicas by default if you have replicas, and offer a way to go directly to the primary on top. The complicated part is going to be choosing among multiple replicas if there's more than one.\nOther than that, glad you've liked using Massive thus far :). @momirov sure, tracking and managing them is on you but it's perfectly possible to spin up as many instances as you like.. oh right, I factored the test initialization into a reload instead of swapping instances out a while back :woman_facepalming:\nThis could get complicated and I don't have time to really dig in at the moment, but if you'd like to try hacking at it feel free!. Delete too, all three persistence statements just throw a RETURNING * on so options.fields and options.exprs are basically ignored. It makes sense that you'd want to do that with column security though. If you'd like to take a stab at it feel free, or I'll have time soon enough.. Done in 5.4.0 :tada:. I usually just copy out the document table DDL into my migration scripts if I need to customize them. You'll need to replace the tokens in that file: schema and table should be pretty obvious, index can be whatever you like, and pkType + pkDefault should be either SERIAL and nothing, or UUID and one of the UUID generation functions.. Migrations are out of scope for Massive, but of course you can publish whatever extensions you like to the registry :) You're right it couldn't hurt to be a little more specific about it in the docs though.. I know with regular JSON arrays stringifying them beforehand works but I don't think that'd hold for arrays-of-JSON. Does #598 offer any leads?. Yes in the abstract, but it doesn't work for this case. You need to turn the pgFormatting driver option on in order to bypass pg-promise's parameter processing, which will also mean you can't use named parameters. If you don't care about that at all, you can pass it in the driverConfig when you initialize Massive. Otherwise enable then disable it once the insert has succeeded or failed.\nTagging @vitaly-t in case you're interested :wink:\njs\n  it('inserts arrays of json/jsonb', function* () {\n    db.instance.$config.options.pgFormatting = true;\n    return db.normal_pk.insert({\n      field1: 'nu',\n      array_of_json: [{val: 'one'}, {val: 'two', nested: {val: 'three'}}]\n    }, {\n    }).then(res => {\n      assert.equal(res.field1, 'nu');\n      assert.deepEqual(res.array_of_json, [\n        {val: 'one'},\n        {val: 'two', nested: {val: 'three'}}\n      ]);\n    });\n  });. You would want db.instance.none, that's a pg-promise method not a Massive method. I tried with rawType and toPostgres yesterday and it didn't seem to be running the custom formatter, but I haven't had time to fully dig into it yet.. @nkramaric use of toPostgres is slightly off in your example. Since each field's value in the map becomes a parameter in a prepared statement, you have to define the value of the JSON array field as an object implementing toPostgres. rawType is necessary, and you can make the formatter reusable by defining a data attribute like so:\njs\n  it.only('inserts arrays of json/jsonb', function* () {\n    return db.normal_pk.insert({\n      field1: 'nu',\n      array_of_json: {\n        data: [{val: 'one'}, {val: 'two', nested: {val: 'three'}}],\n        rawType: true,\n        toPostgres: (p) => {\n          return db.pgp.as.format('$1::jsonb[]', [p.data]);\n        }\n      }\n    }).then(res => {\n      assert.equal(res.field1, 'nu');\n      assert.deepEqual(res.array_of_json, [\n        {val: 'one'},\n        {val: 'two', nested: {val: 'three'}}\n      ]);\n    });\n  });. Nice catch, thanks! The gh-pages branch is automatically regenerated from the docs/ directory in the master branch, so could you target docs/frameworks.md on master instead?. Pushed the doc updates so it should be showing up in a bit. Thanks again!. Fixed in 5.5.1.. Just making sure: you have seen the example in the v5 docs? I'm not familiar with the Express generator but unless they've really changed things around a lot since I last used Express it should be usable.. Look for the server.listen(port); call in bin/www. When you connect Massive, app.set and call server.listen in the .then() instead of where it currently is. (If you're feeling like having some fun, you could put it in an async IIFE).\nYour route/controller can be an async function and await the find. All the Massive documentation uses .then() to maximize compatibility, but async/await is much nicer.. Any function that returns a promise can be made an async function. Invocations of functions which return promises (async or not) can be awaited but only from inside async functions; the await keyword is invalid outside that context. For general documentation, MDN is basically the gold standard.. You'll have to use a script, function, or view that pre-filters occupations; criteria keys can't be arbitrary SQL snippets, since that invites injection attacks.. Have you tried? Did you receive an error message, did one or both fields retain their previous values, what happened? \"I can't do\" doesn't give me anything to go on. Please use the full issue template.\nI can tell you that I've used update to modify mixed ordinary and JSON values going back to Massive v2, and that this testcase passes:\njs\n  it('updates mixed column types', function () {\n    return db.normal_pk.update(1, {field1: 'zeta', json_field: {a: 'b', c: {d: 'e'}}}).then(res => {\n      assert.equal(res.id, 1);\n      assert.equal(res.field1, 'zeta');\n      assert.deepEqual(res.json_field, {a: 'b', c: {d: 'e'}});\n    });\n  });. Merging into existing JSON/JSONB values is only supported with updateDoc. Your other options at present are to make sure you have the full body beforehand and use update, or to create a script that merges your body changes with the || operator.. Document tables excepted, I tend to consider DDL out of scope -- Massive's a data access framework, not a database management or migration framework. That said, it does make enum values available since that's a pretty common use case.. So first, thanks for finding the bug! That shouldn't happen, and I'll have 5.5.2 up with a fix for query files shortly. On to the rest:\nThere's no real risk associated with false matches, assuming you test your code, since as you discovered they raise an error rather than permitting exploitation; and in the opposite case where an intended parameter doesn't match, nothing is interpolated. For parameters that are correctly identified and interpolated, pg-promise sanitizes the values to ensure nothing goes out of bounds and introduces injection vulnerabilities.\nTo your second point, when you emit SELECT $1... to Postgres directly, $1 is a literal not an identifier. Not even casting it to a name (which takes double colons, ::) allows you to do what you're thinking it does. With pg-promise you can use named parameters to generate dynamic SQL, but again, 1) pg-promise sanitizes parameter values to ensure that they're properly escaped and quoted, and 2) false positives generate ordinary errors while false negatives aren't acted on.\nNone of this is to say that you can't shoot yourself in the foot with Massive. It's a data access framework and using data access frameworks safely takes a minimal amount of thought and care. There are even areas that definitely open you up to injection if used unwisely -- check out the documentation for the exprs option! But this isn't the horrorshow you think it is.\nAnd to give Vitaly his due, you did kind of come in swinging telling me I should seriously consider abandoning his library :). No worries :) It is actually 5.5.3 thanks to a minor versioning snafu but it's published.. No worries, looking forward to it when you're ready! :). Just merged, stay tuned!. welcome :) It's up as 5.6.0 now.. Well, it's another way to decompose results. It stops being strictly better around the point where you have multiple joins or deeper levels of nesting, and it's a lot less flexible than applying a schema after the fact. I don't know if you're bringing this up as a suggestion for Massive, but the return on effort is not very good due to these limitations.\nIf you have fairly simple requirements and a lot of data to process, then yes, you'll likely see better performance using array_agg and json(b)_build_object instead of decompose. That's why Massive loads views and script files: the API features are about maximizing convenience for as many use cases as possible, but sometimes you really just need to fall back to SQL.. Did you do something with db.instance.$config, most likely as part of setup, or maybe pass undefined explicitly as the third argument when instantiating Massive? If you look at the part of the Massive code indicated by the stack trace, that's what it's trying to pull the promise property out of. The stack trace also points to save, not one of the inserts, and if that's accurate your real problem is that Writable.save is not getting what it wants. Fixing db.instance.$config should make it give you a more useful error message.. I'd even cached the promise implementation on db up front, but there were several places it wasn't being used :woman_facepalming:\nUpdate to 5.7.1 and you should start seeing more informative error messages in transactions/tasks.. I don't know how I missed the one in executable, but loaders don't have access to db and shouldn't be getting called in a transaction.... although Postgres does have transactional DDL so maybe it'd work? I'll have to look into that.. All looks good, thank you! Published v5.7.2.. Autoincrements are fine; your problem is in your CREATE TABLE definition :) You've got a column named text instead of full_name (and it's an int, for good measure). The full_name in your save() call isn't a valid column, so you effectively have a no-op instead of the insert you want.. Good catch, thanks! Released v5.7.3.. This is one of the ramifications of save not being a real \"upsert\": it doesn't play well with primary keys that aren't generated from sequences or functions. Specifically, it will generate an UPDATE statement for tables like account no matter what, since creating a new record means you include an email. If you have to set a value for the primary key in new records, you need to use insert, although I note the docs don't currently mention this.\nFor what it's worth, I have only rarely had reason to prefer a natural over a synthetic key, and emails and other plausibly-mutable facts in particular make bad primary key candidates due to the possibility of breaking external links and references (bookmarks, reports, etc) when they change.. Your guess that the camel casing is interfering with introspection seems to be on the money :) The receive option can be set after initialization, so you'll want to start looking in the reload function in lib/database.js to ensure that the driver (this.pgp) doesn't have it set while the reload is actively going on.. I had the name wrong -- try this.instance (the actual instantiation of pg-promise) not this.pgp.. I think it'd be useful! What you'd need to do is cache instance.$config.options.receive in a variable at the beginning of the reload function, delete it, and put it back at the end. For testing, look in test/connect.js; you can copy and paste the \"returns a database connection\" testcase and add the driver config with receive to validate the new behavior. It should be fairly straightforward as these things go, but let me know if it gets confusing.. 1. It failed because this.instance.$config was undefined when you tried to pull options.receive off. I think that's a peculiarity of reloading within a transaction instead of the global context; just make sure $config exists before you try to do anything with it.\n2. I'd check whether receiveOptionsCached is truthy (if (!!receiveOptionsCached) ...) before restoring; that way, if there wasn't anything there you don't need to worry about what to put back.. Everything looks good, thank you! I've released v5.7.5.\nIf there's other stuff you're interested in tackling, feel free to have at it and I'll do my best to field questions :). congrats! And beware, it's definitely habit-forming..... :metal: :joy: :metal:. @bighappyworld I can't pull a commit from a deleted repository to credit you, but you're right that that could be clearer. I've updated the docs, thanks for bringing it up!. I did a little digging and I think you're right that the inner clone doesn't need to be recursive! Fire away with the pull request whenever you're ready :). published as v5.7.6, thank you!. I think this is always going to be true, since searchArgs creates a QueryOptions no matter what and queryOptions is a function on that. The default sort will be by the primary key column if the table has one or by 1 (first column in result set) otherwise, so you should be able to just add args.options.queryOptions() to the sql no matter what.\n. I think calling it dropTable instead loses the consistency with createDocumentTable but makes it more obvious that it's generally applicable -- there's nothing specific to document tables in the functionality.\n. Some dead code in here, and it looks like you're never calling this function with the collection parameter set -- were you going for dynamically dropping and removing views?\n. You want to delete stuff here: setting undefined doesn't actually remove the key so Object.keys(), hasOwnProperty, etc still see something there.\n. Oh, no, createDocumentTable should definitely stay as-is -- it's just dropDocumentTable will work equally well on a document or any other table, so there's no need to call it out specifically.\n. I was going to say this testcase is redundant but shouldn't it return the 'Starsky and Hutch' row from the default test database?\n. oh whoops, I misread that, yeah. In that case I think we only need one test for filtering.\n. eslint is complaining because this line is missing a semicolon.. you've got a missing apostrophe here which is messing up the format. I think the db.query statement is sufficient, though -- the effect is already demonstrated with the schema above so there's no need to duplicate the sample input/output here too.. I think you should be able to recurse by calling generateConjunction directly and appending the new params and predicates as with the generateDisjunction invocation immediately above; from what I can see, directing it through generateInnerConjunction just adds another stack frame.. This is a good start but if you look at the next few testcases in describe('with subgroups'), there are three more validating disjunctions ('should not pollute other fields', 'should return a usable predicate if only given one subgroup', and 'recurses'). I don't think the 'one subgroup' test is necessary but the others would be really useful, especially 'recurses'.\nIt might be clearest if you were to copy+paste the describe and separate conjunctions and disjunctions that way.. oh that's right, my bad :woman_facepalming:. \"as an array of objects\" to be precise.. Can you add to this that the documents being saved have to be all new or all existing?. Since the function also exists on Writable a note about it would also be good under the writable-level saveDoc section near the end of this file.. yes, but this is as good as it's going to get: you can't batch updates the same way you batch inserts unless you're setting the same body value on every record.. ",
    "Sharkwald": "@xivSolutions thanks! I'll be honest, I'm a bit nervous about this, as it's all new to me. \nThe solution I have is very naive -- I'm just ensuring that any refs to db objects that I've come across are wrapped in double quotes. It's worked in the tests I've written and when I've played about in the REPL against Chinook. You can see the in prog here. Do let me know if this is an unsuitable approach.\n. Thanks Rob! There's another minor wee issue surrounding the PK being used in the order by when using findOne. I think  my PR solves it though.\n. @xivSolutions that's the version of Chinook I used, yeah.\nThat's kind of why I'm pro supporting databases with non standard casing; it means that if you're a developer without full control of your app's DB schema, you may or may not be able to use MassiveJS OOTB.\nI'm slowly updating my fork to support stuff like Add/Update/Delete operations on vanilla Chinook Tables, and so far, it's generally a matter of keeping things like the table names and PK names delimited. \n. I may be missing the wood for the trees here, but so far just adding quotes to the generated sql is doing the job for the Ins/Upd/Del functions. Using the new delimitedTableName, Where.forTable() and adding a new delimitedPrimaryKeyName() are keeping things relatively clean. So far the only additional places requiring further intervention are the preparation of the insert and update statements, and they're not especially onerous, e.g. update now contains:\n_.each(fields, function(value, key) {\n  f.push(util.format('\"%s\" = $%s', key, (++seed)));\n  parameters.push(value);\n});\nWhich isn't the end of the world, surely?\n. You're most welcome! And thanks to you and @xivSolutions for making my first attempt at open source contribution so pleasant :).\n. ",
    "paulxtiseo": "Not sure if this issue is related or should be its own ticket, but I have a function in postgresql as:\nsql\nCREATE OR REPLACE FUNCTION \"getOrderByOperator\"(IN opid bigint)\nNow, MassiveJS gives me a db.getOrderByOperator to call. But, when I call it, it errors with [error: function getorderbyoperator(unknown) does not exist]. True, it does not exist, but only because it's (I'm assuming) lowercasing the whole thing rather than using it as-is.\nIs this fixed? Fixable? Is there a workaround short of constraining to snake casing or abandoning MassiveJS altogether?\n. @neverov, so did you find that calling something like Promise.promisifyAll(db) did not promisify the table-specific functions, like db.yourTable.save()? Seems like promisifyAll(db), when I used it, only did the built-in top-level methods and the postgresql-derived functions. What, if anything, did you do in practice for this?\n. I think I have a fix. I am changing line 263 from:\nsql = \"select * from \" + fn.name;\nto\nsql = util.format(\"select * from \\\"%s\\\"\", fn.name);\nDoes that seem correct? This will force all sql to use quoted identifiers.\n. :+1: \n```\n$ npm test\n\nmassive@2.0.4 test /Repositories/~Personal/massive-js\nmocha .\n\n0 passing\n```\n. Ah, true. I didn't see any red, so I too quickly assumed passing tests.\nSo, not sure why tests didn't run? I did run npm test from the CLI in the Massive root directory... Looking into it...\n. Strange. Never had this before. And I did confirm I have an up-to-date Mocha.\n```\n[~/Repositori...onal/massive-js] $ mocha\nDocument queries\n    1) \"before all\" hook\n0 passing (34ms)\n  1 failing\n/usr/local/lib/node_modules/mocha/lib/reporters/base.js:197\n      var match = message.match(/^([^:]+): expected/);\n                          ^\nTypeError: undefined is not a function\n    at /usr/local/lib/node_modules/mocha/lib/reporters/base.js:197:27\n    at Array.forEach (native)\n    at Function.exports.list (/usr/local/lib/node_modules/mocha/lib/reporters/base.js:161:12)\n    at Spec.Base.epilogue (/usr/local/lib/node_modules/mocha/lib/reporters/base.js:317:10)\n    at Runner.emit (events.js:129:20)\n    at Runner.uncaught (/usr/local/lib/node_modules/mocha/lib/runner.js:593:8)\n    at process.uncaught (/usr/local/lib/node_modules/mocha/lib/runner.js:612:10)\n    at process.emit (events.js:107:17)\n    at process._fatalException (node.js:236:26)\n```\n. Stupidly, that was it. I modified the connection string and it ran the tests.\n43 tests failed, but all for TypeError: Cannot read property 'findDoc' of undefined. Or, for searchDoc.\n. ",
    "dpen2000": "Yeah I was testing on Windows (if that makes a difference) and this was failing for me. I install massive via npm. Then I save this to index.js\njavascript\nvar massive = require('massive');\nmassive.connect({connectionString:'url'},\nfunction(err, db) {\n});\nMy url is in heroku so I have to set an environment variable PGSSLMODE=require to connect. Once it is able to connect, I get:\n```\nfs.js:665\n  return binding.readdir(pathModule._makeLong(path));\n                 ^\nError: ENOENT, no such file or directory 'C:\\2015\\massivejstest\\node_modules\\mas\nsive\\db'\n    at Object.fs.readdirSync (fs.js:665:18)\n    at walkSqlFiles (C:\\2015\\massivejstest\\node_modules\\massive\\index.js:93:17)\n    at Massive.loadQueries (C:\\2015\\massivejstest\\node_modules\\massive\\index.js:\n30:3)\n    at C:\\2015\\massivejstest\\node_modules\\massive\\index.js:169:8\n    at Object.next (C:\\2015\\massivejstest\\node_modules\\massive\\index.js:51:7)\n    at null.callback (C:\\2015\\massivejstest\\node_modules\\massive\\lib\\runner.js:5\n2:16)\n    at Query.handleReadyForQuery (C:\\2015\\massivejstest\\node_modules\\massive\\nod\ne_modules\\pg\\lib\\query.js:80:10)\n    at null. (C:\\2015\\massivejstest\\node_modules\\massive\\node_modules\n\\pg\\lib\\client.js:158:19)\n    at emit (events.js:117:20)\n    at CleartextStream. (C:\\2015\\massivejstest\\node_modules\\massive\\n\node_modules\\pg\\lib\\connection.js:109:12)\n```\nHere it's looking for db folder in node_modules\\massive\\, not in massivejstest\\ where my script resides. My understanding is at this point the current executing file is massive-js's https://github.com/robconery/massive-js/blob/master/index.js, not my index.js. The dirname documentation isn't very clear that this would be the case but the __filename documentation says:\n\"For a main program this is not necessarily the same filename \nused in the command line. The value inside a module is the path to that module file.\"\nBoth have in their documentation a line similar to:\n__dirname isn't actually a global but rather local to each module.\nMassive-js actually uses __dirname with the assumption it will be relative to it's own files here:\nvar tableSql = __dirname + \"/lib/scripts/tables.sql\";\n(https://github.com/robconery/massive-js/blob/master/index.js#L34)\nI think the reason the tests succeed is they all use the helper that sets the scripts folder variable so the __dirname in index.js is never used.\nMaybe it's only on windows that the call fails when the directory does not exist.\n. Thanks! The tests would work fine I think. The scripts path for the tests is set here:\nvar scriptsDir = path.join(__dirname, \"..\", \"db\");\n(https://github.com/robconery/massive-js/blob/master/test/helpers/index.js#L5)\n__dirname here I would expect to give [locationOfMassiveSource]\\test\\helpers\\ (because the currently executing file is in helpers folder), the .. takes you down to test\\ and then the db folder is at that level so it finds the db folder correctly there.\nFor what it's worth, my node version is 0.10.33. Great project by the way: really like the jsonb functionality in particular.\n. ",
    "neverov": "It's quite easy to do by yourself if you want to use promises. For example, if you use bluebird library you can try the following:\n``` javascript\nvar Promise = require('bluebird'),\n    massive = require('massive');\nvar connect = Promise.promisify(massive.connect);\nvar query = function(f) {\n  return connect({ connectionString: yourConnectionString }).then(function(db) {\n    if (db.yourTable) { // or check if table corresponds to specific schema or any other condition\n      Promise.promisifyAll(db.yourTable);\n    }\n    return f(db);\n  });\n};\n```\nand then you can use it like this:\njavascript\nvar queryPromise = query(function(db) {\n  return db.yourTable.findOneAsync(id).then(function(result) {\n    // do smth\n    return result;\n  });\n});\n. @paulxtiseo there is an example above, it looks a little bit dirty but definitely worked at the time. I mean:\njavascript\nreturn connect({ connectionString: yourConnectionString }).then(function(db) {\n  // THIS PART \n  if (db.yourTable) { \n    Promise.promisifyAll(db.yourTable);\n  }\n});\n. ",
    "luisrudge": "Nice. THanks!\n. ",
    "FransBouma": "I agree, the support for other databases is piss poor. At least add Sybase Adaptive Server Enterprise support, it now looks like it only supports the databases the maintainer likes. \n. ",
    "mstum": "While we're on it, can we also get native support for XML datatypes, incl. XPath Queries and XSLT to hook it directly into a view engine?\n. ",
    "vintagexav": "Additional info, confirming that everything goes right: \nnode massive.js -c postgres://user:pass@host:port/database?ssl=true\nWhere, in runner.js I added \n- console.log(\"runner\"); after DB.prototype.query = function () {\n- console.log(\"connect\",err); after assert.ok(err === null, err);\n- console.log(\"query\",err); after db.query(args.sql, args.params, function (err, result) {\nWhere, in index.js I added\n- console.log(\"loaded tables\"); after massive.loadTables(function(err,db){\n- console.log(\"loaded functions\"); after massive.loadFunctions(function(err,db){\noutputs:\nrunner\nconnect null\nquery null\nloaded tables\nrunner\nconnect null\nquery null\nloaded functions\n. In Navicat:\ntables.sql:\nselect tc.table_schema as schema, tc.table_name as name, kc.column_name as pk\nfrom \n    information_schema.table_constraints tc\n    join information_schema.key_column_usage kc \n        on kc.table_name = tc.table_name\nwhere \n    tc.constraint_type = 'PRIMARY KEY'\norder by tc.table_schema,\n         tc.table_name,\n         kc.position_in_unique_constraint;\nOutputs 47 records\nTakes, by itself, 0.25-0.30 secs - which is already unacceptable if this has to happen before each client's request. \nfunctions.sql:\nSELECT routines.routine_schema as \"schema\", \nroutines.routine_name as \"name\", routines.data_type,\n(\n  select count(1) from information_schema.parameters \n  where routines.specific_name=parameters.specific_name\n    and parameter_mode = 'IN'\n) as param_count\nFROM information_schema.routines\nWHERE routines.\"routine_schema\" NOT IN('pg_catalog','information_schema')\nAND routines.routine_name NOT LIKE 'pgp%'\norder by routine_name\nOutputs 1245 records\nTakes 8-9 secs \nIn an API which cruise speed averages 10-15ms that's quite a punch in the face.\nIs Massive supposed to be used for production purposes or only for data crunching?\n. @xivSolutions \nnode massive.js -c postgres://user:pass@host:port/database?ssl=true does not load anything, it just runs connect only - I didn't modify anything there. \nThe measurement happens in index.js, after exports.connect = function(args, next){\n. Removing the params in the routines selection is accelerating things quite a bit (but still a unusable 1/2 sec long response for 1160 records):\nfunctions.sql:\nSELECT routines.routine_schema as \"schema\", \nroutines.routine_name as \"name\", routines.data_type\nFROM information_schema.routines\nWHERE routines.\"routine_schema\" IN('public')\n. I suspect postgis to be the source of the creation of most functions.\n Still, this just raises a flag. \nWhy load all tables and all functions names every time whereas functions and tables could be passed as a String for free? \n. I will try to look at it next week @xivSolutions. \nI'd be interested seeing what's the best way to call massive.connect only once @robconery \n. app.set('db',db_instance) and eq.app.get('db') seems the most straightforward - definitely worth to go in the README. \nHowever:\n- 10-20 seconds to init the db is a lot of downtime every time the app is rolled out. \n- Perfs are still just so bad. a SELECT FROM WHERE id = 1 on a table with approx. 1k rows is <100ms using node's pg.js connect and query functions whereas it sums up above 500ms for Massive with run func (and I am rounding in favor of Massive)\n@xivSolutions:\nFYI  all my tests here and above are performed over network, not localhost. \n@robconery:\nYes, changing or getting rid of the function read at the beginning brings connection time down to <1sec (mostly the tables read is weighting there) see https://github.com/robconery/massive-js/issues/43#issuecomment-86817642 \nAlso all my own functions start with same prefix - that'd make the trick for my particular case to filter:\nSELECT routines.routine_schema as \"schema\", \nroutines.routine_name as \"name\", routines.data_type,\n(\n  select count(1) from information_schema.parameters \n  where routines.specific_name=parameters.specific_name\n    and parameter_mode = 'IN'\n) as param_count\nFROM information_schema.routines\nWHERE routines.\"routine_schema\" NOT IN('pg_catalog','information_schema')\nAND routines.routine_name LIKE 'myprefix%'\norder by routine_name\nHowever I realized most of the heavy work is caused by the param_count computation\nI couldn't find anything that distinguishes my functions from those other functions with: \nSELECT *\nFROM information_schema.routines\nWHERE routines.\"routine_schema\" NOT IN('pg_catalog','information_schema')\nAND routines.routine_name NOT LIKE 'pg%'\norder by routine_name\n. Gonna break it down.\n. Looks like pg.js I'm using is optimizing use of the connection / leverages cache.\nThe first request with pg is pretty much as slow as massive select. \nHowever the next ones are significantly faster with pg. \n*local srvr->real_world db: *\n1st pg select: 500ms\n1st massive select: 500ms\n2nd pg select: 100ms\n2nd massive select: still 500ms\n*real srvr->real_world db: *\n1st pg select: 25ms\n1st massive select: 25ms\n2nd pg select: 1-2ms\n2nd massive select: still 25ms\nThen I broke down each request.\nI realized pg.connect is 0ms after the first request (then sometimes reaches back to similar level than massive.connect) whereas massive.connect is always on same bad level (as bad as 90% of the time needed to perform the select) and never reaches the close to null time that pg.connect reaches.\nMy measurement of Massive.connect is:\nconsole.time(\"massive-connect\");\n  pg.connect(this.connectionString, function (err, db, done) {\n    console.timeEnd(\"massive-connect\");\nWhy do you need all the extras provided by pg? Maybe you guys should give a shot to pg.js instead of pg module.\n. ",
    "tracker1": "@robconery another suggestion would be to keep a cache of pools, and return the same pool for the same connectionstring/parameters...  That should reduce the chances of accidentally creating multiple pools that load in the same resources... I did this for mssq-ng which worked out pretty well.\n. ",
    "nmccready": "Thanks for the quick response. I'll get back to you w/ better questions/examples when I'm not on a mobile device and exhausted. \n. ",
    "mj1856": "I'll see if I can give you a concrete example shortly.  But yes - I understand that Massive is just handing things from one to another.  In doing so, I assume it's responsible for mapping types from one system to the other - and that's where the trouble usually lies.\nUnless the mapping is already being handled by a dependency?\n. Nice discussion here.  The key to understanding this is that not all contexts are equal.  The advice \"always use UTC\" or the equivalent we're discussing here of always having a time zone, is very context specific.  It holds up for timestamping which is one of the most common things we do, but it doesn't apply to necessarily everything you might do with dates and times in computing.\nFor date-only values, the example of a birthdate is a good one.  Mine is 1976-08-27.  It's not 1976-08-27T00:00:00, nor is it 1976-08-27T00:00:00Z or 1976-08-27T00:00:00-07:00.  The same thing applies to any kind of anniversary dates, such as work hire date.  If you want to think about time/timezone context on a date like this, consider the arguments I made in this blog post.\nFor times without dates, consider business opening and closing hours.  A business may be open from 8:00 AM to 6:00 PM, which is usually based on the local time zone of the business location.  However, the offset from UTC may be different depending on whether or not daylight saving time is in effect, so storing a time+offset or a UTC time would be problematic.\nFor combined dates and times, consider that there's still value in timezoneless data, often referred to as a \"floating time\".  There may be a time zone, but it comes from some other context which may vary.  For example, NBC may have a schedule where tonight's evening news is aired across the US at 2016-04-27T18:00, but the exact instant depends on each station's local time zone.  Another example of this is the alarm clock used by most mobile phones.  Mine is scheduled to fire next at 2016-04-27T06:30.  Even if I get on a plane and travel to another time zone, it will still go off at that time in whatever local time zone I'm in.   There are many other cases, I'm just giving a few examples.\nAs it applies to this discussion, I have to agree with @rasantiago.  With all due respect @robconery, the decision of what context should be in a date/time value is something the app developer should decide, on a case-by-case basis.  The dev stack should enable all possibilities.\nFor a recent example of fail in this department, consider OData, which decided to drop support for DateTime in v4 due to similar arguments (no time zone).  This has created quite a lot of pain for developers, as evidenced by OData/WebAPI#136 and http://stackoverflow.com/questions/25189557/how-to-get-web-api-odata-v4-to-use-datetime\n. ",
    "rasantiago": "I have a good example of this problem.  Struggling mightily with it right now. \nI have a date field called start_date which is of data type date.  When someone creates a new record through our app the date ends up getting set properly.  But, when the date is retrieved massive ends up tacking on the default timezone (in our case UTC) so technically it comes back as Midnight UTC of the date.  When someone in Pacific timezone pulls up the date it is set to 8 hours prior (the difference between UTC and Pacific) which now makes it look like the date has been moved to the prior day.  \nIn short, Massive is translating data type date into datetime and pushes the date around based on default timezone from the server which can be very different from timezones of the end user.  \nNot sure what the correct solution to this problem is.\n. Thanks for the response!  So I think the core issue here may be that there is not a pure Date construct to map the Date data type from postgres into.  I wonder if we could have an option to choose to map to JS Date (default), JS Date with explicit locale declaration or simply to text.  In my case the last one (text) would resolve the issues instantly but that is just the case I am dealing with and do not know how broadly that would apply.\n. So I really love Massive and am very much a fan of the minimalist approach to most things DB.  But, I have to respectfully disagree with @robconery.  The correct answer, IMHO and with the benefit of 25 years of software engineering, is that Postgres has it correct and the PG module has it wrong (because in some ways JS has it wrong) and this error is simply passed along through Massive.  \nHere is my reasoning.  Postgres has a DATE data type which is different from TIMESTAMP data type and for that matter TIME data type.  Each of these data types within Postgres is handled consistently and elegantly.  DATE simply holds a date with no care for time or timezone. TIME by default holds a time with no concern for date or timezone.  TIMESTAMP data type by default only cares about date and time with no concern for timezone. DATE, TIME and TIMESTAMP can optionally support timezone but it is not required.\nThe problem happens when we try to map all these Postgres data types into JS objects.  All of these data types from Postgres get mapped into the Date Object in JS.  The Date object in JS is improperly named since it requires date, time and timezone.  Thus, while Postgres will allow us to handle DATE and TIME cleanly without care for timezone, the JS Date object forces us to tack on time and timezone which then causes a number of problems when we put the data back into Postgres.  All this makes engineering (and architecting) an application more complicated than needed in my opinion.\nThe real answer, again IMHO, is for JS to support date and time objects without requirement for timezone.  Short of that there are a lot of ways to engineer around the issue.  I make use of moment.js and moment-timezone.js to clean up a lot of the issues along with a few custom filters in Angular.  This discussion thread may finally give me the last push I need to engineer lightweight Date and Time objects and create an augmentation for PG and/or Massive to cleanly answer these issues.\nLastly, and again with all due respect to you, I must disagree with you again @robconery.  @swissspidy is completely correct and in fact is trying to consistently handle the timezone issue in a way that does not corrupt his data.  Pushing everything into UTC is one approach to dealing with this issue and it would be nice to set this through Massive. \n. @xivSolutions All good points and I agree.  My caveat about the PG driver getting it wrong because JS gets it wrong was my ineloquent way of saying what you said much more clearly (i.e. its an impedance mismatch between JS and Postgres)\nTotally appreciate the discussion.  Many thanks!!\n. @robconery thank you so much for reopening this and I wish all online technical discussions were this civil.\nSo there are many applications and use cases where timezone is not necessary.  In those cases time and date are meaningful without timezone and it would be nice to not have to deal with timezones for those use cases.  Back in the day (dang i'm getting old) I loved Oracle for this reason.  I never had to deal with timezone unless I explicitly wanted to.  \nWith respect to Postgres, I think it is important to look at the bits that get written to memory and disk.  In the case of the DATE data type, it only uses 4 bytes and has a resolution of 1 day.  So there are not enough bits there to store time or timezone.  Date is simply just a date and does not have an implied timezone to its definition. But, you are correct that there are a number of situations where Postgres will infer a timezone (e.g. Date/Time math).\n. @robconery Timezones are generally unnecessary when the scope of concern is inherently local.  \nThere are countless examples. Years ago I helped build a system for a city to track servicing of municipal controlled public areas (e.g. the median strips that need mowing, the traffic circles that have bushes that need trimming, etc.).  There is nothing about that system or about the reporting from that system that is not local (i.e. Australians are not a concern).  They just want to make sure every area gets serviced every N number of days and to know when any area hasn't.\nBy way of another example, right now I am working with organizations that offer kids day camps.  The dates and hours of the camp are inherently a local concern (i.e. no one is flying their kids in from Australia to attend these camps).  All the users of that system are local and always will be.  Thus the date and time of camps are not recorded with timezone. \nInterestingly, when parents register online we do record the full time/date and tz because we have to coordinate with a credit card processor who is not local.  In that latter case, the scope of concern is not local because of the credit card processor.  \nBut, I understand your concern and I do think its worth thinking through when date, time, datetime and timezones are needed.\n. @robconery All really good points and very realistic scenarios.  I've had to work from the point of view of forensics and the point of view of prepping records for a client who was being sued.  \nWhen I engineer systems I always keep some standard fields like created_at and updated_at.  Those never get seen by most users and are populated through triggers.  They  are there for auditing purposes.  Those fields, of course, have full timestamps with tz info.  Also, there are some change log tables I use for particularly sensitive data which also record tz.  Again, this is not seen by the user but is there for a number of concerns.   In addition, whenever I am contracted to architect or engineer a system I take special interest in asking about legal liabilities and of course make sure we have adequate tz coverage on any particularly sensitive data.\nStill, when capturing fields like a birthdate or day of a kids camp or similar, I still stick with simple date.  Interestingly, I did experience a legal situation where the choice of the simple DATE  and simple TIME data types provided a stronger protection for a client.  Specifically, the date and time as captured from the user could not be corrupted by shifts from differing time zones.  Knowing the date and time as it was entered by the user became the critical question in a dispute a client of mine was having. \nAnyway, we all do our best to engineer for the future but only get paid for answering the problems of the present, often to our chagrin.\n. ",
    "swissspidy": "Just as a follow-up question:\nI have a database with a date column using the timestamp data type (i.e. no time zone information stored) and a row with the value 2016-03-20 14:10:01.000+00.\nMassive returns that value as Sun Mar 20 2016 14:10:01 GMT+0100 (CET), which results in Sun Mar 20 2016 13:10:01 UTC, i.e. one hour earlier. Isn't that wrong?\nLooks like that's a bug (or feature, rather) in the pg module, which always uses the system time zone for dates.\nHowever, you can change this behaviour by setting the parseInputDatesAsUTC option to true. I just don't see a way how to modify pg.defaults via Massive.\nSee https://github.com/brianc/node-postgres/wiki/pg#pgdefaults for the defaults documentation.\n. That's a valid point. I'm dealing with various timezone issues daily and know how helpful timezone information can be.\nMy plan was to store the date in UTC and then display the value in the end user's time zone, i.e. doing the conversion as late as possible.\nAfter my last comment I switched to using the timestamp with time zone data type in Postgres to always store the date with the timezone (even if it's just UTC). Now, the returned date no longer differs from the expected value.\nIt now seems that I don't need #240 for my use case at the moment, but you never know.\n. ",
    "cwgabel": "I have user_code tied to a default value using users_sequence (see the gist for the DDL). The sequence must be working for the INSERT that did work in the code (via db.run) for it did not contain a user_code value (per DEFAULT). I added the db.run just to make sure I could INSERT correctly and help alleviate reason as to why the db.users.save is failing.\n. Thanks for looking at this. \nI got the same same thing with userstest.  Not sure what the update is all about.\nLogs still show an update...\n015-04-27 19:59:33 CDT 42P00ERROR:  inconsistent types deduced for parameter $1 at character 37\n2015-04-27 19:59:33 CDT 42P00DETAIL:  text versus character varying\n2015-04-27 19:59:33 CDT 42P00STATEMENT:  UPDATE \"userstest\" SET \"username\" = $1, \"firstname\" = $2, \"lastname\" = $3, \"password\" = $4, \"birthday\" = $5, \"provider\" = $6, \"salt\" = $7 \n    WHERE \"email\" = $1 RETURNING *\n. Thanks for your help, works great now. \n. ",
    "paulovieira": "I note a bit of irony in your reply, which is probably justified given the lack of clarity in my message.\nObviously this is only a \"problem\" for those who don't want (or can't) upgrade from 0.10 to 0.12. But given that 0.12 is only a few weeks old and people have had apps in production using 0.10 for a long time, I would say the percentage of developers who would like to try massive and are using 0.10 is quite large. \nI would suggest at least adding the \"engines\" property to package.json (or simply adding the 0.12 requirement to the readme). If that information is not present, I would assume the module works with 0.10.\nBy the way, the parse-filepath module seems to be a good substitute for path.parse:\nhttps://www.npmjs.com/package/parse-filepath\nIf there's interest, I'm happy to submit a PR with this change, thus making massive compatible with 0.10 (hopefully).\n. Meanwhile I found the bin entry in package.json. But as I wrote in the pr, the given instruction (bin massive) are not working in linux.\nShouldn't the bin entry be in the scripts property, along with test?\n. Your sarcasm is unnecessary. My intention when opening this issue was simply to report something that is not (or was not) working and a possible workaround, so that others won't loose time.\nI see the readme wasn't changed about this. I can submit a pr if you're interested.\n. ",
    "stephensong": "Hi Rob,\nMany thanks for responding, and for massive too.  I like it a lot.  \nYes, I am using Version 2.0\nThe real code contained a callback  - which was called exactly 10 times, never with an error.\nI also suspect it to be 'an async issue', or more likely, me being a dufus.   But when I saw the above code  suffered no such issue I was at a loss to figure out the difference.  Fwiw I also went looking in the postgres logs but found nothing helpful there either.   I know I should really spend more time trying to figure it out, rather than raising half-assed issues, but I am under some time pressure at the moment.  Sorry.\n. I will revisit later today, and see if I can reproduce it with a standalone example.  Thanks.\n. Ok,  Thank you both for your attention.  I am grateful.\nI've given this all another red hot go, and am still none the wiser.  I've been over and over it, and cannot see what I might be doing wrong.  The only thing I can think of, is that I am running on Windows.   Or might it be that I am running Node with the --harmony flag (I am using Koa for my http server).\nFwiw. here is pretty much all the relevant code:\n```\n\"use strict\";\nlet _ = require(\"underscore\")._;\nlet url = process.env.DATABASE_URL;\nlet massive = require('massive');\nlet pg = require('pg');\n// following copied from pg, as I could not see a way of accessing them directly\nfunction arrayString(val) {\n  var result = '{';\n  for (var i = 0 ; i < val.length; i++) {\n    if(i > 0) {\n      result = result + ',';\n    }\n    if(val[i] === null || typeof val[i] === 'undefined') {\n      result = result + 'NULL';\n    }\n    else if(Array.isArray(val[i])) {\n      result = result + arrayString(val[i]);\n    }\n    else\n    {\n      result = result + JSON.stringify(prepareValue(val[i]));\n    }\n  }\n  result = result + '}';\n  return result;\n}\nvar prepareValue = function(val, seen) {\n  if (val instanceof Buffer) {\n    return val;\n  }\n  if(val instanceof Date) {\n    return dateToString(val);\n  }\n  if(Array.isArray(val)) {\n    return arrayString(val);\n  }\n  if(val === null || typeof val === 'undefined') {\n    return null;\n  }\n  if(typeof val === 'object') {\n    return prepareObject(val, seen);\n  }\n  return val.toString();\n};\nfunction prepareObject(val, seen) {\n  if(val.toPostgres && typeof val.toPostgres === 'function') {\n    seen = seen || [];\n    if (seen.indexOf(val) !== -1) {\n      throw new Error('circular reference detected while preparing \"' + val + '\" for query');\n    }\n    seen.push(val);\nreturn prepareValue(val.toPostgres(prepareValue), seen);\n\n}\n  return JSON.stringify(val);\n}\nfunction dateToString(date) {\n  function pad(number, digits) {\n    number = \"\"+number;\n    while(number.length < digits)\n      number = \"0\"+number;\n    return number;\n  }\nvar offset = -date.getTimezoneOffset();\n  var ret = pad(date.getFullYear(), 4) + '-' +\n    pad(date.getMonth() + 1, 2) + '-' +\n    pad(date.getDate(), 2) + 'T' +\n    pad(date.getHours(), 2) + ':' +\n    pad(date.getMinutes(), 2) + ':' +\n    pad(date.getSeconds(), 2) + '.' +\n    pad(date.getMilliseconds(), 3);\nif(offset < 0) {\n    ret += \"-\";\n    offset *= -1;\n  }\n  else\n    ret += \"+\";\nreturn ret + pad(Math.floor(offset/60), 2) + \":\" + pad(offset%60, 2);\n}\n// my own little helper\nlet formatValue = function(x) {\n    if(x === null || typeof x === 'undefined') return 'null';\nlet y = prepareValue(x);\nlet typ = typeof x;\n\nif (typ != 'number' && typ != 'boolean') y = \"'\" + y + \"'\";\nreturn y\n\n};\nlet insert_rows = function(tblName, data, next) {\n    if(!data) throw \"insert should be called with data\";\n    if (!_.isArray(data)) { data = [data]; }\nvar delimitedColumnNames = _.map(_.keys(data[0]), function(key){return util.format('\"%s\"', key);});\nvar sql = util.format(\"INSERT INTO %s (%s) VALUES\\n\", tblName, delimitedColumnNames.join(\", \"));\nvar rows = _.map(data, function(r) { return util.format(\"(%s)\", _.map(_.values(r),formatValue).join(\", \") ); });\nsql += rows.join(\",\\n\") + \"\\n RETURNING *\";\n//console.log(sql);\npg.connect(url, function (err,client, done) {\n    client.query(sql, null, function (err, res) {\n        done();  // return client to pool\n        if (!err) rslt = res.rows[0];\n        if (next) next(err,rslt);\n    });\n});\n\n};\n//  I added the following loadSync routine in manually, as it wasn't in the npm release.\n//  with the following additional line at the top:\n//   'var result = null;'\n//  as it otherwise referred to a global result var.\n//  I tried replacing this with the connection syntax used in xivSolutions' example above, but got\n//  precisely the same results :-(\nlet db = massive.loadSync({connectionString: url});\nlet insert_solution = function (soln) {\n    // this line executes correctly (225 times)\n    console.log(\"inserting solution for : \" + soln.puzzle);  \n// this line works fine - all 225 rows inserted\ninsert_rows('solutions', soln, function(err, rslt) {\n\n// if we comment out the above line, and replace it with the one below,\n// we only ever insert 10 records, and the callback is only called with the successes.\n//db.solutions.insert( soln, function(err, rslt) {\n\n    if (err) console.log(\"failed : \" + err);  // never executes, despite 215 records going missing \n    else console.log(\"solution inserted for : \" + soln.puzzle);\n});\n\n};\nlet solns = // an array of, in my case, 225 rows loaded from elsewhere\n//  the technique of calling insert just once with all rows batched together works fine \n// either way, so  I have not included it here.\nsolns.forEach(function (r) { insert_solution(r); });\n```\n. Thanks to you both.  To John: yes I tried what you suggested (as indicated in my comments in the code), and it changed not one iota.  I have tried (and removed) the deasync thing in several contexts now, and I am satisfied that it itself is not the source of the issue.\nIf it is indeed 'an async issue', then why does the code I posted work where the massive code doesn't, when they appear to be entirely equivalent from the async / sync perspective.  Given that neither of you can reproduce the issue I guess we will have to just leave it there.  The issue is real though - for me at least, and I just hope that an explanation will present itself at some point in the future. Until then, I will be feeling like I am building castles on sand.  Whilst it is true that in this particular case there is an obvious and workable alternative, one can easily envisage circumstances where such was not the case.   Should I close the issue?\n. Ok.  Back from a break, I have looked into this yet again, and have found what is causing my problem.  In the file runner.js, starting around line 30, the following lines appear:\n```\npg.connect(this.connectionString, function (err, db, done) {\n  //throw if there's a connection error\n  assert.ok(err === null, err);\ndb.query(args.sql, args.params, function (err, result) {\n    //we have the results, release the connection\n    done(db);\n```\nIf I change the last line to remove the parameter, i.e.\ndone();\nmy problem disappears!\nI have tried this out in my own code (as posted previously), and the (mis)behaviour is entirely consistent.  If the parameter is supplied to the provided 'done' function, then I only ever see 10 records inserted.   If not, everything is hunky-dory. \nObvious questions remaining:\n- Why is the parameter supplied in the done call?   The example in the pg wiki doesn't supply it.\n- Should it be there or not?\n- What is it supposed to achieve? \n- What is the downside of not supplying it? \n  and:\n- Why am I the only one seeing this problem?\nFor the moment I am going to remove it in my local copy, and hope nothing bad happens.\nThanks.\n. Yes, you are right.  The pk is not an integer - it's a uuid.  I see now, thanks.  (Except that I don't quite understand why it appeared to work when I remove the pk delete.)   \nPerhaps it should be made more explicit in the docs that (much of) massive is predicated on primary keys being autoincrement (or sequences or whatever pg calls them).\n. ",
    "mzedeler": "Thanks for the very, very fast feedback. I'm somewhat overwhelmed. I'll reply soon.\n\n. ",
    "AdrianRossouw": "That seems like something to leave up to the application, because it will know best how to manage it's configuration.\nAlso, It not entirely out of bounds that there might exist an application that has multiple massive instances. Any config file auto-loading would require jiggery pokery on the implementor's part.\n. One of the unfortunate cases where generating id's doesn't really work is where you might need to shard databases.\n. ",
    "warmuuh": "yes... thats what i thought... i helped myself with a small hack:\nhttps://github.com/warmuuh/massive-js/commit/5add55bd77bf6fa8a95cacd3d601cf7af7983303\nbut that is not very performant, i guess.. (though i am just using this to write some utilities, so performance is no problem..)\n. i edited the comment... but it mainly adds the parameter-part of the sql dynamically based on the given number of arguments\n. ",
    "tigernassau": "thks - agree 100% re only connecting one time - just was not sure how with massive.\u00a0 your respnse helps and wii rewrite example.\u00a0 would be good to pick one example whether it is artists, or our lead page signups or even std 5 min blog\nSent from Type Mail\nOn Jun 15, 2015, 04:53, at 04:53, John Atten notifications@github.com wrote:\n\nThe idea here is that you only ever call massive.connect() once.\nThere is a load penaly associated with the load process, so once you\nhave a massive instance, you're done with connect(), and you want to\navoid calling it again. You are correct, the docs really only show a\nfew snippets at this point. \nI am traveling at the moment, and can't actually test the following, so\nbeware I may sprinkle in a few mistakes. The concepts are basically\ncorrect though...\nOnce you use app.set('db', massiveInstance) the massive instance is\nthen available as a property on the app object:\n``` javascript\nvar configure = function(done) { \nvar connectionString = \"postgres://massive:password@localhost/chinook\";\n var scriptsDir = path.join(__dirname, \".\", \"db\");\n// connect to Massive and get the db instance:\n massive.connect({\nconnectionString : connectionString}, function(err, massiveInstance) { \n// Set a reference to the massive instance on Express' app:\n   app.set('db', massiveInstance);\n// Let the called know the db is ready...\n   done();\n });\n};\nconfigure(function() { \n // Run it...\n http.createServer(app).listen(port);\n});\n```\nFrom there (for example) you could define another module (say, named\n'routes.js') like so:\n``` javascript\nvar Routes = function(app) { \napp.get('/artists', function (req, res) {\n   var db = app.get('db');\n   db.artist.find(function (err, artists) { \n     res.send(artists);\n   });\n });\napp.get('/artists/:id', function (req, res) { \n   var db = app.get('db');\n   var id = Number(req.params.id);\n   db.artist.find(id, function (err, artist) { \n     res.send(artist);\n   });\n });\n};\nmodule.exports = Routes;\n```\nThere are a number of ways to work this into an Express application\n(and note, I am not an expert on Express at this point). But the\nover-arching idea is to call connect() once at application load,\ncache the instance, and use that cached instance to work with your\ndatabase. \n@robconery may have something to add, and/or set me straight in the\nabove, as I think he has worked with Express more extensively than I\nhave. \n\nReply to this email directly or view it on GitHub:\nhttps://github.com/robconery/massive-js/issues/69#issuecomment-112017216\n. On 07/02/2015 11:15 AM, John Atten wrote:\nWith the run function, looks like you are passing an int as an \nargument when no corresponding param is indicated in the SQL.\nThe function is not finding your call back.\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/robconery/massive-js/issues/84#issuecomment-118097101.\n\nThanks - noticed this too, but earlier effort to leave param out \nresulted in error - but putting in {} passes\nso maybe can add another example for select all\ndb.run(\"select * from mydocs;\", {}, function(err,records) {\n   res.render(\"myview\", {records:records});\n\nTiger Nassau, Inc.\nwww.tigernassau.com\n. ",
    "SimonDegraeve": "Indeed would be nice to have support for Promise.\nSee http://bluebirdjs.com/docs/api/promisification.html.\n. Well, it is 2015. If people want to use Promise, just by adding one export to the module, they can. It is not a big deal.\nI don't think, the \"feeling\" thing is pertinent here. For me it is just an abstraction like any others. If you use Express instead of Node HTTP Core, do you \"feel\" something? \nFor me the selling point for Promise is the developer experience: cleaner code, centralised error handling, serial-parallel, etc... \nIt is not about forgetting that the code with-in is async.\n. ",
    "sheldonkwok": "\"Most libraries can be promisified by requiring the library's classes (constructor functions) and calling promisifyAll on the .prototype. This only needs to be done once in the entire application's lifetime and after that you may use the library's methods exactly as they are documented, except by appending the \"Async\"-suffix to method calls and using the promise interface instead of the callback interface.\"\n. Yielding promises is already a supported feature in the latest versions of node so it would be nice to leverage this. I think it's a nice abstraction that makes async code easier to manage once you understand the concept. You don't have to use it but it's there and the concept is being improved on in es7 with \"async/await.\" \nEdit: link: https://thomashunter.name/blog/the-long-road-to-asyncawait-in-javascript/\n. ",
    "shawn-simon": "Ugh was about to try out this library then read your remarks in this thread...\n. Sorry, didn't mean to derail your thread. However, as nice as this project looks it won't work with my promise-based JS architecture. I would encourage you to learn and understand promises since they make your code much, much better. They are a bit difficult to grasp at first, so I understand your reasonings.\n. ",
    "vitaly-t": "\nI really want to avoid syntax/abstraction wonkiness and promises. I hate them.\n\nAnd that is really a subjective and narrow look at things that won't help other developers here, and won't help you either, in the long run. Sometimes it is useful to meditate on the new technologies to see what is good, instead of just scorning them.\nI am the author of pg-promise, and I can tell you from long experience that when working with databases and NodeJS, promises offer absolutely the best syntax for writing transactions, because the logic of transactions maps perfectly 1:1 to the logic of promises.\nAnd since the logic of connect->query->disconnect also maps into the promise chain naturally, the combination of the two allow the perfect symbiosis for managing connections and transactions in a single chain. No other approach can do the same.\n. >  I'm curious as to what kind of strain that might put on the database\nA bad one, as the database IO is the most expensive resource in a scalable system.\n\nIt sounds like you have a large number of records that need to be updated. No matter how you slice it, there is a lot of database writing that needs to be done here\n\nNot true, a single multi-row update requires just one query to update everything at once, and it is atomic in nature, meaning: if one update fails, none will succeed, so no transaction required, which makes it even more efficient.\nMulti-row update is the only right approach for the described sitiation.. > Multi-row updates can't apply different data to each row.\nThey can, and they do. See the very example this question started with, it shows you how it works. Applying different data to each row is exactly what it does.\ncopied:\nsql\nupdate users as u set -- postgres FTW\n  email = u2.email,\n  first_name = u2.first_name,\n  last_name = u2.last_name\nfrom (values\n  (1, 'hollis@weimann.biz', 'Hollis', 'O\\'Connell'),\n  (2, 'robert@duncan.info', 'Robert', 'Duncan')\n) as u2(id, email, first_name, last_name)\nwhere u2.id = u.id;. I wasn't going to mention pg-promise here, not to upset anyone, but since the author asked... :)\nThe library does it in a very generic way, it can generate in the same way:\n\nsingle-row insert\nmulti-row insert\nsingle-row update\nmulti-row update\n\nHere're some examples:\n\nMulti-row insert with pg-promise\nPostgreSQL multi-row updates in Node.js.\n\nBasically, it is all done through generic type ColumnSet that's highly reusable, and optimized for performance - it is smart enough to cache query generation, so every repeated query is generated much faster, with the absolute minimum of the query formatting that's required.\n. @robconery cool :) if you have any questions - let me know ;) By the way, pg-promise-demo offers a good idea for large projects.\n. > If you're updating in a loop, constantly, forever - that will not \"bottleneck\" your database.\nThis is what I argue against (not to start a flame here \ud83d\ude04) in Performance Boost.\n. That article, Performance Boost, was written for applications that require scalability, those that already exceed 1,000 queries per second, and on its way to 10,000 queries per second.\nEven for applications that do not require consistent scalability, many developers stumble upon how slow data imports can be. When you have an external CSV/file that contains way in excess of 10k records to be inserted or updated, is when you start taking the approach seriously. And this happens way more often than you might think ;)\nWith the approach I describe there, I was able to import a CSV with 10m records in under 1.5 minute. How long do you think it would take to import the same? :wink: Just a hint - the direct approach, query-by-query would take you at least 10 times longer :wink:\n. You are welcome! I've only started looking at this update, and the first thing that caught my attention - use of co and ES6 generators. This things is, pg-promise has embedded support for ES6 generators, so co is no longer needed.\nAnother most likely candidate - how you format the queries. With pg-promise you get access to the most flexible query formatting engine that should simplify things a lot ;). Added a memo to help with a possible upgrade.\n. @dmfay one of the recommended improvements - reuse of the pg-promise configuration for the custom promise library when using its db object.\ndb object gives you access to the generic promise protocol via db.$config.promise. It is a simplified protocol, with methods as explained in the $config.\nNote that support for .all was added in v5.9.6.\nThis way, whenever a query request is made, you can use the same promise library as configured for pg-promise, rather than suddenly switching to ES6 Promise.\n. > I've been trying to do something weird with proxies for the connection pools -- mixed results so far :)\nAre you talking about the use of pg-promise version 6.x?. The latest pg-promise version is 6.0.26.\nWithin pg-promise architecture you get a new pool with each new db object you create by calling db = pgp(connection).\nThe idea is to have a separate db and thus a separate pool in the following situations:\n\nWhen each db represents a separate database on the same server/port. This is perhaps the least useful separation, because everything still goes through the same IO, and likely to give 0 performance advantage.\nWhen each db represents a separate database on different servers/ports. This is the most useful separation, because the IO is completely separate, and therefore you get a huge performance boost and real parallelism.\nWhen each db represents a different API priority for the same database, including a different pool size. This is a virtual pool organization to make sure the priority API calls get access to a larger connection pool.\n\nHopefully this will help you in making the decision in how to use it best ;)\nAlso worth noting, pg-promise discourages creating more than one db object with the exact same connection details, producing a warning in the console like this:\nWARNING: Creating a duplicate database object for the same connection.\n    at Object.<anonymous> (D:\\NodeJS\\tests\\test2.js:14:6)\nSo currently, if you go for scenario 3, you are expected to at least specify a different pool size. Otherwise, like, why creating it in the first place? :) i mean, it makes no sense having multiple pools of the same size going to the same database.\n. pg-promise 6.x has been officially released.. @dmfay just curious, did you publish/submit your library somewhere after the 3.0 release? I've noticed a good spike in new stars \u2b50\ufe0f \u2b50\ufe0f \u2b50\ufe0f \u2b50\ufe0f \u2b50\ufe0f \nMine got stale of lately, so I'm thinking of posting it somewhere :smile:. Must have been some twitter, it looks really in play :smile:, though I suspect most people come from here: http://nodeweekly.com/issues/194\nCome to think of it, how did you get it published there? Not by twitting? :smile:\nOr maybe because it got re-twitted here: https://twitter.com/javascriptdaily\n\ud83d\ude2f I won't tell anybody \ud83d\ude2f    \ud83d\ude38 . Version 7 of pg-promise has already been implemented within branch v7.\nSee the related issue: Version 7.x of the driver.\nThe remaining bits and pieces are tests + documentation, tons of each.\n. @dmfay what do you think of this type of change for version 7? - https://github.com/vitaly-t/pg-promise/issues/371. @dmfay if you update the driver to v6.5.0, please note the breaking change.\nRelated question: #411 . @dmfay You might like this update: https://github.com/vitaly-t/pg-promise/releases/tag/v.6.10.0\n. @dmfay \nOne feature in pg-promise that's been missing (made unavailable) in massive.js - Database Context, something that matters when more than one database needs to be supported, like think master + replicas, for example.\nSee this question: https://stackoverflow.com/questions/46243680/rebuilding-connections-in-nodejs-pg-promise\nIt is the second parameter into the Database constructor, the value which is then propagated through the entire interface to provide the database context.\n. Version 7.0.0 is out. You've got some catching up to do :wink:\n@dmfay . Well, now i'm myself fresh out. I just did a major rewrite of the documentation, no code changes.\nAnd since I'm starting a new job on the 16th, I'm likely to take a very long break away from GitHub.\nYou get your time to catch up ;)\n. @dmfay Congrats on 2000 stars! :smile: \nWhen are you planning to upgrade the pg-promise dependency? :smile:\n. Now that you have updated to the latest version of pg-promise, time to consider how you can make your library faster and simpler by using these features:\n\nmulti-result methods multi and multiResult - they are great for performance, executing multiple queries and getting all results back with a single IO operation.\nnested named parameters - can simplify the formatting logic on your side and/or client's :wink:\n\n. @dmfay Unrelated to the subject,...but, your library can do all that - right? - What library do you use for postgres+jsonb in Node?\n\nIf so, you may want to publish an answer ;)\n. @dmfay \nI suggest that you start using fixed versions of all packages. Otherwise, you will exposing yourself to unknown issues that suddenly pop up.\npg-promise just had a bad update - version 8.4.1, which was then replaced by 8.4.2.\nI've started using only fixed versions within pg-promise awhile ago :wink:\n. When you put such a query through the query-formatting engine:\nselect * from airplanes --where passengercount < $1\nthat engine will parse the string only for variables, not for comments.\nWhy do you have comments like this in your query in the first place? You really shouldn't. Maybe if you explain how this happened, I will be be able to advise how to deal with this.\nI'm guessing, you are reading some external SQL file that contains that comment? If that's the case, then you can read such files through type QueryFile, just like this library does now, and indicate that you want the comments thrown out (minify: true flag).\n@robconery does the library expose the driver instance somehow, so the client can access it? The client should be able to say something like: var QueryFile = massive.driver.QueryFile, to be able to use it, and then pass such QueryFile object into any query method. I haven't tried it with this library yet, so that's why I'm asking...\nOh, just found: https://github.com/dmfay/massive-js#accessing-the-driver, it is funny I guessed it damn right! :smile:\nOps, correction, that driver that I mean is actually massive.pgp here, while massive.driver is the database object.\n. So, the example I was talking about would be something like this:\n```js\n// create it just once:\nvar sql = new massive.pgp.QueryFile('./path/my-file.sql', {minify: true});\n// and then use it:\nmassive.run(sql, parameters).then(...).catch(...)\n``\nThat{minify: true}` will squash the sql through pg-minify, so your problem with the comments is then gone ;)\nSee also: Query Files.. There is a problem with this line:\njs\nsql: new pgp.QueryFile(f, {minify: true}).query\nOne should never use property .query of the object directly, it is the one for internal purposes only, as mentioned in documentation.\nYou should only use the object itself directly, passing the object into all the query methods.\nOnly by using the object itself the error handling will be done correctly within the query methods of pg-promise.\nAll query methods within pg-promise support type QueryFile internally, they know how to handle its error state, resulting in a proper reject, and how to re-load the file when necessary.. I would suggest a small correction for this text line:\nMassive exposes the pg-promise driver as `db.driver`\nThe things is, .driver is the database object from pg-promise, while the driver itself is available as .pgp, from the look of the code.\nIn truth, what you have currently exposed as .driver is sufficient for everything, because for integration purposes that object has property $config, so the root of the library can be accessed from it like this:\njs\nvar pgp = massive.driver.$config.pgp;\n$config:http://vitaly-t.github.io/pg-promise/Database.html#.$config. That, plus I'm guessing that db.company.new_company should actually be return db.company.new_company, if it returns a promise.. > I could not find anything about this \"pg_matviews\" in search engines.\nWhat search engines did you try? Google immediately takes you to the official documentation: https://www.postgresql.org/docs/9.3/static/view-pg-matviews.html\nThat table has got to be there, or else I don't know what type of PostgreSQL installation you've got.\n\nI am using Postgres version 9.4\n\nAre you sure about that? The error suggests that you are using a version of PostgreSQL that's even older than 9.3\nAnyhow, the Materialized Views are somehow missing in your PostreSQL setup and need to be created.\n. > I will look up for Materialized Views to see why they are missing\nThis must be the case ;). No idea if it is possible to disable, perhaps @dmfay knows ;). About these lines:\n\nhttps://github.com/dmfay/massive-js/blob/74aa8ab254c465646583e475d8e39ce9e67b6d0d/lib/database.js#L64\nhttps://github.com/dmfay/massive-js/blob/74aa8ab254c465646583e475d8e39ce9e67b6d0d/lib/database.js#L121\n\nYou can replace it with this.instance.$config.promise.all, just as you update the pg-promise dependency to 5.9.6, in which I added support for .all primarily for this.\nProvided that this.instance can be used there.. I'm only guessing, the issue may be related to this code: https://github.com/dmfay/massive-js/blob/master/lib/database.js#L165\nqueryResult.any mask would result in an array always, while queryResult.one | queryResult.none results in either null or one object, which is consistent with the driver's methods any and oneOrNone.\n. Looks like your Node.js is earlier than 6.x, which according to travis.yml, Massive.js doesn't support.\n@dmfay may be it should? Would be easy to make sure of it, since the driver supports it ;)\n. You can use the standard $1 or ${name} variable, if you wrap your type into a Custom Type:\njs\nfunction srid(lng, ltd) {\n    return {\n        _rawDBType: true,\n        formatDBType: () => 'ST_SetSRID(ST_MakePoint(' + lng + ', ' + ltd + ', 4326))'\n    };\n}\ni.e. you can then pass srid(123, 456) as a value for your query-formatting variable.. @andrerpena Please note that when Massive updates its driver to v6.5.0, there will be a breaking change for the Custom Type Formatting.\n. The warning comes from pg-promise when creating multiple QueryFile objects for the same file. That type works as a link, with the ability to auto-reload a changed file, so creating more than one such link is a waste of IO, and thus considered an anti-pattern, hence the warning.\nIt happens on this line: https://github.com/dmfay/massive-js/blob/master/lib/loader/scripts.js#L14\nwhich seems to be able to supress warnings by passing option noWarnings: true during the library's initialization.\nIt is generally not a great idea, the library shouldn't create more than one QueryFile for the same file.\n@dmfay It should be possible to refactor that code so that one does not need to pass in noWarnings. I'm only guessing, cannot advise exactly how at the moment.. P.S. Breaking my head over how to support the multi-result sets introduced in node-postgres 7.0.0... \ud83d\ude15 . Well done! ;). pg-promise doesn't do any automatic casting, because it doesn't work with the database structure, it only executes queries.\n. pg-promise escapes every array according to the proper PostgreSQL syntax, of which there are two:\n\nthe old/legacy approach was to use {value, value,...}\nnew new one since 9.0 is to use array[value, value,...]\n\nAnd while it is using the second approach, it separately escapes each value in the array according to its type.\n@tamlyn you have 3 options to do it right:\n\nmake sure the array contain numbers, and not strings\nor, execute the query with the type cast of ::bigint[]\nor, override the type by using your own custom type\n\nFor the third option it would look like this:\njs\nconst bigints = a => ({formatDBType: () => '{' + a.join() + '}'});\nso instead of values you would be passing in bigints(values).\n. @tamlyn that's probably the best solution :) as working with 64-bit numbers in JavaScript is awkward, and best is avoided, if possible ;)\n. A typical ON CONFLICT clause is static, which means it can be simply appended to the generated query.  And this is how it is done within pg-promise, b.t.w.. The following explains it nicely: https://stackoverflow.com/questions/39168501/pg-promise-returns-integers-as-strings. > My data are stored as Decimals in the DB....guessing it is the same issue.\nYes, as there is more than one way to present those values in JavaScript, the values are provided as strings, to avoid ambiguity.\n. @cjevning This usually pertains to the client-side who decides to use or not to use custom parsers :wink:\nSurely nothing changed in pg-promise or the underlying driver for that.. @dmfay you still haven't released the important security patch :wink:. Strange the build on Travis CI failed. This doesn't seem related to the update in pg-promise.\n. No wait, it is related, I see that you are using formatDBType here: https://github.com/dmfay/massive-js/blob/master/lib/loader/scripts.js\nI will do  new PR in a moment ;)\n. @j0k3r be nice, don't yell :smile: \nI initially did that breaking change in 7.x, but since I got stuck with 7.x, and its release date is unknown, I did it as 6.5.0, because that change is so long overdue. I should have done it at least a year ago.\n. @j0k3r oh I see. Massive now uses ~ for its dependencies, which is a good safe practice. Lots of packages get broken on minor version unintentionally, or worse - on builds. One such terrible example - pg-query-stream, the guy wrote the whole thing from scratch, and then upped it to 1.1.0 from 1.0.0, which is another reason pg-promise 7.x got stuck.\nAnyhow, sorry for the trouble, I haven't done breaking changes in pg-promise in minor version since its inception, will remember not to do it again.\n\nI think you should have deprecate the old function instead of removing it.\n\nIt was one case when it wasn't possible, because there were 2 properties renamed that couldn't use getters.\n. @dmfay I start to think it may cause problems for anyone using the latest Massive 3.2.1 which use pg-promise reference as ^6.4.0. Maybe you should release an update asap.\nDamn it, I will never release a breaking change again :smile:\n. Looking at this release note:\n\nIf you've been wondering what formatDBType is, grab the upgrade!\n\n...made me realize that Custom Type Formatting documentation isn't good, so I've just rewritten it from scratch, because it is the most powerful feature in the query-formatting engine.\n. > I have a use case where I need to initialise the Database instance first and only much later do an actual database connection.\nClass Database doesn't create any connection, it is only when you start executing queries the connections are created.\nFrom the documentation:\n\nObject db represents the Database protocol with lazy connection, i.e. only the actual query methods acquire and release the connection automatically.\n. > Finally exposes the Database class as part of the public API so that consumers of this library can, if they so choose, import the Database class themselves and instantiate it on their own, leaving the responsibility to call .reload() in their own hands (this is the primary purpose of this PR)\n\nBy the way, class Database was designed as the main integration point, so any library can take it as a parameter. I even wrote some documentation on it, see Integration. This is why the object contains such important properties as $config, $pool and $cn.\nThat Integration document needs updating, since class Database started instantiating a connection pool internally. Basically, exchanging that class is the best, as it gives you everything, and avoids any connectivity issues.\n. > what's going on with the duplicate QueryFile warnings in the test output\nI wouldn't worry much about it. pg-promise in its own tests lets it happen all over, because those are tests. In fact, it passes in option noWarnings: true to avoid warnings in tests.\nBut strictly speaking, if you are getting a warning, means you are creating a file link in more than one place.. A related example: get JOIN table as array of results with PostgreSQL/NodeJS. > Massive is meant to be connected once and for everything to route through a single instance. \nAnd not following this will cause all kinds of problems, which is most likely the case for @jnystrom . Clarification: explicit type casting is required when trying to pass in an empty array, because only in this case the type cannot be inferred automatically.\n. True, that's like re-implementing that wrap function to just this:\njs\nfunction wrap(a) {\n    return a.length ? a : '{}';\n}\nwhich will work the same :smile:. @dmfay  Oh, wow, I was actually still considering the change, but you went and did it on your side. I thought there was going to be a discussion on how best to approach it :smile:. As per here: https://github.com/vitaly-t/pg-promise/pull/400\nIt won't work for nested empty arrays. Not to worry, just seat back and wait for pg-promise 6.7.1\n. After much huffing and puffing, version 6.7.1 is finally out.\nAfter many more tests it came out as pretty much the same as what you did within Massive.js, for the reasons explained in the release notes.\n. From the logs it looks like the latest pg-cursor goes nuts over the latest version of node-postgres? \ud83d\ude15 \nNeeds some investigation. @dmfay let me know what you can find :wink:\nFrom the first look seems like an issue with pg-cursor unorthodox way of including pg: https://github.com/brianc/node-pg-cursor/blob/master/pg.js. @dmfay Looking at the tests errors, I didn't expect those... \ud83d\ude15 \nI was sure the change I made remained 100% compatible with how streaming worked before. Maybe you are using some streaming specific that I'm not aware of?\nOr maybe you are using some tests that also only worked with the older version of pg-query-stream.\n. > there wasn't any streaming support in pg-promise until now\nActually, there have always been, as method stream was there since the library's inception :smile:\n. 6.9.0 is out that may (hypothetically) change something ;). That streaming issue was biting me for a long time. It is your turn now \ud83d\ude08 . @dmfay  Any luck sorting out those stream-related issues? :wink:\nI'm getting the feeling that if you leave any of those till BrianC gets around fixing something, it could be a very long wait as usual.\n. A-ha! Nice! \ud83d\udc4d You'll get many new features now available to you...\nHowever the ones that may have the biggest effect on your internal use of the driver are:\n\nmulti-result methods multi and multiResult - great for performance\nnested named parameters - can greatly simplify the formatting logic on your side ;)\n\n. @tony-gutierrez you should investigate the issue against the node-postgres driver, which is what ultimately creates the connection.\n. I believe the issue is with your user name - pguser@pg you cannot use @ in the user name directly, it must be escaped properly. This is most likely the issue you are having. Or it could be a similar issue in the password, if you are using again something that needs escaping.\n. username@hostname isn't the user name.\nThe right connection string syntax: \npostgres://username:password@host:port/database\n. The underlying driver pg-promise supports it fully...\njs\ndb.instance.task(t => {\n    // t = task context to execute all queries against\n})\n.then(data => {})\n.catch(error => {})\nSee Tasks and Transactions.\nPlus, there is a manual singleton for special cases, like LISTEN/NOTIFY, see connect.\nThough I don't know if Massive supports any of those.\n. @jaimesangcap b.t.w., if your queries do not involve JSON, then you won't need Massive support of it ;)\ni.e. you can execute any queries within a task.. >  by JSON you mean stored as Documents (jsonb)?\nYeah :)\n\nUnfortunately I'm using massive as document store.\n\nThen you do need Massive's support for that.\n. A, finally! :smile:. @dmfay As I revisited this with this question: https://github.com/vitaly-t/pg-promise/issues/563\nI would say that the following approach is better:\njs\nconst rawSQL = pgp.as.format(script.sql);\nThe formatting method knows internally how to deal with all types that support Custom Type Formatting (both explicit and symbolic syntax), and QueryFile is just one such type.\n. It would take to replace all the query generation with the code that instead generates a query-formatting template, using either Index Variables or Named Parameters, and then pass all the formatting values in via either as.format method or the query methods directly. The driver will take care of escaping everything correctly and prevent SQL injections.\nIt would also let you throw away most of the JavaScript-side type casting that's currently in the code.\nExample from a library that does exactly that: https://github.com/parse-community/parse-server/blob/master/src/Adapters/Storage/Postgres/PostgresStorageAdapter.js\nAlthough that example is far from optimal ;). > A while back I was looking at doing some stuff with proxies to support multiple connection pools by switching out what happened under the hood when Massive invoked the driver's query functionality\nAs it stands now, each Database object creates its own Pool object, and each instance of Massive creates one Database object, so you should end up with one Pool per Massive root object :wink:\n. @dmfay If you are to use pg-promise transactions, you would never resort to manual commit/rollback. Those things work automatically within pg-promise transactions, that's the most important feature of those transactions. More importantly, without those you would run into all sorts of trouble that you may not even know exist. Here's just one example: https://github.com/brianc/node-postgres/issues/1454\nOther things that pg-promise provides for transactions:\n\nAutomatic configuration, see Configurable Transactions\nAutomatic nested transactions as save-points - the most complex piece.\nAutomatic tracing / debugging transaction context\netc.\n. > Is that issue resolved by the things you mentioned pg-promise does automatically?\n\nYes. As that link explains, pg-promise is implementing a work-around for that issue, not to attempt execution of ROLLBACK when a transaction fails as a result of a failed connection, to avoid halting the process.\n. > If not I would just love to use the query builder part of massive. Dont know if this even makes sense. I just dont want to use knex, bookshelf.js, sequelize, et al. Also I am a fan of pg-promise.\nI don't know if Massive.js supports this, like pg-promise does, exposing the complete query-formatting engine separate from everything else, i.e. namespace pgp.as, and in particular method pgp.as.format(query, values, options), on which the entire library relies. But if it doesn't then it should, and then it would be possible to combine the query-building facility of Massive.js with an external query execution engine.\n. If you take your time, pg-promise 8.x will be released, which will also have a number of breaking changes, although none of those will affect your code, but still, it will be a major update :wink:\n. Well, I'm all done with 8.x, in fact did 2 updates since :smile:. @jnystrom If the inserts you are doing do not require much JSON processing that's supported by Massive, then you can do it directly through its pg-promise driver. See Multi-row insert with pg-promise.\nIn fact, you can do via the driver either way, because the reach syntax of Column lets you format the value any way you want.\n. @mike-engel such an upsert would be quite inefficient, performance-wise. You'd want to do just a single multi-row insert here. For example see Multi-row insert with pg-promise\n\n```js\nconst helpers = db.pgp.helpers;\nconst cs = new helpers.ColumnSet(['plaid_id', 'amount', 'category', 'date', 'location', 'payment_meta', 'name', 'pending', 'transaction_type', 'account_id', 'item_id'], {table: 'transactions'});\n// data = array of your insert objects;\nconst insert = helpers.insert(data, cs) +  ' ON CONFLICT(plaid_id) DO UPDATE SET ' +\n    cs.assignColumns({from: 'EXCLUDED', skip: 'plaid_id'});\nreturn db.query(insert);\n```\nThis will execute just one atomic insert, which will be significantly faster and more reliable.\n. Once upon a time I ran into the same issue within pg-promise: https://github.com/vitaly-t/pg-promise/pull/290\n@dmfay You'll see there how to fix it :wink:\n. Hah, it's never gonna happen. Travis CI PostgreSQL upgrade path was and remains piss-poor.\n. @dmfay Wasn't it fixed by now? :wink:. @AiDirex simpler version of your Custom Type Formatting would be:\njs\ndb.foo.insert({\n    bar,\n    rawType: true,\n    toPostgres: db.pgp.as.json\n})\nBut that's if you want the whole object. For just the value you would do:\njs\ndb.foo.insert({\n    bar,\n    rawType: true,\n    toPostgres: a => db.pgp.as.json(a.bar)\n}). > But I think it is a better idea to have some way of providing formatting in queries for pg-promise.\nI think I wrote it somewhere in the past, that it would be nice if Massive.js were to fully rely on the pg-promise query-formatting engine. And everything that needs custom formatting would be handled via Custom Type Formatting.\n. It is more efficient and safer to replace this:\njs\nreturn db.pgp.as.format('$1::jsonb', [JSON.stringify(p.data)]);\nwith this:\njs\nreturn db.pgp.as.json(p.data) + '::jsonb';. Caution to be advised around use of table named user, as it is also a reserved word, so it must be always double-quoted \"user\", or else problems are imminent ;)\n. My two cents on the caching idea.\nCaching can only be efficient when it is implemented according to the business logic of the application. A general-purpose database library cannot encapsulate caching efficiently.\n. Better promise approach:\n```js\nconst massive = require('massive');\nlet db;\nexports = module.exports = function () {\n  if (db) {\n    return Promise.resolve(db);\n  }\nreturn massive({\n    host: 'localhost',\n    port: 5432,\n    database: 'massive'\n  }).then(instance => {\n    db = instance;\n    return db;\n  });\n};\n```\n. Unwraping a static promise has no performance cost ;)\nAnd the general promise theory goes like this: A promise method must always return a promise never mix it.. In case somebody wants to access multiple schemas by default whilst dynamically, i.e. without changing the database configuration, it can be done with initialization option schema of pg-promise:\njs\nconst initilizationOptions = {\n    schema: ['schema-1', 'schema-2', ...]\n};\nso you have multiple default schemas. And keep in mind that if your schemas suddenly contain a table with the same name that you are trying to access by default, it will be used from the schema that's defined first on the list you specified.\n. The syntax shown in the example requires Node.js v9.2+. @rijnhard I don't know what feature request you could open against pg-promise, as it already supports both ES6 generators and ES7 async/await in a generic way.\n. > in that it allows you to handle large datasets with backpressure just like streams\nFor that pg-promise supports sequences, see method sequence and page that let you implement any custom back-pressure, to process an abstract data source.\n. You already have several approaches to diagnosing errors - error event, pg-monitor, and Bluebird long-stack tracing. Using at least one of them is sufficient.\n. > the error event doesn't have context information in the stack\nBluebird stack has all the details.\n\npg-monitor needs to be attached to log queries, which will usually occur after the error happened\n\nWhat do you mean after? You attach to events at the app start, and it logs everything.\n\nBluebird is not an option in my opinion.\n\nIt is the best option, better performing and way more flexible, especially when it comes to stack tracing. I don't understand your arguments against it.\nIn all, you have all the right tools already.\n. @DriscollRichie The error tells you, that what you defined as get_bins in file binsCtrl.js, is not a function. Looks like an error on your side.\n. > Don't code when you've been drinking\nSeriously? That is the only way to do it! :smile:\n. @dmfay May I ask why you decided to keep the same cb, [options] order in place of the pg-promise version of [options], cb?\nThe reason it is this way in pg-promise is to make the code much easier to read. You would see it this way once you start passing in options.\nHere's a typical example of using tags:\njs\ndb.tx('my-tx-reference', t => {\n /// implementation\n})\nIn your approach you would end up with the tag being at the end of the transaction, which makes it awkward to spot right away, especially with a long implementation body.\n. > Yes in the abstract, but it doesn't work for this case.\nAre you sure? Custom Type Formatting can custom-format any value, it can work with any possible case.\nIf you can provide an example of what kind of format is expected, I can show how it can be done.\nUse of option pgFormatting is an extreme, I don't see how this is going to help, and would advise against anyway. I was tempted to remove that option completely in future versions of the driver, as obsolete.\n. For what I can see, pg-promise escapes everything correctly by default. And if you pass in ::jsonb[] type casting, the server will understand the type correctly, and it will work:\n```js\nconst obj = [{a: 'something'}, {b: 'hello'}];\ndb.none('insert into things(data) values($1::jsonb[])', [obj])\n//=> insert into things(data) values(array['{\"a\":\"something\"}','{\"b\":\"hello\"}']::jsonb[])\n```\nAnd if you want it to happen automatically, based on the formatting parameter, you can use Custom Type Formatting:\njs\nconst arrJSONB = a => ({toPostgres: () => pgp.as.format('$1::jsonb[]', [a]), rawType: true});\nso instead of passing in obj, you would pass in arrJSONB(obj) as the value, and then no type casting is needed inside the SQL template:\njs\ndb.none('insert into things(data) values($1)', [arrJSONB(obj)])\n//=> insert into things(data) values(array['{\"a\":\"something\"}','{\"b\":\"hello\"}']::jsonb[])\n. @nkramaric what is db.pgp? :smile: The method is pgp.as.format.\nI suggest that you use pg-monitor in your project, so you can see what exactly is being executed :wink: Then we can nail the issue :wink:\n. As per my previous post, if we can't see what's being executed, then it's all speculation. You should integrate pg-monitor, then we can get to the bottom of it fast :wink:\n. This looks wrong, i.e. the input doesn't correspond to the output, because rawType is misspelled as rawtype. Please make sure you got this one right :wink:. In this case you also can replace this line:\njs\nreturn db.pgp.as.format('$1::jsonb[]', [p.data]);\nwith this:\njs\nreturn db.pgp.as.array(p.data) + '::jsonb[]';\n. @dmfay It won't build, till you update [pg-query-stream] to version 1.1.2, as per the release notes :wink:\ni.e. you would have to update the two dependencies at the same time :wink:\n[pg-query-stream]:https://github.com/brianc/node-pg-query-stream. Your code examples and the error do not add up. The only way such an error would pop-up, if you were to pass in [] as the formatting parameter into the query method:\njs\nawait db.query(sql, [])\nYou should check your code again, and make sure you do not pass it an empty array, as it would try to format the query, and correctly throw that error.\nI seem to have found the problem...\n. @dmfay There seems to be a problem with your implementation of method query:\njs\nDatabase.prototype.query = function (query, params = [], options = {}) {\nYou cannot default formatting parameters params to [], you have to default it to undefined. That's how pg-promise works. If it finds [], it runs format on the query, for the syntax of $1, $2, ..., and if it finds any such variable, it tries to replace those. And because you default it incorrectly, any query with text that looks like those variables will result in that RangeError, as reported.\nI'm a bit surprised this didn't come up earlier. This is how pg-promise worked always, b.t.w.\n@IsaiahJTurner As a work-around for now, it will work, if you execute it through pg-promise driver directly:\njs\nawait db.instance.query(sql)\n@dmfay Here're some query-formatting test cases, to give you an idea, what tests you might want to add, along with the code changes...\n```js\nconst format = require('pg-promise').as.format;\nformat('$1') // does not format\n//=> $1\nformat('$1, $2') // does not format\n//=> $1, $2\nformat('$1', null)\n//=> null\nformat('$1', undefined) // does not format\n//=> $1\nformat('$1', [undefined])\n//=> null\nformat('$1, $2', 123)\n//=> 123, $2\nformat('$1, $2', [])\n//=> RangeError: Variable $1 out of range. Parameters array length: 0\nformat('$1, $2', [123])\n//=> RangeError: Variable $2 out of range. Parameters array length: 1\nformat('$1, $2', [123, 456])\n//=> 123, 456\n```\n. @nkramaric The underlying driver will safely format it, and happily execute:\njs\nfunction getPersons(occupation) {\n    const match = '%' + occupation + '%';\n    return db.instance.query('SELECT * FROM persons WHERE ANY(occupations) ILIKE $1', match);\n}\n@dmfay This won't invite any injection attack :wink:\n. This may be just too many wrong assumptions in one single post \ud83d\ude04\n. The change appears to be a continuation on #642.\n. > Did you do something with db.instance.$config\nThat object and all its properties are read-only :wink:. @dmfay I suspect it is because $config exists only on the root level of the library, and not on the transaction levels, and you end up making the call against the transaction context where the property does not exist? Then it would make sense why the error occurs.\nhttps://github.com/dmfay/massive-js/blob/d6a359858484153e3fdd1442c1040a06ef0c277b/lib/writable.js#L78\nI suspect that above this.db.instance refers to the transaction context, somehow, and not to the root Database interface. This is the only way to explain why property $config would be missing.\n. > @vitaly-t, would it make sense that it works sometimes?\nDepending on the business logic, and how it works underneath, perhaps, but it is hard to see how exactly.\n. @jnystrom Obviously, since the failing code is conditional on that check - \nhttps://github.com/dmfay/massive-js/blob/d6a359858484153e3fdd1442c1040a06ef0c277b/lib/writable.js#L76\nMakes it a work-around for you, but not a fix.\n. @dmfay places missed:\nhttps://github.com/dmfay/massive-js/blob/a429162c08b9c08a2029f54a43cb9171fa0ed6bd/lib/loader/functions.js#L6\nhttps://github.com/dmfay/massive-js/blob/fd15567acbca628da5152a8c0e20712c86220f49/lib/executable.js#L101\n. @andrerpena  And in case you do have to use text as column name, it needs to be in double-quotes: \"text\", to let PostgreSQL differentiate from text reserved word (type).\n. It doesn't support $\\name\\, but supports $/name/. Punctuation is off also. A comma is needed after \u2018as well\u2019 and before \u2018with\u2019.. ",
    "zacbarton": "Thats exactly what I was after thanks.\nFeel free to close this or if you are going to auto filter postgis functions then keep it open as a reminder?\n. ",
    "AlexZeitler": "I think I can anser it by myself...\nCREATE TABLE CUSTOMER(CompanyName1 VARCHAR(20));\n\\d+ test\n                                   Table \"public.customer\"\n    Column    |         Type          | Modifiers | Storage  | Stats target | Description \n--------------+-----------------------+-----------+----------+--------------+-------------\n companyname1 | character varying(20) |           | extended |              |\nAnd this SO answer.\n. Sure, I'll stick with the Postgres convention.\n. Sounds like a plan.\n. Is it ok to add Docker for testing purposes (spin up a container, create the rob:password@localhost/massive database, run the tests)\n. Ok. Do you have another idea how we could simulate a temporarily dropped connection for testing purposes?\n. That might works, thanks.\n. Sorry for being late on this.\nI tried to reconnect in the runner.js as suggested but that didn't work out as expected.\nSo I tried to handle the error in my application (as suggested also):\nasync:\njs\nmassive.connect({connectionString: connectionString, scriptsDir: '../db'}, function (err, db) {\n    if (err) {\n        console.log('connection err')\n    }\n});\nThis results in that output:\n``` bash\nassert.js:86\n  throw new assert.AssertionError({\n        ^\nAssertionError: Error: connect ECONNREFUSED\n    at /app/node_modules/massive/lib/runner.js:32:12\n    at /app/node_modules/massive/node_modules/pg/lib/pool.js:75:25\n    at /app/node_modules/massive/node_modules/pg/node_modules/generic-pool/lib/generic-pool.js:274:11\n    at /app/node_modules/massive/node_modules/pg/lib/pool.js:27:26\n    at null. (/app/node_modules/massive/node_modules/pg/lib/client.js:171:5)\n    at emit (events.js:107:17)\n    at Socket. (/app/node_modules/massive/node_modules/pg/lib/connection.js:59:10)\n    at Socket.emit (events.js:107:17)\n    at net.js:459:14\n    at process._tickCallback (node.js:355:11)\n```\nSo I cannot handle err.\nThen I tried to use the sync version using try / catch:\njs\ntry {\n    var db = massive.connectSync({connectionString: connectionString, scriptsDir: '../db'});\n} catch (e) {\n    console.log('exception', e)\n}\nBut it never hits the catch path:\nbash\nassert.js:86\n  throw new assert.AssertionError({\n        ^\nAssertionError: Error: connect ECONNREFUSED\n    at /app/node_modules/massive/lib/runner.js:32:12\n    at /app/node_modules/massive/node_modules/pg/lib/pool.js:75:25\n    at /app/node_modules/massive/node_modules/pg/node_modules/generic-pool/lib/generic-pool.js:274:11\n    at /app/node_modules/massive/node_modules/pg/lib/pool.js:27:26\n    at null.<anonymous> (/app/node_modules/massive/node_modules/pg/lib/client.js:171:5)\n    at emit (events.js:107:17)\n    at Socket.<anonymous> (/app/node_modules/massive/node_modules/pg/lib/connection.js:59:10)\n    at Socket.emit (events.js:107:17)\n    at net.js:459:14\n    at process._tickCallback (node.js:355:11)\n. This might be related: http://stackoverflow.com/questions/15969770/how-to-handle-should-js-assert-error\n. Using this, I can keep the application running but it feels wrong:\njs\nprocess.on('uncaughtException', function (err) {\n    console.error(err);\n    console.log(\"Node NOT Exiting...\");\n});\nI also tried it using plain pg which allows me to handle my errors and retry connecting.\n. I think it could be fixed by instead of throwing connection errors just bubbling them up and thus using existing retry implementations like node-retry.\n. @dmfay @robconery I would be very happy to hear your opinion about how to proceed. Thanks.\n. Ok, thanks!\n. Just wanted to make sure I broke nothing and tried to run the tests before changing anything.\nI ran mocha ./test/*.js and got this result:\n``` bash\n  Document queries\n    \u2713 returns a db\n    Querying documents\n      \u2713 returns all documents when passed \"*\"\n      \u2713 returns all documents when passed only \"next\" function\n      \u2713 finds a doc by primary key\n      \u2713 finds a doc with > comparison on primary key\n      \u2713 finds a doc with >= comparison on primary key\n      \u2713 finds a doc by title\n      \u2713 parses greater than with two string defs\n      \u2713 parses greater than with a numeric\n      \u2713 parses less than with a numeric\n      \u2713 deals with arrays using IN\n      \u2713 deals with arrays using NOT IN\n      \u2713 executes a contains if passed an array of objects\n      \u2713 works properly with dates\n    Full Text Search\n      \u2713 works on single key\n      \u2713 works on multiple key\n      \u2713 returns multiple results\n      \u2713 returns properly formatted documents with id etc\nDocument saves\n    To a non-existent table\n      \u2713 creates the table\n      \u2713 returns the doc\n      \u2713 updates the doc\n      \u2713 finds the updated document\n      \u2713 deletes the doc\n    To an Existing Table\n      \u2713 Saves a new movie\n      \u2713 updates the movie title\n      \u2713 finds the updated movie title\n      \u2713 deletes the movie\nFunctions\n    \u2713 executes all products\n    \u2713 executes all myschema.albums\n    \u2713 executes artists with param\n    Loading of Functions from PG\n      \u2713 has an all_products function attached\n      \u2713 has a schema-bound function attached to myschema\n    Function Execution\n      \u2713 executes all_products and returns the results\n      \u2713 executes schema-bound function and returns the results\n    Functions with Cased Names\n      \u2713 executes camel-cased function AllMyProducts and returns the results\n      \u2713 executes schema-bound, camel-cased function AllMyAlbums and returns the results\nOn spin up\n    \u2713 returns a valid db interface\n    \u2713 loads non-public schema as namespace property\n    \u2713 loads tables in db schema as properties of namespace\n    \u2713 loads up 7 tables with 2 in schema object in array property\n    \u2713 loads up 7 queries\n    1) loads up functions\nSynchronous Load\n    2) loads the db synchronously and blocks execution until complete\n    3) returns a valid db instance from sync load function\n    4) loads non-public schema as namespace property\n    5) loads tables in syncLoaded schema as properties of namespace\n    6) loads up 7 tables with 2 in schema object in array property\n    7) loads up 7 queries\n    8) loads up functions\nOn Load with Schema Filters (these tests may run slow - loads db each test!!)\n    9) loads all schema tables when no schema argument is passed\n    10) loads all schema tables when passed \"all\" as schema argument\n    11) loads only tables from specified schema when passed schema name as schema argument\n    12) loads only tables from public schema when passed \"public\" name as schema argument\n    13) loads only tables from specified schema when passed comma-delimited list of schema names\n    14) loads only tables from specified schema when passed an array of schema names\nOn Load with Table blacklist (these tests may run slow - loads db each test!!)\n    15) loads all tables when no blacklist argument is provided\n    16) excludes tables with name matching blacklist pattern as a string argument\n    17) excludes tables with name and schema matching blacklist pattern as a string argument\n    18) excludes tables with name and schema matching multiiple blacklist patterns as a comma-delimited string argument\n    19) excludes tables with name and schema matching multiiple blacklist patterns as a string array argument\n    20) allows exceptions to schema filter\n    21) allows exceptions to blacklist filter\n    22) allows exceptions to schema and blacklist filters\nOn Load with Table whitelist (these tests may run slow - loads db each test!!)\n    23) loads all tables when no whitelist argument is provided\n    24) includes ONLY tables with name matching whitelisted table names as a string argument\n    25) includes ONLY tables with name matching whitelisted items in comma-delimited string\n    26) includes ONLY tables with name matching whitelisted items in string array\n    27) whitelist overrides other filters\nOn load with Function Exclusion (these tests may run slow - loads db each test!!)\n    28) excludes functions at load whenever it is told...\n    29) includes all functions at load whenever it is told...\n    30) includes all functions at load by default...\nOn load with Function Blacklist (these tests may run slow - loads db each test!!)\n    31) loads all functions when no blacklist argument is provided\n    32) excludes functions with name matching blacklist pattern as a string argument\n    33) excludes functions with name and schema matching blacklist pattern as a string argument\n    34) excludes functions with name and schema matching multiiple blacklist patterns as a comma-delimited string argument\n    35) excludes functions with name and schema matching multiiple blacklist patterns as a string array argument\nQueries built from files\n    Loading of queries\n      \u2713 returns a db\n      \u2713 has a schema query attached\n      \u2713 has an inStockProducts query attached\n    Execution\n      \u2713 executes inStockProducts without args with only a callback\n      \u2713 executes productById with non-array arg and callback\n      \u2713 executes productByName with multiple args and callback\n      \u2713 executes a deep namespaced query\nSchmema-Bounds Document queries\n    \u2713 returns a db\n    Querying documents in a schema-bound table\n      \u2713 returns all documents when passed \"*\"\n      \u2713 returns all documents when passed only \"next\" function\n      \u2713 finds a doc by primary key\n      \u2713 finds a doc with > comparison on primary key\n      \u2713 finds a doc with >= comparison on primary key\n      \u2713 finds a doc by title\n      \u2713 parses greater than with two string defs\n      \u2713 parses greater than with a numeric\n      \u2713 parses less than with a numeric\n      \u2713 deals with arrays using IN\n      \u2713 deals with arrays using NOT IN\n      \u2713 executes a contains if passed an array of objects\n      \u2713 works properly with dates\n    Full Text Search in a schema-bound document table\n      \u2713 works on single key\n      \u2713 works on multiple key\n      \u2713 returns multiple results\n      \u2713 returns properly formatted documents with id etc\nSchema-Bound Document Saves\n    To a non-existent table\n      \u2713 creates the table\n      \u2713 returns the doc\n      \u2713 updates the doc\n      \u2713 finds the updated document\n      \u2713 deletes the doc\n    To an Existing Table\n      \u2713 Saves a new movie\n      \u2713 updates the movie title\n      \u2713 finds the updated movie title\n      \u2713 deletes the movie\nSchema - Bound Tables -Add/Edit/Delete\n    Add/Update/Delete records in a schema-bound table:\n      \u2713 adds an artist \n      \u2713 updates an artist \n      \u2713 deletes an artist \nSchema - Bound Tables -Querying\n    Simple queries with args\n      \u2713 returns Artist 1 with 1 as only arg\n      \u2713 returns first record with findOne no args\n    Simple queries without args\n      \u2713 returns all records on find with no args\n      \u2713 returns first record with findOne no args\n    Simple queries with AND and OR\n      \u2713 returns Artist 1 OR Artist 2\n      \u2713 returns Artist 1 AND Artist 2\n      \u2713 returns Artist 1 with params as not array\n    Simple queries with count\n      \u2713 returns 2 for OR id 1 or 2\n      \u2713 returns 1 for id 1\n    Simple comparative queries\n      \u2713 returns Artist with id greater than 2\n      \u2713 returns Artist with id less than 2\n      \u2713 returns myschema.artists IN 1 and 2\n      \u2713 returns Artist NOT IN 1 and 2\n    Limiting and Offsetting results\n      \u2713 returns 1 result with limit of 1\n      \u2713 returns second result with limit of 1, offset of 1\n      \u2713 returns id and name if sending in columns\n    Ordering results\n      \u2713 returns ascending order of myschema.artists by name\n      \u2713 returns descending order of myschema.artists\nSynchronous goodies\n    36) \"before all\" hook\n96 passing (2s)\n  36 failing\n1) On spin up loads up functions:\n  AssertionError: 30 == 20\n  + expected - actual\n\n  -30\n  +20\n\n  at Context.<anonymous> (test/loader_spec.js:31:12)\n\n/home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/reporters/base.js:199\n      var match = message.match(/^([^:]+): expected/);\n                          ^\nTypeError: undefined is not a function\n    at /home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/reporters/base.js:199:27\n    at Array.forEach (native)\n    at Function.exports.list (/home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/reporters/base.js:161:12)\n    at Spec.Base.epilogue (/home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/reporters/base.js:319:10)\n    at Runner.emit (events.js:129:20)\n    at Runner.uncaught (/home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/runner.js:593:8)\n    at process.uncaught (/home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/runner.js:612:10)\n    at process.emit (events.js:107:17)\n    at process._fatalException (node.js:236:26)\n    at Function.module.exports.runLoopOnce (/home/alex/src/massive-js/node_modules/deasync/index.js:54:10)\n    at Object.connectSync (/home/alex/src/massive-js/node_modules/deasync/index.js:31:19)\n    at Context. (/home/alex/src/massive-js/test/sync_spec.js:8:18)\n    at callFn (/home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/runnable.js:266:21)\n    at Hook.Runnable.run (/home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/runnable.js:259:7)\n    at next (/home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/runner.js:271:10)\n    at Immediate._onImmediate (/home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/runner.js:292:5)\n    at processImmediate [as _immediateCallback] (timers.js:358:17)\n    at Function.module.exports.runLoopOnce (/home/alex/src/massive-js/node_modules/deasync/index.js:54:10)\n    at Object.loadSync (/home/alex/src/massive-js/node_modules/deasync/index.js:31:19)\n    at Context. (/home/alex/src/massive-js/test/loader_spec.js:44:38)\n    at callFn (/home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/runnable.js:266:21)\n    at Test.Runnable.run (/home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/runnable.js:259:7)\n    at Runner.runTest (/home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/runner.js:390:10)\n    at /home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/runner.js:473:12\n    at next (/home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/runner.js:315:14)\n    at /home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/runner.js:325:7\n    at next (/home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/runner.js:260:23)\n    at Immediate._onImmediate (/home/alex/.nvm/versions/node/v0.12.3/lib/node_modules/mocha/lib/runner.js:292:5)\n    at processImmediate [as _immediateCallback] (timers.js:358:17)\n```\nMaybe I did not set up the expected database correct - is there a script or do some tests really fail currently?\n. After re-installing Postgres 9.4 the tests ran - but I had to run them twice - first run failed.\n. @ldesplat Thanks, using the domains approach catching the error works. \nI have already created a PR for bubbling: https://github.com/robconery/massive-js/pull/104\n. No worries, take your time.\n. Thanks, but error handling now is broken if no connection can be made:\n/home/alex/code/massive-retry/node_modules/massive/lib/runner.js:36\n      next(err,null);\n      ^\nReferenceError: next is not defined\nI've send you an updated PR that fixes 2.0.8 and works with retry. (index.js also needs to be updated to work with retry)\n. Thanks, that approves my apprehension. \nAFAIK @robconery is not a friend of handling schema stuff / changes inside the application so that massive behavior makes even more sense.\nAs I'm already using .sql files I think I'll run them using psql upfront or I'll place them in my Dockerfile.\nThanks again :+1: \n. @robconery @xivSolutions @dmfay Beside the conflicts that must be resolved because the code in the PR is now outdated anything thats blocking this PR?\n. ",
    "ldesplat": "I really like this. I try very hard to keep away from libraries that throw. You can catch the exception by using domains. Since it is async underneath, try/catch will never work. BTW, domains have a lot of issues but it's what we have.\n```\nvar Domain = require('domain');\nvar d = Domain.create();\nd.on('error', function (error) {\nconsole.log(error);\n});\nd.run(function () {\nmassive.connect({connectionString: connectionString, scriptsDir: '../db'}, function (err, db) {\nif (err) {\n  console.log('connection err')\n}\n\n});\n});\n```\nBut then, I agree that if it throws only during connect then I can understand keeping it. Still, if there is an error bubbled up during connect.. I would handle it myself quite easily and most likely prevent the application from starting up. \n. ",
    "stephenkubovic": "Is there a rough timeline for when this might be landing in npm?\n. ",
    "jfbaquerocelis": "thanks for your help , now if I understood!\n. @xivSolutions All right bro! Thanks!!\n. OMG! :(\nthats weird. I keep trying, but I always throws a .log with error.\n. \nI'm trying but do not let me upload the file :(\nI will continue trying anyway, it is a log with 6000+ lines of code, .txt step but does not want to load it.\nHere I send an image to see if you have an idea, I think the problem is deasync.\n. @trentrichardson  any results, so I tried not massivejs installed.\nI tried to install separately deasync npm install -g deasync globally, but not successful. I think the problem is deasync. For now I will work with node@0.12.7, but wait that the problem is resolved. Thank you :)\n. @trentrichardson and @dmfay, unfortunately I could not solve the problem. I'll keep trying and be very attentive to any comments you make. :relaxed: \n. Because, I have several modules in my app and i need to save the database instance in a variable. Now, How could i do? Should i insert the code in the callback ?\njavascript\n      massive.connect({ connString : connString },\n                 (err, db) => {\n                         // My Code...\n                 })\n:blush: \n. ",
    "ghost": "Any idea when this will make it into a stable release? With it being a breaking change I'm guessing we won't see this until 3.x? Also living through this pain (Table.save() in particular resolving to an array or object depending on whether update() or insert() was called).\nGlad to see this has been fixed though. :+1: \n. Yes, I use only \"users\", never \"Users\".  Just a typo here.  All references in the code are correct.\n. ",
    "stevenmwade": "@xivSolutions Adding multiple columns always returns an empty array even if there are valid results.\n. At least for me, running that select statements works in pgAdmin3. Massive v2.1 is what I'm using.\n. @dmfay I think it may have to do with a unique column. I'm just playing around right now and I'm doing this:\nfor (var i = 0; i < data.length/2; i++) {\n        db.parts.findOne({name: data[i].name}, function(err, part) {\n            if(part) {\n                db.parts.save({\n                    id: part.id,\n                    name: data[i].name,\n                    status: data[i].status,\n                    fake: 'duper'\n                }, function(err, inserted) {\n                    if(err) return console.log(err);\n                });\n            } else {\n                db.parts.save({\n                    name: data[i].name,\n                    status: data[i].status,\n                    fake: 'super'\n                }, function(err, inserted) {\n                    if(err) return console.log(err);\n                });\n            }\n        });\nI keep getting the error duplicate key value violates unique constraint \"parts_name_key\". I would have just assumed, even though it's unique that it would update, granted it's not going to change so I don't need to update it anyhow.\nBack to the point, I think. So db.table.save will only do an update if a primary key is required, which means I will always have to do a db.table.findOne each time I want to run an insert/update if the data I receive doesn't have their keys in the json?\nAlso, good to know on the jsonb stringify trick. I come from NoSQL and learning PostgreSQL for new job and they have 2-3 million lines of tests/tests on 5-6k parts and it's great to have the best of both worlds.\nIf I knew enough about SQL it would be great to create a db.table.updateOrInsert type method. Thanks for the clarity!\n. Ok, I will try that. As for running the loop with async calls inside, I don't really care when the calls get run, just as long as they do and as long as they aren't running multiple times. Maybe that's just bad practice on my part and I should use each from async or just write a recursive function.\n. Sounds good, thanks for the help!\n. I'm using the same function from the other issue I wrote. I'm starting with a clean table and have limited the data set to only run two. If I run it from Node, the error happens when this.db.query runs the pg commands. I tried it straight in pgAdmin3 and it worked fine when adding the new value. I come from NoSQL and just learning PostgreSQL, so it could be my fault in that end.\nSaid function (currently fixing for async inside loop):\nfor (var i = 0; i < 2; i++) {\n        console.log(data[i].name);\n        db.parts.findOne({name: data[i].name}, function(err, part) {\n            if(part) {\n                console.log('found');\n                db.parts.save({\n                    id: part.id,\n                    status: data[i].status,\n                    fake: 'test'\n                }, function(err, inserted) {\n                    if(err) return console.log(err);\n                    // console.log(inserted);\n                    // res.send(inserted);\n                });\n            } else {\n                console.log('not found');\n                db.parts.save({\n                    name: data[i].name,\n                    status: data[i].status,\n                    fake: 'super'\n                }, function(err, inserted) {\n                    if(err) return console.log(err);\n                    // console.log(inserted);\n                    // res.send(inserted);\n                });\n            }\n        });\n    }\n. So we run tests every night on parts to pass certain parameters and they are, mostly, the same parts, but new ones are added every so often. Essentially the run the tests with jenkins and then need to post that json to PostgreSQL to have it displayed in a web environment. They don't have IDs so I used id serial primary key not null to create the ID. To be able to do the update you need the key so I have to go find the row matching the name so I can do that.\nEssentially I need to find the key that matches the part name so I can either run an update or an insert. I ran it earlier with a different json object and it ran fine, now for some reason these part names are getting duplicate name errors.\n. Well, it fixed itself with changing it to using async.each. Sorry to waste your time guys. Amazing how not handling async correctly can really mess things up.\n```\nasync.each(data, function(part, cb) {\n        db.parts.findOne({name: part.name}, function(err, foundPart) {\n            if(foundPart) {\n                foundPart.id = part.id;\n                foundPart.status = part.status;\n                foundPart.fake = 'test';\n            } else {\n                // console.log('async part name: ', part.name);\n                foundPart = {\n                    name: part.name,\n                    status: part.status,\n                    fake: 'super'\n                }\n            }\n        db.parts.save(foundPart, function(err, parts) {\n            if(err) return console.log(err);\n        });\n    });\n});\n\n```\n. They are, still results in null;\n. @robconery Sorry for the late reply, there actually was no error. The result was just empty. Actual code:\ndb.part_rules.search({columns: ['result', 'name'], term: 'warn'}, function(err, parts) {\n        if(err) return console.log(err);\n        console.log(parts);\n        res.send(parts);\n    });\nReturns []. If I take out name then I get an array of length 1396.\n@xivSolutions  Thanks for the fix! And this is just a question but when you use searchDoc is there a way to search on a document column that isn't named body?\n. Somehow missed primary key in the statement.\n. @xivSolutions Ah, fantastic. Thanks. Sorry for the lack of knowledge!\n@robconery I'm sorry I didn't specify that name was a variable, but I can see how that would work.\n. Changed it to run a SQL command and works fine. Something is getting messed up on the way.\n```\n    var cardSQL = 'INSERT INTO cards(id, barcode, form_factor) VALUES ($1, $2, $3);';\ndb.run(cardSQL, [card.id, card.barcode, card.form_factor], function(err, savedCard) {\n    if(err) {\n        console.log(err);\n        return res.send(err);\n    } else {\n        console.log(savedCard);\n    }\n});\n\n```\n. ",
    "jadams74": "What happened w/ this pr? It is not clear to me. We have the same use case.\n. We've updated several of our project's dependencies. Upon doing so line 99 in document_table.js started throwing a TypeError. Here is the stack trace:\n160809/135207.679, [error] message: Uncaught error: Cannot convert object to primitive value stack: TypeError: Uncaught error: Cannot convert object to primitive value\n    at __dirname.getWhereForDoc (/home/jadams/projects/generators/generator-samples/hapi-app/todo-list/app/node_modules/massive/lib/document_table.js:88:34)\n    at exports.findDoc (/home/jadams/projects/generators/generator-samples/hapi-app/todo-list/app/node_modules/massive/lib/document_table.js:76:20)\n    at Object.services.find (tasks.js:84:24)\n    at handler (task-find-get-config.js:25:63)\n    at Object.internals.handler (/home/jadams/projects/generators/generator-samples/hapi-app/todo-list/app/node_modules/hapi/lib/handler.js:96:36)\n    at Items.serial (/home/jadams/projects/generators/generator-samples/hapi-app/todo-list/app/node_modules/hapi/lib/handler.js:63:26)\n    at done (/home/jadams/projects/generators/generator-samples/hapi-app/todo-list/app/node_modules/hapi/node_modules/items/lib/index.js:31:25)\n    at done (/home/jadams/projects/generators/generator-samples/hapi-app/todo-list/app/node_modules/hapi/node_modules/items/lib/index.js:63:25)\n    at pre (/home/jadams/projects/generators/generator-samples/hapi-app/todo-list/app/node_modules/hapi/lib/handler.js:49:28)\n    at finalize (/home/jadams/projects/generators/generator-samples/hapi-app/todo-list/app/node_modules/hapi/lib/handler.js:281:20)\n    at wrapped (/home/jadams/projects/generators/generator-samples/hapi-app/todo-list/app/node_modules/hapi/node_modules/hoek/lib/index.js:872:20)\n    at internals.Response._processPrepare (/home/jadams/projects/generators/generator-samples/hapi-app/todo-list/app/node_modules/hapi/lib/response.js:457:16)\n    at internals.Response._prepare (/home/jadams/projects/generators/generator-samples/hapi-app/todo-list/app/node_modules/hapi/lib/response.js:396:21)\n    at process.nextTick (/home/jadams/projects/generators/generator-samples/hapi-app/todo-list/app/node_modules/hapi/lib/reply.js:151:22)\n    at _combinedTickCallback (internal/process/next_tick.js:67:7)\nThe code in question is the second part of the OR statement:\n|| conditions == \"*\")\nI think this should be:\n|| conditions === \"*\")\nThe change does solve my problem. However, I am not a javascript expert and wanted to call it out here. There may be a legitimate reason why you chose the coercive syntax. Note that by changing it to ===, the code evaluates to false as I would expect. It does not attempt to convert {} to a string.\n'conditions' itself is populated in arg_types.js.searchArgs, where the default is set to {}. That is the value the 'conditions variable' that is causing the error.  I would also like to know how to set that value instead of taking the Default. This might also help me solve the problem without requiring a change here.\nThis is our failing code:\n``\nservices.find = function (query, { limit, offset }, callback) {\n    // the { limit, offset } part above is objectdestructuring`\nconsole.log(query);\n\nquery = query || {};\nconst options = {\n    limit: limit + 1, // add one to limit so we can tell if there are more items on the next page\n    offset: offset\n};\n\nconsole.log('limit: %s', options.limit);\nconsole.log('offset: %s', options.offset);\n\ninternals.db.tasks.findDoc(query, options, (err, tasks) => {\n\n    console.log(err);\n    if (err) {\n        internals.server.log('error', err);\n        return callback(Boom.badImplementation(err, query));\n    }\n\n    if (!tasks) {\n        tasks = [];\n    }\n\n    let hasMore = false;\n    if (tasks.length > limit) {\n        hasMore = true;\n        tasks.pop();\n    }\n\n    // sometimes only one item is returned but we need an array\n    if (!_.isArray(tasks)) {\n        tasks = [tasks];\n    }\n\n    const result = {\n        data: tasks,\n        meta: {\n            pages: {\n                hasMore: hasMore\n            }\n        }\n    };\n\n    return callback(null, result);\n});\n\n};\n```\n. we are trying to find all tasks with the query {} and setting limit and offset.\n```\nvar query = {};\nvar options = {\n  limit:10,\n  offset: 0\n};\ndb.tasks.findDoc(query, options, function (err, tasks)  {\n    // do something\n});\n```\nI did some more work on this today. Turns out upgrading hapi.js from 11.x to 12.+ causes the condition to manifest.  For now we are staying at hapi 11.x till I get this figured out.  Also, I am currently putting together a hapi project using massive to attempt to reproduce minus the complexity of the rest of our project. I am having no luck here either. I have no idea what code state is actually triggering the error :/ \n. Don't case this for now. I'll keep working my end. I suspect its one of our dependencies somewhere that causes the problem. I'll reopen if I can ever reproduce in a simple project.\n. I am going into sprint planning this afternoon. How long does it generally take massivejs to release a feature to NPM?\n. We do not want the metadata in the document body.  created_at and modified_at as metadata help w/ indexing, filtering etc and keep us from having to worry about date formatting and the like inside json body. I opened an issue earlier on modified_at and @robconery is waiting my pr. I'm including it as part of the discussion here as they are related fields. \nWe would like the metadata beside the body, like we see in db.things.find(). \n[{\n      \"id\":1,\n      \"body\":{\n        \"name\":\"thing1\",\n        \"useage\":\"useless\"\n      },\n      \"created_at\":\"2016-09-06T16:12:48.149Z\",\n      \"modified_at\":\"2016-09-06T16:12:48.149Z\"\n    }]\nConcerning saving of created_at date: It seems that created_at should be ignored completely when processing update/create. Something can only be created once. modified_at however, should be applied as expected. \nPart of me thinks created_at and modified_at should be ignored entirely upon save/create and massivejs should create these values. This solves some issues around date/time formatting of json but makes a pretty opinionated statement about management of server-side time stamps. Doing so would be consistent with current behavior of the created_at field - at least what I've seen of it.\nIt would probably work well to put this option in startup. However, I was confused by the inconsistency between find and findDoc. It is definitely a 'hmm. Why is this guy different moment,' the first time it is encountered. Either option will work, but I do prefer consistency. \n. Roger, we can do that. Performance is a concern in the docs: http://massive-js.readthedocs.io/en/latest/document_queries/#why-finddoc-is-preferred\n\nThis runs a full table scan (or a \"Sequential Scan\") of the data and is not very performant as it does not use the GIN index we built for. If you use findDoc() however, we'll use the containment operator @>:\nThis query will take full advantage of our index.\n. I am going to start on updated_at this week. I can work this issue too if there is consensus.\n. \n",
    "basarevych": "Yes, I was expecting it to print \"done\" as success path and just exit immediately. Didn't know process.exit() was required here.\nIt's not the real script i'm working on. I just wanted to understand why it takes massive so long to shut down. Thank you for clarification.\n. ",
    "seymores": "I would love to help out with the docs but I am not sure how to fork the docs, or is the docs at readthedocs abandoned?\n. Happy to write the tests and do the PR.\nI tried running the tests on my local but am stuck at\nDocument updates\n       1) \"before all\" hook\n       ...\n      Error: timeout of 2000ms exceeded. Ensure the done() callback is being called in this test.\nI have created the test database to match \"postgres://rob:password@localhost/massive\" and tested to work fine using psql.\nAny idea? :-(\n. Thanks.\nTurned out the 'rob' account doesn't have admin permission to create extension.\n'permission denied to create extension \"pgcrypto\"'\n. @dmfay \nCompleted a working and test backed updateDoc, see https://github.com/seymores/massive-js/blob/master/lib/document_table.js#L58\nand the test spec here https://github.com/seymores/massive-js/blob/master/test/document_update_spec.js\nHowever I come to think that the function name 'updateDoc' is not appropriate because it can only update one attribute at a time, limitation of jsonb_set.\nI am thinking of renaming the function to updateDocAttribute but just want to run by you if you think the name is better than updateDoc.\n. Done.\n~~Creating a PR shortly.~~\nPR here https://github.com/robconery/massive-js/pull/182\n:-)\n. I am sorry -- I should have say I am already using the sql file to do those array manipulation, particularly appending data.\n@dmfay thanks, and at some point there should be a doc entry about this.\n@xivSolutions Got it, thanks.\n. +1\n. ",
    "robinjmurphy": "Was there a plan for the API docs in docs/API/tables.md and docs/API/functions.md? I'm guessing some kind of full API docs.\nI've got a branch where I've started to fill out the docs for the table's .find method. Would be interested to see if you like the approach/style.\nAn alternative to doing this manually would be to use JSDoc and generate the Markdown from that. Happy to get writing these once there's an agreed approach.\nhttps://github.com/robinjmurphy/massive-js/blob/api-docs/docs/API/tables.md\n\n. @xivSolutions Agree about the return values being clearer. Maybe a separate section outside of the table. I'll take a look and see what I can come up with.\n. I've made an update to the format. I've included an example at the top of each function. I've started to add headings for each method as well. Let me know what you think. If you're happy I'll move the branch out of my fork and into the main repo and we can contribute there.\nhttps://github.com/robconery/massive-js/compare/master...robinjmurphy:api-docs\n\n. Cool. Branch is here if anyone wants to contribute.\nhttps://github.com/robconery/massive-js/tree/api-docs\n. Thanks for the explanation.\n. Hi @squarejaw. I've updated this in #198 to remove the warning. @robconery might be able to explain the SubSonic reference in the license.\n. Any chance of getting this merged? Just spotted that the library no longer works with Node < 4 on master (it uses endsWith). Travis would pick this up. \n. Fixed in #167 \n. Happy to update to return an error instead. I'm inclined to agree that sounds better looking at it again. \n. Hi @dmfay. Is there any plan to continue to support 9.4? The tests now fail when running against 9.4.\n. Cool. Will dig in and see how big a change it is to the current implementation.\n. Might be possible to run some tests conditionally depending on the version of Postgres? Then it's just a case of clearly documenting what's supported in each version. Would definitely be good to take advantage of upsert whilst maintaining support for 9.4.\n. Looks like Travis build is happy (it's running 9.4). I'll investigate if it's possible to run the Travis tests against both 9.4 and 9.5 as a separate task.\n. Yep that was my thinking. I've only use the skip function for the 9.5 specific stuff. I updated the function count test to just use >=.\n. It'll be because the 9.5 tests aren't skipped in this branch. \n. Thanks @xivSolutions. Will do that from now on. Yep, we're using it at the BBC to power bits of iPlayer.\n. Also using assert here to be consistent with other methods. Was previously throwing a string.\n. Also adds some basic development documentation.\n. I'm not too sure why there's a need for console.log in this file, but have ignored the rule for now.\n. posttest will run the linter automatically at the end of npm test. This will mean that Travis should run the linter automatically once it's hooked up.\n. Disabling the console rule here as it's part of the REPL.\n. Use skipBelow95 just as you would it\n. I'm not too familiar with Postgres version strings, so not sure how fragile this pattern is. Just matching the major and minor version numbers and ignoring the rest.\n. Rather than switch these tests based on version, I'm just using a >= check to avoid getting into the specifics of how many functions each version ships with.\n. Possible feature here would be appending something like 'skipped as Postgres version <= 9.5' to the test description.\n. ",
    "vertiman": "the concept of de-async seems so foreign in node/js.  Keep in mind iojs is the next node, it's being merged/finalized as we speak.\nwhat are you using deasync for?  I'll poke around in the code and see if I can come up with something and help!\n. ",
    "alexishevia": "@keithlayne I had the same issue when trying to use massive-js with iojs (deasync wouldn't install), so I decided to move back to node 0.12.x. \nHowever, I agree the ideal solution would be updating the deasync dependency, because sooner or later people are going to move to node 4.0 and will face the same issue.\n. I get the same result with master. I'll try to figure out how to get the error code back and will send a PR if I can.\n. @diegoaguilar I found out the root cause. On commit 0b56fd5148591bdd15c98d8f0250b209b4770d4f we stopped bubbling up the error given by db.query, and instead we construct a new Error object. Since only the message attribute is attached to the Error object, all other info is lost.\nI was able to get the error code back on this commit: https://github.com/alexishevia/massive-js/commit/a8842cde885cebd61ebad9e44a9be278bf36ee5d\nI wanted to add a test before sending the PR, but haven't had time for that. Will probably submit it in a couple of days (unless you want to take over). \n. ",
    "keithlayne": "Okay, I was totally doing it wrong. I just didn't have the local db set up for testing.  Not that it's hard, but I think it's completely undocumented in the repo.\nSo the good news is that updating deasync to v0.1.1 and using node v4.0.0 does not break any tests.\n@robconery I understand you don't feel compelled to change this right now.  I only really made the pull request because I saw #115 and I already had a fork.  Once I get more done on my current project I will be able to report back with more concrete info about how these versions work in practice.\nAlso, if anyone was interested in testing this with their code, I moved my changes to a branch: https://github.com/keithlayne/massive-js/tree/update_deasync.\n. See https://github.com/robconery/massive-js/pull/116#issuecomment-139346461.\nThe test setup is not documented, but pretty simple.  The tests all pass.  You can run them with mocha test - to use npm test you just need to update package.json.\nI'll try to do a documentation PR tonight about the test setup. \n. @xivSolutions I've been pleasantly surprised with how responsive both you and @robconery have been about this.  \nJust to be clear, I have run the test suite against node 4 with the updated deasync dependency, and all tests pass.  It also ran it against iojs 3.3.0 for good measure, and everything was green.\nNot that I think that's sufficient to merge - like I said in the other thread, I'll be using this against node 4 from my branch to see if anything comes up.\n. @stephenmathieson See https://github.com/abbr/deasync/commit/47827733c3e57b6b1669ab26426f8c3022a99d9d - looks like somebody took your advice eventually.\n. ",
    "belfour": "Very new to node-js, postgres & github so please direct me to the proper place to ask this question if not here.  \nWhen I use NPM to install massive I get the error below.  Can I correct that?  My assumption is that I need to manually pull and install each dependency from Github.  Not a big deal but is there an easier way?  \n\nBuild failed\nnpm ERR! Windows_NT 6.1.7601\nnpm ERR! argv \"C:\\nodejs\\node.exe\" \"C:\\nodejs\\node_modules\\npm\\bin\\npm-cli.js\" \"install\" \"massive\" \"--save\"\nnpm ERR! node v4.0.0\nnpm ERR! npm  v2.14.2\nnpm ERR! code ELIFECYCLE\nnpm ERR! deasync@0.0.10 install: node ./build.js\nnpm ERR! Exit status 1\nnpm ERR!\nnpm ERR! Failed at the deasync@0.0.10 install script 'node ./build.js'.\n. UTF!  Curses!  Thanks for the help, UTF was the issue.\n. ",
    "kvnlnt": "@xivSolutions and @keithlayne thanks for looking into this. \nAccording to the news release last Tuesday Node and IO.js are the same (https://nodejs.org/en/blog/release/v4.0.0/)..\n\n\"... Node.js project and the io.js project that are now combined in a single codebase.\"\n\nI'll try to see if I can't get the test suite up and running and run tests on my update here. Hopefully something as small as upgrading deasync isn't something big (as it often is).\n. ",
    "stephenmathieson": "somewhat related: https://github.com/vkurchatkin/deasync/pull/1\n. @keithlayne yup see #119 \n. The link still points to the wrong repo, which was the point of this PR.\nI'm glad deasync has been updated; hopefully that makes it possible to use massive on newer versions of node. \n. ",
    "vkurchatkin": "Just to give you some backstory: I've originally came up with an idea of deasync, but quickly realised that it's terrible and broken by design, so I never actually publsihed it on npm. I strongly recommend not to use it for anything. It has weird semantics and seriously messes with event loop. It goes without saying that deasync approach is not supported by node.\n. ",
    "matheo": "req.body is going as expected, I didn't mention my console.log(req.body) here. In the first case, the object is fine: \n{ titulo: 'Entrada', fecha: '144102924600' }\nand when I jsonify it goes well too:\n{\"titulo\":\"Entrada\",\"fecha\":\"144102924600\"}\nnot even hardcoding the record works:\n```\ndb.eventos.save({titulo:'Entrada',fecha:'144102924600'}', function(err, event){\n    if(err){ return next(err); }\nres.json(event);\n\n});\n```\nI have to specify all the table fields? it's supposed to be not required right?\n. For the record, here is my spec:\nUbuntu 14.04.3 LTS\nnpm 2.14.2\nnode v4.0.0\nmassive 2.0.9\nexpress 4.13.1\n. Wooot, that was the problem.\nMistakes in the table definition all around! the first exception (array value must start with &#34;{&#34; or dimension information) was because my field was an array, that was an accident.\nNow the save runs smootly.\nThank you very much!\n. ",
    "jmealo": "Thanks for the quick reply. Neither of those apply, so either my server is misconfigured or we hit an edge case relating to foreign tables. As you can see below, there's not very much going on in this database:\nSchema |                 Name                  |       Type        |  Owner   \n--------+---------------------------------------+-------------------+----------\n public | content_areas                         | table             | spark\n public | content_areas_id_seq                  | sequence          | spark\n public | matchbook_id_seq                      | sequence          | postgres\n public | people                                | foreign table     | postgres\n public | pg_all_foreign_keys                   | view              | postgres\n public | s2_apply_projects                     | foreign table     | postgres\n public | s2_assessment_types                   | foreign table     | postgres\n public | s2_assessments                        | foreign table     | postgres\n public | s2_conference_resources               | foreign table     | postgres\n public | s2_guiding_questions                  | foreign table     | postgres\n public | s2_history_apply_projects             | foreign table     | postgres\n public | s2_history_assessments                | foreign table     | postgres\n public | s2_history_conference_resources       | foreign table     | postgres\n public | s2_history_guiding_questions          | foreign table     | postgres\n public | s2_history_learn_links                | foreign table     | postgres\n public | s2_history_teacher_resources          | foreign table     | postgres\n public | s2_learn_links                        | foreign table     | postgres\n public | s2_teacher_resources                  | foreign table     | postgres\n public | s2_vendor_domains                     | foreign table     | postgres\n public | s2_vendors                            | foreign table     | postgres\n public | sparkpoint_standard_alignments        | table             | spark\n public | sparkpoint_standard_alignments_id_seq | sequence          | spark\n public | sparkpoints                           | table             | postgres\n public | sparkpoints_edges                     | table             | spark\n public | sparkpoints_edges_id_seq              | sequence          | spark\n public | standards                             | table             | spark\n public | standards_documents                   | table             | spark\n public | standards_documents_id_seq            | sequence          | spark\n public | standards_edges                       | table             | spark\n public | standards_edges_id_seq                | sequence          | spark\n public | standards_grades                      | materialized view | spark\n public | standards_id_seq                      | sequence          | spark\n public | standards_jurisdictions               | materialized view | spark\n public | standards_subjects                    | materialized view | spark\n public | tap_funky                             | view              | postgres\n public | vendor_hostnames                      | table             | spark\n public | vendor_hostnames_id_seq               | sequence          | spark\n public | vendor_standards_crosswalk            | table             | spark\n public | vendor_standards_crosswalk_id_seq     | sequence          | spark\n public | vendors                               | table             | spark\n public | vendors_id_seq                        | sequence          | spark\nThis is my development configuration file, so I realize that there's a lot of logging, but that shouldn't be impacting the SELECT that much...\ndata_directory = '/var/lib/postgresql/9.5/main'\nhba_file = '/etc/postgresql/9.5/main/pg_hba.conf'\nident_file = '/etc/postgresql/9.5/main/pg_ident.conf'\nexternal_pid_file = '/var/run/postgresql/9.5-main.pid'\nlisten_addresses = '*'\nport = 5432\nmax_connections = 100\nsuperuser_reserved_connections = 3\nunix_socket_directories = '/var/run/postgresql'\nssl = true\nssl_cert_file = '/etc/ssl/certs/ssl-cert-snakeoil.pem'\nssl_key_file = '/etc/ssl/private/ssl-cert-snakeoil.key'\ndynamic_shared_memory_type = posix\nlog_destination = 'stderr'\nlogging_collector = on\nlog_directory = 'pg_log'\nlog_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'\nlog_file_mode = 0600\nclient_min_messages = log\nlog_min_messages = warning\nlog_min_error_statement = error\nlog_min_duration_statement = -1\nlog_duration = true\nlog_line_prefix = '%t [%p-%l] %q%u@%d '\nlog_lock_waits = off\nlog_statement = 'all'\nlog_timezone = 'UTC'\ntrack_activities = on\ntrack_counts = on\ntrack_io_timing = on\ntrack_functions = pl\ntrack_activity_query_size = 1024\nupdate_process_title = on\nstats_temp_directory = '/var/run/postgresql/9.5-main.pg_stat_tmp'\nautovacuum = on\nautovacuum_max_workers = 3\nsearch_path = '\"$user\",public'\ndefault_text_search_config = 'pg_catalog.english'\nrestart_after_crash = on\nmax_connections = 100\nshared_buffers = 1GB\neffective_cache_size = 3GB\nwork_mem = 5242kB\nmaintenance_work_mem = 256MB\ncheckpoint_completion_target = 0.9\nwal_buffers = 16MB\nwal_level = logical\nfsync = on\ndefault_statistics_target = 100\nmax_wal_senders = 10\nshared_preload_libraries = 'decoderbufs'\nshared_preload_libraries = 'decoder_json'\nshared_preload_libraries = 'decoding_json'\nmax_replication_slots = 10\nRelevant version numbers:\nNode v4.1.1, PostgreSQL 9.5 Alpha 2 on Ubuntu 14.04 and OSX 10.9.5\n. @xivSolutions I do have 1282 functions. The query issued by \\df takes  550.892 ms. That's still a far cry from what we're seeing but is surely a contributing factor.\nspark=# \\df\nLOG:  statement: SELECT n.nspname as \"Schema\",\n  p.proname as \"Name\",\n  pg_catalog.pg_get_function_result(p.oid) as \"Result data type\",\n  pg_catalog.pg_get_function_arguments(p.oid) as \"Argument data types\",\n CASE\n  WHEN p.proisagg THEN 'agg'\n  WHEN p.proiswindow THEN 'window'\n  WHEN p.prorettype = 'pg_catalog.trigger'::pg_catalog.regtype THEN 'trigger'\n  ELSE 'normal'\n END as \"Type\"\nFROM pg_catalog.pg_proc p\n     LEFT JOIN pg_catalog.pg_namespace n ON n.oid = p.pronamespace\nWHERE pg_catalog.pg_function_is_visible(p.oid)\n      AND n.nspname <> 'pg_catalog'\n      AND n.nspname <> 'information_schema'\nORDER BY 1, 2, 4;\nLOG:  duration: 550.892 ms\n. ```\nLOG:  statement: EXPLAIN ANALYZE SELECT n.nspname as \"Schema\",\n  p.proname as \"Name\",\n  pg_catalog.pg_get_function_result(p.oid) as \"Result data type\",\n  pg_catalog.pg_get_function_arguments(p.oid) as \"Argument data types\",\n CASE\n  WHEN p.proisagg THEN 'agg'\n  WHEN p.proiswindow THEN 'window'\n  WHEN p.prorettype = 'pg_catalog.trigger'::pg_catalog.regtype THEN 'trigger'\n  ELSE 'normal'\n END as \"Type\"\nFROM pg_catalog.pg_proc p\n     LEFT JOIN pg_catalog.pg_namespace n ON n.oid = p.pronamespace\nWHERE pg_catalog.pg_function_is_visible(p.oid)\n      AND n.nspname <> 'pg_catalog'\n      AND n.nspname <> 'information_schema'\nORDER BY 1, 2, 4;\nLOG:  duration: 170.564 ms\n                                                         QUERY PLAN                                                          \n\nSort  (cost=364.52..367.49 rows=1188 width=138) (actual time=169.169..169.323 rows=1282 loops=1)\n   Sort Key: n.nspname, p.proname, (pg_get_function_arguments(p.oid))\n   Sort Method: quicksort  Memory: 390kB\n   ->  Hash Join  (cost=1.33..303.84 rows=1188 width=138) (actual time=117.786..167.026 rows=1282 loops=1)\n         Hash Cond: (p.pronamespace = n.oid)\n         ->  Seq Scan on pg_proc p  (cost=0.00..276.46 rows=1404 width=78) (actual time=0.127..161.492 rows=4103 loops=1)\n               Filter: pg_function_is_visible(oid)\n               Rows Removed by Filter: 12\n         ->  Hash  (cost=1.20..1.20 rows=11 width=68) (actual time=0.016..0.016 rows=12 loops=1)\n               Buckets: 1024  Batches: 1  Memory Usage: 10kB\n               ->  Seq Scan on pg_namespace n  (cost=0.00..1.20 rows=11 width=68) (actual time=0.005..0.010 rows=12 loops=1)\n                     Filter: ((nspname <> 'pg_catalog'::name) AND (nspname <> 'information_schema'::name))\n                     Rows Removed by Filter: 2\n Planning time: 0.186 ms\n Execution time: 169.506 ms\n(15 rows)\n```\nI'm not sure if that helps.\n. I think your time would be best spent updating the docs and we'll try doing the whitelist/blacklist on functions. I didn't see any introspection going on that should be querying the remote tables. Do we consider my tables to be excessive for your typical use cases?\n. @dmfay: My application is a REST API. My route handlers are stateless so route handlers can be hot swapped (loaded, unloaded, modified) without restarting the process. I need to do the same with SQL. I want to lint the files so in the event of a failed file transfer or to help during development I fail fast/early. If the syntax checking at load time isn't worth the trouble of dealing with ecpg I understand. Are you not interested in either features?\n. In a cluster zero-time scenario it'd be much better to respawn the process allowing massive to redo it's initialization, but if you were to do that on lets say 8 workers, we're talking about a huge hit depending on the size of your DB. This is the obviously the safest way to do things (and probably best practice if you can stomach the CPU).\n@robconery Effort would probably be better placed by supporting clustering and zero downtime deployments in massive, which would just be a parent worker doing all the initialization and then pushing that down to each worker. You could then send a signal or use REPL to tell the master to reinitialize massive and it could gradually respawn the workers as their connections empty.\nI didn't test this yet, but I'm not entirely sure how $1 placeholders would impact the syntax checking. I was using pg-promise and liked how my code looked. I'd need to figure out more about how connection pooling and a few other things work. I may be interested in being a major contributor to Massive depending on how it looks under the hood.\nMy first steps would be adding transparent temporal support and some helpers do ACL with RLS as well as a way to use the GUC or plv8 to store the web application user when doing audit logging and enforcing RLS constraints.\nI haven't researched if EXPLAIN would be a safe substitute for syntax checking (I know that EXPLAIN CONTRIBUTE is not).\n. IMHO, PM2 served its purpose when cluster wasn't stable. Now that it's stable and the Node v4 rebirth has occurred it's time to embrace cluster. If someone is using PM2 and the workers aren't stateless to the point where a PM2 run service can be converted to a cluster service than it's not really stateless.\nIf you just add a \"stateless\" property to your modules you can easily do hot swap without respawing the process by checking for that and acting accordingly depending on how you reference your external dependencies. Things aren't going to grind to a halt, if a reference is held V8 isn't going to pull the module out from under someone who is using it. If all goes well and you delete the reference from the require cache it should be garbage collected when the last thing using it is finished executing or isn't referenced anywhere.\nDepending on how mindful one optimizes their code for V8 and being disciplined to not rely on side effects and closures your memory usage, performance, JITing and ability to do things like hot swap are improved. That being said it sounds like Massive isn't safe to do that (most code isn't, closures are tempting and sometimes a must, it's just not a strong part of the JS community...) so running in a master process and then passing the initialization data to each of the workers makes the most sense to me.\n- https://github.com/petkaantonov/bluebird/wiki/Optimization-killers\n- https://nodejs.org/api/cluster.html\n- https://github.com/doxout/recluster\nIf cluster mode is something you're interested in then that should go on another a ticket but if it were unclear that's where this discussion ended up going.\n. Understood, it's a lot to bite off, not relying on side effects and trying to use stateless and functional paradigms isn't natural or widely practiced in JavaScript. Forcing yourself to do so results in much more robust code, but it requires much more effort to write if you're flying by the seat of your pants with loosely defined requirements. It's nice to follow the practices, write a few hundred lines of code and it runs right the first time (never having run the file while coding)... it's the most satisfying feeling I've had developing. I'm still getting the hang of it but it's necessary in building high scalable, fault-tolerant applications.\nIf you're looking for a rush on a rainy day try to think in terms of minimizing side effects and mutability. It's kind of hard to do for a library like Massive. It's nice that you don't force promises or anything too opinionated. \n. I think it's a good fit because this is the minimalist way of streaming changes that requires no changes to your database, only its configuration. (No triggers, listen/notify, schema changes are needed).\nI could see it going both ways.\nSince Lapidus and Massive can play nice together maybe there's no point, however, I think that the same people who would appreciate one would appreciate the simplicity of the other.\nI would rather not implement a subset of Massive's features in Lapidus if possible (mostly all of the introspection), I'm trying to avoid keeping a \"control connection\" open to PostgreSQL if I don't need it.\n. @xivSolutions Just to clarify, this is not pub sub, this is full streaming of changes at the database level. I just imagined that each table would be an event emitter where you could listen for update, insert, or delete events. Since that's more than what most people are interested in, you would just listen for the events you're interested in. It's possible that decoding_json may offer configuration options at some point to do filtering at the logical decoding side to prevent those changes from even being streamed.\n. I'll need this for my RLS setup as well. Anything I can do to help get this merged?. @deoxen0n2: check out: https://github.com/posix4e/jsoncdc and: https://github.com/JarvusInnovations/lapidus\nWhat is your use case?\n- Publish/Subscribe to database/schema/tables/rows/columns?\n- Attach event handlers to tables\n- Stream changes sequentially as an event log\n- What are your delivery requirements (at least once, only once)?\nTaking into consideration your applications delivery and ordering requirements logical decoding becomes appealing as it leverages PosgtreSQL's replication slots to make sure all interested parties have replayed WAL segments before deleting them*.\nIf you decide to try out logical decoding, be sure to delete the replication slot when you are done with it or you will eventually run out of disk space (could be minutes/hours/days/weeks, but it will happen).\n. ",
    "rohbotics": "This is my query SELECT p.* FROM users_projects up, projects p WHERE user_id = $1 and p.project_id = up.project_id;\nThe query runs fine when copy-pasted into psql. It also works when cat and piped into psql.\nHowever, certain editors put a Byte Order Mark at the beginning of a UTF-8 encoded file. The Byte Order Mark is not shown by text editors, and is generally ignored by most file i/o tools. Massive sends the entire file including the Byte Order Mark to postgres, causing postgres to return an error.\n. How do I run the tests?\n. After upgrading to Postgres 9.4\nAll looks good, 213 passing (11s).\n. ",
    "creole": "Yes, how should I catch and handle it without letting my app crash?\nBTW, I love the jsonb api and am curious to see what you will come up with when they release postgres 9.5.\n. I'm using your express example with db = app.get('db'); the connection was established fine, but then the postgres server must have somehow become unresponsive, so that the error was thrown when I called db.\n. Sure:\nvar express = require('express'); \nvar app = express();\nvar massive = require(\"massive\");\nvar DATABASE_URL = 'postgres://etcetera';\nvar connectionString = DATABASE_URL + '?ssl=true';\nvar massiveInstance = massive.connectSync({connectionString : connectionString});\napp.set('db', massiveInstance);\nvar http = require('http').createServer(app);\nvar db = app.get('db');\ndb.w.findDoc(1, function(err, result)\n{\n    console.log('result[0] = ' + util.inspect(result[0], { showHidden: false, depth: null }));\n});\nSo the above is all fine, and result[0] gives the expected - well, result. Following calls with:\ndb.w.findDoc(id, function(err, result){});\nalso work as expected, until one of them throws the error I mentioned above and crashes my app.\n. I thought a stack trace was what I included with my first post. Do you need more? If you do, you might also find it here: https://github.com/brianc/node-postgres/issues/821\nThe original commenter seems to have found a way to handle the error by using pg.on(...). Would exposing the pg object be an option?\n. OK thanks.\n. ",
    "aderbas": "Hi @dmfay The user is allowed to access the database and all the tables. I print on output the db object and have a list of functions on database but list of tables is empty. I added handler on connection and do not get any error.\n. grant all privileges on database test to aderbas\nI did it for all tables too, just in case. But not working\n. @robconery the user already owns the database, by guarantee I did what you said.\n@dmfay \n\ndoes not yet work =/\n. @dmfay I have already put the user 'aderbas' as owner of the database, tables and sequences.\nI tried to use another node client to postgres, (https://github.com/brianc/node-postgres) and got success, same connectionString. \n. Oh, the table \"books\" have a primary key but \"articles\" don't have. I create pk in articles and now works fine.\nThanks @dmfay for insisting on help =D\n. @robconery yep, is solved, I believe in my attempts to put everything to aderbas as owner\n. /var/www/mobileapi/lib/node_modules/massive/node_modules/deasync/index.js:38\n            throw err;\n                  ^\nerror: no pg_hba.conf entry for host \"127.0.0.1\", user \"foo\", database \"fooserver\", SSL off\n    at Connection.parseE (/var/www/mobileapi/lib/node_modules/massive/node_modules/pg/lib/connection.js:539:11)\n    at Connection.parseMessage (/var/www/mobileapi/lib/node_modules/massive/node_modules/pg/lib/connection.js:366:17)\n    at Socket.<anonymous> (/var/www/mobileapi/lib/node_modules/massive/node_modules/pg/lib/connection.js:105:22)\n    at Socket.emit (events.js:95:17)\n    at Socket.<anonymous> (_stream_readable.js:765:14)\n    at Socket.emit (events.js:92:17)\n    at emitReadable_ (_stream_readable.js:427:10)\n    at emitReadable (_stream_readable.js:423:5)\n    at readableAddChunk (_stream_readable.js:166:9)\n    at Socket.Readable.push (_stream_readable.js:128:10)\n. Yes, but why not accept any connection from an unsafe environment, no SSL connections. There is no way to make this connection even the script running on the same machine as the database. \n. Ops, I'm have added ?ssl=true on connectionString and works =D\nThk's for help @robconery \n. Okay, any idea how to enter into a postgres table with 'geometry' type? I'm try this\n```\nvar q = \"INSERT INTO ae_complaint (title, description, geom, date_created, user_created) VALUES ($1,$2,ST_GeomFromText ('POINT ($3)', 4326),$4,$5) RETURNING id\";\ndb.run(q, [\"foo\", \"bar\", \"valid lat valid lng\", new Date(), 1345], function(err,res){ .... });\n```\nAnd I get this error: could not determine data type of parameter $3\n. I insert the line using the terminal and it worked. To work in Massive I did so:\nvar q = util.format(\"INSERT INTO ae_complaint (title, description, geom, date_created, user_created) VALUES ($1,$2,ST_GeomFromText('POINT (%s %s)', 4326),$3,$4) RETURNING id\", point.lat, point.lng);\n. I'm having the same problem the topic created by @ludvigsen but in a different situation. In my case the procedure already exists but when trying to run\napp.get('db').insertcmdusr(['1455', '3020', '8154908', 'true', '0','0','0'], function(err, res){\n  ....\n}\nError: bind message supplies 7 parameters, but prepared statement \"\" requires 1\nWhen we perform the function using psql, it works correctly. I'm use Node v6.9.1 and Massive 2.5.0\n. The procedure already exists and I use on another system with the same user normally. I think the problem is the type of parameter procedure, in fact only has one parameter but is a character varying[] type. I see in procedure signature: \nCREATE OR REPLACE FUNCTION insertcmdusr(VARIADIC params character varying[]) ...\n(dba staff who created)\nThen, the procedure has only one parameter but this is variable.\n. Yes, I made a sql call and it worked. app.get('db').run(\"SELECT insertcmdusr(....)\", function(err, result) ....\nFor now, it will look like this. Thanks.\n. ",
    "weagle08": "can anyone explain why a primary key is necessary on a table for massive to expose it as available table object?. Ok, thanks for the update. May go through and update the dependency that is causing it asap.. ",
    "Atulvermaon18": "Not able to connect to postgres database, getting error, please tell me what I have done wrong\nexports = module.exports = (connection, loaderConfig = {}, driverConfig = {}) => {\n                                                     ^\nSyntaxError: Unexpected token =\n    at exports.runInThisContext (vm.js:53:16)\n    at Module._compile (module.js:374:25)\n    at Object.Module._extensions..js (module.js:417:10)\n    at Module.load (module.js:344:32)\n    at Function.Module._load (module.js:301:12)\n    at Module.require (module.js:354:17)\n    at require (internal/module.js:12:17)\n    at Object. (/home/shoppertreat/postgres/index.js:3:17)\n    at Module._compile (module.js:410:26)\n    at Object.Module._extensions..js (module.js:417:10)\n. Yeah It was a Node version Issue. when I upgraded it worked well but I am not able to connect to Postgres getting error \n(node:27380) UnhandledPromiseRejectionWarning: Unhandled promise rejection (rejection id: 2): error: password authentication failed for user \"postgres\". ",
    "bjnortier": "@robconery \n\nI'll throw this back at you: can you explain why a primary key is not necessary? I do know that legacy systems and other completely wrong ideas lead to tables without primary keys... in that situation you're welcome to use straight up SQL to get around the issue. Or, perhaps, use a different DB tool that allows you to shoot yourself in the face :).\n\nDisclaimer: Just looking into TimescaleDB, not an expert on that or Postgres.\nTimescaleDB (https://www.timescale.com) is a time-series extension for Postgres. It doesn't use primary keys, well, not conventionally defined ones anyway. I would like to use MassiveJS (which is great, btw), with TimescaleDB, but seems like I can't at this point. Investigating further but I'll probably just use custom queries instead of db.foo.find() etc.. ",
    "tylercollier": "For me it was permissions. I used this hint about using \\z from stackoverflow on a particular table to realize I had not assigned permissions to the user properly (I had run my grant privileges without having first switched to the correct database using \\c).. Is there a way to set global config options for massivejs (I looked all through the doc, which is well written and organized, and didn't see it)? I'd like to default { deepInsert: false } everywhere.. ",
    "rcalabro": "Thank you @aderbas !!\n. ",
    "odupuy": "If you want to go further, either rejecting a self-signed certificate from the server (accepted by default) or implementing mutual authentication with certificates on both sides, look at https://github.com/brianc/node-postgres/wiki/SSL-support for an example of the configuration to use.\nEither you use ?ssl=true or you use the ssl property of the configuration object but not both.\nTest first that >psql shows an SSL connection before doing more in Massive.\n. ",
    "lfreneda": "Thank you so much @robconery :dancer: \n. @robconery Wow, so fast D: r u the Flash?\n\nSince coffeescript is\n```\nclass Activity\n  constructor: (params) ->\n    binder @, properties, params\nreportAsProblem: (problemDescription) ->\n    @status = ActivityStatus.reported\n    @problemDescription = problemDescription\nmodule.exports = Activity\n```\nit will be converted to:\n```\nvar Activity;\nActivity = (function() {\n  function Activity(params) {\n    binder(this, properties, params);\n  }\nActivity.prototype.reportAsProblem = function(problemDescription) {\n    this.status = ActivityStatus.reported;\n    return this.problemDescription = problemDescription;\n  };\nreturn Activity;\n})();\nmodule.exports = Activity;\n```\nThank you a lot\n. Just found out that the problem is when updating \nI created a repository https://github.com/lfreneda/massive-js-issue-260 replicating the exception\ncode failing: https://github.com/lfreneda/massive-js-issue-260/blob/master/src/index.coffee\n{ Error: column \"reportAsProblem\" of relation \"activities\" does not exist\n    at DB.query (/home/ryuk/lfreneda/work/issues/massive-js-issue-260/node_modules/massive/lib/runner.js:22:11)\n    at Table.update (/home/ryuk/lfreneda/work/issues/massive-js-issue-260/node_modules/massive/lib/table.js:100:11)\n    at Table.save (/home/ryuk/lfreneda/work/issues/massive-js-issue-260/node_modules/massive/lib/table.js:121:10)\n    at Object.next (/home/ryuk/lfreneda/work/issues/massive-js-issue-260/src/index.js:28:28)\n    at .callback (/home/ryuk/lfreneda/work/issues/massive-js-issue-260/node_modules/massive/lib/runner.js:67:14)\n    at Query.handleReadyForQuery (/home/ryuk/lfreneda/work/issues/massive-js-issue-260/node_modules/pg/lib/query.js:89:10)\n    at .<anonymous> (/home/ryuk/lfreneda/work/issues/massive-js-issue-260/node_modules/pg/lib/client.js:163:19)\n    at emitOne (events.js:101:20)\n    at emit (events.js:188:7)\n    at Socket.<anonymous> (/home/ryuk/lfreneda/work/issues/massive-js-issue-260/node_modules/pg/lib/connection.js:109:12)\n  message: 'column \"reportAsProblem\" of relation \"activities\" does not exist',\n  code: '42703',\n  detail: undefined,\n  length: 127,\n  severity: 'ERROR',\n  hint: undefined,\n  position: '60',\n  internalPosition: undefined,\n  internalQuery: undefined,\n  where: undefined,\n  schema: undefined,\n  table: undefined,\n  column: undefined,\n  dataType: undefined,\n  constraint: undefined,\n  file: 'analyze.c',\n  line: '1997',\n  routine: 'transformUpdateStmt' }\n. @robconery, Since insert is working as expected, I might have a clue..\nobject properties on insert are retrieved with undercore keys _.keys(object)\nobject properties on update are retrieved with underscore omit _.omit(fields, pkName)\nit was changed on this commit: https://github.com/robconery/massive-js/commit/591c430c66799e0a93c545ef2340c1888f0980b8\n. Can I code something to fix it?\n. Got it, give me some minutes haha\n. What u think? https://github.com/robconery/massive-js/pull/261\n. My pleasure! \nBig fan of yours :angel: \n. Any idea when a new release will be publish to npm? \n. <3\n. @igrayson I was wondering, what did you end up doing?. @dmfay I'll check it out for sure! thank you. @dmfay in my case, that was exactly what I did.. But I was wondering that it can be useful outside massive. \nAre you guys okay with a massive subset fork for query generation? :flushed:\n. Thanks, @dmfay, I will let you know what I got.. ",
    "kesavkolla": "Sorry to add more comments on it.  The timestamp with timezone in PostgreSQL works well when we issue SET SESSION TIME ZONE 'zone'.  This setting of time zone is needed to get the correct date back from PostgreSQL.  If you can provide a callback/event on a open connection then application can can write custom logic.\n. ```SQL\ncreate table reservation_hhh\n(\n    property uuid not null,\n    reservationid text not null,\n    confnum text,\n    creationdt timestamp with time zone,\n    updatedt timestamp with time zone,\n    status text,\n    arrivaldt date,\n    departuredt date,\n    constraint reservation_hhh_pkey\n        primary key (property, reservationid)\n)\ninherits (reservation)\n;\ncreate index reservation_hhh_property_stayrange_idx\n    on reservation_hhh (property, stayrange)\n;\ncreate index reservation_hhh_property_marketcd_idx\n    on reservation_hhh (property, marketcd)\n;\ncreate table reservation\n(\n    property uuid not null\n        constraint reservation_property_fkey\n            references property,\n    reservationid text not null,\n    confnum text,\n    creationdt timestamp with time zone,\n    updatedt timestamp with time zone,\n    status text,\n    arrivaldt date,\n    departuredt date,\n    constraint reservation_pkey\n        primary key (property, reservationid)\n)\n;\n```. I took the tables.sql that it gets generated and ran in my database here is the output I get\n\n. ",
    "michaelhue": "Made an error when changing the variables, will resubmit the pull request.\n. Error message for reference:\napi_1 | [nodemon] starting `node --harmony --use-strict .`\napi_1 | /app/node_modules/massive/lib/arg_types.js:7\napi_1 | exports.queryArgs = function(arguments){\napi_1 |                              ^^^^^^^^^\napi_1 | \napi_1 | SyntaxError: Unexpected eval or arguments in strict mode\napi_1 |     at exports.runInThisContext (vm.js:53:16)\napi_1 |     at Module._compile (module.js:404:25)\napi_1 |     at Object.Module._extensions..js (module.js:432:10)\napi_1 |     at Module.load (module.js:356:32)\napi_1 |     at Function.Module._load (module.js:313:12)\napi_1 |     at Module.require (module.js:366:17)\napi_1 |     at require (module.js:385:17)\napi_1 |     at Object.<anonymous> (/app/node_modules/massive/lib/runner.js:5:16)\napi_1 |     at Module._compile (module.js:425:26)\napi_1 |     at Object.Module._extensions..js (module.js:432:10)\napi_1 |     at Module.load (module.js:356:32)\napi_1 |     at Function.Module._load (module.js:313:12)\napi_1 |     at Module.require (module.js:366:17)\napi_1 |     at require (module.js:385:17)\napi_1 |     at Object.<anonymous> (/app/node_modules/massive/index.js:1:76)\napi_1 |     at Module._compile (module.js:425:26)\napi_1 | [nodemon] app crashed - waiting for file changes before starting...\n. ",
    "ravestack": "Alright, I did createdb massive and ran mocha .\nI got this: 0 passing (2ms)\nI'm guessing wildly that this has something to do with this line in test/helpers/index.js:\nconnectionString = \"postgres://rob:password@localhost/massive\";\nI can look into getting tests to work. Just curious if there's any particular local development-specific things I should look out for.\n. ",
    "eymengunay": "@dmfay I've seen that v2.1.0 has just been released but unfortunately doesn't include this pr. Should we expect a release including the fix anytime soon?\n. @xivSolutions @dmfay Thank you for the explanation. I'm already using the master branch although as you said it is not an ideal solution. \nI'm looking forward for a new release, thank you all for this awesome library :)\n. Hi @robconery, do you have any updates on this?\n. Perfect! Thank you\n. Sure thing! I wasn't sure if it was done intentionally so I kept the changes to a minimum. I'll also check the update method and add the tests.\n. Hello again @robconery,\nI have some doubts about the test implementation. I was going to add a new length column to the products table but I have seen that it already has one under dimension jsonb. \nI don't want to create confusion by adding a similar column to products table and I guess it doesn't make much sense adding it to user or doc.\nI think it might be an \"overkill\" adding a new table just for this so I wanted to ask you about where to put this new length column.\n. ",
    "jwhitmarsh": "No worries, I wasn't sure if the code would be useful or not (since it's just a simple async loop):\n```\nfunction maGetAll(i, next) {\n  console.time('maGetAll' + i);\n  db.apps.find({}, function(err) {\n    if (err) {\n      return fail(err);\n    }\n    console.timeEnd('maGetAll' + i);\n    next();\n  });\n}\nfunction many() {\n  async.timesSeries(100, function(i, next) {\n    //szCreate(i, next);\n    //maCreate(i, next);\n//szGetAll(i, next);\nmaGetAll(i, next);\n\n}, function() {\n    process.exit();\n  });\n}\nmany();\n```\nI'm never actually doing anything with the results so presumably it's being held in memory somewhere, i'm just not sure where... \nBut it sounds like it's more of an issue with my loop rather than the library? \nEdit: you're assumption is correct, Sequelize doesn't error. The code is basically identical. \n. ",
    "kzar79": "\nI have two tables without a pk, because they are just a collection of attributes linked to another table. So I just have an \"on delete cascade\" on their fk, and when I select them I do it through the fk and always need all of them. If I really wanted a pk on that table, that would be a composite key, anyway.\nComposite keys are quite common in real-life databases.\n\nAnyway, if you don't think that could be useful, no need to include them in the master. I will keep using my fork with my changes, I solved my problem and I'm happy with that.\n. ",
    "skeie": "Thanks for the answer. Would you please state what you mean with use a single SQL file?\n. Yep, thanks, I knew that, I just didn't catch what you mean by use a single sql file, my bad.\n. ",
    "futuremint": "I just looked at your unit tests and it looks like you have this covered there... so I'm trying to narrow this down more in my setup. I'm using nw.js 0.12 which has io.js 2.5.1 in it I don't know if that is the problem or not.\nIts just a basic sql query like this \nSELECT * FROM a_table WHERE something = $1 AND something_else = $2\nAnd I'm just calling that from nw.js like so:\npgsql.findAppReleaseEnv(['First Thing', 'Second Thing'], function(err, rs){console.log(rs);})\nWhen I step through the debugger that first argument is false for instanceof almost right away, yet is true for both _.isArray and Array.isArray.  So I'm kind of baffled :)\n. I just ran your unit tests using io.js 2.5.1 and they pass:\n\u2713 executes productByName with multiple params and callback\nSo... I'm going to go with this is something specific to how I'm calling it that is broken.  You can close this issue if you want, I think I may be premature in thinking that its your fault :)\nYes, nw.js used to be called node-webkit. http://nwjs.io\n. Ok, I just figured out what the problem is.  Its specific to nw.js.\nBecause nw.js has different \"contexts\", the array I'm passing in is created in the \"window\" context (from the DevTools, also from the JS on the HTML page, which is the window context), but massive.js is running in Node's context, therefore instanceof will be different because the prototypes will be different arrays (the incoming param from Window's Array, and massive (and arg.js)'s from Node's Array.\nSo, feel free to close this is you want, this is basically an annoyingly subtle user error with nw.js :)\n. ",
    "vincentgiraud": "Yes, that did work!\nFor reference and update, this was taken from the doc at: \nhttps://massive-js.readthedocs.org/en/latest/document_queries/#deleting\nCheers and keep up the good work!\n. ",
    "pkwnz": "I am interested in this one too, because while this feature is useful, it seems like the original author of the module has big reservations about its use. So, it would be cool if we could choose to leave it out.\n. Very cool. I am looking at using this for two very cool upcoming projects... I really like the philosophy and approach.\nI have a couple of somewhat-related questions:\n1) Regarding \"We use node-pg [...]\", I understand this to be \"brianc/node-postgres\" - do we need to \"npm install pg\" ? and will I get the native bindings?\n2) can we access the connection (pool) object from node-postgres that massive is using to do queries on the same pool ... or is this necessary ... or what do you recommend to combine massive with accessing via node-pg directly (how do I :) ) ?\n3) Im slightly confused about \"and the search field to be called search \" - I am a bit unclear why we need the \"search\" field. Is this the body field redundantly stored but transformed into a ts_vector? As I mentioned I am not an expert on Postgres search... I guess I thought we only need the body, and then the index would transform the body into the right form\n4) I also see now the document-database stuff (creating a document from JSON, GIN indexing it automatically etc)- very cool. I was planning to do something very similar manually so that I could create not just an id column, but also columns for some FKs - in order to make joins with other tables efficient. Basically I want search within documents and FK joins to other documents. I am wondering if there is another way I should be looking at this - this might just be a roll-your-own situation, but any thoughts welcome :)\nSorry for the barrage of questions, I think I am done :) I really appreciate your help!\n. Thanks, great answers to my questions.\nI am very excited about massive.\nI would like to respectfully disagree on the issue of documents and joins though. I believe such a fusion is and will be possible, and will greatly simplify app design. Mongo and the like fall down, not just because they are immature, but because they fail to appreciate the usefulness of relational database system in simplifying the model of a system. While you can use DDD and the like, I actually think this is a distraction. Just my 2 cents though. Maybe I will build something extra on/for massive :-)\n. @dmfay Thanks for this, great to hear :)\n@robconery probably the wrong place to continue this conversation, but it's fun so let's do it anyway ;-) \nMy comment was about Mongo specifically and a few other systems - not all NoSQL systems. I don't mean to be overly negative about it - I'm sure Mongo works really well for some projects. But I would argue other NoSQL systems or very often modern relational-hybrid systems (Postgres, HANA any modern RDBMS with search and document support) would be a better bet for many teams and products. I absolutely agree that you can use the right techniques to work with document stores effectively without relational capabilities, and for analytics and search relational capabilities are of low importance. \nBut the point I was mostly trying to make is that I think that Postgres actually gives many teams something much more useful: the ability to leverage some of the document store facilities (like fast search) alongside useful relational capabilities for processing requests which is really cool :-)\nThanks again for Massive, and for your comments above.  I am considering doing what dmfay is doing.\n. ",
    "Jaeiya": "Ah, okay, I didn't know that. Thanks :smile: \n. @xivSolutions Thanks for the quick reply\nI have a chat application that stores stats for chat messages, such as how many messages per user. In order to load balance, a certain number of these messages are stored in a log file on the file system and then at a specific time, they are aggregated into the database.  I wanted to prevent a database write for every message sent in the system.\nSince the log file will contain messages from all users, when the parsing algorithm completes, it will return stats that need to be updated for those users and each user has their own stat table, hence I need to update multiple tables with different data.\nYour solution was to iterate over all the data one by one. I'm curious as to what kind of strain that might put on the database, which is why I was looking for an all-in-one update solution. I am very interested in your insight as to maybe a better method to do this or insight into my current method of load balancing. I don't really know best practices when it comes to writing to a database.. @xivSolutions - In the meantime I was looking at other database solutions (ORMs) that might support the kind of query I mentioned. I'm currently taking a look at Waterline but obviously, I would prefer for all the work I've done with Massive not to go to waste. I will make sure to get yours and robconery's thoughts before I make any big decisions.\nI look forward to your ideas on the matter and thank you very much.. @dmfay - I should clarify that it's one stats table where the id of a row is a foreign key to a user in the users table. I didn't mean to create the confusion that there are multiple tables being created. That being said, I do like the idea of not even having the stats table to begin with and just using a JSON object as one of the user fields for each users stats.\nThe issue still remains: how do I update multiple rows at the same time, of different users, without a massive amount of iterative writes to the database?. @dmfay - I can't imagine I'm the first person to need to update multiple users at the same time with different data. That being said, can you suggest a better way? Also can you point me to how to use this COPY FROM to put the log file in temporary storage, I've never seen that before.\nLastly, would it be better just to invoke the update on every message? I would give an example like twitch chat, where people are constantly chatting and all those messages need to be updating a count for each user.. @dmfay  - My ignorance is showing here... What is a \"materialized\" view?\nIn order to know that a user has sent X amount of messages, something needs to be stored. If I was storing a total message history, then yes I could use the \"on demand\" approach to filter out a single user, but I'm not storing history.\nCurrently, every time a user sends a message, it gets added to a log file on the file system. That log file is then later parsed and entered into the database to store the \"new\" stats about a users messages. That file is then cleared for the next batch of messages which are cached in memory at a specified interval. All of this was to reduce load on the database and reduce load on the file system. We're talking an update rate at about every 5 minutes.\nYou make a very good point about trying to solve a problem that I don't have right now. I do not have anywhere near the traffic necessary to know how many messages/s it would take to drop performance of the site with a database write per message. My goal is to future proof.\nAny more insights you have, I would be much obliged to hear as I am a total database newb.. @dmfay - You are really make my day today, excellent information, thank you so much.\nCoincidentally I am already using Redis for session storage. I didn't even think about using it for caching message logs. How would caching the messages in Redis transfer over to saving the stats per user in Postgres? I am indeed using primary keys to insert the stats.\nYou're absolutely correct about over-engineering. I think I've given this almost a little too much thought for my use case at the moment haha. I tend to do that quite often, which translates into very complex functions which, in the end, could have been reduced to a few lines of code had I realized the simplicity in the first place.. I see, so Redis would essentially contain a dynamic stat table and when a user sends a message, I would push the change to Redis (increment the amount) and then the result would be written to the Postgres stat table?. You said I would be able to skip the query for the original count by using Redis, which to me means that when a message is saved to Redis, it wouldn't be the raw message (like it is now) but just the stats of that message being saved.\nThat being said, it also means I would have to load the stats from Postgres for all online users into Redis to make sure the count stays accurate, if I want to skip that step. I guess I'm just not sure how you're proposing I can skip the step of querying for the \"current\" count of messages from a user, in order to add the new count.. Ohhhh, you meant a SQL script, there's where I got confused.\nHow would I do that with Massive? I've been trying to figure out how to execute raw SQL code through massive and I can't find it in the docs.. Thanks for that link, I guess I'm blind haha.\nI was able to create a function that updates the count with a single query after a bit of fiddling. The only caveat that I see now is the purpose of the cache. It existed originally to reduce database activity, but if I'm adding the messages to Postgres right after they're sent, I really don't see the purpose of the cache anymore, unless you're implying that I would continue with my previous implementation of waiting a matter of time and then iterating over all the logs, then saving them into Postgres. You eluded to this when you mentioned the \"time-to-live on insert\" with Redis.\nI hope I'm not taking too much of your time, but your advise is invaluable to me at the moment; I really appreciate it.. @vitaly-t Hmm... Well this changes my approach yet again. I'm very glad you commented on this issue :). @robconery Do you think this is functionality that might come to Massivejs in the future, seeing as pg-promise compensates for this so-called \"edge case\"?. @dmfay I agree, I don't have the throughput necessary to bottleneck the database with updates on every message. That being said, when I do, Massivejs will have the facilities to accommodate my need.. What do you mean by \"write twice\"? Do you mean writing something like: \nuser.update('foo'); \nuser.update('bar');\nrather than just updating once per tick, so-to-speak?. @robconery Define a \"separate transaction\". The code I posted would update the same user twice with two different calls. That, to me, means different transaction. But if you mean two parts of the application trying to update the same record, asynchronously, then yeah that's not going to happen.. Okay, so really I'm not at risk even if the same user sends 5 messages per second; it's not enough data to be considered \"bulk\".\nThank you all for taking the time to be so informative, I really appreciate it :). ",
    "jney": "sorry rob it was actually a question for confirmation if views (materialized or not) are supported. \ni've got a materialized view called stores and when i call it \ndb.stores.find({field : '0000-0000-0000-000'}, function(err,stores){});\ni wished it would be supported as tables are, but an error says, it can't call find on a null value\n. sure.\nthanks\n. ",
    "rcyrus": "I had the same question and looking though the code it looks like massivejs is using the 'correct' way to connect to the PG client which will create a connection pool on its own\nhowever @robconery you say you have disabled connection pooling? So are you saying for every postgres query massivejs is making a whole new connection? Or are you saying something else? \nI'd like to have a persistent connection and DB connection pools are a pretty standard thing. Can you point me or any other reader to whatever discourse you are referring to when you say you've 'been though the gauntlet?' I am curious to what is going on here. This seems like a cool library but if it can't do basic pooling then I can't use it. \n. Cool, thanks for the information! \n. ",
    "rdegges": "I'm back from traveling! Yey!\nSo -- just to make sure I'm understanding -- Massive does not (yet?) support pooling connections? Or am I missing something?\nThe way it should work, I believe, is like so:\n- You have Node running in a single process.\n- You configure Massive and say 'maintain 20 open Postgres connections' or something similar upon initialization.\n- Massive will open up 20 Postgres connections, which will handle all the SSL handshaking / etc. up front, reducing the need to do it on each query in the future.\n- As you execute queries with Massive, it will use one of the available 20 connections to send that query off to Postgres, and wait for a response.\n- While Node is waiting for a response, it will simply continue executing other instructions -- these other instructions may be other queries that are finished and can now be used via callbacks, or they may be simple instructions: add / subtract / create a variable / whatever.\n- If you've already got 20 queries in flight, waiting for Postgres to finish generating a response, and attempt to do ANOTHER query, then Massive will open a single additional NEW connection for that query alone, execute it, then close that additional connection once that query has been completed, while NOT affecting the other 20 connections in the database pool.\nThis is how I'd sort of expect it to work as a developer using this thing.\nThe idea would then be that I know how to linearly scale my application logic in a fast way:\n- Have 10 servers, each running 4 Node processes.\n- Each Node process opens 10 connections in a pool.\n- This means I have a total of 10 * 4 * 10 = 400 open DB connections at any given moment.\n- Assuming my application load is consistent and queries are identical, I'd have fairly consistent throughput without a lot of network overhead from opening / closing DB connections all the time.\nDoes the above stuff make sense? And am I understanding that this is NOT what massive currently does? Just a little confused by the conversation above =)\n. Hmm. I don't think this makes sense -- I'm not an expert w/ Node by any means, but you can most definitely have asynchronous IO in the event loop, correct? This means that you can fire off multiple queries one-after-another, and have them all get processed by Postgres while your Node process is doing other things, until the response is retrieved and then the event loop hops back to synchronously execute your callback code, right?\nSo, let's say I write some code like this:\n``` javascript\nvar pg = require('pg');\nvar conString = \"postgres://username:password@localhost/database\";\n//this initializes a connection pool\n//it will keep idle connections open for a (configurable) 30 seconds\n//and set a limit of 20 (also configurable)\npg.connect(conString, function(err, client, done) {\n  if(err) {\n    return console.error('error fetching client from pool', err);\n  }\n  // here we'll execute a fast query\n  client.query('SELECT $1::int AS number', ['1'], function handler1(err, result) {\n    //call done() to release the client back to the pool\n    done();\nif(err) {\n  return console.error('error running query', err);\n}\nconsole.log(result.rows[0].number);\n//output: 1\n\n});\n  client.query('SELECT * from some_enormous_table_with_a_zillion_rows', ['1'], function handler2(err, result) {\n    // do stuff with the data\n    done();\n  });\n});\n```\nThis is based on some code from the node-pg library docs.\nMy understanding is that:\n- Node will execute the first client.query call, then will immediately execute the next client.query call.\n- Since the first query is a simple query that postgres should process quickly, it will likely return first, and then Node will run the first callback handler: handler.\n- Then, once Postgres has finished executing the second query, Node will then execute the function handler2.\nBecause of the above, it means that at some point in time, BOTH of those queries will be processed by Postgres at the same time -- even though they were executed one-at-a-time by my single threaded Node process.\nIs that correct?\n. Will do! Thanks for the info. This is super helpful, heh.\nI appreciate you taking the time to reply ^^\n. ",
    "tdzienniak": "@robconery you are not exactly right or I misunderstood you. Node can't do things in parallel, but libuv, which is C++ lib to perform asynchronous I/O used by Node, can, for example using its thread pool. So, I think it is possible to send few queries one after another to Postgres, then db can execute them how it likes (maybe all at the same time), and then process results in Node. Off course, results will be processed one at a time. Interesting article 'bout thread pool: https://www.future-processing.pl/blog/on-problems-with-threads-in-node-js/ \n. @timruffles that's exactly what I was writing about. Semantics are the problem here indeed.\n. ",
    "timruffles": "@robconery Like so often, I think there's a confusion about semantics here. Nobody is claiming Node can run bits of JS simultaneously (parallel), but it can certainly have multiple outstanding callbacks queued (concurrent), and they'll be dealt with as they come in.\nWhile we can't do parallel processing of queries (e.g munging them with JS using > 1 CPU in one process), but it's certainly possible and desired to make concurrent queries (e.g multiple outstanding queries being sent to be handled by the DB).\n``` javascript\nvar db = require(\"./knex\");\ndb.raw(\"select 1 as qid, NOW() as started_at, pg_sleep(5)\")\n.then(format)\ndb.raw(\"select 2 as qid, NOW() as started_at, pg_sleep(2)\")\n.then(format)\ndb.raw(\"select 3 as qid, NOW() as started_at, pg_sleep(3)\")\n.then(format)\nfunction format(resp) {\n  const r = resp.rows[0];\n  console.log(query ${r.qid} started at ${r.started_at}, returned at ${new Date});\n}\n```\nsh\nquery 2 started at Thu Mar 03 2016 14:00:18 GMT+0000 (GMT), returned at Thu Mar 03 2016 14:00:20 GMT+0000 (GMT)\nquery 3 started at Thu Mar 03 2016 14:00:18 GMT+0000 (GMT), returned at Thu Mar 03 2016 14:00:21 GMT+0000 (GMT)\nquery 1 started at Thu Mar 03 2016 14:00:18 GMT+0000 (GMT), returned at Thu Mar 03 2016 14:00:23 GMT+0000 (GMT)\n. @robconery what issues did you hit with connection pooling? It's been perfectly reliable for me across multiple deploys, and is always important for perf\n. ",
    "tamlyn": "For anyone stumbling across this issue, massive inherits pg's default connection pool size which is 10. If you need to change it, just pass poolSize as an option:\nconst massive = require('massive')\nconst db = massive({poolSize: 20}). Thanks both. So just to recap: this behaviour is actually a feature of Postgres because\n\nINSERT INTO bigints (one) VALUES ('123');         -- works\nINSERT INTO bigints (many) VALUES ('{123}');      -- works\nINSERT INTO bigints (many) VALUES (ARRAY['123']); -- fails\n\n@vitaly-t I'm taking the fourth option which is to change the column type to TEXT[]! They're Twitter IDs which are too big for JavaScript numbers so need to be loaded as text anyway.\n. I see that Massive supports casting in criteria objects. Any thoughts of supporting explicit casts on insert? I imagine another option to insert() something like {cast: {many: 'bigint[]'}} (for the above example). That would enable Vitaly's second option above without custom SQL.. Docs updated.. Not at all! It's a joy to contribute to a project with such a helpful and responsive maintainer.. Yes, that's what I'm doing. \nI would say it's better to introspect the search path from the DB rather than have a config option for it. It's an extra request but should be super quick. Agree?. Hit some issues writing a test for this:\n- SET search_path=... only affects the current connection so would need to set poolSize to 1 for the test.\n- ALTER ROLE postgres SET search_path=... only affects connections created after the query is run so would need to close then reopen the connection in the test.\nThoughts?\nSee also https://github.com/brianc/node-postgres/issues/850. Ah, I hadn't realised there were performance implications to using proxies. Is that the only issue you foresee with that approach? It seems likely that the performance penalty would be minimal when compared to the speed of the database query as a whole. \nAccording to this post about proxy performance, a single proxy invocation is ~0.18\u00b5s slower than a vanilla function invocation. A 180ms database query is 1,000,000 times slower than the performance you just lost to using a proxy. I'm willing to give that up.\nAnd if we're on the subject of performance, what about the cost of awaiting the connection each time? That means creating a promise and queueing up a new microtask each time. I'd put money on that taking much more than 0.18\u03bcs.\nOf course even if you did want to go down this route, it might have all kinds of other pitfalls! I'd rather find a solution that works now. Do you have any examples of (open source) apps using massive that I could look at for inspiration on how to structure the database access logic? . The example in the docs assumes that every function that accesses the db has a reference to the app. In a larger application this would mean a lot of argument passing. With node-pg I'm used to requiring the database connection only in the files that need it. \nI'm sure I'm missing something, but you say that this issue has been raised by others previously so I'm not the only one. Perhaps, the solution is a more complete example rather than proxies or anything else.. :facepalm: So simple! \nYeah it would be great to add that to the docs as a recommended pattern.. Just added spaces to align columns.. IDE was complaining about missing param name.. ",
    "n321203": "Yup, I agree. It's actually only ~70 lines of code so another option is to implement something similar directly into massive (and give credit to the author...).\n(Update: link is now working)\n. Hmm... sounds like it is impossible without breaking backwards compatibility (if we don't look at the objects and assume the first found object containing the key \"stream\" is the options object... but that's ugly!) \n. I would probably change the arguments/signature to: \n- If one argument is passed, assume it is the parameters,\n- If two arguments is passed, require the first to be the parameters, and the second one to be the options. \nThat way, the api will change only for people using streams today without any parameters (passing the options object as first argument)\n. > The problem is that the catch block would need to release/rollback or else we end up with a pending transaction stuck on a connection that's put back into the pool, which would be a roving chaos monkey\nYup, there is a risk that Joe Coder never call rollback() or commit(). The transaction will then stay open (in a zombie state as you called it :). But it is unavailable for others, so we'll avoid the chaos monkey :)\nAfter pg.defaults.poolIdleTimeout it will timeout (reverting any changes) and be destroyed by pg. \nPg uses a default pool size of 10, so at least Joe Coder has a few more connections to burn through...\n\nit's just simpler to add \"BEGIN\" and \"COMMIT\" to your query chain by hand - or better yet to just use a SQL file\n\nYes, a SQL file is probably the best way in many cases. My use case is that I'd like to be able to quickly update multiple tables atomically without having to write a separate sql file. I don't understand what you mean with the query chain \u2013 surely writing\ndb.query(\"BEGIN\", ...)\ndb.table.save( ... )\ndb.query(\"COMMIT\")\nisn't a good idea? The connection is returned to the pool after each query.\n@dmfay: Unless the runner is implemented as a singleton, this shouldn't be a problem? Or maybe I misunderstand you?\n. Doh!:stuck_out_tongue_closed_eyes: Then this approach won't work. Closing it. \n. ",
    "oorst": "I also think the named parameters thing would be pretty cool.  I had a reason to use overloaded SQL functions and named parameters would help that.  At the moment, there doesn't seem to be support for overloaded SQL functions.\n. ",
    "scrudge": "Thanks, I was using peer authentication, so I didn't think it was needed.  I switched to password auth and added username and password to the connection string.  I still get the exact same error.\n. I'm running Debian Jessie, Postgres version:\n```\nphoenix=> SELECT version();\n                                            version                                            \n\nPostgreSQL 9.4.6 on x86_64-unknown-linux-gnu, compiled by gcc (Debian 4.9.2-10) 4.9.2, 64-bit\n```\nI tried going async like : \n```\nvar massive = require(\"massive\")\nvar connectionString = \"postgres:/username:password@lowen-desktop/phoenix\"\nmassive.connect({connectionString: connectionString}, function(err, db) {\n  console.log(err)\n  app.set('db', db)\n});\n```\nBut the err object is:\n{ [Error: connect ECONNREFUSED]\n  code: 'ECONNREFUSED',\n  errno: 'ECONNREFUSED',\n  syscall: 'connect' }\nStrange, is can connect with 'psql -d phoenix'\n. BTW, nothing showing in the postgres log.  Maybe I should specify IP and Port number in the connection string?\n. OK, I think it's working now, using connectSync.  The trick was to add the port number, I'm guessing that Debian uses a non standard port, 5433?\nOne more express question if you don't mind.  I'm using app.set like the example.  How to I access \"app\" in a module, I'm getting \"app not defined\"\nThanks!\n. Ok, thanks.\n. ",
    "gjuchault": "Okay, thank you :)\n. ",
    "jonathana": "Regarding the meta property, I think I can finally elucidate a clear reason why I think it should be there.  Right now, a bunch of methods get added to the massive object, especially with a subdirectory tree of queries, on nested properties, and there is no way to determine from the massive object, today, where those properties came from/why.  So if I have massive.subPropA.subPropB.someMethod(), you can't tie it back to the Evaluator that put it on there.  Even worse, you can have massive.subPropA.someMethod() and you can't tie each back to their own Evaluator, and the Evaluators themselves can't differentiate except that the function body is different.\nThis makes introspection of the resulting massive object difficult, especially if what one wants to do is to either extend or \"wrap\" certain functionality.  In my case, I'm trying to build a library that uses massive, among other things, to generate JSON data and highlandjs to run a pipeline of transforms on it.  I want to wrap the streaming version of the massive methods that get generated to return a highland stream, but there is no way today to introspect a massive instance with certainty (due to duplicate names) and figure out which methods on it were created by e.g. loading up sql files and turning them into methods on the massive object, because the methods themselves don't indicate why they are on the massive object, and the Evaluator instances don't contain the path information for the methods that got added if you introspect them.\nI haven't started the above work of changing around the Evaluator to include this metadata yet, but wanted to explain what I saw in the design.\nI think I may know what the issue is with using glob.  The function as constituted walks from the top down, so creating the new methods on the massive object is simplified because when you see a new directory you can create the outer {} to hold the nested functions first.  glob doesn't give you that guarantee, so you have to make sure the hierarchy you need exists when you get a nested function \"out of order\".\nIn doing my changes with glob, there are at least 5 ways to work around this:\n1. Don't work around it.  Just refactor the existing code using the walk it does today and don't use glob.  I'm pretty sure I can work with that\n2. Easy but impactful is to switch from underscore to lodash for its .get() and .set() methods that support the nested properties for you.  I'm guessing, however, that that's a library switch massive won't want to make but if it's ok, this is literally a drop-in replacement\n3. Easy but requires adding lodash as a dependency anyway: add lodash to the package's dependencies, but only require the 2 functions out of it that are actually needed\n4. Pick one of the multiple npm libraries that does the nested object work that would be needed, add that to the dependencies, and require that to do the work.  https://www.npmjs.com/package/nested-objects is an example, suggested only because I know the \"author\" (he pulled some stuff out of backbone-deep-model to create it) and talked to him about the state of the code, which makes me more comfortable with it anyway\n5. Add get() and set() functions or methods somewhere in massive to do the work, either by writing from scratch or copying existing, tested code with a compatible license\nI already have a personal git branch with choice 2 done and tests passing.  I'll work on choice 1 now.\n. @robconery Thanks for the input, and agreed on the 2 issues.  Before I go off and add more issues, would it be worthwhile to see if we can determine whether either one is worth doing?\nI'll cover the metadata thing in this comment, and deal with glob in a second.\nI'm not sure about the \"restructuring the project to fit your project's needs\" bit.  Yes, the change happens to suit something I want to do, but it also cleans up what I think might be a concern about massive as it's structured now.  Today, there are Evaluator objects attached to self.queryFiles, and there are (potentially nested, if you have subdirectories) methods on the massive instance.  However, there is no way, especially if people don't give each SQL file unique names, to identify which method added to the massive instance came from where.  My proposal to extend the Executable with a propertyPath was strictly because while it can be intuited by looking at this.scriptsDir and removing that and everything before it as well as the .sql extension from the Executable.filePath to find what property on the massive instance goes with that Executable.\nIf being able to introspect the massive instance to determine where the various properties came from isn't a valuable enough use case for massive, then I'm good with that.  I can accomplish the same thing outside of it for my own use cases as described above.\n. As for the glob thing, I'm willing to give up on that as a proposal.  Using glob might result in less code, but at the cost of adding glob, and at a minimum lodash.get and lodash.set as dependencies (lodash lets you npm install individual methods, so I could get the footprint way down in that regard).  However, in looking at it, it would also allow directory names with dots in them, which will get really screwy because someone could create a directory structure like:\n-subA\n    -subB\n        -runSomeQuery.sql\n-subA.subB\n    -runSomeQuery.sql\nand now there's a name collision, although that could be handled in error handling, but then it puts more code back in.\nSo I'll drop the offer to refactor to glob, even with the code as it stands now, because I'm not sure glob would be shorter or more reliable.\nI can implement the metadata stuff inside the existing code if (and only if) that is something you deem worthwhile.  And if you don't want that, then I'll try and work around it on the outside.\n. Agreed on the glob discussion.\nfile sql is just a symptom of what I'm trying to get at, so I don't want to dwell on that in particular.  I didn't consider the quip about restructuring as too direct, and I don't want to come off as too direct in anything either.\nWhat I'm trying to know here is: when I look at methods on a massive instance, nested or not, are they something that got added that's doing database access that I might want to wrap?\nI'm pretty sure without asking for anything new that I can do that for Table and View stuff without needing anything, although the below might be handy.  The problem really came with the adding of SQL files, especially because they can nest and names will only be unique within a particular nested object.\nThe simple proposal I'm left with suggesting here regarding metadata is: given an instance of an Executable, let it either specify as a property on itself where on the massive instance the function to execute it got loaded, so that when the code does this to stick a new method on the massive instance from e.g. a SQL file:\nrootObject[name] = function () {\n        return _exec.invoke.apply(_exec, arguments);\n      };\nI can go back and say \"when I find a method on a massive instance, it was put there through the above code?\".  I did not see anything obvious when looking at any functions on the massive instance to say \"aha, that's doing database access\" to work around it.\nAs I said in a prior comment, I can do it today by interrogating massive.scriptsDir and then massive.queryFiles to get the Executable's and then look at their filePath, however that then creates some implicit dependencies that could break down the road: if massive ever looks for anything other than .sql files, I have to know when that changes.  It would be more reliable if massive gave me a way to get those answers.  Again, whether that's by having a function that looks at Executable.filePath and self.scriptsDir, strips extensions and converts slashes to dots, or whether it just builds the value onto Executable when it's created doesn't matter to me.\n. ",
    "diegoaguilar": "@alexishevia did u achieve it?\n. ",
    "aray12": "@dmfay thanks for the response. The advantage of this would be the fact that number of arguments would be dynamic, but your second recommendation is exactly what I was looking for. Just curious, why would you consider this \"only-slightly-hacky\"? Looking at the documentation I can't seem to determine any serious differences in behavior between ANY and IN.\n. @dmfay understood. Again thanks for the quick response. \n. @robconery What I would expect is being able to pass a function in the configuration object of massive.connect. Some like this: \njavascript\nmassive.connectSync({\n  connectionString: 'postgres://postgres:test@localhost/test-db',\n  scripts: path.resolve(__dirname, 'scripts'),\n  logger: console.log,\n});\nFrom there it seems like you could capture all queries in either DB.prototype.query and DB.prototype.stream in runner.js. The main issue is that only argument passed to the DB constructor is the connectionString. \nI see 3 potential solutions. \n1. Change the expected parameter in DB to an object that options contains a logger property\n2. Add an addition parameter called logger\n3. Pass the logger as the second parameter and extract it from arguments (fairly brittle because code doesn't inherently document order even if it does matter)\nI would recommend 1 or 2. I think 1 would allow for better extensibility down the line, but 2 would be quicker to implement and require little refactoring (just additional functionality added)\nI don't mind submitting a PR, but, given that I have little experience contributing to OS, I'd prefer to get your feedback on intended implementation/potential obstacles before.\n. In my experience the options for node are winston and bunyan. Both are fairly stable packages that support leveled logging like what you want (although both have stagnated in development in my opinion)\nIn my experience, I prefer a simple function. The reason is I can then tie your logging into which ever logging package I prefer. \n. Bunyan uses the concept of streams which are essentially the same where you can set a threshold and pipe to any writable stream (stdout, stderr, file or external service). Either way I think the ability to log generated SQL would be helpful. I am less concerned with logging errors because I catch those with my standard error handling\n. @robconery It was provided above\njavascript\ndb.test(['this is a test'], (err, res) => err ? console.log(err) : console.log(res));\nI could see how my use of test might have been confusing so I edited the example.\n. So this is the result of me exploring the ability to push some functionality to the DB, while still being able to organize most business logic in a single place. This is just a test case. The plan is to pass a variable to the procedure which declares its initial value. Subsequently there would be a loop DB side that would be incrementing (by one day or one minute, so not a simple integer increment) that declared variable. Obviously this specific example could be implemented without any procedural language stuff at all. Just a POC for now. \ni.e. the ability to declare variables at the DB level is the functionality I am really interested in/testing out. If this isn't possible completely understandable. I will just need to declare functions at the DB level, which Massive provides direct access to anyway <3\nBTW: @robconery and @dmfay -- you guys have been really helpful and responsive with respect to stuff I have submitted. I appreciate it and love the lib. The ability to write raw SQL file in my application just feels so right.\n. @xivSolutions thanks for the response. Your suggestion feels a bit too hacky for me, so I decided to just generate functions DB side that I can call seamlessly from massive-js anyway. \n. I probably should have done a quick search... I'm a fan personally but I will continue to promisify for now\n. When you talk about going all-in with es6 are you thinking of adding a transpiling step to pre-publish? Or based on node v6 support?\n. ",
    "cymen": "Excellent -- thanks, I'll look into timestamptz too :).\n. ",
    "cjnqt": "Closing, doesn't work with all types of commands. Had misunderstood how options are passed to the query runner\n. Both this issue and #206 could be implemented quite easily if we had a way to pass options to the query runner. \nAs @dmfay mentioned in #206, \"this is hard since because of how Executable.invoke() sorts out its arguments. You can pass up to three args: parameters (primitive or array), options (object; currently just used to toggle results streaming), and callback. Since both parameters and options are optional, it checks the types of what you pass it and assumes an object is the options and anything else is the parameters.\nIn v 3.0, I think having a way to pass options to the query runner would make sense.\n. Never mind the many commits \u2013 it's only the added link that is changed (commit #3), the others were reverted.\n. Great to hear you're moving to promises, async/await is awesome\n. ",
    "dmitryame": "Actually, my bad,\nI did narrow down the issue to the json object that was being saved -- it had undefined values.\nStill, I would expect the library not to fail with abort 6 and crash the server, but insted have some meaningful message in the log and keep going? \n. I'm still learning all this stuff, so not sure what this means. Are you saying that sync does not deal with the errors as well as async?\nIf that's the case, I'll try to switch to async.\n. OK then,\nI do not mind switching, and I'm typically a fast learner, through, with node I find it's a bit overwhelming, with all the callbacks, promisses, generator functions, KOA, express etc.... ?\nIs there a decent documentation which explains how to use massive the \"right way\"? \n. Just tried switching to callback, here is error I'm getting inside the callback function:\n```\n{ [Error: column \"load\" of relation \"checkins\" does not exist]\n  message: 'column \"load\" of relation \"checkins\" does not exist',\n  code: '42703',\n  detail: undefined,\n  length: 121,\n  severity: 'ERROR',\n  hint: undefined,\n  position: '115',\n  internalPosition: undefined,\n  internalQuery: undefined,\n  where: undefined,\n  schema: undefined,\n  table: undefined,\n  column: undefined,\n  dataType: undefined,\n  constraint: undefined,\n  file: 'analyze.c',\n  line: '2163',\n  routine: 'transformUpdateTargetList' }\n```\nI really believe this has nothing to do with the synch stuff since I was getting the same exact error.\n. BTW, I do not have column name \"load\" in any of my tables\n. Hey, thank you for your help -- I really will try the asycnh stuff -- it's very cool and I'm all for benefitting from doing it the asynch way.\n. I can share the code with you privately, hangon.\n. models/checkin.js:59\n. I was using slightly different pattern, but I think it should work too.\nInstead of passing the object into the method, the method accesses the properties of the object it's called on via this. So, first set the properties of the object, then call method on it which will have access to all the previously set properties. That's why I'm calling db.checkins.save(this,.... inside the checkin.save() method. \nI will try to follow your suggestion and pass the object explicitly, though, not sure if it's gonna help -- will let you know shortly.\n. btw, it was all working fine in 2.1.0, but in 2.3.0 it shows this misterious error:\ncolumn \"load\" of relation \"checkins\" does not exist\nI do realize that massive is not an OR tool, and I acutlaly love it's simplicity, elegancy and power (but mostly simplicity). Though, It's really strange where does the \"load\" column pops from, specially that it was working before the upgrade (and no code was changed on my end).\n. this fixed it, thank you!\n. just for kicks, any eta on when the next version supporting ES2015 will be out? Does not have to be precise, are we talking few days, weeks, months? \n. cool, thanks\n. So, I'm trying to figure out how to correctly thunkify massive functions:\n```\nSubcontractor.prototype.load = function () {\n  var findOne = thunkify(db.subcontractors.findOne);\nco(function *() {\n    try {\n        var res = yield findOne({id:this.id});\n    } catch (err) {\n        console.log(err);\n      }\n      console.log(res);\n    })();\n}\n```\nThe code fails with the following error (this is printed from inside the catch block):\n[TypeError: this.find is not a function]\nI know, I'm doing doing anything meaningful with the results yet, I just want to print the res out for now.\nAny hints what am I doing wrong?\n. btw, not sure if this question will get noticed here since the issue is closed already, so I just moved this question to http://stackoverflow.com/questions/37669469/how-to-correctly-thunkify-massive-js\n. Not really, I'm not bolting Massive functions to my objects\nvar findOne = thunkify(db.subcontractors.findOne);\nand then explicitly passing just an id value into the findOne of db.subcontractors:\nvar res = yield findOne({id:this.id});\n. I would love to do the right thing and use callbacks, however, I'm using KOA with generators and trying to figure out how to use massive with KOA, and all suggestions that I come across on the web lean towards thunkify with co. I'm open to doing it differently, if someone can share a code sample how to use massive with KOA.\n. ",
    "bmoelk": "I'm getting this too. It's happening when I am inserting a row in a rest callback. Happens using either restler or unirest.\n. A bit more information: \nOSX Yosemite/xcode 7.2.1\nUsing node v5.9.0 or v5.8.0 makes no difference.\nmassive@2.1.0\nScenario:\nIn a redis queue response from bull@0.7.2\nMake a rest call via unirest@0.4.2\nIn the handling of the response, insert a row into a table\nUsing pg@4.5.1 directly works fine. It doesn't matter if I use massive's .insert or \"SQL Files as functions\" approach.\nI can try to isolate the problem when I have a bit more time next week.\n. no.\n. Actually I am connecting to the DB via connectSync. Everything else is async.\n. ",
    "kristojorg": "I am seeing this currently. Was using connectSync so I could export the resulting object... do you have a recommended way to use connect instead of connectSync and pass the object around the app?. Thanks @dmfay , I think I've solved it for use in zeit/micro and others without an app.set. \nFor anybody else running in to the, I ended up wrapping connect in a promise and exporting the promise:\n```js\n// db\nconst promise = new Promise((resolve, reject) => {\n  massive.connect(\n    {\n      connectionString: process.env.DATABASE_URL,\n    },\n    (err, db) => {\n      if (err) return reject(err);\n  // wrap functions in promises\n  if (db.users) promisifyAll(db.users);\n  if (db.run) promisifyAll(db.run);\n  return resolve(db);\n}\n\n);\n});\nconst connect = () => promise;\nexport default connect;\nand then when I'm using the db object:javascript\nexport const getUser = async id => {\n  const db = await connect();\n  try {\n    const user = await db.users.find(id);\n    return {};\n  } catch (e) {\n    throw e;\n  }\n};\n```\nnote: You can see I also wrapped the table functions in bluebirds Promise.promisify to make them a little nicer in a promise-based app. . ",
    "subblue": "Just to answer my own immediate question I realised it is probably easier to create the SQL directly and then use the db.run(sql, callback) method:\nsql\nSELECT id, body->>'title' AS title, body->>'updatedAt' AS updated_at\nFROM projects\nORDER BY body->>'updatedAt' DESC\nLIMIT 10\n...still getting used to the new JSON capabilities in Postgres ;)\n. Sorry there was a typo above, my actual code did have this:\njavascript\ndb.projects.find({}, {order: 'body->>updatedAt', limit: 10}, callback);\nwhich was giving the error column \"updatedat\" does not exist.\nLater I will probably end up adding separate columns for the fields that I need in the listing without having to inspect the entire document body for each. For the moment though while things are in flux it is nice not to have to add additional fields to the table until I know they are definitely needed.\n. @robconery thanks for the tips!\nI'm really liking what I've seen in Massive so far :)\n. ",
    "aabenoja": "Would it be better to check if the last argument is a callback function instead of using an asCallback function? The internals use native (to node 4+) promises, only the callback is ~~magically~~ conveniently invoked if passed.\n``` js\ndb.users.find(1)\n  .then(doThingWithuser, reportErr)\n// find could be returning a promise for all anyone cares\ndb.users.find(1, (err, user) => {\n  if (err) { return reportErr(err); }\n  doThingWithUser(user);\n});\n```\n. Seems like I spoke just a bit too soon. It looks great. I can't wait to make use of this in my apps. Do you have any plans on putting this in a pre-release when it's ready?\n. Can we get a release of this, please? It's pretty disorienting that the documentation for the new functions are there but aren't available on the latest release.\n. ",
    "Sinewyk": "I don't know what you mean by mocha is not es6 compatible but yeah, you just have to be careful about not using arrow functions for mocha's after, before, afterEach, beforeEach, etc.\nOtherwise because this is passed around and it's used for hooks, you end up attaching hooks on every single tests \njavascript\n// mocha create a describe block wrapping all your tests\n// so the this here\ndescribe('', () => {\n  // is the same as here because of arrow functions\n  beforeEach(() => {\n    // and so this hook is going to be executed on every tests, of every files\n    // not only on the describe of this file like you probably intended to\n  });\n});\nUse arrow function in your tests, just not for mocha functions. Jscodeshift transform available if you want.\nPS: sorry for jumping in like this I guess ...\n. According to the doc it's https://mochajs.org/#arrow-functions \"discouraged\", whereas there should be a big warning sign with sirens telling you to absolutely not do it at the beginning of the docs.\nMocha can handle ES6 just fine. Just no arrow functions as arguments to mocha functions describe, it, before, after, beforeEach, afterEach and context and you're good to go.\nAlso, because we are monkeys, if you want to take a look at transpiling, juste take a look at how https://github.com/reactjs/redux does it. It's quite complex but handle all the cases. You can skip the browser stuff but the lodash optimization are pretty interesting.\n. I've actually read all the issues. I don't understand the problem. If author does not like Promises and is still nodejs callback compliant on all his asynchronous methods ...\nWhy are issues spawning all over the place when there's already a solution with bluebird promisification + co or bluebird coroutine and stuff for anyone wanting more generator, or async/await over callback or insert latest trend\nI see callback as the lowest primitive possible so that userland can abstract it away as they wish, why the need for Promise directly in core ? It feels like reading the Promise in nodejs core debat all over again.\n@robconery does moving to Promise in the v3 actually improve anything towards improving the library or is it somewhat peer pressure for zero gain ?\nOn a more perf related note: database code should always be as fast as possible, and you can't get faster than callbacks right now. Wrap it in bluebird, and you get the best of both world anyway as right now V8 promises sucks.\nTL;DR: :-1: on Promises in core if library code doesn't actually get better/faster/more maintainable as a direct result.\n. ",
    "purepear": "Promises will make async/await use possible.\n. ",
    "johanneslumpe": "@robconery any updates on this?\n. ",
    "thejones": "As @purepear mentioned Promises will make Async/Await possible and that should be moving forward as of a couple days ago for ES7/ES2017/ES? \nhttps://github.com/tc39/proposals/commit/e8c03544751b071b973064ef7d437a34ac6752cd\n@robconery this guy jumped 25,000ft without a parachute so I say we jump! \nhttp://abcnews.go.com/Entertainment/extreme-skydiver-luke-aikins-prepares-jump-25000-feet/story?id=40986613\n. Rock on!!\n. I have had some strange issues with DataGrip not committing transactions. I am not saying this is the issue but I would say that it is worth checking all of the settings and verifying that DataGrip is not being finicky. In my case, I had issues with a view and ended up needing to drop, close the connection, and recreate the view. This was not involving Massive, just random DataGrip issues. \n. ",
    "igl": "Pretty much just waiting for massive to be Promise or Observable based so i can move away from knex.raw(). \nBoth the v3 and bluebird branch didn't get commits since Mar/Apr \ud83d\ude22 \nI am curious about the state of massive and if those branches need any help.\n. ",
    "mischkl": "+1 for Promises! \ud83d\udc4d \n. ",
    "retorquere": "What branch is this feature being worked on? I'd guess promises as that had a commit 7 days ago, but there's also v3 (last commit on Mar 29, 2016) and bluebird (last commit on Apr 3, 2016). Is there an overview of the state of the promises implementation?. I am starting a proof of concept (towards a production app) which will use the promises branch (installed using npm install --save 'git+https://github.com/dmfay/massive-js.git#promises' should anyone wonder). So far everything from Full JSONB Document Support from the README just works as is, but instead of callbacks I use promises (or to be specific, I use yield in a bluebird coroutine-wrapped generator).\nThe one thing that did initially fail was yield db.createDocumentTable('my_table') when the corresponding table existed, because that does a create table under water, not a create table if not exists. I'm fine with that, but it'd be useful to have a way to test for the presence of tables so I can avoid the error. What I have now is this:\n```\nfunction hasTable(db, name) {\n  var [schema, tablename] = name.split('.');\n  if (!tablename) {\n    tablename = name;\n    schema = 'public';\n  }\n  return db.tables.findIndex((table) => (table.schema == schema && table.name == tablename)) >= 0\n}\nvar conn = massive(connectionString);\n....\nco(function*() {\n  var db = yield conn;\n  if (!hasTable(db, 'my_table')) {\n    yield db.createDocumentTable('my_table')\n  }\n})\n``. Any reason BTW to not just use bluebirdspromisifyAll`?. OK, so that'd become\nfunction hasTable(db, name) {\n  var [schema, tablename] = name.split('.');\n  if (!tablename) {\n    tablename = name;\n    schema = 'public';\n  }\n  return db.tables.some((table) => (table.schema == schema && table.name == tablename))\n}. No sorry, I meant was there a specific reason to rewrite v2 for promises rather than keeping v2 mostly as-is and calling promisifyAll at strategic points.. I'm mostly done with the README, but I'm not sure what to do about the streaming samples.. 1. I changed the name because I figured it's an companion method to createDocumentTable\n2. Oops.\n3. Oops again.\n4. I have made an attempt but I'm not familiar with this test setup.. I'm very sure the test I amended is naive BTW.. I'm not familiar with mocha. What should I do to run the tests locally?. OK, separate tests, running green.. Should I have used this\nif (splits.length > 1) {\n    schemaName = splits[0];\n    tableName = splits[1];\n    potentialTable = this[schemaName][tableName];\n  } else {\n    potentialTable = this[tableName];\n  }\nrather than the scan through tables?. But then perhaps hasTable should be getTable and use find rather than some. Then saveDoc could also use it to call potentialTable.saveDoc(doc).. No problem. Do you have any clues on how I can get my checkout synced to your github repo? I need to incorporate the merge commit. This is one of the things with git which always eludes me.. I've gone for yield over .then. The problem for my specific case is that I'll be using the database connection in more than one file, and I don't want to connect to the database separately in all of them. A resolved promise yields very quickly (certainly under bluebird). But I wouldn't want to argue that this needs to be a universal preference.. I will have one module which sets up the database connection and exports the promise; other modules can then import that module and take it from there. I wouldn't know how to set up that kind of system otherwise. Something like\nmassive(...).then(db => { app.set('db', db); ... })\nmay have other parts of the express app accessing app.get('db') before the promise was resolved. By exporting the promise I get predictable behavior.. Ah, I see now. Hmm. I'll keep that in mind, but I also have non-express files (such as cronjob tasks) that include the db setup module.. Fair enough.. Frick, something went wrong. The previous merge wasn't in this.. OK, got it.. Fair enough. I wasn't looking for massive to do validation though, I was looking for massive to allow validation (plugging in corro or validate.js should you so want). I can get around it with hooker.. I take it then you also wouldn't be interested in allowing findDoc to return an object with methods from a prototype applied? I can do that with hooker too.. Beat you to it :). ",
    "RayLLiu": "myapp.zip\nThe attached file is the source code.\n. Yay! It works! Thank you very much!\nRay\n. ",
    "tkopets": "Thanks @robconery and @xivSolutions for guiding me and pointing to test examples.\nI hope those tests look good, but I would be more than happy to fix/imporve/add more if needed.\nYou do a great job, guys! I absolutely love Massive for it's simplicity and elegance.\n. @robconery Those tests look good to you? \nIs there anything else I can do?\n. ",
    "habermeier": "The query did not fail.  It just doesn't look at options.  I set a few breakpoints, and the options bubble on down but are not used in the later stages (from what I can tell).  To get a feel for it, I looked at how prototype.find is implemented, and there seems to be specific code there to handle the options, but that code is doesn't get called when you go through findDoc, so the options just sit there ... and get ignored.\nI'd say it'd be lovely if we could pass in options, and when it comes to order clauses inside the jsonb, I'd just document how to do it.  From a user perspective, if you show how you can do those things, I'm sure that would help a lot of others as well.\nI'm just converting stuff over from dynamoDB (which is a terribly stunted document store, I might add)... and just happened to find postgres can now do jsonb, which is nice because I don't want to managing schemas for the support app I have to develop.  Long story short: it'd be nice to have some options support at minimum: (order by id, limit, and offset).  To sort inside the jsonb itself, maybe you just document how one would do that with some examples (like you just gave), and users have a way of dropping that in should they need to.\nStill beats having to \"roll your own all the way\"... \n. @robconery thoughts on this change?\n. ",
    "ludvigsen": "@dmfay that makes total sense, i'll try to refactor this to be more like findDoc.\n. @robconery @dmfay Thanks for the feedback! \nTried to refactor this to use options, also added some tests, let me know if I'm on the right track, and if there is anything i should change.\n. @dmfay I changed it so order is not altered any more, I might revisit and create a separate PR for some more robust handling of it. I see that my implementation was a bit naive \ud83d\ude04.\n. @dmfay you are right that if was always true, good catch! I've fixed it now. Anything else you see I should fix?\n. @dmfay I implemented your suggestions. I ended up extending the forTable function to take a prefix, my initial take was to add a forSearch function, but that ended up duplicating a lot of code. So I landed on this approach.\n. Hmm, I get the same output when i try this:\ndb.my_test([1], (err, res)=>{                                                                                                                                                                                                       \n  // Do something                                                                                                                                                                                                                   \n});\nI feel like I am missing something..\n. @xivSolutions I'm also on Node 6.2.0.\nI think the difference is in how the function is added. I apologize for not being clear on this, when I read my description again I can see how this could be confusing. \nI'm loading the function by placing it in its own file and loading it through the scripts parameter. \nTo be concrete I have it in db/scripts/my_test.sql, and loading it with massive.connectSync({connectionString, scripts: './db/scripts'});.\nI guess this is not the way to do it...\n. @xivSolutions that makes sense. Thanks for all the help guys! \n. @dmfay Removed one testcase and changed the other to have a search term that actually had something filtered away by the where object.\n. Wow! That was fast!\nThank you :)\n. Oh, my bad, I only tested using latest, and assumed it was like this in everything above 2.x.\nYour suggestion works like a charm though, thanks for the quick reply!. Well that does not, match the search term. But as I see it now it would be better if the term was empty, because then we would actually have a result..\n. ",
    "pswanson124": "Sorry Rob...I am new to Git and may have jumped the gun on this pull - Thought this was going to my main not yours.  I think I failed the testing so please disregard this...if you even see this since it seems to have failed\n. ",
    "andrerpena": "Oh I see. I just thought there was a 'like' operator for Find because there\nis a greater than and lesser than.\nThank you.\nBTW, love your pluralsight courses.\nEm 14 de mai de 2016 12:55, \"Rob Conery\" notifications@github.com\nescreveu:\n\nI think you can just...\ndb.contacts.where(\"email like $1\", [\"rob\"], (e,r) => {});\nor\ndb.contacts.find(\"email like $1\", [\"rob\"], (e,r) => {});\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/robconery/massive-js/issues/251#issuecomment-219227692\n. @dmfay. That makes sense... Thank you for that.\n. @xivSolutions , thank for the reply.\n\nDisclaimer: I just got here and your opinion is more valuable than mine, these are just my 2 cents.\nIf you are planning to rewrite Massive anyway, then the ES2015 conversion could go along with it. Otherwise, I don't see why a rewrite would be better than progressively changing it. I think the majority of ES2015 features (listed here) are architecturally compatible with ES5 code. Please let me know if/why you don't agree.\nI would do it like this:\n1. Create a ES2015 branch.\n2. Add Babel and implement build scripts.\n3. Start replacing code: Start using imports, arrow functions, string interpolation, destructoring... without actually changing functionalities.\n4. Merge back to master on every iteration once it's tested.\nPlease let me know your thoughts.\n. Ok, I appreciate it.\nFeel free to close this issue if you want.\n. That was quick.. Thank you!\n. connection.end() actually solved my problem with Knex. Thank you Rob.\nOn Fri, Jun 24, 2016 at 6:08 PM, Rob Conery notifications@github.com\nwrote:\n\nClosed #277 https://github.com/robconery/massive-js/issues/277.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/robconery/massive-js/issues/277#event-703707753, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/ABghdQ6Oz1urGMo5QcaNpvYDx_pWIgERks5qPEdVgaJpZM4I-Bfb\n.\n\n\nAndr\u00e9 Rodrigues Pena\n. Just for the science.. If I never use massive to query in parallel,\nconnection.end() works great. Otherwise, it does not. Thank you.\nOn Sat, Jun 25, 2016 at 6:40 AM, Andr\u00e9 Pena andrerpena@gmail.com wrote:\n\nconnection.end() actually solved my problem with Knex. Thank you Rob.\nOn Fri, Jun 24, 2016 at 6:08 PM, Rob Conery notifications@github.com\nwrote:\n\nClosed #277 https://github.com/robconery/massive-js/issues/277.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/robconery/massive-js/issues/277#event-703707753, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/ABghdQ6Oz1urGMo5QcaNpvYDx_pWIgERks5qPEdVgaJpZM4I-Bfb\n.\n\n\nAndr\u00e9 Rodrigues Pena\n\n\nAndr\u00e9 Rodrigues Pena\n. I found my problem. I'm so embarrassed.\nActually, during my Bookshelf -> Massive transition I changed 1 like to ilike. This simple change makes the query 6 to 7 slower.\ndatasource: 1.040.000 contacts intentionally unindexed.\nquery: select * from contact where name ilike 'andre' (+ equivalent count for paging)\ntime: 233ms (like) or 1600ms (ilike)\nThank you @robconery . Anyway, I think the logging will be very welcome.\nFor those coming from Google: Massive is not slower than Bookshelf.\n. I've just tried \nnpm show massive@* version\n\nWhich doesn't show any \"next\" or \"beta\" version for 3.. I can't wait to use it. I've being waiting for native premises since ever. Thank you very much for this. @dmfay . I know Rob was kind of against it. . @dmfay Thank you very much! I'm super excited with this release.. \\o/ Hooraaay! Time to test it!. This doesn't happen if I use an object instead of the primary key.\nI mean...\ndb.foo.findOne(999) // throws\ndb.foo.findOne({id: 999}) // does NOT throw. Hopefully this will work. Thank you very much.. Hum. Strange. \"npm show massive@* version\" does not list 3.0.0-rc1 but if\n\nyou go to npmjs.com then the final version is 3.0.0-rc1. You're probably\nright. It's probably a problem with the way I'm listing versions\nOn Wed, May 31, 2017 at 9:35 PM, Dian Fay notifications@github.com wrote:\n\nnpm i [-g] massive does actually install 3.0.0-rc1; any particular reason\nbehind this?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/dmfay/massive-js/pull/379#issuecomment-305356276, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABghdTsyCvf0gYxsk-KWiZFHwxE0hj6Iks5r_gdqgaJpZM4NsW32\n.\n\n\n-- \nAndr\u00e9 Rodrigues Pena\n. Thanks for the reply. I assume this options object must be from the pg driver.\nIn the title, I stated this is an error with the documentation but I'm not even sure. Maybe the way it is in the docs is the way it's supposed to work.\nPeople working with regular JavaScript can adapt more easier. If an array is returned you just treat it as an array, but when there's TypeScript, for example, things get trickier.. @vitaly-t , thank you very much.. @dmfay I cannot believe I didn't see this \ud83e\udd2aThank you and sorry.\n@vitaly-t Thanks but I just made some tests and apparently reserved words, like null require you to use double quotes. However, column types are not considered reserved words. You can call a column text or varchar, for example, without the quotes.. ",
    "briancray": "Awesome, I was just trying to use ~* operator on body::text\n. Big thank you! awesome work. Any idea when this will be part of a release?\n. ",
    "ryanwalters": "That pretty much answered my question perfectly! I was looking for the \"OO version\", maybe it'll be supported one day.\nThanks.\n. ",
    "swinston100": "@dmfay - yes it is about that issue.\nWhen calling saveDoc it's probably also invoking createDocumentTable - makes sense.\nWhich leads me to another question...\nHow do you use massive to get data into a table (apart from using saveDoc)?\nAnd how do I create a table with a specific model?\ndb.createDocumentTable('test (id serial primary key, body jsonb)', function(err, res) {  });\nIs obviously not the correct syntax.\nThanks!\n. @dmfay - thanks so much, very helpful! Much clearer now :+1: \n. I also tried\njs\n  db.runSync(\"CREATE TABLE IF NOT EXISTS revenue(id serial primary key, \\\"Date\\\" integer, \\\"Account Name\\\" varchar(255) not null, \\\"Total Sold Imps\\\" integer, \\\"Imps Bought\\\" integer, \\\"Publisher Rev\\\" numeric)\");\n  db.revenue.insert(data, function (err, res) {\n    if(err){  console.log(\"error: \" +err); }\n    done();\n  });\nBut had the same results.\n. @dmfay - many thanks\nAny idea how to disconnect using db.run()? I've tried to connect to another database, \\q.. even\njs\n  db.runSync(\"select pg_terminate_backend(pid) from pg_stat_activity where datname= 'excel'\")\nas my connnectionString was\npostgres://postgres:password@localhost/excel\nThanks!\nThis was the error running this command\n\nFATAL:  terminating connection due to administrator command\nserver closed the connection unexpectedly\n        This probably means the server terminated abnormally\n        before or while processing the request.\nThe connection to the server was lost. Attempting reset: Succeeded.\n. Hi @xivSolutions \n\ni tried to re-initialize the db variable\nboth with simply \njs\nvar massiveInstance = massive.connectSync({connectionString : Database.connectionString});\napp.set('db', massiveInstance);\nvar massiveDB = app.get('db');\n//create table\nmassiveInstance = massive.connectSync({connectionString : Database.connectionString});\nDatabase.writeDB(massiveDB, excelData, done);\nand\njs\nvar massiveInstance = massive.connectSync({connectionString : Database.connectionString});\napp.set('db', massiveInstance);\nvar massiveDB = app.get('db');\n//create table\nvar kassiveInstance = massive.connectSync({connectionString : Database.connectionString});\napp.set('dab', kassiveInstance);\nvar kassiveDB = app.get('dab');\nDatabase.writeDB(kassiveDB, excelData, done);\nBut neither reloaded the schema.\nThe database schema is decided at runtime as it is based on an excel spreadsheet given by a user.\n. @xivSolutions  - thanks. I treid resetting in the same manner as index.js but with no luck.\nI was making sure with callacks that insert was only happening after the rest which in turn only happened after the create.\nI have found a solution where I use pg to create the table\njs\nvar pg = require(\"pg\");\npg.connect(\"postgres://postgres:password@localhost/excel\", function(err, client, done){\n  client.query(\"CREATE TABLE IF NOT EXISTS revenue(id serial primary key, \\\"Date\\\" integer, \\\"Account Name\\\" varchar(255) not null, \\\"Total Sold Imps\\\" integer, \\\"Imps Bought\\\" integer, \\\"Publisher Rev\\\" numeric)\");\n  done()\n});\nAnd only after that do I load massive.\nIdeally I would have loved to just stick to massive but this now works well.\nThanks for your help! Really appreciated!\n. ",
    "Permagate": "Ah, as I thought. Thanks for the answers :+1: \nNot that I'm adverse to using SQL, it's just feels weird seeing all these query API without having some ways to join the tables before executing the query. Association is such a common thing after all.\n. ",
    "quinn": "@dmfay could you post what you have somewhere ? would love to take a look at it, building something very similar right now !\n. @dmfay yes, cool thanks :) \n. ",
    "brad-decker": "@dmfay any update on when we might expect to see join capabilities?. This was definitely the issue, @dmfay thanks for the assist !. ",
    "michaelzoidl": "Another comfort solution - for now - is using the sql-bricks library to build those sql-statements, for example:\njs\ndb.run(select('field.team_id', 'team.title')\n    .from('field')\n    .join('team')\n    .on({\n      'field.team_id': 'team.id'\n    })\n    .where({\n      'team.id': '52'\n    })\n    .toString())\nFor a solution in massive, has anyone a suggestion how the api should look? I tried to make a example, but it's quite hard to find a way to fit in the existing massive api. . @dmfay tried your solution, works fine \ud83d\udc4d \nOne question, when do you plan to release this? \nI've installed it from the master branch, but honestly this is not something i will use in production :). Awsome! Thanks for the fast process :) \nOne question, whats the reason for the usage of the pg_matviews  table usage? I mean, if i can toggle it, why do i need it at the first place?. ",
    "hypexr": "Just tried it and I get:\nUncaught TypeError: db[sqlScriptFileName] is not a function\n. @xivSolutions The files are actually available at application start, the name of which one to run is provided at runtime (these sql files are for database migrations and don't take special arguments or anything). Does that mean that it may be possible, without some funky eval implementation?\n. Nevermind, the db[variable] works.  I had just forgot to set the scripts directory in the connect.  Thanks!\n. I have some tests that hit a DB but for the most part I'm looking to test independently on my CI system.  I think my question is due to my lack of javascript experience.  How would I declare the mock save?  I've tried:\ndb.login_sessions = function() {\n    function save(options, callback) {\n        console.log(\"In login_sessions mock\");\n            return callback(null, {});\n        }\n});\nwhere db is the result of the connection. With this approach it reports an error that login_sessions.save() is no longer a function.\nIf I do:\nmassive.login_sessions = function() {\n    function save(options, callback) {\n        console.log(\"In login_sessions mock\");\n            return callback(null, {});\n        }\n});\nIt doesn't properly overwrite the method and I tried:\ndb/massive.prototype.login_sessions = function() {\n    function save(options, callback) {\n        console.log(\"In login_sessions mock\");\n            return callback(null, {});\n        }\n});\nand for both of those it reports can't set property 'login_sessions' of undefined.\nThanks for any help!!\n. Works great!!  That makes sense and I'm glad to have a better understanding.\n. ",
    "naprirfan": "Hi guys, \nPlease help me. I have a working database function called editArtist. This was the creation script : \n```\nCREATE OR REPLACE FUNCTION editArtist (_artistname varchar, _infomarkdown varchar, _artistId integer, _artistSkills int[]) RETURNS integer as $$\nDECLARE\n    i int;\nBEGIN\n    -- Update artist\n    UPDATE Artists SET Name = _artistname, InfoMarkdown = _infomarkdown WHERE ArtistId = _artistId;\n-- Delete skillsets\nDELETE FROM ArtistsSkills WHERE ArtistId = _artistId;\n\n-- Insert new skillsets\nFOREACH i IN ARRAY _artistSkills \nLOOP\n  INSERT INTO ArtistsSkills (ArtistId,SkillId) VALUES (_artistId, i);\nEND LOOP;\n\nRETURN 1;\n\nEND;\n$$ LANGUAGE plpgsql;\n```\nEverything works as expected in console:\n```\ndb=> select editArtist('hello','markdown',100, array[1,2]);\n editartist\n\n      1\n\n(1 row)\n```\nBut I can't seem to call the function from my server.js:\n```\ndb.editArtist(values, function(err, result) {\n    if (err) {\n      console.log(err);\n      res.status(500).end('Database query error');\n      return;\n    }\nconsole.log(result);\n// Success! \nres.redirect('/someUrl');\n\n});\n```\nThis will produce following error:\nTypeError: db.editArtist is not a function\n   at C:\\Users\\me\\Documents\\www\\root_project\\app\\routes\\admin\\artists\\edit\\execute.js:22:6\n   at Layer.handle [as handle_request] (C:\\Users\\me\\Documents\\www\\root_project\\node_modules\\express\\lib\\router\\layer.js:95:5)\n   at next (C:\\Users\\me\\Documents\\www\\root_project\\node_modules\\express\\lib\\router\\route.js:131:13)\n   at Route.dispatch (C:\\Users\\me\\Documents\\www\\root_project\\node_modules\\express\\lib\\router\\route.js:112:3)\n   at Layer.handle [as handle_request] (C:\\Users\\me\\Documents\\www\\root_project\\node_modules\\express\\lib\\router\\layer.js:95:5)\n   at C:\\Users\\me\\Documents\\www\\root_project\\node_modules\\express\\lib\\router\\index.js:277:22\n   at param (C:\\Users\\me\\Documents\\www\\root_project\\node_modules\\express\\lib\\router\\index.js:349:14)\n   at param (C:\\Users\\me\\Documents\\www\\root_project\\node_modules\\express\\lib\\router\\index.js:365:14)\n   at Function.process_params (C:\\Users\\me\\Documents\\www\\root_project\\node_modules\\express\\lib\\router\\index.js:410:3)\n   at next (C:\\Users\\me\\Documents\\www\\root_project\\node_modules\\express\\lib\\router\\index.js:271:10)\nHow do I load this editArtist function so that it can be recognized by massive? Thanks!\nNOTE: This is my connection string:\nvar massiveInstance = massive.connectSync({connectionString : connectionString});\n. @robconery Thanks for the reply!\nI have tried both approach, but still had the same error. \n\nthe first is schemas. Does this function belong to a schema? If so you'll need to reference that.\n\nI didn't create any schema. (Do I have to?) Following are my database creation scripts:\nDROP DATABASE IF EXISTS MyDb;\nCREATE DATABASE MyDb;\n```\n-- Table Artists\nDROP TABLE IF EXISTS Artists;\nCREATE TABLE Artists(\n  ArtistId  BIGSERIAL PRIMARY KEY,\n  Name  varchar(150) NOT NULL,\n  InfoMarkdown TEXT\n);\n-- Table Skills\nDROP TABLE IF EXISTS Skills;\nCREATE TABLE Skills(\n  SkillId INTEGER PRIMARY KEY,\n  Name  varchar(50) NOT NULL\n);\nINSERT INTO Skills (SkillId, Name) VALUES \n  (1, 'Producer'),\n  (2, 'MC'),\n  (3, 'DJ');\n-- Table ArtistsSkills\nDROP TABLE IF EXISTS ArtistsSkills;\nCREATE TABLE ArtistsSkills(\n  ArtistId  BIGINT REFERENCES Artists (ArtistId),\n  SkillId INTEGER REFERENCES Skills (SkillId),\n  PRIMARY KEY (ArtistId, SkillId)\n);\n```\nAnd the third one is function creation script:\n```\nCREATE OR REPLACE FUNCTION editArtist (_artistname varchar, _infomarkdown varchar, _artistId integer, _artistSkills int[]) RETURNS RECORD as $$\nDECLARE\n    i int;\n    result RECORD;\nBEGIN\n    -- Update artist\n    UPDATE Artists SET Name = _artistname, InfoMarkdown = _infomarkdown WHERE ArtistId = _artistId;\n-- Delete skillsets\nDELETE FROM ArtistsSkills WHERE ArtistId = _artistId;\n\n-- Insert new skillsets\nFOREACH i IN ARRAY _artistSkills \nLOOP\n  INSERT INTO ArtistsSkills (ArtistId,SkillId) VALUES (_artistId, i);\nEND LOOP;\n\n-- Return record\nSELECT * FROM Artists WHERE ArtistId = _artistId INTO result;\n\nRETURN result;\n\nEND;\n$$ LANGUAGE plpgsql;\n```\n\nIf you returned SETOF(int) or just RECORD that might work.\n\nUnfortunately not. I changed my return type to Record, as you can see on the above's script. The function still works on console tho. \n```\nmydb=> SELECT editArtist('hello','markdown',100, array[1,2,3]);\n      editartist\n\n(100,hello,markdown)\n(1 row)\n```\nWhat do I miss here?\n. @xivSolutions @robconery Sorry for the confusion. I should've create a new issue instead of posting on a resolved one. \nSo I'll try to make this as clear as possible. I'm not trying to execute sql from a file. Instead, I'm trying to execute an sql function that already exist in the database. \nI've created a repo for this problem. Please see this : https://github.com/naprirfan/postgre_db_connect\nAs you can see from the repo, I can do select query just fine (as seen on app.get('/')). But when I try to call editArtist function, the error occurs (as seen on . app.get('/editRandomArtist')).\nHope It's clear enough\nNOTE : I created the database by order in database_script folder. So it goes from 01, 02 then 03\n. Oh my. I can't believe that I miss this. It's working now after I changed to db.editartist. \nThank you so much for all your help @xivSolutions and @robconery .\nAnd one more thing, I tried to revert the function back to return integer, and It's working just fine. \nHere's the result : [ anonymous { editartist: 1 } ]\nSo I guess your library does support scalar function :-)\nSo thanks again. You saved me! \n. ",
    "gpierrick": "Further to this I am getting the same error messages as above with this:\ndb.bookings_by_date([1, data.start, data.eventId], function (err, res) {})\nBut this works\n``\ndb.run(select * from bookings_by_date($1, $2, $3)`, [1, data.start, data.eventId], function (err, res) {}). Oops sorry it's meant to be data - also I have just tried insert and that works save doesn't\n```\n[{ userId: 1, conversationId: 2, read: true, deleted: false },{ userId: 2, conversationId: 2, read: true, deleted: false }]. I am still using 2.7.2 on my other project that's probably why.. ",
    "AdamAndersonFalafelSoftware": "Yeah, I didn't figure it's exactly a high priority. :) But I can confirm for sure that it hangs on findSync, not on end(). For whatever it's worth.\n. Oh, this is too good. Not only does end() work, but the findSync() afterwards does too!\njs\nmassive.connect({ connectionString: config.postgres.uri }, function f(err, db) {\n  log('connected')\n  db.thing.find({}, function a(err, things) {\n    log('this works')\n    dir(things)\n    log('will this?')\n    db.end()\n    log('closed')\n    log('this probably still won\\'t')\n    dir(db.thing.findSync())\n    log('it did???')\n  })\n})\n(log and dir are just aliases for the console methods)\n. I was going to say that \"all\" you need in order to write sync-looking code is \"just\" to promisify the async calls, then inside a coroutine use the yield keyword... which I do without really thinking about it, but it sure sounds like a mouthful to say, even in its shortest form!\n. ",
    "JohannesRudolph": "Apparently I was wrong about the table vs. document table API, both just check for existence of an id column. Is there any way to force massive-js to do an upsert of a document with a primary key already specified?\n. ",
    "rarkins": "Thanks @xivSolutions and @robconery for your suggestions. Because I'll need this to function perfectly as part of production deploy, I decided I'd better stick with binary psql. The reason I had hoped to avoid that option was that there doesn't appear to be a way to get it without the entire postgresql install. \n. ",
    "mcornut": "Thanks for your answer.\nFYI : \nIf I use only 1 insert:\nsql\nINSERT INTO myTable (column1) values ($1), ($2);\n=>  It works ! \ud83d\udc4d \nIf I use 2 inserts:\nsql\nINSERT INTO myTable (column1) values ($1);\nINSERT INTO myTable (column1) values ($2);\n=> \npostgres_1 | ERROR:  cannot insert multiple commands into a prepared statement\npostgres_1 | STATEMENT:  INSERT INTO myTable (column1) values ($1);\npostgres_1 |   INSERT INTO myTable (column1) values ($2);\n. ",
    "jnystrom": "Is this issue the same reason I cannot have a Sql file with:\n```\nWITH addedRemedy AS (\n    insert into remedy(name, description, remedy_value)\n    values(, $3, $4) returning *\n)\ninsert into location_remedy(location_id, remedy_id)\nselect $1, id from added_remedy;\nselect * from addedRemedy;\n```\nI am getting an error:\n\"cannot insert multiple commands into a prepared statement\"\nI am trying to add a record to a table(remedy), and into a join table (location_remedy), and would like both inserts to be wrapped in a transaction.. @robconery you mean to create a function in postgres directly, and have massive wrap it?  Is this the only way to handle many to many relation inserts?. @dmfay you are correct!  It was the additional select statement.  I am doing a returning * now, which will give me the id of the new remedy, and I can use that to return the new remedy record.  Thanks!. @robconery do you let the underscored columns leak into your properties of your objects, or do you do a manual mapping from user_name to userName?  This really is annoying...but I know it is not your fault... ;)  What about table names?  I assume if I have table named user_locations, massive is going to create a property name to query the table with the underscore, right (db.user_locations)? If that is correct, anyway to have Massive change table properties to camel case?\n. @robconery just looking for advice on how you would do it, not trying to be a pain in the ass.\n. @dmfay so how would I do this using MassiveJS?. @dmfay thank you very much for your support, you have been amazing supportive and responsive!  It worked, as you said.. that would be my expectation as well.  Without being able to have multiple commands in a db script, is the idea to be able to wrap commands in a transaction is to write a Stored Proc or function with Massive?. Are you saying that v3 will have transaction support, or that the architecture will allow for transactions at some point?. @dmfay that is fair.  I am hoping to not have to move to an ORM or write functions... ;). I am given a date, and I am trying to get a record that falls inside the range of startDate and endDate, so I dont think my massive find code is wrong.  I am trying to just create a .sql file that will do the between, and see if that works.. @dmfay and @xivSolutions you are both correct..and that was my problem.  The range is working for me...thanks you both!. Perfect, thank you. ok, I was making the connection in the constructor of my repository class, and I cannot make a constructor async, as I am using Typescript with async/await.  I am going to have to change to use some connect or init function instead of doing it in the ctor.. yup, going to stick with v2.x until I have time to redo . OK, so I am getting around to upgrading.  I have upgraded to v4.0.  I have changed the Massive connection call to use deasync like you suggested.  \nAll of a sudden I am not getting TONS of these errors:\nCreating a duplicate QueryFile object for the same file - \n    /Users/johnnystrom/dev/remedy/coreApi/node_modules/massive/lib/scripts/tables-whitelist.sql\n    at loader.queryFiles.files.reduce (/Users/johnnystrom/dev/remedy/coreApi/node_modules/massive/lib/database.js:157:43)\n    at filesPromise.$p.then.files (/Users/johnnystrom/dev/remedy/coreApi/node_modules/massive/lib/database.js:156:38)\nI am using the repository pattern and have multiple repositories that inherit ( using Typescript) from my MassiveRepository class.  This all worked great with v2.X, now I am getting these errors along with:\nerror: too many connections for role <db connection name>\nideas?\nExample\nBaseRepo\n```\nimport { IRepository } from './interfaces/IRepository';\nimport { inject, injectable } from 'inversify';\nimport Massive = require('massive');\nimport moment = require('moment');\nimport humps = require('humps');\nimport deasync= require('deasync');\n@injectable()\nexport class MassiveRepository implements IRepository {\n    protected db: any;\n    constructor( @inject('config') private config) {\n        this.db = deasync(Massive(config.databaseUrl));\n    }\npublic findOne(tableName: string, criteria): Promise<any> {\n    return new Promise((resolve, reject) => {\n        if (criteria) {\n            this.db[tableName].findOne(humps.decamelizeKeys(criteria), (err, result: any) => {\n                if (!err) {\n                    return resolve(humps.camelizeKeys(result));\n\n                } else {\n                    return reject(err);\n                }\n            });\n        } else {\n            return reject('Criteria is required for using findOne');\n        }\n    });\n}\n\n```\nChild Repo\n```\nimport { MassiveRepository } from './massiveRepository';\nimport { IMessageRepository } from './interfaces/IMessageRepository';\nimport { inject } from 'inversify';\nimport { ClassifierModel, ClassifierClass, NlpMessage, ScoredMessage, Customer, MessageTag, MessageTagStatus } from '@remedy/models';\nimport humps = require('humps');\nimport _ = require('lodash');\nexport class MessageRepository extends MassiveRepository implements IMessageRepository {\nconstructor( @inject('config') config) {\n    super(config);\n}\n\n``. My data are stored as Decimals in the DB....guessing it is the same issue.. I know a new instance is created for each repo I create.  This was never a problem in v2, I am confused why this would be an issue in the new version.. Thanks, I got it all working!. Ok, I figured it out, it had to do with the case of the fields vs db...my fault.  It was a strange error, but that was the reason.. Looks I am hitting a limit on the number of values that can be added to a single insert.  Looks like i will have to find a different way to batch load data.. I am processing a file of data.  Do you all support a better batch insert function?  Copy command?. I shrunk down to a batch of 2500, and it worked for the insert.  Can I pass through massive to useCOPY?. cool, I will try that...thank you!!!. OK, after more looking, this does not seem like a massive issue at all. sorry.. Can I use that syntax in a massive .sql file?  Will play with it, thanks.. @dmfay thanks.  I am still confused as to why thefindOneis sending a query that is not limiting the results to 1?. That is exactly what I was asking, sorry for my terrible explanation.  Thanks!!!. This was my fault.. One more thing.  I got it to work when I switched the last 2tx` calls:\nawait tx.integration_submissions.insert(integrationSubmissions);\nawait tx.sessions.save({ id: sessionId, processed_at: moment().utc() });\nTO:\nawait tx.sessions.save({ id: sessionId, processed_at: moment().utc() });\nawait tx.integration_submissions.insert(formattedSubmissions);\nDoes this make any sense????\n. so I added undefined and that seemed to work, but seeing the same error again.  Here is my Massive connect:\nMassiveRepository.db = await Massive(\n            this.config.databaseUrl,\n            {\n                allowedSchemas: ['public'],\n                functionBlacklist: '%_raster%, postgis%, raster_%, st%, %geometry%, %geography%, pgis%',\n            }\n        );\nstill getting the :\nTypeError: Cannot read property 'promise' of undefined\n    at Writable.insert (/Users/john/dev/company/app-ts/node_modules/massive/lib/writable.js:78:37)\n    at SessionService.<anonymous> (/Users/john/dev/company/app-ts/lib/services/sessionService.js:63:75)\n    at Generator.next (<anonymous>)\n    at fulfilled (/Users/john/dev/company/app-ts/lib/services/sessionService.js:13:58)\n    at process._tickCallback (internal/process/next_tick.js:68:7). @vitaly-t, would it make sense that it works sometimes?. @vitaly-t so, I did more digging in my code, and noticed that I was calling an insert with an empty array, once I did a check there...everything worked.. I added this:\nif (integrationSubmissions && integrationSubmissions.length > 0) {\n      savedSubmissions = await tx.integration_submissions.insert(integrationSubmissions);\n  }\n // update session to processedAt\nawait tx.sessions.save({ id: sessionId, processed_at: moment().utc() });\nAfter adding this, when there is data in the array, it does work.  I agree that it does still seem like there is something in the framework that could be fixed.. ",
    "igrayson": "@lfreneda we continue to use Sequelize.. ",
    "Sayene": "Yes, got the same problem with  massive \"version\": \"2.5.0\" and postgres 9.6. Any clues for the fix?. I'm sorry, I should have made it more clear it was v2. And yes, good point - why not upgraded yet - guess it's mostly due to some 'convenience'  wrapper we use and simple lack of time/resource ... But having checked great progress done in v5 - I think it's time to catch up! Thanks for the great lib! . ",
    "marcelcremer": "Hey guys,\nthank you for your answers. I really appreciate that you're so friendly in this project - I'll try to contribute something when I'm deeper in PostgreSQL and Massive.\nI'll try this out asap. I saw the traversal operation but didn't manage to find out how to use it properly - that example should be a nice start.\n@robconery : I also tried to use the fulltext search, but all I've come up with was\n```\ndb.foobar.searchDoc({\n  keys : [\"person\"],\n  term : \"Marcel\"\n}, function(err,docs){\n});\n```\nWhich matched. All other attempts failed for me (e.g. partial match with term Ma:*). Does the traversal operator work for fulltext searches too?\nHave a great day and best Regards\nMarcel\n. Hey robconery,\nthank you again for your input. I will start off with the current structure and leave the performance-issue for my future-self :) Until this time, the structure might change one or two times anyway.\nHowever, I like to see what the limits of the document oriented approach are and that you gave me those insights to think about - thank you for that.\nHave a nice day and best Regards\nMarcel\n. Nevermind, I'm stupid and the id column is unique anyway. I'm working too long on this part of my application I guess.\n. Hi Vitaly,\nyes - I do not deny that there are some options. But runtime errors, especially sporadic ones are really hard to debug. The current Options all need a reconfiguration of the server to enable the debugging stuff and waiting for the error to occur again:\n\nthe error event doesn't have context information in the stack\npg-monitor needs to be attached to log queries, which will usually occur after the error happened\nBluebird is not an option in my opinion. There are native promises available and a good tool should work and be able to debug without using an external Promise library\n\nOn the other side, adding context information to the pgp-error would be always available, doesn't cost performance and is easy to implement - at least when you know, where pgp is called.\nI don't see negative impact if that would be implemented tbh. Maybe you could explain your concerns?\nBest Regards\nMarcel. Hi Vitaly,\nI hope I catched you on a bad day, so I'll try it once again:\n\nthe error event doesn't have context information in the stack\nBluebird stack has all the details.\n\nOf course it has. But I'm trying to tell you, that the light of my bike is flashing and suggest to fix the bulb. You tell me to rent a mercedes instead, because it's more comfortable.\nMassive doesn't depend on Bluebird and should work perfectly fine without it. The error handling is just not as good as it could be, and I described, how to give some context information with a small change. I also wrote, that I might be able to give a pull request for it and asked, what concerns you have.\nIt's like telling people to use jQuery foreach, because they have problems with the native array foreach function, which is not very productive.\n\npg-monitor needs to be attached to log queries, which will usually occur after the error happened\nWhat do you mean after? You attach to events at the app start, and it logs everything.\n\nI'm talking about runtime errors. They just occur, somewhen, in production. And when they do, they should be somewhat meaningful.\nIt's not recommended to use pg-monitor in production, and neither it is for bluebird longstack:\n\nLong stack traces imply a substantial performance penalty, around 4-5x for throughput and 0.5x for latency.\n\nhttp://bluebirdjs.com/docs/api/promise.longstacktraces.html\nAdding the table that was called, the query parameter and query options to the error message however, doesn't have any penality as it's already there. It just needs to be added to the error, so it get's printed when the error occurs.\nI hope you have a great day and weekend.\nMarcel\n. ",
    "ianp": "Ah, OK, that makes sense, thanks for the clarification.\nFWIW the sync stuff is just in unit tests and I'm in the process of getting rid of it.\n. ",
    "benjie": "Domains, enums and arrays support added (where possible within the limitations of the pg module); have disabled interpreting by default so as to not break backwards compatibility (a suggestion on what to name this would be welcome!); and have turned the throws into bubbles.\n. @dmfay I've taken heed of your feedback, fixed those issues and now added support for setof text-style return values (as well as setof row which was already supported); and have added support for streaming them too.\n. Done; I also added a note about it in connecting.md and changed the configuration variable from the unwieldy interpretFunctionReturnTypes to simply enhancedFunctions - I hope that's okay?\n. I read through pg's source with the same fears before implementing; but it turns out that pg.connect implements the pool based on the config, and this doesn't change the config so it shouldn't cause any more pools - everything will continue to use the same one as before so long as the connectionString doesn't change.\n. I've rebased this on #297 and updated the description at the top.\n. Rebased off of #297 which was just rebased off master. I'm unavailable to work on this for about 5 weeks, but if you wanted to set up a PR into my PR @jmealo you'd be very welcome \ud83d\udc4d . @robconery / @dmfay Any thoughts on this PR?\n. The main driver is to allow lightweight cloning of massive instances to enable #295 (connection wrapper to enable local variables in transactions for every query) and similar functionality others may wish to introduce; the multiple connections thing is just a side benefit that I've not tested yet.\nYour response is definitely fair - the diffs do seem quite daunting, but thankfully that's just a fa\u00e7ade!\nOther than making walkSqlFiles asynchronous it's really just re-arranging the code you already have and splitting bootstrapping into two phases; e.g.\n- instead of loading each thing in turn, we fetch each thing and then apply our learnings all at once:\n\n- fetching is just the first half of loading; e.g. for functions:\n\n- applying the loaded functions is identical (but at a significantly reduced indentation level):\n\nI'm happy to give this treatment to all the changes if it's helpful (or to split them into separate commits to make it easier to review, maybe?). Hopefully you can see from this that there's not much actual change going on, it's mostly just moving code around, so shouldn't break anything sensible.\nThe only breaking change that it has, as far as I'm aware, is that naming one of your SQL files in such a way as to overwrite a built in method is now no longer allowed - but I'm not sure that should have been allowed in the first place (I think it's an accidental feature?). I'm happy to revert this so that the functions can be overwritten if you want...\nAs you may have noticed, I'm invested in the future of Massive.js (since I'm using it for one of my projects) so I'm keen to keep my version as close to yours as possible - let me know how I can make your life easier with my changes and I'll do my best \ud83d\ude04 . I did dip my toes in the bluebird branch too (since I'd like to use async/await) but I got scared and ran away.\n. @robconery I'm not sure if you got a chance to look at this PR in the end; but I've just rebased it on the latest master so it's merge-able again (hopefully).\n. I've rebased this on the latest master, @robconery have you had any further thoughts on this?. Thanks for the quick turnaround! \ud83d\udc4f \n. Whoops, these should be asserts really; I was just being lazy.\n. ",
    "samtheprogram": "In case anyone lands here, I'm working on a solution similar to @benjie's to add this kind of support to v3. I'll be opening a PR once I have something more concrete.. ",
    "jagregory": "Closing this unless something is to be done.. ",
    "gaastonsr": "Promises + Transactions please <3\n. ",
    "Rub21": "Thank you  @robconery , it works fine! :) \n. ",
    "digitalysin": "my mistake, forgot to add sequence.\n. ",
    "ahdinosaur": "@robconery yeah, no worries, as a fellow open source maintainer i understand what you're saying, i'm happy to maintain a fork if you don't feel ignoring dotfiles and node_modules is worth it. thanks for the quick and thoughtful response. :smiley: \n. ",
    "nannastudy": "HI Dian Fay,\nThanks for the response. Yes, it is the same user I am connecting with Massive.\nThe table is in a different schema, but while inserting I mentioned full namespace \"schema.table\".\nCan you please elaborate on permissions and ownership ?\nregards\nKiran\n. After enabling logging in the database, I figured the problem in the insert.\nThe JSON that was inserted had the \"qtext\" mispelled and the coalesce was returning '' and hence the tsvector was not having anything in the column. \nI was able to fix the issue, after correcting the JSON input . \nThanks\nKiran\n. ",
    "j0k3r": "Of course, changed !\n. Great to see that fixed, I was about to yell at vitaly-t/pg-promise about introducing a breaking change in a minor version.\n@dmfay could you release a patch/minor version with that upgrade? Thanks!. I was about to yell because we lost few hours on friday debugging TypeError: script.sql.formatDBType is not a function :slightly_smiling_face: \nI think you should have deprecate the old function instead of removing it.. ",
    "bguiz": "@dmfay The documentations is ... a little sparse on setAttribute too: http://massive-js.readthedocs.io/en/latest/API/json_documents/\n(it only has the function signature, and nothing else)\n. Additionally, I'd like to be able to do a document update with a query (not just the ID), for example\njs\ndb.doggies.setAttribute({\n  sex: 'male'\n}, 'name', 'rover', (err, result) => { /* ... */ });\n(note that this implies being able to update multiple documents with one query)\nFurther to this, I'd like to be able to set multiple attributes at the same time\njs\ndb.doggies.setAttribute({\n  name: 'rover',\n}, {\n  sex: 'female',\n  kennel: 123,\n}, (err, result) => { /* ... */ });\nFrom what I understand about jsonb_set, this may require multiple statements, and therefore a multi-statement transaction of some sort.\n. @dmfay should I open a new ticket for the above since you've closed this one?\n. cc @dmfay \n. There's a similar issue that was closed in August by @robconery - new issue because I have an alternative suggestion.\n. Yeah I got the rationale behind the two formats with the saveDoc(), that\ntotally makes sense.\n... however there are plenty of use cases (I'm running into in my app)\nwhere tables are created on the fly using saveDoc() ... but in some cases\nmight be accessed (Read/Update/Delete) before that happens, or more\naccurately, attempted to. Thus is we have saveDoc() with one format, it\nmakes sense to have the other 3 operations also have the same format, where\nthe order of operations that eventuates cannot be determined ahead of time.\nW: http://bguiz.com\nOn 7 November 2016 at 17:20, Rob Conery notifications@github.com wrote:\n\nI don't see why not... open to a PR if you're willing. The reason we have\nthe override (non namespace) is a convenience for building the table on the\nfly. The only reason you would need/do that is if you're saving - it\ndoesn't make sense otherwise.\nAlso - help me understand the this issue some more?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/robconery/massive-js/issues/329#issuecomment-258755925,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABsQ2e1hXG8ttvcxgb4Dj0c0OxZXWUgsks5q7sMsgaJpZM4KquGb\n.\n. Regrading the this - I looked through the massive-js source, and\nit's the self variable used within Massive.prototype.* functions.\n\nIn my case, I'm doing something along the lines of:\nlet [err, result] = yield righto(db.saveDoc, 'doggies', {name:'rover'});\n(righto is a lib that allow yield-ing callback functions.\nand this works perfectly fine. However, attempting to use the second\nformat means that I need to use bind, like so:\nlet [err, result] = yield righto(db.doggies.saveDoc.bind(db.doggies),\n{name:'rover'});\nSo for saveDoc() this is obviously not an issue, since I'm quite\nhappy to use the former method.\nHowever, with the other three operations, I'm forced to use the latter\nmethod, and hence need to use bind in the same way. This is another\nreason why I would like to have the root db object expose all of its\nmethod such that it accepts the name of the table in the 1st param.\nWhat do you reckon?\nW: http://bguiz.com\nOn 7 November 2016 at 20:53, Brendan Graetz brendan.graetz@gmail.com wrote:\n\nYeah I got the rationale behind the two formats with the saveDoc(), that\ntotally makes sense.\n... however there are plenty of use cases (I'm running into in my app) where\ntables are created on the fly using saveDoc() ... but in some cases might\nbe accessed (Read/Update/Delete) before that happens, or more accurately,\nattempted to. Thus is we have saveDoc() with one format, it makes sense to\nhave the other 3 operations also have the same format, where the order of\noperations that eventuates cannot be determined ahead of time.\nW: http://bguiz.com\nOn 7 November 2016 at 17:20, Rob Conery notifications@github.com wrote:\n\nI don't see why not... open to a PR if you're willing. The reason we have\nthe override (non namespace) is a convenience for building the table on the\nfly. The only reason you would need/do that is if you're saving - it doesn't\nmake sense otherwise.\nAlso - help me understand the this issue some more?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Ping!. Thanks!. \n\n",
    "jlippold": "I tried logging what tables are connected to massive\nvar tables = db.tables.map(function(table) {\n        return table.name;\n    });\n    console.log(tables);\nand I get\nweb_1  | [nodemon] restarting due to changes...\nweb_1  | [nodemon] starting `node ./bin/www`\nweb_1  | DB connected\nweb_1  | Express server listening on port: 3000\n[nodemon] clean exit - waiting for changes before restart\nweb_1  | [ 'districts',\nweb_1  |   'migrations',\nweb_1  |   'roles',\nweb_1  |   'schools',\nweb_1  |   'session',\nweb_1  |   'user_roles',\nweb_1  |   'users' ]\nweb_1  | Cannot read property 'crap' of undefined\nweb_1  | GET /crap 500 22.723 ms - 1143\nWhy isn't crap in the table list, it's so odd. \nI can see crap fine using pgAdmin\nSELECT table_name FROM information_schema.tables WHERE table_schema='public'\ntable_name\n=====\ncrap\ndistricts\nmigrations\nroles\nschools\nsession\nuser_roles\nusers\n. @xivSolutions and @dmfay , thank you\nThe combined solution of removing public AND adding a primary key made the table show up.\nCREATE TABLE IF NOT EXISTS public.crap (\n        crap_id SERIAL,\n        firstName text NOT NULL,\n        lastName text NOT NULL,\n        email text UNIQUE NOT NULL,\n        PRIMARY KEY(crap_id)\n    );\nThis should probably be added to the documentation somewhere.. but basically if tables aren't detected by massive, devs need make sure the table has a primary key\n. yea I actually made a crap table just to test it and figured, ohh im testing who needs a key anyhow. Big mistake \ud83d\udca9 \n. Thanks for the feedback @dmfay, if it's pg's best practice to use snake_case, I'll stick with that. But whats the recommended way to translate them? \nI could have convertToCamel / convertToSnake, or use humps all over my code, but that is less than ideal. Is there's a global hook somewhere, or is there a better way?\n. I ended up just writing a wrapper for massive so I can globally convert case. It's not the prettiest answer but it so far so good.\nmy wrapper is db.js like:\n``` js\nvar massive = require('massive');\nvar humps = require('humps');\nvar env = process.env.NODE_ENV || \"dev\";\nvar config = require('../configs/massive')[env];\nvar db = massive.connectSync(config);\nvar dbWrapper = {};\n//this only exposes methods I wrap manually, like findOne below\ndb.tables.forEach(function(dbTable) {\n    var table = dbTable.name;\ndbWrapper[table] = {};\ndbWrapper[table].findOne = function(input, callback) {\n    var snake = humps.decamelizeKeys(input);\n    db[table].findOne(snake, function(err, output) {\n        if (err) {\n            return callback(err);\n        }\n        var camel = humps.camelizeKeys(output);\n        return callback(err, camel);\n    });\n}\n// needs more methods wrapped, eg..\n// dbWrapper[table].insert = function(input, callback) {}\n\n});\nmodule.exports = dbWrapper;\n```\nthen i can call it like.\n``` js\nvar db = require(\"./lib/db\");\n//inputs will be auto converted to snake\nvar params = {\n    userName: user,\n    deleted: false\n};\ndb.users.findOne(params, function(err, user) {\n    //user output will be converted to camel\n});\n```\n\u00af(\u30c4)/\u00af\n. Hey @robconery, I understand that this might end up requiring long term support, but it feels cleaner to me than having redundant convert statements all over every route, or mixing 2 coding conventions. Are you suggesting I go snake_case across every object I expose to my api users? Should I start writing all of my JS with snakecase, when the standard for JS is camelCase?\nI feel like this is really about choosing the lesser of two evils when none of them feel great. \n. All good @robconery, thanks for the advice. Would you be open to accepting a pull request that adds pre & post execution hooks as I described in the first comment? Or do you feel that it's not beneficial for the tool?\n. ",
    "deoxen0n2": "My use case is streaming changes to clients via WebSocket (say, incoming new order or existing order is updated). So my plan is to use NOTIFY query on Postgres side when the new row is inserted or the existing row is updated and then streaming those changes via WebSocket.\nI'm very new on PostgreSQL so apologize in advance if I missed something.\n. OK then, I think I would go with node-pg directly for this functionality. Thanks you both of you and nice module by the way :)\n. ",
    "AlphaDork": "You are familiar with the Javascript - you wrote it LOL. It's taken from your pg-auth repo. So maybe I should have filed over there come to think of it. This is the JS that runs the SQL to create the view (tables, functions, etc.) Everything else works, just the permissions on the view are wonky.\nIn builder.js - \n``` javascript\nexports.install = function(){\n  var db = Massive.connectSync(dbConfig);\n  var sqlFile = decideSqlFile();\n  var sql = fs.readFileSync(sqlFile,{encoding : \"utf-8\"});\n  return db.runSync(sql);\n```\n. Confirmed that I'm a bozo. LOL. Can I blame it on the new puppy and lack of sleep?  Calling a function later in my SQL load that blew away the view and rebuilt it without granting access to the read.  \nThank you for your patience! IOU - 1 largish beer.\n. ",
    "hpop": "As so often, sometimes you have to explain it to someone to find your error.\nOf course: If I provide an id, massive tries to update the entry. \n. ",
    "deo79": "Thanks for responding.  I definitely get what you're saying.  A view it is then.  It's a bummer because it limits the usefulness of SQL files, which is a great feature of Massive.. ",
    "grawert": "Ok I see the functions in lib/document.js are doing the formatting.. ",
    "codepunkt": "Which dependency is it?. ",
    "sam3d": "Thank you very much for the caution! Our users can't change their usernames due to an inherent limitation of the platform we're integrating with, so it seemed a waste to have a separate ID field. Totally understand your reasoning though, perhaps in future.. Thank you for the help!. *maintaining then \ud83d\ude09. ",
    "lukemiles": "Yeah, after further experimenting with this patch I'm pretty dissatisfied with this as an option. Closing this until we have something better.. ",
    "SBrinkworth": "That was it! :)  everyone runs it in a different folder and so it doesn't see the db folder. Thanks for the help!. ",
    "AxGitHubing": "thank you for following up, but looks like it is me having issues with callbacks in node.js. I return the database instance before the tables are created.\nBut I saw another issue with saveDoc and JSON. After I inserted a JSON a second time, massive added a {\"0\": }\n. at the moment I am quite busy. No time to check it with some simple sample code. I am sorry.. the issue is that I get the connection to the database with no tables created. then I create the tables and then I pass on the database instance. But the database instance does not habe the reference to the created tables. how can I reload/reconnect/refresh the database object?. database is in a docker container at the moment. so I need to setup it up on the fly. just wanted to let you know there is no issue with 'saveDoc and JSON'. \nsurprisingly the bug was in my code ...\n...\n...\n;-). ",
    "pablote": "missed it, thanks!. you're welcome :), please let me know if/when this gets pushed to npm, thanks!. ",
    "rnoelle": "Thanks so much for the reply!\nI was looking at what I assume are the old docs:\nhttps://massive-js.readthedocs.io/en/latest/quick_start/\nOn Wed, May 31, 2017 at 4:33 PM, Dian Fay notifications@github.com wrote:\n\nSynchronous functions are no longer supported, and the connect function\nitself no longer exists -- it's all promises all the way. Connection docs\nare here https://dmfay.github.io/massive-js/connecting.html.\nIf you can use async/await that's an excellent way to manage promises, or\nthere are libraries like co https://github.com/tj/co which let you yield\nthem.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/dmfay/massive-js/issues/378#issuecomment-305337234,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AS0vRjzpVnp5fNUcRjgp_SFECGDwuXJdks5r_erMgaJpZM4NsRtQ\n.\n. \n",
    "chamini2": "Sorry if this is a dumb question, but node-postgres now supports Promises, why not use it now instead of pg-promise?. I did not know pg-promise did that many things on top of promisifying node-postgres. Thanks for the answer.. ",
    "asBrettisay": "@vitaly-t At our coding school, we teach SQL using Massive, and use comments like this to help guide the teaching experience. Thanks so much for addressing this!. @dmfay excellent, thanks for the response! We appreciate the suggestions.. ",
    "TomPridham": "That fixed it. Thanks!\nI'd definitely be willing to try it. Where would be a good place to start with that?. ",
    "coveralls": "\nChanges Unknown when pulling f9b2eb82f32243c2de7a6a78542e14335cd07327 on coveralls into  on master.\n. ",
    "treicadani": "Sorry, I meant that I haven't found anything related to the issue, not the actual definition.\nI have uploaded here a screenshot with the version of PostgreSQL installation in my Linux box https://pasteboard.co/2erBdfIS1.jpg\nI will look up for Materialized Views to see why they are missing, I may have to reinstall and I will come back with a result.\nThanks.. I have re-installed PostgreSQL on the box and was still the same (i guess there is something odd with CentOS 6.8).\nBut then I have created a new Debian box, installed PostgreSQL 9.4 and I am back in business.. ",
    "dotob": "hi, i happily used massive with postgres 8.3 until now. after the update i get this error. i do not understand why i cannot disable this? please help.\nsebastian. ",
    "tadejstanic": "thx @dmfay  for your work!. ",
    "eduardomourar": "@vitaly-t and @dmfay , let me know if an issue should be created for this. Also, I send this pull request to branch v2, because I worked based on v.2.6.1.. On my particular problem, I was querying a body column on document table and surely jsonb_path_ops cover nulls, but I needed to do something like this on the options:\n{\n  or: [\n    // Other dynamic generated comparisons...\n    {'field1': 'TEST1'},\n    {'field2': 'TEST2'},\n    {'field2 is': null}  // when the field was not present on body jsonb\n  ]\n};\nBefore this PR change, I saw myself using 'field2 is not distinct from': null to achieve the same result, but not straight forward to follow it. Of course, I could run the SQL directly, but I had a project directive to avoid this as much as possible. This field2 either did not exist, hold value NULL or a string.. I will be using this when we upgrade late this year, so I do have interest in porting to v3. I will look into it this week for sure. BTW, thank you for this amazing project! Sometimes people take it for granted, you know.. @dmfay and @vitaly-t, thanks. In our particular case, the parse method from nodejs querystring was causing the issue. The note there states: \n\nNote: The object returned by the querystring.parse() method does not prototypically inherit from the JavaScript Object. This means that typical Object methods such as obj.toString(), obj.hasOwnProperty(), and others are not defined and will not work.\n\nI will change massivejs v2 and v4 code and submit PR soon.. ",
    "jakewilson801": "Thanks for the quick response. I was just being a dumb dumb. Closing now... . ",
    "juan-quiver": "As massive.js isn't casting on insert / update just yet, what you can do is turn your array into the second format @tamlyn mentioned, like:\nawait db.testing_table.insert({numbers:{${numbers.join(',')}}})\n. ",
    "Kikobeats": "That's an awesome feature, is it in the roadmap? \ud83d\ude42\nI made a upsert workaround:\njs\nconst upsert = async (table, collection) =>\n    aigle.eachSeries(collection, async data => {\n      const id = data.id\n      const instance = await table.findOne({ id })\n      return isNil(instance) ? table.insert(data) : table.update({ id }, data)\n    })\nusing it\njs\nawait upsert(db.customers, customersPlans)\nawait upsert(db.quotas, quotas)\nawait upsert(db.plans, plans)\none thing, why is not possible use insert with custom id? :((\n. ",
    "lpatoyst": "Using 2.7.1 as of now. Good work :+1: Thanks @dmfay :). ",
    "spikeon": "I have another item to add to this:\nYour examples don't include basic use cases. For example, to get all items from a table without filtering them at all, do I have to use .find({ id : \"*\" }) or .find({}) or .find() or can i just use db.table.then() or is there a db.table.list() function I don't know about?  . I'm sorry, I just took a look at the changes in the lib folder.  I should've been more specific.  I meant the README.md documentation and examples, for both the empty result and for this.  I should've looked at the jsDoc to see if the answer was there before, but now I know.  However, still these things should probably still be added to the README.md.. I agree that duplicating efforts is tiring, however, perhaps at the bottom you can do a bunch of example calls, like font-awesome is doing in their alpha build documentation.  \nFor example: \nFind\ndb.site.find(); // Returns an array of all sites\ndb.site.find(\"*\"); // Returns an array of all sites\ndb.site.find(1); // Returns an array containing the site with id 1\ndb.site.find({url : \"google.com\"}); // Returns an array of sites with the url \"google.com\"\n...\n. Oh, I'm an idiot.  Yes, put in a link to the main docs.  How about a big bold flashing link so idiots like me know it exists. \nSorry I wasted your time.  Haha.. ",
    "bmoquist": "Thanks so much for the help - it worked perfectly for the above. Here is the full solution for  reference. Note the curly braces needed to test in Postgres!\n```\nCREATE FUNCTION get_car_tags(car_ids text[])\nRETURNS SETOF fn_get_car_tags AS\n$$\nSELECT selected_car_tags.car_id, tags.tag_name FROM (\n  tags\n  INNER JOIN\n  (\n    SELECT *\n    FROM cars_tags\n    WHERE cars_tags.car_id = ANY($1)\n  )  AS selected_car_tags\n  ON tags.tag_id=selected_car_tags.tag_id\n);\n$$ LANGUAGE sql;\n--Test function Postgres\nselect * from get_car_tags('{1,2,3}');\nselect * from get_car_tags('{2,3}');\n--Test in MassiveJS\ndb.get_car_tags([['3','2','1']]);\ndb.get_car_tags([['2','3']]);\n```\n. Thanks so much for the clear explanation. The inclusion of findOne was a mistake on my part. \nFor the benefit of others, here's an example change made to my original function. As pointed out, the data fields being changed need to move to the second argument. Adding the single option will return the first matching record instead of an array.\n```js\nconst criteria = {\n   user_id: userId\n}\nconst changes = {\n   last_login: currentTimestamp\n}\nconst options = {\n   single: true\n} \nreq.app.get(constants.db).users.update(criteria, changes, options)\n```\nThanks again!. ",
    "cjevning": "@dmfay the suggested fix no longer parses bigints into strings as of version 4.1.0, not sure if this is related to massive or pg-promise. Whoops sorry, late night typo. I meant that on massive 4.1.0 or later, my bigint's are once again returning as strings rather than being parsed, with no changes to the parsing code:\nconst types = require('pg').types;\ntypes.setTypeParser(20, val => parseInt(val, 10));\nCurrently using 4.0.1 temporarily as a workaround, but bumping up to 4.1.0 or later results in all bigints returning as strings again instead of ints like in earlier versions (and from what I can tell the parse function is never even called). I was changing the type parser before initializing, but now I moved that code to after initialization but the strings still won't parse in >= 4.1:\n```\n  // connect to Massive and get the db instance.\n  massive(dbConfig[environment])\n    .then((db) => {\n      const types = require('pg').types;\n      // Converts pg values of type bigint to js ints\n      types.setTypeParser(20, val => parseInt(val, 10));\n  // Set a reference to the massive instance on Express' app\n  app.set('db', db);\n\n  //other init code, loggers and such not touching db\n\n  app.listen(port, (listenError) => {.....\n\n``.rm -rf` didn't do it, until I restarted my computer \u00af\\(\u30c4)/\u00af now with the code moved to after the db init it works as expected again, thank you!. :+1: sounds good, thanks!. ",
    "robertrossmann": "The problem is that\n\nImporting the Database class directly from userland code feels \"hacky\" because I would have to import it from the lib/ folder, which is generally not considered to be public API:\n  js\n  import Database from 'massive/lib/database'\nUsing the Database class directly would mean that I lose some functionality currently available only when using the massive() function (this is the part which this PR tries to solve by moving the functionality to the Database class)\n\nAnd using the publicly available massive() constructor function does make a connection since it contains .reload() call, so it does not suit my needs (I need to instantiate the client first, and issue a .connect() or .reload() later). Thanks guys for your comments! I have updated this PR with updated docblocks and removed the check for connection object. I have updated the initial PR's description with new entries:\n\n\nRemoves the check for connection object being defined and instead relies on the default values defined in pg (assumes a local postgres instance)\n\nUnrelated - adds the ability to re-generate API docs without jsdoc being installed globally\n\nNotes\n\nThe docblock for the now-exposed Database class is not linked to it's definition - I could not figure out a way to link it properly\nThere are some warnings in the test suite now, I guess it's because massive now connects to two different databases during the test suite (the massive db and travis db, which is the default used when no connection info is specified) - I am unsure how to fix those warnings and how severe those warnings are. Thanks for review and merging! And good find on that JSDoc behaviour.\n\nBy the way, I would love to see this PR released to npm so I can start working on a feature which requires the Database class in my own project. Thanks!. Thanks, I will do that for now!\n\nPS. This PR should not break any code. All the changes are backwards-compatible with current 3.x release line.. Just saw the release notification. Awesome, thanks! \u2764\ufe0f. A little bit late to the party, but you could also export a singleton using something along the following:\n\n```js\nconst Database = require('massive').Database\nconst database = new Database('...')\n// This returns a Promise, sooo... Make sure you do not use\n// the instance before this has been resolved!\ndatabase.reload()\nmodule.exports = database\n```\nAlternatively, you could call .reload() outside of the module, ie. in some initialisation script.. Would be great to have some tests added for this feature.... Also, there is a bunch of linter errors:\n/home/travis/build/dmfay/massive-js/lib/statement/select.js\n   91:7    error    'the_columns' is never reassigned. Use 'const' instead  prefer-const\n   92:31   warning  Missing space before function parentheses               space-before-function-paren\n   92:33   warning  A space is required after ','                           comma-spacing\n   92:36   warning  Missing space before opening brace                      space-before-blocks\n   93:9    error    'dot_index' is never reassigned. Use 'const' instead    prefer-const\n   94:26   warning  Missing space before opening brace                      space-before-blocks\n   95:11   error    'col_infos' is never reassigned. Use 'const' instead    prefer-const\n   96:48   warning  A space is required after ','                           comma-spacing\n   96:96   warning  Infix operators must be spaced                          space-infix-ops\n   96:111  warning  A space is required after ','                           comma-spacing\n   97:25   warning  Missing space before opening brace                      space-before-blocks\n   98:37   warning  A space is required after ','                           comma-spacing\n  100:7    warning  Expected blank line before this statement               padding-line-between-statements\n  100:28   warning  Infix operators must be spaced                          space-infix-ops. And you can also explicitly create and connect new instance without the need to await it everywhere:\njs\nconst Database = require('massive').Database\nconst db = new Database(/* args here */)\nawait db.reload() // I think.... I feel that query routing based on query types would be better suited for pgpool instead of re-implementing it at the application layer. \ud83e\udd14 Application should not be bothered with such details as to how many replicas there are, where are they, which queries should be sent to them etc. There seem to be plenty of solutions for that already, why re-invent the wheel. \ud83d\ude04. ",
    "nikarh": "I have used == for a null check intentionally, since x==null is also true for x===undefined. \nIf you believe that isn't necessary, I can change it to ===.\nBtw, eslint eqeqeq rule can be configured to enforce === everywhere but for null comparisons.. ",
    "karsai5": "I was originally running version 3.2.2 and it would throw the following error: can't find body of null. Then I updated to version 4.0.0 and have the problem described above. . Thought it might be something similar to that. The documentation should be reworded to clarify that. The section below made me think that if it can find it it'll update it otherwise it will insert with the given ID. Maybe even just pointing out that this isn't an upsert method (like you did just now) will help future users.  \n\nThanks for getting back to me so quickly \ud83d\ude04 . I created a pull request updating the readme. Hopefully that makes it a bit clearer\nhttps://github.com/dmfay/massive-js/pull/463. ",
    "markgeraty": "Just reading this again it sounds like a far-fetched scenario, but we ran into it pretty innocently when combining existing records fetched from the database via massive and returned to the client, then appending new records from a separate source and writing them back to the server.\n@failociraptor. ",
    "jamesser": "Sounds good.  Maybe just a documentation change?  I was thrown by this paragraph\n\ninsert writes an object into your table. The object\u2019s keys should match your column names exactly. You do not have to perform a complete insert and specify every single field in your object; as long as you ensure that no NOT NULL constraints will be violated, you can include as many or as few fields as you need. When you insert an object, the record will be returned as an object. If you have an autogenerated or serial primary key or default fields, they will have been calculated and added.. \n",
    "btd": "@dmfay Thank you!. @dmfay i got a problem because of auto minor version upgrade option. I did not expect such thing to happen at all.. I created https://github.com/dmfay/massive-js/pull/629 let me know if you are good with it. I understand that it is not adding anything new.. @dmfay Thank you very much \ud83c\udf89 \ud83c\udf89 \ud83c\udf89 . ",
    "greenkeeper[bot]": "Version 6.10.0 just got published.\nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 13 commits.\n\n856b4be docs.\n2138064 docs update.\nead3be4 updating documentation.\nbc747eb Merge pull request #408 from vitaly-t/nested-names\n2e0123f fixing comments.\n11bab7b throwing away nested names support from the helpers.\nafd50bf helpers seem fully functional now.\n20c5738 adding support for the target property.\n07956f4 initial changes for the helpers.\nf4fba1b finished basics tests for nested properties.\na3a85ef adding edge-case tests, correcting algorithm.\n29e6f94 all works, pending test coverage + documentation.\n8f945ce adding the new utility method.\n\nSee the full diff\n\n. ## Version 6.10.1 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 2 commits.\n\n474e2e2 upped the version.\n6913f05 fixing #409, improving tests\n\nSee the full diff\n\n. ## Version 6.10.2 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 1 commits.\n\n7409cd9 adding support for prototypes in nested named parameters.\n\nSee the full diff\n\n. ## Version 6.10.3 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 5 commits.\n\n90cb4be restyling function call.\nfd7d99d code style+docs+dependencies.\n02a2f8c finished the code styling change.\nc182d87 restyling inline functions.\n5d279ee Use new ES6 Number class.\n\nSee the full diff\n\n. ## Version 7.0.0 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 6 commits.\n\n6b68d5b docs update.\n67f3e9e adding documentation.\n9c43393 Merge pull request #414 from vitaly-t/multi-result\n8bc1266 adding method multi + tests.\n4f28526 adding tests.\na3ee6c7 adding method multiResult.\n\nSee the full diff\n\n. ## Version 7.0.1 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 3 commits.\n\ncc8595e upped the version.\n27f27e9 updating PostgreSQL version + min Node requirements.\n0c0eb86 text syntax fix\n\nSee the full diff\n\n. ## Version 7.0.2 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 20 commits.\n\n7c2bf82 upped the version.\n0afc8ae docs.\nd23495b docs update.\n52dc6bf improving documentation.\n92f0a4f fixing links.\ne23742d adding csv + json docs.\ne2e6dd1 docs + dependencies.\n8ce16ef fixing links\n398ef79 Merge pull request #417 from vitaly-t/docs\nf8ab21b fixing links\n2ca01f8 fixing links\n01523ec another major update.\n88b529e adding filters.\n950db3f rewriting the lot.\n850ec3c adding links\n\nThere are 20 commits in total.\nSee the full diff\n\n. ## Version 7.0.3 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 11 commits.\n\n7094df5 refactoring internal special types.\ncb6b9cb docs update.\na918114 adding inspection to TransactionMode\nbd64871 adding custom object inspection.\n2035a82 fixing links.\n898a35b docs.\nd104a73 updating example.\n626dff6 docs.\nddf4cf9 docs.\nb680ccd docs.\n8cdbd0d fixing a menu link.\n\nSee the full diff\n\n. ## Version 7.1.0 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 12 commits.\n\n6b061fc docs.\n34f370c updating all types for new ctf\nd6a6f3b Adding support for CTF via ES6 Symbol\n16d2a8b docs.\n2576755 Adding support for ES6 type Symbol.\n039822d docs.\n6252581 docs.\n0f355fd Fixing tests for Native Bindings.\n85f6920 no message\n56306f2 dependency update.\n972ae46 Merge pull request #421 from grinnellian/master\n05b8868 Update README.md\n\nSee the full diff\n\n. ## Version 7.2.0 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 3 commits.\n\n4bfca89 misc.\n0bcdf9b refactoring QueryFile against the type misuse.\n844829e refactoring CTF.\n\nSee the full diff\n\n. ## Version 7.2.1 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 3 commits.\n\nf6dbfab docs.\ne70d5e2 Fixing #422.\n67166b9 refactoring tests.\n\nSee the full diff\n\n. ## Version 7.3.0 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 10 commits.\n\nbbe8287 updating driver variables.\n788438f upped the version + dependency.\n03205c5 Merge pull request #424 from vitaly-t/prefixed-assign\n233e5cf refactoring method alias to support digits.\n9b24703 docs.\n164bf69 updating documentation for method Alias\na6429cd updating the typescript.\n699d078 major rewrite, with changes for the alias support.\n620c4ca initial implementation.\n79c21c0 docs.\n\nSee the full diff\n\n. ## Version 7.3.1 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 1 commits.\n\n6b81c27 Improving/Extending on the 7.3.0 API\n\nSee the full diff\n\n. ## Version 7.3.2 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 9 commits.\n\n082924f Add support for Node 9, plus up the version\n440047a docs update.\n9ff95f8 correcting docs.\n6d3cb7d docs update.\n0c03194 hiding the query within QueryFile.\nd0d1a06 docs.\n25302f2 docs.\n6802a3d docs update.\n2484aca docs.\n\nSee the full diff\n\n. ## Version 7.3.3 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 6 commits.\n\nede94aa dep. updates.\n7c30405 dependency updates.\nad11774 docs update.\n99f0779 dependency updates.\n3d9911e dependency + docs.\n162b47a docs.\n\nSee the full diff\n\n. ## Version 7.4.0 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 15 commits.\n\n4c503d4 preparing the release.\nce53cfa fixing the link for #450.\n6408045 improving link for #450\n779914a adding link for #450\n9305c18 #450 is finished.\ncb425ef docs.\n31b4da3 improving #449 for CTF\n72b6352 fixing #449, and adding more tests.\naf98d84 docs.\nabab26c Fixing #448\nec17829 Updated docs for #447\ne9253b9 updating native compilation.\ne7407fa updating #445\n51500b6 Improving on #445, updating deps + version.\n4c18687 Fixing #445\n\nSee the full diff\n\n. ## Version 2.14.1 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 2 commits.\n\n6b026a5 version bump 2.14.1\n689c0ea Fix typing of help function\n\nSee the full diff\n\n. ## Version 7.5.1 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 3 commits.\n\neca75f5 docs update\nda5533a adding alias :list\n6033a39 docs.\n\nSee the full diff\n\n. ## Version 7.5.2 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 3 commits.\n\n1f2a0e9 adding docs + tests\nc1260f2 making ctf symbols global\n3996e14 docs for CSV\n\nSee the full diff\n\n. ## Version 8.1.0 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 8 commits.\n\n20eee6b updating history\nc6b9ee5 prepare release\nd261e89 Merge pull request #471 from vitaly-t/470-task-params\n65cc102 tests finished\nf302c59 initial tests added\ndffaa2a updating typescript\n1ca4265 documenting the method\n8bebb98 initial implememtation finished\n\nSee the full diff\n\n. ## Version 8.1.1 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nCommits\nThe new version differs by 4 commits.\n\ndfd9019 closes #475\n122ecec Merge pull request #474 from vitaly-t/473-remove-locks\n5a38529 make freeze optional where possible\n2eaa525 docs update\n\nSee the full diff\n\n. ## Version 5.0.1 just got published. \nUpdate to this version instead \ud83d\ude80 \n\nRelease Notes\nv5.0.1\n\n196c102 Fix: valid-jsdoc should allow optional returns for async (fixes #10386) (#10480) (Mark Banner)\n4c823bd Docs: Fix max-lines-per-function correct code's max value (#10513) (Rhys Bower)\n\n\n\nCommits\nThe new version differs by 4 commits.\n\ncaeb223 5.0.1\n125dc34 Build: changelog update for 5.0.1\n196c102 Fix: valid-jsdoc should allow optional returns for async (fixes #10386) (#10480)\n4c823bd Docs: Fix max-lines-per-function correct code's max value (#10513)\n\nSee the full diff\n\n. - The devDependency mocha was updated from 5.2.0 to 6.0.1.\nUpdate to this version instead \ud83d\ude80 \n\nRelease Notes for v6.0.1\n6.0.1 / 2019-02-21\nThe obligatory round of post-major-release bugfixes.\n\ud83d\udc1b Fixes\nThese issues were regressions.\n\n#3754 - Mocha again finds test.js when run without arguments (@plroebuck)\n#3756 - Mocha again supports third-party interfaces via --ui (@boneskull)\n#3755 - Fix broken --watch (@boneskull)\n#3759 - Fix unwelcome deprecation notice when Mocha run against languages (CoffeeScript) with implicit return statements; returning a non-undefined value from a describe callback is no longer considered deprecated (@boneskull)\n\n\ud83d\udcd6 Documentation\n\n#3738 - Upgrade to @mocha/docdash@2 (@tendonstrength)\n#3751 - Use preferred names for example config files (@Szauka)\n\n\n\nCommits\nThe new version differs by 9 commits.\n\n6d3d6b4 Release v6.0.1\n2146ece update CHANGELOG.md for v6.0.1\n7c9221d backout deprecation of value returned from suite; closes #3744\nb7cfceb fix --watch not finding any files to execute; closes #3748\nb836d73 Upgrade docdash version - issue #3663\n7926f47 fix --ui issues, closes #3746\n00f2ed9 dev dep upgrades from \"npm audit\" and \"npm upgrade\"\n34afb1a fix(cli/run.js): Revert default glob to match Mocha-5.2\n6d5a0db Bring the example congfiguration file in line with the documentation. (#3751)\n\nSee the full diff\n\n. - The devDependency mocha was updated from 5.2.0 to 6.0.2.\nUpdate to this version instead \ud83d\ude80 \n\nRelease Notes for v6.0.2\n6.0.2 / 2019-02-25\n\ud83d\udc1b Fixes\nTwo more regressions fixed:\n\n#3768: Test file paths no longer dropped from mocha.opts (@boneskull)\n#3767: --require does not break on module names that look like certain node flags (@boneskull)\n\n\n\nCommits\nThe new version differs by 6 commits.\n\n00a895f Release v6.0.2\n1edce76 update CHANGELOG for v6.0.2 [ci skip]\n347e9db fix broken positional arguments in config; ensure positional args are unique; closes #3763\n9e31e9d fix handling of bareword args matching node flags; closes #3761\n6535965 Update \"karma-browserify\" to eliminate Karma middleware warning (#3762)\n37febb6 improve issue template. (#3411)\n\nSee the full diff\n\n. ",
    "AriLFrankel": "Thanks for this, @dmfay\nWhat sorts of functionality depend on primary keys?\nSounds like a quick solution for us would be to simply add primary key indexes to the join tables that don't have them already.. ",
    "tony-gutierrez": "It wont accept the password. I'll look into the other libraries. . No change. Can I specify any pool options while using connection string? Will it have pool by default? . Any attempt to escape the user name results in \"Invalid Username specified. Please check the Username and retry connection. The Username should be in username@hostname format.\"\nI tried uriencoding the pw, no change. Nothing is encoded in the connection string that works. \nAs a side note, a variety of the dependencies cannot be webpacked, which is often done for azure functions, as azure is shitty slow at spawning new servers with a lot of small files. . Yeah, but this is shitty Microsoft we are dealing with, and they cant just do things the normal way. \nThe connection string that works contains the user@host:pw@fullhost syntax. Don't ask me why they do it this way. . ",
    "hammady": "I had the same exact issue with Azure Postgres in my ruby code. I fixed it by replacing the @ in the username by its hexadecimal representation as in: user%40service. I got this idea after inspecting the URI RFC-3986 and finding that the username can have such hexadecimal encodings.. ",
    "jaimesangcap": "Thank you for responding. Yes I'm using Koa on my example. I think the async/promise makes it tricky to make a singleton, no? Even if I have to make getDatabase lets say, I need to do the operations in await/then part.\n```\n// not sure how to do singleton with async :)\nconst getDatabase = () => {\n    return await massive('connection')\n}\nexport createTask = async (task, getDatabase) => {\n  return await getDatabase().then(db => db.saveDoc('tasks', task)\n}\n```\nAny thoughts on this one?. @vitaly-t thanks for the reference. I was hoping massive supports it though. @vitaly-t by JSON you mean stored as Documents (jsonb)? Unfortunately I'm using massive as document store.. @dmfay cool thank you. Looks the same of what I have now. I think I should close this one already. Would you mind if I ask I what are on your list now?. Cool, I think you're using database views and scripts quiet heavily?\nFor customizing the table, I'm thinking of adding table/documents property to the database options. options will have default settings for all documents, but enable overriding per document using the document name as the property/key. Then maybe adjust the document-table.sql depending on the options.\noptions.documents['tasks'] = {\n// some key => value settings\n}\nWhat do you think? \nPS: I haven't really done any open source project before and maybe this project is not a good place for me to start with as a newbie, please let me know.. great! I still have more code reading to do though before getting started. And some tests are failing on my machine. I'm using the docker dmfay/pg-massive\n```\n443 passing (5s)\n  2 pending\n  5 failing\n1) connecting variations connects with undefined connections using default configuration:\n     error: role \"jaime\" does not exist\n      at Connection.parseE (/Users/jaime/Projects/massive-js/node_modules/pg/lib/connection.js:546:11)\n      at Connection.parseMessage (/Users/jaime/Projects/massive-js/node_modules/pg/lib/connection.js:371:19)\n      at Socket. (/Users/jaime/Projects/massive-js/node_modules/pg/lib/connection.js:114:22)\n      at emitOne (events.js:115:13)\n      at Socket.emit (events.js:210:7)\n      at addChunk (_stream_readable.js:266:12)\n      at readableAddChunk (_stream_readable.js:253:11)\n      at Socket.Readable.push (_stream_readable.js:211:10)\n      at TCP.onread (net.js:585:20)\n2) connecting variations connects with empty connection block using default configuration:\n     error: role \"jaime\" does not exist\n      at Connection.parseE (/Users/jaime/Projects/massive-js/node_modules/pg/lib/connection.js:546:11)\n      at Connection.parseMessage (/Users/jaime/Projects/massive-js/node_modules/pg/lib/connection.js:371:19)\n      at Socket. (/Users/jaime/Projects/massive-js/node_modules/pg/lib/connection.js:114:22)\n      at emitOne (events.js:115:13)\n      at Socket.emit (events.js:210:7)\n      at addChunk (_stream_readable.js:266:12)\n      at readableAddChunk (_stream_readable.js:253:11)\n      at Socket.Readable.push (_stream_readable.js:211:10)\n      at TCP.onread (net.js:585:20)\n3) connecting variations connects with empty connection strings using default configuration:\n     error: role \"jaime\" does not exist\n      at Connection.parseE (/Users/jaime/Projects/massive-js/node_modules/pg/lib/connection.js:546:11)\n      at Connection.parseMessage (/Users/jaime/Projects/massive-js/node_modules/pg/lib/connection.js:371:19)\n      at Socket. (/Users/jaime/Projects/massive-js/node_modules/pg/lib/connection.js:114:22)\n      at emitOne (events.js:115:13)\n      at Socket.emit (events.js:210:7)\n      at addChunk (_stream_readable.js:266:12)\n      at readableAddChunk (_stream_readable.js:253:11)\n      at Socket.Readable.push (_stream_readable.js:211:10)\n      at TCP.onread (net.js:585:20)\n4) reload \"before all\" hook:\n     error: role \"jaime\" does not exist\n      at Connection.parseE (/Users/jaime/Projects/massive-js/node_modules/pg/lib/connection.js:546:11)\n      at Connection.parseMessage (/Users/jaime/Projects/massive-js/node_modules/pg/lib/connection.js:371:19)\n      at Socket. (/Users/jaime/Projects/massive-js/node_modules/pg/lib/connection.js:114:22)\n      at emitOne (events.js:115:13)\n      at Socket.emit (events.js:210:7)\n      at addChunk (_stream_readable.js:266:12)\n      at readableAddChunk (_stream_readable.js:253:11)\n      at Socket.Readable.push (_stream_readable.js:211:10)\n      at TCP.onread (net.js:585:20)\n5) reload \"after all\" hook:\n     TypeError: Cannot read property 'instance' of undefined\n      at Context.after (/Users/jaime/Projects/massive-js/test/database/reload.js:16:17)\n      at callFn (/Users/jaime/Projects/massive-js/node_modules/mocha/lib/runnable.js:223:21)\n      at Hook.Runnable.run (/Users/jaime/Projects/massive-js/node_modules/mocha/lib/runnable.js:216:7)\n      at Hook.Runnable.run (/Users/jaime/Projects/massive-js/node_modules/co-mocha/lib/co-mocha.js:43:16)\n      at next (/Users/jaime/Projects/massive-js/node_modules/mocha/lib/runner.js:259:10)\n      at Immediate._onImmediate (/Users/jaime/Projects/massive-js/node_modules/mocha/lib/runner.js:276:5)\n      at runCallback (timers.js:781:20)\n      at tryOnImmediate (timers.js:743:5)\n      at processImmediate [as _immediateCallback] (timers.js:714:5)\n```. ",
    "hjsu": "Submitted a pr: https://github.com/dmfay/massive-js/pull/508. actually @dmfay i'm wondering if this implementation is correct; this would be a list of separate db updates/insertions rather than a batch insert right? . @hjsu note to self, need to add tests for erorr cases.. ",
    "muguang-lijing": "I tested it locally. I've fixed that internal bug. In fact, that internal error is a strict syntax check. It can also run but the 'const' declaration have better performance. ",
    "mike-engel": "@dmfay thanks for the quick response! I won't be able to get to it until tonight either, but I'm more than happy to make an attempt. It sounds pretty easy, so I'll see if I can throw a PR together tonight \ud83d\ude04 . @dmfay Thanks so much for taking the time\u2014it works perfectly! I, for some reason, had the column's backwards (key was value, value was key) which caused some confusion. Reading the docs again though it makes sense.\nAs for the bit about needing id in the columns object, it wasn't necessary unless I wanted to return the id through the api, which I don't need to right now. So, leaving it off was actually ok!\nIf you are ever in need of any eyes on stuff or feedback, let me know. I'm very excited about this project. I'll also keep an eye on issues I think I could tackle to help out where I can \ud83d\ude04 . Thanks for the release! I think the amount of documentation is actually good. I have noticed a couple of broken links, so I'll try and go through and fix those. I think the only thing I'd change is have a consistent way to document the project. It seems like it's split between two documentation systems? One is more user friendly and one is more technical. Maybe that's ok though?. I think it's definitely worth it. Documentation is hard, but has the most impact \ud83d\ude09 . Thanks! Sorry for the grammar mixup. The case I have right now, at least, is probably more complicated but could be broken down. To start, I have two sets of relational data to insert, accounts and transactions. Accounts is an array, so that's already a bulk add, which doesn't work with deep insert (yet?). Then the transactions need to be added/upserted in bulk, but I only have the account name, not the primary key to those accounts. This is a pretty complex case, but having the ability to do inline CTEs like above would be nice, if a nice API could be designed for it. I can, of course, do this as a raw query, but that's no fun \ud83d\ude09 . No worries! And I totally understand the part about limiting vulnerable entry points. Like I said, my use case is pretty complicated. What I ended up having to do is something like so which is horribly underperformant, but I'm prototyping and am lazy \ud83d\ude1b \n```js\nconst accountsByPlaidId = groupBy(prop(\"plaidId\"), newAccounts);\nconst formatTransactions: (\n  transactions: ReadonlyArray\n) => any = pipe(\n  map((transaction: PlaidTransaction) => ({\n    ...transaction,\n    plaid_id: transaction.transaction_id,\n    item_id: item.id,\n    account_id: (accountsByPlaidId[transaction.account_id][0] as Account)\n      .id\n  })),\n  map(\n    pick([\n      \"plaid_id\",\n      \"amount\",\n      \"category\",\n      \"date\",\n      \"location\",\n      \"payment_meta\",\n      \"name\",\n      \"pending\",\n      \"transaction_type\",\n      \"account_id\",\n      \"item_id\"\n    ])\n  )\n);\nconst upsertTransactions = formatTransactions(\n  transactions as ReadonlyArray\n);\nawait Promise.all(\n  upsertTransactions.map((t: any) =>\n    db.query(\n      insert into\n      transactions\n      (id, plaid_id, amount, category, date, location, payment_meta, name, pending, transaction_type, account_id, item_id)\n      values\n      (DEFAULT, $\\{plaid_id\\}, $\\{amount\\}, $\\{category\\}, $\\{date\\}, $\\{location\\}, $\\{payment_meta\\}, $\\{name\\}, $\\{pending\\}, $\\{transaction_type\\}, $\\{account_id\\}, $\\{item_id\\})\n    on conflict (plaid_id) do update set\n      amount = excluded.amount,\n      category = excluded.category,\n      date = excluded.date,\n      location = excluded.location,\n      payment_meta = excluded.payment_meta,\n      name = excluded.name,\n      pending = excluded.pending,\n      transaction_type = excluded.transaction_type,\n      t\n    )\n  )\n);\n```. @vitaly-t I knew that it was incredibly inefficient, but the way I was planning to fix that was much more verbose. Your solution is great, thanks for providing that! It might be useful to have that in the curated docs somewhere (couldn't find it) rather than just the JSDocs. ",
    "terehov": "Hi Dean, \nthat sounds great. afaik there is no substantial difference between tables and updatable views.\nWould that be a big rewrite, or more or less just renaming and throwing together (tables and views)? \nThanks for mentioning PostGraphile. We were following PostgREST, PostGraphQL (PostGraphile) for quite a while and evaluated them for our use cases. They are both great frameworks and I admire what this guys are putting together. In fact, they gave us the initial idea to think this way, rather than keeping everything in Node, like pretty much every other framework out there. \nHowever there is a number of things that we need / decided to do differently (from an architectural point of view). And one of them is exactly the reason, why we need a lightweight ORM-like API on the server, that is not an ORM (I feel the same way about ORMs as you do). \nSo even-though we think they are doing an amazing job, we decided to do some things differently and are quite far already. So it was probably a similar decision, that made you create massive instead of using one of the other great ORMs like TypeORM out there :-)  . ",
    "nireno": "Looks like postgres lists rows with null columns last when it left joins so the actual query produces:\n| user_id | username | test_id | name                      | id | description     | \n|---------|----------|---------|---------------------------|----|-----------------| \n| 3       | \"carol\"  | 2       | \"carol's first test\"      | 2  | \"carol's issue\" | \n| 3       | \"carol\"  | 3       | \"carol's second test\" |    |                 | \nbut the decompose was reordering it.\n. ",
    "samjeffress": "Thanks for the quick response @dmfay \nMy first problem is that I was using v3 :)\nThe issue I have is that my keys are not autogenerated, they're domain id's that will always be passed in, irrespective of a row existing in the database, so the check to determine if the object is new or not doesn't work for my scenario - https://github.com/dmfay/massive-js/blob/v4.6.4/lib/table.js#L145. \nThe following test breaks:\nit('saves a new record if passed both keys', function () {\n    return db.compoundpk.save({\n      key_one: 333,\n      key_two: 666,\n      value: 'I\\'m new here'\n    }).then(res => {\n      assert.isOk(res);\n      assert.equal(res.key_one, 333);\n      assert.equal(res.key_two, 666);\n      assert.equal(res.value, 'I\\'m new here');\n    });\n  });\nI'm not sure what the solution is, besides to do a check to make sure the row is actually in the db - my code currently has table.findOne({id1, id2}).. Sweet, thanks for the help!\nI've created an upsert function using the onconflictignore option you've mentioned. . Added a pr - https://github.com/dmfay/massive-js/pull/551. I went with deepInsert as the option name because that's talked about in doco rather than the junctions. Pleasure :) thanks for your quick feedback \ud83d\udc4d . ",
    "flux627": "Yes, I did. Sorry, had both tabs open!. @vitaly-t, just to clarify- you say \"... without those you would run into all sorts of trouble ... . Here's just one example\". Then, the example issue you link does not seem to be resolved. Is that issue resolved by the things you mentioned pg-promise does automatically?. ",
    "ferdinandsalis": "I really would love to utilise task and tx from massive. Anything I can do to help facilitate this \u263a\ufe0f. > What's missing is the ability to work with the API (db.mytable.insert(...)) in a transaction\nThat is what I was referring to. I am interested though I am sure I lack the knowledge and direction. Any directions would be very helpful. Will start by reading up on the current implementation. Thanks. @dmfay awesome, will take a look!. Hi @dmfay. Yes I did try it out but not enough to judge if it works for me.\nOn a different note. I am currently in the middle of a big refactor and I really want to use massive but I am deploying with \u2018up\u2019 to aws lambda, its stateless and every request will incur the cost of the introspection query. Is there any way to mitigate this (some way of persisting the result).\nIf not I would just love to use the query builder part of massive. Dont know if this even makes sense. I just dont want to use knex, bookshelf.js, sequelize, et al. Also I am a fan of pg-promise.. @dmfay I will try doing this. Thanks for your input everyone.. ",
    "mdbiundo": "Ah, ok I hadn't come upon a need to pass a dynamic number of arguments in this way before, so I didn't think to try it! Thanks for the guidance.\nIn case someone else runs into this issue and comes across this post, the following worked for me:\nvar arr = ['four', 'five'];\ndb.teststrings.apply(this, _.concat('one', 'two', 3, arr));. ",
    "Zolmeister": "proposal: deprecate update({where}, {to}, {options}) in favor of new method updateAll({where}, {to}, {options}) due to the possibility of field conflicts between {to} and {options}. Yes. It was a mistake (the testing environment table was wrong), but debugging it was painful because of the opaque error. To clarify, I'm proposing the default massive promise behave like this proxy object. It's side-effect free\nOtherwise you have to explicitly mutate a global instance or pay the resolution cost on each usage\njs\n// this everywhere\ndb.then (db) -> ...\n// or\n// pg.js\nmodule.exports = {db: null}\n// setup.js\nmassive().then(db => pg.db = db) // side-effect. ",
    "stubar": "done https://github.com/dmfay/massive-js/pull/549. ",
    "vibl": "Thanks!\nAnd thank you for Massive, which looks impressive (just starting with it).\nAbout GitBook, I've just realized it wouldn't make much of a difference. I thought it allowed public editing, but in reality only collaborators can edit pages.\nIt might still be a good option for other reasons (full text search, many plugins dedicated to documentation, coded in JS...), but I imagine you have better things to do :-). Thanks! I think it would be useful to include it in the doc.. ",
    "mjacobson7": "Thank-you for the quick response.  However, the issue only happens when the supervisor is null.  If I am doing a GET request for a user who has a supervisor, I get the nested object back exactly as expected. \nThe issue occurs when the user does not have a supervisor, so the LEFT JOIN is joining the user with the supervisor columns which are NULL. . That makes sense.  Now how would you recommend I get users that have a supervisor as well as those that don't?  Would it be easier to just do two separate queries; one for the user, then one to get their supervisor if it exists? . Appreciate it! It does look like it's working.  I may end up making separate requests because renaming the properties for the supervisor fields will cause some other issues in my particular application, but I'm glad I was able to get your help. \nThanks again!. ",
    "atistler": "Looking through this, I am not sure that automatic casting is happening on standard queries: for example:\n```\ndb.instances.find({'created_at <': new Date()})\nLOG: statement: SELECT * FROM \"instances\" WHERE \"created_at\" < '2018-04-08T23:50:25.314-04:00' ORDER BY \"id\"\n```\nI looked at docGenerator/tableGenerator in where.js and it looks like value casting is only done in docGenerator and not for non 'doc' based predicates (tableGenerator).\nI did attempt to modify mutators.js to handle Date values, however i ran into quoting issues:\n``\nexports.literalizeArray = condition => {\n    if (_.isArray(condition.value)) {\n    const sanitizedValues = condition.value.map(function (v) {\n      if (_.isString(v) && (v === '' || v === 'null' || v.search(/[,{}\\s\\\\\"]/) !== -1)) {\n        return\"${v.replace(/([\\\"])/g, '\\$1')}\"`;\n      } else if (v === null) {\n        return 'null';\n      }\n  return v;\n});\n\ncondition.params.push(`{${sanitizedValues.join(',')}}`);\n\n} else if (_.isDate(condition.value)) {\n    condition.params.push('${condition.value.toISOString()}'::timestamptz)\n  } else {\n    condition.params.push(condition.value);\n  }\ncondition.value = $${condition.offset};\nreturn condition;\n};\n```\n```\ndb > db.instances.find({'sys_period @>': new Date()})\nerror: malformed range literal: \"'2018-04-09T03:53:16.838Z'::timestamptz\"\nLOG: statement: SELECT * FROM \"instances\" WHERE \"sys_period\" @> '''2018-04-09T03:53:16.838Z''::timestamptz' ORDER BY \"id\"\n```\nWill try to dig into this more.. ",
    "dmitriz": "Thank you for your answer. It is good to hear it is not intentional ;)\nI have been trying to understand the best way to deal with it,\nand perhaps this option represents a special case of the map transformer?\nThen passing the empty array with fields:[] would correspond to mapping the result array over x=>{}, so perhaps the correct result should be the array of empty objects?\nThere seem to be very few tests for this option, e.g. here only testing for the length of the result array, where the whole table seems to have only one row, and the testing is needed actually for the fields returned inside the rows:\nhttps://github.com/dmfay/massive-js/blob/master/test/queryable/find.js#L403\nThus I'm not sure what could be a good PR in that direction that will also be accepted ;). I see, thank you, that other test looks more like a test for how things work internally. Not knowing about these, I would feel better to only look for the final effects acting directly on the database and bypassing any SQL.\nI am also a bit uncertain from where the User table gets initialized inside the test (it is not visible from the test file). I would be interested to maybe have a new table populated with more data for more thorough tests in that place, and ideally explicitly require the file initializing the table to make it instantly visible.\nIt would make things easy but I wonder whether such changes would be acceptable.\n. I've made this test PR (to master by mistake for now), just to see if you would accept something like that in principle:\nhttps://github.com/dmfay/massive-js/pull/584\nHere are the file changes:\nhttps://github.com/dmfay/massive-js/pull/584/commits/7b1e6a1e02245fa751be00d56bc44bd79660fca7. There are no tests for errors here:\nhttps://github.com/dmfay/massive-js/blob/master/test/queryable/find.js\nAre they located somewhere else?\nWhat style is used for testing errors inside promises?. Couldn't get it working, still throwing errors:\nhttps://github.com/dmitriz/massive-js/tree/v5-test-polish1\n```sh\n  1) find\n       casing issues\n         throws for empty subset of fields:\n  AssertionError: expected 'assert.fail()' to equal 'The \"fields\" array cannot be empty'\n  + expected - actual\n\n  -assert.fail()\n  +The \"fields\" array cannot be empty\n\n  at db.Users.find.then.catch.err (test/queryable/find.js:426:30)\n  at <anonymous>\n  at process._tickCallback (internal/process/next_tick.js:188:7)\n\n``\n. I've tried tothrow` there instead but that lead to hundreds of failed tests, essentially due to this line: https://github.com/dmitriz/massive-js/blob/v5-test-polish1/lib/statement/select.js#L51\nSo I have tried to put it here instead:\nhttps://github.com/dmitriz/massive-js/commit/6d3ce25a5cd037b8fcb72005c4443771cc0f0804#diff-1b81dc4c5fe2d223e0790ef2ccc192eeR75\nI don't know why the test is failing.. Please see here: https://github.com/dmfay/massive-js/compare/v5...dmitriz:v5\nThe error is added but it does not throw in the test, I don't know why.\n. You are right of course, too much category theory can make one blind ;)\nSo the test is failing now as expected, which is good ;)\nWhat is not so good, the error doesn't get caught with this:\njs\n    it('throws error when field array is empty', function () {\n      return db.Users.find({}, {fields: []}).then(res => assert.fail(\"Should not show up\"))\n        .catch(err => assert.ok(err));\n    });\nI don't have much experience with Mocha used that way, so not sure what is the right way of testing the failure here.\nHere is the branch with this test:\nhttps://github.com/dmitriz/massive-js/tree/v5-fields\n. I have managed to get the failure tested on this branch: https://github.com/dmitriz/massive-js/commits/failure-test-pass\nHere is the commit:\nhttps://github.com/dmitriz/massive-js/commit/b58cde170198b34f50dab4edfc9c3e09fb48b3cc\nThe test message is made intentionally different for now to see that it is actually caught.\nAt the end, we probably just should test for the failure, not the specific message.\nHowever, when I move the throw statement to Database.query, the same failure test passes:\nhttps://github.com/dmitriz/massive-js/commits/failure-test-in-database\nSo does it mean, that might not be the right place?. Ah yes, destructuring is nice. \nAnd sorry for the wrong branch, will have to change the Github's default next time.. ",
    "AiDirex": "I found in pg-promise's doc that it can be fixed by doing this\nlet bar = [{\n    aaa: 1,\n    bbb: 2\n}, {\n    aaa: 1,\n    bbb: 2\n}]\ndb.foo.insert({\n    bar: bar,\n    rawType: true,\n    toPostgres = function () {\n        return db.pgp.as.json(this, false)\n    }\n})\nBut I think it is a better idea to have some way of providing formatting in queries for pg-promise.. ",
    "FGRibreau": "Indeed, as you have seen I edited the code sample, the table name was in\nfact \"signature\"\n\u1427\n. ",
    "jaecktec": "also I have tried:\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public to u_1\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO u_1\nIt ended up in\nSchema |      Name      | Type  | Access privileges | Column privileges | Policies\n-------+----------------+-------+-------------------+-------------------+----------\npublic |    my_table    | table |  g_1/u_1          |                   |\nbut still does not work :(. Proposal (can create a pull request, but I am not very experienced in postgres)\nAdd another union to the tables.sql\nSELECT t.table_schema AS schema,\n      t.table_name AS name,\n      NULL AS parent,\n      NULL AS pk,\n      CASE t.is_insertable_into WHEN 'YES' THEN TRUE ELSE FALSE END AS is_insertable_into,\n      array_agg(c.column_name::text) AS columns\n    FROM information_schema.tables t\n    JOIN information_schema.columns c\n      ON c.table_schema = t.table_schema\n      AND c.table_name = t.table_name\n    where t.table_schema not in ('pg_catalog', 'information_schema')\n     and t.table_schema not like 'pg_toast%'\n    GROUP BY t.table_schema, t.table_name, t.is_insertable_into\ndo you have any objections with this?. Ok cool, I'll try that tomorrow!. Ok I am 100% confused. \nWhen I am doing a ssh tunnel from my macbook into the server and tunnel the database port to my local machine I have access to all of the schemas. However on the server (linux) it is not working... \nFeels like this is not a massive.js issue... But anyways help would be awesome \ud83d\ude48\nNode version (local and remote) is 9.11.1\nThings that are different on the server:\n\nNode Modules is not within the application folder but on a different path. \nMac has case a case insensitive filesystem\n\n. Ok I could solve it but I couldn't find a solution. \nI deleted all the tables and created a new database instance (vm)\ncreated all the tables with a static user and handed it over to the shared group\nThen it was working. \nProbably it had something to do with the case sensitivity of unix / insensitivity of mac... \nI don't know.\nHowever thanks for the suggestions and helps, I learned some things on the way!. ",
    "johncmunson": "Very cool, thanks! Definitely going to spend some time studying Toby's model. I'm working on a TodoMVC application that I hope will demonstrate all of the most essential features in an architecture like this, and will likely be incorporating Massive. It will almost certainly end up being way over-engineered for the type of app that it is, but that's kinda the point.\nDown the line, would you be open to a peer review and possibly a PR to the docs demonstrating how Massive could be used for ES/CQRS?. Thanks!!. ",
    "rijnhard": "Not saying it should be done immediately, but it is a nice usability\nenhancement for the future\nOn Fri, 29 Jun 2018, 20:52 Vitaly Tomilov, notifications@github.com wrote:\n\nThe syntax shown in the example requires Node.js v9.2+\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/dmfay/massive-js/issues/613#issuecomment-401442866,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AEF36Frr26D63dfRPI2-dUkgWj0XWTsfks5uBndogaJpZM4U9DLT\n.\n. Thank you for the feedback @dmfay\n\n@vitaly-t should I open a feature request on pg-promise?\nEdit: about the only thing I haven't tried to figure out is if a standard (not async) iterator will be sufficient, but I doubt it.  . @vitaly-t I may be missing the boat, so excuse me if this is a dumb statement. But I had a look at pg-promise and how it handles generators and thats mainly in the transaction/task area, I'm talking about having an alternative to the stream API, because it has the same use case as the stream API, in that it allows you to handle large datasets with backpressure just like streams. The only real difference is that it's a much nicer API to work with, where streams get clunky and error-prone quickly.. @vitaly-t, @dmfay this may be asking for a bit much, but I'm struggling to figure out how to use sequences in say this example massive-js#search, could you give me an example? . TBH at this rate I think it's probably just going to be easier if I map the stream to a generator on my own, kind of how es6-stream does it (but in a nicer way) and maybe find a way to upstream that with a similar structure to node-pg-query-stream.. So for completeness and anyone else looking to make this work:\nI decided stuff it and forked node-pg-query-stream into node-pg-query-iterator and got about 70% of the way with it (with some babel issues) and then came across this gem and tweaked it into this:\n```javascript\n/\n * Converts a stream to an async iterator\n * @template T\n * @param {ReadableStream} stream\n * @return {Promise|Error>}\n * @see https://jakearchibald.com/2017/async-iterators-and-generators/\n * @todo figure out how it handles stream error events?\n /\nexport async function streamToIterator(stream) {\n    // Get a lock on the stream\n    const reader = stream.getReader();\ntry {\n    // initial read so that we can error handle better\n    let { done, value } = await reader.read();\n\n    while (!done) {\n        yield value; // chunk\n\n        // Read from the stream\n        ({ done, value } = await reader.read());\n    }\n    reader.releaseLock();\n} catch (e) {\n    reader.releaseLock();\n    // on an error the iterator does stop.\n    throw e;\n}\n\n}\n```\nBut then found this stream_readable_symbol_asynciterator\nSo I dont think any of this is needed, just sitback and wait for it to be adopted, and if you cant wait, just use the transformer above (I am using the transformer for now). ",
    "lfurzewaddock": "Notes for commit 16299946fbfa6c74ef994296a4f0c0983f7fcd13\nWe cannot create extension \"uuid-ossp\" in a query file without giving the database account used by the app, superuser privileges.\nA better/more secure way is to use a superuser account to create the extension for the whole database, so the extension is available to the app, without it requiring superuser access.\nThese are the steps I followed;\n```\n$ psql -d postgres -h localhost -p 5432 -U postgres\npsql (10.4 (Ubuntu 10.4-2.pgdg16.04+1))\nType \"help\" for help.\npostgres=# \\c YOUR_DB_NAME;\nYou are now connected to database \"YOUR_DB_NAME\" as user \"postgres\".\nYOUR_DB_NAME=# CREATE EXTENSION \"uuid-ossp\";\nCREATE EXTENSION\nYOUR_DB_NAME=# \\q\n```. Thanks @dmfay for taking the time to review my rough PR!\n\nYeah, you can't run PLPGSQL flow control statements outside the context of a function. It should be possible to avoid duplicating the script using pg-promise's raw text interpolation though! \n\nOK, I've manged as you said to alter the original doc table script to take a column data type name and a raw value for the default statement. Internally, I'm using a switch to generate the default statement, to mitigate the use of a raw value here.\nI had some problems removing the CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\"; from the doc table script, which you may have noticed from a previous commit/comment requires the app to have superuser privileges, which for me is a show stopper (although would pass tests as they are executed using the postgres account). \nI realised the global.resetDb was probably removing the uuid-ossp extension I had previously added manually to the db following the steps;\n```\n$ psql -d postgres -h localhost -p 5432 -U postgres\npsql (10.4 (Ubuntu 10.4-2.pgdg16.04+1))\nType \"help\" for help.\npostgres=# \\c YOUR_DB_NAME;\nYou are now connected to database \"YOUR_DB_NAME\" as user \"postgres\".\nYOUR_DB_NAME=# CREATE EXTENSION \"uuid-ossp\";\nCREATE EXTENSION\nYOUR_DB_NAME=# \\q\n```\nTherefore, I added a new db.createExtension method and call this inside global.resetDb which seems to work. I'm not a 100% sure if you will find this acceptable or agree with the implementation. Please advise if there is a better way that does not require the app to connect to the db with superuser privileges.\n\nv1mc UUIDs are more efficiently indexed than v4 UUIDs so the default should be uuid_generate_v1mc() for best performance\n\nI have some concerns relating to setting a default to something less secure, although I've just noticed you advise v1mc, not v1 (which the tests are currently running). I would expect most people including me ;-) to not properly investigate the options and simply leave the default as is, ignorant of any implications. However, to accommodate, I've added a new setting verUUID which makes choosing the preferred UUID type easy and perhaps we could make use of the uuid_nil which generates a UUID only containing zeros for testing.\nQuick ref: Which UUID version to use?\n\nthe option name is a bit verbose: documentPkType gets the point across imo. Just default that to serial and make sure to set the default when it's uuid\n\nDone.\n\ninstead of changing all document tables created during testing and dealing with all the fallout from that, just add a single new test to test/connect.js that sets documentPkType, creates a document table, and verifies that a new record has a UUID primary key\n\nI've copied the test file /test/database/saveDoc.js, renamed the copy to /test/database/saveDoc-uuid.js\nIn the new file I have replaced the two previously failing tests asserting id to be 1 with regex tests to validate if the id is a valid UUID.. OK @dmfay, getting close - touching less files!\nExtension\nAgreed: db.createExtension and now db.dropExtension are present to support tests and constraints created by the test environment. \n\nYou don't need to then off this.query though, since that already returns a promise. Otherwise \ud83d\udc4d\n\nOops and done. \nUUID versions\n\nverUUID is on the obtuse side; uuidVersion would be clearer.\n\nDone.\n\ngetDefaultSQLForUUID expects users to read the uuid-ossp docs to find out which particular function name they want; I'd rather hide that and offer v1, v1mc, and so on to make that more straightforward. \n\nThere may be a misunderstanding here, probably due the chosen function name. I've simplified the uuidVersion arguments required, if that's what you meant? To clarify; its purpose is to set the constant containing the SQL string/statement to be inserted directly into the raw placeholder within the query file. It protects against typos, injection and sets the default (recommended) setting. Currently it is set to v4 - please confirm which version you want? It does not require any input from devs leaving it set to the default, but accommodates devs who have done some research and wish to prioritise performance over security etc.\n\nThe documentation (docs/documents.md) needs to explain how to use UUID keys and list the available types too, and a note about using v4 for randomness vs v1mc for performance would also be good to have.\n\nI will look at the docs and add some notes/links, but would like the code implementation agreed first in case there are any further changes.\n\nThe constant-returning functions like uuid_nil and the uuid_ns_* set are essentially useless for primary keys so there's no need to include those. Nil is sometimes useful in testing but in my experience it's been used to pregenerate a single entity's PK; there's no point in hardcoding a table-level default to straight zeroes.\n\nOK, I've removed uuid_ns_* options, but left uuid_nil for potential testing purposes. I can also remove that if your prefer?\nTests\n\nsaveDoc is an extra degree out from where the action's happening. What this needs is a new test in test/database/createDocumentTable.js that ensures invoking createDocumentTable with the UUID key configuration really does create a table that sets UUID keys. \n\nOK, I've copied 2 existing tests in test/database/createDocumentTable.js and modified them to use UUID config.\n\nCreate a table, save a document, check the key the way you've already got.\n\nOK, I've deleted test/database/saveDoc-uuid.js and copied 2 existing tests in test/database/saveDoc.js, modified them to use UUID config, and test the automatically generated UUID is valid.\n. > There are good arguments for both v4 and v1mc as the default so I'm fine with v4 winning that one.\nOK, agreed. I think it's the safest/best option for a default/recommended setting.\n\nI did just have the thought that it could potentially be reduced to a single configuration parameter -- so if you set (spitballing) documentPkType = v4 it's a v4 UUID, with the default still being SERIAL. \n\nIMO, its best to leave it as documentPkType = 'uuid'. That will make more sense to devs who may not even know there are different uuid variations to choose from. Also, there are other pg extensions other than 'uuid-ossp' which may require alternate variation names.\n\nuuid_nil just isn't useful for testing: if you set that as the table-level default, you can only ever insert one record without having to specify a pk value yourself, which defeats the purpose of defaults. So let's pull that one out too.\n\nDone.\n\nAs far as the tests: what you've got in the createDocumentTable tests now confirms that the table exists, but doesn't actually test the PKs of a stored record. If you save a document and test for a UUID key there, I don't think you need to touch the saveDoc tests at all -- the createDocumentTable tests will have proved it out sufficiently.\n\nI'm slightly concerned by adding saveDoc to createDocumentTable tests I am mixing otherwise separated concerns, but accept your judgement, so it's done.. Woohoo!. @cpainterwakefield - I can thoroughly recommend Kyle Simpson's JS courses on Frontend Masters and his series of books: You Don't Know JS, which are available free on GitHub and published through O'Reilly.. ",
    "devlemire": "More Context\nI'm calling this utility function (startChargeTransaction ) from a method that gets called when hitting /api/payments/class_deposit. The method looks like this:\n```js\n  pay_class_deposit: async (req, res) => {\n    try {\n      // Get the db instance from app\n      const db = req.app.get('db')\n      // Get the stripe instance from app\n      const stripe = req.app.get('stripe')\n      // Get the user_id of the logged in student\n      const { user_id } = req.user\n      // Get the required payment information from the request body\n      const { token, cost, class_session_id } = req.body\n      // Get the most up to date email for the logged in user\n      const [{ email }] = await db.get_user({ user_id })\n      // Get the user's salesforce id from the enrollments table\n      const [{ sf_id }] = await db.get_salesforce_id({\n        user_id,\n        session_id: class_session_id\n      })\n  // Format the Stripe charge description\n  const description = `Class deposit for ${email}`\n\n  // Call the startChargeTransaction utility function\n  await payments_utils.startChargeTransaction(db, stripe, {\n    token,\n    cost,\n    email,\n    user_id,\n    description,\n    column_to_update: 'has_paid_deposit',\n    class_session_id,\n    sf_id\n  })\n\n  res.sendStatus(200)\n} catch (err) {\n  console.error('pay_class_deposit method failed in payments_controller.js:', err)\n  res.status(500).send(err)\n}\n\n}\n```\nThe server logs\nstudent-hub-node-container | The record has the following Stripe ID: ch_1Cmngk228emtMZTzIFD96ADv\nstudent-hub-node-container | The record has the following description: Class deposit for james.lemire.programmer@gmail.com\nstudent-hub-node-container | startChargeTransaction failed in payments_controller.js: { error: relation \"enrollments\" does not exist\nstudent-hub-node-container |     at Connection.parseE (/var/server/node_modules/pg/lib/connection.js:553:11)\nstudent-hub-node-container |     at Connection.parseMessage (/var/server/node_modules/pg/lib/connection.js:378:19)\nstudent-hub-node-container |     at Socket.<anonymous> (/var/server/node_modules/pg/lib/connection.js:119:22)\nstudent-hub-node-container |     at Socket.emit (events.js:180:13)\nstudent-hub-node-container |     at addChunk (_stream_readable.js:274:12)\nstudent-hub-node-container |     at readableAddChunk (_stream_readable.js:261:11)\nstudent-hub-node-container |     at Socket.Readable.push (_stream_readable.js:218:10)\nstudent-hub-node-container |     at TCP.onread (net.js:581:20)\nstudent-hub-node-container |   name: 'error',\nstudent-hub-node-container |   length: 109,\nstudent-hub-node-container |   severity: 'ERROR',\nstudent-hub-node-container |   code: '42P01',\nstudent-hub-node-container |   detail: undefined,\nstudent-hub-node-container |   hint: undefined,\nstudent-hub-node-container |   position: '8',\nstudent-hub-node-container |   internalPosition: undefined,\nstudent-hub-node-container |   internalQuery: undefined,\nstudent-hub-node-container |   where: undefined,\nstudent-hub-node-container |   schema: undefined,\nstudent-hub-node-container |   table: undefined,\nstudent-hub-node-container |   column: undefined,\nstudent-hub-node-container |   dataType: undefined,\nstudent-hub-node-container |   constraint: undefined,\nstudent-hub-node-container |   file: 'parse_relation.c',\nstudent-hub-node-container |   line: '1160',\nstudent-hub-node-container |   routine: 'parserOpenTable' }\nstudent-hub-node-container | pay_class_deposit method failed in payments_controller.js: Error: startChargeTransaction failed. However, queries were successfully rolled back and the stripe charge was refunded!\nstudent-hub-node-container |     at Object.startChargeTransaction (/var/server/utils/payments_utils.js:119:13)\nstudent-hub-node-container |     at <anonymous>\nstudent-hub-node-container |     at process._tickCallback (internal/process/next_tick.js:182:7)\nstudent-hub-node-container | POST /api/payments/class-deposit 500 2889.988 ms - 2\nThe record that was created in the database\nch_1Cmngk228emtMZTzIFD96ADv | Class deposit for james.lemire.programmer@gmail.com | 75000 | \u00a0 | paid | 5219 | 2018-07-11 19:09:23.955138+00 | 2018-07-11 19:09:23.955138+00 | 3270 | \u00a0\n-- | -- | -- | -- | -- | -- | -- | -- | -- | --\nThe first column is the stripe_id, followed by the description column. As you can see, they match up.\n. P.S I appreciate the quick reply and you looking into this for me. I'm a huge fan of the massive ORM and this is a real road block right now for me!. I'm unable to use version 5.1.1, on NPM it also says the latest is still 5.1.0.  Using yarn add massive@latest provides me with 5.1.0 and doing yarn add massive@5.1.1 results in an interactive menu to choose a version with the latest being 5.1.0.. Booyah! Transactions are working like a charm now! Awesome! Thanks for your speedy help, you made my boss very happy \ud83d\udc4d . ## File Tree for development/production\n- db\n  - enrollment\n    - existing_enrollment.sql\nBoth development and production are connecting to a staging database as the same user hosted on an amazon RDS server. So same permissions and same file tree. \nI'm in a weird scenario where I gotta test code in a production environment, just try to ignore that for now. \ud83d\ude02 . Ah sorry my bad for the confusion, I thought the terminology was SQL Function. \nLog in Development\n[ 'enrollment.create_enrollment',\n  'enrollment.existing_enrollment',\n  'enrollment.update_enrollment' ]\n[ 'enrollment.create_enrollment',\n  'enrollment.existing_enrollment',\n  'enrollment.update_enrollment' ]\nLog in Production\n[]\nThis is super strange to me.... I can see the files when I ls the directory of db on production.... I used the ls -l command to check the ownership of the root directory of the project, the db folder, then the enrollment folder. Everything checks out.. man I've never had this issue with Massive in countless other production projects. This issue has me stumped. . But anyways thanks for your time, it looks like there's something weird happening on my end.. @dmfay I ran the project in \"production mode\" on my local machine and the db.listFunctions came up empty again! Are you aware of any weirdness with Forever and Massive playing together?\nProduction Mode\n```sh\n!/bin/sh\nSCRIPT_DIR=\"$(pwd)/$(dirname $0)\"\ngit checkout master\ngit pull\ncd $SCRIPT_DIR/../\nyarn\n$SCRIPT_DIR/start-server.sh $SCRIPT_DIR\n```\n```sh\n!/bin/sh\nforever stop sf_listener\nforever start -l sf_listener.log -a --uid sf_listener $1/../index.js\n``. Rats! Alrighty thanks. At least I have a lead now. . For any one who stumbles on this thread. I fixed it by changing the scripts directory. For reasons I don't understand, when running a process withforever` it loses sight of the root directory of the project (for lack of better words/understanding on my part). \nIn order for forever to see my massive scripts, I had to use the second argument of the connect method and pass in the following object:\njs\n{\n  scripts: __dirname + '/db'\n}. ",
    "DriscollRichie": "@dmfay It showed it as an available function in the console when I console.logged db.listFunctions(), but it turns out that the await was the problem. Such a simple mistake. Don't code when you've been drinking! Sorry for the foolishness. Thank you!. ",
    "momirov": "Is there at least a way to have 2 instances of massive running at the same time? One for writes and one for reads? We will manually use the correct one.. When I try to initialize 2 instances I get a warning:\nWARNING: Creating a duplicate QueryFile object for the same file - \n    /src/node_modules/massive/lib/scripts/drop_table.sql\n    at files.forEach.file (/src/node_modules/massive/lib/database.js:227:46)\n    at initPromises.push.$p.then.files (/src/node_modules/massive/lib/database.js:226:13)\nWhen I try to use second instance I get an error:\nTypeError: Cannot read property 'QueryFile' of undefined\n    at Object.Database.query [as value] (/src/node_modules/massive/lib/database.js:361:33)\n    at Database.options.query (/src/node_modules/pg-monitor/lib/index.js:291:34)\n    at Executable.invoke (/src/node_modules/massive/lib/executable.js:84:18)\n    at Database.executor [as getSalesForReport] (/src/node_modules/massive/lib/database.js:89:46)\nI'm using query file for this query.\nInit script:\nPromise.all([\n  massive(config.db.url),\n  massive(config.db.readUrl),\n]).then(([db, readDb]) => {\n  Repository.initialize(db, readDb);\n});. @dmfay thank you. ",
    "apsavin": "It seems that the same thing with update method.. Thanks for the quick response and for massive.js!\nI made a workaround on my side for now. Unfortunately, I have no time for the PR at the moment. But I don't think that there's need to hurry. I will come with PR later if you will not fix the issue until then.. ",
    "nkramaric": "Unfortunately both stringifying the items in the array and also stringifying the array doesn't work. I looked at #598 and it offered some clues but I did not look too deeply into that \"toPostgres\" function documentation.\nIs that \"toPostgres\" function something I add to the item object in my example and it will get run in the save function?. Thanks @dmfay and @vitaly-t.\nI like the approach you outlined @vitaly-t however when I try to apply it to the insert function it doesn't seem like the toPostgres function is being called. Here is my setup:\n```\nconst item = {\n  data: [{\n    a: \"something\"\n  },{\n    b: \"hello\"\n  }],\n  toPostgres: (p) => db.pgp.as.format('$1::jsonb[]', [p.data]),\n  rawtype: true\n};\ndb.items.save(item);\n```\nI have also tried with insert db.items.insert(item). I am following the example in #598. No matter what I put into toPostgres it does seem to fire so maybe I am putting it in the wrong place? What do you think?\n. db is the instance of the massive js connection:\nmassive(process.env.DB_CONNECTION).then(db => {    \n});. Makes sense! Below is the entire example. I will see if I have time to create a mini repo so you can see what is happening. (The tables are the same as described in the original post).\n```\nconst massive = require('massive');\nconst monitor = require('pg-monitor');\nconst debug = require('debug')('db');\nconst m = massive(process.env.DB_CONNECTION).then(db => {  \n    debug('connected to database.');\nif (process.env.MONITOR_SQL) {\n    monitor.attach(db.driverConfig);\n}\n\nconst item = {\n    data: [{\n        a: \"something\"\n    },{\n        b: \"hello\"\n    }],\n    toPostgres: (p) => {\n        return db.pgp.as.format('$1::jsonb[]', [p.data]);\n    },\n    rawtype: true\n};\n\ndb.things.save(item);\n\nreturn database;\n\n});\n/*\nmonitor output:\nINSERT INTO \"things\" (\"data\") VALUES (array['{ \"a\": \"something\"}','{ \"b\": \"hello\"}']) RETURNING \n10:06:42 error: column \"data\" is of type jsonb[] but expression is of type text[]\n/\n```\nI put a breakpoint in the toPostgres function and I can see that it is never firing.. I tried with the rawType correction. Still getting the following response:\nINSERT INTO \"things\" (\"data\") VALUES (array['{\"a\":\"something\"}','{\"b\":\"hello\"}']) RETURNING *\n12:40:26 error: column \"data\" is of type jsonb[] but expression is of type text[]\n         query: INSERT INTO \"things\" (\"data\") VALUES (array['{\"a\":\"something\"}','{\"b\":\"hello\"}']) RETURNING *\nIt just doesn't seem to run the toPostgres function.\nWhen I do db.query it works. However when I tried db.none like you mentioned above it did not work as none was undefined. From what I have gathered, massivejs uses pg-promise to craft and execute the queries, is the toPostgres function not exposed in the insert and save methods?\n. Awesome! This works great. Thanks for all the help.. ",
    "nixel2007": "Sure, give me a minute. ",
    "pketh": "the documentation recommends app.set but is really vague on how to use this. I agree that a specific example would be useful. update: found one on https://massive-js.readthedocs.io/en/v2/quick_start/ that's helpful\n```\nvar express = require(\"express\");\nvar app = express();\nvar http = require('http');\nvar massive = require(\"massive\");\nvar connectionString = \"postgres://massive:password@localhost/chinook\";\n// connect to Massive and get the db instance. You can safely use the\n// convenience sync method here because its on app load\n// you can also use loadSync - it's an alias\nvar massiveInstance = massive.connectSync({connectionString : connectionString}) \n// Set a reference to the massive instance on Express' app:\napp.set('db', massiveInstance);\nhttp.createServer(app).listen(8080);\n``. the docs may be out of data becauseconnectSync` looks to be removed https://stackoverflow.com/questions/45747517/massive-connectsync-not-a-function. ",
    "cpainterwakefield": "Yes, the docs you reference above are specifically for version 2, whereas I believe massive is now up to version 5.. Yes, that was the example that I was talking about in my original post; Express generator provides a separate file, bin/www, that actually runs the webserver.  This is separate from app.js, apparently because you want to be able to run the app without a webserver for unit testing purposes.  (That isn't critical to me at this time, but it might be in the future.)  I can't figure out how to ensure I have a (non-promise) db object by the time the app starts up; you can't put \"await\" in app.js, for instance.  I was just hoping there was another way that someone could suggest.  Right now all my controller code looks like:\nmodule.exports.foos = (req, res) => {\n  var db = req.app.get('db');\n  db\n  .then(db => {\n    db.foo.find()\n    .then(results => {\n      res.json(results) \n    }); \n  });\n};\nwhich is annoying for the amount of nesting.  I am still new to js and node, though, so probably I will figure out better ways in time.\n. Thank you, that is very helpful!\nI'm still a little fuzzy on where I can use async and await in a node app, and the proper way to use them.  Documentation on the web is all over the place!. ",
    "ritwickdey": "Hi @dmfay, Sorry for late reply.\nIt actually override the existing body object. \nIs this possible to patch body object ?. ",
    "ethanresnick": "Thanks for your constructive comment, @vitaly-t!\nMore seriously, though, I've been thinking about this a bit overnight...\nIt's probably safe, or even necessary, to assume that the developer (not the end user) controls the query text. In that case, the misidentification of $x as a placeholder within a string literal isn't really a problem, because the developer can always pull the value with the dollar sign out of the query text and have pg-promise substitute it in. \nThat said, I think there are still two issues here worth acting on:\n\n\nThe narrow bug I identified at the start of the post, where user scripts intended not to have any placeholders (e.g., data seeding files) get formatted anyway because massive defaults the arguments to an empty array.\n\n\nConsider a query like: SELECT $1 FROM table WHERE $1 = 'x'. If the parameter substitution for this query is happening on the Postgres server, I'm pretty sure it's safe from SQL injection, because the PG server knows that $1 occurs in an identifier position and will escape the value provided for the placeholder appropriately. When parameter substitution for this same query is done by pg-promise, pg-promise doesn't know that $1 occurs in identifier position (because it doesn't do a full query parse), so the parameter's value won't get escaped as an identifier, and the query will be vulnerable, if I'm understanding correctly. Instead, the developer has to write: SELECT $1:name FROM table WHERE $1:name = 'x'.\nMy reaction to this yesterday was a bit of horror \u2014\u00a0and it certainly is the case that not doing a full query parse, and so requiring the developer to specify the context,  creates something that the developer can easily mess up, leaving their code vulnerable. On reflection, though, there are certainly arguments for pg-promise's approach: it obviously has better performance than sending the query to the server to parse, and way less complexity than including a full SQL parser in pg-promise (esp. since, as I learned looking into this a bit yesterday, the SQL parse result can depend on server settings around allowed string formats). Edit: although, actually, there is a module that seems to expose the parser provided by postgres itself to node, so using that might be a good idea, although it's still missing a lot of serialization features.\nNevertheless, if the user has to change how they write their queries to get proper security, that should be very prominently documented in the massive docs. Right now, I don't see any mention of this, so I imagine there's a lot of vulnerable massive code out there.. >  That shouldn't happen, and I'll have 5.5.2 up with a fix for query files shortly.\n\n\nThanks!\n\nThere's no real risk associated with false matches, assuming you test your code, since as you discovered they raise an error rather than permitting exploitation\n\nFair enough. \ud83d\udc4d \n\nTo your second point, when you emit SELECT $1... to Postgres directly, $1 is a literal not an identifier. Not even casting it to a name (which takes double colons, ::) allows you to do what you're thinking it does.\n\nTotally my mistake; I didn't realize you couldn't use Postgres prepared-statement placeholders for column names. Sorry to make this a bigger deal than it is.. ",
    "randrews": "Sorry, didn't mean to create this PR (yet). I want to get it reviewed by my team at Rackspace first and then try to submit it back to mainline. :) That comment explains the problem I'm trying to solve though.. Okay, there you go! Let me know what you think. Thanks! Do you like to merge them yourself, or should I go ahead and merge it?. Thank you! :). Thank you!. You'd have to change this to take an array, the point of generateInnerConjunction is that it takes an array of criteria instead of one criteria. You could essentially paste the body of generateInnerConjunction in here by moving the _.reduce call here, but the same argument is true for generateDisjunction.... There, just did exactly that :). ",
    "charmander": "@vitaly-t No, it doesn\u2019t.. ",
    "johnbiundo": "@dmfay: Thanks for the pointer.  I've tried to figure out how to set an option after initialization, but can't seem to suss it out.  Would you have a further pointer for that?\n(It seems I need to access the $config.options member of the underlying db connection, but that is not the same thing as db.pgp.  Constructs like db.pgp.$config don't resolve.  I'm probably off the track on how I'm looking for this, so would welcome any help).\nMy thought was to see if I could get this to work by setting up receive post initialization and ignore the reload function until after I prove that the startup condition works.. Awesome, that did it, thanks so much.  \n@dmfay: For my use case, this is probably all I need, as my DB is static and doesn't need to be reloaded during the lifetime of the node process.  However, as I offered, I'll be willing to take a look at doing something that might be useful in the more general case if you think it would be worthwhile.\nIn case anyone else needs this, here's the change from the original problem example above.  It's just a matter of setting the receive option after connecting (to avoid fouling up the introspection process), in the promise resolution, instead of in the driver settings.  The magic bit is using db.instance.$config.options to access the driver settings.\nLike so:\n```\nreturn massive(massiveConnSettings, {}, driverSettings).then(db => {\n  console.log('connected');\n  db.instance.$config.options.receive = (data, result, e) => {\n    camelizeColumns(data);\n  };\ndb.test.insert({ column_one: 1 });\n  db.test.find({}).then(resp => {\n    console.log('test rows: ', resp);\n  });\n```. OK, I'll take a look as soon as I can, and won't hesitate to ask for help :). @dmfay Feel free to say no if my experience level appears inadequate for this task.  I'm pretty willing to roll up my sleeves, and go backfill areas I need more background on to proceed, but I probably need a little help getting started, and don't want to be a burden.  \nFor what it's worth, I'd probably rate myself a good and diligent, but not greatly experienced JS developer.  I do have a ton of RDBMS experience, including primarily PostgreSQL the last couple of years (but going back, believe it or not, to the early Ingres days, when I worked there as a support person), but mostly as a user, not a developer.  And FWIW, I'm greatly interested in becoming a useful contributor to an OS project like this one.\nAnyway, I've cloned the repo and wanted to start by running tests, but I'm struggling a little to get them to run properly.  A first attempt yielded many failures :-(  Is there any testing documentation?  I presume I need to set up a DB with permissions and stuff, and could probably reverse engineer that by looking at a bunch of tests, but maybe there's a faster way?\nThanks in advance for any help, and willing to participate, or not, at whatever level makes sense.. Oops. Just found https://github.com/dmfay/massive-js/blob/master/CONTRIBUTING.md \nI'll start there :-). @dmfay OK, progress update. \nI've added a few lines to the reload function in database.js:\nat the top...\n```\nDatabase.prototype.reload = function() {\n  this.clean();\n// cache pg-promise receive option config\n  const receiveOptionCached = this.instance.$config.options.receive;\n  delete this.instance.$config.options.receive;\n```\nand at the end...\n```\n        // restore pg-promise receive option config\n        this.instance.$config.options.receive = receiveOptionCached\n          ? receiveOptionCached\n          : null;\n    return this;\n  });\n\n});\n};\n```\nOne issue and one question:\n1) Issue: the transactions > withTransaction > reloads and applies DDL: test fails with:\n1) transactions\n       withTransaction\n         reloads and applies DDL:\n     TypeError: Cannot read property 'options' of undefined\n      at Database.reload (lib/database.js:66:72)\n      at db.withTransaction.co.wrap.mode.db.pgp.txMode.TransactionMode.tiLevel (test/database/with.js:164:23)\n      at db.withTransaction.co.wrap.mode.db.pgp.txMode.TransactionMode.tiLevel.next (<anonymous>)\n      at onFulfilled (node_modules/co/index.js:65:19)\n      at process.internalTickCallback (internal/process/next_tick.js:77:7)\nAll other tests pass, including a manual test passing in a receive: camelCase function (exactly like my original code at the top of this report - yay! :) ).  I couldn't really figure out why this one failed.\n2) Should I save and restore an empty options.receive as null?  undefined?\nOnce I get your feedback, I can build a test and send a PR.\nThanks for your assistance!. Awesome.  OK, this was my first OSS contribution.  I'll keep an eye out for more.... I may get addicted :)\nThanks for supporting the effort.  . ",
    "bighappyworld": "Nevermind, I found the reason. It comes from pg-promise. But pg-promise supports a variety of different syntaxes for Named Parameter substitution and they work (just tested). Updating the docs on Massive to say it supports the ones from pg-promise would be helpful because the current docs make it look like the above syntax is the only syntax (at least to me).. Added to the docs: https://github.com/dmfay/massive-js/pull/669. thanks, adjusted to forward slash. ",
    "pauldprice": "Thanks for confirming - and thanks for maintaining this project! I submitted PR #674 to address this issue.. ",
    "henrik1": "Happy to change the name. Though I do disagree that there is no specific functionality as these tables are created specifically as document store tables. Still in this context I completely agree that changing the name to createTable makes sense. \ud83d\udc4d \n. Yep, but I can remove it for now since it is not sufficiently tested and also not in use yet. \ud83d\udc4d \n. \ud83d\udc4d \n. It's late night, my bad :)\n. "
}
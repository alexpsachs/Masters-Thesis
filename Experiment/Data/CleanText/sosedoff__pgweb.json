{
    "sosedoff": "Thanks!\n. @stevencothren check master, i made a tweak to the makefile, it should not fail on fresh checkout now. \n. thats alright for now\n\u2014\nSent from Mailbox\nOn Fri, Oct 24, 2014 at 9:06 PM, Steven Cothren notifications@github.com\nwrote:\n\nOk. This is the change I made previously here : https://github.com/sosedoff/pgweb/pull/3\nIt was pointed out there that this change results in the repository still not being 'go get\"-able.\nReply to this email directly or view it on GitHub:\nhttps://github.com/sosedoff/pgweb/issues/2#issuecomment-60468431\n. Im not sure about release assets being included in the repo, feels like duplication to me. Compiling\nassets is only required for development purposes, people can download binary releases for general purpose usage in this case and they don't have to compile anything at all. For local development you'll have to install all dependencies anyway. \n\nI'm okay with updated go-bindata package that can support non-absolute paths in the bindata.go file.\n. @cbandy if you commit bindata.go it will include environment-specific constants, which is why file was excluded from repo.\n. Connection string format:\npostgres://user:password@host:port/database\nNotice that you have postgresql instead of postgres \n. @jonathanwiesel yeah, thats possible. i'm exploring this possibility, homebrew does not allow binary releases in the main repo, they have homebrew-bin repo for that.\n. thats also an option. are you interested in working on homebrew integration ?\n. seems like homebrew-cask is for GUI applications\n. I think github releases is fine for now. As an alternative we could use https://bintray.com/ to host binary releases. \n. Good work @jonathanwiesel :+1: \n. This seems to be interesting, however i'm not planning on getting this implemented in the next releases.\n. @kespindler javascript timer would be irrelevant in case if you run a query on non-local servers. Results\ntable also takes some time. So if we want to have query execution time, it has to be done via backend. \n. Stopping a running query is currently in works.\n. Implemented in master\n. Closing this issue unless anyone is interested on working on the feature  / @cbandy @bill-myers \n. Good catch. This has to do with my local OSX settings where scrollbars are only visible during scrolling. \n. Column sorting is planned. See TODO. \n. hang tight, im working on it. \n. Implemented in master\n. Im open to pretty much any web framework that would simplify development. Currently there's \nbunch of jquery code that works, but at some point it will be painful to add new features. \n. Great, you can just fork repo and start building up the app. Let me know when you have something to look at. \n. I would prefer to have a very minimal amount of dependencies if possible. \n. I dont mind, just keep number of dependencies as low as possible. \n. We cant really use CDN as it would break app without internet connection. \nBrowsers: chrome, safari, firefox and ie. App should be simple enough to work on \nlast 2 major revisions. \n. This will be merged in for 0.5 release. I'd say this could happen in the next couple of weeks.\n. Good catch @bblack! Will fix\n. I've seen that repo but haven't played with it yet. Lets hold for now, thanks for suggestion. \n. Does it work with blank password ?\n. There's a breaking change now, which results in Error: pq: SSL is not enabled on the server. Please make sure that your changes are compatible with existing implementation. \n. Just to clarify, default OSX postgresql installation with homebrew has ssl mode set to disabled.\n. I somehow forgot to add pass flag. \n. @abourget yes, i want to implement database connection screen via web. Its not a top priority right now, but i'll work on it when i fix all other bugs. \n. Thats possible, gin has BasicAuth middleware - https://github.com/gin-gonic/gin#using-basicauth-middleware\n. See #44 \n. I'll release a new version today with a fix. \n. See #16 \n. Fixed. \n. Im not sure how binary gets executed. Dockerfile lists /go/bin/app as executable, but make dev only produces pgweb file. Also, It makes more sense to have CMD like this:\nCMD pgweb --host $PGHOST --port $PGPORT --user $PGUSER --db $PGDATABASE\nThoughts?\n. Never mind, i missed the part where you set directory for the app. Anyways, having pgweb as entry point still makes more sense to me.\n. The issue is with --url flag. Once set, any other flags like --ssl are ignored. Its a bug. \n. Fixed in master.\n. Can you try running the same command, but set --ssl flag to foobar ? Server should respond\nwith error stating that sslmode is invalid. This should verify if sslmode is properly passed into the server connection string. \n. I think i might have found the issue. Will test more.\n. @donnex Server is started on 0.0.0.0, not on localhost, see here - https://github.com/sosedoff/pgweb/blob/master/main.go#L102\nAnd you can specify server port with --listen flag. \n. Yeah, using localhost as a default bind interface it a good idea. Thanks. \n. Implemented in https://github.com/sosedoff/pgweb/commit/d325527aa642a89a1a9c9ae972dbe40f1045d8d0\n. See #16 \n. Can you please provide a screenshot?\n. This should be fixed in master now. \n. Thanks!\n. This is already fixed in #16, will release a new version today. \n. Fixed. \n. Hm, which key binding should it be? \n. If it works on all operating systems that would be great.\n. Implemented. \n. I will check out your branch later today. As for the roadmap, i'll write something down and push to the repo. \n. So far i have some issues with running this branch (mostly due to conflicts with master). Due to other issues i'll have to postpone angular rollout until v0.5 release. 0.4 release will only include bugfixes and minor changes. I'll give us some time to test everything and make sure there's no breaking changes.\n. I would go ahead with angular for 0.5.0 release. There's been a lot of changes since this PR was introduced. Would be great if you can go over all the changes (no major backend changes though) and port that into angular app. Im currently working on another major feature to add SSH gateway support. \n. @abourget Ping. Would you be interested in updating angular code to reflect all recent changes?\nMost of the ui and api did not change much.\n. Sure, i've done some simple angular stuff before. \n. Closing due to no interest.\n. Closing this since there was another PR for docker\n. Can this be part of #32 ?\n. Closing due to no interest.\n. Hm, thats strange. Are you running pgAdmin locally or inside vagrant ?\n. Ok, seems like something is broken. Im going to test with vagrant to replicate the issue.\n. Just tested on vagrant box. Works fine. Can you run the following on your host machine:\npsql -h 192.168.248.33 -U vagrant -W mydbname\nAnd when connection is established, this:\n\\dt\nOutput should include all available tables in the database. \n. Im not sure what could cause this. Can you try to run pgweb with url from this gist: https://gist.github.com/sosedoff/20f1799215d35e2bfc32\nIts a dummy heroku database with 1 table and no rows. If it does not work i'll probably have to add a bunch of debug statements to see where command/connection fails. \n. Yes please, i would like to see your postgresql.conf and pg_hba.conf configs. Would prefer gists thought. \n. @kohenkatz any luck ?\n. @fijosh This makes sense, although its not a quick fix and requires a few evenings of work. \n. Closing this since i was not able to reproduce the issue on most common setups (local and remote).\n. I just experimented with this feature and it could somewhat confusing. \nSay you opened the editor for the first time and you selected a table from sidebar. When you switch to query view, editor will be populated with generic SQL query to get 100 rows. But when you select a different table and switch to query view again, editor will hold previous query since its persistent and does not necessary relate to the selected table. \nIm going to close this unless someone has a better idea. \n. Yeah, CHANGELOG.md is a good idea.\n. You can grab release notes from this page - https://github.com/sosedoff/pgweb/releases\n. Merged. \n. You can specify connection url via DATABASE_URL environment variable. \n. Yeah, im interested in merging this, but haven't gotten to the point or running this on heroku.\nIm okay with Godeps as its the most convenient way to vendor packages, but my main concern is with including bindata.go into codebase. If go-bindata package provided a way to use 2 files, one for dev (bindata.go) and another one for release (bindata-release.go) that would be way better, but now it will require recompilation of assets every time they change (run make dev) which will slow down development flow a lot. \n. @cbandy go-bindata with -debug flags embeds user's full path into the bindata.go file, like this:\ngo\n// static_js_jquery_js reads file data from disk. It returns an error on failure.\nfunc static_js_jquery_js() ([]byte, error) {\n    return bindata_read(\n        \"/Users/sosedoff/go/src/github.com/sosedoff/pgweb/static/js/jquery.js\",\n        \"static/js/jquery.js\",\n    )\n}\n. what are you suggesting then?\n\u2014\nSent from Mailbox\nOn Thu, Oct 30, 2014 at 11:20 PM, Chris Bandy notifications@github.com\nwrote:\n\n\nIf go-bindata package provided a way to use 2 files, one for dev (bindata.go) and another one for release (bindata-release.go) that would be way better\nThe -o option lets you specify the output file, but I suspect having two files that export Asset will be irritating as well.\nReply to this email directly or view it on GitHub:\nhttps://github.com/sosedoff/pgweb/pull/43#issuecomment-61215391\n. You can probably use --bind=0.0.0.0 instead ?\n. Since we have authentication now, i think i'll set binding to 0.0.0.0 by default\n. @pvh Lets get this merged in tonight. Btw, do you have any additional instructions regarding godep ?\n. @pvh ping\n. Looks like its a better way, i was testing docker image with custom params as well. Whats your use case for dockerized pgweb ?\n. @srijs So use CMD instead ?\n. First, i feel like PGWEB_ prefix is wrong since we're not connecting to pgweb, but to postgresql instead. So i suggest it should be PG prefix, like PGURL, PGHOST and so on, less typing too.  Im also not sure if i want to keep connection settings as CLI arguments since a lot of people expressed interest in having an web page to connect to database. That way usage will simplify by just running pgweb with no postgresql flags.\n\n\nSecondly, im using go fmt plugin for sublime so all my code is automatically structured on each file save, no need to change formatting of import statements. \n. I think having env var wont hurt. Btw, there's env var missing for ssl flag. Something like PGSSL would work. \n. Closing this since there's a better approach to specify connection url, via DATABASE_URL environment variable. I think there should not be too many options on how to specify connection settings, so having environment variables for each flag is an overhead in this case. \n. Implemented and will be released soon. \n. Its an interesting idea but its definitely too early to consider building support for mysql databases.\n. Im aware of that. My original thought was to implement a double-click action that will show a view with all column data. \n. This is fixed in master now. Explain results are no longer cropped. Double-click on row cell will transform its value into textarea. \n. @gglanzani please provide a screenshot\n. Ah, i understand now. I pushed a change to master, give it a try. \n. Great, just tested - everything works as expected.\n. This is done intentionally \n. @corbanb https://github.com/caskroom/homebrew-cask/blob/master/Casks/pgweb.rb\nNot the most recent release though\n. Closing this obsolete PR, multiple schemas are implemented in master and will be part of the upcoming release. Thanks.\n. Can't accept that: duplicate and unnecessary commits that modify app.js while PR proposes only changes to the readme. \n. I just tested it on linux amd64, works:\n```\nwget https://github.com/sosedoff/pgweb/releases/download/v0.3.1/pgweb_linux_amd64.zip\nunzip pgweb_linux_amd64.zip\n./pgweb_linux_amd64 --version\npgweb v0.3.1\n```\nCheck SHA1 sum for the file, it should be:\nd6965ed93720aa1c5d4c6380a85d147c29a81e3e\n. Yes, there is a plan to have a simple row editor, but its not something to be released next, maybe in a new next releases. \n. no, its not worked on right now and i don't have an ETA on when its going to be available.\n. This behavior is already changed in master and will be released in 0.4.0.\nIf no flags provided to pgweb binary it will open the web page with database connection URL prompt.\n. Are you using pgweb binary from downloads ?\n. @petere ping\n. Added a note about invalid postgresql connection string format. Closing this since there's no response regarding my question. \n. There's a favicon right now, but new version is not released yet.\nIcon: https://github.com/sosedoff/pgweb/blob/master/static/img/icon.png\n. Im closing this because i'm unable to reproduce any issues described in the PR. After a small change to the docker file here https://github.com/sosedoff/pgweb/commit/41cf9cf205bed667e040560c66e6cdd7fd8cc088 i was able to build an image from scratch and start container without any problems. There's also no need to build assets because they are baked into the repository now. \n. Command make dev should only be used on a dev workstation as it modifies the bindata.go file to read assets from files.\n. Interesting idea. Revisit later thus closing the issue for now. \n. Yeah, i posted instructions on how to connect via ssh gateway. But the reason why i want to merge this functionality into pgweb is to simplify usage. Go has ssh support so it just makes sense to implement native support. \n. Closing this issue for now. SSH support might be added some day. \n. 1. Remove \"background\" queries from the history. -  True, no need to record internal queries.\n2. Move queries inline - I would prefer having a single file that contains all query statements. They don't have to be defined as constants and could be formatted as multiline strings.\n3. SELECT COUNT(*) FROM fix is great, thanks\n4. Yes, bindata.go should be updated every time assets are changed\n. Will this work on windows machines ?\n. Im not sure either. \nBtw, could this shell git ls-files -io --exclude-standard static/ | awk '{ gsub(/[.]/, \"[.]\"); printf \"%s\", \" -ignore=\" $$0 }' be extract out of task into var, like BINDATA, for the sake of readability ? \n. Is this good to go ?\n. Added in 742384327c3e18b6cc4bcb4866988cdeec9f69e7\nShould be available in the next binary release. Thanks.\n. TOML-based bookmarks are implemented in master as read-only. \nWiki page: https://github.com/sosedoff/pgweb/wiki/Server-bookmarks\n. pgweb does not server any assets from filesystem (unless in dev mode) so it cant use gin's static \nroute.\n. > Button to refresh table list on demand\n+1, people were asking about this feature.\n\nList tables and views based on the current search path\n\nThis could take some time to test. Could you provide a test sql file that i can just load into a new database? I've been thinking about including a sample sql file that will create tables in multiple schemas.\n\nUse regclass to generate properly quoted table identifiers based on the search path\nI dont see any breaking changes, so its good. \nButton placement and styling.\n\nNo worries, i can restyle the button to make it fit the ui\n\nOrder of table names.\n\nMy guess is that tables should be ordered by name asc. \n. In addition to all above, i'd really appreciate if you can provide a test sql file and instructions on how you test all new changes. \n. Is this still work-in-progress ?\n. Yes, eventually its going to make into master, but its a slow progress so i cant guarantee any ETA's at this time. \n. Closing this obsolete PR, multiple schemas are implemented in master and will be part of the upcoming release.\n. Are you using pgweb on osx ? If so, there's an example plist available - https://github.com/sosedoff/pgweb/wiki/Plist\n. Daemon mode is not something that i would personally include into codebase at this time.\nThis project aims to run on multiple platforms and myself being fairly new to golang i don't see any \nsimple way of implementing cross-platform daemonized version. There are plenty of alternatives\nthat could handle the task of controlling background execution. I could name a few examples: init.d (linux), upstart (linux), supervisor (x-platform), launchd (osx). On linux you can even use nohup to run\nprocess in background.\n. @insectatorious Issue confirmed.\nI was planning on getting rid of os/user anyway, might as well do it now. The only reason why we need os/user is to figure out current user for localhost connections, for remote connection its most likely that user is something else than postgres.\n. @insectatorious Oh btw, i see that your url format is invalid: postgresql://. It should start with postgres://. Format checking is already fixed in master, but new release is not out yet. So if you specify a proper url prefix it should work as you have user defined. Used detection from os settings only happens when user is not provided. \n. Closing this issue since it was fixed in \"lib/pq\" repo. pgweb will also try to fetch user from OS environment first. \n. Which pgweb version are you using? Also on which OS ?\n. Closing this issue unless there's a problem. No feedback received. \n. I dont think this is a big deal. If you're doing lots of prototyping this is the cost you've gotta pay.\nImagine doing prototyping using a plain-old psql, you cant just \"select\" a query and run it. So unless you have any code to support that i wont be working on this.\n. Im alway open for a good PR!\n. Thanks for reporting. I will check. \n. All bookmark attributes should be encoded as strings, i corrected the wiki page. Thanks!\n. Thanks, i'll fix it. \n. This is fixed in master.\n. :+1: \n. Hi. You could use nginx as a proxy in front of pgweb, configuration in this case would super simple\nif you already have your ssl certificate. The reason why ssl is not supported out of the box is because\nof the same lack of ssl support in gin web framework used by pgweb. \n. Thanks! I will add that to wiki\n. @kurotsukikaitou also keep in mind that running pgweb on your production servers is not recommended for security reasons. \n. There are a couple of options to consider if you need to connect to your production database:\n1. Setup pgweb username and password (there's no default), and perhaps a read-only database user.\n2. If you're using ssh keys to connect to the server you can use pgweb's built-in ssh tunnel feature or using native ssh client (https://github.com/sosedoff/pgweb/wiki/SSH-Gateway)\n. Thanks\n. Next time make sure to include updated bindata.go, which could be regenerated with make assets\n. Hi @tcn,\nThanks for your report. Im aware of 0.6.0 version issue and will fix soon.\nRegarding the second issue, i think this might have been a problematic binary build.. I'll need to redo it\nto fix the problem. \nWhat about the empty tables list ? Do you see any errors in the console? Alternatively you could run a direct request against api: GET http://localhost:808(or 8081)/api/tables \n. Do you have any tables in public schema ?\n. I see. Yeah, right now pgweb only supports public schema but im working on having multiple schema support as well. Dont know ETA, but hopefully in the next version or two. \n. @tcn could you please try https://github.com/sosedoff/pgweb/releases/tag/v0.6.1 binary ?\n. There was an issue with binary packaging that i did not notice so i'll have more clean build process in the future. If 0.6.1 works fine i'll go ahead and push that to homebrew. \n. 0.6.1 is released to homebrew cask repo\n. Implemented in master.\n. So this error happens all the time or randomly? \n. Thanks for the examples, i'll try to reproduce and hopefully fix the problem. \n. I ran all your example and havent gotten a single crash. Using PostgreSQL 9.3.\nHere's the profiler log (start with -d flag):\n[DEBUG] Goroutines: 17, Mem used: 643597512 (613 mb), Mem acquired: 838741264 (799 mb)\n[GIN] 2015/08/01 - 15:38:09 | 200 |   1.227644ms | 127.0.0.1:53274 |   GET     /static/css/bootstrap.css\n[GIN] 2015/08/01 - 15:38:09 | 200 |    208.369\u00b5s | 127.0.0.1:53296 |   GET     /static/css/app.css\n[GIN] 2015/08/01 - 15:38:09 | 200 |    484.422\u00b5s | 127.0.0.1:53298 |   GET     /static/css/font-awesome.css\n[GIN] 2015/08/01 - 15:38:12 | 200 |   1.396976ms | 127.0.0.1:53298 |   GET     /api/tables/test_table/rows\n[GIN] 2015/08/01 - 15:38:12 | 200 |  43.729672ms | 127.0.0.1:53274 |   GET     /api/tables/test_table/info\n[GIN] 2015/08/01 - 15:38:20 | 200 |   1.074291ms | 127.0.0.1:53274 |   GET     /api/tables/test_table/rows\n[GIN] 2015/08/01 - 15:38:20 | 200 |    797.674\u00b5s | 127.0.0.1:53298 |   GET     /api/tables/test_table/info\n[GIN] 2015/08/01 - 15:38:21 | 200 |     448.68\u00b5s | 127.0.0.1:53298 |   GET     /api/tables/schema_migrations/rows\n[GIN] 2015/08/01 - 15:38:21 | 200 |    980.176\u00b5s | 127.0.0.1:53274 |   GET     /api/tables/schema_migrations/info\n[DEBUG] Goroutines: 17, Mem used: 644277808 (614 mb), Mem acquired: 838741264 (799 mb)\n[GIN] 2015/08/01 - 15:38:39 | 400 |    266.471\u00b5s | 127.0.0.1:53274 |   POST    /api/query\n[DEBUG] Goroutines: 17, Mem used: 644285704 (614 mb), Mem acquired: 838741264 (799 mb)\n[GIN] 2015/08/01 - 15:39:19 | 200 |   2.653159ms | 127.0.0.1:53302 |   POST    /api/query\n[GIN] 2015/08/01 - 15:39:26 | 200 |   2.533056ms | 127.0.0.1:53304 |   POST    /api/query\n[DEBUG] Goroutines: 18, Mem used: 53719480 (51 mb), Mem acquired: 838741264 (799 mb)\n[GIN] 2015/08/01 - 15:39:37 | 200 | 5.243092543s | 127.0.0.1:53306 |   POST    /api/query\n[DEBUG] Goroutines: 18, Mem used: 1910728640 (1822 mb), Mem acquired: 2776425168 (2647 mb)\n[GIN] 2015/08/01 - 15:40:13 | 200 | 29.986914504s | 127.0.0.1:53308 |   POST    /api/query\nI ran the queries you provided and also these two:\nSELECT * FROM test_table\nSELECT * FROM test_table2\nIm not sure, whats the expected behavior here? If you're running out of memory there's not much could be done, maybe rescue from that exception and show an error message to the user. Let me know. \n. Closing due to no response\n. Could you provide a more detailed example?\n. Thanks, i'll have a look later today\n. Fixed in latest release\n. Ah, i think someone mentioned about this before. Will fix. \n. Fixed in latest release. However, you should be using proper escaping for the full uri's\n. Amazon RDS should now be supported. I pushed support for multiple schemas into master.\n. Which piece of code in particular are you talking about?\n. I dont see any maps in the code you provided. Am i missing something ? rows.SliceScan() returns []interface{} https://github.com/jmoiron/sqlx/blob/master/sqlx.go#L540\n. @tylerlong did the issue only appear on longer (hours) requests? The behavior is indeed a bug, but i personally havent tested pgweb with longer running queries since it was not intended for that in the first place. Im currently running a long test using pg_sleep, maybe i can reproduce the bug. \n. @tylerlong i was able to run sleep query for 300 seconds, and web ui responded correctly. could you specify your stack? (postgresql version, web browser and its version). also, are you running latest pgweb version too?\n. @tylerlong im going to close this for now, feel free to reopen the issue once you have some steps to reproduce the issue. \n. Could you please rebase and also make sure to run make assets ?\n. Pardon me, i meant squash not rebase. Just wanted to make sure there's a single commit with all your changes. \n. > Is it possible to add visual analylize like in pgadmin3?\n\nof course it is! all PRs are welcome\nofftop: 9.4 support?\nshould be working fine with anything 9.x at this point\n. Closing this for now since postgis really has nothing to do with pgweb. Unless the question was more related to the UI part. Feel free to reopen though. \n. This should be fixed in master which adds support for multiple schemas\n. I'll be working on table browse page in the next couple of days and should take care of this. Thanks.\n. Implemented in master and will be part of the upcoming 0.8.0 release\n. This makes sense, yes, but unfortunately pgweb does not use any sort of database to store its own settings or state so having any kind of authentication system will require it to have one. I was thinking about it for a while now since i sometimes too want to use pgweb in multiple \"sessions\". Adding this feature will require a significant refactoring effort so its not something i can work on right now, unless you are willing to help out on your end. Feel free to reopen the issue. \n. @smurfpandey thanks, yeah i get that, but i think the original question was not only related to database sessions, but also involves some sort of permission model. Anyways, i was hoping to get to session implementation soon. \n. Perhaps this could be solved by casting all rows' values to strings in JSON response. Its not the best solution (and def not the correct one), but otherwise it'll require some bignum js library to be brought in. Any other ideas ?\n. Yeah, i did experiment with automatic encoding of bigint's as strings and it works fine, will probably fix the issue sometime this week. \n. @skabbes try latest master, the issue should be fixed now.\n. I'll release 0.7.1 in a day or so, working on a few other things that should go in too.\n. FYI 0.8.0 is published to homebrew cask.\n. Done\n. 0.9.0 is now available in docker hub\n. Can you be more specific? Also, are you using latest master or latest binary release?\n\nOn Wed, Jan 13, 2016 at 8:34 AM, gstoffer notifications@github.com\nwrote:\n\nHi,\nI tested your web interface and I can't navigate through schemas :-)\nIs it normal ?\nIs this supported ?\nRegards\nGuillaume S\nReply to this email directly or view it on GitHub:\nhttps://github.com/sosedoff/pgweb/issues/113\n. Closing due to no response. \n. Here's the postgres version im using locally:\n\nPostgreSQL 9.4.4 on x86_64-apple-darwin14.3.0, compiled by Apple LLVM version 6.1.0 (clang-602.0.53) (based on LLVM 3.6.0svn), 64-bit\n. Are you sure about that? If you run \\dm using psql tool, it'll produce the following sql:\nsql\nSELECT n.nspname as \"Schema\",\n  c.relname as \"Name\",\n  CASE c.relkind WHEN 'r' THEN 'table' WHEN 'v' THEN 'view' WHEN 'm' THEN 'materialized view' WHEN 'i' THEN 'index' WHEN 'S' THEN 'sequence' WHEN 's' THEN 'special' WHEN 'f' THEN 'foreign table' END as \"Type\",\n  pg_catalog.pg_get_userbyid(c.relowner) as \"Owner\"\nFROM pg_catalog.pg_class c\n     LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace\nWHERE c.relkind IN ('m','')\n      AND n.nspname <> 'pg_catalog'\n      AND n.nspname <> 'information_schema'\n      AND n.nspname !~ '^pg_toast'\n  AND pg_catalog.pg_table_is_visible(c.oid)\nORDER BY 1,2;\nThats pretty much what is being used in pgweb and is verified on postgresql 9.3+\n. Pushed to master\n. Yeah, you will have to provide a sample database in order to connect using demo instance. The reason why there's no database is because demo is hosted on heroku and they dont provide ability to create read-only users, at least not on free plan. \n. What do you mean? The demo site does not save any data that passes through it, you can check the source code.. I set timeout to 2 minutes, that should be enough for most queries. Use master branch to verify the fix\n. Done\n. Speaking of resizes, other components should be able to be resized, not just the query window. Planning on working on it in the next releases (will open another issue). \n. Will work on it this week probably. I think i just fixed this bug in master, please give it a try. \n. Hi, this issue is a basic regression and will be fixed in the next minor release. Thanks for the feedback.\n. Fixed\n. Hi and thanks for the feedback. I agree that having more friendly links is better and helps\na lot when sharing pages, but unfortunately its not a priority for pgweb (or myself) at the moment.\nI see that this feature could be easily implemented with any sort of routing/lightweight js framework, perhaps even with vanilla jquery. Will keep this open for now, feel free to contribute if you have any\ninterest in making this feature happen. \n. Added wiki page regarding keyboard shortcuts (there only few available): https://github.com/sosedoff/pgweb/wiki/Keyboard-Shortcuts\n. Right now structure tab is very simple and does not have ability to display data from multiple \"sources\", i.e it just displays a single table as any other view. There's a need to rebuild structure view to include more data but i don't think its a priority for me right now, at least not until i get to refactor the way different table views are displayed. I'll keep this issue open for now. If you wish to help out - please do and open a PR and i will happily review your contributions. Thanks.\n. Closing due to no action (im personally not interested on working on this feature)\n. Implemented in master. UI is not superb, but it does the job. UI will be improved in the next releases. \n. Try running pgweb from master, it should have a fix in place.\n. You're not blind, there's no button to close connection. If you're using single connection mode you can just stop/kill the app.\n. Cool, i'll look into it and perhaps add the feature to the next release\n. Fix has been pushed to master. Please verify if it works for you.\n. Try now\n. Cool. Closing for now then. \n. Closing due to no need. Dropping all tables in schema is a very specific behavior which i personally had very little use for. Better to drop the schema/db instead.\n. As far as i know there's no alternative to MySQL's mysql_affected_rows function in PostgreSQL.\nhttp://dev.mysql.com/doc/refman/5.7/en/mysql-affected-rows.html\n. i can see that it somehow does it too, but there are no extra sql queries in the server log, so it must be something internal. I'll have a look later today.\n. Try master, it should now display number of affected rows if sql statement starts with UPDATE or DELETE.\n. Yes, this feature could be supported, but only in a single session mode.\n. Closing due to no activity. I've personally never used pgpass files and nobody else expressed interest in the feature so far. \n. Btw, while i havent added the support for pgpass file, you can get away with using bookmarks. By using bookmarks you're not exposing any passwords in the web or cli.. Sure, if you have a patch that does not introduce any breaking changes i'd gladly accept it.\n. On the other hand i would probably prefer a solution that changes the path prefix for the whole application. That would work better IMO. Thoughts?\n. I don't necessarily agree that having a url prefix does not provide any value. Having a url prefix baked into the app's makes it super easy to use with software like nginx.\nI pushed a new git branch url-prefix that implements url prefix for pgweb, you can compile the source code from that branch and then start pgweb with --prefix=db/. \nFor nginx, then you can configure pgweb as follows:\n```\nupstream pgweb {\n  server 127.0.0.1:8081;\n}\nserver {\n  # other stuff\n  # ...\n# proxy any database stuff directly to pgweb\n  location /db {\n    proxy_pass http://pgweb;\n  } \n}\n```\nIf that works for you - great, otherwise please provide your own solution and will gladly review and accept the patch for the feature in question. Cheers.\n. Implemented on master. Feel free to reopen if you find any issues\n. @andreibancioiu when i start pgweb with --prefix=pgweb/ i can access the url with that prefix just fine. did you verify that by accessing http://localhost:9090/pgweb? if it does not work you will get 404 page. . Thanks for the report. Will fix.\n. Fix is pushed to master, please give it a try.\n. Query history is stored in memory while pgweb is running. Activity data is actually not something that is being stored and comes directly from postgresql server. Right now there's no way to disable both these features so your only option is to build from source.\n. FYI, if you want to rebuild pgweb without activity and history tabs, you can remove the following lines here (https://github.com/sosedoff/pgweb/blob/master/static/index.html#L27-L28) and then build pgweb:\nmake build     # build for current OS\nmake release # build for all supported OS\n. Try running make assets first.\n. Hm, i just did the following:\n1. opened index.html file, removed both tabs (activity/history)\n2. ran make build (which runs asset build task as well)\n3. started pgweb from current dir ./pgweb\nand both tabs are gone from the interface. are you sure you're not starting your system-wide pgweb version instead of freshly built one?\n. I opened this ticket from a private conversion (as a reminder), but i think its no longer valid. In other words, pgweb should not be used in untrusted environments, or used with a read-only user. Closing this for now. \n. Fixed in master. \n. Couple of things here:\n- There needs to be a an automated test for this feature to make sure it works on all supported postgres versions (9.x)\n- Whats the reason for js timeout change to 1 hour?\n- Probably a good idea to change new flag to --query-timeout (and the var in struct) so that its clear that timeout is related to sql stuff\n. Having an extra API endpoint just to fetch timeout setting is a bit overkill in my opinion. There arent any other options that could be exposed so i would really prefer to have a hardcoded \"max\" value for the timeout. 5 minutes sounds about right as pgweb was not designed to be running any queries that could take more time than that.\nAlso, since i dont have any windows machines around to test pgweb stuff, im not sure i understand why FORCE_WIN_TEST is \"enabled\" (by setting it to \"0\") by default.\n. Thanks. Next time please make sure to run make assets before submitting pr so that update bindata.go is included. \n. This issue has already been fixed in master and will be available in the next release.\n. This seems to be a legit issue. Btw, how are you connecting to your server? Via host/port/etc or via database uri string?\n. No feedback. Closing. \n. Thanks for the feedback. This change should be a fairly easy one, i might get to work on it this week unless you have a PR ready. \n. Implemented. Should be soon in master. \n. @akarki15 if you'd like to fix this - go ahead. otherwise let me know if i can close the issue, it hasn't been a problem for me afaik.. Do you have postgresql installed locally?\nFor my local dev i use mostly homebrew-provided postgresql:\n$ which createdb\n/usr/local/bin/createdb\n. Since you're running pgweb programmatically, you can make a call to the api to set the schema. Here's an example:\ncurl http://localhost:8081/api/query?query=SET%20search_path%20TO%20myschema\n. You should be using port 8081, i think i havent updated wiki to reflect the change in default port used by pgweb.\n. I updated the wiki to include the proper command. It was missing the image name (sosedoff/pgweb).\n. sure thing. i should probably automate that, always forget.\n. done\n. There's a multi-session support available right now, but its slightly different from what you're asking. Although it allows you to run number of connection with different servers in the same browser.\nThanks for the suggestion, i'll look into it as soon as i have spare time.\n. This feature would require a major overhaul of the current backend and frontend implementations, so closing this until better times. \n. Thats cool, although i just don't see an immediate need for the feature at this time. If someone really wants it  they're free to implement it and open a PR. . Good idea. Might as well add go1.6 to travis CI.\n. Just needs a git squash and it should be good to go\n. Thanks!\n. Please provide a sample SQL file so that i can test and fix the problem.\n. Thanks, i will check on this issues once i get some free time. \n. I just created a table from your example using pgweb 0.9.3 (latest) on OSX (chrome) and this is what i got so far:\n\nNothing was downloaded automatically. Also tried Safari and Firefox, both work.\nWhat OS and browser are you using? \n. Ah i see what you mean now. What is the behavior you expect? \n. Pgweb under the hood uses lib/pq library to connect to postgresql, please submit your request in that project.\n. You're right. I think i pushed a wrong tag.\n. Fixed. Public registry should contain proper latest and 0.9.2 release rebuilt using alpine image and as a result its image side has been reduced to 8Mb. \n. @ekiyanov could you provide the following:\n- go version used to compile sources\n- build commands used to make a binary\nRPI is not \"officially\" supported (no precompiled binaries) but i dont see why it wont run on the device. In fact i ran a cross-compiled binary on RPI once i think, didnt have any issues.\n. Pgweb now has support for ARMv5. Since i have a few raspberry pi's around i was able to verify that cross-compilation works fine. Im using Go 1.6 for all development just FYI.\nI dont have any pre-built binaries out yet (will be available starting with 0.9.3) but you can build one yourself. Make sure to pull latest changes and then run make release. After its all done, grab a binary file ./bin/pgweb_linux_arm_v5 from the current directory and drop it onto your pi. Should work.\n. P.S Here's the command used to cross-compile: https://github.com/sosedoff/pgweb/blob/master/Makefile#L56-L58\n. How did you get extra tables in the test database?\n. PG_OBJECTS statement only returns objects from the current database. Also, if you look at client tests, it drops and then recreates the database on every run so you should not have any extra tables in your database. Would need more info on your setup since i cant reproduce your issue. \n. You should not have any tables while running make test since it will drop and recreate the database on every run, see https://github.com/sosedoff/pgweb/blob/master/pkg/client/client_test.go#L332\nClosing for now since its an edge case and has something to do with your local setup. I tried to reproduce with no luck.\n. Sorry but i don't have Excel around, only Google Docs. I made a sample CSV export document that has both date and datetime columns. Try to import it and see if that works. Here's the link: https://dl.dropboxusercontent.com/u/486271/pgweb-1467084910.csv\nIm not 100% sure how timestamps should be formatted in CSV files for Excel ingestion, but with a bit of googling it looks like its a standard yyyy-mm-dd hh:mm:ss. I pushed a new branch with the fix if you're interested in trying it out.\n. Switched to alpine.3.3 instead. Overall size is 17Mb comparing to 308Mb\n. If you did not change any frontend code (html/css/js) there's no need to include updated bindata file in the PR. \n. Thanks for bringing this up and sorry for the late response. Indeed, COUNT(*) is very slow on large data sets so having a hybrid approach would solve the issue (partially). Ideally, we could first check the estimated rows count and if its below 100k (just an example) rows we use COUNT, otherwise use the estimated rows count for pagination. I would also prefer keeping the pagination where possible instead of removing it altogether, i know this could be a tricky feature, but still. \n. Does anyone have any interest in tackling the problem?\n. Coming back to this. So im thinking of having a map (per established connection) that will hold information about tables that are somewhat large (> 100k rows). For table browse requests we will first check if that map contains the table and if it does we will fetch the rows stats using count estimates. On database switch that map will be cleared out.\nAny thoughts?. @xujif was not having quotes causing any issues? \n. @xujif Could you provide an example SQL so i can reproduce, add a test and fix the issue?\n. Nevermind, i was able to fix the issue in #191.\n. 0.9.4 is out, also pushed to docker registry. https://hub.docker.com/r/sosedoff/pgweb/tags/\n. Fixed. I accidentally committed dev version of bindata (which is used to store all static assets).\n. Im not sure i understand the purpose of this PR. All the listed changes are already in master.\n. Closing due to no response, also its not clear what's the purpose of the PR. \n. Were there any changes to the asset files? It looks like nothing was changed (except for bindata). \n. Does that mean you manually changed the bindata.go file?\n. RE issue with export - thanks for catching that. Would love a separate clean PR. \nRE other things: i don't see the point of updating the custom query when you browse table rows with any applied filters. If i'm in the process of writing the custom query it will override it as soon as i hit \"rows\". At least that's what i see in the source code. The purpose of custom sql query editor was to able to use both features at the same time. Perhaps the best way to address what you're asking for is to record all queries that happen on /rows/ endpoint to the history. \n. I understand that you have a very specific use case for pgweb, but having an extra functionality just to show the sql queries being used in the browse view is an overkill imo. As i mentioned before, the best way to solve the problem is to implement query logging for all browse queries so that users can quickly glance at those in the history view.\nAnother solution would be having a smaller panel (aka \"console\") that will show all the queries that pgweb runs. This will however require extra development time, which i d'ont have at the moment. If you're interested - feel free to open a new PR. (P.S this is something that SequelPro does and its great).\nCheers.\n. Closing this for now. Part of the PR has been already merged, the rest is not going to make it in. \n. Sure, i'll have a look at this\n. Implemented in master and will be available in the next release.\n. But this kind of behavior would require the server to keep tabs on types of fields used in each table. I understand that it will be a much nicer user experience, but i dont really have the time to work on something like this. If you have any suggestions / code snippets i would definitely help getting the feature added, but otherwise i'll leave this issue open. \n. You're right. Fixed the issue and will probably do something about it for the long run. \n. What is your use case for such feature?\n. Cool. Sounds interesting, few other folks have reached out to me with the same request. I'll see what i can do. \n. Implemented in master\n. You're not crazy, these types of features (editing, etc) are not part of the pgweb yet. I've been experimenting with a few options but havent committed to anything just yet. \n. Sorry, but this is something pgweb is not going to support at this time. I'll tag this issue so i can revisit it in the future.\n. Fixed in master and will be available in the next release.\n. If you're familiar with Go, please include a simple unit test so that we can verify the feature works correctly. If you're not i'll add the test case myself. Let me know.\n. Closing this in favor of #191 \n. I'll look into this. Base64 was not really a good choice so it'll be replaced with something more robust in near future. \n. Im more that happy to merge this PR, could you please include a few example queries that did not work for you. I tried the query you provided and it went through just fine (no issues with decoding). \nAlso, which pgweb version are you on?\n. Pagination/Filters are only intended for browser view (aka rows). There's no supporting functionality for other pages in the backend so the correct behavior is not to show them on any other pages. If you wish to have pagination on activity page, feel free to submit PR. Closing for now since its not a top priority nor requires any special attention. \n. Ah, misunderstood your report. Indeed, its a bug. \n. Fixed in master, will be available in the laster release.\n. yeah, i was thinking about adding a database (sqlite) to store settings and other db-related stuff (mostly queries). not sure when i get to implement that. \n. @akarki15 Server bookmarks are already implemented. This issue is about how to initialize a connection from bookmarks, which are stored under $HOME/.pgweb/bookmarks/*.toml \n. Bug fixed. See #205. \n. LGTM, just needs a make assets run. \n. Done\n. Create a fresh branch off master and cherry pick your commits\n\nOn Nov 10, 2016, at 12:26 AM, Aashish Karki notifications@github.com wrote:\n@sosedoff last 5 commits are the ones relevant to this feature. the first one is a merge commit in my fork to get upstream changes. do you know how i should go about issuing this pr?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @akarki15 I see that you've changed port handling in the bookmarks. Its a breaking change since any existing bookmarks will error out with Type mismatch for 'bookmarks.Bookmark.Port': Expected integer but found 'string'..\n. Also, what happens if i specify invalid bookmark (typo, name mismatch, forgot extension, etc). Right now i get Error: dial tcp [::1]:0: connect: can't assign requested address or a panic\n. Regarding the change to int - change itself is fine (although its breaking right now) but in future you should keep the PRs scoped to a single feature.\n\nAs for the error: i removed all bookmarks from the path and specified -b dummy pgweb flag. Results in panic.\n. It should be already fixed in #186 and available in master. Try that.\nNew pgweb version (with fixes) is not released yet.\n. Using latest pgweb (0.9.6) you should be able to connect to the bookmark using -b or --bookmark flag. The bookmark must be saved under $HOME/.pgweb/bookmarks first.\n. Btw, the url you provided does not work that way. If you have a few bookmarks saved, they'll be available on the login page. There's no other way to use them in the web app, only via the command line option. \n. Thanks will take a look\n. This is fixed in master. Please give it a try.\n. Im not sure why you're getting the error, perhaps older version of go? Anyways, to simplify testing - i attached a binary that you could try.\npgweb_linux_amd64.zip\n. Hi @adamjaso and thanks for the feedback. I've thought about some similar use case but most of those integrations are somewhat non-trivial and dont really belong in pgweb. Maybe in the next major version i will come up with plugin system but for now i dont think any of the custom stuff makes any good use for the regular users. . Thanks for the extra setup script. The only problem i see with it is with hardcoded path to the binary, usually stuff like that should be declared as a variable at the top.. Why would you need sudo to run pgweb? It does not start on port 80. Oh nvm i misread some of the config stuff. I'll take another look at this when i have time. . I added your example init.d config to the repo.. @andreibancioiu there's a systemd service file at https://github.com/sosedoff/pgweb/blob/master/examples/pgweb.service as well.. Thanks for the report, i'll fix that in the next release.. I just tested auth by setting user/pass via command line options and env variable using your example password and everything works. Could you provide full command that you're using to start pgweb? Also, which version are you on?. Ah, i see. You're trying to set password with spaces as an environment variable. AFAIK that wont work on Heroku and thats why you're getting auth failures.. Closing this since the issue has nothing to do with pgweb.. Fixed in master.. Thanks for the bugfix!. Fixed in master. See example.\n\n. Currently - no. Im still trying to find some time and implement row editing functionality. It should be fairly straightforward as long as you try to edit rows in the \"browse\" mode. Editing rows in custom query result would be complicated and does not make sense.. Thats correct, row editing is not supported in any form in pgweb at the moment. Textbox is meant to easily copy cell's data.. As far as connecting to the postgres (from pgweb) it should just work as long as the postgres server is reachable and credentials are correct (host/port/user/db). From your example its not clear how you start postgres container. Could you provide more details from your compose file?. Postgres does not start right away, it takes a few seconds after start to become fully available.\nSince pgweb depends on the postgres container, you have to specify that in your compose file:\npgweb:\n  container_name: pgweb\n  restart: always\n  image: sosedoff/pgweb\n  ports: \n    - \"8081:8081\" \n  links: \n    - postgres:postgres\n  environment:\n    - DATABASE_URL=postgres://postgres:postgres@postgres:5432/postgres\n  depends_on:\n    - postgres\nAlso, default postgres container does not have SSL enabled, so just disable that. New variables:\nDATABASE_URL=postgres://postgres:postgres@postgres:5432/postgres?sslmode=disable. What would you like to modify in wiki? . Sure, go ahead.. Its hard to just guess, but you can check the request status in the chrome debug tools. Pgweb makes AJAX call for each query so you'll be able to see if anything is being returned at all.. Care to share a bit more info on the problem? Its always good to know what kind of stuff people do so i can work out a better ux. I thought about it, but don't see the immediate need to switch. . Would like to hear more about that particular setup. . @od0 if you wish to move forward with changes (current or new ones) please include some tests. There's a sample database sql file, you can add new instructions to the end. . @od0 hey, any update? are you still going to work on this issue?. yeah, you dont have to touch any existing tables, just add a new one and the test case to verify it works.. @od0 Feel free to resubmit when you're ready.. This should be fixed as i've switched master to use go 1.7.5. . Hi,\nI think all those issues really have nothing to do with pgweb for a few reasons:\n- pgweb project has not been tested to work with linuxbrew so i cant really tell if its homebrew issue or something else. osx is supported. \n- when installing from source you must set your GOROOT and GOPATH correctly for the build to work. \nAlternatively you can always download a prebuilt binary file for your OS: https://github.com/sosedoff/pgweb/releases/download/v0.9.6/pgweb_linux_amd64.zip\n. @TomoGlavas check https://github.com/sosedoff/pgweb/wiki/Installation. Are you using the same user for pgweb and your other db tool? I might suspect it has something to do with permissions.. @zoujd-github ping, care to provide more details? otherwise i will close this issue since i don't have any way to reproduce the problem.. Closing due to no response.. Implemented in #224 . Possible alternative: local SQLite3 db.. Hi, what kind of sql queries are you running?. Hi again,\nI'm closing this due to no activity and also due to the fact that pgweb was never meant to run any \"heavy\" queries. If you feel like fixing the problem, or making the option to customize the default query timeout via CLI option - go ahead and submit a PR. I just don't have any time to work on this. \nThanks.. Hi, while i havent personally tested the SSL cert stuff with pgweb, you might try adding required ssl paths to the postgresql connection string. See docs for more details.\n```\nsslcert\nThis parameter specifies the file name of the client SSL certificate, replacing the default ~/.postgresql/postgresql.crt. This parameter is ignored if an SSL connection is not made.\nsslkey\nThis parameter specifies the location for the secret key used for the client certificate. It can either specify a file name that will be used instead of the default ~/.postgresql/postgresql.key, or it can specify a key obtained from an external \"engine\" (engines are OpenSSL loadable modules). An external engine specification should consist of a colon-separated engine name and an engine-specific key identifier. This parameter is ignored if an SSL connection is not made.\nsslrootcert\nThis parameter specifies the name of a file containing SSL certificate authority (CA) certificate(s). If the file exists, the server's certificate will be verified to be signed by one of these authorities. The default is ~/.postgresql/root.crt.\n```. @johicks any luck with certs? i didnt have any time to look into your issue, sorry. . Would love to get more feedback. Closing due to no activity.. Not at the moment, but i thought about adding a very basic charting feature in the past. . Im going to keep the issue open in case i have time to work on it or someone else decides to contribute. . You can try hitting the \"Export\" button before running the actual query. . Yeah, you can type the query and hit export, it will not load anything in the browser. Regarding the pagination you mentioned, its true, theres a need for something like that, but i dont have any solutions since i use psql for big data dumps. \n\nOn Mar 23, 2017, at 9:48 AM, Tristan F. notifications@github.com wrote:\n@sosedoff I did not even think about it, I assumed we had to execute the query first.\nMy go skills are limited so the solution I implemented is really just truncating the result array before sending it back to the browser, but maybe some sort of pagination, or truncating but telling the user that the results are truncated would be a better option?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Quick update on this issue:\n\n\nI've been working on getting https://clusterize.js.org/ integrated into pgweb. It offers a way better experiment on any large datasets. \nLimiting the result dataset will likely result in faster page loads. We could add a limit to any custom query passed by a user, and maybe add more results to the table dynamically. Still experimenting.. Damn IE, i'll take a look.. Issue: \nhttps://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/of\nfor x of array is not supported in IE11. . Yeah, its a known issue. waiting is not available in pg_stat_activity. I haven't had time to fix this. . @bjeanes thanks, i'll check it out. . Should be fixed in master now. . Interesting, i haven't used cockroach db. Pgweb has more psql specific features like getting indexes and table structure so i don't know if that will be portable.. Yeah, i meant to write this comment in the issue, not in this pr. . @tamird do you have a sample cockroachdb database dump that i can use for testing?. @tamird i'd happy to support cockroachdb, but i personally don't use it and don't think i can find any spare time to really dig in and implement a such a bridge. Pgweb does not have any active contributors so its kind of hard to say when non-postres stores could be supported.. How exactly the semantic UI would benefit pgweb at this time? And whats wrong with using bootstrap? . No response, closing.. Should fix the issue on all 9.x branches, with a dumb default on any other pg version. . @bjeanes True, i missed those fields while porting the query across all supported versions.. I don't think pgweb is a good tool to do database dumps with, pg_dump has been around for years and has all the options you need, although i can probably agree with you that having a very basic \"db clone\" feature could be useful. Under the hood it would still use pg_dump though, and might as well work on windows too.. @tistvan btw, this is doable, i was playing around with this feature today. however it will only work 100% if you have pg_dump installed on your system. So in other words, there's no way to package pg_dump with pgweb.. Work is done here #270 . @janpieper Seems like there was some change related to the x-font-woff mime type in go 1.8.3 (i believe). I was using go 1.8.1 until today and tests were all passing. I pushed a change to master that fixes the problem. . Hey, sorry for the late response.\n\nI can definitely see your use case, although it will require a bit of work to implement the\nfunctionality in question.\nFew thoughts here:\n\n\nPassing a database connection string in the url would be the easiest solution. That way we can have a connection token on the client (browser) that will correspond to a server connection. There's already something like that in place: session connections. When sessions are enabled (--sessions flag) you can have multiple connections, one per browser tab (using session storage). This approach will still require user to specify the connection credentials or choose a bookmark. \n\n\nYou can have a bookmarks file in your system that will provide access to all your connections. You might as well manage that file dynamically (once you allocate/deallocate resources). Each bookmark must have a unique filename (*.toml). Pros: we don't expose any server credentials in the url and just use the bookmark name as a reference. Cons: each client that has access to the exposed pgweb instance will be able to use any of the available bookmarks without any restrictions. Perhaps we can add a new attribute to the bookmarks file that will clearly indicate that the bookmark file is not \"discoverable\", meaning it won't be listed anywhere in the pgweb. \n\n\nThere are a couple featured that aim at making the pgweb be used in \"exporer\" mode (--readonly, --lock-session flags). However, they're not 100% bulletproof as they're not enforcing any particular rules on the database. Try those, maybe they're good enough for your use case. \n\n\nWith all that said, my only concern is that not enough people expressed their interest in the functionality in question. I'm interested to work on this since i have the same issue once in a while, but i'd like implement it in a most generic way and keep things simple.\nLet me know anyway. Or if you have a POC - i'd like to have a look.\nCheers!. @maximRNBack If you have any meaningful code for us to review/test, please share it somewhere (Github PR/etc). I have a spec that i would like to move forward with but i'm not sure if there was any developments on this front. Let me know.. @tzachshabtay @maximRNBack i implemented the authentication feature via http backend here #266, have a look. Let me know if you have any questions. . @maximRNBack Interesting. Have you looked at https://github.com/bitly/oauth2_proxy? I've used that proxy to setup oauth stuff in front of pgweb instead of rolling my own. . @maximRNBack will look into your stuff.\n@tzachshabtay yes, any http(s) endpoint will work.. @maximRNBack reviewed your stuff, and here's my thoughts:\n\n\ni would rather prefer to keep out all the authentication (oauth/etc) functionality out of pgweb, all that stuff is really out of scope of this project. mainly to keep it as simple as possible.\n\n\nwe can make pgweb work really nice with oauth2-proxy project since it has all the necessary bits and pieces in order to make the auth really work.\n\n\nBased on points above i made a few changes to the ongoing #266 proposal. Here's how it would work in combination with oauth2-proxy:\n\n\nStart the proxy with all the config flags and settings you need, add the -upstream flag to point to the pgweb\n\n\nStart the pgweb server with backend flags:\n\n\npgweb \\\n  --sessions \\\n  --connect-backend=http://your-internal-url/ \\\n  --connect-token=test \\\n  --connect-headers=X-Forwarded-Email,X-Forwarded-User,X-Forwarded-Access-Token\nWe will still need the --connect-token in order to make sure that all the \nrequests originating from pgweb are verified by a third-party connect backend\napi. \nThe next part is --connect-headers. Any listed headers names will be grabbed\nand passed through to the connect backend api. In this particular case, oauth2-proxy\nwill inject those headers automatically, except for X-Forwarded-Access-Token. \nYou'll have to start oauth2-proxy with -pass-access-token flag.\n\nConnect phase\n\nWhen user visits http://yourhost/connect/ID, first it hits the oauth2-proxy that\nwill validate credentials (google/github/whatever), then it will make a call to \npgweb on /connect/ID path. Pgweb on its part will build a JSON payload (see below)\nand make a call to the connect backend. If backend responds with a valid \ndatabase_url key (response must be json-encoded) then pgweb will try to connect\nto the provided resource url.\nExample JSON payload pgweb will send to the connect api:\njson\n{\n  \"resource\": \"testresource\",\n  \"token\": \"backendtoken\",\n  \"headers\": {\n    \"x-forwarded-access-token\": \"ya29.GlvNBAv52OrEW7Wx-tF9ntL442...redacted\",\n    \"x-forwarded-email\": \"dan.sosedoff@gmail.com\",\n    \"x-forwarded-user\": \"dan.sosedoff\"\n  }\n}\nAs far as i can see this strategy covers pretty much all the ends while keeping\npgweb slim and simple.  \nLet me know if that works for you or if you have any other thoughts.. Right, the session token wasn't exposed before. Feel free to open the ticket for that, although im not going to get on it right away.. I've merged the #266 and will continue to improve the security side of things moving forward. \nWrote a new wiki entry for the reference: https://github.com/sosedoff/pgweb/wiki/Connect-Backend.\nI'll leave the issue open for the time being in case if someone has any questions. \ud83e\udd18\ud83c\udffd. Im going to close this one since it has been quiet for almost 2 months now. Feel free to reopen or submit a new ticket with any questions. . There's already a JSON export button available on the custom query page. Is that what you're looking for? . Im closing this issue since there's a very little need for this specific use case. I've just added a new feature to allow downloading query results as a formatted JSON file (see master), that should be enough to get by. If that does not suit your needs, feel free to submit a patch, i'd be happy to review and merge.. List of functions could be added, but i'm not sure about function editor. You can always use custom sql query tab for that. Pgweb does not really have any way to modify any database stuff (except for the query tab) at the moment. . Please submit issues in english. Even with the translation i don't understand your question.. In regards to this project, there are two make tasks available to compile assets:\n\nmake dev (will build assets that are auto-loaded from the dist on every page serve)\nmake assets (will build assets that are served from the app itself and not from the disk).\n\nCheck Makefile for more details.\nAnd yes, as previously mentioned, assets are built with go-bindata package. . Third-party lib is fine as long as it does not pull the whole jQuery-UI stuff in. . Closing this since the feature has been implemented.. @skabbes new version 0.9.8 is out. @Vonatzki which pgweb version are you using? Also could you provide the output (redacted if needed) of http://localhost:8081/api/objects ?. You need to start pgweb is a single session mode, connect to you db and visit the link i provided earlier. \n\nOn Aug 30, 2017, at 12:26 AM, Von Yu notifications@github.com wrote:\nI'm not quite sure if I followed your instructions to a tee.\nGoing to that link churns out {\"error\":\"Session ID is required\"}.\nI have two databases in my postgresql right now. The other one has its schemas showing up on pgweb just fine.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Ok i will have to reproduce this issue locally. . @Vonatzki if its possible for you to send me the test dataset (raw sql dump) i'd fix it faster. My email is in my profile. . On the other hand, if you dont see the schema on the sidebar, there's a chance that it does not have any objects (tables/views/etc). I created an example schema with a single table in it and i could verify it shows up on the sidebar: https://www.dropbox.com/s/dl0jt9gghpx1g5h/Screenshot%202017-09-14%2000.10.15.png?dl=0\nSame goes for the views / etc. See my previous comment: i will need a sample sql dump to reproduce your issue. . @Vonatzki can we get this wrapped up? I don't really have anything to work with to reproduce your issue, so please send some example sql data, OR close the issue if it's no longer the case. . https://github.com/jackc/sqlfmt would be a good candidate for this. Does not seem to be feasible at the moment. There's no good go package that could format sql queries without any issues. . Implemented.. I personally don't have a specific need for this feature as i've always written a custom query to get the exact records i need, but if you have a strong use for this - feel free to submit some code that i can use to implement this. Does not have to be perfect, but if it works that's a good start.. @varp another feature you could work on.. Which postgresql version are you running ?. Im closing this issue due to no debug information. I'd need extra info on db/permissions to troubleshoot this issue.. Implemented in master. @Garbee yeah i get that, but i'd rather have an official manual on hand for all the newcomers to follow. Since i don't use pgweb behind nginx i'll just go over all the manual steps myself, but if you have any configuration samples (excluding already mentioned post) you're welcome to reference them here or send to my email directly. As for the basic app, pgweb has the auth feature, so i'm not sure why people don't use it. . Implemented in #268 . @varp this is something you can work on if you'd like :) there are a few options on the table right now:\n\n\ngolang native https://github.com/sclevine/agouti <- def worth looking into\nkarma/phanomjs/mocha\n\nThe complexity of the app has been increasing steadily over the time without any backing of the actual integration tests. I've been testing all the features directly in the browser, which is not much actually, but having an automated suite would definitely make it all piece of cake. Integration suite would also help with the ongoing effort to move Pgweb onto a real web framework, like react.. @DylanGriffith Agouti requires a specific setup, so if you follow their readme you should be able to get it figured. I usually group my test files by functionality. \nAs far as the tests go, we just need coverage for the most functionality like connecting to the database, switching tables, running custom queries, sorting, filtering, pagination, data export and so on. . Thank you. There was another issue #235 related to the CockroachDB, but\njust to reiterate on it - no pgweb does not support CockroachDB at this time\nsince it has a bunch of pg-specific queries under the hood. It is possible to make\nit work, but that would require a serious effort. I dont use CockroachDB myself so\ni cant really say if the support will be available in the future. . Implemented in master. Implemented. @yathuganesh ?. I dont think we need to determine the type, just insert everything as a varchar/text. . Go for it!. If you still want to implement this feature, here's a few things to do:\n\nadd cli option --idle-timeout (int) to specify maximum connection idle time in minutes, default to 3 hours (180 mins)\nadd option --disable-connection-idle-timeout (bool) to disable the timeout feature altogether\nadjust the logic in the session cleanup worker here: https://github.com/sosedoff/pgweb/blob/master/pkg/api/session_cleanup.go. Yeah, 2 new flags should control how the cleanup worker terminates idle connection. If we have --disable-connection-idle-timeout set to false we should just turn off the worker here https://github.com/sosedoff/pgweb/blob/master/pkg/cli/cli.go#L201. This has been implemented via the following flags:\n\n--no-idle-timeout  Disable connection idle timeout (false)\n--idle-timeout=    Set connection idle timeout in minutes (180). How many rows are we talking about? . Im almost 100% sure the slowness is due to the DOM-building. 1MB per row is a lot of data to be displayed in html. When you click on the table on the sidebar, you'll see a new API query in the pgweb's log:\n[GIN] 2017/10/02 - 10:02:53 | 200 |   17.092465ms |       127.0.0.1 |  GET     /api/tables/public.items/rows\nThat will give you a rough idea on how long did it actually take to process the request. If you see the request duration is very large - let me know so we can troubleshoot this. . any tables that contain large blobs of text or other data will result in UI slowness, there's not much that could be done in the current pgweb version. closing for now.. Hm, that should be the only step needed for the local dev. Did you try flushing your browser cache?\nAlso, how did you setup the project?. Ah you probably cloned it under your own namespace in $GOPATH/src, you'll need to clone it as $GOPATH/src/github.com/sosedoff/pgweb so that all package references could stay the same. . @cristianoliveira have you made any progress on this feature?. This is not going to happen on 0.x branch, so closing for now. Im planning to rewrite the pgweb frontend to accommodate all the editing functionality in pgweb 1.x, coming in 2018.   . Will implement this in the next release. . I believe this is already done in #268. Row sorting was implemented a while ago. I was referring to the \"cursor: pointer\" change made in the linked PR. . If you're talking about making the sql editor view resizable, there's already an old ticket: #118. Dupe. Raw SQL export (whole db/single table) is already implemented and available in 0.9.9. How would this feature work on the subset of table data? My guess is that you can always write a custom sql query and export the resulting data as csv, then import that file using postgres csv import feature.\nI don't think this feature request worth implementing (from my point of view) since it's based on a very specific use case.  If you'd like to make it happen - send some code ;). @cristianoliveira any progress? regarding the panic, its actually not related to bookmarks, there was a bug with nil ssh info on the bookmark struct which i already fixed in master. . @cristianoliveira i fixed this in #300 . Im closing this since the fix has been merged to master. This change should also account for different modes. If in single session mode - the query data should be available in every tab (just like it works right now), but when it runs in multi-session mode it should only use session storage. . I thought i had it before, but seems like that piece of work has never made it back to master. \nMulti-session flag is not currently present in /connection endpoint, so you can add it as well. See example: https://github.com/sosedoff/pgweb/blob/master/pkg/api/api.go#L391 . Looks good, i might tweak some UI but this should be all.. I've tweaked your changes a bit and merged in. Thanks!. Yeah, copied from data-id, but without the namespace public.mytable -> mytable. Ok, this is available in master now, closing. . Why do we need a regex here? str.split(\".\")[1] would do the same. Im merging this, but next time when you submit PR please make sure to run make assets so that bindata.go file is update with the most recent static assets.. Don't check for ConnectionIdleTimeoutDisabled in the  client code, we should disable the worker here (https://github.com/sosedoff/pgweb/blob/master/pkg/cli/cli.go#L200) if the flag is set to true. . It appears that you've checked in development assets into this PR. bindata file gets updated every time you run make dev. You should run make build or make assets which will produce the correct bindata file. . Hi there, which pgweb/postgres version are you using? Could you provide a scrubbed dump with a sample set of data?. Hm thats strange. Could you try creating a new test db and importing the dump file i attached?\nThen try browsing data/running query in pgweb.\npgweb_issue_304.sql.gz\nHere's what i see:\n\nAlso, i noticed that your payload number is incrementing with the record id. Not sure if that matters, just wanted to bring this up. \n. Can you send me the SQL dump so i can reproduce and fix the issue? You can post it here or send it my email (in profile). . @rex-sheridan i'm not sure what to do about this issue. i've never used largeobjects feature before. Given pgweb's limited feature scope i don't think it's worth supporting in the long run, although if someone wanted to create a pr i'd happy to review and merge if possible. I also don't have any time to work on something like this, there's plenty of other bugs/features that i need to take care of first. I also checked Postico to see if they support large objects - same deal, you only see the reference id. . @varp why do you need this issue reopened? i thought we've pretty much covered all the ends here. . Well, when time comes - sure, but it seems like a lot of hustle right now.. Closing this issue for now.. Whats this ?. I dont know whats up with this PR. Closing.. There are a couple of issues that i noticed:\n\nNo way to specify delimiter for the imported CSV file\nWhen import fails, there's no error message\nThe table name import field is too wide, it should probably take 1/4 or 1/3 of the page\nPanics\n\nExample panic:\nruntime error: invalid memory address or nil pointer dereference\n/usr/local/Cellar/go/1.8.3/libexec/src/runtime/panic.go:489 (0x102a81f)\n    gopanic: reflectcall(nil, unsafe.Pointer(d.fn), deferArgs(d), uint32(d.siz), uint32(d.siz))\n/usr/local/Cellar/go/1.8.3/libexec/src/runtime/panic.go:63 (0x10296ce)\n    panicmem: panic(memoryError)\n/usr/local/Cellar/go/1.8.3/libexec/src/runtime/signal_unix.go:290 (0x103fb0f)\n    sigpanic: panicmem()\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/pkg/api/api.go:503 (0x14529b4)\n    DataImport: defer file.Close()\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/context.go:98 (0x137ddfa)\n    (*Context).Next: c.handlers[c.index](c)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/pkg/api/middleware.go:31 (0x1456089)\n    dbCheckMiddleware.func1: c.Next()\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/context.go:98 (0x137ddfa)\n    (*Context).Next: c.handlers[c.index](c)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/recovery.go:45 (0x138d86a)\n    RecoveryWithWriter.func1: c.Next()\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/context.go:98 (0x137ddfa)\n    (*Context).Next: c.handlers[c.index](c)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/logger.go:77 (0x138cb3f)\n    LoggerWithWriter.func1: c.Next()\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/context.go:98 (0x137ddfa)\n    (*Context).Next: c.handlers[c.index](c)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/gin.go:290 (0x13843f0)\n    (*Engine).handleHTTPRequest: context.Next()\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/gin.go:271 (0x1383d0b). Closing due to no activity. . Where does the change to vendor/golang.org/x/crypto/ssh/keys.go come from? . Uh, i'll check on this one. I thought it was already fixed. . Looks like it has been fixed in 0.10.0 . Thanks. @mdnight Do you have any additions to this PR? I dont see any changes to the UI where you're supposed to put your private key password. . @mdnight which might be misleading. the ssh password field is supposed to represent the password for the user account the end user is trying to login with.. @techiadarsh Whats with the change? It looks like the JSON is invalid. . Closing this due to no response. From what i know heroku postgres creates a database with hobby-dev plan by default.\n\n. @Wilbeibi i've added guard to CI recently. Although all the releases pushed out to github should not be affected. Which pgweb versions that you've tried had this asset bug ?. 0.9.9 binary release and source code does not contain any dev assets, i'm not sure how you ended up with the version that does. if you know the git sha i can check if it's broken. . If you're building from master - then yes, assets get broken sometimes, that explains why you were seeing the error. . Pgweb is already distributed via homebrew cask, however i'm not sure if casks support plists. Since im a pgweb developer i dont really use the service plist, however i've made one before and it worked fine. I just posted my question on homebrew-caskroom to figure out if this is something we can add. \nHere's the plist that works with the latest pgweb installed from the cask:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n  <key>KeepAlive</key>\n  <true/>\n  <key>Label</key>\n  <string>homebrew.mxcl.pgweb</string>\n  <key>ProgramArguments</key>\n  <array>\n    <string>/usr/local/Caskroom/pgweb/0.9.9/pgweb_darwin_amd64</string>\n  </array>\n  <key>RunAtLoad</key>\n  <true/>\n  <key>StandardErrorPath</key>\n  <string>/usr/local/var/log/pgweb.error.log</string>\n  <key>StandardOutPath</key>\n  <string>/usr/local/var/log/pgweb.log</string>\n</dict>\n</plist>. To answer my own question regarding the plist support in homebrew casks: no, its not supported. If we wanted to move pgweb to homebrew core we would have to support building from source, which i don't find useful.. I haven't seen any issues with this test for quite some time, so closing for the time being. . There seems to be an issue with deps. Some of the packages are missing. Don't change the deps unless you mean to. That's why all the CI specs are failing (linux/win). . Script script/test_ginkgo.sh seems to be doing something odd. At first it tries to build a specific image, then goes on to test on various versions of postgres. The goal here is to utilize the make test and make test-all to run the full test suite. . We could even reduce number of CLI options (which has grown a bit) by using a single option to pass the host/port/user/pass like this:\n--ssh=user:password@host:port\nI dont use SSH with pgweb directly, only via bookmarks, so let me know what you think.. Closing due to no activity. . This has been implemented and released in 0.9.11. . Hm, i've actually implemented this in #329. There was another PR somewhere in the list that did the same - change the local storage to session storage. This change breaks the indented behavior when pgweb runs in a single session mode: queries should persist unless running in multi-session mode. Main reasoning here is that the last query will always be available in the single session mode, which is handy for local dev. . @muhammedabad sorry for the late response: no, its not possible to delete a row from the row browser UI. . pgweb is a database browser tool so backup and restore functionality is really out of scope. use pg_dump / pg_restore commands. pgweb however offers a pretty simple way to export the database as a sql.gz dump without any options, and that's pretty much it. . Please update the PR if still interested. Otherwise im going to close it.. I just checked the changes, not sure i want to have this checkbox on the bar. Maybe a better UX is to add a small \"wrench\" icon somewhere on the same bar (where we have \"run\", \"explain\", etc). When user clicks on it we can open up a small modal with timeout settings or anything that really belong there. Just my two cents. . Honestly, i'd prefer a --query-timeout param in the CLI at this point. Feel free to reopen. . How are you trying to connect ? Need a bit more info . @adammoisa ping. Stale issue, closing due to no activity. . What do you mean by session still freezing ? \nThere are 2 flags related to the connection idling:\n--no-idle-timeout  Disable connection idle timeout (false)\n--idle-timeout=    Set connection idle timeout in minutes (180)\nThese deal with the hanging connections:  if pgweb runs in session mode and the session is abandoned it'll be automatically closed, otherwise the server will hoard the database connections.. No activity - closing. . Yeah, sure. I will add that.. Added in master some time ago. . We can save the table filters and sorting state and restore it when switching between tables. I'll see what i can do here.. @joemccann thats because by default pgweb is listening on localhost only. you can bind the server to any network interface with --bind=0.0.0.0 flag.. Ah, the # sign must be escaped for this to work, ie postgresql://user:%23password%23@hostname/database. \ud83d\udc4d. Mobile devices were never meant to be supported by pgweb, and there's no plan on supporting in the the current version. . Thanks for the contribution. I'd love to see a set of tests that run against real Cockroach db instance, like in the script/test_all.sh script, which runs tests against multiple Postgres versions using Docker. At least i will be able to test all this stuff locally. . @backy-io I just started some work on getting the CockroachDB integrated (partially) into Pgweb, so your contribution will be useful here. Thanks.. This could be added on top of #285, but yeah the keyword/db objects autocompletion is something that i've been meaning to implement for a long time now. Hopefully i get to it when i get a bit more of free time. . You can try putting https://github.com/gopenguin/ldap-proxy in front of pgweb, i haven't tried that thought. Maybe in future pgweb can offer native LDAP authentication backend, but there's not much need for it at the moment. . Sorry i forgot to reply in time. Can you clarify on why the connection details should not be visible if bookmark is being used? No sensitive information is exposed on that page, and any user can actually get the same info by running a sql query themselves. . No feedback, closing for now. I dont think hiding any connection details would make any sense, see my previous post in this thread. . Are you talking about the SQL query pane?. Closing this for now, there's no plan for transactional support for the time being. . @ericdagenais sorry its been a while, could you perhaps update this PR? Im planning on a new release so it you can make it in time i will include your changes.. Thanks! Good catch on missing pg_dump. . Interesting, i had a very similar use case for Heroku where i had to connect to a few databases. I made  a hacky \"addon\" for pgweb, but all this work is not open sourced.. @bsx how are you running pgweb? on your dev machine or on the EC2 instance ?. Im not sure i'd merge this feature as-is since it's very vendor/api specific. Why stop with AWS in that case? Heroku and other vendors have api to get list of databases for apps/accounts, etc. The goal of Pgweb is to be as simple as possible.\nHowever, i recommend you look into \"Connect\" feature that is already implemented in Pgweb and could replace your customization. Check out the wiki page here: https://github.com/sosedoff/pgweb/wiki/Connect-Backend. You can build a web page that will fetch all available databases from RDS and then list them on the page where users can simply click and connect to the database. When user opens up the special link, Pgweb will make a HTTP request to your website with the resource identifier and will expect the connection details in the response. Then it'll try to connect to the specified database and make a new session, everything else will work the same from there. I'm not sure if you tried that solution. Let me know.. @bsx Checking in, did you try the approach i described above?. @bsx Another ping, have you tried anything that i outlined in the previous posts?. @bsx @Garbee take a look at #391 PR.\nPushed out an initial feature set with 2 providers: Heroku and AWS. Technically speaking, existing bookmarks feature could be implemented as a provider since all providers are using Bookmark model to return connection details.\nI find it really easy to connect to Heroku now since i don't even have to configure the API token with CLI flag, pgweb will load the token from ~/.netrc file where Heroku CLI keeps its config. \nHere's how to enable the feature:\npgweb \\\n  --sessions \\               # Turn on multi-session mode\n  --discovery \\              # Turn on discovery feature\n  --heroku \\                 # Enable Heroku provider\n  --heroku-token=... \\       # Heroku API token (optional) -> HEROKU_TOKEN\n  --aws \\                    # Enable AWS provider\n  --aws-access-key=... \\     # -> AWS_ACCESS_KEY\n  --aws-secret-key=... \\     # -> AWS_SECRET_KEY\n  --aws-region=us-east-1     # -> AWS_REGION\nYou'd need to checkout git branch discovery to test out the feature. Just clone the repo, switch branch and run make build.\nLet me know what do you think.. Ping! Would love to hear some feedback here. . Closing in favor of #391 . Im closing this issue for now since there's not much i can help with at this time.\nPgweb uses https://github.com/lib/pq to connect to postgres, so if something regarding certs/CA stuff does not work i'd definitely look into that repo.\nPgweb 0.10.0 is finally out with a bunch of updated deps (including lib/pq one) so give it a try. . Fix has been added to master. However, the app will require the user to specify the database name, because a lot of endpoints expect the \"correct\" environment, ie. postgres will try to use the current username as a database (and fail if it does not exist), but that's not the case with cockroach.. You're right, there's a dumb check for ?sslmode presence in the url, so if it's not the first parameter it gets added to the url. However, i'd like to know how did you connect to the database? CLI or via web? . Fixed. Pgweb does not officially support CockroachDB, even though CDB supports the Postgres wire protocol. . All these minor issues are addressed and going to be released in the next version.. There's no way to disable the pagination (which uses count(1)) at this time. I don't think i would have any time to get this issue resolved, unfortunately. I'd rewrite a lot of backend code to get everything smooth, but that's a completely new time sucker. . Looks good, i'd add a test to verify TableRowsCount works with a large table, which you can create and fill dynamically. . Tests look good. I think you forgot to include schema name in the estimated row count sql query. . You need to bind to 0.0.0.0, pgweb is listening on localhost only by default.\n\nOn Jun 17, 2018, at 1:26 PM, Jaspreet Singh notifications@github.com wrote:\nError \"Bad Gateway\"\nFollowing is my docker setup\nTraefic Reverse proxy\nversion: \"3.2\"\nReverse Proxy\nservices:\n  traefik:\n    container_name: traefik\n    restart: unless-stopped\n    image: traefik:1.6.4\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - \"/var/run/docker.sock:/var/run/docker.sock\"\n      - \"/mnt/traefik:/etc/traefik/\"\n    command: traefik --docker --docker.domain=docker.localhost --logLevel=INFO --docker.watch --acme --acme.onhostrule --acme.httpchallenge.entrypoint=http --acme.storage=/etc/traefik/acme/acme.json --acme.email=dev@jas.bio --acme.entryPoint=https --entryPoints='Name:http Address::80 Redirect.EntryPoint:https' --entryPoints='Name:https Address::443 TLS' --defaultentrypoints=http,https\n    #command: traefik --web --docker --docker.domain=docker.localhost --loglevel=INFO\n    networks: \n      - gateway\nnetworks:\n  gateway:\n    driver: bridge\npgweb\npgweb:\n    container_name: pgweb\n    image: sosedoff/pgweb\n    restart: unless-stopped\n    networks: \n      - traefik_gateway\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.frontend.rule=Host:db.abc.com\"\n      - \"traefik.acme.domains=db.abc.com\"\n      - \"traefik.port=8081\"\n    command: pgweb --host postgres.abc.com --user abc --pass abc --db abc --ssl disable\nLemme know if you have further questions\nThx\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @jas99 btw was that a traefik config or docker compose or something else? i was replying from email after looking at your sample config, which didnt really ring a bell. . @jas99 i was able to start both services with a modified config:\n\n```yaml\nversion: \"3.2\"\nservices:\n  traefik:\n    container_name: traefik\n    restart: unless-stopped\n    image: traefik:1.6.4\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - \"/var/run/docker.sock:/var/run/docker.sock\"\n      - \"/Users/sosedoff/docker/traefik:/etc/traefik/\"\n    #command: traefik --docker --docker.domain=docker.localhost --logLevel=INFO --docker.watch --acme --acme.onhostrule --acme.httpchallenge.entrypoint=http --acme.storage=/etc/traefik/acme/acme.json --acme.email=dev@jas.bio --acme.entryPoint=https --entryPoints='Name:http Address::80 Redirect.EntryPoint:https' --entryPoints='Name:https Address::443 TLS' --defaultentrypoints=http,https\n    command: traefik --web --docker --docker.domain=docker.localhost --loglevel=INFO\n    networks: \n      - gateway\npgweb:\n    container_name: pgweb\n    image: sosedoff/pgweb\n    restart: unless-stopped\n    networks: \n      - gateway\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.frontend.rule=Host:db.abc.com\"\n      - \"traefik.acme.domains=db.abc.com\"\n      - \"traefik.port=8081\"\n    command: pgweb --bind=0.0.0.0\nnetworks:\n  gateway:\n    driver: bridge\n```\nHad to modify my /etc/hosts file with 127.0.0.1 db.abc.com. \nI think main reason why you got 502 error is because pgweb did not start correctly due to invalid credentials / etc. . Im closing this issue since it has nothing to do with pgweb itself. To reiterate the issue: pgweb does work just fine with docker compose / kubernetes / swarm or any other container scheduler so if something does not work correctly make sure to double check the start arguments and network configuration. Also, feel free to ask any questions in this tread just in case. . While the basic ssh proxy feature is available in pgweb, there's no way to configure a ssh jump-host (bastion) at this time. You would probably need to configure the local ssh tunnel through your bastion server using your local ssh configuration files. What's your current setup look like ? . How are you connecting to the postgres server?. I dont have a windows machine around to test this, but could you wrap your connection string with single quotes? At least that is required on mac/linux because of & symbol. 0600 is not an error, its the required permissions mode on those certs. most likely you have too open permissions right now, so try changing that first. you're almost there.. I dont know, the error is coming from the pgweb's dependency (postgres driver) so you'd have to research on the required permissions level. For the reference: https://github.com/lib/pq/issues/489. Any luck getting everything running properly?. Cool, i'll update the dep. @natiki 0.10.0 is released with updated dependencies.. No idea, the pgweb image is public so it should work. What version of docker are you running? Also, try docker login with your docker user and see if that works. . I think you can start the container with --network=host so pgweb can talk to locally running server.. The way you start container does not look right. The error says that the container you're trying to link is not running. Also, when you're starting container with host network you dont need to specify ports because they map to the host automatically. Finally, when you're using certs with pgweb you have to make sure they're in container, or at least they're mounted via a volume. \nAlternatively, you can run pgweb without host network, but you will need to change the DATABASE_URL from postgresql://root@localhost to postgres://root@db. I'd try that first. @natiki Im closing this issue for now. Seems like a docker problem. I just tested the pull with a fresh DO server and was able to pull the image without any login. Also, here's the issue for reference: https://github.com/docker/hub-feedback/issues/1098. . Here's the baseline script for you to get started with pgweb and cockroach db using docker containers:\n```\ndocker run -d --name=cockroach-server -p 26257:26257 -p 8080:8080 cockroachdb/cockroach:v2.0.5 start --insecure\ndocker exec -it cockroach-server ./cockroach sql --insecure -e 'CREATE DATABASE test'\ndocker run -p 8081:8081 --link=cockroach-server -d -e DATABASE_URL=postgres://root@cockroach-server:26257/test?sslmode=disable sosedoff/pgweb\n```\nAlso, host networking might not work on mac (or win) the same way it works on linux so i cant really say if whatever you're doing is going to succeed. But the example i provided works and should be the same across all stacks. . I don't see what's the problem with current connection screen, you have a very simple (schema) view with a single field to enter Postgres URI. The bare minimum is postgres://localhost/dbname. The standard connection screen covers all the needs for local / remote browsing. For SSH there's no way around, you need all that info. So yeah, users should definitely know about host/port stuff.\nMost users don't even need multi-session mode when working with pgweb locally, you can already connect to the db with: pgweb --host=localhost --db=name.  Maybe i can go a bit further and set --host parameter to be localhost by default, similar to default psql behavior. \nThere's also bookmarks, you can create a bunch of files with connection details and then pick the bookmark to connect to with UI or with pgweb -b my_db_bookmark. . @leotu could you be more specific with your request?. Closing until there's some background for this issue. Personally, i don't have time to research into Greenplum integration, so cant really add any useful info. . Yeah i think i've seen this error, you need to connect to a valid user schema, not the virtual one like in your screenshot. I have ran only basic tests with cockroach, so its not officially supported by pgweb at this time, although they're compatible. Closing this for now: feel free to reopen if you'd like to improve integration with cockroachdb. New env var URL_PREFIX is available in 0.10.0 release. Which version of pgweb are you using ?. I'll have a look. @ric2z  is this still an issue for you? i tried to replicate your setup, but i had no problem seeing different schemas in my test db on heroku.. Finally was able to replicate the issue on my machine. There's a bug with row estimation query that causes this error. Will fix.. This issue has been fixed, please upgrade to 0.11.0. Nothing is wrong :) The edit feature was never implemented, what you're seeing is a helper functionality to copy the cell contents. . I'll have a look.. Which pgweb version are you using?. Should be fixed with latest 0.10.0 release. @bsx thanks for  the feedback. i'll add the missing aurora stuff (i've used your code as a base) and aws config file support. . @bsx have a look at the latest build here, i've added AWS config support (with profiles). It does work, although it's kinda limited because there's no way to switch AWS profiles using the web UI. . @bsx ping. This is quite interesting. Nothing has changed in regards to the build/distribution process in years.\nI don't have a windows machine around, but i'll ask someone to look into this. . I have ran the binary through Windows Defender Security Intelligence reporting tool as well as VirusTotal database, both reports came back clean. The trigger could be related to the fact that pgweb has embedded content (html/css/js). Closing due to false positive. . Attaching the review from Microsoft.\n\n. Thanks for the PR, i'll give it a try some time this week. . I did try to run the installation steps on OSX but had issues getting the snapcraft running (something to do with homebrew). Are those steps for linux only or they're exactly the same for OSX? \nI'll have some time this week to test again. . Thanks! I re-uploaded the win/amd64 release. Are you using the latest docker image (sosedoff/pgweb:0.11) ? In any case - i'll have  a look. . I just tried to replicate your issue by having pgweb started using DATABASE_URL env var:\n\nstarted postgres\nstarted pgweb with DATABASE_URL\nmade a few queries\nkilled postgres\npgweb now shows the login screen\nstarted postgres again\nrefreshed the pgweb page, seeing the same database objects again after a few seconds\n\nThe change you've mentioned (#399) does not make any difference here since it only runs during the start up. . Oh i see what you mean, i just noticed that when i was running another test. . What's with the blank space in the base64 file? Also, i don't see it being included in the main index.html file. . Found the issue. Basically, the pgweb docker image does not have USER env var and libpq library fails to detect the current user, which we use as a default like psql does. Will fix.. The issue with docker image has been fixed in pgweb v0.11.1. . Spam.. Well, if the authentication credentials are correct - yes, you should be able to start pgweb and access it on port 8081. Looks like you might have some modifications made to authentication methods in your pg_hba.conf file, which produces the error you're seeing. . Can't reproduce in 0.11.1 (latest) on OSX and with Heroku test database. Need more info: OS, Postgres version. . Alright, thanks for the info. I'll try to fix this today.. Fixed in 0.11.2. . Should be fixed in 0.11.2 release. In order to answer your question i'll need some details about your environment, like what web server are you running, where is it hosted, etc. . If that VPS does not have any other web services you have 2 options:\n\n\nDownload and install Docker and then start pgweb with docker run -d -p 80:8081 sosedoff/pgweb\n\n\nDownload the precompiled binary from releases page (linux/amd64 version), unpack the archive and start pgweb process with pgweb --bind=0.0.0.0 --listen=80. I haven't used CentOS for a long time so im not sure what kind of system manager it runs, but if it happen to be systemd you can use a service file from here: https://github.com/sosedoff/pgweb/tree/master/config. It could, it just depends on how you configure apache virtual hosts file. You would still need to start the pgweb on some port (it runs on localhost:8081) and run it in the background. Then Apache would proxy traffic to it from your subdomain. \n\n\nThere's an old comment about how to run pgweb and nginx: https://github.com/sosedoff/pgweb/issues/84#issuecomment-96741961. Please make sure to password protect your subdomain. You can either enable password in pgweb (pgweb --auth-user=admin --auth-pass=pa$$word) or with apache's htpasswd. . @jmptrader not sure what this PR is meant to do. care to elaborate? . Are you on OSX? It might have something to do with database credentials. Here's my pg_hba.conf file:\n```\nTYPE  DATABASE        USER            ADDRESS                 METHOD\n\"local\" is for Unix domain socket connections only\nlocal   all             all                                     trust\nIPv4 local connections:\nhost    all             all             127.0.0.1/32            trust\nIPv6 local connections:\nhost    all             all             ::1/128                 trust\nAllow replication connections from localhost, by a user with the\nreplication privilege.\nlocal   replication     all                                     trust\nhost    replication     all             127.0.0.1/32            trust\nhost    replication     all             ::1/128                 trust\n```\nMake sure you have postgres user created. Other than that you should be able to run make test without any problems. \n. I posted a few more notes. As for COPY with psql - im okay with that approach as long as it works with SSH tunnel (see database dump feature). Yeah, tests are kind of tricky due to the way how pgweb is written (this'll change with 1.0 version). \nI'd be interested in a test in client_test.go that handles the actual CSV import, don't bother too much with testing the API endpoint. If you're not sure how to proceed - do the full squash of all your changes and i can add the tests. . 1. ParseFieldDelimiter belongs to api/helpers.go\n2. Replace regular expression with case-insensitive and shorter version: (i?)^[\\w][\\w$]*$\n3. Do not panic in isPostgresqlIdentifierRequiringNoQuoting, just return an error (and handle it). Sorry, i've been busy with other stuff. I'll definitely test drive the updated features this weekend and hopefully merge. . Could probably use %v or %d for port here\n. I used latest versions of all dependencies at a time (about 3 weeks ago).\n. Pgweb is not considered to be used as a \"real\" api. So its api is exposed solely to be consumed by the frontend application.\n. I understand that, but while i was testing int64/float64 queries i noticed that when i fetch table rows without a query (basically a SELECT * FROM table query) rows for real/double precision columns identify as float64 in go, but when i ran a custom query, like SELECT val FROM table, the column type was set to string. While you're saying that float64 conversion is not necessary, it is in fact necessary to correctly format the value in order to properly render it in javascript. Otherwise, formatting float64 values via go's json will cause the same issue as with int64 values. Maybe im missing something, but without above mentioned conversions i could not get the numbers to show up correctly on the frontend.\n. Ah, i get what you're referring to now, sorry it was not clear to me what the problem was at first.\nI'll probably grab a few parts of this PR. Thanks\n. Do not use this error definition pattern. Its not being used anywhere in the codebase.\n. When bookmark does not exist, this line triggers panic. It should really be \nreturn Bookmark{}, fmt.Errorf(\"bookmark does not exist:\", bookmarkName)\n. Whats the point in returning error here ?\n. You can use latestRelease.GetName() instead. This change will block the server from starting until the user types Y/N. \nWe could try and figure out if it's possible to do a background update until the next start, which is\na definitely better UX here. . Better to say why the update has failed instead. URL format here is wrong, it should be /releases/download/v%s/%s.\nI tried running the auto-update and it panic'ed because of the wrong url.. I get permission denied on OSX, this is due to invalid binary path. My local pgweb is actually installed at /Users/sosedoff/go/bin/pgweb. StartProcess will terminate pgweb if it's unable to start the process. I get permission denied error, which might be unclear to a regular user. . Btw, fmt.Error does not print anything to stdout/stderr, this method is used to generate an error from the formatted string, like fmt.Sprintf. You can print with fmt.Println(err) and i'll work just fine. . Use camelCase for vars: fieldDelimiter. Use badRequest(c, err) instead of c.JSON(400, Error{err.Error()}). Same applies to the rest of the error checking. . You can add a db.CreateTable / db.DropTable functions instead of using db.NewTable which does not do any specific work, you can use db.query instead. . We should cleanup that newly created table (and only if it was created during import) if the insert fails. Alternatively you can perform the import in a single transaction so if something fails the whole import is rolled back and there would not be any unwanted artifacts. . Yep, i didn't realize at which package i was looking at a the time. db.query is not available in this case.\nI suggest you rename NewTable func into something else, like QueryWithoutHistory, QueryInternal, or Exec so that we can use it for any queries that don't need to be recorded. . Im fine with your proposal, that approach would be similar to the sql import (using psql) and in case of an error it's up to the user to decide what to do next. . Needs to be camelcase'd . Could you reformat to /import/csv?. ",
    "stevencothren": "Ok. This is the change I made previously here : https://github.com/sosedoff/pgweb/pull/3\nIt was pointed out there that this change results in the repository still not being 'go get\"-able.\nI see now that you replied there too.\nIs there no way to resolve both issues? Just curious\n. It actually looks like the absolute paths are only included when go-bindata is run with -debug option.\nWhat do you think about including the non-debug version in the repository? \nAlternatively, I have been looking at adding an option to go-bindata to use relative paths, which would mean you could include the debug version along with that option.\nThoughts?\n. Alright, that makes sense. I was originally thinking that adding bindata.go would be the best, but then I changed my mind.\n. ",
    "cbandy": ":-1:\nThis repo is not \"go get\"-able unless it commits all generated files.\nThe proper fix is to either:\n1. Stop using go get to fetch its own dependencies.\n2. Commit bindata.go so that a fresh checkout contains code that compiles.\n. 8080 is referenced other places. See e80b90710b2d96b13ff2774a19ae956b77c00e31.\n. Foreign keys seems reasonable. +1\nGuessing foreign references sounds less good. -1\n. I think we should support multiple schemata. The simplest approach would be to show all of them all the time.\nSomething nicer may be to show those for which the current user has USAGE privilege. This could change with a SET ROLE statement, so maybe a button to refresh on demand.\nBasing the list on search_path also makes a lot of sense. Again, this could change, so a refresh button may be appropriate.\n. #33\n. :thumbsup:\n. > I had to do in the end is explicitly set GRANT ALL ON TABLE ______ TO vagrant; where ______ means doing the same thing for every table in the database. \n9.x has ALL TABLES IN SCHEMA schema_name\n. > now it will require recompilation of assets every time they change (run make dev) which will slow down development flow a lot.\nI don't understand. The purpose of go-bindata -debug is to not compile assets during development. When local development is complete, you do a make build then commit. One build per... feature.\n. > go-bindata with -debug flags embeds user's full path into the bindata.go file\nYes, debug bindata should not be committed--it is for local development. From the docs:\n\nyou do not want to rebuild the whole server and restart it every time you make a change to a bit of javascript. You just want to build and launch the server once. Then just press refresh in the browser to see those changes... When you are finished developing and ready for deployment, just re-invoke go-bindata without the -debug flag.\n\nI suppose I'm just used to this flow. Grammars generate parser Go files, bindata generates asset Go files, etc. Committing changes to a grammar, parser or asset involves a call to make clean build.\nIf it is a burden, we can probably rig up a pre-commit hook that builds and stages bindata.go when committing something in /static.\n. > If go-bindata package provided a way to use 2 files, one for dev (bindata.go) and another one for release (bindata-release.go) that would be way better\nThe -o option lets you specify the output file, but I suspect having two files that export Asset will be irritating as well.\n. I suggest that by version 1.0, we commit compiled assets into version control (making the project \"go get-able\".) I've added some make targets in #52 that should make building assets for commit easier.\n. @pvh I don't see godep being invoked anywhere. Have you tried this? If it works, I don't think its necessary to commit the Godep workspace.\napp.json has links only to https://github.com/pvh/pgweb.\n. > I don't see godep being invoked anywhere.\nAh, I see now that the buildpack invokes godep.\n. > they don't even have an open source gui tool like pgadmin\nMySQL Workbench?\n. Hmm. The build probably failed due to the colon in GOPATH. Investigating.\n. > The build probably failed due to ...\nI rebased with one fewer commit. Rather than install gox and go-bindata as target dependencies, we call make again to build the dev assets during setup.\n. https://github.com/sosedoff/pgweb/#compile-from-source\n. I don't know Docker very well. Can you explain the changes to Dockerfile?\n. Can't someone who knows how to tunnel just set it up before running pgweb? I think it is normal to setup a tunnel or SOCKS proxy separately from a client. Do other pg clients make this easy?\n1. We could write the feature to support ssh only.\n2. We could allow the user could configure any executable to build the tunnel. ~~It should print host:port or host port when ready.~~\nI've only used Putty on Windows, and it was a long time ago. It supported SOCKS, at least.\n. 2. Fixed in 96d7d6eb9471ebee520440fdd5ebfad58cdb0a36\n4. Fixed in 159f589c0c1ec30346a5b4fda60826e42707dcdd\n. Ah, not sure. I'm not familiar with tools available in Windows powershell (?).\nIf they have make, they probably have awk?\n. I thought of doing that, but what if some other target generates/leaves some file that should be ignored?\n. OTOH, a var like that could be overridden by the environment.\nI'll do that.\n. - Renamed existing variable\n- New variable\n- sed instead of awk\nMy concern about when the variable is expanded was unfounded. This variable is expanded each time it is used.\n. Yep. Works fine for me in bash.\n. Support was added to lib/pq in April.\n. This may be easier to reproduce with pg_sleep().\n. https://golang.org/pkg/database/sql/#Result\n. @akarki15 check the template1 database.\nhttps://www.postgresql.org/docs/current/static/sql-createdatabase.html\n. What if the schema has a . in it?. Godeps is pinned to a lib/pq version from ~3 years ago. The fix for Windows permissions was merged ~2 years ago.. I think --network=host needs to come before the image name.. What error(s) do you get when using Greenplum?. This used to be true when input was only whitespace.\n. This changed in 7f9344a572a354d8343f9e5922a6b6af91c7206c.\n. No version on these two?\n. Is it possible to use make build-assets here?\n. gox is only useful when cross-compiling, I think.\n. bindata.go is under version control. It should not be necessary to build assets here...\n. We don't need to pull in the entire github client for this single call. Their API is quite friendly:\nsh\n$ curl -s 'https://api.github.com/repos/sosedoff/pgweb/releases/latest' | jq -r '.name'\n0.9.11\n```go\nvar latestRelease struct{ Name string }\nresponse, err := http.Get(\"https://api.github.com/repos/sosedoff/pgweb/releases/latest\")\nif err != nil {\n  // TODO\n}\ndefer response.Body.Close()\nerr = json.NewDecoder(response.Body).Decode(&latestRelease)\nif err != nil {\n  // TODO\n}\n```. ",
    "dmnlk": "ok,I'll fix soon.\n. I am  github newbie,therefore cant resolved merge confilcts.\nso,I closed pull request, and do again.sorry\n. see #9 \n. ",
    "jonathanwiesel": "how about homebrew-cask?\n. @sosedoff it also works for different kind of applications, would work on it and update this issue when done :wink: \n. Done. The only issue is that, because the way Github builds release urls, there's not a fixed latest download URL, so the cask definition will need to be updated manually every time there's a new version indicating the version number and the zip's sha256 hash.\n. I would recommend SourceForge, but like you said, for now Github is fine. Either way, this nice project is now installable through Homebrew Cask, I hope more people will adopt it. Keep up the good work :wink: \n. ",
    "arikfr": "Actually homebrew has a binary tap: https://github.com/Homebrew/homebrew-binary (this is what projects like packer.io are using).\n. ",
    "kespindler": "It'd also be useful to know how long an ongoing query has been running. A simple javascript timer on the page would suffice for this purpose.\n. ",
    "codeBearer": "Is it possible to use pgweb as a service? Or what should be the proper way to deploy it? \n. ",
    "spaquet": "+1\n. ",
    "flesler": "I think this should be pretty easy to make.\nYou already made the Activity tab, it has which one is active and what's the pid.\nI imagine active queries having a \"kill\" button that executes this:\nsql\nSELECT pg_cancel_backend(pid);\nAnd Activity tab is refreshed. That should do no?\n. Oops, it does. I was sure it didn't restore it. Maybe because I didn't execute the query.\n. ",
    "bill-myers": "And in fact, even if there are no foreign keys, it should probably look for a table called \"foos\" given a column called \"foo_id\", since things like Rails have a broken default setup that doesn't setup foreign keys.\n. ",
    "champioj": "I agree with cbandy.\nOn the same subject, what about an option to navigate in the other direction?\nYou click on the id of an entity and it shows a list of every linked table. Selecting a linked table would show it filtered with every element referening the entity.\n. ",
    "abourget": "@bill-myers you mean in the \"Content\" view ?\n. So I was thinking of following in the lead of the Google Web Starter Kit, with some Gulp goodness, angular and following the Google best practices for Angular (there's a Google Doc about it somewhere).\nI'm so tired of using pgadmin3 and I think you've found a sweet spot with Go and bindata :-)\n. That would add npm and node as a development dependency.  We can still package the whole thing in one executable.. which is so sweet :)\n. ok, so you don't want any of npm/node stuff in there ?\n. I'll start without\n. Want to vendor in the angularjs dependencies or use CDNs ?\n. Also, which browsers do you want to support ?\n. I guess you're okay with A-grade browsers, as you used jQuery 2+\n. Got it\n. Guys it's all done.. we just need to merge it.  Angular is a fine framework, has massive adoption and talent around it. Don't see why we'd need to rewrite it at this point.\nhttps://github.com/sosedoff/pgweb/pull/32\nWe can migrate to AngularJS 2 when we want.\n. What is that recent announcement ?\n. It's great that there is innovation on that front, and is yet another reason to learn Angular.\nYou prefer frameworks that are stuck in the past ? If Ember doesn't have a path for future greatness, it'll rot and become obsolete, along with your knowledge of it.  I prefer investing my time learning things that I'll be able to use and reuse for many years, even if that means some work at some point.\nAngularJS 1.X won't disappear (the just released Angular 1.3 is already years ahead anyway), but at least there's going to be hope that we can migrate to something even more excellent in the future.. \n. heeheh, sorry @miguelcobain didn't want to irritate.. guess I failed.  Shouldn't have said stuck in the past, should've delayed my response too :) .  Thanks for pointing the Ember 2.0 roadmap.  I actually don't know much about Ember, and I get pretty excited for Angular related stuff. I ask for your forgiveness.\n. I'm doing more Polymer these days :) You're free to pitch it what you want.. I think we should close this one.\nSomeone else will take leadership in doing a solid UI..\n. I think username/password as well as database selection should be done through the GUI.  Somewhat like phpMyAdmin, or any other GUI for databases.  What do you think ?\nI could work on that later tonight, if the AngularJS branch gets merged.  It'd be slicker I think.\n. So, what do you think ?\nI'd like to continue to implement stuff, but don't want to put too much in a single PR.\nAlso,  is there a roadmap or a list of the next things you wanted to work on ?\n. Any news ? \n. Are you interested in learning AngularJS ?\n. /cc @sosedoff whoops, this PR only adds commit 988996b on top of the other AngularJS PR.\n. Yes.. same here.. the table defs aren't defined in the same place perhaps.. I remember using \"information_schema\" or something like that. I know there are two ways of accessing the same meta info.\n. There was no version before, and no version is written inside the files.  Maybe @sosedoff can tell ?\n. Do we need to change it ?\n. I implemented the same change here, but I think there's still a risk of injection, especially since AngularJS takes the table name from the URL you're viewing (that is $stateParams.table).\n. Thanks for the feedback.  Do you want to take a shot at it and refactor it a bit ?\nIt's better if more people get their hands dirty.\n. I skipped the gulp part to avoid overloading with new techs, but through ng-annotate, minifers deal with those things very cleanly. We can cross that bridge once there.\n. Which binding ?\n. Ok, got it.\n. ",
    "alibitek": "This is an important feature, it is very rare in my experience that I see tables in the default public schema. \nUsually they are in different schemas with different security definers / policies in place where everything in public schema is the API.\nEverything else (tables, internal functions, triggers, views, etc.) is visibile only to a particular user or group of users that are connecting from the backend service to a certain schema name.\n. Related with #21 \n. \n\n. ",
    "cryptix": ":+1: \n. ",
    "dmelamedcl": ":+1: \n. ",
    "jordanarseno": ":+1:\n. ",
    "cjthompson": "I just ran into this problem also.  My database doesn't use the public schema, so I can run queries but cannot select any tables.\n. ",
    "jerome-peng": "yeah,I just ran into this problem also. My database doesn't use the public schema, so I can run queries but cannot select any tables.\n. Does this proble solved? \nI use pgweb_windows_386 version 0.5.0, schema support?\n. yes,also waiting this feature\n. ",
    "gglanzani": "Is this also the reason why I cannot see materialized views?\n. @donnex I have been using the --host variant :)\n. Yes. This is especially annoying when doing explain analyze\n. @sosedoff This is still not optimal. Especially in the case of explain analyze you'd like to have all rows expanded, not row by row. With difficult queries with a high level of nesting you want a total overview with the fewest clicks possible.\n. \nLike this?\n. Yes, this fixes it. Thanks.\n. ",
    "alfonsodev": ":+1: \n. :+1:  if you want help push the branch with your progress or your idea how would you like to implement it, \nI really would like to see this feature, wouldn't mind to help. \nthanks for your effort! I use this tool every day. :) \n. :+1: \n. ",
    "dreuse": ":+1: I just ran into this problem also. My database doesn't use the public schema, so I can run queries but cannot select any tables.\n. ",
    "brian-vogogo": ":+1: Would like this schema support. Willing to help.\n. Yeah I agree. Meant this is a minor feature suggestion. If you are open to a pull request I can get it going. If not, no worries - thanks for a great tool.\n. Simplifying the select list seems to help (e.g. changing from t.* to count(*))\n. I think that fixed it. Will reopen if I see it again. Thanks!\n. psql does it somehow:\n```\n$ psql mydb\npsql (9.3.9)\nType \"help\" for help.\nmydb=# update mytable set color = 'blue'\nUPDATE 3\n```\nbut not sure how it does it.\n. :+1: Looks good!\n. ",
    "sammcj": "+1\n. ",
    "miguelcobain": "With the recent announcements regarding AngularJS 2, I think EmberJS is a safer choice.\nThere will be no migration path between Angular versions.\nJust a heads up. It might be worth to wait for Angular 2 as well.\n. @abourget Check http://jaxenter.com/angular-2-0-112094.html\ntl;dr:\n\nIt\u2019s also been confirmed that there will be no migration path from Angular 1.X to 2.0.\n\nI come from an EmberJS background and I was astonished by this announcement. Ember community also puts migration and incremental updates at first.\nSo, it looks like upgrading to AngularJS 2 won't be as easy as \"when we want\".\nBut maybe by then there will be other solutions for that!\nGreat job on #32!\nAnd sorry for not checking the PR's before, I didn't know you already wrote the Angular version!\n. @abourget \nFirst of all, I absolutely DIDN'T mean to turn this into a silly Ember vs Angular discussion. I'm definitely not a fanboy of anything, so I really feel my opinion is unbiased.\nBoth frameworks are really really awesome, but they're also different. I see frameworks, libraries, operating systems, etc, as tools. You wouldn't fasten a nail with a screwdriver, right? This happens in developers everyday lives as well. Angular and Ember must have their own set of problems and users.\nI like Angular, just didn't like that announcement of Angular 2. And many people using angular in production or OSS projects like pgweb didn't like this as well...\nHowever, I must disagree in one thing. Ember isn't at all stuck in the past. There is a difference between stability and stagnation that people don't seem to understand. Ember is stable, but not stalled. Ember's motivation is Stability without Stagnation (credits to @tomdale on this one :D).\nConsider reading the brand new Ember 2.0 plan and see the differences for yourself. Also, it is known that Angular will implement features as routing, es6, etc, that Ember is supporting right now.\nEmber is now looking at things like Virtual DOM (definitely isn't \"past\") and borrowing things from React and Angular itself.\nI wouldn't say Ember is ahead of Angular and neither vice-versa. They've just taken different paths.\nAnyway, my point was to add something of value to this project. And I said that without seeing your PR, and I'm terribly sorry for that!\nI seriously believe pgweb is very well served with your hard work at #32! I didn't mean to devalue your work, sorry!\n. ",
    "kenhahn85": "Is this conversion still in progress? ReactJS has taken the JS community by storm, and is a very maintainable way to build front-end apps.\n. ",
    "tindzk": "I noticed the same bug in the last two pgweb versions. My table is called \"Client\", but when I click on it in the sidebar, the \"Rows\" window shows the following error:\n\nERROR: pq: relation \"xyz.client\" does not exist. \n",
    "waltonseymour": "Yes, it defaults the password to the empty string and it works for users without passwords on my machine.\nFeel free to test it out yourself before merging.\n. Should have the proper behavior now\n. Fair enough. Using the URL argument would also work. I felt an additional argument would make things more explicit to the end user.\n. ",
    "exslim": "It's not needed I guess. You can prepend your command with PGPASSWORD and it will do the trick\n PGPASSWORD=mypass ./pgweb --user myuser --db mydb\n. ",
    "davetoxa": "Yeah, thanks\n. @sosedoff :+1: \n.  @sosedoff done :smiley: #40\n. ",
    "srijs": "The golang parent image is set up in such a way that it is naming the binary after the directory it is produced in. I changed the working directory to /go/src/pgweb, and as a consequence, now the command calls \"pgweb\".\n. Utilising the ENTRYPOINT directive for this is not a good practice. You are sacrificing a clear and idiomatic behaviour for a small and subjective improvement in call syntax. I would like to encourage you to read the Dockerfile best practices, especially the section about ENTRYPOINT.\nThe environment variables I had introduced could be overwritten from the outside, e.g. docker run -e PGHOST=192.168.1.2, allowing for configuration without explicitly having to set command line flags. This plays well with existing docker tooling like fig, and is in the spirit of the twelve-factor app manifesto.\n. @sosedoff Maybe someone can explain what was wrong with the old Dockerfile? What have been the shortcomings that were addressed through this pull request?\n. ",
    "chancez": "Turns out this works, but it seems kind of pointless to use CMD and env variables when you could have simply used entrypoint to set flags. \n. Mostly for running simple one of queries on a postgres database I use for personal projects. I use docker for everything, so it made sense to use pgweb in a docker container.\n. Alright. I can fix the env naming, would you like to see every flag available as an env variable then?\nAbout the imports, I didn't even notice. I use goimports which wraps gofmt, and separates stdlib and external imports automatically.\n. @sosedoff Updated\n. ",
    "bgruszka": "I agree with @gglanzani \n. @donnex how did you set password without url option? ;)\n. @rainbow-workspace thx, it works well :)\n. ",
    "donnex": "I had the same problem, try to use --host --db etc. instead of URL. At least that workaround fixed my problem.\n. Ah nice, I forgot to use the boot2docker IP when trying to reach my Docker container. It bind to all interfaces just as you said.\nI think it would be better to bind to localhost default and allow the users to override the listen adress. This way you don't leave it open by accident without knowing what IP it listens to.\nThe message says \"To view database open http://localhost:8081 in browser\", that made me believe it listened to localhost only per default.\nFor example the Django does it this way. Default to only bind to localhost and allow override if needed.\n. ",
    "rainbow-workspace": "Before this is fixed, appending ?sslmode=disable to the url seems to work fine for me on Postgres 9.3, like:\npgweb --url postgres://postgres:postgres@localhost:5432/db?sslmode=disable\n. ",
    "saulshanabrook": "I can't get -ssl disable to work either in the CLI but does work in the web app.\n$ pgweb --ssl disable --host=(boot2docker ip ^&-) --user=postgress --port=5432\nPgweb version 0.4.1\nConnecting to server...\nError: pq: SSL is not enabled on the server\n. no error when passing foobar\nbash\n$ gweb --ssl foobar\nPgweb version 0.4.1\nStarting server...\nTo view database open http://localhost:8080 in browser\n[GIN] 2014/12/12 - 18:05:59 | 200 |     267.56us | 127.0.0.1:57641  GET /\n[GIN] 2014/12/12 - 18:05:59 | 200 |   2.383953ms | 127.0.0.1:57641  GET /static/css/bootstrap.css\n[GIN] 2014/12/12 - 18:05:59 | 200 |   1.556328ms | 127.0.0.1:57646  GET /static/js/ace-pgsql.js\n[GIN] 2014/12/12 - 18:05:59 | 200 |   9.165307ms | 127.0.0.1:57645  GET /static/js/ace.js\n[GIN] 2014/12/12 - 18:05:59 | 200 |   3.285483ms | 127.0.0.1:57644  GET /static/js/jquery.js\n[GIN] 2014/12/12 - 18:05:59 | 200 |    260.692us | 127.0.0.1:57643  GET /static/css/app.css\n[GIN] 2014/12/12 - 18:05:59 | 200 |    633.444us | 127.0.0.1:57642  GET /static/css/font-awesome.css\n[GIN] 2014/12/12 - 18:05:59 | 200 |    370.716us | 127.0.0.1:57641  GET /static/js/app.js\n[GIN] 2014/12/12 - 18:05:59 | 400 |     80.391us | 127.0.0.1:57641  GET /info\nexit\n^C\u23ce\n. ",
    "brianlow": "Ctrl+Enter perhaps.\nIt will depend on what key combinations can be captured from the browser. \njsfiddle.com uses ctrl-enter. Works for me on OSX with Chrome and Safari.\n. ",
    "mephux": "Is it really a smart idea to couple the view with business logic? Wasn't the whole point of jQuery to write unobtrusive javascript?\njs\n<input type=\"button\" id=\"run\" value=\"Run Query\"\n+               ng-click=\"doQuery(query)\"\n+               ng-disabled=\"loading\"\n+               class=\"btn btn-sm btn-primary\" />\nMaybe i'm old fashion but this just seems wrong? Can't you at least use the data- prefix so it's valid html?\n- Edit: if you're going to rewrite this for no reason - why angular? To me the code just looks overly complex for no reason. Just trying to understand but obviously I have no skin in the game.\n. ",
    "benjamin-thomas": "What's the status on this?\nI tried the latest binary (0.5.1) and sorting is not available. Do you have a roadmap?\n. ",
    "kohenkatz": "I'm running pgAdmin on my Windows Host machine, and connecting to the IP address of the Vagrant machine, exactly the same as I'm trying to do with pgweb.\n. The Linux version running on the VM does exactly the same thing:\n```\n./pgweb_linux_386 --url=\"postgres://vagrant:vagrant@192.168.248.33/mydbname\"\nboth versions do the same thing\n./pgweb_linux_amd64 --url=\"postgres://vagrant:vagrant@192.168.248.33/mydbname\"\n```\nshows this:\nConnecting to server...\nChecking tables...\nError: Database does not have any tables\n. The Vagrant box is running Ubuntu 14.04 LTS and Postgres 9.3 installed from the package manager.\n. Running psql -h 192.168.248.33 -U vagrant -W mydbname and \\dt works perfectly - it lists all of the tables.\nHere is the query shown in the query log when I run \\dt:\n2014-10-29 02:58:28 GMT 545057d1.287b vagrant mydbname LOG:  statement: SELECT n.nspname as \"Schema\",\n      c.relname as \"Name\",\n      CASE c.relkind WHEN 'r' THEN 'table' WHEN 'v' THEN 'view' WHEN 'm' THEN 'materialized view' WHEN 'i' THEN 'index' WHEN 'S' THEN 'sequence' WHEN 's' THEN 'special' WHEN 'f' THEN 'foreign table' END as \"Type\",\n      pg_catalog.pg_get_userbyid(c.relowner) as \"Owner\"\n    FROM pg_catalog.pg_class c\n         LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace\n    WHERE c.relkind IN ('r','')\n          AND n.nspname <> 'pg_catalog'\n          AND n.nspname <> 'information_schema'\n          AND n.nspname !~ '^pg_toast'\n      AND pg_catalog.pg_table_is_visible(c.oid)\n    ORDER BY 1,2;\n. I am able to run pgweb with that URL, from both my Windows machine and my Vagrant VM.\n. I can post my postgresql config files somewhere if that helps.\n. @Jellyfrog The default schema is named public - and my database is using the default.  As I mentioned previously, if I run that exact query in pgAdmin on Windows or in psql on Linux, I get the table list as expected.\n. Here are my config files: https://gist.github.com/kohenkatz/d53eeafa27e33fb623ed\nOther than adding two access lines in pg_hba.conf and two logging lines in postgresql.conf, the two files are completely Ubuntu default settings, AFAIK.\n. I tried turning off SSL on the server so that I could use WireShark on the connection to see if that shows anything.  (This requires adding ?sslmode=disable to the end of the connection URL.)\nI'll see if that turns up anything interesting.\n. @fijosh Actually, you are right that running that exact command gives no output. The query only returns results when I run it as the postgres user or as the owner of the database (the application-specific user).\nHowever, it doesn't seem that granting access to the public schema actually fixes this.  I tried all of the following, with no success:\n- GRANT ALL ON SCHEMA public TO vagrant;\n- GRANT USAGE ON SCHEMA information_schema TO vagrant;\n- GRANT USAGE ON SCHEMA pg_catalog TO vagrant;\nWhat I found that I had to do in the end is explicitly set GRANT ALL ON TABLE ______ TO vagrant; where ______ means doing the same thing for every table in the database.  I would like to think that there's an easier way to do that, but that is all I could find.\n. ",
    "Jellyfrog": "Same here.\nIts because of: \nPG_TABLES        = \"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' ORDER BY table_schema,table_name;\"\nhttps://github.com/sosedoff/pgweb/blob/bf2df4e74f875e1aa3faa392ddde1fc3bcbf6698/client.go#L14\ntable_schema = 'public' we don't have a schema named public\nAlso; Why do we need tables to start the UI? Maybe you want to run CREATE TABLE.. in the UI?\n. ",
    "fijosh": "I am able to reproduce the problem when I use an user that doesn't have permissions to see the 'public' schema.\nWhen I user 'postgres' user which has the permission, the problem does not occur.\n@kohenkatz do you really see some table(s) when you run\npsql -h 192.168.248.33 -U vagrant -W mydbname -c \" SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' ORDER BY table_schema,table_name;\"\nEither way, @sosedoff I think it would be better to rewrite the query for schemas and tables, in order to list tables from all available schemas, not only from public (which is not always accessible, as it looks to be the issue here).\nFor example, apart from the 'public' schema with only table 'dual', I have also 2 other schemas with tens of tables in the DB, but in the WebUI I can only see the sad and empty table 'dual'.\nIf you already have this on the roadmap or maybe I am doing something wrong, then I apologize, I just stumbled upon this here and had very little time to look around so far ;)\n. @sosedoff agreed on that, definitely not a small fix, but it will bring so much value to this tool :)\nAnyways I would discard this issue as it seems it was simply an issue with permissions for given user\n. ",
    "ajunior": "+1\n. ",
    "jartek": "Fixed in https://github.com/sosedoff/pgweb/pull/42\n. Added a -s flag in https://github.com/sosedoff/pgweb/pull/50\n. ",
    "pvh": "@sosedoff so, thoughts on this one? Given that vendoring bindata.go and Godeps appears (to my surprise) to be good practice, I guess I would kind of consider accepting this if it were me. The \"allow no tables\" commit is probably the only patch here that might be controversial.\nAre you interested in merging some form of this PR?\n. Ah, yeah, the app.json needs fixing. I'll update that in the PR. \nIn order for the Heroku button to work, it needs the compiled assets... or at least the debug assets. It's a similar problem to go get we push the code to Heroku to compile and later run, but we respect the standard go build interface and don't know about Makefiles and things.\nSimilarly, committing the current working versions of the Godeps is necessary to guarantee reproducibility. If someone breaks one of the upstream dependencies this guarantees that we'll still be able to build pgweb. This is again (as I understand it) best practice for godep. \n. Oh, also - any suggestions on a logo image to use? I used Dan's github avatar...\n. @sosedoff See my comment on --bind= on the previous patch. If you change that, you can take it out of the Procfile. Other than that, I added basicauth support with this patch as well and tested it out. It's pretty cool to deploy pgweb with a single click! I wish Heroku had some way of pulling in a DATABASE_URL from some other app by default but a user will still have to configure that for themselves right now. So it goes.\nAnyway, let me know if you want any other changes to this PR.\n. Yeah, I just used --bind= but binding to localhost specifically felt really unusual to me for a web service and I assumed that it was an oversight.\n. Cool, I think that makes sense. You wanna merge this thing? I hate having a PR open for a long time. Would it help to decompose it or rebase it or something?\n. Yusss. Thanks!\n. sweet.\n. Back when I was a Windows user there was a pretty standard tool for solving\nthis problem that most people used to create ssh tunnels. I wish I could\nremember what it was called. Maybe I used to use Putty for that? Could be.\nI agree with Chris' assessment which is that merging this functionality\ninto pgweb would be weird. Maybe not full-on wrong, but not really high on\nthe list of things I'd expect such a system to do. I'd probably start with\na documentation fix and go from there, particularly since each platform\nwill be different and probably get more different over time.\n-p\nOn Fri, Nov 21, 2014 at 12:59 AM, Chris Bandy notifications@github.com\nwrote:\n\nCan't someone who knows how to tunnel just set it up before running pgweb?\nI think it is normal to setup a tunnel or SOCKS proxy separately from a\nclient. Do other pg clients make this easy?\n1. We could write the feature to support ssh only.\n2. We could allow the user could configure any executable to build the\n   tunnel. It should print host:port or host port when ready.\nI've only used Putty on Windows, and it was a long time ago. It supported\nSOCKS, at least.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sosedoff/pgweb/issues/66#issuecomment-63942283.\n. \n",
    "fdr": "On Nov 6, 2014 1:41 AM, \"Peter van Hardenberg\" notifications@github.com\nwrote:\n\nYusss. Thanks!\n\nI just tried it and it worked great!\n. ",
    "smurfpandey": "@santagada I know this is a old thread, but I wanted to share my fork in case you still need a similar tool for MySQL.  https://github.com/smurfpandey/mysqlweb. @sosedoff About the multiple pgweb sessions in different tabs, I have implemented this in my fork mysqlweb.\nBasically I am storing a UUID for each new connection, and then sending that UUID on the frontend. On every API request, that UUID is passed in header.\n1. Return UUID when new connection is created: https://github.com/smurfpandey/mysqlweb/blob/master/client.go#L50\n2. Pass this UUID on each API call: \n   https://github.com/smurfpandey/mysqlweb/blob/master/static/js/app.js#L1430\n   https://github.com/smurfpandey/mysqlweb/blob/master/static/js/app.js#L57\n3. On the backend get the the DB connection for this UUID\n   https://github.com/smurfpandey/mysqlweb/blob/master/api.go#L128\nHope this helps :)\n. @sosedoff I implemented the same feature in mysqlweb using bootstrap-tabs. Every tab contains it's own query window & result window.\nIf it helps:\nhttps://github.com/smurfpandey/mysqlweb/commit/cdedd5f8f920e2b933c6f915e624f5cecfa5713d\nhttps://github.com/smurfpandey/mysqlweb/commit/ba875b6c487cd64d28ba428636e7c05ef7d67a04. ",
    "llitfkitfk": "All right. \n. How can I remove the app.js commits\n. @mbertheau Thanks a lot.\n. @cbandy \ndocker is an engine. or simply, you can just image the Docker like a virtrual machine\n1. change to get go-bindata:\n   I try the previous docker demo.\n   It show Error reading asset static/index.html at /my-path-on-my-host/to/static/index.html: open /my-path-on-my-host/to/static/index.html: no such file or directory.\n   I think it needs to be reconvert the static file during buiding the docker image.\n   So, I change it with the Make file command.\n2. make build-asserts:\n   You are right. And I will fix it.\n. @sosedoff \nfor the Error reading asset static/index.html problem.  If it is caused by different path of $GOPATH, when I build the project on my own: using  make clean  && make buid && make dev.\n. @sosedoff got it\n. @IIInsomnia \npgweb \u4f7f\u7528 go-bindata \u7f16\u8bd1\u524d\u7aef\u6587\u4ef6 \n\u4f60\u53ef\u4ee5\u5b66\u4e60\u4e00\u4e0b https://github.com/jteeuwen/go-bindata. https://github.com/jteeuwen/go-bindata. ",
    "corbanb": "thanks guys!\n. @sosedoff I guess I should of asked this also. Are you planning on having one that will stay updated? :smile: \nP.S. This is the exact kind of project I have been needing. thanks for the work!\n. ",
    "mbertheau": "@llitfkitfk use git rebase -i as described in http://git-scm.com/book/en/v2/Git-Tools-Rewriting-History\n. ",
    "bcho": "Oh, that must be my network's problem! I can get the correct file now, thanks! :beers:\n. ",
    "dufferzafar": "@sosedoff Has this been put off-shelf or will this be worked on now?\nHaving an inline editor would be really great :)\n. ",
    "petere": "Yes.\n. ",
    "domjitsu": "Sweet!\n. ",
    "tlhunter": "A DROP TABLE / ALTER TABLE command would also be good candidates for updating the table listing.\n. ",
    "AkeemMcLennon": ":+1: \nIm really interested in seeing this feature :-) \n. ",
    "avitex": "Any news on this being pulled in? :smile: \n. @sosedoff Cheers :+1: \n. ",
    "ManuelWeiss": "Can you please merge this PR? Seems like a lot of people are waiting for this feature.\n. +1\n. ",
    "insectatorious": "@sosedoff Doh! You are correct sir. Fixed it and it's working like a charm! Thanks.\n. ",
    "lfx": "Hello, @joshstrange I think this may be your case http://blog.lerner.co.il/quoting-postgresql/ \n. ",
    "Garbee": "ah, nginx proxy! I was looking for some way to handle it with my current stack, forgot it does proxying. Thanks for the recommendation.\nThis issue may help others too searching at least. Will close up since that handles what I need.\n. For anyone else wanting this, I setup an nginx site with this config:\n```\nserver {\n    listen [::]443 ssl spdy;\n    listen 443 ssl spdy;\nservername sub.site.com;\n\ninclude h5bp/directive-only/ssl.conf; # From the H5BP Nginx Server Configs, very handy stuff.\n\nlocation / {\n    proxy_pass http://localhost:9999; # set port/domain to whatever you run pgweb bound to.\n    proxy_set_header Host $host;\n    proxy_set_header X-Real-Ip $remote_addr;  # Not even sure if these last two are needed, just pulled from the nginx wiki example.\n}\n\n}\n```\nBoom, nginx proxying to pgweb just fine with full SSL!\n. @kurotsukikaitou Just search for Apache reverse proxy instructions. That's all my nginx config is doing.\n. Protect it with a password (htaccess) and it should be fine unless it gets brute forced. Otherwise I don't see any other potential hole from it in this context as to any other part of your server getting attacked.\n. Most of the results I've seen are paginated by default. Does your query have its own LIMIT or something on it?\nThat number of results is going to cause quite a massive result set. Unless we separate pagination into a front-end piece in certain contexts I don't know of any other way to solve this. The problem with doing pagination in the front-end though, is then you have literal DB pagination that is done and front-end level. Which is difficult from a user-interface perspective to handle right.\nI think we could improve by perhaps getting the count of returned rows back, and on any calls that will try to display more than like 15k rows or something (we can play around to find a decent number that most systems can handle) then show an alert to the dev that things can be slow and if their goal is to export they could just export it directly without rendering it.. I think a \"Copy as JSON\" button is quite doable for modern browsers. But, a JSON view I feel is not incredibly useful to most users.. My old config post is still working for me. But as comments later brought up do remember to add a note about using basic authentication to protect the system. Since once you log in anyone can hit the URL and get DB access unless you have auth in place.. I don't use the built in auth since that means I need to pull in a user/pass to yet another place on the system. Using nginx's basic auth I can reference my existing accounts in the .htpasswd file for nginx which I use with other protected services as well.\nIf pgweb could pull in the existing file to authorize access then I'd be all for it. But as I see the options now in the help explanation it doesn't pull in an existing access file.. Is there an issue with some background on why this feature should be added, scoping it, and discussing config plans? It seems like a fairly massive PR to dump on a project out of the blue.. Ah ok, I can see that it is mostly the SDK being included looking at the changes again. The files where the diff doesn't load isn't because it's a big change, it's because it's a whole new file so the diff isn't relevant. (Bad github UX)\nI'm just thinking (not a maintainer or anything of the kind) that if one cloud provider is added, then it would be nice to have it done in an adaptable system. So let's say someone wanted to add Google Cloud SQL support or some other provider, they could implement an interface, register it, and then the system could pick things up from there. However, I'm just a pgweb user and not a go author (well, to any extent that matters) so I don't know yet if this kind of system is feasible in go either. Just a random thought to make multiple clouds supported more easily if someone wanted to contribute another one.. CockroachDB is not supported by pgweb. There was a PR to add it but the author closed it until they added test setup so tests get executed on that engine too.. ",
    "kurotsukikaitou": "Any idea how to do it using apache2?\n. Well.. I planning just running it only when I need to access it. Otherwise, I'll kill the process.\n. ",
    "tcn": "Hey,\nNope, console LGTM. Here's the complete output (incl. the GET which returns \"[]\"):\n$ ./pgweb_darwin_amd64 --user foo --host localhost --db foo --pass foo\nPgweb version 0.5.2\nConnecting to server...\nChecking tables...\nStarting server...\nTo view database open http://localhost:8080 in browser\n[GIN] 2015/06/05 - 17:49:27 | 200 |    237.966\u00b5s | 127.0.0.1:63115 |   GET     /\n[GIN] 2015/06/05 - 17:49:28 | 200 |  33.348966ms | 127.0.0.1:63117 |   GET     /static/css/font-awesome.css\n[GIN] 2015/06/05 - 17:49:28 | 200 |  20.655069ms | 127.0.0.1:63115 |   GET     /static/css/app.css\n[GIN] 2015/06/05 - 17:49:28 | 200 |  20.550049ms | 127.0.0.1:63119 |   GET     /static/css/bootstrap.css\n[GIN] 2015/06/05 - 17:49:28 | 200 |  17.738562ms | 127.0.0.1:63121 |   GET     /static/js/jquery.js\n[GIN] 2015/06/05 - 17:49:28 | 200 |  15.190712ms | 127.0.0.1:63123 |   GET     /static/js/ace.js\n[GIN] 2015/06/05 - 17:49:28 | 200 |   5.865284ms | 127.0.0.1:63125 |   GET     /static/js/ace-pgsql.js\n[GIN] 2015/06/05 - 17:49:28 | 200 |    368.272\u00b5s | 127.0.0.1:63117 |   GET     /static/js/app.js\n[GIN] 2015/06/05 - 17:49:28 | 200 |   1.541463ms | 127.0.0.1:63123 |   GET     /api/connection\n[GIN] 2015/06/05 - 17:49:28 | 200 |    1.32848ms | 127.0.0.1:63123 |   GET     /api/tables\n[GIN] 2015/06/05 - 17:49:28 | 200 |   1.871071ms | 127.0.0.1:63123 |   GET     /static/fonts/fontawesome-webfont.woff\n[GIN] 2015/06/05 - 17:49:42 | 200 |   1.413779ms | 127.0.0.1:63123 |   GET     /api/tables\n[GIN] 2015/06/05 - 17:49:42 | 404 |         86ns | 127.0.0.1:63123 |   GET     /favicon.ico\n. No, the tables are actually in a dedicated schema. Found no way to specify it, though.\n. ",
    "RobertAudi": "@sosedoff I just tried v0.6.1. Seems to work. But at first I installed pgweb via brew cask install pgweb, which installed v0.5.2, and I encountered the same problem.\n. ",
    "tobijb": "It happens all the time on a number of our tables.\nHere are the two test cases.\n1) pgWeb 'select * from test_table' (a large table with a relatively simple schema and 1 million rows) and the browser rendered view never returns (get the 'aw snap') error in Chrome.\nCreate 1 million rows\ncreate table test_table as ( SELECT generate_series(1,1000000) AS id, md5(random()::text) AS descr1, md5(random()::text) AS descr2, md5(random()::text) AS descr3, md5(random()::text) AS descr4 );\nPgweb v0.6.2 (git: 68ceb212b583b5737a86f357d05978c551c3351d)\nStarting server...\nTo view database open http://localhost:8081 in browser\n[GIN] 2015/07/29 - 13:12:51 | 200 |   8.028248ms | 127.0.0.1:58739 |   GET     /static/css/font-awesome.css\n[GIN] 2015/07/29 - 13:12:51 | 200 |   1.123979ms | 127.0.0.1:58738 |   GET     /static/css/bootstrap.css\n[GIN] 2015/07/29 - 13:12:51 | 200 |    285.512\u00b5s | 127.0.0.1:58741 |   GET     /\n[GIN] 2015/07/29 - 13:12:51 | 200 |    651.532\u00b5s | 127.0.0.1:58740 |   GET     /static/css/app.css\n[GIN] 2015/07/29 - 13:13:00 | 200 |     391.83\u00b5s | 127.0.0.1:58741 |   GET     /static/css/app.css\n[GIN] 2015/07/29 - 13:13:00 | 200 |   1.972954ms | 127.0.0.1:58740 |   GET     /static/css/font-awesome.css\n[GIN] 2015/07/29 - 13:13:00 | 200 |   3.094008ms | 127.0.0.1:58739 |   GET     /static/js/jquery.js\n[GIN] 2015/07/29 - 13:13:00 | 200 |   3.346939ms | 127.0.0.1:58738 |   GET     /static/css/bootstrap.css\n[GIN] 2015/07/29 - 13:13:00 | 200 |    186.698\u00b5s | 127.0.0.1:58740 |   GET     /static/js/app.js\n[GIN] 2015/07/29 - 13:13:00 | 200 |    234.626\u00b5s | 127.0.0.1:58741 |   GET     /static/js/bootstrap-contextmenu.js\n[GIN] 2015/07/29 - 13:13:00 | 200 |   1.546753ms | 127.0.0.1:58745 |   GET     /static/js/ace-pgsql.js\n[GIN] 2015/07/29 - 13:13:00 | 200 |  15.472393ms | 127.0.0.1:58743 |   GET     /static/js/ace.js\n[GIN] 2015/07/29 - 13:13:01 | 400 |     54.423\u00b5s | 127.0.0.1:58743 |   GET     /api/connection\n[GIN] 2015/07/29 - 13:13:01 | 400 |    185.374\u00b5s | 127.0.0.1:58743 |   GET     /api/bookmarks\n[GIN] 2015/07/29 - 13:13:35 | 200 |   5.467798ms | 127.0.0.1:58743 |   POST    /api/connect\n[GIN] 2015/07/29 - 13:13:35 | 200 |    771.586\u00b5s | 127.0.0.1:58745 |   GET     /static/fonts/fontawesome-webfont.woff\n[GIN] 2015/07/29 - 13:13:35 | 200 |   9.180807ms | 127.0.0.1:58743 |   GET     /api/tables\n[GIN] 2015/07/29 - 13:14:07 | 200 | 24.706151284s | 127.0.0.1:58745 |   POST    /api/query\n[GIN] 2015/07/29 - 13:15:25 | 200 |     282.12\u00b5s | 127.0.0.1:58745 |   GET     /\n[GIN] 2015/07/29 - 13:15:25 | 200 |    936.821\u00b5s | 127.0.0.1:58739 |   GET     /static/js/ace-pgsql.js\n[GIN] 2015/07/29 - 13:15:25 | 200 |   5.241221ms | 127.0.0.1:58741 |   GET     /static/js/jquery.js\n[GIN] 2015/07/29 - 13:15:25 | 200 |    5.03265ms | 127.0.0.1:58740 |   GET     /static/js/ace.js\n[GIN] 2015/07/29 - 13:15:25 | 200 |    1.96152ms | 127.0.0.1:58745 |   GET     /static/css/bootstrap.css\n[GIN] 2015/07/29 - 13:15:25 | 200 |    558.966\u00b5s | 127.0.0.1:58743 |   GET     /static/css/font-awesome.css\n[GIN] 2015/07/29 - 13:15:25 | 200 |    220.591\u00b5s | 127.0.0.1:58738 |   GET     /static/css/app.css\n[GIN] 2015/07/29 - 13:15:25 | 200 |     334.75\u00b5s | 127.0.0.1:58739 |   GET     /static/js/bootstrap-contextmenu.js\n[GIN] 2015/07/29 - 13:15:25 | 200 |    441.934\u00b5s | 127.0.0.1:58738 |   GET     /static/js/app.js\n[GIN] 2015/07/29 - 13:15:25 | 200 |    342.704\u00b5s | 127.0.0.1:58740 |   GET     /api/connection\n[GIN] 2015/07/29 - 13:15:25 | 200 |    738.346\u00b5s | 127.0.0.1:58738 |   GET     /static/fonts/fontawesome-webfont.woff\n[GIN] 2015/07/29 - 13:15:25 | 200 |    1.23567ms | 127.0.0.1:58740 |   GET     /api/tables\n2) pgWeb 'select * from test_table2' (a large table with a relatively simple schema and 5 million rows) and the browser rendered view never returns (get the 'aw snap') error in Chrome.\ncreate table test_table as ( SELECT generate_series(1,5000000) AS id, md5(random()::text) AS descr1, md5(random()::text) AS descr2, md5(random()::text) AS descr3, md5(random()::text) AS descr4 );\n```\nPgweb v0.6.2 (git: 68ceb212b583b5737a86f357d05978c551c3351d)\nStarting server...\nTo view database open http://localhost:8081 in browser\n[GIN] 2015/07/29 - 13:25:42 | 200 |   3.515789ms | 127.0.0.1:58776 |   GET     /\n[GIN] 2015/07/29 - 13:25:42 | 200 |    704.997\u00b5s | 127.0.0.1:58779 |   GET     /static/css/font-awesome.css\n[GIN] 2015/07/29 - 13:25:43 | 200 |  18.812191ms | 127.0.0.1:58783 |   GET     /static/js/jquery.js\n[GIN] 2015/07/29 - 13:25:43 | 200 |    290.342\u00b5s | 127.0.0.1:58780 |   GET     /static/css/app.css\n[GIN] 2015/07/29 - 13:25:43 | 200 |    411.346\u00b5s | 127.0.0.1:58780 |   GET     /static/js/app.js\n[GIN] 2015/07/29 - 13:25:43 | 200 |    448.703\u00b5s | 127.0.0.1:58779 |   GET     /static/js/bootstrap-contextmenu.js\n[GIN] 2015/07/29 - 13:25:43 | 200 |  10.569147ms | 127.0.0.1:58776 |   GET     /static/css/bootstrap.css\n[GIN] 2015/07/29 - 13:25:43 | 200 |  25.179581ms | 127.0.0.1:58784 |   GET     /static/js/ace.js\n[GIN] 2015/07/29 - 13:25:43 | 200 |  17.919021ms | 127.0.0.1:58786 |   GET     /static/js/ace-pgsql.js\n[GIN] 2015/07/29 - 13:25:43 | 400 |     51.242\u00b5s | 127.0.0.1:58784 |   GET     /api/connection\n[GIN] 2015/07/29 - 13:25:43 | 400 |   1.191882ms | 127.0.0.1:58784 |   GET     /api/bookmarks\n[GIN] 2015/07/29 - 13:25:54 | 200 |  81.823107ms | 127.0.0.1:58784 |   POST    /api/connect\n[GIN] 2015/07/29 - 13:25:54 | 200 |   1.832567ms | 127.0.0.1:58786 |   GET     /static/fonts/fontawesome-webfont.woff\n[GIN] 2015/07/29 - 13:25:54 | 200 |  37.664201ms | 127.0.0.1:58784 |   GET     /api/tables\nfatal error: runtime: out of memory\nruntime stack:\nruntime.SysMap(0xc2d9210000, 0x505a0000, 0xc208083b00, 0xb276d8)\n    /usr/local/Cellar/go/1.4.2/libexec/src/runtime/mem_linux.c:149 +0x98\nruntime.MHeap_SysAlloc(0xb2cdc0, 0x505a0000, 0x42e1a2)\n    /usr/local/Cellar/go/1.4.2/libexec/src/runtime/malloc.c:284 +0x124\nruntime.MHeap_Alloc(0xb2cdc0, 0x282ca, 0x10100000000, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/runtime/mheap.c:240 +0x66\ngoroutine 13 [running]:\nruntime.switchtoM()\n    /usr/local/Cellar/go/1.4.2/libexec/src/runtime/asm_amd64.s:198 fp=0xc208156aa0 sp=0xc208156a98\nruntime.mallocgc(0x50592e70, 0x733260, 0x1, 0x602abe)\n    /usr/local/Cellar/go/1.4.2/libexec/src/runtime/malloc.go:199 +0x9f3 fp=0xc208156b50 sp=0xc208156aa0\nruntime.newarray(0x733260, 0x50592e70, 0xc208020a80)\n    /usr/local/Cellar/go/1.4.2/libexec/src/runtime/malloc.go:365 +0xc1 fp=0xc208156b88 sp=0xc208156b50\nruntime.makeslice(0x724f40, 0x50592e70, 0x50592e70, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/runtime/slice.go:32 +0x15c fp=0xc208156bd0 sp=0xc208156b88\nbytes.makeSlice(0x50592e70, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bytes/buffer.go:191 +0x6a fp=0xc208156c20 sp=0xc208156bd0\nbytes.(Buffer).grow(0xc20808a0b0, 0x20, 0x1)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bytes/buffer.go:99 +0x243 fp=0xc208156c98 sp=0xc208156c20\nbytes.(Buffer).WriteString(0xc20808a0b0, 0xc25850b000, 0x20, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bytes/buffer.go:136 +0x4c fp=0xc208156ce8 sp=0xc208156c98\nencoding/json.(encodeState).string(0xc20808a0b0, 0xc25850b000, 0x20, 0xc25850b000, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:854 +0x16d fp=0xc208156d58 sp=0xc208156ce8\nencoding/json.stringEncoder(0xc20808a0b0, 0x732d60, 0xc22f0e9fb0, 0x58, 0xc22f0e9f00)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:548 +0x29f fp=0xc208156e00 sp=0xc208156d58\nencoding/json.(encodeState).reflectValue(0xc20808a0b0, 0x732d60, 0xc22f0e9fb0, 0x58)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:299 +0x72 fp=0xc208156e30 sp=0xc208156e00\nencoding/json.interfaceEncoder(0xc20808a0b0, 0x72f4a0, 0xc25265ce70, 0xd4, 0x72f400)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:557 +0xd9 fp=0xc208156e80 sp=0xc208156e30\nencoding/json.(arrayEncoder).encode(0xc20804e0d0, 0xc20808a0b0, 0x7336e0, 0xc27e79f188, 0xd7, 0x60ad00)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:687 +0x11e fp=0xc208156ee8 sp=0xc208156e80\nencoding/json.arrayEncoder.(encoding/json.encode)\u00b7fm(0xc20808a0b0, 0x7336e0, 0xc27e79f188, 0xd7, 0x282c900)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:694 +0x58 fp=0xc208156f20 sp=0xc208156ee8\nencoding/json.(sliceEncoder).encode(0xc20804e0e0, 0xc20808a0b0, 0x7336e0, 0xc27e79f188, 0xd7, 0x282c9600)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:664 +0xb4 fp=0xc208156f58 sp=0xc208156f20\nencoding/json.sliceEncoder.(encoding/json.encode)\u00b7fm(0xc20808a0b0, 0x7336e0, 0xc27e79f188, 0xd7, 0x733600)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:673 +0x58 fp=0xc208156f90 sp=0xc208156f58\nencoding/json.(arrayEncoder).encode(0xc20804e0e8, 0xc20808a0b0, 0x722ba0, 0xc20811df08, 0xd7, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:687 +0x11e fp=0xc208156ff8 sp=0xc208156f90\nencoding/json.arrayEncoder.(encoding/json.encode)\u00b7fm(0xc20808a0b0, 0x722ba0, 0xc20811df08, 0xd7, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:694 +0x58 fp=0xc208157030 sp=0xc208156ff8\nencoding/json.(sliceEncoder).encode(0xc20804e0f8, 0xc20808a0b0, 0x722ba0, 0xc20811df08, 0xd7, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:664 +0xb4 fp=0xc208157068 sp=0xc208157030\nencoding/json.sliceEncoder.(encoding/json.encode)\u00b7fm(0xc20808a0b0, 0x722ba0, 0xc20811df08, 0xd7, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:673 +0x58 fp=0xc2081570a0 sp=0xc208157068\nencoding/json.(structEncoder).encode(0xc20811c3f0, 0xc20808a0b0, 0x7ebc00, 0xc20811def0, 0xd9, 0xc208157200)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:584 +0x2ba fp=0xc208157248 sp=0xc2081570a0\nencoding/json.structEncoder.(encoding/json.encode)\u00b7fm(0xc20808a0b0, 0x7ebc00, 0xc20811def0, 0xd9, 0xc20811de00)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:598 +0x58 fp=0xc208157280 sp=0xc208157248\nencoding/json.(ptrEncoder).encode(0xc20804e108, 0xc20808a0b0, 0x7e5bc0, 0xc20811def0, 0x16, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:706 +0xeb fp=0xc2081572d0 sp=0xc208157280\nencoding/json.ptrEncoder.(encoding/json.encode)\u00b7fm(0xc20808a0b0, 0x7e5bc0, 0xc20811def0, 0x16, 0xc20811de00)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:711 +0x58 fp=0xc208157308 sp=0xc2081572d0\nencoding/json.(encodeState).reflectValue(0xc20808a0b0, 0x7e5bc0, 0xc20811def0, 0x16)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:299 +0x72 fp=0xc208157338 sp=0xc208157308\nencoding/json.(encodeState).marshal(0xc20808a0b0, 0x7e5bc0, 0xc20811def0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/encode.go:270 +0xca fp=0xc208157390 sp=0xc208157338\nencoding/json.(Encoder).Encode(0xc2860214c0, 0x7e5bc0, 0xc20811def0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/encoding/json/stream.go:160 +0x9a fp=0xc208157438 sp=0xc208157390\ngithub.com/gin-gonic/gin/render.jsonRender.Render(0x7f80e8da8190, 0xc20808a420, 0xc8, 0xc28601d7a0, 0x1, 0x1, 0x0, 0x0)\n    /Users/sosedoff/Go/src/github.com/gin-gonic/gin/render/render.go:60 +0x125 fp=0xc208157490 sp=0xc208157438\ngithub.com/gin-gonic/gin/render.(jsonRender).Render(0xb25890, 0x7f80e8da8190, 0xc20808a420, 0xc8, 0xc28601d7a0, 0x1, 0x1, 0x0, 0x0)\n    :2 +0xeb fp=0xc2081574d8 sp=0xc208157490\ngithub.com/gin-gonic/gin.(Context).Render(0xc20808a420, 0xc8, 0x7f80e8da8168, 0xb25890, 0xc28601d7a0, 0x1, 0x1)\n    /Users/sosedoff/Go/src/github.com/gin-gonic/gin/context.go:322 +0xab fp=0xc208157550 sp=0xc2081574d8\ngithub.com/gin-gonic/gin.(Context).JSON(0xc20808a420, 0xc8, 0x7e5bc0, 0xc20811def0)\n    /Users/sosedoff/Go/src/github.com/gin-gonic/gin/context.go:331 +0xf1 fp=0xc2081575a8 sp=0xc208157550\ngithub.com/sosedoff/pgweb/pkg/api.HandleQuery(0xc208116600, 0x1a, 0xc20808a420)\n    /Users/sosedoff/Go/src/github.com/sosedoff/pgweb/pkg/api/api.go:197 +0x64c fp=0xc208157718 sp=0xc2081575a8\ngithub.com/sosedoff/pgweb/pkg/api.RunQuery(0xc20808a420)\n    /Users/sosedoff/Go/src/github.com/sosedoff/pgweb/pkg/api/api.go:81 +0x1bc fp=0xc2081577a0 sp=0xc208157718\ngithub.com/gin-gonic/gin.(Context).Next(0xc20808a420)\n    /Users/sosedoff/Go/src/github.com/gin-gonic/gin/context.go:114 +0x95 fp=0xc2081577d0 sp=0xc2081577a0\ngithub.com/sosedoff/pgweb/pkg/api.func\u00b7001(0xc20808a420)\n    /Users/sosedoff/Go/src/github.com/sosedoff/pgweb/pkg/api/helpers.go:54 +0x45 fp=0xc208157878 sp=0xc2081577d0\ngithub.com/gin-gonic/gin.(Context).Next(0xc20808a420)\n    /Users/sosedoff/Go/src/github.com/gin-gonic/gin/context.go:114 +0x95 fp=0xc2081578a8 sp=0xc208157878\ngithub.com/gin-gonic/gin.func\u00b7006(0xc20808a420)\n    /Users/sosedoff/Go/src/github.com/gin-gonic/gin/logger.go:49 +0x68 fp=0xc208157ad8 sp=0xc2081578a8\ngithub.com/gin-gonic/gin.(Context).Next(0xc20808a420)\n    /Users/sosedoff/Go/src/github.com/gin-gonic/gin/context.go:114 +0x95 fp=0xc208157b08 sp=0xc208157ad8\ngithub.com/gin-gonic/gin.func\u00b7008(0xc20808a420)\n    /Users/sosedoff/Go/src/github.com/gin-gonic/gin/recovery.go:96 +0x61 fp=0xc208157b28 sp=0xc208157b08\ngithub.com/gin-gonic/gin.(Context).Next(0xc20808a420)\n    /Users/sosedoff/Go/src/github.com/gin-gonic/gin/context.go:114 +0x95 fp=0xc208157b58 sp=0xc208157b28\ngithub.com/gin-gonic/gin.func\u00b7009(0x7f80e8da64b8, 0xc20808d9a0, 0xc2082ea0d0, 0x0, 0x0, 0x0)\n    /Users/sosedoff/Go/src/github.com/gin-gonic/gin/routergroup.go:57 +0xa3 fp=0xc208157bd8 sp=0xc208157b58\ngithub.com/julienschmidt/httprouter.(Router).ServeHTTP(0xc20801e6e0, 0x7f80e8da64b8, 0xc20808d9a0, 0xc2082ea0d0)\n    /Users/sosedoff/Go/src/github.com/julienschmidt/httprouter/router.go:281 +0x188 fp=0xc208157c88 sp=0xc208157bd8\ngithub.com/gin-gonic/gin.(Engine).ServeHTTP(0xc20809e100, 0x7f80e8da64b8, 0xc20808d9a0, 0xc2082ea0d0)\n    /Users/sosedoff/Go/src/github.com/gin-gonic/gin/gin.go:126 +0x4a fp=0xc208157cb0 sp=0xc208157c88\nnet/http.serverHandler.ServeHTTP(0xc208034a20, 0x7f80e8da64b8, 0xc20808d9a0, 0xc2082ea0d0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:1703 +0x19a fp=0xc208157d08 sp=0xc208157cb0\nnet/http.(conn).serve(0xc20808c1e0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:1204 +0xb57 fp=0xc208157fd8 sp=0xc208157d08\nruntime.goexit()\n    /usr/local/Cellar/go/1.4.2/libexec/src/runtime/asm_amd64.s:2232 +0x1 fp=0xc208157fe0 sp=0xc208157fd8\ncreated by net/http.(Server).Serve\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:1751 +0x35e\ngoroutine 1 [chan receive]:\nmain.handleSignals()\n    /Users/sosedoff/go/src/github.com/sosedoff/pgweb/main.go:102 +0x10f\nmain.main()\n    /Users/sosedoff/go/src/github.com/sosedoff/pgweb/main.go:140 +0x99\ngoroutine 5 [syscall]:\nos/signal.loop()\n    /usr/local/Cellar/go/1.4.2/libexec/src/os/signal/signal_unix.go:21 +0x1f\ncreated by os/signal.init\u00b71\n    /usr/local/Cellar/go/1.4.2/libexec/src/os/signal/signal_unix.go:27 +0x35\ngoroutine 6 [IO wait]:\nnet.(pollDesc).Wait(0xc208010920, 0x72, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_poll_runtime.go:84 +0x47\nnet.(pollDesc).WaitRead(0xc208010920, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_poll_runtime.go:89 +0x43\nnet.(netFD).accept(0xc2080108c0, 0x0, 0x7f80e8da4d48, 0xc208118490)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_unix.go:419 +0x40b\nnet.(TCPListener).AcceptTCP(0xc20804e248, 0x5414de, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/tcpsock_posix.go:234 +0x4e\nnet/http.tcpKeepAliveListener.Accept(0xc20804e248, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:1976 +0x4c\nnet/http.(Server).Serve(0xc208034a20, 0x7f80e8da62d8, 0xc20804e248, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:1728 +0x92\nnet/http.(Server).ListenAndServe(0xc208034a20, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:1718 +0x154\nnet/http.ListenAndServe(0xc20800b730, 0xe, 0x7f80e8da5170, 0xc20809e100, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:1808 +0xba\ngithub.com/gin-gonic/gin.(*Engine).Run(0xc20809e100, 0xc20800b730, 0xe, 0x0, 0x0)\n    /Users/sosedoff/Go/src/github.com/gin-gonic/gin/gin.go:131 +0x170\nmain.func\u00b7001()\n    /Users/sosedoff/go/src/github.com/sosedoff/pgweb/main.go:91 +0x18b\ncreated by main.startServer\n    /Users/sosedoff/go/src/github.com/sosedoff/pgweb/main.go:96 +0x2c9\ngoroutine 9 [IO wait]:\nnet.(pollDesc).Wait(0xc208010a00, 0x72, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_poll_runtime.go:84 +0x47\nnet.(pollDesc).WaitRead(0xc208010a00, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_poll_runtime.go:89 +0x43\nnet.(netFD).Read(0xc2080109a0, 0xc2080fa000, 0x1000, 0x1000, 0x0, 0x7f80e8da4d48, 0xc2081191d0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_unix.go:242 +0x40f\nnet.(conn).Read(0xc20804e2a0, 0xc2080fa000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/net.go:121 +0xdc\nnet/http.(liveSwitchReader).Read(0xc20808cb88, 0xc2080fa000, 0x1000, 0x1000, 0x4eb135, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:214 +0xab\nio.(LimitedReader).Read(0xc20801fc80, 0xc2080fa000, 0x1000, 0x1000, 0x4713f8, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/io/io.go:408 +0xce\nbufio.(Reader).fill(0xc208034cc0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bufio/bufio.go:97 +0x1ce\nbufio.(Reader).ReadSlice(0xc208034cc0, 0x50000000000000a, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bufio/bufio.go:295 +0x257\nbufio.(Reader).ReadLine(0xc208034cc0, 0x0, 0x0, 0x0, 0xc207fee300, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bufio/bufio.go:324 +0x62\nnet/textproto.(Reader).readLineSlice(0xc20811ced0, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/textproto/reader.go:55 +0x9e\nnet/textproto.(Reader).ReadLine(0xc20811ced0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/textproto/reader.go:36 +0x4f\nnet/http.ReadRequest(0xc208034cc0, 0xc20803d860, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/request.go:598 +0xcb\nnet/http.(conn).readRequest(0xc20808cb40, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:586 +0x26f\nnet/http.(conn).serve(0xc20808cb40)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:1162 +0x69e\ncreated by net/http.(Server).Serve\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:1751 +0x35e\ngoroutine 10 [IO wait]:\nnet.(pollDesc).Wait(0xc208108140, 0x72, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_poll_runtime.go:84 +0x47\nnet.(pollDesc).WaitRead(0xc208108140, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_poll_runtime.go:89 +0x43\nnet.(netFD).Read(0xc2081080e0, 0xc20800f000, 0x1000, 0x1000, 0x0, 0x7f80e8da4d48, 0xc2081190e0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_unix.go:242 +0x40f\nnet.(conn).Read(0xc20804e2a8, 0xc20800f000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/net.go:121 +0xdc\nnet/http.(liveSwitchReader).Read(0xc20808c048, 0xc20800f000, 0x1000, 0x1000, 0x4eb135, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:214 +0xab\nio.(LimitedReader).Read(0xc208094160, 0xc20800f000, 0x1000, 0x1000, 0x4713f8, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/io/io.go:408 +0xce\nbufio.(Reader).fill(0xc208034ae0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bufio/bufio.go:97 +0x1ce\nbufio.(Reader).ReadSlice(0xc208034ae0, 0xa, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bufio/bufio.go:295 +0x257\nbufio.(Reader).ReadLine(0xc208034ae0, 0x0, 0x0, 0x0, 0xc20185aa00, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bufio/bufio.go:324 +0x62\nnet/textproto.(Reader).readLineSlice(0xc20811cd50, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/textproto/reader.go:55 +0x9e\nnet/textproto.(Reader).ReadLine(0xc20811cd50, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/textproto/reader.go:36 +0x4f\nnet/http.ReadRequest(0xc208034ae0, 0xc20803d790, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/request.go:598 +0xcb\nnet/http.(conn).readRequest(0xc20808c000, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:586 +0x26f\nnet/http.(conn).serve(0xc20808c000)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:1162 +0x69e\ncreated by net/http.(Server).Serve\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:1751 +0x35e\ngoroutine 11 [IO wait]:\nnet.(pollDesc).Wait(0xc2081081b0, 0x72, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_poll_runtime.go:84 +0x47\nnet.(pollDesc).WaitRead(0xc2081081b0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_poll_runtime.go:89 +0x43\nnet.(netFD).Read(0xc208108150, 0xc208097000, 0x1000, 0x1000, 0x0, 0x7f80e8da4d48, 0xc208118eb0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_unix.go:242 +0x40f\nnet.(conn).Read(0xc20804e020, 0xc208097000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/net.go:121 +0xdc\nnet/http.(liveSwitchReader).Read(0xc20808c0e8, 0xc208097000, 0x1000, 0x1000, 0x4eb135, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:214 +0xab\nio.(LimitedReader).Read(0xc2080941e0, 0xc208097000, 0x1000, 0x1000, 0x4713f8, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/io/io.go:408 +0xce\nbufio.(Reader).fill(0xc208034b40)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bufio/bufio.go:97 +0x1ce\nbufio.(Reader).ReadSlice(0xc208034b40, 0xa, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bufio/bufio.go:295 +0x257\nbufio.(Reader).ReadLine(0xc208034b40, 0x0, 0x0, 0x0, 0xc201686e00, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bufio/bufio.go:324 +0x62\nnet/textproto.(Reader).readLineSlice(0xc20811c990, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/textproto/reader.go:55 +0x9e\nnet/textproto.(Reader).ReadLine(0xc20811c990, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/textproto/reader.go:36 +0x4f\nnet/http.ReadRequest(0xc208034b40, 0xc20803d5f0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/request.go:598 +0xcb\nnet/http.(conn).readRequest(0xc20808c0a0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:586 +0x26f\nnet/http.(conn).serve(0xc20808c0a0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:1162 +0x69e\ncreated by net/http.(Server).Serve\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:1751 +0x35e\ngoroutine 12 [IO wait]:\nnet.(pollDesc).Wait(0xc208108220, 0x72, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_poll_runtime.go:84 +0x47\nnet.(pollDesc).WaitRead(0xc208108220, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_poll_runtime.go:89 +0x43\nnet.(netFD).Read(0xc2081081c0, 0xc20811b000, 0x1000, 0x1000, 0x0, 0x7f80e8da4d48, 0xc208118760)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_unix.go:242 +0x40f\nnet.(conn).Read(0xc20804e028, 0xc20811b000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/net.go:121 +0xdc\nnet/http.(liveSwitchReader).Read(0xc20808c188, 0xc20811b000, 0x1000, 0x1000, 0xc000000004eb135, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:214 +0xab\nio.(LimitedReader).Read(0xc208094260, 0xc20811b000, 0x1000, 0x1000, 0x4713f8, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/io/io.go:408 +0xce\nbufio.(Reader).fill(0xc208034c00)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bufio/bufio.go:97 +0x1ce\nbufio.(Reader).ReadSlice(0xc208034c00, 0xa, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bufio/bufio.go:295 +0x257\nbufio.(Reader).ReadLine(0xc208034c00, 0x0, 0x0, 0x0, 0xc207fee300, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bufio/bufio.go:324 +0x62\nnet/textproto.(Reader).readLineSlice(0xc20811c7e0, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/textproto/reader.go:55 +0x9e\nnet/textproto.(Reader).ReadLine(0xc20811c7e0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/textproto/reader.go:36 +0x4f\nnet/http.ReadRequest(0xc208034c00, 0xc20803d1e0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/request.go:598 +0xcb\nnet/http.(conn).readRequest(0xc20808c140, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:586 +0x26f\nnet/http.(conn).serve(0xc20808c140)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:1162 +0x69e\ncreated by net/http.(Server).Serve\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:1751 +0x35e\ngoroutine 14 [IO wait]:\nnet.(pollDesc).Wait(0xc208108300, 0x72, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_poll_runtime.go:84 +0x47\nnet.(pollDesc).WaitRead(0xc208108300, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_poll_runtime.go:89 +0x43\nnet.(netFD).Read(0xc2081082a0, 0xc2080e9000, 0x1000, 0x1000, 0x0, 0x7f80e8da4d48, 0xc20800b120)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/fd_unix.go:242 +0x40f\nnet.(conn).Read(0xc20804e038, 0xc2080e9000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/net.go:121 +0xdc\nnet/http.(liveSwitchReader).Read(0xc20808c2c8, 0xc2080e9000, 0x1000, 0x1000, 0x4eb135, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:214 +0xab\nio.(LimitedReader).Read(0xc208094380, 0xc2080e9000, 0x1000, 0x1000, 0x4713f8, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/io/io.go:408 +0xce\nbufio.(Reader).fill(0xc208034d80)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bufio/bufio.go:97 +0x1ce\nbufio.(Reader).ReadSlice(0xc208034d80, 0xa, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bufio/bufio.go:295 +0x257\nbufio.(Reader).ReadLine(0xc208034d80, 0x0, 0x0, 0x0, 0xc235cb6f00, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/bufio/bufio.go:324 +0x62\nnet/textproto.(Reader).readLineSlice(0xc20811cf00, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/textproto/reader.go:55 +0x9e\nnet/textproto.(Reader).ReadLine(0xc20811cf00, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/textproto/reader.go:36 +0x4f\nnet/http.ReadRequest(0xc208034d80, 0xc2082ea000, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/request.go:598 +0xcb\nnet/http.(conn).readRequest(0xc20808c280, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:586 +0x26f\nnet/http.(conn).serve(0xc20808c280)\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:1162 +0x69e\ncreated by net/http.(Server).Serve\n    /usr/local/Cellar/go/1.4.2/libexec/src/net/http/server.go:1751 +0x35e\ngoroutine 15 [chan receive]:\ndatabase/sql.(*DB).connectionOpener(0xc20808d4a0)\n    /usr/local/Cellar/go/1.4.2/libexec/src/database/sql/sql.go:589 +0x4c\ncreated by database/sql.Open\n    /usr/local/Cellar/go/1.4.2/libexec/src/database/sql/sql.go:452 +0x31c\n```\n. ",
    "enr2114": "Hey,\nSo my query included a line \"application_funders.amount +\nCOALESCE(application_prior_fundings.buyout_amount,0) as \"total approved\namount\" FROM applications \"\nI have no problem running the query but when I try to download it I get an\nerror \"pg: syntax error at or near \"COALESCE\"\"\nThanks,\nELana\nOn Mon, Aug 3, 2015 at 3:41 PM, Dan Sosedoff notifications@github.com\nwrote:\n\nCould you provide a more detailed example?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/sosedoff/pgweb/issues/95#issuecomment-127383220.\n. \n",
    "yarons": "Confirmed, I must say I'm impressed, blazing fast and very easy to use, thank you for the wonderful tool!\n. ",
    "lenguyenthedat": "It does work for me. But it doesn't show any of my tables on the left hand side, is that what happened to you?\n\n. Same problem with Amazon RDS (Postgresql) btw.\n. ",
    "goldalworming": "I think this one which generate object to return as json in pgweb\n```\nfunc (client Client) query(query string, args ...interface{}) (Result, error) {\n    rows, err := client.db.Queryx(query, args...)\nif err != nil {\n    return nil, err\n}\n\ndefer rows.Close()\n\ncols, err := rows.Columns()\nif err != nil {\n    return nil, err\n}\n\nresult := Result{Columns: cols}\n\nfor rows.Next() {\n    obj, err := rows.SliceScan()\n\n    for i, item := range obj {\n        if item == nil {\n            obj[i] = nil\n        } else {\n            t := reflect.TypeOf(item).Kind().String()\n\n            if t == \"slice\" {\n                obj[i] = string(item.([]byte))\n            }\n        }\n    }\n\n    if err == nil {\n        result.Rows = append(result.Rows, obj)\n    }\n}\n\nreturn &result, nil\n\n}\n```\n. ok, I'm sorry, I thought it was same with\n        arrayobj := make([]map[string]interface{}, 0)\n        for rows.Next() {\n            // get RawBytes from data\n            err = rows.Scan(scanArgs...)\n            if err != nil {\n                panic(err.Error()) // proper error handling instead of panic in your app\n            }\nvar value string\n        var vobj = make(map[string]interface{})\n.\n .\n .\nthank you for give me direction to code in go\n. ",
    "tylerlong": "@cbandy brilliant idea!\n@sosedoff  Yes only appear on longer requests. But not hours, minutes is able to reproduce.\nI just tested this: SELECT pg_sleep(300);. This should be finished in 300 seconds, aka 5 minutes. But I have been waiting for 30 minutes and I am still waiting. I don't think pgweb will ever tell me that it's finished.\n. @sosedoff I just tested latest version with Chrome browser, it did work without problem.\nWhat I had problem is Cocoa webview: https://developer.apple.com/library/mac/documentation/Cocoa/Reference/WebKit/Classes/WebView_Class/\nSo this might not be pgweb's issue. Let me test more and get back to you later.\n. Or can we show a popup message to tell the user that it has been timeout. It's better than to let the user wait for ever.\n. @sosedoff Could we make it 5 minutes? There is no disadvantages to set a higher value.\nAnd by the way, if it does timeout, could we show a popup to notice the user?\nThank you for your hard work!  :+1: \n. I just find that it is possible to use the bookmark feature to let command line support ssh tunnel.. We could even extend the url option:\n--url=postgresql://user:pass@host:port/db?sslmode=sslmode#ssh://user:pass@host:port?sshkey=~/.ssh/id_rsa\nI am also OK with --ssh=user:password@host:port?sshkey=~/.ssh/id_rsa\nThe feature is already there, we should expose the feature via command line.. ",
    "niiyz": "I executed rebase and make assets. please confirm.\n. I was aware . It is all of my change to the minute that are currently committed .\n. ",
    "dersteppenwolf": "It would be great some kind of integration with postgis like \"PostGIS Preview\" http://chriswhong.com/data-visualization/introducing-postgis-preview/ . ",
    "skabbes": "+1 !\n. I haven't really dug into the code much, so bear with my naming...\nFrom a quick glance, it seems like you have the column names only, but I believe postgres allow you to extract the type information as well as the column names.  So perhaps we could add another field to Result enumerating the types as well.\nThen when you marshal the data to json, you could take the types into account and bigint should be send down as strings.\nAnother possible solution would be to parse the response form the API with something like json-bigint.\n. Sounds great - thanks so much.  This is an awesome product I use daily.\n. Thanks so much!  Any chance you'll release a 0.7.1 soon?\n. All that said, I'm not going to complain about free software :)\nThanks for you're awesome contribution to the OSS community.\n. Awesome! Muchas gracias, could you push to dockerhub too when you have a chance?\n. In short: I don't think it adds any additional value.  To elaborate...\nI think the value of relative paths is being able to run pgweb alongside another web app (so pgweb doesn't need an entire subdomain).  However, to do that, you're going to need an HTTP reverse proxy in front of it anyway (since how else would you share the same tcp port)?\nI could see 3 possible scenarios here to run Pgweb next to another web app.\n```\n1.\nINTERNET ----> NGINX ---  /db/* ---- > PGWEB\n                 --- * ---> OTHER WEB APP\n2.\nINTERNET ----> OTHER WEB APP ---  /db/* ---- > PGWEB\n                     ---- * ---> OTHER WEB APP\n3.\nINTERNET ----> PGWEB ---  /db/* ---- > PGWEB\n                   ---- * ---> OTHER WEB APP\n```\nFor solutions 1 and 2, I think there is absolutely no value in adding a prefix in Go, since every single reverse proxy basically provides this out of the box.  (This is so common, in fact, I think it might be a lot of extra work to leave the prefix on).\nFor solution 3, I suppose it would be useful - but I don't know if I would recommend proxying all your web traffic through pgweb :)\n. Ah, cool. I think you can also put:\nlocation /db/ { proxy_pass http://pgweb; }\u00a0\n(Note the trailing slash on /db/) and it will strip \"db\" off the proxied request. One extra character for no additional code in golang.\n\u2014\nSent from Mailbox\nOn Fri, Feb 19, 2016 at 7:30 PM, Dan Sosedoff notifications@github.com\nwrote:\n\nI don't necessarily agree that having a url prefix does not provide any value. Having a url prefix baked into the app's makes it super easy to use with software like nginx.\nI pushed a new git branch url-prefix that implements url prefix for pgweb, you can compile the source code from that branch and then start pgweb with --prefix=db/. \nFor nginx, then you can configure pgweb as follows:\nupstream pgweb {\n  server 127.0.0.1:8081;\n}\nserver {\n  # other stuff\n  # ...\n  # proxy any database stuff directly to pgweb\n  location /db {\n    proxy_pass http://pgweb;\n  } \n}\nIf that works for you - great, otherwise please provide your own solution and will gladly review and accept the patch for the feature in question. Cheers.\nReply to this email directly or view it on GitHub:\nhttps://github.com/sosedoff/pgweb/issues/131#issuecomment-186497480\n. But either way works for me, thanks so much for doing this!!\n\n\u2014\nSent from Mailbox\nOn Fri, Feb 19, 2016 at 7:30 PM, Dan Sosedoff notifications@github.com\nwrote:\n\nI don't necessarily agree that having a url prefix does not provide any value. Having a url prefix baked into the app's makes it super easy to use with software like nginx.\nI pushed a new git branch url-prefix that implements url prefix for pgweb, you can compile the source code from that branch and then start pgweb with --prefix=db/. \nFor nginx, then you can configure pgweb as follows:\nupstream pgweb {\n  server 127.0.0.1:8081;\n}\nserver {\n  # other stuff\n  # ...\n  # proxy any database stuff directly to pgweb\n  location /db {\n    proxy_pass http://pgweb;\n  } \n}\nIf that works for you - great, otherwise please provide your own solution and will gladly review and accept the patch for the feature in question. Cheers.\nReply to this email directly or view it on GitHub:\nhttps://github.com/sosedoff/pgweb/issues/131#issuecomment-186497480\n. I would also really appreciate if you would do another release if this is acceptable.\n. You're awesome! up and running in Prod!\n. I saw that! Thank you, already updated\n\nOn Sat, Aug 5, 2017, 11:45 AM Dan Sosedoff notifications@github.com wrote:\n\n@skabbes https://github.com/skabbes new version 0.9.8 is out\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/sosedoff/pgweb/pull/254#issuecomment-320463065, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAkJMqcddqUjxMHXuN-FaI4lQ8-RGCVPks5sVLhCgaJpZM4OuMBu\n.\n. For what its worth, I don't really agree with dynamically changing the shape of the json object based on the values.\n\nFor example, consider this code:\njavascript\nwhile (true) {\n    var result = JSON.parse(getValueFromApi());\n    var oneHigher = result.usuallyNumber + 1;\n    saveToDb({usuallyNumber: oneHigher});\n}\nIf result.usuallyNumber is all of a sudden a string (because your values got too big), the + operator in javascript ends up becoming string concatenation.\n1\n2\n3\n...\n9007199254740989\n9007199254740990\n9007199254740991\n90071992547409911\n900719925474099111\n9007199254740991111\n. All numbers in Javascript are by double precision floating point, this conversion shouldn't be necessary at all, and will probably cause similar bugs to my comment above.\n. ah, no, what you're saying totally makes sense.  I didn't realize the formatting was different.\n. ",
    "Josiah": "Thanks for this feature @sosedoff!\nWhen do you think you'll get time to publish this to the docker hub?\n. :fireworks: thanks @sosedoff!\n. ",
    "pavel-kiselyov": "P.S. Are you using old postgres? Materialized views are removed from information_schema since 9.3\nhttp://stackoverflow.com/a/19129980\n. I am using 9.4.5 on OSX - it just means that materialized views structure should be fetched by different query than the one for tables\n. Have you thought about moving to glide?. @sosedoff You're talking about statements.PG_OBJECTS query, yep, that works. But I'm talking about next step - statements.PG_TABLE_SCHEMA, which is querying information_schema, where is no info about materialized views.\nDo you have materialized views working now?\n. ",
    "lukecyca": "Thanks for the clarification.\nAgain, awesome project.\n. ",
    "ArturFormella": "Do you save connection data somewhere? :). ",
    "ryantuck": "any timeline on this? as it stands it's basically the only gripe i have with an otherwise awesome tool.. ",
    "akarki15": "agreed. i have to constantly go back to the login screen and change the database name to switch\n. i can look into adding this feature\n. I don't have a PR yet, so feel free to work on it!\n. You can close this one. the issue was i didn't have the postgres command lines tools installed. i fixed it now and it is working.\n. can you detail how you went about running the binary? and what the output of invoking the binary was?\n. i didn't add table to the test database. statements.PG_OBJECTS returns tables from all  databases\n. sorry for replying late @sosedoff. i found out that for some reason whenever i create a new pg database, it was automatically creating a table named \"person\". i think there is some default schema being applied to every new table and that was polluting the test db. since this is an issue very specific to my machine setup, im going to close the pull request as well. thanks for getting back though!\n. holy shit that was it. thanks a lot @cbandy!\n. this is no longer an issue. closing it.\n. Syntax completion would be awesome too!\n. should the bookmark be a local file that stores db connection settings? \n. better - where would the bookmark info be stored?\n. i have a prototype that works. you can assign this to me\n. closing this because of weird merge commit\n. @sosedoff last 5 commits are the ones relevant to this feature. the first one is a merge commit in my fork to get upstream changes. do you know how i should go about issuing this pr?\n. @sosedoff that's because the port no. in the toml file should be a number. see the example in data folder\n. if the port no. is of type string, the user can still pass a non integer. its just that the error would get caught later when we'd be doing strings.Atoi().\nWhat bookmark settings is giving you Error: dial tcp [::1]:0: connect: can't assign requested address? \n. Are you thinking of persisting every single query in this log? or would there be a custom \"store query\" option?\nAnother question - how would we uniquely identify clients? Say one client saves the query, quits and comes back up. How would we identify that the new client is the same one that stored the query? \nI think we should store every single query and NOT separate queries by clients. This means that two concurrent clients may get each other's queries in their history. I think this is fine since this is how a lot of terminal emulator's history works as well (think iterm, mac's terminal etc).. i see your point about not using a new pattern to keep things consistent. i would however like to point the advantage of this pattern - having an explicit typed error means that in the unit tests, i can assert equality on types of error rather than doing a vague string assertion. example here -\nassert.Equal(t, ErrNonExistingBookmark(\"bar\"), err)\n. i found the reason for panic. i should have type casted error to string before passing it to sprintf-\nfunc (e ErrNonExistingBookmark) Error() string {\n    return fmt.Sprintf(\"couldn't find a bookmark with name %s\", string(e))\n }\n. https://github.com/sosedoff/pgweb/pull/201/commits/68db9345071bd806ae7ea0eaaa08019048febbfb gets rid of this err pattern\n. https://github.com/sosedoff/pgweb/pull/201/commits/68db9345071bd806ae7ea0eaaa08019048febbfb - doesn't panic anymore if you pass non existing file\n. ```\ngithub.com/sosedoff/pgweb  Issue-189-take-2 \u2714                                                                                                                                3m\n\u25b6 ./pgweb --bookmark=dummy\nPgweb v0.9.5\nError: couldn't find a bookmark with name dummy\n```\n. you're right. no point in returning error here. forgot to remove the error when i changed port from string to int\n. https://github.com/sosedoff/pgweb/pull/201/commits/fedde804e30cf072d491b4e306071c1ab28d687d\n. ",
    "danilobuerger": "I am using the session mode.\nSo in that case, this is a feature request for a \"Close Connection\" button :).\n. I can help out if you want.\n. Closing the connection works as expected. However there seems to be a glitch with the interface:\n1. Log in\n2. Click on Connect\n3. Click on Cancel\n4. Click on Disconnect\nNow the connection is closed, but i have the ability to Cancel again. Which brings me back to the interface with all the loaded data still being presented. However new queries are errored out with \"ERROR: sql: database is closed\".\n. Looking good!\n. ",
    "iillyyaa": "I just found (and liked) your UI, and support for ~/.pgpass is the first thing I went to check.\nI would strongly prefer to use ~/.pgpass, precisely for the reason original posted requested: don't want passwords in history and in command line. psql client requires that ~/.pgpass be readable only by the file owner, whereas no such check exists for the shell's history file.\nI would offer to implement this, but alas I haven't written (and until today - not even read) a single line of go code.. Since I posted my original comment, I have indeed found bookmarks, which do provide this functionality. (.pgpass would have the benefit of working w/o me adding a password to yet another file).\nThank you for your response and for reconsidering this ticket.. ",
    "andreibancioiu": "Currently, the solution proposed by @skabbes works for me, but the solution using --prefix=... does not (I receive a 404 error from the Go application - not from nginx).\nThis is problematic:\npgweb_linux_amd64 --listen=9090  --prefix=pgweb/\nnginx config:\nlocation /pgweb {\n    proxy_pass http://127.0.0.1:9090/;\n}\nAnyway, running without the prefix argument, but with the trailing slash in the location section seems fine:\nlocation /pgweb/ {\n    proxy_pass http://127.0.0.1:9090/;\n}. Here are some steps to run pgweb using systemd (I guess this only works on Ubuntu though):\ntouch /etc/systemd/system/pgweb.service\n```\n[Unit]\nDescription=pgweb\nAfter=network.target\n[Service]\nUser=webadmin\nGroup=www-data\nWorkingDirectory=/home/webadmin/pgweb\nExecStart=/home/webadmin/pgweb/pgweb_linux_amd64 --listen=9090\n[Install]\nWantedBy=multi-user.target\n```\nReference on how to create a systemd Unit File.. ",
    "dovahcrow": "similar to #95 \n. @sosedoff thanks\n. ",
    "imkritesh": "Thanks.\n. Changes in static/index.html are not reflecting after building. I can not figure out whats the problem?\ncommands I have used:\nmake setup\ngo get\nmake build\n. I have tried that too. Even if I remove index.html from static folder it still works and no change is reflected.\n. ",
    "joncrlsn": "I don't understand this \"bug\".  This tool lets you run any SQL you want.  There is no need to inject SQL into another SQL statement when you can run any SQL you want directly.\n. ",
    "goldfix": "I agree,\n1) I'm checking and modifying automated test.\n2) well... I try to set js-timeout like server timeout, but I have a problem to parametrize app.js... I'm investigating (this is my first go-coding...).\n3) ok\nthanks\np\n. Hi Dan,\nI made recommended modifications.\nThanks\nciao\np\n. FORCE_WIN_TEST: I do not understood because you bypass test of client_test.go on windows  environment. Well for security I do not change your check and I prefer add this  environment variable to bypass your check.\nAnd yes, I work on windows :)\nthanks\nciao\np\n. ",
    "jonaz": "Oh, thanks. Closing this as this solves my issue!\n. ",
    "alexmironof": "Thanks for the response I fugured it out and update comment :+1: \n. Everything works fine with port 8081\n. ",
    "benlowry": "```\ncreate table tmp (\n    blob bytea\n);\ninsert into tmp(blob) values('\\xABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789 ABCDEF0123456789')\n```\nI've been digging into the code, I don't know golang but I've managed to tentatively identify a bytea field and substitute its size, it's possible I'm not identifying it properly but maybe this gives you a head start.  I was trying to look for the '\\x' that byte data starts with (I think always but again not sure). \nfunc (res *Result) PrepareBytea() {\n        for i, row := range res.Rows {\n                for j, col := range row {\n            t := reflect.TypeOf(col).Kind()\n            if(t != reflect.String) {\n                continue\n            }\n            str := col.(string)\n            if str[0] == 255 {\n                res.Rows[i][j] = len(str)\n            }\n        }\n    }\n}\n. That is what you downloaded, you just need to provide a lot more to see the effect, try some 10s of megabytes in the bytea value or reproduce the row 1000s of times and you will probably see your browser start straining.  I should have mentioned this I just didn't want to put an incredibly massive example.  In our case we have a table with about 2000 rows and a small amount of binary data for each row that totals 150 megabytes.\n. Ideally clicking the tables wouldn't include the raw data at all, that could be solved by:\n- only returning the bytea data when selecting it explicitly, either dropping it at the server or preferably not selecting it\n- a way to edit the 'you selected the table' query so you can exclude columns\n. This was pretty much fixed for me by limiting the number of rows returned when clicking a table.\nThanks for all your wonderful work @sosedoff! \n. ",
    "CzechJiri": "thank you! Opened ticket, got really quick great responses. It looks like the pq connection string supports it, it does not follow same naming as JDBC does, pq does does not use currentSchema, it uses search_path instead, the connection looks like this and works like a charm :-)\npostgresql://localhost:5432/mydatabase?search_path=myschema\n. closing... this seems to be very rare close to non-existent in 0.9.2 ... I was using latest release which corresponds to 0.8\n. ",
    "ekiyanov": "There was no output at all. \n./pgweb --bind=10.8.0.30 --listen=3005 -s --host=localhost -d --user=cambolive_user --db=cambolive_db\n./pgweb --bind=10.8.0.30 --listen=8081 -s\n./pgweb --url postgres://cambolive_user:secret_pass@cambolive_db:5432/postgres?sslmode=false --bind=10.8.0.30\nroot@minibian:~/pgweb# ls -la\ntotal 11136\ndrwxr-xr-x 11 root root     4096 Jun 12 10:52 .\ndrwx------ 13 root root     4096 Jun 14 03:08 ..\ndrwxr-xr-x  8 root root     4096 Jun 12 10:20 .git\n-rw-r--r--  1 root root       17 Jun 12 10:20 .gitattributes\n-rw-r--r--  1 root root       34 Jun 12 10:20 .gitignore\n-rw-r--r--  1 root root      242 Jun 12 10:20 .travis.yml\n-rw-r--r--  1 root root     6556 Jun 12 10:20 CHANGELOG.md\n-rw-r--r--  1 root root      177 Jun 12 10:20 CONTRIBUTING.md\n-rw-r--r--  1 root root      240 Jun 12 10:20 Dockerfile\ndrwxr-xr-x  2 root root     4096 Jun 12 10:20 Godeps\n-rw-r--r--  1 root root     1108 Jun 12 10:20 LICENSE\n-rw-r--r--  1 root root     2413 Jun 12 10:20 Makefile\n-rw-r--r--  1 root root      107 Jun 12 10:20 Procfile.bak\n-rw-r--r--  1 root root     3271 Jun 12 10:20 README.md\n-rw-r--r--  1 root root      183 Jun 12 10:20 SCREENS.md\n-rw-r--r--  1 root root      665 Jun 12 10:20 app.json\n-rw-r--r--  1 root root      475 Jun 12 10:20 appveyor.yml\ndrwxr-xr-x  2 root root     4096 Jun 12 10:20 data\n-rw-r--r--  1 root root     2512 Jun 12 10:20 main.go\n-rwxr-xr-x  1 root root 11295392 Jun 12 10:32 pgweb\ndrwxr-xr-x 12 root root     4096 Jun 12 10:20 pkg\ndrwxr-xr-x  2 root root     4096 Jun 12 10:20 screenshots\ndrwxr-xr-x  2 root root     4096 Jun 12 10:20 script\ndrwxr-xr-x  5 root root     4096 Jun 12 10:26 src\ndrwxr-xr-x  6 root root     4096 Jun 12 10:20 static\ndrwxr-xr-x  4 root root     4096 Jun 12 10:20 vendor\n. ",
    "msaron": "Yes that worked! Thank you!!!\n. ",
    "felixbuenemann": "It's also worth noting that OFFSET pagination will get slower the further you paginate, because postgres has to first fetch all rows preceding the offset and throw them away. A faster way is to use keyset pagination, something like WHERE id > (last row id from previous page) LIMIT n. Not sure if that's practical to implement here, because it requires knowledge of the underlying table.\nAlso see We need tool support for keyset pagination for more info.\n. The DZone article Faster PostgreSQL Counting has some useful tips on getting good estimate counts on large tables.\nInstead of keeping a map of large tables in memory you could just look at the table stats in postgres on the fly, I don't think caching that info is worth the trouble. . Thanks, I'll try it out once the new release is out and pushed to docker hub.\n. I'm not familiar with go, but would it be easier to do the hex encoding in go?\nIf it's possible to reflect on the result's column type to see if it's a Bytea, it should be possible to encode it using something like hex.EncodeToString().\n. I would like to give developers access to databases for certain projects. They should not be able to connect to any other databases from pgweb.\n. To add a little more detail: The database server is running in a virtual private network and I would like to deploy one pgweb container per app I would like to give database access to (with different basic auth credentials).\n. ",
    "xujif": "@sosedoff  can not order the data when click the field which with uppercases\n. ",
    "abocd": "There is no modification of files asset, but I have changed the way of displaying files asset in bindata.\nBefore change:\n``` go\nvar _staticJsThemeTomorrowJs = []byte(\"\\x1f\\x8b\\x08\\x00\\x00\\x09\\x6e\\x88\\x00\\xff\\x94\\x56\\x7b\\x6f\\xe2\\x46\\x10\\xff\\x2a\\x2e\\xfd\\x07\\x24\\x9b\\x47\\x02\\x04\\x13\\xf1\\x07\\x21\\x4e\\xd5\\xab\\xd4\\x36\\x77\\xcd\\x55\\xd7\\xaa\\xaa\\x16\\xef\\xb0\\xac\\xb0\\x77\\xdd\\x7d\\x04\\x10\\xca\\x77\\xef\\xac\\x7d\\x84\\xc7\\x2d\\x8f\\xf3\\x0a\\x33\\xb6\\x67\\x7e\\xf3\\x9b\\x87\\xc7\\x4b\\x52\\x68\\x52\\x98\\x71\\x01\\xf5\\x1a\\xca\\x2d\\x33\\x87\\x1c\\xcf\\x32\\x97\\x4a\\xc9\\x65\\x2d\\xfc\\xbb\\xa6\\xe0\\x3f\\xcb\\x15\\xd4\\xc2\\x1a\\xac\\x0a\\xa9\\x8c\\x46\\x29\\x97\\xd4\\x66\\xee\\x96\\x33\\xc9\\xf8\\xb4\\x45\\x65\\x5e\\xfb\\x27\\x9c\\x59\\x91\\x1a\\x2e\\x45\\x1d\\x42\\x13\\x8a\\xc6\\xc6\\x34\\xb9\\x7e\\x24\\x6a\\x31\\xfa\\xa1\\x13\\x9a\\x66\\xaa\\xf5\\x24\\x23\\x5a\\x8f\\x9c\\x55\\xb4\\x73\\x51\\x3e\\xf9\\x03\\x56\\x66\\x54\\x6b\\xee\\x3f\\x09\\xdc\\xd5\\xbf\\xcc\\x1a\\x03\\x2a\\xd8\\x4c\\x49\\xba\\x60\\x4a\\x5a\\x41\\x87\\xc1\\x8f\\xb3\\xbe\\x5b\\xf7\\xa9\\xcc\\xa4\\xc2\\xcb\\xee\\x23\\xae\\xc9\\x9b\\xc7\\xba\\x50\\x5c\\x98\\x28\\x27\\x8a\\x71\\x11\\x6c\\x96\\x9c\\x9a\\xf9\\x30\\xe8\\x14\\xab\\x7b\\x0f\\xdc\\x91\\xfd\\x9e\\xc7\\x68\\xeb\\xe8\\xa9\\x3c\\xae\\xf0\\x9b\\x5a\\xa5\\x25\\xb2\\xde\\x6a\\x8e\\x93\\xf1\\xd3\\xf8\\xd1\\xa7\\x89\\xdc\\x16\\xa0\\xa2\\x8c\\xac\\x31\\xca\\xf2\\x8e\\x86\\x0c\\xca\\x34\\x1e\\x05\\xfd\\xd8\\x77\\xeb\\x10\\xa3\\x82\\xb0\\x99\\xe1\\x95\\xd5\\x11\\x42\\x75\\x65\\x88\\x32\\x88\\x25\\x57\\x91\\x9e\\x13\\x2a\\x97\\xc3\\xa0\\x8d\\xeb\\xb6\\x58\\x05\\x6d\\xfc\\x6d\\x83\\xba\\x92\\x9c\\x81\\xe2\\x90\\x97\\x62\\xd3\\xfa\\x4d\\xaf\\x17\\x06\\xe5\\xa9\\xdd\\xb8\\x0e\\x67\\xaa\\x10\\x02\\x90\\x56\\x55\\x9b\\x61\\x10\\x75\\x1c\\x1f\\x5c\\x51\\x59\\x1e\\xa9\\x28\\xa8\\xb2\\x54\\x81\\x96\\x19\\xa7\\x18\\x7e\\xc7\\xad\\xeb\\xd0\\x09\\x46\\xff\\x0a\\x51\\x86\\x6d\\x7d\\x94\\xc4\\xe4\\xc9\\x2d\\x1f\\x4a\\xd5\\x68\\xd1\\x09\\xd3\\xaa\\x05\\x02\\x44\\xa0\\xa9\\x5b\\xdf\\x53\\x4a\\xa0\\xd1\\x12\\xe3\\x71\\x25\\xf8\\x36\\x2a\\x4f\\x51\\x2b\\x5b\\x2e\\x5e\\xb9\\xe6\\xd3\\x0c\\x76\\x5d\\x74\\x3a\\x05\\x0b\\x58\\x3b\\x17\\xa1\\x8f\\x15\\x18\\xe2\\xbb\\xaf\\x8d\\x54\\x84\\xc1\\x99\\x47\\xa5\\x6c\\xd6\\x85\\x5f\\xc7\\x16\\x6e\\x1a\\xbc\\xeb\\xec\\x58\\x0e\\xe2\\x5e\\x3c\\x1e\\x9c\\x61\\x59\\xca\\xb2\\x00\\x45\\xcc\\xfe\\x3b\\x72\\x9b\\xc4\\x71\\xec\\x2d\\x4d\\x2a\\x05\\x36\\xb1\\xa8\\x9c\\xa5\\x73\\x82\\xbd\\x83\\xa5\\xf2\\xb1\\x3a\\xd0\\xcc\\x88\\x60\\xf6\\x44\\x88\\x07\\x8a\\xc2\\xe6\\xa0\\x78\\xea\\xd3\\x3b\\xa0\\x8c\\xc3\\x51\\x95\\x92\\x15\\xdc\\x5c\\x4a\\xca\\xd6\\x83\\x4f\\xef\\x95\\x28\\x4e\\xb0\\xb4\\xd5\\x8c\\xc2\\x78\\xb0\\x48\\xb0\\x97\\x8a\\xa7\\xde\\xe0\\xae\\x73\\x39\\x15\\x25\\xa1\\x9d\\x55\\xbf\\xdf\\x8f\\xfb\\xf1\\x89\\x5e\\x22\\xae\\xdd\\x36\\x47\\x83\\xcc\\x33\\xe2\\x26\\x83\\x9b\\xc1\\xcd\\x39\\x90\\x52\\xa6\\x50\\x28\\x48\\x09\\xb6\\xf6\\x35\\x98\\xa7\\x3b\\x62\\x26\\x33\\xea\\x9d\\xb4\\xdd\\x9b\\xbb\\xce\\x38\\xf9\\x3a\\x07\\xa2\\xcb\\x03\\x17\\x84\\xe1\\x66\\x5d\\x55\\x13\\xb3\\x59\\x81\\x7f\\xfd\\x1c\\x5d\\xaa\\xd4\\x39\\xbd\\x6d\\xa5\\x76\\x61\\x56\\xcc\\x7c\\x1c\\x0e\\xaa\\xef\\xbe\\x74\\xdf\\xf7\\xde\\x4c\\xe2\\x38\\x69\\xb7\\x7d\\xc0\\x73\\x20\\x94\\x0b\\xe6\\x7d\\xbb\\x71\\xe6\\xd8\\xe2\\x92\\x96\\x36\\xf8\\x1d\\x64\\x3b\\x57\\x77\\x9d\\xc1\\xc4\\xef\\xca\\x97\\x47\\x43\\xbc\\xa0\\x7b\\xaa\\xbb\\x37\\x83\\x18\\x74\\x35\\xb5\\x06\\x22\\x67\\x7e\\x6a\\x1e\\x9d\\x83\\xad\\xb8\\x96\\xa2\\x02\\x86\\x7b\\x8e\\xeb\\xca\\x72\\xba\\x6f\\x53\\x99\\xe7\\xc8\\x75\\x6f\\x40\\x25\\x71\\x7b\\xe0\\xed\\x22\\x2e\\x28\\x6a\\x46\\xcc\\x72\\x7a\\xf4\\xe1\\xb0\\x2a\\xab\\x53\\x62\\xc8\\x90\\xe7\\x38\\x51\\x5a\\x85\\x60\\xd8\\xe8\\x1a\\xfa\\xdd\\x90\\x7f\\x7e\\xf8\\xed\\xe3\\xb2\\xfd\\xcb\\x4f\\x4c\\x8e\\xf1\\xf8\\xf5\\xd3\\xcb\\x3c\\x79\\x61\\x4e\\x4c\\xdc\\x69\\x32\\x19\\x7f\\x71\\x7f\\x7f\\xb1\\xe9\\x17\\x51\\xde\\x6d\\x67\\xc9\\xf3\\xe7\\xe7\\x9f\\xf3\\x3f\\x7f\\xef\\xb6\\xf0\\x98\\x75\\xa7\\xd4\\xde\\xce\\x5a\\x0f\\xcb\\x71\\x36\\x63\\xa9\\xa1\\xf6\\x61\\xd0\\x7b\\x76\\x9a\\x0f\\x1f\\x3e\\xbe\\xf4\\x12\\xb5\\xf8\\xc0\\x18\\x1b\\x8d\\x1a\\x81\\xe2\\x6c\\x6e\\x02\\x05\\x05\\x10\\x13\\xad\\xdf\\x6a\\xf7\\x98\\x85\\x40\\x8d\\x70\\xe7\\xd6\\x6c\\xbe\\xef\\xc2\\x1a\\xf7\\xaa\\xc9\\x73\\xd7\\x62\\x13\\xad\\x3f\\x95\\xb9\\xac\\xbf\\xef\\xb0\\xf6\\x76\\x61\\x8d\\xb7\\xc6\\xff\\x01\\x00\\x00\\xff\\xff\\x3a\\x10\\x34\\xb4\\xfc\\x09\\x00\\x00\")\nfunc staticJsThemeTomorrowJsBytes() ([]byte, error) {\n    return bindataRead(\n        _staticJsThemeTomorrowJs,\n        \"static/js/theme-tomorrow.js\",\n    )\n}\n```\nAfter the change:\n``` go\nvar _staticJsThemeTomorrowJs = []byte(\"\")\nfunc staticJsThemeTomorrowJsBytes() ([]byte, error) {\n    return LoadFileContent(\"static/js/theme-tomorrow.js\")\n    //return bindataRead(\n    //  _staticJsThemeTomorrowJs,\n    //  \"static/js/theme-tomorrow.js\",\n    //)\n}\n```\n. I'm sorry, I was wrong, I do not know this document is generated\n. ",
    "cchesk": "I've filed a seperate clean PR for the export issue.\nI see why you're not a fan of the custom SQL query override. For my use case, it would be great if users without knowledge of SQL would be able to export filtered and sorted results. To avoid the conflict with overriding custom SQL queries, it might make sense to add export buttons to the rows tab instead.\nAdditionally, by displaying a SQL version of the current request, users can learn the basics of SQL without having to write any. Maybe it might make sense to (optionally) include the SQL version in the rows tab also?\nAre you interested in adding the features above if I add seperate PRs?\n. ",
    "syamantm": "+1\n. ",
    "jamesdixon": "@sosedoff - thanks, much appreciated!\nOverall, loving pgweb! Appreciate your hard work.\nCheers!\n. ",
    "vpsm": "Likewise.\n. ",
    "Kiran-N": "Thank you. ",
    "SjonHortensius": "Apparantly this PR duplicates #169\nAnd yes; this breaks when a sorted column contains a capital letter\n. I'll look into it\n. Maybe; but the currently implementation is broken and my commit fixes it.\nIf you include this fix; there is less need for an alternative solution imo\n. It seems a ; fell off, added that to my initial comment. I'm using the latest release, 0.9.5\nProviding additional queries is difficult as I cannot predict when multiple padding-chars are added\n. Please re-open; this is a bug-report. The pagination on the activities page is not hidden after switching from the Rows tab.\nI wanted to fix this; but didn't know what the intended behavior is\n. ",
    "Vonatzki": "Label-able SQL query tabs would be rad, too :)\n(I'm coming from Postage, which is almost a viable pgadmin substitute but then the developers decided to lessen their support due to failure to monetize the project. It's worth checking out and see if some of its features can be incorporated in pgweb.). Another feature that would be nice is convenient copy pasting of selected portions of any tables being displayed in the app.\nAs of 0.9.8, highlighting desired portion of a table is tricky and often require highlighting the whole table instead.\n. This is a very nice feature to have :). I'm not quite sure if I followed your instructions to a tee.\nGoing to that link churns out {\"error\":\"Session ID is required\"}.\nI have two databases in my postgresql right now. The other one has its schemas showing up on pgweb just fine.\nI'm running pgweb 0.9.8 on a VM with Windows 2012 Server OS. Logging in to the problematic DB and accessing the link, it churns out an empty JSON {}.\nLogging in to my other DB and using the link, it churned out JSON containing schemas and the database objects therein.\nAgain, the main difference of the two DBs is that the problematic one doesn't have any Table objects in its schemas, only Views.\nAlso, the views located in the problematic DB are created using foreign data wrapper that links it to views located in the other DB.\nAccessing those View objects via the pgweb SQL Query produces results just fine though.. Thanks! The app looks promising and I like it that it's only an executable with no dependencies (Is this the Go magic people are raving about?)\nI'd help you on the technicals but I don't know Go. I'll certainly learn more about in the future :). > Same goes for the views / etc. See my previous comment: i will need a sample sql dump to reproduce your issue.\nApologies for not making a prompt response on this issue.\nCopy, will make an anonymized SQL dump of the problematic DB. I really think it has something to do with schemas exclusively containing foreign-wrapped Views only.. Hi @sosedoff, will close this issue for now. The issue still remains though but I don't have time to recreate an anonymized version of the said issue for you. Apologies.\nIf you have time, the tables were created with the same process like the one in this link when creating the foreign data wrapped tables: \nhttps://robots.thoughtbot.com/postgres-foreign-data-wrapper. This feature will be a heaven-sent for those who manages hundreds of tables in their database like me :). ",
    "kblomqvist": "Works only with --url and not with --host --port --user --pass --db. Is that intentional?\n. Updated to the latest release. Works fine now.\n. There's also a --lock-session related question in #195. Seems like --lock-session works with --url only.\n. I have never compiled Go code before so I tried following (on Debian) and got the build error shown below. Any ideas?\n```bash\ncd workspace\ngit clone https://github.com/sosedoff/pgweb.git\ncd pgweb\nsudo apt-get install golang-go\nexport GOPATH=~/.gocode\nexport GOBIN=$GOPATH/bin\nexport PATH=$GOPATH:$GOBIN:$PATH\nmake setup\nmake dev\nmain.go:12:2: cannot find package \"github.com/sosedoff/pgweb/pkg/api\" in any of:\n    /usr/lib/go/src/pkg/github.com/sosedoff/pgweb/pkg/api (from $GOROOT)\n    /home/kiblomqv/.gocode/src/github.com/sosedoff/pgweb/pkg/api (from $GOPATH)\n```\n. Thanks! I got it working and the fix is working too.\nHere are instructions if somebody else is struggling with the build. Read the installation instructions from https://golang.org/doc/install. I did the following steps:\n```bash\nwget https://storage.googleapis.com/golang/go1.7.3.linux-amd64.tar.gz\nsudo tar -C /usr/local -xzf go1.7.3.linux-amd64.tar.gz\nexport PATH=$PATH:/usr/local/go/bin\nmkdir -p $HOME/workspace/go\nexport GOROOT=$HOME/workspace/go\nexport GOPATH=$HOME/workspace/go\nexport PATH=$PATH:$GOROOT/bin\ngo get github.com/sosedoff/pgweb\ncd workspace/go/src/github.com/sosedoff/pgweb\nmake setup\nmake dev\n```\nNot sure if it's best practice to have equal GOROOT and GOPATH.. I was not able to solve how to use variables in sudo's -c command flag. Where's the original startup script?. To run pgweb as pgweb user -- not as root.. ",
    "gallifabio": "Hi Dan, done, sorry for the delay. Had a little headache to set-up go and the environment on my windows machine. I launched pgweb locally and tested the change: everything looks good to me.\n. ",
    "gytisgreitai": "Sorry to bump into old thread. Do I understand correctly that there is no way how to dynamically link to a connection via url? E.g. we have a lot of dynamic environments which are autogenerated and I would like to automatically create links for pgweb to autonnect\nOne option is to probably enable cors and do a post to /api from the web app where I want to include dynamically generated links and then redirect the user?\nEdit: \nSorry, should have read through wiki first. Connect-Backend will do jus fine. Thank you. ",
    "krainboltgreene": "Sure. The setup is via the \"Deploy To Heroku\" button.\n\nI pressed the button.\nI created the instance.\nI deleted the hobby addon.\nI added a new Standard 0 database.\nI restarted.\nI changed the AUTH_PASS url to the password.\nI restarted.\nI tried to login, failed.\nI changed to xxxx.\nI restarted.\nI successfully log in.. Dang!\n\nOn Fri, Dec 30, 2016 at 10:53 AM, Dan Sosedoff notifications@github.com\nwrote:\n\nAh, i see. You're trying to set password with spaces as an environment\nvariable. AFAIK that wont work on Heroku and thats why you're getting auth\nfailures.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/sosedoff/pgweb/issues/210#issuecomment-269808114, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAUb2aKbtfzLtQC8aWdBu7MrllJVfGg3ks5rNVMhgaJpZM4LUsSQ\n.\n\n\n-- \nKurtis Rainbolt-Greene, Software Developer\nFounder of Difference Engineers\n2985 San Marino St.\nLos Angeles, CA 90006\n202-643-2263\n. ",
    "Falconerd": "I would also like to see this. As pgweb is a nice looking GUI, I thought it'd be targeting users who may not necessarily know SQL.\nIf you could update data through the row view, then someone like an office admin could use this pretty easily.. ",
    "VaWa": "Hello dear Dan. \nFirst off all thank You very much for your great work. \nThis project is very valuable and useful. \nI will also appreciate very much if row editing and adding functionality is implemented and this feature will bring this wonderful project to whole new level of usability for many users.\nThank you very much.. ",
    "awildeep": "@sosedoff I think I am missing something.  When:\n\nI use the sidebar navigation and drill into a table (No custom query, no views)\nThen I double click a row that has a value\nI see a textarea box which allows me to change the value\nHowever I see no way to save my change and issue the update\n\nI am assuming this is what you mean by \"Currently - no. Im still trying to find some time and implement row editing functionality.\"?\nUsing Chrome [58.0.3029.96 (64-bit)] on OSX [10.11.6 (15G1421)]  . ",
    "cheungpat": "Hi @sosedoff, I need this for some of my projects. Can I work on this? I intend to extend the API to support insert/update/delete and update the UI to support modifying data.. ",
    "jdalrymple": "@cheungpat Any chance you got around to adding editting functionality?. lol its still a wicked app, just missing some functionality. Id make a PR but id have to really dig into it to understand it all. ",
    "MostHated": "Man, the whole reason I was excited to use this was that I figured you could edit the data. Surprised that is not the case. Just set it up, and... nope : (. ",
    "joekohlsdorf": "Could we at least make the fields not change into an editable text box while this function isn't implemented? I have users coming to my desk confused all the time.. ",
    "RONNCC": "+1 readonly field\nOn Thu, Mar 14, 2019, 1:02 PM joe notifications@github.com wrote:\n\nCould we at least make the fields not change into an editable text box\nwhile this function isn't implemented? I have users coming to my desk\nconfused all the time.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/sosedoff/pgweb/issues/215#issuecomment-473037780, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABQLHtOxZG1_lmOLB4s_ac9S7EUGb-PQks5vWqrVgaJpZM4LmKMc\n.\n. \n",
    "zfrenchee": "My mistake, I didn't look closely at the wiki! \nhttps://github.com/sosedoff/pgweb/wiki/Docker\nThis mostly answers my question. I'll try to figure the rest out on my own. . Okay, so I added to my docker-compose file the following: \npgweb:\n  container_name: pgweb\n  restart: always\n  image: sosedoff/pgweb\n  ports: \n    - \"8081:8081\" \n  links: \n    - postgres:postgres\n  environment:\n    - DATABASE_URL=postgres://postgres:postgres@db:5432/postgres\nI have my functional postgres db in the same file\nWith the above, I get: \nConnecting to server...\nError: dial tcp: lookup db on 192.168.65.1:53: no such host\nIf I change the environment line to \n- DATABASE_URL=postgres://postgres:postgres@postgres:5432/postgres\nI get\nConnecting to server...\nError: dial tcp 172.17.0.2:5432: getsockopt: connection refused\nI changed the name just in case since my image is called postgres not db, the way you name it on the wiki https://github.com/sosedoff/pgweb/wiki/Docker\nAny ideas? What's the correct DATABASE_URL? (I'm assuming that's the issue) @sosedoff . Hi @sosedoff \nHere's the postgres piece of the compose file: \npostgres:\n  container_name: postgres\n  restart: always\n  image: postgres:latest\n  volumes:\n    - ./postgres-data:/var/lib/postgresql/data\n  ports:\n    - \"5432:5432\"\n. @sosedoff that worked! Thanks so much!! Would you like to update the wiki or can I? . Hi @sosedoff \nI would just add the correct docker-compose config from your last comment to the bottom of that page, just in case there's anyone out there like me who'd like to add this to their docker-compose setup. I personally think that for folks running postgres in a compose setup, plopping in pgweb if they don't already have a tool for looking through their database would be a godsend. . @sosedoff would you anticipate pgweb to fail if the db were too large? On a small DB, this works fine, but on my production DB I can't get pgweb to load. The log has no errors in it, the page just won't load.... nevermind I'm a moron. =). The port was closed. \"Huh, that's funny, it won't load...\" =). ",
    "od0": "@cbandy good question - I wanted to include that but I think it would require a bit more of a refactor to store the schema and table separately (instead of a single string that gets parsed by getSchemaAndTable). That would also mean updating the API routes.. @sosedoff sounds good will get that added. @sosedoff sure - apologies for the delay. I started to write a test for this but I'm not 100% sure what's helpful here. Should I modify the existing database creation script to include tables with . in the name, and a test in client_test.go to verify that they're parsed properly?. ",
    "TomoGlavas": "A short how to or link to how to install would be nice.\nHaving problems getting the right binary and method for installing on Ubuntu.. ",
    "zoujd-github": "pgweb start error . but this schema \"dbms_rls\" is exist,wy?\n\n. ",
    "evanushechkin": "Hi. yes. I, run a query that is pulling a lot of data, and is taking longer than 300 ms.. ",
    "johicks": "Thanks, that makes sense. I'm testing it out, and it looks like it's at least loading the root now, as I am not getting the error: 'x509: failed to load system roots and no roots provided' error.\nHere's my string (that I'm throwing into the browser...):\npostgres://ab-arm@[redacted]/ab-arm?sslmode=verify-full&sslcert=/a/secrets/app/ssl_cert/ssl_cert.certificate&sslkey=/a/secrets/app/ssl_cert/ssl_cert.private_key&sslrootcert=/a/secrets/app/ssl_cert/ssl_cert.chain_cert\nIt's throwing the following:\npq: connection requires a valid client certificate. ",
    "antoniomo": "Sorry for reopening, when testing with cockroachdb 2 (which AFAIK should support pgweb) on secure mode, I can't get a self signed root cert to work:\n```\npgweb --url postgres://root@localhost:26257/testdb?sslmode=verify-full&sslcert=client.root.crt&sslkey=client.root.key&sslrootcert=ca.crt\nPgweb v0.9.12 (git: 9af721176bf7b41366f0de8251fff0b47da8fce3)\nConnecting to server...\nError: x509: certificate signed by unknown authority\n```\nAny ideas?\n. I see! However nothing on that PR points to a different treatment of the certificates on the url string, so I think the author used it without ssl. Are self signed certificates supported by pgweb in vanilla postgres?. ",
    "mytototo": "I think it should be great to have this kind of feature even it's really basic. If people want more they always are able to customize it (and make a PR).. ",
    "Esya": "@sosedoff I did not even think about it, I assumed we had to execute the query first. \nMy go skills are limited so the solution I implemented is really just truncating the result array before sending it back to the browser, but maybe some sort of pagination, or truncating but telling the user that the results are truncated would be a better option?. ",
    "bjeanes": "Here's the fix for this in a similar query: https://github.com/demikl/pg_activity/commit/10cfc484b87e3ad018b9d6048954ff85f58ae904\nMight be useful for anybody wishing to address this before @sosedoff can.... It looks like this just removes the waiting column when in 9.6. waiting wasn't just removed in 9.6, though, it was replaced with wait_event and wait_event_type, to provide better context about why the query was waiting.\nTo quote from here:\n\nIn addition to the existing status information as pid, query_start or query provided in current versions, PostgreSQL 9.6 maintains two new columns in pg_stat_activity; the columns and wait_event and wait_event_type, which contain detailed information on locking. These replace the column waiting, which reported only TRUE or FALSE depending on the status of each row, signalling that has been waiting for a heavyweight lock.\nThe new wait_event column can also shed light on whether - and what kind of - LWLocks are waiting. Where a backend is waiting for a lock, the column contains the name of the wait_event events, or defaults to NULL.\n\nGiven that, the query should at minimum replace waiting with wait_event IS NOT NULL as waiting, if not explicitly selecting both wait_event and wait_event_type.\nWhat do you think @sosedoff?. ",
    "tamird": "It's possible to retrieve much of that metadata from information_schema, which is a portable alternative to the pg-specific meta tables. In any case, we should discuss that in #235 - this PR is a strict improvement, would you agree?. @sosedoff is this good to merge, then?. @sosedoff you can easily test against just an empty CockroachDB database for now (even that doesn't work). We have a library that makes it easy to programmatically run a CockroachDB process: https://godoc.org/github.com/cockroachdb/cockroach-go/testserver. ",
    "casoetan": "No plans for this yet?. ",
    "fire": "Looks supported in Cockroachdb 2. https://github.com/cockroachdb/cockroach/pull/23334. ",
    "tistvan": "Yes, you are right. The reason behind this ticket is I would like to create sql dumps from Postgresql databases with a web UI and without ssh access. :). I never said I needed pg_dump inside pgweb package :) . I just said it would be nice to create SQL dumps using pgweb. It doesn't matter if it is done using pg_dump or not. I just want to create SQL dumps from a web UI. :)\nAnyway I am closing this issue because it is done in #270 .. And thanks for this feature :). ",
    "janpieper": "@sosedoff No, this isn't the problem. I'm not complaining about a failing test. The whole testsuite is crashing (Error 2) without any changes to the code. The x-font-woff test was failing in both cases.\nOfficial sources\nFAIL    github.com/sosedoff/pgweb/pkg/api       0.009s\nok      github.com/sosedoff/pgweb/pkg/bookmarks 0.005s  coverage: 97.4% of statements\n?       github.com/sosedoff/pgweb/pkg/cli       [no test files]\nok      github.com/sosedoff/pgweb/pkg/client    6.641s  coverage: 50.6% of statements\nok      github.com/sosedoff/pgweb/pkg/command   0.005s  coverage: 64.7% of statements\nok      github.com/sosedoff/pgweb/pkg/connection        0.004s  coverage: 90.0% of statements\n?       github.com/sosedoff/pgweb/pkg/data      [no test files]\n?       github.com/sosedoff/pgweb/pkg/history   [no test files]\n?       github.com/sosedoff/pgweb/pkg/shared    [no test files]\n?       github.com/sosedoff/pgweb/pkg/statements        [no test files]\n?       github.com/sosedoff/pgweb/pkg/util      [no test files]\nMakefile:28: recipe for target 'test' failed\nmake: *** [test] Error 1\nMy fork\nFAIL    github.com/janpieper/pgweb/pkg/api      0.009s\nok      github.com/janpieper/pgweb/pkg/bookmarks        0.005s  coverage: 97.4% of statements\nMakefile:28: recipe for target 'test' failed\nmake: *** [test] Error 2. ",
    "maximRnback": "@sosedoff \nHi,\nI'm working on embedding pgweb in our system. \nWe have a SSO system for authentication and authorization.\nTo make the integration seamless (\"bypassing the pgweb login page\") I'm adding a REST route that takes an id. The handler goes through authorization process with the authorization server, gets the connection credentials (using the id) from another service, and finally invokes the connect handler.\nI could either implement it directly in pgweb's code, or implement a gin middleware.\nI can make it generic enough for others to use.\nWould you like me to make a pool request with this once it's done? Do you have any preferences  regarding the implementation?\nThanks\n. #207 \nI see i'm not the first one to come up with this suggestion.. @wshi94 @tzachshabtay How do you guys authenticate your users?\nWe use Keycloak. I'm currently working on a gin middleware for  OIDC Authorization Code Flow using the go-oidc package.\nIt should work with any OIDC compliant authorization provider. Would this work for you?\n\n\nThe changes @maximRNBack is proposing seems to fit our use case, although it would also need to work well with sessions.\n\nMy solution assumes you use pgweb's sessions flag.. @tzachshabtay  I have something working but the code is a mess. Would you like me to clean it up a bit and upload it? :) . Hi guys,\nSorry for the late response. It took me longer then I thought.\nIf you want to see the final result check out rnback's staging subdomain. Soon we'll move it to rnback's production subdomain.\nThe source diff can be found here (github.com/maximRNBack/pgweb/)\n@sosedoff  I made an effort to make a minimal impact on pgweb's codebase to simplify merges with future versions of pgweb. If a pull request is an option I'd be happy to make the necessary changes to fit with pgweb's code in a cleaner manner.\nMy high-level design:\nThe integrating website has a page with a HTML <iframe /> pointing to https://pgweb-domain/authorize/<requested db id>.\nAccessing this domain would:\n1. seamlessly authenticate the user against the identity provider (SSO - OIDC) using my gin_oidc middleware\n2. authorize the access - make sure the user should have access to the requested DB\n3. get the DB's connection URL\n4. connect the user to the DB\n5. send redirect response to pgweb's homepage (he'll skip the connection form since he's connected)\n6. if there was an error in the process - redirect to an error page\nnote on (2) + (3): (\"getConnUrlEndpoint\") developers should add a POST endpoint (possibly with CORS and OPTIONS enabled - depends on the architecture) to their website's backend that will receive  user's info (oidc claims acquired during authentication) and the <requested db id> and upon successful authorization will return the DB connection URL. Upon failure an error will be returned. My current implementation requests a bearer access token from the Identity provider configured in \"credentialsOidcConfig\" in the json configurations file, and sends it in the authorization HTTP header. I guess it's an overkill if the endpoint is on the same server and limited only to localhost but that's not the case in my architecture, plus we're passing credentials so I prefer a safer implementation. I guess this could be optional.\nnode on (6): (errorEndpoint ) developers should add a GET endpoint to their website's backend that will receive an error message in the query parameter err and show an error page (or do whatever they want)\nconfigurations file example(secrets.json in executable's dir):\n``javascript\n{\n  //(string) id in the authorization service (OIDC provider)\n  clientId,\n  //(string) secret from the authorization service (OIDC provider)\n  clientSecret, \n  //(string) the URL identifier for the authorization service. for example: \"https://accounts.google.com\" - try adding \"/.well-known/openid-configuration\" to the end of the path to make sure it's correct\n  issuer, \n  //(string) pgweb's url. for example: \"http://localhost:8081/\" or \"https://pgweb.mydomain.com/ - we can also extract it from pgweb's--bindflag\n  pgwebUrl,\n  //(array of strings) OAuth scopes. for example: [\"openid\", \"profile\", \"email\"]\n  scopes, \n  //redirection url after logout\n  LogoutRedirectUrl,\n}\n{\n  //users authentication OIDC configurations\n  \"usersOidcConfig\": {\n    //(string) id in the authorization service (OIDC provider)\n    \"clientId\": \"pgweb\",\n    //(string) secret from the authorization service (OIDC provider)\n    \"clientSecret\": \"cd850d22-791e-4347-9220-fb6da8f8271d\",\n    //(string) the URL identifier for the authorization service. for example: \"https://accounts.google.com\" - try adding \"/.well-known/openid-configuration\" to the end of the path to make sure it's correct\n    \"issuer\": \"http://localhost:8080/auth/realms/Developers\",\n    //(array of strings) OAuth scopes. for example: [\"openid\", \"profile\", \"email\"]\n    \"scopes\": [\n      \"openid\",\n      \"profile\",\n      \"email\"\n    ]\n  },\n  //pgweb's bearer OIDC configurations (might be the same as 'usersOidcConfig')\n  \"credentialsOidcConfig\": {\n    \"clientId\": \"pgweb\",\n    \"clientSecret\": \"cd850d22-791e-4347-9220-fb6da8f8271d\",\n    \"issuer\": \"http://localhost:8080/auth/realms/Developers\",\n    \"scopes\": [\n      \"openid\",\n      \"profile\",\n      \"email\"\n    ]\n  },\n  //(string) pgweb's url. for example: \"http://localhost:8081/\" or \"https://pgweb.mydomain.com/ - we can also extract it from pgweb's--bind` flag\n  \"pgwebUrl\": \"http://localhost:8081/\",\n  //(string) redirection url after logout\n  \"logoutRedirectUrl\": \"http://localhost:3000/\",\n  //(string) the endpoint developers should implement for the authorization and retrival of connection credentials\n  \"getConnUrlEndpoint\": \"http://localhost:3000/pgweb/get-conn-url\",\n  //(string) the endpoint developers should implement to handle with errors\n  \"errorEndpoint\": \"http://localhost:3000/pgweb/error\"\n}\n```\nto run pgweb use:\nbash\n./pgweb --server-sessions --sessions --skip-open\nHere's an implementation of getConnUrlEndpoint and errorEndpoint in an express app:\n(NOTE - access token validation is not included. It depend's on your setup)\n```javascript\nconst express = require('express');\nconst cors = require('cors');\nconst assert = require('assert');\nconst router = express.Router();\nconst corsOptions = {origin: true}; // in production should be something like {origin: ['https://my.pgweb.domain'],methods: ['POST', 'OPTIONS']};\nrouter.options('/get-conn-url', cors(corsOptions));// enable pre-flight request\nrouter.post('/get-conn-url', cors(corsOptions), (req, res) => {\n  const dbId = req.body.id;\n  assert(!!dbId, 'didn\\'t get db id in request');\n  const email = req.body.oidcClaims.email;\n  assert(!!email, 'didn\\'t get email in request');\n  const emailsWithAccess = getUsersWithAccessToDb(dbId).map(u => u.email);\n  assert(email in emailsWithAccess, 'email mismatch');\n  const c = getDbCredentials(dbId);\n  res.status(200).send(postgres://${c.user}:${c.password}@${c.host}:${c.port}/${c.database}?sslmode=require);\n});\nrouter.get('/error', (req, res) => {\n  res.render('ErrorPage', {\n    title: 'CONNECTION ERROR',\n    subtitle: 'failed to authorize access to database',\n    paragraph: 'The error was logged. Please try refreshing the page, or in a few minutes. If the error persists please contact us.',\n  });\n});\nmodule.exports = router;\n```\nI've also made some UI changes but I didn't include them in the commit. Is anyone interested? \nOne last note - if you're trying to clone my repo and build it - the build will fail because some of the dependencies assume you're in sosedoff source dir. You'll need to clone sosedoff repo into $GOPATH/src/github.com/sosedoff/pgweb, add my repo as another remote, fetch my branch and then build.\n@tzachshabtay  @sosedoff  @wshi94  let me know what you think . At a glance this would solve half of the problem for me. The authentication of users (i.e. is this my user?) would be solved, but the authorization (i.e. should the user be able to access the requested database?) and the secure database credentials request (I need to be sure it's pgweb requesting the credentials) are still missing.\nThanks for the quick reply. I'll look into it and get back to you.. I got oauth2_proxy to work. But like I said, it only takes care of the authentication aspect.\nI propose adding two optional features/flags on top of your #266 proposal. You can see the changes here. \n\n--connect-token-conf - path to a configurations file for bearer access token request (it's OAuth2's client credentials grant flow). The token is attached to the url-connection request. This is a bit safer than the fixed token. [this is the relevant commit]\nTo be able to authorize the request, I need to know who is trying to access the resource. For that I added --connect-with-header flag that takes a http header name. It passes the header with that name from the browser's request to the url-connection request. [this is the relevant commit]  The browser's request also passes through oauth2_proxy, where I can attach some identifying details I can trust.\n\nWhat do you think? @sosedoff . @sosedoff Looks great. I can work with that.\nThanks!\n\nOne other note. It might be off-topic. I can open another issue if you'd like. It's regarding pgweb's \"x-session-id\" http header.\nLets say user A and user B both sign in and access their DBs. If user B somehow gets a hold on user A's sessionId he can go to http://mypgweb.domain/?session=<user A's session id> and access a DB he shouldn't be able to access. It's pretty hard to get a hold on the sessionId value, since http request header & query parameters are end-to-end encrypted with https. Still there are security issues I don't want to get into. \nThe bottom line is - it might be a good idea to integrate a more advanced approach internally (it won't affect pgweb's API). Maybe replace the sessionId with JWT.. ",
    "wshi94": "Hi,\nWe've been attempting to integrate pgweb into our system and attempted some of the suggestions given by @sosedoff, however our use case is such that we'd like to hide connection credentials from the user and and be able to have any number of sessions. The sessions flag works well, but we'd need a way of passing credentials for each session as well.\nThe changes @maximRNBack is proposing seems to fit our use case, although it would also need to work well with sessions.\nThanks!. ",
    "tzachshabtay": "@maximRNBack we use google for our authentication, so if I understand correctly it should work fine with OIDC.. @maximRNBack yes, that would be sweet, thanks.. @maximRNBack any updates?\nThanks.. Wow, thank you guys for your hard work!\n@sosedoff saw you wrote in the PR (\"sourced from a third-party backend (http).\"), will it also work for https backend?\nI'm currently on a long vacation, so won't have time to look at the two solutions offered at least until mid October. Perhaps @wshi94 will have time to examine them.. ",
    "maksm90": "Do you mean the button that downloads a query result?\nI propose the feature of printing result on window in specified format so that it can be copy-pasted to somewhere. Moreover, this format should be pretty to look at.\n. > List of functions could be added, but i'm not sure about function editor. You can always use custom sql query tab for that\nYes, sql query tab is intended for editing any queries including function body. But it would be nice to see the list of functions on left panel and have some tabs describing properties of selected function.. ",
    "PrincipalsOffice": "I am interested in working on this. :). For autocomplete, is there a preference on whether to utilize a third party library or not?. I tried to ran it on Safari instead of Chrome and still can't see any changes.\nI cloned the repo under $GOPATH/src/, and git diff shows that pkg/data/bindata.go is modified.. Thanks!. No problem!. ",
    "01e9": "Feedback: I like pgweb modern design, but adminer is more feature rich, for example foreign key management. pgweb looks nice but it's good only for data browsing.. ",
    "varp": "@sosedoff you didn't accept previous ones. I need motivation =) \n\n\n@varp another feature you could work on.\n. @sosedoff ok. I got it. Just finished reading a book on Golang =) . Sorry, but in #268 announcements there nothing about rows sorting. Ohh, I understood. Then I'm closing the issue. But not on the SQL results. I have tables with more than dozen millions of rows and sometimes I need a couple of rows from production Postgres DB to a developer MySQL table and the simplest way to do it is just to copy/paste and execute SQL. @rex-sheridan please reopen the issue, please.. @sosedoff sometime we need to do it, right? As you said, someone else might want to do PR. I thought it\u2019s logical to leave the issue opened. @icalF I can suggest you to use Posgres COPY function, which is faster than INSERT INTO. About script/test_ginkgo.sh. I left comments in PR with explanations of goal which I'am trying to reach. Shortly, \n- run pgweb GUI from docker image\n- looping through PG 9.1 - 9.6 backends and on each step we test pgweb GUI against different versions of Postgres . There is adjacent issue #287. @sosedoff I think you can close my one - #287. I've added disable query timeout checkbox in UI. Look at PR https://github.com/sosedoff/pgweb/pull/339. And that? \ud83d\ude42 . OK! . I\u2019ll update the PR  on this week. The PR is updated. Please check and give me feedback. The TravisCI pipeline fails because of TLS error during connecting to the GitHub. At the moment I don't why it fails, but I'm still thinking about it.. Ok! I've resolved the issue with TLS using Alpine 3.8 as base image. Are there any chances that the work will be merged in? Maybe I need to do some changes?. I'm open to any constructive critic related to the code or something else, my friend =). @sosedoff what about that?. @sosedoff First it starts pgweb's container for acceptance testing. Let point it as PGWEB_CONTAINER.. There I'm starting different PG versions to which I will connect from the PGWEB_CONTAINER in acceptance tests. Here I'm running acceptance test from the PGWEB_CONTINAER to different PG backend. Hm, very strange! I'll check when I be at home.. Because of that I've added --disable-check-update CLI option. You right, I hurried and made the typo!. OK. Very, very strange. I also work on OSX and run pgweb form the similar place..... >We could try and figure out if it's possible to do a background update until the next start, which is\na definitely better UX here.\n\n\nYou know this an idea.. I didn't know about json.Decode. Thx. Can not reproduce.... Fixed. Fixed. Fixed. Fixed. Fixed. ",
    "DylanGriffith": "I'd like to have a go at this. I'm not super familiar with Go, but generally familiar with integration testing.\nI was thinking I'd follow the instructions on Agouti for setup but I imagine that you'll want me to add a make task to get the test running on Travis. I assume that phantomjs is probably the easiest way to get the test running on travis.\nI imagine I'll write at least one test that does the following:\n\nConnects to a database\nQuery the database\nDisconnect\n\nWhen I've got that up I can try to figure out what are the next features to tackle.\nDoes that seem like a good strategy @sosedoff ? Do you have any recommendations about how to structure the tests and the code? It looks like the agouti docs have a specific format for naming files and setup etc. but I'm not exactly sure where I should put those files. I assume the root directory is not the most appropriate. I could create some directory called integration perhaps.. ",
    "anpleenko": "+1. ",
    "yathuganesh": "custom uerry contents leak\n. ",
    "koneko096": "I'm interested to help with this. Then, how could we determine type of the columns?. Nice. Will work on it ASAP. ",
    "BharatKalluri": "Can I take this?. I am sorry, but I am not able to figure out how I should fix this. Any help?. ",
    "ldinc": "Is fixing client.IsIdle() func up to using with 2 new clli flags enough?\nOr i missunderstood session cleanup worker?. https://github.com/ldinc/pgweb/commit/3067b06bf6e8b7f0efd6849983bd4205ddb48915\nOr better change timeout minutes from float64 to int?. Hi, shoud it be copied to clipboard from \"data-id\" value or value from ...?. Yeah, you are right, I will update it to string split! thx. My bad(\nfixed. ",
    "coline-carle": "I didn't see if we can set the pagintion to a lower number, than the default, so it's actually 100 row displayed for a total of 15000 row on my dev database, about 15 million on production if it matter, but didn't try  to render it in pgweb installed on my production server.. I can't test for a long moment, you can probably close the issue meantime  I'll come back when I'll have information to report back to you.. ",
    "cristianoliveira": "Hey @sosedoff I will try to do it! I've been using the pgweb and I think it would might be easy :). I'll take it. Hey @sosedoff Sorry. I couldn't take a look into it still. Do you think some change regarding this error on  pgweb/pkg/cli/cli.go is still necessary? If so I can take a look later or other day. Ahh great! :). ",
    "th3hamm0r": "+1. ",
    "szemek": "@sosedoff maybe I missed something, but is there any option to check which mode is used on JavaScript side? If not I was thinking about passing this piece of information via HTTP header or in payload (/connection or /connect endpoint).. Hmm, I thought about endpoints and I'm not sure whether, in current state of codebase, extending GET /connection will be enough.\nPretty much there are 2 scenarios when connecting to database\n\nGET /connection (400 error) -> POST /connect, we are not connected, we fill form and connect\nGET /connection, we are already connected\n\nAdding info about single / multiple session(s) to responses for both endpoints GET /connection and POST /connect seems like a bad idea to me.. ",
    "rex-sheridan": "I also tried a query like the following to force the text column to render:\nselect '''' || payload || ''''  from payment_event. We are using pgweb as of commit aba81334a5d1774a5a347ae45ce326ee4e91745f.  We are using PostgreSQL 9.6.4.  \nAttached screenshots and JSON dump.\n\n\npgweb-1508514613.txt (JSON)\n. When I used your dump file everything looks fine.  Not sure what is going on with my schema.\n\n. I was able to recreate the issue by dumping my own data and importing it to a new DB.  It appears related to data populated as large objects.\nI see in the dump file from my schema stuff like the following:\nSELECT pg_catalog.lo_open('41006', 131072);\nSELECT pg_catalog.lowrite(0, '\\x7b227665...');\nSELECT pg_catalog.lo_close(0);\n. I have attached the SQL dump.\npgdump2.sql.gz\nI am using the standard postgres JDBC driver to populate these objects in my application.  Since the contents of the column can be arbitrarily large in my application I am using Clob functionality.  I investigated the source code to see  how the driver is implemented.  When using Clobs the steps it takes are basically the following:\n\nInvoke the LargeObjectManager as described at Chapter 7. Storing Binary Data and create a handle (oid) to a large object entry in pg_largeobject and pg_largeobject_metadata\nWrite the bytes to pg_largeobject using the handle\nWrite the handle oid value to the column of the target table\n\nThis explains why you saw numerical incrementing values in the payload column.  They were oid references to pg_largeobject.\nI am not sure what a good solution would look like here since a text column can also hold normal strings.  How would one differentiate between an oid reference and a string?  Perhaps provide a way to manually tell pgweb \"this value should be dereferenced in pg_largeobject+pg_largeobject_metadata\"? . Now that I am aware of the behavior I think it's totally acceptable to leave pgweb as-is.  No need to clutter the application.  Thanks for your help and for creating a really useful application.. Reopened at the request of @varp. . ",
    "budden": "Hi! I picked up this PR and want to finish it. Can you please elaborate about panics: how did you achieve it? . I created one case of panic by specifying a file with an incorrect structure, but it seem to have recovered from it successfully.. > Are you on OSX?\nI'm on Debian Stretch, but I did some manual setup when installing postgres (forgot them already). I think first of all I'll reinstall postgress afresh in an official way, and then retry. \n. > Here's my pg_hba.conf\nThanks, now tests are working. I also had issues with my non-US locale. I tried to avoid that and did some changes like SET lc_messages='C', it helps for some cases. If you like, I can add the commit with that to this PR, or as a separate PR. But I was unable to change psql's (client side) messages language, so \nfinally I had to change my Linux's locale. \nSo I'll try to fix the rest of your notes. What goes to cleaning the table, I still suggest not to do that. Just write in the import that we are adding records, instead of calling it \"import\". But if you confirm your decision, I'll implement it. . Haha, I found this one: https://www.postgresql.org/docs/\uff19.6/sql-copy.html\nIt allows to feed data via stdin and can read csv format. So it looks like we should not parse csv ourselves, but instead delegate the file contents to the psql via stdin... \nI'll experiment with it. . So something simple as \n/y/go/src/github.com/sosedoff/pgweb$ cat /y/a.csv | psql -c \"\\\\copy zzz (text,id) from stdin with (format csv, header true)\" does the job. \\copy command is advertized to be a counterpart of copy which one can run from psql if having no permissions to call copy. Also one can use just copy, w/o backslashes - it works either. \nSo one way to go is to simply run psql from stdin. We check that fields match, generate field order so that csv and postgres table match, direct file contents to stdin of psql and spawn psql from pgweb. We can fail however under stupid platforms like ms windows where stdin can be broken sometimes (e.g. fail to support utf-8), or if there are some bugs/limitations related to spawning. So furhter experiments are required. Can you please give a feedback about this approach? \n. Fixed all the issues that I was able to understand and which I believe are really issues. The only thing that is missing is tests. . Seem to fix everything. Also added refreshing of table lists and some simple input data validation. Most questionalbe is the change in css files - I removed a fixed alignment for output pane. All seem to be ok at a glance, maybe I overlooked something. Without that change import button didn't work. \nDidn't add the test for the entire import. I didn't find an essential api tests so there is no infrastructure, and so far I'm not smart enough to generate one quickly. Any hints? What goes to COPY, I decided not to go into it now - it is just another project. . Added an integration test. Maybe it is horrible due to lack of understaning of pgweb's globals, but I made it work finally :) . I dislike this way of testing, but I don't know how to do it better. What is actually done here is a fork of main module. After the fork, application was initialized so that testing scenario has access to both web API (can send requests and get the response), and to the database directly. When testing web api, session stuff is kind of faked, so that there is no login / call api/ logout sequence. IMO this is messy. What I really want to do here is:\n\ncreate and import the DB\nrun pgweb as an application from the script (with Exec.Command and pipes)\nlogin into it\ncall /import/csv api to import data\ncall /query api \nlogout\ndrop db\n\nThis way it is a sort of API testing. \nHowever, there are questions. I don't see how to do that with an embedded testing framework, because I need to build pgweb before running the test. \nThere are options:\ni) make the thin layer between main() and app functionality, like func main() { return Main() }. For testing purposes, run Main in a goroutine, and also run a test script in a main thread. Test script only communicates with Main via HTTP and when done sends termination signal. \nii) use Docker. I think that's an overkill\niii) write a separated golang application and use a makefile to run it (we will be unable to use go test)\nYour ideas? . Actually I'm in progress to test it via API endpoint and it seems that I'm not very far from obtaining a prettier test. I personally prefer integration tests because they give more value per line of test code :) I hope to finish that today and maybe will do another squashed PR. . Well, now I'm satisfied with the result. Didn't do squash (too tired today), and recognized that there were no need to export initOptions. Also not sure about removing \"absolute\" from some s.. Superseded by #422. This discussion seem to be relevant: https://github.com/golang/go/issues/18478 . I think I made it work finally. Maybe test suite is a bit over-engineered, which was not expected initially. Can you please review ASAP. Actually a motive beyond this work is to collect a portfolio for subsequent hiring in  a Golang-related area. So far I only have contributed to gitql, but that's too trivial and not very typical golang application. I started this project as a \"low hanging fruit\", but this become a sort of moving target hunt and I grandly underestimated the effort required for the test infratstructure :) . > 1. ParseFieldDelimiter belongs to api/helpers.go\nDone\n\n\nReplace regular expression with case-insensitive and shorter version: (i?)^[\\w][\\w$]*$\n\n\nIt is a wrong one, because \nSQL identifiers and key words must begin with a letter (a-z, but also letters with diacritical marks and non-Latin letters) or an underscore (_). Subsequent characters in an identifier or key word can be letters, underscores, digits (0-9), or dollar signs ($).. Your suggestion will match 12. Took ^[a-zA-Z_][\\w$]*$\n\n\nDo not panic in isPostgresqlIdentifierRequiringNoQuoting, just return an error (and handle it)\n\n\nDone. (However I have no idea how regex matching can err)\n. Hi! Any news? . Hi! Sorry for annoyance, but what about this one? :) . > Alternatively you can perform the import in a single transaction\nThere is a contradiction. Huge transactions are bad. So if a table is really huge, it is impractical to populate the entire table in a single transaction. But if we use multiple transactions, then to clear the table we must issue an additional \"truncate table\" or \"drop table\" statement explicitly. Then, what if the import failed due to lost sql server connection? In this case we can't enforce cleanup. So the state of the table after failure depends on the nature of failure. \nSo if I was to decide, I would not do that. If something failed, I would leave cleaning table up to the user. I would just document that and that's it. So, failures get simpler to understand: whenever failure happened, user knows that (s)he must clean the table by himself. \n. Sorry, I didn't understand this.. Fixed. Fixed. Done. Done. ",
    "mdnight": "These are my own changes. I've added my own function decodePrivateKey to the vendor/golang.org/x/crypto/ssh/keys.go and modified some functions in that file in order that would be possible to decrypt a protected key. it supposed to use a field 'ssh password'. ",
    "transcranial": "Same issue, occurs with trying to show the \"Structure\" tab for a table containing uppercase letters as well:\nERROR: pq: relation \"public.exampletable\" does not exist, for exampleTable. @sosedoff would love to see this merged. Using docker compose:\nyml\nversion: '3'\nservices:\n  postgres:\n    image: postgres:9.6-alpine\n    ports:\n      - 5432:5432\n  pgweb:\n    restart: always\n    image: sosedoff/pgweb:0.11.1\n    ports:\n      - 8081:8081\n    links:\n      - postgres:postgres\n    environment:\n      - DATABASE_URL=postgres://postgres:postgres@postgres:5432/postgres?sslmode=disable\n    depends_on:\n      - postgres\nCreate table using:\nsql\nCREATE TABLE \"testTable\" (\n  id    integer PRIMARY KEY,\n  name   varchar(40)\n);\nClick on \"Rows\" tab, get following message:\nERROR: pq: relation \"public.testtable\" does not exist\nAlso tried combos of:\npostgres:10.6-alpine\npostgres:11.1-alpine \nand \nsosedoff/pgweb:0.11.0\nsosedoff/pgweb:0.10.0. \n\"Rows\" tab works fine on sosedoff/pgweb:0.10.0.. ",
    "kevflynn": "I believe the same issue occurs when trying to add query constraints for columns with uppercase letters - i.e. select * from table where columnName = 'test'\nERROR: pq: column \"columnNmae\" does not exist. ",
    "alfozan": "@transcranial : try public.\"exampletable\"  instead of \"public.exampletable\"\n@sosedoff : any plans to fix this in the upcoming release?\nThank you. ",
    "wilbeibi": "@sosedoff 0.9.9. Oh, sorry, thank you for the suggestions. \nAfter git pull I get the latest version and it's working now (weird, why go get does not give me the right one?). And the brew cask issue is because my environment PATH found the go get one first.\nThanks again for your work.. ",
    "codyleyhan": "I think I can handle this!\nThis is referring to the exported queries correct?. I ended up with this compromise as a solution, save to both session storage and local storage.  When we are attempting to pull from storage, first check if there was a query in session storage, if there was nothing in session storage then pull from local storage.  This allows for intended behavior in single session mode as well as the isolation in multi session mode.  I have also set it to on change write to both local and session storage, so the most recent query no matter the session will update the local storage.. ",
    "fzumstein": "This would indeed be my biggest UX issue. Instead of your suggested split, you could also think of leaving the current layout but to either split the window using a slider or even just have the ability to show/hide the lower/result part. . ",
    "joemccann": "ahhhh sorry I didn't see that in the docs!. yup that fixed it. Thanks!. ",
    "wyangsun": "need use '--sessions' options.. ",
    "MstWntd": "Yup that worked!.. btw you have amazing project here and i really appreciate it!. ",
    "backy-io": "Hi @sosedoff \nIclose request, and write more test for cockroachdb.\nThx. ",
    "aksdb": "For now - since this is the only place where I can manipulate data.\nIf pgweb gets form / table based editing support sometime in the future, that would be a good candidate for transactions as well.. ",
    "bsx": "The size of the PR comes mainly from the inclusion of the AWS SDK. As for the background: I'm using pgweb mainly in AWS environments and thought it'd be a nice addition if it could autofill the existing instances. If you'd like to discuss scope or changes to configuration I'm happy to incorporate suggestions. If you ignore the additional libs required in vendor, it's actually a fairly small feature addition IMHO. :). I'm running it on EC2 instances in multi session mode for multiple users.. Sorry, haven't had the time yet but I'll definitely check it out\n. I've played around with the \"Connect\" featuee a bit. Seems promising so far. I've noticed however that it is not possible to combine it with the \"Lock Session\" feature, but maybe I'm misunderstanding the session lock thing.\nI'm currently thinking about a \"companion\" app that handles auth and connection details so the user would probably never see the connection form of pgweb itself.. Cool, I'll look at it over the weekend. Hey, love the discovery stuff. Since i don't use Heroku i can't comment on that, but here are my thoughts on the AWS bits:\n You currently require passing access_key and secret_key for AWS authentication. That is quite limiting in a lot of situations. I'd suggest that instead of failing if either one of those is not passed via CLI or ENV to fall back on the default credential chain in the AWS SDK. It takes into account several ENV variables, the aws-cli config files (~/.aws/config and ~/.aws/credentials) as well as EC2 instance profiles and ECS container profiles if you deploy inside your AWS account. It can deal with assumed roles and the required additional tokens (AWS auth can be quite complex ...). It would be kinda like you already read the HEROKU_TOKEN from .netrc.\n It'd be cool if in addition to the plain postgres instances the discovery would also list the aurora-postgres clusters (see my original PR) ideally, although not strictly required, with the ability to connect to the read-only endpoint if pgweb is running in read-only mode.\n* As a long term goal, definitely out of scope for the basic discovery feature, it'd be cool to support IAM authentication towards the database although tbh i have no practical experience with that yet.\nAnyway, really excited about this feature and can't wait to see where it goes. Feel free to take whatever bits you need from the PR with my attempt at this and then close #359. Thanks for looking into this!. Hey, sorry for the delay. For the AWS config files i'd really recommend against parsing them yourself. I'd say if the access_key and secret_key aren't passed via CLI flags just hand of the credential acquisition to the SDK. It's way more flexible in its capabilities and can deal with all sorts of weird auth configs.. ",
    "tomholub": "Once I issue the following statement & refresh the page, error goes away:\nsql\nSET database = db_name;\nIdeally, the app would work properly w/o a DB selected.. ```\n2018/05/23 17:02:22 [Recovery] panic recovered:\nGET /api/activity?_=1527066075832 HTTP/1.1\nHost: localhost:8081\nAccept: /\nAccept-Encoding: gzip, deflate, br\nAccept-Language: en-US,en;q=0.9\nConnection: keep-alive\nDnt: 1\nReferer: http://localhost:8081/\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36\nX-Requested-With: XMLHttpRequest\nX-Session-Id: 333ae22c-1f32-2aba-09ce-57de76262c2a\nruntime error: slice bounds out of range\n/usr/local/Cellar/go/1.10/libexec/src/runtime/panic.go:505 (0x4298d8)\n/usr/local/Cellar/go/1.10/libexec/src/runtime/panic.go:35 (0x42878d)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/pkg/client/client.go:234 (0x814fd9)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/pkg/api/api.go:397 (0x821908)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/context.go:98 (0x763392)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/pkg/api/middleware.go:31 (0x824fbe)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/context.go:98 (0x763392)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/recovery.go:45 (0x7719b9)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/context.go:98 (0x763392)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/logger.go:77 (0x770dd6)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/context.go:98 (0x763392)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/gin.go:290 (0x768fec)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/gin.go:271 (0x768a02)\n/usr/local/Cellar/go/1.10/libexec/src/net/http/server.go:2694 (0x67110b)\n/usr/local/Cellar/go/1.10/libexec/src/net/http/server.go:1830 (0x66d330)\n/usr/local/Cellar/go/1.10/libexec/src/runtime/asm_amd64.s:2361 (0x456a60)\n[GIN] 2018/05/23 - 17:02:22 | 500 |     549.957\u00b5s |       127.0.0.1 |  GET     /api/activity\n2018/05/23 17:02:32 [Recovery] panic recovered:\nGET /api/activity?_=1527066075833 HTTP/1.1\nHost: localhost:8081\nAccept: /\nAccept-Encoding: gzip, deflate, br\nAccept-Language: en-US,en;q=0.9\nConnection: keep-alive\nDnt: 1\nReferer: http://localhost:8081/\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36\nX-Requested-With: XMLHttpRequest\nX-Session-Id: 333ae22c-1f32-2aba-09ce-57de76262c2a\nruntime error: slice bounds out of range\n/usr/local/Cellar/go/1.10/libexec/src/runtime/panic.go:505 (0x4298d8)\n/usr/local/Cellar/go/1.10/libexec/src/runtime/panic.go:35 (0x42878d)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/pkg/client/client.go:234 (0x814fd9)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/pkg/api/api.go:397 (0x821908)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/context.go:98 (0x763392)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/pkg/api/middleware.go:31 (0x824fbe)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/context.go:98 (0x763392)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/recovery.go:45 (0x7719b9)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/context.go:98 (0x763392)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/logger.go:77 (0x770dd6)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/context.go:98 (0x763392)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/gin.go:290 (0x768fec)\n/Users/sosedoff/go/src/github.com/sosedoff/pgweb/vendor/github.com/gin-gonic/gin/gin.go:271 (0x768a02)\n/usr/local/Cellar/go/1.10/libexec/src/net/http/server.go:2694 (0x67110b)\n/usr/local/Cellar/go/1.10/libexec/src/net/http/server.go:1830 (0x66d330)\n/usr/local/Cellar/go/1.10/libexec/src/runtime/asm_amd64.s:2361 (0x456a60)\n```. This is while connected to CockroachDB 2.0.2. ",
    "wiliamsouza": "Can you provide some tests?. ",
    "allisson": "@wiliamsouza test_EstimatedTableRowsCount \ud83d\udc4d . @sosedoff sounds good.. @sosedoff Take a look at the tests. @sosedoff done \ud83d\udc4d . ",
    "jas99": "Didn't work for me, but I am considering something weird might be with my reverse proxy; I am gonna stash this for later on my todo list to figure I out.\nI will post here once I do, anyways thx for suggestion @sosedoff \n. @sosedoff Yup those are docker compose files, the traefik configuration is actually done with command in compose file itself.\ncommand: traefik --docker \n--docker.domain=docker.localhost \n--logLevel=INFO \n--docker.watch \n--acme \n--acme.onhostrule \n--acme.httpchallenge.entrypoint=http \n--acme.storage=/etc/traefik/acme/acme.json \n--acme.email=dev@jas.bio \n--acme.entryPoint=https \n--entryPoints='Name:http Address::80 Redirect.EntryPoint:https' \n--entryPoints='Name:https Address::443 TLS' \n--defaultentrypoints=http,https. ",
    "natiki": "I have spun up a local development cluster of CockroachDB using the secure mode by following the instructions found at https://www.cockroachlabs.com/docs/stable/secure-a-cluster.html\nThis talks PostgreSQL on the wire and according to a Cockroach forum post people have had success using pgweb to connect to it.\nSo according to the primary node my connection string is:\npostgresql://root@localhost:26257?sslcert=certs%5Cclient.root.crt&sslkey=certs%5Cclient.root.key&sslmode=verify-full&sslrootcert=certs%5Cca.crt\nSo I attempt to call that as follows:\n\n. OK we are getting somewhere - double quotes is what is needed:\n```\nC:\\CockroachDB>pgweb --url 'postgresql://root@localhost:26257?sslcert=certs%5Cclient.root.crt&sslkey=certs%5Cclient.root.key&sslmode=verify-full&sslrootcert=certs%5Cca.crt'\nPgweb v0.9.12 (git: 9af721176bf7b41366f0de8251fff0b47da8fce3)\nError: Invalid URL. Valid format: postgres://user:password@host:port/db?sslmode=mode\n'sslkey' is not recognized as an internal or external command,\noperable program or batch file.\n'sslmode' is not recognized as an internal or external command,\noperable program or batch file.\n'sslrootcert' is not recognized as an internal or external command,\noperable program or batch file.\nC:\\CockroachDB>pgweb --url \"postgresql://root@localhost:26257?sslcert=certs%5Cclient.root.crt&sslkey=certs%5Cclient.root.key&sslmode=verify-full&sslrootcert=certs%5Cca.crt\"\nPgweb v0.9.12 (git: 9af721176bf7b41366f0de8251fff0b47da8fce3)\nConnecting to server...\nError: pq: Private key file has group or world access. Permissions should be u=rw (0600) or less.\n```\nSo as this is a windows machine I am not sure how you would like the permissions to avoid the 0600 error?. I understand it is a permissions issue. However this is a Windows system so what are you expecting the permissions to be on Windows?. Not as of yet. Basically even if I set the files to read only on Windows then it is still not happy. I tried same as the original reporter some other restrictions on the permissions of the files with no joy either.  According to that issue it appears that the fix is in to work around this in pq. Is it possible that pgweb / pg don't know what OS they are running on?. Cool. Will test new version when you release it and then hopefully I can close this out.. I logged in and was then able to pull. Strange. Anyway I am now trying to use the docker image to connect to a CockroachDB instance (see here for details) running outside of Docker.\nSo how to I modify the example given in your instructions to link to the cluster?\n. docker run -p 8181:8181 --link db:db -e DATABASE_URL=\"postgresql://root@localhost:26257?sslcert=certs%5Cclient.root.crt&sslkey=certs%5Cclient.root.key&sslmode=verify-full&sslrootcert=certs%5Cca.crt\" sosedoff/pgweb --network=host\nC:\\Program Files\\Docker\\Docker\\Resources\\bin\\docker.exe: Error response from daemon: Cannot link to a non running container: /reverent_wiles AS /peaceful_hawking/db.\nThe docker daemon is running as verified with docker info. docker run -p 8181:8181 --link db:db -e DATABASE_URL=\"postgresql://root@localhost:26257?sslcert=certs%5Cclient.root.crt&sslkey=certs%5Cclient.root.key&sslmode=verify-full&sslrootcert=certs%5Cca.crt\" --network=host sosedoff/pgweb\ndocker: Error response from daemon: conflicting options: host type networking can't be used with links. This would result in undefined behavior.\nSee 'docker run --help'.\nand then:\ndocker run -p 8181:8181 -e DATABASE_URL=\"postgresql://root@localhost:26257?sslcert=certs%5Cclient.root.crt&sslkey=certs%5Cclient.root.key&sslmode=verify-full&sslrootcert=certs%5Cca.crt\" --network=host sosedoff/pgweb\nWARNING: Published ports are discarded when using host network mode\nPgweb v0.9.12 (git: 9af721176bf7b41366f0de8251fff0b47da8fce3)\nConnecting to server...\nError: dial tcp [::1]:26257: connect: connection refused. ",
    "andrewnazarov": "Yes, a possibility to define the base URL would be a thing.. ",
    "ric2z": "@sosedoff I used the \"Deploy to Heroku\" button, so I'm guessing I'm on the latest version.. ",
    "foadgr": "@ric2z The schema and table should appear in the database browser when a table is created in a schema other than the public schema:\ncreate schema as blanket;\ncreate table blanket.event as ([your_table_here]);\nMight be an issue with schema DDL syntax. \n. ",
    "ttencate": "Same here:\nPgweb v0.10.0 (git: 54da27ef70b1d2ae9297cc5ea65158148b36107d)\n\nThe schema and its tables do appear in the left sidebar, but if I click one of them, instead of seeing the table's contents, I get an error:\n\nERROR: pq: relation \"public.tablename\" does not exist\n\nIndeed it doesn't, because tablename is not in the public schema and pgweb shouldn't be looking for it there.\n\n. ",
    "ayang": "Pgweb v0.9.12 (git: 9af721176bf7b41366f0de8251fff0b47da8fce3). ",
    "HaraldNordgren": "@sosedoff Ping. ",
    "JNFoo": "Good to hear! Thank you for closing the loop on this. \nBest,\n-JNFoo. ",
    "igorljubuncic": "Hey Dan,\nHappy new year!\nDid you get a chance to test the snaps? Do you have any questions or need any clarifications?. It should be straightforward:\nbrew install snapcraft\nsnapcraft (in the directory containing snapcraft.yaml). Hey. Just a quick follow up. Anything I can do to help here and get you up and running with snaps?. ",
    "mattburton": "Excellent thanks!. ",
    "dimrozakis": "The change in behavior described above was perhaps caused by #399. I'm using sosedoff/pgweb:latest which seems to be pointing to the same image id (81c2f0ef3f5a) as sosedoff/pgweb:0.11.0.. The issue I described occurs if postgres is down when pgweb is starting. So the steps to reproduce would be the following:\n- started pgweb with DATABASE_URL\n- started postgress\n- pgweb will display the login screen. ",
    "Nhat002": "Hi @sosedoff. \nFor the script, I take it from http://www.webtoolkit.info/, so not sure if I am allowed to remove the space. But of course space is redundant, I will remove it.\nFor the index.html, it's a mistake. So sorry about that. I will update in my commit.. @sosedoff  Hi Dan, any chance you could help look on this?. ",
    "majestique": "Thanks for that, it's running now and it looks like it's listening but I can't seem to connect. (I can connect to port 5432) just fine.\n\n\nI can connect to port 5432 just fine:\n\nmy firewall is open on port 8081 as well:\n\nAny idea why it's not working?. Got it to run with --bind=0.0.0.0. It's hosted on vultr.com running on CentOS 7.6. Apache isn't running right now.. Ah so pgweb isn't able run with other web services such as apache on the same VPS?. Beautiful!\nGot it working with this in apache:\n```\nNameVirtualHost *:80\n\n    ServerAdmin webmaster@mysite.com\n    ServerName mysite.com\n    ServerAlias www.mysite.com\n    DocumentRoot /var/www/html/\n\n\n    ServerAdmin me@mysite.com\n    ServerName pgweb.mysite.com\n    ProxyPreserveHost On\n# setup the proxy\n<Proxy *>\n    Order allow,deny\n    Allow from all\n</Proxy>\nProxyPass / http://localhost:8081/\nProxyPassReverse / http://localhost:8081/\n\n\n```\nThank you!. I see my problem now, missed the -- for auth-user. ",
    "msalcantara": "https://github.com/sosedoff/pgweb/wiki/Docker. ",
    "naiba": "Please change your docker run command to \ndocker run -e SESSIONS=true. ",
    "ke-da": "It's recommended to use array dependency inject annotation. See examples in: https://docs.angularjs.org/guide/di . This will save you trouble when minify JS with generic JS minifier.\n. Better to refactor this into Service and return promise instead?\n. Why not use 1.3 version? It's got some pretty nice update including the bindonce expression. See http://swirlycheetah.com/native-bind-once-in-angularjs-1-3/\nPerfect for oneway binding app such as this one.\n. The use of $rootScope is not justified here. You should use service instead to store global state. It's much more extendable that way.\n. Yeah, nice initial work. I'll do some refactoring and send you a pull request. :)\n. "
}
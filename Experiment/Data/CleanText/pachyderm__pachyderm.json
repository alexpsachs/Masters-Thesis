{
    "dontdieych": "oops, I got #1. >.<\n. core@coreos_production_qemu-494-0-0 ~ $ etcdctl ls /\nError:  Cannot sync with the cluster using peers 127.0.0.1:4001\ncore@coreos_production_qemu-494-0-0 ~ $ cat /etc/os-release         \nNAME=CoreOS\nID=coreos\nVERSION=444.5.0\nVERSION_ID=444.5.0\nBUILD_ID=\nPRETTY_NAME=\"CoreOS 444.5.0\"\nANSI_COLOR=\"1;32\"\nHOME_URL=\"https://coreos.com/\"\nBUG_REPORT_URL=\"https://github.com/coreos/bugs/issues\"\ncore@coreos_production_qemu-494-0-0 ~ $\n. ",
    "jdoliner": "Hmm, that seems to be trouble connecting to etcd. One theory is that etcd is down, you can test this with etcdctl ls /.\nIf that fails etcd is probably down. Otherwise something may have gone wrong with the networking of the containers. Also what version of CoreOS (cat /etc/os-release)?\n. This seems to indicate that something has gone wrong with etcd. A quick way to solve this problem would probably be to just create a new cluster. To investigate as to what caused the failure run:\nshell\n$ systemctl status -l etcd\n$ journalctl -b -u etcd\n. Hiya,\nPfs currently has a very simple distribution model.\nEvery pfs deployment has a static number of shards(S). A shard contains a subset of the files in the file system. Every file belongs to exactly 1 shard. The shard that a file belongs to is computed by hashing its file name.\nFor every shard pfs launches one master service and R replica services. Where R is a deployment wide value called \"the replication factor\". Each of these services is an http server running inside a Docker container that announces itself to etcd.\nPfs also launches a router service. The router takes request from users and forwards them to the appropriate master based on the file name.\nAn example of how a file gets committed in a pfs deploy:\nS = 2 and R = 1\n- User runs: curl -XPOST host/pfs/foo -d file.\n- router receives request and forwards it to master-1-2.\n- master-1-2 receives post and modifies local file system.\n- master-1-2 returns 200 (success) to router.\n- router returns 200 to user.\n- User runs: curl host/commit\n- router receives request and forwards it to master-0-2 and master-1-2\n- master-0-2 and master-1-2 receive requests and make a btrfs snapshot of of the local file system.\n- master-0-2and master-1-2 btrfs send the new snapshot to replica-0-2-1 and replica-1-2-1 respectively.\n- replica-0-2-1 and replica-1-2-1 respond to master-0-2 and master-1-2 with 200\n- master-0-2 and master-1-2 respond to router with 200\n- router responds to user with 200\n- user life is more fulfilled\nHope this helps your understanding. Our model is designed to be dead simple and just powerful enough that you can build more interesting things on top of it. I'm going to keep this issue up and repurpose it to be \"have documentation which answers this question. \n. This feature is underway and schedule to ship in v0.3. You can track progress here.\nThe API for this right now works (or is going to work) as follows:\n``` shell\nCreating a job\ncurl -XPOST pfs/job/foo -d '{\"input\" : [\"inputDir\", \"parentJob\"], \"container\" : \"jdoliner/container\", \"command\" : [\"/usr/bin/webserver\", \"arg1\", \"arg2\"]}'\nChecking the status of a job being run on \ncurl -XGET pfs/job/foo/status?commit=\nAccessing output from a job\ncurl -XGET pfs/job/foo/file/bar\n```\nOnce jobs are created via a post to /job they will automatically be kicked off following the next commit to the branch they were created on.\nAfter the initial commit jobs will be rerun any time new data is committed that changes the input to the job.\nJobs currently consist of a piece of JSON with 3 fields \"input\", \"container\" and \"command\". \"container\" and \"command\" are used in a call to docker run to spin up the webserver that documents are passed through. \"input\" defines which documents should be passed to the server. Input can either be directories from the raw filesystem or output from other jobs. Pfs will automatically schedule jobs based on their dependency on one another.\n. This issue also affects the api for /commit and /pfs. They will need to take a branch parameter.\n. This is present as of #6. The api was not quite as planned. Instead we now have 2 parameters that commands take branch and commit\nshell\ncurl [-XPOST, -XPUT] localhost/branch?commit=<commit>&branch=<branch>\n. Closing this in favor of #7\n. Release checklist:\n- [x] update README.md\n- [ ] Figure out what we're doing about changes. CHANGES.md\n- [ ] extend tester, right now it just sends traffic with a small amount of work we can also have it check correctness\n- [ ] tag v0.2\n. Fixed by #10.\n. This is implemented and ships in v0.2\n. This address #11.\n. :+1: on all of this. It seems like the most important use case might be merging jobs which actually doesn't suffer from the problem I mentioned above because jobs are human amounts of data.\n\nis there a notion of branching or forking or cloning a job? If so, what would merge/publish look like then?\n\nRight now jobs are just data that's stored in a special place. So branching, cloning etc. apply to those jobs the same way they apply to data. We could conceivably separate these 2.\n\nGiven the parallel tree structures of commits for raw data and MatViews, how do MatView on top of other MatViews fit into the picture?\n\nThis is the only part I don't fully understand. What does a MatView on top of a MatView mean?\n. One possible way of addressing this issue is using overlayfs as described in #28 .\n. I flagged 1 or 2 things with JAZ here.\n. # One proposal\nWhat if we referred to every type of object the same way we reference files, using the URL. The general idea is that the types are referred to hierarchically. I think this maps fairly well to the concepts we have. For example to access the filesystem at a specific commit we use a file subroute of commit:\n``` shell\naccess  as it was in \ncurl pfs/commit//file/\nthe default value for  is still \"master\":\ncurl pfs/file/ # reads from master\nso that use case doesn't get longer\n```\nHere's what it would look like:\n- /file\n``` shell\nAccess to master doesn't change\ncurl -XPUT pfs/file/ -d @\ncurl -XGET pfs/file/\nAccessing outdated data does\ncurl -XGET pfs/commit//file/\nAnd writing to other branches does as well\ncurl -XGET pfs/branch//file/\n``\n-/commit`\n``` shell\ncommitting a branch is now\ncurl -XPOST pfs/branch/foo/commit\ncommitting master remains unchanged\ncurl -XPOST pfs/commit\nListing commits remains unchanged\ncurl -XGET pfs/commit\nGetting commits from \ncurl -XGET pfs/branch//commit\n``\n-/branch`\n``` shell\ncreating  from \ncurl -XPOST pfs/commit//branch\nwriting  to \ncurl -XPUT pfs/branch//file/ -d @\n``\n. This has a very natural extension to the next primitive we're going to add which is jobs. Here's how the/job` route will look in this scheme I think:\n``` shell\nscheduling  on \ncurl -XPUT pfs/job/ -d @\nscheduling  on \ncurl -XPUT pfs/branch//job/ -d @\naccessing  from the output of \ncurl -XGET pfs/job//file/\nthis filesystem is read-only\ncurl -XPUT pfs/job//file/ -d @\nMethod not allowed.\n``\n. This is now feature complete. Tests are also passing. The rest of this month's window will be spent implementing a nontrivial pipeline as a more rigorous test and writing up that development process as a tutorial.\n. This has been subsumed by replication work being done now and isn't serving much purpose.\n. Interesting, there should at least be some logging for the files that get posted in to the server. Is there anything in theidentity` directory in pfs?\nEither way this case should definitely give users more of an idea what's going on than it currently does.\n. A few comments on this issue in general:\nI removed the v0.4 label because this obviously won't be shipping in that release. We've had to to rethink this issue a bit because materializing now happens asynchronously. This is good because materializing can take a while which may lead to timeouts and thus reported error message will be lost.\nWhat if we added a route like:\npfs/job/foo/log\nwhich would return stderr from the job.\nEdit: This would also work nicely because you could see the results of materializing on a particular commit using pfs/jobs/foo/log?commit=<commit>\n. > I think it'd be useful to have some blocking call that would stream the logs to my local machine. I'd like to know when a job is finished. I would like to have timestamps for logged messages.\nDefinitely agree on the value of both of these things. My idea for how this would work is that calls to get logs pfs/job/foo/log and job output pfs/job/foo/file/... would block until the job completed. So for example if I do:\ncurl pfs/job/foo/log?commit=<commit>\nthen that request will block until the job foo is materialize for <commit>\n\nHow is running the job on multiple machines implemented? I think, depending on that the logging could be also complicated.\n\nEach shard runs the job for its local data, recording the output from stderr of all of the containers it runs. This definitely makes collecting the logs a bit trickier. I'm not sure what it means for the interface for getting logs. Maybe we'd want a way to get logs from a particular physical machine or shard?\n. This has been fixed for a while. You can pull containers from any registry you'd like.\nWe had to temporarily unsupport the registry in the cluster due to some annoyances with certificates.\nWe'll be trying to get that reenabled soon!\n. No longer applicable with the new pfs/pps.\n. Hi!! Just read a bit more about OverlayFS and it seems like it should be doable.\nHere's how we use btrfs:\n- read only subvolume snapshots for commits, obvious analog in ofs\n- writeable subvolume snapshots for commits, The docs I read didn't explicitly says this but I assume ofs doesn't mind if 2 file systems share a lower fs given that they use it in a read only manner.\n- what changed - btrfs has a what changed command which tells you which files changed between snapshots. If I understand correctly, in ofs we could accomplish the same thing by just looking at the upper filesystem by itself.\n- send / receive are used for data transfer, this should work the same way as what changed\n- parent info, btrfs records where volumes came from which we use to record our commit history. This seems like the only thing that ofs doesn't have a direct analog for but it wouldn't be too hard to record it on the side.\nDo you happen to know much about the performance characteristics of ofs? One thing I would worry about is that under normally usage we expect users to create commits with something like a per minute granularity. Depending on how ofs is implemented lookups on files that haven't changed in a long time could get very costly because it will need to traverse several layers to reach them.\n. The above is also true for Kubernetes as far as I know. We'll aim to eventually have support for both of those platforms.\n. > Our problem with Kubernetes is how slow it is to provision new VMs.\nI'm a little confused about what you mean by this, in my experience with Kubernetes it's always run on VMs (or bare metal) that are provisioned by something external like an AWS scaling group or a GCE instance group. I wasn't aware that it was possible for K8s to provision VMs directly.\nFrom what I understand it's not uncommon for people to run Kubernetes as a framework on Mesos which allows scheduling things with the Kubernetes API without requiring a separate cluster. I realize that running k8s on mesos is kind of lame but would something like that be an option for you guys?\nIt would definitely be good to get Pachyderm running on other platforms, but unfortunately as a small team with very limited resources right now we need to focus on the needs of existing users and don't have a lot of resources to spend on projects that might bring in new users (but also might not). That being said I think we'll likely have more resources available for things like this soon and this particular issue is among the highest priority issues of that type.\n. Hi, really sorry you ran in to this. I think I have an idea what's going on here.\ntl;dr: I think it's a problem with dns.\nPfs uses etcd to announce its services, this is handled by the announce-master-*-*.service services. Those services do etcdctl set /pfs/master/0-1 {HOST_NAME}:port. The router service then uses this to figure out how to contact the master. However this only works if the router can do a dns lookup. How did you setup the CoreOS instances? I remember running in to this problem when I tried to get pfs setup on Vagrant.\nThere are a few easy ways I can think of to fix this for you short term:\n- echo \"coreos-1 10.132.128.22\" >/etc/hosts is a quick hack that should make things start working. This would also be a good way to confirm the DNS theory.\n- GCE and EC2 both have DNS by default, shoot me an email at jd@pachyderm.io and I'll setup a hosted instance for you to play around with.\n\nAction items for pfs:\n- [ ] If DNS is indeed the problem this error message should explicitly mention that as a likely cause.\n- [ ] Docs should do a better job of describing this in the \"getting started\" section.\n- [ ] We should look in to making pfs not depend on DNS. I think this will be doable via flannel or something similar.\n. This feature should feel similar to the way you can go get a library in golang.\nOne question that we should think about is how we're going to do versioning in this system.\n. Yup.\n. Hi!\nGlad you like the idea!! Now let's see if we can get it working for you in practice.\nUsing the hostname is actually expected behavior.\nThe way that pfs does its routing is that each shard has a sidekick service which runs next to it called the announcer. This service publishes the hostname at which the shard can be found as long as the shard is still alive. That's where the hostname is coming from.\nSo that's all as it should be, what's not as it should be is that the server is erroring. The fastest way to get to the bottom of this would be to get the logs. The easiest way to grab those would be fleetctl journal master-0-1 (assuming a 1 shard cluster).\nReally sorry you ran in to this. I'll try to get to the bottom if it ASAP.\n. This is a really good point, thanks for opening this issue. I'll make sure to get it in to the next release (meaning v0.5, v0.4 is technically the next release but the dev cycle for that has basically ended.) Does anyone know if there are really good tools for documenting http apis written in go? Since our server code has such a common structure there might be some nice way to autogenerate the docs and make sure they stay in sync with the actual code.\n. And I would have gotten away with it too if it wasn't for those damned meddling kids.\nI think @JoeyZwicker just fixed this one. Thanks for the heads up @damnMeddlingKid we appreciate it.\n. Definitely agree on both of these points.\nWhat do you think would make it clear in the first case? My instinct would be PFS_HOST, since it makes it clear that we're talking about a hostname and the fact that it's all caps will hopefully indicate that it's not meant literally like the rest of the parts of the path.\nAs for the second one, what if we had a script scripts/launch-dev which builds an image using the contents of the local directory and then runs the launch script with it as the target?\n. The PR above contains code which should address this issue.\n. Hi @neojski I think I know what's going on here, and it's entirely our fault. Reduce is currently broken in single node mode because it requires etcd to be present so it can look up the other nodes. This is sort of silly because there's actually only one node in a single node cluster so it's failing to look up itself.\nI'll get this fixed ASAP.\n\nI think that a simple working demo with map and reduce would be very useful.\n\n100% agree, we'll want to have this in several languages as well so the fact that you're doing this in node is extra awesome.\n\nAlso, I think that you should describe how to prepare docker images. I've seen that you mentioned somewhere that they should be http servers. What about ports? Can I have multiple maps (or map + reduce) in one container?\n\nI'll make sure to get this documented as well. The answers for posterity:\nServers should always listen on port 80.\nYou can have as many maps and reduces in the same containers as you want. The image name command pair is what defines the behavior of a job. In general storing things in the same container makes sense when they share libraries because it decreases the amount of container data to be pushed around.\n. Alrighty, I just tested this in next and it seems to be working. git checkout next && scripts/dev-launch to try it out. Let me know if that works for you.\nAlso I was able to test this using the job you created because you pushed the image to docker hub but the Dockerfile seems to be missing from the GH repo, probably just a forgotten git add. When you have a second would you mind pushing that so others can build the pipeline.\nThanks for your patience with this issue.\n. > There's a Dockerfile in both map and reduce. I'll try to test it later.\nWhoops, I'm blind.\n\nI updated my map-reduce and now it calculated the total length of all files in data directory.\n\n:+1: Awesomeness. Do you mind if we references this as an example pipeline?\nThis issue seems resolved, mind if I close?\n. This is definitely an annoyance that I'd like to have go away. It's a bit tricky though, here's the situation as I understand it, it may well be that there's a better solution that I don't know about.\nThere are 2 components of btrfs that pfs depends on. There's the btrfs kernel module and there's btrfs-tools which is a normal binary (not a kernel module). Unfortunately kernel modules are one of the few things that Docker doesn't fix, the kernel you see inside a container is the same as the one you see on the host machine and the same image can encounter different kernels and thus different behaviors if it's run on different machines. This is problematic because we need btrfs-tools to match with the kernel module which means btrfs-tools can't be part of the image without seriously limiting the places that the image can run.\nWe've struggled with this problem for a while and ultimately arrived at the solution of mapping in the host's filesystem so that we can use the btrfs-tools that are installed there. This is definitely a suboptimal solution since it creates a dependency other than Docker but so far it's the best we have. There might be a better solution to this out there, I'm pretty novice at kernel module management.\n\nThere are a few things we could do to make this better.\n- The requirement of 3.14 is probably way to strict, I suspect we could function fine using an older version.\n- Supporting multiple backends could help with this. In particular I think an overlayfs backend might wind up being a good default in the long run because it seems like everything we need is in the kernel by default and the API is pretty stable (mostly because it's so simple.)\n. Ugh that should be fixable in the short term. That's just bad version checking. Sorry about that, I'll get this fixed.\n. The fix mentioned by @brendanashworth has been merged in to master. Closing.\n. Thanks for the pull!\nI think the easiest way to compare version numbers that won't break on new versions is with sort -V [0]\nAlso would you mind make this a pull in to pachyderm:next. Merging in to master automatically ships the code so as a general rule we merge in to next and then in to master when we want to ship. (This should be in the Readme but it's not.\n[0] http://stackoverflow.com/questions/16989598/bash-comparing-version-numbers#answer-24067243\n. Hi @alexed1 first off git issues is 100% the right stuff for things like this.\nUnfortunately we don't have great support for running pfs on OSX. I'm going to look in to what it would take to get this working nicely with boot2docker. The only difficult part is getting btrfs up and working and it looks like that should be possible according to this.\nFor the time being I think your best bet is going to be running a CoreOS VM using Vagrant. I realize that's not the best install experience, we'll try to have a more native OSX experience soon.\n. Good questions.\n\nIn the HDFS variant, the whole key for performance, if any, was: \"move computation to the da\nta\", which was achieved by maintaining multiple parts of a big file across machines as multiple copies.\nHow is this addressed in pfs - does it also maintain multiple copies of same file as multiple blocks / parts (if yes, how) ?\n\nPfs has a similar concept of bringing the computation to the data. Right now we don't do block storage for large files but we'll be implementing that in the near future. I've opened #58 to track that issue and there's a more in depth description of what the implementation will look like.\n\nIn the MapReduce variant of hadoop, for any given job, same copy of map-reduce method gets run on all machines, which makes it not suitable for data-flow computations, and which is not the case or concept for 'micro-services'. The core concept of micro-services is to build larger complex computation out of small and different disparate components /services. Which gives more control because you can tune / control individual micro-services without affecting the whole network.\n\nI'm not 100% sure I understand what's being asked here so apologies if my answer doesn't address your concerns. Pachyderm organizes jobs in to pipelines, this gives it an understanding of where the data's next destination is which allows us to efficiently stream data between jobs.\n. Implemented.\n. :+1:\nNice one.\n. I like this direction as well. Sorry about the long turn around on this one.\nWhen you get a second would you mind signing our CLA?\n. In general I like golang:1.4.2 as a default. In this case size is really important though we want our main pfs container to be as small as possible.\nIs there a good way to get it small and still use the official golang images? I feel like I remember trying out this pull and seeing a pretty big image. \n. @bitwiseman we do indeed want > 3.14. So I think what you have here is correct.\n@DimShadoWWW would you mind signing our CLA?\n. @DimShadoWWW did you get a chance to look at the CLA\n. Yup, closing.\n. Many thanks for the pull! \nWould you mind signing our CLA?\nI'l try to take a look at this first thing on Monday.\nSend me your tshirt size and mailing address at jd@pachyderm.io for some swag.\n. Yeah turns out CLAHub is now defunct. We'll figure something else out.\n. Sorry for the delay.\nFirst off I really like the direction of this PR. Makes our scripts much more sane. Big :+1: on this.\nSecond, round 2 of our CLA is ready, this time I'm 99% sure that it works :D https://pachyderm.wufoo.com/forms/pachyderm-contributor-license-agreement/\n. Hmm, I'm getting an issue with how the scripts source each other :(\n$ scripts/clean\nscripts/clean: 10: scripts/clean: source: not found\nThis is run from the root of the source tree. Is this a system specific issue?\n. I'm on zsh 5.0.5. Using . makes sense :D\n. I think this is a good idea as well. The way flags are propagated down from script to script is really janky.\n. Hi!\nMany thanks for the pull. I think the approach here is correct. The ground moved a bit underneath you here unfortunately. As of v0.8 all of the container code is in lib/container which is where this code is going to need to go. If you get this working with the newly merged changes in master we should be good to go!\nWould you mind signing our CLA?\n. Just ran tests on this, all seems to be well save for the small fix needed to get things compiling. After that this should be good to merge in.\n. :+1:\n. Just took a swing at setting this up with busy box and it looks like that will be an pretty easy way to improve this a lot. I got an 11.86 MB image vs. 577.2 MB before.\nThis link is helpful:\nhttps://blog.codeship.com/building-minimal-docker-containers-for-go-applications/\n. Code is at for those interested.\nhttps://github.com/pachyderm/pfs/tree/minimal_container\n. :+1:\n. Cool, just ran tests on this and alls well.\nMerging.\n. Cool, I'm on board with all of these standards.\nJust ran this through tests and everything is passing. Merging in to master.\n. Definitely in favor of this.\nMotivation was mostly, \"this seems like the easiest way to get this script to work right now.\" So rethinking is welcome here.\n. Thanks so much for taking the time to write this up, it's going to be really useful for us in putting together noob guides.\nEverything seems to have gone fine with your deploy. The docs for what a healthy deploy looks like were out of date (I just updated them).\n\nIt's unclear which host and port I'm supposed to use so I'm going for whichever CoreOS instance I choose and then port 80.\n\nYup, that's the correct thing to do. We'll have to make sure this is clear in the docs.\nI think the only thing going wrong here is that you're using pfs in the URL after localhost. Try:\nshell\ncurl -XPOST localhost/file/shakespeare -T shakespeare.txt\ncurl -O localhost/file/shakespeare\nOur docs sometimes refer to routes as pfs/file/... and we're using pfs there to indicate \"the host on which pfs is running\" but we've had a few people get confused by it so I think we're going to change that.\n. Glad this worked and please do let us know if you run in to any problems running Wordcount!\n. Problem 1:\n\"pfs://data\" is a confusing way to describe it sorry about that. What we're trying to say is that data is a directory in pfs which contains the text to be processed. There's no need to create directories in pfs, they get created when there's files to put in them. You should get the data in the correct spot with:\ncurl -XPOST localhost/file/data/shakespeare -T shakespeare.txt\nProblem 2:\nWild card listings should return a multipart message containing a part per file that it finds in the filesystem. However they're not doing what they should be doing for you :( they're erroring due to a bug. Sorry about that we'll get it fixed ASAP. I've setup #86 to track progress.\nProblem 3:\nRequests for the value of a pipeline at a specific commit will block until the pipeline gets run on that commit and the value becomes available. However there's a number of ways that this can go wrong such as if the pipeline crashes. Any chance that you could take a look at one of the log files for your servers? It's a bit annoying right now since you have to ssh to the machine and look at local host. They're locate in /var/lib/pfs/log on each of the hosts.\nThis is definitely a fragile part of Pachyderm that we're going to need cleanup. I've create #87 to track it.\nProblem 4:\nJobs are now deprecated in lieu of pipelines. We'll be doing a big purge ASAP.\nProblem 5:\nYou should see some additional docker container(s). This might be silly but did you try docker ps -a so you'd see jobs that have already exited? It's also quite possible that something went wrong and the container was never started in which case logs will probably have the answer.\nAgain thank so so much for putting up with our very raw documentation and telling us how to make it better. :heart:\nShoot me an email at jd@pachyderm.io with your TShirt size and I'll get you some swag.\n. Fixed by #95.\n. The harder part of this has been fixed by #94 which made it so that the request will return when the command errors.\nI'm actually not sure the first bullet point is a good idea anymore. It seems useful to be able to wait on a commit that doesn't exist right now but that might exist in the future. For example you might want to wait for the commit for tonight's run of the pipelines well before the commit has been made.\nI'm going to close this, feel free to reopen if you think we should reevaluate this.\n. Dukes, I just merged this in to next. I'm going to label some more noob friendly issues.\n. This is 50%. s3 is implemented. GCS not yet.\n. Added!\n. Since this one's ready to go now I'm going to merge it in.\nMany thanks! If you drop me a line at jd@pachyderm.io with your tshirt size I'll get you some swag.\n. Oh, didn't realize that :p. I'll get that merged in as well.\n. This is half of #99.\n. Looks like a good direction.\nHow do we feel about a utils package? We already had one so this pr isn't to blame but does\nanyone know where other projects normally put code like this?\nAlso we might want to look at moving the other functions in lib/shard/test_helpers.go somewhere.\nIt might make sense to move them over to a bindings library which would serve as go bindings to our api.\n. This should be ready for review.\nAll tests are green.\n. Already merged. But this looks good :+1:\n. Alright, this pr now does all the basic functionality. I'm going to respond to all the comments here and get it merged in.\n. Alright, I dug into this a bit and this seems to be a bug in grpc. They just changed how ClientConn works internally over the last few days. And inspect the code it seems quite possible for the shutdownChan to get closed twice.\nPeter you already know these guys and wrote some of grpc, can you chat with them about it?\n. Just took a look, looks like it has a concept of directories and we could totally implement our api using it.\nIt's also so bleeding edge that it doesn't have a version number yet, but it's pretty simple and clearly meant to do exactly what we want so we'd probably be fine using it.\n. Hmm, probably still looking to do this long term but in the lead up to to 0.10 I'd prefer not to introduce any big changes that aren't critical.\n. Nope, close.\nOn May 6, 2016 10:32 AM, \"Sean Jezewski\" notifications@github.com wrote:\n\nIs this still relevant @jdoliner https://github.com/jdoliner ?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/133#issuecomment-217508183\n. Actually we need to do this with basically every way that our apis can error.\n. pfs list-commits bar\nrpc error: code = 2 desc = \"btrfs subvolume list -a --sort -ogen /pfs/btrfs/bar: exit status 1\\n\\tERROR: can't access '/pfs/btrfs/bar'\\n\"\n. Given the absence of btrfs in our code base... I doubt this bug still applies. :)\n. It's gone.\n. The output we want for list-commits and commit-info is:\n\nCOMMIT_ID  PARENT  STATUS  TIME_OPENED  TIME_CLOSED  TOTAL_SIZE  DIFF_SIZE\n. We clean all of the paths before we use them now and handle the leading slash explicitly.\n. I actually meant firewall in the other direction, google machines have a firewall up by default that will block pachctl from talking to pachd. I still think closing this is legit though because we've made the error message you get in this case a lot more descriptive.\n. Nope, close this.\nOn May 6, 2016 10:33 AM, \"Sean Jezewski\" notifications@github.com wrote:\n\nIs this still relevant @jdoliner https://github.com/jdoliner ?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/148#issuecomment-217508469\n. We have tests covering permissions and they seem to be correct. Closing.\n. I think removing completely is best.\nIt seems pretty reasonable that if we're returning info about a job that means it's been created.\n. No longer applies.\n. Hi @rrnewton. Really sorry we haven't made this better yet. We're still putting the finishing touches on this release functionality wise and are waiting for a few features in other projects (kubernetes and docker) before everything will work. I'm going to add this issue to our v0.10 milestone to make sure we get this fixed before shipping.\n\nI'm also going to make a pass right now to update things. Check back soon :)\n. Alright our README now links directly to the fruit stand demo which is our go to quickstart guide. I'm going to call this one closed.\n. Cool, that's an acceptable behavior then. Wish we could make that error message better, but I'm not sure if fuse will allow us. Closing.\n. Nope, it's not. Just checked master does indeed have the desired behavior here and a test to check it.\nClosing!\n. Hi @brinman2002,\nReally sorry you ran in to this, as you basically figured out from the error messages ppsd requires a running kubernetes instance to launch jobs. That means that deploying on docker-compose isn't possible anymore, at least you won't get a fully functioning cluster because ppsd won't be able to launch jobs. Pfs should still work fine under docker-compose.\nTo get a working cluster I'd recommend taking a look at:\nhttps://github.com/pachyderm/pachyderm/blob/master/QUICKSTART.md\nwe're just now rolling out this documentation so it's a bit raw but I believe the steps there will get you a working cluster. And if it doesn't please please let us know what went wrong so we can fix it. :)\n\nAction item for this issue:\nRemove docker-compose since it's not a viable way to deploy.\nUpdate make launch to us Kubernetes.\n. Docker compose has been removed for deployment along with vagrant so I think this issue doesn't apply anymore. Closing.\n. So I'm a little confused on the status of this issue. You have Kubernetes up and running but it's the wrong version? What happens when you try to start the correct version?\nAlso could you try this without Vagrant? All you need is Docker and Golang which I think makes Vagrant sort of unneeded. I think we should consider just removing the Vagrant file.\n. @brinman2002 what sorts of issues are you hitting with bare metal? make launch works for me and I think all I did to get the environment working is install Docker and Go, but there's probably some little things that I've forgotten at this point.\n. Hmm, I don't fully understand go get but go get github.com/pachyderm/pachyderm seems to work for me.\n. If you're on master make launch should launch you a kubernetes cluster and then get pachyderm running on that cluster. You will still see some docker-compose output because our containers are still built through docker compose.\nI'm trying to get docker-compose ripped out soon, but our unit tests still use it.\n. Thanks so much for reporting, just updated the quickstart to not reference foo anymore.\nRegarding issue with the hang. I've created #162 to track that.\nThe issue with files being returned from unfinished commits is tracked in #159.\nI think these are both fairly simple issues so I'll try to get them fixed soon.\n. Hi @teodor-pripoae, sorry you ran into this.\nWhat's going on is that pfs needs a loop device to mount the btrfs volume on.\nSometimes there's a weird bug where when the container finishes the loop device can get leaked and there's no way to unmount it.\nWhat do you get when you do losetup -a?\n. Nice, makes sense that this would be fixed too given the new storage backend. Closing.\n. Many thanks for the pull. Merged.\n. Many thanks for the pull, merged.\n. Does this fail immediately or do some of the containers come up?\nlaunch-kube is just running a script which starts 3 docker containers, if none of those containers are coming up then it's probably a problem with Docker.\nAlso can you run other containers, like maybe just docker run -ti ubuntu bash and see if that works.\n. Hmm, so it's either the kubelet or the proxy that's failing. What does docker ps and docker ps -a look like after the failure?\n. Hmm, I think this might be an issue with btrfs storage driver for docker. That's the only thing I can think of that's different. I'm on devicemapper would you mind trying with that?\n. Yeah there can be some weird interactions with storage drivers. Glad it's fixed. Is this safe to close?\n. :+1: \n. Sry for delay on this.\nWound up manually merging this one in because getting the vendor directory to compile as a bit of a pain.\nWe're on govendor now though :) ty for the pull.\n. Hmm it looks like one of the kubernetes containers isn't being created properly. What's the output of docker ps following that error?\nWe've seen problems like this before with different Docker storage drivers. We've had the best results with devicemapper. Would it be a pain to redeploy with devicemapper and try again? Here's a tutorial on how that I think should still work: http://muehe.org/posts/switching-docker-from-aufs-to-devicemapper/.\n. Many thanks for the pull1 Everything looks correct here except I think we should enforce this in a different place. Reason being driver is an interface that may have more than one implementation in the future and we don't want to have to enforce this separately for each.\nInstead could you put the check in pfs.APIServer.CreateRepo?\nAlso would you mind filling out our CLA? https://pachyderm.wufoo.com/forms/pachyderm-contributor-license-agreement/\n. Whoops, just noticed you've already signed the CLA, feel free to ignore that :)\n. Indeed, merging!\n. Err, sorry I clicked the wrong button there.\n. There we go :)\n. Hmm, I haven't seen an error like this. Objd isn't doing anything terrible sophisticated with volumes either, it's just got a single emptyDir volume.\nWhat version of Kubernetes are you using and how are you launching it? Also which storage backend are you using for Docker?\n. Is it possible to bring up other k8s examples or do those fail as well?\nSorry, wish I could be more useful on this but I haven't seen anything like this form k8s :(\n. Looks like you figured this one out for yourself.\nWe should mention this problem somewhere on the README since others will probably run into this as well.\n. Request granted, 1.2 shipped six days ago. Will submit a pr.\n. Good, change. I like this change. Merging.\n. Nice! Thanks so much for this @mattnenterprise will merge.\n\nWhat are the definitions of a pipeline and a job ? How to they relate to each other ?\n\nIf you're familiar with k8s, pipelines are to jobs as replicationcontrollers are to pods.\nJobs are simpler, and are the basic unit of computation in Pachyderm.\nThey take a set of commits as inputs and create a single output commit.\nThe output commit is guaranteed to exist and be finished once the job has completed.\nPipelines are a way to automate jobs. They take a set of repos as inputs. Any time a commit is made to one of those repos Pachyderm will create a job which processes the new commit and ultimately creates a new commit in the pipelines output repo. \n\nHow do I make this truly distributed if I have a lot of files, and want to process them in multiple containers ?\n\nIt's already truly distributed. Your code doesn't need to do anything special to be distributed. Jobs (and Pipelines) have a Shard parameter which defaults to 1. If you crank this up then Pachyderm will create multiple copies of the container and each of them will see a different subset of the files.\n\nIt currently looks like the job-shim binary is required to be in the container. Will this always be required ?\n\nNo, this is incredibly annoying and needs to go away sooner rather than later. Long term I want to create pods with 2 containers, one which mounts the fuse volume and once which is the users container and runs code. Right now there's no way to propagate a fuse mount between containers but that should be changing soon (it might have already).\nSo tl;dr we're waiting on mount propagation in Docker before we can get rid of this.\n. Created a pull removing us from docs, we no longer have a volume driver.\n. Yeah it wound up not making sense for us to have a docker volume. Pfs isn't usable as general purpose storage so it wouldn't really fill the same use case as other storage drivers. Furthermore it's just a fuse plugin which means I think there should be a generic volume driver we can use.\n. Looks good to merge.\n@teodor-pripoae would you mind signing our CLA? https://pachyderm.wufoo.com/forms/pachyderm-contributor-license-agreement/\n. Many thanks :D. Merging.\n. And yup, still applies. Didn't realize you contributed before.\n. Oops, that's an error that's not being propagated correctly.\nPipelines expect their input repos to exist before they're created so just do pachctl create-repo data before you create the pipeline.\nI'll get a fix for the error not propagating merged soon.\n. First, thanks so much for taking the time to write this up for us :D. It's a huge help.\nSend me an email at jdoliner@pachyderm.io with your address and tshirt size and I'd love to send you some swag.\n\nI'm working right now to setup a vagrant file with everything needed for working with pfs. Is this something I can contribute ?\n\nIdeally I'd like to keep developing using docker-machine, I've found that flow to work really well. That being said it looks like there are some annoyances that need to be dealt with here.\n\nI can't mount by default to /pfs without beeing root\n\nNot super attached to this default, I think it's good for inside containers since it saves you some typing but on user machines it's cumbersome. What would be a good default for this?\n\nIf running on osx I need to install osx-fuse from homebrew which looks buggy.\n\nhttps://osxfuse.github.io/ is the best bet for this, adding it to the README.\n\nPort forwarding from docker-machine does not work well: In the quickstart it is recommented to port forward host port 650 to port 30650 running inside the vm. I can't bind to this port using ssh without beeing root. If I run it as root, ssh will not work since I don't have the public/private key set up.\nAfter binding on another port and setting PFS_ADDRESS and PPS_ADDRESS to localhost:30650 and 30651, ssh port forward breaks gRPC connection becouse it is tcp over tcp. I had this problem while port forwarding other gRPC services too, for example youtube vitess or my own services.\nif I set PFS_ADDRESS=192.168.99.101:30650 and export PPS_ADDRESS=192.168.99.101:30651 in my zshrc it works and I can succesfully mount the fuse filesystem, but it does not correctly get replicated. I lost my energy at this point :disappointed:\n\nUgh, this sounds really annoying. Really sorry you ran into this. One easy thing we can do here is make it so that mount looks for pfs on port 30650 by default, then you can bind w/o sudo.\nI'm not sure I totally understand the TCP over TCP problem what goes wrong with that?\nFor the replication problem are you seeing an issue where you create a repo and the results of ls /pfs don't change? If so that's an issue with the OS caching results. On OSX there's a flag you can pass that prevents it but last I checked it was obfuscated by the library. We're working with the maintainer to get it punched through so we can use it which should fix the problem.\n\nIn general I'm in favor of having a number of having different ways to deploy, but I have a pretty strong preference that our recommended deploy be possible with just Docker, no VMs needed.\n\nRegarding the panic, pb definitions are still changing kind of fast, so it's possible just rebuilding will fix it. If it happens again definitely grab the back trace for me. I can't find a way to reproduce right now.\nAnd regarding delete-pipelines that one's still unimplemented :(\n. Good addition. I'm going to merge this and expand on it a bit.\n. lgtm\n. lgtm\n. Really like both of these ideas.\nWhat do projects normally call this page? Maybe glossary.md\n. Number 2 is done. We now have autogenerated docs at docs/pachctl\n. We were using godeps before but had some troubles with it and have moved on to govendor which is supposed to be the new way to do this.\nI'm not strongly attached to either but given we just had trouble with godeps I think we'll be sticking with govendor until it gives us a reason not to.\n. Sorry for the delay on this guys. I'm working on this one right now.\n. Alright, our README now has a section title \"Our Vision.\" Closing for now, feel free to reopen if you think that section should be expanded.\n. Protolion is breaking for us, seems like there's some new assumptions about what types of data are logged.\n```\npanic: json: unsupported type: map[uint64]bool\ngithub.com/pachyderm/pachyderm/vendor/go.pedge.io/lion.(errorHandler).Handle(0x1266e00, 0x7f3006701050, 0xc8203c65a0)\n        /go/src/github.com/pachyderm/pachyderm/vendor/go.pedge.io/lion/defaults.go:37 +0x78\ngithub.com/pachyderm/pachyderm/vendor/go.pedge.io/lion.(logger).print(0xc82018a070, 0x2, 0xc8203d8300, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /go/src/github.com/pachyderm/pachyderm/vendor/go.pedge.io/lion/logger.go:221 +0xc9\ngithub.com/pachyderm/pachyderm/vendor/go.pedge.io/lion.(logger).LogEntryMessage(0xc82018a070, 0xc800000002, 0xc8203d8300)\n        /go/src/github.com/pachyderm/pachyderm/vendor/go.pedge.io/lion/logger.go:216 +0x50\ngithub.com/pachyderm/pachyderm/vendor/go.pedge.io/lion/proto.(logger).Info(0xc820115650, 0x7f3015880550, 0xc8203d82e0)\n        /go/src/github.com/pachyderm/pachyderm/vendor/go.pedge.io/lion/proto/logger.go:38 +0xea\ngithub.com/pachyderm/pachyderm/vendor/go.pedge.io/lion/proto.Info(0x7f3015880550, 0xc8203d82e0)\n        /go/src/github.com/pachyderm/pachyderm/vendor/go.pedge.io/lion/proto/protolion.go:201 +0x42\ngithub.com/pachyderm/pachyderm/src/pkg/shard.(sharder).fillRoles.func1(0xc82029e0c0, 0x0, 0x0)\n        /go/src/github.com/pachyderm/pachyderm/src/pkg/shard/sharder.go:915 +0x103b\ngithub.com/pachyderm/pachyderm/src/pkg/discovery.(etcdClient).watchAllWithoutRetry(0xc8200b6290, 0xc82029e060, 0x2e, 0xc8200b0ba0, 0xc82029e090, 0x0, 0x0)\n        /go/src/github.com/pachyderm/pachyderm/src/pkg/discovery/etcd_client.go:217 +0x3e6\ngithub.com/pachyderm/pachyderm/src/pkg/discovery.(etcdClient).WatchAll(0xc8200b6290, 0xc82029e060, 0x2e, 0xc8200b0ba0, 0xc82029e090, 0x0, 0x0)\n        /go/src/github.com/pachyderm/pachyderm/src/pkg/discovery/etcd_client.go:61 +0x5c\ngithub.com/pachyderm/pachyderm/src/pkg/shard.(sharder).fillRoles(0xc820119c20, 0xc8201e0fc0, 0xe, 0x7f3015886e18, 0xc8200b09c0, 0xc8200b0b40, 0xc8200b0ba0, 0x0, 0x0)\n        /go/src/github.com/pachyderm/pachyderm/src/pkg/shard/sharder.go:952 +0x18c\ngithub.com/pachyderm/pachyderm/src/pkg/shard.(sharder).Register.func3(0xc8201e1200, 0xc820119c20, 0xc8201e0fc0, 0xe, 0x7f3015886e18, 0xc8200b09c0, 0xc8200b0b40, 0xc8200b0ba0, 0xc8201e11f0, 0xc8201e1140)\n        /go/src/github.com/pachyderm/pachyderm/src/pkg/shard/sharder.go:125 +0xa7\ncreated by github.com/pachyderm/pachyderm/src/pkg/shard.(sharder).Register\n        /go/src/github.com/pachyderm/pachyderm/src/pkg/shard/sharder.go:131 +0x2bf\n```\n. Cool, ty. And ty for da pull.\n. Hmm, just tried this. I'm still seeing the same error :(\n. You should get it if you do:\nmake kube-launch\nmake launch\n. For me pfsd and roler both fail to come up due to the error.\n. Yay docker with aufs can't start containers if you add files to them, it's completely broken. You need devicemapper.\n. Can confirm, fixed. Just merged it.\n. lgtm\n. lgtm\n. :+1:\nJust ran these tests locally, all is kosher.\nMerging.\n. lgtm\n. Ugh, this looks annoying.\nSo first off we're kind of in a fractured state with tests right now.\nmake test runs the old integration suite which is built on docker-compose. We ditched that for kubernetes and added some integration tests that run within k8s as a pod.\nI'm thinking we should tear out the old tests along with docker-compose and integrate them into the k8s pod since most of the tests require cluster state to run. Also we should have make test run the k8s tests that work instead of the broken ones that don't.\nReally sorry for the confusion on this.\n. The tl;dr is that I'm not quite sure what's breaking the test compilations, but we're probably better off just sidestepping it by removing the tests.\n. Alright I finally figured out what was wrong here. I was having trouble reproducing this due to some caching issues. Finally reproduced this and got it fixed.\nIt only required a little bit of hackery to get a functional vendor directory.\nIt still feels like we're in a fragile place with our vendoring. This is probably due to the number of experimental projects we depend on.\n. Feel free to reopen if this is still broken in anyway. But I'm hoping it's fixed.\n. This addresses, #184 among other things.\n. This has already been merged manually. Closing.\n. Pachyderm is now 100% btrfs free!\nPachyderm's containers are now stateless and we use object storage (s3, GCS.. etc) for data and a database (RethinkDB) for metadata.\nOpenstorage doesn't really make sense for the data since object storage is normally outside the container but it probably will for the database which we also deploy as a container.\n. Ugh, that would definitely make things hard.\nShould be fixed now:\nhttps://hub.docker.com/r/pachyderm/pachd/\n. Hmm, everything here looks like it should work except the last part.\nIt looks like you're starting up bash in a new container with the image mypachyderm, that container won't have be able to talk to pachd because the port forwarding is on the local machine not in the container.\n2 options:\n- You can just make install and pachctl mount on your Mac, no need for Docker.\n- If you want to run a container that has access to the pachd API I'd recommend running it under k8s, pachctl should work automatically due to k8s injecting the correct env variables to find the pachd service.\n. Ahh, make install is just doing a go install under the hood so you might need to put go/bin on your PATH env variable.\n. Sorry, by go/bin I actually meant $GOPATH/bin what do you get if you do ls $GOPATH/bin/pachctl?\n. Closing feel free to reopen if there are more issues.\n. lgtm\n. Hi @cxxly,\nBenchmarks that allow people to easily compare Pachyderm's performance to hadoop/spark are definitely something we would like to make readily available. However doing benchmarks correctly such that they actually help users compare the products in a meaningful way is quite an undertaking so we can't quite commit to a timeline right now. This thread would be a good place to collect our thoughts about what these benchmarks should look like.\nIt seems like the obvious starting point is terasort. I'd be interested to see how Pachyderm did both in the total throughput section and the cost/gb section.\nOne thing I think terasort misses is latency which I think should be a part of our benchmarks. The fixed cost of jobs can be just as detrimental to user experience, even more so, because it often slows down development.\n. Overview:\nThe big change is the addition of src/pkg/dag which implements some computations on directed acyclic graphs. This package is used by src/pfs/driver to implement pfs's version control semantics. Over time I expect src/pkg/dag to get more featureful and pfs will be able to lean on it harder. We'll also use it in pps to implement pipelines.\nThis pr also adds src/pfs/server/server_test.go which implements local pfs server tests. Being able to test the pfs layer locally should help us a lot. Their coverage isn't great right now but you gotta start somewhere.\n. This looks to be a dupe of #201. Closing in favor of that.\n. Believe this one is fixed. Reopen if that's not true :)\n. One small clarification on this, it's actually possible to reproduce w/o a job/pipeline, all you need is to create repos manually:\nshell\n$ pachctl create-repo foo\n$ sudo pachctl mount &\n$ ls /pfs\nfoo\n$ pachctl create-repo bar\n$ ls /pfs\nbar\n. That's correct.\n. Not sure quite what this is referencing. If pachd dies it gets rescheduled and brought back up.\nThere are definitely some bugs in there but I think we have issues open for a lot of them and issues to improve CI to expose more as well.\nClosing this one since it seems like it's subsumed by those other issues.\n. This issue has been solved using protofix. Closing!\n. @derekchiang this can be closed correct?\n. PFS is designed to use pluggable object stores for all the reasons you list here :)\nWe abstract object storage behind this interface Client in src/pkg/obj/obj.go The only things needed to add support for a new object store are:\n- implement obj.Client for the object store\n- add a way to pass the necessary credentials to pachd as a k8s secret\nRight now we have clients for s3 and GCS. And secrets are implemented for s3.\nWe're planning to expand this a lot post 1.0.\nAdding a new obj backend would be a great project for people looking to get started with Pachyderm dev so I've marked this issue as \"noob friendly.\"\n. It doesn't and I'm not sure if that would be possible. AFAIK most object stores don't expose locality (I don't think s3 or GCS do) so this API couldn't either.\n. @JonathanFraser thanks so much for the leg work.\nMy thoughts on this are that the ideal solution is to use radosgw since then we can reuse the code from our s3 implementation. That being said it's a relatively small amount of code the make Ceph work, it seems like the most painful part will be the build system.\nWhen you say that it'll be a while before we see the changes in a stable version what does a while mean? I have no notion of what release cycles on ceph look like.\n. @JonathanFraser these are good points, I just submitted pr which fleshes out the docs in obj.go a bit more. To answer you're questions directly, if an object already exists then you should return an error. PFS will handle the error depending on what type of object it was writing to obj store. The name parameter is indeed, I renamed it to prefix in the pr. If the backend doesn't have support for prefixes then we may need to figure something out, does ceph not support that? We only need it on a directory level (delimited by /s if that makes things easier).\n. Update on this, we now have support for both s3 and GCS!\nI'm going to move this out of the 1.0 milestone, we'll add more object stores after we get it shipped.\n. Alright everything here looks good to me. @sjezewski would you like to hit the big green button or shall I?\n. All looks good to me :)\n. lgtm\n. Thanks so much for the pull. Even for a little thing like this I still gotta ask for a CLA :)\n. https://pachyderm.wufoo.com/forms/pachyderm-contributor-license-agreement/\n. As soon as that's signed this is good to merge.\n. CLA signed! Merging. Many thanks @erikreppel.\n. Good catch! I changed it so it now looks for Address as its usage string promises.\n. Alright, easiest way to solve this then would be to not import kube packages for pachctl. The only reason we're doing it is so that we can use their API objects to create pachyderm manifests... but we could probably do that with a template or something.\n. This should be pretty trivial to do post pfs refactor.\n. So we currently do 2, and I think 3 is unlikely to be something we want to do ourselves. It's unclear that 1 is really something we'd \"do\". It might just be as simple as: turn on cloud front and see what happens. So I'm going to close this. . Cool, all looks good to me :+1: \n. @JonathanFraser thanks for the heads up. I'm doing merges all day, this is next up so hopefully will have this working and in master by eod today.\n. I've hit a strange issue on this. make test passes locally... somehow when it runs on travis I get a database error saying that the database \"pachyderm\" hasn't been initialized. Does anyone conceive of a hypothesis to test here?\n@sjezewski\n. I saw a similar thing and I think I figured it out. Between 1.1 and 1.2\nthey seem to have made it so the kubelet starts its own proxy as a pod.\nPreviously the proxy was created by our script. This meant that we were\nstarting 2 proxies which led to problems.\nOn Mar 26, 2016 10:15 AM, \"Jonathan Fraser\" notifications@github.com\nwrote:\n\nI was having some issues with this locally myself, it seemed like pachd\ncould not contact the rethinkdb database. I kept trying to do that on a\nsubnet that didn't seem to exist anywhere in the container.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/pachyderm/pachyderm/pull/229#issuecomment-201894904\n. Alright, CI is green on this one so as soon takes a look at this and gives me a +1 I'll merge.\nQuick summary of changes since this ballooned a little.\n- Bumps us to version 1.2 of Kubernetes\n- This led to a few refactors with the k8s startup scripts, several files got deleted\n- K8s changed the default for image pulling so I had to change our manifest slightly\n- the test container went away, we now just run tests using go test, much simpler\n. It would be awesome to have this for 1.0, but I don't think we're going to want to hold the release for it. I'm going to put this under the milestone for now in case we do have time but it might get removed later.\n. @JonathanFraser we hear you on this one, this is going to be among the first things we tackle once 1.0 is shipped later this week, moving this into 1.1 to reflect that.\n\nLooking forward to that I'd love to understand your use case a little bit more and start talking about a potential design for this one. It seems to me that binary files are slightly different from what we're doing with new lines because there's nothing in the actual data that indicates where it's safe to split, instead we need to take that information from the users when they give us the data. I also think this mode should be the default because it's the least magical of the behaviors.\nHere's what the behavior would look like in terms of our proto API:\n``` proto\nenum Chunking {\n  CHUNKING_NONE = 0;\n  CHUNKING_LINE = 1;\n  CHUNKING_JSON = 2;\n}\nmessage PutFileRequest {\n  File file = 1;\n  FileType file_type = 2;\n  bytes value = 3;\n  string handle = 4;\n  Chunking chunking = 5;\n}\n```\nCHUNKING_NONE would be the behavior you'd use for binaries, with this behavior the PutFileRequest would never be broken across multiple blocks. This would give the user complete control over where we chunk.\nCHUNKING_LINE be our current behavior.\nCHUNKING_JSON would chunk based on matching {s.\n. I was thinking that in CHUNKING_NONE we'd do exactly as you describe and let the only limit on chunk size be object store, we'd just propagate whatever error the object store gives back to the user.\nThis does have the downside that now behavior would change with different object stores. We could actually take it a step farther and add a way to spread a single chunk across multiple objects. Similar to what we do with blocks but we'd hash all of these objects as a unit so they'd all wind up in the same container together, that would effectively remove any size limitations.\nI could see a couple of ways for fuse layers to work. Doing it per mount does make some sense because that basically turns into a per job granularity for delimiter behavior which could work. I could also see making the delimiter property of the repo as a whole since repos are supposed to contain similar data. Finally we could also do it based on file extensions. For example file with a .json suffix would automatically use json chunking.\n. Scheduling this for v1.0, it should be pretty easy and will be nice to get in for API consistency.\n. Sooo this is passing CI... I think it's merge\n. This looks almost completely correct to me.\nA few small points: \nGetFile doesn't return a FileInfo, I suspect that's just a typo though.\nFor InspectFile we don't necessarily need to multicast. FileInfo has space for children but we could in theory remove that and make people do the more expensive ListFile to get them.\nThe only other snag seems like the behavior of DeleteFile on directories, I think your approach is probably right, let's chat a bit more offline.\n. We could actually go even farther and completely remove grpc-gateway. We don't really use it, there's one method exposed on pfs as an example but that's it.\n. This looks good to me. Merge at will.\n. Looks good. Merge at will!\n. Well, resolve merge conflicts first I suppose.\n. You could just do InspectJob it'll have the shards which is a good enough check in my opinion.\n. Ahh, I see. You should be able to construct a k8s client using the New method here:\nk8s.io/kubernetes/pkg/client/unversioned/helper.go.\nYou'll have to set a few flags to get it to work insecurely.\nI don't think it's necessary for merging this PR though.\nThis looks mergable as is.\n. Hi @lavvy, Pachyderm uses rethink internally, but there's no connector for it yet. As of now this feature hasn't been scheduled for a release, thank you for +1ing it helps us know to prioritize this feature. One of the reasons we haven't scheduled this feature yet is that we don't feel like we fully understand the use cases for it. If you wouldn't mind giving us a few more details about what you'd like to do with this feature it'd be super helpful for designing it. In particular:\n- what database are you interested in connecting to?\n- what would you expect this data to look like in pachyderm's filesystem?\n- what are you looking to do with the data once it's in there?\n. That videos a bit out of data at this point, however the workflow that you're describing here should still be possible today with a little bit of leg work and in the future we'll be making it much easier. Right now the simplest way to get a working database connector is with a 0-input pipeline. 0-input pipelines don't get triggered by commits to input repos but by calls to run-pipeline. To make a connector all you've got to do is create such a pipeline that in its job logic dumps data out of your database and writes it to /pfs/out. This should get you consistent snapshots of your database dumps. You can then write downstream pipelines which process the scraped data as it appears reading each row as a file. You can also use a database to process it by reading the snapshotted data into the db.\nLong term we want this to require as little work as possible so we'll be adding a builtin way to do this easily for common databases and a way to extend it to different databases. I'm going to try to tentatively schedule this for 1.3 which is our next release cycle. I think we've got the groundwork we need and this is an feature that's important to enough people that it's worth doing.\n. Looks good to me.\n. Minor nit, I prefer the name stream to streaming because it feels like giving an instruction rather than describing something.\n. A few clarifications I'd like:\nWhat if there are multiple input repos, can you specify a commit to catchup on each of them?\nWhat if the commit came before the current commit, do we allow users to rewind with this command?\nAlso how do people feel about fast-forward git uses that term for a similar concept.\n. Few clarifications on this, I don't think we ever want to completely remove create-job from the API, what we do want to do is obviate it's use for standard operations making it a poweruser feature.\n@JoeyZwicker in your mind under this scenario what's the pipeline workflow that's the equivalent of what create-job does now? create-pipeline followed by run-pipeline? I was expecting it to just be create-pipeline which automatically runs the job once but not more (see #246). Your other justification for run-pipeline makes sense. But I think it's orthogonal so should be separate issue.\nTransforms are saved inside of persist.JobInfo and persist.PipelineInfo which get serialized to RethinkDB\n. I'm just going to go ahead and merge this since it only touches example code.\n. Versioning the code is definitely going to be the key here.\nI think a pretty easy way to get the behavior we want would be the following:\nWhen a pipeline is first created we resolve the image name to a hash. We do this only once on the node the receives the CreatePipeline request so there's no chance of a race. This means that latest tags work fine, because they'll still get resolved to a hash when the request is received.\nUpdating the pipeline will reresolve the image hash if an image is present in the update request. If an image changes there is no incrementality.\n. > I mean really, what you guys have built here is a distributed build system for data\nThat's actually a really good way of describing Pachyderm... I'm stealing that :stuck_out_tongue: \n. @JoeyZwicker how are you imagining that commits get started here and by whom?\n. > > by whom?\n\nI'm not sure what this means\n\nBasically are users explicitly calling StartCommit in your mind, my guess is no we want StartCommit to be automatic like FinishCommit. But there's a wrinkle with that, if they don't call StartCommit then how do they get the name of the commit they should write to? Branches can solve this, and now that I think about it a bit more, this feature might make more sense as a property of branches rather than a property of repos.\n. :beer:\n. Yeah, nothing in here is fixing a bleeding from the neck wound and I don't have the cycles to get it compiling.\nClosing.\n. The problem here is that we do want to finish the commit so that users can explore what data was written, but we want to do so in such a way that it doesn't trigger pipelines that are watching the repo. How would people feel about calling this \"cancelling the commit\" Cancelling a commit would be done using a FinishCommit request, but we'd add a boolean flag, cancel. Cancelled commits wouldn't be returned in ListCommit by default so they wouldn't cause pipelines to trigger. We could also add a flag to ListCommit all which would include cancelled commits. I also think we should prohibit using cancelled commits as parents of new commits.\n. This should probably be implemented a little bit deeper than Pachctl.\nIt seems like we're going to need to make a few requests to kubernetes PodInterface\nFirst the list the pods which are part of the job and then a series of calls to Logs to get logs from them.\nhttps://godoc.org/k8s.io/kubernetes/pkg/client/unversioned#PodInterface\n. You can now see logs from any job (failed, running or succesful) using pachctl get-logs.\n. I think so, this actually breaks a number of very common jobs... for example it breaks the fruit stand.\nSo would be good to get this in.\n. Yup here's how I'd do it. First we should extend InspectCommit to take a FilterShard parameter which it'll pass to each of the sub calls to inspectFile that it does.\nThen in pps.APIServer we can do the following\ngo\nShard:\n    for shards := request.Shards; shards > 0; shards-- {\n        for i := 0; i < shards; i++ {\n            if InspectCommit(commit, FilterShard{Number:i, Modulus: shards}).Size == 0 {\n                continue Shard\n            }\n        }\n}\n. I suspect we're going to see a lot of interesting uses of empty commits. Over time we'll migrate the common use cases into full fledged features. It seems like we already have several of them, in summary:\n- \"kicking\" a pipeline to manually get it going (#248)\n- cron like behavior (#235)\n- provenance tracking (#335)\n. As we come up with more people should feel free to turn them into full fledged issues. Until we feel confident that empty commits truly aren't useful anymore we'll allow them. But long term it's probably going to make sense to try to push to disallow them since it'll issues like this a little more obvious.\n\nFor the time being I think we should offer the following semantics:\n- If a Job has no actual data in any of its inputs we run it with the number of shards requested.\n- If a Job does have data we guarantee that each container will see data from at least 1 of its inputs\nDo those semantics seem reasonable to people?\n. Letting users be explicit about what they want is something I'm generally in favor of because it prevents them from inadvertently doing something they don't want to do. However there is a downside, by making people be explicit you make it a little bit harder for them to do what they want to do. Specifically with a flag for empty inputs I think we'd have 2 impacts:\n- we'd protect users from a case where they inadvertently create an empty commit which triggers a pipeline that can't handle empty inputs\n- we'd confuse users in the case where they have a pipeline that can handle empty inputs, they want to trigger it with an empty commit, but they haven't learned about the flag yet so for some reason unknown to them their pipeline won't trigger\nOf those 2 the latter worries me a lot more. Because in reality we're not protecting people from anything that bad. If they have a pipeline that can't handle empty commits and they make an empty commit then it'll error which will hopefully make it clear to the user what was going on. But the latter case seems like it could really leave someone confused and frustrated for a while.\nAn alternative idea:\nIt seems like the real worry with the first case is that commits might unexpectedly be empty. Imagine a service thats being repaired 1 day and didn't emit any logs. If we're worried about that case maybe we should make users be explicit when they commit that they know the commit is empty. Ie for an empty commit you'd get the following user experience:\n$ pachctl finish-commit foo bar\nError: commit foo/bar is empty (add --allow-empty to commit)\n$ pachctl finish-commit foo bar --allow-empty\n. Many thanks for the pull. I'd love to get this merged but it looks like travis doesn't quite support that version of docker yet which breaks or CI. What a pain.\nI think we could get by doing just to go upgrade for now, the newer versions of Docker don't add anything that terribly important to us.\n@sjezewski what would expect the timeline for travis updating their docker version to be? Maybe we could reach out to them again.\n. @JonathanFraser nice, all looks to be passing. We should also change our README to say docker 1.8 since that's all we require.\n. Alright, all looks good team merge.  Merge at will!\n. Logic looks correct. The only corner case I can think of is if the caller sets Pipeline and ParentJob but for some reason ParentJob wasn't part of the same pipeline so the commits were made to a different repo. I think we should error in that case. I actually don't think the code before was handling that error case either.\n. Looks good, it was basically good to go except for the parentJobInfo thing and that's fixed now.\nMerge at will.\n. @derekchiang would you mind taking a look at this for me?\n. @derekchiang anything else you'd like changed here or is this good to merge?\n. Hmm, I think you can get this so it's just one proto target that's called in both places. Rather than a docker-proto-run target.\n. @JonathanFraser I think your diagnosis is correct. Will investigate and report back.\n. I suspect what's going on here is that there's something about your access pattern that we haven't implemented correctly. Really sorry you hit this we'll get on it asap. Would it be possible for you to get us the pipeline that triggered it and the container images?\n. Could you do docker export f7b5c3e1894f -o image.tar.gz and send the file it spits out?\nThanks :)\n. :D you da man\n. Err, hopefully last thing I need: anyway I could get the dataset?\n. @JonathanFraser awesome, thanks so much for distilling this down to a test case for us :) we'll get this solved for you asap\n. @JonathanFraser interesting, your diagnosis is right we do error when we hit long lines.\nThe fact that it isn't getting returned by the Job is definitely a problem. I'll look into that and see what I can find.\nI also got your sample code and started using it to push data in. I'm working on profiling and optimizing that as well but I'm going to switch gears now to try to address this bug.\n. No reason in particular. Scanner seemed a bit better suited to our needs, and the docs for readline actually recommend that it's too low level for most callers and lists scanner as one of the alternatives so I figured we should heed that.\nI've added a test to our test suite which creates a job that outputs a line that's too long. I'm seeing the job getting stalled, stuck in the RUNNING state. Does this matchup with what you're seeing? pachctl list-job should tell you.\n. Progress!\nGrpc behaves differently than I thought wrt streaming calls.\nIt turns out that the receiver of a streaming rpc must always call Recv from it until it gets EOF otherwise the sending end can block.\nThe second part of this issue is to make the job fail correctly.\nIt's currently reporting SUCCESS.\n. Whoops, didn't mean to close this.\n. Few updates on this, we have the start of a fix in PR: #275.\n. Updates: #275 has fixes for the initial problem in this thread and a few other fuse issues that arose. We're working on getting CI passing on it, then we'll merge it in!\nThanks for bearing with us on this one, it led to us discovering that we were using FUSE incorrectly in many ways.\n. Alright we're finally ready to close this one. The fix has passed our CR and been merged in.\nSorry this took so long, it wound up revealing several issues in our fuse layer that had to be dealt with.\nAs always please feel free to reopen if you see the problem again.\n. @derekchiang whenever you have a sec mind giving this a look for me?\n. Alright, GCS isn't 100% working yet, but I'm pretty sure all of the fixes are going to be changing permissions stuff with the GCE console. @derekchiang whenever you have a sec take a look at this for me\n. Alright pretty sure I addressed everything here\n. Just the one comment about needing a separate goro. Other than that this looks good to me.\n. Needed to add a flag to have gcs access within the cluster.\nGCS is mostly working here, still a few integration tests aren't passing but it's clearly writing to buckets. So I think it must be something other than the storage that's going wrong.\n. Status on this:\nWe've turned on a number of tests for fuse and rethink in travis. Travis currently seems to not want to give us fuse. Also the rethink tests can't find the database. The original pipeline stalling issue should be resolved. The job is still not reporting the error correctly though. I wrote a test for the fuse logic that should propagate the error and it passed. So that points to a problem in the job logic somewhere, or the propagation of the error within pps. Job shim is probably the place to check next.\n. I'd +2 this if I could.\n. Part 1 is complete, #619.\n. @sjezewski you're a prince among men.\n. Just whipped up a pr on this. @derekchiang whenever you have a sec take a look at it, it's only 2 real lines.\n. Yeah most likely cause of this is that the DeletePipeline request caused the server to crash, which broke the connection.\n. Hi @jbowles, thanks for reporting this.\nI think what's going on here is that make install expects pachyderm to have been checked out on $GOPATH, otherwise it complains. I think you ultimately did manage to get everything installed correctly using go get but there's a few things we can do to make this smoother.\n- detect if pachyderm wasn't checked out on $GOPATH, we could have make install do ls $GOPATH/src/github.com/pachyderm/pachyderm this would abort the build if pachyderm isn't where it needs to be\n- we should fix all of these errors that occur on go get\n. Alright, now the hypothesis is that by clicking the big green button I will also close #280 \n. Moving this out of 1.0. We'll reevaluate this when we've seen the snags people hit on deploy and have a better idea of how to make it easier for them. Until then we're just going to focus on making the documentation for deployment better.\n. Let's just remove this from pachctl for 1.0 and implement it later. It's going to take a bit of design process figure out the semantics of delete-commit\n. The lack of atomicity is pretty unavoidable in these operations since the delete is a distributed operation. Thus we'll never be able to completely eliminate the possibility that the cluster is left in an incoherent state so what we need to do instead is try to return sensible results from this state. There are going to be a lot of corner cases there that we need to think through though.\nSo tl;dr this definitely does worry me, I think we should probably get other stuff right first and then start writing some tests that simulate distributed failures to test / fix these behaviors.\n. LGTM, feel free to merge as soon as CI passes.\n. Bleh, my bad I introduced this. There's the same bug in InternalAPIServer.PutFile would you mind fixing that one too?\n. Other than that this is LGTM.\n. LGTM\n. LGTM\n. @JonathanFraser sorry you ran into this, that is some strange output. I'm going to take a crack at reporting this locally and see if I can figure out what's going on. Will report back.\n. What happens if you do pachctl get-file test a01f9e808b41461c9516fb29637a3f49 a? I'm wondering if maybe the fuse layer is doing something weird.\n. Alright I got this working locally, I think what's going on here is that the semantics of the > operator in shell don't quite play nice with our fuse layer. The command echo 3 > ... sends the PutFile request to pachd with the data 3\\n2\\n... very strange.\n@tv42 any ideas why we'd see this behavior? My guess is that we might need to make our fuse layer open files in a non seekable mode but that's just a guess.\n. For the time being, I think the easiest way to sidestep this problem would be to just use pachctl for loading data.\n. For containers using pachctl isn't a great option, we're going to try to get these fuse bugs worked out asap.\n\nI'm also getting this error is a decrease the pachd replicas down to 1.\n\nLittle confused by what this means, would you mind clarifying?\n@tv42 Yup seekability doesn't come into this, my mistake. I think it's likely we're not handling the truncate properly. I've got a test that exposes this error and I'm going to turn on debugging to see if it shows what we want it to.\n. Alright here's the output:\nout.txt\nThis is from a test that does:\nStartCommit\necho 1 >\necho 2 >\nFinishCommit\nStartCommit\necho 3 >\nthe weird part is the second write has the data 3\\n2\\n rather than the expected 3\\n.\nThis test is available in the issue_270 branch (there's enough overlap between this issue and 270 that I'm working on both in the same place). It's called Test296.\n. @tv42 I'm kind of confused I think you're saying that the kernel is storing the contents of the file and thinks that the echo 3 >... should result in the file having the contents 3\\n2\\n but that seems weird because isn't the expect after that command that the file will just contain 3\\n.\nIs there anything we can do to get it so that the kernel sends us that write 3\\n instead of what we're getting now?\n. What's the correct way to set fuse.OpenDirectIO I'm noticing that in my Open callback the response object is nil I feel like I must be missing something kind of obvious here.\n. Whoops, I figured it out I was doing something pretty silly.\nI just tried setting fuse.OpenDirectIO and that seems to fix the problem.\n. @JonathanFraser yup!! just ran it\n. Update on this. We have a PR fixing this in review.\nWe've also added a regression test for this issue: Test296 \n. Fix for this has been merged into master.\nThanks for bearing with us while we got it dealt with and don't hesitate to reopen if there are other unexpected quirks in the behavior.\n. 1 very minor comment about an error, other than that everything LGTM\n. What's an example of an unrecoverable error for which we should stop the pipeline completely?\nIt might well be that what we want to do here is just log errors that runPipeline gets but always restart it. I haven't thought about it too much though so that might not actually be the right idea.\n. That tarball could also be just the .proto files might make things a little easier.\n. I think we'll need to expose this through fuse, probably the way to do it would be to catch truncations and propagate those as delete-file that way should be able to use standard shell techniques:\necho foo > /pfs/out/a # overwrite a\necho foo >> /pfs/out/a # append to a\n. 1 more question is this a behavior you're seeing in reduce steps or map steps? Looking at our code it seems like reduce steps always output to parentless commits so this could be indicative of a bug somewhere else. That won't be true forever though so this issue does still apply to reduce steps and it certainly applies to map steps.\nMaking the commit a no-op if it's the same content should be pretty easy as well thanks to everything being content addressed.\n. Reductivness affects both. We check if any of the inputs are being reduced and if they are we remove the parent commit:\nhttps://github.com/pachyderm/pachyderm/blob/master/src/server/pps/server/api_server.go#L131\nNow this is probably not what we want to do long term since it removes an option of incrementality.\n. @JonathanFraser that makes sense to me too. We should implement that behavior as part of fixing this bug.\n. @JonathanFraser yeah sorry... the threads are all starting to blend together. This should definitely be kept separate.\n. Minor clarification on this, I think our lives will be a bit easier if we stick with a single binary in a single pachd container and just add flags to it like --pps --pfs to change which services it actually launches. Conceptually this accomplishes the same thing, just with a bit less boilerplate code and fewer images to pull.\n. LGTM\n. We should aim to do more of this in 1.2.\nIn particular I'd like to see some faults that affect running jobs and see how the system handles it.\n. Looks good to me.\n. Yeah geez @derekchiang, I'm not made of code review you know.\n. This LGTM.\n. Oh wow, I think this is going to fix a looot of problems we've had with hung jobs and unmounted volumes.\nMany thanks, will merge this as soon as CI passes.\n. Merging!!\n. So with this pr will it error if we try to delete a repo that the cluster has never heard of at all?\n. What's being asked of me here?\n. Eh, I feel like this is kind of bikeshedding, but switching to docs seems a little weird to me. In my mind \"docs\" is short for \"documents\" and \"doc\" is short for \"documentation.\" The latter is what we're really trying to get across since it's the more specific concept.\nAlso it's always a little weird to pluralize the name of a directory, since one can reasonably assume that a directory is going to hold more than one of whatever it's intended to hold. Otherwise why would it be a directory?\n. This LGTM. Merging.\n. It will forever be remembered as the day protoeasy got even easier\n. Could someone give this a quick look for me? Assuming CI passes I want to get this merged asap so I can use it.\n. This could also conceivably happen if the node wasn't assigned any shards. But in all the cases we've seen there were more than enough shards to go around which means that's likely not the full story.\n. This is should go away with the pfs refactor. I'm going to add it to 1.2 so we remember to check it when we're done, although since we couldn't reproduce this reliable it may be hard to verify.\n. LGTM.\n(Good thing you didn't redo the PR, just to get those 4 letters.)\n. Does this mean that we're going to be ok with people starting commits with unfinished parents, just not finishing them? That seems a little bit like a weird workflow, because once you start a commit with an unfinished parent you don't really have a way to know when it's safe to finish the commit.\n. Few more name changes, but after those LGTM.\n. Fix for this has been merged into master.\nThe commit that fixed it was: ed8e0f6a7c9656a94f4306380f8442a48677f7e0\n. Hmm, looks like that mkdir might have crashed our fuse layer, which is weird.\nFew ideas, I normally have to sudo my mount.\nWhat happens if you try: \n$ pachctl create-repo foo\n$ sudo pachctl mount\n$ ls /pfs\nYou should see a single directory foo.\nAlso careful with doing sudo pachctl mount & and not seeing the prompt for your password because it's backgrounded. I've wasted more time than I care to admit stumped by that one.\n\nI'm going to see if I can reproduce a crash using mkdir\n. Yup, that's the expected behavior!\n. Happy to help. Looks like this issue is closable unless there's something here I'm missing?\n. I love this idea, I tried to do this some with integration tests like TestGrep which at one point matched the grep example. But they drifted out of sync and it was never good enough to prevent the types of failures we've been seeing as people start up using the product.\n. LGTM after that one comment.\n. LGTM\n. CI's broken. Blame @JoeyZwicker \n. Pretty sure that's it. Travis saw we added FAQ.md and was like \"omg guys, I can't handle frequently asked questions, I can hardly handle infrequently asked questions.\"\n@sjezewski do you think it's worth reaching out to the travis guys to see if they can make it more tolerant?\n. Looks like this was already resolved. Closing.\n. Few clarifications here, we assign inputs to jobs just by hashing the name of the block. So I don't think there's any guarantee about the nth block going to the nth job. However this does mean that with large files we can expect each job to see each file. Having each job see a piece of each file is generally a good thing, but doing them all in the same order.\nI think this issue ultimately comes down to a question of what order ListFiles returns it's FileInfos in. Right now there's no guarantees because it reads them out of a map and although golang maps aren't guaranteed to be in order they're also not guaranteed to be out of order. In fact they normally iterate in the same order. We should add a randomization step to ReduceFileInfos and I think this issue will go away.\n. Yeah, not much we can do if the user is lexicographically sorting the filenames after the fact. Documenting is a great idea though.\n. I don't think there's anything to actually do here. This issue should be fixed by #529.\nI'm putting it in 1.2 so we remember to check that's the case.\n. @derekchiang bit o' dat review?\n. Yup that's a great call.\nHow does --upstream and --downstream sound? Default should be both.\n. Updating this proposal based on offline conversation. Here's what we'll be adding to complete this feature:\n- Repo provenance: pfs will expose the ability to set other repos as provenance in CreateRepo. This will be used by pps to indicate which repos are derived from others. This allows us to figure out all the repos that could be affected by an incoming commit.\n- Commit provenance: pfs will expose the ability to set other commits as provenance in StartCommit. This will be used to pps to indicate which commits derive from others. It will be an error to start a commit on repo A with a commit from repo B as provenance if repo B isn't provenant of repo A.\nUsing provenance tracking we'll fix a number of issues:\n- Finding the provenance of a commit or repo. The simplest use of this feature just lets you find the repos and commits that led to a commit or repo.\n- Correctly handling propagating data through the dag. #367 \n- Extending ListCommit and ListRepo so that you can filter by provenance. Ie being able to say \"give me all the commits for which this commit is provenant\"\n- Flush a porcelain command that waits for all the commits of which a set of commits are provenant to be finished and returns the commits.\n. Development on this is happening in #391 \n. Merged into master!\n. @munchee13 first snag I seem to have hit. I setup openshift using this guide:\nhttps://docs.openshift.org/latest/getting_started/administrators.html#running-in-a-docker-container\nand seem to be hitting and issue where none of the nodes become available and thus pods can't be scheduled. Any ideas on how to fix that?\n. Alright so far I tried setting openshift origin up inside a container using https://docs.openshift.org/latest/getting_started/administrators.html#running-in-a-docker-container\nI managed to get this working, the issue with nodes not becoming available was caused by a full disk, restarting the machine fixed it.\nThe next snag I hit was with privileged containers. That seemed to go away when I did oc edit scc and enabled privileged containers for all the profiles. This seemed to work.. but only sometimes. Sometimes it accepted containers with privileged security contexts and sometimes not.\nLastly I hit a snag with ports. Apparently openshift, unlike kubernetes, won't let you bind to low ports inside containers.\n. Only thing left to do is document steps necessary in SETUP.md\n. @jarcher we haven't, are you interested in using Pachyderm on those versions?. Losing your jobs when you blow away the cluster is expected I think and we should maybe even see if we can cleanup the bucket. But you can also lose your jobs due to a node failing and the rethinkdb pod being rescheduled which is a problem. Using persistent volumes seems like the right answer to me as well, we should use them for both RethinkDB and etcd. This is definitely 1.0 worthy but we could solve this in a couple of ways, ideally we can just ship a manifest that works out of the box. That may be wishful thinking based on the state of the art with persistence in containers and the fact that we target 2 major cloud platforms. We could also solve this with documentation about how to productionize your cluster on various cloud platforms.\nCould you do a bit of research about what the state of the art is here? There must be some other people who have deployed etcd (and possible rethinkdb) on kubernetes using persistent volumes. Would be good to know what they did.\n. @derekchiang any other comments on this one, and are you satisfied by my answer about the key hardcoding, I'd like to get this merged in by eod.\n. Alright readme is added, think this is good to merge.\n. Last step for this is that the online manifest needs to be update if we push this image. I'm going to do that as soon as this is merged in.\n. Technically speaking I think I could just say package p I think \ud83d\udc45 \n. This LGTM\n. With the exception of those few comments everything here LGTM.\nI'd also like to see a test for run-pipeline which is another good reason for putting the logic server side rather than in pachctl.\n. This LGTM after those 2 things are fixed.\n. I think we're going to do away with shards in the pfs refactor so this will become a non issue.\nClosing.\n. This would be very complicated to change and I'm not sure I understand the reasoning for wanting to change it. Having a static number of shards known throughout the cluster makes a number of things easy and there are a lot of difficulties which switching this number dynamically. For example, suppose we double our shard count, the DiffInfo for shard 0 now needs to be split in half because half of the files it has info for are now supposed to be stored in another shard. Doing this safely using object storage semantics is going to be really tough because you need to overwrite the old version of the DiffInfo with a new version. If that write fails then you don't have a way to recover the data that was there. So you'll need to do something like write a copy of the file somewhere else just in case the write fails and then delete it. There are a bunch of issues of this sort and I'm fairly convinced that keeping a dead simple access pattern to obj storage is going to be an asset for us long term.\nSo what can we do here?\nFirst we should identify the actual issues that we're trying to solve:\nThe first issue is that we can only use 32 nodes with the current setup. That first order problem is pretty easily solved by upping the number of shards, but then we start to hit some other issues. They are:\n1. each shard requires 8 bytes per shard every time roles are assigned, this is particularly bad when there are many more shards than nodes\n2. each commit will write a DiffInfo per shard, even if they're empty, this can lead to a write explosion\n1 could probably be substantially aided by assigning ranges of shards rather than single shards.\n2 is trickier though.\nIn terms of actions on this issue, I think for the time being we should table this. The issues here are definitely performance and for 1.0 I just want to focus on correctness. After 1.0 ships we'll start doing more performance testing and we'll hopefully hit some of these issues through use and will then have a better idea how to fix them.\n. I think we're going to do away with shards in the pfs refactor so this no longer applies. Closing.\n. Updates on this, it seems to be due to kubernetes credentials issues, for some reason when we send the job to K8s it says we need credentials for it. Likely the culprit here is a k8s configuration we haven't seen before but unfortunately we don't know anything more specific than that.\n. @dwhitena it just occurred to me that another thing that might be handy in debugging this is the logs from kubernetes itself. You can get those with kubectl logs k8s-master-127.0.0.1\n. Closed via #352 \n. So for some reason this PR seems to be consistently reproducing the Test296 bug.\n. Could someone +1 this for me? CI was failing before but just passed, I think it was just a case of the intermittent 296 bug, not a real failure.\n. @sjezewski would you mind taking a look at this one for me?\n. Careful, it's a lot of code\n. The actual behavior of reading with a from commit is covered in a few places.\nWe currently don't have anything covering the pachctl implementation, ie that when you pass --from <commit> it actually passes that through.\nWe'll need tests like that eventually I suppose, but I think it's fine for now given how simple the implementations are here.\n. Closing this issue since it's just notes and doesn't seem like there's a win condition here.\n. I think we should punt on this until we have indexed jobs in kubernetes. We'll want to use them anyway since it simplifies things a bunch and they make this issue trivial.\nAlso afaik we haven't had a user request this yet so I think we're ok with it taking a little while.\n. Eh, I don't think just the delay on a PutFile would result in an error, but perhaps.\nFeel free to dive in I don't have an immediate thought.\nAlso is the code for the test committed somewhere? I'd like to have a look.\n. echo > does a truncate before it writes to the file which might account for the difference. I wasn't aware we were handling it differently though.\n. Hi @erikreppel really sorry you ran into this. I think I know what's going on here.\nI updated our manifest last night in such a way that old versions of the pachyderm/pachd image will crash. By default right now if the pachyderm/pachd image already exists in docker then a new version won't be pulled. Is this on a k8s instance that already had had Pachyderm running on it?\nI think it's probably time that we start shipping manifests that reference versioned copies of our container image. @sjezewski what are your thoughts?\nIn the meantime @erikreppel I think you could side step this by grabbing a copy of the manifest and manually changing the PullPolicy field for pachyderm/pachd to \"Always\" instead of \"IfNotPresent\". Note you'll have to do it in 2 places, one for the pachd and one for the pachd init.\n. make test does build the image so that should fix it as well. What are you test failures you're seeing?\n. Yeah, we've been plagued by this a bit. Every once in a while kubernetes seems to get incredibly slow on us and just takes forever to schedule pods causing tests to time out. We've found that restarting kubernetes tends to help but other than that we haven't been able to do much once this starts happening.\n. I'm actually going to repurpose this issue as the issue for referencing versioned containers in the manifests.\n. Hi @dwhitena really sorry this is affecting you. @erikreppel's solution is probably the easiest one.\nMake launch looks for the manifest at etc/kube/pachyderm.json. Here's the modified manifest.\nOne thing I'm noticing about your situation though is that you don't seem to have an init container that's crashing, just the pachd containers, I'm still hopefully that those solutions will work but if they don't the first step is probably checking the logs from one of those pachd pods.\n. Hi Erik,\nReally sorry you ran into this, we'll look into it ASAP.\nThanks so much for providing the code that'll make things much easier on our end.\n. Oh hmm, I think I have an idea what's going wrong here, this is going to send a FilterShard with 0 for BlockModulus and FileModulus. I bet that's leading to a panic somewhere. Will investigate.\n. Those calls to IntVarP are actually setting default values of 1 for both FileModulus and BlockModulus.\nStill this certainly shouldn't be crashing our server.\n. Also I may have misheld the most important piece of info here. If what you're trying to do is just return everything you can just pass nil for Shard. I'm going to make sure we document this as part of this issue.\n. @derekchiang could you take a look at this one for me?\n. Alright these are finally passing CI. @derekchiang mind giving this a look at some point?\n. This pr mostly involves updates to the fruit stand demo, some to make it work with changes that have happened since the last time I demoed, some where improvements like adding an explicit image.\nThere is one actual change in here which is that I added colors to list-job.\n. @sjezewski can has review?\n. docker-build-fruitstand build an image called fruitstand which is used by the pipeline in the demo.\nThis was one of the changes @JoeyZwicker and I decided to make, the fruitstand image is empty in that it's just a copy of our job-shim but it makes it more clear to the user that they can use their own images and how that works.\n. Alright I did some investigating and I think I understand what's going on. The problem here is that our behavior for pipelines that take 2 inputs doesn't quite match up with your expectation. Right now what we implement is the following: if a job takes 2 inputs then we'll make sure that every datum in those repos is seen by exactly 1 container. The sum repo in your example contains a single file which means that only 1 of the containers sees it and the other 2 don't. If the sum repo were the only input then pachyderm would cull the other 2 containers because they're won't see any data and we don't start empty containers. However they're actually not empty, because you have another input square and it has enough data for each container so some containers get started with data from square but no data from sum which leads to the crash.\nI think the behavior you were expecting was basically a cross product, you want every line in square to be seen by a container with the file in sum present so that you can use it. @erikreppel does that describe the behavior you were expecting?\n. @erikreppel thanks, that all makes sense this is a behavior we've talked about a bunch but wanted to understand how users perceived this issue before we started implementing things. Based on this and other conversations we've had I think that the cross product behavior is going to be what the average user expects so I think we'll make that the default.\n\nAny ideas on how to efficiently do this?\n\nRight now I think your best bet would be to just scale down to a parallelism factor of 1 so that you can get unblocked. We're going to look into getting the cross product implemented for you ASAP and then your old code should just work.\n1 more question: does the behavior as it stands now seem useful to you? and if so how would you expect it to be exposed to you?\n. Alright, we just had a big discussion of this offline here's what we came to for a direction on this feature. Here's what it's going to look like:\n- The default for parallelism is going to switch from union to cross product, meaning that when a pipeline takes 2 inputs we guarantee that the each pair of datums will be seen by exactly 1 container. This means a single datum might be seen more than once.\n- Inputs will have a boolean global flag on them. This flag indicates that this input repo should be seen by every container run in the job.\n. @JonathanFraser what you're saying makes total sense, I think what we have is already pretty close to what you're describing. In our system a map shards over blocks, meaning that a single file may be split over several different containers. In reduce jobs we shard over files, so each file will be seen by exactly one container but any 2 files could wind up in separate containers and not be seen together. I believe those are the semantics you're describing but correct me if I'm wrong. With this issue we'd be adding a 3rd mode of input global which would specify an input that every container in a job sees all the files of no matter what.\n. Besides using that helper function this all LGTM.\n. Made #365 to track the usage issue.\n. All lgtm\n. I think our best bet would be just getting it into the library with a PR, printing usage with the wrong number of args seems a better default to me. Hopefully getting it merged in will be painless. If not a suppose we could just write our own we're talking about some pretty simple functionality here.\n. All LGTM.\n. This all looks good to me, although there's enough here that I can't claim I verified 100% of logic.\n. One big open question is if there's a case of a DAG that's not a transitive reduction where you actually do want to process commits with different provenance together, I can't think of one right now.\n. This is now solved via #391.\n. This all looks reasonable to me. +1\n. Sounds good to me.\n. The best thing about godoc is that we don't need to do anything to get it :). When we merge this PR the docs here should just automatically get better: https://godoc.org/github.com/pachyderm/pachyderm/src/client.\nI agree about it being too noisy. AFAIK there's no mechanism for controlling what's in docs besides making things private. I think the right solution is to take all of the util functions, lift them into src/client and relocate the version stuff that's in there into src/client/version. That way the godoc for client will be everything you need and nothing else.\n\nAlso - In a lot of the descriptions you're breaking down the arguments which is great. In other doc formats there are specific formatting mechanisms for arguments. I'm not sure if godocs has those, but if they do we should use them\n\nAFAIK you're just supposed to do it in the doc for the function\n. Alright refactor is done. Think this is good to go.\n. Merged into master!\n. @JoeyZwicker you mind reviewing this one?\n. This LGTM.\n. Makes sense to me as soon as CI is passing this should be good to go.\n. These cli docs are looking much nicer than they did before :)\n. This all LGTM.\n. I shipped it.\n. @sjezewski did a few special things in the mount cmd to make usage show up when the command errors. I think given the new cmd package that's now redundant and should be migrated to just using RunFixedArgs. Let's do that as part of this PR too.\n. Nice LGTM\n. @derekchiang lemme get dat sweet sweet +1 here?\n. Scraper branch has landed and I can confirm that this test passes when run.\nIt's slightly strange, the test runs fine if I run it alone but it hangs if I do all the tests in 1 run.\nClosing this since I think that's a different bug, all of the things in here have been addressed as far as I know.\n. Only right that I follow it up with the +1 of the year.\n. Fixed via #434 \n. Hey @elsonrodriguez, really sorry you ran into this. I just checked the manifest online and realized that somehow we wound up with an older version of the manifest hosted on our site. I'm going to update it right now so this should be resolved shortly.\n. Just pushed the correct version of the manifest, it may take a few minutes for it propagate through CDNs and such. Please let us know if this doesn't resolve the issue.\n. This shouldn't be too hard to add. Question: is it possible to detect what namespace we were deployed in? If so we can just do the right thing. If not then I guess we'll have to take an environment variable to specify the namespace we should deploy in.\nEither way shouldn't be too hard to get this in for 1.1.\n. > The following information is available to a Pod through the downward API:\n\n\nThe pod's name\nThe pod's namespace\n\n\nSounds like that would do it.\n. Hmm I don't know much about how elb load balencers work. Are they generally compatible with grpc?\n. So the fruit stand demo actually used to look like this, we switched to using a fruit_stand image so it was more obvious that you could add your own images. pachyderm/job-shim is actually the default image so we could in theory just remove both those lines.\nWhat do people think is the most clear here? There seems to be a bit of disagreement.\n. Great catch @elsonrodriguez. CLA has already been signed so merging.\n. Hmm, I saw an issue like this due to some docker image version incompatibility. I think what's happening is that the version of pachyderm/job-shim on Docker Hub isn;t compatible with pachyderm/pachd:v1.0.0(849) the pachd image you're using.\nI just repushed the job-shim image so I think if you rebuild the fruit stand image it will just work. If not please let us know.\nReally sorry you hit this snag, I think we need to start versioning our API to prevent stuff like this.\n. This seems to have been resolved. And suggestions have been spun off into other issues. Closing.\n. @derekchiang let me get a quick +1 on this?\n. LGTM\n. LGTM\n. Hi @Justin-Kuehn sorry you hit this, I think your expectation on what this command should do is pretty reasonable. The reason it behaves this way is that CreateJob was made idempotent to make handing off responsibilities for pipelines a little easier. We should add a way to override that behavior for run-pipeline, probably a force flag on the CreateJob protobuf which causes a new job to always be run if it's set to true. Does that seem reasonable to people?\n. This is actually a bit curious. Unless I'm misreading the code it seems that in the amazon obj client we're using an exponential backoff writer which should catch errors like this and retry.\n@derekchiang any thoughts on this one?\n. I think it'd be better to keep this open and move it into v1.2 just to make sure we remember to do this. It'd be pretty easy to inadvertently keep the old behavior during the refactor.\n. It actually used to do this but I removed it because it made the output lines too long and basically unreadable. One idea, what if we made inspect-job and other inspect commands return extended information about the objects. Basically list-* would return a small enough amount of info that it fits in one line. While inspect-* would span multiple lines and tell you everything you might want to know about the object.\n. Hi @kevinemoore really sorry you ran into this.\nWhat version of go do you have installed?\n. You were really close. go version will do it.\n. Ahh, this might be some GOPATH issues. What does echo $GOPATH get you and where is the repo in your filesystem?\n. Ahh, that's the problem I think. Our repo expects to be located at $GOPATH/src/github.com/pachyderm. Probably the easiest way to get it setup correctly would just be go get github.com/pachyderm/pachyderm. That'll clone the repo into the right spot on the GOPATH and get the dependencies as well.\n. Fixed.\n. LGTM\n. @sjezewski quick +1 on this?\n. Provenance is in master :)\n. LGTM\n. LGTM\n. This PR ballooned a little bit but it's finally ready for people to take a look at it.\nThe main change here is the addition of a scraper example in examples/scraper it's similar to our old scraper in that it uses wget for everything.\nAdding the scraper revealed several issues with our fuse layer which I then started in on fixing.\nThe first one was support for directories which was simple enough and we now have support for.\nThings got more complicated with correctly supporting deletes and the > operator in shell.\nI also realized that our plan of never displaying files in the /pfs/out doesn't really work because it allows people to write a file over a directory which isn't well defined.\n/pfs/out now displays output files so this won't happen.\nAnother issue I ran into is that if 2 containers in a job try to delete a file then one will fail because it will get file not found so we now use handles for deletions as well as for put file so that each containers can't see each other's deletes.\nThere was one issue I couldn't figure out in this test, TestRemoveAppend broke on travis but I could reproduce the error locally. After trying for a few days I decided to just add a Skip to the test and kick it down the road a bit. I think it's an issue which will quite likely be fixed with the pfs refactor.\n. Alright CI is passing on this and I think I addressed all the comments.\n@sjezewski could I get a LGTM on this? Preferably in gif form if possible.\n. Did the tests complete when you ran them? I'm getting a really strange issue. If I run the tests with:\ngo test -timeout 60s ./src/server/pfs/fuse >test_out\nThen I get a timeout because the test TestCommitFinishedReadDir hangs. However if I run just that test then it passes. The tests don't interact with each other as far as I can see.\n. Lol, gif LGTM. Merging.\n. LGTM as soon as CI passes.\n. Hmm, this may require a make assets\n. \n. Hmm, could you do kubectl version for me?\n. Hmm, so one thing I'm a little confused about. It looks like your server is v1.2.1 there but make launch-kube seems to use the 1.2.2 docker image. What commit are you on that you did make launch-kube from?\n. Hmm, what does docker ps give you?\n. Ahhh that makes sense. Where's docker running is it on your local machine or is it running on a vm / in the cloud? If the later you may need to forward port 8080 from your local machine to wherever docker is running. While you're at it you may want to forward 30650 as well since that's what pachyderm uses. If you're using docker machine this command will do what you want:\nshell\n$ docker-machine ssh <docker-machine> -fTNL 8080:localhost:8080 -L 30650:localhost:30650\n. Hmm, I'm not familiar with localkube is it possible it wrote a file to ~/.kube/config? Which kubectl is using?\nAlso it might be that when you did the port forwarding localkube was already bound to port 8080 in which case I think it silently fails. So you might have to kill the port forwarding process and restart it.\n. \"Client\" just means kubectl in this case so it's just reporting its own version. Does 192.168.99.100 resolve to your machine? If so we know it's not ~/.kube/config and I'd try killing the port forwarding process and restarting it.\n. Nope, just rm it.\n. This seems to be resolved. Please reopen if not.\n. LGTM\n. LGTM\n. This is definitely a good idea.\nI think major thing we'd need to figure out is how to blow away the output from the stragglers. So that we could restart them. In theory this wouldn't be too hard handles already do almost what we want.\nA few random thoughts:\nWe could conceivably have a more aggressive definition of a straggler. We start jobs with approximately the same amount of data per worker and we can use the % of the data they've requested as a proxy for the jobs progress. This could allow us to detect workers that are straggling a lot sooner.\nBackup executions of stragglers is great but what if we went a step further and broke the stragglers up into more shards so we could increase the parallelization factor and decrease the straggling.\n. It might make sense in the new model to run multiple copies of the same datum if we get to the end of a job and a few datums are straggling.. lgtm\n. LGTM\n. Save for that 1 comment this lgtm\n. LGTM after that 1 comment.\n. LGTM\n. LGTM\n. \n. Will merge the pic as well.\n. \n. Deleting a file in a finished commit is disallowed along with other write operations like PutFile. Commits need to be immutable once they're finished so that downstream jobs can safely consume them and not worry about them changing during execution.\nWhat you can do though is start a new commit with the commit you want to delete the file from as the parent, this commit will allow you to call DeleteFile while it's open and once you finish the commit it will look like the old commit with the file deleted.\n. lgtm\n. As part of this can we make it so that golint errors fail CI when they exist?\nAnd although we can't run the mac test we could probably compile it in CI.\n. Looks like testFuse was cleaned up at some point. I don't see any debug lines left in it.\nPretty sure this issue is complete, closing.\n. We've had another report of this same error using our 1.0 manifest (the one we host online) and a baremetal Docker installation.\n. Progress on this. The culprit here seems to be iptables not being configured correctly which prevents kubernetes services from doing their jobs. The solution is to enable the proper kernel modules, in some cases can be as easy as:\nmodprobe netfilter_xt_match_statistic netfilter_xt_match_recent\nIn others it may require recompiling the kernel. Please reach out to us here if you're hitting this issue, we've only seen a few instances of it and the solution tends to be a little bit different so if the things listed here don't work it could just be down to the specifics of your system.\n. I think the thing to do here would be to change the error message users get when they hit this so it explains in more detail what's going on. Perhaps point them to this issue so they have a chance at resolving it.\n. Gonna keep this open so we remember to do that.\n. Fixed via #488.\n. Hi @ShengjieLuo sorry we didn't respond earlier your comment slipped through the cracks.\nI think what you have here is actually a different problem. It seems that the pods are failing to talk to rethink which makes sense because the rethink pod isn't coming up successfully. Could you check the logs from the rethink pod, that will probably tell us what's going on.\n. Fixed via #488.\n. make launch-kube has a loop at the end which waits for kubernetes to come up. However if it can't get in contact it'll just sleep forever. If you kill make launch-kube and then do kubectl get all what does it get you?\n. Hmm, so that error suggests that the repo is located outside of the $GOPATH but based on what you told me that doesn't seem to be the case. Would you mind trying this:\nrm -rf /work/src\ngo get github.com/pachyderm/pachyderm\nThen try the commands again, that should download the repo in the right spot and also download all of the dependencies.\n. Closing this as there isn't anything actionable left to be done.\nOne concrete change that came out of this is that we now check that kubectl is installed before we enter the wait loop. That should prevent the initial issue here.\n. Hmm, looks like some missing C headers, I think what you want to do is:\napt-get install build-essential\nlmk if that works for you.\n. Looks like you need to install go.\n. @sjezewski nice catch! Fixed by #478.\n. This LGTM after the temp dir comment and hopefully the comment about the test sleep are addressed.\nThe later might be too much work but the former I'd like to see.\n. \n. Ignore the second part of that gif.\n. LGTM. I'm gonna restart CI and see if it passes now.\n. \n. Many many thanks for the pull @sambooo.\nThis all LGTM and you already signed our CLA so as soon as CI passes on this I'll get it merged in.\n. Similar issue reported by by @dwhitena the Dockerfile is right here:\nhttps://github.com/dwhitena/gofordatascience/blob/master/golang-builder-latest/Dockerfile\nWe should put some fail safes in the system to monitor the job as it's running and notice when it disappears without send the FinishJob rpc.\n. Alright I have a PR in for part of this issue. #476. It should fix the panic. Unfortunately I couldn't find a way to reproduce the issue, since it required some sort of corruption of the diffs that would cause ListRepo to error.\nI'm going to leave this open but we should merge the PR since it'll let us get an error the next time this happens and learn more about the corruption.\nWe should also try adding some corruption tests to see if we can repro it. \n. @JoeyZwicker this one is actually in your wheel house.\n. @sambooo thanks for another pull!!\nCI is failing for good reasons here. This PR adds linter to CI but the linter fails because we don't have comments on all our exported classes. I'm going to add some commits of my own to this PR to get the linter fixed since I don't think @sambooo should be expected to document all of our undocumented things as part of this PR.\n. Updates on this. Turns out I can't add commits directly to this PR since the branch is on @sambooo's gh account not mine. I pulled it in to my own branch and fixed the immediate linter issues with src/client. Unfortunately there's a snag pb.go files don't pass the linter. I think what we want to do is use go list little more creatively to give us file names instead of pkg names and the grep -v pb.go to get rid of the stuff we don't want to lint.\n. Cool, this all lgtm now.\n. @sjezewski quick review on this?\n. we have a strict no meme no merge policy\n. nvm closing in favor of #471 \n. piclgtm, in fact it's the best lgtm pic I've seen in my 28 years on this earth\n. This was fixed in #480.\n. \n. \nI really like the new vendor readme. Very high signal to noise documentation :)\n. Hi @SoheilSalehian really sorry you ran into this.\nI think this will fix your problem:\nsh\nmake clean-launch\nmake docker-build\nmake launch\nThe problem is that you're using the latest version of the manifest but it's pulling down the 1.0 version of the container which isn't compatible. make docker-build will build containers from HEAD which will then be used when you do make launch. Lmk if this doesn't fix it.\n. @SoheilSalehian awesome!! Glad that worked.\n. pic lgtm!\n. Scraper has a tutorial attached to it. Closing.\n. Hi @ChaiBapchya sorry you hit this, looks like a frustrating issue.\nUnfortunately there's not much we can do about this since the problem is with Kubernetes and not Pachyderm.\nI'd recommend heading over to https://github.com/kubernetes/kubernetes and opening this issue over there.\n. The issue with failing to negotiate an API version with is a new one for me. What version of kubernetes were you using?\nThe second one looks like a connection problem. You should check port forwarding etc. to make sure kubectl can talk to kubernetes.\nNeither of these issues could be fixed by code added to this repo though so I'm going to close this. I think the kubernetes folks will be able to tell you more, you should try running this issue by them.\n. Similar to #498. Your problem here is likely that you don't have port forwarding setup correctly or something like that. However it's not a Pachyderm issue, I'd recommend taking this issue to the kubernetes team they'll be much more able to help you with issues like this one.\n. This isn't an essential part of setup. file is just used to check if you're blowing away data on the drive.\nIf you want to accomplish this on CoreOS you should run a container with /dev/xvdf mapped in that has file installed. Ubuntu should work well for that.\nEither way we don't have the ability to fix this. We don't determine what goes into the CoreOS images.\n. Nice One.\nAll LGTM.\n. Looks good!\n. Hmm, this looks like it's probably a mismatch between the pachctl version and the server version.\nWhat happens if you try rebuilding both of them?\n. Pretty sure this was just a version mismatch. Closing.\n. This is a helpful clarification for the state of things right now. They're about to change though so that manifests reference specific versions in which case people shouldn't have to build images as a part of the normal deploy process.\nI think this could make sense as a section called \"Deploying from Head\" though We could put that at the bottom and explain to people how head is generally less stable but has new features. We could also explain how to deploy from head on cloud infrastructure which requires pushing to docker hub or another container registry to get the image to the machines.\n. We considered doing this awhile ago when we were first designing the CLI and ultimately decided against it. We saw the same benefit you do, that commands could automatically be organized by the noun rather than the verb which would make the docs more browsable. After using it for a little while though we realized it was too confusing so we switched it back.\nIf cobra does indeed recommend that commands be APPNAME VERB NOUN --ADJECTIVE then that seems like an argument not to do this. Since I think that's what we already have. Furthermore that's the way kubectl organizes things and people are normally going to be using both of them which means it'll be even more confusing if they call it delete job while we call it job delete.\nI think we can get a lot of the benefits of this by making a custom help function, that way we can directly control how the commands are printed.\n. Lol, glorious pic.\n. I assume that's +1 for every time you had to recompile containers to turn on fuse debugging.\n. \n. Could you write up a design doc about what you consider as your approach to writing this code? I'd like to understand some of the motivation better.\n. \n. All looks reasonable.\nIs there a similar problem with not found errors? I.e. if the system tries to read an object that's not there will it get stuck trying to find the same file again and again?\n. @derekchiang just the one change and then I think this is good to go\n. The day I ask you to merge a PR w/o a meme is the day I lose all credibility.\n\n. Looks gif to me\n. All looks good save for the one comment (gif will be posted upon completion)\nDid you try adding any tests that call Flush explicitly? I'm still pretty confident that we could get our delimiter code to break with that. But that was already the case before this change so it shouldn't prevent us merging this in.\n. \n. Hey @ChaiBapchya sorry you ran into this, here's what's going on.\nmkdir ~/pfs\nmkdir: cannot create directory \u2018/root/pfs\u2019: File exists\n[root@chai /]# pachctl mount ~/pfs\n<pfs is now mounted>\n^[[D^Z\n[1]+ Stopped pachctl mount ~/pfs\n<pfs is still mounted but the mount process is now stopped>\n[root@chai /]# pachctl mount ~/pfs &\n[2] 3349\n[root@chai /]# ls ~/pfs\nHANGS\n<this hangs because the mount process is stopped>\nThis is definitely a confusing behavior. I think we should add a comment so the user has some idea the mount succeeded. Something like: \"Successfully mounted pfs on /mount/path. CTRL+C to unmount.\"\n. The old mount is probably still there. What if you do umount ~/pfs and try again?\n. I've hit this bug a few times too. I think this one basically comes down to local block store issues. There's some pachd node that's assigned a shard that it wasn't assigned when the commit was finished so it doesn't have the necessary file to delete. @derekchiang's hostpath fixes should fix this issue as well.\n. I believe this is a duplicate of #614. Should be resolved as part of the pfs refactor.\n. This is what a healthy cluster should look like:\nNAME                   DESIRED      CURRENT       AGE\netcd                   1            1             3h\npachd                  2            2             3h\nrethink                1            1             3h\nNAME                   CLUSTER-IP   EXTERNAL-IP   PORT(S)                        AGE\netcd                   10.0.0.85    <none>        2379/TCP,2380/TCP              3h\nkubernetes             10.0.0.1     <none>        443/TCP                        52d\npachd                  10.0.0.57    nodes         650/TCP                        3h\nrethink                10.0.0.26    nodes         8080/TCP,28015/TCP,29015/TCP   3h\nNAME                   READY        STATUS        RESTARTS                       AGE\netcd-g8g4v             1/1          Running       0                              3h\nk8s-etcd-127.0.0.1     1/1          Running       0                              52d\nk8s-master-127.0.0.1   4/4          Running       0                              41d\nk8s-proxy-127.0.0.1    1/1          Running       0                              41d\npachd-2w1gl            1/1          Running       0                              3h\npachd-r90mo            1/1          Running       1                              3h\nrethink-9zsh0          1/1          Running       0                              3h\nThose pod states meanings are:\nPending: kubernetes is still scheduling the pod\nRunning: alls well the pod is running\nImagePullBackOff: the image failed to pull and is waiting to retry. Most likely this means you made a typo in the pipeline\n. For this issue I think the only actionable part is catching the error messages that arise when trying to connect and making them more friendly. I think that's definitely worth doing.\n. If you're not actively working on the branch I'd say submit it for code review with a Skip() in the test so we can merge it in and not break CI. That way it won't accrue conflicts and if someone wants to pick this up later they can just delete the Skip() and have a failing test case to work with.\n. Definitely agree on documenting these more. I'm not sure if I understand what the additional state would correspond to though. Pipelines can't terminate successfully because the point of the pipeline is that it keeps running indefinitely waiting for new inputs to come in.\nI think we already have states for all the other ones proposed don't we?\n. Ah, I see that does make some sense as a feature. I'm currently adding a feature where when you do inspect-pipeline you'll see a count of job states from the pipeline, i.e. how many jobs are running, completed and failed. I think this gives you the info you want and then some, if a pipeline has 0 running jobs then it can be considered up to date. This also gives you an idea of how far from up-to-date a pipeline is.\n. Job counts are now in master.  Closing.\n. My intuition is that this should be called delete-all to match our other names. That feel good to people.\n. Fixed via #547.\n. \n. @ChaiBapchya the problem here is that you're passing a.py as stdin to the script rather than as an argument. Your pipeline should look like this:\n{\n  \"pipeline\": {\n    \"name\": \"scrape7\"\n  },\n  \"transform\": {\n    \"image\": \"acf2a3ec273a\",\n    \"cmd\": [ \"python\", \"a.py\"]\n  },\n  \"parallelism\": \"1\",\n  \"inputs\": [\n    {\n      \"repo\": {\n        \"name\": \"abode2\"\n      }\n    }\n  ]\n}\n. Closing this for staleness and because it doesn't seem like this will lead to changes to code. Feel free to keep asking questions here, we're happy to support you in this thread.\n. We wound up just exposing them by default in #623.\nClosing.\n. \n. Closing for staleness. Please reopen if there's new information.\n. Also we have some error strings that start with capitals which is against the go style guide so we should fix that as well.\n. Does -f - read from stdin?\n. I'd kind of expect it to, because create-pipeline supports that. Although you'll need to error if they try to do -f - without providing a file name.\n. This no longer applies.. \n. Now that this compiles it looks even better to me.\n. Off the top of my head I think the easiest way to implement this would probably be to have an in memory version of the block store that stored and served blocks from memory.\nWe'd also need to figure out a way to GC the blocks once we're done with them, but that shouldn't be too hard.\n. Fixes #528 \n@derekchiang whenever you have a chance\n. This should have a test job counts.\n. There's a bug with the InspectCommit template that I discovered while demoing. >.<\n. @derekchiang  Just added a test that restarts all of the pods to make sure it works. Take another quick look for me?\n. One issue here is that there isn't a good way to tests this. Calling DeleteAll from the integration tests will definitely interfere with the other tests passing.\n. This now has a test as well. Still waiting to see if it works on travis... I have my doubts.\n. Uh oh, I think I somehow completely blew away this pull request with git... that's less than ideal.\n. One way I know of reproducing this is just restarting the cluster and recreating the pipeline.\nSince currently pfs data survives restarts and pps doesn't that will leave you in a state where you get errors. Have you seen other ways of reproducing it or do you think it might have been that one?\n. This has been fixed now by a couple of different prs. Closing.\n. Probably not... but I'm gonna do one anyway\n\n. I don't really see the point of this. When create-pipeline returns with a 0 exit code most users should be able to infer that the pipeline was successfully created. Also if this was something we wanted to do why would we only do it for create-pipeline and not for other commands?\n. When does create-pipeline take a while? I haven't seen that and looking at the code it looks like all it does is create a repo and then insert a row into the database which should be really quick. I also don't understand which errors can occur that won't result in the command returning really quickly with that error. Are there errors that cause create-pipeline to hang for extend periods?\nConsistency of the output of pachctl seems really important to me because people are likely going to write scripts using it. Right now we have the simple rule that create-* returns an ID if one is generated, otherwise it returns nothing. If we're going to do something like this then I think it's important we do it consistently across all of the commands but I also don't think we want every command to return: \"successfully did the thing you told me to do\" do we?\n. Hmm... it's possible this is because the fuse driver is doing unsafe reads.\n. \n. I think the issue here is that it can't find the fruit stand image. We can fix this by pushing the fruitstand image to Dockerhub.\n. After thinking about this a bit more I'm not totally convinced that pushing the image to docker hub is the right solution here. The argument for it is that it helps people who are trying to run the example on a Docker host that they don't have direct access to (ie on AWS) complete the example. But I think that means that it compromises the efficacy of the example as well. If they want to write their own jobs then they're going to need to solve this problem anyways. Sidestepping it in the demo just means they have no guidance on how to do it for real.\nI think a better solution to this problem is to just remove the custom image and have the demo reference pachyderm/job-shim, that way the first demo people do is more bullet proof and people are more likely to complete it successfully and later demos can teach people how to use custom images.\n. Many thanks for the pull. All looks good to me and you already signed the CLA merging.\n. pachctl should be able to access pfs from any container that is scheduled by Kubernetes. Accessing from an arbitrary Docker container is harder because Docker blocks access to things on the outside machine. I'd recommend scheduling with Kubernetes.\nClosing for staleness and because it seems unlikely this will lead to code changes. Feel free to keep asking questions on this thread though.\n. I'd expect to see the branch name in addition to commit ID not in place of it. So you could get the same result from /pfs/repo/commitID and /pfs/repo/master until a new commit is created on master at which point /pfs/repo/master will advance and the commitID will become the only way to access it.\n. This is present in master now, closing.\n. \n. @JonathanFraser I'm not sure I understand. When a pipeline has more than 1 input the input that doesn't have a new commit for you to process will display all of its values so that you can cross them with the new ones in the other repo. Since it's already displaying every value it's unclear to me how either proposal would impact that behavior; in my mind both leave it unchanged. How would you like to be able to modify that behavior?\n. Partition specifies how the data is partitioned between multiple containers when the job is run in parallel. So when a repo has no new data you'll see the entire repo, but it might be split between multiple containers for parallelism purposes. If partition is FILE then we guarantee that files don't get split between multiple containers.\nThe interplay between this feature and the Partition field is pretty subtle but I think it makes sense. Basically setting incremental = changed means that instead of seeing a diff you see whatever type of object is specified in incremental. I think this makes sense because the Partition field is basically specifying how this data needs to be grouped together in order for the computation to be performed correctly. If we did a lower granularity for incremental stuff then it'd be really easy for a computation to accidentally return a wrong result with out realizing it since different data is available under different conditions.\n. Bumping this to 1.2.\n. Now in master.\n. \nShould look into the CI failures before you merge obviously. Could just be the same types of intermittent failures we've been seeing.\n. Thanks for reporting this @pratheekrebala.\nI think we've found the issue, jobs in Pachyderm have deterministic names so when you recreate the pipeline it will recreate jobs with exactly the same names. To avoid conflicts we delete the jobs along with their corresponding Kubernetes jobs. However deleting a job in k8s doesn't delete the pods that it created and when you recreate the job those same pods will count as completed pods for the job so the job won't spawn any new pods.\n. Scheduling this for 1.3.\n. @derekchiang take a look at this when you have a chance.\nWound up doing something that I thought was simpler than what we talked about. All that matters to k8s in terms of job scheduling is the selector for the job so I just used a uuid for the app name instead. That way jobs can still have the same name in k8s and pachyderm which makes it easy to cross reference them.\n. Meme lgtm.\n. Fix has been merged and is now present in master. Should be shipped as part of 1.1\n. This would definitely be a nice to have for 1.2.\nWe can probably just catch the error and retry with some sort of a timeout.\n. Hey @JonathanFraser sorry you ran into this there's what's going on:\nJob ID's for pipelines are generated in a deterministic way, this is convenient because it makes job creation idempotent which means that we can service pipelines in a distributed setting without having to worry about some of the nastier corner cases that pop up. Unfortunately the IDs are created by hashing the pipeline name, transform and input commits which means if you recreate the pipeline it creates jobs with the same name. I can think of a few of ways that we can solve this:\n1. just use unique job ids and handle corner cases\n2. delete pods when we delete a job\n3. change how we compute job ids for pipelines so that it'll be unique across different runs of the pipeline\nI'm leaning toward 2 since it seems like it's the easiest to write and it feels clean that deleting a pipeline deletes of the objects that it created.\n. Fixed via #584.\n. @derekchiang whenever you have a second, this PR should hopefully make our CI more stable.\n. @derekchiang could you take a look at this today? Would like to merge it in before too many conflicts accrue.\n. Hey @JonathanFraser really sorry you ran into this. We have several tests that cp data from input repos so I'm guessing there's something subtly different here. Would it be possible to share the code that created this error so I can reproduce it locally and figure out what's going on?\n. @JonathanFraser thanks! Will repro on our end and figure out what's going on.\n. @JonathanFraser update on this. I created a test in our test suite which mimics your pipeline as best I could. You can see the test here. Unfortunately I wasn't able to get this test to fail. 1 thing that comes to mind: what do your files look like? I'm creating 100 files, each of which is 10,000 lines long I'm guessing maybe that's the difference but I'm not sure. Any chance you could share the data files? That may be the best hope for getting a faithful recreation on our end.\n. @JonathanFraser update on this, I tried downloading some of your specific data files in a further attempt to recreate and wasn't able to get it to show up. I think there's a decent chance that the error was caused by getting pachd into some sort of a weird state which caused pfs ops to error. It might be that this issue goes away if you do a clean deploy. Please let me know if you reproduce the error again with a fresh deploy, hopefully if that happens it'll teach us a bit more about how to reproduce the issue.\nSorry we couldn't find a better solution to this one for you.\n. meme lgtm\n. @derekchiang whenever you have a chance\n. I'm kind of confused how we're opening so many connections. It looks like we pass a single connection into NewMounter (wrapped in a PfsAPIClient). Is it possible that streaming rpcs result in a new connection being opened?\n. As mentioned offline, we actually don't need to solve this issue directly. We're planning a refactor of pfs in #411 which will make it so that pachd nodes no longer need to connect to each other which sidesteps this problem.\nIt would still be really nice if we had a reliable way for long running processes to keep grpc connections to one another... but alas I don't see how to do it right now.\nMoving this issue to 1.2 which is where we're planning to do the refactor.\n. 1 small tweak here. I'd prefer we call it --stdin rather than --script since that's the name of the field it's filling in.\nAlso as a clarification, if you do pachctl create-pipeline --stdin script.py (no --dry-run) then it should just embed script.py and send the pipeline request off to the server.\n. @sjezewski whenever you have a second.\nFixes #578.\n. I'm not totally sure this works. What happens if a pachd pod restarts and comes back up with the same address? Since we never evict connection it seems like we'll be left with a defunct connection and anytime we try to use it it'll block.\nI think you could test this pretty easily by creating a test similar to TestRestartAll which just deletes 1 of the pachd pods.\n. Ahh, I didn't realize gRPC could redial. Assuming that works then this PR is lgtm. Will begin hunting for a meme.\n. \n. The file etc/user-job/Dockerfile is meant to document what a user needs to add to their container to make it work with the job-shim. It's definitely a lot more verbose than we would like though.\nProbably the best solution to this problem is to figure out how to separate the job-shim code and the user code into separate containers. I'm not sure kuberenetes has all the features we need yet. But I think it's getting close.\n. > We should probably refer to it somewhere in our documentation.\nThat seems like a very sound idea.\n. @sjezewski meme game is on point\n. I would have really liked to add a failing test too, but have no idea when fuse triggers this bug unfortunately.\n. \n. My hope is that CI will start passing once https://github.com/peter-edge/proto-go/pull/5 is merged.\n. @sjezewski quick review on this?\n. Hi @aberoham and many thanks for the pull!\nThis looks good to merge we just need you to sign our CLA (even for a small change like this)\n. @sjezewski hmm my hunch is that this is a different bug. It looks like for some reason the pipeline isn't triggering when it should be. It'd be nice to have this as part of the test suite. Maybe it's time we get the fruit stand demo running as part of CI.\n. @sjezewski you did pachctl finish-commit data master too right?\n. \nLast step is me testing on my machine right?\n. \n. Still lgtm, does it need a new meme?\n. This is in master, compliments of @msteffen.\n. @derekchiang this is fixed now right since BlockInShard takes a filename?\n. \n. This is caused by a version mismatch between pachctl and the server that you launched.\nProbably the best way to fix it would be to do:\nmake clean-launch\nmake docker-build\nmake launch-dev\nThis will launch you a cluster from head.\n. Closing since I think it's just a version mismatch error. Please reopen if the above steps don't solve your problem.\n. I think we're likely going to have to limit ourselves to shard boundaries for work recovery for the reasons you describe here about not really being able to know if a file has been processed or not. That can probably be ok though because we can scale up the sharding to get whatever recovery granularity makes sense.\nWe've talked about supporting other computation paradigms like classic Map/Reduce. In those tracking completion on a per file basis would totally be doable so maybe it's worth thinking about what we'd need to implement to get that use case working.\n. I think we can still do a lot here with job recovery on commit granularity by finding stragglers and restarting them with a higher sharding factor. We'll lose some amount of work along the way but I think the approach can still make things dramatically faster. Especially if we're aggressive about when we call a job a straggler / runaway. For example suppose we have a job with 4 pods running. After a few minutes we notice that 3 of these pods have read 75% of their input data while the fourth has read only 15%. We deem that pod a runaway and so we further shard its input into 8 more containers, those have to start from scratch but they only have 1/32 of the total data so they'll likely finish much earlier than previous run would have.\n. Can confirm, work recovery is as good as it's going to get.. \n@sambooo is this ready for merge or was there more you were planning to do?\n. > A good chunk of the comments could probably use more info\nPuts them on par with the rest of the comments in our codebase I'd say :)\nMerging!\n. @derekchiang do you think you could find time for this one before 1.1?\n. \n. Closing this one, seems the immediate issue has been solved and #612 should make this problem go away.\n. I think the simplest solution here is just to not report not found errors. They aren't really errors in the sense of requiring anything from the user and whichever process issued the syscall that resulted in that error will likely know how to handle it.\n. Possible fixes I could see for this:\n1. show cancelled commits by default, this is the easiest solution the downside is that users may not realize the commit is cancelled\n2. mark the commits as cancelled somehow (maybe by prepending cancelled_ to the front\n3. add a flag to mount, maybe mount -a which would include the cancelled commits\nOf these I think I like 1 the best since it's simplest and users have other ways to know that the commit was cancelled. In most cases we've seen people seem to be mounting to view a specific commit that they already know about from list-commit so they'll know it was cancelled.\n. \nWaiting on README ofc\n. I don't think this one has anything to do with the pfs refactor. I'm gonna remove the label.\n. This is now in master.\n. Fixes #276.\n. Closing this for staleness. Will reopen if there's active work being done on it.\n. @sjezewski whenever you have a chance\n. This meme would be so much better if the kid was doing this:\n\nbut I suppose I'll merge anyways.\n. We actually do not yet, this is a bit tricky because of scalability concerns. The best way I can think to do this is keep a bitmap for each output file that records which datums contributed to it. If we have a pipeline that takes 1 million files as input and outputs 1 million files, well within the scale we want to address, then that's already 1 trillion bits or 125 GB. That's only for 1 commit and it's not like 1 million files is an upper bound on how many files we're going to have.. An initial proposal for how to do this:\n- Add a global generation counter\n- All commits are assigned to the current generation on creation\n- Any time a block is written as part of a commit we record the generation it's being written in. This allows us to know the most recent generation a block has been written in.\n- When we want to gc we increment the global generation. If the generation is n then we know that all blocks in generation [0,n-1] that aren't referenced in commits [0,n-1] can be GCed.\n- There will never be any new Commits in [0,n-1] so there's no writes that this computation can race with. If Commits are later deleted though GC can be rerun to reclaim the space.\n. I think the above implementation would service this. Calls to pachctl delete-all would leave all of the blocks unreferenced which means that they would eventually be GCed and the space would be reclaimed.\n. Epic meme\n. Weren't we already using awk in much the same way though?\n. Anyways, merging when CI passes.\n. Just 2 relatively minor issues. LGTM after those. Working on a meme.\n. \n. \n. I wound up just silencing the not found errors since they were the main offenders that confused people. No other errors have been causing confusion so I suspect when those errors do show up they're useful.\n. Hey @erikreppel this is due to some go libraries being updated. I think to fix this all you have to do is:\nshell\ngo get -u google.golang.org/grpc\ngo get -u github.com/golang/protobuf\ngo get -u go.pedge.io/proto/...\n. @erikreppel did that make the issue go away?\n. @sjezewski thinking about this more is there really any point? Whatever we accomplish by requesting the new version of the docs will be accomplished by the first user who requests the docs. Which is also the first time the fact that we requested them could be seen by someone.\n. \n. @ShengjieLuo glad it worked!! Closing.\n. Just did a read through on the code and left a few comments on things that stood out to me. It's totally reasonable if the code isn't supposed to be fixing the things I commented on yet. Just wanted to give thoughts early and often.\n. Left a few comments but nothing major, by and large this lgtm.\n. \n. Hi @jnevin sorry you hit this.\nKubernetes uses that directory to store it's internal state. I think you just need to add /var/lib/ to that list of shareable paths and it'll work.\nAlso if you're looking for the path of least resistance you might want to look into Minikube I believe it can work with xhyve similar to Docker for Mac and shouldn't have issues like this. I haven't tested it personally though.\n. Oh interesting, this is probably because we mark the directories as read only right?\nAssuming that's true I'm not sure what we should do about this. Marking the directories as read-only makes sense while they're under the mount since you actually can't write / delete them. I didn't realize that would be carried over with cp though.\n. \n. The fruit_stand image gets built when you do make docker-build.\nIt's built from the Dockerfile you mentioned.\nThere's two methods for establishing the container in that you can specify it or leave it blank in which case the default pachyderm/job_shim is used. There's nothing special about the map.go file, it's just a file that gets put in a container, it could just as easily be written in ruby.\nHope this clears things up.\n. Also it doesn't look there's something actionable here to fix so I'm going to close this issue. Feel free to keep using it for questions though.\n. \n. @JonathanFraser real sorry you hit this. My hypothesis on this is that it's related to the pfs state locking, we've had issues like this in the past although hadn't seen them in a while.\nI think this issue is likely to be fixed by #641, however we should also expand our test suite to include lots more deletions and recreations of pipelines and repos.\n. You know I believe it can be.\n. @derekchiang whenever you have a chance this is in need of review\n. Sure, this pr adds a few extra features to our API. The main one is that pps rather than incremental being a boolean it's become an enum with 3 values NONE DIFF and FULL. NONE and DIFF correspond to true and false before. FULL is a new behavior in which files that have nonzero diffs are shown in their entirety.\nThis also required adding a FullFile flag to file operations in the PFS api which means that you want to see FullFiles if there's any diff. It's part of a new DiffMethod object.\n. Not scheduling this for a milestone yet because this hasn't been released publicly yet.\n. This was already fixed, in master it returns an error \"commit  not found in repo \". That seems reasonable to me, so closing.\n. Hi @ShengjieLuo, sry you ran into this.\nThe reason your Cmd isn't working is that it's not quite as smart as a shell. It's just an exec. To get shell like semantics you should do something like in the fruit stand example.\nMaybe we should add an Env section to the Transform to make that experience a little better?\nAlso small github tip. you can do extended code snippets with 3 back ticks ``\n. Hi @ShengjieLuo really sorry you hit this. We already have an issue that's scheduled for the 1.2 release, #576. I'm going to close this one in favor of that. To clean up the existing mounts you can doumount ~/pfs`. Please feel free to keep asking questions in this issue even though it's closed.\n. In master.\n. Closing in favor of #559.\n. As @JoeyZwicker there's no way to modify a commit after it has been finished. This is an intentional design decision, it's important that the immutability of commits be a strict guarantee so that we have a consistent record of what the data looked like when during processing.\nI'm going to close this since there's no code modifications needed to fix it. Feel free to keep asking questions in this thread though.\n. @sjezewski whenever you have a chance.\n. Alright this now has tests for secrets as well as an implementation for them.\nReady for another look I think.\n. @sjezewski another look? I think this is ready to go.\n. Really just a meme I guess, all I did was add it to docs.\n. Meme lgtm.\n. Our Makefile does use git in a few places for the build process.\nWhat command are you getting the error messages in response to?\n. Hmm, so it finally exited the Pending state is that correct?\nWhat sort of hardware is this running on? I have seen ContainerCreation slow down sometimes if the system is running out of resources.\n. @ShengjieLuo I think there's a lot more mounts going on here than you realize. Each time a pod runs the secrets volume gets mounted along with a mount for pfs so that's at least 2 mounts per pod. Since the pods are restarting the pods are getting recreated several times leading to even more mounts.\n. This is a pretty lulz bug right here. I think the behavior there was technically correct.\n\n. @ShengjieLuo what version of kubernetes are you using. I just had some issues getting pachyderm to work on 1.3 and had to downgrade to 1.2. Also could you grab logs from the server? make logs from within our repo will grab them for you.\n. I think right now it's got to be 1.2 due to the pending bugs. Are you proxying port 8080? What does kubectl version get you? You might need to downgrade it. What's in ~/.kube/config?\n. Is the port proxied or Docker running on localhost?\n. That seems like a likely cause of the problem. What do logs from the apiserver say?\n. Proxying the port is something you have to do if you're using docker-machine but you're not so that shouldn't be an issue. I'm not familiar with the kubernetes issue unfortunately. I'd try the k8s slack channel they're normally quite helpful.\n. @ShengjieLuo I can confirm that I'm currently running Kubernetes 1.2.2 on Ubuntu 14.04 without any issues. I'm using a GCE VM via docker-machine and deploying kubernetes using docker. I haven't encountered any issues with it.\n. Glad that worked.\nClosing.\n. Yeah that is an A+ meme you just blew on a pretty trivial patch.\n. Example has been merged!!\n. @SoheilSalehian this is awesome. Thanks so much for the pull. Will work on getting this merged in next week. Only thing I want to figure out is some other way to host the data, adding 10MB to ever clone doesn't sit too well with me. I think there are some other good options we can explore though.\n. Ah I see, I think you inadvertently included a few compiled binaries in the PR. Could we get those purged from the history for similar bloat reasons. Other than that this lgtm @JoeyZwicker could you do a pass on content?\n. All looks good to me. I did a git filter-branch to remove the binary files from the commit history so git won't store the objects. That means I need to merge this in from the CLI though.\n. Alright, this is in master. Closing the PR.\nThanks again for adding an awesome example @SoheilSalehian.\n. This pr is now completely out of date and not relevant. Closing.\n. \n. We should use either mixpanel or heap since we're already using those for this type of thing. No need to bring a third technology into the mix.\n. Oh I didn't realize it was builtin support. If that's the easiest route we should just do that.\n. Also includes some cleanup of the job-shim that I did while I was here.\n. @derekchiang whenever you have a chance. This is probably ready for a review.\nSummary of changes:\n- The high level change here is that pipelines can now be update. It uses the same rpc as CreatePipeline it just adds a flag Update. This made the implementation easier since the rest of the API is identical.\n- When a pipeline is updated its previous output is Archived this prevents those commits from showing up in ListCommit and FlushCommit but the data can still be accessed.\n- To get the synchronization of updating the pipeline and archiving the commits right I had to add StopPipeline and StartPipeline methods.\n. 2 things that aren't implemented here:\n- commit archiving isn't persisted because the way our persistence works we only persist commits once and archiving happens after the first persistence. This should be trivial to fix post pfs refactor though so it doesn't seem worth it to fix it in the current system.\n- Certain operations shouldn't archive commits but do (such as changing the parallelism of a pipeline) there's currently a flag --no-archive that will prevent this but users shouldn't have to figure out if they want to archive or not.\n. I fixed all of your inline comments. I'm a little confused by your longer comment though. I think you've missed the point of what archiving a commit is supposed to mean and are treating it like a synonym for delete which it isn't.\n\nSome of these I do think we have coverage for already based on your tests (e.g. FlushCommit(), GetFile(), PutFile()). And some of the coverage I'm asking for is a bit redundant (quite a lot of things depend on the behavior of inspectFile() for instance) ... but I think still make for good end-end tests:\n\ninspectFile isn't changed by this PR so the things that depend on it will have exactly the same behavior as before. Theoretically we could test that adding this flag didn't change this behavior but I'm kind of willing to take that one on faith since the function didn't change.\n\nInspectCommit() should report the right size (it should include only the diffs that are not archived)\n\nSo if I understand correctly this would just mean that if a commit is archived we report 0 for its size rather than the actual size that it's taking up. To me that doesn't seem like the behavior we want. Also it shouldn't be possible for some diffs to be archived while others aren't, and with the pfs refactor that will be enforced.\n\nListCommit() should list only non archived commits when the 'normal' type is specified (you test the 'all' case but I didn't see the 'normal' case ... maybe I missed it)\n\nhttps://github.com/pachyderm/pachyderm/pull/694/files#diff-169b88a58cc85335a06d29c2e7678bc9R966\n\nInspectFile() should only list files that are created by non archived commits, and report the right size\nListFile() should only list files from non archived commits\n\nThis would defeat the purpose of archiving. If we wanted to make the data in them inaccessible we would just delete them.\n\nListRepo() should report correct sizes omitting the sizes from archived commits same for InspectRepo()\n\nThis one's kind of interesting, on the one hand it's a bit weird if you have a huge repo and then you do ListCommit and see nothing. But on the other hand I think having the repos report honestly how much space is actually being used to store the repo is important. So I think what we're doing now counting the archived commits as part of the repos size is the right thing to do.\n. @sjezewski another quick look when you have a chance to make sure you're satisfied with everything.\n. Exposing the commands seems reasonable to me. I'll add that before I merge. The example you describe is a bit confusing in that you have commits that are archived while their parents aren't. In terms of GetFile behavior though it doesn't have any effect because archive is supposed to leave things in a gettable state.\n. This actually errors in master, closing.\n. Turns out this was never a bug, I have a test in CR that would reproduce it if it existed.\n. @derekchiang could you review?\n. Hi @ShengjieLuo afaict everything you're seeing here is expected behavior. Our performance isn't as good as we'd like it to be but the wordcount pipeline is designed to do a lot of work to stress the system. We'd like this particulary job to be quicker for sure but we certainly don't consider jobs that last 30 minutes to be inherently too long for a data processing platform, people regularly do jobs that last this long (and much longer) on Hadoop.\nThe directory error messages are harmless as well, they indicate that the system is checking for the existence of files before it attempts to write them and finding that they aren't there which is what's supposed to happen.\nClosing since I don't think there's anything to fix here.\n. @derekchiang whenever you have a second. This one should be pretty quick.\n. Hmm, I'm thinking the most likely culprit here is filesystem latency. If I understand your tests correctly the local version is using a local filesystem while the version in amazon is using s3. That means that the reduce stage needs to do a lot of writes in s3, 1 per word that it counts which would be pretty slow. In the future this will be optimized so that those writes can be packed into a single s3 write which should greatly increase the speed.\n. @ShengjieLuo glad this worked for you and sorry we didn't mention it as an option earlier. kubectl port-forward was broken for a while so it wasn't something we used internally.\nStarting in 1.2 you'll be able to do:\n$ pachctl port-forward\nWhich will have the same effect as what you've done above but saves the user having to find a pachd pod. Will of course be documenting that as well.\n. Closing as this seems to be resolved.\n. Hi @ShengjieLuo thanks for the issues this seems like a great change. Btw when you have a change this specific in mind you can submit it as a Pull Request. If you've made the change to a forked copy then I think all you have to do is push it to GH and go to the repo and you'll get a big PR button. As a bonus you get your name in our contributors list \ud83d\ude38.\nIf that's not feasible for some reason we'll happily add this, but latency on this stuff will invariably bet a bit slower than if there's just a big green merge button I can hit.\n. I think a unit test in src/pfs/cmd/ testing some of the meatier functions would go a long way.\n. Test cases:\n- [ ] what happens with deploy and port-forward if kubectl isn't installed?\n- [ ] what happens with port-forward if the pods aren't up yet? What happens if the pod goes away?\n. - [ ] unmount, basic usage and when the mount is degraded in various ways\n- [ ] mount reconnection in flaky environments\n. Scheduling for 1.3 and assigning to @msteffen.\n. lgtm too\n. I think the problem here is just that you're running the job-shim from master against pachd v1.1.0 which isn't compatible. That would explain why it works in other cases. Is it that simple or is it something more complicated?\n. Backwards compatibility between versions is something that we'll be trying to have in the near future. Right now things are still moving pretty quickly and I think it would slow things down a little too much. We'll get there soon though. I'm going to close this unless there's more that should be done. Feel free to reopen if you disagree or keep asking questions here if you need more support.\n. @ShengjieLuo that's the documentation I did read. As you can see all of their commands have --region even if they also use LocationConstraint. That's what I'm worried about.\n. I think we should also update InspectJob such that if BlockState is set it returns if the job is Empty since that's as successful as the job will ever be. Other than that this lgtm.\n. \nExcept that one does simply give a lgtm...\n. @sjezewski let's chat about this one on monday\n. Nice, just going to wait for CI to pass and I'll merge this in. :)\n. lgtm after the one comment\n. Nice lgtm. Feel free to merge when ci passes.\n. @sjezewski if you wouldn't mind making a pass on this\n. K, I think I've addressed all of your comments except for the one about writing a benchmark. Basically writing a benchmark would require 2 things, one is writing a simple golang benchmark which reads and writes files. The other is having a way to deploy the server with and without a cache. I think it'd take a lot of scaffolding to get that working so I'd prefer to get this merged in and then do a bigger push on benchmarking and performance in the next release cycle when we'll write all of the scaffolding code and come up with different workloads. Does that seem reasonable?\n. Could we make the funcs oneliners? 3 lines of vertical space at the top of each function for a log statement seems a bit wasteful to me.\nOther than that lgtm.\n. \n. \n. Note the test is currently skipped so that CI will pass. The test should be unskipped once the pfs refactor is merged.\n. meme lgtm\n. \n. Fixes #757.\n. No worries, I'll generally ping you if it's urgent and otherwise am just assuming you'll get to it when it's convenient.\nI updated the pr description to better reflect the changes.\n. \n. Just restarted the CI run, it looks like it was a spurious failure.\nI think this is probably ready to merge, @ShengjieLuo would you mind doing git merge master and fixing the merge conflict?\n. @ShengjieLuo yup you're right about the CI failure being spurious.\nMerging! Many thanks for the pull :)\n. \n. I think your comments make sense, I'm just going to remove the make task and the new versioned manifest. It really doesn't make sense as is because the versioned manifest has port information for pprof but the 1.1 server doesn't expose it.\n. @sjezewski another look when you have a chance\n. Or I guess you already said LGTM excepting the version stuff which I removed so I can probably just merge this.\n. Perfect meme, did you make this for the PR or somehow find one this perfect?\n. \n. Lol\n. Scheduling this for 1.3. And assigning to @sjezewski since he's working on it.\n. \n(as soon as CI passes)\n. Does this implement the logic behind the API or just the API change?\n. This all lgtm so far.\n. Hey @statik sorry you hit this. We made some updates to the pipeline spec and the version of the fruitstand pipeline in master isn't currently compatible with the v1.1.0. To fix this you first need to delete the filter pipeline like so:\npachctl delete-pipeline filter\npachctl delete-repo filter\nThen to make it work:\ngit checkout v1.1.0\npachctl create-pipeline -f examples/fruit_stand/pipeline.json\n. Glad that worked! :)\n. The test I'm changing here exhibits some intermittent failures, this should make it more reliable.\n. This is fixed by pr #735.\n. Fixed in master.\n. Few minor nitpicks but after those LGTM.\n. \n. Meme is of high quality... but will the CI pass that is the question\n. Hmm, my guess would be that a version mismatch between pachctl and pachd is to blame. The mounting process pachctl mount ... should have some logs about the input/output error that would reveal more. To fix try:\ngit checkout v1.1.0\nmake install\nThen try doing the get-file again, hopefully that'll work.\nSorry you hit this, we're going to be adding some measures to maintain backward compatibility so people don't hit this soon.\n. Awesome, btw instead of modifying etc/kube/pachyderm.json you can just use etc/kube/pachyderm-versioned.json\n. Here's a proposal for how to fix this and probably a few other issues with job execution.\nRight now the way a job gets executed is that we create a k8s job, which in term creates a set of workers running the user's code. These workers each make a call to StartJob which gives them back some work to do (input data to mount and a cmd to run), when the cmd finishes they call FinishJob and report success or failure. If any of them fail we report the job as failed. Instead of this model I think we should start think in terms of leases, that is when we give a pod some data to work on we can at a later point declare that pod as failed, take back the work and give it to another pod. Here's a sketch of the implementation:\nPod:\n- On startup pods behave much the same, they boot up, they call StartJob and pachd responds with some work to do, at this point the work has been leased to the pod.\n- Unlike the current implementation, while the pod is running it sends periodic heartbeats to pachd so it knows the work is still being done. Let's call those ContinueJob.\n- The response to ContinueJob will normally be empty which means the pod should keep doing what it's doing.\n- The response to ContinueJob might contain a new lease, in which case the pod abandons its old lease and starts working on the new one.\n- When the pod finishes a piece of work it still calls FinishJob. The response will either contain a new lease, in which case it starts processing it, or be empty in which case the pod exits with return code 0.\nServer:\n- The server now needs to keep track of the leases that it has handed out and who's currently working on them.\n- If a pod hasn't pulsed us with ContinueJob for too long then we reclaim the lease and give it to the next available pod.\nI think this will give us a good way to solve a few other issues we've had. For example straggler detection: In this system straggler detection, from the perspective of the server, looks like a single pod that's still processing (sending ContinueJobs) while all the other pods have called FinishJob. We could solve it by timing out a lease when it's been running for too long and splitting the work between the idle pods.\n. @derekchiang another look when you have a chance.\n. \n. @msteffen whenever you have a chance\n. This meme, pure class\n. Yeah, the container package isn't imported by anything. I think it can safely be removed.\n. Definitely like this idea. Here's how I'd imagine implementing this.\nStep 1, add a cache to our fuse implementation. It should be able to cache actual files (the results of GetFile) and also the results of meta queries (ListFile et al). The latter is probably much more important since reading a file normally requires several meta ops followed by a single GetFile.\nStep 2, make it so that the fuse layer proactively tries to fill the cache using whatever information it has handy to try to fill it with things the user is likely to request next. A couple of thoughts on this algorithm.\n- I'd prefer not to have a prefetch flag if we can avoid it but instead make this happen transparently for users. I'm worried about having too many performance nobs for users to turn making it too confusing. In this case I think an oblivious algorithm should work really well if it follows the simple rule that as long as the job is still running, the cache isn't and there's data the job could read that isn't cached it should attempt to read whatever data it thinks the job is likely to need in the future. If the job winds up finishing without needing that data then we wasted some bandwidth and memory which seems fine.\n- Likely this algorithm should start by requesting all of the meta ops, listing out all the files in the commits the job is processing since those add a lot of round trips. Actually it would be even better if we could start computing this information in CreateJob and have it ready for the worker pods once they come online.\n. Glad that worked!\n. Glad that worked!\n. I think you guys are almost certainly right that the bottle neck is the fuse filesystem layer (and all the stuff behind it). It'd be hard for it to be anything else since the only other thing is the process running in a container which I'm pretty confident shouldn't be any slower than in your tests. #777 should do a lot to ameliorate this.\n. @msteffen whenever you get a chance\n. @derekchiang whenever you have a chance this could use a look\n. @ShengjieLuo your diagnosis is correct I believe. The issue is that the image is only present on the k8s master node which might not be the same docker host that the pod gets scheduled on, thus it can't find the image and needs to pull it but that fails too because the image isn't available in a registry.\nProbably the easiest to fix this is to use a registry, to do that you'd do:\ndocker tag wordcount-map <your-name>/wordcount-map\ndocker push <your-name>/wordcount-map\nThen change the manifest to point to <your-name>/wordcount-map\n. Hey @rexmortus, as @JoeyZwicker mentioned this is a version mismatch issue. Probably the easiest way for you to get pachyderm up and running would be to deploy v1.1.0. To do that all you need to do is:\n$ git checkout v1.1.0\n$ make launch\nSorry you ran into this, it should get a lot better starting with v1.2.0.\n. I'll take a look, in the future when you want someone to look at a pr add them as an assignee. I'm used to just scanning that column for my face to figure out what to do.\n. \n. Hi @alexsbromberg, sorry you ran into this. Definitely doesn't sound like the behavior I'd expect either. Would you mind gathering a bit more information for me about which jobs run? You can do this with: pachctl list-job which will show you all the jobs that have run. Then for each of those do pachctl inspect-job <job-id> which will give extended information about the jobs and what they're running on.\n. Thanks for that, was able to reproduce your issue with a test here: https://github.com/pachyderm/pachyderm/pull/805. Will take a crack at fixing the issue tomorrow.\n. Just tried reproducing on the pfs-refactor branch. It still fails but differently than on the the previous pfs. Rather than triggering a job that can only see data from 1 repo it doesn't trigger a job at all.\n@derekchiang could you try running the test on the pfs refactor branch when you get a chance and see if you can understand the behavior?\n. :rocket:! What was the issue?\n. Fixes #794\n. @derekchiang close this? Or is there still more you want to do?\n. Do you think we could make things simpler and just use the rethink server that's spun up by make launch? That already has readiness checking. And it has the really nice advantage that it's guaranteed to be deployed the same way as you get from the manifest. So we'll avoid issues resulting from things like different versions of rethink being spun up?\n. Readiness check should probably keep going indefinitely.\nWe also might want to consider removing it, at least for non-dev clusters, it seems like it's an added liability to a functional system coming up and we already have a pretty effective readiness check for pachd which can't pass unless rethink has also come up correctly.\n. Do our tests depend on the readiness check?\n. @sambooo yeah, the intention of putting ADDRESS=0.0.0.0:30650 is to indicate the default value. I can see how that's confusing though seeing as that line right there is the one thing you'd never want to actually type. I'd change to something like ADDRESS, the server to connect to, defaults to 0.0.0.0:30650.\nMight as well fix the type while you're at it, although the correct spelling is actually Environment isn't it?\n. Give a man a meme and he'll meme for a day.\nBut teach a man to meme and he'll meme forever.\n(Sorry for the gendered language, it sounded right that way).\n. Hi @alexsbromberg, sorry you hit this. The way you were trying to make a branch is supposed to work but I think you hit a bug in pfs. We're about to merge in a big refactor of pfs that we've been working on that I'm hoping will fix this issue because it handles branches in a different and much more consistent way. I'm going to add this issue to the 1.2 to milestone so that we remember to check that it works before shipping 1.2.\n. So should this be closed?\n. This is an interesting question that will probably require extending our pipeline semantics a bit to support. There are a few things I'd like some clarification on so I can better understand the use case.\n- You mention that to compute a rolling time window you'd want to have access to the whole data history, this would mean that the job logic would have to filter out some of the data that it's shown right? Would it possibly be simpler if pachyderm only showed the job data that's part of the sliding window?\n- You mention wanting output commits from successive runs of the sliding window computation to be parented on each other. Parenting makes sense for datasets that are built incrementally. That is if results from the first run of the job could still be relevant output for the 10th run of the job. Is that true of sliding window computations? My impression was that the output of each run is distinct since it considers a distinct \"slice\" of the input?\n. Storing them under version control is definitely a good pattern, that was a big part of our rational for having a simple text format for pipelines. I like to structure things like this:\nproject/\n   pipeline.json # pipeline file\n   Dockerfile\n   <other things the Dockerfile uses>\nIf I need more than one docker image I put each of them in top level directories.\n. Yeah, project would correspond to a git repo, and have a name that makes sense to you. So for example if I'm working on analyzing political speeches my git repo might be called speech-analysis.\nI like to mount the pachyderm filesystem on ~/pfs, so in a different folder.\nHaving many pipelines / pachyderm repo in the same git-repo is probably the way to go. You normally need several pachyderm repos and pipelines to do something interesting and keeping them all in separate repos would quickly get unwieldy. Especially because the pipelines need to match, in the sense that what one pipeline outputs others must be able to consume. That could get tricky if things are spread across multiple repos.\n. I like this idea in theory, I'm a bit worried that the performance of a travis box is so variable that it will be hard to separate the single from the noise when looking at CI results.\nBut given that the cost is adding a line to our travis file I can't see a reason not to do it.\n. Does a post-job task run and get reported but not block the results from CI coming back? That would be perfect for our needs :)\n. Even tho we're not merging this... it still LGTM.\n. I think there's 2 things worth doing here.\n- The best pattern for unmounting is to just CTRL-C the mounting process. In 1.1 pachctl mount would just silently block, what if in 1.2 we made it print out \"Filesystem mounted, CTRL-C to unmount.\" This would nudge people toward what I think is the most intuitive control scheme.\n- We could also add a command pachctl unmount (maybe umount will be an alias) which unmounts existing pfs mounts. It seems like between these 2 things most users will be able to figure out how to unmount.\nDo people feel like that would address the concerns in this issue?\nMounting pfs to multiple locations is currently possible and I think it's reasonable to allow that because not all pfs mounts are necessarily the same. pachctl mount accepts a variety of flags and it's possible to have 2 different pfs mounts that include completely different data sets, have different caching properties etc.\n. Also for a little history on where the term umount comes from check here: http://unix.stackexchange.com/questions/9832/why-is-umount-not-spelled-unmount. Kind of fun read.\n. I'm down with a --daemonize flag in theory. However from what I understand it's actually nontrivial to do this in go (fork doesn't work). So we'll need to see what our options are, it looks like there is some hope though: https://github.com/sevlyar/go-daemon\n. This all looks reasonable to me. That \"error\" is actually total benign. Before the OS attempts to write a file it checks to see if the file exists to make sure it's not a directory or something that would cause the write to fail. NotFound gets logged as an error but in this context that \"error\" means you can proceed with the write. In 1.2 we've silenced this particular error because it confuses people.\nSo that's probably not why it's failing, let's see if we can figure out why that is. Grep has the somewhat annoying behavior that if it doesn't find any matches it returns with error code 1 is it possible that, that's what's happening here?\n. Looking at grep's docs here it seems a return code of 1 means that no lines were found. You probably just want to silently swallow this error since outputting nothing is a reasonable thing to do in that case. You can do this by adding \"acceptReturnCode\": [1] to your transform.\nIf that grep command finds stuff on your local setup but not in the cluster then maybe there's a parsing issue that's going on. What happens if you run a pipeline with:\n\"transform\": {\n    \"cmd\": [ \"grep\", \"Places of Worship\", \"/pfs/landmarks_and_places_of_interest/data\" ],\n  },\nOr maybe just try:\n\"transform\": {\n    \"cmd\": [ \"cat\", \"/pfs/landmarks_and_places_of_interest/data\" ],\n  },\nas a sanity check.\nNeither of these commands will output anything since they don't write to /pfs/out. But their logs should help you with debugging.\n. Thanks! We appreciate your patience with our rough edges.\nWe have pachctl update-pipeline for that, it works just like create-pipeline but the pipeline can already exist :). It's not present in 1.1 but it's in master and is going to ship in 1.2 in a week or so.\n. @sjezewski whenever you have a chance this could use a look.\n. False alarm, I had an old version of the rethink database running with out of data tables and indexes.\n. \n. Nah, it's \"Good to me looks,\" still technically the same meaning.\n. \n. \n. @msteffen whenever you have a chance.\n. Yeah, 100% agree. These were all made in a panic as I was trying to get the demo working in time for the talk. I considered splitting them up but it seemed small enough that it wasn't really worth it.\n. \n. All looks good to me.\nRegarding testing that Kubernetes actually creates the requested number of workers.\nYou could test that by actually scheduling a job and then using the kubernetes api client to list all of the pods that were created as part of the job.\nI don't think it's necessary to do that, we have a few other tests that measure how many k8s pods get scheduled so I think it's reasonable that we trust the k8s api to schedule jobs with the parallelism we tell it to.\n\n. Hi @frstie sorry, you hit this. We're in a bit of flux right now regarding versioning as we gear up for the 1.2 release. I think your quickest path to a working cluster would be to use the 1.2.0 release candidate.\nIf you do:\nbrew untap pachyderm/tap && brew tap pachyderm/tap && brew unlink pachctl && brew install pachctl\nThat should get you a 1.2.0 version of pachctl.\nThen you can just do:\npachctl deploy\nand it'll deploy a matching version of pachd.\n. Fixed via #995 \n. This is fixed in master.\n. This is confirmed to work in master.\n. @derekchiang whenever you get a chance\n. Hi @fjukstad, many thanks for the pull. This all looks good to me, as soon as you sign our CLA here: https://pachyderm.wufoo.com/forms/pachyderm-contributor-license-agreement/ I can get this merged in!\n. Merged! Thanks again for taking the time to contribute.\n. More importantly, the test that @alexsbromberg added to reproduce this issue now passes and is part of CI. So closing seems quite reasonable to me.. @derekchiang good catch. Looks like the PR that fixes this has failing CI right now. It'd be nice to get this fixed in time for 1.3. @derekchiang do you think that's possible?. Is anyone working on this for 1.2?\n. Hmm, that doesn't look like omitting results though. Are we sure that's the same bug or maybe something different?\n. Can you pull logs to see what the error fuse got was?\n. Did you try blowing away /tmp/pach?\n. Anyone working on this for 1.2?\n. I think rolling back might make more sense than waiting to move HEAD the reason being that the common workflow here is to do:\nStartCommit(\"foo\", \"master\")\nPutFile(\"foo\", \"master\", \"file\")\nFinishCommit(\"foo\", \"master\")\nFor PutFile and FinishCommit to work in that way HEAD has to point to the newly created but not yet finished commit.\nRolling back seems pretty reasonable to me in terms of semantics, basically it would give us the invariant that cancelled commits are confined to their own branches. The only thing that seems a bit dicy is that we're kinda renaming commits, so when master/2 gets cancelled we'll want to reclaim the name master/2 so a non cancelled commit can use it. @derekchiang how hard do you think it'd be to get that working with the current pfs?\nAnother question: should the system even let people create commits when the parent is cancelled? It doesn't seem like there's much legit use for that since the data in there will still be somewhat incoherent due to the parent being cancelled.\n. Added 2 of those. It already had something about new parallelization features. GF case study seems weird to me since it's not part of the repo. Maybe I should put a blurb about it in the release post though?\nI like the contributors idea but I think maybe it'd make more sense to just have a CONTRIBUTORS.md file for that.\n. I agree in principle but am feeling too lazy to get them all together right now. We'll add contributors in the next one.\n. Is this about ads as in advertisements? Or something different?\n. \n. Anyone got bandwidth for this?\n. Fixed via #1031.\n. \n. @msteffen could you update doc/development/pipeline_spec.md to reflect that it has to be caps too?\n. I can't claim to have read it all but lgtm.\n. This still seems a little fragile but let's just get this merged for now. LGTM\n. Me too :)\n. I think that's fine, wordcount is a more advanced example so people doing that should hopefully be comfortable with the idea of a single pipeline file that includes multiple steps.\n. Can this be closed? I think the magic of rm /tmp/pach is just that you were assuming that make launch blew away database state when it actually doesn't. So it makes perfect sense that schema changes would require rm /tmp/pach\n. \n. I agree, although I think this should be an automated part of tests rather than something that we do manually.\n. I'm going to close this since it's pretty old and we have a number of specific issues about new k8s features we want to use.. This issue is a lot more about the second part than the first. Being able to specify your images directly from a git repo. That's a feature I'd still like to find a way to implement if possible although it's unlikely to make 1.3 so I'm going to reopen this and move it out of the milestone. The fact that internal registries aren't usable makes this tough. Likely we'll have to give users a way to specify which registry they want to use in the cluster and specify credentials for it.\n\nI dont think it was because an internal registry wasn't useful, but that\nsecurity for it is annoying enough right now that it's not really a viable\noption.\n\nThis is nonsensical it wasn't viable precisely because it wasn't useful. If it'd been useful then more or less by definition it would have been viable.. Adding to 1.3 milestone.\n. Moving this to 1.3 milestone.\n. @derekchiang you're actively working on this right? I'm adding you as the assignee assuming that's true.\n. I agree with most of this but I don't like the idea of having a single cluster for the company that needs to be maintained. It seems too much like a big pet and it's a serious pain to have to check with others before running your benchmarks. How hard would it be to make it so that cluster turn up and turn down is automated to the point where people can just create a new cluster when they want to start benchmarking? It seems like that would make things easier. Limiting ourselves to GCE for now will probably make this a bit easier since they have GKE which should make spinning up a cluster pretty painless.\n. @sean recording this stuff is definitely important but I think most of this stuff should be recorded by git rather than another system. Given a commit hash that a benchmark was run at I should be reconstruct:\n- the images, by running make docker-build and seeing what I get\n- the pipeline manifests, since all the assets for running benchmarks should be under version control. Or other code needed for the benchmark.\n- The benchmark cluster, since the scripts used for cluster turn up and turn down should all be under version control as well.\nRecording the results of the metrics does make sense, I think we could write a simple reporting tool that just gave you plain text output with the runtime and other metrics we care about. These reports would provide all the context one needs to understand them and would be pretty easy to send to each other over slack, store in git or on the web if we wanted a record of them. Etc.\n. Yeah, I think what we'll want to do is develop a standard set of benchmarks and a standard set of environments. So our benchmarks might be 10G-wordcount, 100G-wordcount and 1TB-wordcount and our environments might be 10Node-n1-standard-16, 100Node-n1-standard-16 and 1000Node-n1-standard-16. Then we could talk about the performance of 100G-wordcount on 10Node-n1-standard-16 and everyone would be on the same page pretty quickly.\n. Also cost is worth keeping in mind, right now we still have quite a few GCP credits so you can just go nuts. But those are going to run out soon and then we'll need to start thinking about managing our compute instances because it can add up quite quickly. That's probably another argument for making cluster turn up and turn down as painless as possible. It'll give people less reason to keep clusters running over night when they're not using them.\n. Sounds pretty reasonable.\nGo benchmarks actually have built in iterations exposed on the testing.B object as a field called N. You should look into how that works before implementing stuff for yourself.\n. I'm going to take a swing at this one since it opens up some new use cases and I think it'll be nice to have as much of the release cycle as possible to play with them and see what works. Starting with a design doc:\nThe first step in getting this working is taking the fuse specific messages that describe the set of mounted commits and making those work with an APIClient instead. The message in question is:\nmessage CommitMount {\n    pfs.Commit commit = 1;\n    pfs.DiffMethod diff_method = 2;\n    bool full_file = 3;\n    string alias = 4;\n    pfs.Shard shard = 5;\n}\nI'm going to rename this to CommitView and relocate it into src/client/pfs.proto since it's no longer fuse specific. Fuse can still use it in the same way. APIClient can also use it to create a limited view of pfs in which only some repos and commits are visible. Furthermore I'll make it so that the constructor NewInCluster looks for serialized CommitViews somewhere in the filesystem which will allow us to inject views using kubernetes.\nI think that'll give us everything we need to get a working example of a fuse free job. There are still a couple of open questions though:\n- Should the API allow users to specify if jobs are fuse based or API based or should we expose both and allow jobs to use whichever one they want?\n- Should views be a client side or a server side thing? I see downsides to both. If they're clientside it means that implementing an APIClient for a new language requires duplicating the View semantics. If they're server side then all Commit and File requests are going to need to have the View object attached to them. For the time being I'm going to stick to client side because that's what we have now for fuse so it's less work. I'll consider switching as I progress on the issue.\n. This didn't make 1.3. With the datum changes we're thinking about I'm less sure of the value of this issue so I'm going to leave it unmilestoned for now.. \n. ArchiveCommit doesn't exist in 1.4 which is exceedingly sensible.. In the process of opening a new issue right now, will close / reference this one when it's up.. Closing in favor of #1108 since that has a plan to get rid of FullFile. Yes, I believe that's the case.\n. I agree in theory that using python for scripting isn't that different from using bash for scripting. But I also see a lot of value in unification so I think the fact that we have a lot of bash scripts is a much stronger argument for more bash than for introducing python.\nLooking at the particulars of your script I think it would be cleaner, both in absolute terms and in terms of fitting in better with our codebase, if it was a combination of go and shell/make.\nA lot of the python functions are just shelling out to subprocesses which is easier to do in shell/make.\nThe regex stuff you're doing to get the manifest you want seems fragile to me. Considering that the file you're modifying with regexes is actually programmatically generated by pachctl I think you'd be better off just extending that programmatic generation to do what you want. It's already pretty close I think and this would have the added benefit that this behavior can now be easily used in other places. For example being able to build an image and push it out into a big cluster seems pretty useful for correctness testing as well as benchmarks.\nLastly I'm a little bit dubious of using the latest tag and a pull policy of Always, that's screwed me so many times in the past that I think it'd be a lot safer to just use explicit tags and bake them into the manifest.\nAll this being said, @sjezewski's point about prototyping being nicer in scripts still stands. I agree with him that when it comes time to merge this it'd be better not to have the python script but feel free to stick to whatever lets you get something working fast for now and deal with this later. You'll probably know a lot more about the problem constraints by then.\n. Merging to master seems fine I guess, since this won't block development.\nStill I feel pretty strongly that keeping our codebase strictly go and shell/make is pretty important so merging stuff like this does make me a little uneasy. How about if we add an issue in the 1.3 milestone to rewrite this?\nAlso @sjezewski has a few outstanding comments you should respond to. Particularly the one about the unused command line arg.\n. Also the downside with Always/:latest is that there's a lot of ways it can confuse you.\nEspecially if someone is updating the Docker repo at the same time as you. Pods getting rescheduled can lead to a cluster with mismatched versions. Also there's a bit of lag between when an image is pushed and when it gets published on Docker Hub. I suspect due to caching. So you may run into situations where you rebuild, redeploy, and wind up with the old version of your code. Very easy to get confused and accidentally think that your performance tweaks did nothing.\nAll of this is avoided if you use a static tag, or maybe even better a hash.\n. Many thanks for the pull. Would you mind signing our CLA and I'll get this merged in as soon as CI is green.\n. A good stop gap measure might be to just put a header at the top of each doc with a link to the doc on RTD saying: \"This documentation is meant to be read at: ____.\"\n. I don't think I agree with this approach to garbage collection.\nGarbage collection should be something that happens transparently when things are deleted, not something that the user has to initiate for themselves.\nThis issue also doesn't contain much in the way of details about what the semantics of pachctl garbage-collect-blocks would be. The local use case isn't so obvious to me. Does it just delete all local blocks regardless of if they're referenced or not or only unreferenced blocks?\nI'm going to close this because we already have #626 and that has more details. Feel free to weigh in on that one.\n. Never has a PR LGTMed like this PR LGTMs.\n\n. I agree that that is heavy handed. Metrics breaking shouldn't prevent users from using the system.\n. @msteffen I think these are very reasonable concerns. In general, I think we should avoid using mixpanel as a debugging tool and try to use more cluster local things for that. Having our processes build up with Mixpanel specific stuff would definitely be risky. Right now everything that gets to mixpanel also gets logged by pachd so you can recover that information by checking logs and you'll have more context with it since you can see what else is being logged around the same time. That will hopefully help to ameliorate this a bit.\nI like the direction of keeping prometheus + logs focussed on things that allow people to understand and debug the cluster's state right now and having mixpanel focussed more on allowing us to understand broadly how it's being used in the wild by people.\n. Job runtimes is definitely something that makes sense to track in Prometheus. They actually have a page on instrumentation that lists common gauges to take and the section for batch jobs lists runtime as one of the gauges you should definitely be taking. I think you're misunderstanding a little how Prometheus relates to time-series, it's not that the data you throw at it needs to be a time series, it can be much less structured than that, it's more that what it exposes to you, when you're querying the data, is time-series based. So you can just throw runtimes at it everytime a job completes, and from that it will be able to give you the average job runtime as a function of time, which is a time-series. That's my understanding at least.. \n. Hi @mwaaas we'd certainly accept a PR adding support for Docker Swarm. However the implementation is somewhat non-trivial and may prove almost impossible. We rely pretty heavily on Kubernetes jobs to implement Pachyderm jobs and it seems like Docker Swarm doesn't yet have an equivalent judging by this issue: https://github.com/docker/docker/issues/23880. There are other things that we rely on Kubernetes for that we may not find alternatives to in Swarm but that's the most obvious one.\nStill I'd be very interested to find out places where this breaks down and even getting pfs working on Swarm (which doesn't require jobs) would be a big step in eventually supporting swarm as a deployment target.\n. Everything Pachyderm runs is a container including the jobs. The cluster management layer is separate from Pachyderm but Pachyderm does require some fairly specific things from its cluster management layer, jobs being an example. Unfortunately just because Kubernetes and Swarm are both containerized cluster management systems they don't necessarily have total feature parity yet.\n. @ukd1 how are you uploading the file? Is it with pachctl put-file? or through the fuse layer?\nAlso is pachctl running on the same machine as the cluster or are they separate?\nPachyderm can also pull in files via http which should be faster because the cluster machine, if it's running on some cloud provider normally has a very fast connection compared to laptops. You can do this with pachctl put-file repo branch path -f http://example.com?\nSorry you ran into this, we're working on hammering out our performance problems in the upcoming release.\n. This lgtmed before and it lgtm now too.\n. My only worry on that would be size, I think for the 2 schemes the sizing looks like this:\n- hash + lower + upper + (delimiter * lines) = 512 + 8 + 8 + (8 * objects)\n- (hash + lower + upper) * lines = (512 + 8 + 8) * objects\nFor large numbers of objects, which I think is what we should be optimizing for, thats a 66x difference which seems pretty meaningful to me.\n\nSo each BlockRef refers to a section with a block, and each delimiter refers to a section within a BlockRef. This strikes me as a little strange.\n\nI 100% agree that this is strange, looking at it now it seems like we actually don't need lower, upper and delimiters we could theoretically roll them all into one.\nproto\nmessage BlockRef {\n  string hash = 1;\n  repeated int64 delimiters;\n}\nThe first value in delimiters would be the index of the first byte of the first object, the second value the first byte of the second object and so on. The last value in delimiters would be the byte after the last byte of the last object, or put another way the first byte of the first object that isn't part of the diff. So if you want lower or upper you just look at delimiters[0] and delimiters[len(delimiters) - 1]. \n\nAlso we talked about this in slack but I just want to state for the record that I don't like the name delimiters here and @derekchiang doesn't either (at least that's what he said). How do people feel about the name Chunks? I feel like we kick that one around a lot in conversation but it's not used by anything in our code.\n. Unless I'm very much mistaken that commit does not fix this issue.\n. Also, those properties are really nice :)\n. Thinking about this issue some more I realized there's one major flaw with the proposal above, it only really works well for append based workloads. If a user changes a single line in a file then that will lead to a completely new block that will be 99% the same. This negates a lot of the benefits of our version control system. It also makes it pretty hard for us to give people semantics they expect from a version control system. In particular the ability to see line based diffs between commits easily. I think those should both be goals of the new system:\nGoals\n\nChanging a single line in a file should result in about the size the new line in extra needed storage\nIt should be easy to show users a line by line diff between different commits.\n\nTo this end I'd like to introduce a slightly different proposal for this issue. It starts with the concept of a Datum which is our smallest unit of data. For line based files this would be a line, for json files a document, etc. In protobuf a Datum would look like this:\nprotobuf\nmessage Datum {\n  string hash = 1;\n  string block = 2;\n  int64 lower = 3;\n  int64 upper = 4;\n}\nIt's pretty similar to what a BlockRef was before but with the major difference being that we have a hash of the datum's content instead of a hash of the block's content. That's a nice thing to have because it means the same datum can be written in other files (or other places in the same file) and we'll be able to detect that it's the same just by hashing. This will also require a new table in rethink for the Datums in which the hash will be the primary key.\nUnder this scheme a Diff would look like this:\nprotobuf\nmessage Diff {\n  map<int64, Datum> insertions = 1;\n  repeated int64 deletions = 2;\n  // along with a bunch of other stuff ofc\n}\nUnder this scheme modifying a line would be result in a Diff like:\nDiff{\n  Insertions: {\n    4: {...}\n  },\n  Deletions: [4],\n}\nReading a file over a range of commits would require pulling out the relevant diffs and combining them, I won't detail the algorithm for combining diffs but I think it should fairly straightforward and we might even be able to find something off the shelf that works. Our diffing algorithm is now similar enough to how git works that I think they're pretty well understood. This also makes it pretty straight forward to render diffs for users.\n. > The metadata-to-data ratio seems really big. Currently we store one Block struct per 8MB of data. Under this proposal, we'd store one Datum struct per datum, which can be as small as a line for normal files.\nVery valid concern. One some level this is inescapable, if we want to have content addressability for Datums we're going to need to store a hash for each one. If we use a 128 bit hash that's 16 bytes, for really short lines that could be as bad as doubling the size. But I think for larger datums this works well and that's likely closer to our sweet spot than lines around 16 bytes in size.\nThe above proposal would require a lot more than 16 bytes per datum because it also requires Diffs to reference 1 Datum at a time which is at least another 16 bytes. Here's what I think we could do to fix that: introduce a Block table, which would also be the place that we keep track of Datum hashes. The schema would look like:\n``` proto\nmessage DatumIndex {\n  string hash;\n  int64 offset;\n}\nmessage Block {\n  string id;\n  repeated DatumIndex datum_indexes;\n}\n```\nThis table would require having a secondary multi-index on the datums present in the block so later writes/reads could find them. Using this scheme Diffs could reference Datums a lot more efficiently. They could still reference a single datum since the Block table would have an index on them. But they could also reference entire blocks by id which could then easily be turned into Datums by joining to the Block table. They could also reference ranges of Datums within a block. I think we could get the data sizes down low enough for this to work.\n\nIf I understand the proposal correctly, the int64 in Diff specifies the position of a Datum. However, this seems to mean that in order to write any data at all, you'd first need to figure out how many datums the file already has (let's say it has N datums). Then you create datums that are N+1, N+2, etc. In other words, it seems like appending datums to a file requires either 1) tracking the number of datums in the file, or 2) read/construct the file to figure out how many datums it has. This is in contrast with the current implementation of PutFile where we don't need to read/reconstruct the original file since we are just appending blocks.\n\nTracking the total number of datums in the file wouldn't be too hard, just an extra int64 in the Diff. That would make it so you could easily append to the end of the file. It would also make deleting the file and completely overwriting it pretty simple since you'd know all the lines you need to remove and then you could just start writing like it's a new file. Those are the two access patterns we support well right now.\nFor more complicated operations, like changing a single line in the middle you'd probably need to reconstruct the Datums so you can figure out which ones changed and which ones didn't. This will be a bit of added overhead while writing the data, but I don't think it's untenable.\nCaching\nSomething that occurred to me while writing this. I think with this scheme we'd be able to have a much more effective caching layer by caching Datums rather than Blocks. It's better because there's going to be less overlap. Right now you could theoretically have 2 blocks that are almost identical except for a few lines but they'll still be separate blocks with separate hashes and thus they'll be separate cache entries and separate entries to s3. This system will deduplicate a lot of that.\n. @JonathanFraser we are basically trading a filesystem tree for a datum tree. This is appealing though because a datum tree gives us much higher granularity for detecting changes to data. For example, a pretty common pattern in Hadoop land is to dump your database into hdfs and have each table correspond to a file. This is possible in Pachyderm right now but it doesn't play very nicely with our version control semantics. If the rows get dumped in a different order, or one of them gets deleted or then the blocks are unlikely to align correctly and we'll wind up storing a separate copy of the data. However if we think in a datum centric way, then each row in the database can be a datum and if that datum is unchanged our system will be able to detect that and we can just reference the previous version of the datum. If people are doing a database dump every night then it's likely that a very small percent of the rows will have been changed, so this technique can actually provide massive space savings.\n. @JonathanFraser not heresy at all I think it's totally reasonable for users to lay their data out like this. However in Pachyderm today this layout won't perform very well because 1 file per row implies 1 block per row which implies 1 s3 object per row. This is going to make having large numbers of rows very expensive since s3 charges per transaction. It's also a lot less efficient in terms of total bandwidth for small objects vs bigger objects.\nThat's really the root problem we're trying to fix in this issue, Blocks define both the granularity at which we content address and the granularity with which we write to object storage. That's bad because you normally want fine granularity for content addressing but coarse granularity for object storage and with the current system you can't have both.\nWhile I agree that your layout for data is totally valid and one that we should support well laying a bunch of rows out in a single file also seems valid to me. Furthermore from talking to users it seems to be something that most users expect our system to be able to do, probably because it's something they're used to from Hadoop. Ultimately I'm hoping we can come to a solution here that supports both workloads well.\n. I'm not sure I see this as quite the fundamental difference between Pachyderm and Hadoop that you do. In my experience and from what I can find online it seems that for the most part in Hadoop files are broken up by line. If users want something other than that they can specify a string used for the delimiter. I didn't find much about more complicated behaviors with injected code, but I agree that, that's way too complicated and not something we want to implement in Pachyderm. Your point that it needs to be possible to indicate delimiters entirely through the filesystem is well taken, right now we let users indicate that with the suffix of their filenames ie, .json indicate delimiting by json objects. I like that because it seems like a pattern that's pretty common and people are used to it. Still I can see how it's weird to have something like a suffix modifying behavior.\nUltimately I think we can consider this issue without worrying too much about delimiters since it's somewhat orthogonal. We want to upgrade the system so that it can pack multiple file modifications into the same s3 object. This change should leave the functionality of delimiters untouched and there's not much in the design that wouldn't need to be implemented if we were completely eliminating the concept of delimiters. Even if a file can't be composed of multiple Datums that were broken apart due to delimiters it can still be composed of multiple Datums that were added in successive commits. We'll need approximately the same query patterns for those 2 cases, since in both you'll need to be able to get the Datums individually or all grouped together.\n. Took an initial crack at a design for this feature, below is a diff of src/client/pfs.proto which adds a new type of message for Objects. This is what I was referring to as Datum above, I actually think the name Object makes more sense because Datum suggests that it should be a single piece of data which based on the discussion in this thread doesn't seem like a good direction, we'll likely want. Object is the name git uses for content addressed blobs so I think it makes sense here.\nHere's the API:\n```\nmessage Object {\n   string hash = 1;\n }\nmessage Block {\n  string id = 1;\n}\nmessage ObjectInfo {\n  Object object = 1;\n  uint64 size_bytes = 2;\n}\nmessage ObjectInfos {\n  repeated ObjectInfo object_infos = 1;\n}\nmessage PutObjectRequest {\n  bytes value = 1;\n}\nmessage GetObjectRequest {\n  Object object = 1;\n}\nmessage InspectObjectRequest {\n  Object object = 1;\n}\nmessage ListObjectRequest {\n}\nmessage DeleteObjectRequest {\n  Object object = 1;\n}\nmessage FlushObjectRequest {\n  repeated Object object = 1;\n}\nmessage ObjectRef {\n  Object object = 1;\n  uint64 offset_bytes = 2;\n}\nmessage PutBlockRequest {\n  Block block = 3;\n  bytes value = 1;\n  repeated ObjectRef object_refs = 4;\n  reserved 2;\n}\nservice BlockAPI {\n  rpc PutObject(stream PutObjectRequest) returns (Object) {}\n  rpc GetObject(GetObjectRequest) returns (stream google.protobuf.BytesValue) {}\n  rpc InspectObject(InspectObjectRequest) returns (ObjectInfo) {}\n  rpc ListObject(ListObjectRequest) returns (ObjectInfos) {}\n  rpc DeleteObject(DeleteObjectRequest) returns (google.protobuf.Empty) {}\n  rpc FlushObject(FlushObjectRequest) returns (google.protobuf.Empty) {}\nrpc PutBlock(stream PutBlockRequest) returns (google.protobuf.Empty) {}\n  rpc GetBlock(GetBlockRequest) returns (stream google.protobuf.BytesValue) {}\n  rpc DeleteBlock(DeleteBlockRequest) returns (google.protobuf.Empty) {}\n  rpc InspectBlock(InspectBlockRequest) returns (BlockInfo) {}\n}\n```\nA few things I want to point out about this API:\n- there's a FlushObject request, this is the key thing that allows us to write a bunch of Objects and have them get grouped together into blocks and serialized to s3. PutObject will not actually record the object in a persistent way until FlushObject is called, we'll be calling FlushObject as part of FinishCommit\n- This removes the notion of BlockRef files will reference Objects instead.\n- Delimiter is removed from the BlockAPI this layer of the API, as per discussion with @JonathanFraser. We won't be completely removing delimiters but this will confine them to the PFS layer where I think they make more sense and where it will be easier to remove them if we decide that's what we want to do.\n. This is how 1.4 works.. Yup I like that habit too. It's nice to be able to scan down the commits and see the red commits followed by the green commits. Unfortunately our CI is flaky enough that just because it's red doesn't necessarily mean you had a failing test... it might have just failed for unknown reasons. But c'est la vie.\n\n. For the 2 that are directed at me there, I think we can just delete them along with the rest of the code in the file. I'd be very surprised if we use the mock client anywhere considering that it's missing some key methods (ie the ones that say // TODO jdoliner).\nThat comment about a huge race has been there for a while, looking at the code it doesn't seem like there's much of a race going on and that code has changed a lot so I think that comment is useless at this point.\n. I think we definitely still want to merge this since there were some pretty fatal errors without the upgrade weren't there. This lgtm since it's almost 100% vendored changes. Going to rerun CI and see if it'll pass.\n. I don't think #1076 covers the same stuff as this PR, it's only updating grpc stuff. Not everything.. Sorry for delay on this one, this lgtm.\n\n. Many thanks for the pull, this all looks good to me, thanks for signing our CLA ahead of time!\n. Sadly, my commit to fix CI broke CI :(\n. @msteffen whenever you have a chance, also would you mind giving a run on your machine. I remember k8s 1.3 worked on some devices and not others so we should test this one too. The upgrade was painless for me this time so I'm hopeful it will be for you too.\n. @jpoon not stupid at all, updating the vendored dependencies seems like a good idea so I'm going to do it. Judging by the fact that CI is passing it seems like k8s backward compatibility is good enough that it's not strictly necessary. But still, it's best to be up-to-date.\n. @jpoon will try to get this merged in by eod today. Just haggling with govendor to get things compiling.\n. @jpoon done! Apologies for the delay.\n. @alexsbromberg wow, awesome pull request thanks so much. Failing tests are incredibly helpful for our development. Merging this is a slightly different process from other PRs because CI isn't going to pass, which is ofc the point of this PR :). Still we can't break CI in master without disrupting development so I think the best thing to do would be to add t.Skip(\"this test fails\") and then merge it in. One of our devs will pick this up and remove that line and then use the test to fix the bug. Seem reasonable?\n. Looks good to me.\nLast step is I need you to sign our CLA and I'll merge this in.\nhttps://pachyderm.wufoo.com/forms/pachyderm-contributor-license-agreement/\n. Responded to all comments except the trailing whitespace one. That one seems obvious to me. I've never heard anyone argue in favor of trailing whitespace and I can't imagine a legitimate purpose for it. Is there one?\n. Linter is go specific but adding a CI step that checks for trailing whitespace seems very easy so I'd definitely be in favor of that.\nI agree it's outside the scope of this PR.\nThis document is really just codifying what we do and don't like style wise, only a small portion of it is actually enforced in anyway and I think our codebase is rife with violations. I'm hoping that over time people will make passes to fix the violations and add CI checks to make sure they stay fixed.\n. \ud83d\udc4d\nNo reason I can think of assuming we can get everything to work, and I'd be surprised if we couldn't. I think a good policy here is to stick to the LTS releases which both 14.04 and 16.04 are and not bother with the intermediate releases since they have shorter support windows.\n. Yup exactly. Basically just exposing that functionality in pipelines.. Hi @bhueth sorry you ran into this. I've reproduced it locally and we should have a fix a ready soon.\n. Hi @bhueth this actually looks like it might be correct behavior. The fruit stand job reads from /pfs/prev to try to find the old value of the pipeline, if this is the first run of the pipeline then /pfs/prev won't exist and it'll get an error which is what you're seeing here. However it handles this error by using 0 in that case since it's performing a sum. The result is you'll see this error in the logs but the job shouldn't fail because of this. Are you seeing a failing status for the job?\n. \nDidn't realize we were so many versions behind.\n. The issue isn't printing things in list-commit that one could obviously work either way. It's that commands like flush-commit need to reference multiple commits in different repos. If you want to play the other side then commands like that are the ones you need to offer another proposal for. The syntax of flush-commit repo1 commit-in-repo1 repo2 commit-in-repo2 seems unpleasant to me since there's this implicit connection between different command line args. Furthermore this is a command that can only function with an even number of arguments and we'll have to give users error messages to that effect which seems kind of confusing.\n. @JoeyZwicker don't get rid of /prev while the behavior is still present in master. We're trying to get in the habit of updating docs as patches that change behavior are merged in so you shouldn't have to worry about that one. Whoever removes /prev will update the docs too.\n. All looks good, many thanks for the pull @jpoon.\nLast step is I need you to sign our CLA and I can get this merged in.\n. Really excited to finally get Azure support :)\nTwo very minor additions:\n- [ ] data disk for etcd, should be identical to the situation for rethink\n- [ ] make target for easily spinning up the cluster, similar to make amazon-cluster\n. In terms of how best to go about adding this support, I'd start with the storage driver. That's probably the most interesting bit. From there we should be able to get something up and running to test and start building to other parts.\n. Yes, Pachyderm currently uses etcd and rethink for different things because they have different consistency models. Unfortunately k8s won't give applications access to the internal etcd cluster because it would be a potential security hole.\nWe're working on getting rid of etcd but currently we still need it.\n. Yup registry is needed.\nSteps needed to get it working would be:\ndocker tag pachyderm/pachd:local jpoon/pachd:local\ndocker push jpoon/pachd:local\npachctl deploy microsoft --dry-run --dev | sed 's/pachyderm\\/pachd/jpoon\\/pachd' | kubectl create -f -\nSorry that's a bit awkward.\n. Yes, you can run them with go test ./src/server.\nThey can be a bit flaky if you try to run them all at once so you may want to play with how much parallelism the tests have.\n. Awesome possum indeed :). Should have the release out early next week.\n. I can't think of why these commits would be archived. Are you sure they're getting archived and not cancelled?\n. @sjezewski that's a reasonable framing of this PR. I don't think that's really going to work generally for PRs though. Just looking at the PRs we have open right now this is the only one that you can really frame like that. It probably fits better in the design stages of a new feature, which this PR should have included, my bad on that one.\n. The make launch-registry stuff was the first initial way I was spinning up the registry, will remove it now that the registry is part of deployment.\nVendored deps are changing because I added in go-dockerclient and it had a few dependencies that needed to be updated.\nTODOS for this pr:\n- [x] remove yaml manifests and make launch-registry target.\n- [x] regen + extend docs\n. I decided not to tackle adding storage to the registry in this PR. Getting that to work correctly on every provider would require a lot of work. And it's unclear exactly what the right way is on each provider since there are a few options. More over I don't think that productionized registry inside the cluster are really the best solution, people are probably better off using things like gcr if they're on gce or EC2 container registry for amazon.\n. This is ready for review.\n. @msteffen 100% agree on that, we recently added a config file for pachctl stuff that would be a great place to put that. Gonna consider it outside the scope though.\n. Bug fix looks good to me. Seems like a few extra commits snuck into this PR though.\n. \n. Cool this all looks reasonable / correct to me.\n. \n. \n. @msteffen did you get a chance to push the image?\n. @jpoon awesome! In terms of automated tests, our existing set of tests should be able to provide pretty good coverage on the code. Of course the difficulty is deploying Pachyderm with access to Azure Storage in CI. We haven't quite figured that out for s3 and GCS either so for the time being it's something we'll just have to live with. That error you saw is one I see from time to time so I believe it to be transient.\nLooks like this is pretty much ready for review and then merge or is there more still to be done?\n. Awesome, we generally prefer small PRs to big ones too so this works great for us. Will dive in and review now.\n. Alright, this all LGTM.\nCLA is already signed so merging.\nWe like to do LGTM with memes, so please see below.\n\n. This is definitely a feature we can consider leveraging but it's worth noting that this feature doesn't necessarily give us what we want. Imagine if we have a kubernetes cluster with 10 nodes and 5 of those nodes have other pods scheduled on them that are consuming a lot of CPU. Kubernetes scheduler takes the CPU constraints of existing pods into account when scheduling so left to its own devices there's a good chance that if we ask it to schedule 10 pods it'll put them on the 5 nodes that aren't being used at all which is actually what we want. If we use affinities then we'll force it to schedule on the 5 nodes that are under heavy load which will be much worse.\nBefore we start messing around with affinities we should look at playing with the CPU and mem quotas and seeing how that affects k8s' scheduling. Affinities actually don't strike me as the right tool for our use case, they're designed to be used if there's some special relationship between pods, ie 2 pods happen to talk to each other a lot so things are more performant if they're colocated. That's not true for us, it's just that we want our pods to evenly use the cluster's CPU/mem/network resources. That use case is exactly what the default k8s scheduler is designed to satisfy though so I'm hopeful that we can get it to work for us.\n\nLike right now, even if you have parallelism N and you have M nodes, and M > N, there's no guarantee that you actually get one pod per node.\n\nThis is definitely something that we should address directly because it's tripped users up in the past. My hunch is that if we expose a CPU shares field in the pipeline spec then by setting that to the right value you'll be able to get 1 pod per Node.\n. > Although this approach can get tricky if you have a heterogeneous cluster.\nI actually think that's one of the advantages of using CPU/MEM quotas. Kubernetes detects how many CPUs and how much memory each node has and uses it to make scheduling decisions. So heterogeneous clusters should work well. Daemon Sets, on the other hand do not seem well suited to a heterogeneous cluster.\n. It'd be tricky to get 1 pod per node in that case but I'd argue that's because 1 pod per node isn't the right goal in a heterogenous cluster. If every job were to schedule 1 pod per node then the 2 cpu node would become the bottle neck for every job. Performance wise your cluster would effectively be a cluster of 2 CPU nodes.\nI'm not advising that we use CPU quotas as a way to force pod spreading, we should have users set their CPU quotas to accurately reflect how many cores their job can utilize and then let k8s make scheduling decisions based on that. There will be times when this doesn't translate into 1 pod per node and that's fine because in many real cases that'll be more efficient.\n. There are a bunch of reasons not to run pachd as a daemon set:\n- Daemon sets are less flexible than rcs, they cannot be updated.\n- They're much more unwieldy, if a user has a 1000 node cluster and they just want to try pachyderm they'll wind up deploying 1000 containers. That takes longer, consumes more resources and will probably annoy people.\n- if it's a heterogeneous cluster or, if one of the nodes is oversaturated users might randomly get very bad performance. That's because by using a daemon set, we force k8s to schedule pachd even in places that it otherwise wouldn't due to cpu/mem constraints. Any clients that connect to those pods will get really bad performance.\n- Replication controllers support horizontal auto-scaling which scales the number of replicas based cpu metrics from the pods. That sounds like a dream setup for an ops team to me.\nAlso I'm not sure if I agree with the premise here, the path of least resistance for connecting to pachd is to type pachctl port-forward & which will automatically connect you to a node that does have a pod running. Furthermore k8s can be made to deploy load-balancers for Services which I think give pretty good performance if users need to do more serious ingress traffic.\nLook into horizontal pod autoscaling. That is a newer k8s feature that I think could help us. Also checkout replica sets, and deployments those are the new primitives that caught my eye as a potential improvement to how we deploy pachd.\n. Had 1 minor suggestion and 1 clarification but this is basically lgtm.\n. \n. @jpoon awesome work again :)\nRegarding running things in a container to work around their being no Azure CLI. That seems like a totally reasonable solution to me. Do you think it'd be possible to add the Dockerfile you used to create the container to this PR under etc/microsoft? That way we can maintain it and understand what's going on under the hood in case anything changes.\nIs this ready for a review pass or still planning to add more?\n. @jpoon this all LGTM. It looks like CI failed, that was an issue we had in master for a short time that's since been fixed, there also seems to be a small merge conflict that crept in. Merging in master should solve both problems if you wouldn't mind doing that. Sorry for the inconvenience.\n. @jpoon thank you!!\n. So with this patch there shouldn't be any need to do port forwarding at all since pachctl will be smart and find k8s nodes to connect to. Is that accurate? The only reason I could think of that maybe user would still want to use port forwarding is security concerns of opening up ports in their cluster. In which case you can still use ssh port-forwarding to a k8s node and then use ADDRESS to point at local host. And once we have a good security story even that will go away.\nCode all LGTM here. I guess we're still waiting on #962 to merge.\n. Even with a load-balancer users normally need to do some sort of proxying to get their laptops talking to kubernetes. Depending on how they deploy pachyderm this process is different so it's pretty difficult for us to have bullet proof documentation that will always get users up and running with a pachyderm cluster. This command was just an attempt to have a more bulletproof way of proxying the port since many users were getting stuck on that step.\nAlso this PR is actually removing that functionality because of how unperformant it was.. We've actually removed the registry for all the reasons you just described. The idea was appealing because we were hoping we could make things more bulletproof to new users who aren't very familiar with Docker and Kubernetes but unfortunately things get too messy with certificates and stuff. So we now have a \"bring your own registry\" policy which I think is the best we can do.\nAlso @msteffen that means this pr should be mergable now.. Let's close this in favor of #573. Reflinking, move and rename will all be fairly similar under the hood I think. At least enough so that the same person implementing them all together makes sense. I updated that issue to reflect that reflink support should be added as part of it.\n. Definitely understand the use case here and think it's a good thing to address.\nI have some questions about the implementation though, afaik it's not possible to return an error and a response from a grpc call. It'll only serialize one of those things. So it seems like the only way to get FlushCommit to behave like this would be to make it never error. That would definitely cause problems with some of the ways we're using FlushCommit internally and also seems like it might be a slightly weird set of semantics. I'm wondering if maybe this issue should be about introducing a different API call that does what we want rather than about extending FlushCommit. Or maybe we can just add a flag to FlushCommit that does what we want. Interested to hear what others think.\n. @brendanacassidy thanks for clarifying, very helpful. I think what you've said totally makes sense. Flush returning should indicate that everything that's going to run due to the commit set has. That fits well with the semantics we already have and shouldn't be too hard to incorporate.\n. The commit-invariants branch adds a make pachd target which just compiles the binary for you.. This has been fixed by #1018.\n. We now have our logging module which can be spun up with make logging in our repo.\nIt deploys a number of manifests to whatever kubernetes cluster it finds with kubectl, adding this into pachctl would make things even easier for users since they wouldn't have to clone our repo to get the stack.. It might make more sense to add a --version and --registry flags instead so you would do\npachctl deploy --version :v1.1-<commit-id> --registry my_registry/jdoliner\nAnd it would deploy my_registry/jdoliner/pachd:v1.1-<commit-id> and my_registry/jdoliner/job-shim:v1.1-<commit-id>. It seems like those are the only pieces of information users are going to want to modify so this would let them do what they want but would decrease the risk of deploying mismatched job-shim and pachd versions which can lead to weird bugs.\n. So one problem I just realized is that --registry already controls whether or not a registry gets deployed so we're going to need to use a different name for one of those. Maybe the other one could be called --cluster-registry.\n. \n. I feel like I have a distinct memory of a time when it was. I feel like I used to be able to do:\nmake clean-launch-dev && make launch-dev\nto my hearts content and it wouldn't leave around an init job. Maybe I just imagined that though?\n. Epic meme :)\n. Confirmed that this is indeed an issue. Thanks for a nice descriptive report @JoeyZwicker. \nThe cuplrit here is the auto reconnect functionality, it was deferring a method to cleanup some things, however the function it's in never returns until the an unmount happens, so memory would build up while the mount was active.\n1003 fixes the issue.\nI also noticed while debugging that our cpu usage for a mount was pretty high, this was the same code it was doing its retries if a very tight loop. I made that loop only run every 5 seconds which fixed the problem. That's in the same pr.\n. Fixed via #1003.\n. Sounds like the right diagnosis. I'd prefer using a static version to hosting our own version. Hosting our own seems like added overhead w/o a huge benefit. And also using a static but external version seems like it's better for pointing users in the right direction for further exploration.\n. I'm fine with this as a temporary measure. My understanding right now is that this test was fairly reliable for a while and started failing recently for legitimate reasons. @sjezewski is working on getting it passing again and making it more resilient to future failures. The issue seemed to be the tensorflow image changing underneath us, I believe the plan is to use a static image reference so that won't happen again. @sjezewski is that all accurate?\n. Oh man, I did not know that there was a Canonical option for encodings, this is awesome. This means no more flipping values in maps right?\n. @msteffen whenever you have a chance.\n. Hi @moadben, really sorry you hit this.\nIn the short term we can get this fixed for you guys by upping the array limit.\nI'll make that change and do a point release for you guys so you can redeploy and not have this error.\nThat of course will only take us so far. In the long term we should switch to doing list-file in a streaming fashion so it won't hit the array limit. That should also be more efficient.\n. A short term fix for this has landed in master and will be released in 1.2.3. For further work on this issue track #1006.\n. What about a temporary meme?\n. @msteffen whenever you have a second\n. \n. \n. \nLove it, can't wait to run some benchmarks on this.\n. It'd probably be better if we just did started time and duration and left out finished. Since you can more or less figure out when it finished from the first 2 numbers. And that would have much nicer resolution for cases like this.\n. Looks a bit over engineered but lgtm.\n\n. @msteffen whenever you have a chance\n. pachctl doesn't have the ability to build a copy of the pachd image, since it doesn't store a copy of our repo in it.\n. @sjezewski whenever you have a chance.\n. > I notice that we removed all the log statements that report timing at the end of the request. What is the reason for that?\nI just removed the duplicates, timing info is still present in the logs it's just you'll only see 1 copy of those messages instead of 2. I agree with your sentiment that that's important info we don't want to lose. This PR shouldn't remove any information from the logs... except for the word pachyderm which we can hopefully all remember ourselves at this point :stuck_out_tongue: \n. LGTM\n. Many thanks for the pull. If you wouldn't mind signing our CLA I can get this merged in.\n. Hey @russelhowe would love to merge this since it's an important fix. Did you get a chance to look at our CLA yet?\n. Looks like the one about sleeping in tests might have slipped through the cracks.\n. \n. Many thanks :)\n. The reason for this change is that without connection pools rethink will start to have connectivity issues when it's put under load.\n. Closing in favor of #710 since that has more info.\nWe do have some tests for pachctl but they are very limited.\n. @derekchiang whenever you have a chance take a look at this.\nI was never able to get TestFlushCommitAfterCreatePipeline to fail locally even though it fails all the time on CI. I've been testing this by repeatedly rerunning CI, it appears to be much more reliable than it normally is and I've yet to see a failure in TestFlushCommitAfterCreatePipeline which is normally a very flaky test.\n. One small question. But not strictly necessary for merge.\n\n. This seems like a solid solution to this problem. Can't think of what we'd do to make it better.\nClosing.. Hi @willguxy, the concept of versioned pipelines is something we've talked about quite a bit internally but haven't yet gotten to a place where we feel like we understand the use case and how to address it well enough to proceed.\nThe workflow you describe here seems pretty compelling to me since it allows people work on a pipeline and see how your work will affect downstream pipelines without disrupting the normal operation of those pipelines. This seems very much within our core concept of collaboration. I'm going to think about this a little more and follow up with a concrete proposal for how we'd implement this.\n. Alright here's our working proposal for this one.\nPipelines will no longer use their names as the primary key, this will allow users to create multiple pipelines with the same name, outputting to the same repo, with different implementations.\nDownstream pipelines will automatically process the results from these new versions of the pipeline since they'll be committed to the same repos they're reading from. There should also be some way to indicate which pipeline is the \"master\" so that it's easy to distinguish from the experimental versions.\nOne open question that come to mind: what should be done about FlushCommit, since we'll now have multiple commits in the same repo with matching provenance. We need some way to \"bless\" the output from the master version of the pipeline so it's the one that gets returned. \n. I like how right now the name of the pipeline and the name of the output repo match each other. It seems like we'll be introducing some potential confusion by allowing them to not match each other. I also don't like encouraging people to do string based versioning, ie things like financial_model_v1, financial_model_joe.\nIt seems like list-pipeline wouldn't change much, except that you'd see multiple pipelines with the same name (but different IDs). We could add the ability to filter pipelines by name, so you could get every version of a specific pipeline. For inspect-pipeline you'd need to reference pipelines by ID rather than by name that doesn't seem so bad. It seems like that would cover all the major workflows people would want or is there something missing?\n\nRegarding the issue with provenance above. I think we can solve it by storing a count to with each provenance entry of the number of pipelines there are between the repos. That way flushes can know how many commits to expect when a commit is written to the upstream repo and can block accordingly.\n. After discussing this a bit more internally we've realized that we'd also like to be able to target a pipeline at specific branches in a repo. To motivate this consider a case in which a user has a repo which contains database dumps, and wants to perform a schema migration on that database. They can start working on building their app on top of the new schema and start dumping into a different branch, call it exp for experimental. This already works in Pachyderm today, but what won't work is that now the old version of the pipeline, which is likely dependent on the old schema will be processing new data with a different schema and will fail. Users want to be able to work on a different version of the pipeline that processes the exp branch of the input repo and outputs to a different branch of the output repo. To support that I think that all we need to do is add a branch field of type string to pps.PipelineInput and pps.Pipeline. The branch field on pps.PipelineInput will indicate the branch you're inputting from while the one on pps.Pipeline indicates the branch you're outputting to. Putting the branch field on pps.Pipeline rather than on PipelineInfo might seem a bit odd but read below for why.\nThere's 1 corner case that needs to be addressed with outputting to specific branches: what happens if there are 2 pipelines outputting to the same branch (of the same repo). The answer is that, that can't work and we should disallow it. The simplest way to do that might be to make the output branch part of the primary key for pipelines so pipelines would be referred to as pipeline/branch and stored in the database under that key so if a user tried to create 2 pipelines outputting to the same branch it would fail. If you're doing that then having the branch be part of the pps.Pipeline field makes total sense. This also resolves the question from earlier in this issue about whether or not we should have a master version of each pipeline, the answer is yes we should and it's simply defined as the version of the pipeline that's outputting to the master branch, of which there can be only one.\nI think these features will make it possible for users to do all the things we want to let them do... but some of them will be unwieldy. To fix that we can add some sugar to make them easier. A couple of unwieldy cases come to mind so I'm going to handle them 1 by 1.\nCase 1\nA user wants to create a new pipeline which takes a repo foo as input, that repo has several branches and the users wants to process all of them exactly the same way. (This, by the way, is the current default behavior of Pipelines.) Based on everything above that means that the user will need to create a pipeline per branch with specs that differ only in the branches the input and output from and to. Pretty annoying. I think we can solve this pretty elegantly by saying that a pipeline can be created with empty branch strings and we will create multiple pipelines from that one request 1 per possible input branch. This will be an opportunity to add branch propagation as well.\nFor example: if their input is a repo foo which has branches master and exp they'll get 2 pipelines 1 reading from foo/master and output to pipeline/master and 1 reading from foo/exp and writing to foo/exp. \nIn the case of multiple inputs repos you'll need to take a cross product. For example if you also have a repo bar with branches master and exp you'll get 4 pipelines likes so:\nfoo/master, bar/master -> pipeline/master\nfoo/exp, bar/master -> pipeline/exp-master\nfoo/master, bar/exp -> pipeline/master-exp\nfoo/exp, bar/exp -> pipeline/exp\nCase 2\nThe user has created a new branch on a repo (or a new version of an existing pipeline which also results in a new branch). If the repo is an input to downstream pipelines they're now in a tricky spot because to get this new branch flowing through their DAG they'll need to create a new copy of each downstream pipeline targetting the new branch. To make this easier I think we can add a new API call:\nproto\nmessage ExtendPipeline {\n  pfs.Repo repo = 1;\n  string branch = 2;\n  repeated Pipeline source = 3;\n}\nThis call will look at all pipelines that take repo as an input and make a copy of the master version of the pipeline which reads from the new branch and outputs to a new branch with a matching name . source can be used to copy different pipelines than the master version, but can also be left blank to only get the default behavior of using the master version.. This is actually not yet all implemented, but with the new commit invariant changes it could be. I think I'm going to leave this one open for now since the use case still seems pretty important to me and like something we'll address soon.. Actually I'm not so sure we're going to have problems with libc. This is what I get from running ldd on the job-shim binary:\nroot@e02feee63aaf:/# ldd /job-shim \n        not a dynamic executable\nThis makes sense since go compiles everything statically. I think as long as job-shim doesn't use cgo this solution will just work.\n. Few changes requested regarding file closing in pull and the name of the directory used to pass job-shim to the user container. After those this lgtm.\n. \nSadly CI is still being annoying.\n. LGTM, just going to wait for CI to pass before merging.\n. @msteffen whenever you have a chance, this PR does 2 things. It removes the registry that we deploy with pachyderm, I think that's the best choice because it's pretty tough to actually use it effectively due to port-forwarding issues and it's not need to make --push-images work out of the box since it uses docker hub by default. It's also pretty easy for users to add their own registries to the cluster since it's a kubernetes add-on, I think that's a better option for them now than having our own built in one.\n. Yeah I think having a registry that we start for the tests could make sense as soon as we have some tests that actually use it. The kubernetes add-on would probably be a good avenue for that though since it's a more standard way of doing it and can be done with just a file. Worst case, if we decide we want the registry back it's in our git history so we can easily resurrect it.\n. @sjezewski could you take a look at this when you have a chance?. I'd still kind of like to merge this but I had a lot of trouble getting CI to pass on it. I'll take another swing now that we've made some CI improvements.. This was closed by a git mishap but it's stale enough that I'm going to leave it closed.. These tests already run in parellel. They could maybe be made a little more parallel by running all the test suites together but I doubt it would have much effect given that almost all of our tests are in a single package.. I think we can just get rid of those tests instead. Given that the server is stateless now testing restarts doesn't really do much for us and it seems like a bit of a pain. If we wanted to have meaningful restart tests it would be a lot more meaningful to restart the database than pachd itself.\n. That does sound like a use case that's worth supporting.\n\n. \n. Subsumed by #1584 . I was actually thinking that we would just have a single monolithic binary with job-shim and pachd functionality built into it. That'd probably make for a smaller image since those 2 binaries share a huge amount of code.\n. Yup, still makes sense. I'd say not incredibly impactful but might be a nice to have.. @derekchiang whenever you have a chance.\n. We have --push-images for this. It was added in #962. As you yourself explained in the pull request:\n\n\"As a user ... having a private registry that pachctl updates when I create/update a pipeline allows me to seamlessly update my pipelines when my code is under active development\"\n. \n. @JoeyZwicker whenever you have a second.\n. Hmm, that is a good point. Is there a directory that's not /tmp that users have access to?\n. Nah that doesn't work, kubernetes won't let you use $ in path names, even if it did I suspect it treat it literally rather than evaluating it in a shell since I'm pretty sure it doesn't run a shell to evaluate path names. Similarly ~ isn't allowed in path names. I think they do this to prevent people using the home dir (and probably some other reasons) since there's no guarantee that the user kubernetes uses has a home directory.\n\nOther ideas?\n. Based on this Stack Overflow answer: http://stackoverflow.com/questions/18514447/what-goes-in-var I think /var is probably a decent place to put this. I also think given the purpose of /var it's going to be hard to deploy kubernetes in such a way that its pods don't have access to /var.\n. Yeah I think a Helm chart could make sense for us. Basically all we'd need to do is turn pachctl deploy into a template. All that pachctl deploy does is generate a k8s manifest and send it off to k8s. If you want to get this manifest in you can add the --dry-run flag to write it to stdout. You could in theory roll your own helm chart from that if you need compatibility with Helm immediately.\n. \n. To fix CI just merge in master.\n. Actually I'll do it for you :P so CI can start running immediately.\n. @msteffen whenever you get a chance.\nTo give a bit of context, this makes it so if you do:\npachctl put-file repo branch path -i file_list\nit uses path as a prefix for the files in file_list rather than putting all of the files as the same file called path which I don't think is a very useful behavior.\n. @JonathanFraser that's a very good practice generally.\nI think we should still fix this bug though, the cause is something we didn't intend to have happen and I think it could quite possibly manifest in other ways.. > Pachyderm just needs to cache the digest and submit the jobs with the digest instead of the user provided container tag.\nThis is something we're going to look into doing in the future. It gets a bit hairy though because you might have an image that can only pulled using some pull secrets that pachd doesn't have access to. We have a fix right now that fixes the worst manifestation of this problem where users will call update-pipeline and have archived output and now jobs will run at all which is really confusing. This system should work exactly as expected if you use tags which uniquely identify the images or digests. As you point out there are still some thorny edge cases when using latest we'll see what we can do to further harden this system in the next release.. Alright I think after a lot of discussion we're going to not fix this for now since it's unclear that the proposed fix would be considered an improvement. The biggest source of trouble here was the fact that our content addressed jobs could get deceived into thinking we'd already computed something when we hadn't actually computed it. We'll find a way to address that directly instead.. \n. @derekchiang whenever you have a chance.. Decided not to merge this since user comments made us realize that it's not actually a good fix for the problem.. How hard would it be to get rid of the fuse in /pfs/fuse/prev? That seems like a kind of ugly wart to me and like it would only require a small modification to the fuse layer to allow mounting a single commit of a single repo as a single directory rather than having an extra layer of directories in between. \n. \n. \n. \n. This is implemented, commits now store full copies of their metadata.. This doesn't strike me as the simplest solution to this problem. Afaict you're creating a table with a random uuid and exponential backoff on the creation process because it might fail then deleting the table and proceeding to create the new tables under the assumption that they're now guaranteed to create successfully. I see a couple of issues with this:\n\nif the process dies in the middle we'll be left with a bogus uuid table, not the end of the world but a little bit annoying.\nthe assumption that after the first table creation is finished all future ones will succeed is true under normal operation but in some cases won't be\n\nWhy don't you just put the retry code around the real table creations? Those are the things you actually want to unsure get created right? Why ensure that a different table gets created then delete it and assume that the things you actually care about are going to succeed?. \n. \n. LGTM. \n. \n. LGTM. Couldn't we in theory propagate the user information from the CreateJob or CreatePipeline request into the pod so that we could attribute stats here? Merging this still seems like a good idea to me since it's a quick fix to a real problem. But long term these metrics should be just as attributable as others.. No I meant you could actually attribute it back to real users. Basically for every pipeline you'd keep track of the id for the user that last touched the pipeline. Then when that pipeline creates jobs (or when a user directly calls CreateJob) those requests would have a user id in them which could be propagated to the pods for the job and finally back to pachd in the file requests the job makes.. Batching them together would decrease the metadata size but it would negate the benefits to incrementality that we're going for. This is really a fundemental tradeoff that we have to navigate. If the metadata for 2 inputs gets batched together then as far as our system is concerned it's really 1 input. If 1 of those inputs gets changed then we have to rerun both of them because we have no way to figure out which output is from the data that changed and which output is from the data that didn't.\n\nbut saves a (potentially-very-large?) amount of network traffic/metadata storage\n\nThis would definitely save a fairly large metadata overhead. Back of the envelope I think we're going to need to at least store a hash for each datum tuple processed, assuming that's a 256 bit hash that's 32 bytes per datum. Which could get quite large, but I think is worth it for the computation speed ups we'll get. I don't think this'll have much impact on network traffic though. It should be about the same amount of data uploaded.\nFor the json case I was thinking you'd still use the filesystem and just pass in a json object with the names of the files to process, not the entire content of the files.. We do this now.. CI is actually failing here for real reasons. make assets needs to be rerun so that the pipeline spec changes propagate to pachctl.. I'm not sure I agree with the approach in this PR. It seems like you're doing a couple of refactors here to fix a relatively small bug. A couple of things I'm not sure I agree with.\n\nWe now have AddCmds which adds the cmds to a root command rather than just returning the cmds, seems a little less flexible to me\nAddCmds assumes that the rootCmd it receives has a boolean flag called \"no-metrics\" if it doesn't then it will fail silently and we won't receive metrics. Not the end of the world, but we've had issues like that that we didn't notice for weeks so less risk of that seems like a good thing. In general I don't like substituting an implicit constraint for an explicit one which we had before.\n\nIt seems like all of this was done to solve the problem that you're passing the noMetrics variable by value before it gets initialized by rootCmd.Execute couldn't you just pass a pointer to it instead? It seems like that would be a much simpler solution.. I'm a little confused about what's going on here. What are the input methods of models and data in this example? Unless one of them is REPO I wouldn't expect it to ever be necessary to download either one in its entirety.\nAlso will #1108 help with this?. @derekchiang I think you're mostly right, although I believe we still get metrics if the user id isn't set. They're just not attributed to users.. @derekchiang is CI failing for real reasons here?. \nLGTM. Nice!\n\n. \n. LGTM after that one comment about logging is cleared up. Not sure if that's actually something you want to change.. LGTM. Hmm, that is very strange indeed. I can't think of where the failed login attempts would be coming from given that it clearly succeeds in at least one of these attempts.. This issue seems like it's relevant: https://github.com/docker/docker/issues/22188. \n. Few minor nitpicks, but after those LGTM.. \n. Looks even better to me... and it already lgtm.. Assuming that they're developing their code inside a container they can do docker run -v ~/pfs:/pfs .... At least I think they can do that, there might be some weirdness with it being a fuse mount.\nThis problem with also go away if we switched to the model where the user's program is called with cmd input1 input2 ... since then there's no reason to hardcode paths.. I think this is pretty well supported by put-file --overwrite now too.. This definitely appears to be a bug, I think I may have introduced this recently while fixing another bug. Will have a look.. Fixed by #1161 . \n. \n. Nuts to think how many times we've had to work around not being able to delete commits and it's like 30 lines of code.... Just did this.. \n. With the addition of docs, this pr still lgtm.. lgtm. One small tweak, but after that lgtm.. @sjezewski whenever you have a chance.. \n. \n. One small tweak, then should be good to go.. LGTM. LGTM. While we're at it we should modify the Makefile so it doesn't blindly put everything in doc in the assets file. There's lots of stuff in there we don't want to have.. I think probably the best way to do this would be to have streaming versions of the calls.\nNot sure if that's what you meany by paginization.. This is implemented.. I don't think this has any effect on the semantics of the code. Deferred functions are called in a last in first out order so the eg.Wait() will get called before the FinishCommit.\nOn the other hand this PR is net negative code... so LGTM. @derekchiang ahh, now that is a real bug. Nice catch. This retroactively lgtm as it has already been merged.. Bug was introduce in 41d38573.. > Can't meme on mobile\nA problem deeper than any this PR could ever hope to solve. LGTM although I'd remove the Exercises section.. Another use case we should be considering in this issue is how to make these workflows collaborative. It probably doesn't require much different than the initial issue. If we can check point and restore a jupyter notebook then we should be able to restore 2 copies of it to let people collaborate.. This feature still makes sense post 1.4 with etcd in place of rethink.. Saving it to object storage is actually something it'll do to. Since it's better to go straight from pachd to object storage than have to go through whatever machine pachctl is installed on.. This is actually how it works in 1.4 so closing.. Fixed by #1223.. Yup all looks good!\n\n. LGTM. This seems like a fine idea. I'm a little confused by the API though, what does /path/in/container/secret_name do if we're exposing the secrets as EnvVars? I'm imagining it would be something like:\nEnv: {\n    \"name\" : {\n        \"secret\": \"my-secret\",\n        \"key: \"my-key\"\n    }\n}\nAlso this seems to require using one_of in protobuf which we've had our issues with in the past. As always though there's a none one_of way to do it using an enum so I suspect we should probably do something like that.. \n. Yeah, all pachctl port-forward does is find a pachd pod and do kubectl port-forward <pod>. This lgtm, you should merge it.. This has been fixed by #1230.. @jnevin sorry you're running into this. As @msteffen said the fix should land soon but in the meantime here's a manifest that should work:\nmanifest.txt\nNote however that I think there are still going to be some breaking issues with 1.5. #1193 hasn't been addressed in master yet so probably you're best path to a working cluster is k8s 1.4 and that manifest. 1 way to get a k8s cluster is to just do make launch-kube from the root of the pachyderm repo. This will launch an entirely dockerized k8s cluster.. This has been fixed by #1230.. Should we also modify pachctl deploy amazon so that the token is an optional argument?\nThat would probably help nudge people in the direction we want them to go.. Docs for this are available here: https://github.com/pachyderm/pachyderm/blob/master/doc/deployment/deploying_on_the_cloud.md#start-pachyderm-1\nThe relevant bit is that all you need to do is leave the token empty and make sure that the user has the correct permissions and the cluster should operate indefinitely.. @orangefuzzball yup, that's correct.. @sjezewski whenever you have a chance. I can't track down the issue you're talking about but I think deleting the output directory as part of delete-pipeline seems pretty reasonable to me. Seeing as delete-pipeline should be the inverse of create-pipeline which is what's creating the repo. This should be super simple to do as well, I'm going to schedule it for 1.4.. Hi @binary-finary sorry you hit this. Jobs normally fail when the code running in the pod encounters some sort of an error. Could you do pachctl get-logs <job-id-of-failed-job> and paste the results in here? That should give us an idea of what's going wrong.. That's the correct command. It's possible that kubernetes has garbage collected the job or something. What do you get if you do kubectl get pod -a? You should see a pod with that uuid as a prefix followed by 5 random characters. If you do kubectl logs <pod-name> that should hopefully get you the same thing. If k8s has already gced it then you won't be able to get the logs, in that case I'd recommend deleting the pipelines and their output repos and then creating the pipelines again. This should recreate the k8s jobs and thus the logs.. By far the easiest debugging option is to get logs from the job before it gets removed by k8s. Unfortunately I think minikube is pretty aggressive about garbage collecting completed pods. Would you mind doing one more run through and capturing the logs of the jobs right after it fails before k8s has a chance to garbage collect it?. @binary-finary hmm, that's a somewhat unexpected result maybe. Few more questions:\n\nCould you grab logs from pachd with kubectl logs pachd-g46bp?\nwhen you delete the minikube and then recreate it is kubernetes empty at this point? kubectl get all should just have a single kubernetes service running, no pachyderm jobs / pods. Is this the case?. > did I coin that term?\n\nYou did! And I really like it, expect to see that in common usage from now on :P \nAlso I think I've found the guilty party here:\n2016-12-20T22:37:50Z ERROR error running pipeline filter: Job.batch \"67c30d70ba9d2179aa133255f5dc81db\" is invalid: spec.template.spec.initContainers[0].env[1].valueFrom.fieldRef.apiVersion: Required value; retrying in 620.581479ms\nK8s seems to be rejecting the job that pachyderm is trying to create. That would also explain why we can't get logs from it, it's not that the job is failing during execution it's just failing to create entirely. This is an issue I'd seen with k8s 1.5 but I'm guessing that k8s 1.4.7 exhibits this behavior too. In the short term running an earlier version of k8s is probably your best bet to get something working. This issue is fixed by #1223 we should have that merged in and shipped by the end of the week in which case this problem should go away.. @binary-finary 1.3.2 has been released so this should no longer be an issue, let us know if things still aren't working.. Right now our thinking on Spark is this, in 1.3 we added Services which allow you to embed other applications inside of a Pachyderm cluster with access to version controlled data. This is the first small step toward Spark support. Afaik no one has tried it yet but if you wanted to you could in theory deploy a spark cluster as a Pachyderm service and run Spark workloads against it. However you'd have to manually manage the Spark cluster which will get unwieldy. Services currently can't be run in such a way that they get relaunched with new data once it's available.\nIn 1.4 we're planning to extend Services in a number of ways, adding the ability to keep them up to date with new data as it comes in. This will also open the door for pipelines that leverage outside services. Ie you could setup your pipeline such that every time a new commit comes in it spins up a spark cluster, pumps the new data into it, runs your Spark code and finally writes its output back into pfs. Does that sound like a workflow that would do what you need?\n\nIt's already possible to get a pretty close approximation of this type of workload by running a Spark cluster on kubernetes and having your job logic talk to it. Your job logic will have access to the Spark cluster simply by virtue of being deploying in the same kubernetes namespace after the Spark cluster has been deployed.. \n. That is not the ask here. Having cloudfront still doesn't give you a way to request blocks for a file directly from it because you don't know which blocks to request.. Hey @orenyomtov sorry you hit this and sorry for the delayed response.\nIt looks like this might be 2 different things going on, 1 rethink seems to be getting a query that's too deeply nested and is blowing the call stack, the other is that there seems to be an error in pps that causes it to access an array out of bounds. I don't see a way that one could be causing the other. A few questions: are these errors reproducible or did it just happen once? Did rethink and pachd get restarted and did the services come back up following this error?\n@derekchiang can you take a look at the line that's crashing in pps when you have a chance? It looks like there's an assumption that 2 arrays should be the same length that isn't being met but I'm not totally sure why that would be.. This LGTM, is it ready for merge or is there more to come?\nAlso we'll need you to sign our CLA before we can merge: https://pachyderm.wufoo.com/forms/pachyderm-contributor-license-agreement/. Merged, many thanks :). Going to take a look at this now, this is only part of the story for completion as it will only add completion for command names, not for parameters.. This has been partially added via #2352.. Looks good.. @msteffen this is ready for a review whenever you have a chance. Thanks for the pull.\nI do think the reorganization of the commands here might be a good idea. Back when we were first writing pachctl we debated having an ordering like this. Another direction that seems interesting to me would be something like kubectl where there's a single create command that receives a more descriptive manifest. It could also have sub commands i.e. pachctl create repo repo_name to create specific objects maintaining much of the functionality.\nFor the time being I think we're better off not disrupting pachctl a lot of docs would have to be rewritten and too much else is being refactored write now.\nOut of curiosity why do you prefer the other cli library you used?. This seems like some very valuable cleanup. Many thanks.\nLooks like CI failed but I think it might be transient, I restarted it.\nThis seems to update us to the latest version of grpc, does everything seem to function with this version? If so that'd be a huge piece of technical debt payed down!\n. Looks like there's a small issue that's keeping this from passing in CI. There's a rethink query that is trying to access a field called Size on a document but the document's field is called SizeBytes. It looks like you went through and just changed every instance of Size to SizeBytes which I approve of since it unifies things. I think all you need to do is go through src/server/pfs/db/driver.go and change instances of Size to SizeBytes and CI will pass.. @roylee17 our tests can unfortunately be very sensitive to different environments. I often have to iterate on CI to get stuff to pass.. Hi @willis7, sorry you ran into this.\nIt appears the problems is that this method of deploying kubernetes creates a cluster that can't schedule privileged containers, Pachyderm can't work on such a cluster because the pachd container won't be able to spawn k8s jobs which is how it processes things. To get it working you just need to add a flag to the kubelet --privileged=true, it looks like the file you'll need to modify is this one: https://github.com/flix-tech/vagrant-kubernetes/blob/ff4673988b68a235ca5430fe8392db45d63657ca/conf/kubelet.service\nI'm going to close this issue since there's nothing we can change about our code to make this work, but please feel free to keep using this thread for support if what I suggests doesn't fix the problem.. Hmm, unfortunately I don't really have an explanation for why that didn't work. Setting that flag is all we've ever had to do to get privileged containers running. If this is the only viable way to deploy k8s I'd recommend getting in touch with the package distributor or maybe the k8s team over in their dev channel.\nYou might also consider going to minikube route, you can minikube ssh to get onto the VM which should be an environment you can install pachctl in. I believe it's just an ubuntu machine under the hood so apt-get should work. If not it's possible to schedule an ubuntu container on k8s that will serve as a \"jump box\" from which you can run pachctl commands with kubectl exec. It's a little hacky but it should work.\nAlso you could just setup a normal ubuntu vm and install dockerized kubernetes on it. make launch-kube from the root of our repo will do this.. There's another callsite where we delete jobs here: https://github.com/pachyderm/pachyderm/blob/master/src/server/pps/server/api_server.go#L2375 could you update that one to too?\nAnd arguably those should be using the same helper function so there's only one callsite to update. @derekchiang could we try to get this one merged ASAP, I'd like to do a point release this week that'll fix some of the disk fill up issues that people are seeing.. \n. Can we change the name of this PR to something a little more accurate... this is already more than the first commit for Merkle Trees.. These changes look quite reasonable to me.. LGTM. Hey sorry you hit this, it looks like we might have hit our limit on Slack guests, or Slack might have pulled the credentials of the inviter bot. We should have this fixed by eod today.. Hi @sashabaranov, many thanks for the PR. This all LGTM. Even though it's small, before I can merge it I need you to sign our CLA: https://pachyderm.wufoo.com/forms/pachyderm-contributor-license-agreement/.. @sashabaranov :+1: merged. True, although I don't see any particular reason to keep that data around once it's been persisted to pfs, there's not really anyway it can be useful. Assuming that even when GC is working correctly there's going to be some latency between when the pod finishes and when it gets GCed this should help decrease the amount of disk space we need in the intervening time.\nAlso with the new datum changes we'll be deleting all but the last datum processed anyways to give a clean slate to each run.. @dwhitena is this ready for review / merge? If so you should generally assign it to someone and ping them for a review. I just took a look at this one and it all LGTM so you can go ahead and merge once CI is pasing.. @derekchiang It seems pretty unlikely that GC is not running here given that the k8s jobs have been deleted, right?. @derekchiang already got #1266 ready which manually deletes the pods. Just need a lgtm from you.. Alright, should be fixed in master for real this time.. @davidgasquez many thanks for the pull! Everything here LGTM. There's 1 minor thing which is that CI is failing because the document you modified gets compiled as a binary asset for pachctl and thus needs to be recompiled. If you do make assets from the root of the repo and push the results it should get CI passing.. @davidgasquez awesome, thanks! That should do it as soon as CI passes I'll hit the merge button.. @JoeyZwicker would that work with ReadTheDocs? I think ReadTheDocs might require a RST.. @davidgasquez yeah, unfortunately our CI can be flaky at times. Anecdotally it seems to coincide with high traffic periods such as the end of the day on Friday. I'll keep trying it, and I bet it'll pass eventually.. @davidgasquez pretty sure this are spurious failures so not much to be done on your end. I'm going to rerun a few times and see if I can get it to pass.. Praise the CI gods it passed. Merging.. :+1:. I think a Repo for parameters makes the most sense. In my mind parameters are just a different kind of data that pipelines might want to consume so putting it in its own repo makes a lot of sense. Putting it in the same repo as other data would be kind of weird because you might wind up with a pod that only sees parameters which doesn't sound particularly useful.. @harshavardhana thanks so much for the PR, very excited to have support for minio and other s3 compatible stores. A few things I'd like to see added / changed before we merge this in:\n\nI'd prefer to just call this the Minio backend rather than s3-compatible. A few reasons for this 1) it's a bit weird to have an s3-compatible backend and an s3 backend. Users might find that confusing. Eventually this backend might subsume the s3 backend in which case we'll consider this. 2) Lots of users have asked about a way to run this on prem, when this PR lands we're going to start recommending Minio as the best way to do that so it'll be more discoverable if we make the names match. 3) you guys wrote the code so I feel like you should get the name recognition that comes with it :)\nIn addition to the backend code for Minio we'll also need some code that creates a k8s secret with the Minio credentials for deployment. For an example checkout how we create AmazonSecrets. @dwhitena you've already done a bit of legwork on this yes?\nIt'd also be nice to have an example of how to deploy Minio on k8s that we could put in our docs so people have a complete guide for how to deploy an on prem Pachyderm cluster.\nLastly we'll need you to sign our CLA. @dwhitena @harshavardhana you can get deploy to just print out the manifest with --dry-run and then manually edit the images referenced to be what you want.. @harshavardhana I haven't seen any problems like that with wrong IPs being assigned. Looking at the logs it seems that pachd-init is failing to get in contact with rethinkdb while pachd is succeeding but not finding the tables it needs because pachd-init isn't creating them. I'm a little confused about what exactly is being assigned a wrong IP address here, is it the rethink pod?. Yeah that sounds reasonable, we're likely going to need to make things a little bit more sophisticated now since people will probably want to be able to do minio with an ebs volume for amazon deploys or a PD for GCE deploys or what have you. But that's not necessary for the purposes of this PR.. LGTM after those comments. Yup, we rate limits this now, closing.. \n\nOne comment about a potentially weird merge that we should look at before we merge this in.. This is currently the expected behavior here, if we wanted PutFile to throw an error rather than FinishCommit we would need to do an expensive check for every PutFile which isn't worth it. The error message in 1.4 is a lot more descriptive though.. LGTM.. Hi @Amit-PivotalLabs, sorry you're hitting this.\nWhen I've seen issues like this most of the time it's been due to something being wonky about the cluster's DNS setup. In this case it looks like either pods can't connect to the kube-dns server or when they do connect it's unresponsive. However the fact that you're able to curl the files from other pods that are deployed suggests that it does work in some cases so maybe pachd is doing something different than curl. Could you try execing into the container and using dig to check that the DNS server works? You can do this with:\ndig raw.githubusercontent.com @100.64.0.10:53\nDepending on what that returns we'll have an idea of whether or not the cluster DNS is functioning correctly. You may be able to sidestep this issue by bypassing kube-dns and just using a public dns server. This can be done by passing --cluster_dns=8.8.8.8 (or some other dns server you like) to the kubelet on startup.. Sorry I didn't explain this better.\nThat's to be expected from the pachd pod because it's built FROM scratch so it doesn't have exec or anything else in it. It makes for a quick deployment but unfortunately prevents stuff like this. The rethink pod would probably be a good target for this since it's a more standard ubuntu container (you will probably need to apt-get dig). We're operating under the assumption that if the pachd pod is having trouble talking to DNS then the rethink one probably is as well.. > Will dig more into why not later.\nI see what you did there...\nOverall I'd say this is good news since it's pretty strong confirmation that the issue lies with the DNS server. In terms of fixing the problem, as far as pachyderm is concerned there's no need for kube-dns. It's used as a method of service discovery in k8s for some types of services but we don't use it. So if it's easy for you to just pass a different dns server for the cluster (--cluster_dns) I'd do that.\nIf that doesn't work then my best guess is that the dns server is in some way resource starved due to the machine size. What does kubectl get all --namespace=kube-system get you?. I think a better idea would be to just update pachctl so it doesn't take a token by default, that would help coax people toward the better deployment path. We can add it back in as a flag. Quotes are fine if the funny characters actually cause problems, my experience has been that quotes aren't really necessary unless I want to pass an empty token.. Hi @Amit-PivotalLabs pachyderm won't let you delete a job that's part of a pipeline, you can however delete the whole pipeline which will delete the job with it.\nUnfortunately this case doesn't get cleaned up naturally when the job is stuck in image pulling. Once you delete the k8s job then it's really just a record in the database so it should be fairly benign. However it's definitely annoying to have those jobs lying around so we should be able to figure out a way to clean them up. Or at least make it so delete-job can work in those cases.. Alright, this is finally ready for another look. I think I addressed all your concerns from before, with one exception which merging GetTag and GetObject into a single request. I decided this might not be a great idea because it makes both lookups slower and makes both require requesting an object which doesn't exist. S3 seems to start responding a lot slower if you send too many requests for objects that don't exist so it seemed like it wasn't worth it.. I'm not sure if changing the signature of CreatePipeline in this way is something we want to do. Most of the point of the go client is that it saves you having to construct a bunch of objects inline such as Transform. If you strongly desire this behavior we could consider adding it as a CreatePipelineTransform method or something like that but right now I don't see a lot of benefit to being able to specify the Transform as a struct rather than function arguments.. Yeah, making those available in the function signature is definitely reasonable.\nWhen using a client you can always call client.PpsAPIClient.CreatePipeline and pass it a CreatePipelineRequest directly. You can define that request inline as go code or parse an embedded json string to get it. Both should work.. For context you can just pass: context.Background() and add import \"golang.org/x/net/context\" to the top of your file. You can also use that parameter to specify timeouts and cancel operations.. Closing since we've realized the change isn't necessary, feel free to keep asking questions here.. Probably not, I don't think proto checks for unused imports so we probably were using it at one point.. This is because we actually run a shim process between that sets up he filesystem state before the user code runs. Is it possible to detect the ENTRYPOINT and CMD from within a container? If so I think we could get this working, otherwise it might be kinda hard.. @davidgasquez that's unfortunately the same thing we were talking about above. We can't use the entrypoint cmd because we need to run our own wrapper command which pulls in the data and gates the user code.. I actually think our current client should support this pretty well. Afaik we don't need to do anything special to get DNS resolution to work for kubernetes. If DNS is enabled in your cluster then the pachd service should be accessible as pachd.default.svc.cluster.local (or whatever namespace you've got it deployed in instead of default). Then you can just do:\ngo\nclient.NewFromAddress(\"pachd.default.svc.cluster.local\")\nto connect to the server. We also have a method on the client called KeepConnected that basically automates the hacky heartbeat you were doing. You probably want to run it with:\ngo\ngo c.KeepConnected(nil)\nLet me know if this satisfies your needs. One thing we could maybe add would be a HealthCheck method to check if a connection is healthy. So you can use it and exit if it errors. That's generally a very sane strategy in distributed systems, if the thing you depend on isn't up just crash. There's no particular need for panics to do this and generally I like to avoid panics since they're pretty unfriendly. The HealthCheck method would probably just return a normal error and you could exit when you got that back.. > Is there any specific reason that NewInCluster uses a constant IP address instead of a flexible kube-DNS string?\nThere's a few reasons I'd prefer not to make this the default behavior.\n\nNot all kubernetes clusters have dns servers but all of them support service discovery through environment variables. I'd prefer to optimize for reliability with this method.\nDNS can be kind of finicky and exhibit some weird behaviors like caching old addresses. Which makes me uneasy about having it be the default.\nIt doesn't really save much typing, NewInCluster() vs NewFromAddress(...) isn't that big of a change.\n\nWe could add a NewInClusterDNS method which would just call NewFromAddress with the right address. To me that doesn't seem like it's really saving enough typing to be useful and it would be kind of inflexible since it would have to hardcode the namespace which might change.. I don't think this is much of an issue.\nThere's a very small window when new jobs could get kicked off on the order of milliseconds. It's unlikely that jobs could complete in that time and cause unnecessary processing, you probably can't even launch a container in that time. If they did it would be because the jobs are very, very lightweight in which case it's not such a big deal that we did some extra processing.. @derekchiang third line is being logged by the caching server, there is some protobuf magic that automatically logs things (technically grpc magic). However that magic shouldn't be present anymore in master since we have a more vanilla usage of protobuf now. I'll test this in master but I'm pretty sure it shouldn't be an issue anymore.\n@josibake which version of Pachyderm did you encounter this on?. This is fixed in master and v1.3.5 by the grpc upgrade. It was caused by one of the wrapper libraries we were using.. CWL is something that's been on our radar for a while and is definitely something we're interested in exploring. A few questions I have:\n\nyou can of course run CWL workflows on Pachyderm by packaging up the runtime into a container and running it like that. Is that appealing as a stopgap measure or does that have too many drawbacks to be useful?\nOne obvious drawback is that this wouldn't be distributed. How do CWL implementations distribute the workload? Can they distribute individual steps in the workflow or just the different nodes of the workflow?\nWould running a side cluster of one of the implementations on their site be an appealing way to run CWL workloads? This is a patterns that's emerging in Pachyderm right now for incorporating other protocols into pachyderm pipelines.. LGTM. LGTM. So I'm pretty sure what's happening here is that put-file isn't erroring because it's actually a correct usage. The default behavior is to block for input from stdin if there's no -f or -i flag. We could maybe add a message about this so users know it's blocking on stdin, but I don't think it's a great idea to try to turn all these cases into errors. Trying to figure out if the user actually meant the flags they typed or something different can be really tricky / annoying error prone code to write.. What's the meaning of --cloud google if it doesn't set the object store to gcs? I'm assuming it would have to be setting the disk types that we use to back etcd and rethink but in that case I think it'd be better to just be explicit and say --persistent-disk ebs or something like that. It seems to me that there should be 2 different arguments --object-store and --persistent-disk since those are the 2 pieces that users can swap out.\n\nAlso, if at all possible, I think it'd be good to keep around pachctl deploy google ... and friends as quick aliases for deployment on a single cloud. For one thing those already exist, people are using them and it's always better not to break people's code if you can avoid it. Second it seems like those will still be the most common usage patterns so it'll be good to have quick aliases for them going forward.. @dwhitena so anything left here or close?. Any particular reason you don't like Marshal and Unmarshal? Those seem like the more standard go names to me, it's what the encoding packages and the proto packages use.. Ah, I see. that's reasonable.. I think we should probably keep the functionality but as an optional flag rather than as a required one. Docs should clearly present permanent credentials as the default option.. Get hype. Github also has the downloads feature which might be an easier way to accomplish this. Not really experienced with it but my intuition is that it's going to be a little more user friendly than s3 bucket.. The place where I use it in the PR is an example use case. There are a bunch of places in our own code where we construct a client directly from an existing APIClient rather than using a constructor. In one of those cases I wanted to be able to set the streamSemaphore. Arguably we could just have another constructor method to do that but this seemed like a fairly reasonable way as well. Also this means you can now do:\nc := NewInCluster()\nc.SetMaxConccurrentStreams(...)\nwhich you couldn't do before so I think it's a pretty convenient method to have.. @sjezewski meant to assign you here. Does this look good to merge?. Lovely. This was fixed by #1368 . Because it adds our code at the end rather than before we do the fuse download and compilation. As is we're doing the FUSE downloading and compilation every time we do make docker-build but with this change it'll be cached.. Should be fixed now, unfortuantely CI doesn't seem to be running.... Should we maybe consider having pachyderm/pachyderm.github.io be the source of truth from now on? Seems like that might be easier, unless there's some benefit to having them split.. Merging this since I want to do a release w/o pushing to s3. Many thanks for the pull, all looks good. Merging.. And just for posterity sake, CLA was already signed. . One small comment on the go docker image we're using. Then lgtm.. Cool, feel free to hit the big green button as soon as CI passes.. Just the one comment which isn't strictly speaking necessary but might make this a bit nicer. Other than that lgtm. All LGTM. LGTM, many thanks for the pulll. Mergin!. @philipithomas many thanks for the pull. This is awesome. CLA is signed and this all looks good to me so I'm going to merge!. So the one thing that seems a little bit weird to me about using --compute-backend is that we'll likely have support for different kinds of persistent disks in the future that aren't tied to a specific compute backend. For example k8s has support for gluster volumes which could be a good way for us to offer platform agnostic deployments. But it'd be awfully weird to have a command line with --compute-backend gluster in it since gluster isn't really a compute engine.\nOther than that this all LGTM.. Alright all comments are addressed.. @dwhitena does this look good for merge to you?. @harshavardhana there's not, but you can with the golang api. Groovy.. I believe we now have everything discussed in this issue including a logging stack provided by make logging. @sjezewski I think you need to recreate this pull request due to some git mishaps.. This is how 1.3 works. The new system is a more robust replacement for it. It requires more work to check things the way you're describing here because you need to check that a file was present in the previous commit and that, that commit was processed by the pipeline code you're using... which is exactly what the 1.4 system allows you to check quickly.. I like this idea, should be pretty simple given the existence of the sync code.. Is this ready for review?. Oh nice, CI is actually working on this branch now.. This looks reasonable, although because the cache size is used by both the object server we're really using twice as much cache as the user specifies.. This change no longer applies in master since the file containing the link has been migrated to an .md file in which the link syntax is correct. Thanks anyway for the pull @brollb . This was closed due to a git mishap but in this case that's actually ok, I should have closed it before because the thing it was fixing has changed enough that it doesn't apply.. Fixed.. And implemented.. @derekchiang whenever you have a chance. This turned out not to be a workable direction for migration (which I originally wrote this for) and because I made the same change in v1.4 I'd get a merge conflict if I merged this... so I'm going to not do that.. I think we should just have -c only take branches, that's really the most useful form imo anyways and then I think it's clear.. How can this give a too few arguments error when, exactly the same command with master in place of /pics/ works? The branch parameter isn't optional, so like with any non-optional positional argument if you omit it the parser has no magical way of knowing that you've skipped an argument it just reads strings and interprets them as the arguments it's supposed to. This error message is a completely reasonable thing to return if someone passes a commit that doesn't exist as a commit argument.. I'm pretty sure delete-commit does work on cancelled commits in 1.3. Also it seems like he could just delete the upstream commits (which aren't cancelled) and then call rerun-pipeline, or delete the downstream commits to get the same effect.. We no longer have cancelled commits and have #1586 for tracking delete-commit in 1.4. Closing.. The integration test does do a recursive one, albeit for only 2 files so it's not exactly the best test coverage. I can make the directory structure a bit more complicated.. @derekchiang alright, test is now harder and it uncovered a bug. Good call on making it harder.. The clarify this a bit, ListCommit is already paginated at the API layer, to paginate at the pachctl layer you can pipe to less (| less). We should also look at making ListFile paginated in the API since it isn't yet.\nOne other thing to look out for with this, we currently use TabWriter to write our output which I believe buffers the output. We should maybe bypass that or write multiple chunks with a different heading for each.. Turns out ListCommit is not paginated.\nSubscribeCommitInfos is kinda sorta the same and it is paginated. The Number field limits the number of commits you'll get in response so that with From can kinda give you pagination.\nLet's just proceed for now with unpaginated and we'll make these paginated after 1.4 ships.. This is fixed in 1.4. 1 small fix, then lgtm. And also what inspect-job shows for on of the empty jobs would be helpful.. In 1.4 archive doesn't exist does it?. I find no references to no-archive in our code base.. I think another must is:\n\n[x] put-file -c and start-commit repo branch (#1422)\n\nThat syntax is pretty ubiquitous from what I understand.\nI also think merging with master is kind of a must, for one thing it has a number of bug fixes that have been added since the last merge and it'd be pretty bad to regress on those. But also just from a development hygiene perspective I think getting everyone onto the same branch at release time is a good practice and will prevent some headaches.. Some of the bug fixes are pretty generic stuff though, like all of the fixes I made for object store retries. There isn't a good way to distinguish between stuff that applies and stuff that doesn't other than to merge it in.\nAnd yeah, breaking current master off into 1.3 so we can backport fixes if need be is a good idea.. The 2 remaining issues have been broken out into separate issues #1584 and #1580.. This is correct, it's a little bit weird because if you omit the path and don't include a -f argument then it will error because it doesn't have anywhere to get the path from. But that still counts as optional in my book so we should make this change.. This is identical to the behavior in 1.3.. Most of these use cases are covered by the copy-file command.. What would the semantics of rerun-job be?. In particular how would it be different from run-pipeline?. Given that the two ways to create a commit both require you to specify a branch, ie:\nstart-commit repo branch\nput-file repo branch -c ...\nThis isn't going to save users anything except maybe in the rare case where they're using the protobuf api directly. On the other hand, doing this would make it impossible to create a commit not on a branch unless you already had master defined which is a really weird behavior.. Pipelines specify their output branches directly in 1.4.. I think that's correct... that behavior seems pretty reasonable to me.. Why not? Pipelines don't output to the same repo.. Yup that's correct, and strictly speaking it doesn't have to be master but given that there can only be one branch at this point it probably doesn't make much sense to have it be anything but master. In the future we'll probably allow either multiple versions of the same pipeline, or multiple pipelines targeting the same repo (conceptually those are really the same thing) at which point we'll need to have some validation to make sure repos can't target the same branch.. Just 1 the comment, then LGTM. No, it should be a field called \"fromCommitID\" in place of \"from\". Fixed.. AFAIK opencv is currently the beginner tutorial in 1.4 but there are some issues with it. @JoeyZwicker is that correct and could you expand on what the issues are.. One idea I had for a second stage was using ImageMagick to turn the outputted images into a montage. You can do some pretty cool stuff with a simple command line invocation: http://www.imagemagick.org/Usage/montage/ and I think it would work well for the demo because you could just open up a single image at the end which shows the original images and their edges side by side. And of course the montage would automatically update as new images come in.. Moving this to 1.5 milestone, port-forward seems to have been having a lot of issues in general in newer versions of kubernetes. Unfortunately I'm not sure exactly what we can do about this, we should look into it more and try to document things better, maybe in the worst case if it's become completely unreliable we should just remove the functionality.. @hchauvin thanks for the good information here. I think making a port-forward each time we want to make a request would actually be a really nice improvement.. I think this could actually be a bit more general and just allow passing of any kubectl flags, that would satisfy the kubeconfig case but also solve a bunch of other problems. We had one user ask for this in #1206 . LGTM. Fixed, good to merge?. Note, this won't work until master is merged in since it has a fix for egress.. @emk thanks for weighing in and don't worry we agree that this makes sense as an improvement. We won't be closing this one due to staleness.. Closing in favor of #1553. Closing in favor of #773. This issue is now fixed in the sense that if the worker does encounter such an error it will be propagated correctly and the job will be marked as failed. There's still other fixes I'd like to make to pipes but I'm going to do them as separate issues and close this one for now.. - is the default in that it's what you get if you don't pass -f at all. If you do pass a -f then it needs to be with a value, there's no meaning to an empty -f you should always just omit it in that case. I think this is pretty in keeping with how flags normally work.. This commit syntax died a while ago so this shouldn't apply anymore.. Partially fixes #1452. This should be part of a 1.4 point release.. I do, I think #2631 has the most up to date info on it.. LGTM . I think we definitely want delete-commit.\n\nDeleting the commit's subvenance (opposite of provenance) seems like it's probably the right answer. I don't think having commits with provenance that doesn't exist is all that nasty though so there should be an option to leave them intact.\nDeleting a commit c should result in any commits that have c as their parent having c.Parent as their parent.\nIf a branch b is pointed at c and c is deleted then b should point at c.Parent.. I'm actually not sure how to do that anymore. We just need to remember to close this once #2506 is merged.. get-file erroring makes sense, delete-file probably should too but there should be a flag to ignore not found errors since that'll be a common use case.. It seems like we don't really need to worry about which language the kafka connector is written in, within the Pachyderm architecture the most reasonable way to implement this would be a container which reads from a set of kafka topics and writes them out to disk. This is similar, conceptually to a scraper pipeline. This could be written in whatever language is most convenient for kafka, so probably Java, and the rest of the system wouldn't really have to care about what's running inside the container, same is true for pushing data into kafka.. subsumed by #1587. Rather than adding features to glob patterns maybe it would make sense to also accept regexs? That would give us a lot of higher level functionality, including this, pretty easily.. Repurposing this issue a bit for regex in place of globs.\nI think the original thing @derekchiang posted would be solved #1491. It is done.. Yes, this has been merged.. As discussed offline, this should be implemented as a provenance system in pps land that leverages the existing provenance system in pfs and adds extra functionality on top of it.. We do this now.. We now retry when user code fails.. LGTM and CLA is signed.. Looks good and CLA is signed.. LGTM and CLA is signed, merging.. It is, I'll cherry pick it and do a separate PR.. This seems to make sense to me, I think we definitely still want the \"append\" behavior for files that are put with --split. I.e. if I put files foo/1 and foo/2 with one put-file the next one should put foo/3 and foo/4 not overwrite foo/1 and foo/2 but appending to a single file doesn't seem to make a ton of sense in the new model.. This looks to be an error that's coming from the hashtree library, on the call to finish-commit (which happens because of the -c parameter). Likely the reason @dwhitena can't reproduce this is that it's actually not the put-file that's causing the problem but some state that's already in @abourget's  master branch. @abourget, if you've still got that data hanging around you could confirm this by doing pachctl start-commit mama master && pachctl finish-commit mama master.\n\nI'm not quite sure what would put a hashtree in a state where it returns this error though, @msteffen @derekchiang maybe you guys know more?. > - pachctl put-file sessions master -c -r -f ./2017-03-28 will add the files under ./2017-03-28/files.json in the commit as 2017-03-28/2017-03-28/files.json.\nUgh, this is a silly bug, we'll fix this. The path should clearly be just: 2017-03-28/files.json \n\n\npachctl put-file sessions master 2017-03-28 -c -r -f ./2017-03-28 will add the files under ./2017-03-28/files.json in the commit as 2017-03-28/files.json.\n\n\nThis one seems  reasonable to me, if you explicitly a file name in a recursive call that gets appended as a prefix to all of the files it finds in the directory.\n\n\npachctl put-file sessions master -c -r 2017-03-28/ just stalls there, I guess awaiting input from stdin. But tihs doesn't make sense for recursively loading directories.\n\n\nThis one is admittedly a bit confusing, I think it's not too terrible to have a -r flag do nothing in the case of putting a single file, recursive linux commands like du can also be called on single files and have no effect. I think one easy thing we could do to make this less confusing is print out a message that it's waiting for input from stdin so people know what's going on.\n\nThe idea of having put-dir does seem interesting to me. It would make both commands a bit less cluttered and also probably increase the chances that the commands are implemented correctly since we've had several problems caused by how much functionality is packed into put-file. What do others think?\n. (You already review this for 1.3). You can filter by job, pipeline and datum but not by worker. The original part of this issue is fixed by #1539 . This is fixed in 1.4.1. This should definitely be from_commit rather than just from. Is sending the datum back through retCh actually correct there? It seems like the code was sending it back through dataCh because the datum had failed to process for unknown reasons and it wanted to requeue it for processing. Returning it on retCh seems to suggest that it's going to be interpreted as having been processed.. I think the best recommendation for this is to just use a k8s namespace. That's probably going to be a lot cleaner than trying to bake the namespacing into the names.. I'm a bit confused on this one, I was under the impression that when you ask k8s to list its nodes it would only return worker nodes to you. I've never seen it return anything else. Will need to confirm exactly what's going on here.. transport is closing normally indicates that the pachd process encountered an error or that there was some error with a connection such as a port-forward breaking. Unfortunately this might prove hard to diagnose without logs from pachd from when the error happened. @jonandernovella has this error proven to be reproducible?. I think this can be accomplished simply using by passing the right kubernetes flags to the deploy command. I thought that was a feature we had but I'm not seeing it in master.. Also was there a reason this was closed?. This would be pretty nice to have, also would be useful to see if we can use etags to make sure we don't upload the same data twice.. I think s3cmd has an rsync like feature but I don't see it anywhere in the client library. I think we're likely going to need to roll our own for clients which makes this a bit trickier.. I'm not sure if #1547 is exactly the same thing, although it seems related. Also I don't think GPU is a great example of a use case for this because k8s 1.6 now has GPU requests as a first class part of the API so there's no need to meddle with explicit node-selectors.. I don't think glob-file will quite work for the use case you're describing here @sjezewski. From my understanding glob-file will just give you a way to pass a glob pattern and get all of the files that match it. This is still pretty useful since it allows you to quickly check what your datums will be for a particular pipeline. But it won't tell you which workers processed a given datum, for that I think get-logs is the answer. You can already do get-logs per datum, to satisfy the use case we just need to add a field to the log message about which worker it comes from.. @JoeyZwicker again, this will not tell you about which datums were processed by which pods. That's #1554 . Does this even exist yet in 1.4? I can't seem to find any evidence of it in the protos.. Does this even exist yet in 1.4? I can't seem to find any evidence of it in the protos.. I think I recall that it was intentional, but it shouldn't be too hard to add it back in.. This will make it easy to correlate datums with workers and figure out which datums are spiking your cpu / memory on a particular worker.. @sjezewski is this still an issue?. And is it worth doing for 1.5. This sounds more like rsync functionality than get-file I'm not sure if it makes more sense to extend get-file or add a new command. We've been talking about splitting some of the more complicated functionality of put-file out into it's own command as well.. @sjezewski unfortunately gotta remake this pull request due to some git mishaps. @derekchiang whenever you have a chance, it's pretty simple but let me know what you think of the defensive test for IsNotExist, I think it's a good idea but not totally sure, it's a little weird.. I actually think it might make some sense to leave it as something that happens on startup. Even when we have test coverage for object stores we're unlikely to have it for every object store, especially since minio can be used with a bunch of different object stores. This seems like a pretty easy way to give a more meaningful error to users rather than the very confusing ones they're bound to get if NotExist doesn't work.. One comment but otherwise LGTM. So it is, it's actually frustratingly difficult to find the issues via search.\nWould you mind just closing dupes I've opened when you find them but transferring over the milestone and tag info when you do? I just did so with this one.. Closing in favor of #1472.. One big part of this would be not buffering the log output of datums, currently we don't see any output from a datum until the datum completes.. @JonathanFraser this is a good answer to this problem, thanks for chiming in. However there is one downside to keep in mind here. If you group keys together then they will all get processed together which might be faster, but if a new file is added to the group then the whole group needs to be reprocessed which can lead to a lot of extra processing.. One comment but other than that lgtm.. Some of the best code I've ever read.\nMerge asap.. All LGTM, is there already a test which reproduces this already? If not would you mind adding one?. LGTM. LGTM. Alright, so turns out pipes don't work quite like I thought they did this actually doesn't work for what we want it to do. Don't think there's actually much here worth merging.. Looks like this needs to be a MkdirAll. What's the import?. LGTM. Oh, I'm not reviewer.. LGTM. I just tried this and it worked for me first try. I've deployed on GKE several times over the last few days and I've yet to see this issue. I think there might be some flakiness around kubernetes attaching drives to machines, unfortunately I'm not quite sure what we can do to fix that. I think it's a bit more reliable if you use dynamically allocated drives rather than statically allocated.. @dwhitena I normally use dynamic volumes but I just tried with static volumes and that worked for me too. The only thing I did that was a little different from the docs was I did --dry-run so I could manually edit the image name because pachctl installed via make install outputs weird image names with uuid in them. But this shouldn't really change the behavior.. @abourget awesome many thanks. This all LGTM, before we can merge I'll need you to sign our CLA. @abourget sorry, I sent you the wrong link for the CLA, you shouldn't have to log in to sign it. Here's the right link: https://pachyderm.wufoo.com/forms/pachyderm-contributor-license-agreement/. @abourget it wouldn't be too hard have source commits as env vars. I'd like to understand the use case a bit better though, output commits already keep track of their source commits through the provenance system. This information also propagates through the system as it's processed downstream. I'm wondering if that's actually suitable for this use case.. dupe of #1588, I copied over the labels. Lol, literally just fixed this myself an was about to open a PR, my fix is identical to yours so... looks quite good to me.. LGTM. I'm actually surprised the linter doesn't catch  this, unless I'm mistaken a single statement select is completely useless and likely a bug.. LGTM. Fixes #1498 . I'm a little bit confused about what's going on with CI here, it seems to be timing out but all of the tests that are part of the segment that's timing out seem to be passing. I suppose I'll just try rerunning.. LGTM. This is approved and passing CI so I'm gonna merge it.. We should implement this, along with an OutputCommit field.. Why would it be repeated? Jobs only have 1 output commit.\nAnd yes, I agree that it should be exact match making, i.e. the return job will match all InputCommits and OutputCommit if specified.. This is implemented.. Implementing OutputCommit is already part of #1645.. I don't understand what's being request here. The way to recover from bad code is to write new code and use update-pipeline.\nWhat does this issue cover that #1654 doesn't?. There's definitely at least one bug afoot here. Seems like there might be a few. However I think there's a decent chance that the \"already been processed\" log messages are a red herring. If a job fails and gets rerun it's likely that there will be datums which have already been processed that won't need to be processed again, it seems this might be misfiring a bit #1654 so we should look at that. The fact that the job never ends is also clearly a problem.\nWhat versions did these bugs occur on?. Hmm, that's surprising to me, looking at the code it clearly seems to be hashing the transform as part of the HashDatum method. @msteffen any idea what could be happening?. So what's going on here is that if the glob pattern doesn't match anything there won't be any logs because no code ever gets run. I'm not quite sure how we'd make this better, maybe get-logs could say explicitly \"no logs\" or something like that.. So technically from, pachyderm's perspective, the job was run it's just that running it didn't entail any actual data processing. One thing that may help with this, as of 1.4.3 you can see datum progress in jobs, so if you do pachctl list-job you should see a job that succeeded but its progress will be 0 / 0 which is a tip off the glob pattern is wrong.\nHaving a dry run seems a little bit weird, I'm not quite sure exactly what it would return, but we can think about it a bit more if the datum status stuff doesn't satisfy your needs for this.. This has been substantially rewritten since the this issue was open. I'm not quite sure what I meant when I opened this though, we still have a client per worker but that seems reasonable to me now, we can't share a client between workers because they're on different machines so it's a different connection. Ofc that client is actually a workerClient not a pachClient so maybe that's what I was talking about? Either way I think this has been addressed or, at the very least, I'm not sure what we'd do to address this since I can't really understand what I meant. Closing.. This looks good to me, although I don't feel totally confident that I've understood all the concurrency logic in worker_pool. But that was true before this pr.. Yeah, I think using some mounting tricks could definitely pay dividends. Ideally we'd be able to just symlink in the data the job needs to process.. @gabrielgrant good point, everything here has been implemented.. Hmm, merge is a little bit weird here, looks like this was based off of master not ui_deploy. One comment about naming, but other than that LGTM. Many thanks for the pull, would you mind signing our cla quickly and we can get this merged in. https://pachyderm.wufoo.com/forms/pachyderm-contributor-license-agreement/. Awesome, merging. This should be pretty simple to fix by using an emptyDir instead of a hostPath.\nHowever there's an unavoidable problem here that I don't think it's possible to process datums that are larger than the disks the VMs have. You could do it with streaming but you'll still get in trouble with the output dataset being too big.. @sjezewski that's correct and in particular if you want to have them at the root directory you can put / or \"\" as the path argument. So your command would look like:\npachctl put-file kallisto_input master / -r -f gs://pici-east/kallisto-data/sampled\nThe reason we don't do this by default is that you can pass multiple urls to -f and if they happen to have 2 files under them with the same name then you'll get a very strange result since their data will get concatenated.\nDoes this work for your purposes @rj3d?. Looks like you edited this to not include it so you've probably realized this yourself but just to clarify there doesn't seem to be much sense in having --from default to HEAD since in that case rerun-pipeline won't do anything. It'll just process the next commit that comes in with the code it would have used anyways, exactly what it would have done anyways. It seems like by default rerun-pipeline should leave FROM blank which will cause all commits to rerun, that's what I'd expect from a command called rerun-pipeline.\nAlso worth discussing here is how --from is going to work with multiple inputs.. LGTM. 1 comment but other than that lgtm. And not totally sure if that comment is worth addressing.. Good catch, LGTM.. One minor comment, then LGTM. Hmm, this gets a little weird I think because what you're describing is also how rerun-pipeline is going to work.. Jobs have a Reason field now which is used for this. It reports some errors, probably not all errors it could.. LGTM. All LGTM. The only 2 here that I think should really be the same are created_at and created, probably preferring created since it's shorter. Other than that these names indicate different meanings.. Reminder that I should remember to use these on the worker pool connection code once the helpers are merged.. We use fairly reasonable defaults now.. I extended the TestDatumStatusRestart test to make sure that it sees 2 workers concurrently processing datums. Seemed like the most obvious way to test it.. Alright, this has turned out to be a little bit complicated. It seems that k8s services don't consitently round robin route, sometimes it just makes every single connection go to the same worker. I'm going to make the logic a bit more sophisticated tomorrow but this unfortunately isn't quite ready to merge.. This all LGTM.. So is the hypothesis that this will fix #1674?. I have not gotten it to fail on master yet, outputting enough data to exceed the limit seems to take a prohibitively long time to run the job.. Actually one thing I could do is try running with an artificially low MaxMsgSize then the test wouldn't necessarily work to alert us if this broke again but we'd at least know that it's not doing nothing.. Do you remember what you did to reproduce this? I just tried this out and the Service seems to get deleted.. I'd prefer not to do this, it's very common to have to look up whether bounds are inclusive or exclusive in all walks of programming and I haven't seen many examples where the name of the field alone makes it completely clear. Neither of the names you recommend seem better to me. Do you want to process commits that were added after / since this commit was finished? Or data that was added after / since the commit was started? I'm very dubious that there's a name which conveys exactly what we want short of latest_commit_not_to_process or something overly verbose like that.\n\nMore generally speaking, I'm not a fan of the logic that anytime something confuses a user that means we should change it. It's basically impossible to come up with single word names for all the fields that are going to mean exactly the same thing to everyone, if we change things every time it confuses people then we'll likely wind up with a spec that's not any clearer but is also very unstable.. I actually think we can this automatically without needing a flag. The only stick in the mud right now is that we include the version number as part of the hash for the pipeline. If we stop doing that then we'll do the right thing in basically all cases. The only thing is we'll want a way to rerun things without changing the Transform, maybe we could just move the version into the Transform itself as a way to update just the Transform?. Closing in favor of #1963. Hardest lyfe, lgtm. A UUID per run might be a reasonable way to do this, but it's also pretty easy right now to do this by looking for the start processing and finish processing log messages. You can get this with --raw.. Actually we can kind of have an integration test for this, I added a test that creates a pipeline with a GPU request and checks to make sure it propagates to k8s. The pod won't actually schedule because the machine doesn't have a gpu so it's not a real test for gpus but it tests things up to and including the k8s api which is something.. As of 1.7 this should be the case.. Could we also make it so that the error messages that get printed when the UI isn't up get swallowed?. Fixes #1767 . Good call with this, LGTM.. We have a check for this now as well.. LGTM. I don't know, seems kinda over engineered. But I suppose I'll allow it, LGTM.. This makes me a little uneasy, initially we added that flag because people got themselves confused by pulling an image that overwrote the existing image which was actually the one they wanted to use. This also can get kinda weird if you're using single node docker mode which most of us do for development.. I'm a bit confused, it looks like the only way you're going to get a none \"\" value for the flag is if you set it manually in your manifest which people probably aren't going to do. So effectively we just won't have a PullPolicy here which means we're kind of accepting k8s somewhat weird default, right?. I don't think it's a particularly sensible default, for example a pretty common workflow for me on dockerized k8s is to build the opencv image, tag it as opencv and then make a pipeline that references. This now breaks because k8s tries to pull an image called opencv which it can't because there's no username or registry on it. I'm pretty dubious of adding features to make using latest easier because in my experience it's more trouble than it's worth and makes other things confusing.. Thanks for sharing this use case @kalugny, I'm still not totally sure I agree that :latest would solve this well. The issue is that Pachyderm won't just d/l the new image the next time it's run. Pachyderm keeps container's up between runs of a pipeline, if those containers are using a :latest tag then when it gets updated you might still be using the old version of your code. Even worse, you might wind up with a heterogeneous set of clusters some of them being the new version some the old. This could get very confusing. To ensure that you're using the new version of the containers you'd need call pachctl stop-pipeline pipeline_name && pachctl stop-pipeline pipeline_name on all of the pipelines that use the image. This strikes me as a worse experience than calling update-pipeline which I think is really the right tool for the job since you are actually updating what your pipeline does with this operation.\nHere's how I'd do this, first organize all your pipeline specs that use the same image into a single file, this just makes things easier organizationally, it's not really required. Then when you want to update your images you can replace the image using jq (or just use a text editor) and then call pachctl update-pipeline -f pipelines.json. I realize this isn't a ton better than what you came up with yourself, we may be able to make this experience better with #1833.. @kalugny we've hit similar thing with the go client. Normally I've seen this with sending messages that are too big. Is it possible that what you're seeing is from the server returning that error rather than the client? Also just to confirm this is happening on a call to PutFile, if so it seems unlikely to me that the problem is the size of the response because PutFile has an empty response.. I see, perhaps we should add a way for requests to specify what the largest response size they can accept is? Is that number easily available to you on the sender side?. LGTM. I think this should be configurable on a per pipeline basis rather than a per deployment basis.. Comments should be addressed.. After talking internally a bit I think there's already a pretty good way to solve this problem with the existing system by giving an explicit name to your inputs. Using this feature your spec would look like this:\njson\n{\n  \"pipeline\": {\n    \"name\": \"example-pipeline\"\n  },\n  \"transform\": {\n    \"image\": \"ubuntu\",\n    \"cmd\": [\"sh\"],\n    \"stdin\": [\n      \"my-command $(find /pfs/data -not -path /pfs/data) -o /pfs/out\"\n    ]\n  },\n  \"inputs\": [\n    {\n      \"repo\": {\n        \"name\": \"input_repo\"\n      },\n      \"glob\": \"/*\",\n      \"name\": \"data\"\n    }\n  ]\n}\nWith this spec you could freely edit the name of the repo you're using and the code would keep working.\nI prefer this to having env vars for each input because it's a little bit more explicit since the names are meaningful to humans and can be used to document what this input data means to the transform code. Furthermore I think you'll get a more subtle version of this same issue with env vars when you add a new input. Adding a new input means that some of your inputs will go from being INPUT_N to INPUT_N+1 and you'll have to figure out which inputs should be incremented and which shouldn't. With explicit names you avoid this problem.\nDoes this work for your needs @brollb?. LGTM, let's merge!!. 1 comment you may wanted to address about a lingering Printf other than that looks good to me.. This seems reasonable to me. Although I don't really know how people do these things. @sjezewski do you happen to know if this is the standard way people do stuff like this with Travis? I thought maybe they had their own encrypted secrets method or something. Also where's the private key for this encryption stored?. LGTM. I believe this is a fairly isolated issue caused by the fact that there was some data lying around which was created by the 1.4.6 RC which had a slightly different schema from the actual 1.4.6 release. 1.4.6 should migrate smoothly from 1.4.5 but not from the RC state unfortunately.. I believe this issue was due to the fact that we cut an RC for @emk before we released 1.4.6 publicly and the schema got tweaked in between. Sorry for the annoyance on this one @emk. Since this was an isolated issue that shouldn't be a problem again since I don't think anyone will be using that RC again I think I'm going to close this issue.. Hi @vuc4, sorry you're hitting this. Would you mind grabbing logs for us so we can see that the error is? You can do this with kubectl logs <pachd-pod>. You should see a line like error running pipelineManager ... which will have the error in it.. LGTM. Hey @swimablefish I just tried reproducing this and wasn't able to. Pachyderm appends by default, so unless you're explicitly calling delete-file before you do your put-file you'll be doing appends. Could you try doing pachctl get-file abc c7ab05ae6bc54fafa3c6c0bd64a5ced7 pachyderm.sh | wc -c and see what you get? That should tell you the size of the file at that commit, when I did this, using a delete-file so as not to append, I got the size of the file I'd put which was the same as the number of bytes I got out from get-file.. One small change on naming of a method, other than that looks good to me. Also, there should be a test for GC right?. LGTM, just need to rerun make doc so the doc page is called garbage-collect instead of gc. One huge benefit of this PR is that it greatly reduces the time needed to compile pachyderm_test.go since that file imported assets.go which was 18K lines. Afaict that was literally the only place in our code base that was using assets.go. LGTM. @derekchiang this is finally ready for a look. It doesn't quite do everything we talked about. Notably missing is the ability to display old / new data, right now you can only do new data. This means it works for additive workloads, but workloads where you are overwriting files don't work, I think this actually might be ok since I'm not sure if we have an real use cases for incremental that involve overwriting files. As I was writing tests I realized that the user experience for writing jobs with new / old is pretty bad so I want to think about it a bit more and see if maybe there's something better.. This strikes me as a bit outside the purview of pachyderm. You can get a really good user experience for this stuff using jq it's very unlikely that we'd ever be able to have an experience anywhere near as good jq's.. Newest at the top is fairly common for unix tools (ls for example) so I'm somewhat reluctant to break with that standard. This normally works because you can pipe the output to another command which will give you more control over how you view it. I'd try doing pachctl list-job | less that will start you at the top with the new jobs and allow you to scroll down to older jobs. You can also do pachctl list-job | tac which will get you the newer jobs at the bottom. One last one that might be useful is pachctl list-job | head, that will give you the 10 newest jobs.\nDoes this seem suitable for your purposes @vuc4 ?. I added some docs for --split in this PR as well to address another issue.\nFixes #1770.. These errors were silenced before, we inadvertently unsilenced them at some point.. Example of what the output of the second step looks like:\n\n. @dwhitena fixed. Looking at load-steak-pipeline.log it does seem that a datum was processed twice by different jobs. I'm wondering if this is a case of the bug with 2 jobs running concurrently. @derekchiang you're planning to look into that one soon aren't you?. This was fixed as part of https://github.com/pachyderm/pachyderm/pull/1832.. @dahankzter we should be able to get this into the next RC, @dwhitena could you ping the minio guys and see if they have any guidance on this? (Or maybe just want to do it since they know the code best?) otherwise it seems like it shouldn't be too hard for us to figure it out.. Awesome, thanks for your patience while we addressed this. Closing.. @rj3d pipelines have a field for Secrets which will mount a k8s secret inside of containers for that pipeline. I don't think it'd be a great idea to mount all secrets by default, that seems easily exploitable. Does what we have now work for your purposes?. Happy to help.. Afaik there are no API-breaking changes in this release.. PR that only removes lines... insta LGTM. \n. The only thing that needs to be fixed here is the docs. edges.json at master has the correct format.. @thedrow agreed that this is not a very clear error, we should catch it and output something clearer.. This LGTM, what's standing in the way of this working on finished commits as well?. This seems like a reasonable thing to support and it shouldn't be too hard.. Definitely see the use case and think that this is something we'll want to service. I'm not sure that TTL is a good abstraction for this though, because we should be able to know the lifetime of the files ahead of time, since they're being consumed by downstream pipelines. I think #546 is probably a better implementation path for this functionality.. Pachyderm supports only very dumb append semantics. It just appends the bytes of the different files together, it has no notion of the format of the file or smarter ways to append files. This is ok, and something we're unlikely to change, because you can implement smarter merging semantics as a different pipeline step. For example instead of writing your Parquet files to /pfs/out/foo you'd write them to /pfs/out/foo/1, /pfs/out/foo/2, ... and then chain another step onto them which had a glob pattern of /* your code would then see a directory /pfs/repo/foo which would contain all of the parquet files and your code could merge them together. The downside of this ofc is that it'll store more data because it has to store the intermediate step. #546 is the best solution that that problem.. Job specs are updated, they still have an Inputs field for backwards compatibility reasons. That field will be removed in 1.6 when we fully shut off that feature. The original point of this issue still stands though, we should update the example.. This has been implemented and merged into master.. I think @msteffen saw something like this too when he was running tests. Can the answer here be as simple as just setting an explicit request to 0 by default rather than letting it override?. Surprisingly simple. LGTM. Hey @rj3d sorry this isn't working for you.\nI'm a little confused about what's going on here so sorry if this is a dumb question. Is gs://tesla-data/metadata/intervals a directory or a file?  If it worked with the -r flag then that probably suggests that it's a directory which means passing the same path without the -r flag probably wouldn't work. Also is intervals.csv a file that's present or no?. Hey @rj3d, sorry this isn't doing what you expected the behavior here is a little bit tricky. The reason we don't create the files with the structure that you expected is that it's a little unclear what the correct behavior would be if two of the paths had a common filename. I.e. if you had gs://tesla-data/raw-data/pt1 and gs://tesla-data/metadata/intervals/pt1 in that case our options would be:\n\nconcatenate the files\nhave 1 file overwrite the other\nerror\n\nAll of these seem kinda like gotchas so we opted to make the command in this form a little bit harder to get name collisions with. We could consider changing this since maybe name collisions are rare enough that we shouldn't make the decision based on them.\nAs for the WARN you got, that's likely harmless but probably indicates that you have a file in your gcs bucket which is literally called metadata/intervals/.. LGTM, is there anyway we can get more informative error messages out of failed JSON parsing? I seem to remember us having that at some point but this just seems to be returning the error message directly from Unmarshal. Will merge as soon as CI passes.. This is pretty tricky and may never really be feasible.\nHaving commit_modified for a file wouldn't get you much closer to this. A 1000 datums across 1000 commits might have gone into creating a file, the most recent of those commits is going to be commit_modified that will only get you 1/1000 of the logs you want.. > I thought appending output only happened within a single commit\nThat's the right way to think about what the output is, but remember that pps is smart enough not to reprocess datums, so it might be that the actual processing of the datums which are being merged together for this commit was done in a different job with different input commits that happened to have the same content for datums.. There is also somewhat doable now with stats.. Fixed by #1956 . update-pipeline isn't supposed to accept --description, \"description\" is a field in the spec but that doesn't mean it's a flag. I don't think adding a flag for each field in the spec really makes sense because many of them are difficult to represent as simple strings.. Hmm yeah, --description is needed for repos because there's no repo spec that you pass. For pipelines I think it makes a bit more sense to have people just pass it as part of the spec.. The underlying issue here is #1414. master will give you the latest commit unless you're committing to multiple branches at the same time in which case the latest commit isn't super useful. Because files will appear and disappear from it.. I think I'm going to close this for now in favor of #1414. Feel free to reopen if you think there's something different between them.. I think we actually support this pretty well, if you do an input like this:\n{\n  \"cross\": [\n    { \"atom\": {\n      \"repo\": \"myrepo\",\n      \"glob\": \"file1\"\n    }},\n    { \"atom\": {\n      \"repo\": \"myrepo\",\n      \"glob\": \"file2\"\n    }},\n    ...\n  ]\n}\nThis will have myrepo/file1 and myrepo/file2 in every single datum along with whatever ... produces.. Gonna close this because I think we satisfy this use case but feel free to reopen if I'm wrong.. @banchee the issue you're hitting is that the inputs are winding up with the same name because we use the repo name when there's not an explicit name specified. You can fix this by explicitly setting the name, so something like:\n\"input\": {\n    \"cross\": [\n      {\n        \"atom\": {\n          \"name\": \"profiles\",\n          \"repo\": \"platform_borrower\",\n          \"glob\": \"/profiles/borrower_all.jsonl\"\n        }\n      },\n      {\n        \"atom\": {\n          \"name\": \"applications\",\n          \"repo\": \"platform_borrower\",\n          \"glob\": \"/applications/borrower_all.jsonl\"\n        }\n      },\n      ...\n    ]\n}\n\nThat syntax for doing multiple glob patterns is a lot more concise, however it's unclear if it's a union or a cross. I could reasonable see that meaning \"show me all files that match either glob pattern in a single datum\" or \"show me all files that match either glob pattern in different datums\". We'd need to have some way of indicating which one of those the user wanted which would start to duplicate the functionality of cross and union.\n. Closing in favor of #1963 . Meme confirmed as glorious.. Closing in favor of #1036. #1036 is a bigger feature but the functionality here is a subset if its functionality and it makes more sense to implement them together rather than have 2 different ways to store multiple copies of a pipeline.\n\nI think this could be as simple as just adding repeated PipelineInfo pipeline_history = 23; or whatever into the PipelineInfo\n\nThis is a very weird schema, representing different version of an API object recursively. What does pipelineInfo.PipelineHistory[0].PipelineHistory represent?\nWe don't want to just hack in this functionality we want to do it in a coherent way that's going to match with the rest of the API.. #1036 requires having multiple versions of the same pipeline so that you can have, for example a production version and a dev version that people are working on. These would be the same pipeline, in that it would have the same name and output to the same repo. But they both exist concurrently within the system. My plan for #1036 would be to first implement having multiple versions of a pipeline stored in etcd with only the most recent one running. That would satisfy this issue I believe, then from there we'd start building the more complicated parts of the workflow like having mutliple running versions and things like that.. And it does seem a bit weird to me to have a history populated on a top-level PipelineInfo it seems a lot better to have the different versions of the pipeline exist at the same level in the api rather than recursively referencing each other.. Simple enough, LGTM.. This is now supported. You can't name unions but atoms within a union can be given the same name.. I think I prefer solution 2. It seems cleaner because pachd is the thing that normally makes the RCs so this way it's very easy to reason about potential race conditions. It also means not having to add a step to the upgrade process which is always preferable to adding a step.. I don't see any inherent problem with the commit durations not corresponding to the duration of the job. Jobs record their own durations. As it exists now the duration of the commit indicates the time it took to create the commit within pfs which is likely to be small and not particularly interesting, but that's absolutely fine. Changing our implementation so that we can create a meaning for a field seems pretty silly to me.\nThat being said there may be some other good arguments for creating commits incrementally such as allowing people to explore the output of a job while it's running.\nSimilar to this, duration for jobs are a bit wonky. They start counting when the job is scheduled but that means if a bunch of commits come in together then the last commit will start a job that has to wait for all the other jobs to finish and count that time as part of its duration. Ideally job duration would only count actual time spent processing.. This PR is a bit unfortunate, it turns out we were using different naming conventions in our protocol buffers which is going to require some migration to fix.. This one we're going to be looking at doing in 1.8. :goat: meme. One comment about some printfs, but other than that lgtm.. LGTM. @gabrielgrant yes, it is, I'm going to close that issue in favor of this one.. Storing multiple versions is going to be tracked in https://github.com/pachyderm/pachyderm/issues/1036 not in this issue. It is something that we'll add though.. We definitely could, but I think it's going to be good to have true version numbers such that any change results in it incrementing and users can always tie a particular version number to a particular pipeline info. It's really weird to think about not incrementing the version number in a scenario where we're storing multiple pipeline versions because you'd inspect the different versions of the pipeline and see 5 copies of version 2.. @mightyguava everything here is implemented here except for a nice way to rerun specific datums from previous runs that ran without error but still outputted \"incorrect\" results. I'm going to spin that off into a separate issue so I can close this.. Makes sense regarding the log statement before Process seems like you're cool with removing it now that the Master change seems to have fixed things. FWIW, @derekchiang and I have a planned refactor that will make the long running Process call go away completely that I think we're pretty sure we want to do. Happy to explain that more to you in person but the upshot in terms of this PR is that we don't need to optimize to figure out if we want to get rid of the Process call.. @abhitopia sorry you hit this, seems like this should be a very reproducible issue, I'm going to take a crack at reproducing it right now.. I wasn't able to reproduce this, is it possible that the file was actually put under the path configurations/config.json/configurations/config.json? The semantics of put-file can be a bit confusing sometimes so it's possible to wind up with paths like this.. Another thing that occurred to me while I was working on this was that it might be handy to have a field on each object which records which version of pachyderm it comes from. Certainly not a requirement for this PR but I'd be interested to hear your thoughts on if you think that'd be useful / worth it.. @ssaamm CI failure is indeed unrelated to the change. It has to do with not being able to get a private key because Travis protects it in external pull requests for security reasons. No worries, we'll handle it.. This looks good, does this PR actually move the vendored code too? It seems to just move dependencies.. Ahh, I see. LGTM. Fixed by #2218 (although it was kinda already fixed).. One thing worth considering here is whether or not we want to merge the auth stuff as part of this PR. It's pretty useless, I added it in an attempt to use the paywall stuff from the ui but it doesn't seem to correctly check tokens. Only reason to keep it in is if it's useful to @msteffen by giving him some starter code on auth stuff.. Alright, comments should be addressed.. Oh believe me, she tried, that's why commas are so triggering for me.. One other wrinkle that's probably worth talking about here is that these features are only going to be present in an enterprise build. So we need to somehow engineer this in such a way that we can also have a no-users version where everyone is still allowed to do everything.. I'd prefer the url be:\nGET http://pachd:658/pfs/<repo-name>/<commit-id>/<...path/to/file...>\nseems bit redundant to have files when the f in pfs already stands for file. Also, even though this isn't adding support for repos or commits I think we'll want those to be under pfs/<repo-name> and pfs/<commit-id>.. @derekchiang alright, pretty sure all comments have been addressed. To answer your question about testing this has been tested manually several times including on the most recent commit in this PR.. One comment about something I think could be done more easily. Other than that lgtm.. This totally LGTM, my one worry is that I think this is likely to cause a pretty big merge conflict with a few PRs I'm about to land. How easy was doing this migration? It might be a little easier to get my stuff merged in and then do the file renaming. (Although that's definitely easier for me personally so maybe I'm not right about totally easiness.. I'm really having trouble understanding what this is asking for. The default behavior of update-pipeline is to not re-process things and provenance should never change after commits are finished so in what sense is it not maintained?. LGTM. So far this PR has no API changes. I think I'm likely going to merge this in and then do any API changes in a separate PR.. @derekchiang all comments should be addressed, another look whenever you get a chance.. Small tweak requested but this LGTM. \n. This seems reasonable to me.. Looking at this again, I actually think all the data you guys are after is already present in the system, it's just not exposed in the nicest way. You can think of a branch has having an updated_at field equal to the Finished field of the head commit a created_at field equal to the Finished field of the root commit of the branch.\n@JoeyZwicker I don't think your use case applies here, this is about Branches, not Files. The information you're after is already present though, it's just the timestamp of the first commit a file shows up in.. I'm not sure if adding a --force flag makes sense for this, especially since kubectl delete (which is what pachctl undeploy is using under the hood) already has a --force flag with a different meaning which we may want to push through at some point. If you're looking for a way to get around the interactive aspect of undeploy for the purposes of a script I'd recommend doing:\nsh\nyes | pachctl undeploy -a. Alright, made a pass addressing all comments. Save for the the visual ones which you said you'd work on.. The simplest solution to this would be to shuffle around the master responsibilities a little bit. The master pod currently does 3 things:\n\nwatch for new commits, and create jobs when they arrive\nrun those jobs, pumping data through the workers\nfinish those jobs and materialize the final result\n\nThe first responsibility is the one that forces us to keep a single pod running.\nThe easiest solution to this would be to move the logic that covers the first responsibility into the pipeline master thus eliminating the need to keep a pod running for the master.\nThe down side to this is that it concentrates the workload slightly more on a single node but I don't anticipate it being a problem unless there's a very large number of pipelines.. Overall seems to make sense, a few things I'd like to see changed in tests and a comment that I think is incorrect.. LGTM, I like have NewTestingBackoff. Closing in favor of #2075. I'm actually not quite sure what we'd put in these docs, pachctl deploy deploys etcd in the best configuration we know of, which doesn't really entail anything fancy. I'm a little unsure what we'd put in this doc that we couldn't just include in pachctl deploy so that it happens automatically. Right now there just aren't any special deployment options that we know of that help make etcd work better for pachyderm.. Datums don't correspond to a single job though, there's more than one job you can request to get the same datum.. One clarification requested. Other than that lgtm. Fixes #2075 . LGTM. \n. The issue here is that FlushCommit when the output commit is finished which happens before the job is completed because one of the ways a job can fail is if FinishCommit fails. I can think of a couple of ways to solve this: you could wait for the the job with complete by doing InspectJob w/ block = true within the test, this would be correct with our current semantics. You could also make it so that if you call ListDatum on a job that hasn't completed then it waits for the job to complete before looking for the stats commit.. Per offline discussion I think I'm going to close this. While this behavior is a bit confusing it is technically in keeping with the semantics we've established at this point and we seem to have a fix for the immediate problems in the test that was failing.. Take a look at the CI runs here, it looks like one of them is only a partial run which is a little disconcerting. One comment about a comment, but other than that LGTM.. We have a fix for this which is in master and should be released shortly in 1.5.2. Until then you can work around this issue by adding --dry-run to your pachctl deploy invocation and then  manually editing the BLOCK_CACHE_BYTES environment variable to be a nonzero value. . @ritazh many thanks for the pull. This all LGTM. Would you mind signing our CLA and we'll get this merged in asap.. CI is something on our end, it won't run on external PRs for security reasons. I'll fix it.. CI passed on #2141.. No thank you! I just hit the merge button :stuck_out_tongue: . This will happen as part of a bigger overhaul to make it possible to create pipelines directly through the k8s api. Which will naturally open up all the various k8s extensions to pipelines. . I would agree with this.. fixes #1531. \nHall of fame PR. LGTM.. I'm not opposed to running this, although if we're going to run it it should fail ci if it fails, otherwise what's the point. Do we currently pass the extended linters? or is this going to require fixing a number of issues to merge?. @thedrow I don't have any preconceived notions about which linters we want and don't have the bandwidth right now to dive in and figure it out. My requirements for merging this are:\n\nlinters must run as part of CI\nCI must pass\n\nIf you can get it to that point we're happy to merge, but we don't have the bandwidth on the core dev team to complete this PR unfortunately.. Alright, comments should be addressed. Whenever you have a chance take another look.. I think most of the things here are worth doing before 1.6 release with a few exceptions:\n\nBuildCommit also does no ACL checks (along with GetCapability, RevokeToken and a few other API endpoints)\n\nSince BuildCommit is a write we could conceivably skip it since it can't leak any data.\n\nCreate a guide to using Kubernetes's RBAC features with Pachyerm\n\nThis seems like it might entail some integration code in addition to docs. Probably shouldn't block release on it.\n\nFigure out how to avoid exposing users' S3 credentials to anyone with direct access to their kubernetes cluster. Keep S3 credentials private to Pachyderm, so that users can't circumvent GetFile to see repo data\n\nHas this changed as part of 1.6? I guess we're starting to have actual security guarantees so that's a change. I'm not 100% how we'd secure creds from people who have access to kubernetes. I think it might make sense for this release to assume that k8s still needs to be locked down for pachyderm to be secure and then handle this in the next release.\n\nWe need to support authentication via LDAP, for customers that already have LDAP systems set up\n\nThis is valuable but I think not needed for this release. We'll hopefully be working with customers to get this implemented in 1.7.. @ritazh many thanks for the pull. CI passed in #2164.. Talked about this offline with @JoeyZwicker. I think the easiest way to solve this is to prevent making commits to repos that don't have a provenant commit for each of the repo's provenant repos. That will make it substantially harder, although not impossible to commit directly to an output repo.. Committing to an output repo breaks a number of Pachyderms internal assumptions about how provenance works. That's the reason that it doesn't process the data, it has no provenance which Pachyderm uses to figure out what to process. The other big issue with committing to an output repo is that if new data comes into the input repo then the committed data is going to get overwritten which will lead to very confusing results. One fairly reasonable way to do this would be to have your pipeline take 2 inputs in a union, 1 being the output repo it already takes, the other being a side repo that you can commit raw data to. That way it'll be possible to easily add data.\nLong term it would be nice if there was a way to force reprocessing on particular datums, it's not immediately obvious to me how that would work with our API but I think it should be possible to get it to work.. @ClaytonJY manual commits to output repos break too many of our internal assumptions. Provenance is used to figure out which data should be processed together. For example if I have a DAG like this:\nB\n  /  \\\nA     D\n  \\  /\n   C\nPachyderm uses provenance to make sure that D will only process commits from C and B that came from the same commit to A. Without this you get a lot of user code which crashes because it expects to be able to match things between its 2 input commits. Worse, sometimes it gives wrong answers, such as numbers getting normalized to the [0,1] range and winding up outside the range because the max value you're using for normalization is the max value of a different commit. Allowing output commits would require, at the very least relaxing this safety, and possibly sacrificing it all together.\nThe other major issue with manually committing to output directories is that the data won't actually \"stick\". Output commits, unlike input commits, don't inherit content from their parents. Each time we build the commit from scratch by merging together the outputs of all the datums that were processed to create it. This is why deleting an upstream datum causes its output to disappear. If we wanted to be able to allow manual output commits and keep the ability to delete a datum we'd need someway to distinguish which data is actual \"output\" data and which data is \"faux\" output data so that we could preserve the latter and not preserve the prior. This opens up a whole can of worms, you could in theory have a file that's part real output and part faux output, you can get merge conflicts between the two.\nSo the tl;dr is that manual commits violate far too many of our assumptions to be workable. It's not so much the implementation work, although it would be a lot of work, but more the work required to figure out all of the corner cases in how the system should actually behave. I'm not fully convinced any amount of work would get us to a sensible set of semantics for manual output commits.. Yeah, it's just checking file count, which I think is a pretty good indication that overwrite is working correctly. I'll make the content different between the puts and check that it's right though. Just to be safe.. @dwhitena ok, it checks for the right content too now.. Hasn't gotten any less relevant, but also hasn't gotten any easier.. One more thing to do on this which is adding the files in the datum. Writing this more to remind myself than anything.. Fixes #1883.\nFixes #1204.. Fixes #2159.. This approach won't work without a larger scale change that I'm not quite sure we want to make. So I think we're just going to not do this for now.. Oh lovely, this will make logs much easier to read.. Fxied by #2206. It's implemented. Closing.. The main thing this PR is doing is adding FileInfos to DatumInfos` which tell you about the files that were processed as part of a datum.\nI also wound up cleaning up a few things such as the InputFileID field being renamed to DatumID since this was confusing to people.. We have more than a proposal actually, this is in code review right here: #2231. Lol, that is a good point. I don't really feel strongly either, although I think we should do that same thing for update-repo that we do for update-pipeline and there we have a separate command. It seems easiest to just do that for update-repo too.. This has been fixed in a slightly different way by including the fields in RepoInfo and returning that directly from InspectRepo.. This PR makes it so that common pipeline failure causes, currently only image related issues, result in a pipeline failure rather than the pipeline getting stuck in starting. It should be very easy to extend this to other types of failures as long as we can figure out what the Reason k8s gives you for the failure is so we can add it to the list.. > The fix in #2233 works for arbitrarily many data repos? (i.e., a different env var for each name?)\nyes, each env var will be the name given to the input, which defaults to the name of the repo if unspecified.\n\nonly issue here is you'd still need to use stdin because it's not giving you the subpath from the repo for the datum.\n\n$input will be the full path to the datum. For example if you have a repo foo with a file bar in it and you use a glob pattern of * then $foo will be /pfs/foo/bar. The only time you should have to deal with subpaths is if your datums are actually directories which contain multiple files in which case you'll have to traverse those files. I'm not sure if there's a way we could make that use case better, we can't really bind them to env vars because we don't even know how many files there are so user code is always going to have to handle that variability.. Realized I didn't answer your last question, the env vars should work outside stdin too. So you can just as easily have your python code access them directly.\nAlso I'm not totally sure I answered your question 2nd question. When you pass /pfs/out/${input##/.*/.*} what is that communicating to your code?. Subsumed by #2293.. We could push that image, although I was under the impression you didn't actually need it anymore because k8s could auto provision drives.. This PR has a few changes which are only tangentially related to services:\n\npipelines that don't specify a parallelism spec now default to CONSTANT:1 instead of COEFFICIENT:1, this is a change I've wanted for a while because it helps to sidestep the issue where the service account doesn't have sufficient privileges to inspect the nodes. It's also nice for services because services have to run with CONSTANT:1 parallelism right now and this saves users some writing.\nPipelines now respect the user working_dir and entry_point of docker images. This is convenient for services since it allows you to, for example run jupyter notebook containers out of the box. It's also convenient for normal pipelines.. @derekchiang another look when you have a chance, comments should be addressed.. @derekchiang alright, cancel comment has been addressed as well.. LGTM and @poopoothegorilla has already signed the CLA.\nMerging this even though CI hasn't run on it since it's just a very small docs change.. @poopoothegorilla has signed the CLA and this isn't code that's tested by CI anyways so I'm merging.. Hi @itssimon unfortunately we haven't gotten this implemented, could you tell us a bit more about your use case so we can understand the requirements for this feature better and also recommend potential workarounds for you?. @itssimon @gabrielgrant  the recommended way of doing this is to have to branches in place of the \"non-trigger\" input. One that's actually hooked up to the pipeline that you commit to only when you want to run the pipeline (master) and one that's not hooked up to anything and you can commit to it whenever you want (staging).\nNow when you want to run the pipeline with the latest version of the data available in staging you just need to copy staging to master with pachctl copy-file repo staging / repo master /. Copying is just rewriting pointers so it's a very efficient operation.. Nope, beat ya to it.. I'm not totally sure it makes sense. There seems to be some user evidence that there might be some cases where this particular call fails but others will succeed in which case we'd only lose pipeline error dectection with this but other stuff would still work. I'm not sure if that's true though, I haven't been able to produce the errors myself.. Changes in this PR:\nrun k8s diagnostics on startup and log results\npachctl deploy will now block until the server is up and return those log results\npachct get-logs with no arguments no returns pachd logs\nremove some other log output that wasn't super useful and made the output you see on deploy too noisy. LGTM, one more clarification asked for.. Is this just as easy as having worker pods use the same service account as the pachd pod? Or is there something more to it?. Fixed by #2605.. Sim Link:\n\n\n. @sjezewski another look whenever you have a chance. Also I tested this by exporting the images from dockerhub and importing them to gcr.io. The process is killed and relaunched, but in the same pod. This happens pretty quickly because the data gets downloaded in the background. And then swapped out via bind mount.. This is the function we use: https://golang.org/pkg/os/exec/#CommandContext\nwhich claims to send a SIGKILL so no opportunity to do graceful shutdown unfortunately, we could probably find a way to add that. I'm not sure how much flexibility the go library gives us though.. I'm guessing that kube 1.8 is the issue here. Something must have changed about the schema that we're running afoul of. Given that your server is still 1.7 I suspect that turning of validation may well work.. It seems that this is actually an issue with kuberentes: https://github.com/kubernetes/kubernetes/issues/53309 the best we can do for a fix on our end is setting --validate=false by default until this gets resolved in k8s.. @gauti038 are you able to create a simple non-pachyderm deployment like the one here?\n https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment. LGTM. LGTM I'm on the phone so I cannot hit the official button. But feel free to merge.. I really have no idea... you'd think that would be a major piece of test coverage.. Merging this so we can get it into a release.. Fixed by #2574 . I'm not sure how much of this we should do. Services, like anything deployed on k8s, could be killed at any moment or the machine they're running on could simply cease to exist. So we can never offer a real guarantee that services will have a chance to clean up before they're shutdown. It's also unclear what the use case would be for needing to clean up. All the use cases I can think of, such as syncing in memory data to disk or other services, are things that people shouldn't really be relying on, since we can't offer a hard guarantee that they'll work due to the aforementioned reasons and aren't really the sort of thing that you'd be ok with sometimes working.. We could probably do something like this, I think a better solution might be to just piggy back on docker login such that if you've called docker login before doing a --push-images would just work.. Fixed by #2574 . I'm not sure I see what the benefits of storing this would be. What would it allow us to do that we can't do now?. Getting --dry-run to work for pipelines would require a decent sized architectural shift. Not impossible but kind of difficult and it doesn't seem like that's the easiest way to solve this. Much easier would be to just added a section for tolerances like we've done for other k8s fields in the past.. I suspect this has to do with a message being too big which causes the pachd server to crash. @homme could you confirm this by checking a) if pachd does indeed increment its restart count when you run this command and b) if so could you do kubectl logs <pachd-pod> --previous to see what the crashing error message was?. This should already be fixed by the streaming list-file changes.. > There's easy ways to make the error go away\nAre there?\nThe status is that this comes up if there's an object that Pachyderm thinks it's written to s3 which turns out not to be there. That type of situation can cause all sorts of bugs, none of which are particularly fixable. It's possible we're doing something dumb which causes this particular issue but it's also possible that it's just a general corruption issue. Perhaps this should be moved out of 1.7 since there's no concrete thing we know of to fix.. > Have we changed our code to exit here instead of proceeding? Should we be doing that?\nYes\nBut all that means is that you'll get a nicer error here.. I really like the goals here and user story. I have a different proposal for the implementation, as discussed in the 1.7 planning meeting today.\nThe major change is that while the above approach gives us a way to very quickly get the users code into the cluster without having to build a docker container, this approach allows us to not push the code into the cluster at all, instead simply running it on the users machine. I think this actually simplifies a lot of things and gives us a couple of nice primitives that are useful on their own and combine to give a good experience.\npachctl checkout\nThe first thing we need is a way to get data onto the users local system. I think a checkout-* family of commands is the obvious choice since that's the name git uses. Here's the invocations I think should work:\n```\nCheckout a file at a particular commit, no file is the same as passing /\n$ pachctl checkout-file repo commit [file]\nCheckout a datum from a pipeline (uses the head commits for each of the inputs) datum id can just be a number from 0 to n. Most common use case will be datum 0 which is the default.\n$ pachctl checkout-datum --pipeline  --datum id\nCheckout a datum from a job\n$ pachctl checkout-datum --job  --datum id\nCheckout a datum for a pipeline that doesn't exist yet\n$ pachctl checkout-datum --pipeline-file  --datum id\n```\nThese are all useful but that last one is particularly useful for this issue since it allows users to quickly checkout what the datums will be for a job they're writing.\nrunning the code\nThis already gives users a pretty good way to run their code, checkout a datum, run the code they've written see what happens. There are a couple things though that might get tricky about this:\n\nThe worker also sets some env vars for the users code which may be needed by the code.\nRunning code repeatedly puts the onus on the user the clean out the /pfs/out directory so their code gets a clean slate.\n\nFor these reasons I think it might be useful to have something like pachctl test-pipeline -f <pipeline-file> --datum id which would checkout the datum, then run the code in one porcelein command.. > I think that will be a huge win for local dev as well.\nIt seems weird to call this local dev, this is really just semantics but in my mind local dev means the code runs on the same machine the code is getting written on. I guess this would be local if you're running Minikube, but that's extra overhead for the devs as well. I also think that even having to do a git commit to test your code is a bit too much overhead to have to do everytime you want to test it, so I really like the idea of being able to just test it by running code locally.\n\nMy concerns with test-pipeline are mostly around the fact that it isn't really a \"test\" of the pipeline. If I'm using my local environment, the pipeline could still behave differently when I actually build the images and deploy it. Am I misunderstanding @jdoliner?\n\nThis is a fair point, it definitely could behave differently, I'm not sure if that means we should use a different name though. The only thing that's really going to be a 100% accurate way to check if a pipeline will work is to actually create/update the pipeline and see it run on your cluster. We obviously don't want to call that test-pipeline so it seems like this is the most reasonable place to apply the name. A couple of other things we could do to make this a higher fidelity test:\n\nmake it easy to process all the datums, perhaps that should be the default\nmake it possible to run the code in docker which would allow you to check that the image exists and you don't have docker specific errors like user stuff, this should not be the default since part of the point of this issue is to save people having to run docker. > @jdoliner Regarding that point, is that different than injecting the code via GH, zip, or tar ball or maybe having a GH repo as input? In any event, I think something like that is needed.\n\nThis would be Docker running on the local machine, so the code wouldn't need to be shipped anywhere it could just be a volume mount. Taking a GH repo as an input is definitely needed. It just serves a slightly different use case than the other stuff we're talking about here since it's slightly higher overhead, requiring a git push to trigger it, but also higher fidelity since it's closer to prod.. Filling in some details on how I think we could accomplish this.\nI think we should add a new type of input called a git input. It would look something like this:\njson\n{\n    \"name\": \"pachyderm\",\n    \"url\": \"github.com/pachyderm/pachyderm\",\n    \"branch\": \"master\"\n}\nThis would be an input that would yield a single datum which would be the whole repo. Having this datum in a pipeline would cause the pipeline to subscribe to the repo via commit hook so that when a new commit comes in on the specified branch the pipeline gets triggered. This would allow the classic Heroku style workflow of git push as a way to deploy your code to Pachyderm.. Slightly addresses #2411 by making the error message nicer, still doesn't solve the underlying problem which is more complicated.. I think we only want to set the limits section, according to: https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/#api. I think the point of TLS is that it'd protect people snooping outside the k8s network. If you have access to the k8s network then there's probably not much stopping them snooping, but there's good ways to prevent people from having that access. There isn't a good way to send pachctl traffic over a secure channel.\nThis is affecting some users so we're going to get this scheduled ASAP.. This seems a little bit weird and potentially confusing for a couple of reasons.\n\nYou could cross 2 files and only get a single file showing up in /pfs if it happens to be the same file.\nIn the case where it's different branches (which isn't possible right now) you wind up with a deeper file structure than normal which means code will have to be rewritten.\n\nOverall it doesn't seem worth complicating this matter unless there's a clearer use case that we're not satisfying here.. I believe so, but we haven't audited all error messages / names.. I believe if you do export DOCKER_CONFIG=/home/kav/docker then it will work.. We simply use this function to get configs: https://godoc.org/github.com/fsouza/go-dockerclient#NewAuthConfigurationsFromDockerCfg\nWe could start trying to proof ourselves against various OSes but my impression was that the DOCKER_CONFIG env var was at least somewhat standard. It'd be nice if we could just rely on the standards to work but perhaps that's too optimistic.. In a cross if one of the inputs didn't have a commit yet you wouldn't have any datums at all since a single empty repo means there's not going to be any tuples. This isn't so in a union.. This was actually fixed here #2610. . Closing in favor of #3069 since that's more fleshed out.. One thing I want to add to this before I merge is the ability for users to specify how many chunks they want.. Why would we do this?. That makes sense, this is, as you may have guessed, not terribly hard to implement.. The idea of using k8s state as a source of truth seems awful to me. Most of our major releases change our k8s assets in some non-trivial (not a version bump) way. How on earth is this going to work for those? I don't see any way it can unless we find some way to \"merge\" our changes with the users changes... and then we'd have to deal with potential merge conflicts... the UX on all these failure cases is really going to be awful.\nI also don't agree with the basic premise of this issue. I don't really see the pachctl undeploy pachctl deploy flow as deficient except in cases where you're manually modifying the manifest. Having to store a set of command line with arguments in version control so you can deploy the same way again doesn't seem unreasonable at all. What if the cluster gets destroyed by an outage or an intern or something? Users should enable themselves to deploy the same cluster again and the currently running cluster is not a good place to store that information. That information is code that should be in version control, not stored in live servers. Cases where a user has manually modified the cluster are, admittedly more difficult but I think a) those cases are relatively rare, b) we can't do anything about them. There's no consistent way we can reapply those changes. If users want to make their manual modifications to k8s manifests automated then they're going to have to automate them themselves.. > Interesting point. Why is that awful? It seems like that's what k8s does for their components.\nI think ultimately this comes down to controlling the data format. K8s assets are a format that k8s controls so it makes for them to use those as their source of truth, when the schema changes they understand the changes and can somewhat easily write migration code. That being said I don't think they even really do this consistently. We commonly have to change our assets to support new versions of k8s because they've changed the format in non-backward-compatible ways.\nIn terms of dump-cluster it sounds like most of the functionality you're describing is covered by kubectl get all -o json -l suite=pachyderm (ignoring the fact that all doesn't actually mean all) I suppose we could add a comman in pachctl which just sends the right command to k8s to dump stuff. This doesn't seem a lot better to me than just having them do the k8s command themselves. We might save them some typing, but I think having them type the actual command helps them understand the underlying k8s system a bit better which is helpful. Especially since this is designed for people who have already manually edited their k8s manifest it's likely they'll already know how to do this. I also want to be clear that this doesn't help with upgrading at all, you'll still need to reapply your manifest changes to a new manifest generated by the new version of pachctl.. This issue is mostly implemented. We wound up calling it extract instead of dump. The only thing from this issue that's missing is that users need to replicate their deploy arguments in order to upgrade.. I suspect the issue here is with queueing. Pachyderm distributes work evenly between the workers it can talk to and workers are able to enqueue more than one datum to process at a time. This helps them by allowing them to upload and download other datums while they're processing a different datum. However, it behaves badly when the number of datums is close in magnitude to the number of workers because the first workers to come online enqueue all the datums and there's nothing left for the others.\nYou can confirm that this is what's going on by doing an inspect job while the job is running, worker statuses should tell you how many datums each worker has enqueued.\nThere's a pretty good fix to this right now which is that you can decrease the max_queue_size field in the pipeline spec, unfortunately there's a bug where you can't set it to 0 since that'll get interpreted as the default value, but if you set it to 1 it should sufficiently spread out your datums. This should also be fixed by #2473.. > Could we consider potentially making the default queue just 1\n:+1:, this is a good answer imo. Except actually it's going to be 0 because the datum being processed doesn't count as queued. Implementing this actually just requires removing the logic that interprets as 0 in the protobuf as 10, the default that I initially choose.. This is actually done, this issue was just about docs which we now have.. Also, just occurred to me that we merged this without docs which is fine although generally something we should do. Can you make an issue / other pr to add docs for this when you have a chance?. > downloadTime+processTime+uploadTime does not match with the real total duration of the job\nThey shouldn't match, a lot more goes into a job than download, process, upload. Other sources of latency are:\n\nwaiting for k8s to schedule the pods\nmaster election of the pods\nreading and merging output trees\nnetwork latency all over the place\n\nMost of these things are hard to measure and will have very little meaning to users so we don't include more fine grained measurements of them.. Repo provenance is going away soon but I think this is actually computing the full transitive closure. We are only iterating through one level of the DAG but we're iterating through the transitive closure of those nodes, so there's no need to traverse further because we store transitive closures at every node.. This was probably due to the fact that GetLogs worked by spawning a goroutine for each pod it wanted to get logs from, but would return those logs in a static order. So you might dispatch 20 requests for logs, and the 0th request happens to return last so you won't return any logs for a while. This model was changed as part of #2574 so that we return the logs as we receive them. I think that will mitigate this issue as much as we reasonably can. We'll still be bursty if k8s is bursty getting back to us, which I've observed it to be sometimes. But I think that's the best we can do so I'm closing this. Feel free to reopen if you encounter this again.. There should also be per datum timeouts.. A couple of things I want to clarify / add.\n\nThis implementation will have support for partial provenance. That is if you have a branch C with branches A and B as provenance you'll get a commit on C if there's a commit on A even if there's no commits yet on B. This is because if the inputs are union such a job can actually have output. If the inputs are cross the job will just output an empty commit. (Commit with no data, not a nil tree.)\nAs part of this I'm going to rename SetBranch to CreateBranch to bring it inline with our other naming. This makes a little more sense now that you can create branches independent of commits.\nre: 3a, We can actually do a little bit better with this on cancellation. I think we can have the workers watch the output commit while the process and use that for cancellation so the moment a commit is deleted the jobs will get cancelled and the workers can move onto relevant work.. I wondering if we should try to figure out something to do about that. It seems just annoying / confusing enough that it might be an issue for people.. Another small tweak to existing models was brought about by the new commit invariants design. We're removing the DeleteJobs and DeleteRepo options from DeletePipeline this is because, in the new model, some combinations of these flags won't make sense, for example DeleteRepo:true with DeleteJobs:false will leave the cluster in a bad state because the record for jobs is now partially stored in the repo that was just deleted due to DeleteRepo:true. This will manifest as ListJob failing.\n\nI also think that it was a mistake to add so many knobs to DeletePipeline, I think it's much better to have it be simple and mean \"delete everything.\" The only use case I can think of for partial deletions is that you want to still be able to access data and maybe job logs but don't want the pipeline to consume cluster resources. StopPipeline works quite well for that and has the advantage that it's not permanent so if you decide that actually you do want to run this pipeline again you can just StartPipeline whereas if you'd done DeletePipeline you'd have to recreate it which would reprocess everything from scratch.. Actually 1 very minor comment about a missing comma in the docs.. Part of this might be that I made the test run serially instead of in parallel a while ago, although that didn't seem to increase the runtime by that much at the time. Even if this does prove to be a source of slowness I'm not 100% we should go back to running in parallel, the advantage of running serially is that if a test hangs (which is a fairly common way for errors to manifest) you can easily see which test is hanging.. We can test this once the new version is out.\nI don't, off the top of my head, know why this DAG would behave differently but it doesn't surprise me that it does.. Closing this for staleness..  This has been open for a while and already reviewed so I'm going to merge this in.. Closing for staleness.. Just updated 1.6.5 on the release list. I don't think this was due to an error beyond human error. We just forgot to keep updating both the CHANGELOG.md when we wrote the changelog to the release page.. I'd be interested to see what the build times are with this patch. But my general feeling is that adding a node dependency to CI to get a spellchecker doesn't quite work out from a cost benefit perspective.. This doesn't seem to get a big speedup and it would blow our local caches of images so I'm going to default to not merging. Thanks for taking to time to speedup the build process though.. Just an update on this, this is being actively worked on and should be available in 1.7.. Wound up not merging the other PR. Closing.. Pretty sure this has been fixed.. One more change we talked about in person and then all set to merge, I already hit the approve button so I have no way of making this official.. Hi @wardn, many thanks for the pull! To get this merged in we'll just need you to sign our CLA right here: https://pachyderm.wufoo.com/forms/pachyderm-contributor-license-agreement/. Yeah, nice easy win here. I don't see a need to stand on ceremony for CI since this is such a simple change. Merging.. @jgerber-suplari none yet, although it seems that EKS is now generally available. Have you attempted to deploy on EKS, I imagine it should work out of the box given that it's just a Kubernetes endpoint that they provide.. So it turns out that the reason this wasn't implemented initially is that our current GCS library only supports getting credentials through GCE node's credentials, you can't encode the creds as a secret and add them to, for example, an aws node. This shouldn't be too hard to fix, there's a new library that we can use that should do what we want. Still it'll require migrating to a new library so it's not totally trivial work.. Fixed in #2610 . @JoeyZwicker I thought you might :sunglasses: Did you notice the part where it prints it in scary red so we'll be extra careful not to delete stuff we shouldn't?. \n. I don't see much point in lowering it since it's a request and not a limit it shouldn't have any impact scheduling wise. And we use etcd very heavily so having a full cpu for it can meaningfully improve performance.. Closing this due to oldness. Feel free to reopen if we can get this to a mergable state.. Yeah, this all makes sense technically. I'd like to see this merged, I left one comment though I'd like to see addressed.. CI is failing for real (ish) reasons here. There's some linter issues. Run make lint vet and do what it asks of you.. It might be that the errors are actually from vet not lint and your IDE is just doing lint? That's just a guess on my part.\nLooks like you are now hitting some intermittent CI issues, I took the liberty of restarting your test for you.. @williballenthin thanks for the detailed bug report. I think the potential fix you propose is spot on so that's what we're going to do, this should be available in the 1.7 release.. @williballenthin what kubernetes state do you see when you create a pipeline? There should be a new rc created for the workers for your pipeline which will cause the pods to spin up. That's the thing that creates jobs so if you're not seeing it, there's still some sort of k8s problem. Otherwise it's a pachyderm bug.. Fixed in #2605.. So 1.7 does have alpha support for RBAC, but some of the field names you need to set changed and I didn't think it was worth it to add a special 1.7 compatibility mode to get it working. 1.8 is the first release with stable API RBAC so I think telling people they need to run that to use RBAC seems reasonable to me.. @dionjwa the suggestion by @williballenthin should work for you, alternatively you could disable RBAC, the flag I use to turn it on is: --extra-config=apiserver.Authorization.Mode=RBAC try passing None in there or leaving it blank and see if that gets you an RBACless cluster.\nWe can cut a release with this in it by the end of the week.. The main change that this pr allows is running pachctl deploy storage google which adds a google storage secret. This can be used to access GCS from outside of GCE.. Fixes #2578.\nFixes #2538.. @brycemcanally don't believe so, and this would be a great issue to dive in on some of Pachyderm's internals since it's very self contained. src/server/pfs/server/driver.go is probably the best place to start looking, that's where the real guys of pfs is. We have a function in there for validating repo names (ValidateRepoName) but not paths.. Yup that's totally correct, I was mistaken when I said we don't have a function for validating paths. While you're at it, want to change that function's name to validatePath? That seems a bit clearer to me, and a bit more greppable.. Nah, let's keep it private. Exporting ValidateRepoName seems to be vestigial, it's not used outside of that package. If you feel like it unexporting ValidateRepoName would be nice bit of cleanup to include with this PR.. :+1:. @brycemcanally wow, this looks great. I'm gonna get CI running on this and take it for a spin myself. Thanks so much for taking the time to make this PR. . This is actually just a version issue, you can fix it by bumping the k8s version you're using to 1.8.0 in the script. The linked PR above should give you an idea how to do that, it's pretty simple. Or you can wait for us to merge it which should be in a few hours.\nYou also could not use the one shot script at all since I believe AWS now has first class support for kubernetes I think that's a better way to deploy than this script.. @mightyguava glad that worked, par has been merged as well so one shot script in master should work out of the box.\nClosing, feel free to reopen if there's further issues.. Fixes #2619 . Looks like we're good to go. Many many thanks for the pull requests @brycemcanally. . CI is passing, CLA has been signed and code all LGTM. Merging.. Thanks for another great pull @brycemcanally, I'm going to get CI running on this. One other thing I'd like to see for this PR is some tests that exercise the logic. Ideally some negative (i.e. returning errors) and some positive.. You were just testing out our code review process to make sure we'd catch it :wink: . Even though this is only half passing CI I'm going to merge it because the CI failures seem to be due to it not being able to get credentials since the code originally comes from outside our GH org. The fact that one leg of CI is passing though confirms that the code does in fact run.. Closing this due to staleness.. Being able to delete files based on a glob pattern would definitely be a nice feature. I think glob support should eventually exist for delete-file and get-file in addition to list-file which is already support via glob-file. The more interesting question is how we unify this syntax. We could have glob-file become list-file-glob and add delete-file-glob and get-file-glob that seems kinda lame though. I think a better idea might be to change those commands to have a -g flag that you can pass a glob too. We could also try to do some sort of magical detection on if the path being passed to *-file is a glob or a normal path... but that seems like it could be a little difficult and too magical.. Treating the filename as a glob pattern by default is tempting but has some issues.\n\nIt's ambiguous, if I have a file called * then I can't get that file without getting all the other files in the directory.\nThere is a very real performance overhead here. Glob patterns are predicates over filepaths, so evaluating them requires a linear scan. We could try to do something smart like detect when something is or isn't a glob pattern but that gets complicated.\n\nAll that being said, there is something very appealing about being able to just type pachctl get-file foo master dir/* and have it just work, so if we can figure out a way to get this to work I think we should do it.. Good points both. I'm convinced, getting an equivalent to is-glob shouldn't be too difficult. It looks like it mostly just applies the fairly trivial regex: /\\\\(.)|(^!|\\*|[\\].+)]\\?|\\[[^\\\\\\]]+\\]|\\{[^\\\\}]+\\}|\\(\\?[:!=][^\\\\)]+\\)|\\([^|]+\\|[^\\\\)]+\\))/. Hi @mightyguava sorry you're hitting this. To clarify a bit about the example, wordcount is meant to demonstrate how to do mapreduce style workloads in Pachyderm so it opts for the simplest implementation, not the most performant implementation. The simplest way to increase performance in Pachyderm is to crank up parallelism. In this case you have 1 worker per node, but only one of the workers gets used, because you only have 1 file. Splitting that file up into multiple files will allow the workers to all upload at the same time which should reduce the job's runtime to 1/n what it was before (where n is the number of workers.) Note that the reported Upload Time will remain the same, since it's an aggregate, but the time the full job takes to run will be lower. You also will probably want to increase your parallelism coefficient since wordcount is not terribly resource intensive and this will allow you to saturate your connection.\nIf this doesn't prove sufficient you can further increase performance by grouping things together. Rather than having a file for each word you could have a file that contains counts for all the words with the prefix \"a\", \"b\", \"c\" etc, this will limit you to 26 files which should still be enough to saturate all your workers, ifi t's not then doing 2 character prefixes should.\nI suspect you could also speed this up by switching to a different object store, we've observed s3 to be one of the less performant object stores (GCS has generally been the best). I think almost all of the overhead here is in uploading to the object store so it should have a big impact.\nFinally there are definitely some things we can do on the backend in Pachyderm to make this more efficient by packing small files into a single large object. We haven't done them yet because it complicates the implementation and most of the workloads we've seen in the wild don't involve such small files, but issues like this help us know what to prioritize so we'll definitely be taking a look at how we can improve the out of the box experience in this instance.. My intuition is that it's unlikely to be worth it from a performance perspective due to the added latency. And even if it wasn't I think there's a lot of added complexity to running across multiple clouds.. Hi @mightyguava sorry you hit this and sorry for the slow response. My guess here is that the overhead of the connections to s3 is what's causing the OOM error. What's the memory limit / request of your pachd pod and do you happen to know how high the memory usage gets during the put-file? We use a semaphore to limit the number of concurrent uploads but it's possible that, that value is too high so it's overwelming the memory.. kops delete cluster should do what you want. This was fixed in #2640.\nApparently adding \"Fixes #XXXX\" doesn't close #XXXX on merge like I thought it did.. This will be available in 1.7.. I'm a little confused by the title of this issue. There's already a way to block on a job completing, although you obviously know that since you reference it in the issue. It sounds more like you're asking for flush-commit to report failures which it will in 1.7. If that's the ask I think we should rename this and move it out of backlog since it'll be done as part of 1.7.. Fixes #2640 . @brycemcanally good point, I think that's been broken since inputs became hierarchical. I'll fix that in this pr as well.. @threefoldo hi and sorry you hit this. Pachyderm should generally work with whatever docker images you throw at it but sometimes it doesn't due to permissions in the container or something else. There's a lot of different ways people can make containers these days so we haven't seen them all. Can you pull logs from the worker pod? That should give us an idea of why it's failing.. No problem @threefoldo let us know if you hit any other snags. We're here to help.. @mthpvg many thanks for the pull. And welcome to Pachyderm!\nAll looks good to me, CI is failing because it's an outside PR. So I'm going to merge anyways since it's only a docs change.. So is the issue here that with those steps you wind up with no pipeline pods at all? Do you have a pipeline rc? I can't think of any reason why this would work if you create the pipeline before the data but not the other way around.. This one's a little weird. I'd expect Kubernetes to be validating that themselves. I suppose we could figure out what they'll accept here and try to do their validation for them. That probably wouldn't be too hard. It might make more sense to just see if we could get them to do it or submit a PR, this code makes a lot more sense in K8s than it does in Pachyderm. All that being said this is definitely a bad user experience so it's worth fixing, however we do it.. These only don't contain the State field because the datums failed so the State field has a value of 0 which gets omitted. This is easily fixed by setting EmitDefaults in the Marshaler, but the user could also just work around this by making their code understand that a missing value means failed.. Hi @antjkennedy, many thanks for the pull. Before we can get this merged we'll need you to sign or CLA. After that we should be able to get this merged in pretty quickly.. I'm good with them, let's see what CI says.. Code changes LGTM.. Kubernetes has a built in way to wait for rollouts: kubectl rollout status deploy/pachd --watch=true that's a better way to wait for pachd to deploy than pachctl status command would be because the latter will block indefinitely if it doesn't have a way to connect to pachd. pachctl version is a good way to check if pachctl is able to talk to pachd and obviates the need for a pachctl status command, returning information about etcd etc through status isn't something I think we want to do since there's a ton of information that might be relevant to the user and we'd rather people talk directly to Kubernetes for that since Kubernetes / kubectl are designed to give you that information. Waiting on readiness in pachctl port-forward might make sense though.. I'm working on that right now, I think we're just going to do it by querying kubernetes.. Done.. FileInfos have a hash in them so it'd be very easy to include this in what's showing in inspect-files output. This can be accessed right now by doing --raw to see everything as json. I'm not sure if the hash algorithm we're using under the hood is one the user's going to have easy access to though. I'm thinking maybe a better idea would be exposing an ability the do a sync similar to what workers do when a datum finishes, which uploads some content only if it's new.. Many thanks for the pull @sillystring13 before we can get this merged we'll need you to sign our CLA.\nOther than that everything LGTM.. Closing this due to oldness. @sillystring13 this is good to merge as soon as you sign the CLA. Feel free to reopen or comment here if you decide you want to sign it.. Fixed by #2719 . One more new comment, and I think my earlier comment:\n\nCan we just have the APIServer get the cluster ID out of etcd rather than passing it in? That seems a bit simpler and a bit more resilient in case we add a way to change the ID or something in the future.\n\ngot lost in the mix.. Actually thinking about it a bit more I'm not really sure if having the APIServer talk to etcd is better in any meaningful way so you can just ignore that one if you prefer what you've got now. The ScrubGRPC one is worth doing though.. Migrating to v3 API does seem like a nice bit of additional cleanup to do in this PR. It'd be nice to be all v3 API. We could in theory do that and still pass the value in, or make it talk to etcd, I'm pretty copacetic with both.. LGTM. Hey @hchauvin, thanks for a very clear and concise feature request. We can definitely do this. I have a few minor tweaks to your proposal: I think it makes more sense to let people set the number of \"tries\" than the number of \"retries\", the reason being that we want to use 0, the unset value, to indicate the default value of 3. However if you're setting \"retries\" then 0 would also be the way to indicate \"don't retry this\" which is your original ask. If you're setting \"tries\" though then 1 try is the behavior you're looking for, and 0 tries is meaningless so we can use that as the default. We should be able to get this in for an earlier 1.7.x release.. Good points both. It's completely arbitrary that ListPrefix uses a paginated iterator and List doesn't, as I mentioned though,a few more things need to change before we can unify it.\nMaking it a load test seems to make sense. I'll do that before merging.. Actually do we have load tests separate from Benchmarks? Thinking about it more this doesn't quite make sense as a Benchmark, since those are a bit semantically different and it is a test for correctness rather than speed, albeit one where the correctness being tested only has to do with high loads.. This should be very easy to fix, PR incoming.. I think we can definitely soften this a bit. Your suggestion would help quite a bit, and I'm guessing would make your autogenerated branch names pass the test. In addition we can check that the 13th character is a 4 since that's part of the UUID spec. I suspect at that point it'd be quite hard to run afoul of the restriction.. The issue here is actually the cron spec. The first field in the cron spec is seconds, not minutes, so you're getting a job every second. This is kind of easy to get confused about though because standard cron doesn't have seconds, but our cron does, because we use this library: https://godoc.org/github.com/robfig/cron. We could fix this in a couple of ways, we could remove support for seconds to make it more standard, or we could document better the fact that we support seconds in our cron. I'm not quite sure how useful second granularity on cron is, it seems like minute granularity might be good enough.. This PR extends / modifies our existing http server in the following ways:\n- it will now route you to services\n- simplified the GetFile handler using the newly added GetFileReadSeeker\n- improved error handling in the GetFile handler\n- relocated http server from src/server/pfs/server/http.go to src/server/http/http.go since it's not PFS specific.. Alright, comments addressed. I didn't wind up using the notFound helper method because it says \"route not found\" which doesn't make sense if it's actually a commit we didn't find. Also I modified that method slightly to not set a Content-Type header because the http.Error method sets that for you.. Looks good to me, 1 comment about using require in the test to make things a bit shorter and more in line with our other tests.. Merging this despite the fact that CI is failing, the first CI is failing because of creds issues on external PRs. The second is failing because of a linter issue that has been fixed on master. The other parts of CI passed and it seems silly to block this until we shuffle around some commits to get the right thing to run when the meaningful checks have already run.. Having the pipeline name as a label so you can do things with it totally makes sense to me. I'm less sure about the version though, version is an internal value that's not supposed to have meaning to users and has no guarantees about the backward compatibility of its semantics. It also shouldn't be possible to have two pods from different versions of the same pipeline running at the same time, so I'm not sure how you'd use it to distinguish anything meaningful. What's the thought behind adding the version label?. Merging, the failed CI run is due to a linter issue we had in master which is now fixed.. CI is failing here for linter issues that have been fixed in master merging since the other, more relevant parts of CI have passed.. So, the issue you have logs for here should be pretty easy to solve, it's just a question of ignoring things that already exist. However, this is likely a secondary issue to whatever caused the first failure. Would it be possible to rerun this and capture logs from the first 1.7 pachd that comes up? You could do this pretty easily by starting up a kubectl logs -f right after you deploy.. @dwhitena yup, #2746 should address the issue you're seeing here with a repo already existing and that stopping the process. It still seems unlikely to me that, that's the only issue since I can't explain how the repo would already exist unless the migration process ran partially and then failed for some reason.. These have been resolved for a while, closing.. The behavior we have now, where deleting an upstream commit deletes downstream commits regardless of access seems like it might be right to me. At least it doesn't seem obvious that it's wrong. The question, in my mind, is what it means to \"not have write access to an output repo.\" No one really writes to an output repo, only the pipeline does that, users write to the input repo and that causes downstream writes to happen. If a user without write access to an output repo but with access to the input repo creates a new commit on the input repo that writes to the output repo, so it seems fairly reasonable that deleting that same commit would also delete the downstream job.. Adding a namespace flag to port-forward is easy enough, (#2749). We could also add this to the config. However I think the best practice is actually to set this at the k8s level with:\n$ kubectl config set-context dev --namespace=development \\\n  --cluster=<cluster> \\\n  --user=<user>\nThat way things like kubectl get all and kubectl logs will just work.. We now have a --namespace flag on port-forward we should get that documented.. I have a mild preference for the new message in this PR, since the previous one describes it as an \"Error retrieving\" which makes me think it's a network hiccup rather than a not found. Admittedly a network hiccup would cause trigger this too but mostly users will see this message when the dash image isn't found.\nWhile we're at it could we change the message to say \"for pachyderm 1.x.x\" rather than \"for pachctl 1.x.x\" it seems weird to say the dash image is for pachctl rather than the system as a whole.. This is enough of an improvement that I'm going to merge for 1.7.. This isn't reproducing for me, can you give me the command you used to deploy? I did: \nLAUNCH_DEV_ARGS=\"--namespace=test\" make launch-dev. Ahh, the issue is that it's using the etcd stateful set, fix incoming.. Actually, it turns out this is possible with put-file today. The way to do it is:\npachctl put-file repo master /  -f dir -r -c\nNotice the / there as the third argument, that overrides dir so that files wind up in /<file-name>. This does have the risk @JoeyZwicker outlined above, I think I'm going to add a warning message about that in pachctl. I'll also add an example of this behavior to the doc string for it so this is more discoverable.. To summarize the changes here: this PR removes ScaleDownThreshold field on pipelines and replaces it with a Standby field. This field is a boolean rather than a duration. When set to true this will cause pipelines to go into Standby when they don't have any work to do, there is no threshold for this, it happens as soon as there's no work to be done. The pipeline also changes its state to STANDBY during this time. Unlike in the previous scale down implementation a pipeline in STANDBY will have no running pods, rather than a single running pod.\nStill left to be done here: pipeline spec documentation.. I don't think there's a need, since the comment basically suggests that and it should be what people want to do. And there's no real risk here now that we have the error message.. You can still do @every 1s there's no way to do, for example every 3, 5 and 10th second. But I don't think there was ever really a use case for that.. This should be fixed by #2777.. @DSchmidtDev do you know which Kubernetes version and RBAC settings create this issue? The ClusterRole has, among other rules, this:\n{\n      \"verbs\": [\n        \"get\",\n        \"list\",\n        \"watch\"\n      ],\n      \"apiGroups\": [\n        \"\"\n      ],\n      \"resources\": [\n        \"nodes\",\n        \"pods\",\n        \"pods/log\",\n        \"endpoints\"\n      ]\n}\nwhich seems like it should give the required permissions. In addition this works on both 1.8.0 and 1.9.0 kubernetes clusters with rbac enabled.. @stevef1uk the most likely cause here is that the IAM role for the cluster doesn't have the proper permissions. You can change that in the GCE web console.. LGTM. This is an issue we've seen before and currently our azure driver breaks data up into 2MB chunks before sending it over the wire. I'm gong to do some digging and see if there's somewhere we're failing to break it up, but that seems unlikely since the breaking up happens at a pretty low level in the API. I'm also going to add some more logging so we can maybe understand better where that error message is coming from.. The current streaming list-* PR is a direct solution to this issue.. This is an interesting idea, but I'm not quite sure the costs outweigh the benefits. It would be a boost in convenience but I feel like we'd be leading people away from best practices for installing and maintaining packages on their machine. It's generally better for people to go through brew / apt to install their packages, even if that's a bit more work up front. This also would add some non-negligible overhead to the pachd container, pachctl is about 50MB right now.\nAs an alternative I think we could maybe have a tab in the ui that gives people apt / brew commands to install the appropriate pachctl and a link the the raw binary if that's what they need to resort to.. So I think you're hitting another error in auth before you can get to the part where we check if the repo has an branches with subvenance.. @kaktus42 sorry that you're hitting this. It should be recoverable in the next version of Pachyderm by deleting and recreating your pipeline. Are you getting the same reproduction as the original post?. Closing for staleness.. Closing this for age. Reopen if we can get this to a mergeable state. I think we've done some stuff that supercedes this though.. Wait so did this mean that any time you tried to restart a cron that had previously been running it would segfault? It seems a little hard to believe that was the case and we hadn't noticed... but that's what the code seems to suggest. Anyway, definitely LGTM.. One fairly large change requested, I can't submit an official review on this though because it's technically my PR.. Few more minor requests.. All LGTM, again, can't officially approve this since I opened it.. This change based on a conversation in our user's channel.. We do use the phrase \"node at\" in other error messages. I haven't seen any of those error messages, or the \"cannot canonicalize\" error message escape the system, but in theory any error message can. Even if it's provably impossible now, code changes so they might escape in the future. I assume the idea in asking this is that you're suggesting maybe we should change those too. I don't think that makes sense, we can talk about it a bit more offline, it's kind of a long explanation to put here.. So CI is failing here, I think it's because when we deploy there opts.Namespace is empty which leads to an invalid ClusterRoleBinding. Hey @emmanuel thanks for a very descriptive feature request. This is definitely something we can get in for you. I think it's already possibly to state most of the relevant info in the pipeline spec. For other stuff such as the work dir and user we can add it. I think this will likely just be a field in the pipeline spec that indicates whether or not it should have the privileges.. @emmanuel one question on this, how is this policy actually enforced in your company? I'm wondering if we can just detect the failure and respond to that rather than making users explicitly set it.. I still don't think there's anything reasonable to do for this, we could start filtering out kubernetes error messages but that's difficult to do for a number of reasons:\n\ndifferent versions of kubectl have different error messages\nthese error messages change\nthere's a very large risk associated with this which is that we might filter out a meaningful error message leaving the system in a state\nlastly, these error messages get logged because they're not always benign, although they certainly seem to mostly be\n\nI don't think these issues are really plaguing users as you suggest, they're coming up and I've seen them mentioned a bunch of times but mostly users mention them as \"hey, I'm seeing these errors but everything is working, do these matter?\" they're annoying, but I don't think they're blocking many people from doing work. Maybe just a message at the top that says these messages are benign would help.\nAlso, why are you insisting on labeling this as 1.7 backlog? This issue was opened 3 days after we shipped 1.7, in what sense is it 1.7 backlog?. >  for every user that asks about it there are 10+ that don't\nI'm sure there are several users that don't ask because they are familiar enough with networking to know that a connection being reset is normally benign and even if they don't the word \"reset\" suggests this and it's easy to confirm that things are still working. If more than 90% of users are able to figure this out for themselves and a few others who are less familiar with networking need to ask then I think we're doing pretty well. And also, I'd like to reiterate that these error messages might not always be entirely benign, I'm pretty sure for a long running operation like a put-file these resets could actually cause problems in which case it's important that the user has some sort of a log message to go on because otherwise they have no chance of figuring out what's going on.\nI also very much disagree that this is an example of the product being unpolished, developer tools like Pachyderm and Kubernetes are generally willing to risk giving too much information if it means mitigating the risk that important information doesn't get reported. The prior might cost you a few minutes asking a question in slack, the latter might cost your entire team several weeks as you try to figure out why things are broken. This is the right attitude.. Also, we may be able to mitigate this a bit by doing what helm does and just doing a new port-forward for each request, I'm not sure that that will totally remove these logs though.. @eclosson that sounds like it would definitely trigger this bug. Sorry you're hitting this, we'll get it fixed shortly. If you'd like a workaround then if do pachctl delete-branch _name_ stats before you delete the pipeline you should be able to redeploy it without issue.. Got a PR fixing this and other messages. @JoeyZwicker you're reviewer on that.\nError messages like this are basically the Pachyderm equivalent of a merge-conflict. So they occur when merges happen. Merges normally happen in FinishCommit however, when you attempt to read from an open commit (ie, list-file) we perform a partial merge on all the writes we've gotten to the commit so far, and that can also result in a conflict, as it is here.. This is the intended behavior. Why would there be one {}? --raw prints out each object in the stream, if there are no objects in the stream it doesn't print anything.. Closing this, reopen if there's some justification for why what we have now isn't the correct behavior.. This appears to be a ReadTheDocs issue, the md file definitely exists: https://github.com/pachyderm/pachyderm/blob/master/doc/pachctl/pachctl_deploy_storage.md\n@JoeyZwicker any idea what's going on with this?. I don't think I like this. Adding extra paths under /pfs to support a particular type of iteration seems like it's not worth the clutter. Furthermore I don't think that this use case is that important to users, it's rare that users want to iterate through all of the inputs, normally each input has a name that's meaningful and so you handle it directly by name rather than needing to find it via iteration. This also would have a big downside in terms of backwards compatibility, users who are doing iteration over their directories would suddenly have these new paths inputs and output which would probably break their existing code.. This PR is actually fixing an auto-generated file. To fix this for real you need to change it in src/server/pfs/cmds/cmds.go. And then run make doc.. It needs /bin/sh, /bin/mkdir and /bin/cp.\nIn general you'll get faster responses to support requests like this if you ask in our slack channel than via github issues.. I believe this is the same root cause as #2816.. Another question, why do all of these graphs end with the line trailing off to the right at a static value? I thought we figured out how not to do that.. Few polish things left, other than that this is pretty much mergeable.. LGTM. This should prevent scenarios where progress is negative or it reports more datums processed than existed. It accomplishes this by updating the progress in the same txn in which it marks the chunk as complete which guarantees that we never count the same progress twice. Previously this was done in a separate txn and errors encountered between the 2 would cause progress to count twice.. @kaktus42 I'm unable to reproduce this error on 1.7.1, when I do:\npachctl put-file test master -f s3://pachyderm-data/chess/file000000000 -c\nThe url it tries to get from is:  https://pachyderm-data.s3-us-west-1.amazonaws.com/chess/file000000000\nWhat does your amazon secret look like? Obviously don't post the actual secret parts here, but take a look at the region, also are you using a cloudfront distribution?. Subsumed by #3149 . We should probably accept something like:\n\"egress\": {\n    \"URL\": \"s3api://<endpoint>/bucket/dir\"\n}\nSpecifying the endpoint in the storage secret will make it hard to egress to multiple endpoints, and just moves the information farther away from where it's actually used which will make it harder to understand / audit.. This needs make doc run for the changes to actually propagate to our online docs and not just pachctl.. @kaktus42 \n\nThis needs make doc run for the changes to actually propagate to our online docs and not just pachctl.. We're happy to merge after make doc is run, we could even do that step for you it's just a bit of a pain to commit it back to this PR since it's based off of your personal branch, so I'd have to create a separate PR.. Merging this, docs will propagate in the next release. Sorry for letting this sit so long @kaktus42 . Few more changes requested, after those this should be good to go.. Thanks for the pull @jkinkead, same deal as the other one, I'm just going to let CI run and then we'll get this merged in. CLA was already signed.. CI is failing here for credential reasons, since this is an outside PR. I'm going to merge anyway since this LGTM.. @jkinkead this is awesome, thanks so much for the PR. I see you've already signed the CLA. I'll get CI running on this and we can get it merged in ASAP.. Linting can be run with make lint I'll add it to the contributing guide. Thanks for your help!. @jkinkead tests can be run with make test although it's a bit finicky and normally encounters an error on local machines if you try to run the entire test suite from scratch. CI runs it though so I'd just rely on that.. @jkinkead yeah, we get intermittent failures on Travis where Kubernetes doesn't come up for us. We haven't been able to determine quite what causes it, we normally just rerun the CI when that happens. I just hit the button for you.. Alright, CI is passing. Merging.. We should totally do this. It looks like s3, GCS and Azure all have workable atomic-copy APIs. I legit didn't know they existed. This can make a bunch of things much easier and more efficient.. This is no longer relevant as using object storage's namespace for dedupe lookups is way too slow. Still good to know that copy semantics are available in object storage.. One more minor change requested, after that LGTM.. This should probably say,  the branch \"master\" has no head or something like that.. @arriqaaq thanks for the PR, I see that you've already signed our CLA. The code all looks good to me here, I'm just going to wait on a CI run to pass and then merge this in.. This seems like a good idea to me.\nI don't think I like any of the intermediate solutions to this though. I think we should just wait until we're ready to do 2 and then implement that. This is going to require pushing globs through most of the places where we reference branches and repos, particularly branches will need to be able to have glob based provenance, which means we'll need to have a system for checking new branches against all of the branches with glob based provenance to see if they match. This does get somewhat complicated but I think it's doable.. This idea seems correct to me, @msteffen you know more about auth and have more of the design in your head so you'll know for sure if this is a good idea.\n\nIt seems we're gradually migrating all of our repo specific things into branch specific things, this has happened already with inputs and provenance. Maybe we should start building things to be per branch whenever we're planning to make them per repo, we seem to always change our mind later.. Thanks for understanding :+1: . @brycemcanally you should also be aware of this: https://github.com/pachyderm/pachyderm/blob/master/src/server/pkg/hashtree/hashtree_bench_test.go. In-lining is definitely going to be a big improvement. And there's no hard limit on the size threshold we use, it could even be configurable. It should definitely be higher than 64 bytes though, object hashes are 512 bytes.. @brycemcanally just checked an actual FileInfo, you're way less wrong than I was, but there's still 1 wrinkle. Here's one the FileInfo for reference:\n{\n  \"file\": {\n    \"commit\": {\n      \"repo\": {\n        \"name\": \"TestSimplePipeline_data1b51649365e3\"\n      },\n      \"id\": \"f61942aaa2694fb8bf522f5af58abf3c\"\n    },\n    \"path\": \"file\"\n  },\n  \"fileType\": \"FILE\",\n  \"sizeBytes\": \"3\",\n  \"objects\": [\n    {\n      \"hash\": \"f7fbba6e0636f890e56fbbf3283e524c6fa3204ae298382d624741d0dc6638326e282c41be5e4254d8820772c5518a2c5a8c0c7f7eda19594a7eb539453e1ed7\"\n    }\n  ],\n  \"hash\": \"oyiJsC9v26djlDC3ci+ODADeea9N00AI2OBF7Aqttq0=\"\n}\nThe hash field is actually 128 bytes, that's because it's 64 bytes but it's encoded lexicographically so each byte gets encoded as 2 characters, doubling the length.. That makes sense for most errors, we have some really dumb errors though where you get stuff like: can't delete X because X doesn't exist which, because deletes often cascade to sub objects can block you from deleting other objects that do exist. Nonexistence shouldn't prevent you from deleting in my opinion, your goal with deleting is to achieve nonexistence. For other errors they should definitely require a --force although I'm not sure if having a --force flag for commits is going to make sense, there's too many really bad states you can get in by force deleting commits we don't want you to delete.. 1 thing before you merge this actually, we have some docs that outline the requirements for user created container images. You should update those to no longer include sh.. I think it's in the doc/reference/pipeline_spec.md. I approve of this, I'm not sure if we need a switch or if update-pipeline should just have upsert behavior by default.. > When ingesting data into Pachyderm, it is recommended that large files containing many logical datums (eg CSV files, object-per-line JSON streams, etc.) are split into one file per line (ie --split)\nWe only recommend this in cases where you want to be able to process each line in parallel. If you want to process them all together then you should just leave it as a single big file. That's a perfectly reasonable pattern. The only way I could see this being an issue is if you have a data source that you want to process map style and reduce style, which is a kind of weird use case but something I could potentially see happening. In that case you'd probably want to split it, since that's the only way you can map over it, and then get all the files in a directory for the reduce. This will have the issues you mention so there is some value in being able to concatenate all of them but it's not immediately obvious how that would work. For example what would the concatenated file be named. And it's also not clear to me that this is an important enough use case to justify the complexity. We'll see how much it comes up.\nOne other potential solution is the glob based file commands @brycemcanally has been working on. I don't think we talked about this one, but you could in theory be able to do:\npachctl copy-file repo master file/* repo2 master file\nto make repo2/file hold the content of repo/file/* concatenated.. This is probably never going to be possible. A lot of what we do with etcd would be much easier with a SQL DB, but some of what we do seems downright impossible. In particular I don't see a good way to watch keys for changes in a resilient way or to have leases on keys so that they expire.\nIt's possible that instead of moving off of etcd what we really want is to use both of them for what they're good at, storing the bulk of our data in SQL and only using etcd for coordination. That has all the complexities that 2 systems normally entails, including the really annoying fact that we won't be able to do atomic transactions between them. So that may be more trouble than it's worth, we had a system like this at one point with RethinkDB in place of the SQL database and it had its benefits but also many downsides.\nIn considering switching databases we also might want to think about using a graph database. Most of the most difficult operations in Pachyderm are graph traversals, we've had to do a lot of work to get passable performance on a few specific graph operations we need. Those would be much simpler if we had a database that supports them natively.. Haven't done a ton of digging, but I've had my eye on: https://github.com/arangodb/arangodb for a while.. Maybe you should try raising the limiter value on this, it looks like this change alone might have added 10+ minutes to the runtime of tests. Which is a way bigger impact than I would expect for this.. For others who may be confused, the key thing here is that this isn't an extension to globbing, it's just a second glob pattern that, when it matches, excludes that path, rather than including it.\nWe do support set notation, we also support ! for exclusion in character ranges, but there's not way to do {foo, bar, !buzz}. At least as far as I know, I'm just going off the docs of the library we're using which can be found here: https://godoc.org/github.com/gobwas/glob. I wound up temporarily disabling the prometheus test after a bunch of retrying and not being able to get it to pass.. This index is kind of exposed in that it denotes the order that datums will come back if you do ListDatum on an unfinished job. But once the job is finished we sort by failed datums first so it ceases to be accurate. It's a little unclear to me what the value of exposing it would be, it doesn't really correspond to anything the user can see, except what I described above which is kind of tricky to describe, and there's nothing you can use it for. It's also going to have the confusing property that the same datum in different jobs will have a different index. So it's a property of datum / job pairs, not of datums. Granted that's actually true of most of the other datum properties as well.. This should leave the pipeline in a failed state. Does it not?. One small request about commenting a mutex. Other than that all LGTM.. This is some code that runs with etcd deployments, I just forgot to apply it to stateful sets as well.. I can't reproduce this on master, I think I might have already fixed it I remember running into this once. What version are you on?. This format seems pretty usable to me, most languages seem to have a way of reading a stream of json objects, Go's is called json.Decoder, it's also consumable by jq (newline delimited would be as well) and you can pipe it to jq -c to convert to newline delimited json, we could in theory add our own -c flag (which stands for compact) to output new line delimited json.. So it seems we're missing a newline on this message. But I think the invalid characters message is coming from somewhere else.. Hi @suneeta-mall, many thanks for the pull, we'll get it merged ASAP. Before we can do that though we'll need you to sign our CLA: https://pachyderm.wufoo.com/forms/pachyderm-contributor-license-agreement/. @suneeta-mall very sorry for the delay on this. I'm rerunning CI right now.\n@gabrielgrant there's actually no need to create a second PR, if you do all steps for creating the second PR up to but not including actually creating the PR then one of the 2 CI jobs (the lower one) will be able to see the enterprise secret.. Closing for oldness feel free to reopen if we can fix the issues that prevent pachd from coming up.. Closing for oldness, feel free to reopen if we can get the vendor problems fixed and get this merged in.. I'm pretty sure it's a real flake. I got a PR coming that deflakes both of these tests.. @jkinkead this unfortunately isn't easily replaced with a symlink. That's actually how the behavior used to be implemented, but we found that with some container images the symlink would silently fail and show up as an empty directory. Switching was the only way we could find to implement it that worked with all containers.\n@JoeyZwicker yeah, that was only for pachd.. Small update on this, I think we have found a path forward on this that will allow us to use symlinks. I'm not sure if we'll be able to land the fix before the 1.8 release but it should come out in a 1.8.x release or 1.9 at the latest.\nThanks for bearing with us.. Hi @vasilak,\nThere are a couple of standard ways of accomplishing this with Pachyderm:\n\n\nBuild the control flow into your pipeline. Your upstream code would output either its cropped face to /pfs/out/face or something else to /pfs/out/other then your downstream pipeline would do something different based on whether the file it finds is called face or something_else these names can be changed to your liking so that multiple faces don't conflict with each other.\n\n\nHave separate pipelines consume different directories. Your upstream code would output faces to /pfs/out/faces/face and other things to /pfs/out/others/other. You could then have 2 downstream pipelines, one with a glob pattern of /faces/* that would handle faces and one with /others/* that would handle the others.\n\n\nWhich one you use depends on how granular you want control over the 2 operations to be. If you want to be able to update one without updating the other, have different resources requests for them, have one get scaled down while the other is up and running, etc. then 2 different pipelines is the better choice. If those things aren't necessary the simplicity of a single pipeline is probably \n\nGenerally speaking you'll get faster responses to support questions like this in our users channel: slack.pachyderm.io. GitHub issues are better used for reporting bugs and feature requests.. This PR is now contained in #2986 so it's no longer needed.. I think this could probably be removed without causing too much trouble now that scaling up and down workers is handled by the pps master. I don't think there's anything else that we need it for, but I could be mistaken.. This just needs to be tested on some more cloud providers before we're confident it doesn't break things. We'll get it merged in before 1.8 ships though.. This is definitely an optimization we want to make and I think we're in a good place to do it in the near future. I have a slightly different design in mind. I don't think we need to store a list of changed files, we can just get that by using the existing DiffFile API call, this solves the deleted file problem that you mention above, allows us to treat all commits (output and input) the same, and saves us having to store additional information. It does have a cost in that it requires reading 2 trees instead of 1, but that's a much smaller performance issue than the fact that we need to send an object store request per skipped datum. Especially considering that the parent's tree should be cached since it was recently processed. This is also strikes me as a bit more stable, for example if a commit gets skipped (i.e. if a Pipeline was stopped), or if its parent failed to process we can just diff with the parent's parent. Granted we could also merge the list of changed files, but now we're reading out potentially a large number of trees rather than just one.\nThe second issue that needs to be taken into account here is that jobs may fail, which means that just because a datum isn't new it may not have been processed. This seems fairly easy to solve, because we can just find the most recent job that did succeed and use the input to that job as the old argument to DiffFile.\n. @gabrielgrant you're really talking about 2 optimizations here:\n\nusing diffs to make processing more efficient\nmaking diffs more efficient to compute\n\nThe second optimization only makes sense after we've done the first one, no one really cares about the performance of DiffFile right now, partly because it's already pretty good, partly because it's not on the critical path for anyone. Just making the first optimization and leaving DiffFile as is will have a huge impact on the performance of a ton of workloads (the majority of Pachyderm workloads I think). After that, DiffFile will be on the critical path for important parts of the system and we'll be able to benchmark it and decide then how we want to improve it. But I think the answer is likely going to be a secondary index of some sort that tells you which parts of the tree are new.. @gabrielgrant this all seems accurate to me, a few things to add though. First, it's important that we be explicit about exactly which cases we're optimizing. The case that these optimizations are most relevant to are the append only cases. Those are the most common workloads, and they are sinfully slow. Once we start talking about workloads with deletes (or overwrites) the benefits get a lot less clear, it's still a big win if people are frequently deleting a small number of keys, but I haven't seen that very much. More what I've seen is deletes are a rare occurrence, used when data was mistakenly committed, in which case it might not be a big deal if, in those rare occurrences you have to merge from scratch. Or, deletes happen constantly because they're overwriting the same few values, in which case these optimizations won't save you much over remerging everything from scratch.\nAll that being said I think this is a very doable optimization, the solution that makes the most sense to me is 1), that way we can just zip through the objects and remove the ones we don't want. 2) is actually something I've wanted to do for a while and just haven't gotten to yet, there's a bunch of nice things you can do with it: for example if you have timeseries data where the filenames are timestamps and you want to extract a certain type of event to a file called event then when the job finishes event will contain the relevant events ordered by timestamp, despite the fact that the timestamp isn't explicitly recorded anywhere in the file. However I don't think this would quite solve of problem of knowing which object to remove, since you'd need to find all the other datums that contributed to this output file, which would either require a linear scan, or would require being able to index from output files to input datums, at which point you might as well do 1) since it takes the same amount of space. 3) won't work if we want to care about order at some point in the future, which I think we do.. This is a decidedly bad use case for Pachyderm right now and some major changes would need to happen before we could really have first class support for this. The crux of the problem is that an iterative model plays very poorly with our provenance system which works by laying out all the downstream commits and then filling in the results as they become available. In the case of an iterative algorithm this set of commits is infinite, the processing shouldn't actually be infinite, although if the algorithm fails to converge for some reason it could be. But even if it doesn't our provence system doesn't have any way to know at which point things will converge. To represent this in a first class way we'll likely need to have some sort of lazily computed provenance system that can compute things symbolically without actually writing out all of the commits.\nIn terms of a work around I think the method you describe here is as good as it's going to get. Basically manually ferrying data back to the top of the dag when you want to trigger another iteration. This makes it explicit when you consider the operation to be done, whenever you stop ferrying data back, and can also be done quite performantly using the CopyFile method.. :+1: thanks for the fix. Even without a repro this shouldn't be too hard to solve in a defensive way, 2 things we can do that would defend against this:\n\nwrap the k8s create calls in retry logic\nmake it so that the pps master periodically attempts to sync its state to kubernetes, this includes creating pipelines that should exist but don't, and deleting pipelines that shouldn't exist but do. This will help us in cases where we couldn't talk to k8s and then pachd crashed.. @jdelfino many thanks for the pull, I see you've already signed our CLA. I'll get CI running on this and get it merged shortly.. I'd be down for this. We had something like this at one point and they went out of business which was when we switched to wufoo. Wufoo has been pretty simple and worked though so I wouldn't say this is a burning need.. We have a fix for this in code review right now.. Good call, I do a grep for anything that needs to be changed.. Besides this one, our examples are all named based on the use case they address or the outside tools they leverage to do it. Not that this isn't an example of cron but we can only fit so much in a name and cron-db just seems kinda lame to me. We do have a cron section in the cookbook, which links to this demo and which seems like a more natural first step when you want to understand a particular feature better.\n\nAll references should be changed now as well.. Actually the addition of the spec repo should be the thing that prevents this from happening. I think we have a bug in how we implemented that though and we're just using the latest commit instead of the one for the job.. @jdelfino no worries, we understand the issue and have enough to fix it. And I don't think tags are going to work as a workaround.. I haven't tried it, but that would make sense. Whoever fixes this should be sure to test leading and trailing.. @shayshahak thanks for the report.\nJust to make sure I'm understanding correctly you're past the issues you're describing here and are documenting how you did it so we can add it to our docs. We'd be happy to do so, would you mind clarifying a bit about what you're saying here. Particularly what does SPN stand for and what does one need to do to add it to the resource group where the Storage Account is? It may be easiest to communicate that with screenshots of the Azure console.\nAlso, you may not know the answer to this, but I recently deployed on Azure and didn't have to do anything with SPN or Storage account, it seemed to just work for me. Do you have any idea what might be different between our Azure setups that required you to do this but not me?. @shayshahak no hassle at all, thanks for taking the time to check on this.. Doesn't look like there are unfortunately, oops. Do you min adding them?. @suneeta-mall did you restart your shell after installing? I just tried it on my machine and it worked perfectly.. @suneeta-mall it's bash completion, you're not going to see anything in zsh.. We unfortunately haven't had the time to write zsh completion, and cobra doesn't support it as nicely so it's more work than bash completion. I use zsh too.. Hi @suneeta-mall pachctl undeploy is designed to remove as much of the Pachyderm deployment as possible without doing anything dangerous. The only thing we really consider dangerous though is deleting data since that's irreversible, we gate that behind the -a flag and have a big warning message to make sure people understand the consequences. We don't consider it dangerous to do things that will impact the availability of the Pachyderm cluster because we feel the user intent of pachctl undeploy is to make the cluster unavailable and that this is pretty clear from the name, furthermore it's easily reversed by rerunning pachctl deploy. pachctl undeploy also doesn't check the consistency of the cluster for a couple of reasons: \n1. consistency is fairly hard to define in this case, for example not all clusters have RBAC roles\n2. pachctl undeploy is designed as the tool that people use to get out of inconsistent states so they can have a fresh slate to redeploy to.\nWhat you can do to get the behavior you'd like is to save the command you used to deploy pachyderm and then do:\npachctl deploy ... --dry-run | kubectl delete\nthis will delete everything your deploy command created because it knows ahead of time what \"everything\" is. Unlike pachctl undeploy which needs to figure out what was deployed by querying Kubernetes and thus only knows about what it finds.. Copy pasting from our users slack:\nWe've found over time that the best way to model behavior like that isn't with pipeline flags but with data, that tends to be a lot more flexible. In your case I'd model it as a cross of 2 inputs: the first is the one you've already got, the second is a \"load identifier\" which is just a single datum, that you can change anytime you want to force a complete reload.\nThis is nice because you now have 3 distinct options:\n- write a new avro file, and a new identifier: reload from scratch, your desired behavior\n- write (or delete) just a new avro file: don't reprocess everything, nice to have for correcting mistakes\n- write just a new identifier: reprocess everything, no need to commit something new, nice for development and debugging. @gabrielgrant having this as a config in the pipeline doesn't really change the provenance that is tracked. Provenance is a little weird in this use-case generally, because your output commits are empty, so you can know which empty commit came from which input commit... but what does that tell you? This is actually another benefit of having an identifier datum, you can use that to distinguish your different loads so you have a kind of pseudo provenance.. CLA is signed.\nCI failed due to permission issues for outside PRs, but because this is only a docs change I'm going to go ahead and merge.. So I think this actually does most of what's talked about in #3005. It's just not terribly clear from the diff that that's what it's doing because a lot of the logic was already there but was being bypassed. Previously when we failed to talk to k8s we would mark the job as failed and then continue the master loop. This is bad because it means the pipeline won't get considered again unless it gets updated. With these changes we instead error out of the master loop when we can't talk to k8s,  this means that we'll retry everything from scratch when we complete the backoff timer. It also means that another node has an opportunity to become master during this time, which is good if our node is disconnected from the k8s master.\nThe only case that isn't covered is when a pipeline gets deleted but the k8s resources don't get cleaned up. Which can also potentially happen with partial undeploys. That one is a bit trickier to fix, but also already has it's own issue, #2995.. Thanks for the issue, we are using satori/uuid, we haven't seen this issue though. Still it shouldn't be hard to migrate off of it so we'll do so.. One question:\n\nEach worker will merge a subset of the datums that a job processes, and then the worker master will merge each worker's hashtree to form the final output commit\n\nHow is this data being exchanged between the workers and the master? I can think of 3 possible ways:\n\nWorkers merge their subset, write that into object storage, and then the master merges all of those chunks.\nWorkers merge their subset, store it on their local disks, the master requests it directly from them.\nEverything gets merged in a streaming fashion, workers merge their subset into a unified lexicographic stream which gets stream to the master which essentially merges a stream of streams.\n\n3 seems like it's going to be the most performant, but if a worker dies you basically have to start that stream from scratch. Unlike with 1 where we have a checkpoint for each merge. I think workers dying is going to be fairly uncommon so I think right now I prefer 3. 2 seems like the worst option right now because we have to deal with things being stored on disk and those machines potentially dying and having to recreate the files, seems a bit harder to get the error handling correct than the other 2 options.. Closing for oldness. @GuillaumeDelaporte let us know if you decided to sign the CLA and we'll merge this in.. CLA has been signed. Since this is just a docs change I'm going to merge without waiting for CI to pass. CI doesn't check anything that this touches anyways.. Overall I think all of the major pieces are in place for this and it's coming together nicely. I think it would benefit from some more extensive testing, particular of some weirder cases with multiple tables in the same repo, deleting of some of the subfiles, tables within tables, etc. I'd also like to see some more examples of PGdumps from different versions and some corrupted ones to see how it responds to negative data. Also I think you'd benefit a lot from putting these tests in src/server/pfs/server/server_test.go it's much faster to run and you can easily drop into a debugger while you're running it. That's been the only way I've been able to get new pfs features with complicated semantics right, such as the original --split code.. All LGTM, I realized one more thing during reviewing which is that I think we need to update CopyFile to handle the Header and Footer we'll want to copy those over when someone does CopyFile on a directory with a Header/Footer. Sorry to move the goal posts again, this one should hopefully be pretty straighforward.. We have a template for feature requests like this, please use that, it's helpful to have issues in a common format.\nThe general concept for this feature seems to make sense but I'm not so sure about the details. In particular doing things on a per job basis doesn't seem like a great idea. Suppose you have a pipeline that's getting a steady stream of new commits, each with a very large number of datums. The pipeline would be in a steady state with a large number of workers running. Then somebody makes a small commit that doesn't add many datums, it could even be a commit that adds 0 datums because it's deleting some data that was incorrectly added. Now we scale down to a minimal amount of workers, which causes a huge amount of etcd traffic. This commit finishes processing pretty quickly, likely before we're even finished scaling down. Then the next commit comes in, a big one this time so we start scaling back up, which is a ton more k8s traffic. This will make the second job run much slower because it has to wait for workers to spin up. It also puts a ton of extra stress on the cluster, likely making everything else run slower too. And this can get very pathological too if you have a pattern of alternating commit sizes which isn't too hard to imagine happening in the real world.\nSo in summary, jobs are a bad way to think about this, because job boundaries can be fairly arbitrary, what really matters is total datums / time that a pipeline is receiving. If those are split across several jobs or all clumped into the same job it doesn't really matter, you'd still want this feature to do basically the same thing.\nThis feature may not be a good idea in general. Abstractly speaking, our current parallelism spec forces users to give us some information about the future capacity they expect this pipeline to need to support. When that fails they need to reevaluate that expectation and give us updated values. Predicting the future is hard, so obviously we'd prefer to save users having to do this. But it might not be possible to have sane parallelism behavior without getting predictions from the user. And I'd rather inconvenience the user a bit upfront than have a system which can blow up in the user's face later once they're relying on the pipeline.. @suneeta-mall can you tell us more about what you're doing when you're babysitting your pipelines. Specifically what changes do you make to the pipeline / kubernetes cluster and what circumstances do you make those changes in response to?\n\nI need to understand why pipelines are by default pods and not deployments\n\nPipelines are replication-controllers (RCs) instead of deployments. RCs and deployments both create pods, that's always going to be the end result whatever controller we choose. You as an enduser can scale RCs in exactly the same way you scale deployments, with kubectl scale and you can also use autoscalers on them, with kubectl autoscale. However I think autoscalers k8s' default autoscalers are unlikely to do what you want. Autoscalers work by monitoring the utilization of the pods and scaling up if necessary. This makes sense when your pods are serving requests from the outside world and high utilization means you don't have enough pods to deal with the traffic. When pods are churning through a predetermined data set they'll be at 100% utilization until they run out of data at which point they'll be close to 0%. So as long as there are datums your scheduler will continually scale up the job until it consumes all cluster resources. You can cap this, so it doesn't go too high, but then you have a system which will just gradually scale jobs up to static limit, which doesn't seem better than the system we have now which just starts at the static limit. For most jobs this will get you results faster, the only place it'd be worse is if you have a very small job that would never get all the way scaled up because it finishes to quickly.. > I find it annoying that increasing parallelism (even though nothing on i/p or transform step has changed), all pods gets recreated - terminating what they were doing. Again, there might be justifiable reason for that, which I dont understand just yet.\nThere isn't a great reason for this, we just only have 1 code path for updating the pipeline and it does the less efficient but always correct thing of redeploying the pipeline from scratch. This is something we can definitely look at improving long-term, in the short-term though you can get the behavior you want with kubectl scale rc pipeline-..., that'll allow you to scale a pipeline up in the middle of a job without interrupting anything.\n\nWhen I babysit, I basically monitor how much datum it is yet to process and increase parallelism basing on that (if I want it faster which so far has always been the case -- who does not want faster - scale horizontally! :-D).\n\nIt sounds like if you're always needing to increase parallelism you should just set the parallelism to a higher value to begin with. I can see two downsides with that:\n\nWhen the pipeline isn't running it'll consume more resources than it would otherwise.\nFor small jobs you'll have more workers up than you otherwise would.\n\nI think both of these problems are heavily mitigated by using standby in your pipelines, the first becomes a complete non issue, because when the pipeline isn't running no pods are up anyways. The second isn't totally fixed, but is no where near as bad because the pipeline will finish small amounts of data much faster due to the heavy parallelism. Even if there are fewer new datums than workers there's other non-processing work that will get done more quickly with the added workers and after it's finished the pods will all go away.\n@suneeta-mall how much of your problem would the above solution solve?. > Another config request: allowing export as the same PG dump format that we take for input, so that multiple postgres steps can be chained\nThis should be the default, and probably only, option for SQL pipelines.. Good point, CSV and SQL dump are the 2 main formats we want to support. CSV should be usable for chaining SQL pipelines as well.. > The one kinda tricky issue is that, although widely used, CSV is a pretty crappy format, with a ton of different subtly-different implementations\nI don't disagree with this, although I don't have quite the level of CSV hate that @gabrielgrant does, but I think it's a common enough format that we're going to need to support it sooner or later. JSON can also be made to work with our SQL support. Parquet is probably going to be necessary eventually as well... but let's not expand to scope of this issue any further.\n\nMore generally, this seems like a pretty powerful pattern (would be super useful for the spark stuff I've been working on). Could we generalize this into a \"pipeline-data adapter API\"\n\nDefinitely, that's the direction this is all going, we'll be keeping an eye on how to generalize this but I don't want to creep the scope of this too much to support everything in one PR. But we'll make sure not to do anything that will make it harder to support in the future. Re: the different containers @gabrielgrant describe. Those will likely going in our Pipeline definition as Extract and Load fields to match our existing Transform field.\n\nAnother consideration for the pg-specific case is that, for a multi-stage SQL pipeline, dumping and loading at every stage can be incredibly time consuming\n\nThis would have some performance benefits, but I don't think this is as clear a performance win as you think it is. A couple of things that mitigate it:\n\nwe parallelize that loading process, and it will often be a relatively small part of the total runtime compared to the processing\nthis benefits from deduplication, so you generally won't need to load the same data twice\ndownstream pipelines being able to consume data directly out of upstream pipeline's database, while more efficient, creates a coupling between 2 pipelines that didn't exist. Allowing load from one pipeline to bring down the other.\nthe amount of data being loaded into the database per datum may be very small, as small as a single row, in that case optimizing how we put it will have very little effect and the other costs associated with a datum are going to dominate.\n\nJust generally I think we should punt on performance related goals for this project save for not implementing things that we know will make it impossible to get reasonable performance out of this system going forward. Caching things rather than reloading them is a great optimization to make later on, because regardless of what we do, we're going to need all of the code that loads the data in to the DBs and extracts it. Since these DBs that we're keeping up as caches need to come from somewhere. These types of optimizations will make sense at some point, probably when we're getting user pressure to make a workload more efficient.\n\nHigher level question: in cases where people really want to distribute big SQL queries, wondering if we should maybe consider just recommending/integrating something like Citus\n\nI think this plays pretty nice with our side car model, right now we're thinking of the side car as a single container, but it could also be a cluster of containers and the API wouldn't have to change much if at all to support it. I will say though that this isn't quite letting each system play to their strengths, Citus and Cockroach are both designed as distributed SQL for transactions, we're doing distributed SQL for analysis, the major difference is that we know a priori exactly how much data we have and can chunk it up however we want ahead of time, Citus and Cockroach are designed to receive a constant stream of inserts and distribute those live with low latency while keeping storage and compute usage fairly even between nodes. That's a much harder problem than the one we're looking for a solution to. Presto and Hive are probably the best examples of what we'd like an outside system to look like... unfortunately they're both written in Java and bring over some of the crappiness of the Hadoop ecosystem, but it may be fine integrating them since they'll be off in their own containers, doing their own thing. If there was a go implementation of Presto that would really be the best case scenario for us, but I don't think it really exists.. Adding an initial design for this: I think we can get most of the value of this feature by adding tracking to commits. Not every action in Pachyderm results in a commit, but almost every action does, and I think for the purposes of this issue being able to see who created a commit will be sufficient. The other major thing that users want auditability for is pipeline creation and updates, that works great because since 1.7.0 pipeline specs are actually stored in commits, so we can use this system as the source of truth and then render that information nicely as part of the PipelineInfo you get back from list/inspect-pipeline \nWhich user should be listed as the creator of downstream commits is an interesting question. The following options seem reasonable to me:\n\nthe user who created the commit that triggered the output\nthe user who created the pipeline that created the output\nboth of the above 2 (creator would become a repeated field)\neveryone who created a commit in the output commit's provenance, all of the above and then some\noutput commits have no creator\n\nI'm actually thinking that the last one might make the most sense because all of the other options are basically pulling info from some or all of the commit's provenance in as the creator field, which is duplicating data. Instead we should just use provenance to understand where output commits come from, since that points the full picture and lets us have a single source of truth.\nAnother kind of interesting question is who gets counted as the creator of a commit when the StartCommit and FinishCommit (and potentially the put-files inbetween) call are done by different users. I think we should just say that the user that calls StartCommit is the creator regardless of who makes the other calls. It seems like the sanest option, if we didn't choose it then open commits wouldn't have creators which would be confusing. I think that's probably one of the more important use cases for this feature \"We have an open commit that's been sitting here for 3 days, who do I ask to make sure this is safe to delete.\"\nThe last thing I'm not 100% sure on (except the things I haven't thought of yet), is what exactly we should store in CommitInfos to reference users, this is probably a question for @msteffen, it looks like in the auth protos we normally just store strings called username so that's probably what we'll want to do here? If so that seems simple enough.. @msteffen good point. One simple way we could achieve that is by adding a secondary index to the commits collection that indexes on user. I think that would allow us to quickly get commits by user. I'm not sure if that would get problematic when we had a very large number of commits for a single user.. I'm not totally sure this is an improvement. \"What is the ask?\" and \"What is the goal / desired outcome?\" do seem a little redundant, I think people have been interpreting them as \"What is your situation / use case?\" and \"How do you imagine this being solved?\" which is getting us the information we want. I worry that \"What is the goal / desired outcome?\" will just get us short answers like \"I want to run my pipeline on historical data.\" without pushing people to tell us more about their use case / situation from which we normally discover that they're doing something else wrong with the system and won't need the behavior they think they want if they use the system as intended. That was the main purpose of this template initially.. Another worry I have is that by asking: \"What is your proposal for a feature to solve this?\" it makes it seem like we're looking for people to have a thought out implementation for the feature, some people will have this but we probably don't want people to feel like that's a requirement for opening an issue.. Changed this ever so slightly, ordering and specifying that feature recommendation is optional. Not sure if it really improves it much, I probably should have just merged this initially.. This is definitely a needed feature. \n\nOne downside is that would create an extra, potentially-unneeded directory layer within the repo, instead of putting them at the root.\n\nThis is also a problem that put-file --split has but I think it's fine, forcing people to be explicit and give the file a name is a good practice and makes it less likely that people will get confused (I think). If we wanted to support that we could conceivably allow them to replace /pfs/out with a regular file rather than a directory. This could also have a meaning outside of the split use case, it would create an output repo with a single file in it whose name is \"\". Not sure if I like this idea, it might be too weird, but it's a possibility.. @suneeta-mall thanks for the pull request, I'm going to let CI run, then I'll merge this.. Hi @suneeta-mall we actually do have this feature it's just not documented, sorry about that. Here's an example of what it would look like:\njson\n{\n  \"secrets\": [{\n    \"name\": \"secret\",\n    \"env_var\": \"secret\",\n    \"key\": \"key\"\n  }]\n}\nthis will take key from secret in k8s and bind it to $secret in the environment of your process.. @suneeta-mall thanks for the pull request, we'll get this merge asap.. This PR adds a new command pachctl debug-dump which dumps the running goroutines from pachd and all the running workers, this is something I've wished I had in the past when users clusters get locked up and it's hard to tell what's going on in them.. Is this only for services or for normal pipelines too?. There's nothing we can really do here, we neither control that error message nor bash's behavior. I also think it's a pretty reasonable error message that should allow people to figure out what's going on, and this error message is also the one you'll get if you accidentally type an extra argument, which is much more common, and will be incredibly confusing to people if they get back some error message about globs when they don't even know what that is.. There's a way to override the library we're using's toggle based on the type of file it's outputting to.. There were a couple of reasons for switching out the FUSE library. When we first implemented FUSE the previous one was our only option, at this point the new one seems to be a bit better maintained and have a bit more traction, so I feel generally a little better about it. The main thing it has that the previous one doesn't is that it allows you to implement simple readonly filesystems more easily without having to worry about some of the more complicated things like inodes and such. The the code is meaningfully shorter, although the vendor changes obscure it a bit, the actual pach-fuse implementation is almost 1000 lines shorter.. A better approach to this might be having pachctl automatically port-forward for each command. This is, according to user comment, how helm talks to tiller and it is, again according to user comment, quite reliable.. If you maintain the salt between creations of the pipeline (which update-pipeline does) you won't get any reprocessing, you will still get a job. That seems fine except that it redoes the merge and the egressing. The prior should be short circuiting very quickly due to 1.8 changes. And if we move the egressing step into the merge as we plan in #3223 that should shortcircuit as well.\nWe may want to consider an earlier short-circuit for this case.. copy-file is recursive by default, if src is a directory, and errors if dst is already a regular file. Semantics are inspired by ln, and internal implementation is most similar to ln, despite the fact that the name suggests it would be most similar to cp.\n. put-file can't copy data within the same cluster, unless you use pfs://localhost/... as the -f parameter, in which case it's hopefully clear that data is being copied over localhost. That's not something you really ever want to do though, if you're copying within the same cluster you should use copy-file put-file from pfs:// is meant for importing files from other clusters.. This would be fairly easy to do, it just comes down to removing a line of code. There will also be some migration pain associated with it but it's manageable. As we've discussed offline though this won't mean that completely disparate pipelines will be able to reuse each other's results. Doing that would be a much larger design decision that I'm not yet convinced would make sense for our system. The ability to rename pipelines would definitely be very nice though, so we'll look to allow that.. This is what's known as a \"straggler\" in distributed computing. Processing a datum entails a certain amount of overhead, so instead of paying that overhead for each datum we pay it for chunks of datums. This greatly decreases the fixed cost of processing a datum, but has the disadvantage that there can't be parallelism within a single chunk, so at the end when there's only one chunk left it gets processed by a single worker. You can control how we chunk the job using the chunk_spec parameter in the pipeline spec. But it's very unlikely that you'll make the job run faster. 26 datums is about .1% of your job that's taking 3 times longer than it needs to, this translates to a very small amount of overall runtime that you could save by making your parallelism perfectly distributed.. This is the intended behavior, when you create or update a pipeline the intent of that is to have the pipeline's output repo be kept up to date with the heads of the branches it's targetting, processing historical commits isn't part of the intent, although it happens as a natural consequence of the head commit becoming a historical commit as new commits are added. But everything that a pipeline processes was at one point the head of the input branches while the pipelines existed. There are several reasons for this, by far the biggest is that it doesn't make sense with our delete semantics. The most common use case for delete-file is to get rid of data that messes up processing in some way, either causing it to fail or to take a really long time, perhaps hanging forever. If we processed old datums then at best it would greatly delay when you got out real results from the new pipeline because it would force the pipeline to go through a bunch of failed jobs before it processed a commit that might succeed, at worst it would prevent you from getting results all together, because it would reprocess the datum that made the code hang. If you want historical datums to be processed when you update the pipeline then you should avoid deleting (or overwriting) those datums and instead keep them present in the head of the branch.. update-pipeline will only process the head commits of the input branches, not historical commits, for reasons described here. --reprocess has no effect on which commits get processed, it affects whether or not we skip previously processed datums or use cached versions of them. To confirm, are you seeing no new jobs getting processed here, or a single job? The prior would be a bug, the latter is expected.. LGTM. This is due to list-datum doing an in memory sort to make sure it returns the failed datums first. This can be fixed by converting it to a streaming model.. This was changed as part of: https://github.com/pachyderm/pachyderm/pull/3031 it's necessary that the scratch space for /pfs be under /pfs itself, (previously it was /scratch) otherwise we can't use symlinks to put the data into place and have to use bind mounts which require privileged containers.\nSorry this came as a surprise, unfortunately our backward compatibility guarantees can't be strong enough that we guarantee the filesystem that user code sees will always be identical between releases. We can guarantee that the inputs and outputs under /pfs will be in the same place but sometimes we need to add some other hidden files or move stuff around to get things to work.. As workaround here you can have your find invocation ignore hidden directories.. @suneeta-mall and @jdelfino sorry that this affected you. Going forward we'll make sure not to put changes like this in point releases, which means that there will also be an RC period for changes like this that will give you more of a heads up. I hadn't fully thought through the various access patterns this could break. I was thinking of it as just adding some hidden files that most user code wouldn't even notice, but with things like find that's easier to notice than I was counting on. And changing to symlinks from bind-mounts is a much bigger deal than I thought it would be. Apologies again an thanks for bearing with us.. LGTM. Thanks for reporting this @jdelfino this is definitely something we can get fixed.\nNo worries on not having a repro, we can figure this one out internally.. This is something we can add, although it's going to be a job state, not a pipeline state.. No worries, deleting seems pretty reasonable here so I'm going to do so.. Are we worried that these files could also be included in the Docker build? I guess that would require us to add them in the Dockerfile which seems pretty unlikely.. Seems reasonable to me.. Hi, @quantonganh thanks for taking the time to do a PR but as of a little over a year ago uuid.NewV4 returns 2 arguments. See for yourself here. Your local version is probably out of date, you can update it with go get -u github.com/satoori/go.uuid and things should start compiling.. @suneeta-mall was this pipeline run for a while without stats enabled and then had them turned on later?. @suneeta-mall I think what's going wrong here is that the code that marks jobs as finished isn't as fault tolerant as it could be. The oom-kills prevent the job from getting marked as completed despite the fact that  the output is getting materialized. We'll look at fixing this, in the short term you can tell if jobs are completed if their output commits have been finished and you should be able to read the data out of those commits.. The issue here is that you're using 1.7.6 pachctl with a 1.7.3 series pachd. One has #3118 while the other does not, that PR changed around how PutFileRequests are sent, such that when you do a put-file -r all of the files get sent in the same stream, 1.7.3 interprets this as appending all of the data to a single file.\nI just did a test to confirm that this does indeed produce the right results with pachd and pachctl at master.. I also renamed some files from readme.md to README.md, which wasn't necessary to fix the bug, but seemed like a nice thing to do here as well.. The high level summary of these changes is that they make the fuse layer much faster. They do this by sending a single request for the object, writing it out to a file and then using that file to server later read requests. Previously each read request would be another request to Pachyderm which was very slow because the OS issued a request for each 128K chunk of the file, so even a decently size file would result in many many requests.. Sorry you hit this @suneeta-mall, the symptoms you're describing here all seem to point to pachd have encountered a crash (probably a segfault) in response to that command. Probably to get logs that show the backtrace for the crash you're going to have to do kubectl logs <pachd-pod> --previous.. I think if we go with the commit-style syntax then that's likely going to be true at the API level as well. I.e. you'll pass protocol buffers that have a pipeline named foo^^ This is the way it works for commits as well, parsing actually happens server side. This has the advantage that it works across all clients, not just the cli.. We have ancestry syntax but we still want pachctl list-pipeline --history to work.. @suneeta-mall I think at least some of this is just the eventual consistency model of s3. The only part that I'm not sure that can explain is objects disappearing. We do overwrite puts during egress. We never, do deletes. s3's docs claim that overwriting objects is atomic, meaning that reads will return either the old data or the new data but never partial or corrupted data. Which presumably means it won't return nothing. However it doesn't document what happens if you attempt to overwrite an object and then crash while doing so. My assumption was that this would revert the object to its previous value, but maybe that's not the case. One thing you could do to debug this is turn on logging for the bucket, that way you could see all of the requests that are being sent to it. If there's deletes in there then we're doing something unexpected, since egress shouldn't ever issue a delete. If not then there's something not totally accurate in the s3 docs and this is going to be an expected behavior of egressing to s3, that's just inherent in the eventually consistent storage model.. That's a nice looking bike shed, I'd be cool with using that, |>might work for similar reasons. Although it might get conflated with >.\nMaybe we should just embrace the true bike sheddy nature of this problem and go with the obvious choice: |\ud83d\udeb2\ud83c\udfe0. A little added info on methodology here for posterity, these issues were uncovered using error injection. Very hacky error injection that just modified the code in src/server/worker/{master.go,api_server.go} so committing that code would probably be too damaging to the code base to be worthwhile. That unfortunately means that those changes can't be used to enhance our CI suite right now. See #310 for a more in depth discussion about adding this to pachyderm in a more long term way.. Note to self, this syntax also needs to be applied to the extract-pipeline. No worries about taking a while to review, I would have pinged you if it was important earlier.. Many thanks for the pull @AmundsenJunior. CLA has been signed and since this is only a docs change we can merge without CI passing.. Possible direction for solving this: https://github.com/cockroachdb/cockroach/tree/master/pkg/sql/parser. Is Pachyderm deployed in a namespace? I think it might be that undeploy is deleting the unnamespaced resources here clusterrole clusterrolebinding and storageclass are all cluster scoped, not namespace scoped, but it's missing the other resources because they're in the namespace. If you pass the --namespace flag it should be able to find the relevant resources.\nThis interface could probably use a bit of work, it's a little tricky though because it needs to take into account the case where there's 2 clusters deployed in different namespaces.. I think it's probably not even a dash level change at all, we don't want these objects to be accessible through the API or pachctl either.. This LGTM, can we merge it?. CI might be failing for real reasons here.. set-branch's doesn't match with the rest of our naming scheme. We use create for all other objects, and put/get for requests that take/return streams of bytes. We have an eventual plan to move to a more kubernetes style CLI where you'd have a subcommand create which you can then use to create various objects. It'd be really weird at that point to have a subcommand set which only had branch under it. There's also the fact that set-branch can actually be used to create branches, in which case I think it's a slightly confusing name. When the branch already exists I agree that it's a more confusing name, but I think the consistency makes it worth it. We could maybe add update-branch like we have update-pipeline and update-repo, which are really just create-pipeline and create-repo under the hood.. @HaraldNordgren yeah this is a permissioning issue, I'll get CI running for you.. CI running in #3197. Closing this so we don't have 2 PRs.. Hey @HaraldNordgren this is all ready to merge, I just need one more thing which is for you to sign our CLA, even for small stuff like this we still like to cover our bases with a CLA. . Yup, many thanks. Merging.. This is basically only docs so I'm going to count that as a fully lgtm. Few requests and questions but mostly LGTM.\nOne other thing I noticed while reading this, there's a method called LogReq that the auth packages use, does it log the name of the call correctly? The code it's calling seems to find the function RPC name based on call stack depth, so I think it might be returning something different because LogReq adds an extra layer to the callstack. Not necessary to fix for this PR, although I suspect it's pretty easy because there's another method that lets you adjust where it looks in the stack.. Ahhh, that makes total sense... yeah those anonymous functions are pretty dumb.. This lgtm but I don't see an implementation here. I just see tests changing. Did we already have this behavior or am I missing something?. Oh yeah, I was indeed just being blind there. I was on my phone so maybe it displayed things weirdly. Anyways, still lgtm.. Merging this w/o CI, since it isn't checking the example anyway. I checked locally and it runs and produces the results the README describes.. Related to this, I've been thinking that we need to make one of update or create optional. Right now you have to do create-pipeline the first time, and update-pipeline every subsequent time. I think at least one of them should work no matter what. Possibly they should just become synonyms and implement upsert behavior.. Lgtm just so I understand is this a problem because pachd service isn't always on 650 anymore?. We don't actually want an egressing state for jobs, because now that merging is done in parallel it makes sense to do egressing as we merge. This will massively increase the scalability of the egress feature. However it makes more sense to call the state merging than egressing, since merging will always happen during this time, but egressing will only happen if it's set.. Changes seem to make sense. I'm guessing merge stuff had to be removed cuz of how different merging is now?. Yeah, these might actually not be flakes. I'd try grepping for everywhere in the codebase we use the running state and see if we should be using merging there too.. @ysimonson nope, that big green button wants you to hit it :). This is a nice complete proposal. In terms of value I'd rank the various commands as follows:\nCrucial:\n- create-branches\nThis is by far the most valuable one because with this you can technically do all the other workflows, it's just not quite as nice.\nImportant:\n- start-commits\n- put-files\n- create-pipelines\nThis are all heavily used and get a lot more ergonomic if they're variadic and atomic.\nNice to haves:\nEverything else, these will all help in particular situations and it'd be weird not to have them be variadic and atomic eventually, but the near term value isn't huge.\nComplications\nFor start-commit, I think we could make the second argument required without breaking much, leaving the commit/branch field empty is very rarely used, it allows you to make a commit that's not on any branch, which isn't very useful. We could allow people to get the same behavior by passing an explicit empty string as the second argument, ie pachctl start-commit <repo> \"\". That way you could in theory atomically open multiple dangling commits, I can't think of why anyone would want to do it but I think we could feel pretty comfortable compatibility wise if we did.\ncreate-pipeline actually already accepts multiple json documents in the same request, it doesn't require them to be newline delimited, but they can be, so for that one we'll probably want to introduce create-pipelines as an alias.\nWe might want to even go a step further on standardizing and say that we represent paths as: repo@branch/path/to/file and then you'd have some prefix of that pattern for commands based on what type they take. So for repo commands you'd still just have repo for commit / branch commands you'd have repo@commit and for files you'd have the full thing. This will be a compat issue so we should probably think about this a bit longer and do the breaking pachctl changes in one fell swoop, and probably have an interim period where you can still get the old api with an env-var.\nAlternatives\nI think it'd probably be a good idea to start with adding create-branches since that's the highest leverage one and it'll be a good test case to see how the functionality works and try out the ideas here.\n. I think I like your syntax better, although I think we should also probably outlaw /s in branch names.. This LGTM.. @ysimonson I think that's correct although I didn't write a lot of the code that uses the new reversed stuff @brycemcanally would be the one to know for sure. Is that change required for fixing this bug? Or was it just a general improvement you wanted to add?. I think that'd be the safest bet. There could be some subtle things that break with that. I don't think we have full coverage of all walk calls in our test suite unfortunately.. I think this can effectively be a replacement, the FileInfo.File.Commit field effectively tells you the commit modified here, because you get back files at commits where they were modified. Note this isn't quite what we're returning now but it can be.\nAnother question I have on this is what happens if you delete a file. I.e. suppose I have a file, then delete it, then in a later commit readd it. If I go for the history of that file at master should I see the versions from before it was deleted? I think the answer is no because the pre-delete version is a different file. This is also nice from a performance perspective because it saves us having to always traverse back to the first ever commit to see if maybe a previous version of the file existed then.. > Not sure if there are other \"gotchas\" or limitations\nOdds are roughly 1000% that there are, but minikube is rife with gotchas and limitations too... so it might still be worth it.. I'm cool with this, although it's potentially a gotcha for a dev I guess. But I almost always ignore vendor when I grep for stuff (I don't use ripgrep either) so this is probably a very sane default.. Was planning to have this just be docs but I also added a Committed field to file infos with the timestamp of when they were committed and extended the pretty printer so the feature is a bit easier to use.. I'd love to have more opinions on the above. Here's an example of what it looks like now:\n\u279c  pachyderm git:(reason_in_list_job) \u2717 pachctl list-job\nID                               OUTPUT COMMIT                                         STARTED        DURATION           RESTART PROGRESS  DL UL STATE                                                                                                       \neab040ec83614c70937ba1238a498891 pipeline69a6e2c9163c/e1bac109825b480ebbc105b4aeadbec1 54 seconds ago Less than a second 0       0 + 0 / 1 0B 0B failure: failed to process datum: 902882cfc4d4024423cd2577f27f0a39ca7b74de90f7295869e4a9930934c7c3. Yeah, I realized that too as I was playing around with the command. I think that should be title Pipeline rather than Output Repo though, it's shorter and probably clearer. The one other thing that occurred to me is that if you do list-job --pipeline foo then those values will be the same for every job, we may want to elide them in that case since it's redundant and useless info.. I also added the last job state in list-pipeline. Output now looks like this:\n\u279c  pachyderm git:(reason_in_list_job) pachctl list-pipeline \nNAME                 INPUT                                   CREATED           STATE / LAST JOB                    \npipelinea30cb903dfff TestPipelineFailure_data1c3eca4a4f4c:/* About an hour ago running / failure \n\u279c  pachyderm git:(reason_in_list_job) pachctl list-job     \nID                               PIPELINE             STARTED           DURATION           RESTART PROGRESS  DL UL STATE                                         \nd1f6ec497c974eb895dce5591ccc9c51 pipelinea30cb903dfff About an hour ago Less than a second 0       0 + 0 / 1 0B 0B failure: failed to process datum:.... Yeah, I agree, de-uglifying all of those uuids floating around would probably make this problem go away.. @gabrielgrant this is all right except you're mixing up JSON patches and JSON merge patches (I can't imagine why).\npod_spec is a JSON merge patch, however this does not allowing appending. For that you need to use pod_patch which is a JSON patch (not a merge patch), these do allow appending. So you'll need to use those if you want to append volumes.. I think we should just merge this, now that 1.8.1 is out and there's no migration needed between it and 1.8.0 most people should have 1.8.1 in their hands. If they hit this issue we can just tell them to upgrade and hopefully it won't cause too much pain.. How many tests had to be disabled as part of this?. This seems reasonable to me as an option, maybe the default option. But having a mechanism to install the completion for the user seems pretty valuable too. This was prompted for not having the right install path for osx right? Could we add that as a separate installation target?. > We think filtering the list of files returned by globbing to include files, not directories, is probably more likely to be useful\nI think an even better option would be to filter to only include paths that don't have any paths beneath them that match. That would mean you could do the following semantics:\n\n**, all files (and empty directories, but it's hard to get empty dirs\n**/ all directories that don't contain other directories.\n**/foo all files named (this one should already work)\n\nThis does add the big complication that GlobFile now can't tell if a file should be returned just by looking at it, it has to iterate through the paths under it as well. This might not be so bad though, it already has to iterate all paths, this just changes when you can return values a bit.. Yeah, that probably makes sense. Although it raises a few questions:\n- Should not cumulative cron inputs also use timestamps for names? Seems like that'd be the most consistent option.\n- Should the files still contain content given that that content is now redundant with the names of the files. My gut says no.. > So a job can never really \"fail\" any more or can it still fail just not in the specific user-defined ways?\n\nIf the error handler returns an error, that will fail the job.\n\nThink of it like an exception handler.\n\nBut if there is no error transform defined then the pipeline will continue with the current behavior where any failed datum means the whole job fails and an output commit is not finished?\n\nYes. Also to clarify another point, output commits do get finished post 1.7.0.. We're scheduling this for 1.9. Reading this now I want to make one tweak to the original design which is that I don't think it makes sense to have a whole separate Transform for the error handler. It seems pretty unlikely that you'd want to have a separate container image for the error handler, and it adds a lot of complication to the implementation. If it's just a separate field in the Transform called OnErrorCmd or something of the sort then it should be pretty straightforward to just call it right after Cmd exits with an error.. Also it probably makes sense to expose the exit code of Cmd as an env var, $? is the standard in shell for the exit code of the previous command, so we should probably use that.. @JoeyZwicker you've basically got it. The main details you're missing are how does the data get into and out of the KubeFlow jobs. For getting data in I think our options are:\n\nusing an init container that loads the data into a shared volume.\nimplementing a CSI driver for Pachyderm.\n\nThe first is a bit better understood (by us) but I'm leaning toward the second, since I think it's a better defined, more \"kubernetesey\" and will be useful for other things. You should describe a CSI plugin as \"Data getting mounted in,\" the same as you describe data getting mounted into Pachyderm jobs, it's even more accurate in this case.\nIn terms of data getting back into Pachyderm that's the most interesting remaining question, I'm a little unclear on how {Kube/Tensor}flow likes to output data. It seems like it can output it straight to serving frameworks which I suspect a lot of people will want to do in which case pachyderm doesn't even need to see that data. If not we can figure out some way to reap the data.. I did some research on the underlying k8s APIs and have got 2 concrete first steps for this. Both of which should be pretty impactful all by themselves and will pave the way to a fully redesigned pipelines:\n\nPachyderm CSI plugin. This will allow any k8s pod to mount in a slice of pfs. This is a pretty good example of a simple CSI plugin.\n\nKubeflow support in pipelines. For this we'll add a field next to Transform in CreatePipelineRequest called Kubeflow that contains a TFJob manifest. Instead of running the transform for this job we'll just submit the TFJob to k8s and wait for it to return. Before we submit it though we'll inject a CSI plugin from above into the manifest so that the data they want is visible to the job. Part of the difficulty here is that client-go doesn't have objects for CRDs (such as TFJob). You have to use the Unstructured API.. A couple of additional thoughts I've had on this:\n\n\nWe may want to look at jsonnet for our templating needs. It's already used in other kubeneretes projects like ksonnet which is used by kubeflow and it seems to provide some nice templating features out of the box.\n\nWe should add the ability to run a one off job that's not part of a pipeline as part of this. Lots of user are doing things like that and it would be nice to have direct support for it.. Will take a more in depth look at this soon, but just wanted to add one note before I forget it: could you document hashtree_spec in the pipeline spec docs as part of this PR? I've been pointing a few people at that field and not having much to point them at. Feel free to do it as a different PR if that's easier but it seems to make sense thematically here.. Worth noting on this: pachctl deploy deletes the kubernetes deployments, but doesn't actually delete the underlying data. So an accidental undeploy can be undone by reissuing the pachctl deploy that was originally used to deploy the cluster. pachctl undeploy -a is the version of the command that does delete things and that one does have a prompt attached to it. Maybe for this we should clarify when undeploy starts running that everything it's doing is reversible, and if they want to truly undeploy everything then the -a flag is needed.. The broader context here is that this code is inside the pipeline master loop, which gets run every time to pipeline is updated, and setting a pipeline to PAUSED counts as an update. So without this we go into an infinite loop of updating the pipeline again and again.. I think this will work. The main thing I'm not sure about is if the process gets booted up in openshift and the worker process is running as a random user will calling exec create a process that's also running as that user? That would make sense but I can't seem to find any docs confirming that, if that's how it works then this LGTM.\n\nAnother way that you might be able to solve the same problem is just on startup set the uid and gid fields in apiserver based on what our process is running as. That might be a simpler way to do it.\n. @gabrielgrant given that default I think the code here all makes sense. We may want to add something to handle the case where they try to switch users a bit better. Right now I think that'll actually result in a datum failure, since the failure will come back from exec. @ysimonson up to you on what to add for error handling I'm fine with code as is here.. Fixed in #3427 . This is probably going to break a lot of people's workflows when we release it initially. So we should probably accept the old variable too for a while and print a deprecation notice.. LGTM, many thanks for the PR @setgree . LGTM, many thanks for the PR @setgree . I have signed the CLA, but I think because this was open when the bot was added this one isn't marked as signed and I can't sign it when I click through.. I'd say start a new PR, since this one is LGTM and can be merged.\nThat's a good point about other s3 clients. The only other client I know, besides the official amazon one's which afaik can't be targeted at non-s3 endpoints, is boto, we should definitely test that one to see if anything lights up.\nMy instinct would be that there should definitely be a way to expose this over TLS but it'd be a pain if that was the only way to access it because it would mean people needed to go through some configuration overhead just to get things started, which might derail a lot of people early on.. @gabrielgrant that's correct, I think #2868 is the simplest thing we could do that would satisfy the use case.  However there's still probably some need for http authentication. I'm not sure quite how deep we want to go on this functionality. Recreating all of curls auth flags is probably overdoing it a bit, at a certain point it probably makes sense for people to just run:\ncurl ... | pachctl put-file ...\nFrom a pod within the cluster, that way you have complete freedom to do whatever you want and the performance should be pretty close to equivalent.. @gabrielgrant exactly, it would work wherever those style of urls are used. This is in reference a comment on #2868 for those wondering where s3api came from.. As good as it would feel to just kill these I think it's probably worth updating / keeping at least some of them. The only one I think can for sure be sacrificed is fruit_stand since it was always intended as the \"hello world\" example and the opencv pipeline has replaced it as such. The other ones I think give examples of things people really want to do and we're going to keep getting questions of how to do a Jupyter Notebook in Pachyderm until we re-add them.. Doing it this way should also be much easier since it can use the same script we use to compile our other binaries. If you take a look at docker-build-pachd it looks like this:\ndocker-build-pachd: docker-clean-pachd\n    docker run  \\\n        -v $$GOPATH/src/github.com/pachyderm/pachyderm:/go/src/github.com/pachyderm/pachyderm \\\n        -v $$HOME/.cache/go-build:/root/.cache/go-build \\\n        --name pachd_compile $(COMPILE_RUN_ARGS) $(COMPILE_IMAGE) /go/src/github.com/pachyderm/pachyderm/etc/compile/compile.sh pachd \"$(LD_FLAGS)\"\nthat argument on the last line pachd could be changed to pachctl to compile pachctl instead. I suspect a few other complications will arise but that should be most of it.\nThe harder question here is how to make pachctl connect to pachd from within a container. Have you thought about how we might pull that off @LaurentGoderre?. MAny thanks for the pull @mdaniel. Merging.. LGTM and build is passing. Merging.. I think this seems reasonable. Could you point me at which point of the code is actually fixing the bug here? I feel like everything I'm seeing here is mostly renaming things.. That was on a normal CI run I believe.. This changes the trimpath code to actually work, it wasn't before because we were passing $GOPATH as the trimpath which I guess make was expanding to $G (which resolved to nothing) and OPATH which wasn't a path prefix and thus didn't strip anything. This also makes it so the trimpath gets applied to pachctl in addition to pachd and worker binaries.. 2 small changes requested.. I think it makes a lot of sense to just block on it and not return it, the point of FlushCommit is to return the downstream stuff anyways, and users will already know about the commit they're flushing, given that they specified it. It's unclear what use there would be in having it returned.\nAlso super convenient in terms of not having to rewrite tests :smile:. Hey @mdaniel thanks for fixing this, and even better adding a test for it! Only additional thing would be adding this test to our CI, but I can do that for you. I'm sure you have better things to do than dig around in our CI scripts and you've already gone above and beyond on this one.. CI is passing and good all LGTM, merging.. Hi @suneeta-mall, sorry this is affecting you. I think it's a dupe of #3452 which should be fixed as of 1.8.4. Lemme know if that fixes the issue, if not I'll take another look at this.. Definitely cool with merging that branch into here. I'm not sure how I feel about branch.repo as the naming scheme, is there a reason to do that over repo.branch? Having branch before repo feels weird to me since branches are subobjects of repos.. Yeah, this seems suspiciously like it might be use misusing bolt so that would be a good thing to check on.\nWould you mind adding a test that exposes the bug you fixed so we can be sure it doesn't come back? For filesystem only tests you can add them in src/server/pfs/server/server_test.go.. Much thanks for the pull @mdaniel code all lgtm, I'll merge this as soon as CI passes.. LGTM, merging.. Closing this in favor of #3069 since that's more fleshed out.. This all looks reasonable to me except I think you're vendoring the wrong thing here. I only see one file which is the main function for a CLI for Archiver, not the library you're importing. I suspect this won't compile on CI. It's up to you but I don't think you really need to include an outside library for creating tar archives, the go standard library has good support for both tar and gzip, I just whipped up this program which works with the tar command line utility:\n```golang\npackage main\nimport (\n    \"archive/tar\"\n    \"compress/gzip\"\n    \"io\"\n    \"os\"\n)\nfunc main() {\n    gw := gzip.NewWriter(os.Stdout)\n    defer func() {\n        if err := gw.Close(); err != nil {\n            panic(err)\n        }\n    }()\n    w := tar.NewWriter(gw)\n    defer func() {\n        if err := w.Close(); err != nil {\n            panic(err)\n        }\n    }()\n    for , name := range os.Args[1:] {\n        f, err := os.Open(name)\n        if err != nil {\n            panic(err)\n        }\n        fi, err := f.Stat()\n        if err != nil {\n            panic(err)\n        }\n        hdr, err := tar.FileInfoHeader(fi, \"\")\n        if err != nil {\n            panic(err)\n        }\n        if err := w.WriteHeader(hdr); err != nil {\n            panic(err)\n        }\n        if , err := io.Copy(w, f); err != nil {\n            panic(err)\n        }\n        if err := f.Close(); err != nil {\n            panic(err)\n        }\n    }\n}\n``. I'm cool with vendoring the lib, this seems right on the cusp to me between worth it to bring in a library vs. just rolling our own from the stdlib, so either way is fine by me.. Other things that might be nice to have in the sos:\n-kubectl get all- logs from pachd and workers\n- system information (would be nice for debugging fuse). Code LGTM, will wait on tests for final approval but this is coming along nicely.. Pachd emits logs each time theWhoAmIrequest is received, which happens a lot, that's necessary to validate requests aren't doing things they shouldn't be. I don't really so any reason that the expectation should be that this endpoint is only called once, pachd isn't checking who it is with this request, it's checking who the caller of the request is, so it needs to be done for every request.. I will add that we're working on the efficiency ofWhoAmIcalls so this doesn't become a bottleneck, but we'll still probably be making a lot of them, because of the nature of authentication it's important to recheck it frequently because using stale information can lead to some pretty disastrous consequences.. We're going to need more information to fix this, we don't see this error on clean Pachyderm deployments, so I'm thinking it has something to do with state that was in the cluster when you upgraded.. So the reason we do this is because these help messages get rendered for the docs here: https://github.com/pachyderm/pachyderm/blob/master/doc/pachctl/pachctl_create-pipeline.md seeing that output in the CLI is pretty jarring though. Long term we may want to have a flag we specify when we're generating docs that tells pachctl to output markdown or not.. Needless to say, this is not a difficult implementation, since this is merely a branch with some features restricted. One interesting question is if there's any need to have tag to tag provenance or something like that, or if the underlying commit provenance is good enough. I can't see a meaning for tag-to-tag provenance right now.. Agreed on this being a problem. I think just using an environment variable would be a pretty good step in the right direction. (#71 setup to track this)\n. Minor preference for calling this \"PFS_DOCKER_HOST\" just so that when it's seen outside the program it'll be immediately obvious who this environment variable is for.\n. ChangeOstoosto get this to compile. You'll also need to add\"os\"` to the import list.\n. > I'll make another pull request to fix this then.\n:+1: awesome.\n@peter-edge you had an example of some code that did deal with tls-enabled daemons right?\n. This should still be Check I think.\n. That's on purpose. AWS turns the next line in to a set of user defined args so we need a space there to make sure they get parsed correctly.\n. Good point, removing.\n. Hmm, so this overflows but I don't think that actually leads to an incorrect behavior. In the case when *shardNum == -1 we run FindRole which sets it to a real value. It is a bit weird though. How much does it bug you?\n. Add arg length checking for all of the commands. I checked to see of cobra can do this for us and it doesn't seem like it can which is a bit lame :(\n. Fixed. :D\n. :white_check_mark: \n. I suspect yes. I'm not 100% sure yet though. The bazil/fuse docs say that having a low collision rate on Inodes can improve the performance of some unix operations but it's semantically valid to return 0 for inodes. How about if we remove inodes from the scope of this pr and do them later?\n. Yeah... I had a good lol at that autogenerated code too.\n. :white_check_mark: \n. How do you feel about having the standard be:\ngo\nresult := make([]fuse.Dirent, 0, len(response.FileInfo))\nfor _, fileInfo := range response.FileInfo {\n...\nresult = append(result, RESULT_FOR_FILE_INFO)\n}\nI'm pretty sure it has identical performance. But it has the advantage that you can put breaks in the loop without messing stuff up.\n. K did the latter.\n:white_check_mark:\n. This should be in line with our standards now. Double check that for me though.\n. Right now you do because it has the pointer to the apiClient as wells as the repo and commit.\nWe could refactor this, maybe those things should be moved in to context?\n. I'll make these constants. These are just the strings that show up when you do mount from the command line.\n. Right now it looks like this:\nshell\npfs://test on /home/jdoliner/go/src/github.com/pachyderm/pachyderm/remote type fuse.pfs (rw,nosuid,nodev,relatime,user_id=1000,group_id=1000)\n. Should we have a make target for this? Maybe make mock?\n. I see. Makes sense.\n. This is the actual change, I'd changed the flag pachd looks for in pachd but not in the manifest. This fixes the bug.\n. Most of the changes to this file are superficial due to json encodings not being deterministic.\n. These 4 lines are really nice to get refactored. They were quite kludgy.\n. We now have a single diffMap, much saner.\n. These fields store indexes into the diffs structure.\n. canonicalCommit implements the branch resolving logic\n. 3 annoying functions that we no longer have to deal with :)\n. Although I guess some of these just moved around.\n. Convenience function I finally decided to add.\nActually I'm going to go through now and use this in some places.\n. This is the good stuff right here. Standalone pfs tests :).\n. This was a latent bug.\n. Should this be called YesError to match NoError?\n. This is a bit OCD but could we have this be the same buffer as below and just reinitialize?\n. This is a latent change from pachctl being updated without regenerating this file.\n. This line isn't necessary I think, t.Errorf calls Fail for you.\n. This part seems a little weird, I'm not quite sure what happens when you have 2 json tags for the same field. It might break some of the indexes we have in Rethink (not on these protos but on the ones in src/pps/persist/persist.pb.go).\n. This is hilarious... but I'm afraid I'm going to have to ask that you change it to simply walker :p\n. Could this be removed?\n. If functionally balloons here we should move this over to using cobra like our other binaries. But no need for now.\n. Yeah, since it's just a native proto behavior and it's not breaking anything I see no reason to worry.\n. To prevent fracturing this API too much let's just merge this behavior into ListCommit and update the other callsites.\n. Since we're inspecting a single repo we expect pfs.ReduceRepoInfos(repoInfos) to always be of length 1. Let's check that it is, if so return [0] otherwise return an error.\n. For errors like this I like to say something like:\n\"incorrect repo returned (this is likely a bug)\"\nIt makes it clear that this is an internal issue and not an input issue and will hopefully prompt the user to open a GH issue. I worry that mentioning things like internal API servers will be scary to them.\n. Same as above we should make sure that this reduces down to a single CommitInfo, return it if it does, otherwise error.\n. Good thought leaving this around for when we need it.\nI'd go 1 step forward and put this code into a separate test called TestDeleteFuture or something like that and then put a t.Skip() at the top. That way it will stay compilable as stuff changes.\n. Throughout our code in other places I normally like to use chan bool for purpose like this since it's a little less typing and then call close(ch) rather than sending a value.\nThe first part is fairly superfluous but I think a consistent convention is good.\nThe second part of using close rather than sending is because it's really easy to accidentally leak a goroutine if never recv from the channel. Or worse get yourself in a dead lock.\nYou're not actually hitting either of those problems here but I think we should all just get in the habit of using chan bool and close for stuff like this. \n. This can be done concurrently with a WaitGroup and a channel to collect the results.\n. Same as above. This can be done concurrently with a WaitGroup and a channel to collect the results.\n. I'm not sure this comment applies to this function.\n. Is there a reason inspectFile can't be used in place of getFileType since the pfs.FileInfo it returns will have the file type set.\nI guess it might be a little be less efficient but it seems like this is duplicating some code.\nErr, actually I guess it would be a lot less efficient since this exits much earlier.\n. Would you mind adding a comment here about what each parameter does?\nIt should have already had one, so my bad on that one.\n. The abstraction in this function is a little bit weird, you're first creating a slice of requests, then a closure to handle each of those requests, then iterating through the requests and calling the closure on them. I think it would be a lot easier to just have the first loop dispatch the requests inline. Then handle the original request afterward, no closures or intermediate slices needed.\n. really good explanation of this decision :)\n. could you do clientConn := clientConn here, I like keeping that standard across the code base.\n. You use the word \"remove\" in a few places in this file, could we change those to \"delete?\" In my mind their synonyms so I'd prefer to stick to just one of them.\n. could we just call this dirs? The word get is generally superfluous and ancestor has enough other meanings in this file that I think it'll be confusing.\n. Camel case for packages is a bit weird, could we do kube_api?\n. Did we need to add the ability to create jobs with the JobID already set?\nWhy was that?\n. Could we switch the names on this to VerbNoun rather than NounVerb to match other functions?\n. This is kind of pedantic but I think it's good form to not put a newline between error declartion and checking since they're really part of the same operation. Would you be cool with that?\n. Do we make sure that the output commit has no shard filtering? User might see some really strange effects if it doesn't.\n. I'm a little confused about why we check parentJobInfo.Inputs rather than the JobInfo.Inputs to figure out if we're doing  Reduce. Was it this way before?\n. Hmm yeah, looks like we were doing that in StartShard does that actually make sense?\n. This makes sense having read the rest of the code. How do you feel about going 1 step farther and erroring if it's not set? I'd prefer to only have 1 way to use the API.\n. I think parentJobInfo is supposed to just be the jobs JobInfo. Here's the behavior we want I think:\nIf JobInfo.Pipeline is set use the pipeline repo. Otherwise create a repo just for this job.\nIf ParentJob is set and Job is not a reduce (not parent job like we're now doing) then the commit we create should use the parent job's output commit as a parent. Otherwise the commit should have no parent.\nDouble check that to make sure it makes sense to you... I don't entirely trust myself at this point given we were checking the ParentJobInfo.\n. Err nvm ignore this. I thought this was previously being used and you'd merged it into the above commit_mounts.\n. I think it's a little weird to put this as part of the Dockerfile. Could we keep it in the Makefile?\n. This container does more than just protofix so it should be called proto I think.\n. Can this be docker-build-proto to match our other targets that build docker images\n. We should have 2 versions of this command, make proto and make docker-proto so that the other version can still be used. I think people on macs are likely to need the old version of the command for a while.\n. I'm a bit confused about why this proto is in this file, is it used in one of the API calls defined here?\n. In general I like to avoid waiting for state with sleep unless it's really hard to get around.\nI think this test would be just as good if you just manually created 2 identical jobs and then checked to make sure that it only resulted in 1 job actually being created.\nWe have other tests that check that pipelines create jobs so it's fine to tighten the scope of this test.\n. No need to define an empty request, you can just have the method take google.protobuf.Empty if there's no arguments needed like ListPipelineInfos does.\n. How about if we call this SubscribePipelineInfos?\n. Nice! Making these types is a great idea :D\n. So it seems like if there's an error from client.Recv() then we're going to have pipelineInfo == nil which means that newPipelineInfo(pipelineInfo) will crash.\nThis function can be made a lot simpler. You can just call client.Recv() in a for loop. If cancel has been called then client.Recv() will error and you can just break the loop there.\n. I think you can avoid needing to calls by passing the flag include_initial to the rethinkdb query.\n. It seems like based on how AddShard and DeleteShard are implemented and the fact that calls to them can't overlap we'll never hit this. Is that true or am I missing something?\n. Err... nvm that was the case already\n. Good call, how about pach-deploy? pachctl-deploy suggests to me that it's deploying pachctl.\n. That seems like a legitimate concern to me. We could pass an environment variable to pachd specifying a backend and it would then error if it didn't find the correct secrets for the backend.\n. >.< nope 100% typo. Good catch.\n. Is this change made in the deploy binary as well? This file is actually auto generated.\n. This makes me happy :)\n. This too.\n. Diff is kinda weird but this is the real change here right, just having the rethink server registered.\n. It looks like this got un go fmted somehow.\n. Hmm, so I just realized it could still be possible for 2 identical jobs to get created here if both calls check for the other's job at exactly the same time.\nMaybe we should make it so the id for a job id is a hash of the pipeline and inputs, then we can just call insert and it will error if you try to insert 2 identical jobs.\n. So I think there's a couple of issues with this implementation of AddShard.\nIf a pipeline gets created for shard 0 before AddShard(0) is called then you'll ignore the pipeline (because the channel for shard 0 will be nil) and you'll never see it again in the call to SubscribePipelineInfos. So we'll have a pipeline that isn't being run anywhere.\n. Ahh cool, didn't realize it worked that way. This does look nicer.\n. Think this needs to be in its own goro.\n. What does this mean?\n. It seems like this could lead to a long line spanning blocks, what if we check that bytesWritten > blockSize && !isPrefix?\n. Bummer that we have to do a sleep here... but I guess there's no way to wait for something to not happen.\n. Incredibly minor point, I think you want present not presented\n. Wait so does this mean if a test fails and we didn't pass -v then we won't see the logs from lion?\n. Can we call this cancel since it's giving a direction \"cancel this commit?\"\n. Let's call this just cancelled, it seems implicit that we're specifying what type of object we want.\nAlso this is very nitpicky but could we put it above block? In my mind repo, commit_type, from_commit and cancelled all describe what you're getting while block describes how you're getting it and it makes sense to me to organize it like that.\n. Minor, but you could just use |= here\n. I'm not quite sure if this works. According to the documentation for CondVar there's no guarantee that the condition is true when Wait returns and you need to wrap it in a loop.\nI generally like to avoid CondVars, mostly because the Wait semantics are so weird. I think what you're doing here would probably be better done with a simple channel. CondVars are useful when you have a condition that can switch between true and false repeatedly, but commits only get finished once so you can basically swap close(c) for Broadcast and <-c for Wait and get exactly the semantics you want.\n. No need for this, DiffInfos only get serialized when the commit is finished. So we can assume that the commits we receive in AddShard are all finished.\nWhy don't we make that an assert in this function so the invariant is obvious?\n. Yeah I'm a fool, |= is for bitwise or, it doesn't work on logical or.\nKindly ignore this :p\n. I like all\n. Logic checks out. Removing.\n. Oh this test is recreating the situation described in #296 . I kind of like adding regression tests for specific issues, what about TestIssue296, and maybe a comment with a link to the original issue?\n. Can this be cancel?\n. includeCancelled -> all\n. cancelled -> cancel\n. Could you check if the aws impl has this behavior implemented as well? I'd like to keep them in sync as much as possible.\n. Cool, everything looks good here then.\n. These fields introduce a number of ways for the data structure to become inconsistent which will likely lead to bugs in the future. currentBlockSize is only ever set to pfsserver.ByteRangeSize(r.blockRef().Range) so it should just be a method that returns that. Otherwise you're implicitly adding the constraint that every time someone updates index they have to update this field as well. Eventually someone's going to forget to do that and it'll turn into a really tricky bug.\n. I wondered about that myself but I think this is actually a reasonable thing to do. Segment keys aren't actually a security issue since they're write only, it's pretty common for people to just embed them in their websites javascript. Putting them in a secret wouldn't do much since we'd be shipping the secret as part of our manifest, which wouldn't keep it secret from anyone.\n. So we're defaulting to a recursive list file? Does that behavior match what we had before?\n. I see, that behavior for pachctl seems reasonable. The name recurse is a little weird imo though. Since I'd expect it to mean that we actually return records for the files in subdirectories not just include them in size counts. Do you think there's another name that describes this better? I can't think of one of the top of my head.\n. It's fine to not know the correct sizes of directories, ls doesn't report the size of files stored in directories for normal filesystems either. And generally speaking the size of directory shown by ls isn't normally taken to be the size of the objects contained in it.\n. How hard would it be make this logic be serverside? I generally feel a little uneasy with putting real logic in frontend things like pachctl because it creates this walled garden of stuff you can do with pachctl but can't do using other interfaces such as the go api.\n. I got a little bit confused trying to understand this logic, how would you feel about calling viableFilterShards nonEmptyFilterShards instead? I think that would have helped me get it sooner.\n. I think if there's an intermittent issue we're trying to track down leaving printfs in a test can be fine since they shouldn't be seen most of the time when tests pass and there's not much chance of a user running into them. On the other hand we may hit a point when there's too many things being printed at which point it'll stop being useful and we'll have to cull them. But we can deal with that when we get there.\n. I was thinking that we could augment the semantics of CreateJob a little bit to get us there. Basically make it so that you can call CreateJob with just CreateJobRequest.Pipeline set and it will go to the db to find the transform. We could probably then simplify the way we call CreateJob from runPipeline as well. Does that make sense?\n. I have a pretty strong preference for keeping it a single request. It seems like any option we could possible want to add to RunPipeline we're also going to want on CreateJob so we're just setting ourselves up to either implement everything twice or have their functionality diverge.\n. Would you mind doing an InspectJob here like in the other tests? Also add a timeout to the context so that the test can error after like 30 seconds rather than blocking indefinitely and causing a timeout.\n. Could you remove this comment? Now that it's called parallelism it seems like the comment isn't telling us much.\n. lol @derekchiang big lol\n. Oh good point. I guess you would have to do a ListJob first, that's a bit of a pain though so maybe just add the timeout to the call to ListCommit and call it a day.\n. There's actually a helper function the check arg length, example\n. It also lets you just return errors to instead of having to call os.Exit\n. Ah ofc, this would have been using the variable length version.\n. I agree that's better. Unfortunately that's what our other targets return as well. We should see if there's a way to get the helper function to print usage when there's an argument count mismatch. But that's getting outside the scope of this PR\n. Seems reasonable, was this something we should have been doing before or did the changes here require this?\n. Hmm, I'm not sure that this is correct. What we're basically saying here is that if the offset is past the amount we've written (ie there's a gap) we treat this data as if it's repeating all the data we've already seen, which it probably isn't.\nAlso I don't think there's any guarantee here that when repeated < 0 h.written < len(request.Data) which it has to be because we use it as an index on request.Data.\nUnder what circumstances do we get repeated < 0?\n. I went back and forth on these myself, how about CommitTypeNone and for the other constants Map and Reduce we'll call them InputTypeMap and InputTypeReduce.\n. So what's going on here is that c.CreateJob and c.PpsAPIClient.CreateJob are actually different methods. c.CreateJob is the nice client method that we define and that is documented. c.PpsAPIClient.CreateJob is the grpc call. In a lot of places in our tests we still need to punch through to the grpc method because c.CreateJob doesn't expose all the functionality of the underlying method.\nI agree with your point in general though so over time we should expand the functionality of the client methods so you can do everything with them and then use them in tests instead of the grpc methods so that tests can server as documentation.\n. Yup this was one of the nicer wins of this refactor.\n. I feel like having a dependency on pkgcobra for ErrorAndExit is a little bit silly. We can just printf the error and then call os.Exist ourselves.\n. Could we add InputCommit as an option here.\n. I think it would be good to include the extend pipeline spec docs that we have on GH in this help string.\n. Given that we're bumping up against deadlines I don't feel too strongly about this one. I'm fine merging w/o it.\n. What's this change doing? Can't quite grok it, were we just not checking the filterShard when we should have been?\n. I've been using c for the variable name to avoid this conflict. But this works too.\n. Doing this in pachctl makes me a little uneasy since it means you can't use the aliases if you're using the API directly. Does oneof work for this in the protos?\n. <3 this comment\n. Yeah... starting with the name \"protocol buffers.\" Like what exactly does that name mean?\n. This file is actually autogenerated by pach-deploy check in src/server/pkg/deploy/assets/assets.go for where you should add it.\n. Could you use pflag instead? It's what we use in other CLIs because it's bundled with cobra. Also it's more posix compliant I think... which seems like a good thing. Just import github.com/spf13/pflag, it's already vendored.\n. I figure we might as well include the input commits as well while we're at it. Maybe as PACH_OUT_COMMIT_ID or something of that ilk.\n. Assuming this is supposed to be a reference to our canine friend it's actually leni... Needless to say this is not a real defect in this code review.\n. I think I was the one that pushed for StreamingReduce but looking at it now I think IncrementalReduce makes a bit more sense.\nAlso I think we should name the field Incremental rather than Incrementality.\n. Could we make this a const map from string to bool? I feel like that would be a bit more readable.\n. I think these should be PARTITION_BLOCK to match the other enums we have.\n. Conclusion on this was that oneof is too unwieldy with protos so we're going to go with this.\n. We should probably add some checks here that the parent job have the same set of input repos as the child job. There's technically nothing in the API that forbids this.\n. Could we rename to just shardModuli I feel like compute doesn't add much.\n. Lol\n. Could we call this just filterNumber. Compute feels implied to me.\n. definitely\n. You should test this but I think by doing Debugf here you'll make it so these don't show up when users run the code since the debug level is set at Info. Infof will show up though.\n. It is an invariant. Asserting it gets a bit tricky though. For one reduce functions don't return errors so we'd have to add that and handle them. For a second there's no guarantee about the order of the Provenance although it should in theory be the same. We'd probably want to compare them as sets. Considering that we're going to be getting rid of this code when we do the pfs refactor my vote would be to ignore for now. I can implement it though if that makes you uneasy.\n. #lockclosure\n. I think long term we're going to want a PrintDetailed____Info for every object we have and it should probably be a little bit prettier than just printing the JSON. This is fine for now though.\n. We use \"from\" not \"from-commit\" in GetFile could we use that here as well?\n. Could we make this ErrJobNotFound to match the other errors we have?\n. I see some value in incremental reduce since it makes it a bit more discoverable. On the other hand I think that making the jump that incremental-reduce is just reduce with the incremental flag set to true is pretty reasonable and helps the user get started understanding the full power of the method object so I think removing the alias is pretty reasonable.\nI suspect as more use cases emerge we'll be adding new fields to the method object and probably a few new aliases. We can reevaluate this as well as the other ones that crop up.\n. Will these errors get returned by the golang API? If so does it maybe make sense to put them in src/client so people can use them there?\n. Yeah I think it's ok. We might have to restructure the demo a bit.\n. Could you just inline the sanitizeErr calls? Storing it in a variable each time seems like a pretty big waste of vertical space.\n. Do these lines make sense. Isn't the error you're checking on line 106 guaranteed to be nil when you reach there?\n. Also it's really weird to me to have the bool returned by a type check not be called ok\n. The behavior of PutFile remains the same. What changes is the behavior of > because > starts by sending a truncate request to the file and we now correctly implementing that. This is the semantics we'd talked about previously where to overwrite a file you do > and to append to it you do >> just like shell.\nI tried to go through the tests and fix them so they kept with the original intent, since this test is for overwriting I figured that it was more reasonable to use > instead of >> and just change the output we expect.\n. Adding a comment to commemorate it!\n. Yeah, we should note however that this is a horribly incomplete implementation since it doesn't implement truncation to any other offsets.\n. What does this number signify?\n. I don't think you need the versionChanClosed map because I think Version should only get called once with each value.\n. Is this file still necessary?\n. Looks good right to me.\n. Added a few more.\n. It errors, I added a test in src/server/pfs/server/server_test.go\n. TempDir is normally used so multiple concurrent programs can get unique directories. It also creates the directory which doesn't really make sense for our manifest generation code. I think we should just us /tmp/pach as the dir name here.\n. Is there a way to wait for this with the kube client?\n. Does this code address that? All this will do is guarantee that the machine that generates the manifest has that directory. The machine that generates the manifest is seldom the machine that runs pachd since most people use the statically generated manifests in the repo. I think for us to use a hostpath in k8s we have to be assuming that k8s will create the directory if it doesn't exist.\n. This seems unlikely but could we bulletproof this against calling FinishJob below, then getting a panic recovering and calling FinishJob again? That could be a nasty bug since the job would finish while one of the pods is still running.\n. In general I think it's good to defend against things like this even when it seems hard for the current code to exhibit the bug. Someone could easily come along and add some buggy code after the call the FinishJob and wind up with an incredibly confusing error to debug.\nBut also I disagree that there isn't room for panicing because we defer another function on line 85 to unmount the volume. If that function panics then we'll hit this bug. So all it really takes is someone making a mistake in Unmount and we'll hit this issue.\nI think it's worth an extra boolean variable to defend against that case.\nAlso could we add a log statement when we recover so we know that it's a bug in the job-shim that caused the failure rather than an actually failure? I think that'd help with debugging too.\n. @sjezewski we talked about this while we were writing it I think. The logic is slightly weird but correct I think it's correct. There's a case where err != nil which implies that writer.blockRefs == nil but retErr != nil in which case we'll still run the else block and set blockRefs to nil which is what it already was. This has confused all 3 of us at this point though so maybe we should refactor to:\ngo\ndefer func() {\n  err := writer.Close()\n  if err != nil && retErr == nil {\n    retErr = err\n  }\n  if err != nil {\n    blockRefs = writer.blockRefs\n  }\n}()\n. This link is broken right now, but will unbreak when we merge this PR.\n. While we're changing names it's a little weird to me that we have JOB_FAILURE and PIPELINE_FAILED. Could we change to PIPELINE_FAILURE to match?\n. Could we move this logic into the amazon and google implementations. This file shouldn't have anything specific to either of those implementations. I think we want an IsRetryable method on the Client interface which will dispatch to the right method.\nThat way when you implement a new obj store you know that you have to implement the method for your own error types.\n. Minor but I think .bin is a little more common than .blob for raw binary data. Could we do that instead?\n. Good point, ridiculously enough proto won't let me use states as map keys... even though they compile to int32s. I guess maybe that wouldn't work in other languages?\n. Surely.\n. Done\n. :D\n. Lol, we should not be.\n. Should be fixed now.\n. Great point.\n. Alright, now it calls deletePipeline just like DeletePipeline does.\n. I think the check that the filterShard here is none nil and that BlockNumber > 1 is redundant. It doesn't seem it should be possible to have FILE_TYPE_REGULAR and no blockRefs unless we eliminated some of them due to a filterShard. Is that true or is there some way to hit that code?\n. Makes sense to me. I actually didn't write this code as part of the PR, just relocated it from DeletePipeline so I'm not sure if there was some reason why returning the error here was correct. But it certainly seems like just logging it would be better so I'll do that.\n. I'd vote we name this IsIgnorable then the information in this comment would be implicitly understood.\n. I rejiggered the way type dispatch was done in this file in my DeleteAll pr. Could you make this function match the others?\n. Yup... classic copy pasta error.\n. We could make it less you think 10 seconds sounds good?\n. That's not what I'm asking.\nIt seems like there are 5 states we can be in when we hit this line:\n- File had blocks, we didn't have a filterShard -> FileFound\n- File had blocks, we did have a filterShard but some blocks matched it -> FileFound\n- File had blocks, we did have a filterShard but it filtered all the blocks -> FileNotFound\n- File had no blocks, we didn't have a filterShard -> FileFound\n- File had no block, we did have a filterShard -> FileNotFound\nIn the cases where the File has blocks checking the filterShard is redundant because the only way a file can wind up with FILE_TYPE_REGULAR is if some of the blocks get filtered, which implies that it's not nil.\nAnd in the case where the file truly has no blocks, which actually isn't possible right now I believe, checking the filterShard doesn't seem to do anything sensible. Adding a filterShard somehow takes a inspect-file from Found to NotFound even though it doesn't actually filter any blocks. Which seems like a really weird behavior to me.\n. I was thinking it'd be:\ngo\nif fileInfo.FileType == pfs.FileType_FILE_TYPE_REGULAR && len(blockRefs) == 0 {\n    ...\n}\n. This could lead to restarting the rethink or etcd pod which I don't think tests what we want. You should do app=pachd instead.\n. Interesting, I think that suggests we have a different bug. It's sort of incidental that filterShard is always nil when we have an open commit isn't it? Here's a question, did this PR make it so that empty writes will result in no blocks being appended? Until recently empty writes would lead to a single empty block which was probably shielding us from this behavior before.\n. This seems like a kind of weird behavior to me that one container sees an empty version of the file. I was thinking that you'd only see the empty file if there were no blocks at all the show to any of the containers.\n. Nice breadcrumb for the user here.\n. Pachctl calls fuse functions directly when you do pachctl mount so I think it makes sense.\nWhat does PersistentPreRun do for us?\n. I generally like the standard of defining predicates in the same file as the errors themselves. This one could maybe be called IsPermissionError or something.\n. Cool, makes sense to me.\n. Do you still need to mount if you're using pachctl?\n. Why send this to a file instead of just letting it go to stdout and getting it with get-logs? Doing it with get-logs would allow you to get the values while it was running and you wouldn't need to remind people what it is here.\n. Yeah I think that's a good solution. We should silence fuse errors in the job-shim I think now that we have Transform.Debug to turn them on if we ever want them. I'll make an issue for that.\n. Whoa whoa whoa, no snake_case in go.\n. There's a function on cursors called One which reads out a single value, checks for errors and closes the cursor. Would make this function a little bit nicer.\n. Isn't this potentially racy if multiple nodes are trying to start commits on the same branch?\n. How does this work if there's already a commit with that clock?\n. Couldn't this use the NewClock function above?\n. Is this actually used somewhere? It seems kinda weird to define an empty clocks as being its own child.\n. This could be a potential race if 2 commits come in on the same branch at the same time couldn't it? In that case it seems like we'd wind up with 2 commits with the same branch clock wouldn't we?\n. We can't report it to the user though because we already have another error to report. I made it so we can log it which is the best we can do.\n. In the example from aws s3ap create-bucket help they have both LocationConstraint and --region set. Does it work with only the LocationConstrain?\n. time.Duration is just an int64 so I think you can just do time.Duration(0) to get the same effect. You might as well inline it rather than defining a function since zeroDuration is only a little bit shorter.\n. This should be sanitizeErr(err) I think.\n. copy paste error\n. I think you could just do this with c.ListCommit doesn't look like you're passing an arguments it doesn't support.\n. These comments are a bit out of date with the code. I'd remove them and expand to comment below to say something like: We break if the repo has Provenance because repos with provenance will already have been archived because archiving is transitive over provenance.\n. This should be pfs.CommitStatus_NORMAL. There's no need to archive cancelled commits since they already don't show up in ListFile and there's no reason to archive commits that are already archived.\n. This seems kind of weird to me. This doc is going to cover everything you need to know to deploy which is already a separate doc. I think it's probably better to just link to that doc.\nAlso how is this doc going to be different from getting_started.txt and beginner_tutorial.rst it seems like there's going to be a lot of overlap with those.\n. This document is probably going to need a section on Port Forwarding or using ADDRESS to get pachctl talking to the deployment.\n. This file probably shouldn't be committed right?\n. Great catch, I'm not sure what I was thinking here\n. Yup will do.\n. Small nitpick, could we call this pachd-binary. make grab-binary reads weirdly while make pachd-binary seems much nicer. In general I think make targets shouldn't include verbs since make is already the verb.\n. This should probably be in an else block right? Won't make docker-build-pachd emit a tar archive to stdout in this patch?\nAlso if you do \ncd _tmp\ntar cf - ${BINARY}\nthen when we untar it we'll have just a binary and no directory, seems a bit nicer to me.\n. This should go back to how it was in master.\n. Could we expand this a bit to cover what all of the parameters mean?\n. What's the point in obscuring this behind an interface and having getter methods instead of just make index public and having the functions below return one directly?\n. Could this just be a var instead of a function?\n. This is true but we should figure out if /foo and foo are the same file or different.\n. Do we even need this type anymore? Can't we just use pfsclient.APIServer? Also the comment is wrong now I think.\n. This file should get renamed to something that doesn't have refactor in it.\n. Too many spaces.\n. Was a problem before but could we fix this spacing too?\n. Why have them do a version commands that's guaranteed to fail?\n. This sentence seems really weird to me, we know that pachd isn't up yet so they won't find it when they do kubectl get all why are you having them do that?\n. This should be pachctl deploy now right?\n. Actually these are supposed to be temp docs for right now so probably not.\n. Nice informative comments :)\n. Could we call this CreateBranch? It seems like that matches with our other naming scheme and describes this rpc well.\n. Should these maybe be called SquashCommit and ReplayCommit I like having our commands be VerbNoun to give an idea of what they operate on.\n. This usage looks like it might be wrong. Don't you need to pass parent-commit with -p?\n. How attached to the \"s\" are you?\nOther requests, namely FlushCommit take multiple commits but use a singular which I think makes sense since they can take a single commit. It also just seems nicer to me to have singulars everywhere.\n. Would it make sense to relocate this to src/server/pfs/pretty?\n. This should always be true unless r.size is 0 right?\nAssuming that's true I think it'd be clearer to just do: if sizeLeft != 0\n. Makes sense, I like ForkCommit. The other option I thought about was BranchCommit but I think it's a bad idea to use Branch as both a noun and a verb in our api.\n. Makes sense, let's just leave it as is.\n. This would be pretty easy to change, go has a bunch of hash functions and they all implement the same interface. Looking at them though it seems like they're all smaller output sizes, 64 bit being the largest so that would increase the chances of collision. Also sha is what git uses, for much the same purpose we're using it for here, so I think it's a reasonable choice.\nInteresting SO post about this: http://stackoverflow.com/questions/28792784/why-does-git-use-a-cryptographic-hash-function\n. Pretty sure we still want this test, the issue with Null characters wasn't that our backend doesn't handle them. It's that anything written in C (like ls) gets tripped up because if we return a file called ihavea\\0characterinmyname it interprets that as just ihavea which the user then tries to read and gets a file not found error.\nWould you mind adding a comment to this test explaining the reasoning behind it so there's no confusion in the future?\n. This file name is so ugly it's almost art\n. I feel like this change was part of another bug fix that already got merged wasn't it?\n. Did that PR just get rolled into this one?\n. I think it's fine to just reuse the variable as a argument for both create pipeline and update pipeline. We do that with a bunch of other command arguments and it works fine because only one of the commands is run per invocation of the program.\n. I think this would be better named pipelineReader, config seems too generic to me.\n. Even better imo.\n. Why not just directly read that file then?\n. Setting the last parameter on ListCommit to true will make it block until there's a result. Probably easier than a loop.\n. Remember to remove printfs before committing.\n. This is a kind of weird way to test this since it isn't test the version of this file in the commit but instead the version in master. If a commit deleted this file it wouldn't fail on CI but would, once merged into master cause all other commits to fail CI.\n. Won't this now fail if you try to test a commit locally that hasn't been pushed to github?\nI feel like doing the simple thing and just reading the copy of this file that's in the local filesystem as part of the commit would be a lot simpler and less fragile. The only thing it wouldn't cover is if github changes the format of their URLs, but I think that's acceptable.\n. Yeah I think I'd still prefer using the local files. Fwiw we do have TestPutFileURL which tests the request for a PutFile with a URL. We just don't have a test that the CLI creates such a request properly. But I think we'll have that soon and decreasing the fragility seems worth it to me.\n. I agree on having company policy for this stuff. I'm going to put together a document on it. I'm not sure what the guidelines should be in this case though given that the file in question is prose. Will people be ok with having the guidelines on prose just be \"follow standard English style.\" In my mind that precludes things like trailing spaces and obviously none english things like that. But in terms of interesting English stylistic things I don't think there's much need to be prescriptive since we just won't run into them too much. Does that seem reasonable to people?\n. I don't think you need to do this with the goroutine, instead you can just do:\ngo\nerr := deploycmds.DeployCmd().Execute()\nrequire.NoError(t, err)\nrequire.NoError(t, w.Close())\ndecoder := json.NewDecoder(r)\n. Could we have this be a constant that's stored somewhere rather than a magic value.\nI feel like that would prevent us getting in the same situation as before where we changed the value in one place but not another.\nProbably locate it in src/server/pkg/deploy/deploy.go\n. Effective go mentions this so we are kind of saying it since this guide links to it. I think that's enough, does that feel good to you?\n. Oh lol, I actually copy pasted this one from k8s guide and didn't even read it >.<\n. Small nit here, this could just be called dbMetrics to match externalMetrics.\n. Little confused by this comment, #960 is the Azure Support issue so it seems like this PR is kinda dependent on itself. Wasn't there something about needing k8s 1.4 to do provide data disks to rethink/etcd? Is that what you're referencing here?\n. Yeah, that's a totally reasonable thing to do for the time being to get things testable.\n. One of the things I really liked about the previous metrics PR was that it moved us away from storing metrics in memory to storing them in a persistent way. This was nice because it means that nodes can restart without us losing metrics information. Why are we going back to storing things ephemerally here rather than just using the db for this?\n. You don't need to branch here, you can do this in 1 line with:\nuserActions[incrementAction.user][incrementAction.action] = userActions[incrementAction.user][incrementAction.action] + 1\nAlso this code has a bug in it, the first incrementAction will leave the value at 0 rather than 1, so the counter will always be 1 lower than it should be.\n. In general stuff like this is good use case for channels there are a couple things wrong with the way you're using them here.\n- Performance. The above for loops blocks to send metrics to segment twice via r.reportClusterMetrics() and r.reportUserMetrics() this could easily take a couple of seconds. During this time calls to IncrementUserAction will block and it's totally reasonable that that function could be called as part of every single request. So this code will probably wind up adding seconds to the worst case of every single request we have, a pretty huge performance hit.\n- You're over using channels here. You really only need the incrementAction and the ticker channel. Everything else seems like over channeling to me.\n. What happens if the user isn't new? Is there another function to use?\n. Why have this be a separate function? It doesn't seem like there's much need for reporting 1 set of metrics and not the other.\n. I'm a bit confused, is there an identifyExistingUser or something that I'm not seeing somewhere? This function isn't called anywhere so it's a little tricky to understand how it's going to be used. How does one figure out if they have a new user or one that segment already knows about?\n. I still don't think I understand why wanting to reset the counts requires a channel.\n. UserClient doesn't really suggest to me that it's a client that reports metrics. Could we call it MetricsClient instead?\n. It'd be nice to put a printf in here as well telling the user where the config file has been written so they can find / modify it.\n. Can we abstract this into a function? I'm really not excited about having 10 lines of boilerplate at the top of every method.\n. Why did make this global? I'm generally very against having global structures since they make a number of things more difficult down the line. Is it really necessary here?\n. So doesn't this mean that this method reports metrics and there's no way for the user to turn them off? And isn't this true for every other method as well except version and deploy?\n. Neither of those strike me as good reasons to make Reporter global. You can still check if metrics are enabled and re-use the segment client, you just have to explicitly pass the Reporter instance to the pieces of code that are using it. That should only be the 2 api_servers both of which are instantiated in pachd's main function so it doesn't seem like it'd be that big of a change.\nI'm a bit confused about what you mean by \"both of those metrics flags.\" What are the 2 flags?\n. Why is pods repeated here? I would expect Chunks to be assigned to 1 and only 1 pod.\nWhatever the answer it'd probably be good to include it as a comment on this field the clarify.\n. Why don't you just use InspectJob with BlockState: True to wait for the jobs states to be set? That would be less fragile.\n. Is there actually any pod killing that happens as part of this test? There doesn't appear to be. Or by manual did you mean we actually need to ssh into the machines to kill the pods whenever this test is running?\n. Manager isn't a good name. Even though this will read as lease.Manager when it's imported. There are entire posts written about how bad SomethingManager is as a name. I think this could just be called Leaser that would match with the go convention an interface whose main method is Foo be called Fooer admittedly this will then read as lease.Leaser which is redundant. But I think that's fine (and it's shorter than lease.Manager by a whole character so clearly a win).\n. Realized this is documented below, I think you should just copy paste that comment up here.\n. Do you think it might make sense to call this StartPod now? It's always struck me as a little weird that jobs would get started multiple times. Now that pods are an explicit part of the API referencing them in this message name seems like a reasonable thing to do.\n. This is basically the maximum number of retries for a chunk right?\nIf so it may make sense to have that be a configurable part of the Job/Pipeline Spec.\nThis isn't strictly necessary for merging this code though. \n. This could use a more descriptive name and maybe a comment about what this function does.\n. Nice thorough error checking here but you I believe you can get the same effect with:\n_, err := os.Stdout.Write(manifest.Bytes())\nreturn err\nGo specifies in the semantics of io.Writer that:\n\nWrite must return a non-nil error if it returns n < len(p).\n\nhttps://golang.org/pkg/io/#Writer\n. Yeah, otherwise we're going to need to setup CI to send you a slack message to go in and do your thing. I think there's examples in the tests of killing pachd pods that you can copy paste.\n. Does it make sense to have this filtering be part of the ListJobRequest so that it can be done efficiently on the server?\n. Sounds reasonable.\n. Will this get files that are in sub directories?\n. You should use an errgroup to fetch all the files in parallel. Should pay huge performance dividends.\n. Was there a reason you choose to use a buffered writer? Files normally have their own in memory buffering that's handled by the OS so we're probably better off just using that directly.\n. You're swallowing errors from f.Close() here. You can report them with a closure. Look at this for an example.\n. Like above, this should use an error group to do things in parallel.\n. Like above, don't swallow this error.\n. I strikes me as a little weird that you're putting the job-shim under a directory called /pfs given that it really has more to do with pps.\n. What a beautiful file diff.\n. Any reason not to use 16.04 here and in the other Dockerfiles?\n. Do you think it'd make sense to expose this in pachctl so users can get data to and from local disk? Not as part of this PR though since that would expand to scope. Also I would have called this method Checkout that seems like the closest analogy in git to me but it's not perfect since git's checkout doesn't involve going over the network,\n. This is a file descriptor leak waiting to happen. If this function exits early then the file won't get closed. You need to defer the close. It should look like this:\ngo\ndefer func() {\n  if err := f.Close(); err != nil && retErr == nil {\n    retErr = err\n  }\n}()\n. Same as above.\n. Could change /pfs to just a more general /pachyderm here?\n. Actually yeah, clone is pretty clearly the best analogy here.\n. It lets you recursively put the contents of an s3 directory. So if you have objects dir/foo and dir/bar in s3 then you can do:\npachctl put-file repo branch -r -f s3://dir\nand it will put both dir/foo and dir/bar\n. It's not useful, in fact it just errors. I think that's reasonable though, the alternative would be to make it detect that it's a directory and automatically do the recursive traversal. That's a little too magical imo though, recursive puts are a different and expensive enough thing that users should have to ask for them explicitly. This is the way that most unix tools work too. For example cp dir somewhere will error unless you add the -r flag to do a recursive cp.\n. Might be a good idea to acquire a mutex while this function runs because it's modifying os.Args which is global. 2 concurrent copies of this function running could produce some really confusing results.\n. What's \"final\" about these metrics as opposed to just being normal metrics?\n. Yeah, your understanding is correct it won't run in parallel as is. This is just a defensive thing in case someone comes along and makes the tests parallel.\n. Superficial but I would write the above 2 lines as:\ngo\nfor <-tick {. This seems like a potential bug, os.Exit will cause the program to exit immediately bypassing all deferred function calls. There's 1 later in this function that unmounts the mountpoint which won't run if we hit this line. I think that could potentially cause the pod to not clean up correctly, I've seen leaked fuse mounts cause some problems like that.. This line seems pretty dubious to me. I'm pretty sure this will encounter a bounds error won't it? This example hits a bounds error and seems pretty equivalent: https://play.golang.org/p/5WxLwi0VeO On the other hand CI is passing, which gives me pause since it suggests that maybe this code isn't being run? How big is tables here? The bounds error won't manifest is len(tables) <= 1 but I'm pretty sure it's bigger than that isn't it? I could just be wrong about this being a bounds error... but like I said my example seems pretty identical.\nThe problem here is that you're trying to wrap the creation of all the tables in a single retry loop which means you need to keep track of what you accomplished in previous attempts. Instead just wrap each TableCreate in its own retry loop. This should be the same amount of code just moved around a little and no need for list splicing.. Last little tweak, this should log using lion instead of fmt to match the rest of our code base. Also it'd be good to describe where the error is coming from, ie Error creating table. Also using %s and err.Error() instead of %#v might be a good idea but those might also be equivalent. I'm not quite sure.. This usage string is wrong now isn't it since you can't do a repo-name without a -i before it right? I think rather than changing it to match the behavior it might make more sense to change the behavior. What if we made it so that positional arguments are interpreted as include commits and you only need a flag for exclusion? This is how a number of git commands work. Also I think -x for exclude might be more intuitive than -e what do you think?. The control flow in this function seems a little bit weird / confusing to me. What would you think of doing something like:\ngo\nfor i, commit := range append(include, exclude...) {\n  // do stuff for all commits\n  if i < len(include) {\n    // do stuff for include commits \n  } else {\n    // do stuff for exclude commits\n  }\n}\nthat seems a lot simpler to me.. It seems a little weird that we log this above and returns this as an error, which will presumably lead to it getting logged a second time by the calling code. It seems like if we get an explicit restart then we'll log it twice but if the call to ContinuePod errors we'll log it once. Seems a little weird. Also the messages are almost the same but slightly different so if we do keep both they should at least be totally the same.. I think it's probably better to leave this out of the main docker-build target since that's people's main development loop and netcat isn't strictly necessary for development. To get it to work on CI you could just call docker-build-netcat directly.. I'd put a comment here that the presence of this field changes the behavior of jobs. Small nit, would you be opposed to just calling this jobOptions? I think \"get\" in function names is often superfluous, if you're reading code and see jobOptions(...) you generally assume that evaluating that expression is going to get you jobOptions. Also this will make it match the functions around it which are job and service not getJob and getService.. That's even better.. Ahh ofc, in that case you could call it newJobOptions since it's a constructor... but considering this method isn't even exported publicly I think you should just ignore this, getJobOptions seems fine, it gets the point across.. Did you mean to include these logging statements? If so could they be lion instead of fmt?. I think we only want to delete the repo if the job was a singleton and not run as part of a pipeline (ie if jobInfo.PipelineName == \"\").. Actually on that note could you also add a test where we delete a job that's part of a pipeline to make sure it doesn't mess anything up?. This code looks so much prettier now :). Good point, I think if you delete a job from a pipeline that's already run then nothing will happen immediately because runPipeline will have already seen the relevant commits and won't be listening for them anymore. However, if runPipeline were to get restarted for this pipeline (due to a node failure, or a pipeline update) then it would recreate the job when it received those commits. Not particularly sane behavior. What would you think of just erroring if the user attempts to delete such a job and saying something like: \"job foo can't be delete because it's part of pipeline bar, delete pipeline bar instead\"?. I generally like to do these as single statements, ie:\nif err := assets.WriteLocalAssets(manifest, opts, hostPath); err != nil {\n    return err\n}\nsaves that precious vertical space.. Minor knit, there's a semi formal standard in go that error messages should begin with lowercase letters and not end in periods. The reason being that they're normally printed embedded in other sentences so it looks a little weird.. Same for the ones below.. Reminder to remove these Printfs before merging.. This seems only partially implemented was this supposed to be included?. This was actually a latent bug that I encountered working on this and decided to fix. We weren't actually checking the error that gets returned above.. I'm not sure we need an exercises section for people here. I don't imagine people are going to be like doing problem sets from our design docs.. Is bento/ubuntu-16.04 a standard place to get an ubuntu vm from?. Good enough for me, I don't really use vagrant for dev.. Cool. What was it that lead you to the vagrant environment? We should probably correct that to nudge people toward the docker flow. Or was it just the existence of the vagrant file?. I'd prefer to keep using pkgcobra for this, any particular reason you chose to drop it?. Good thinking, I actually just tested this and it turns out statefulsets are counted as part of all. Which might seem obvious but given that jobs aren't included as part of all it was worth testing.. No need for this, you can just call sort.Strings on a []string to sort it.. Was this part of the changes? It looks like it might have gotten included from a merge or something weird. I think I made this change about a week ago to fix the jobs not getting removed correctly. I'm a little unsure why it got included in the diff though since the new version of the code seems to be what's in master. We should check on this before we merge just so we don't get bitten by any weird merge issues.. Yeah I think that makes sense, a few questions:\n\n\nWhat do you think is the right behavior if multiple keys are set? The two options I see are error or use the \"first\" value where first would just be defined by which order we check them in? I think the latter might be simpler and is unlikely to cause many problems in practice since I don't think people will be having duplicate entries and if they do at least this will be deterministic behavior so hopefully not too confusing.\n\n\nAny reason that I shouldn't migrate the ParallelismSpec code over to this method as well? Seems like it shouldn't be too hard and is basically backward compatible with existing pipelines.. Did you mean to leave this in? It doesn't seem like a particularly important thing to inform the user of, you'll basically just see this once on startup. Also log messages should use lion right now to match the rest of our code.. Small nit, I'd call this secure, no need for the \"is\". Another very small nit, I'd call this recursive not isRecursive. I think this is pretty unlikely to happen because the way runPipeline is implemented it's pretty hard for there to be 2 concurrent CreateJobs running for the same pipeline. Also because this is temporary and I don't want to spend too much more time on it.. Minor knit, don't think this should be plural since it's just a single semaphore.. Changing this signature seems like it's likely to break some existing user code. Could we use default here and have a different method for creating one with a non default number of streams?. Minor stylistic thing, I generally like to make these into 1 liners to conserve vertical space.\nGofmt doesn't mind it either.. Why does MaxMsgSize need to be bigger than a block?. >.< Pretty sure I wrote this initially.. I don't understand. Even in the system without that buffering change you could send a block as multiple small messages or you could send multiple blocks as a single large message. The 2 things seem completely decoupled to me. MaxMsgSize should affect how we break things up in streaming calls but it doesn't seem like it should be related to the size we use for blocks.. I see, would you mind terrible just decoupling them within this PR? It seems like it should be a pretty quick fix.\n\n\nAlso I dug into the grpc code for MaxMessageSize a bit and it seems like the MaxMsgSize only applies to incoming messages, not outgoing ones. Don't think that changes anything about this PR, just fyi.. This is a really unwieldy / unsafe way to test this. If there are other tests that use os.Args that run in parallel it'll crash. You should just call the function you want to test directly.. This is insane. Your shelling this test out to run itself again (which will cause the test to be recompiled) to check if a function returns an error when it's passed a string with a / in it.. Is this a particularly common error case with aws credentials? . In benchmarks I got the best results with a chunk size of 10 MB, we cut this value in half to account for extra overhead in the message so this change puts us on the observed sweet spot for best performance.. Hmm, not sure I follow, are you saying that the signature would be:\nTagObject(tags ...string)\nIt seems like that would be a weird signature since you could call it in ways that didn't really make sense, ie with a single tag.\nImplementation wise I still kind of like distinguishing tags and hashes from each other since they come from different places and I can imagine a number of weird bugs that could happen if we accidentally overwrote a hash with a tag.. There is not, and actually compaction doesn't really work with the caching layer at all here because it assumes immutability and to do compaction you need to move things around. I think I might rip out the compaction code before I merge this actually. However, I was thinking an optimization which would actually work is to make it so that you can write multiple objects in a single streaming request and on the back end those could easily be compacted into a single block. It seems like that would be pretty good for the case where there are several small files in the same commit. However in the interest of getting this merged into master so you guys can use it for the new pps stuff I think it's probably better if I do that as a separate PR.. I was able to get about 37 MB/s of upload speed, 40 MB/s download when the cache was empty and 120 MB/s down when the cache was already loaded. All this was from multiple clients running on a single pod.. I think it might be good to have them there even though they're lower level. They came in handy for debugging stuff and I could definitely see wanting to have a user debug something with their pachctl, that's the only use case I can think of though.. I see, I think abstracting those functions out would actually be a more robust way to do this. I didn't realize this was recommended from Andrew Gerrand's talk. I guess if we absolutely need to test that a process crashes this could be a reasonable thing to do, in this case I think we don't strictly speaking need to test at that level though.. I don't think you need to be serializing this, the whole point of lion is that it should be able to accept your log structures directly and do the serializing itself.. Actually you should just use the rpc logging method that's even better. FilePath and ObjPath fit nicely in that protobuf as the File field and the URL field. This way a recursive request will produce all the log messages that the individual requests would have produced.. Yup, we already use it for our pretty printing.. Sleeping for stuff like this always makes me a bit nervous. I actually made the check_pachd_ready.sh script generic in another PR that was never merged. You could cherry pick the commit: 98958587bc3082a8e3b0c227ee60a5c90d2dee80 and it should allow you to use the script instead of the sleep.. Yeah, if you just cherry-pick it into this PR that should work well.. I was about to comment on this as well.. Yes, which is arguably pretty weird. I think we'll just not use split through fuse.. This should be RLock and RUnlock. We should probably have some sort of log message about the fact that we're omitting this file.. 1. I don't think makes sense, the Mutex isn't meant to be accessible publicly, it's just meant to be used to protect access to the pipes field in methods.\n\n\nDefinitely, will do.\n\n\nThe space argument makes sense, I'm inclined to leave it though since it does make setting values a little more verbose and we also use. Sure, I was wondering about that.. Good call.. Hmm, is it? This is the issue sean saw where a user had data in s3 that included a file with a / suffix from a weird sync to s3 and it caused conflicts.. Freed up a pair of parentheses and an equals sign for use elsewhere in the cluster.. I can, I'll have to rejigger how it gets instantiated a bit but shouldn't be too hard.. In that case you'd get 2 records, there's a 1-to-1 correspondence between records and put-file requests.. Sure, will do.. Not even close to defensive enough, the good news though is that in order for  this to happen it requires a user not just writing uuid values (without dashes) to the top level of their bucket but writing the correct uuid value, so they need to win the uuid lottery. And if this does happen then the server will just restart and role a new uuid.... if someone has written all the uuids to their bucket I expect pachyderm to have some issues though.. How would you feel about not having the preservePrefix argument and just implementing only the false behavior. Callers can preserve whatever prefix they'd like by passing that prefix as part of root, seems a little bit cleaner to me to only have 1 way of doing it.. They're so that the lines below have some log messages to find. This might not be necessary if we add back in some of the default log statements.. I'm not sure what's the most useful actually, if you're getting logs from a particular job and a particular datum then raw totally makes sense but if you're getting logs just from a job then it might be handy to see which datum each log message came from.. What does using SliceStable here get us? It seems like maintaining the order of the inputs shouldn't matter if all of the names are unique which is a requirement right?. \n. It is but it's used in an implicitly cooperative way.\nThe goro that calls CleanUp is the only goro that writes to it, setting it to true.\nThat same goro then opens all of the pipes which unblocks the other goros which notice that we're cleaning up and exit. I'm pretty sure that this behavior is entirely deterministic.. This would be tricky because that COMMIT_ID doesn't exist until after the job has finished and the commit can be created.. You are right observing this, although it's not a super strong preference. Basically at some point Peter started doing this and I kept doing it for consistency sake which I feel more strongly about than about which option we pick. There are a few things I don't like about plurals, they're irregular which can make it hard to grep for them, worker_status happens to be a substring of worker_statuses but the same is not true of datum and data for example.. I'm actually not sure I understand how that makes adding fields easier. It seems like the major concern would be cross version compatibility but if I understand protobufs correctly then it'd be totally possible to deserialize a serialized google.protobuf.Empty as a RestartDatumResponse if they were both messages without any fields. So it seems like just introducing RestartDatumResponse the first time we want to add a field to it would work about as well.. It would have, I can check to make sure that the started time has changed.. So after trying to do this I'm not quite sure it makes things nicer, there's a couple of things that make it annoying.\n\n\ncancel isn't part of the protobuf struct, since it's a function, so we still need that one\n\ndata isn't only used in status, it's also used to download inputs, furthermore the protobuf uses Datums not FileInfos so this actually gives us 2 fields called data that need to be maintained.\nwe can't return the WorkerStatus object directly from the Status method because it contains pointers which might get modified before serialization happens. So we need to do a deep-copy before returning.\n\nAll in all it's feels like it's not quite worth it to me.. It could, the only reason I passed it through is because I'm imagining that we'll want to make it a configurable part of pipelines in the near term future.. Hmm, that makes this pretty tough then. Is there are way to make a worker pool without running these goroutines?. Added.. Done. Good point, this wasn't true before when Cancel and Status were methods on workerPool but it is now. I'll remove it.. Hardest lyfe. Should we maybe call this CancelJob or FailJob, to me StopJob makes it sound like the job could be restarted, which is how it works with StopPipeline. This is minor but it seems a little ugly to print the CTRL-C message in one of the messages and not the others. Why not just print 1 single message for all of them? It's not like the timing of the messages actually correspond to when the ports become available.. This seems like it might get us in an awkward position if we want to ship a breaking change of the web-ui because as soon as we do then anyone with an out of date pachctl will be spinning up a broken version of the web-ui. How would you feel about just using the hardcoded value for now and users can override it with the flag if they want to get a more advanced version. Seems like the simpler approach for now.. This really doesn't seem that much more convenient to me than just passing an argument to pachctl. It also seems like it could be really confusing if you're trying to deploy a new version of the UI and accidentally don't because our site was down that second. It just seems like it's creating a weird coupling between our release / deployment process and our website that doesn't need to be there. If we really want this behavior we could in theory get it by doing --dash-image $(curl https://pachyderm.io/versions/...) then at least if there's an error it'll be obvious what's going on.. Kinda minor but I would have expected this to be resource_spec since the field of type ParallelismSpec is called parallelism_spec how do you feel about that? I also am not totally sure having users type spec everywhere is especially good, but consistency seems more important in this case.. Is there any worry that an IP address will be reused by k8s and that will lead to some weird effects since we use that as the key in etcd?. I think you could remove the If true part of the help message here, people will assume that the description applies to the case where the value is true. Also because users are unlikely to type --dashboard=true instead of --dashboard, although I think that both will work with this flag library.. We do, but that might yield a slightly different answer if the size of the cluster changes. Ie suppose we have a coefficient of 1 and 5 nodes, then we'll create 5 workers. Then the cluster gets resized to 4 nodes but we'll still have 5 workers. This is a weird case in general that we might want to handle better but doing it this way ensures that we're using the actual number of worker that exists in the cluster.. Yes, that requires DNS, which I don't think we can count on all clusters having. We normally use env vars but that won't work here because they can't get injected after the pod is created.. I set it below only after the value has actually been set in etcd. This means that if for some reason a value fails to set we'll try again on the next one. I think this makes sense, although maybe it doesn't because if we get a failure from etcd it might be because it's under too much load and this would just overwhelm it.. Will do.. Yes, good call.. Actually just talked with @msteffen about this and he's wrapping those options up into some nice convenience methods so I'm just going to merge as is and then once those methods are merged should be pretty trivial to add them to my code. Gonna leave a note on the issue you opened to remind myself.. Yes... I'm a fool >.<. Fixed now.. Hmm, I don't think it's necessary here because that's guaranteed to be called after the pool is no longer in use. We'd have some other big problems if that wasn't the case since we'd be closing connections that had just been returned and might be in use.. \n. It'd be cool if doFullMode and doPFSMode were a bit more composable / duplicated less code. It seems like they both have in common that they both instantiate some servers and ultimately register them as part of grpcutil.Serve. What if you made a function like PFSServers(appEnvObj interface{}) func(s *grpc.Server) that returns a function which registers the servers it instantiated. It seems like that way you could pretty easily compose whatever behavior you want.. The name sidecar doesn't seem super descriptive to me, could we just call it pachd throughout? That seems more descriptive to me.. I agree that having users see a bunch of pachd containers would probably be a bit weird. sidecar just seems a little too generic for me, it really just tells you what type of container this is but not much about it's purpose. It also might not be the only sidecar we ever have on the worker Would you be opposed to pfs-sidecar or something like that?. I think it's fairly reasonable to panic here, just because that's the behavior with slices and other datum factories if you access out of bounds. The other option would be to return nil or an error which seems more complicated.. Yeah, I agree, will change.. Drive by commenting is always welcome :D, currently it does not support partial GPUs and it's unclear if that's in the cards, I didn't see any indication that it was.. When people use inputs they get translated into the input field so no matter what input should reflect their pipeline and give them an accurate impression of what's going on.\nAlthough I think I should change the word Inputs to Input here.. What about something like Repo1 X (Repo2 U Repo3) it seems like that would be a pretty readable shorthand.. I considered doing it that way, this seemed slightly easier just because there wouldn't need to be an extra docker image that got built on all of our machines and could potentially get cached which would be kinda weird. I think they're pretty equivalent complexity wise though.. Oh nice, this didn't even require an init function.. I'd prefer to have this in terms of the MaxMsgSize rather than implicitly linked to it. If we were to lower that constant to below this value we would start getting crashes. Actually how would you feel about moving this code into the grpcutil package? It seems like it's pretty intrinsically linked to it since the buffers are intended specifically for grpc copying.. If we're going to recommend that people pass an empty arg for this I think we should just make it a flag for now.. Any reason you're using a different logging package here than we use in most other places in the repo? I'd prefer to keep things unified if possible.. Minor, but I feel like you could just inline this check rather than having a separate function.. It seems a little weird to me that we swallow no retryable errors here, I'm guessing we'll recover most of them when resp.StatusCode >= 300 but I'm worried there might be a few that don't get reported here and don't get caught below either. You can handle stuff like this the Notify function below which I think might be a bit more idiomatic. I.e. in your Notify function you'd check if the error is isNetRetryable if it is you return nil, otherwise you return the error. Seems like a slightly safer way to handle the errors.. That seems reasonable to me.. Looks like this didn't get changed, I actually don't really feel that strongly about it. sidecar seems fine now. Looks like there's still an empty string argument for the cloud formation ID there.. Same here.. It seems like there's probably a matching call to create the workers that might error now because the workers already exist. Is that true and if so do we trap that error in some way?. Yes, good catch. Thanks for the review @ferrouswheel. This was my favorite code though :(. Not sure you meant to commit this.. Yes, I do. We use a singular for this in the other methods in this file.. Also, why not just have the prefix be part of the ListObjectsReqest? if it's empty then you just return everything which is what you'd expect from a prefix field.. Nvm, these are actually substantially different methods, one's for listing with objects the other's for listing with tags. Could we just call the second one ListTag?. Doing this one object at a time might prove to be kind of inefficient, no need to fix this for this pr, just something to keep in mind.. That seems fine to me, the InspectTag method below returns an ObjectInfo it's sort of in keeping with the idea that Tags are just aliases for Objects. Sure, I'm just thinking about the latency of sending each object one at a time over the streaming API. We should either be using it everywhere or using it nowhere. I'm only using it here because it's what's used in the rest of the fuse package so I need to pass the SetLevel call to it for it to do anything.. Why not just use Empty for these? I don't think you get any backward compatibility benefits by creating your own empty types.. I think we should use the more verbose name garbage-collect for this. Also we may want to consider calling it compact instead because it's a little more broad and I think it may be good to also pack smaller objects into single s3 objects as part of this process.. Is this actually used anywhere? I don't see a place where we read from it.. I see, for some reason github elided that diff.. So right now there's no way to cancel this goro? That's probably ok given that this is normally supposed to run for the life of the problem.. Yup, I generally like to have the public stuff at the top of the file so it's what you start reading.. This seems pretty fragile, are there really errors we need tocatch here that aren't caught by s.objectClient.IsNotExist(err)? Also it seems a bit weird that we're alos using IsIgnorable below.. It looks like we're calling strings.Join on a single string here. Is that correct?. I got 99 programs and a watch is one.. Are you aware that the splits in keys are used to determine how the key gets routed? That might play kind of weirdly with this code.. Oh, so it is. I was thinking it was splitKey[len(splitKey)-1:] which would have always been a singleton... but obviously it's not that.. I see, nope that should work fine.. Probably need to call make doc again and get rid of this file.. This warning message is a little inaccurate, normally a feature is called \"deprecate\" when it still works but isn't the recommended way to do something. It'd be more accurate to say: \"inputs is deprecated and will be removed in v1.6...\". Like a summary of the whole file? At the top it says:\ngo\n// Package sync provides utility functions similar to `git pull/push` for PFS\nthat seems like a pretty good high level summary although I could expand on it a little more.. Ahh, I see. What changed is that I added a PullDiff method which pulls a different view of the filesystem based on the diff rather than the full content of the directory. Also as part of this I refactored some shared logic between Pull and PullDiff into helper functions.. Added.. Why not just make this a required value in appEnv?. Is it possible for Input.Repo to be nil in this PR? That doesn't seem like it should be a valid input.. Does SubscribeCommit ever return io.EOF? I thought the point of that command was to block forever which means that this command will block forever, and in the case of where raw=false it won't return anything at all since it'll never call Flush. Is this correct? It's been a while since I touched this code buy I seem to remember that the bucket gets passed directly to the objClient so I think by Joining url.Host we're adding the bucket name to the front of the object we're requesting which means we won't find the object.. Am I right in thinking that this is a goroutine which will never go away, which means that the Pool can never be garbage collected? If so that seems like a bit of a problem.. So it seems like this pool still doesn't solve one of the bigger issues we had with the old pool which is that it would return duplicate connections to you and you'd have to figure out when you had a duplicate connection using the busy signal. It seems like that wouldn't be too hard to solve if you kept track of the ips that various connections came from and then could easily tell which IPs you hand't already connected to.. It seems pretty unlikely that we're going to have a service that exposes 2 ports both of which are serving grpc and both of which we want to connect to using a pool. I'd say we should just assume there's only 1 port here and use that.. It seems kinda weird to make this a separate package considering it's so intimately tied to pps.. This is another case where we're using channels and concurrency and I'm not at all clear why. Am I correct in thinking that this code works approximately like so:\n\ncreate a branchSetFactory\nfor each set of branches it produces spawn a job\nrun that job until completion\nrepeat\n\nIf so it doesn't seem like there should be any need for multiple goroutines or channels, all you need to do is iterate the results from the branchSetFactory and run the jobManager for each of them.. Ok but doesn't that mean that this command is effectively useless without --raw since it won't ever flush the tabwriter?. This looks unrelated, was this a latent bug?. Doesn't this mean that we could delete a repo that another pipeline is waiting on for changes? That seems like it might be a problem.. Ahh, I see so, it does call Flush but you can only call Flush on a TabWriter once which should happen after the last call to Write. There's an inherent problem here which is that there is no last call to Write when there's a never ending stream of things to Write.. Hmm, if subset.Ports has more than 1 port in it won't we use more than 1 port?. I see, that seems to make sense.. I think you need to call writer.Init in here too otherwise it will use the alignment of the first call to Flush for every subsequent print.. What's the reason for this, it might be good to give a little bit more explanation here than just \"technical reasons\".. If it's not too much trouble I'd like to see this with an API more similar sync.Mutex with a Lock and Unlock method. Actually I'm wondering if maybe it'd be simpler to just define an interface with Lock and Unlock that concurrency.Mutex satisfies and return it directly.. I actually think it would make a lot of sense to relocate server/pkg/worker into server/pps/server seeing as server/pps/server is supposed to be the spot for all servers needed to run pps which is what worker is. It would probably lead to a decent amount of code shared between the 2 packages being made private which would be a nice benefit.. This isn't strictly necessary for this PR to be completed though.. I am so happy that this file is now called branch_set.go instead of commit_set.go. Could we name this something to indicate that it's doing provenance matching? considerEachSet seems a bit too generic to me because it doesn't tell me what it's actually considering about them.. Purely in the interest of not having too many words for the same concept, how would you feel about calling this trueInputs that's what we'd been using before.. Similar to above how would you feel about rawInputs for this?. Yeah I guess root and direct are more descriptive names than raw and true. . This is pretty minor but it seems a little weird to me that one of these methods takes a ctx while the other one doesn't.. These should probably be removed.. We already have a log statement for finishing user code execution, we should probably delete the other one since this has more information. I'm not sure there's much value in logging the environment though, that doesn't really change datum to datum.. No need to log the inputs here, the logger already has inputs present in a structured way.. Same here, no need for logging inputs, they're already present in the LogMessage. We already have a \"Received request\" log message that's going to have almost all of this information in it. Also why are you logging response err and duration when they're always going to be nil here? There's basically no new information in this call compared to the \"Received request\" log below, all the information in the request is already part of the log lines.. So we're logging the beginning and end of uploading output twice now... why?. I'm a bit dubious about logging things for read only calls. Especially since you're using an empty Logger here so these logs won't be recoverable by any of the methods implemented by get-logs.. Same here as with Status.. We already have this information available in jobInfo.Restart. More redundant logging here, this has exactly the same info as on line 270 with an extra copy of the retry count that's already present in JobInfo.. Nice catch.. We don't need a log line to tell us we update progress... we'll be able to see that by checking progress.. I think almost all the logging you added in this file is redundant and not particularly helpful, it's almost all available via things like inspect-job and status. The one exception might be egress which I think we might want a job state for. Other than that though I don't think we should merge the logging in this files, it just makes our log files messier and bigger.. I'm not sure they really need to see if it's Cancelled either, since presumably they were the ones who Cancelled it, it seems like there's pretty rare cases when that log line is going to tell them something they don't already know.. It doesn't get logged, although we log when the job gets restarted, so I think just adding the restart count in that log line would work well. You have the JobInfo available to you in that scope too so it should be pretty easy.. I like how unified the logging in this file is now.. Why are we adding these targets? Our Makefile already tags and pushes images doesn't it? I'd prefer not to add a second way to do that if it's possible to use / extend the existing way. Also both of these commands are using pachctl but don't list install the target that installs pachctl as a dependency.. I think you could remove this flag completely. Migrating to a different version from what pachctl is doesn't seem like it'd ever be useful, they'd need to reinstall pachctl to deploy a version that matches.. I see, could we make push-bench-images depends on these targets then rather then duplicating the logic? Also they should depend on install otherwise there's no guarantee they're using an up-to-date pachctl which seems to be the whole point.. I can buy this. Although I still can't imagine a use case where you would specify --to. One thing that this doesn't address is how we'd migrate data that's stored in the object store rather than etcd. That's fine for now since we don't need to that for this migration version but I'd be interested in hearing what you think that would look like.. Was this done by manually editing the pps.proto file? If so won't this change disappear if you do make proto. I see, makes sense.. Good call, done.. Shouldn't this really check if Strategy == \"CONSTANT\" && Constant == 0? Joey hit this error because he was unwittingly setting Strategy == \"COEFFICIENT\" but only setting Constant to a nonzero number.. I actually like @derekchiang original wording better. You could just take the \"Due to technical reasons\" if you find that to be too tongue in cheek.. I don't really see how derek's is incorrect. Will pipelines that are in failure state automatically recover and get out of failure state? Or is there some action the user needs to take?. You probably need to save the Env value here and reset it after  you Marshal otherwise when this function returns Env will be nil and the Env vars won't get passed at all.. I see, would you be opposed to doing it anyway? Even though it's not necessary now I feel like it's possible that at a later point someone could use HashDatum and not realize that it's modifying the PipelineInfo it gets passed and wind up with a pretty confusing bug. Alternatively I guess we could just leave a comment, but since it doesn't require much code to actually fix eliminate the issue that seems like the easiest path to me.. Isn't this going to fail if someone happens to have a branch that's the same length as a UUID w/o dashes?. This should be cache_size right?. Will comment, tree is an object which the Pulled data gets mirrored into. treeRoot is the path to use as the root for that mirror.. Hash is used so that we can have an ID for each datum, this is used in logs and also as the path in the stats output so we don't have to do something like _path_to_datum_a+path_to_datum_b. I'll add a comment about this as well.. Yup, that's correct.. Yes, this allows us to do pachtctl get-logs --job ... --master to see what's going on with the master. Works even if the master has switched between pods.. As discussed in person, it's needed only for migration.. Hmm, that's a good point we could not include it in the hash (which we're doing on line 492 above) however we still want to include the HashPipelineName prefix because that's used by garbage collection.. Probably want to remove these printfs.. How would you feel about just embedding the router object in HTTPServer so the caller of NewHTTPServer can just pass it directly to ListenAndServe?. I think we should just use 80 for this.. At the top of this function there's a goro that runs ListenAndServe so that the tracing tools will work, I think we don't need those now that we've got this server going. Would you mind confirm that's the case and if so removing it?. Probably easier to just inline 0s instead of having to explicitly specify offsetBytes and sizeBytes are int64s. This is the only section that actually changed, the other stuff is just formatting.. This point is minor but still hasn't been addressed.. Might as well just delete the line. It's easy enough to recover if we want it back.. It seems like an easier way to implement this function would be to just wrap the io.Reader in type that doesn't implement the WriteTo method but implements Read as a pass through to src.Read then you wouldn't need any extra code.. The other requests in this file use wrapper structs for identifiers, i.e. rather than string job_id = 2; they use Job job = 2; would you mind doing it that way for consistency. You'll need to create a Datum message too.. Why not split the time up a bit more here between D/L, U/L and process time? With only 3 fields there should be plenty of space.. I doesn't actually implement the logic (as you've no doubt seen) I added this flag before I split off the other branch to implement user space batching and figured it wasn't really worth rewriting git history to get rid of it.. I like your idea so I'll just do that, and add a few comments as well.. Actually we can do something even better which is make this number go away all together.. This one we'll make into a constant though.. Comments added.. It seems to actually return NotActivatedError so either the comment or the implementation should be changed.. This will run for 15 minutes if it doesn't find the pipeline state which is probably longer than is ever going to be needed. Why don't we set it to something like a minute?. Same here.. And here.. This is making sure we don't error when the process doesn't exist, which is what happens the first time you call port-forward. I don't think there's much reason to log that error since you'll always get it the first time you run port-forward and there's note much the user would do in response.. Seems reasonable.. I really think you should put more info here given that the info is interesting and it doesn't seem like we've got enough columns here to fill the horizontal width of most people's terminals. I feel like we might as well use the space if it's there.. Is this function doing anything besides stripping out extraneous information like metrics? If so what? It seems like the metadata library distinguishes between incoming and outgoing messages but I can't quite understand to what end it does this.. One minor point I just realized, listDatum and inspectDatum should probably both take the --raw flag to just print the datums as json.. I don't think you want to use statsBranch.Head here since that means that the result of ListDatum will change as new jobs run. Instead just use jobInfo.StatsCommit and if that's nil you know there's no stats output. This should also allow you to throw out several lines above.. This is probably incorrect since I changed stats output to not be prefixed by the job id anymore.. We actually have a Limiter package now that simplifies this pattern a little bit.. Are you just trying to check for the existence of the failure file here? If so InspectFile is probably an easier way to do that.. Just use a bytes.Buffer here, much easier than using an io.Pipe and an errgroup.. Again, no need to look for the stats branch, jobInfo has the correct commit to use in it.. This comment is a bit out of date, you're not listing under /jobID.. I'd be down to make it a bit bigger, maybe 25 anything larger than that really seems like overkill to me though. I don't think so, objSize only gets touched here and in Close which first calls eg.Wait() so.. The last thing that runs in Process is a final log statement which records that Process has exited as well as the return value and error if there is one. This is a bit of an awkward situation since we don't know what the return value is or whether the function will error until we finish writing logs to the stats repo and serialize that repo. Unfortunately that means that the final log message can't be included in the stats repo.. Is there a reason that the generic OneOfEquals doesn't work for this?. I'm not sure how much a comment here is worth, it's not particularly discoverable if it's just on the default value we use, I think it might make more sense on the NewAPIServer function.. Makes sense, it seems like the generic version of this function is actually pretty unwieldy then.. I guess we could change it so that it takes a interface{} rather than []interface{} and then do runtime checks to see if it's a slice.. I can do that.. I'm cool with that, I was thinking that it has the disadvantage that you have to do Visit twice and you need both a call to InpsectRepo in validate and CreateRepo in CreatePipeline. But neither of these are really that big of a deal.. Sounds reasonable to me.. I think that would be ok. Afaict there aren't any cases were we want to return an error without calling cancel.. It is, I'll try to refactor a bit although I may need to grab you for help. That could is still a bit mystifying to me.. This is kinda inefficient. tag.Name is exactly the same value as hash and statsTag is just that value with _stats appended to it. So We're storing 3 copies of the same value in the cache. Given that it's sized for 1000000 datums I think it's pretty worth it to cut down the size of the objects we're storing as much as possible. Actually while you're at it maybe we should take the tag and statsTag out of ProcessResponse. Since that's extra bandwidth / latency and we'll already be computing that on the master.. This name references a different test.. There should be a helper somewhere that you can use for this. If not it's probably worth it to add one to this file for the one below as well?. I think I told you to implement it this way with an extra tree... but looking at it now I think you could just drop skippedTree and anywhere and use statsTree in its place. That way there's also no need to Merge since you're writing the files to the place they should ultimately go to.. That 0 there is a default value so you can just change that to -1 and you'll be able to see if it was specified or not. Also I don't really think unsigned ints are worth it, even though the value shouldn't ever be negative, you just wind up having to do a million casts.. Probably don't want to commit this long term.. This won't work if someone does commit^^5, right? I think that's fine. Seems like a pretty silly case to support, just want to be sure I understand the limitations.. You could just check StatsCommit != nil here. It being non-nil implies that EnableStats must have been true too.. It's maybe a bit weird that above we use activate for the process of activating auth but here we use enable. Would unifying on one word make sense to you? I'm assuming you had some reasons for switching from activate to enable here.. Why is called an enterpriseClientInfo instead of just an enterpriseClient? The Info part doesn't seem to really match with what Info normally means after a noun. Also the comment doesn't refer to the actual type name, which I think the linter will complain about.. Rather than commenting out this test just put a t.Skip at the top. That way it'll keep compiling.. I think this package needs to be server if it's in a directory called server.. This is likely to result in a pretty large reason message, why not just do the datum ID here and then they can get more info about it by doing inspect-datum. I see, makes sense.. \ud83d\udc4d . Seems like a reasonable idea, I think I'm just going to test for it being non empty though rather than a particular error message since the message might change.. So I tried checking for a substring, it passed for me locally but then on CI the substring couldn't be found. I'm unclear how but somehow even though it's the same version of kubernetes running in both environment they seem to return different error strings. It doesn't really seem worth it to me to go down a CI debugging rabbit hole just to check this string. It's not really clear to me how much more coverage we'll be getting out of our tests if they check some substring for the Reason.. Good suggestion, implemented.. Would it make sense to put this at the collection level rather than this level. It's pretty conterintuitive to need to handle not found errors when you using an iterator. So maybe we should just make it impossible as part of the api. Would probably insulate us from other potential bugs and also make this code a bit less confusing. At the very least we should have a comment here about why we need to handle not found errors during iteration.. Ah, that's a very good point.. Should be fixed, by line 137 below.. That seems worthwhile, I'll make the change.. Why not leave this as the default? You should always be able to construct a local backend and, all else being equal I'd rather not add an extra way for deployments to fail.. I think we already have a method called sanitizeErr that we've been using to do effectively this same thing. Maybe this is a good opportunity the throw this method in a common place and use it throughout the code base,  I think we copy paste the method around to a few places.. Is there a reason we no longer need the splitting code here? Or is it happening somewhere else?. Could we call this PushFile? File seems a little too generic to me.. I changed this because it was causing a really confusing bug where the master sometimes couldn't connect to the workers because I added an extra port to the service for exposing the users service. I wound up making a separate k8s service for it so I could do nodeports but this was a confusing enough bug that I wanted to change it so someone else doesn't lose time to it.. Surely.. Good point, are you looking for %v or %s here though?. I suppose we are making some assumptions here... I think it's a pretty safe assumption since I'm pretty sure this is how libc does it and I don't think it's really possible to have a linux distribution that libc doesn't work on... but it should be ez enough to handle a not found error here so I'll just do that.. Will unfix.. Lol, good point.. To detect the user, working_dir and entry_point, that's all done via a call to docker inspect.. It is, that cancel was only ever called by the defer cancel() below it so it wasn't serving any purpose that I could tell.. Yes, my thinking is that DeleteRepo == true means, clean up all of the repos which were created as part of this pipeline, this includes the output repo and any cron repos but not the input repos because those existed before the pipeline was created.. The point of leaving them around when DeleteRepo == false is that then you still have a full copy of the output data with provenance.. It seems a little bit weird to pack multiple string arguments into a single one, although we do that in other places like with commits that's normally for a list of things that are the same type. How would you feel about having separate --id, --secret, --token flags? That strikes me as a little more standard for command line interfaces.. Is region required to be present even when the others aren't? If so could we have a comment to that effect?. What is this annotation used for? I don't see it being read anywhere.. I don't feel to strongly so this will work. Are all the places we use the old scheme changed in the docs for this pr?. This is a fine way to do this but workload actually has a NewReader method which will give this to you as a reader rather than a string you have to convert into a reader. Advantage is slightly less code and your test will use less memory which may make it run slightly faster on CI.. So it seems like this PR is substantially lowering the object size at which we bypass the cache from 1/4 of total cache size which I think was 250MB by default to 16 MB by default. It seems like using a cache for objects greater than 16MB in size might make a lot of sense. What's the reasoning behind this change?. >.< this is the actual bug right here, right?. This seems very sub optimal to me. The whole point of this code was to prevent us from blowing our entire cache on a large object. That can now happen easily. Suppose we have an object that's the same size as our cache. A get-file for that will cause the following to happen:\n\nread hashtree into cache\nread all chunks for object into cache (the last one of which will evict the hashtree)\n\nIf someone reads the same file again we'll have to start by reading in the hashtree, which will evict the first chunk, then we read the first chunk which evicts the second chunk, etc. So the cache is completely useless for this workload, and useless for all other workloads while it's running.\n\nThe problem was that even though a sidecar had 64MB of cache by default, we are bypassing the cache for 16MB objects since 16 == 64/4, which is not ideal, since we definitely do want to cache those objects.\n\nDo we definitely want to cache those objects? If we're caching those objects then it's only going to be useful if there's <3 of them, since we need some space for the hashtree as well. If there's more than 3 of them then we'll like wind up in a situation like the above where we never read things from the cache, since they're constantly getting evicted by other big objects.\nI think we should go back to the old behavior i.e. bypass if it's greater than 1/4 the size of the cache. However we should tweak it so that with GetObjects we either cache all the objects or don't based on the cumulative size of the objects (and whether they're more than 1/4) of the cache. It doesn't really make sense to decide for each individual object since they're almost certainly all going to be read at the same time.. Yeah, no reason to make this more complicated, I didn't notice you were reading it later when I wrote this.. :+1:. Err, yup it very much is, I copied this over from the other PR and forgot to change it.. Good call, will fix.. So what's going on here is that only one of these auth configs is valid for the registry we're trying to push to. But there doesn't seem to be a great way to figure out which one that is. So the least fragile thing I could think of was just trying all of them. The code is setup in such a way that if pushing to all registries fails then it'll return an error. I think I'm going to make it so it returns all errors so the user has a better chance to diagnose the error.. Good point, I'll remove this, it was my first pass at the implementation before I decided to implement them in pachctl instead.. Same here, will remove.. Is this function really necessary? It seems like any situation where you need to results of multiple channels to be aggregated into a single channel could be greatly simplified by just giving everyone the same channel to begin with.. Any particular reason with start with a capacity of 10 here?. Hardest lyfe.. Does git rev-list HEAD --max-count=1 return the same thing as pachctl version would in this case if it was connected. I was under the impression pachctl version would be something like 1.6-<commit-hash> rather than just a commit hash.. It seems kinda over engineered to me to have our own special purpose matching library for. I don't feel too strongly but is it not possible to achieve this with existing tools like grep?. Are we losing test coverage in this file? It seems like it's getting a lot shorter here.. This should be called a Git input, even if it only works for Github right now eventually we'll want it to work for all git repos. Also I was under the impression that it would already work for other git hosts. Is that true?. This should work like the other inputs, ie input should have Repo field on it that you use directly here and fill in, in setPipelineDefaults.. Again I think it's better to use Git instead of Github for the name here. Just to leave things generic. Also it's weird to me that this takes a name argument just to check if it's \"\", I think the caller of this function should just do that.. Git instead of Github. Unless this only applies to github clone urls, but I don't think that's the case.. Consolidate these into 1 line.. Yeah, there's no need to implement those now, but particularly for protos we should always try to minimize the chances that we'll have to rename something since that causes all sorts of migration headaches.. Actually after reading more I don't think you actually need this helper at all. You should just implement this behavior in setPipelineDefaults that way you set the Name field once when the pipeline comes in and from that point forward you can just use it directly.. This should happen in validateInput.. This function is equivalent to path.Base I believe.. Check this error inline.. Same here.. What's this function doing?. I don't understand what this code starting at line 82 is doing.. This code is badly broken, it will never log an error because err is shadowed below. You can see this more easily in this snippet: https://play.golang.org/p/ALqJc-_MF0 . This should be done with an inline function that returns an error instead.\nWarning bells should go off in your head if you find yourself writing something like this that's not a pattern you'll find anywhere else in the code base. Chances are there's a reason.. This can just be if triggeredRepos[repoName] {. You shouldn't be implementing default values here, implement defaults in one place.. Same problem as above, err is going to always be nil here.. My mistake.\nSo if you do a, err := Err() then that actually doesn't shadow a previous declaration of err... but will shadow a previous declaration of a. That's some pretty wild behavior.\nAnd yeah, I was imagining you'd just wrap the whole body, but actually I think there's an even better way, just make HandlePush return an error, then register it with:\ngo\nhook.RegisterEvents(func(payload interface{}, header webhooks.Header) { \n  if err := s.HandlePush(payload, header); err != nil {\n    // log err\n  }\n}, github.PushEvent)\nYou could also define a second function instead of inlining that, I tend to like inlining if the function isn't going to be used anywhere else but up to you.. Definitely, although that's not quite what GetBlock does, it just blocks until there's a value. So if there's already a value it's behavior is identical to Get it's just that if the value doesn't exist yet GetBlock will block until it does. I'll add a comment to this effect, do you think there's maybe a name that better suggests that behavior. I considered GetSafe because it's safe to call concurrently with Create.. I think my names might be a bit confusing here. chunks is a collection that maps from JobID to a proto message defining what the chunks are. So there's only 1 value per job.\nThen there's another collection which I call locks which is scoped on jobs and has a lock per chunk defined in the chunks message for the job. . Sure thing on adding that comment.\nAnd yes this is one lease for k documents.. Also to clarify, I wound up doing something a bit simpler than what's in the design doc in that I have the master layout all the chunks at the beginning rather than workers dynamically laying out chunks as they go since the latter proved to be much trickier to get right. We could still implement the more complicated behavior if it feels like there's a limitation in this one, but I think we should wait until there's a good reason and just do the simple thing until then.. This should happen in validateInput otherwise we might create other input repos and then have to bail out when we realize this one is malformed.. Could we capture / check the ok parameter here? I'm guessing this library guarantees that the value it gives to this function will be of type PushPayload but given that it's not reflected in the type I think we should be defensive about it since if it's a different type this will segfault.. You could also make HandlePush take a github.PushPayload so you only need to do the cast once.. So I was under the impression that if an STM failed due to contention it would be retried. Do you happen to know if that's the case? If not I guess we should wrap this in retry logic and perhaps add a randomized delay, if it does retry by itself I think that's sufficient though.. Good catch, we absolutely need this, otherwise we can't process datums that take longer than 30 seconds.. I think we basically have to resign ourselves to the fact that if a worker gets stuck because the user code never completes then the job is going to get stuck. This doesn't seem to me like a great place to insulate ourselves from that.. Also #2503 will help a lot with this.. Yeah I found that kinda off putting while writing this code too, but also opted to defer working on it.. Done.. I took a crack at break this up and found the improvements weren't that great. The code got more organized looking but the downside was that the functions I broke it up into wound up needing to take like 10 arguments each just to have the variables they needed present.. Done. I think there should be a comma here, although it looks like it was already missing.. :+1: . These shouldn't be examples they should just name the type of the data.. These shouldn't be on the base CreatePipeline method. They're too rarely used to make people specify them as 0 everytime they make a pipeline.. These proto values should all be Durations. So as it exists now if you hit that zombie case the zombie processes are left running but we continue as if they've exited which means when we run the next datum we're going to have mutliple processes running at the same time modifying /pfs/out. That doesn't sound great. Also in that case we'll leak the above goro, is that correct? If that's true I think we're better of just blocking in that case which will hopefully lead the user to force restart the pod or something like that.\nDo we have a sense for how common that case is?. I'm very confused what's going on here with retErr, I actually don't think you need it at all.. So I think what you can do here is instead of calling cmd.Wait() call cmd.Process.Wait() that waits for just the process to return, it doesn't wait for IO to finish. Then, after that returns, check if ctx.Done() is closed, if it's not then you call cmd.Wait() to make sure the IO has finished, if it is closed then just return, this will potentially lose you some logs that are actively being written... but I think that's an ok compromise to make at this point.. This is an example: https://play.golang.org/p/-mPSMvn7NY (that doesn't run... I suspect because go playground won't let you shell out). Could we drop commit here, I think it's redundant since the proto message is already a CommitInfo and could we use description instead of message since that's what the equivalent field on repos and pipelines is called. Git does call it a commit message so I see the argument but I think internal consistency is more important than consistency with Git.. I think having --message and -m work with finish-commit is nice ergonomics. I think having --description as a synonym is probably good too. Also can we have these flags on start-commit and finish-commit? I feel like you may know what your doing when you start the commit, then you add a bunch of stuff and you come back and are like \"wait, what did I just add?\". I would call this simply WaitIO the name, as is, is a bit of a mouthful and I don't think makes things that much clearer.. Yeah, we do it outside the goro because if we did it inside we could spawn 2 goros and have the second one get the lock before the first since go offers no guarantees about goro spawn order. This code is arguably a little weird, because in the non-follow case eg.Go call immediately below this comment could actually be removed and you could just call the function inline (at which point you don't need a function) and get the same semantics. However I think this is overall fine because it allows us to use the same code for both paths with complicating things too much which is why I did it this way. . One more clarification on this: GetLogs will have slightly higher latency with this implementation than the previous one. This is because, in the non-follow case, it requests each pod's logs serially, sending those logs to the caller before it moves on to the next pod. In the previous version it would request logs from all pods concurrently, and then send them 1 at a time. This implementation has the advantage that it doesn't use as much memory as the previous version which I think is an overall win since logs can actually be quite large. Furthermore the latency should actually be similar in terms of time to first byte, just slower in terms of time to last byte, so I suspect in most cases users won't notice.. Oh I just removed this because I realized it was redundant, ListPipeline is already calling ScrubGRPC on the error before it returns it. Maybe I should just do a larger pass and make sure all scrubbing happens in the client. It seems like the sanest option to just having the scrubbing right next to the actually GRPC calls.. Is there a reason we need to use a channel here? It seems like we're just using it to get all the log lines to a goro so they can be printed but we could just print them at the point we send them through the channel and I think we'd get semantically equivalent code.. It may well be, if it is I'll remove the library from vendor as part of this PR.. I'm assuming this changed from 3 to 6 because of the addition of collections. Could you update to comment to explain why?. This is a bit pedantic but I find the name constructTreeFromPrefix a little incongruous with the other names in this file. For one thing I think you should use get in place of construct since they have about the same connotation and get is what other similar functions use. Another thing is that it doesn't quite feel like it's constructing the tree \"from\" a prefix. Maybe getTreeForPrefix, how would you feel about that?. Merge this with the line above into a single return.. Did you check to make sure that the first write to the system doesn't get recorded as revision 0?. The diff on this last part is a little bit hard to follow but I think it's almost all superficial changes that just come from the fact that you are now handling multiple write records and thus need to put the code in a loop... which leads to indentation... which leads to hairy diffs. Is that accurate?. What happens if the last character is the maximum value here so incrementing it leads to wrap around behavior? Is that something we need to worry about?. I'd remove the With from this method and ListWithPrefix I don't think the With is adding a ton of clarity.. This case seems a bit weird to me. Why does it return a single KV if there are no more Kvs left and shouldn't we do something with that single KV it returned?. This seems a bit weird to. Is it that the first key is going to always be the from key which we've already returned? Is this true even of the first request we send out?. I'd recommend adding a template to this even though you don't use indexes. In the commit invariants branch that will be merged in soon we use the template to make sure that you don't accidentally put one type in the collection and read out a different one. It's just an added safety check that I think is nice to have.. It definitely would be clearer, but it would flip the default. My thinking with this was that it's better to turn it on by default since there theoretically shouldn't be any reason not to have it outside of legacy support which should tail off over time.. Same here, I do think --rbac is clearer... but it means that the default is no rbac, which seems like a worse default to me.. Ah, I see, and then you could do --rbac=false to explicitly set it to false. I can definitely see the argument for that, since it's weird having the no- prefix. On the other hand it also seems weird that you'd have a flag --rbac where just passing it does nothing since it's already the default value. I could see arguments either way but given that we already have no-guaranteed I think the weirdest thing would be to change one and not the other, and since no-guaranteed is already out in the wild I'm reluctant to change it around. I think I'm just going to stick with what I've got for now.. It has to be below the code that figures out / fills in the ParentCommit. The appendSubvenance call below takes commitInfo as an argument and it needs that commitInfo to have the ParentCommit field filled in so that it can figure out of one of the CommitRanges in the subvenance has its parent as the upper bound.. Unrelated to the changes, but these should save us some keystrokes.. This seems reasonable.. I see, and this won't trip us up if there really only is 1 total key because we won't even get to this. That call to Get is above.. I'd make this 2 a constant numPipelines. Could we have a test with the metadata repo case too? I.e. where one commit has several downstream subvenant commits?. This seems like a nice change, do you think it makes sense to not log the file and line redundantly below?. I feel like it actually might make sense for DeleteCommit to start jobs sometimes. For example suppose I've got a branch with 20 commits on it, I make a pipeline that points at the branch. This should start a job for the head commit, then I delete the head commit. That should cancel the job, but I'd presumably want to then start a job for the new head commit, otherwise I won't be able to see any output from the pipeline until I do another commit.. Calling the ReadWriteCollection repo seems a little bit weird here, I'd just call it commits.. How do we compute commit sizes? If we're computing it as the \"new\" data that's not present in the parent commit then this is pretty close to correct. I don't think we are but maybe we could.. How does this work with commit ranges? I think we might want to simplify things a little bit and say you can't delete commits that have provenance, you have to delete them through their provenance instead. I feel like there are some weird situations we can get into where you delete a commit and then pps just recreates it.. So we could in theory fix this by having commits store references to their parents and the branches they're currently the head of right? That seems like it might be worth doing in the future but probably not as part of this release.. For this case I normally like to do map[string]bool that way you can do if visited[repo] which is a little more concise / clear.. Yeah if it seems easy I'd go for it. 1 clarification though, it seems like there's 2 options here, we could start storing the new size for commit sizes and use that value directly or we could store what we've been storing and compute the new size when a commit gets deleted. I'm inclined toward the first one since it leaves the meaning of commit sizes the same but that might just be inertia on my part, I could also see an argument that displaying the new size for commits is better.. Ahh, good point about why it's stable. Sounds like we're in agreement about not being able to delete subvenant commits making things simpler. I also think this is better from a user experience perspective. Suppose I have a pipeline that outputs some garbage data which crashes a downstream pipeline, I might think that a good way to solve this is to delete that outputted garbage data, but the data that initial caused it to output garbage is still committed to the input repo so the next time I commit to it the garbage is going to come right back. Forcing people to only delete input commits forces them to solve the root problem.. So this code isn't changing anything about the upstream branches, it's just collecting the branches and head commits. This had to be added in this PR because previously propagateCommit was only run when a new commit, with no provenance, was created. Now it's being used for commits that were created by createBranch, those commits often start with provenance of their own so we need to collect it at the beginning so we can put it in the downstream commits provenance.. So this is actually using branchInfo.DirectProvenance to assemble the provenance argument we pass to createBranch. We do this because we need to call createBranch to reset the head of the branch to commitInfo.ParentCommit, but we don't want this to clear the provenance. Previously, createBranch couldn't be used to remove provenance, only to add provenance, I fixed that as part of this PR but then had a bug where deleting a commit would leave a branch with no provenance and thus would prevent triggering.\nAll this being said, we may be able to remove this given that I think we agreed in the delete commit PR that you can't delete commits that have provenance, in which case provenance would always be nil here.. I'll change to what you suggest here, that seems much more descriptive.\nI've adopted the general policy of leaving changes that I make when debugging to figure out where an error message came from since I figure those have some long term value.. Will remove.. What's the point of these changes?. Why are we cleaning out our .dockerignore here? Although most of this is no longer needed I think .git would still be good to exclude.. Makes sense. We may want to consider moving the Dockerfiles out of the top level directory now. We originally put them there so that they could add src but now that they're ignoring it there's no reason for them to be there. No need to do it as part of this review.. Ahh, I see, so this is actually a bit of a pain. We've actually known about this bug for a while but haven't fixed it because doing so required migration. The issue here is that the result of these fmt.Sprintfs is changing, which means that we won't be able to find the indexes we computed in previous versions. It might just be time to bite the bullet and do this migration since we're doing other stuff.. That is a good point, we should probably do the same for output commits as well.. Is this needed? It looks like all of the methods that call this call tb.Helper themselves before passing it in. And it doesn't look like there's any place where it's nil. I could just be missing them though.. We have some codified pfs errors in src/server/pfs/pfs.go you could just add ErrCommitDeleted there to enforce that those to use the same error texts.. UUIDs also have the property that the 13th character will be 4, because we're using uuid4 here. We should use that in the check as well.. Breaking commit resolution out from inspectCommit is an awesome change I've been wanting to make for a while. A few questions:\n- it looks like resolveCommit doesn't implement the ancestor syntax. I think it probably should, and we should add a test for calls that use resolveCommit to use the ancestor syntax if we don't have that yet.\n- it looks like inspectCommit doesn't use resolveCommit but instead resolves the commit itself. Could it just use a call to resolveCommit or is it different enough that it needs to do its own resolution.. Do you want to sort by subvenance or provenance here? The comment says subvenance but the code is doing provenance.. Also in going from upstream to downstream is there a problem wherein a branch is still pointing at a deleted commit and gets triggered by propagateCommit from createBranch and thus creates a commit with a parent that doesn't exist because it uses the head of the branch as the parent commit.. :+1:. Does this actually do anything? I though that tb.Helper would only be relevant if the function (or a function it calls) might call t.Fatal in which case it gets omitted from backtraces. But it doesn't look like this function can call t.Fatal. These are gorgeous.. What's the purpose in wrapping this in an inline func rather than just calling these things inline? Also if you do still want to do the inline func you can call it directly inline, i.e. \n``go\nif err := func() error {\n    ...\n}(); err != nil {\n    ...\n}. This is a nice semantic change here, I guess previously there were ways thatFinishCommitcould fail that would cause the scratch space to be cleaned up so you'd have to recreate the entire commit.. I think it make sense to have this error insrc/server/pfs/pfs.gowhere we keep other errors that the outside world might depend on.. I think we may actually want to exposeresolveCommitas an rpc in the future, I wrote a similar function insrc/server/pps/server/api_server.gowhich just callsInspectCommitand only considers the commit it returns. So I think it'd be more efficient and probably make more sense to just use this function. Anyways, nothing necessary for this PR, just figured I'd mention it.. Similar to the comment on the other PR, it might be nice to haveIsCommitDeletedandIsCommitNotFoundfunctions insrc/server/pfs/pfs.goso that we have a canonical way to recognize these errors.. Is it possible for for this to get called on a commit with upstream commits? I was under the impression you had to delete from the root of a provenance chain..?s on the above 3 lines.. And on Environment below.. Makes sense.. I think that makes sense.. I didn't actually wind up going through with allowingBuildCommitwith an explicit ID to create commits without parents, it messed too many things up including the way we keep track of the size of repos. So now we have to order them.. Yup it does.. @sjezewski I think you originally implemented this. It struck me as really weird because we're writing non-tab delimited data (viaPrintDetailedDatumInfo) to atabwriterwhich I think can lead to some pretty weird output. I tried it though and the output doesn't look that weird so I'm not sure what's going on. But I don't think this works generally, so just wanted to call it to your attention.. Yup. Yes, although looking at it now I think I should fix this backend, since the other invocations ofGetLogsreturn lines w/o the\\ns. I'll just make thepachdlogs case do the same.. Exactly, you'd get a pod not found error.. Not for the error case I don't think. I can add that.. The name of this method is inconsistent with our other names. RPCs that returnXInfoshould be calledInspectX. I also don't think this makes sense in theVersion` API, since it's not really providing versioning information. Let's relocate this to the admin API, getting info about the cluster seems like an administrative task to me.\nUnrelated to this PR would you mind renaming the GetVersion method to just Version? That seems more consistent to me. All the other GetX commands return a stream of data.. Too many names here, change this to:\ngo\ninspectCluster := &cobra.Command{\n    Use: \"inspect-cluster\"\n.... I was under the impression this snippet of code was fairly specific to version since we expect that to be the first command that people run. Is it needed here too? I don't think we have it on most other commands.. Can we just have the APIServer get the cluster ID out of etcd rather than passing it in? That seems a bit simpler and a bit more resilient in case we add a way to change the ID or something in the future.. This is missing a ScrubGRPC call. We generally put those in src/client/... though so I think you should add InspectCluster to src/client/admin.go, put the ScrubGRPC call in there and call it from here.. Oops, I didn't read this comment before setting the variable... are we setting it to \"rc1\" elsewhere in this PR?. So I might be misreading this code but it seems like this is basically stripping GithubPrefix off the front just so it can add it back to the front in canonicalizeGitHubUsername. This makes me uneasy, if we ever do somehow get into this situation won't that mean that the cluster is permanently unusable because calls the getClusterID are failing?. I'm cool with changing this to \"turned off,\" \"turned down\" is a phrase I've heard to mean more or less the same thing with respect to servers though.. Because we can always recompute the output if need be. So there may be some small amount of data that gets processed during migration and then needs to be reprocessed in the new cluster, but this at worst wastes a few CPU cycles, there's no risk of losing something you can't get back.\nShould I include this explanation in the docs?. Actually I think you can just ignore this case, since I don't think a vanilla Get like this can ever return more than 1 Kv. I'd just return resp.Kvs[0].Value and assume it's the only one without printing a warning. I think it's very unlikely that warning will ever get triggered but it's likely to confuse us later when we're reading this code and wondering in what circumstances that could ever come up. A comment documenting this could be nice though.. Check out src/client/pkg/require it has some helper functions which would make these all 1 liners.\nThey'll also print out a bit more information when the test does fail.. I was under the impression we needed to do rc1 with lowercase to satisfy some Python thing with read the docs or something. Is that no longer the case?. Good call.. Yup, I'll do that.. Will do.. It isn't exported afaict, this is just copy pasted from the other tests of the http server. I agree that ideally we would always use names for these ports, but it doesn't seem quite worth it to dive in and clean them all up right now.. Could this be done in a goro like the other startup calls? Or does construction need to block on it?. Man, this a slick abstraction :smile: . Yup, nice catch.. It's actually an old implementation of migration from 1.4 to 1.5. Not really useful to anyone anymore and likely to confuse people.. For static errors like this I think it's a little more standard to just do:\nvar NoTokenError = errors.New(\"\"no authentication metadata found in context\"\nand then you don't need an IsNoTokenError because you can just do err == NoTokenError. I'm basing this off of how io.EOF works in the standard library.. Why not just do this with a backoff, it won't really change much but it'll make it look a bit more official. Also it's an easy way to add a notification when we retry (not that it'd be hard to do with this code as is) which I think is a good idea because should this somehow become an infinite loop due to a bug or something we'll at least have a chance at debugging it.. Another place where I think it'd be good to use our backoff library and have a notification when we retry so we have a chance of catching the bug if this becomes an infinite loop.. I think git inputs use a repo as well don't they?. It's possible that this could race with pipelines being created right? Probably not that big a deal since auth activation is a very rare occurrence. But how would things fail if it did race and a pipeline missed auth getting activated?. I see, yeah doing with a string comparison makes sense then. You could still have it be a static error I think, rather than make a new one each time, but it doesn't really matter. I'm not sure I understand the need to put it in the client library though. Does other client code call this method?. I don't know... seems a bit over engineered.. Good point, fixed.. This looks like a typo.. It seems weird that callers should launch this in a goroutine, why not have InitPachRPC be the one to call go. That way InitPachRPC can just be called like a simple function.. I believe etcd is actually at on 2379 and 32379 is the port we expose with node ports.\nI could be wrong about that though, or it could be that this comment is just wrong, or it actually is using 32379 when it should be 2379 in which case presumably CI would catch it. Do you happen to know which of those it is?. Would it make sense to do a select here on ctx's Done channel in addition to the pachClientReady channel?. How could this be non-nil if this func hasn't run?. So with 10 chunks it seems like you should expect chunks of size 2, which is big enough to trip the bug, but just barely. Wouldn't this be a little more likely to fail if we set 2 chunks and thus had chunks of size 10?. It's probably better to make this a Fatalf rather than an Errorf so that the test exits immediately if this condition is ever met.. A good point, I retract my previous question.. If you think there's a more meaningful term tell me what it is. Child is the most standard term I know for this.. Regular file means a file that contains data, the other type of file being a directory. Both of these are standard terms. And no, I don't think \"file\" is description enough because then the message would read: \"malformed file at /foo/bar: it's neither a file nor a directory.\" Which is completely inscrutable because you're saying that /foo/bar is and isn't a file in the same message.. I'm not sure if I agree that that's clearer... maybe having the word already there makes it a bit clearer. I'm not sure what adding the word the is gaining in terms of clarity though.. This is also why we were using the word \"node\" originally, which is also a very standard term. We need some way to refer to a general node in the filesystem which could be either a regular-file or a directory. The only 2 terms I know of for that are \"node\" and \"file.\" \"Node\" is a more foreign word in general, but it might actually be more clear because the alternative \"file,\" is more familiar but I think for most people it doesn't immediately have the association we want. It also necessitates the use of \"regular-file\" to distinguish what users normally think of as files from generic files which might be directories.. The files contained within a directory are referred to as \"children\" of the directory.. Writing out \"file or directory\" seems pretty lame. Does this message: \"malformed file or directory at /foo/bar: it's neither a file nor a directory\" seem clear to you? Referring to a dir as a file seems like a pretty natural aspect of the Pachyderm ontology to me. Afterall, most -file commands accept directories as arguments and return them as results. The exceptions being put-file and get-file. Is there a place we could explicitly spell out that file is a generic term? And that the specific examples of files are regular-files and directories? . Should this be holding the mutex while it accesses v.leaseDuration? It looks like that could possibly race with the code above. But I'm not quite sure.. Weird... how was this compiling before?. Ahh, right. Makes sense.. Good catch, this is one of my classic moves right here, just substituting a completely different word with a few common letters >.<. So this would be good to confirm experimentally but I believe the issue with this code is that while the pipeline is in standby this for loop will busy wait. Because the select has a default it will always enter that block when there's not a new commit available, notice that there's nothing to do because it's already in standby, loop and repeat that process. I suspect if you run this and put a pipeline in standby you'll see a pachd node that should be idling consuming 100% of a cpu core. Assuming this is the case this necessitates 2 separate selects because there's no way to have a conditional default block and we only want the default block to be active if we're not already in stand by.. Good call, looks like I should be.. Good call, will do.. The check is actually correct here, but the message is wrong. I'll fix the message.. I think that would make sense if we had multiple job states we wanted to block on, which could happen in the future. For the time being we only have need for the ability to block on whether or not it's done, where done is defined as being in success or failure state.. This should probably have a nil guard like the other functions do.. inactivate -> inactive. Good point, I'm going to change it and see if CI passes. I'd be very surprised if it didn't since I'm pretty sure if we got an error there it would cause a segfault as is, so we must not be getting errors there.. Done. Fixed. I can definitely see the logic for that, I did it this way because throughout the code base bool values always default to false since that's what go naturally defaults them to and so we invert a lot of conditions so that the default value of false corresponds to the behavior we think makes for a better default. In this case it doesn't really make sense for this struct, but I think it does make sense for the feature as a whole, because the flag to enable it at deploy time is --no-expose-docker-socket if the flag was --expose-docker-socket with a default of true, then the only use for the flag would be --expose-docker-socket=false which I think is a little more confusing. This is pretty hairsplitty though.. Yup, good catch, removed.. Rewrite this as return resp.State == enterprise.State_ACTIVE. enterpriseEnabled is a bad name for this argument. Arguments should reflect how they'll be used, not where they came from. This should be called something like exportMetrics.. One of the biggest worries I have with changes like this is that the metrics code can introduce new ways for things to fail, so it's really important that this code be written defensively so that even if it errors, that doesn't prevent the rest of the program from functioning normally. It's very worrisome to me that you're using WithLabelValues instead of GetWithLabelValues because WithLabelValues panics where GetWithLabelValues would return an error. Why are you using this version of the function?. Similar to another comment, why are we using panicking version of this function when there's a non-panicking version?. log.Fatal? Why on earth would we want to crash the entire program if we get an error here? That means all it takes is some random network hiccup and we kill the whole program?. I think it'd probably be reasonable to store whether or not enterprise is enabled (the variable name should be exportStats or something, as discussed before) as a field on APIServer that gets set once on startup. If people enable enterprise they can just scale up and down their pipelines or delete pods or something. And that transition should be quite rare so I don't think it's worth it to do the check for each datum.. Nice fix :grinning: . Download stats will also probably make sense.. I think \"worker\" probably makes more sense as the subsystem name than \"user.\". Small nit, but I think \"Number of datums processed by pipeline\" is a bit more accurate. I'm not quite sure what the \"and state\" at the end means.. Another small nit, but I think this should be \"Time running user code\" or \"Time processing datums.\" The first one seems a bit better to me.. Why are all of these Histograms? They all seem like things that could in theory be counters? I'm guessing there's some queries you couldn't do with counters, but it might make sense to include the counter version as well since it's more basic and I think there are some useful things we could do with it.. I think metrics or exportMetrics would be better actually. We use stats to denote the stats branch, and there's actually another argument to this function that references those stats. It seems really confusing to have 2 arguments to the function that reference stats when they're not the same stats.. Great catches here.. I think it might make sense to move this below the line where we get the logger so we can log this in a way that can be consumed by get-logs, users are unlikely to see the message as it is now because they'll need to do kubectl logs rather than get-logs which most users don't do.\nIt also has the benefit that it makes the code a bit shorter, because you don't need an exportStats variable, you can just do server.exportStats = resp.State == enterprise.State_ACTIVE. How would you feel about factoring this into its own function? I feel like this defer block is getting big enough that it impacts readability of downloadData, would be nice if it just read defer downloadMetrics(time.Now()) or smth like that.. Same here, I think this would be nice if it was factored into its own function.. And here, factoring into a function.. Even this is probably worth its own function.. Seems worth its own function as well.. We should compile this Regex once rather than everytime we call isGlob it looks like that function will be getting called a lot.. This can just be, return nil, nil. I don't think there's a meaningful difference between an empty slice and nil slice.. It's always preferable to initialize everything in the struct in one place, particularly if you're declaring the struct inline. In cases like this one, where you're defining a constructor, I think the constructor is the one place that all the state is being kept in so the rule applies a little less. You know that no one is going to be touching the struct or experiencing sideeffects from it, since it's not accessible from outside the constructor.. Could we make it so this shows what matched in the regex so they have an idea what needs to change.. This could probably be made more efficient. getTreeForFile is a potentially heavyweight function because it takes the tree for the parent commit and then applies all the writes it finds in etcd, of which there might be many. We create the full tree in the call to globFile though, so I think you just need to refactor things a little bit by copying some of the code in globFile into this function so you can hold onto that tree and reuse it.. Any reason you don't like just returning this line? Saves some vertical space.. It seems a bit weird that this might return an error and a valid map, I generally like to make sure the other returned values are nil if we're returning an error.. :+1: yeah, ErrBreak is pretty confusing and is likely to encourage buggy implementations of iteration methods.. Yeah that probably makes more sense, or maybe just TestList I called it TestPagination because the backend logic that it's testing is the pagination of etcd requests. But probably makes more sense to name it based on the interface it's using, rather than the implementation it's using.. We do actually have a CommitRange proto message, but it's used in different places. It might make sense to group these together at some point.. Could we also take this opportunity to make these error messages more targeted at things users understand?\nIn particular I think the word context here is unlikely to mean anything to our non go-developer users. Something like \"not authenticated, try logging in\"  would be a bit better though. The one below is probably even a bit more confusing because it doesn't give them any suggested actions to take to remedy the problem.. What's the inlined function accomplishing here?. This inlined func too I don't understand the purpose of, I feel like I must be missing something.. Could we just return the a.sudo call above?. So having thought about this a little more I think I like what we have now better.\nHaving every return line in an iteration function have to specify that it doesn't want to break by returning false leads to a lot of clutter, and it prevents some particularly nice cases where we can pass GRPC Send methods directly to iteration functions since they have the right signature.\nI also did some more reading of the standard library function this approach is based on, and they do a similar thing where they have a special purpose error that the callback can return, to control the flow of iteration.. I see, that makes sense as a goal. I think that the name superUserClient should probably be warning enough not to use it when it's not needed, and maybe a comment as well. When I see something wrapped in a closure like this it doesn't immediately convey to me that I should avoid using it if I don't have to.. :+1: . Also has the benefit of making the comment above correct.. Minor thing, but it seems a little weird to compute the state in this function rather than in ReportMetric below, especially since you're already passing in the duration, you could pass err in addition and jut compute the value in there.. Does Prometheus not accept camelcase names?. Make these constants.. This seems unsafe, what if 2 calls to ReportMetrics are running concurrently? Won't this be a concurrent map access?. Yeah, I had a similar thought, but I'm cool with it. pachd_GetFile_count is very ugly.. My only preference against it is that it's a bit clunky to use compared to real maps, but it's not that bad. Either approach is fine imo, although if you do want to use a Mutex I think it's probably worth it to use an RWMutex since these maps are going to mostly experience reads.. Is this test no longer valid?. One of the goals I had for this PR was to remove this iterator pattern from the code base, at least from the collections library and other parts that I'm touching in this PR. After using this pattern I'm convinced that it's not a very good way to do iteration in go for a couple of reasons:\n\nIt almost always leads to longer code, you can see this if you look above, you have code that reads:\n\niter := newEtcdIterator(c, prefix, limitPtr, opts)\nfor !iter.done() {\n    kvs, err := iter.next()\n    if err != nil {\n    return err\n    }\n    // real code begins here\n}\nIf this was using a callback it would look like:\nreturn iterate(c, prefix,limitPtr, opts, func(kvs *mvccpb.KeyValues) error {\n    // real code begins here\n})\n2 lines of boilerplate vs 8, this stuff really starts to add up too because anytime you want to compose one iterator to make another you need to write another for-loop. I probably cleaned up several hundred lines of iteration boilerplate as part of this PR.\n\n\nThere's a lot more reasonable ways to do iterator interfaces and as a result you have to read each individual iterator to see how it was implemented. In particular: some interfaces opt give you method to figure out if the iteration is complete, you've called yours done and it's true if iteration has finished, but it often makes more sense to have one that's true if iteration has not finished It's also common to have neither method but to have your next method return ok as it second return value if you can continue. Or sometimes it just returns an error and if you're at the end of the iteration that error is io.EOF. With a callback there's only one reasonable way to implement it.\n\n\nThere are certain optimizations that iterators make much harder. In particular, a really nice optimization for this code would be to asynchronously request the next batch will the first batch is being processed. To do that with iterators you would need to have a goroutine running in the background that you launch from you next, but now that goroutine could potentially leak, so you need to add a cancel or close method to your iterator, and callers of the iterator need to remember to defer that method everytime they create an iterator. Which is more boilerplate and potential bugs.\n\n\nSo long story short, let's migrate this over to callback interface like the other iteration methods in this PR and I'm going to add these thoughts on iteration to our official coding guidelines. I was actually planning to do that anyways as part of this PR.. So I was just reading through this and I think it all makes sense. The one thing I'm worried about is if you get a lot of log requests then you might get a growing backlog of goros waiting on this mutex. I think it's fine because I think what happens in this function should be very fast, so it shouldn't really be possible to get a backlog. How would you feel about adding a gauge for the number of active ReportMetric calls we have going? That way at least if this is a problem we'll be able to see it in our metrics.. I think you can probably remove this nil check, it shouldn't be possible for this method to be called without NewLogger having already returned and thus the Gauge having been set.. Ahh, I see. Makes sense then. Just for my own knowledge, do you have a sense of what could cause registration with Prometheus to fail and how common that's going to be?. This is a bit pedantic but how would you feel about renaming the functions in this file from iterator to list to match a little more with the other ones? I also think this function in particular is close enough to listF that you could just rename it to listF it seems like the only difference is that the readonlyCollection is an argument rather than the receiver.\nAlso if you want you could drop the Fs from the ends of these functions, I only named them that because I was gradually migrating from code that had a bunch of functions called List already.. Makes sense, could we replace it with a test that makes sure that we error when people try to do put-files on open commits?. Very minor points: I think this would be a little bit clearer if you called mutex descriptionsMu, that just makes it a little clearer which structure it's protecting. I also think you can drop the * and have it not be a pointer, since you're passing around *cacheStats you don't need it to be a pointer. This will also save you having to initialize it because the zero value for an RWMutex is usable.. Any particular reason you choose to embed this rather than just having it be a field? I don't think it really affects anything but I normally only embed for a particular reason. I also think you could drop the * here and things would all work the same. Both minor points, neither is really necessary before merging.. While you're at it might as well move us to Go 1.10 proper instead of beta.. It looks like this value has to be updated in sync with the value in the Dockerfile for it to remain accurate. Is there anyway to make one depend on the other?. Do you happen to know how much this line adds to the image? My guess is it's not that bug, but is something we could remove pretty easily by using a multistage Dockerfile.. As discussed offline, this used to be how this code worked, but records need to be applied in the same txn that reads and rewrites the branch pointer so that if it races with another put-file it doesn't break.. Exactly. The pipeline actually stops pretty quickly in terms of its state, but GarbageCollect actually waits for all the workers to be gone as well, which can take a bit longer.. Good call, will do.. The test deletes everything on line 4267 though. I think the comment is maybe a little misleading in that originalObjects and originalTags should both be empty.. Yup, specifically GC will fail if there are any pipelines in a state other than \"STOPPED\" or if there are any pipeline workers (it queries kubernetes to check) so this normally has to retry a few times while K8s actually spins down the workers.. Will do.. Indeed it was.. :+1: . Sounds good.. Yup, much more foolproof and requires less information too.. Good call, done.. It doesn't seem to be, although I would suspect you're right.. So this actually wound up being harder than I thought, I changed the code but where to put the constant is a bit tricky due to import dependency cycles. I decided to just create an issue for this and not block the PR. Since this isn't a new problem introduced by the PR but rather one we already had.. I think it makes more sense to store this field in the spec commit rather than Etcd. In my mind the EtcdPipelineInfo is for the \"real world state\" of the pipeline rather than the users intent. That's one of the nice benefits of this PR in my mind, it lets us move stopped away from the volatile state into the spec commit.. So I don't think it should be an invariant that Stopped -> State == PAUSED. I think it makes sense that there's a window between when you stop the pipeline and when the master actually picks it up during which Stopped is true but State != PAUSED. These helpers should be the same though shouldn't they?. Having this set Stopped to false seems a bit weird here, updating the state should be something that happens distinct from the Stopped flag, since one is updating state and the other is updating intent.. This mutex is protecting historgram and counter right? If so a comment about that would be a nice addition. When a mutex is protecting one thing I generally like to do counterMu at which point a comment is redundant. But if it's protecting 2 histogramCounterMu is too dorky, so I prefer a comment.. Why don't you just return the error from Walk and then log it below. I think that would condense this code a lot, you'd only need 1 line to handle each error rather than 3 and you wouldn't need copyErr.. Oh right, makes complete sense. :+1: . Makes sense \ud83d\udc4d . Nice, this is a good overall code improvement. \ud83d\udc4d . Any reason you changed this? Doing it with var is more standard for our codebase. And it's slightly shorter, 25 characters vs 26, think of all the bits we'll save.. Minor point: When using a map as set, I prefer to use map[string]bool instead of map[string]struct{} since it allows you to do:\ngo\nif seenPaths[rootPath] {\n     ...\n}\ninstead of having to use ok.. I think you could just make this a function that returns one of these 3 addrs, or do it with an if else. Let's not bring gotos into the codebase though.. Will do.. Nope, they can go away. I did a direct translation of the existing HashTree implementation to get the semantics right, now that it's complete and I've got all the tests passing I can start removing the scaffolding.. :+1: . ServerOptions seems like a better name for this than ServerSpec to me, was there a reason you changed it?. This comment seems a bit out of place since it seems like within this block there's only one copy of each server. Am I missing something?. Oh, I guess this this comment just refers to the block as a whole, maybe it should be above the eg.Go call?. I'm guessing this was just a temporary change? You probably want to either change it back or remove line 326 and update the comment.. This error handling seems a bit weird because it only prints out the warning for at most one of the files, even if there's errors with both. Could we just have 3 ifs here, one for each error and then one if both errors are nil?. Remove this line for real since it's presumably not needed.. What if I actually do want to have a filename that looks like a URL though? The only reason I think this might be a real use case is people sometimes do that with scrapes. Wget downloads sites into directories named for the URL for example. I also think this error doesn't protect them from that confusing of a case. People would presumably hit this if they do:\npachctl put-file repo branch s3://my_bucket\nwhich should print out reading from stdin which hopefully makes it clear this isn't doing what they want.. A a warning I like it, seems like a good ergonomic improvement.\n2 minor things\n\nShould this be going to stderr along with the one below? And probably the \"reading from stdin\" message as well.\n\nWhat if we said \"did you mean to put from it with: -f %s and spliced in their url? That gives them an easy route to understanding the concept.. This parsing feels a bit fragile, and writing non-fragile parsing yourself sounds quite annoying. Check this out and see if it's worth using: https://github.com/xwb1989/sqlparser. Go will actually allow you to do just a naked return makes it even clearer what's going on.. I think this function creates a couple of opportunities for deadlocks. The pattern for using it seems to be to launch 2 goros in an ErrGroup. 1 running  this function and 1 that sends buffers on the channel. This can deadlock if:\n\n\nThe send goro encounters an error and fails to close the channel. Right now I don't think this can happen, since the only function used in that context is hashtree.Merge and the first thing it does is a defer a function which closes the channel. But this is possible if someone uses this function in a different context.\n\nThis function encounters an error, it returns without having read everything from the channel. This seems like it could easily happen with the code as it is today.\n\nIn both of these cases one of the goros will block indefinitely so the ErrGroup will never return, so you'll never see the error.\nI can think of a couple of ways to fix this:\n\nyou could use the context stored in the APIClient, and pass it to the ErrGroup so it gets cancelled if any of the goros error.\nyou could use an io.Pipe although I think you rejected this because it makes copies of the data. Although most of the implementation is based on channels and similar to what you have here.\nyou could create a new Pipe that doesn't do any copying and only uses channels, this might be a nice type for us to have, since this situation occurs fairly frequently in our code base.\n. I think this would be a little bit clearer as ReadRaw rather than ReadBytes since it indicates that this is a raw proto.Message that you're reading and the fact that you're reading bytes should be clear from the signature.. This is a potential site for an optimization, instead of copying the parent tree we may want to mutate it instead. It makes this operation faster at the cost of making it slower if we want to read from the previous tree later. This could be done as part of this PR but could also just be a comment documenting this for the future.. It seems like this function could potentially leak goroutines, if you error out of the for loop then you're left with nothing reading from the channel and the goroutine above will block indefinitely.. Does the multiple workers actually help much here? It seems like most of the work they're doing is guarded by a mutex anyways? It also seems like this is another potential leaked goroutine. If more than one of these workers errors then one of them is going to block indefinitely attempting to send that error into done.. It does, you're not going to see it in the diff though because the code was already there and hasn't changed. The main change here is that we're cancelling all the monitor goros, and clearing the map when the master process goes into backoff. This is because during this time we're not master so we shouldn't be doing master things such as making cron commits.. This is the other major bug fix. The issue here was that if you wind up with a job whose PipelineVersion is lower than the worker's current version then it'll go into backoff, expecting that the worker will be recreated soon and have the correct version. This works when the PipelineVersion is higher, but when it's lower we want to just skip the job.. I was using this at one point but got rid of the call and just used the method above. I'll get rid of it, since I don't think it's really going to be useful to anyone.. That should be pretty doable. Right now we're not actually pulling debug dumps from the side cars, we can do it but it didn't seem super important to me since the main thing that will tell you is if there's a hanging call to the API but the only thing that makes calls to it is the worker and so we'd see the client side of the hanging call.. We could possibly make that an argument... I may punt on this to see if it's actually needed. I mostly want this command so I have a quick way to get users to give me a goro dump, and I can filter out the parts I want from there.. Good catch, will fix.. The only reason you have to do this is because your code doesn't fit in with the code around it. Each of the other cases reads a single instance of the delimited value off of the wire. Yours is reading multiple which is where the problems are coming from. Make it read a single value, and everything will make a lot more sense. Alternatively you could make the outside code expect to read multiple datums in each iteration of the loop... but I think that would be worse.. Would also be good to have tests that do multiple PutFile to the same SQL split to append stuff to it, including doing so with a different header and after files from the previous PutFile have been deleted.. It would also be good to have a test which creates multiple tables in the same repo /table1 and /table2 and then tries to get them out with /**, that should give you:\n\n<table1 header>\n<table1 row1>\n<table1 row2>\n...\n<table1 footer>\n<table2 header>\n<table2 row1>\n<table2 row2>\n...\n<table2 footer>\nI think this would reveal some bugs in GetFile. This code seems overly specific to reading everything from a single table, I don't think it's going to work for reading from a single datum in a table or reading from multiple tables. We should hammer out some more test cases for this and then I think it'll be clearer what this code should look like.. This would be a bit clearer, and a bit shorter if you just check to see if parent == filepath.Dir(path) that's the intent here right? You only want to attach the header / footer to the direct parent of the path being put.. So is it accurate that we're passing the Header and Footer once for each datum we write out? That's probably not a big deal performance wise but it is a little inefficient and I think it might make more sense to instead have a special call, like maybe tree.PutHeader and tree.PutFooter so you can explicitly put the correct values into the directory once rather than doing it implicitly with each file you want to put.. I'm generally not a fan of using named return parameters, except for causes where you need reference them in closures (ie retErr) I've shot myself in the foot a bunch of times by inadvertently shadowing the parameters. I also don't think these are really making the code much shorter, they're just saving you writing a few :=s instead of =s I think. You also probably won't need _err if you don't use up that name in the return parameter.. Does this reader have support for just reading a series of statements w/o reading a header and footer? That seems like something we should be able to support as well.. I could see users wanting to manually add a few rows to their table after having done a dump. Suppose I just did a dump then did an additional insert. It's kind of annoying to have to recreate the header and footer just to do the put-file for this row.\nI also think even if this isn't reflected in the user facing API it makes the code a bit more modular and will probably come in handy later.. I add the pod name ahead of all the stacks I dump, so things are delimited. Definitely could be nicer but I think just having a way to dump this and then doing manual filtering after the fact will get us most of what we need for debugging.. You can actually just remove the argument names entirely here and write this is:\nfunc (a *InactiveAPIServer) Activate(context.Context, *auth.ActivateRequest) (*auth.ActivateResponse, error) {\nsaves us another 4 characters :). Could we keep this as 1 line?. Could we keep this struct inlined in the return value? I find that a bit more readable.. This is a slightly awkward case that we may want to think about expanding a bit. Having to pipe in echo '' definitely isn't super pretty. You can achieve a similar effect by passing -f /dev/null, not sure if that's much better. Maybe we should consider adding put-header and put-footer which just update the header and footer, and those won't have any weird interactions with the existing put-file flags. They could just be different pachctl commands and still send put-file requests under the hood.. Also this example seems to be invalid because it doesn't have a --split flag.. It's not a particularly big deal but why did you move this up here? I generally like to keep declarations close to where they're used.. Seems like this code is repeated below. You can probably DRY things off if you just set the header and footer values here so that the code below will run.. Doesn't GetFile with a glob return things in sorted order? It certainly looks like there's some sorting going on in there.. I don't total understand what this block is doing. Why do we error if len(dirNodePaths) != 1? That doesn't seem like correct semantics, ie if someone did get-file */0000000, which gets the first row of each table at the top level, it seems like this would error if there were multiple directories, but that's a perfectly valid get-file.\nAt a high level I'm a bit confused about why you think you need separate code paths for the case where your glob pattern matches a directory and the case where it doesn't. It seems like you should be able to handle everything with the stack approach.. Probably better to do this check before you push rather than after you pop, for deeply nested directory structures pushing and popping for every one could actually start to be a meaningful amount of memory moving around.. This seems incorrect, if node.SubtreeSize != 0 then we'll be overwriting the existing value.. Put the return on the line above, there's no reason to store an error in a variable if the next thing you're doing is returning it.. I think this code would be a lot cleaner and easier to understand if you didn't have 2 implementations, one for when there's objects and one for when there's not. I think it should be pretty easy to merge those into a single implementation.. This change is unrelated to the FUSE changes but was something I noticed while testing it. This was causing us to have to store a ton of metadata when we inserted a large file. We were storing about 128 bytes per 16 MB in the file. Which was big enough to start to be a problem, particularly because it had to pass through etcd in many cases.. Changing the pfs.ChunkSize made this test take way too long, so I lessened it to compensate.. This change is unrelated to the FUSE changes but was something I noticed while testing it. This was causing us to have to store a ton of metadata when we inserted a large file. We were storing about 128 bytes per 16 MB in the file. Which was big enough to start to be a problem, particularly because it had to pass through etcd in many cases.\nWill update the comment.. It can be, it's actually valid to call functions with a nil receiver, since it's effectively just an argument. You only segfault if you try to access fields on it with the function, so getCommits is implemented as:\ngo\nfunc (o *Options) getCommits() map[string]string {\n    if o == nil {\n        return nil\n    }\n    return o.Commits\n}. Good call, I'll take a look at this for the PR and see what's worth covering with it.. Good points all, will do.. I think it's a requirement for Python in general, its standard library seems to do a seek(0) whenever you open up a file.. Yeah, afaict there's really nothing for it, when you send the call to the OS to mount something that call just blocks until it gets unmounted, but there's no way to tell when the mount is actually serving real data.. Linter seems to be fine, I think it's because it's in an _test.go file which means that it doesn't actually get exported since those files are only taken into account during tests.\nI don't think it's worth relocating it, the main worry with copied code is that you might change it in one place but not another. But when we're talking about a universal constant like a how many bytes are in a MB there doesn't seem to be much risk of that. And most of the reason I want these as constants is to make later lines shorter. So going from 10 * MB to 10 * testing.MB, while not a huge deal would defeat the purpose a bit.. The linter will complain if there isn't a comment there that starts with PutFileWriter, so there's not much choice. But I think it does make some sense because go uses it to autogenerate go doc so if a user has this particular implementation and calls go doc on the method it doesn't do them much good if the comment is present somewhere else in the file.. Is there a reason we don't want to indentation? Seems like it makes it more readable which is what this command is meant for.. Using a testing backoff here seems a little weird to me, we might change it for test related reasons and not realize it can affect the behavior of the rest of the server.. Good call, I was thinking that if the Send has already calling Close isn't going to make much sense. In many cases it won't because Close will just error in the same way, i.e. in the case of network errors. But there are a lot of cases where it will actually do something and in the cases where it errors it won't hurt anything to call Close.. Yeah, question is confusing because I was very confused when I asked it, somehow mixed up which was the new code and which was the old code. I get it now.. Will do.. There is, and I considered doing it but it would require making some fairly big changes to how makeCommit works. I figured that would make more sense as a separate pr rather than bundling it into this one. I also think it likely makes sense to wait until your performance changes are merged in because how we stream stuff in is probably going to be affected in some ways by your hashtree changes.. Might be good to move this to above the switch, since that can cause the call to error and we may want to see logs for it.. Generally speaking raising a variables scope isn't a great thing to do unless you it's necessary. This is really all the same scope, just higher up in the scope. Still I think it's worth moving it down so it's declared close to where it's used.. Yup that's the intention here. FUSE sends requests for 128K chunks so I wanted something not divisible by that so we'd get some differently sized requests at the end.. Yup, good catch, will remove.. I think it's fine to not return an error in that case, since we've been able to do everything the user requested of us without encountering any errors. It seems like a pretty rare case, but I can't imagine what would go wrong if people didn't get an error back there.. Yeah, that's my main worry. The FUSE mount is often backgrounded within a shell, so if it gets too chatty it makes the shell unusable.. Looking forward to when we merge this with @brycemcanally's performance changes this is going to need to be implemented as a single pass over the paths, otherwise it's going to be painfully slow. I don't think it should be too hard to refactor this code to be a single pass. We can chat about how to do it in person.. Cobra doesn't support that, but no reason to let that stop us, go-units does.. Hmm, I must have been confused about the scoping of this, looking at it now it seems to be at the minimal scope it can be at. I don't think have 2 of them makes sense.. :+1: . pc -> pachctl so it's consistent throughout. Yeah, this has existed for a while like this, I'll document it.. Actually the comment already mentions that, but I agree, this is good to note in review.. Yup, this blocks the pipeline from scheduling, but all the test does is wait for the pod to exist (it can exist without being scheduled) and then check to make sure that its pod spec reflects the values in the pipeline. I'll add a comment explaining this.. Should this maybe take the client rather than creating its own? Given that you're almost always going to create one right afterward.. Would it be dangerous to roll the behavior of lenientCanonicalizeSubject into CanonicalizeSubject it seems a little lame to have both, and it seems like you're mostly using the lenient version here. Are there cases where you need to be sure it actually has its prefix?. Could we just call this updateConfig it seems like it's trying in the sense that it might fail and return an error. But that's normal enough behavior that I don't think you need to specify it in the function name.. I see, that makes sense, I like it better this way then, maybe a comment on deleteAll that explains this a bit.. Is it possible to have this struct just be a proto and thus obviate the need for the ToProto method, or would that be too inconvenient?. Could we send this to stderr instead of stdout.. I'm still planning to do that but I wanted to just do this the quickest way to get it available for customers.. Updated the doc to explain this.. Added.. Got it.. Remove disclaimers from this before merging into master, since once we do that it's going out in the next official release.. suggestion\n1. Pachyderm's authentication is built around GitHub by default. Without this. Might as well make this an env struct like our other packages.. I don't think you'd have to initialize the env struct somewhere else. You can read env vars into a struct either by calling cmdutil.Main(mainFunc, envStruct) or by just cmdutil.Populate(envStruct) which you should be able to do from anywhere. I think it'll probably ultimately be less code to do it this way.\nThat being said, this is definitely not a big deal so if it seems like it's going to be a real amount of work or slow you down feel free to not do it.. Also I just noticed looking at our cmdutil package that there's a way to read config files using it too. Not sure if that'll make things easier but it might.. How would you feel about removing code and switching to --one-time-password. one-time-password seems like the more descriptive name and I think it's worth it to have a loner more descriptive flag name, given that there's also -o for the short version. It seems like there's some slightly different handling of the 2 cases though, is that hard to role into one case? If so what we have here is fine.. suggestion\n    // Get scope based on group access. suggestion\n    // Expand scope based on group access\nPR is extra based.. Is it infeasible to use a protocol buffer as the canonical config? Having to convert between a go struct and a protobuf go struct seems to lead to bugs where you add a field but forget to add the field to the ToProto method. Sometimes it's unavoidable, if you need non-data fields in the struct but just glancing at it seems like canonicalConfig might not have any of those.. Nice general improvements :). This should be path.Base I believe, filepath is compatible with host operating system conventions but since this is a pfs path we don't want to change based on that.. Can we add this back? Do the tests pass?. Having async methods in Go is a bit of a weird smell to me. Since the caller can always make things async by wrapping it in a go directive, and it seems to promote dropping errors. Also looking at the code it looks like this is indeed async but the more significant feature of the async writer is that it's buffered.. Actually reading the code below I think we should just call this PutObjectWriteCloser, it's only async in that Write calls return immediately before anything is actually written, but that's an assumed feature of WriteClosers, os.File can behave the same way. That might give us a naming conflict in which case, maybe we call this BufferedPutObjectWriteCloser, or just eliminate the other implementation?. Can we get comments on these about how these fields relate to each other? Can you have tree and trees? Or is tree effectively deprecated at this point?. Hang on a sec, 1T default memory request?. Lower number here? Does this still require more than one page of results as the above comment suggests?. Similar to above, do we need to lower this?. Can this be skipped like other tests? If you're going to comment out a test you almost always might was well remove it. It'll stop compiling real quick.. This seems a bit weird to have request.Block == nil on the same line, it's return err which will always be nil in that case, so it has the right semantics but it reads weirdly.. size_bytes is a hard cap on the number of bytes that will be returned right? Comment above seems a bit ambiguous if that's the case.. > Go big or go home\nlol\nThat makes sense, is there a way we could have it be 0 and have that indicate no limit at all? Or does it need to be a real value? Not really a big deal, just would be a tiny bit cleaner.. Yup, that's accurate, I changed that a while ago. Would you mind removing the comment?. Makes sense. Wanna open an issue for making datums faster? It's going to matter to users for sure and we should have somewhere to track it.. :+1. Is there a reason to make these global variables rather than put them within the driver? I think we mostly only do one driver per process, except in tests, but having global state potentially shared between drivers seems like a bit of a funny smell.. Updating a global variable from within a constructor without a sync.Once seems a bit weird to me.. The signature of this function seems a little bit weird because it's returning a cleanup function that needs to be called to cleanup the state of the hashtree.Readers I would expect the type to be hashtree.ReadClosers and satisfy the ReadCloser interface. That seems a little more idiomatic.. Do these files get closed anywhere? The os actually won't cleanup files if you Remove them but don't close them (until the process exits). This would actually be a good behavior to exploit here I think. You can Remove the files as soon as you create them but keep using them through the open file decriptor. That way they'll get cleaned up on crash, and all you have to do is call Close on them in the hashtree.ReadCloser's Close method and they'll get cleaned up for real.. Would this be easy to get in?. So we're still using the bolt hashtree in a few places I saw in the code. Would it be hard to remove those? Would be nice to remove this code, but that might be too hard  right now.. This seems worth taking a look at before merge.. This function seems a little bit hard to follow. I think it's correct but I can't totally understand what it's doing. Can you think of any ways to make it clearer with comments or maybe more descriptive names?. This can be done in 1 line as:\ngo\nmq.q[i], mq.q[j] = mq.q[j], mq.q[i]. How do we know that mq.size+1 isn't out of bounds here? I don't see where mq.q gets initialized. Could it be an append instead?. This will be removed as part of incremental removal as well I assume.. It seems a little weird that this is returning a closure that does the work rather than just being a definition for the returned function. Callers can pretty easily turn it into a closure if they want to call it later, or just not call it until they're ready.\nAlso setupStats seems like a weird name for it, it seems like it's doing more than just setting them up, it seems like it's also merging them and writing them out.. Remove this?. I'm all for async writers like this. I don't disagree on anything except naming. I don't think you need async in the name of this type, WriteClosers are assumed to have async semantics like this by golang convention.. suggestion\n  // The number of bytes requested\nYeah, I'm just nitpicking the comment. Sorry I should just make a direct suggestion if I'm going to be so nitty. See above, I like that a little better.. I'm on board, go big or go home.. Yeah, commands datum related. How fast is inspect-datum these days?. I feel like moving it into the type is worth it just to not have to do this. And when we're actually running a container we only have 1 driver at a time right?. We're these tests failing? Or no longer compiling?. We'll probably want to refactor to have MalformedGlob just be it's own global type, maybe in a globutil pkg that this and hashtree can import. Feel free to not do this for now though.. So this means that header and footer are leaving the API explicitly and only accessible through --split? Is that permanent or temporary?. Why not call this nodeToFileInfoHeaderFooter or even better just fold the logic into nodeToFileInfo and wrap it in a conditional.. Also I don't think it's good to assume that only input repos will have headers, outputting stuff with headers is probably a valid usecase.. It seems like things might get easier if we moved Shared into the FileNodes rather than having to recurse back to the parent. Might be too much data bloat though.. So I agree in general on supporting as narrow an interface as possible. But it seems like this puts an onus on us to add parsers for every file format people care about. We'd probably have to add a csv parser almost immediately, and I bet a bunch of other formats from there.\nI agree that it does expose some kind of weird calling patterns though. It's reasonable if succussive PutFiles always have the same header set, but as soon as it's mix and match it gets dicey. . I'm cool with not doing this.\nI do think it probably makes sense to have the function's name reference Header/Footer or Shared, since the input part seems more incidental to me. I guess it's a question of naming for where the function is used rather than what it does.. Isn't it illegal to update though?. Yeah, that seems like a great feature that covers a lot of bases, and shouldn't be too hard to implement.. That makes sense as where it's used. I still think a name that conveys what's different about the other function of the similar name rather than where it's meant to be used conveys a little more information.. Right, makes sense.. So just to be clear I'd prefer to see the name changed here.. Hitting the merge button would make this untrue, right?. Can we make these ascending numbers for those who read markdown in text format?. It seems a little weird referencing yourself in these docs. You might want to say, Pachyderm is test with an Okta app with the following configuration.. Why remove all containers? This would probably kill a kubernetes cluster that's running in containers.. I feel like 1 week might be a bit aggressive for this, I don't think it changes that often, what if we did a few months?. This doesn't guarantee that the job enters state merging, but I think that'd be pretty annoying to do in a test and would be flaky, so I'm cool with this.. It seems like semantics here are slightly different, does osx grep not have --exclude-from?. Why are we adding these lines but commented out?. Makes sense, can't approve to confirm because I already approved.. I see, @msteffen added this originally I think. My sense is that there's a lot of things in our .gitignore that often contain the thing we're worried about. So it'll trigger a lot during development. . Good nitpick will change.. Go doesn't forbid this in the language or any style guides afaik. It does have a risk of being confusing, I only use _foo to mean foo in a lower lexical scope where I need to be able to reference both. In a case like this I think it reenforces the relationship between fi and _fi so I feel like other names would probably be more confusing. So my stance on it is that it's kosher but should be used with caution.. No more license?. Will this message show on every pachctl command? That may get a little too chatty quickly.. Small nit, I'd call this pachdLocalPort since there are multiple daemons.. This probably shouldn't error. Pachd is designed so that you can have more than one copy of it running at a time and use any of the instances as endpoints. We should probably just select a random item from the slice, and only error if it's empty.. Oh cool, that sounds good.. I don't think it'll be confusing. You should get exactly the same semantics if you're sending multiple commands to different pachd endpoints as if you were sending them to the same endpoint. Random is pretty close to the semantics you'll get if you connect through a service (stateful round robin instead of stateless random).. Yeah, I'm not quite sure what that logging is hooked up to but I don't think it's hooked into the stdlib package. I think you can get away with not logging this even in verbose. As long as errors get logged if it fails it's fine if it succeeds in the background silently.. This is fine yeah, we only want to exclude that one file at the root level.. \ud83d\udc4d much more informative error.. Do we need convert methods for things that haven't changed? Looks like the codes already written, but that's going to be a pain to write each time. Can we do some sort of proto based conversion?. @brycemcanally do these changes look correct to you? They look reasonable to me but you know the code better.. This is a little wonky, couldn't it just pass the hashtree as an argument and be a top level function?. Yeah, per offline discussion let's not do this. If this is sufficiently annoying for new migrations I'm sure the engineer tasked with doing it will have every incentive they need to come up with something simpler.. This would be better placed in validatePipeline than validateInput since the validation isn't specific to inputs. . Referring to this as a label is probably going to be confusing to users who get it back. Also we should probably disallow empty strings for pipeline names, I can't think of a real use case for that.. Very minor nit, there's a general standard in Go that error messages shouldn't start with capital letters because they're normally prefixed in other messages. I.e. this will be emitted to logs as \"Error uploading output: Filepath is not...\" which reads a little bit weirdly.. This all seems to make sense, and I think it makes the code a bit cleaner.\nCould you just describe a bit where the bug was coming from and how this fixes it? . Very minor nit, but it's probably better to use strconv for this, if for no other reason than to save an import.. I think it should error if you specify both of these, since it's ambiguous where the image should be coming from. We probably want to keep our examples referencing images rather than Dockerfiles, particularly this one because the opencv container takes a while to build.. Minor nit, let's just call this createPipeline terser and equally clear imo, if not a bit clearer.. This is conflating the meaning of WorkingDir in Transform it's used right now to control which directory usercode is run from (ie what it sees when it does pwd).. It seems a little bit weird to me that you need a new dockerManager for each image you build / push. I feel like this could just be the functions build and push that return the relevant arguments.. This should be called build I think, since you're not necessarily rebuilding when you use it, you could be building for the first time.. Right, I forgot that even after you build the image you still need something to tag it as. We could just default to using the pipeline's name, but having the image in there seems to make sense as well.\nAlso it just occurred to me that if someone does --build without setting \"dockerfile\" we should default to Dockerfile in the current directory, since that's almost always what it's going to be.. Having multiple specs in the same directory is pretty common I think, especially because we support having multiple specs in the same file. But having multiple specs using the same docker image / dockerfile (with different commands) is also pretty common. So I think the default is still going to make sense in a lot of situations.. There's a helper function for this types.TimestampFromProto that also does some error checking.. Btw, this will fail if you use New instead of NewWithRegion because the driver will send a request to try to figure out the region of the bucket which will fail because we don't implement it yet. I explicitly specified the region to get around this. You should get this working in the case where region isn't explicitly specified before merging it.. Playing around with this on my machine I get the following when I try to run TestSimplePipeline:\nerror running the master process: ReplicationController \"pipeline-testsimplepipelinec5ed94a91679-v1\" is invalid: spec.template.spec.containers[1].volumeMounts[2].mountPath: Invalid value: \"/pach\": must be unique; retrying in 3.6971187s\"\nWhich makes sense because in local deployments we put a hostpath mount there, so we get an overlap. The place where we do this is a few lines above (starts on 81), I think if you put this in an else block for that conditional it would pass CI. There's still the open question as to where that MkdirAll is coming from that's creating the error. I did some more checking and can't figure it out. It might be a good idea to run a binary with a runtime.PrintBacktrace() call next to the error handling for that MkdirAll just so we can see the execution path that's leading to it.. Could you implement the Delete method too? Right now the underling localcache.Cache.Delete method is exposed but takes a string while Put and Get take int64s.. :+1:, nice idiomatic Pachyderm style functional iteration.. Are there any implementations of this type that actually return an error. I did a quick check and didn't see it, if not could we remove the error return argument?. We tend to prefer using map[string]bool for sets in our code base which allows you to do:\nif c.keys[key] {\n  ...\n}\nrather than having to extract an ok parameter.. I think this would be a good place to describe a bit more what kind of data is stored in this collection, unlike jobs and pipelines, shards is a lot less self explanatory.\nAlso while you're here would you mind updating the comment above on plans? It could use a little more explanation too, and definitely should say \"The progress collection,\" I think that description is the product of bad autocomplete on my part.. Why serialize into a bytes.Buffer here rather than just directly into w below? Does it help to batch things up? And is that worth the extra memory used for the bytes.Buffer?. What happens here if it fails to renew. It seems like one major reason would be if it's too slow to update the lock. But that will mean that another worker probably has this shard in which case won't we have both nodes processing the same shard? I think we need something similar here to what we have in acquireDatums where claimShard takes a function that it calls with the shard once it's been acquired and a ctx that it can cancel if it loses the shard.. Similar to above, what does using a bytes.Buffer gain us here over writing directly to objW below?. To match with our other API calls could we call this GetChunk? Collect isn't a keyword we use anywhere else and this seems to basically be the same behavior as our other Get calls.. Package here should be s3, I think this is something that I probably did originally when I started this code. I'm actually a bit surprised go will compile a main package that doesn't have a main function in it.. Do you think this means we should move to being case insensitive with our bucket names? That makes sense to me as a general rule, people shouldn't be using foo and Foo as different repo names, and it'll remove some awkward cases mapping buckets to repos.. It's a little more standard in our tests to use a backoff.Retry loop for stuff like this than a busy wait. See an example here.. Minor nit since this is test code, but putting this is a deferred block is preferred.. Responding to ping with anything but pong seems borderline illegal. Not an actual request for change here.. I generally prefer to use http.StatusNotFound in place of hardcoded values. Although it's a minor preference since at this point 404 has kind of transcended magic number status and is probably easier to read than http.StatusNotFound for most programmers. But as soon as a start adding in some of the more esoteric error codes I want to use the values in the http package, and then it just throws off my feng-shui to have some of them be numbers and some constant variables.. Sure thing.. Aren't DNS and the filesystem both case-insensitve? I'm not sure I understand where we'd get into trouble if we went case-insensitive. Either way this isn't a decision that needs to be made for this PR.. Yeah... very good point.. It's used in Write to exit early if previous async calls to writeBlock have failed. Without it errors would still get returned in Close but you might waste of cycles / bandwidth pushing blocks that aren't going anywhere.. I'll add a comment to this effect.. s[:0] creates a new slice which uses the same underlying storage as s, but has size 0. This is useful because it allows us to use it as the underlying storage for a bytes.Buffer and have that buffer start writing from the beginning of the slice. If you simply did bytes.NewBuffer(bufPool.GetBuffer()) then I'd get a bytes.Buffer that contained 100MB of 0s and when I wrote to it, it would reallocate the slice and tack the write onto the end of those 100MB. Semantically this would work find if I just didn't initialize buf, since it would still be a buffer of size 0 so I could append to it until I hit 100MB and then write it like I do now, the difference is that it would need to reallocate the buffer several times during those writes, this way it never has to reallocate and we can reuse the buffers.. I will attempt to, this format was compliantly copied over from the old version of the code that used this format for the block ids. I'll see if I can find some docs.. Makes sense, I didn't see you were using the buffer below to put it into the datumCache.\nI think you could do this pretty easily with io.MultiWriter.. This is probably a good place to use io.CopyBuffer and a buffer pool. io.Copy can be kinda slow by default because it uses a 4096 byte buffer by default. It can also create a lot of garbage because each call allocates a new 4096 byte buffer. . It seems like contention might become a real issue here given that this lock is held while files are copied to and from disk. It could maybe be made better with a sync.Map?. I don't think what you did quite fixes the problem, because it only checks lostShard at the beginning of mergeDatums but you can still lose the shard after you're past that point. I think this is a case where you kind of need ctx, because that's the only way to cancel an ongoing rpc which you need to be able to do when the shard gets lost.\nAlso I think the code as is will deadlock as is because lostShard isn't initialized (unless I'm missing it). nil channel is valid in go but is channel that blocks all sends and receives. I think you'd run a risk of deadlocks even if it were initialized because you send to the channel in a loop in claimShard so eventually you'll fill it up and then that goro will be blocked and you won't be able to claim shards anymore.\nThese are the types of edge cases I always worry about with channels and why I prefer to avoid them in cases like this where I think passing a closure works a lot more cleanly. Another similar case is function based iteration vs a chan based iterator type.. >.< good call. They're used throughout our codebase (for non-batched reads) when you need a slightly different interface. Implementing the most basic one for now totally makes sense. Other ones will likely be added as they're needed in specific cases in our code.. Yup, just fixed it.. That seems a little weird but not necessarily wrong. Is there a reason Send can't be implemented here? I'm assuming that Recv basically needs to be implemented so that this type has the right signature to be used by the grpcutil functions?. limiter is used to limit the number of outstanding requests you have so you don't overwhelm the backing services. Things should work without it until you do really high stress tests.. To match with GetBlocks in this file I'd call this GetFiles although it's also worth considering if maybe this functionality should just be folded in GetFile. My sense is no, since it's likely going to have an even more divergent signature at some point.. This is generally kosher, but you should use the context in the client rather than creating a new one (c.Ctx()). We tend to write wrappers around these raw grpc methods to make them a little bit nicer to access, so you'd have c.GetFileStream that calls c.PfsAPIClient.GetFileStream, but using the underlying methods is totally kosher, just a little more unwieldy.. isOpen is a slightly confusing name to me, after reading a bit I think it indicates that the commit is being started by this call (which I had actually forgotten this call could do) but at first I thought it indicated that the commit was open before the call started. Maybe we could call it isStartCommit or something, I think that'd remove any confusion.. I think this comment should be below the line below, or maybe just inside the if block since I think that's what it applies to and it would prevent it getting separated from the code it's commenting on.. Empty structs like this are a totally reasonable way to implement an interface. There is a slightly nicer way (imo) that the standard library uses in the http server code. More just an fyi than anything, I'm happy to approve either method.. That is true and something occurred to me while implementing this. In general I'm not crazy about it, although before this existed we were already sending a read request to etcd, but now it's a read and a write. Maybe we should make it so update doesn't actually do the write if nothing is changing, as will be the case with these updates.. Extended your example to show what I'm talking about: https://play.golang.org/p/6IBl1z7WooI\nThe main difference is that rather than using a struct{} as the receiver for the func that satisfies the interface you can actually use a func itself as the receiver. Because funcs are first class objects in go they can act as receivers. Like I said, not a required change at all.. Let's call this TestPutObject. It might make sense to have this be separate handlers. I think that'd be slightly less code and make it a little more organized, over time each of these blocks is probably going to grow so we might as well do it now. I think the built in router for the http package should have everything you need but if not feel free to bring in something like: https://github.com/gorilla/mux. I think reflect.DeepEqual is always going to return false here because expectedHash is a string and actualHash is a []byte. To properly compare these you'll want to convert expectedHash into a []byte and pass them to bytes.Equal.. Sounds good, shouldn't be much work.. Makes sense not to do this optimization until we have some evidence that it's an impactful one.. What makes sense to me, having not thought about this as much as you have, is to add a new method getFiles that looks something like this:\ngo\nfunc (d *driver) getFiles(pachClient *client.APIClient, file *pfs.File, func((*pfs.File, io.Reader) error) error {\n    ...\n}\nAs I've mentioned before I really like callback style iteration in Go, but I think it gets really powerful when you have it at every layer because it's very easy to compose.. There's a function in this file nodeToFileInfoHeaderFooter that will give you back a pfs.FileInfo with all the relevant information filled in. You could take a look at / just use that. I think for the Name field it's just copying it over.\nAlso I'm not sure if it makes sense to return the Hash as part of GetFiles, that's part of the metadata for the file which is returned as part of ListFile or InspectFile. Generally once you're to the point of downloading the files you already know the hash, or if you don't it's because you don't need it for anything.. This is so nice, those grpc logs stuck out like a sore thumb.. This is a nice ergonomic improvement. Should we do a similar thing for s3://, wasb:// for those object stores.. Being able to start the server with pachctl seems pretty handy, but I think a more important access pattern is having the server get started in pachd's main function and add that port to the pachd service. That way other pods scheduled on kubernetes will be able to talk to it to for their workloads. It's also the first step to exposing an s3 api to worker pods since they have a pachd instance running in the sidecar.. Would you mind adding a mention of the format we're using for this timestamp in here so people know how to parse it. (We probably should have had that in the old version of the code as well, but alas.). Minor nit but could we call this controllerServer or maybe controllerSvr if you want to keep it brief? That seems a little more in keeping with how we name such types throughout the rest of the code-base.. Same here I'd call this identityServer or identitySvr.. I'm a little confused by this loop, maybe I'm just misreading, is there a way to get out of it without erroring? And what's the need to loop the Stat and the RemoveAll it seems like after the first time through it'll definitely be in the state we expect.. We had issues in the past with needing to be privileged to perform a bind-mount. Are these containers already privileged so that's not a concern? Also maybe there's no other way to do it.. Probably shouldn't use your personal dockerhub account here. Could you push it to pachyderm/tensorflow_0_10_rnn_got:v1. Newline here would be nice.. Same here.. Is there a reason to lift these types rather than just using them directly? Seems like this is just some extra typing that's not needed.. This might be a little bit cleaner if you used the encoding/xml package. I think you could just define a struct that you fill in, similar to the map[string]interface{} below and it'd serialize the right thing for you. I'm not sure if you need more control over the format than that gives you though.. :+1: love that we can pull in some external tests for conformance tests here.. We have a function IsGlob at src/server/pkg/hashtree/db.go:1474. Probably better to reuse that here.. A few thoughts on multipart uploads:\n- using the local filesystem has the problem that it will fail when there are multiple pachd nodes because requests will likely get routed to different nodes. @brycemcanally is working on using the local filesystem for normal puts though so we can probably find a common solution to this.\n- It might make sense to stream this data directly into object storage rather than using the local filesystem. Given that most of the object stores have a notion of multipart uploads themselves.. So I was thinking the underlying object storage, not pfs. Locally could cause some problems, I think we'd need to implement a multipart upload call in our obj api, that way we could avoid needing a roundtrip to the cloud (and needing to get local to have access to a cloud object store which is probably more annoying, and defeats the purpose of local mode.) Anyways, not sure if this is a great idea, just a thought I had.. So doing this will make it so that people can delete commits that were emitted by a spout pipeline, which seems to make sense, since they're more like input commits than output commits.\nIt would be good to have a test where we delete a commit that was outputted by a spout to make sure this behavior works as we expect and doesn't break in the future.. Could we also have a test that attaches a downstream pipeline to the spout. I have no reason to suspect that won't work but that's going to be the main use case for spouts so it'd be good to have some test coverage on it.. Let's do something a little prettier than an empty string here, maybe we could do \"none\".. This is an unrelated improvement you're able to make now because VisitInput correctly handles nil right? If so that's a nice bonus, I wonder why we were already defending against nil inputs here, I guess at one point we actually allowed them.. Passing in a string here is a great way to solve the problem. Minor nits:\n- could we call the variable masterType that seems to match up with how it's use a bit better.\n- for the values passed in could we have them be: \"service\", \"spout\" and \"pipeline\", I think that will lead to the most understandable logs for users.. Kubernetes has recently added the ability to port-forward to a service, which I think basically does the work you're doing here for you.. Good writeup, can we relocate it to doc/deployment, etc is a good place for the yaml file but not docs. We'll probably want to point users at this sometime soon and having it rendered on RTD will be nice for that.. Why is the Jaeger service named pachd and not jaeger?. Should this be set through an environment variable?. This would probably be better located in src/server/pfs/pretty I think there's some other similar functions there.. So it looks like all the changes in this file are moving around the ctx parameter. Are these actual semantic changes or are they refactors?. One last thing, to make sure that this pipeline is actually functioning properly could you do a FlushJobAll and check that the jobs are succeeding. You can see an example on line 1231 in this file.. Any particular reason we need goto here? It looks like you could just as easily put the error handling in an if err != nil block so to get the same semantic effect a little more cleanly.. A persistent volume would really only help if you want to be able to pick up where you left off in the event of a pachd crash. That's probably not so important to have, it doesn't solve the problems with multiple pachd servers afaik.. Just to clarify, none of this has to be done before this gets merged in. I'm just speculating on how we can get this working with fewer restrictions in the future.. Yeah, you can't have a volume that's shared between multiple pods. I think doing it as a separate repo could work, and get a similar result to writing it directly to the object store, but I think you'll face a bit of an impedance mismatch. You'll either need to write a document per chunk into etcd so they can be represented as files. Or you'll need to do a one-off commit and have a document per commit in etcd. Neither one is super pretty. I think instead you can basically just proxy the multipart upload stuff directly to the object store and then have a single etcd document that keeps track of the existence of multiple part uploads. You might not even need that since multipart uploads already have an id associated with them, you might be able to just proxy directly.. suggestion\ndefault to simply writing new files each tick. What this means is that by. I'd reword this a little bit to say:\n``\nby default, pachd expects only the new information to be written out for each tick, and will combine that data with the data from previous ticks. If\"overwrite\"is set totrue` it expects the full dataset to be written out for each tick and will replace previous outputs with the news data written out.\nYou wording suggests to me that pachd is doing more than it actually is.. Yup, this is very much not needed now.. Small nit, Go tends to prefer NewFoo for constructors over MakeFoo.. I think you can do this a bit more tersely by doing:\ngolang\ntype CreateBranchRequest pfs.CreateBranchRequest\nBasically just lifting the type into this file so you can define methods on it. I also think we're better off just sticking with the word \"Request\" for everything and not introducing a new word \"Op\", validating and executing a request both seem like totally reasonable things to do, so I think the language fits.. suggestion\nand pipelines using the Pachyderm Pipeline System (pps). The Pachyderm. Slight tweak, you may want to specify that these are just general api features rather than something specific to pachctl.. Could we call this performOperation or maybe even runOp to be really terse. I like having a distinction between a request as a dumb data structure that lives in proto land and an Operation as a smarter interface that lives in driver, but we should be consistent with the naming.. \\\"%s\\\" can be done as %q, for quoted string. You can do this a bit more tersely with as:\ngolang\nerr := a.driver.performRequest(a.env.GetPachClient(ctx), (*CreateBranchRequest)(request)). executeTransaction makes sense to me, it would be weird if it ran both a validate method and an execute method, but if you've slimmed it down to just an execute method then I think the name fits perfectly. I think I have a slight preference for run over execute since it's shorter and we sometimes use Txn as shorthand for Transaction. I'm happy with any of these options, they're all descriptive enough and sufficiently terse.. Thinking about this a bit more, I think I like the name CommitProvenance a little bit better for this. It avoids introducing Origin as a new noun in the schema, and looking at the description of it in the comment above I think that corresponds to the intuitive meaning people will assign to . While ORIGINAL BRANCH is a little more descriptive I don't think it's worth the extra characters because it's only in very specific situations that there will be an ambiguity over what it means and very few users will encounter those situations. Those that do will likely be able to figure out that we're referring to the original branch. It's certainly something we should have in documentation.. I think repo/branch/commit is going to read a little weirdly, since we sometimes use that to represent files. Could we do repo/commit (branch), the branch feels like kind of auxilliary data to me since it's mostly going to be master under normal usage.. Let's change this to branch, similar to the logic below, while original_branch is a little more descriptive I think it's only in very specific situations that there will be multiple branches so I prefer the terseness. This would also be a good place to document that this is the branch the commit was created on.. It is slightly more idiomatic (and a whole character shorter) in go to do this as:\ngolang\nvar commits []*pfs.Commit\nThe nil value for a slice is basically identical to a slice of 0 elements, I think the only way to tell the difference is by comparing it to nil.. Git doesn't do intra-file diffing either, this is a common misconception, it does more or less the same thing we do, keeps a full copy of each version of the file and content addresses it so that if the same file is stored twice it can reference the same content. The big difference is that git has a lot of tooling around textual diffs such as the git diff command, which does textual diffs of files by default, whereas our diff-file only does inter-file diffing because it doesn't assume it's text. But in terms of storage our implementation is very similar to git's.. suggestion\nThis argument tells Pachyderm how you want the file split into chunks.. suggestion\n- Specifying both flags, `--target-file-datums` and `--target-file-bytes`, will result in each split-file containing just enough data to satisfy whichever constraint is hit first.  Pachyderm will split the file and then fill the first target split-file with line-based records until it hits the record limit. If it passes the target byte number with just one record, it will move on to the next split-file.  If it hits the target datum number after adding another line, it will move on to the next split-file. Using the example above, if the flags supplied to put-file are `--split lines --target-file-datums 2 --target-file-bytes 100`, it will have the same result as `--target-file-datums 2`, since that's the most compact constraint, and file sizes will hover around 40 bytes.. This is just changing \"will result each\" to \"will result in each.\". suggestion\ncount.txt/0000000000000005: Six. suggestion\nThis would result in one datum being processed in any pipelines that use this repo: the new file `count.txt/0000000000000005`.. I'd mention explicitly here that the reason for this is that the filename is taken into account when hashing the data for the pipeline. This will only result in 1 new piece of content being stored \"Zero\", but all of them will get reprocessed.. suggestion\nWhen creating a pipeline, you can specify one or more input repos.  Each of these will contain files.  Those files are filtered by the \"glob\" you specify in the pipeline's definition, along with the input operators you use.  That determines how the datums you want your pipeline to process appear in the pipeline: globs and input operators, along with other pipeline configuration operators, specify how you would like those datums orchestrated across your processing containers. Pachyderm Pipeline System (pps) processes each datum individually in containers in pods, using Pachyderm File System (pfs) to get the right data to the right code at the right time and merge the results.. This just fixes a small typo, but the diff is big.. PPS is the thing that does the assigning, but is reading data from pfs ofc, so I wouldn't say this is really meaningfully inaccurate.. I think you can drop essentially here, it's very much a cross-product.. :+1: that sounds even better.. Clarification on this: why is this %s* and not %s**? My understanding of s3's path listing was that it doesn't really have a concept of directories so when you do a list with a prefix you get back everything underneath that path including nested directories. Am I wrong about that? Or am I wrong about what this is doing? Or am I just wrong about everything?. Oh, I guess that's what listRecursive does above, that makes sense.. updateFileInfo seems like a bit of an odd name for this, not sure what a better one would be, maybe convertFileInfo?. Also the fact that it updates the passed FileInfo in place and returns it seems a little bit weird to me. Reading just the signature of the function I'd expect it to be making a copy.. We have a standard way to do this with errutil.IsAlreadyExistError.. I would have expected del to delete the repo in addition to the branch, since put above will create the repo if it doesn't exist. Am I misunderstanding something?. These would be better placed in src/server/pfs/pfs.go since they may well be of use elsewhere, and we want to minimize error check fragmentation as much as possible. They should also be wrapped in IsXErr functions.. This should never error, but we probably don't want to panic here and risk crashing the entire server when it's in the middle of something.. This error could be something other than a not found right? That's the most likely error but we might not want to assume that's what it is.. Note that this timestamp will advance when new files are committed even if this file isn't modified in the commit. This may break the use cases for this, which I assume is doing If-Modified-Since requests.. The meta file is the only part of this PR that I'm architecturally not on board with. It seems like it's going to interact weirdly with other stuff, we filter the files from view in the s3 handlers, but they'll still show in list-file and the like. Furthermore it seems like it only allows us to have md5s for objects that were put through the s3 api, which means in a lot of cases it won't be there. I think we should just use the file's Hash field as the ETag, the s3 spec doesn't guarantee that the ETag will be an MD5, and in fact in the case of multipart uploads it never will be, which is a pretty common case.. Yeah, we use a different glob library with some extensions, that being the main one.. This could be made more efficient by adding a GlobFileF that calls a callback with each file rather than loading them all into memory and returning them. For large filesystems this could consume a ton of memory as is.. ",
    "derekchiang": "MapReduce is just a particular paradigm for data analytics.  For instance, Spark is not MapReduce.  Pachyderm is not limited to MapReduce either; it's more general and powerful than that.  The issue is that Hadoop as a platform is the only available option for big data analytics.  Updating the title to clarify that.\n. Merge has been added in #641.\n. We now have new examples: https://github.com/pachyderm/pachyderm/tree/master/examples/\n. Fixed by #313 .\n. Some preliminary benchmarks are reported here.  Keeping this issue open as we plan to add more comprehensive benchmarks.\n. Fixed AFAIK.\n. Test github integration\n. @JonathanFraser absolutely.  PFS, being an immutable data store, makes this simple since there can't be race conditions where your data gets modified while you are downloading blocks.\n. Bumping the priority of this issue.  We think that by offloading pressure from pachd, we will see less errors related to API timeouts.. The way we will probably go about doing this is to run a PFS server on the worker node itself, so that it can just talk to localhost to upload/download data.. @JonathanFraser I just did some research into DaemonSet.  Turns out that there's currently no easy way to discover the daemon running on the local node.  For instance, if a worker is running on node A, then it'd ideally want to use the daemon on node A.  Unfortunately that's not currently possible.  You'd have to put a Service on top of a DaemonSet and then k8s will route the traffic to a random node, thus defeating the whole point.\nSee this issue for details.  A PR that exposes host IP has been merged into master but probably won't see the light until k8s 1.7.. @jdoliner actually feel free to wait till I've added all the tests before you do the review.\n. Hi @cxxly, we were not previously aware of this paper that you found, but it's a very interesting read!  The system described in the paper is largely motivated by the same problems that motivated the design of pfs/pps, although their implementation and UI differ a lot from ours (e.g. they use postgres as the backend while we use a block store like s3; they expose datasets through a SQL-like language while we use fuse; etc.).  I can certainly see us draw some inspirations from this paper down the road.\nAs for your question specifically, we have not written any paper for pfs/pps just yet.  However, we do plan to write some white papers about pfs/pps in the near future.  We will make sure to let you know when that happens!\nIn the mean time, if you would like to learn about pfs/pps in order to contribute, the best place to look is the code itself.  I suggest starting by playing with the fruit stand example in order to learn how Pachyderm works, and then read the protobuf files in order to learn the internal protocols.\nDo not hesitate to reach out to us if you have more questions!\n. 1.0 worthy? @jdoliner @sjezewski @JoeyZwicker \n. As in, should we put out an architectural document that describes the internals of pfs/pps.  Similar to this page for Hadoop: http://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html\n. @jdoliner \n. @jdoliner comments addressed\n. Fix #210 \n. @jdoliner \n. Ideally in the test, I want to check if the number of jobs spawned are indeed equal to the number of nodes, but I'm not sure what a clean way to obtain a kube client in the test is.  Thoughts?\n. @jdoliner comments addressed.  So from JobInfo I can check if shards > 0, but ideally I want to check that shards is actually equal to the number of kube nodes.  Maybe this will do for now though.\n. There's a user that wants to add about 10 commits per second (one commit per sensor reading).  Triggering a pipeline in a sub-second basis would be unnecessary and wasteful.  Having a custom policy such as \"trigger per this much data\" or \"trigger per this many commits\" would help.. Discussed offline with @jdoliner.  Here is the solution.  For the sake of discussion, say that we have a pipeline whose input repo is A and output repo is B.\nImagine the following sequence of events:\n1. A receives 1 commit, the pipeline runs successfully, B receives 1 commit.\n2. A receives 1 commit, the pipeline fails.\nNow, the following things happen:\n1. B receives a \"cancelled\" commit that contains whatever data that the pipeline had written before it failed.  This commit's parent is the last commit in B.\n2. The state of the failed pipeline job is set to \"failed\", which can be seen through pachctl list-job.\nNow, let's say that A receives another commit.  The pipeline will run with the last two commits in A as its input.  The new commit in B will have the last non-cancelled commit in B as its parent.\nCancelled commits do not trigger downstream pipelines.\nIf the pipeline is a reduce pipeline, the only difference is that we always run the pipeline with the entire history of the input repo, which is already what we are doing right now.\n. Upon further discussion, we discarded the idea of \"clumping commits.\"  Instead, we will not retry a failed job at all; the user will have to fix the pipeline by removing the offending commits or branching.\n. Fixed in #321 \n. @jdoliner 1.0 worthy?\n. @jdoliner the only solution I came up with was to have job-shim scan the content of /pfs and if there are no files (in the case of a reduce job) or if all files are empty (in the case of a map job), then instead of running the user-provided cmd, we simply report that the job has succeeded and exit.  But that does't seem particularly elegant.  Thoughts?\n. It just occurred to me that there is a class of pipelines that might be ok with empty inputs.  For instance, in my 50GB wordcount use case, I have a pipeline whose input repo simply serves as a \"trigger\".  What I do is that I make an empty commit to the input repo, which will kickstart the pipeline, which then starts downloading the 50GB of input data from the internet and putting it into pfs.\nSo in general, there is a class of pipelines who are really \"workers\" that don't necessarily operate on any inputs.  If we were to get rid of jobs with empty inputs, this class of pipelines won't work.\nMaybe there can be a flag in the pipeline spec that specifies whether the pipeline is ok with empty input?  If the flag is set, then we always respect the shards flag even if some shards might be started without any input data.\n@jdoliner thoughts?\n. @JoeyZwicker I thought the only way to start a pipeline was to add a commit to the input repo.  What did you mean by \"manually starting pipelines\"?  How would you trigger a pipeline by cron?\n. I think this mostly makes sense, but I feel like it should be more explicit.  Why don't we just have a flag in the pipeline spec that specifies whether the pipeline is ok with empty inputs?\n. I'm also in favor of having a run-pipeline command.  Having a \"trigger\" repo does not seem particularly elegant.\nBasically, I see run-pipeline being used for pipelines that have external effects (e.g. start a DDOS attack) or are non-deterministic (web scrapper).\n. Discussed offline with @jdoliner and @JoeyZwicker.  Here is what we are planning to do:\n1. Allow pipelines to have no input repos.\n2. When a pipeline has input repos, we make sure that every parallel job sees some input data. In other words, we dynamically adjust the degree of parallelism based on how much input data there is.\n3. When a pipeline has no input repos, we always respect the degree of parallelism specified in the pipeline spec (the option is currently called \"shards\" but we are renaming it to \"parallelism\" for clarity).\n4. We add a run-pipeline command that can be used to manually trigger a pipeline.  This will be necessary for running pipelines with no input repos, since they can't be triggered by commits.\n. Related #250 \n. The solution in #268 works if your docker daemon runs locally, ant not otherwise.\n. Fixed in #316 \n. @jdoliner ready for review!\n. @jdoliner I rewrote the logic in CreateJob.  Please take another look to make sure it's correct!\n. @jdoliner final look?\n. It seems a little weird to me that we simply concatenate all pod logs together.  For certain workloads, I can imagine a very confusing output.\nMaybe we can do it similar to how docker-compose does it?\nhttps://calazanblog-assets.s3.amazonaws.com/media/editor-uploads/docker_compose_logs.png\nSee that each line is started with a pod name.  In our case, pods do not have names, so maybe just pod_1, pod_2, pod_3...\nAlso, it could be cool if when you run pachctl get-logs while the job is running, you get a nice continuous output so you can watch your jobs in real time.\n. Or maybe have pachctl get-logs require two arguments which are job-id and pod, where pod is just a number from 0 to the number of pods.  Then it'd just print the log for that pod.\n. LGTM\n. @jdoliner \n. Just tried. Unfortunately the docker image does not have such thing as sudo\n. LGTM\n. LGTM\n. @jdoliner reminder to take another look at this\n. @jdoliner can you take another look?\n. @jdoliner take another look please\n. Fixed in #297 \n. Repo size increased to around 1GB before the errors occurred, at which point repo size becomes zero (as returned by pachctl list-repo).\n. Actually, it's unclear if this error was caused by me running some destructive commands accidentally.  Will try to reproduce this and close it if I can't.\n. Seems reproduce-able:\nwikipedia_50GB/\nwikipedia_50GB/file64\nwikipedia_50GB/file221\nwikipedia_50GB/file80\nwikipedia_50GB/file219\ntar: wikipedia_50GB/file80: Cannot close: Input/output error\nwikipedia_50GB/file21\nwikipedia_50GB/file213\ntar: wikipedia_50GB/file21: Cannot close: Input/output error\nwikipedia_50GB/file93\nThis time repo size gets stuck at 3GB.\n. Had to do with one of the bugs that affected #270.  Fixed in #292.\n. LGTM\n. Fixed in #368\n. Fixes #233.  LGTM\n. Log from one of the servers:\n2016-04-13T17:43:57Z INFO  shard.StartRegister {\"address\":\"10.0.2.7:650\"}\n2016-04-13T17:43:57Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.objBlockAPIServer\",\"method\":\"ListDiff\",\"request\":\"shard:30 \",\"duration\":{\"nanos\":295769774}}\n2016-04-13T17:43:57Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.objBlockAPIServer\",\"method\":\"ListDiff\",\"request\":\"shard:26 \",\"duration\":{\"nanos\":299462658}}\n2016-04-13T17:43:57Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.objBlockAPIServer\",\"method\":\"ListDiff\",\"request\":\"shard:24 \",\"duration\":{\"nanos\":303240713}}\n2016-04-13T17:43:57Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.objBlockAPIServer\",\"method\":\"ListDiff\",\"request\":\"shard:28 \",\"duration\":{\"nanos\":303935971}}\n2016-04-13T17:43:57Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.objBlockAPIServer\",\"method\":\"ListDiff\",\"request\":\"shard:11 \",\"duration\":{\"nanos\":302368382}}\n2016-04-13T17:43:57Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.objBlockAPIServer\",\"method\":\"ListDiff\",\"request\":\"shard:27 \",\"duration\":{\"nanos\":305500954}}\n2016-04-13T17:43:57Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.objBlockAPIServer\",\"method\":\"ListDiff\",\"request\":\"shard:31 \",\"duration\":{\"nanos\":309894018}}\n2016-04-13T17:43:57Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.objBlockAPIServer\",\"method\":\"ListDiff\",\"request\":\"shard:23 \",\"duration\":{\"nanos\":308860850}}\n2016-04-13T17:43:57Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.objBlockAPIServer\",\"method\":\"ListDiff\",\"request\":\"shard:25 \",\"duration\":{\"nanos\":319433566}}\n2016-04-13T17:43:57Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.objBlockAPIServer\",\"method\":\"ListDiff\",\"request\":\"shard:29 \",\"duration\":{\"nanos\":323622398}}\n2016-04-13T17:43:57Z INFO  shard.AddServerRole {\"serverRole\":{\"address\":\"10.0.2.7:650\",\"version\":\"2\",\"shards\":{\"11\":true,\"23\":true,\"24\":true,\"25\":true,\"26\":true,\"27\":true,\"28\":true,\"29\":true,\"30\":true,\"31\":true}}}\n2016-04-13T17:43:58Z INFO  protoserver.ServerStarted {\"port\":650,\"http_port\":750}\n2016-04-13T17:43:58Z INFO  pkghttp.ServerStarting {\"port\":750}\n2016-04-13T17:44:01Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"ListRepo\",\"response\":\"repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input\\\" \\u003e created:\\u003cseconds:1460568593 nanos:833831712 \\u003e \\u003e repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e created:\\u003cseconds:1460568588 nanos:696029205 \\u003e \\u003e \",\"duration\":{\"nanos\":459552}}\n2016-04-13T17:44:01Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"ListRepo\",\"response\":\"repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input\\\" \\u003e created:\\u003cseconds:1460568593 nanos:833831712 \\u003e \\u003e repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e created:\\u003cseconds:1460568588 nanos:696029205 \\u003e \\u003e \",\"duration\":{\"nanos\":38085790}}\n2016-04-13T17:44:01Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.objBlockAPIServer\",\"method\":\"ListDiff\",\"request\":\"shard:18 \",\"duration\":{\"nanos\":187771459}}\n2016-04-13T17:44:01Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.objBlockAPIServer\",\"method\":\"ListDiff\",\"request\":\"shard:20 \",\"duration\":{\"nanos\":193857639}}\n2016-04-13T17:44:01Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.objBlockAPIServer\",\"method\":\"ListDiff\",\"request\":\"shard:22 \",\"duration\":{\"nanos\":210657255}}\n2016-04-13T17:44:01Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.objBlockAPIServer\",\"method\":\"ListDiff\",\"request\":\"shard:8 \",\"duration\":{\"nanos\":240389080}}\n2016-04-13T17:44:01Z INFO  shard.AddServerRole {\"serverRole\":{\"address\":\"10.0.2.7:650\",\"version\":\"3\",\"shards\":{\"11\":true,\"18\":true,\"20\":true,\"22\":true,\"23\":true,\"24\":true,\"25\":true,\"8\":true}}}\n2016-04-13T17:44:01Z INFO  shard.RemoveServerRole {\"serverRole\":{\"address\":\"10.0.2.7:650\",\"version\":\"2\",\"shards\":{\"11\":true,\"23\":true,\"24\":true,\"25\":true,\"26\":true,\"27\":true,\"28\":true,\"29\":true,\"30\":true,\"31\":true}}}\n2016-04-13T17:44:03Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.objBlockAPIServer\",\"method\":\"ListDiff\",\"request\":\"shard:17 \",\"duration\":{\"nanos\":135179358}}\n2016-04-13T17:44:03Z INFO  shard.AddServerRole {\"serverRole\":{\"address\":\"10.0.2.7:650\",\"version\":\"4\",\"shards\":{\"11\":true,\"17\":true,\"18\":true,\"20\":true,\"22\":true,\"8\":true}}}\n2016-04-13T17:44:04Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.objBlockAPIServer\",\"method\":\"ListDiff\",\"request\":\"shard:15 \",\"duration\":{\"nanos\":139181848}}\n2016-04-13T17:44:04Z INFO  shard.AddServerRole {\"serverRole\":{\"address\":\"10.0.2.7:650\",\"version\":\"5\",\"shards\":{\"11\":true,\"15\":true,\"17\":true,\"18\":true,\"8\":true}}}\n2016-04-13T17:44:04Z INFO  shard.RemoveServerRole {\"serverRole\":{\"address\":\"10.0.2.7:650\",\"version\":\"3\",\"shards\":{\"11\":true,\"18\":true,\"20\":true,\"22\":true,\"23\":true,\"24\":true,\"25\":true,\"8\":true}}}\n2016-04-13T17:44:04Z INFO  shard.RemoveServerRole {\"serverRole\":{\"address\":\"10.0.2.7:650\",\"version\":\"4\",\"shards\":{\"11\":true,\"17\":true,\"18\":true,\"20\":true,\"22\":true,\"8\":true}}}\n2016-04-13T17:45:03Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"ListRepo\",\"response\":\"repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input\\\" \\u003e created:\\u003cseconds:1460568593 nanos:833831712 \\u003e \\u003e repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e created:\\u003cseconds:1460568588 nanos:696029205 \\u003e \\u003e \",\"duration\":{\"nanos\":4835010}}\n2016-04-13T17:45:12Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"DeleteRepo\",\"request\":\"repo:\\u003cname:\\\"workcount-input-trigger\\\" \\u003e \",\"duration\":{\"nanos\":68834}}\n2016-04-13T17:45:12Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"DeleteRepo\",\"request\":\"repo:\\u003cname:\\\"workcount-input-trigger\\\" \\u003e \",\"duration\":{\"nanos\":7085979}}\n2016-04-13T17:45:15Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"DeleteRepo\",\"request\":\"repo:\\u003cname:\\\"workcount-input\\\" \\u003e \",\"duration\":{\"nanos\":34415}}\n2016-04-13T17:45:17Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"ListRepo\",\"response\":\"repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input\\\" \\u003e created:\\u003cseconds:1460568593 nanos:833831712 \\u003e \\u003e repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e created:\\u003cseconds:1460568588 nanos:696029205 \\u003e \\u003e \",\"duration\":{\"nanos\":175587}}\n2016-04-13T17:45:22Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"ListRepo\",\"response\":\"repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e created:\\u003cseconds:1460568588 nanos:696029205 \\u003e \\u003e repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input\\\" \\u003e created:\\u003cseconds:1460568593 nanos:833831712 \\u003e \\u003e \",\"duration\":{\"nanos\":72470}}\n2016-04-13T17:45:30Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"ListRepo\",\"response\":\"repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input\\\" \\u003e created:\\u003cseconds:1460568593 nanos:833831712 \\u003e \\u003e repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e created:\\u003cseconds:1460568588 nanos:696029205 \\u003e \\u003e \",\"duration\":{\"nanos\":115114}}\n2016-04-13T17:45:32Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"DeleteRepo\",\"request\":\"repo:\\u003cname:\\\"workcount-input\\\" \\u003e \",\"duration\":{\"nanos\":21627}}\n2016-04-13T17:45:32Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"DeleteRepo\",\"request\":\"repo:\\u003cname:\\\"workcount-input\\\" \\u003e \",\"duration\":{\"nanos\":6722273}}\n2016-04-13T17:45:34Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"ListRepo\",\"response\":\"repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input\\\" \\u003e created:\\u003cseconds:1460568593 nanos:833831712 \\u003e \\u003e repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e created:\\u003cseconds:1460568588 nanos:696029205 \\u003e \\u003e \",\"duration\":{\"nanos\":138435}}\n2016-04-13T17:45:38Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"InspectRepo\",\"request\":\"repo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e \",\"response\":\"repo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e created:\\u003cseconds:1460568588 nanos:696029205 \\u003e \",\"duration\":{\"nanos\":44300}}\n2016-04-13T17:45:38Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"InspectRepo\",\"request\":\"repo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e \",\"response\":\"repo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e created:\\u003cseconds:1460568588 nanos:696029205 \\u003e \",\"duration\":{\"nanos\":2469950}}\n2016-04-13T17:45:38Z ERROR protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"CreateRepo\",\"request\":\"repo:\\u003cname:\\\"wordcount-input\\\" \\u003e created:\\u003cseconds:1460569538 nanos:229046511 \\u003e \",\"response\":\"\\u003cnil\\u003e\",\"error\":\"rpc error: code = 2 desc = \\\"repo wordcount-input exists\\\"\",\"duration\":{\"nanos\":615936}}\n2016-04-13T17:45:38Z ERROR protorpclog.Call {\"service\":\"pachyderm.ppsclient.API\",\"method\":\"CreatePipeline\",\"request\":\"pipeline:\\u003cname:\\\"wordcount-input\\\" \\u003e transform:\\u003ccmd:\\\"sh\\\" stdin:\\\"wget ftp://ftp.ecn.purdue.edu/puma/wikipedia_50GB.tar.bz2 \\u0026\\u0026 tar xvjf wikipedia_50GB.tar.bz2 -C /pfs/out\\\" \\u003e shards:1 inputs:\\u003crepo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e \\u003e \",\"response\":\"\\u003cnil\\u003e\",\"error\":\"rpc error: code = 2 desc = \\\"repo wordcount-input exists\\\"\",\"duration\":{\"nanos\":4404003}}\n2016-04-13T17:45:44Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"DeleteRepo\",\"request\":\"repo:\\u003cname:\\\"workcount-input\\\" \\u003e \",\"duration\":{\"nanos\":43072}}\n2016-04-13T17:45:46Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"ListRepo\",\"response\":\"repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input\\\" \\u003e created:\\u003cseconds:1460568593 nanos:833831712 \\u003e \\u003e repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e created:\\u003cseconds:1460568588 nanos:696029205 \\u003e \\u003e \",\"duration\":{\"nanos\":106370}}\n2016-04-13T17:45:49Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"InspectRepo\",\"request\":\"repo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e \",\"response\":\"repo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e created:\\u003cseconds:1460568588 nanos:696029205 \\u003e \",\"duration\":{\"nanos\":62971}}\n2016-04-13T17:54:50Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"ListRepo\",\"response\":\"repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input\\\" \\u003e created:\\u003cseconds:1460568593 nanos:833831712 \\u003e \\u003e repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e created:\\u003cseconds:1460568588 nanos:696029205 \\u003e \\u003e \",\"duration\":{\"nanos\":166809}}\n2016-04-13T17:54:59Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"DeleteRepo\",\"request\":\"repo:\\u003cname:\\\"workcount-input\\\" \\u003e \",\"duration\":{\"nanos\":45589}}\n2016-04-13T17:55:02Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"ListRepo\",\"response\":\"repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input\\\" \\u003e created:\\u003cseconds:1460568593 nanos:833831712 \\u003e \\u003e repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e created:\\u003cseconds:1460568588 nanos:696029205 \\u003e \\u003e \",\"duration\":{\"nanos\":119560}}\n2016-04-13T17:55:14Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"ListRepo\",\"response\":\"repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input\\\" \\u003e created:\\u003cseconds:1460568593 nanos:833831712 \\u003e \\u003e repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e created:\\u003cseconds:1460568588 nanos:696029205 \\u003e \\u003e \",\"duration\":{\"nanos\":125771}}\n2016-04-13T17:55:17Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"DeleteRepo\",\"request\":\"repo:\\u003cname:\\\"workcount-input\\\" \\u003e \",\"duration\":{\"nanos\":36350}}\n2016-04-13T17:55:20Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"ListRepo\",\"response\":\"repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input\\\" \\u003e created:\\u003cseconds:1460568593 nanos:833831712 \\u003e \\u003e repo_info:\\u003crepo:\\u003cname:\\\"wordcount-input-trigger\\\" \\u003e created:\\u003cseconds:1460568588 nanos:696029205 \\u003e \\u003e \",\"duration\":{\"nanos\":40941}}\nWhich doesn't seem to say much.\n. lmao i misspelled\n. Although this does bring up the issue that if the repo we are trying to delete does not exist, we should report an error instead of being silent.\n. @jdoliner comments addressed\n. @jdoliner \n. Fixes #281 \n. @jdoliner plz review\n. If the pps node that's running a pipeline experiences a network partition from the pfs node that's pulsing the next input commit, the pps node will be unable to run the pipeline until the network partition repairs.  If the partition is persistent, the pipeline will never get to be run.\nSo, what do you think of an exponential backoff mechanism, plus the policy that if we have backed off for more than n times (where n = 3, for instance), we crash the node by panic() in order to force the pipeline to be reassigned to some other node?\nThough, by crashing the node, we might experience the cascading failure scenario that you described offline.\nIt seems like we can't have a perfect solution unless we can separate pps and pfs.  What do you think we should do for 1.0?  Probably simply have exponential backoff and hope that all errors will be transient?\n. Closing this issue due to the fix in #306.  The rest of the solution will be provided with #305.\n. The reason that the current docker-build-proto target doesn't work on Macs is that in order to compile protobuf files in the docker image, you need to mount the pachyderm repo into the docker image.  However, this requires that the docker daemon is running locally.  For instance, it wouldn't work if the daemon is running in GCE or in boot2docker (which is common for a Mac setup).\nI found some issues that might be worth looking into:\nhttps://github.com/boot2docker/boot2docker/issues/413\nhttps://github.com/docker/docker/issues/7249\n. Actually it just occurred to me that you can do this: instead of mounting a directory into the docker image which is what we are doing right now, you'd pipe a tarball of the source code into the docker image.  So the docker image will expect to receive a tarball from stdin and will write the compiled source code out as a tarball to stdout too.\nSo we run the docker image like cat sourcode.tar.gz | docker run pachyderm/protobuf > compiled.tar.gz.  Then we extract the output tarball.  That way we don't need a remote service and can still use the docker image in a platform-independent way.\n. Fixed by #316\n. We currently support delete-file through pachctl.  To overwrite a file, you'd delete the file before you write to it.  However, this currently cannot be done through fuse, which means that you can't delete files in pipelines.\n@JonathanFraser does the pachctl route satisfy your current need?  Or do you need to be able to delete files in pipelines?\n. Briefly summarizing the FUSE semantics that we are implementing here:\n- Writing to a file from any offset results in appending to the end of the file.\nFor instance, if you have N parallel jobs and each of them is writing 100 bytes from offset 0, you end up with N * 100 bytes (as opposed to 100 bytes).\nAs another example, running echo foo > /pfs/out/file in 3 parallel jobs results in foofoofoo, as opposed to foo.\n> and >> will essentially do the same thing.\n- A file (or directory) may be deleted by invoking the unlink (or rmdir) system call.\nAs such, linux utilities such as rm, rmdir, and unlink will work as expected.\nIt's important to note that, while you will be able to rm /pfs/out/file, you will not be able to actually see the file /pfs/out/file (through ls or cat, for instance).\nAlso note that removing a file that doesn't exist in any prior commit will result in a File not found error, similar to how rm non-existent-file will fail on a unix machine.\n- To overwrite a file, you'd remove a file before you write to it.\nFor instance, rm /pfs/out/file && echo foo > /pfs/out/file will ensure that the file ends up with foo, regardless of its previous content.\n@sjezewski @jdoliner @JonathanFraser does the above make sense to you?\n. Another very important note: deletion only affects previous commits.  For instance, echo foo > /pfs/out/file && rm /pfs/out/file will have exactly the same effect as rm /pfs/out/file && echo foo > /pfs/out/file: the file will end up with foo.  While this may appear unintuitive, consider what happens if you run echo foo > /pfs/out/file && rm /pfs/out/file in 3 parallel jobs.  If deletion affects the current commit as well, the file can end up with anything between foo, foofoo, foofoofoo, or be deleted altogether.  Whereas under the current semantics, the file is guaranteed to end up with foofoofoo.\n. Partially fixes #300 \n. We need to be explicit about the version of kubernetes/kubectl that we are supporting.  See #327\n. Made some progress for 1.1 by adding tests for dynamic membership.  Removing the 1.1 label as we will be revisiting this issue post 1.1.\n. The need for this tool is getting stronger, as we are seeing more people pumping a serious amount of data into Pachyderm and bugs are being caught in production.  Most of those bugs only manifest when certain requests fail due to transient network failures, which can be simulated by such a tool.. Here's how Kubernetes does it: https://github.com/kubernetes/kubernetes/tree/master/pkg/client/chaosclient\nI think this makes sense for us as the first step as well.  Basically the faults are injected at client side: in our case it will be code under src/client/.... @jdoliner it's a lot of code so plz review carefully\n. A huge milestone in the history of this project\n. LGTM\n. @jdoliner need a careful review\n. @jdoliner ready for review now.  Expanded the scope of the PR to include handling errors in pps as well.\n. @jdoliner looks good?\n. LGTM\n. Discussed offline with @jdoliner.  Two solutions:\n1. Persist DiffInfo in PutFile, such that when membership changes, nodes can pick up the unfinished DiffInfos.  The downside to this approach is that we are introducing extra traffic on the object store by persisting DiffInfos on every PutFile.  This can be a problem if we have a lot of calls to PutFile.\n2. Instead of having the entire cluster deal with each commit in each repo, for each repo we only talk to a set of nodes.  Let's call this set of nodes the \"repo set\".  That is, we'd have a separate sharding scheme for each repo, such that a PutFile for file foo will go to the server determined by hash(foo) mod size_of_repo_set instead of hash(foo) mod size_of_cluster.  Essentially, under this approach, picking the size of a repo set comes down to making the trade-off between availability (i.e. the larger the repo set is, the more likely that a node in the repo set will fail, resulting in commits not going through), and throughput (i.e. the larger the repo set is, the more nodes we can write to/read from).\nWe are currently in favor of the second approach.\n. Will be fixed as part of #411, since then we won't have stateful API servers.\n. @sjezewski can you confirm that we want to include this in 1.1?  I'm not sure what's the difficulty/return ratio here.\n. Just a comment on the second error you were seeing: it's saying that a kubernetes job could not be created, which resulted in pachyderm not being initialized, which might have resulted in some of the errors that you saw later.  It's very likely that you are using an outdated version of kubernetes/kubectl.  As a reference, this is the output of kubectl version in our cluster:\nClient Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.0\", GitCommit:\"5cb86ee022267586db386f62781338b0483733b3\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.1\", GitCommit:\"50809107cd47a1f62da362bccefdd9e6f7076145\", GitTreeState:\"clean\"}\n. @dwhitena make launch-kube launches the kubernetes server, which is of version 1.2.0, which we support.  What you need to do is to update kubectl, since you are running 1.0.1 right now which is outdated.  The version of your kubectl needs to be at least 1.2.0.  Let me know if you need any help on that.\n. @jdoliner made a few changes.  Wanna take another look?  You were right in that the cause was some sort of off-by-1 error.  See the very last commit\n. @jdoliner take another look?  I added a mechanism for retrying both reads from and writes to object store.  I was seeing reads fail due to 500 internal server error from gce.\n. @jdoliner actually, I found that ListFiles is already returning files in a random order.  It's just that my jobs, by using golang's ioutil.ReadDir, are implicitly sorting files by filename.  So I'm guessing this is a non-issue after all.  Our user just needs to be smart and process files in random order.\nI guess the only thing we can do about this issue is to talk about it somewhere in our upcoming documents, so our users know how to avoid this flash-crowd behavior.\n. LGTM\n. Added instructions to SETUP.md.  Credits to @sambooo.\n. Quick update on this.  With the help from Daniel Mewes from rethinkdb we were able to clear some roadblocks.  So here is the story of data persistence:\n1. For a GCE deployment, we will be using GCE Persistence Disk.\n2. For a AWS deployment, we will be using Elastic Block Store.\n3. For a local deployment, we could use hostPath, but that's probably unnecessary since we are currently assuming that the local cluster is inconsequential and can be blown away.\nSo there are two things to do in order to close this issue:\n1. Update pach-deploy so it's able to generate manifest for GCE Persistence Disk and Elastic Block Store.\n2. Update SETUP.md so the user knows how to generate the said manifest, and how to create GCE Persistence Disk and Elastic Block Store.\n. Fixed in #368\n. Let's add a few words to the README saying that we are sending anonymized data to our cluster to track usage patterns, and that that's something that can be opted out by changing an environment variable in the manifest, or something along those lines.  Other than that, the code looks good.\n. Will be fixed as part of #411.\n. Can you simplify this PR a bit?  As it is it's too much code.\n. Fixed\n. @jdoliner take another look?\n. I realize that this is a little more complicated than I initially thought.  Since each DiffInfo is tied to a shard on creation, we need to ensure that, by dynamically adjusting the number of shards, we don't accidentally create dangling DiffInfos that belong to shards that no longer exist.\n@jdoliner am I right in understanding that there is no fundamental reason why a DiffInfo has to be tied to a certain shard?  As in, we could just as well hash a DiffInfo and modulo it to a certain shard.\n. @jdoliner @sjezewski do you guys have any immediate thoughts on this before I dive into it?  You guys know more about the fuse driver than I do.\n. @jdoliner https://github.com/pachyderm/pachyderm/blob/9a568816576612475621cfed986b8d98f707ca4e/src/server/pachyderm_test.go#L366-L411\nIt hangs on ListCommit because ListCommit does not return cancelled commits by default.  The reason the commit was cancelled was that two of the three parallel pods failed, which I found out by looking into server logs.\n. Rather curiously, if I use dd instead of echo > in the test, the test passes: https://github.com/pachyderm/pachyderm/blob/fix-355/src/server/pachyderm_test.go#L494\nSo it seems like echo > interacts with the file system in a way that's not correctly handled by our driver?\n. Actually I was using >> instead of >.  In either case the test fails.\n. A new development on this.  So I found out that, in the echo test, 2 of the 3 PutFiles returned prematurely.  Specifically, they returned here, meaning that 2 of the 3 PutFiles somehow got EOF from FUSE, instead of the actual bytes foo.\nThis is very bizarre.\n. I think I've identified the issue.\n>> means append, which means that the file system will try to write the data at an offset that is the current length of the file.\nWhich means that this check will fail if you are doing echo foo >> file and there is already some data in file, since h.written will be zero and request.Offset will be greater than zero.\n. And I was wrong earlier in that using > instead of >> does make the test pass, which makes sense since > will result in the file being written at an offset of zero.\n. This will be fixed along with #303\n. Ah, we've seen these before.  These are not real test failures.  Since we run tests in parallel, sometimes you get these failures due to kubernetes being slow.  These failures go away if you run the tests sequentially or run them on a beefy machine.\n. LGTM\n. LGTM\n. Gonna merge this cuz this fixes something that I'm seeing.\n. #393 added a way to have a complete view of a repo in a job.  Refer to the updated pipeline spec for details.  Feel free to reach out to us if anything is unclear!\n. @jdoliner before I merge this, can you take another look at this line and make sure that it actually makes sense?  That's the change that I'm least sure about.\n. @jdoliner another review please.\n. @jdoliner @JoeyZwicker @sjezewski can one of you guys review this?  I'm still testing the AWS setup but I don't expect major changes.\n. @jdoliner review plz?\n. @JoeyZwicker changed the example.  Another look?\n. @jdoliner \n. @jdoliner \n. @jdoliner comments addressed.  Notably, pachctl now directly reads from doc/pipeline_spec.md to include the entire thing in the help string for create-pipeline.  Wanna take another look?\n. LGTM\n. @jdoliner \n. LGTM\n. Another notable change in this PR:\n3) If a job's output commit has a parent commit (due to one of its inputs being incremental), we expose the parent commit to the job under /pfs/self.  This allows a pipeline to see the result of its previous runs, thus enabling more efficient reduce use cases.  For instance, the second stage of this pipeline will be able to run incrementally (as opposed to summing up all numbers over and over again).\n. 4) \"reduce\" no longer means partitioning by files.  Rather, we only partition by top-level objects (files or directory).  That is, if your repo looks like:\nrepo\n  |--foo\n      |--foo1\n      |--foo2\n  |--bar\n      |--bar1\n      |--bar2\n  |--buzz\nAnd you use the reduce strategy, then your jobs will see the entire foo directory and/or the entire bar directory and/or the buzz file.\n. @jdoliner wanna take another look?\n. Closed via #617.  More performance figures will be tracked via #199.\n. This seems to be what we want:\nhttps://github.com/kubernetes/kubernetes/blob/release-1.0/docs/user-guide/downward-api.md\n. @jdoliner https://github.com/pachyderm/pachyderm/pull/451\n. LGTM\n. Reminder that we probably want some documentation stating the fact that the commit ID is available through an env variable.\n. The document above captures the offline discussions we've had over the past week.\n@jdoliner could you take a look and make sure that everything makes sense?\n. Moving this to v1.2.  We will be focusing on stabilization for v1.1. \n. @sjezewski could you take a look?\n. @erikreppel we've increased the backoff rate and added more logging so that if this error happens again, there will be more info to work with.\n. Will fix with #411 \n. Or it shouldn't.  Won't fix.\n. Fixed in #433 \n. Yeah showing grpc error might be fine.  I guess I just wanted an error message that would make it clear that the commit does not exist.  The current error message is indecipherable for users who don't know what diffs are.\n. Actually after a little more thought, I think showing grpc errors on the client side is probably not a good idea.  It just makes the error messages ugly and the fact that we are using grpc is an implementation detail anyways.\nI found this function that can be used to expose the original error (i.e. getting rid of the grpc wrapper error).  We don't have to do it as part of fixing this issue, but that's probably what we want to do at the CLI in the long term.\n. LGTM\n. Fixed in #437 (which is wrongly named fix-436 lol).\n. Actually this is not completely fixed.  You can still delete a repo when a running pipeline is outputting to it.  That shouldn't be allowed.\n. Fixed in #440.  Now you will should be able to see that your pipeline errored if you removed your pipeline's output repo, by using list-pipeline or inspect-pipeline\n. @jdoliner I like the idea of monitoring the amount of downloaded data in order to detect stragglers.\nI had a similar thought about breaking up input into more shards.  Basically we'd update the FilterShard structure such that it can contain a pointer to another FilterShard.  Let's call that a \"FilterShard chain.\"\nSo, let's say that we want to break a container's input into two equal parts.  We'd simply generate two FilterShard chains by chaining the original filter shard with 1) a shard with modulus 2 and number 0, and 2) a shard with modulus 2 and number 1.  And we can trivially update BlockInShard and FilterInShard to accommodate for the chain.  Basically, these methods only return true if the object matches all FilterShards in the chain.\n. An update: we have migrated PPS to a model where pods request \"chunks\" to process.  This model naturally lends itself to straggler mitigation: if a chunk takes too long to process, it can be split into multiple smaller chunks so the other pods can share the work.. Seems to only occur when jobs fail (i.e. when output commits get cancelled).\n. Could not reproduce\n. @sjezewski I'm just changing the output of inspect-job.  Here is an example of an output:\n\u279c  pachyderm git:(fix-418) \u2717 pachctl inspect-job 9f3a899161c8451da550bbe4813f0c15\n{\n    \"created_at\": {\n        \"nanos\": 1.0490792e+07,\n        \"seconds\": 1.46473184e+09\n    },\n    \"inputs\": [\n        {\n            \"commit\": {\n                \"id\": \"8b4f172520994f4fbfc5376a114394d9\",\n                \"repo\": {\n                    \"name\": \"TestJob_data_0bc02786db63\"\n                }\n            },\n            \"method\": {\n                \"partition\": 1\n            }\n        }\n    ],\n    \"job\": {\n        \"id\": \"9f3a899161c8451da550bbe4813f0c15\"\n    },\n    \"output_commit\": {\n        \"id\": \"6a6c9bfd84734b6ca3169de5c6fd65df\",\n        \"repo\": {\n            \"name\": \"job_9f3a899161c8451da550bbe4813f0c15\"\n        }\n    },\n    \"parallelism\": 4,\n    \"pipeline\": {},\n    \"state\": \"JOB_SUCCESS\",\n    \"transform\": {\n        \"cmd\": [\n            \"bash\"\n        ],\n        \"stdin\": [\n            \"cp /pfs/TestJob_data_0bc02786db63/* /pfs/out\"\n        ]\n    }\n}\n. PICLGTM\n. From the look of it, it seems like pachd nodes are unable to communicate with etcd for some reason.  Port 2379 is etcd's port.\n. Closing since there is nothing to be done.\n. To be clear, this PR merely ensures that the cluster will be functional after membership changes.  Outstanding requests are still going to be interrupted when membership changes, which is very much less than ideal, especially considering the fact that we currently don't have a way to restart a pipeline after it fails.\n. @jdoliner could you review?\n. lmao\n. The PR was merged.  Also, it wasn't about corrupted diffs.  It was about nodes using the local object store got reassigned shards, but couldn't actually find the shards written by other nodes since the local object store is not shared among nodes.\n. \n. \n. epic\n. \n. Funny, i literally did the very same thing on my branch just now.  LGTM\n. \n. \n. \n. \n. That's a good point.  We should probably have a whitelist (or blacklist) of errors that we know to retry (or not).  Lemme investigate a bit.\n. @jdoliner alright, now we are only retrying on a whitelist of errors.  Wanna take another look?\n. @jdoliner another look?  Also can't merge without a decent meme\n. Hi @ChaiBapchya, I think we merged a patch a couple days ago that should've fixed this issue.  Is this a recent build of Pachyderm that you are running?\n. We definitely need more documentation for pipeline states.  We also currently have a mechanism for inspecting pipeline failures (pachctl inspect-pipeline), although that needs to be documented as well.\n. LGTM\n. Can't find the PR, but apparently this has been fixed.\n. \n. @ChaiBapchya You can absolutely use Python scripts in your pipeline.  There are two ways to do it:\n1. Set cmd to python and have stdin be your python script.  This approach is ideal for very short python scripts where you don't mind putting them into the pipeline spec.\n2. Use your own Docker image.  Basically it comes down to three steps:\n   1. Build your own Docker image which contains your python script.\n   2. Set the image field to your own Docker image.\n   3. Set cmd to your python script.\n. @ChaiBapchya yeah we definitely need to work on our documentation.  Wanna move the conversation to Slack so we can troubleshoot your cluster?\n. @ChaiBapchya hi sorry for the delay.  We are experiencing some technical issues with Slack so let's just do it here.  You probably just want to re-launch your cluster altogether.  This usually does the trick:\nmake clean-launch && make full-clean-launch && make launch\n. memelgtm\n. @jdoliner no.  Should it?\nTo be clear, you can still read from stdin the exact same way as before.  You just don't provide the -f flag.\n. This issue is closely related to #606: if we were to run the computation in memory, then we'd need a way to recover lost work since memory is not persistent.  (or alternatively, we can replicate data in memory to provide fault tolerance).\nAlso related to #624: running pipelines in memory opens up a good opportunity for streaming data among pipelines.\n. For a large class of pipelines, storing the intermediate results in memory is not necessarily a requirement.  The only requirement is that the intermediate results are not persisted, i.e. that they are removed from the storage layer as soon as they have been consumed.  Renaming the issue to clarify the requirement.. Prettier == better\n\n. I'm assuming that you have tested that it works.  \n. \n. @jdoliner can't you just not do t.Parallel() in the test, so that the test doesn't interfere with other tests?\n. We do have the code right now which is supposed to do exactly this: https://github.com/pachyderm/pachyderm/blob/master/src/server/pps/server/api_server.go#L868-L872\nHowever I have run into instances where jobs failed to be removed.  I did not pinpoint the cause though.\n. Yeah I definitely remember deleting a pipeline, restarting the cluster, and some jobs were still there.  I'm not sure if that was 100% reproducible though.\n. When I'm creating multiple pipelines together, sometimes it takes a while, and there is no way for me to tell if it's making progress or if something has gone wrong.  Some other commands can definitely benefit from progress reporting as well though.\n. That makes sense.  In my case I'm creating three pipelines which takes around 5-10s.  But yeah I'm guessing that's not necessarily long enough to justify progress reporting.\n. @jdoliner \n. The above example demonstrates how to make your custom image compatible with pachyderm.  Another way is to inherit from the job-shim image directly like this: https://github.com/pachyderm/pachyderm/blob/master/examples/fruit_stand/Dockerfile\n. \n. @jdoliner LGTY?\n. @jdoliner comment addressed\n. Makes sense.\n\n. Here is what I've learned from debugging with @tv42, about why a job that writes a large amount of files can use a huge amount of memory:\n1. The Go FUSE driver internally uses a pool of buffers, so that it doesn't have to allocate a new buffer every time it needs one.  However, the pool is implemented such that Any item stored in the Pool may be removed automatically at any time without notification (from the godoc).  For instance, if your program is using a lot of memory, the Go garbage collector can decide to recycle the items in the pool.\n2. For reasons yet to be determined, when you write a lot of files in your jobs, you end up with a lot of open gRPC connection (see #588).  These gRPC connections eat up a lot of memory, causing the garbage collector to run frequently, at which point it might collect buffers from the aforementioned pool.\n3. The implementation of the FUSE driver is such that, when there aren't enough buffers in the pool, it just allocates new buffers and put them into the pool.\n4. Step 2 and 3 just keep cycling.  That is, FUSE keeps allocating new buffers and putting them into the pool, whereas the garbage collector keeps taking them out.\n5. Now, apparently the garbage collector does not immediately free the buffers it collects (I'm going to read more about Go's GC to figure out why).  As a result, we get more and more buffers stuck in memory waiting to be freed.  That's why the memory usage keeps growing.\n. LGTM other than potentially changing the sleep time.\n\n. \n. \n. An update on this.  gRPC opens a new TCP connection whenever a RPC is invoked.  This in itself is not problematic, since the port will be reclaimed after the RPC is finished.  However, the way TCP works, closed connections can remain in the TIME-WAIT state for a few tens of seconds, waiting for out-of-order packets.  Since we are making RPCs very rapidly, we are observing a large amount of TCP connections in the TIME-WAIT state.\n\nHowever, there is currently no evidence that this is a problem.  The aforementioned high memory usage is mostly caused by GC, as described here.\nA related issue: https://github.com/grpc/grpc/issues/5582\n. Reopening and renaming this issue to reflect the new finding that it's the pachd servers that are opening too many connections, as @tv42 pointed out.  We haven't run into a noticeable problem with that yet, but it's definitely very much less than ideal.\n. @tv42 yup, we have since realized that gRPC does try to reconnect by default.  Just merged #591 which allows us to reuse connections.\n. If the pod restarts and results in a membership change, then connections will be evicted.\nIf the pod restarts immediately without a membership change, then grpc will simply redial.  A gRPC connection only ever shuts down if either side explicitly called Close().\nBut yeah, I can certainly add that test case.\n. @jdoliner no meme no merge\n. @jdoliner Interesting.  I was not aware that etc/user-job/Dockerfile exists.  We should probably refer to it somewhere in our documentation.\n. An update.  Seems like the issue that @sjezewski was seeing was just Pachyderm/Kubernetes being slow.  After all, hundreds of containers are created if you run the generate script.  I just tried and the pipelines do complete successfully after a while.\nI will take a look and see if @JoeyZwicker 's issue still exists.\n. Turns out there were a couple bugs in PPS/PFS that together caused this issue.  Not going into the details here, but the issue has been fixed on the refactor branch.\n. @jdoliner added some code to fix a bug where if DeleteShard comes in while a pipeline is being restarted, the pipeline ends up getting restarted anyways when it shouldn't.  Another LGTM?\n. nah one merge one meme\n. Yes.\n. @jdoliner sure, I can look into that\n. Hi @ShengjieLuo, did you format the persistent volume and rm -rf before you launch pachyderm?  RethinkDB requires a completely empty disk in order to run.  The instructions can be found here: https://github.com/pachyderm/pachyderm/blob/master/SETUP.md#format-volume\nAfter you've done that, you should be able to relaunch the cluster with make clean-launch && make launch.  Let me know if you run into any issues after that.\n. @ShengjieLuo yeah it's a common gotcha: the disk utility usually dumps a bunch of garbage files into a hidden lost+found folder, so your disk won't be completely empty unless you explicitly rm -rf.\nAlso, make sure to launch with the correct manifest: make clean-launch && make MANIFEST=manifest launch where manifest points to the file you generated via make google-cluster-manifest\n. Hi @SoheilSalehian the error messages you are seeing can be benevolent.  The fact that you can't use the mounted directory might suggest some other issues.  Could you elaborate on what you meant by you can't use the mounted directory?  How are you using the mounted directory?\n. Glad to help!  IIRC umount is safe, although you can also simply terminate pachctl mount.  Let me know if you run into problems.\n. Hi @ShengjieLuo, so I assume that you used make google-cluster to create the cluster?  That's where the firewall rule would be added.\nCan you double-check that 104.197.165.43 is indeed the external IP address of the instance?  Each GCE instance has an external and an internal IP address.  You want to make sure to use the external one.\nAnother thing to check: since you are running 2 pachd pods on 3 GCE instances, it's possible that you just happened to be connecting to the instance that does not have a pachd pod.  So you might want to try using the IP addresses of the other two instances.\n. @sjezewski do I need to make release-manifest too?  I only updated the dev manifest.\n. In the process of adding a README for wordcount.\n. To clarify, we may not need etcd anymore in 1.2.\n. We should look to using local disks (ideally SSDs) for rethinkdb instead of GCE persistent disks / elastic block storage, since the latter is networked and therefore has much worse performance.\n. One minor comment but otherwise LGTM\n\n. \n. One can however argue that, if you care about having a pipeline run as soon as some data comes in, you can just split the input data into many small commits, as opposed to one single large commit.\nHowever, creating many small commits can be unwieldy in many situations.  Sometimes you really just want your data to sit in one single commit, so it's easier to reason about your data.\nOne alternative solution is to introduce something like a \"commit set\", which is basically a large commit that composes of many small commits.  Users would use \"commit sets\" like they use commits today; and commits become more like a unit of streaming data.\n. Note that for the purpose of #606, it may be sufficient to provide job-level lineage.  For instance, if 3 out of 4 parallel jobs succeeded and 1 failed, it's sufficient to be able to keep the output of the 3 successful jobs and rerun the failed job, even though we might have to redo the work that the failed job already did.\n. It's arguable whether it's \"simplified.\"  The code is shorter but we are now using awk... but anyways I'm fine either way.\n\nIgnore top caption\n. \n. well deserved meme\n. @jdoliner can this issue be closed now that #694 has landed?\n. LGTM but no meme for PRs this short\n. @jdoliner can you summarize what this PR does?\n. LGTM, other than maybe adding some comment to the protobuf file explaining what fullFile and DiffMethod do.\n\n. LGTM\n\n. The Kubernetes people seem to have an idea of what's happening: https://github.com/kubernetes/kubernetes/issues/28709#issuecomment-234149947\n. I have a record number of two one-character PRs at this point I think.\n. Not sure if this PR deserves this one but:\n\n. Ah, nice catch.  Thanks @angl !\n. LGTM\n\n. @ShengjieLuo thanks for bringing the performance issues to our attention!  So far we've been spending most our engineering efforts on the correctness of the system , and not as much on performance, so I'm sure there's a lot of room for improvement there.\nI agree with @jdoliner that the performance difference between experiment 1 and 2 is mostly due to the frequent writes to S3.  In general, Pachyderm is much better at handling large files than handling many small files, as is the case for the wordcount example, where each unique word results in a small file.\nAs for the performance difference between 2 and 3 (local vs Hadoop), I'd attribute it to the fact that both the map.go program and the reduce bash script make use of the local file system, while the Hadoop program probably does the bulk of the work in memory.  Note that in theory, Pachyderm can run in memory as well: https://github.com/pachyderm/pachyderm/issues/546\n. Nice table!  But just to be clear, in case someone reading this thinks that Hadoop is 174 times faster than Pachyderm: it's an unfair comparison because the Hadoop setup runs purely in memory, while the Pachyderm setup sends data over the network.  It'd be more fair if the Hadoop setup was also writing data to a remote location.\n. @ShengjieLuo oh and for your follow-up question: Pachyderm only uses one S3 bucket by default.  It's just that currently each time we write a small file, we are sending a write request over the network.  What @jdoliner was suggesting was that we can pack the write requests into one big write request, so as to reduce latency introduced by the network.\n. @ShengjieLuo ah, I think I misunderstood what you were saying before.  Somehow I thought that the first experiment involved writing to S3, but from what you just said, that doesn't seem to be the case.\nSo just to be clear:\n- In the first experiment, you were running Pachyderm on a single node, with no S3 involved.  This means that Pachyderm was writing data to the local disk.\n- In the second experiment, you were running the map and reduce programs manually, on a single node.\nIf that's the case, then what you found out is that adding the Pachyderm layer alone causes the processing time to jump from 2.5 min to 29 min, is that correct?  If that's the case, that's certainly very concerning, and we should be looking into this as soon as possible.\n. @ShengjieLuo thanks for taking the time to evaluate Pachyderm!\n. @JoeyZwicker could you review?\n. LGTM\n. LGTM\n. Hi @ShengjieLuo , from what you described, seems like you have 2 pachd pods running on 5 nodes?  If that's the case, then the pachd pods can be on any of the 5 nodes, so you can try the public IP address of each of them.\n. FYI here is a tutorial on pull requests: https://help.github.com/articles/using-pull-requests/\n. @JonathanFraser sorry that you ran into this.  If you have the error messages could you post them?\n. Making pachctl test-able is basically a two-step process:\n\nRefactor the code such that instead of calling fmt.Print*, it calls custom Print* functions.\nThe custom Print* functions use stdout/stderr by default, but in tests we'd use custom output buffers so we can verify the output.\n\nReference: https://github.com/upspin/upspin/blob/fc03dec61223732cc6d46d83b55d74387c1ff1f3/cmd/upspin/cmd_test.go#L66-L90. Fixed\n. @JonathanFraser just merged #715 which fixed this new issue that you brought up. \n. Fixes #711 \n. Nice catch!\n. Thanks for the evaluation.  I think this confirms the finding in #701 that Pachyderm as it is introduces significant latency to workloads where many small files are written.  We will be looking into this issue soon.\nMeanwhile, would you be able to evaluate Pachyderm against a workload that operates on a few big files?  Something like writing 10GB of data into one single file.  In that case, I'd hope that Pachyderm maintains the same level of throughput as the filesystem allows .\n. For the purpose of this evaluation, a simple script like this will do:\ndd if=/dev/urandom of=/pfs/out/file bs=1M count=10000\nThis just writes 10GB of random data into PFS.\nOn Wed, Aug 24, 2016 at 1:13 AM Luo Shengjie notifications@github.com\nwrote:\n\n@derekchiang https://github.com/derekchiang Could you give me some\nrecommended applications that involving little file operations but large\nfile size? I would work on it tomorrow.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/755#issuecomment-241989668,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABduxpAGRG_Zznw-Hf7XR8sld8j3Wa1Cks5qi_0NgaJpZM4JrpRw\n.\n. \n. Code looks good, but can you update the pachctl create-pipeline/create-job helper messages to indicate that this functionality exists?\n. LGTM when CI passes.\n. Another thought on this.  If we ever decide to prefetch all input data for a given job, then we don't even need FUSE anymore.  /pfs can just be a normal file system.  Then we can either send /pfs/out when the job finishes, or have only /pfs/out run on FUSE.  We can skip a lot of overhead/bugs that way.\n. @ShengjieLuo currently Plan B is how it is.  Every read/write involves a roundtrip over the network.  See #777 for some ideas about how we might optimize this.\n. Hi @ShengjieLuo , could you explain what you meant by Several dockers are built by Pachyderm on one node for the test.?  Did you mean that you are running the pipeline with a parallelism greater than 1?\n\nIn any case, what you observed is similar to something we've seen in the past, where the machine runs out of memory and the kernel kills the job.  Then kubernetes and Pachyderm fail to detect that the job has been killed, so the job is stuck in the \"fake running\" state that you were talking about.\nTo verify if that's the case, I'd check the syslog (/var/log/syslog) on the host machine and look for OOM errors.\n. Fixed.\n. Fixed\n. Removed a couple.\n. Fixed.\n. LGTM\n. Fixed.\n. Fixed in the refactor branch.  Will keep the issue open until we merge it in.\n. - To generate CommitInfo which has a ParentCommit field, we have to go to the database to find the parent commit.  But once all commits are of the form master/2, parent commits should be self-evident.\n. Fixed by #831\n. Independently wrote the same code... so LGTM I guess\n. I think this has been fixed in master?\n. Dup #808 \n. Discussed with @jdoliner offline.  We will be hashing the primary key to make it shorter.\n. That's like the opposite of LGTM?\n. Hi @willguxy , it's not a bug, it's a feature!  Pachyderm automatically detects when the input size is too small to be distributed to the specified number of parallel jobs, and reduce the parallelism automatically to save resources.  The number you specified (2 in this case) serves as an upper bound: Pachyderm promises to spawn no more than this many jobs.\nIf you had a larger input (~20Mb, for instance), you would've seen 2 jobs.  Let us know if you have more questions!\n. Hi @willguxy, your use case makes sense.  Currently, we always partition data for parallel jobs; there isn't a way to run parallel jobs and have them all see the same set of input.\nWhat I'd suggest you to do is to create one pipeline per model.  These pipelines will have the same input, but run with different model parameters.  So every time an input commit comes in, all these pipelines will be triggered with the same input.\nDoes that make sense?  Does that work for your use case?\n. Hi @willguxy that makes sense.  I think I have a pretty good understanding of your problem now.  Here is what you can do:\nFirst all, you create two repos: raw_data and model_parameters.\nThen, you create a pipeline that runs your model.  The inputs section of your pipeline spec will look like this (refer to the pipeline spec for a full description of all the flags):\n\"inputs\": [\n    {\n      \"repo\": {\n        \"name\": \"raw_data\"\n      },\n      \"method\": \"global\"\n    },\n    {\n      \"repo\": {\n        \"name\": \"model_parameters\"\n      },\n      \"method\": {\n        \"partition\": \"file\",\n        \"incremental\": true\n      }\n    }\n]\nTo briefly explain:\n- The global flag tells Pachyderm that any job should see the entirety of this repo, which is what you want for the raw_data repo.\n- The method of the model_parameters input repo tells Pachyderm that this repo should be partitioned by files.  Here I'm assuming that in the model_parameters repo, you will have a number of files like model1.json, model2.json, model3.json, etc.\nNow, the pipeline will spawn N jobs, where N is bounded by the parallelism you specify.  Each of these jobs will see a subset of your model parameters, and the entire raw_data repo.\nDoes that sound like what you want?\n. Yup, a block is typically 8MB of data, so if your parameters are small, then they might all fit in one block, and as a result only one job gets spawned.  By using the file partition method, you are telling Pachyderm to partition input by files, regardless of file size.\nNote that Pachyderm is JSON-aware, meaning that you can also have a file called models.json that contains a number of JSON objects, each of which corresponds to one model.  Then you'd use the block partition method, and Pachyderm will make sure that each block contains one JSON object.  But that's kind of an advanced use case; I'd just use one file per model for simplicity.\n. I was mistaken when I said that each JSON object is stored in its own block.  Currently Pachyderm still groups JSON objects into 8MB blocks.\nThis is an issue when you have a pipeline that gets a small file (< 8MB) which contains a number of JSON objects, and you want to spawn parallel jobs such that each job gets some of the JSON objects.  But currently you can't, since all the objects are in one block.\n. duplicated\n. Still WIP\n. @jdoliner comments addressed.  LGTY?\n. Code LGTM.  CI is not passing though.\n\n. Merged this branch into my new-pfs-api branch.  Will close this PR when I merge mine so we don't accidentally miss this fix.\n. There have been 4 point releases since this issue was opened, during which a lot of PPS bugs were fixed.  Gonna close for now unless this issue can still be reproduced.. @jdoliner actually the test was never enabled.  I just enabled it and turns out it still won't pass.  Reopening.. \n. We have TestScraper in our example tests already.  Does that solve the issue?  Feel free to reopen if it doesn't.. @jdoliner proposed the following solution:\n- Have a counter field in the commit document, which is the number of diffs this commit contains.\n- PutFile increments this counter field before inserting diffs.\n- FinishCommit blocks until the actual number of diffs in the database matches the counter.\nThis solution leads to another question: what happens if PutFile fails after updating the counter, but before finishing inserting the diffs?\n. We have TestOpenCV in our example tests already. Does that solve the issue? Feel free to reopen if it doesn't.. \n. I can get to it\nOn Wed, Sep 21, 2016 at 7:21 PM Joe Doliner notifications@github.com\nwrote:\n\nAnyone got bandwidth for this?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/870#issuecomment-248794378,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABduxtUFScNjbWt_YPNtHkGFreg7rKl-ks5qseYsgaJpZM4KDc_r\n.\n. I think this issue has already been fixed.  Currently you can switch versions for readthedocs.  Feel free to reopen if I'm wrong.. Forgot about this.  Gonna close for now.. \n. LGTM\n. Yeah the reason I don't want it to be part of CI is that it can take a while to run a production workload and I don't want that to slow down our dev process.  But I do very much agree that it should be as automated as possible.  It can also run asynchronously with regard to CI.\n. I think this issue was fixed and then subsequently unfixed.  We added an in-cluster registry and a way to push images to the said registry (via create-pipeline --push-image), then we decided that it's not as useful as we thought, so we removed it.\n\nAm I summarizing this correctly?  @jdoliner \nFeel free to reopen if it's still an issue someone wanna fix.. This was fixed by #1059 .. A couple non-trivial benchmarks have since been added: https://github.com/pachyderm/pachyderm/blob/master/src/server/pachyderm_bench_test.go\nGonna close this for now.  What's left to do is to run the benchmarks regularly; that will be tracked by #813 and #892 . Here is a proposal for how the infrastructure might look like:\nFirst, we create a GCE or/and AWS cluster that's shared among Pachyderm committers.\nSecond, we write a script that does the following in order:\n1. Check if there's already a benchmark running on the cluster.  If so, prompt the user to see if they want to kill the running benchmark.  We only want one benchmark to be running at a time because we don't want multiple benchmarks to interfere with each other.\n2. Build the pachd and job-shim images locally.\n3. Push the images to some personal/private repos on docker hub.  The specific repos will be specified via environment variables.  For instance, I might push them to derekchiang/pachd:latest and derekchiang/job-shim:latest.\n4. Generate a manifest that refers to the private images.\n5. make launch: launches Pachyderm on the cluster\n6. make bench:  creates a kubernetes job that runs the benchmark.\n7. Report benchmark numbers.\n8. make clean-launch: removes Pachyderm from the cluster.\nThe script assumes that your kubectl has been authorized to talk to the GCE/AWS cluster, and that your docker has been authorized to push to the private repos of your choice.\nSo, ideally, a developer will just need to run the script to benchmark Pachyderm in a real cluster.\n. I think everyone brought up some good points.  Taking these points into consideration, how does this sound:\nThe script takes the following parameters:\n- A regex expression that specifies the benchmarks you want to run.  This regex expression will just be passed to go test verbatim.  Defaults to *, meaning running all benchmarks.\n- The cloud provider to use.  Defaults to GCE.\n- The number of nodes to spawn up.  Defaults to 4.  Each node will also default to some reasonable specs such as 4 vCPUs and 16GB memory.\n- The number of times the benchmark should be run.  Defaults to 1.\n- Names of the private repos to use, e.g. derekchiang/pachd:latest.  No default.\nThen, the script does the following:\n1. Build the pachd and job-shim images locally.\n2. Push the images to the specified private repos on dockerhub.\n3. Generate a manifest that refers to the private images.\n4. Launch a GCS/AWS cluster.\n5. Launch Pachyderm on the cluster.\n6. Create a kubernetes job that runs the benchmark.\n7. Record the performance numbers.\n8. Delete the cluster (including the bucket).\n9. Repeat step 4 to 8 N times, where N is the number of times the user wants to run the benchmarks.  By relaunching the cluster, we ensure that the benchmarks always start from a clean state, e.g. no pre-existing blocks, etc.\n10. Generate the final report.  This report should include not only the performance numbers, but also the parameters that the script started with, e.g. number of nodes, node specs, etc.\n. #1078 partially addresses this.  There are more ways of prefetching that we can do, such as prefetching data at the pachd layer.  Gonna move this off 1.3.. @jdoliner has since proposed a more general framework (the one where we have partition and incrementality).  @jdoliner do you wanna describe it here or open a new issue?. Closed by #1022 \n. Am I right in understanding that /pfs/out right now presents the exact same content as /pfs/prev before any writes?\n. #1078 makes /pfs/out a real (i.e. non-FUSE) directory, thereby making /pfs/prev necessary again.  Closing.\nP.S. we have discussed some designs that would remove /pfs/prev in favor of a better abstraction, such as #1057.. I don't think using python for scripting is that much different than using bash for scripting, and we do have quite a number of bash scripts.  Using golang for scripting purposes seems a little strange to me.\n. The latest tag is just a default; you can specify whatever images you want to use.  There doesn't seem to be any downside with using Always though?  As in, if you use a versioned tag like 1.2.1, then Always does the same thing as IfNotPresent.  If you use something like latest, then Always does the thing you want which is that it will pull the image again even if there's already a latest image locally.\nAnyways, sounds like you guys would both rather have this be golang/bash, but don't have a strong objection to merging this in for now so we can work on the actual performance stuff.  I'm certainly down to take a look at this again later once this script gets more users and more feedback.\nCan I get a LGTM?\n. \n. Update: updating the dependencies doesn't seem to solve the problem.\n. Update: This page seems to imply that GCS can have problems with burst traffic (e.g. > 1000 writes per second): https://cloud.google.com/storage/docs/best-practices#traffic\n. Looks like this issue has been identified.  Basically we need to compile Pachyderm with golang 1.7.3 or above.  Reopening so we remember to do that.\nhttps://github.com/GoogleCloudPlatform/google-cloud-go/issues/389#issuecomment-255881983\n. @JoeyZwicker I agree that if the docs are not meant to be read on Github, then they should probably be less discover-able.  When I try to find the docs for a Github project, the first place I look at is usually the doc/ directory.  I'm not really sure where we should hide it though.\n. What's the motivation for this?. gRPC integration with OpenTracing: https://github.com/grpc-ecosystem/grpc-opentracing/tree/master/go/otgrpc. Apparently things are breaking.  Gonna break down this PR into smaller PRs to isolate the issue.\n. I think I might've stumbled upon a gRPC issue that could explain the slow upload/download.\nTo summarize, gRPC-go currently uses its own incomplete implementation of HTTP/2, and it hasn't implemented dynamic flow control.  Specifically, the window size is hardcoded to 16KB (!!), which significantly limits throughput in high latency networks, since you need to pay one RTT per 16KB.\nOn the bright side, there seems to be work under way to address this issue, so we can probably expect a fix in the next release of gRPC.\nIf we don't want to wait (or if we don't trust gRPC), we can also add a new server-side implementation for GetFile and PutFile that doesn't use gRPC.  For instance, the new implementation can simply speak HTTP.  Then we can update the Go client (which is used by pachctl) to talk to the HTTP API.\n@jdoliner @msteffen thoughts?. Since the issue is from a while ago, I think the first step is to confirm that get-file/put-file is still slower than what you would expect.  That seems to be the case from my personal experience, with or without port-forwarding.. So I ran some benchmarks and was able to confirm that uploading reasonably big files (10MB+) over plain HTTP is about 100% to 150% faster than uploading the same files over gRPC.\nI then investigated the potential solution which is to implement GetFile/PutFile on HTTP.  However, there currently isn't a satisfying solution for running HTTP and gRPC servers on the same port.  See this and this and this for details.\nCurrently the best solution for multiplexing HTTP and gRPC onto the same port seems to be to use cmux, which is used in etcd and CockroachDB so presumably it's a reasonably solution.  However, we consider re-implementing GetFile/PutFile and introducing cmux to be too high of a cost.\nTL;DR: we've decided to wait for gRPC-go to fix the issue upstream, as opposed to implementing our own solution.  In the mean time, if you need the best performance for uploading files, use URLs as opposed to plain files.  That is, you'd first upload your files to the web (e.g. S3, if the file is not there already), then you'd do put-file -f s3://....  That way pachd would download the file over HTTP.\nIf your application is severely impacted by this issue, let us know and we can escalate the issue.. If I understand the proposal correctly, BlockRef is gonna looks like this:\nprotobuf\nmessage BlockRef {\n  string hash = 1;\n  uint64 lower = 2;\n  uint64 upper = 3;\n  repeated int64 delimiters = 4;\n}\nSo each BlockRef refers to a section with a block, and each delimiter refers to a section within a BlockRef.  This strikes me as a little strange.\nWhat about something like this:\nEach BlockRef is a unit of partition.  BlockRef still looks like:\nprotobuf\nmessage BlockRef {\n  string hash = 1;\n  uint64 lower = 2;\n  uint64 upper = 3;\n}\nBut it's guaranteed that lower and upper are both at delimiter boundaries.  So if we are putting a JSON file with 3 JSON objects, then we end up with 1 block but 3 BlockRefs, each corresponding to 1 JSON object.  Then in PPS, we distribute work by distributing BlockRefs.\n. Yeah your concern about size is totally legit.  I like this scheme:\nprotobuf\nmessage BlockRef {\n  string hash = 1;\n  repeated int64 delimiters;\n}\nThis has the following property:\n- The size of the BlockRef is delimiters[n-1] - delimiters[0] where n is the number of delimiters.\n- The size of object k is delimiters[k+1] - delimiters[k]\n- The number of objects is n-1.\nThen, to distribute data, we can either make sure that:\n1) each shard gets roughly the same number of objects, or\n2) each shard gets roughly the same number of bytes.\nBoth of which are easy to enforce.\n. I like where this proposal is going and agree that our current scheme is very append-oriented and doesn't work well with modification-heavy workloads.\nSome concerns that came to my mind:\n- The metadata-to-data ratio seems really big.  Currently we store one Block struct per 8MB of data.  Under this proposal, we'd store one Datum struct per datum, which can be as small as a line for normal files.\n- If I understand the proposal correctly, the int64 in Diff specifies the position of a Datum.  However, this seems to mean that in order to write any data at all, you'd first need to figure out how many datums the file already has (let's say it has N datums).  Then you create datums that are N+1, N+2, etc.  In other words, it seems like appending datums to a file requires either 1) tracking the number of datums in the file, or 2) read/construct the file to figure out how many datums it has.  This is in contrast with the current implementation of PutFile where we don't need to read/reconstruct the original file since we are just appending blocks.\n. Closing in favor of #1076 . Could you explain what you mean by CPU shares?  Is it in the same sense as Kubernetes' CPU resources?. I believe this issue has been fixed.  This line ignores /prev if it doesn't exist.. Closing in favor of #842 . \n. This is actually a huge problem.  Just ran into a user that used put-file in correctly:\npachctl put-file -r fish master -c focusnoise -f /path/to/a/directory\nwhich didn't work because -f doesn't accept a directory.  However, unbeknownst to the user, a commit was created and cancelled.  The user then tried a few correct put-file calls, but those resulted in cancelled commits as well, since the first commit was cancelled.\nWe should fix pachctl put-file so that if the usage is incorrect, no commit is created.\n. That's a good point.  Also can confirm that you can force Kubernetes to schedule one pod per node by messing with CPU/MEM quotas.  Although this approach can get tricky if you have a heterogeneous cluster.\nI was also looking into Daemon Sets which runs one pod per node for long-running services.  If PPS ever moves into a model where we have \"worker\" pods requesting work (as opposed to PPS scheduling jobs directly), then it will be pretty easy to run the workers as a daemon set, and we will be able to enforce that jobs are spread across nodes as well.\n. I thought by using CPU/MEM quotas you meant something like, if all your nodes have 4 CPUs, then you can ensure that each node gets at most 1 pod by requesting 4CPUs for each pod.  How would that work when you have a heterogeneous cluster?  Like if you have 1 node with 2 CPUs and another with 4CPUs, how would you use CPU quota to deploy 1 pod per node?\n. Good points!  The third point especially sounds like a deal breaker.\nGonna close this issue for now.\n. fork-commit is branching.  What's unclear about the current documentation?\n. Another problem is that FinishCommit computes the size of the commit.  So if there's an ongoing PutFile while FinishCommit runs, the commit can end up with an incorrect size.\n. Turns out update-pipeline already has a --no-archive flag that avoids reprocessing input commits.\n. Also, if there are more than one cancelled commits in the fan-out case, we should report all the cancelled commits.\n. @dwhitena \n- I'm imagining that a user would look at the logs from their failed container (e.g. an error message like error parsing JSON object) and realize that there's something wrong with the data.  Could you elaborate a bit on how this head functionality would work?  What does this offer over pachctl list-file and pachctl get-file?\n- I think so.  Also I think it's actually pachctl update-pipeline --archive=false\n. @dwhitena copying your comment regarding head here so we don't forget:\n\"...in many cases there are patterns in the data that are visible very quickly by looking at the head (e.g., missing columns, bad formatting, etc.). I think these types of issues are most common. Moreover, this gives a lot of context in terms of an eventual UI, even for successful jobs. If a user can have a quick understanding of what data in coming in and out (without having to look at entire files) alongside indications of job status and logs, that could go a long way to increase confidence. I think this is probably covered by functionality that already exists (because we could just mount pfs and head the files). I'm just try to think about what I look at when debugging data science workflows. I like to see a bit of the data, the logs, and the output. Most all debugging is taken care of if I have quick access to this.\"\n. Update: #1022 has added a retry mechanism to PPS that improves its resilience to internal failures.  Now the only failure case that's still not well handled is that of erroneous data.. We can close this issue now since we have an answer for all three forms of failures:\n\nErroneous code: pachctl update-pipeline\nErroneous data: just make a new commit that fixes the data.  This is now possible in 1.4 because we are able to process data modifications/deletions correctly.\nInternal failures: we are working on improving our handling of internal failures and there are many other more specific issues for that.. @jdoliner I think you requested this update a while back.  Could you take a look?\n. Potential inaccurate PR name.  Not sure if init job was ever idempotent :P\n. \n. LGTM.  Not meme-deserving cuz it's a temporary fix :P\n. \n. Note that the user was using \"recursive\" ListFile, meaning that it had to traverse the entire FS hierarchy.  It's unclear how the performance characteristics of non-recursive ListFile are like.\n. The direction we are going right now is to implement a ListFileLite which only returns filenames but not other metadata such as sizes, which are a lot harder to compute due to internal data structure reasons.\n. Fixed by #1011 . LGTM\n. LGTM\n. @JoeyZwicker done\n. @jdoliner all comments addressed.\n. Looks a bit over engineered but lgtm.\n\n\n. If the pipelines have the same name, how do users distinguish them?  For instance, what do you see when you list-pipeline?  How do you use inspect-pipeline on a particular pipeline?\nCould we also just allow different pipelines to output to the same repo?  E.g. you can have a pipeline called financial_model_v1 and another called financial_model_v2 and you can let them both output to the financial_model repo.\n. Just tested a pipeline that uses alpine:latest; it worked.\n. That's cool.  I guess then the only thing we need to worry about is CPU architecture (e.g. x86 vs ARM).\n. @jdoliner this PR could use another look\n. Breaking this PR into two separate PRs:\n1. Remove dependency on job-shim (#1075)\n2. Download/upload data directly (as opposed to using FUSE)\n. Fix #710. \n. @JonathanFraser starting from 1.2.4 (see #1075), user images no longer need to inherit from the job-shim image, so you shouldn't need to update your pipeline images anymore!. \n. I think this can be closed now since #1075 landed.\n. LGTM\n. Wouldn't this require pachyderm to have write access to /var/pachyderm to function?   What if this user doesn't have such access?\n. $HOME/.pach/?\n. LGTM\n. GGWP\n. @hunter We currently only support the default namespace, although we really should be using our own namespace.  I think there's an issue for that somewhere.\nAFAIK there's no reason why we can't use Deployments.  We just haven't got around to it since it's a relatively new feature.\n. @hunter actually I made a mistake.  You can actually deploy Pachyderm to any namespace you want by using the --namespace flag when you use kubectl create.\n. Yeah, it's not unreasonable to make this non-fix and require the user to explicitly use image tags if they want the pipeline to rerun.. @jdoliner comments addressed. Also fixes #573 I think.. Not sure if there's a YAML-protobuf converter, but we can go YAML-JSON-protobuf using this library: https://github.com/ghodss/yaml\n. Actually the original diagnosis was incorrect.  The real cause is that we are calling ListCommit with the heads of the branches we know about, which means that when a new branch is added it's undetected: https://github.com/pachyderm/pachyderm/blob/master/src/server/pps/server/api_server.go#L1531-L1540. Yeah it's autogenerated from here: https://github.com/pachyderm/pachyderm/blob/master/src/server/pps/cmds/cmds.go#L153\nShould probably also fix this while we are at it:\nhttps://github.com/pachyderm/pachyderm/blob/master/src/server/pps/cmds/cmds.go#L510. ^this comment above was meant for #1104 . LGTM from sean: https://github.com/pachyderm/pachyderm/issues/1101#issuecomment-263381271. I think by \"user\" JD just meant the cluster.  Like if you run a lot of jobs in a cluster, you want the metrics to go to the cluster's user ID.\nWould that be easy to do?\nIf we don't do that, then wouldn't we only be collecting metrics for command line operations?  Like only when someone manually put a file, etc.?. . @JonathanFraser summarized the problem pretty well.\n@jdoliner #1108 will help solve the first part of the problem, which is data might not fit into disk.  It doesn't solve the second problem which is that sometimes you really only need a subset of datums, and only the application itself knows which subset it needs (usually by looking at filenames).. @jdoliner has an interesting proposal:\nInstead of using normal files in /pfs, we use unix domain sockets.  Each socket corresponds to a file in PFS.  This has the following benefits:\n\nWe still get a non-FUSE file system layout.  That means meta operations (e.g. listing files) will be fast and non-buggy.\nData is loaded lazily, thereby circumventing both problems mentioned in this issue (i.e. data doesn't fit onto disk, and only partial data is needed)\nData is cache-able.  The process behind the socket can cache downloaded data in case the same file is read again.\n\nThe downsides are:\n\nSockets are still distinguishable from files, which means that certain applications might break.  For instance, sockets don't support seek.. @JonathanFraser It's slow due to the lack of caching.  When you read the same file twice, it's downloaded twice.  Even worse, every time you list a directory it translates to a ListFile over the network.\n\nThere is technically no reason why we can't just cache everything, but our FUSE implementation was already very buggy as it was, so instead of adding more code to it, we thought we might as well just get rid of it.\nImplementing (and maintaining) a FUSE driver is hard because FUSE has many callbacks, since it's supposed to behave like a normal file system.  Compare this with a unix socket, which only needs to implement read and write.. With #1130, we now have a solution for this problem.  Basically you can set the lazy flag in your pipeline spec to download files lazily.  This is possible because when lazy is set, we put named pipes instead of normal files in /pfs.  The named pipes only download the corresponding files when they are explicitly read from.  Note that named pipes do not support certain syscalls such as seek.. I think I'm a little confused because I'm not sure what the expected use of metrics is if they don't include calls from job-shim.  Shouldn't we expect the vast majority of calls to PFS/PPS be from job executions?  Am I right in understanding that right now metrics only capture calls that users manually invoke via the CLI?  If that's the case, it feels like we really should solve this issue before metrics can give us any meaningful information.. In general it's annoying if you have to sudo to get your code to work. Also\nyou can certainly imagine someone developing via SSH on a company machine\nthat's shared among many users, so they can't create /pfs\nOn Thu, Dec 1, 2016 at 4:17 PM Sean Jezewski notifications@github.com\nwrote:\n\nInteresting. Can you expand on cases where this might happen.\nIs there a reason you couldn't do something like:\nsudo mkdir /pfs\nsudo chown whoami /pfs\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/1124#issuecomment-264336670,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABduxpV4Qe5Zs68rGfTaAGKCYCTMzrcNks5rD2N4gaJpZM4LCB6c\n.\n. One solution to this problem would be that, instead of using /pfs, a user's code is expected to use $PFS.  Then, the user can just set this environment variable to whatever (e.g. ~/pfs) when they are developing.  Then in PPS, we'd just set this environment variable to whatever we want (e.g. /pfs).. This PR looks pretty straightforward, which is a good thing.  I think as future work, we can:\n\n\nCache (not necessarily in memory, but potentially on disk as well) data, such that if you read the same file multiple times, you only have to download it once.\nUse something like select or epoll to manage the named pipes, as opposed to spawning one goroutine per name pipe.  If we have a lot of files (think tens of thousands), the amount of goroutines might overwhelm the system.\n\n. Oh by the way, you probably wanna add some documentation for the lazy flag as well.. Docs added in #1140. I think I was reluctant to implement DeleteCommit this way mostly because it lacks any sort of atomicity guarantees.  If you delete commits from a branch that's concurrently being worked on by other clients, it can easily get into a corrupted state.. I'm going to add a warning message to pachctl actually, to tell people that this is a dangerous feature.. LGTM. It's worth noting that deploying etcd in cluster mode does have the benefit of enhanced availability: Pachyderm will continue to function even when a etcd pod is down.  Otherwise (i.e. if we only have one etcd pod), Pachyderm will be unavailable until a new etcd pod starts up.. Yeah all points taken.  These are all on my to-do list for migration.. @msteffen yeah, see #1177 . Verbal LGTM by @sjezewski on slack.  Also just to be clear, I did test this manually.. @jdoliner you are right.  I just found the real issue: the classic golang gotcha where only the last element of an iterator is captured by the closure.. Take another look?. Can't meme on mobile but LGTM. LGTM. LGTM. Related: #223 . . Unsure about why Rethink crashed the way it did, but the PPS crash should have been fixed by this PR: https://github.com/pachyderm/pachyderm/pull/1252\nLeaving the issue open until we've investigated the rethink crash.. This PR is extremely helpful.  We have been trying to migrate to grpc v4 for a while but the deps are just too hairy to tackle, but looks like you've mostly got it working.  Would really love to see this merged asap.. @roylee17 I fixed the issue that JD mentioned here, so feel free to cherry-pick it.  At the same time, I also discovered another issue: pipeline specs are not being parsed correctly.\nSo if you deploy this code and try to create a pipeline: pachctl create-pipeline -f doc/examples/fruit_stand/pipeline.json, you are going to get errors similar to this one:\nunknown value \"CONSTANT\" for enum pps.ParallelismSpec_Strategy\nSo for some reason, the JSON string \"CONSTANT\" is not being parsed correctly into the enum pps.ParallelismSpec_Strategy, even though the value CONSTANT is in fact defined in the enum.  The same problem applies to all other enum types we have such as pps.Method.  I'm not really sure what's causing this issue.  Probably something changed in the new version of protobuf?. @jdoliner done. This has been merged into #1272. The code looks really good overall.  I especially like the abundance of documentation.  Made a couple comments.. Just one more comment regarding the Merge API and it should be good to go.  The comment appears to be hidden since the code has been changed.  You need to click the \"Show outdated\" button.. LGTM as soon as the CI passes.  From our discussion offline, the next step will be to add a Finish API that computes the hashes at once, as opposed to computing the hashes every time we do PutFile.  It will be an important optimization because in the current state, putting a large amount files can be intolerably slow since each PutFile involves computing O(log n) hashes.\n\n. LGTM.  Ideally we'd have a working garbage collector so we don't have to do this, but it seems like right now we are at k8s' mercy as to when the containers are actually removed.. Currently persisting input/output data is useful for debugging purposes because of the chunking system: it's not always obvious which particular chunk of input data a pod has processed (you'd have to mess with filters), especially if there are multiple inputs.  If the input/output data is persisted, the debugging workflow can be as simple as:\n\nUser see that a pod is failing via inspect-job\nUser inspects the pod using kubectl\n\nBut yes, we'd lose this functionality after the datum change anyways.. @rrichardson what does your GC policy look like?  GC only runs after a configurable period.. That's a good point.  I asked because from what he posted, it seemed like the jobs only ran 2 minutes ago.\nI guess we will just really have to manually enumerate and delete the pods then.. . Hmm actually I just realized that this doesn't quite work for pachctl installed via apt-get or brew.  @sjezewski any idea how you might install the completion script via apt-get/brew?  Basically we just need to pipe the output of pachctl completition to a file under /etc/bash_completion.d.. @brollb yeah, I think that should be fine for now.  pachctl completion will print the completion info.\n@sjezewski LGTM this PR?. No longer relevant in 1.4 as Rethink is gone.. Done in #1272.  get-logs accepts a --inputs flag that can be used to specify filepaths.. I think @msteffen makes a good point, but now that I think about it, the \"dangling commit\" problem seems to exist in the current implementation as well.  If CreateJob fails at any point after it creates the output commit, then I don't think we have a mechanism to clean up the dangling commit.  The next invocation of CreateJob will probably fail because it will be trying to create the same commit (e.g. master/3) that already exists.\nTwo observations:\n\nIn pfs v2, this won't be a problem because commits are not tied to branches anymore, i.e. the next invocation of CreateJob will just create a commit with a unique UUID, as opposed to something tied to a branch like master/3.\nAs for the solution, we do have DeleteCommit that allows us to delete the head of a branch.  So maybe we can do something like, in CreateJob, if you find out that the output commit you are trying to create already exists, and that it's open, then you delete the commit and re-create it.  Does that sound like a safe thing to do?. This has been done by another PR.. @jdoliner any idea what's logging the third line here?  I seem to remember that you mentioned that there's some protobuf magic that automatically logs requests/responses which is probably what's happening here.. LGTM. Also fix an unrelated bug where if the given path doesn't exist, pachctl panics.. LGTM. Done in #1272. I think this is just changing the CI environment?  I think what we really want is to use Go 1.8 to compile pachyderm.  The way it works is that we use this Docker image to compile our code, so the patch will necessarily involve updating this Dockerfile to use Go 1.8 instead: https://github.com/pachyderm/pachyderm/blob/master/Dockerfile. I see.  I changed the title of the PR to better reflect the content.  Also for your question: our users typically use our Docker images.\n\nI think we might as well remove the Go 1.7 environment.  I don't see any reason why we'd want to run two copies of the test suites in parallel, especially considering that our CI is already heavily loaded.  Can @sjezewski chime in?.. We now run integration tests on AWS for every PR.. Removing high priority because this is less of an issue in 1.4. I think this is fixed in #1772 . LGTM as long as CI passes. @sjezewski yeah basically it happens when two nodes both think they are responsible for a pipeline, i.e. that they both run the runPipeline goro for the same job.  Then when a new commit triggers the pipeline, they both call CreateJob, thus the race.\nAs for why two nodes would both think they are running the same pipeline, the most likely reason is that there's some sort of network partition between etcd (which handles the sharding) and the nodes.. Overall LGTM; would be nice to have a test for recursive PutFile since that's what users will use to migrate their repos.. The challenging part about rate limiting here is that we don't know what the rate limit is.  S3 (and other object stores) for us is essentially a black box.  It adjusts its own rate limit internally.\nTherefore, any rate limit that we enforce will never be perfect; it will either be too high in which case it's useless, or it's too low in which case it hurts our performance.\nSo I think the thing to do here is just to tighten our internal retry mechanism.  That is, we make sure to catch all errors caused by the rate limiting and retry the operations via exponential backoff.  The goal is to ensure that a job never fails purely due to rate limiting.. Discussed offline with @jdoliner and @sjezewski.  We agreed that we shouldn't impose any internal rate limit, for the reasons listed above.  Rather, we should capture ratelimit-related errors at the worker layer when download/upload happens, and retry those operations in an exponential backoff loop.. @sjezewski has started using CloudFront for AWS deployments to get around this issue.  Removing the high-priority tag for now until the issue becomes more actionable.. I suspect that most of the \"bug fixes\" are for code that don't exist anymore in 1.4, but I suppose it makes sense to at least try to merge master and see if we are indeed missing important stuff.\nJust to be clear though, the current master branch will need to be moved to something like a 1.3 branch anyways, right?  We talked about how we want to support both release branches for a while.\nUpdating the task list to reflect the discussion.. Had an offline discussion with @jdoliner and @JoeyZwicker.  Basically the conclusion is that instead of having a retry button, we should just have an infinite retry loop for our jobs, similar to how k8s jobs work.  Meanwhile, a retrying job shouldn't block subsequent jobs from running and finishing.  A retrying job will basically have its output commit open until it succeeds.  In this world, there basically isn't such a thing as \"failed\" jobs.\nOpen question: should we support a policy where jobs do not infinitely retry?  If so, should there be a way to manually restart jobs that have been marked as \"failed\".. LGTM.  I'm going to merge this since I need to use it in some benchmarks.. LGTM after CI passes and an example for -c is added.. Is this for 1.3 or 1.4?  The part about get-file failing silently.. This would be super cool.  Just to be clear though, we don't have to have a Java client to build a Kafka connector right?  Seems like Kafka has a Go client.   https://github.com/Shopify/sarama. Fairly sure linter is getting run.. We might want to consider bumping the priority of this issue.  @sjezewski and myself have been in situations where we are looking at a super generic-looking error such as grpc connection unavailable, and have no idea which part of the code is actually spitting out this error because pretty much any code that uses grpc can return this error.  So we had to resolve to manually adding debug statements to pin down the faulty code.\nIf we solve this issue the way right way, then every error will embed the call stack and we will have no problem pinning down the source of the error.. I believe I've fixed the issues mentioned in the first and third comments since 1.4.1.. * Under \"Get more help\" section in both the PFS and the PPS page, the slack invite link is invalid.\n On the PPS page, all links are 404\n This is not necessarily a \"bug\", but the Bill of Rights should have links to each subsections, so that the \"enabling Accessibility\" and \"\"enabling Reproducibility\" links can link to those subsection directly, as opposed to the top of the page.. The AWS deployment page doesn't document pachctl deploy amazon, which means people who already have k8s running on AWS won't know how to deploy.. All Slack links under the \"Get more help\" section are using the old, inactive link.. On the pipeline spec, the \"Refer here for more information on delimiters\" link is 404.. The migrations page refers to automatic migration for 1.4, which doesn't exist.. I'm tentatively in favor of changing the default to replace as opposed to append.  However, I do think that we need an --append (or -a) flag for put-file that allows for appending.  Otherwise appending to a file, which we've agreed that is still a reasonable use case, would have to consist of 1) reading the file, 2) manually append to it, then 3) put it back.\nI'm also not sure if having --split default to append makes sense.  It just doesn't feel very consistent to have two defaults.  I think it's fine if you have to do --split -a to append to a split file.. Chatted with @JoeyZwicker offline.  One open question that occurred to us was what the behavior should be when you are putting the same file twice in an open commit.  Should one put overwrite another?  Or should they append?  The former seems dangerous since you are losing writes.  The latter feels inconsistent with what we are proposing which is to have the default behavior of put-file being replace.. Furthermore, currently running jobs in the pipeline should be stopped as well.  I believe that they are already being stopped; it's just that we are not updating their job states.. @jdoliner, ah, good catch.  That's what retCh is for.  Fixed it.. Yeah retCh is what you use when you wanna requeue a datum.  If you wanna return a response, you use respCh.  The names are arguably confusing.. @JonathanFraser that's a good idea.  However, if you process all jobs in parallel, it's likely that it takes a while for any given job to complete.  Whereas if you process jobs in order, then earlier jobs will be completed faster (since they get all the cluster resources).. @JonathanFraser good points.  We will probably do the simple thing for now which is to run jobs in order.  In the future we can add more sophisticated scheduling logic that would enable us to run jobs in parallel while prioritizing earlier jobs.. Seems to be a duplicate of #1541 ?. @jonandernovella could you describe your infrastructure?  Are you doing the put-file remotely or in the same cluster?  I've put many multi-GB files with no issues on AWS from within the cluster.. @dwhitena @jonandernovella could you guys try using a lower parallelism, like -p 10?  The default is 100 and I wonder if that's too much.. @jonandernovella Great.  We will lower the default parallelism to 10 in the next release.. Fixed by #1676 and #1683 . This is done as far as I can tell.. We've added stop-job for this purpose.. We should check if the datum exists in parallel to uploading the datum, so as not to slow down the common case where the datum doesn't exist.  The upload can be cancelled via context cancellation if it turns out that the datum does exist.. Fixed by #1600.. Oh didn't see that issue.  Closing this.. I had thought about adding a clone command similar to git clone, but extending get-file with a --recursive flag actually seems to make a lot of sense.. Discussed offline with @jdoliner @sjezewski @gabrielgrant.  The conclusion is that we want the user to be able to write to /pfs and /tmp, but nowhere else.  We then clean up /pfs and /tmp after each run.\nThe way to implement this is to mount the user container in readonly mode.  Then we mount /pfs and /tmp separately as read-write hostpath volumes.\n@JoeyZwicker @dwhitena do you see any user containers that might want to write to a path that's not under /pfs or /tmp?. LGTM.  Will merge as soon as CI passes.. Makes sense.. Removing high priority tag until more details are added.. Removing high priority tag until we can reliably reproduce this.. Fixed in #1630 . the meta-review. LGTM. LGTM.. More like making it so that when a job has multiple input commits, if one of the input commit is empty, the job doesn't process anything.. Something like google/api.  Can you reproduce?. Fixed. Related: #1481 . Discussed offline with @jdoliner.  We should implement a standard interface for processes to register themselves as pipelines.  That way you can create pipelines by any means (including Deployments).. I will take that. Closing for now until we can reproduce this issue.. Funny bug, cool fix. LGTM. Overall makes sense, but there are a couple potential issues where the goros that update the job progress might race with each other, or with the goro that sets the job as success.  I made the fix since it would be nice to get this merged before the release.  Lemme know if anything doesn't look right.. Jesus. I fixed a couple other places that have similar code as well.  LGTM.. That's a good point.  Maybe go vet would've caught it.  Also maybe the compiler itself shouldn't allow this since like you said it's complete useless.. LGTM. Thanks for the patch; supporting pachctl on Windows would be wonderful.\nThere are two other changes I would like to see before we merge this PR:\n\nAs you said, we need to fix the filepath versus path issue.\nUpdate the release process to include building and releasing pachctl for Windows.. One more comment and one more question.. Might be related:\n\nin 311.310947ms\n2017-04-12T00:34:42Z INFO  goro for worker 10.244.4.59 is exiting\n2017-04-12T00:34:42Z INFO  goro for worker 10.244.4.60 is exiting\n2017-04-12T00:34:42Z INFO  goro for worker 10.244.2.42 is exiting\n2017-04-12T00:34:42Z INFO  goro for worker 10.244.0.45 is exiting\n2017-04-12T00:34:42Z ERROR error discovering workers: watcher for prefix pachyderm_pps/workers/pipeline-pachsupport-boxes-flatten-v1/ closed for unknown reasons; retrying in 627.662717ms\n2017-04-12T00:34:42Z INFO  goro for worker 10.244.0.11 is exiting\n2017-04-12T00:34:42Z INFO  goro for worker 10.244.2.8 is exiting\n2017-04-12T00:34:42Z INFO  goro for worker 10.244.3.10 is exiting\n2017-04-12T00:34:42Z INFO  goro for worker 10.244.3.9 is exiting\n2017-04-12T00:34:42Z INFO  goro for worker 10.244.4.13 is exiting\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x78 pc=0x1696962]\ngoroutine 113 [running]:\ngithub.com/pachyderm/pachyderm/src/server/pps/server.(*apiServer).jobWatcher.func1(0xc420278a20, 0xc420511748)\n        /go/src/github.com/pachyderm/pachyderm/src/server/pps/server/api_server.go:931 +0x1c2\ngithub.com/pachyderm/pachyderm/src/server/pkg/backoff.RetryNotify(0xc4202b1cb0, 0x25dc760, 0xc420278a20, 0xc421a07f98, 0xc4205117a8, 0xa3ead7)\n        /go/src/github.com/pachyderm/pachyderm/src/server/pkg/backoff/retry.go:35 +0x3f\ngithub.com/pachyderm/pachyderm/src/server/pps/server.(*apiServer).jobWatcher(0xc4200bdb80, 0x7f6d811a2a40, 0xc4204ca380, 0x6)\n        /go/src/github.com/pachyderm/pachyderm/src/server/pps/server/api_server.go:970 +0xed\ncreated by github.com/pachyderm/pachyderm/src/server/pps/server.(*apiServer).AddShard\n        /go/src/github.com/pachyderm/pachyderm/src/server/pps/server/api_server.go:1592 +0x2c9. More logs:\npanic: send on closed channel\ngoroutine 270 [running]:\ngithub.com/pachyderm/pachyderm/src/server/pkg/collection.(*readonlyCollection).WatchByIndex.func1.1(0xc420360360, 0xc420627fd0, 0x25dc8e0, 0xc42045f610)\n        /go/src/github.com/pachyderm/pachyderm/src/server/pkg/collection/collection.go:392 +0xb3\ngithub.com/pachyderm/pachyderm/src/server/pkg/collection.(*readonlyCollection).WatchByIndex.func1(0xc420360360, 0x25dc8e0, 0xc42045f610, 0xc4203603c0, 0xc4205f0c00, 0x25d2660, 0xc42000e3c0)\n        /go/src/github.com/pachyderm/pachyderm/src/server/pkg/collection/collection.go:419 +0x5bc\ncreated by github.com/pachyderm/pachyderm/src/server/pkg/collection.(*readonlyCollection).WatchByIndex\n        /go/src/github.com/pachyderm/pachyderm/src/server/pkg/collection/collection.go:439 +0x1b3. Unsure about the first logs, but the two above both happened when we are re-deploying a three-node cluster with a new pachd image.. Capturing the discussion today: we are going to add a command like pachctl rerun-pipeline, which basically re-runs the pipeline as if the pipeline has never been run before, i.e. that no de-duplication or other smartness happen whatsoever.\nIn terms of implementation, rerun-pipeline increments the pipeline's version field, and internally we will take the version field into account when deciding if a datum has been processed.  That is, even if a datum has been processed by v3 of a pipeline, we still process it again for v4.. In #1685, we made it so that if you update or re-create a pipeline, the input data will be re-processed (even if the manifest is exactly the same as before).  Therefore we consider this issue fixed.\nI have opened a separate issue for adding rerun-pipeline which would be a sugar on top of update-pipeline -f the_same_manifest --from HEAD_COMMIT.. Also, this PR seems to lack pachctl updates.  Will inspect-job print input correctly?. To clarify, if the user code fails more than 3 times, the job is marked as failure and thus not retired.  From the logs that @dwhitena posted, it looks like the job is failing due to non-user-related reasons.  Specifically, the job seems to have trouble retrieving certain objects that it thinks exist.. The minio guys were kind enough to provide the following info:\n```\n...looks like an object is being read before the multi-part upload is completed. \nBasically, in a multi-part upload, there are three steps involved - initMultipart upload, actual upload of parts, and finalisation of the uploaded object. We observed that initMultipart is done, but before the finalization, there are multiple attempts to read the object. In the end upload is being finalized, but there are no read attempts after that.\n``. Fixed by #1723 . If this really requires a migration we should just revert the change.  I don't think the benefit of changing the name toKILLEDjustifies the cost of a migration.. Turns out that we always set the total datums to 100% when the job successfully completes.. Actually, we should still consider setting datums to 100% when all datums have been completed, instead of when the job completes.  That way if we see a job at 100% but it's still running, we know that the job is having trouble finishing, not processing the last couple datums.. Closing since we are no longer usingDeploymentfor pipelines.. Hi @NitishT thanks for the bug fix!  I left a review.  Also, could you sign the [contributor agreement](https://pachyderm.wufoo.com/forms/pachyderm-contributor-license-agreement/)?. Well, I guess in the worst case this is no better thanmaster, but it certainly shouldn't be worse.  I'm fine with merging it for the purpose of releasing today.. LGTM.  Gonna merge this for the release.. Actually, it turns out the problem is really that when youmake clean-launch-dev, only the RCs but not the services are deleted.  The solution is to update theclean-launch-devtarget to delete the services as well, which I will do in a PR.. We might wanna look into rewriting themap.gocode.  I think it was written at a time when/pfs/out` was still a FUSE volume, which is why the code tries to parallelize the writes, presumably to boost performance.  Right now there's no reason why we can't write it in the most straightforward way.. HOF candidate. Test added.. I followed the maintenance guide here and am now no longer getting the error.  It's worth noting that I was running 3 etcd nodes and I had to run the commands against each of them.\nWe still need to look into 1) why we are getting this error and 2) how to automatically resolve this issue.. Fixed by #1771 . Hi @kalugny, this PR will make it so that if you use the :latest tag, the PullPolicy defaults to Always.. Fixes #1763 . @jdoliner The flag is still there; we just don't set it by default.  If someone runs into a problem, they can still set the flag.  It's not documented tho.  I guess I can leave it in assets.go with the value default to an empty string.\nWhy would this get weird in local deployment?. Yeah, that's the point of this PR, which is to accept k8s' default.  I thought we agreed that it was a sensible default tho?  That if your image is tagged with latest (or if it's not tagged), the pull policy is Always.  Otherwise, the pull policy is IfNotPresent.. Good point.  I just tried with my own cluster and can confirm that what you said is accurate.  Closing this PR.. LGTM. The code is arguably clearer the way it is IMO.. That's in fact how we wanted it to behave. We just haven't got around to\nwriting the code yet.\nOn Thu, May 4, 2017 at 6:30 PM Gabriel Grant notifications@github.com\nwrote:\n\n@gabrielgrant commented on this pull request.\nIn doc/reference/pipeline_spec.md\nhttps://github.com/pachyderm/pachyderm/pull/1783#discussion_r114920502:\n> -    /pfs/bar/file-a\n-Datum2:\n-    /pfs/foo/file-2\n-    /pfs/bar/file-a\n-\n-Datum3:\n-    /pfs/foo/file-1\n-    /pfs/bar/file-b\n-\n-Datum4:\n-    /pfs/foo/file-2\n-    /pfs/bar/file-b\n-```\n+It's important to note that if a pipeline takes multiple atom inputs (via cross\n+or union) then the pipeline will not get triggered until all of the atom inputs\nCurious why it doesn't make sense for union inputs to be triggered before\nall atoms inputs have a commit? Couldn't the pipeline just run as soon as\nany datums are present?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/pull/1783#pullrequestreview-36423140,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABduxhykq2egw6vyXOEpkGO7s5swOjUpks5r2nupgaJpZM4NQDnk\n.\n. +1.  In fact, once we support the $INPUT_REPO environment variable, we don't even need the /pfs prefix anymore.  That way, internally we can mount the input data wherever we want, which also means that we can process multiple datums concurrently on the same worker, since there won't be any path collision.. Closing in favor of #1796 . Fixed in #2156 . @emk I think you likely just need to upgrade your pachctl as well.  Use pachctl version to see which version of pachctl you are using.. My bad I didn't see that you did pachctl version already.  I don't know what's going on then.. @vuc4 could you post the full log on Gist or pastebin or something?  The issue was about pipeline stalling but the error messages you posted are for jobs.. Hi @vuc4, just to be clear, were you seeing that the pipeline getting stuck in restarting state, or was it the job?\n\nI'm confused because looking at the logs, it doesn't appear that the pipeline went into the restarting state.  Also, while the errors about the job are concerning, from the logs it looks like the job was restarted successfully and only failed due to user-code-related reasons.  Could your code be returning a non-zero exit code for some reason?\nOr could you have posted the wrong logs?. Test added.. This is some HOF bug candidate here.  To provide some context (credit goes to @sjezewski and @msteffen for identifying the issue), our Golang client internally embeds a health check loop which fires every 5 seconds.  What this means is that if a client connects to a server that doesn't have a health check endpoint (which is how the sidecar server is), then any request that takes more than 5 seconds to complete will fail because the health check will fire and fail.. Potentially relevant issue: #1493. This PR is still WIP.. Ready for review!. I've been testing it manually but yeah adding some real tests is a good idea.. Good catch.. HOF candidate. Will look more closely at the code later but I think we need some docs on incremental.. Code LGTM.  Just need docs now.. Docs LGTM.  I will open an issue for adding a cookbook/example/best practice section for incremental pipelines.. I can see this being useful once we get to a point where people start sharing pipelines online.. LGTM. I haven't personally tested this, but I think one work-around would be to have the container (which is launched as root) chown /pfs to the non-root user.. As of 1.6.0-RC1, /pfs does not work with chown and therefore cannot be used by non-root users.  If you need access to a scratch space, it's recommended that you use the /scratch directory.  Note that the scratch space persists between datum runs, and therefore if your code depends on a clean scratch space, you need to clean up the scratch space yourself before your code exits.. From the pachd logs, it doesn't look like those two jobs were running concurrently.  @vuc4 what cloud provider were you using?  Were you using any custom caching solution such as Cloudfront?. We have a fix underway.. This PR will be done as part of a larger refactor.. Closing in favor of more specific issues.. Fix #1975 . LGTM.. Update: delete-commit has been implemented which allows for deleting open commits.  It's useful when you've messed up your ingress and want to start over.. It could work on finished commits; it's just that then we could end up with commits that have nonexistent provenance and we haven't really thought through the implications of that.\nAlso deleting a commit that's being read from (e.g. by a job) can be nasty.. I think this design mostly makes sense.  Just a couple comments:\n\n\"Tracing\" is probably not the right term to use here since that usually means tracking a request end-to-end, i.e. distributed tracing as in Dapper/Zipkin/OpenTracing.\nWhile there's not much data storage overhead since PFS de-dups, there's a considerable amount of metadata overhead since we will need to store at least 1 more hashtree for each datum.  This may or may not matter but it's something to think about.. Can confirm that on Azure, the default CPU request is 0.. The bottom line is, since we don't set default resource requests, the actual resource requests used by worker pods are determined by the cloud provider, which is probably not what we want.. @rj3d could you upload the pachd logs to pastebin/gist and post a link here?\n\nAlso, to be clear, was it put-file or finish-commit that was hanging?. @rj3d are these logs from when the hang occurred?. @jdoliner I looked and couldn't find a way to get more informative error messages.. @thedrow hmm I was using Go 1.8 already.  I just upgraded to 1.8.3 (the latest version) and still got the same error messages.. @msteffen I refactored the code a bit and updated some doc.. \n\n\n. Is this supposed to be a list of changes since 1.4?  If so I think we are missing a large number of items.  See the changelogs here: https://github.com/pachyderm/pachyderm/releases\nI think maybe we should move the changelogs on the release page to this file, and then have the 1.5 changelog be the changes since 1.4.8.. ^I can do that once we are close to cutting the release.. Actually this turned out to be a nonproblem.  You just pachctl undeploy and pachctl deploy and all workers will be started with the latest version.. @gg agreed that being able to explore a job's output commit while the job's running could be cool.\n@jdoliner the refactor makes it such that a job is only created when it's ready to run, so the problem you brought up will no longer exist.. \n\n\n. Closing in favor of #1970 . Re: the first \"Actionable change\":\nCouldn't we keep the version number in the hash, except that we only increment the version number for semantically meaningful updates?  For instance if a user updates input or transform, we increment the version.  Otherwise we don't.. Is the idea that you can then run migration no matter how messed up the state of the cluster is?  Like a cluster could be in a state where some objects are at version A, some objects at version B, and some at C, etc.?  I'm not sure if that's a super real scenario that we have to worry about.  Migration as it currently stands is already idempotent between two versions, i.e. if the migration failed you can just re-run it.. We still need the vendored code for our own binaries (pachctl/pachd) to compile.  This PR just made it so that people who merely use our client package don't have to vendor those code.. Fixed in #2014 . Yeah like @JoeyZwicker said I think the fix for #2000 will make the parallelism spec validation code in this PR obsolete.  I will reduce the scope of this PR to only making PPS master tolerant of kube-related failures.. Yeah we've seen that happen before.  Basically your containers can fail due to the underlying OS running out of threads, or if you hit the Go runtime thread limit (which I think is 10k), since we currently spawn one OS thread per lazy file (which is admittedly very much less than ideal).  It's on our roadmap to address this issue soon.. Fixes #2003 . Hype!. Agreed that CRC (or any hashing schemes designed specifically for integrity checking) would be a better choice than SHA since we don't really need to worry about attacks at the datum hash level.. Also not sure if I'm doing something wrong, but after I pasted the token, pachctl crashed with the following stacktrace:\n```\n(1) Please paste this link into a browser:\nhttp://github.com/login/oauth/authorize?client_id=d3481e92b4f09ea74ff8&redirect_uri=https%3A%2F%2Fpachyderm.io%2Flogin-hook%2Fdisplay-token.html\n(You will be directed to GitHub and asked to authorize Pachyderm's login app on Github. If you accept, you will be given a token to paste here, which will give you an externally verified account in this Pachyderm cluster)\n(2) Please paste the token you receive from GitHub here:\n29ff900a6858c35bd66ab9a01da140e9a8bbe4e9\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x18 pc=0xd4032c]\ngoroutine 1 [running]:\ngithub.com/pachyderm/pachyderm/src/server/auth/cmds.LoginCmd.func1(0x240d560, 0x0, 0x0, 0xc4204101b0, 0xc420149ce0)\n        /home/ubuntu/go/src/github.com/pachyderm/pachyderm/src/server/auth/cmds/cmds.go:71 +0x3ac\ngithub.com/pachyderm/pachyderm/src/server/pkg/cmdutil.Run.func1(0xc420482900, 0x240d560, 0x0, 0x0)\n        /home/ubuntu/go/src/github.com/pachyderm/pachyderm/src/server/pkg/cmdutil/cobra.go:45 +0x47\ngithub.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra.(Command).execute(0xc420482900, 0x240d560, 0x0, 0x0, 0xc420482900, 0x240d560)\n        /home/ubuntu/go/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra/command.go:572 +0x22b\ngithub.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra.(Command).ExecuteC(0xc4201d6000, 0xc42019f750, 0xc42019f6d0, 0xc42019f730)\n        /home/ubuntu/go/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra/command.go:658 +0x339\ngithub.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra.(*Command).Execute(0xc4201d6000, 0x0, 0x0)\n        /home/ubuntu/go/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra/command.go:617 +0x2b\nmain.main.func1(0x23e6b20, 0xc4200a8d80)\n        /home/ubuntu/go/src/github.com/pachyderm/pachyderm/src/server/cmd/pachctl/main.go:21 +0x42\nmain.main()\n        /home/ubuntu/go/src/github.com/pachyderm/pachyderm/src/server/cmd/pachctl/main.go:25 +0xc3\n``. Fixed in #2293. Hi @landonbjones, thanks for your (and your mom's) suggestions!  I've made the corrections which should be reflected on the website pretty soon.. (this PR is necessarily failing CI since master is broken and this PR re-enables CI). Fixed in #2154 . Currently a job immediately fails once a datum has failed, so even if we had adata_failedcount it would never be more than 1, which would not be helpful.  In other words, if a job is running, there are no failed datums.. Fixed in #2225 . Fix #2223 . Turns out that we won't be able to set the max ops per txn until v3.3.0 of etcd is released.  What we are doing here is using our own etcd image that is the same as v3.2.7 except that the ops limit is set to 4096 (as opposed to the default 128).  The docker image is available atpachyderm/etcd.. Comments addressed. @jdoliner one more comment regarding thecancel.. @thedrow yes, it does fix #2089.  PFS itself doesn't necessarily know if the whole file has been uploaded, since it doesn't know how big the file is supposed to be.  It's up the the user to decide if it's time to commit.. Upon some research I think what @dcjohnson said is correct.  Although according to [this doc](http://kubernetes-on-aws.readthedocs.io/en/latest/user-guide/iam-roles.html), it's more involved than simply leaving the credentials blank; you'd also have to annotate the pod in a certain way.  Anyways, sounds like it's doable and I will see if I can get it working.. Sure.  I don't have super strong opinion either way so feel free to merge it.. Yeah I was seeing this too.  I will look into it.. @emk That seems to make sense.  Should be easy for us to do as well.  I will take a crack at it and hopefully it will make it into the next release.. Fixed in #2328.. Thanks for the patch.  Merging.. Yeah that's a good idea and should be fairly doable.. The protobuf changes are due to an update in the protobuf image, btw.  In other words, you guys probably also need to rebuild your image or otherwise these (supposedly good) changes may get reverted.. Yeah, this exits at the first commit but inspectFile goes through every commit.\n. What do you mean?  I removed this because it's not used injob-shim.\n. I'm not sure either.  This code seems to have the effect that, if a job has a parent, and its parent is a reduce job, then create a separate repo for this job (instead of using its parent's repo).  Do we want that?\n. What is the \"this\" you are talking about?\n. oops\n. That's what I did at first, but then I realized that I needed to manually create the pipeline repo, which I thought was a little hacky.  I can certainly lower the sleep time though.\n. It's because I wanted to add the optimization which was to only subscribe for pipelines for a certain shard; that's when we will need this request.\n. Yeah I haven't rigorous analyzed whether under some corner cases a pipeline can be run twice.  This is more of a safety measure.\n. Could you elaborate?  Not sure if I understand what you are talking about.\n.deployseems like over-generic of a name?  maybepachctl-deploy?\n. It's a little strange that if you want to use GCE/local backend, you have to get some error messages.  Also, it can surprising to the user if they expect to use an S3/GCE backend, and that fails, and they end up with a local backend.  I feel like the user should explicitly state which backend they want to use, and we just fail if that doesn't work.\n.manifestshould be changed to the new name.\n. Is this intentional?  I'm assuming you want to do nothing if the prefix is not found.  But this error could mean other things, like gce connection error or whatnot?  Is it possible to check for specific error types?\n. It's cuz the empty line in between.  It's intentional\n. We need an english linter\n. calling itcancelledseems to imply that we only want to list cancelled commits though?\n. Maybe just call itall?  the flag tolist-commitcan also be--all. Go compiler seems to suggest that|=` is not a thing\n. Not sure if I totally understand the comment here.  Am I not wrapping it in a loop?\nThe reason I didn't use channels was that i'd need to release d.lock before I block on a channel, and acquire d.lock again after the channel unblocks, which seemed clumsy to me.  The good thing about a CondVar is that it's assossiated with a lock (in this case d.lock) so it's convenient to use.\n. It does\n. I don't think we want to hardcode keys?  Use kubernetes secret maybe?\n. I think this document should be simplified to only contain what works (i.e. specific instructions about how to vendor things), not what we don't know and what doesn't work, because I feel like the latter should probably belong in an issue for govendor, not in our source repo.\n. But tv's fix fixed that, right?\n. Did you want to leave this here?\n. Did you want to leave this here?\n. Did you want to leave this here?\n. Did you want to leave all these here?\n. To be clear, setting the recurse option to true means that we will correctly compute the sizes of children directories at the cost of performance.  I think this is a reasonable tradeoff for pachctl since pachctl is meant to be used manually (as opposed to programmatically), and the human user probably expects to see the correct sizes of directories.\nThis does not match what we had before, since before if you did list-file, you'd see child directories' sizes being zero.\n. Putting this logic server side would require us to create RunPipelineRequest and a RunPipeline API, right?  I don't have a problem with that necessarily; just wondering if that's the only way to do it.\n. I already added RunPipeline and RunPipelineRequest and it doesn't look too bad.  It will probably scale better as well since we might be adding more options to RunPipelineRequest in the future.\n. nah we should change it to this is the number of shards for clarity\n. Wait but this is a pipeline test. What job do I inspect?\n. Basically, imagine that in a commit, you do rm /pfs/out/dir && mkdir /pfs/out/dir.  Without this change, we end up with an append with Delete set to true and Children being nil.  But looking at the append, we wouldn't be able to tell whether there is a directory there, or if the file has simply been deleted.  Having Children being an empty map (as opposed to nil) tells us that a directory has been created here.\nSo yes, the changes require this, because the changes now allow us to delete a file and create the file (or directory) in the same commit.\n. It happens whenever you are appending to an existing file for the first time.  You get h.written = 0 and request.Offset = len(file) which results in repeated < 0.\n. Yeah basically.  I think previously if you call InspectFile on a directory, all children get returned regardless of the given filter shard.  Now only the children that match the filter shard get returned.\n. Ah I wasn't aware that oneof exists.  I was looking for variant and couldn't find anything.  The protobuf guys just had to invent new names :(\n. Protobuf compiles this to contants like Partition_BLOCK.  If we rename it to PARTITION_BLOCK, it'd become Partition_PARTITION_BLOCK, which seems unwieldy.\n. Should we patent this?\n. Could you make pipeline C use data from both A and B, and make sure that pipeline C is actually producing the expected output?\n. What does this function do?  Some comments would help.\n. Is it an invariant that the Provenance returned by each server should be the same?  If so, can we assert that?\n. Same comment as above\n. OK that seems reasonable.  Ignore it then.\n. Yeah but the thing is that we are not actually partitioning files.  We are partitioning the files/directories in the top-level namespace.  Agree that \"top-level\" objects are not ideal though.\n. Did you talk about this with @jdoliner?  I think he originally proposed this.\n. Makes more sense to me that you need to explicitly specify both, otherwise we error.  Most users are expected to use aliases anyways.\n. Seems like we are losing information here by making this change.  Can we make ErrCommitNotFound a struct that contains repo name, commit ID, and shard, and implement the error interface so it can be used as an error?\n. Also just to be consistent, we probably want to use ErrCommitNotFound in other places where we are returning custom error messages for when commits are not found.  For instance: the inspectCommit() function\n. Just to be clear here: I get that you wanted me to change it to \"file\".  But then do I proceed to explain that \"file\" really means files/directories residing in the top level?\n. Note that the sum part of the fruit stand demo currently uses incremental reduce.  You cool with it using a JSON object input method?\nAlso, I added some actual code to this PR.  Could you review?\n. It's generated from pach-deploy.  It's just a random directory.  Running pach-deploy again will result in a different directory.\n. But what if /tmp/pach/ doesn't exist?  Does our code address that?\n. Currently we immediately returns after calling FinishJob, so there isn't really a room for panicing after calling FinishJob.  However, I agree that this approach is not very bulletproof; the most bulletproof way would be to have each pod call FinishJob with a unique ID, so we can do the de-dup on server side.\nI guess I can do that when I fix #354?\n. This seems to be a strange pattern.  So basically the function always return blockRefs as nil, but in the defer function, we set blockRefs to an actual value if err == nil or retErr != nil?  So if err is not nil and retErr is not nil, we still return blockRefs?\n. Why not just use this boolean expression in the for clause?\n. FileNumber and FileModulus default to zero anyways.\n. same comment\n. This comment doesn't apply I think.\n. Same comment\n. I'm guessing this comment doesn't apply anymore.\n. It seems a little strange that you'd pass both reader and decoder since they represent the same piece of data.  Also decoder may not even be used if Delimiter is not json.  Would it make more sense to pass only reader, and only construct a json decoder inside readBlock if Delimiter turns out to be json?\n. I see.  That makes sense.\n. Can you explain what \"everything\" means in the Long help string?\n. Maybe briefly state what \"everything\" means here as well?\n. Smart\n. We are ignoring errors from DeleteRepo?\n. Not sure if this is quite enough.  What about the goroutines that are running runPipeline?  Probably wanna cancel them using cancelFuncs.\n. State to number?  Might want a comment.\n. I think it makes more sense to log the error instead of exiting here.  Kubernetes jobs are in a way outside of our control; if the user manually deleted a job, they won't be able to use DeleteAll anymore.  Or we can just ignore not found error or something.\n. Yeah I wrote the code.  What is it like to review your own code in someone's else PR.  #feelsgoodman\n. You sure we wanna sleep for this long?\n. I'm only checking filterShard != nil so I can check filterShard.BlockNumber safely.\nAnd yeah the point of this code is that if, after filtering, you get a file with not blockrefs, then you just return \"file not found\".  That way jobs won't see empty files.\n. I'm confused.  Can you just tell me what you think the boolean expression should be?\n. So I just tried that and some FUSE tests failed.  I looked into why, and I think it's because when you write a file, the following sequence of FUSE events occur: create -> attr -> write.  When you attr (and therefore InspectFileUnsafe), there is no block in the file yet.  So now you get a file not found error if you don't check for the existence of a block filter.\n. How would setting FUSE log level on client side be useful?  Isn't pachctl entirely separated from FUSE?\n. Not sure if you wanted to include this in this PR.  But either way the test looks good.\n. Maybe add a comment on the server-side code to remind developers that the message is used on client side?  So they don't accidentally change the message and break things.  Or, if we have a test for this, that works too.\n. Oh I see that you have a test below.\n. What's the motivation for this block of code?  If the pipeline already exists, wouldn't we get an error when we call CreatePipelineInfo below?  This code can also race with two concurrent calls to CreatePipeline (i.e. they both get not found and both move on to create the new pipeline; although they won't actually end up creating two pipelines since CreatePipelineInfo will prevent that).\n. I see.  I guess this code looks good then.\n. looks like you just want to return strings.Contains(err.Error(), \"has already been finished\")\n. If you look below, we insert the clock into the Clocks table to ensure uniqueness.  That should work?\n. nice\n. Since this command permanently deletes data, can we put a simple command-line prompt here?  Something like The following script will delete your AWS bucket and volume.  The action cannot be undone.  Do you want to proceed?  (Y/n)\n. Need file type conflict checking.  See PutFile.\n. I'm not sure if CreateBranch captures the fact that a new commit is being created.  Further, Fork always takes a parent commit, whereas CreateBranch sounds like it can create an entirely new branch, which is actually what StartCommit does.\nHow about ForkCommit?\n. Since this function is defined on the Clock struct which is defined in the persist package, it will be hard.  Also the readable form is actually stored internally (in the database), and it feels like pretty is only for printing purposes.\n. This will result in an error actually.  The second argument is not allowed to be empty.  Maybe just use \"master\"?\n. Same for this one.\n. Same\n. Thanks.  You basically just need to roll on your keyboard then just ctrl+v a couple times\n. oh yeah i was literally killing the pods manually.  should probably figure out a way to kill the pods programmatically.\n. Yeah it was called LeaseManager then golint complained that lease.LeaseManager stutters.\nJust tried make lint with Leaser and there was no complaint.  GG.\n. Makes sense.\n. Agreed. We can do that in a separate PR.\n. That would make sense, but then you'd have to create a PipelineVersion message because you need a way to signify that you don't care about the version.  And 0 won't do because 0 is a valid version number.\nHow about let's do that when we have other code that needs to filter jobs by pipeline version.  Right now only this parentJob function does that.\n. Good comments.  I copied the download/upload code from my PoC and forgot that it was just a PoC.\n. I like the name Pull better because it's the opposite of Push.  But yeah I think exposing it in pachctl makes sense.  I'd call the command pachctl clone since this is most similar to git clone but pachctl checkout would be fine too.\n. Good point!\n. Same as above.\n. None other than that we know that it works on 14.04 and I don't want to bother debugging the opencv example in case it somehow breaks on 16.04.\n. Yeah I framed it and hung it in my room.\n. I'm not sure if I understand this flag.  If the url points to a file/obj, then recursive is useless.  If the url points to a directory, then presumably you should upload everything under the directory, so recursive is useless again.\n. But what would putting s3://dir mean without the recursive flag?  Putting an empty directory?  How would that be useful?\n. OK that's reasonable enough.\n. Good point.. Good point!. Actually, that doesn't work cuz <-tick is not boolean.. Actually if you look at line 272 it's already using positional arguments as include, but I think getting rid of -i and only using positional arguments make sense too.  -x sounds good too.. Good point.  I like this better.. oops this is from a while ago.  old laptop stuff. OK I was thinking that it might help for people to understand the system by actively asking themselves those questions, but I have no problem removing it.. I have two questions:\n1. Why would a node's parent not be found in Fs?  If the parent is not found, isn't that a consistency error?\n2. If we know that pnode is nil, what's the point of calling update?. Interesting approach to error handling.  I think the pattern I've most commonly seen goes like:\nif err != nil {\n  if err, ok := err.(hashTreeError); ok {\n    switch err.Code {\n      // deal with different error codes...\n    }\n  } else {\n    // it's not a hashTreeError\n  }\n}. I have two questions:\n\n\nIf a node's parent does not exist in Fs, is that a consistency error?\n\n\nIf pnode is nil, what's the point of proceeding?. Do we want the distinction between DeleteFile and DeleteDir?  Currently there's only pfs.DeleteFile() and it's used to delete both files and directories.. This test is pretty cool!. This is kind of my fault since I wrote this interface, but it just occurred to me that we will most likely be merging trees in batches, since each datum will have a tree and we will be merging many datums for each commit.  Would it be hard to change this to Merge(trees []Interface)?. I see.  It's assuring that gRPC does this.  Feel free to keep it this way.. Ah, I see.  That makes sense.. Yeah sorry I should've worded it more clearly, but yes I think my concern was mainly that we'd be merging many trees at once, so it'd be ideal if we only rehash the entire tree once all trees have been merged.  I see that you've updated the interface, but I think the implementation is still such that we rehash the tree once per datum?\n\n\nIn v2, each datum is an object selected by the glob pattern.  For instance, if the glob pattern is /*, each datum will be a file/directory under the root directory.. Why did you want to return all errors, as opposed to just the first error that you encounter?  Is it so that users can resolve all merge errors at once?  I'm a little worried about the fact that the error string is basically unboundedly long, since eventually the error needs to be displayed to the user on some UI.  Can we hard limit the number of errors we return?  Similar to how when you have a lot of compilation errors, the go compiler only displays a couple and then just says \"too many errors\".. In a discussion offline we talked about changing the API to something like:\n```\nmessage Object {\n  string name = 1;\n}\nmessage PutObjectRequest {\n  bytes value = 1;\n}\nmessage TagObjectRequest {\n  Object from = 1;\n  Object to = 2;\n}\nservice ObjectAPI {\n  rpc Put(stream PutObjectRequest) returns (Object) {}\n  rpc Get(Object) returns (google.protobuf.BytesValue) {}\n  rpc Tag(TagObjectRequest) returns (google.protobuf.Empty) {}\n  rpc Compact(google.protobuf.Empty) returns (google.protobuf.Empty)\n}\n```\nThat way we can tag objects retroactively.. Actually, it occurred to me that we also need an Exists API that simply checks for the existence of a tag.  This is useful for a job to check if a datum needs to be reprocessed.. I'm a little worried about calling DeleteCommit with a branch name.  Could hit some weird race conditions where we end up deleting a commit different from the one we intended to delete.  Would it be hard to use the parent ID to compute the real commit ID (e.g. master/3) of the commit we are trying to create?. I would also add some debug log statements just so we know if/when this code path has been hit.. outdated comment?. Might be missing something obvious, but if you call h.canonicalize(\"/\") in every PutFile, aren't you hashing the entire tree every time?. Shouldn't b be true instead of false?  That way, if runEmpty is set, we will double the modulus until it exceeds the parallelism.  The way it is right now, I think parallelism will just be ignored and you will always have 1 pod.. Not that important but I think this is kind of an anti-pattern.  You should check for !ok and return the error in the if clause, and return h3 in the main function body.. This also seems a little strange.  It seems to make more sense to perform the merge on hcopy, and only set *h = *hcopy if there's no error, as opposed to \"resetting\" *h if there's an error.. I think you can use DeepEqual directly on interfaces; no need to convert them to concrete types.. What about Deserialize()?. No more 1.4h right?  Also what's an op here?  Is that whatever that's inside the b.N block?. Not that it matters but why did you make this change?. Can hash be another tag?. Is there automatic compaction?. What are the results of the benchmarks?. Should these be exposed to pachctl?. Wrong name. Same here. Does this mean that if you save a file as foo.json in a FUSE mount, we somehow end up with a directory foo?. Shouldn't we also check that the children files are named the way they are supposed to?  Right now looks like we are only checking the number of children.. Also this should be put into server_test.go which is for PFS-only tests.. Did you want this?. Don't we still need this function to compute parallelism?. Can we have examples of how this command is supposed to be used, similar to other commands?. Did you really want these log messages?  Or were they just for debugging?. Pretty sure we don't need any code related to job-shim anymore.  Looks like it might've been merged into pfs-v2 mistakenly earlier.  Could you remove it?. This comment might be outdated?. Probably don't need this?. Oh I misunderstood.. Very nice explanation of the reasoning behind making files as the smallest processing units.. This command only migrates the head of the branch, right?  Should we make that clear?  Should we talk about how one might migrate all commits, i.e. preserving commit history?. Did you want to use a real link here?. Same here.  Should there be a real link?. Should we talk about how the \"more advanced form of incrementality\" is basically the equivalent of /prev?. What field?  Should probably make it clear that it's the branch field.. Looks like you made all the methods on Driver private.  If that's the case, should we make Driver private too?. lol. but are you defensive enough to defend against UUID collision. I don't completely understand how this works so just checking, but would you end up with more than one record without --split if you call PutFile on the same file twice?. Should we make these magic numbers into constants?  Wasn't sure what they were at first glance.. Presumably you'd use the aforementioned constant in place of the 16 here.. As mentioned offline, I'm not sure if an OpenHashTree returns the right sizes for its internal nodes.  Can @msteffen confirm?  Regardless, the usage in this patch should work since we are not really using sizes.. Good point.  Will do.. What are these echos for?. Shouldn't we print the raw logs by default?. So I changed my mind about these log statements: they actually proved really helpful when I was debugging GF's pipelines.\nWhat do you think about this:\n\nKeep these log statements, but prepend them with something like PACH:.\nIn GetLogs, filter out the lines that contain PACH: if raw is false.\n\nThat is, normally we don't see these lines (assuming raw defaults to true; see the comment below).  When we really need to debug something, we turn off raw and get these lines.. Yeah, not much. Just being extra safe.. Yeah that was pretty hilarious. This variable seems to be used by different goros but not protected by mutex.. Good idea. Yeah will factor that out.\nThe idea is that you can use storageclass if you are on GCE or AWS, because that's where dynamically provisioning is officially supported.  If you are on Azure, you can still specify a PVC template and dynamic provisioning will work, according to my own testing.. Actually, I realize you can't factor this out without writing some really hairy code, since it's all interface{} and you have to cast them to maps and all.. Added a check.. Can we add some comments explaining what a datum filter is supposed to look like?. This is more of a question, but am I right in observing that when a field is inherently plural (i.e. an array), you still like to use a singular name for the field?  Why is that?  I'd personally prefer to call this worker_statuses.. We don't have to do this right now but I figured I'd say it, but I read a little about gRPC recently and I think that the best practice when it comes to choosing a response type for an API is to always use its own response type.  So here, it'd be an empty RestartDatumResponse.  The reason is that it makes it much easier to evolve the response type in the future, by simply adding fields.  And ergonomically typing types.Empty{} is not that much easier than typing pps.RestartDatumResponse{} anyways.. 20. Not sure if I'm missing something, but wouldn't this test have passed even if RestartDatum was a no-op?  In other words, are we actually testing the restart?. Ah, this method looks super handy!. Can we just put these four fields into a WorkerStatus struct or something?  The code as it is looks a little rough since we are always manipulating the four fields at once anyways.. Oh so we do have WorkerStatus.  Can we use it to address the comment above?. Are these comments supposed to be here?. Same thing here; ideally we'd have StatusRequest even if its empty, for future extensiblility.  No need to fix it rn tho.. A comment about what started means?  When the worker launches or when the worker started processing this datum?. 5 seems like a lot.  What about 3 which is what we used?. It seems strange that we pass this constant into every invocation of workerPool.  Could newWorkerPool simply use this constant itself?. If I understand the code correctly, there might be a leak here.  Before, there's this invariant that a workerPool is always tied to a pipeline or an orphaned job.  When the pipeline/job terminates, the worker pool (along with the goros it spawns) is deleted by means of context cancellation.\nNow, it seems like every call to WorkerStatus or RestartDatum can create a worker pool, which in turn spawns a number of worker goros.  Are those goros being terminated somehow?  If not, that could be a leak.. Do we need this mutex?  I think addWorker and delWorker are only ever called from the discovery goroutine.. Why are we checking jobID and dataFilter here?  Looking at the implementation of Cancel, it already checks dataFilter.  Also, checking jobID on the server side might not be enough, since there can be a race where the worker moves on to process datum for another job after you called Status.  Maybe Cancel should take a jobID and the worker (instead of the server) can check that the job matches?. @jdoliner I think distinguishing between STOPPED and FAILURE makes sense.  FAILURE is when things actually went wrong.  STOPPED is when you manually make the job stop, even though it might succeed eventually.. Yeah that's basically it.  Technically having the old IPs around shouldn't stop new jobs from completing.  We'd just have a couple zombie worker goros on the server side that get shut down once the old IPs expire.. Yeah that makes sense.  Will do.. It was necessary to refactor it such that branchSetFactory returns a channel so jobManager can use select to block on either new input commits or job completion signals.. The idiomatic way to check for empty string in Go is s == \"\".  Could you do that just for the sake of consistency with the rest of the codebase?. Not sure if that belongs to a help message.  I think users will be able to grasp what kill does regardless.. Would it make sense to check that this value is equal to the defaults?  Are we sure that this check fails if resource requests were not specified?  Cuz I can see a world where k8s automatically sets some default resource requests and this check succeeds regardless.. I can't think of a problematic effect here.  PPS will get a new event saying that the key has been inserted (updated) and does the right thing, I think.. This needs to be StopJob. I believe we have a GetExpectedNumWorkers function that returns the number of works using the parallelism spec.. This is more of a question, but am I right in understanding that we can't simply use the service's name and expect it to be resolved into the correct IP?  Is that something that requires a k8s DNS setup?. Might wanna consider using the gRPC options that Matt was talking about this morning.  Otherwise might end up with unnecessary network errors.. Can't you just return the original error?. I could be missing something, but wouldn't you want to set setProcessedData here as well?. I could be missing something, but I think if we return the error here, then we won't get to the channel receive below, in which case the goro in newMinioWriter might forever block on sending.. @msteffen Look at Close().  It blocks on receiving from a channel, and that channel is only sent on when c.PutObject has completed.. But yeah maybe we should document Writer() to make it clear that Close needs to be called after you use the writer to ensure that the write completes.. I believe the idea is that whereas before, you'd do:\nobj.Write(some_bytes)\nRead(obj)\nThis is problematic because the fact that Write returns doesn't imply that the object has actually been created.\nNow you'd do:\nobj.Write(some_bytes)\nobj.Close()\nRead(obj)\nAnd Read is now safe because Close guarantees that PutObject actually completes.. Looking at the code for io.PipeWriter, it always returns nil for Close, so we are good here.  I'm going to go ahead and merge the PR.. This is just a question, but was there any particular reasoning behind choosing this value?. Doesn't this map need to be protected by a lock?. Use the lock here as well?. I've thought about this lately, and I think we need to be consistent about how \"simple\" we want the client API to be.  This API looks like a classic 1.3 API where in the default case, most of the arguments are just default values such as \"\" and false.  This makes the API hard to use in the default case and look ugly too.\nI think we should really just optimize the client API for the default/simple thing.  If you need to do a complicated thing, use the underlying type directly.\nIn terms of this API NewAtomInput, I think it should just take repo name and glob, the only two required arguments.. That's what I did initially but it turned out to be harder than expected.  The tricky part is that there are all these inner dependencies between the servers so it's actually pretty hard to factor out each of them into their own function.\nDoing it the current way is probably less bug prone anyways since it doesn't affect the pachd code path at all.  I think we can look into refactoring the code when we have more than two modes.. My intention was to distinguish between pachd and sidecar.  To me, pachd is the centralized servers that store a lot of state, handle user requests, store cache, etc.  sidecar is, well, a sidecar to the user container.  I feel like having the user see a bunch of pachd containers running inside workers can be a little confusing.\nTo be clear, the term sidecar is pretty widely used within the k8s ecosystem.  Google kubernetes sidecar and you will see a bunch of things.. Do we want to panic?  I think panicing is fine if this is a truly exceptional, unrecoverable case, but just wanted to double check.. I like the abstractions in this file.. Yeah sounds good.. I feel like input is a pretty important piece of information, certainly more so than creation time.  Can we just print the set of repos that are used as inputs, regardless of atom/union/cross?  If you wanna do that in another PR that's OK too just make sure to open an issue for it.. What about inputs?  If we are ok with people using pipelines that have inputs but not input, then I think we should support printing inputs as well.. The code above this block checks if it's a directory and returns if it is.. Yeah I had to break the import cycle between this package and grpcutil, but I suppose moving it to grpcutil makes sense.. comma here. This needs to be updated as well.. And this. We don't have .input anymore.. Same here, no .input.. We do trap the error if the workers already exist.. em memories. Did you want to use size as the channel size here?. This seems somewhat confusing.  Can you somehow make it more explicit that you can't actually have more than one input types at the top level?. Same comment: you can't actually have more than one input types at the top level.  Has to be one of atom/union/cross.. Why this change?  I personally like the original formatting better.. Same here.. The implementation of Walk retrieves objects in batches, I think.. The name is definitely kinda verbose, but calling it ListTag doesn't seem to make sense since it returns objects.  This API basically returns objects that are tagged with tags that have a given prefix, hence the name.. I see, that makes sense.  Will change.. Have you look into this?\nhttps://github.com/kubernetes/client-go#how-to-use-it. I don't have a particular strong argument for why this is better, but I like the consistency/extensibility of it.  For instance, when I was iterating on this PR, I had to update certain message types many times (e.g. ListTagsRequest/Response).  It felt good that I only had to add/remove fields within the messages, as opposed to changing the message types themselves from/to Empty.\nIn this case in particular, it's very possible that we will be adding options to GCRequest soon, and I had plans for making GCResponse into a stream that reports progress (since GC can take a long time).  Now, why can't we just use Empty for now and change them to specific types later?  Again I don't have a good answer.  I guess this just comes down to personal preference.\nI also read somewhere that having dedicated request/response types for each API is a good practice, but that's of course debatable but at least I'm not the only person who likes it that way.. Sure, compact sounds good.. this meaning the incrementGCGeneration function?  It's used in the GC function above.\nIf you are talking about the generation number itself: it's used in obj_block_api_server.go.. Yeah that's my thought too.\nAlso it's the life of the program, but I suppose programs are problems.. That's why I put the generation number at the end of the key as opposed to the beginning.  Would that still be a problem?. splitKey[:len(splitKey)-1] is an array, no?. All implementations of objectClient.IsNotExist are fragile anyways... this is sort of like a \"catch-all\" since it calls objectClient.IsNotExist in addition to doing some other checks.  I think there were certain NotFound errors that were not caught by the object client, and I thought about just updating the object client, but then I realized that I had no idea if other object clients' IsNotExist functions work correctly.  So I just decided to catch the errors at the object server layer just to be safe.  Determining error types across RPC boundaries is just such a pain in the ass.. Here we have a test cast that overwrites a file repeatedly.  Can we have a test case for an append-only workload?  For instance, you'd have a folder (which is a datum) that contains files, each of which contains a single number.  When you add a file, in the new job you should only see the new file in the folder.. good ol' debug statements. same. Could you summarize what's happening in this file?. What's happening in this block of code?  Are we trying to find the parent job?  Maybe add some comments.  Also this code looks strikingly similar to the block of code above it, so I wonder if there's some opportunities for refactoring here.. No I meant like what changed.. Like maybe just a github comment here about what changed so I know what I'm looking at.. Yeah I tested it.  See the documentation here: https://golang.org/pkg/net/url/#URL. I don't think it returns EOF.  This is just a helper function that's used by both SubscribeCommit and FlushCommit, and FlushCommit does return EOF I think.. It's a subtle bug introduced by another PR yesterday.  There is an assumption in our code that io.EOF should be returned from this function, but if you look at the code below it wraps the error.. I'm kinda confused.  I think you might've misread the code?  It looks like it flushes if raw is false.. It exits when pool.Close() is called, since it closes the watch.. Not sure if I understand this comment.  We are only using 1 port here?. My intention was to leave pool-related optimizations to future PRs.  Here I'm just doing the minimal to get around the bug that a pod cannot talk to itself via service IP.. This package is used in both the pps package and the worker package.. Yeah that's a good point.  There isn't a reason why this has to be two goros.. Oh I see.  Yeah in reality subset.Ports should only have 1 port.. Why are we creating this variable here and returning it later?\nAlso now that I think about it, maybe we can make the following code part of maybeKcCreate, so that we don't have to have duplicated code in 4 different places.  You just need to pass opts as an argument to maybeKcCreate.  Lemme know if that doesn't make sense.. It was an intentional choice to name them rootInputs and directInputs.  I found them more intuitive/descriptive than rawInputs and trueInputs, cuz honestly you can say that rawInputs really means trueInputs and vice versa and they will still make sense.. I think they could be useful.  We need the from flag because someone might be running the migration after they've redeployed their cluster, in which case we need the user to tell us what version they were originally using.  Hiding to from the user also seems unnecessary since we do have the ability to launch migration between any two versions, and if the user doesn't specify to it just uses pachctl's own version.  I don't think we need to intentionally hide power from the user.  You could certainly imagine the scenario where an admin uses pachctl to manage multiple Pachyderm clusters.. make docker-build tags the images as *:local but I needed a way to tag and push images with the commit ID, so I could test a particular build in the cloud.  push-bench-images kinda does that but it also tries to build and push the benchmark image, which is not relevant for what I was trying to do.. Sure that seems reasonable.. Yeah we will probably just need to expand the interface to include information needed to talk to the object store.. This proto file is manually copied from 1.4.8.  make proto doesn't affect it.. This code should probably go into validatePipeline.. We don't automatically recover (since it's unclear how to recover if k8s won't let you create workers), and there's no meaningful action that the user can take other than maybe rebooting the cluster and hope that something happens.  This PR mostly tries to solve the problem that currently if we fail to create workers for a pipeline, all subsequent pipeline creations are ignored.. Can we be more clear that only one of these fields should be set?. Can we check in the validatePipeline function (in pps/server/api_server.go) that only one of the fields is set?  That way CreatePipeline can return an error if the parallelism spec is invalid.. That's what I thought originally too, but turns out that the env vars are set as part of the ReplicaSet which is done by the PPS master when the workers are created.  So the Env field isn't directly used by the workers.. Yeah, in that case they will always get a \"commit not open\" error message.  I did add code that prohibits people from creating branches that are of UUID length, but it's still going to impact people who already have UUID-length branches.  What do you think we should do?. Sure, that sounds reasonable.. Why did we remove this check?. Would it be more reliable to simply use string.Contains()?. Seems kinda weird that we have a function specifically for 10s.  Would it be hard to simply use NewExponentialBackoff, and then manually set the MaxElapsedTime before using it?. Good point.  Updated.. What are tree and treeRoot?  Some comments would be nice.. Can you explain a little bit what's happening here?  What's this hash for?. Am I right in understanding that:\n\nFor each input datum, there's a log object.\nEvery time Logf is invoked, we write the log to this object.. Am I right in understanding that a worker master now prints structured logs?. I thought we don't need ID anymore?. What's the point of including pipeline name in the hash if we are including the salt?  It's a UUID right?. I'm guessing we don't need this.. Nor this.. Has this migration procedure been tested?. Also was it an intentional decision to not remove the old tag?. What happens here if a pipeline is recreated with the same name?. Same question here. Same question here.  It concerns me whenever ID is replaced with Name, since the former changes when a pipeline is recreated while the latter doesn't.  Should we not instead use the salt here?. What's the rationale behind explicitly including a version number in the message name?. I would add one more link break before and after the link to make it stand out more.. Is there a particular reason why this isn't HTTPS?. Probably doesn't matter much in this CLI context but in general ignoring the cancel function leaks a goro.. Yeah, that's a potential problem, but like you noted it does save us a read on very API call.  Maybe the solution is to delete their token from etcd after someone's admin access has been revoked.. Ah, that's a good point, I forgot about that.  Will change.. Good point.  We might still keep a local variable as a cache, since we don't really support \"deactivation\".  But yeah I agree that we should store the activation state in etcd somehow.  Will make the change.. Could you keep the newlines?  They are to distinguish different logical \"sections\" of the API.. Do these changes still make sense given that we are using the Auth API directly?. Activate should take --admins too.. What's the use case for this command?. You don't seem to be actually using admins.. acl.Entries would be a nil map if the Get above failed, I think.. Does this method need to be public?. This should probably be put below the nil commit check below.. Or here?. I wonder if it makes sense to put this into CheckIsAuthorized to reduce boilerplate.. This wouldn't compile. Need to change the function names.. What's this flag for?  I thought this PR doesn't handle user-code batching?. Is this a connection and the number of datums that has been sent to it?  Maybe add some comments?. I was confused by this code at first but upon reading the code below I understand it.  I think we can improve readability by assigning a connCount that doesn't have a conn, instead of assigning nil.  Then, in the Do function when we check if mapConn is nil, we check if it's a connCount that doesn't have a conn instead.\n\nBasically it's somewhat confusing that we are assigning nils to map entries, in order to signal that the entry exists.\nOr, we could also just add some comments here.. I think for atomic variables, you are supposed to use an atomic.Load* function to get the value of it.. hype. Can we make this magic number a constant?. Not sure if I'm missing something, but why is this 10 while the other number is 5?. Good point.. Per our discussion this is not necessary since Watch already returns the current items.. Could this error message be misleading since GetACL doesn't modify anything?. Not that big of a deal, but since these two if clauses have the same code, we could get rid of the code duplication by just or-ing the two boolean expressions.. Not sure if I'm missing something, but what do you have to de-reference here?. Why don't we just return this error like we return any other error?  Why do we need a special authzErr?. Per our discussion this shouldn't be removed.. Wouldn't this race?. Can we make the buffer size even larger to lower the chance that Logf blocks on channel send?. How could there be future calls to Logf after Close is called?. Probably could use a comment here explaining that this is the number of objects we cache.. Oh, that's a good point.. Having a test where cron is combined with other input types would be nice.. Having code with side effects in validateInput seems like code smell.  Could we move this into CreatePipeline?. If we have this, we should remove code that uses a.pachConn.  Code that uses API clients directly (such as pfs.APIClient) could use a.pachClient.PfsAPIClient instead.  We shouldn't have both a.pachClient and a.pachConn (and their Onces).. I could be missing something, but instead of having all these cancel()s preceding returns, can we not just call cancel in a defer (if retErr is not nil)?. The following code looks strikingly similar to the code that handles Atom inputs above.  Are there meaningful differences that I'm missing or should we refactor the code to minimize duplication?. Yeah that won't work.. I think this will require a migration, right?  I think we actually found this issue before (i.e. that indexVal is not printed correctly) or a similar one, but decided to not fix it because it'd break existing deployments.. Shouldn't we test for the reason too?. Well if the message changes we can just update the test.  See how I tested for job reason.  I basically check for a substring.. That makes sense.. mapConnCount?. I could be missing something, but seems like the only way for this to unblock is for new workers to be discovered.  But shouldn't we unblock also when a worker's queue is no longer full?. Using %s with .Error() is an anti-pattern.  You can use %v with the error type directly.. See comment above.. Same here.. Same.. Same.. Same.. Should we just update driver.listRepo if includeAuth is never gonna be false?. Yeah, the ReadWrapper above, combined with io.CopyBuffer, ensures that we will always split messages into chunks of size len(buffer).. The problem with switching to a new hashing algorithm is that it'd require migrations with existing deployments (since the hashes are effectively persisted), so I'm not sure if the benefits outweight the costs here.. I'm not sure if I understand.  It's true that the last chunk may be less than 16MB, but how does it waste bandwidth?. This is a pretty good test.  Can we check the file content here as well?. Not sure if %+v is necessary for err.. Are we making certain assumptions about the user container here?  For instance, what if the container doesn't have /etc/passwd?. I think this fix is no longer necessary after Matt's fix this morning.. Wouldn't it be easier to check pipelineInfo.ParallelismSpec.Constant != 1?. Why do we need docker.sock?. Looks like we are getting rid of a cancel() here.  Is that intended?. gg. %v is more idiomatic I think.. Not sure if I understand.  The point is that if we ever retry in this retry loop, we cancel the context so that all network calls that were triggered in the previous retry iteration are cancelled.. I feel like having three flags is kinda painful.  Personally speaking I prefer a single comma-separated --credentials flag like the way it is.  I guess if I had to give a reason, it'd be that it wouldn't make sense to provide, say, an --id flag without a --secret, though --token is admittedly optional.. The pods need the annotation so they can assume the node's IAM role.  That's just how k8s on AWS works.  It's not really directly used by our code.. The AWS Go client seems to take a Region option, so I'm assuming it's needed for some reason.  I will add a comment.. @emk Thanks for the info.  That's good to know.. Yup.. I need to store the strings so I can compare them later.  Tho I suppose I could also create a new RNG and generate the same readers again, but that sounds more complicated.. The idea is that most objects will be <= 16MB due to how PutFile works today.  So most objects should be cache-able.  The problem was that even though a sidecar had 64MB of cache by default, we are bypassing the cache for 16MB objects since 16 == 64/4, which is not ideal, since we definitely do want to cache those objects.  So I'm changing the code so that the cache is only bypassed for objects larger than 16MB.  In reality, there's only one way for an object to be greater than 16MB, which is to have a huge serialized hashtree.. That's the bug yeah.. The point about how a large object would render the cache useless seems to make sense.  Agree that we should consider the cumulative size with regard to eviction.  I will change it.. This seems redundant since RunFixedArgs already checks for the argument count.. ",
    "JoeyZwicker": "To define merging you have to look at the use case for branching. In git, you branch so that you have a sandbox to design, build, experiment, test, and finally complete some new piece of code without messing up the whole system others while your feature is in development and is constantly breaking or buggy. Merging a branch in git is akin to saying \"I've solved the problem or finished this feature now that's it's working, I want it to be part of the main codebase for others to build on top of.\"\nThe analogous use case for data analysis is developing a MapReduce job to create a materialized view. That MatView is either the resulting data that you want to present (e.g. in a dashboard) or will become the input data for additional analysis later.\nWhile developing the job, you may need all the data, a subset, or maybe just a downsample while building the job, but in the end you probably want it to run over your entire data set automatically in some time interval (e.g. nightly batched analytics, etc) as new data is added to pfs. \nIMO, merging in pfs is basically saying, \"this MapReduce job is complete, produces the MatViews I want, and is now ready for anyone else within the organization to build on top of the MatView data -- whether is a dashboard or additional analysis.\" Merge to me feels like it could be called something like \"publish.\" No one should be building on top of an unfinished (aka \"unpublished\") job because it's still in development and will break too many things further down the pipeline. \nI'm going to continue thinking about this more and drawing more pictures on the whiteboard :)\nOther complications that I foresee and want to address:\n- is there a notion of branching or forking or cloning a job? If so, what would merge/publish look like then?\n- Given the parallel tree structures of commits for raw data and MatViews, how do MatView on top of other MatViews fit into the picture?\n. I meant to update this earlier:\nJD and I talked about this in person. We generally liked the idea of publishing jobs as a method to officially share them, have them built upon, and run automatically on all new incoming data. We'll have to continue exploring different terminology and implementation details as well as see how it resonates with early users. \nAs for \"Matviews on top of Matviews\", I was more talking about the underlying architecture of having materialized views rely on the data of other materialized views. It turns out to be a pretty simple DAG problem that I was just misunderstanding. \n. Woot! JD you rock! Let me know what I can do to help in terms getting any tests or release demo prep.\n. Right now this is done by de-scheduling and rescheduling services in fleet. I imagine this will be improved with Kubernetes integration and will eventually result in a \"rebalance\" command of some sort.\n. New pps supports errors nicely.\n. fixed\n. New pps pipeline tool supports this.\n``` sh\nimage pachyderm/foo\ninput bar\ninput baz\nbar and baz are visible here, qux is not\nrun ...\ninput qux\nbar, baz and qux are all visible\nrun ...\n```\n. @versae We're thrilled that it excites you so much. Us too!  :)\nWe're about to push out pfs v0.4 in the next few hours and I think you'll really like it. It includes support for privately-run docker registries inside the cluster and it's in high-availability mode by default. \nWe also will be rolling out a pretty in-depth integration with git that includes pulling docker files and bunch more cool stuff. \n. This issue is going to snowball into a much bigger API for specifying pipeline priority levels, SLAs, and general order of execution primitives. There's no exact timeline for this feature, but it's something that many enterprise customers are going to need long-term.\n. Minor optimization that we'll implement should it become a problem in production.\n. We've been in discussions with Mesosphere discussing a joint project to get this working, but there hasn't been significant progress in that direction yet. One of the things we're looking for is a customer who wants to use Mesos and Pachyderm to help us sponsor the development and has a specific use case in mind. \n. Closing. Feel free to reopen if this is still a problem.\n. one-line installation is available and there C2D on AWS.\n. A lot of this is available in the new pipelining system\n. released in v0.8\n. We're going to do a massive documentation improvement this release cycle. \n. Closing. @KrishnaPG If you have other questions, feel free ask on GH or email us founders@pachyderm.io. \n. Hmm, https://www.clahub.com/agreements/pachyderm/pfs works just fine for\nme. We'll figure it out.\nOn Sun, Jun 7, 2015 at 2:32 AM, Peter Edge notifications@github.com wrote:\n\nHmm, I've tried both Chrome and Safari and CLAHub and both are returning\nan error. Can you see on your side if it actually went through?\n[image: image]\nhttps://cloud.githubusercontent.com/assets/4228796/8023337/d760441e-0d08-11e5-82e4-02c5fb4b684d.png\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pachyderm/pfs/pull/69#issuecomment-109729341.\n. fixed thanks to #74 \n. Signing the CLA is required for us to be able to merge your changes. It looks like you already did so we're all good to go there.\n\nWe do all our development in the open on GitHub. \n. +1000 We're working on this. This month's release cycle is going to include a ton of documentation that should cover these types of issues. Making that early hacking experience better is huge. \n. part of the documentation purge happening this release cycle\n. We're working on the documentation right now. The plan is to post a distributed grep tutorial in the next few days. The tutorial will include everything from installing Pachyderm and getting the cluster running, to adding data into pfs, and then analyzing it with pipelines. \nWe're really sorry for the delay on this -- we've just been working a ton on the development side of things and the documentation is lagging behind, but that'll be fixed this week! Thanks in advance for your patience and we're thrilled to see your interest in playing with Pachyderm. \n. I already have a start to this glossary under doc/cli/pfs. \n. From a high level, this blog post we wrote last year talks about our overall direction and approach. https://medium.com/pachyderm-data/lets-build-a-modern-hadoop-4fc160f8d74f\nYou're right though, we probably should add a more detailed near-term roadmap. \n. Wanted to add a few more details here based on discussions. There are a bunch of ideas here, not all of which probably make sense to implement.\nThe pipeline should have a trigger struct to let users kick off a pipeline job based on time or other constraints such as amount of data or number of commits. By default, the trigger setting for a pipeline is \"new input commit.\"\nCron/time:\nThis makes the most sense for pipeline with no inputs such as a scraper. Theoretically, a user could want to trigger a pipeline based on time instead of per commit even when it has an input repo so this should probably override the triggering on each new input commit. \nNumber of commits:\nUser may make frequent input commits but only want to trigger the pipeline much less frequently because of expensive \"reduce\" jobs. This could easily be adding files to a datum over time. Some use cases may make more sense to be expressed as time, but this also seems reasonable.\nAmount of data:\nTrigger the pipeline when n MB of new data has come in. The use case here is less well-defined, but we had one user ask if they just streamed logs into pachyderm, they could want a pipeline to trigger after 1GB of logs as built up. \nCombinations: Would it be possible for users to specify some sort of compound trigger argument? E.g. Every 10th commit OR if it's been 24 hours since the last job. <-- seems pretty contrived, but just a thought. \n. hi @lazuhrus. We haven't built any specific database connectors yet. The easiest way to toll your own for an Oracle database would be to just use cron to trigger a put-file request that pulls data from the DB and add its to Pachyderm. The internal architecture of Pachyderm has changed a bit since this issue was last updated, so @jdoliner can add a few more specific details.. I think a combination of cron, our split functionality and/or spouts should cover this use case already. We'll want to create examples and docs about it but we already have some: https://github.com/pachyderm/pachyderm/tree/master/examples/db. This may be obvious and our current sharding implementation already supports this: If the downsample factor doesn't change when a pipeline is updated, the input data should remain the same, not resampled each run or update. \n. @jdoliner We'll take that under advisement and add it to the alternative names list. :P\n. > I was expecting it to just be create-pipeline which automatically runs the job once but not more (see #246).\nI was thinking this. We don't want users to make two commands to run a pipeline. \nI can't think of another strong use case for run-pipeline that not my second reason, but I feel that there is one. I'll think on it more. \n. I'm thinking these constraints would be set for both starting and finishing commits. To me, this basically seems like letting the user script starting/finishing commits within Pachyderm.\n\nby whom?\n\nI'm not sure what this means\n. I guess, my original comment did explicitly say \"finish commit\". I was kind of living under the git assumption that you'd always have an open commit, but that's not how we want users thinking about it because data in \"dirty commits\" isn't safe and there could be a lot of it. So yes, they'd have to specify behavior for both starting and finishing the commits. \n. Bringing info over from #235 \nWanted to add a few more details here based on discussions. There are a bunch of ideas here, not all of which probably make sense to implement.\nThe pipeline should have a trigger struct to let users kick off a pipeline job based on time or other constraints such as amount of data or number of commits. By default, the trigger setting for a pipeline is \"new input commit.\"\nCron/time:\nThis makes the most sense for pipeline with no inputs such as a scraper. Theoretically, a user could want to trigger a pipeline based on time instead of per commit even when it has an input repo so this should probably override the triggering on each new input commit.\nNumber of commits:\nUser may make frequent input commits but only want to trigger the pipeline much less frequently because of expensive \"reduce\" jobs. This could easily be adding files to a datum over time. Some use cases may make more sense to be expressed as time, but this also seems reasonable.\nAmount of data:\nTrigger the pipeline when n MB of new data has come in. The use case here is less well-defined, but we had one user ask if they just streamed logs into pachyderm, they could want a pipeline to trigger after 1GB of logs as built up.\nCombinations: Would it be possible for users to specify some sort of compound trigger argument? E.g. Every 10th commit OR if it's been 24 hours since the last job. <-- seems pretty contrived, but just a thought.. This feature is likely something we'll still need. Has been requested by users frequently and came up at Kubecon a few times.. What would the parental structure look like with a cancelled commit? Would the next started commit know to trace the tree back until the latest non-cancelled commit and make that it's parent?\nAlso, has to deal with branches. \n. I don't think we want to be encouraging empty commits or \"trigger\" pipelines as a hacky way to get around users needing to manually start pipelines or have them triggered by cron. \n. Currently this is true, but #235 exists because I don't think we want this to be the case for users asap\n. There are a few open discussions/issues around adding a run pipeline command (\"running a pipeline manually\") and creating a pipeline with no input repos that instead gets run based on some timing parameter, something like cron.\n. It's been moved to https://github.com/pachyderm/pachyderm/blob/master/examples/fruit_stand/GUIDE.md\nWhere did you see the incorrect link?\n. Fixed. Thanks @hsaputra \n. About to get a the plane but I'll do full editing pass PR tomorrow. \n. docs/faq.md doesn't exist yet. I actually have a long FAQ doc that I wrote up a few weeks ago that I need to add there (and will likely go on the website). And I think it should be \"docs\" not \"doc\" and I want to change that back. :) @jdoliner \n. nothing. sorry. I'm working on a PR to add /docs/FAQ and I just wanted to let you know i'm changing \"doc\" to \"docs.\" If you have a strong disagreement with that because you like everything as 3 letters, then  speak up. \n. This is 5000% bikeshedding so I'll drop it. Still irks me tho. \n. Travis doesn't have support for FAQs...\n. There's also a use case where you'd want the input a repo and commit that is the result and track provenance up the commit tree to see what data went into it. \nI think the provenance command should be able to go both up and down the full parental structure. provenance should return a full accounting of everything that went into creating a set of commits as well as everything that was triggered by them. We could have a behavioral toggle for either direction or just have it do both ways by default. \n. @munchee13 Hey Chris, we're working on testing out Pachyderm on OpenShift, but we're getting a bit stuck because of our unfamiliarity with OpenShift and how those primitives translate to Kubernetes'. Any chance you'd be able to help us out with the OpenShift side of things? @jdoliner can chime in with more specific questions.\n. Chris also suggested:\nMaybe try with just the all-in-one VM instead: http://openshift.org/vm\n. @sambooo \n. @adrianog Yes, we've seen successful Gluster deployments in the past. It's not something we've tested much ourselves, but let us know if you run into any problems. . Except for potentially changing the example, everything looks good.\n. We discussed potential other tweaks the example offline. Let's merge this for now and we can always change the example a bit with feedback from @jdoliner \n. A user suggested we add another non-NLP tensorflow example similar to this at some point in the future. https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html\n. Yup i'm on this. We have a few other links broken too. \n. Thanks for pointing those out and for the PR Anchal! Link stability on GH is a constant problem. :) \n. Thanks @anchal-agrawal!\n. Thanks so much for finding all of these! I can just fix both of those now myself. Great catch!\n. We get questions about timeout errors during cluster deployment such as\npachd starting up before rethinkdb is ready. Would be good to document that\nthis is normal.\nSame for fuse throwing benign errors\nOn Fri, Jul 1, 2016 at 12:16 PM, Sean Jezewski notifications@github.com\nwrote:\n\n(going to start listing questions we've seen more than once here so we\nremember what to add to these docs)\nWe've fielded a few questions for PullPolicy settings for custom\nmanifests. Lets ID the symptoms for when this is set wrong (e.g. when you\nhave old features bcz the new default is Always when you're trying to use\nnewly compiled versions so you should set it to IfNotPresent)\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/543#issuecomment-230027167,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/ABtjwHjMSj_drg9gPl_i7vKuR6nBulp1ks5qRWeTgaJpZM4I25Y-\n.\n. Hi @KshiteejK, this hasn't been a feature requested by users too often, but it's recently come up a few times and something we're looking to slot into near-term development. Could you tell more about your current use case and why this feature would be useful or required for you? The more data points we can get from users, the better we can prioritize this feature and make sure it addresses your needs. Thanks!. @suneeta-mall -- relevant to our recent discussions. The implementation details above are probably quite out of date. Capturing our offline conversation. Change job-shim name to \"default\". Have\nno image specified in Fruit stand. Add mention of what going on in\ntutorial, \"Pachyderm will use a default image. if you want to use a custom\nimage, check out this guide.\" Add the same under Next Steps at the end of\nthe guide.\n\nOn Fri, Jul 15, 2016 at 1:36 PM, Joe Doliner notifications@github.com\nwrote:\n\nAfter thinking about this a bit more I'm not totally convinced that\npushing the image to docker hub is the right solution here. The argument\nfor it is that it helps people who are trying to run the example on a\nDocker host that they don't have direct access to (ie on AWS) complete the\nexample. But I think that means that it compromises the efficacy of the\nexample as well. If they want to write their own jobs then they're going to\nneed to solve this problem anyways. Sidestepping it in the demo just means\nthey have no guidance on how to do it for real.\nI think a better solution to this problem is to just remove the custom\nimage and have the demo reference pachyderm/job-shim, that way the first\ndemo people do is more bullet proof and people are more likely to complete\nit successfully and later demos can teach people how to use custom images.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/559#issuecomment-233063443,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwBtQyr7jGAzcgCcoYgw2Yn_Tmp_uks5qV-9RgaJpZM4I44qU\n.\n. @derek Is there a way to stop fuse from throwing those errors if they are\nmeant to be ignored? When I was doing the demo this week it messed me up a\nlittle too.\n\nOn Thu, Jun 30, 2016 at 9:14 PM, Soheil Salehian notifications@github.com\nwrote:\n\nClosed #611 https://github.com/pachyderm/pachyderm/issues/611.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/611#event-710037729, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/ABtjwFXeStASJ0tVeRebiW5Eu9SAgtNWks5qRJQTgaJpZM4JCxVg\n.\n. @jdoliner Seems like this fuse error showed up again, possibly with the new grpc update. We should silence the Spotlight errors again. \n\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ pc mount ~/pfs &\n[1] 93850\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ Filesystem mounted, CTRL-C to exit.\nERRO[0000] directory:<file:<commit:<repo:<> > > > name:\"DCIM\" err:\"repo DCIM not found\"\nERRO[0000] directory:<file:<commit:<repo:<> > > > name:\".Spotlight-V100\" err:\"repo .Spotlight-V100 not found\"\nERRO[0000] directory:<file:<commit:<repo:<> > > > name:\".metadata_never_index\" err:\"repo .metadata_never_index not found\"\nERRO[0000] directory:<file:<commit:<repo:<> > > > name:\".metadata_never_index_unless_rootfs\" err:\"repo .metadata_never_index_unless_rootfs not found\"\nERRO[0000] directory:<file:<commit:<repo:<> > > > name:\"._.\" err:\"repo ._. not found\"\nERRO[0000] directory:<file:<commit:<repo:<> > > > name:\".metadata_never_index\" err:\"repo .metadata_never_index not found\"\nERRO[0000] directory:<file:<commit:<repo:<> > > > name:\".metadata_never_index_unless_rootfs\" err:\"repo .metadata_never_index_unless_rootfs not found\"\nERRO[0000] directory:<file:<commit:<repo:<> > > > name:\"._.\" err:\"repo ._. not found\"\nERRO[0000] directory:<file:<commit:<repo:<> > > > name:\"._.\" err:\"repo ._. not found\"\nERRO[0000] directory:<file:<commit:<repo:<> > > > name:\"._.\" err:\"repo ._. not found. We actually do get various forms of this request. Even had it come up in slack channel today. We don't need it for blocks because we only deal with files (updated issue name). It's an easy feature to support so not sure where it'll land on our roadmap, if at all, but it's still a real request worth thinking about. . Not sure if this is relevant to the conversation, but one of our big\nenterprise customers was thinking of building a \"data catalogue\" on top of\npachyderm using elastic search and contributing it. The goal is it would\nsuck in all this type of metadata info about Pachyderm and then allows\npeople to query it.\nThis is a long-term plan by them, but the discussion items they had in mind\nwere things like:\n- Show all commits that contain file foo\n- Show me all files that went into creating file foo (which I believe\nexactly what you're asking above)\n- Data governance things such as Show me all commits added by UserX.\n- Show all commits in repoX that added more than 100MB of new data.\n- List our all pipeline specs that were used to create commit123. This\nrequires multiple hops and is easily scriptable right now, but not a\nnatural view in provenance.\nThese were just a few ideas ask by customer. All of these things (except #3\nright now) is info that technically exists in Pachyderm, but isn't easy to\nglean/query. Food for thought.\nOn Thu, Jan 31, 2019 at 4:56 PM Gabriel Grant notifications@github.com\nwrote:\n\nOne way I think this could possibly be done more easily/efficiently with\nsome minimal support from pachyderm, is through use of the datum factory to\nbuild a list of datums by (input) file, rather than building that list by\nquerying and re-organizing info in the stats branch\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/625#issuecomment-459565528,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwOX64pPtMU2RIKWj33WLX2gAbra8ks5vI5CzgaJpZM4JGiMc\n.\n. Not supported anymore. @dwhitena I think we've talked about adding this as a cookbook multiple times.We got stuck trying work on it during 1.6, but might be worth revisiting. We have a lot of users who are starting and ending with SQL data and we'd like to give them a best practices usage pattern.. How do you actually set up an S3 bucket for a local deployment? pachctl deploy local doesn't offer an option to take S3 so users would have to update the manifest manually?. You can create a 4th commit where you delete the one incorrect item and add the correct one. You don't need to go all the way back and branch. \nOn Jul 17, 2016, at 11:34 PM, Luo Shengjie notifications@github.com wrote:\nFor example, if for the first time I submit 40GB data, the second time I submit 20GB data, and the third time I submit 40GB data. Totally, 100GB is stored in the disk.\nNow, I find one data item in the second submit need to be revised. Then is it necessary to come back to the first version, and then repeat the second and third submit? If I do need this, there will be 100GB(first branch) + 60GB(second branch) data in the disk. Is it any method to revise only one item?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Hi @ShengjieLuo and @montenegrodr So sorry you guys are having trouble with this. You should be able to get to image you need from DockerHub at pachyderm/job-shim. If you change the pipeline \"image\" descriptor from Fruit_Stand to job-shim, it should automatically pull it correctly from DockerHub. You'll have to delete your current pipeline and then recreate it with the new image field. \n. Outdated. We have some docs of various parts of this and it will be documented in more details when current features land in 1.7 such as #2414 . Per conversation with JD.\n\nUpdate pipeline shouldn't archive in all cases such as changing\nparallelism, debug, or stopped. Would also makes sense to add a\ndont-archive flag of some sort for users\nOn Mon, Aug 8, 2016 at 11:56 AM, Joe Doliner notifications@github.com\nwrote:\n\n@derekchiang https://github.com/derekchiang whenever you have a chance.\nThis is probably ready for a review.\nSummary of changes:\n- The high level change here is that pipelines can now be update. It\n  uses the same rpc as CreatePipeline it just adds a flag Update. This\n  made the implementation easier since the rest of the API is identical.\n- When a pipeline is updated its previous output is Archived this\n  prevents those commits from showing up in ListCommit and FlushCommit\n  but the data can still be accessed.\n- To get the synchronization of updating the pipeline and archiving\n  the commits right I had to add StopCommit and StartCommit methods.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/pull/694#issuecomment-238340277,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwGLrTIA5bt6cjq8iCsDdM0OA7nB_ks5qd3vdgaJpZM4JXtd-\n.\n. LGTM\n. @jmbrunskill Thanks for catching this! Definitely confusing (and broken) the way it was written before. \n. Sorry you ran into this. Yeah there are some version mismatch errors with\nour examples right now, but our new docs site has all of these fixed and\nwill be released in the next 2 weeks.\n\nOn Mon, Sep 5, 2016 at 12:11 AM, Luo Shengjie notifications@github.com\nwrote:\n\nHi! For some reasons, I deploy the pachyderm cluster in local server with\ncurrent version. Some problems I meet in the deployment are list as\nfollowing.\nKubernetes version:\nkubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.2\", GitCommit:\"528f879e7d3790ea4287687ef0ab3f2a01cc2718\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.2\", GitCommit:\"528f879e7d3790ea4287687ef0ab3f2a01cc2718\", GitTreeState:\"clean\"}\nPachctl version:\npachctl version\nCOMPONENT           VERSION\npachctl             1.1.0-ba43f31be2be7e013f1373eac6581f9093f37f48\nUse make launch to launch the pachyderm, then we meet problems,\nkubectl get all\nNAME                   READY        STATUS             RESTARTS                       AGE\netcd-ukarn             1/1          Running            0                              4m\nk8s-etcd-127.0.0.1     1/1          Running            0                              50m\nk8s-master-127.0.0.1   4/4          Running            3                              51m\nk8s-proxy-127.0.0.1    1/1          Running            0                              50m\npachd-40dqd            0/1          ImagePullBackOff   0                              4m\npachd-ic0zi            0/1          ImagePullBackOff   0                              4m\npachd-init-r4gb8       0/1          ImagePullBackOff   0                              4m\nrethink-1ezrq          1/1          Running            0                              4m\nImagePullBackOff lasts for a long time and no results.\nUse make launch-dev to launch the pachyderm, we can deploy Pachyderm. But\nthe version of Pachd and Pachctl is not the same.\npachctl version\nCOMPONENT           VERSION\npachctl             1.1.0-ba43f31be2be7e013f1373eac6581f9093f37f48\npachd               1.1.0\nHowever, the pipeline does not work because of the version conflict.\n./examples/fruit_stand/run.sh\n- pachctl create-repo data\n- pachctl start-commit data master\n  c4976921bab1419299fb37c76f063f70\n- cat examples/fruit_stand/set1.txt\n- pachctl put-file data master sales\n- pachctl finish-commit data master\n- pachctl create-pipeline -f examples/fruit_stand/pipeline.json\n  error from CreatePipeline: proto: bad wiretype for field pps.Transform.Debug: got wiretype 2, want 0\nUnfortunately, the local Pachyderm deployment fails.\nTurn the Pachctl version back to 1.1.0. The reason why I use pachctl\n1.1.0-ba43f31be2be7e013f1373eac6581f9093f37f48 before would be explained\nin the following comment.\nCOMPONENT           VERSION\npachctl             1.1.0\npachd               1.1.0\nRUN ./examples/fruit_stand/run.sh. The example has a problem in JSON file.\n- pachctl create-repo data\n- pachctl start-commit data master\n  9c1cf5de91d0441682d32b7560beea0a\n- cat examples/fruit_stand/set1.txt+\n  pachctl put-file data master sales\n- pachctl finish-commit data master\n- pachctl create-pipeline -f examples/fruit_stand/pipeline.json\n  error marshalling JSON into protobuf: json: cannot unmarshal string into Go value of type bool\nRUN word-count example. No problem.\nIn conclusion, please recheck the following steps in deployment.\n1. Fruit_stand example has a problem in JSON file.\n2. Using make launch-dev instead of make launch does not obey the\n   mannual\n3. Make install give the Pachctl 1.1.0-ba43f31be2be7e013f1373eac6581f\n   9093f37f48 instead of Pachctl 1.1.0 which lead to the version\n   conflict. We have to change it to Pachctl 1.1.0 by curl -o\n   /tmp/pachctl.deb -L https://pachyderm.io/pachctl.deb && dpkg -i\n   /tmp/pachctl.deb\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/778, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwIb3YFhBd1SoCFAWKfpJIwAlM0Nvks5qm8CXgaJpZM4J0y6x\n.\n. Obviously this doesn't let you leverage the distributed properties of Pachyderm, but you can easily run pachyderm locally on OSX or Linux using our Quickstart guide. We've got a MUCH better guide coming within a week that shows you how to easily run Pachyderm in minikube locally too. It hasn't been pushed out yet but you can also email me and I'll give you early access. \n. Yeah no need to spend money if you just want to try things out locally. Below is an abridged version of the guide using minikube. That should get you most of the way and feel free to post here if you run into any problems you can figure out.\n\nThis Minikube Guide will get you kubernetes running locally.\nYou then just need to get pachctl installed; \nshell\n$ brew tap pachyderm/tap && brew install pachctl\nDeploy Pachyderm\nkubectl create -f https://pachyderm.io/manifest.json\nWait for pachd pods to show \"running\" and then forward the ports:\n```\nkubectl get all\ncopy one of the pachd pod names\nkubectl port-forward  30650:650 &\n```\npachctl version should return:\nshell\n$ pachctl version\nCOMPONENT           VERSION\npachctl             1.1.0\npachd               1.1.0\n. Sorry you ran into this @rexmortus! You've got a version problem in that you're using  pachd 1.1.0-ba43 but we only have v1.1.0 container pushed to DockerHub. You'll either need to build the container from HEAD or change the manifest to v1.1.0 instead. \n. @jdoliner Wanna check on this or should I merge myself?\n. fixed. Gimme dat sweet meme love?\n. We thought that deleting the extra toc tree in the Analyze Your Data section would solve this. I just did that and it doesn't fix it. Also seeing some other odd behaviors like sections I've deleted still showing up in the sidebar -- I don't think it's caching because I've reset the cache in my browser and I'm just viewing local files.\n. nevermind, it was a caching problem of sorts. I didn't blow away my past build files correctly like I thought I had. \n. The major issues with multiple references has been solved. We just need to check on getting nagivation_depth to work or not: https://github.com/snide/sphinx_rtd_theme/issues/281#issuecomment-245805348\n. I hadn't tried this locally ( I will shortly), but it doesnt seem to be working on the live RTD site. \n. @sjezewski Yeah it seems to have built correctly on https://readthedocs.org/projects/pachyderm/builds/ but still has the dropdown on RTD. I'm going to test locally now, but open to theories if you have any\n. Works locally for me. not sure why its not working on RTD. \n. @sjezewski Does this solve our problem perhaps? RTD has a place to declare your requirement file? I would assume it automatically grabs its from the root of the project, but maybe not? https://readthedocs.org/dashboard/pachyderm/advanced/\n. +1 I even struggled with this a bit last week and I work here.\nOn Fri, Sep 9, 2016 at 3:33 PM, Alec Sloman notifications@github.com\nwrote:\n\nHello again gentlefolk! \u2764\ufe0f\nPresently, pachctl provides mount which mounts the pachyderm filesystem.\n\ud83c\udd97\nWhen you're ready to un-mount pfs, it might be a nice feature (in\nparticular, for those less-experienced with *nix) for pachctl to provide\na \"safely\" un-mount pfs command.\nOf course we can use fusermount -z or umount, but I imagine that some\nundefined slice of potential users won't know much about\nmounting/unmounting filesystems, let alone how to interpret such messages\nas:\nls: cannot access pfs: Transport endpoint is not connected\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/815, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwO-hcQOK0kVa3bny3EieQvHturNaks5qod7KgaJpZM4J5iBM\n.\n. hey @rexmortus I'm also going to add you our users slack channel so you can reach us more directly there for help.\n. Thanks @jbowles We'll fix that right away. @sjezewski \n. I like the idea of mentioning the GF case study in the blog post\n. It's up to you re: contributors. GH technically already shows contributors so it doesn't make sense to have an md file for that. I just noticed a few other Changlogs (rethink being one example) that call out contributors the same way you did for groupcache in the blog post. Ack-ing people also makes them more likely to contribute to the next release and that's something we want to encourage as much as possible\n. I still think we should add contributors, but the rest LGTM!\n. pachyderm.readthedocs.io has an ad display in the bottom left. It must be something from readthedocs and we just noticed it when we pushed. Sean was there so I didn't add more info. We want to figure out how to get rid of it, obviously. \n. We started paying for RTD and got rid of ads. Also got a few new nice features we can consider using such as a Canonical URL such as docs.pachyderm.io instead of pachyderm.readthedocs.io.\n. LGTM\n. I'll add these to the docs. Are the help functions for these are already written so that'll be autogenerated?\n. Just to make sure I'm understanding correctly, the restriction would be that http://malicious.com/files could only contain URLS, not data files?\n. I think this can be closed. . I already did that in my docs branch. Will be merged shortly. Also updated\nin the incrementality section because those docs overlaop.\n\nOn Thu, Sep 22, 2016 at 7:07 PM, Joe Doliner notifications@github.com\nwrote:\n\n@msteffen https://github.com/msteffen could you update doc/development/\npipeline_spec.md to reflect that it has to be caps too?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/pull/884#issuecomment-249083513,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwJ2LTZBbvZXLm97CXMNe2jP5hvjEks5qszRfgaJpZM4KEeoz\n.\n. LGTM\n. I dont think it was because an internal registry wasn't useful, but that\nsecurity for it is annoying enough right now that it's not really a viable\noption. Anyway, the reason doesn't matter, just wanted to clarify the info\nin this issue for when we come back to this later because an internal\nregistry is probably something we may eventually want.\n\nOn Wed, Nov 23, 2016 at 6:51 PM, Derek Chiang (Enchi Jiang) \nnotifications@github.com wrote:\n\nClosed #896 https://github.com/pachyderm/pachyderm/issues/896.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/896#event-870552693, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwL3PIFD5v95vixZfvdC3-VhSpCaSks5rBPuegaJpZM4KHCzN\n.\n. Definitely! Sorry about the slowness on that one. :)\n. This is actually a bit tricky and a solution isn't immediately obvious to me. These docs are not technically meant to be read on Github as all the links here on the docs portal work fine.  \n\nOne solution would be to hardcode in the urls instead of having them be relative links. We'd lose RTD's link-checking if we did that. Also, our other docs on GH have links that don't really work too, we just \"intercept\" users with a README sending them to our docs portal before the navigate there. We can hide the PFS/PPS docs in a folder and do that same if that'd be better. What do you think @derekchiang ?\n. But than that header shows up on the RTD site, which looks kinda dumb. But\nyeah, it's an easy stopgap.\nOn Mon, Oct 3, 2016 at 3:13 PM, Joe Doliner notifications@github.com\nwrote:\n\nA good stop gap measure might be to just put a header at the top of each\ndoc with a link to the doc on RTD saying: \"This documentation is meant to\nbe read at: ____.\"\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/913#issuecomment-251242894,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwLqB0tNb5y70D9ywEzwD-J3BOQMEks5qwX4QgaJpZM4KJffH\n.\n. I think we have a plan for this. We're going to add a new folder to contain those top-level docs and a readme for each directory that pushes people off of GH to RTD instead.\n. We accidentally still link to them on the getting_started page -- we should fix that since the docs aren't done yet. . Thanks @JustinTRoss for catching those typos! LGTM\n. @ukd1 Thanks Russ!  Would you mind signing our CLA and I'll get this merged in right away.\n. @sjezewski @msteffen Is this relevant?. Thanks @html5cat! If you could sign our CLA, i'll get this merged in right away. \n. This is a great idea. Let's add this to 1.3 milestone as a docs upgrade that shouldn't be too hard.\n. Sorry you ran into that. Give me a few minutes and I'll fix that. \n. Should be working again. @mwaaas \n. Your version of pachctl is out of date. pachctl deploy was added in v1.2 \n\nbrew uninstall pachctl && brew untap pachyderm/tap and then `brew tap pachyderm/tap && brew install pachctl``\npachctl version should show v1.2.1\n. Ah, I see the problem. The command should be pachctl get-file sum\n2a5de2d05f0c41b9abccee30cd6b4aaa/0 apple. get-file takes the arguments\n  . Sum is the repo name and\n2a5de2d05f0c41b9abccee30cd6b4aaa/0 is the commit ID. I can definitely see\nhow that's confusing so we can change the headers there to make that\nclearer.\ndoes pachctl get-file sum 2a5de2d05f0c41b9abccee30cd6b4aaa/0 apple return\ncorrectly?\nOn Thu, Oct 6, 2016 at 3:52 PM, Brent Hueth notifications@github.com\nwrote:\n\nI think so? Here's what I see:\npachctl create-repo data\npachctl put-file data master sales -c -f set1.txt\npachctl create-pipeline -f pipeline.json\npachctl list-job\nID OUTPUT\n5eebd488fbd96dee3dc4d746a879b14c sum/2a5de2d05f0c41b9abccee30cd6b4aaa/0\n67c30d70ba9d2179aa133255f5dc81db filter/0fc94f46efb4401090ea09c22c948fca/0\npachctl list-repo\nNAME CREATED SIZE\ndata About a minute ago 874 B\nfilter 22 seconds ago 400 B\nsum 22 seconds ago 24 B\npachctl get-file sum sum/2a5de2d05f0c41b9abccee30cd6b4aaa/0 apple\ncommit sum/2a5de2d05f0c41b9abccee30cd6b4aaa/0 not found in repo sum\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/948#issuecomment-252110706,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwGCNukBvghEmx5H7jgG6Wo_UNHGwks5qxXvJgaJpZM4KPrO6\n.\n. I'm all for unifying, but just to play the other side, we could also print\nthe repo name and commitID in separate columns for list-commit\n\nOn Thu, Oct 6, 2016 at 6:30 PM, Joe Doliner notifications@github.com\nwrote:\n\nWe have a few competing ways that we represent pfs objects in pachctl.\nSometimes we use spaces to delimit them like in:\npachctl put-file repo commit file\nBut sometimes we use slashes like in:\npachctl flush-commit repo/commit repo/commit...\nWe should unify this to always using slashes, spaces do look a little bit\nnicer but it gets confusing if you need to reference more than one object.\nThat's actually the reason that flush-commit uses slashes. Unifying to\nslashes should be less cognitive overhead for people and lead to fewer\nerrors.\nThis is in response to an issue a user hit where he got confused and tried\nto do:\npachctl get-file repo repo/commit filename\nUnderstandable how that could happen given that's how we print commits in\nListCommit.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/956, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwOH2a6bNGLW9FsqEH4vDPCaOET3Mks5qxaC4gaJpZM4KQlqP\n.\n. I didn't actually check yet to see if the /prev PR landed and what release it's under, 1.2.1 or 1.2.2(e.g. HEAD right now, or not at all yet), but I won't change the docs until it is. It's on this list as something to check on.\n. Hey @jpoon! This would be great and we're here to help! I think that basically covers it!\n. @jdoliner After seeing this issue I immediately realized this is what broke our live demo today. I had moved files around since the Strata demo so my first put-file request couldn't find what it was looking for and created a cancelled commit. Even though my other requests were correct, it wouldn't show up. List-repo also showed that there was data from later correct commits even though they were cancelled. \n\nDamn -- I should have just tried deleting the repo and doing put-file again, but that's why live demo demons suck. Bummer.\n. This is still a problem that still bites people. \nAfter thinking/talking about this more, it seems like put-file just isn't a good place to be using cancelled commits. If a put-file fails, keep that commit around isn't useful and we should basically just delete it. We don't have commit deletion yet. \nThis also has the implication right now that a single bad put-file request can effectively \"kill\" a whole branch by making everything else downstream cancelled as well (as Matt said above). \n. Im too tired to meme. LGTM\n. This is great. LGTM\n. Old with no activity. LGTM\n. LGTM\n. pachctl deploy checks your version of pachctl and then tries to get that image from DH, right?\nYou'd need to have the version of pachctl that matches the version you want to deploy. Alternatively, would it make sense to have pachctl deploy check to see if the container is on DH (or just know that that's only true for point release versions) and if it's not, then docker build pachd and use that?\n. As we discussed offline, we should move the documentation for this to a relevant section (either advanced or misc probably make the most sense for now until we add a production section). Make sure to update the relevant table of contents too. \n. docs move part LGTM. Someone else will have to comment on the actual code. \n. Since Matt and I wrote and debugged this the first time, I'll try to take a crack at it today if @msteffen has some time to help\n. Ran through the guide and turns out we dont need to add anything to get this to work. They seem to ahve fixed the k8s start script. Updating our guide to not include those 3 lines and closing until someone runs into a problem. \n. These docs have been updated to include a location constraint.. Github says, \"135 additions, 112 deletions not shown because the diff is too large.\" So I can't oficially say LGTM because I'm missing the L, but this is all just autogenerated so should be good to go. \n. This still need to be merged?. pachctl.md is autogenerated. All the links need ../pachctl/ in front of them. I think this needs to be added to make assets or whatever we use to autogenerate the pachctl docs. I could obviously change the links in the md file, but that doesn't solve the problem and I probably shouldn't be the one to futz with the make file. \n. adding an rst file for now. I'll touch up the rst to include 1-liner cmd summaries. . CI keeps failing... are we sure these are just flaky tests?\n. I can confirm that you changed /tmp to /var, but I cant actually confirm it does what we want. LGTM\n. +1 to this issue from the PhenoMeNal team. @gabrielgrant @jonandernovella is the Dash part of the helm chart all sorted out or still a WIP?. I agree with @LaurentGoderre on this I think. \n@LaurentGoderre Pachyderm has plans to make this Helm chart officially supported, which will involve making it more user friendly and keeping it updated as part of our official release process. We haven't quite gotten there yet, so our continued thanks to @jonandernovella and others for maintaining the chart in the interim. . We continue to see requests for this. I may make sense to evaluate non \"simple fix\" options -- plus its been a year and there may be other libraries that can do this out of the box. Marking for 1.7 so it gets discussed as part of the planning. . LGTM. LGTM. Good catch @benjaminsims. Can you sign our CLA?\n. On second thought, @jdoliner aren't these docs autogenerated? So is this something that needs to be fixed elsewhere?. @sjezewski  Does this still need to be merged? If so, can you take a look at this since its auogenerated docs stuff. This seems to be fixed already. closing. \nYou should check to make sure there aren't any other instances where we use about pachctl deploy local. The pachctl docs might be need to be auto-generated again.\nOther than that, LGTM when CI passes. \n. CI failed again -- it flaky. Regardless -- this was actually already merged via #1131 . @dwhitena Can you check the docs to make sure there aren't any other places where we need to update this and if not, just close this PR. . dont forget docs for services :). Have the server check to make sure parallelism is set to 1 because anything else would get weird errors. Ideally a services spec could just omit parallelism entirely, but right now in our system that defaults to the number of nodes which is incorrect.. closed via #1155. I agree that this could be better, but there are a few options. They could start-commit each day, stream the logs lines to pachyderm as they come in and finish-commit at the end of each day. Alternatively, if you wanted the log file to live in Pachyderm and in the application, having a different log file each day could be ok too. Neither solves exactly this use case though. \n. Just to clarify, it's not that the aliases were incorrect, but they were a bit hard to reason about. In this scenario we had a CSV file with column headers and even though the pipeline operation was multiplying each value in a column by 3, which \"felt\" like a map to the user, we needed to use reducebecause the header was needed to be seen by each container so the whole file had to be kept together. \nAs JD mentioned, this would also be a reduce in Hadoop because of the headers. It probably would have been better to omit the headers from the file and retain that information another way. . Dan or I can certainly put something together for this. We're racking up a bunch simple pipeline arrangements that could be documented -- they're not full-fledged data science examples or anything so should be easy to crank a few of these out. . LGTM. Thanks @orenyomtov!. @sjezewski not 1.3 urgent of course. Stable is latest point release, right? LGTM. I actually dont think you need to delete the .md file because RTD will choose the rst over the md I believe. I tested this back when we used to have the rst (we had both at time). The md is still autogenerated too and unless we change that people will have to manually delete it every time. . the rst addition LGTM. This is great Sean! LGTM. LGTM. @sjezewski minor docs fixes. . LGTM. @sjezewski I think this is ready to merge. The conflict is literally one line in the readme I added in a different PR that accidentally got merged before this one. Any chance you help me resolve that and merge this is as I have to run. @sjezewski Tensorflow CI is failing is some way. Can you take a look and help me get this merged?. Redshift is also very common and I assume was included in \"friends.\" \nElastic Search is a common one too.\nKafka has come up occasionally -- being able to egress data as a kafka topic (I don't really understand the terminology of kakfa so I might be using it wrong)\nHDFS has obviously been requested. \n. there are a number of workflows where users need to generate the manifest using dry run to troubleshoot and make sure things are getting set correctly. I think the impetus was document that somewhere. Not super important. Thanks @jnevin for taking the time to write this up and sorry you ran into this! v1.3 came out right before k8s 1.5 and this naming change broke a few things for us. We're going to get this fixed asap as there are a few other k8s v1.5 issues that have cropped up too. \nFor future reference, you can also use the flag --deploy-rethink-as-rc with pachctl deploy which will deploy it as a replication controller as before instead of a PetSet/StatefulSet. For local deployment, having Rethink as a PetSet doesn't really get you as much anyway. :)\nRegardless, we'll definitely get this all tidied up ASAP. @msteffen . +1 I'll put it on my list and get to it soon since everyone is bust with other things. That's a great idea IMO. Maybe it makes more sense for the PR to be that change and include updating the docs to reflect it and push users following our AWS deployment steps to set it up correctly. \nThinking about this more, I actually don't know squat about dealing with AWS tokens or dealing with iam users, so assigning this to myself probably isn't the best answer as I'll need help anyway.. I think other pachctl commands never time out if port-forward isn't set. create-repo definitely has the same behavior.. Update pipeline isn't exactly the same and create plus delete plus create pipeline again because the initial create also makes an output repo of the same name. The second create then can't create that repo but the current repo foo might contain data from the previous instantiation. We don't want to wipe that data (necessarily) so the create-pipeline fails. This is expected behavior although I think there was a different open issue discussing having delete pipeline automatically remove its output repo, but that's not the current implementation. \n\nOn Dec 18, 2016, at 2:25 PM, Matthew Steffen notifications@github.com wrote:\nI apologize if this is already recorded somewhere, nor am I sure what the expected behavior actually is (or if this is even a bug), but I wanted to write it down before I forgot:\nif you run pachctl create-pipeline foo -f foo.json and then pachctl delete-pipeline foo and then pachctl create-pipeline foo -f foo.json (so roughly equivalent to update-pipeline), you get repo foo exists.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Hey @alexisvincent -- Just fixed that so you should be able to join slack. Sorry about that! We're absolutely going to fix this with either gitter or something similar. . Hey Milan, sorry for the delayed response here as holiday schedules have been hectic and this seemed to have slipped through the cracks. \n\nI'd love a chance to talk in more detail and understand fully what you mean by \"roll back the clock.\" Commits in Pachyderm are immutable and you'd expect from a git-like system, but you can correct files in a later commit and reprocess it to get the updated result. I think our system would do what you want, but it'd be easiest to talk about it live and fully dive into the details. \nAs for working directly with Spark, you can also similar have a spark cluster constantly running and pachyderm and push work over to it. Similarly, it'd be really helpful to understand the details of what you're looking for and discuss how our current system could work as well as our roadmap for more full-featured Spark support.\nEmail me at joey@pachyderm.io and we'll set something up. . @binary-finary ^. I think this is doc'ed in the pipeline spec. . #2319 is the issue for deploying pachyderm from an internal registry and I already have it marked as a docs issue. Is there something here that's different?. @orangefuzzball Ok I think this is fixed. Slack changed some permissioning requirements in the last update I guess -- invite bots need to be admins, but it seems to be working now. If you have any problems joining, you can also email me directly at joey@pachyderm.io and I'll manually send you an invite. . The rst format is needed for some features of ReadtheDocs that we use. In general, docs that are mainly read on the documentation website are in rst and things that are meant to be viewed on GH (such as Readme's for examples) are md. Although we're not totally consistent with that. \nFor these files I think we want to keep the formats as is, but it's something we've certainly considered unifying in the future. . Add section about scaling rethink and pachd pods. Should include that rethink needs one pv per pod. . Sorry you ran into this! The key not found error comes from an initialization problem adding the clusterID to etcd if the etcd pod crashes or is rescheduled. We fixed this in pachyderm v1.3.4. Would you mind upgrading to that and try redeploying to see if that fixes the problem?\n\nOn Jan 15, 2017, at 5:02 PM, Dan Kortschak notifications@github.com wrote:\nI am trying out pachderm locally, but I am having some problem getting it to consistently start (I have had one successful start).\nNow I am seeing things like the following:\n~ $ minikube start\nStarting local Kubernetes cluster...\nKubectl is now configured to use the cluster.\n~ $ kubectl get pod\nNAME            READY     STATUS             RESTARTS   AGE\netcd-s1cm1      1/1       Running            3          2d\npachd-pxz84     0/1       CrashLoopBackOff   29         2d\nrethink-1bgrv   1/1       Running            3          2d\n~ $ kubectl logs pachd-pxz84\n100: Key not found (/cluster-id) [2]\n~ $ dpkg -l pachctl\nDesired=Unknown/Install/Remove/Purge/Hold\n| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend\n|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)\n||/ Name                 Version         Architecture    Description\n+++-====================-===============-===============-=============================================\nii  pachctl              1.3.3           amd64           ?\nminikube version is v0.15.0\nkubectl version is v1.5.1\nI can leave this for quite some time, and the only result is to increase the number of pachd restarts.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Just to double check before looking at a other possible causes. Did you\nredeploy Pachyderm by bringing down everything back to just the bare\nminikube cluster and then pachctl deploy local again? So pachctl\nversion returns v1.3.4?\n\nTo just make sure there's no state being carried over, you can restart\nminikube again too. minikube delete then minikube start.\nOn Sun, Jan 15, 2017 at 5:15 PM, Dan Kortschak notifications@github.com\nwrote:\n\nThat does not fix the problem. Do I need to blow the old instances out of\nthe water (they're just tutorial data, so this is not a problem)?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/1271#issuecomment-272753446,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwA2zLhBOG5fwhzItBvTsp12A4Y_gks5rSsS3gaJpZM4LkF59\n.\n. Hi @TimVen. Pachyderm won't pull a new image from DockerHub if it already has an image with the same name/tag locally. Even though you've updated the image on DockerHub, Pachyderm doesn't actually know that because the name is the same. \n\nWe recommend statically tagging each image with the hash of the image instead of using dynamic tags like \"latest\". Another important reason to not use dynamic tags is for versioning your images and tracking provenance. If you use a dynamic tag, you won't be able to tell exactly which version of the image (your code) was used to transform a particular piece of data and you lose some reproducibility. \nIf you use Pachyderm's --push-images flag withupdate pipeline, we'll automatically push your new locally built image to the registry and update the pipeline spec with the new image hash tag. \nThat should solve your problem here, but definitely let us know if that's not what you want. We don't have an imagePullPolicy, but would consider adding one if there was a clear use case for it. . kube-up.sh seems to be deprecated by k8s in favor of Kops and CoreOS tools listed here. We need to update our docs and test the easiest path (most likely kops). Thanks for catching this @svanharmelen! We'll dig into why CI is failing (it shouldn't be since this is a simple docs change) and then get this merged in. In the meantime, can you sign our CLA?. @svanharmelen CI is all ready to go whenever you have a chance to sign our CLA. :). Done! Thanks again! Feel free to join our Slack Channel. We'd love to learn more about how you found Pachyderm, what use case you have in mind, and we'll of course be there to help if you get stuck on anything else. --- FAIL: TestJSONSyntaxErrorsReportedCreatePipeline (0.02s)\n    require.go:153: /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/pps/cmds/cmds_test.go:74\n    require.go:155: Not equal: \"Syntax Error on line 5:\\n\\n    \\\"c\\\": {a\\n          ^\\ninvalid character 'a' looking for beginning of object key string\\n\" (expected)\n                != \"2017/02/02 21:31:44 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \\\"transport: dial tcp 0.0.0.0:30650: getsockopt: connection refused\\\"; Reconnecting to {0.0.0.0:30650 <nil>}\\nSyntax Error on line 5:\\n\\n    \\\"c\\\": {a\\n          ^\\ninvalid character 'a' looking for beginning of object key string\\n\" (actual)\nThe above then causes all the examples (e.g. scraper, tensorflow, etc) to fail too.. --- FAIL: TestWriteToReadOnlyPath (1.53s)\n    filesystem_test.go:889: grpc serve: accept tcp 127.0.0.1:41996: use of closed network connection. I also get this one failing pretty frequently:\n--- FAIL: TestJobGC (13.72s)\n    require.go:153: /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/pachyderm_test.go:4185\n    require.go:155: Not equal: 0 (expected)\n                != 1 (actual). Good catch @joshmeek! You are correct that the commitID shouldn't change and this must have been from when we updated the docs, we changed some UUID's and missed one. Sorry about that!\nI see you just signed the CLA. As soon as we get CI to pass (likely just intermittent failures as its been flaky today) I'll get this merged. Thanks again!. Thanks for this @davidgasquez! The only problem that I can see is the updated slack link. We made it so slack.pachyderm.io points to the correct (new) slack channel so we can just remove that commit and this should be good to merge. . No problem. :) It was a good call to change, but just fixing the domain redirect is cleaner IMO. I'll get CI to pass and then we'll merge this in. Thanks!. LGTM once CI passes. Thanks @brollb for finding this typo (and #1325). Would you mind signing our CLA and then as soon as CI passes we'll merge these. . @Seth-Karlo yeah good point. We can certainly add ways to help catch those simple errors. @jdoliner what would be the simplest and easiest ones to handle?. Is there a reason pachctl deploy --cloud google --object-store gfs is there twice? Just wondering if there was initially multiple variations you where thinking of here?\nMy guess is that you meant GCS (Google Cloud Storage, their S3 equivalent) which is different from GlusterFS, right?. LGTM. Just had another user hit this. For some reason I thought we had fixed this\nalready, but the docs still have the temp creds. Adding to the docs docket.\n@dwhitena, can you make sure this is fixed correctly in the new aws\ndeployment docs? We don't talk about creds anymore since it's just a kops\nscript so just wanted to make sure it sets up permanent creds.\nOn Thu, Feb 16, 2017 at 1:18 PM, Joe Doliner notifications@github.com\nwrote:\n\nI think we should probably keep the functionality but as an optional flag\nrather than as a required one. Docs should clearly present permanent\ncredentials as the default option.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/1345#issuecomment-280463788,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwLRADMU7HzgzKrbWXSrdIM6rZCaWks5rdL0ugaJpZM4MDiwr\n.\n. Hi @willguxy. We can definitely add some convenience flags like this. After v1.4 lands, we plan to do a bunch small usability polishes like this so thanks for pointing that out. If you have others please open issues for them too. :)\n\nIn the meantime, you can certainly use either unix or Go sorting functions to accomplish this in the short-term. You can pipe pachctl list-repo to unix sort or use Go's sorting function from the Go client. . #1439 We should change the beginner tutorial. Looks good. We should still make the folder structure in the repo match,\nbut i like how well this organizes things.\nOn Mon, Mar 20, 2017 at 10:55 AM, Daniel Whitenack <notifications@github.com\n\nwrote:\n@JoeyZwicker https://github.com/JoeyZwicker restructured the index\nhere: https://github.com/pachyderm/pachyderm/blob/v1.4-docs/doc/index.rst\nNote that I added a bunch of placeholders as _PLACEHOLDER. In all of\nthose cases, the doc doesn't exist or needs to be moved, and, in a few\ncases, we need to create the respective folder. I think this will help our\norganization a lot. Let me know your thought and feel free to tweak as we\ngo. I will be pulling often from this branch.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/1383#issuecomment-287843961,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwKhyv8y3_XTF07lWZGI0cNgoBW_sks5rnr2QgaJpZM4MWBNB\n.\n. Note to self: We should check to make sure all screenshots are v1.4. The deployment docs for instance still show rethink pods. Easy to fix. . yup I'm on them.\n\nOn Wed, Mar 22, 2017 at 10:31 AM, Daniel Whitenack <notifications@github.com\n\nwrote:\nYep, good call @JoeyZwicker https://github.com/JoeyZwicker. Adding to\nthe list. I made some rearrangements today. I'm going to work through the\n\"fundamentals\" section, then move on to the other things. I'm still letting\nyou take beginner tutorial and getting started, unless you say different.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/1383#issuecomment-288477102,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwLBK8BJNoE7DcWJnFCihXDMS0xm4ks5roVrXgaJpZM4MWBNB\n.\n. Monday TODO:\n- release blog post\n- fundamentals PR\n- Migration PR\n- merge in @sjezewski Kops script\n- Update dynamic provisioning and add minio flag (test later)\n- Fix PLACEHOLDER links\n-  (non blocking but important) fundamentals/distributed computation (parallelism and glob patterns)\n\n. @dwhitena Add to Monday TODO's above:\n\nUpdate GH README.md for the pachyderm repo. (at least check that everything there is correct and links to the right places)\nmerge examples -- we're alraedy on it, but didn't make our list above\nupdate website to announce/link to v1.4. (assign to Joey). You make a good point in that the distinction I'm talking about is from outside of pachyderm such as using put-file with an S3 bucket or using -i to do scraping. I still feel like I want to be able to see uncommitted data, but we'll obviously wait to add this until a user is asking for it. Basically, the same thought process you're taking from within a pipeline (writing some output, but then manipulating/checking it before committing) also makes sense to me at the raw input step. . @derekchiang didn't seem to think it worked on cancelled commits. I thought delete-commit only worked on open commits, so what do you mean by \"Also it seems like he could just delete the upstream commits (which aren't cancelled) and then call rerun-pipeline, or delete the downstream commits to get the same effect.\"?. @willguxy we're going to try to repro and hunt this down for you. Can you show me what inspect-commit returns for one of the empty commits?. ping @willguxy . No, but it's still in v1.3 I think and we should make sure the functionality works correctly. @dwhitena did you try to repro what @alexsbromberg was hitting?. This is very minor, but is an easy change.. Some info in #1036 might be relevant. . @derekchiang seemed to think that  \"outputBranch\": string was not a required field and defaulted to master if unspecified. . then what happens if you have two pipelines both with output branch unspecified? They both output to master? That doesnt seem right. . ah nvm. I was being silly and thinking that two pipelines could target the\nsame repo if they were on different branches.\n\nSo this basically mean there's only ever 1 branch (master) on pipeline\noutput repos. Or am I missing something else?\nOn Fri, Mar 17, 2017 at 6:56 PM, Joe Doliner notifications@github.com\nwrote:\n\nWhy not? Pipelines don't output to the same repo.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/1434#issuecomment-287508609,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwJ3EsU-OIYEvymdiYL1qVxxxa2inks5rmznWgaJpZM4MhPOk\n.\n. Also see #1737 around naming from something else. . OpenCV is used for both the beginner tutorial but also our live demo. The tutorial is pretty good as is, but there are a few places the demo could be improved in v1.4.\n\n1) Mounting with FUSE was a great way to demonstrate the commit structure of PFS when users could see master/0, master/1, etc. In v1.4 you only see UUID's in random order and it's was less visually meaningful. Having commit names aliased as described in #1591 might be a great option. \n2) Having more info displayed in pachctl inspect-* commands or potentially in the mount as well if that's possible, would also improve the demo experience. \n3) @dwhitena and I have talked a lot about adding another pipeline step after the edge detection, perhaps just a crop or resize to show pipeline stages, updates, provenance, etc. Dan, I actually dont remember the specifics of what we discussed, but can you add them here if you do?\n. Only #3 on my list above have been added and we haven't actually adding the montage step to the beginner tutorial yet -- the UI might solve this once we have the file browser ready to go, but we're not there yet. . sigh -- must be something with my computer then. I can add to the docs.\nOn Mon, Mar 20, 2017 at 10:15 AM, Daniel Whitenack <notifications@github.com\n\nwrote:\n@JoeyZwicker https://github.com/JoeyZwicker I'm unable to reproduce\nthis on:\n\nPachyderm 1.3\nUbuntu 16.04\nminikube 0.17.1\n\nI can bring up port-forward and it stays good for 10+ minutes at least. I\nhaven't tested 1.4 yet. However, it doesn't appear to be across the board\nwith minikube 0.17.1.\nI think we should definitely add a note to the docs however about manually\nsetting ADDRESS.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/1440#issuecomment-287831083,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwAZsbogfdVpMXP4j6I4puFRUEypeks5rnrRKgaJpZM4MhQwK\n.\n. port-forward is still a cause of flakiness in general. Leaving open because I'd love to not have this, but we don't have any real proposals in place.. LGTM. Capturing a discussion that happened offline.\n\nDelete-commit perhaps should only work on input repos (not repos from pipelines). If you delete a commit from an output repo, what happens to the provenant commits in repos further up the DAG? You'd now have pipelines with input commits and no corresponding output and that would cause some very odd edge cases in our current implementation that don't really make sense. . This is v1.3 and non-blocking so not an immediate priority to fix. But, whatever test we don't have in 1.3 to catch this bug needs to exist in v1.4 if it doesn't already. . My example above was for images. It apparently had weird behavior too with files that were being appended to. I haven't repro'ed yet. \nSimon Cross [1:47 PM] \n@joey What we saw was that if we had commits `master/0`, `master/1` and `master/2` then\n`delete-file repo master foo` would delete the contents added in `master/0` and `master/1`\n but `get-file repo master foo` would still return the lines added in `master/2`.. v1.3 I think. In v1.4 it returns with no node at \"/<file_name>\" which could probably be clearer cuz I'm not sure what \"node\" is doing there. . We're unlikely to change the verb-noun syntax at this point, but organizing the pachctl commands in the docs and help section could be useful. Low priority. . I think @jdoliner has been wanting to do a cleanup pass for pachctl and that might actually happen in 1.8 or shortly after. I'm going to reopen because even though the specifics listed here are low priority, they might be part of the next milestone. . #2287 I think this has been done. my bad. wrong branch. ignore this. . I think in v1.4 it is idempotent and works pretty well -- certainly better than before where rethink coming up was often the problem. Probably better to be safe anyway and add it somewhere. Definitely will add in trouble shooting, but where else?. @dwhitena Ok I think this is ready for another review. I think all your changes have been made. . This is fine for now as it's easily discoverable by someone looking for \"joins\". We can edit the blog to update the link. Our docs page has its own search bar and its the top result there. . +1000. > While I think that's interesting (and probably useful to users), we haven't seen requests for this\nI get requests for this. Essentially they want to see (at the very least) jobIDs as part of provenance. Right now you have inspect-commit to get prov, then list-job for every commitID listed and then inspect job. Users would love to be able to see\nprovanance: repo/commitID --> / --> repo/commitID\nexample: images/1234 --> edges/\"pachyderm/opencv:v1.2\" --> edges/5678 ....\nI know it wouldn't actually be like this above, but that's the kind of info users are trying easily consume. Even big customer G ran into this. LGTM and merging. I can only assume this deploy script works so there's not much to review. :). I would have expected http://pachyderm.readthedocs.io/en/stable/deployment/amazon_web_services.html to tell me how ot use `deploy amazon` .. instead of deploying the cluster for me... default deployment node size on GKE is too small. It only has 4GB of memory. We should figure out what GKE side is roughly equivalent to the AWA xlarge and make that what users spin up when following our docs. You can't do anything interesting with the tiny default nodes. . LGTM. Ditto with other PRs. LGTM. Feel free to do a bunch of typo fixes as a single PR if you want. We're flexible. :). Totally up to you. mini-PR's are great too -- just thought it might save you time by clicking fewer buttons on GH. . Thanks Brian. Feel free to merge as soon as Ci passes. . LGTM. @dahankzter I sounds like most of these questions were already answered in our Slack channel, but I'll comment on a few of them here just to keep the issue complete.\n\nIssuing a delete first also requires finding the latest commit and then do a delete right?\n\nThis is what branches are for in Pachyderm. Branch names are essentially tags or aliases for a particular HEAD commit. So start-commit <repo> master will start a new commit with it's parent as the HEAD of master and will then tag this new commit as the new HEAD of master. Finding the latest commit is easy.\nThis is similar to Git. If I commit a file I dont want, I can just delete the file in the next commit. Obviously revert would work too, but perhaps I don't want to revert the entire commit, just get rid of the one file. \n\nSeems a little counter intuitive and a bit cumbersome to code it. Do I have to traverse all the history and delete the file in every commit it has ever taken place?\n\nCommits are immutable so you never go back and delete files from old commits. To delete a file you simply start a new commit on whatever branch you want and delete the file. Let's talk through an example:\n\nCommit1 adds file foo\nCommit2 adds files bar and buz\nCommit3 deletes file foo\nCommit 4 adds baz\n\nIf we list files for each commit, here's what is returned:\n$ pachctl list-file <repo> Commit1\nfoo\n$ pachctl list-file <repo> Commit2\nfoo\nbar\nbuz\n$ pachctl list-file <repo> Commit3\nbar\nbuz\n$ pachctl list-file <repo> Commit4\nbar\nbuz\nbaz\nNotice that in Commit3 we deleted foo. foo is no longer there in any future commit we read from, but we can always still go back and get foo by reading from Commit1 or Commit2. This works the same way for overwrites. If instead of just deleting foo in Commit3, I instead deleted it and added in a new file foo with different content (let's call it foo' to differentiate), then foo' would be available in Commit3 and Commit4 but 'foo` would be Commit1 and Commit2. \n. There are a few edge cases where it's still useful, but we opened this issue specifically because we're thinking about changing the default to replace instead of append. In pachyderm v1.3, appending was a very meaningful use case, but in v1.4 and going forward, pachyderm manages/prefers lots of small/separate files instead of appending to one big log file -- which was the common use case.\nWe'll almost certainly change this behavior, it just hasnt been slated into our dev timeline yet.. Talked with @jdoliner last week. I think one of the simplest options that would help users would simply to add a --overwrite (-o) to put-file so users can have some sugar instead of having to make two calls for every file they want to add. \nThis also puts the control into the users hands for most the above issues without having to actually change the default behavior. \nJust to double check, delete-file shouldn't fail if the file doesn't exist (pretty sure it doesn't).. Subsumed by #1476 since this is likely to be a docs change over an actual command change. . @jonand was just asking about this in #users today -- trying to figure out which datums were processed by which pods.. @dwhitena lets talk through this together next week. I'd like to build this into the fruit stand example and have it as part of the cookbook. \n@abourget, this is an even better way to do your time windowing than we discussed today. It's way more robust and flexible. I will write up a detailed descriptor of the pipelines and then we should chat about it again early next week. Not a big deal if you keep doing it as your are for now, but this structure would be excellent for you especially once #1558 is added. . Just adding some notes to myself for when we write this doc. \n3 types of time windows that we should have examples for:\n- Fixed time windows (e.g. Jan, Feb)\n- Moving Time windows: as JD described above\n- Shuffle-style time window --> don't have a clear use case for this, but have seen other users asking about \"shuffle/reorganization steps\" so it might be good to have an example of that. \n. Thanks for pointing this out and all the helpful info. We'll be doing our\nplanning for the next release over the coming week and will let you know if\nwe're able to schedule this as part of it.\nOn Thu, Oct 5, 2017 at 1:36 AM, Omer Katz notifications@github.com wrote:\n\nFYI occasional development of this feature occurs in\nhttps://github.com/thedrow/pachyderm/tree/alluxio-client\nPRs are welcome.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/1573#issuecomment-334397662,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwBChcyjYJDL5gcm0b3doZ9W64KLLks5spJUegaJpZM4MywWJ\n.\n. @dwhitena Is this going to make it in today? Seems not. did you remake asset.go @dwhitena? The test is failing at assets:\n\n--- FAIL: TestAssets (0.00s)\n    require.go:153: /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/pachyderm_test.go:573. True. But do we have ideas on how we actually want to go about this? I'm sure there are link-checker tools out there we can try. @dwhitena?\nThis is pretty low priority I think because things seem to be working ok for now. . duplicate of #1491 I think.\nMany of the issues listed on the board today already have GH issues open.. yup no problem! Especially for issue like this where we didn't have a good name for it, I'll make sure to do it when I see it. . #1609 might be relevant to this PR. LGTM. LGTM. Had to grab updates from master and run CI again, but then this is good to go. . It looks like your first command was trying to delete a file from the closed commit and that's why it didn't disappear. That certainly should have errored if that's the case -- which I think is a known issue.\nUnless I'm missing something, delete-file needs to be called in a open commit and does not have any of the sugar that put-file does. . This seems similar to what @gabrielgrant  could have been hitting yesterday although he wasnt able to get it to come up either after undeploy and redeploy. \nWe should look into this asap. . @dwhitena is there anything else we can look at to try to figure out if this is just flakeiness of truly reproducible? What was the error you were getting? Was it about the pvc not being mounted correctly? That's what @gabrielgrant was getting and I want to make sure they're the same.. There isn't a clear DB-style or query support path for Pachyderm yet. As you guys mention in the ad-hoc discussion above, getting information/queries that your data wasn't specifically arranged to support well isn't really feasible right now. We occasionally have users who want to \"export pachyderm metadata\" in some fashion to a DB such as elastic search to make these types of queries possible. . I dont think this was very well thought out, but it was something being asked a few weeks ago. basically they wanted to be able to query for file names and figure out which commits and repos contained a file or, for example, which jobs have processed a given file. They wanted a way to use a query language (SQL) to do that instead of pachctl and shell. . @dwhitena Changes made. . This is outdated so closing for now. @abourget We'd be happy to help you update this PR if you want to get it merged.. Thank you as always @brollb . We've continued to see requests for this and may make sense to address in\n1.8 as a performance optimization.\nOn Wed, Feb 14, 2018 at 10:32 AM, stale[bot] notifications@github.com\nwrote:\n\nThis issue has been automatically marked as stale because it has not had\nany recent activity. It will be closed if no further activity occurs. Thank\nyou for your contributions.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/1662#issuecomment-365701571,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwNVLy5gdbUHymrzGlBgFZFPjn9Piks5tUya7gaJpZM4M8AhO\n.\n. One comment, but docs side of things LGTM. I'll need to add this to our production section. I'm just commenting here as a simple place to keep a list for myself:\nAdd section on resource requests and scaledown threshold. @nak3 Thanks so much for updating this for us! We've definitely been a bit behind updating our OpenShift docs. Thanks for signing the CLA too so we're all set to merge. Thanks @savvopoulos . LGTM.\n\nEven though it's very minor, can you please sign our CLA?. @derekchiang also mentioned being able to pass a --from flag that changes the default of HEAD to a specific commit. That commit and child commits will be rerun.. I think @dwhitena is making a good point that we didn't cover when talking about rerun pipeline. When users have a single JSON spec that defines multiple pipelines, this starts to get weird. \nPerhaps only allowing one pipeline per JSON simplifies this? I can see that being annoying in its own right so we might need to tweak our proposal for rerun and/or update pipeline. \nI have some ideas that we should discuss tomorrow because I'd want @jdoliner or @derekchiang to be a counterpoint from the implementation side.. Adding to this issue. We're seeing occasional progress update requests fail due to transient network errors. When this happen, the datum count can now be incorrect.\n1 processed, 0 skipped, out of 2? It should be 1 and 1\n714b06b5-d2d3-4443-b2c2-68bba86f361f detect/348e26d3a0e24067b8e64af144bf9a95 11 hours ago   14 seconds 0       1 + 0 / 2 29.18MiB 1.968MiB success\npc get-logs --pipeline detect --master\nLaunching worker master process\nerror monitoring job state: rpc error: code = Canceled desc = context canceled\nerror monitoring job state: rpc error: code = Canceled desc = context canceled\nerror updating job progress: context canceled\nerror updating job progress: context canceled\nerror monitoring job state: rpc error: code = Canceled desc = context canceled\nWe could add retries, block until successful, or \"fake\" set the datum count manually at the end if they are slightly out of sync. Pros and cons to all options of course. \n. Good question. I thinks its gs:// and as:// but we should document more.. @dwhitena we can easily add the list of example egress urls to the http://pachyderm.readthedocs.io/en/latest/fundamentals/getting_data_out_of_pachyderm.html doc and pipeline spec. . Hi @montenegrodr. Sorry you're hitting this. I'll need some more info to help you debug. It seems like the job completed but with no data (notice the commit size of 0 B). \nCan you show me kubectl get all and pachctl inspect-job 5962ba95-7f62-4cfc-8413-32c52c1c978a. It might also be useful to grab logs from pachd and the worker pod that will be listed in kubectl get all. pachctl get-logs <worker> and kubectl logs po/<pachd_pod_name>.\nI'm also going to just run through the example myself and see if I can repro and will post back here if I find anything.. I was able to repro. Posting logs.\nkc logs po/pachd-rb2k1\ntime=\"2017-04-22T00:28:02Z\" level=info\ntime=\"2017-04-22T00:28:02Z\" level=info msg=\"address:\\\"172.17.0.5:650\\\" \"\ntime=\"2017-04-22T00:28:02Z\" level=info msg=\"serverRole:<address:\\\"172.17.0.5:650\\\" shards:<key:0 value:true > shards:<key:1 value:true > shards:<key:2 value:true > shards:<key:3 value:true > shards:<key:4 value:true > shards:<key:5 value:true > shards:<key:6 value:true > shards:<key:7 value:true > shards:<key:8 value:true > shards:<key:9 value:true > shards:<key:10 value:true > shards:<key:11 value:true > shards:<key:12 value:true > shards:<key:13 value:true > shards:<key:14 value:true > shards:<key:15 value:true > > \"\n2017-04-22T00:28:02Z INFO  adding shard 14\n2017-04-22T00:28:02Z INFO  adding shard 7\n2017-04-22T00:28:02Z INFO  adding shard 3\n2017-04-22T00:28:02Z INFO  adding shard 11\n2017-04-22T00:28:02Z INFO  adding shard 0\n2017-04-22T00:28:02Z INFO  adding shard 9\n2017-04-22T00:28:02Z INFO  adding shard 10\n2017-04-22T00:28:02Z INFO  adding shard 13\n2017-04-22T00:28:02Z INFO  adding shard 5\n2017-04-22T00:28:02Z INFO  adding shard 15\n2017-04-22T00:28:02Z INFO  adding shard 4\n2017-04-22T00:28:02Z INFO  adding shard 1\n2017-04-22T00:28:02Z INFO  adding shard 6\n2017-04-22T00:28:02Z INFO  adding shard 2\n2017-04-22T00:28:02Z INFO  adding shard 8\ntime=\"2017-04-22T00:28:02Z\" level=info msg=\"addresses:<addresses:<key:0 value:\\\"172.17.0.5:650\\\" > addresses:<key:1 value:\\\"172.17.0.5:650\\\" > addresses:<key:2 value:\\\"172.17.0.5:650\\\" > addresses:<key:3 value:\\\"172.17.0.5:650\\\" > addresses:<key:4 value:\\\"172.17.0.5:650\\\" > addresses:<key:5 value:\\\"172.17.0.5:650\\\" > addresses:<key:6 value:\\\"172.17.0.5:650\\\" > addresses:<key:7 value:\\\"172.17.0.5:650\\\" > addresses:<key:8 value:\\\"172.17.0.5:650\\\" > addresses:<key:9 value:\\\"172.17.0.5:650\\\" > addresses:<key:10 value:\\\"172.17.0.5:650\\\" > addresses:<key:11 value:\\\"172.17.0.5:650\\\" > addresses:<key:12 value:\\\"172.17.0.5:650\\\" > addresses:<key:13 value:\\\"172.17.0.5:650\\\" > addresses:<key:14 value:\\\"172.17.0.5:650\\\" > addresses:<key:15 value:\\\"172.17.0.5:650\\\" > > \"\n2017-04-22T00:28:02Z INFO  adding shard 12\ntime=\"2017-04-22T00:28:02Z\" level=info msg=\"serverRole:<address:\\\"172.17.0.5:650\\\" shards:<key:0 value:true > shards:<key:1 value:true > shards:<key:2 value:true > shards:<key:3 value:true > shards:<key:4 value:true > shards:<key:5 value:true > shards:<key:6 value:true > shards:<key:7 value:true > shards:<key:8 value:true > shards:<key:9 value:true > shards:<key:10 value:true > shards:<key:11 value:true > shards:<key:12 value:true > shards:<key:13 value:true > shards:<key:14 value:true > shards:<key:15 value:true > > \"\n2017-04-22T00:28:55Z INFO  protorpclog.Call {\"service\":\"pps.API\",\"method\":\"CreatePipeline\",\"request\":\"pipeline:\\u003cname:\\\"input\\\" \\u003e transform:\\u003ccmd:\\\"sh\\\" stdin:\\\"apt-get update -yq \\u0026\\u0026 apt-get install -yq --no-install-recommends ca-certificates wget\\\" stdin:\\\"wget -e robots=off --recursive --level 1 --adjust-extension --no-check-certificate --no-directories --directory-prefix /pfs/out https://en.wikipedia.org/wiki/Main_Page\\\" accept_return_code:4 accept_return_code:5 accept_return_code:6 accept_return_code:7 accept_return_code:8 \\u003e \",\"duration\":\"0.000s\"}\n2017-04-22T00:28:55Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"CreateRepo\",\"request\":\"repo:\\u003cname:\\\"input\\\" \\u003e \",\"duration\":\"0.000s\"}\n2017-04-22T00:28:55Z INFO  launching pipeline manager for pipeline input\n2017-04-22T00:28:55Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"CreateRepo\",\"request\":\"repo:\\u003cname:\\\"input\\\" \\u003e \",\"response\":\"\\u0026Empty{}\",\"duration\":\"0.002964718s\"}\n2017-04-22T00:28:55Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"CreateRepo\",\"request\":\"repo:\\u003cname:\\\"input\\\" \\u003e \",\"duration\":\"0.000s\"}\n2017-04-22T00:28:55Z INFO  protorpclog.Call {\"service\":\"pps.API\",\"method\":\"CreatePipeline\",\"request\":\"pipeline:\\u003cname:\\\"input\\\" \\u003e transform:\\u003ccmd:\\\"sh\\\" stdin:\\\"apt-get update -yq \\u0026\\u0026 apt-get install -yq --no-install-recommends ca-certificates wget\\\" stdin:\\\"wget -e robots=off --recursive --level 1 --adjust-extension --no-check-certificate --no-directories --directory-prefix /pfs/out https://en.wikipedia.org/wiki/Main_Page\\\" accept_return_code:4 accept_return_code:5 accept_return_code:6 accept_return_code:7 accept_return_code:8 \\u003e \",\"response\":\"\\u0026Empty{}\",\"duration\":\"0.010139109s\"}\n2017-04-22T00:28:55Z ERROR protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"CreateRepo\",\"request\":\"repo:\\u003cname:\\\"input\\\" \\u003e \",\"response\":\"nil\",\"error\":\"pachyderm_pfs/repoRefCounts/ input already exists\",\"duration\":\"0.000985824s\"}\n2017-04-22T00:29:14Z INFO  protorpclog.Call {\"service\":\"pps.API\",\"method\":\"ListPipeline\",\"duration\":\"0.000s\"}\n2017-04-22T00:29:14Z INFO  protorpclog.Call {\"service\":\"pps.API\",\"method\":\"ListPipeline\",\"response\":\"pipeline_info:\\u003cid:\\\"8251ddbd3b714e3ab237e4bf9dde9d0c\\\" pipeline:\\u003cname:\\\"input\\\" \\u003e version:1 transform:\\u003ccmd:\\\"sh\\\" stdin:\\\"apt-get update -yq \\u0026\\u0026 apt-get install -yq --no-install-recommends ca-certificates wget\\\" stdin:\\\"wget -e robots=off --recursive --level 1 --adjust-extension --no-check-certificate --no-directories --directory-prefix /pfs/out https://en.wikipedia.org/wiki/Main_Page\\\" accept_return_code:4 accept_return_code:5 accept_return_code:6 accept_return_code:7 accept_return_code:8 \\u003e created_at:\\u003cseconds:1492820935 nanos:61821309 \\u003e state:PIPELINE_RUNNING outputBranch:\\\"master\\\" \\u003e \",\"duration\":\"0.001051273s\"}\n2017-04-22T00:29:19Z INFO  protorpclog.Call {\"service\":\"pps.API\",\"method\":\"CreateJob\",\"request\":\"pipeline:\\u003cname:\\\"input\\\" \\u003e \",\"duration\":\"0.000s\"}\n2017-04-22T00:29:19Z INFO  protorpclog.Call {\"service\":\"pps.API\",\"method\":\"CreateJob\",\"request\":\"pipeline:\\u003cname:\\\"input\\\" \\u003e \",\"response\":\"id:\\\"f83d573a-89d7-4c6c-bc82-2b896312aa5e\\\" \",\"duration\":\"0.003215310s\"}\n2017-04-22T00:29:19Z INFO  launching job manager for job f83d573a-89d7-4c6c-bc82-2b896312aa5e\n2017-04-22T00:29:19Z INFO  watching `pachyderm_pps/workers/pipeline-input-v1/` for workers\n2017-04-22T00:29:19Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPIServer.Local\",\"method\":\"PutObject\",\"duration\":\"0.000s\"}\n2017-04-22T00:29:19Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPIServer.Local\",\"method\":\"TagObject\",\"request\":\"object:\\u003chash:\\\"c735513fc941aad91a407d03c91b4d2bb77d4a1450733406f9330141be9c38dab5180faee9fc57b45b20fb8c69bc305fd776db344528ec6332dd8687c41a566e\\\" \\u003e \",\"duration\":\"0.000s\"}\n2017-04-22T00:29:19Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPIServer.Local\",\"method\":\"TagObject\",\"request\":\"object:\\u003chash:\\\"c735513fc941aad91a407d03c91b4d2bb77d4a1450733406f9330141be9c38dab5180faee9fc57b45b20fb8c69bc305fd776db344528ec6332dd8687c41a566e\\\" \\u003e \",\"response\":\"\\u0026Empty{}\",\"duration\":\"0.000002431s\"}\n2017-04-22T00:29:19Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPIServer.Local\",\"method\":\"PutObject\",\"duration\":\"0.000278103s\"}\n2017-04-22T00:29:19Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"BuildCommit\",\"request\":\"parent:\\u003crepo:\\u003cname:\\\"input\\\" \\u003e \\u003e branch:\\\"master\\\" tree:\\u003chash:\\\"c735513fc941aad91a407d03c91b4d2bb77d4a1450733406f9330141be9c38dab5180faee9fc57b45b20fb8c69bc305fd776db344528ec6332dd8687c41a566e\\\" \\u003e \",\"duration\":\"0.000s\"}\n2017-04-22T00:29:19Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPIServer.Local\",\"method\":\"GetObject\",\"request\":\"hash:\\\"c735513fc941aad91a407d03c91b4d2bb77d4a1450733406f9330141be9c38dab5180faee9fc57b45b20fb8c69bc305fd776db344528ec6332dd8687c41a566e\\\" \",\"duration\":\"0.000s\"}\n2017-04-22T00:29:19Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPIServer.Local\",\"method\":\"GetObject\",\"request\":\"hash:\\\"c735513fc941aad91a407d03c91b4d2bb77d4a1450733406f9330141be9c38dab5180faee9fc57b45b20fb8c69bc305fd776db344528ec6332dd8687c41a566e\\\" \",\"duration\":\"0.002747552s\"}\n2017-04-22T00:29:19Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"BuildCommit\",\"request\":\"parent:\\u003crepo:\\u003cname:\\\"input\\\" \\u003e \\u003e branch:\\\"master\\\" tree:\\u003chash:\\\"c735513fc941aad91a407d03c91b4d2bb77d4a1450733406f9330141be9c38dab5180faee9fc57b45b20fb8c69bc305fd776db344528ec6332dd8687c41a566e\\\" \\u003e \",\"response\":\"repo:\\u003cname:\\\"input\\\" \\u003e id:\\\"42feef43fdd84cf495288073e734d03c\\\" \",\"duration\":\"0.014229857s\"}\n2017-04-22T00:29:19Z INFO  cancelling job: f83d573a-89d7-4c6c-bc82-2b896312aa5e\n2017-04-22T00:29:28Z INFO  protorpclog.Call {\"service\":\"pps.API\",\"method\":\"ListJob\",\"duration\":\"0.000s\"}\n2017-04-22T00:29:28Z INFO  protorpclog.Call {\"service\":\"pps.API\",\"method\":\"ListJob\",\"response\":\"job_info:\\u003cjob:\\u003cid:\\\"f83d573a-89d7-4c6c-bc82-2b896312aa5e\\\" \\u003e transform:\\u003ccmd:\\\"sh\\\" stdin:\\\"apt-get update -yq \\u0026\\u0026 apt-get install -yq --no-install-recommends ca-certificates wget\\\" stdin:\\\"wget -e robots=off --recursive --level 1 --adjust-extension --no-check-certificate --no-directories --directory-prefix /pfs/out https://en.wikipedia.org/wiki/Main_Page\\\" accept_return_code:4 accept_return_code:5 accept_return_code:6 accept_return_code:7 accept_return_code:8 \\u003e pipeline_id:\\\"8251ddbd3b714e3ab237e4bf9dde9d0c\\\" pipeline:\\u003cname:\\\"input\\\" \\u003e pipeline_version:1 started:\\u003cseconds:1492820959 nanos:315293949 \\u003e finished:\\u003cseconds:1492820959 nanos:366759272 \\u003e output_commit:\\u003crepo:\\u003cname:\\\"input\\\" \\u003e id:\\\"42feef43fdd84cf495288073e734d03c\\\" \\u003e state:JOB_SUCCESS stopped:true outputRepo:\\u003cname:\\\"input\\\" \\u003e outputBranch:\\\"master\\\" \\u003e \",\"duration\":\"0.001654277s\"}\n2017-04-22T00:30:37Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"ListRepo\",\"duration\":\"0.000s\"}\n2017-04-22T00:30:37Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"ListRepo\",\"response\":\"repo_info:\\u003crepo:\\u003cname:\\\"input\\\" \\u003e created:\\u003cseconds:1492820935 nanos:70091325 \\u003e \\u003e \",\"duration\":\"0.000853098s\"}\n2017-04-22T00:31:03Z INFO  protorpclog.Call {\"service\":\"pps.API\",\"method\":\"InspectJob\",\"request\":\"job:\\u003cid:\\\"f83d573a-89d7-4c6c-bc82-2b896312aa5e\\\" \\u003e \",\"duration\":\"0.000s\"}\n2017-04-22T00:31:03Z INFO  protorpclog.Call {\"service\":\"pps.API\",\"method\":\"InspectJob\",\"request\":\"job:\\u003cid:\\\"f83d573a-89d7-4c6c-bc82-2b896312aa5e\\\" \\u003e \",\"response\":\"job:\\u003cid:\\\"f83d573a-89d7-4c6c-bc82-2b896312aa5e\\\" \\u003e transform:\\u003ccmd:\\\"sh\\\" stdin:\\\"apt-get update -yq \\u0026\\u0026 apt-get install -yq --no-install-recommends ca-certificates wget\\\" stdin:\\\"wget -e robots=off --recursive --level 1 --adjust-extension --no-check-certificate --no-directories --directory-prefix /pfs/out https://en.wikipedia.org/wiki/Main_Page\\\" accept_return_code:4 accept_return_code:5 accept_return_code:6 accept_return_code:7 accept_return_code:8 \\u003e pipeline_id:\\\"8251ddbd3b714e3ab237e4bf9dde9d0c\\\" pipeline:\\u003cname:\\\"input\\\" \\u003e pipeline_version:1 started:\\u003cseconds:1492820959 nanos:315293949 \\u003e finished:\\u003cseconds:1492820959 nanos:366759272 \\u003e output_commit:\\u003crepo:\\u003cname:\\\"input\\\" \\u003e id:\\\"42feef43fdd84cf495288073e734d03c\\\" \\u003e state:JOB_SUCCESS stopped:true outputRepo:\\u003cname:\\\"input\\\" \\u003e outputBranch:\\\"master\\\" \",\"duration\":\"0.001600024s\"}\n2017-04-22T00:32:49Z INFO  protorpclog.Call {\"service\":\"pps.API\",\"method\":\"GetLogs\",\"request\":\"pipeline:\\u003cname:\\\"input\\\" \\u003e \",\"duration\":\"0.000s\"}\n2017-04-22T00:32:49Z INFO  protorpclog.Call {\"service\":\"pps.API\",\"method\":\"GetLogs\",\"request\":\"pipeline:\\u003cname:\\\"input\\\" \\u003e \",\"duration\":\"0.018087284s\"}\n2017-04-22T00:40:16Z INFO  protorpclog.Call {\"service\":\"pps.API\",\"method\":\"CreateJob\",\"request\":\"pipeline:\\u003cname:\\\"input\\\" \\u003e \",\"duration\":\"0.000s\"}\n2017-04-22T00:40:16Z INFO  protorpclog.Call {\"service\":\"pps.API\",\"method\":\"CreateJob\",\"request\":\"pipeline:\\u003cname:\\\"input\\\" \\u003e \",\"response\":\"id:\\\"17fbb902-2d6f-46b2-8eb3-9fda8d65a32c\\\" \",\"duration\":\"0.004551738s\"}\n2017-04-22T00:40:16Z INFO  launching job manager for job 17fbb902-2d6f-46b2-8eb3-9fda8d65a32c\n2017-04-22T00:40:16Z INFO  watching `pachyderm_pps/workers/pipeline-input-v1/` for workers\n2017-04-22T00:40:16Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPIServer.Local\",\"method\":\"PutObject\",\"duration\":\"0.000s\"}\n2017-04-22T00:40:16Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPIServer.Local\",\"method\":\"TagObject\",\"request\":\"object:\\u003chash:\\\"c735513fc941aad91a407d03c91b4d2bb77d4a1450733406f9330141be9c38dab5180faee9fc57b45b20fb8c69bc305fd776db344528ec6332dd8687c41a566e\\\" \\u003e \",\"duration\":\"0.000s\"}\n2017-04-22T00:40:16Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPIServer.Local\",\"method\":\"TagObject\",\"request\":\"object:\\u003chash:\\\"c735513fc941aad91a407d03c91b4d2bb77d4a1450733406f9330141be9c38dab5180faee9fc57b45b20fb8c69bc305fd776db344528ec6332dd8687c41a566e\\\" \\u003e \",\"response\":\"\\u0026Empty{}\",\"duration\":\"0.000435504s\"}\n2017-04-22T00:40:16Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPIServer.Local\",\"method\":\"PutObject\",\"duration\":\"0.000848374s\"}\n2017-04-22T00:40:16Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"BuildCommit\",\"request\":\"parent:\\u003crepo:\\u003cname:\\\"input\\\" \\u003e \\u003e branch:\\\"master\\\" tree:\\u003chash:\\\"c735513fc941aad91a407d03c91b4d2bb77d4a1450733406f9330141be9c38dab5180faee9fc57b45b20fb8c69bc305fd776db344528ec6332dd8687c41a566e\\\" \\u003e \",\"duration\":\"0.000s\"}\n2017-04-22T00:40:16Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPIServer.Local\",\"method\":\"GetObject\",\"request\":\"hash:\\\"c735513fc941aad91a407d03c91b4d2bb77d4a1450733406f9330141be9c38dab5180faee9fc57b45b20fb8c69bc305fd776db344528ec6332dd8687c41a566e\\\" \",\"duration\":\"0.000s\"}\n2017-04-22T00:40:16Z INFO  launching new worker for pachyderm_pps/workers/pipeline-input-v1/ at 172.17.0.6\n2017-04-22T00:40:16Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPIServer.Local\",\"method\":\"GetObject\",\"request\":\"hash:\\\"c735513fc941aad91a407d03c91b4d2bb77d4a1450733406f9330141be9c38dab5180faee9fc57b45b20fb8c69bc305fd776db344528ec6332dd8687c41a566e\\\" \",\"duration\":\"0.016902044s\"}\n2017-04-22T00:40:16Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"BuildCommit\",\"request\":\"parent:\\u003crepo:\\u003cname:\\\"input\\\" \\u003e id:\\\"42feef43fdd84cf495288073e734d03c\\\" \\u003e branch:\\\"master\\\" tree:\\u003chash:\\\"c735513fc941aad91a407d03c91b4d2bb77d4a1450733406f9330141be9c38dab5180faee9fc57b45b20fb8c69bc305fd776db344528ec6332dd8687c41a566e\\\" \\u003e \",\"response\":\"repo:\\u003cname:\\\"input\\\" \\u003e id:\\\"737bff4e3d9a4d5e858ea4f8ed2379a7\\\" \",\"duration\":\"0.029341251s\"}\n2017-04-22T00:40:16Z INFO  goro for worker 172.17.0.6 for job 17fbb902-2d6f-46b2-8eb3-9fda8d65a32c is exiting\n2017-04-22T00:40:16Z INFO  cancelling job: 17fbb902-2d6f-46b2-8eb3-9fda8d65a32c\n. Found the problem! Turns out we just don't handle the no-input pipeline use case correctly in v1.4. This was actually a known thing, but we forgot to update the wordcount example to fit the v1.4 structure. \n@dwhitena and I will fix the example on Monday. In the meantime, you can try one of the ML examples, or write your own pipeline that adds onto the openCV one. Sorry again about this!. Going to close the issue since it's less of a bug and more a docs error. Tracking the docs update here #1725 . Agreed. . I disagree with both of these arguments. \n1) As I mentioned, I'm not strongly sold on a particular rename such as \"after\", but it's meaningfully better. For example (this is directly from multiple pachyderm-savvy, not beginner users), in repo where you have 5 linear commits:\nReprocess all commits FROM commit 2 is incredibly ambiguous, if not actually misleadingly wrong.  Whereas Reprocess all commits AFTER commit 2, meaning 3, 4, and 5 get processed, is significantly closer to implementation. Certainly its not perfect.\n2) I agree that \"anytime something confuses a user that means we should change it\" is not a good policy and that's not what i'm pushing for here. \n- This is a significant point of confusion that large portion of our most experienced Pachyderm users have hit and were wrong about the expected behavior. That should tell you something.\n- This workflow for having to rerun things on only a subset of commits is incredibly common and a path that most production users will hit. This is not an edge case that only effects a small number of users.\n- We add new flags/fields all the time and don't put a significant amount of thought into naming them -- which is fine because we're developing very fast. But, I for one expected the implementation of from to be inclusive and would have lobbied for a different name if I had known that wasn't the implementation. I now understand why exclusive is the correct implementation, but dictates needing a better name. It's obviously not reasonable for me to be part of any (or most) naming discussions, but we also can't be averse to changing it \"just because we don't like change.\" Yes, this is API-breaking and should be included in an API-breaking release along with other changes that will inevitably need to be done that break compatibility. \n. From @abhitopia in #1966 \nfrom_commit is counter intuitive as it excludes the commit itself as explain in doc-\n\"input.atom.from_commit specifies the starting point of the input branch. If from_commit is not specified, then the entire input branch will be processed. Otherwise, only commits since the from_commit (not including the commit itself) will be processed.\"\nProposal: Rename from_commit to more intuitive after_commit.\nAlternatively, add provision for both from_commit and after_commit\n``. @abhitopia Yeah good point. That document definitely gives the wrong interpretation in a few ways. @dwhitena we should update this doc and the pipeline spec to be clearer. As @jdoliner mentioned, we're going to get rid offrom_commit` for something that actually works the way users expect in the future, but we haven't gotten there yet. LGTM. We've seen a user hit this too I think. rerunning CI to make sure it passes and then we can merge this in -- sorry it slipped through the cracks. . Places it needs to be documented:\nTime window cookbook\nequi-join cookbook (anywhere where there's a shuffle step honestly)\nI think we should also just make something like a \"Other minor features\" sections. I don't like that name or \"misc\", but there's lots of little things like this we need to document somewhere with just 1-2 paragraphs so we can point people there when they ask instead of having to re-explain.\n@dwhitena thoughts?. I believe each time you're putting the file again you're appending the same content to end of the last file, not overwriting it. That's why the size goes up steadily. \nIf you want to overwrite a file, you should start the commit, the delete-file, put-file, finish-commit. We're working on changing this default appending behavior here: #1531 . If thats not the case though and we're showing file sizes wrong, definitely let me know and we can figure out if there's a bug. . Sorry you're hitting this issue. It's something intermittent and flaky about AWS not releasing volume claims. We're going to change the docs to use dynamically schedules volumes instead of static ones and this shouldn't be a problem anymore. . @gabrielgrant . what version are you using? Just wanted to double check because the name of the field has changed a few times. . I can't see anything obviously wrong with your pipeline manifest. Can you give me a little more detail on what error you're getting? \n@derekchiang @jdoliner . https://github.com/pachyderm/pachyderm/issues/1872. I think https://github.com/pachyderm/pachyderm/issues/1737 is worth discussing too. I know I'm the one who feels most strongly about this issue, but it's so common in user workflows that it's work thinking about. Do we not have other API-breaking changes already in this release?. Thanks @davidthewatson . We'll get this fixed right away. . Sorry you ran into this @davidthewatson. Should be fixed now. Definitely reopen and let us know if you still get that warning. . Adding to above. \nfrom_commit in its current form still has uses in overwriting workloads. But we'd like to make this much more powerful and actually address what users would ideally like to do -- only process a subset of the files. Perhaps diff will enable this in some way? Unclear... but definitely worth discussing how to provide this type of functionality. \nOther ergonomics (some are just suggestions or ideas, not full proposals):\n- modified/created timestamps (and commitIDs) for files.\n- meta operations such as copying repos or subsets of data\n- Addressing @derekchiang's comment in slack about orphaned jobs not really being useful? \n- no manual writes to repos that are \"owned\" by pipelines\n- more info displayed in inspect-* fields. This would just be sugar for the CLI. For example, inspect-commit should show the jobID, pipeline version, perhaps even the transform that was used to create it. \n- commit aliases instead of UUIDs -- some ideas include: master, master~~1, master^2, etc. This would be nice for the FUSE mount too.\n. Hi @thedrow, I know this is a bit of a silly question, but is your pachd pod actually up and running in this case and have you forwarded the port? I haven't seen this response except when there's no pachd connection.\nWhat's kubectl get all show?. hi @paologf minikube delete && minikube start is a great way to reset everything. You'll need to do pachctl deploy local after that before port-forward will work. You can also upgrade to v1.5.0 by following our installation instructions. \ncontext deadline exceeded usually comes from a command being unable to connect to the pachd node -- generally because either your cluster isn't up at all or pachd has crashed and is restarting. You can use kubectl get allto see a list of pods that are running and their states. What does that show in your cluster?. @paologf Yeah that makes sense. minikube has to pull the pachd image from dockerhub when you deploy so it takes about 30-60 seconds depending on your internet connection. Glad your cluster is working now and don't hesitate to post more questions if you run into anything.. @jdoliner re: your above comment, can we catch this and print something along the lines of \"cannot connect to pachd. Please check that pachd pod is running with kubectl get all\"?. hi @merl-dev, sorry you're hitting this as we've generally been unable to reproduce except for when you can't connect to pachd because it's not up. \nYou said it wasn't happening 2-3 minutes after firing up the cluster. By that, do you mean that pachctl version returned correctly? \nThe pod clearly looks like it's running -- are the restart numbers going up, indicating a crashloop backoff? And you've done port-forwarding (and the pf process didn't die), right? \nIs this is minikube or on a cloud provider?\nIf none of the above issues seem to oviously be the culprit, it may be faster to help figure out what's going on live so feel free to join our public channel. \n. We still need to make this error message clearer. Context deadline exceeded doesn't mean anything and users hit this a lot. . I think this has been fixed by #2089. If i'm incorrect @derekchiang or @jdoliner, please reopen. What's the status of this PR?. The pipeline spec doc as specific examples of cross datums. This is definitely a complicated idea, but other than also including those details in the main doc, I'm not really sure there's that much more add here. I'll mark it for @dwhitena to take a quick look through if there obvious improvements. Thanks @brollb . Is this still broken? if it's fixed I'm fine to close it, but if its still\na bug then we should leave open and slot into dev timeline to fix.\nDescription field is actually being used by our prod customers\nOn Thu, Feb 8, 2018 at 5:16 PM, Sean Jezewski notifications@github.com\nwrote:\n\nstale\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/1921#issuecomment-364304375,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwFtJBVmgac_W8csq7anOOCBKu64gks5tS5xqgaJpZM4N02E_\n.\n. Just to add some of the offline discussion we had around this. We think this is a race condition with update pipeline since it's actually making 3 requests. Pipeline C updated before pipeline B which got us commits with \"new\" code that had commits from \"old\" code as provenance -- which is not what we wanted here. \n\nThe automatically incrementing pipeline versions as part of the transform also added to this problem where realistically, only one of the pipeline actually needed to be updated/rerun. I think we need a redesign of update-pipeline since there are too many usage patterns right now that break in weird ways. . LGTM. LGTM. Thanks Joel! Can you sign our CLA and then we can get this merged.. @sjezewski What's the new process to merge this now?. CI passed on #1986. merging.. Thanks @abhitopia. This is something we've been discussing already in #1737. Closing in favor of the other issue, but feel free to comment more on it there. . This was only here because at the time we wanted to easily reference old version of docs so we didn't have to rewrite them. I think they're all so out of date now that they have no value. \nComposing pipelines and provenance info has been documented elsewhere, right?\nPR LGTM. LGTM. LGTM. This also comes up when users have pachd deployed and then want to deploy the dashboard (--dashboard-only). They are forced to manually kill their other port-forward processes and then redo it. \nThis issue should be an easy fix, right? . LGTM. Thanks @ssaamm! Can you sign our CLA and we'll get this merged?. @dwhitena Status here?. Aren't we getting rid of strategy completely? Wouldn't it make sense to do\nthose PR's at the same time instead of writing logic for checking\nstrategy?\nOn Thu, Jun 29, 2017 at 6:32 PM, Joe Doliner notifications@github.com\nwrote:\n\n@jdoliner commented on this pull request.\nIn src/server/pps/server/api_server.go\nhttps://github.com/pachyderm/pachyderm/pull/2004#discussion_r124948692:\n\n@@ -600,6 +600,9 @@ func (a apiServer) validatePipeline(ctx context.Context, pipelineInfo pps.Pipe\n  if pipelineInfo.OutputBranch == \"\" {\n      return fmt.Errorf(\"pipeline needs to specify an output branch\")\n  }\n+ if pipelineInfo.ParallelismSpec != nil && pipelineInfo.ParallelismSpec.Constant == 0 && pipelineInfo.ParallelismSpec.Coefficient == 0 {\n+     return fmt.Errorf(\"cannot specify 0 for both constant and coefficient in parallelism spec\")\n+ }\n\nShouldn't this really check if Strategy == \"CONSTANT\" && Constant == 0?\nJoey hit this error because he was unwittingly setting Strategy ==\n\"COEFFICIENT\" but only setting Constant to a nonzero number.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/pull/2004#pullrequestreview-47290530,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwPwLWMMfGFCMu9dxI99C6N6quscxks5sJFBEgaJpZM4OKCZY\n.\n. Hi @southclaws\n\nThe logs there don't have anything to do with the symptom you're seeing. The most likely cause is that port-forward just died and you need to reconnect. Are your Kubectl commands working? \nWhen you say pipelines are spinning up, are you seeing to pods in 'kubectl get all' but they're stuck in 'starting' and not being scheduled? Or are they crashing? Or are the pods fine but the jobs aren't getting kicked off by new data? \n\nOn Jul 2, 2017, at 2:37 AM, Southclaws notifications@github.com wrote:\nLeft the cluster over the weekend, one of our devs noticed pipelines not spinning up so I tried to pachctl delete-all which sometimes takes a bit of time but seemed to take too long. Decided to check pachctl list-pipelines and left it while I got a coffee... 20 minutes later and nothing, very strange!\nI've tried deleting the pachd pod and letting it come back up but that fails too. Seems like the only way to resolve this is to kill our etcd cluster and eradicate all the data then delete all the pipelines manually.\nDuring the list-pipelines hang, pachd seems to be just in a loop logging these same three types of event:\n2017-07-02T09:13:21Z INFO  protorpclog.Call {\"service\":\"pps.API\",\"method\":\"ListPipeline\",\"duration\":\"0.000s\"}\n2017-07-02T09:13:51Z INFO  objectCache stats: {Gets:17 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:13:51Z INFO  tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:13:51Z INFO  objectInfoCache stats: {Gets:32 CacheHits:17 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:14:51Z INFO  objectCache stats: {Gets:17 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:14:51Z INFO  tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:14:51Z INFO  objectInfoCache stats: {Gets:32 CacheHits:17 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:15:51Z INFO  objectCache stats: {Gets:17 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:15:51Z INFO  tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:15:51Z INFO  objectInfoCache stats: {Gets:32 CacheHits:17 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:16:51Z INFO  objectCache stats: {Gets:17 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:16:51Z INFO  tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:16:51Z INFO  objectInfoCache stats: {Gets:32 CacheHits:17 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:17:51Z INFO  objectCache stats: {Gets:17 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:17:51Z INFO  tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:17:51Z INFO  objectInfoCache stats: {Gets:32 CacheHits:17 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:18:51Z INFO  objectCache stats: {Gets:17 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:18:51Z INFO  tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:18:51Z INFO  objectInfoCache stats: {Gets:32 CacheHits:17 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:19:51Z INFO  objectCache stats: {Gets:17 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:19:51Z INFO  tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:19:51Z INFO  objectInfoCache stats: {Gets:32 CacheHits:17 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:20:51Z INFO  objectCache stats: {Gets:17 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:20:51Z INFO  tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:20:51Z INFO  objectInfoCache stats: {Gets:32 CacheHits:17 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:21:51Z INFO  objectCache stats: {Gets:17 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:21:51Z INFO  tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:21:51Z INFO  objectInfoCache stats: {Gets:32 CacheHits:17 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:22:51Z INFO  objectCache stats: {Gets:17 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:22:51Z INFO  tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:22:51Z INFO  objectInfoCache stats: {Gets:32 CacheHits:17 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:23:51Z INFO  objectCache stats: {Gets:17 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:23:51Z INFO  tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:23:51Z INFO  objectInfoCache stats: {Gets:32 CacheHits:17 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:24:51Z INFO  objectCache stats: {Gets:17 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:24:51Z INFO  tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:24:51Z INFO  objectInfoCache stats: {Gets:32 CacheHits:17 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:25:51Z INFO  objectCache stats: {Gets:17 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:25:51Z INFO  tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:25:51Z INFO  objectInfoCache stats: {Gets:32 CacheHits:17 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:26:51Z INFO  objectCache stats: {Gets:17 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:26:51Z INFO  tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:26:51Z INFO  objectInfoCache stats: {Gets:32 CacheHits:17 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:27:51Z INFO  objectCache stats: {Gets:17 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:27:51Z INFO  tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:27:51Z INFO  objectInfoCache stats: {Gets:32 CacheHits:17 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:28:51Z INFO  objectCache stats: {Gets:17 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:28:51Z INFO  tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:28:51Z INFO  objectInfoCache stats: {Gets:32 CacheHits:17 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:29:51Z INFO  objectCache stats: {Gets:17 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:29:51Z INFO  tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\n2017-07-02T09:29:51Z INFO  objectInfoCache stats: {Gets:32 CacheHits:17 PeerLoads:0 PeerErrors:0 Loads:15 LoadsDeduped:15 LocalLoads:15 LocalLoadErrs:0 ServerRequests:0}\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Yeah we definitely want to help you figure out what's going on here, but\nwe'll need to figure out a way to get more information about the state of\nthe system when it happens.\n\nOn Sun, Jul 2, 2017 at 11:42 PM, Southclaws notifications@github.com\nwrote:\n\nYeah port forward was on (I restarted it many times) kubectl worked too, pachctl\nversion worked too I think. The pods were in running status (there were\nabout 20-25 pods iirc) I didn't check throughput or PFS so I'm not sure if\nthat was the issue.\nI've come across this a few times so if I see it again, I'll grab whatever\nextra information is needed! It's not a huge issue right now since we're\nrunning pachctl delete-all all the time since we're not in production but\nif this did happen in production I'm not sure what we could do other than\ndeleting pachd and all associated data.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/2011#issuecomment-312562024,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwBm8GCfpkESQSRzyfVovucyH5fPYks5sKI1xgaJpZM4OLiZE\n.\n. LGTM. Minor punctuation errors, but I'll fix that in a followup PR. You know JD, considering your mother is an editor, I'm surprised she never taught you how to use commas. :P . Hi @pnovotnak! Large, highly parallelized, and additive data sets are a perfect use case for pachyderm. \n\nThe biggest question will be how you want pachyderm to interact with the database itself. If you're open to just dumping the DB data into Pachyderm (object storage, which is very cheap), Pachyderm can be really smart and only process to new data to massively save on compute requirements. \nWhat does the shape of your data look like? Is it one big table of proteins?\nIf you want to talk through this live, you can jump in our public Slack channel and we can help you structure your data for maximum performance and storage efficiency. \n. fixed JD's comma aversion. LGTM after that. . @msteffen I think this is outdated and not a problem anymore because kubernetes changed the defaults? We also document that users should use larger nodes. closing.. Some measures have already been taken to help with this such as adding resource limits etcd and pachd. Opening a new issue for the scaledownthreshold improvements #2451 . thanks @nickjstevens \n@dwhitena . LGTM. LGTM. Request from a user that I think goes here:\nThey need the timestamps for the files/datums to not be updated when they are skipped. Currently, all files have a timestamp that matches the commit, even when the datum is skipped. They need the updated_at timestamps for each file to be correct for audit reasons. They really like Pachyderm's prov, but need to also be able to prove easily when the data was last changed. . @jdoliner, yup that makes sense. But finding the first commit where a file shows up \u2014 or moreso the last commit where the file was actually updated isn\u2019t actually easy to access. Should I make a separate issue for it?\n\nOn Jun 13, 2018, at 7:53 PM, Gabriel Grant notifications@github.com wrote:\nTechnically those definitions aren't necessarily accurate for manually updated branches, but they're probably sufficient for the use case I'm describing, since they should hold for pach-maintained master/stats branches\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Minor comment, but this looks great @dwhitena . I don't think any of these were actually implemented so closing. I still think these are better names for the commands, but doesn't seem like they're going to be changed right now. do you mean pachctl delete-all? I didn't think undeploy requires interaction. Ah true. @jdoliner or @derekchiang may be able to chime in about how easy that would be to add. I think it should be pretty simple so you can automate those commands. We'll need to address autoscaling with a number of our incoming enterprise users so this will likely need to be discussed more. Interactions with coefficient also seem problematic. . few minor comments but LGTM. I think this was resolved by removing the resource request from the master worker after the scaledownthreshold and may also be address by the worker refactor. @jdoliner reopen if I'm wrong and it's still an issue.. LGTM. Thanks @brollb . LGTM. Is this done or ever going to be done?. @jdoliner has this been implemented in a different PR or is this still relevant? Closing for now unless work is still being done. We actively get this feature request from customers. When we get someone to \"sponsor\" this work, we'll reopen. Thanks Jon. We'll look into this. But a few questions:\n\nWhat do you mean by \"change the pipeline definition to include the new file\"? If the pipeline has repo Y as an input, shouldn't take it already be seeing that file based on the glob pattern? Or are you explicitly naming every file in repo Y?\nAlso, when you update the pipeline, you're saying it does see all of the files from Repo X and the old files from repo Y, but not the newest one? If you make additional commits to repo Y, do any of those files show up?\nIs the above behavior consistent with both cross and union?\n\nOn Aug 12, 2017, at 5:21 AM, Jon Ander Novella notifications@github.com wrote:\nI have observed a strange behavior when using the cross/union of two repos.\nSteps to reproduce:\nRun a pipeline with a number of files from repo X and one file from repo Y. No problem, it should work.\nUpload a new file to repo Y via: pachctl put-file (...)\nChange the pipeline definition for the pipeline to use the new file from repo Y.\nRun pachctl update-pipeline .json --reprocess\nObserve that the job cannot find the new file.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Can you post your before and after pipeline definition? That'll help me repro because my initial attempt in my local setup and pipeline isn't doing it. \nOn Aug 12, 2017, at 9:24 AM, Jon Ander Novella notifications@github.com wrote:\nBy changing the pipeline definition to include the new file, I mean that I change the name explicitly of the file in repo Y in the pipeline definition.\nExactly, it does seem to see the old ones but not the new one. New commits do not seem to help, and the above behavior seems to be consistent with both cross and union.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Is there any immediate plan to add a troubleshooting doc back with all these types of little things? I'm not really sure there is. We've made this slightly better with other issues so I'm going to close for now.. Those deletes look correct to me. Is this still relevant?. Status here?. Seems like this is resolved. closing.. It sounds like this is more than a docs issue and actually that we need to do more testing with GPU resources and getting the drivers to work for various setups. Some of these features have GH issues in the dash repo as part of the 1.7 milestone. Until those are done, we can't really connect everything together as described here. . Still relevant, just didn't make it into 1.7. We've got most of the legal worked out and I'll let you know when it's all\na Go. Waiting on a few final approvals\n\nOn Tue, Jan 22, 2019 at 10:23 PM Gabriel Grant notifications@github.com\nwrote:\n\nSomewhat. At some point (maybe in December?) this was blocked on clearance\nfrom legal. i think we came to a conclusion on that (though not sure what\nit was)\nBut there's stuff to be added both to the front- and back-end\n(specifically, right now we log usage stats in a number of places in the\nbackend, and we'd like those logs to include the enterprise activation code\nwhen one has been installed)\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/2196#issuecomment-456685215,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwMS2fMDw7kJaIs0LF6a2rBQQTvIiks5vF__0gaJpZM4O_IiF\n.\n. status?. thanks @bpb. We'll repro this and get a fix. @jdoliner . @jtratner Thanks for pointing that out. We can definitely improve the docs here for troubleshooting an invalid image. kubectl describe po/<pod_name> is a very useful way to debug things so we should make sure that's documented.\n\nWe can also look into punching through the error from k8s. @dwhitena . We're also have a proposal to expose more meaningful info from kuberenetes directly into the job status for Pachyderm. LGTM. For this to show up in docs it needs to be added to the TOC. fixed via #2259 . I think this is working correctly now. @jtratner We'd love your feedback on this issue to make sure it solving the problems we discussed.. Thanks @thedrow.\n@dwhitena Let's add this to our docs queue.. Thanks @srajpal1990. Technically both are valid and correct, but I agree with you that /* is clearer. Thanks for submitting this @fortytw2! Building a set a standard containers for common operations is definitely something we've talked about. We've implemented a few as part of other examples. We'll make sure to discuss this as part of our next release. \nYou've already listed a few great examples of standard containers above, but if you have more, feel free to add them to this issue. \nSubscribing or emitting to a Kafka topic comes to mind. \n. We'll be adding some of these examples over time. MongoDB w/ cron has been added and the SQL adapter is part of the 1.7 milestone. \nS3 sync commands are already commonly used, but perhaps a stdlib example of that may make this next release cycle as well. The rest will be on backlog for now, but we'll add them as we run into those use cases. \n@fortytw2 We'd love to see any of your connectors open sourced when they're ready. LGTM. Hi @jonalm. Sorry you're hitting this problem and thanks for pointing it out! We can definitely make the docs clearer. The make target you're looking for is the makefile in the root of our Pachyderm repo. \nhttps://github.com/pachyderm/pachyderm/blob/master/Makefile. Jinx @dwhitena . @dwhitena This just requires adding \"clone the pachyderm repo\" as part of the azure deploy docs, right?\nProbably worth mentioning since that's not a requirement for either of the other cloud deploys. . Thanks @poopoothegorilla. LGTM. @dwhitena will get this merged in. . @msteffen I think you were looking at this a while back, but I might be misremembering. Is there anything to do here?. This just needs to be documented in the aws.sh script and/or the awe deploy docs?. @msteffen Status?. It would be great to have a free trial perma-link where we can encourage users to sign up. . Relevant. UI paywall has similar issues, especially when the token has\nexpired, it doesn't offer nice recourse for updating the token (hit by a\nuser)\nOn Wed, Feb 14, 2018 at 9:32 AM, stale[bot] notifications@github.com\nwrote:\n\nThis issue has been automatically marked as stale because it has not had\nany recent activity. It will be closed if no further activity occurs. Thank\nyou for your contributions.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/2266#issuecomment-365683985,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwKCHhqQA4HZ6D8lOEsaQt5RuMg2Hks5tUxikgaJpZM4PT2aS\n.\n. I think this is outdated. reopen if I'm wrong. ping @gabrielgrant \n\nClosing for now, reopen if there us more we need here. What's the status here?. waiting for clarification RE logging in via the dashboard but everything else LGTM. LGTM. What do you mean by Troubleshooting was more confusing than helpful? it's true that it needs updating, but I feel like the goal there was add all the common beginner questions? What would you want it to be instead -- I guess something more like stack overflow?. LGTM. I'm not opposed to taking out troubleshooting, but I worry we'll see an uptick of very common user issues so we'll likely need something similar. Any updates to this issue? How important is this?. So this would be a non-triggering flag that they'd specify as part of the input spec in the pipeline?\nIn the scenario you described, what WOULD they want to happen when the persisted model (PM) is updated? \nDo new updates to there other input use the old persisted model? In other words, do they want to have one input repo be \"fixed\" to a specific commit that is manually updated in the pipeline spec when they want to change it?\nOr does the other input repo always use the \"head\" persisted model, but just doesn't trigger based on PM updates? \nWhat if there are multiple new versions of the PM? Run on all or just HEAD?\n. @kareemk can you clarify your question? In general, a job/pipeline can only write out to a single repo. . \n\nIdeally would be to have a possibility to disable automatic triggering at all, for example, if customer account does not have enough balance when customer uploads new data to the repo, and also possibility to trigger pipeline manually by some api call and command. It would then work on files committed since last time it was triggered.\n\n@pragmasoft-ua We're discussing adding options like this. It's not ideal yet, but stop-pipeline and start-pipeline should give you a way to implement some of this already. stop-pipeline basically just pauses a pipeline so it not longer triggers on commits, and then it'll catch up once you start it again. That catch up will include one job per commit, but as always, pachyderm will only process each new piece of data once even if it shows up in multiple commits. . Updating this from in-person conversations.\nIn cases where you have two separate inputs where you want one to be triggering and non-triggering (in functionality terms), this can be accomplished in the current system using differred processing as follows:\nIn this scenario, I\u2019m assuming that data added to A you want to be non-triggering and data added to B to be triggering.\nFirst, instead of having A and B as separate repos, you\u2019ll need to make those directories in the same repo. Pipelines can take multiple inputs from the same repo, so that\u2019s fine, and you\u2019ll just use different glob patterns. So your inputs will be repo:foo glob: A/ or glob: A/* or whatever glob patterns you\u2019re using and repo:foo glob: B/ etc.\nThe other piece to this is to use the deferred processing method outlined in this document: http://docs.pachyderm.io/en/latest/cookbook/deferred_processing.html.\nYour pipeline inputs will both be taking the master branch of that repo. But when you commit data (e.g. put-file \u2026), you should commit it to the staging branch (you can name that branch something other than \u201cstaging\u201d of course). Whenever you want to trigger a job, you simply call pachctl set-branch staging master which will trigger a job using the HEAD commit on staging. In this case, this means that any time you commit data to B you call set-branch after that, but dont call set-branch when committing to A.\nCurrently, the input sources need to be in the same repo for this to work nicely. We understand that there are logical scenarios where having A and B in the same repo is annoying because there may be other pipelines that consume A and B and they prefer to be separated. Once #3267 is added, this will work as described above in separate repos as well. You'd atomic update the branches in both repos. \nOne unresolved annoyance of this deferred processing method is that it requires that whoever is adding data to the repo to be aware of pipelines that are consuming that repo and follow this differed processing method. This can be very cumbersome to automate and is restrictive in a large organization where adding data to pfs and pipeline creators are very separated. This is a possible argument for still considering \"passive\" (e.g. non-triggering) inputs -- that way this behavior is captured in the pipeline only independent of data getting added to the repo. . @sjezewski is this still relevant? I'm removing from 1.7 milestone, but add it back in if you think its a priority.. Fixed via #2360 \n@jdoliner @dwhitena I'm changing this to a docs issue because we need to document the private registry enterprise usage. This documentation should also provide the list of images (above) that are required and how to get/build them.. >put-file with no destination path\ncan you clarify this? I can't seem to repro via pachctl. Hi @konstunn. We don't officially support those architectures, but you can check out the make file in our repo, make docker -build in particular should get you started. If you'd like to give it a try and let us know what works or fails, we'd love to see the results and can help you from there. . Yeah that should work. Once you have docker set up, you should be able to\nrun Kubernetes and Pachyderm within Docker and get everything deployed.\nOn Sun, Oct 1, 2017 at 2:39 AM, Konstantin Gorbunov \nnotifications@github.com wrote:\n\nI've already managed to build and run docker on i686 Debian (though the\ncpu supports long mode, so its architecture is x86_64, but crossgrading the\nsystem from i686 to x86_64 is not an option right now). I've successfully\nrun hello-world container. So it seems to work, right? Unfortunately, I can\nnot reproduce all the build steps right now. What I failed to do so far is\nto build a debian package so that I could track and manage the installation\nin my system with a package manager.\nI hope that, as long as I have docker set up (though, manually) and\nrunning, things will get simpler to build all the remaining things inside\ndocker. What do you suggest?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/2339#issuecomment-333365106,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwIWfn89dCk_fogqp7MmXj-OAySbRks5sn13dgaJpZM4PlNst\n.\n. @konstunn any luck getting this to work?\n\nI'm going to close the issue since it's not something we're officially supporting right now, but we'd love to hear about any progress you've made. @dwhitena We can do a quick pass through all the examples and make sure the inputs field has been fixed. . ping @msteffen . Hi @uzdry, I'm really sorry you're hitting this. I'm wasn't able to reproduce this on my own just now, but we'll take a closer tomorrow and figure this out for you. \nDid you try setting the --validate=false flag?\n@jdoliner @dwhitena any chance you're able to repro on your setup?\n. Ah my apologies. You can do pachctl deploy local --dry-run and we'll print out the kubernetes manifest. Save it as then pass it to kubectl create -f with the flag. . docs LGTM. LGTM. fixed. @msteffen we should make list-datum, list-commit, and maybe list-repo streaming just like we did for list-file in #2418. this is already in PR so adding to milestone. Should this be closed or is there actually something we need to do here. If so, let's narrow it down to a clear proposal. How does this interact (or not) with the other 1.7 developer cycle workflow GH issues?. subsumed by #2390 . @msteffen this is fixed right?. Any updates on this issue? Has this been regularly reproduced with Flannel, but works fine with Calico or other networking?. fixed in #2397 . @jdoliner Is this something that is likely to be fixed by the worker refactor #2396 ? Or is it an issue that needs to be investigated independently?. Is this related to #2438 at all? Or this just adding retry logic for cloudfront request?. It's definitely worth fixing, but we're keeping the 1.7 milestone very lean. I've labeled it as 1.7 backlog which is basically what we'll use to fill the 1.7 milestone queue once we get through the main pieces.\nThere are lots of small easy fixes like this in the backlog label and it doesn't necessarily mean we'll get to it later than other things, but we want to keep the milestone relatively uncluttered. . Update pipeline in the Pipeline Builder (PB) is one key reason. \nWe want a user to be able to click on the DAG, click an \"update pipeline\" button, and have the PB autofill in the fields that are already set for that pipeline. In the current state, we can't tell the difference between set values and default values populated that weren't explicitly set by the user. And showing the user every field kind of defeats the purpose. \nA second use case is, now that we'll have the PB, users will want a way to get out or download a JSON spec file to save in Git or share otherwise. . We decided against this for now and will work around it in the UI. May revisit later if a new compelling reason emerges. closing. Just as clarity for issues in this milestone, #2396 should address this?. Thanks @jiahao. Please sign our Contributor License Agreement (CLA) and we'll work on getting this merged\n@dwhitena Can you get this merged as part of your other docs projects. . @jiahao I realize we're still waiting for the CLA before we merged this. Any chance you want to sign it even though it's just a one character change?. @jdoliner whats the status of this issue? You marked it as 1.7 and it'd be good to have more details about what we want to fix here. There's easy ways to make the error go away, but is there a bigger problem that needs to be figured out too?. Sounds like I was misunderstanding this issue. \nWith regard to Derek's initial submission: \n\nNotice how, if an error is encountered, we don't exit and instead proceed with the empty buffer.\n\nHave we changed our code to exit here instead of proceeding? Should we be doing that?. add cron. Still relevant, just didn't make it into 1.7\nOn Wed, Feb 14, 2018 at 9:32 AM, stale[bot] notifications@github.com\nwrote:\n\nThis issue has been automatically marked as stale because it has not had\nany recent activity. It will be closed if no further activity occurs. Thank\nyou for your contributions.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/2433#issuecomment-365684003,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwMUyd24n0XOOS5TWv8hbQ3T5Aztpks5tUxingaJpZM4P_8-B\n.\n. Still relevant for 1.8 \"groups\" feature.\n\nOn Wed, Feb 14, 2018 at 9:32 AM, stale[bot] notifications@github.com\nwrote:\n\nThis issue has been automatically marked as stale because it has not had\nany recent activity. It will be closed if no further activity occurs. Thank\nyou for your contributions.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/2434#issuecomment-365684024,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwHohXLgFG2IljzfIIWsAF3xBQ0cOks5tUxirgaJpZM4P_9OF\n.\n. @sjezewski there's a change in this PR to make only 1 etcd pod so we should be good once this lands. . I wonder if there's just an easier or different way to solve the use case you're describing @gg. I've definitely seen users ask questions similar to \"How do I easily run my code over a subset or specific datums only for testing/debugging.\" \n\nOne of the open discussions we've had is around better meta-operations for moving data around pachyderm. A simple example could be clone-repo which would let me make a new pachyderm repo with all or some of the data in an existing repo. clone-repo is just sugar for create-repo and then put-file -f pfs://.... I'm not saying we actually need \"clone-repo\" but what you're describing above could be done by just plucking a few datums from the master branch into a 'test' branch (or deleting all except the faulty datums). \nMaybe I'm misunderstanding, but I'm not sure semantics like you're proposing are quite to right way to allow this type of workflow.. User PR submitted for this. #2667\nOn Wed, Feb 14, 2018 at 9:32 AM, stale[bot] notifications@github.com\nwrote:\n\nThis issue has been automatically marked as stale because it has not had\nany recent activity. It will be closed if no further activity occurs. Thank\nyou for your contributions.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/2443#issuecomment-365684012,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwCbnqrIhPNVntPHXjS9prxRHOEkUks5tUxiogaJpZM4QAVyv\n.\n. download file or logs is broken without this. . Still an issue for prod clusters.\n\nOn Wed, Feb 14, 2018 at 9:32 AM, stale[bot] notifications@github.com\nwrote:\n\nThis issue has been automatically marked as stale because it has not had\nany recent activity. It will be closed if no further activity occurs. Thank\nyou for your contributions.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/2451#issuecomment-365683977,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwIMaCEZ5kfOBSonAZaXXhaaTP3tVks5tUxiigaJpZM4QBQBy\n.\n. @bpb With cross Pachyderm waits for at least 1 commit in all inputs. In other words, in a cross, datums are a tuple of one piece of data from each input and pachyderm waits until at least one complete datum is available. \n\nIn a union, datums are independent so waiting for one datum in each repo isn't required. . I dont care. @dwhitena you are the docs master so its your call. Some ideas, not all need to be included in the first pass of this doc\n\nDeploying to a non-default namespace and any complications from that\nbest practices that include keeping your previous pachctl deploy command saved to reuse\nHow to snapshot your volume in each cloud provider for data safety if something goes wrong\nLink to migrations and mention that they may need to update pipeline specs if there have been changes as part of the update\nknown problems with AWS mounts being finicky and how to troubleshoot\n\nUntil #2451 is solved-- scaledownthreshold resets in a upgrade so they may have resource problems. Perhaps it makes sense to pause pipelines before the upgrade? There's just a lot of surface area for things to go wrong, but given the fast development speed of Pachyderm, we don't want users being scared of upgrading. \n. This issue has confused a lot of early users as it's very common to have a small number of datums when you're first learning and developing pipelines on Pachyderm. \nCould we consider potentially making the default queue just 1 so it does what users expect at that scale and then changing the queue size larger is a performance optimization that users can add when they're productionizing pipelines? The increased performance we're trying to give with a default >1 is nullified for low datum counts.\nThe alternative to this would be to have smarter logic that sets queue size based on the number of datums if not set explicitly by the user. . LGTM. hi @cglewis Thanks for submitting this! \nCan you please sign the Contributor License Agreement (CLA)?. just in case this has to do with some odd dag bug:\nSchema (1 commit) --> image_processor\nData (2 commits) --> image_processor\nimage_proc actually had 7 commits on it do to a few failed attempts, a few --reprocess calls, and stats commits (which we know are on a separate branch)\nCreated uploader with only image_proc as input... no jobs were triggered\nOther notes for repro:\n - Originally tried creating and updating pipeline in same json which failed #2485. Unlikely to be the cause\n- pods/pipelines came up fine and everything looked good, but no jobs. Even update-pipeline for uploader didn't start jobs --> hypothesis: a pathalogical bug with flush-commit as opposed to some race state with pipeline creation/updates\n- running --reprocess on the image_proc pipeline then did trigger uploader to run, but only on those newly created commits. I know it's its own section, I'm simple suggesting adding an \"install pachctl\" bullet point to all the prereq lists so we nip that one in the bud. @jpoon Nope those aren't you. Our CI just needs internal creds so it'll fail for external contributors. @dwhitena Docs are your domain, can you review and merge?. #2500 passed CI. Merging. closed via #2499 . LGTM. +1 to JD's comment. We've talked a fews times about the idea of having non-triggering input repos (#2296). That's basically what we want here, right? No job to trigger when a commit is made to the pipeline_spec repo.. Please make sure you update the pipeline spec docs as well as open issues for broader documentation that may need to be done. \n\nOn Nov 10, 2017, at 2:46 PM, Sean Jezewski notifications@github.com wrote:\nWIP\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/pachyderm/pachyderm/pull/2507\nCommit Summary\nDebug deploying k8s 1.8\nFix compile bug\nString resource limits for GPU through\nSpecify resource requests/limits on a pipeline\nAdd resource limits to pipelines\nUpdate protos, fix compile errors, update test\nFile Changes\nM etc/kube/internal.sh (15)\nM etc/kube/start-kube-docker.sh (4)\nM src/client/pps/pps.pb.go (1069)\nM src/client/pps/pps.proto (12)\nM src/server/pachyderm_test.go (79)\nM src/server/pkg/util/util.go (31)\nM src/server/pps/pretty/pretty.go (18)\nM src/server/pps/server/api_server.go (85)\nM src/server/pps/server/master.go (19)\nM src/server/pps/server/worker_rc.go (39)\nM src/server/worker/master.go (13)\nPatch Links:\nhttps://github.com/pachyderm/pachyderm/pull/2507.patch\nhttps://github.com/pachyderm/pachyderm/pull/2507.diff\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Are we able to clone just a specific commit of the repo instead of the\nwhole repo to make this faster?\n\nOn Thu, Nov 16, 2017 at 4:02 PM, Sean Jezewski notifications@github.com\nwrote:\n\nIt seems like the recent additions of the github input tests adds ~5m+ to\nthe test suite. This is mostly because we're cloning pachyderm as part of\nthe tests (which is a big repo). We should switch that out to a smaller\npublic repo. This is a pretty easy fix, and will knock off a fair bit of\ntime.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/2511#issuecomment-345102937,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwG0njTpR0_yidJIh38HvN0Al9ZAFks5s3M0ggaJpZM4QaNrB\n.\n. @jdoliner, I know we're reworking our job triggering code so this bug may not be relevant soon, but is there anything you can think around the non-transitive reduction part of the code that would break down in this DAG but work for the simple cases we've tested?\n\nI figure that same logic is being rewritten into the new system too and if this DAG fails currently, it may still fail in the new design as well. \n```\nA --> B --> D\n \\    |    /\n      C \n````\nA --> C\nB --> C\nC --> D. This should be fixed now. Sorry about that @amanderson. closing. LGTM. Thanks @drewda. Can you sign our Contributor License Agreement (CLA) and then we'll get this merged. . Thanks for signing the CLA @wardn. merging. . We haven't seen much EKS in the wild yet and are still putting together\nstandardized docs\nOn Tue, Dec 12, 2017 at 9:56 AM, Seamus Abshere notifications@github.com\nwrote:\n\nhey @dwhitena https://github.com/dwhitena do you know of anybody doing\nthis yet?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/2550#issuecomment-351132165,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwHoQit_7E_DY0IDUIz5rwldCKKQYks5s_r5bgaJpZM4Qwp6g\n.\n. Hi @jeffm14vt Thanks for the PR and signing the CLA. \n\nCI fails for external contributors because it needs creds. No big deal, we'll run CI for you and merge this in once it passes. Thanks for catching the typo, so there's no actual reason CI will a problem. . I'll take another look at a solution for this -- manually updating the TOC is a pain. . LGTM\nHas the limits change been documented everywhere else it shows up too?. @dwhitena Let's document this and add it. Reviewed and edited the docs. Docs LGTM. @jdoliner or @sjezewski -- the framework for this is all there, but GCS just hasn't been implemented yet. Should be pretty easy. \nNot sure if this is relevant, but the user also said, \"I get that gcs is a pain because it must be an environment variable to a json file\" . I like this!. I think reducing etcd nodes to 1 will mostly solve this. We leverage etcd heavily and reducing CPU and this will only be a problem for scheduling purposes so I think we're ok to leave as is for now. @jdoliner do you think otherwise?. @yehiyam @dwhitena This actually can just be an absolute link to our docs (not GH) instead of relative link, but should still have the .html suffix. \nSee: http://pachyderm.readthedocs.io/en/latest/fundamentals/getting_data_into_pachyderm.html#pachyderm-language-clients. @dwhitena we also discussed changing the messages slightly in our last checkin. Thanks @brycemcanally! We'll run CI and get this merged when everyone gets back from holiday. . @dwhitena CLA has been signed so this is ready to run in CI and get merged. Hi @williballenthin. Thanks for all the details in this issue. Now that everyone is getting from break, we'll take a look at this and figure out the best way forward for you. . @williballenthin We have a PR in progress that should fix this issue and get Pachyderm to work with RBAC better. . Thanks @frankhinek This is great. @dwhitena will review this when he gets a chance and we'll merge it in from there. . Thanks @brycemcanally! We'll take a look and provide some feedback. \n@sjezewski @jdoliner . Hi @mightyguava Sorry you're hitting this. We just added rbac support in yesterday's release and it looks like that wasn't updated in the one-shot deploy script. We'll get that fixed for you shortly.\n. Hi @raja1212.\nthe -c flag starts a commit, puts the files, and finishes the commit. It looks like you already have a commit started so there are a few easy options based on what you want to do. \n\nYou can finished the commit that's already started which may or may not have files in it already. finish-commit parameters master\nIf you havent already added the parameters file, just take out the -c flag in the command you're using above and then call finish-commit as I mentioned in #1. \n\nTo see the current state of commits and files you can run these commands:\nlist-commit parameters -- this will show you start and finish times to see which commits are open.\nlist-file parameters master will show you what files are in your HEAD commit right now so you can decide what you want to add. \nYou can also ask questions like this in our slack channel if you want quick feedback/help: slack.pachyderm.io. Thanks @nysthee! @dwhitena will review this (it's such a simple change so that'll only take 10 seconds) and get it merged in. \nCan you sign our CLA?. Thanks, we'll get this reviewed and merged in shortly.. @thedrow we have a pachyderm helm chart now: https://github.com/kubernetes/charts/tree/master/stable/pachyderm. That's the plan in the near future. . Thanks @antjkennedy. @dwhitena will review this and get it merged in.. As we discussed offline, we're going to want to upgrade the beginner tutorial and a few other docs to help users use the dashboard now that it's being deployed by default. \n@dwhitena I think you have a good notion of what those changes are. We can just make those additions to the tutorial as part of this PR and merge it all together. . LGTM. Thanks for the info @ryansmith23.. Actually, It looks like this was already updated (not quite the same way) on our end in the interim. Re-closing, but thanks again for the CLA and PR @sillystring13. We always love contributions like this!. This has been added in 1.7.10 via PR #3169. Potentially also include user seats info. @liuchenxjtu a local deployment (minikube) of Pachyderm just uses local storage and not a full object store. It's also not really meant for production workloads where you'd need a real object store. \nIf you want to use minio as the storage backend for a tiny cluster, you'll need to do a single-node or on-prem deployment of kubernetes and then use the custom deploy or helm chart as described above. \nIf you tell me a bit more about your setup and requirement, we can give you additional guidance. For example:\n\ncould you advise what we should be for the local clusters\n\nWhen you say \"local clusters\" do you mean on-prem or do you mean a minikube or \"pachctl deploy local\" cluster?. @williballenthin Commit's in pachyderm, similar to git, are the accumulation of the files that have been added. So when you add yes1.txt, then no1.txt, then yes2.txt, you still have the no1.txt file in your head commit. When pachyderm triggers a new job, it reprocesses all datums that have not been successfully processes. That means, when the job triggers, it's still processing no1.txt, failing, and then marking the whole job as failed. \nIn the example, you've essentially got bad (failing) data in your head commit. To solve this, you need to delete the bad data. \nIf I'm understanding your use case correctly, that's actually not quite the behavior you want. You actually want to keep the no.txt files in the system. That means you'll want to actually change your code to always exit0 for both no and yes files, but perhaps categorize them differently. You could, for example write all your yes file outputs to a yes directory and same for no files. Then have your next pipeline use the glob pattern input/yes/* to only process the output of the yes directory. You may even want to have a separate pipeline that looks at the no files and does something with them (like send alerts) or perhaps not and just ignore them. \nDoes this all makes sense? Both in terms of how pachyderm behaves and how to accomplish what i think you're aiming for?. More users starting to hit this is public channels. I think this becomes very doable with the current streaming list-* PR, right?. reassigning to @dwhitena since he's the deployment guru and this is his domain. This continues to plague users. Any chance we can bump the priority on this? It's not an intermittent thing, it's every time and constant. . Not restricted to Mac apparently. . oh, my labeling was silly, as it was just to note to myself that 1.7 backlog things are things that we want to consider for slotting into 1.8 when we actually make that label update. \nIn terms of fixing, it sounds like there are no good solutions.... but for every user that asks about it there are 10+ that don't and it makes the product feel unpolished when a standard workflow for the basic product is littered with error messages. . kc get pods\nNAME                    READY     STATUS    RESTARTS   AGE\ndash-594597bdb8-6j757   2/2       Running   0          1m\netcd-6f9d874f4f-c5psv   1/1       Running   0          1m\npachd-d7b6b989f-wcvm5   1/1       Running   1          1m\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ pc create-repo images\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ pachctl put-file images master -c -i ~/pachyderm/doc/examples/opencv/images.txt\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ pc create-pipeline -f ~/pachyderm/doc/examples/opencv/edges.json\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ pc create-pipeline -f ~/pachyderm/doc/examples/opencv/montage.json\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ kc get pods\nNAME                        READY     STATUS     RESTARTS   AGE\ndash-594597bdb8-6j757       2/2       Running    0          2m\netcd-6f9d874f4f-c5psv       1/1       Running    0          2m\npachd-d7b6b989f-wcvm5       1/1       Running    1          2m\npipeline-edges-v1-7z98t     0/2       Init:0/1   0          7s\npipeline-edges-v1-ltw66     0/2       Init:0/1   0          7s\npipeline-montage-v1-dpg94   0/2       Init:0/1   0          4s\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ watch kubectl get pods\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ clear\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ pc list-repo\nNAME                CREATED             SIZE\nmontage             2 minutes ago       378.6KiB\nedges               2 minutes ago       105.7KiB\nimages              3 minutes ago       57.27KiB\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ pachctl put-file images master -c -i ~/pachyderm/doc/examples/opencv/images2.txt\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ pc delete-all\nAre you sure you want to delete all ACLs, repos, commits, files, pipelines and jobs?\nyN\nRepos to delete: montage, edges, images\nPipelines to delete: montage, edges\ny\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ pc create-repo images\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ pc create-pipeline -f ~/pachyderm/doc/examples/opencv/edges.json\n(reverse-i-search)`m': pc create-pipeline -f ~/pachyderm/doc/examples/opencv/edges.json\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ pc create-pipeline -f ~/pachyderm/doc/examples/opencv/montage.json\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ pachctl put-file images master -c -i ~/pachyderm/doc/examples/opencv/images.txt\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ pc list-job\nID                               OUTPUT COMMIT                            STARTED       DURATION RESTART PROGRESS  DL       UL       STATE\nbe39f928ad7d44538748b4d68ad1de05 montage/8a7f9508c9824f05a182bdca3b71ab85 4 seconds ago -        0       0 + 0 / 0 0B       0B       starting\n402934e076864bf5acb44cb758fce35a edges/9702e78885be444793e31e18e1a7027e   4 seconds ago -        3       1 + 0 / 1 57.27KiB 22.22KiB running\nJoseph-Zwickers-MacBook-Pro:~ joeyzwicker$ pc list-job\nID                               OUTPUT COMMIT                            STARTED        DURATION RESTART PROGRESS  DL       UL       STATE\nbe39f928ad7d44538748b4d68ad1de05 montage/8a7f9508c9824f05a182bdca3b71ab85 38 seconds ago -        0       0 + 0 / 0 0B       0B       starting\n402934e076864bf5acb44cb758fce35a edges/9702e78885be444793e31e18e1a7027e   38 seconds ago -        10      1 + 0 / 1 57.27KiB 22.22KiB running. Thanks for pointing this out @gkumar7. \n@dwhitena will update the docs to reflect this change in 1.7. Not sure why this is showing up with a list-command instead of at the put-file command though. Hi @oskca, Pachyderm's Enterprise Edition has offline cluster support to do exactly what you're looking for here. Shoot me an email at joey@pachyderm.io and we'd be happy to discuss further to get you set up. . these are being autogenerated, but the \"see also\" section is linking to the .md file instead of .html. I'm not sure why, but can try to to investigate. . Thanks for the typo find and PR. Can you sign our CLA and we'll get this merged in?. hey @DSchmidtDev. Adding minio as an egress endpoint is possibly something we can add in the future. That said, you can accomplish this very easily in the system right now by just making your own egress pipeline that takes as input your current final pipeline and then writes data to a minio bucket and nothing to pfs/out. . @dwhitena feel free to merge after CI passes. Thanks for the clarification. The right step would probably be to just mention in the GKE doc something like, \"if your cluster is having permissioning problems, check out our doc on RBAC (link)\". That's probably the easiest fix. @brycemcanally Is this something that we should be trying to do automatically in pachctl deploy google or have it as a separate cmd that users may need to run?\n@Nick-Harvey Let's get this minor docs change prioritized a bit.. slotting this in for 1.9 (which means it'll likely land in a 1.8.x release). Workaround:\nAfter you create the pipeline, you can do kubectl edit on the RC and just change the config manually to remove  the alpha tags that are incorrect. We know this is a bit annoying having to do this for each pipeline, but you should only need to do it once for each RC and we'll get this fixed as soon as possible. . Hi @elgalu. We've got a few options for you. \nFirst, if you use the --dry-run flag after pachctl deploy ..., it'll spit out the full k8s manifest that you can edit and use manually. \nAlternatively, we do have a Helm chart, but it's relatively recent and is still being improved. \nI'm going to close this issue, but feel free to reopen if neither of these solve your requirements. @williballenthin so you're not reprocessing the old datums it sounds like, only processing the new ones that are being added in the new commit. So if you do inspect-job, you should see a number of datums processed and number skipped, right?. We've got an improvement incoming that will drastically reduce the overhead for skipped files which should pretty much completely resolve this issue. . @brycemcanally Is this issue fixed by the changes you've been making recently?. Thanks @wardn! Can you sign our CLA and we'll get this merged?. Oh yup, you're totally right. Thanks Eddie!. Thanks @Nick-Harvey. CLA has already been signed so merging this in. . Hey @jkinkead and @msteffen Just checking in on the status of this PR. @jkinkead Let us know if you need any other guidance making the vendor.json changes and then we can get this merged in. @jdoliner didn't we remove the need for privileged here: https://github.com/pachyderm/pachyderm/pull/2887?\nI guess not for worker pods?. @iamneal this issue seems resolved so I'm going to close this. Let us know if there's anything else we can do to help you out. . Thanks @checkaayush. I see you signed the CLA so we'll get this merged!. Isn\u2019t this what incremental is for? \u201cI need to look at my result from last time to decide what to do this time\u201d\n\nOn Jun 13, 2018, at 7:33 PM, Gabriel Grant notifications@github.com wrote:\nWhat is the goal / desired outcome?\nPachyderm doesn't currently have a well-trodden path to implement an iterative algorithm, where a result is recursively refined until it reaches some acceptable end state. This end state may be defined by total number of iterations, but is more likely defined as an acceptable convergence point either determined by some measure of absolute accuracy, or by comparing the current iteration's result to that of previous iterations and observing that the estimate is no longer (significantly) improving.\nThe specific example that Drayton Munster (@dwmunster in #users) is trying to work through is parallelizing multiple MCMC runs (something like http://dfm.io/emcee/current/), but the same pattern basically applies to most optimization algorithms (Genetic Algorithms, simulated annealing etc.) / similarly-formulated physics simulations (protein folding, graph layout, etc.):\nchoose a set of start points (possibly parallel)\nsample your function at those points (parallel)\npick the \"good\" ones (possibly parallel)\ndetermine if your results are \"good enough\" (likely not parallel, though ideally this would be able to use an iterative/parallel reduce, if pach support existed)\nif so, stop\nif not, choose a new set of points to evaluate (likely based on the best candidates from the previous iteration) and repeat\nWhat is the ask?\nIdeally: a way to achieve this within the pachyderm execution model, that preserves provenance across iterations and doesn't require hacks.\nAt the least: a clearly-defined recommendation for how to achieve this pattern using the pachyderm primitives available today.\nIf there is a way to accomplish this today via workaround, what does that require?\nIt seems the most straightforward way to implement this today would be to have a \"DAG\" that looks like this:\n[iterations] --> [samples] --> [stop checks]\n    ^                                   .\n    .                                   .\n    . . . . . . . . . . . . . . . . . . .\nProcessing begins by adding a new/first iteration (containing initial conditions). This generates a set of points at which to sample the objective function. The samples are evaluated to determine whether the stop condition has been met. If so, final results get written out; if not, a new iteration, with refined initial conditions, gets added to the iterations repo (dotted line represents this recursive pseudo-link -- since data is directly committed to the upstream repo, rather than passed as output, the provenance tracking chain gets broken here)\nOne way I can think of to get full provenance tracking would be to dynamically \"unroll\" the iteration loop: if the evaluation step determined that another iteration was needed, it could create another downstream step in the DAG. Actually creating a whole chain of pipelines running the same code seems pretty absurd, so I wouldn't recommend implementing this except maybe if the provenance tracking were truly critical, and the expected number of iterations were (very) small. But I could imagine some sort of model baked into Pachyderm that logically implemented cyclical provenance tracking in this manner.\nIn @dwmunster's case, the final evaluation (potentially?) requires all samples from all iterations be present. Because of that, it seems the most straightforward setup would be to have each iteration's samples be written to a new directory in the samples repo. At some point (tens of thousands of iterations over tens of thousands of sample points) having all these files in a single commit will start to cause significant overhead for each iteration. The upper bound on the total number of files in a commit should be significantly increased by #2984 but there is still some cost to transferring this extra metadata to workers when doing processing. There's also a cost to checking that previous iterations don't need to be re-evaluated. I think there may be a way to divide the files into datums in such a way that the cost would be relative to the number of iterations (1000s), not the total number of evaluations (potentially 10s of millions), which wouldn't be so significant, but the naive pattern of having iterations// and taking /iterations// as input to the sampling step would require checking every one of those past evaluations, which would likely become prohibitively expensive. This cost may be mitigated by @brycemcanally 's current work (#2986 ?), and would likely be eliminated by #2993\n@dwmunster was considering writing each iteration to a separate branch to avoid these performance concerns, but that would require complications of having to update downstream pipelines to take different branches (potential use-case for #2882), so is going to try the above pattern first.\nFor many (most?) other cases, such Genetic Algorithms, there's a bit of a difference in that you're generally only concerned with the surviving population at a given iteration, so although it has the same general iteration pattern, the working dataset doesn't need to continue to grow the way it sounds like this MCMC case will, so the samples can likely be overwritten, mitigating the above-mentioned concerns.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. When we make this change along with #3012, we should probably extract/restore the activation token as well. This stops users from having to call activate again, but also stops users who abuse enterprise trial tokens with multiple email addresses from just extract/restoring their cluster easily. . This is required for 1.8, right, since it'll require a migration?. This should be ready to merge?. Thanks @thedrow! I'm traveling at the moment, but will review this when I get back into town and get this merged!. @jdoliner can we just merge in the spelling fixes from this even tho it's not part of CI?. Hi @kevlar1818, did you have any luck deploying on GKE to figure out if this was a minio or helm issue?. LGTM We can merge as is for now and come back for some of the other things in this thread. @jdoliner Are there any docs for this? I don't even see it in the basic pipeline spec docs. Ah perfect. Thanks Suneeta. Sorry that PR slipped through the cracks. We're on it. hi @suneeta-mall What do you mean by \"does not really 'do' completion\"? \n\nThe path creation part I agree we should fix, but is there something specific you expect completion to be doing that it doesn't?. Thanks for submitting this. We\u2019ve already filed a internal report on this issue and will have a quick fix and new dash release out shortly. Really sorry you\u2019re hitting this issue\n\nOn Jul 8, 2018, at 9:41 PM, suneeta-mall notifications@github.com wrote:\nPlease see the issue with UI error/warning alerts. It renders UI completely unable.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Thanks so much for the PR @aarondancer! Can you sign our CLA and we'll get this merged in?. Thanks for catching this @GuillaumeDelaporte . Can you sign our CLA?. oops. Dupe of #2971 . This hasn't been slotted into any immediate sprint yet, but we've continued\ninternal discussions about this feature and will post any design proposals\nthat come from that as well as give a more detailed timeline if/when it\ndoes get scoped into an upcoming release.\n\nOn Thu, Mar 7, 2019 at 9:24 PM gregfriedland notifications@github.com\nwrote:\n\n@JoeyZwicker https://github.com/JoeyZwicker @jdoliner\nhttps://github.com/jdoliner Any update on adding this feature sometime\nsoon?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/3065#issuecomment-470809267,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwKaHO-5u287qkqoq-XrocGxx0Uqxks5vUfPvgaJpZM4VXVfd\n.\n. Yeah we should fix this, especially because it's a valid use to have a glob pattern that might purposely not match anything at times. . LGTM. . Thanks @suneeta-mall. @Nick-Harvey will review this and post if there are any comments or changes needed. @Nick-Harvey This is still waiting for the changes made to be merged. . Thanks for pointing this out! We'll definitely work on getting that fixed @Nick-Harvey . Thanks for the PR @sviterok! I see you've already signed the CLA so we'll go ahead and get this merged in shortly. \n\nThanks again for being a great part of the community and helping us fix these little details. . LGTM. Side thought -- as nice as it is for demos, do we want to get rid of -i and simplify put-file a bit? I've almost never seen any users use it. Just a thought. . It would be ugly (as Yusuf's PR shows) if you're adding multiple URLs, but more commonly we see people use -r and just add a whole directory of files. This is an engineering decision, @jdoliner. We can keep -i if we want, but should then update the semantics a bit.. This is some brain-dump details for a future performance optimization doc that will need to be written eventually. That doc will still require additional input from Bryce at a future time. . If you were able to repro initially and couldn't any longer after the fix, feel free to close. I can test it further when it get released. Just commenting here for future reference and then I'll make the adjustment. When adding new cookbooks or docs sections, it needs to be added to the table of contents (ToC) in the index file otherwise it wont show up in the docs sidebar. I didn't catch that during my review. Talked through this with JD and agreed with @gabrielgrant that for this type of problem, shuffling would be the better solution, not bruteforce cross method. We also don't want the example in ruby because that's just a really uncommon scenario. Closing for now and we'll have the evangelism team (@Nick-Harvey) make a different shuffle join example that's closer to our shuffle docs. . Thanks @d1vanloon! Can you sign our CLA and we'll get this merged in?. CI is just failed for a perms issue so no worries there. I can take care of that. . Agreed. I\u2019ll correct the example docs to empty files. \n\nOn Oct 30, 2018, at 4:44 AM, Gabriel Grant notifications@github.com wrote:\nThe \"lazy shuffle\" example added in #3202 is implemented correctly in terms of code, but the explanation (and even the title) conflate lazy with empty_files. These aren't the same thing, and afaiu actually using lazy here would be ill-advised. Is there a reason basically all references to \"lazy\" in that PR shouldn't be changed to \"empty files\"?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @Nick-Harvey . I wasn't able to repro with the steps where I was in the past. Closing for now, but definitely reopen if you hit this again. . Thanks for filing this big @samuelhug. We'll get this fixed soon.. To add some clarity:\n\nWhile pachyderm doesn't have amaizng ui-oriented \"experiment tracking\" the way users have in mind like with CometML, we do offer the underlying building blocks to do experiment tracking:\n\nversions of training data set used are in input repo\neach run of your training model is up update-pipeline --reprocess --> job\neach output model generated by code run is in output repo\nprovenance shows you which output model came from which version of data set (data set very well might not change)\ninspect-job will show which version of code created each model\n\ntada! Experiments tracked! . Fixed. Thanks!\nOn Mon, Nov 19, 2018 at 5:25 AM, Jeff Hale notifications@github.com wrote:\n\nWhat is the goal / desired outcome?\nIf there is a way to accomplish this today via workaround, what does that\nrequire?\n(Optional) What is your proposal for a feature to solve this?\nEnvironment?:\n\nKubernetes version (use kubectl version):\nPachyderm CLI version (use pachctl version):\nCloud provider (e.g. aws, azure, gke) or local deployment (e.g.\n   minikube vs dockerized k8s):\nOS (e.g. from /etc/os-release):\nOthers:\n\nWhat happened?:\nClicked on the \"our team\" link in the readme\n[image: screen shot 2018-11-19 at 8 21 08 am]\nhttps://user-images.githubusercontent.com/7703961/48709856-b6f67780-ebd4-11e8-97b6-10888dd6d231.png\nin the sentence: \"Learn more about our team and email us at\njobs@pachyderm.io.\"\nResult: 404 Error\nLink is to:\nhttp://www.pachyderm.io/jobs.html\nWhat you expected to happen?:\nWasn't directed to a working webpage.\nHow to reproduce it (as minimally and precisely as possible)?:\nReproduce by clicking the link in the readme\nAnything else we need to know?:\nEnvironment?:\n\nKubernetes version (use kubectl version):\nPachyderm CLI version (use pachctl version):\nCloud provider (e.g. aws, azure, gke) or local deployment (e.g.\n   minikube vs dockerized k8s):\nOS (e.g. from /etc/os-release):\nOthers:\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/3222, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwJMwqFHd2M-hin1Gblmo1ynYMI6Tks5uwrFfgaJpZM4YpGb7\n.\n. Thanks @cbzapata. \n\nWhat's happening here is that between those old versions and current versions of docs, those urls have been removed or renamed/moved and ReadTheDocs isn't handling that linking correctly. We'll investigate and mostly like will just get rid of the docs from super old versions.\n. @Nick-Harvey We still have docs from 1.4 showing up. Is this closed because you have a fix in for everything or is \"the bigger issue is still at large\" -- in which case we should reopen and figure out how we want to proceed there. Thanks for submitting @anishvarghese and sorry that we were slow to respond on slack. We'll get back to you shortly with a debug path. Are the other Pachyderm pods running and working successfully? It's only the dashboard that's causing issues?. @gabrielgrant Can give you some guidance there. Thanks @shimpel, we'll double check, but I think it should be resolved in 1.8.0. @Nick-Harvey or I will review/merge this shortly. Hey guys, changes like this can't be merged unless you make these fixes in\nthe docs TOC (table of contents) tree, index file, etc. Now every link to\nexamples throughout our docs 404s because the dir structure is different.\nhttp://docs.pachyderm.io/en/latest/examples/README.html\nOne reason these were under /doc is because we want them nicely organized\nas parts of our developer docs. I'm sure it's possible to fix this, but I\ndon't have the time to look at it right now so it should be included as\npart of this PR. Sorry that I didn't track this PR and comment on it sooner\nhttps://github.com/pachyderm/pachyderm/blob/master/doc/index.rst\nOn Wed, Dec 12, 2018 at 2:10 PM Yusuf Simonson notifications@github.com\nwrote:\n\nMerged #3257 https://github.com/pachyderm/pachyderm/pull/3257 into\nmaster.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/pull/3257#event-2022760747, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABtjwHsamoDScYJVaHvMbrC8fJ0hSTw5ks5u4X6-gaJpZM4ZHRx1\n.\n. But do you understand why? :p\nOn Dec 8, 2018, at 1:32 PM, Yusuf Simonson notifications@github.com wrote:\nBug fixed\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @msteffen is working on migration for 1.7 to 1.8 that will do exactly this. tech is done. Docs are 90% done and will be published this week. Atomic updates would also be valuable in the use case described here: https://github.com/pachyderm/pachyderm/issues/2296#issuecomment-468499268\n\n@jdoliner certainly not urgent, but what are your thoughts for putting this into an upcoming milestone?. Thanks for submitting @XVilka. While I'm not super familiar with Ceph, it supports the S3 API which means it should be workable with Pachyderm as is, even if it's not full Ceph functionality. \nAdding more first-class Ceph support is definitely we'll consider in the future. . I don't know if they do, but certainly worth exploring. Either that or take our these sections @Nick-Harvey . right now when you call update-pipeline without the reprocess flag, it'll kick off a job, but that job will skip all datums, assuming they're all successful.  it will still retry failed datums, which I think is the behavior we want. \nMy reading of the main requests here are:\n- Atomic updates for multiple pipelines simultaneously -- this seems very reasonable\n- Having update-pipeline not trigger any job. I'm not sure if we want to do this because it then requires multiple steps for a standard \"my code is wrong and I want to push a fix\" workflow\n- A way to reprocess a pipeline without having to grab the json spec file. This only seems to make sense if the pipeline failed for transient reasons. \n@mttwong If I'm misunderstanding some part of your use case, definitely clarify for us. \n. Thanks for the writeup @ktarplee! \n@pappasilenus I believe you were exploring and documenting micro k8s as well? How far did you get and have you bee hitting similar issues?. Just FYI: Please try to label these as docs issues and assign to @Nick-Harvey and myself.\nObviously if there's a bug with push-images, that'll get passed to eng, but it helps us not lose track. I haven't looked at this example in depth, but this minor change is fine of course. I like removing output commit. As you said, that's the lead commonly used info i ever need to grab from list-job. 95% of the time I'm just grabbing the commit directly from output_repo master. Limiting the message length a bit could also be reasonable -- not sure what the right char length is though.. wait, actually -- the important info in list-job output commit is the output repo -- that's what i look at to see what pipeline a given job was part of. So it still needs the repo name (same as pipeline name), just not the commit ID. Just wanted to clarify that. Thanks @discdiver. We'll figure out what's going on there. I'm pretty sure we just broke some formatting thing in the ReadtheDocs and it's not finding the .rst or .md file correctly. @Nick-Harvey and I can investigate. Thanks for catching this @mdgoldberg. Can you sign our CLA: https://pachyderm.wufoo.com/forms/pachyderm-contributor-license-agreement?\nOnce we have that, I'll merge this is since its just a small change. . Thanks for the report @David-Development. We'll test this ourselves and make any needed fixes.. Is this a docs issue or actually a needed change with the deploy custom command?. Interesting enough, it did successfully spin up two worker pods. Maybe that's not the error causing the restart?\nkc get pods\nNAME                                READY     STATUS    RESTARTS   AGE\ndash-866c6c6f5-m7x99                2/2       Running   0          47m\netcd-b4d789754-4x8g5                1/1       Running   0          47m\npachd-6ff98d474d-vd7k8              1/1       Running   0          47m\npipeline-images-pipeline-v5-dbh4w   2/2       Running   0          11m\npipeline-images-pipeline-v5-kdbls   2/2       Running   0          11m. Hi @chungmeng, sorry you're hitting this. For right now, 1.8.x clusters need to be fresh as we don't offer a clean migration path (we're working on it). Check out the docs here for more details: http://docs.pachyderm.io/en/latest/deployment/upgrading.html. Thanks for the PR @ktarplee. Can you sign our CLA? \n@ysimonson I believe you had some similar changes to this that you were waiting to merge as well?. I imagine we basically want to merge both since anyone running 1.8.1 will start getting the warning?. @ysimonson fair point. @ktarplee Thanks for signing. We'll get this merged in when we're back in the office on Wed.. Thanks for the details @ktarplee. We'll work on taking a look at this as soon as possible.. think you pinged the wrong JD :). Thanks @anjor. And I see you signed the CLA already so we're good to go.. These ports also need to be documented so adding docs label. . Just to make sure I understand this: so if I user has this error transform set, then any datums that error/fail for any reason will no longer cause the job to fail? Successful datums will get passed through to the next steps and failed datums will get triaged into their error handling process. So a job can never really \"fail\" any more or can it still fail just not in the specific user-defined ways?\nBut if there is no error transform defined then the pipeline will continue with the current behavior where any failed datum means the whole job fails and an output commit is not finished?. For us less technical-in-the-weeds folks, can you explain in this new pipeline model how something like kubeflow and pachyderm would be interacting as part of a distributed training job?\nMy understanding is as follows (please add clarifications/corrections):\n1. Data commit comes into Pachyderm repo like normal (presumably the training data set and probably parameters/metadata needed as well).\n\n\nKubeflow stuff happens....  does distributed training (this is part I'm trying to get a picture for) -- what are the steps that happen here and components involved (as if we had to draw out an architecture diagram, which we do at some point)?\n\n\nKubeflow outputs resulting model binary/binaries and meta data back into pachyderm. Is this being written into pfs/out in the worker container's file system like normal?. Is that all it takes to fix this? If so, LGTM. This can't be merged as is. it contains a whole bunch of other crap. . I'm not sure there's really much to specifically review. Did CI fail for any real reason? I assume not. . That is expected behavior. Use the -o (overwrite) flag if you want put-file to overwrite instead of append. . I think it was using the .md file instead of the .rst. This has been a constant headache, but maybe it'll work now. started a new docs build. \n\n\n@Nick-Harvey https://github.com/pachyderm/pachyderm/blob/master/doc/pachctl/pachctl.rst needs to be updated to include all pachctl cmds -- i think it's missing quite a few. Agreed that its not worth it for right now. I do think that namespace is a super common use case for enterprise deployments and we might want a shorthand for it at some point. Thanks @setgree! Can you please sign our CLA:https://pachyderm.wufoo.com/forms/pachyderm-contributor-license-agreement/ and then we'll get this merged?\n@Nick-Harvey . I personally like having examples tested in CI, even if it's never all of them, just a few key ones. Do we know what our VM capacity limit is right now and are we going over it if we use another VM for examples? Is using this VM taking away any compute from the other CI runs?. @brokenjacobs Just wanted to check in here again. Have you been able to gather any more information about what may be going on?. @LaurentGoderre We'd be happy to have something like this.  We'd probably want it to look more like our other containers though and be a FROM scratch container. . Thanks for filing this @samhug and we'll make sure this is fixed. Generally, we recommend only running stable releases, which would be 1.8.2 (1.8.3 comes out today or tomorrow) as opposed to dev builds like the one you're using right now. \nChanging your version to 1.8.2 will get you around this for now. . @pappasilenus Unless you are connecting to the dash directly, I don't think you should be able to access the dash via localhost without explicit port-forward, so that's what that message is for. \nThere are so many permutations for connecting that IMO, even though port-forward is one of the least \"good\" ones, it's also one that is available in every setup and therefore I don't thin we need to immediately change anything here. \nMost of the other info contained in this issue is already captured in our other ingress and connecting to pachyderm thread, so should we close this?. This is definitely an easy clarification we can make. Thanks @discdiver !. Thanks @gabrielgrant This is really helpful and we can turn into docs. \nDo we eventually want to add anything about the following in this doc?\n- pach_config or kube_config file?\n- Using an ELB to create a connection endpoint for users?\n- Discuss support (or lackthereof) for namespaces and the nuances for each of these methods\n- Discuss complications, if any, that come from running multiple pachyderm clusters -- using kubectl set-context, etc.\nWe'll also add a mention that port-forward can be slow for data ingestion from you local machine. \n. #2684 also has useful info. thanks @workflow!. Assigning this to @pappasilenus who accidentally (my fault not coordinating well between team) was writing a doc that has significant overlap with this. JK, can you combine these docs (as I described in Slack) and we can probably close this PR and open a new one with the final Migrations, Upgrades, and Backups document.. closing this in favor of JK's upcoming PR -- he's already pulled in many sections from this doc. . Hi @iswaverly. \n\nDoes it means that Pachyderm store the data use S3, but provide POSIX as interface for pipeline component.\n\nYes that is correct. Pachyderm uses S3 to store the content-addressed data that you put into the system, but that data is exposed to your Pachyderm Pipelines simply as files in the local file system of the container. \nI'm going to close this issue, but if for any reason your question doesn't feel fully answered, feel free to reopen or just continue the conversation here. In addition, you can join our Public Slack Channel anytime to get answers like this from the Pachyderm team and other members of the community. . I was getting the error on my cluster yesterday, but have since restarted minikube it seems. I will try to repro\n. This is expected behavior. Branch is an optional argument and not specifying one creates the new commit with no parent and no branch, just a fresh lonely commit. Check out pachctl start-commit --help for details. LGTM. LGTM after CLA. from user: get-logs worked. . Thanks @brokenjacobs. That's a silly oversight on our part. @pappasilenus, please make these fixes and do a search elsewhere in our docs to make sure we don't have similar errors. Thanks for pointing this out @rschoenbeck. Adding docs label and we can make these adjustments. Always open to PRs for little stuff like this as well :). You can pass an optional branch argument flag to list-commit to filter only by branch. e.g. pachctl list-commit edges master will only return commits on the master branch and not the stats branch. \nIt doesn't look like flush-commit offers similar options including a branch field could still be useful. Does it make sense for filter to be a reduce? I feel like this is clearly a map job. I also think the average use case is someone using their own container, not bash. Can we make the image and cmd something more demonstrative? The quickstart guide already has the bash shell example covered. \n. Change \"top-level object\" to \"file\". I don't feel 100% confident that \"file\" is the best option here, but TLO is definitely too long. Open to other suggestions.\n. These should be singular\n. Let's get rid of \"incremental-reduce\" as an alias. At that point we might as well be calling it incremental-file and that's not an alias anymore. Only advanced users who really understand what they're doing will be using this so it's fine to make them specify it in full. \n. What happens if Partition unit is specified, but incrementality is not (and vice versa)? We could have Inc default to true if block and  false for file and repo. \nOr we could error if they set one explicitly and not the other? \n. We need more details on pfs/prev (if we have it at all).  Additional details about when /prev is available (only when incrementality=true?) and more indepth about the \"sum\" example would help. \n. Yeah I feel the same way, but @jdoliner and I talked about this a bit and we can't think of any other options at the moment. \n. I agree. It also means we don't have to have any confusing default logic.\n. We discussed it briefly. @jdoliner can chime in if he disagrees\n. Remove \"3\". Also, add a bunch of description describing how freaking cool this is. This model didn't know english, or names or the difference between dialogue and expositions, but it clearly figure'd it out most of the time. \n. remove vision section for now\n. remove\n. should be: \"...passing two flags, -c and -i. -c means... \n. -i lets you specify a line-delimited input file. Each line can be a URL to scrape or a local file to add. In our case, we've got a few image URLs that we'll add to Pachyderm. \n. Just add & to the actual command we list here so usees can copy/paste. \nOn a separate note, do we want to them to view the images with FUSE at all? I think opening up the local files in a browser is actually better. \n. On nvm. We do have them use the browser\n. I seem to recall this error message was confusing to a user at one point. Do we want to consider changing it (which of course would also include adjusting the docs?)\n. I'm sure it's sound, but can you explain the use case of fork-commit to me and how it's different from branching?\n. I think there was a reason we needed both an .rst and an .md file for pachctl, but I can't figure it out right now. Let me try to figure out why that was the case again before approving this\n. I agree that this example is a little convoluted, but I found a walkthrough like this kinda useful. is there a better way to explain this to a user? I dont feel strongly about this though\n. I thought @jdoliner made it so that you can only do this with URLS for security reasons. Or did I misunderstand that PR?\n. Does it make sense to have the full pipeline spec here? That adds yet another place where we have to keep docs in sync every time the pipeline spec is changed. I'm not sure what the best answer is here, thought @jdoliner \n. typo. \"your\" not you. Would you mind explaining to me why we need these cmds?. I think we want to push users to always use branches because we dont support non-branch things very well yet. The command should be $ pachctl start-commit <repo> master or <branch>. Same for below commands instead of <commit-id>. Is <path> not descriptive enough? We've used  or something like that in the past. We could also just put  here and that might be clearer.. @jdoliner also added functionality for pfs:// so you can from another pfs repo . you can also pass multiple -f flags. worth mentioning.. Also, it'd be good to mentioned that when you pass a directory/bucket, the directory structure is maintained. in this case <path> certainly makes more sense. do you need -r if you're putting an s3 bucket? We should check these for consistency. Might make sense to move the -r flag part further up is it's required to make s3 bucket put-files work properly. Is the scala client not fully functional yet? Neither it or python are v1.4 ready, so we should mention that perhaps and create an issue for it?. should be ...\"results in a corresponding output commit in the \"edges\" data repository.\".... <commit-id> or <branch>? or just leave as commit-id?. dont need /, right?. awkward wording: \"we have one output commit per one input commit on our one input repo\"... output is renamed to egress in v1.4 pipeline spec. We should mention version tagging your images (e.g. dont use :latest) somewhere here. Should also go under best-practices once we create that doc.. archive doesnt exist in v1.4. Instead we just create a new commit-tree for the updated pipeline. We don't actually have a great name for this concept yet. @gabrielgrant and I have been just referring to them as commit-trees or pipeline versions (which is technically in the API, but isn't something we use elsewhere much afaik). . we don't have archive=false anymore. Instead you can pass a from-commit in the \"inputs\" field of the new pipeline spec and it'll only run the pipeline from that commit onward. @derekchiang how is parentage handled with update pipeline and from commits? Is it always a new commit tree? Also, what would be the new equivalent to archive=false (aka: only process future commits, not even the current HEAD [branch])? . Also, I dont actually think from-commit has been implemented yet, right @derekchiang? So perhaps right now it'll just always update everything. That'll have to be a high priority v1.4.1 issue IMO.. Dan and I will add the links in another PR. We need to do a lot of linking once the structure is set.. agreed. introduce fewer concepts. . The user needs to clone this repo or at least pull the files in this example, right? This tutorial assumes they've done that? Maybe it's silly to remind people of that at the beginning of every tutorial.. similar to above. Do we need to tell them to \"download the training data locally\"? Perhaps not. Can we give some indication on how long this should take to complete? minutes, hours? We should give users some very rough estimate.. In pipeline.jpg, there are parallel inference workers. Is something that neon gives us but we don't use pachyderm parallelism?. fix[Pachyderm docs(http://docs.pachyderm.io/en/v1.4.0-rc4/deployment/deploy_intro.html) link? point to 1.4.0-rc4. indication of how long the pipeline should take to run -- especially if its on the longer end. i know our guide doesnt actually have them run the data, but maybe at the end we give it to them for guidance. . why is this being flagged as invalid json? just a GH bug?. Do we want to replace with guide with the beginner tutorial at some point? Not worth it now I guess and I like that we have them build the docker image, but the rest is better in the tutorial IMO since that was recently rewritten. We can earmark this for later. I think you mean cancelled and archived where removed? not closed. Are open and closed actually the status's we use or is it started and finished? Either is fine for this doc I think. . add link to pipeline_spec#input-glob-pattern since this such a major piece of what people will want to read more on next. . link to deployment doc. . removed \"pods\". I think \"spun up\" is ok. Would you prefer \"started\"?. ok. reversed these paragraphs. changed \"sharding\" to \"spreading\" and added internal links. I'm on the fence, but I'm fine with that change. The only reason I can think of for keeping it as ## is for discoverability because this is asked about a ton. . This first sentence is a mouthful. I had to read it 3 times to parse it -- although I am a bit tired. Simplify this sentence. What's you're trying to say is: training data can be updated and your model code will automatically create a new persisted model output with that new data.\n. \"flowing\" is a weird word to use here. It's just \"what input data was used to train the model\". I guess you mean flowing to be the streaming data input, but in that case just say \"live streaming data\" or something like that. Is there not a cleaner name for that data? We have training sets, what is the live data set called?. Minor nit pick -- almost every sentence in this section has started with a prepositional phrase: \"in addition\", \" regardless of framework\", \"at any time\", \"more over\", \"more specifically\". It'll read a bit smoother if we change up the sentence structure a little and simplify the sentences IMO.  . incorrect plural? \"this machine learning workflows\".. Also, check your tenses slightly. I think \"could\" should be \"can\", but my verbal SAT was only OK so don't hold me to that. We just wanted consistent tense and pronouns (you can do blah vs. we can do blah) throughout our tutorial docs. I'm mess this up all the time so just correct with when you notice, but no need to go back and review stuff. . I assume the plan is to add joins here soon, just haven't gotten there yet. . This naming here doesn't seem right. We should discuss how we want #2 and #3 to be named to make the distinction clear. Honestly, they're just kind of two different methods for doing rolling time windows. So maybe they should just be in the same category but we describe both methods?\nAlso worth noting, \"JD's\" method doesnt actually work well until we get the client-side hash checking so perhaps this cookbook shouldn't contain that section at all yet?. This example is actually the core use case and it's has nothing to do \"two time scales\". Time windows are all about collecting data at interval A, but wanted to get aggregates over window B. The most common time window being collecting one file per day, but wanting to keep month totals building up. \nTake out the first two examples in code blocks here. having one file per month is not a good use case for pachyderm because that would be an append-based workload and we dont want that. The 2nd example is also just kind of weak because it implies that you're just adding one new file (e.g. online_sales and in_store_sales) for each month and that's not the point of time windowing. \nHappy to discuss live if I'm not describing this clearly.. Notice that this cookbook isn't breaking things down into map-style and reduce style steps. I'm not saying we should use those terms specifically, but the point of windows is that you need to reorganize your data from step 1 to step 2 into order to arrange it correctly.. rename to \"last_three_days\" for clarity. Also, make the example a little clearer by saying something along the lines of, \"When our Jan 4th file, 1-04-17.csv comes in, we'd pull out the last 3 days of data and arrange it like so:\". Other tiny nit picks on formatting of files.\n\nDoes it make sense to have a / before or after directory names such as sales/? Idk what the standards around this are, just a thought. \nShould we use .json instead of .csv for these examples. I know is super minor difference, but dealing with csv headers is a bit annoying in Pachyderm still (you need it as a separate file or something) so maybe we don;t want to give a wrong impression and we can make our examples have json example files.. I think this can be parallelized just fine. You can have workers in parallel read in files (or file names only with the lazy flag) and only write out to the moving_window directory ones that match whatever they're looking for.\n\nWe also need to make a note here. Somewhere, the user needs to define what day is \"now\" and using time.now() is absolutely the wrong way to do it. We perhaps should discuss and think about this more so we can make sure this cookbook examples is actually correct. . Should be \"...windows, we'll use a two-pipeline [DAG] (LINK to wiki or whatever best docs we have on dags such as pachyderm.io/pps.html)` to analyze and this efficiently.\". add issue number and link?. This whole section is very solid. My only thought is whether a graphic would be helpful here. \n\nI'm not convinced one way or the other and I'm not good at designing graphics, but wanted to add some food for thought.. I think this warning should be a bit clearer and even contain a concrete example of an obvious problem just because users could be silly and really mess this up. \n\"For example, you absolutely should not use a function such as time.now() to figure out what day it is. The actual time at which this analysis is run may be way later. Similarly, if you wanted to rerun a job on old data, you'd need to have a consistent way of specifying in your code and data that \"this job is looking for files that are from 1/3 to 1/5\". . It's worth making a note about why we split up the two steps. Step 1 can be completely parallelized with the datum as individual files. Every worker can just look at the file name and just either write it to the bin if it matches or do nothing if it doesn't. This can be blazingly fast if you also use the lazy field in the pipeline spec (link).\nI just think it's worth mentioning because you could do it all in one step, but that wouldn't scale. . If fractions are actually valid, can you add an example?. Do we want to add here something along the lines of \"A killed job is effectively the same as a failed job, but is manually terminated by a user.\" \nI'm not sure where the appropriate place to add these types of documentation details are, but --help seems pretty reasonable. . If you find yourself writing a pipeline that does a lot of copying, such as [Time Windowing] (http://docs.pachyderm.io/en/latest/cookbook/time_windows.html), it probably falls into this category.. sure easy to fix. . As funny as \"due to technical reasons\" is, just reword to:\n*Note: For v1.4 pipelines that specify environment variables, you will unfortunately need to reprocess the data for those pipelines as part of the v1.5 migration. This will automatically happen as part of the first job that spawns after the migration. Sorry for inconvenience. . Derek's original wording is also incorrect. It should be \"if you use the environment variables field in the pipelines spec...\"\nI don't think it hurts in a migration doc to acknowledge to the users that \"we know this is annoying/sucks\" -- but I dont feel strongly. Even though the note below talks about sim-links, it might make sense to just re-mention that there will be no data duplication by copying/reorganzing this data. Remove the \"currently\" caveat. Instead of \"most...wrapped up in dashboard\" --> \"The primary interface for the Enterprise Edition is the Pachyderm dashboard.\" No \"thus\" needed after that. . missing \"you\". Add note that you can use \"local\" even if you're on cloud deployment. . We have an open issue to have pachctl port-forward kill other port-forwards and then reconnect. It might be easy to ping the eng team to have that added cuz it's a really easy change. This would be a very nice convenience change. https://github.com/pachyderm/pachyderm/issues/1980. typo. miswording: \"Each of these deploys are further details\". \"secure and simple, and it delivers..\" -- too many \"and\". stats should be on this list. \"Detailed job statistics for faster development and data insight\". remove \"or dashboard\". the tone of this paragraph is different than the previous. it's all \"you can even\" or \"you can also\". Make it a littler more enterprise-y. Change the you's to Data Scientists or cluster admins, etc. . mounting --> mounted. \"For example, you can see how much time your jobs spend downloading/uploading data, what data was mounting in workers for processing and what that data looked like when it was mounted, portions of data that were not processed, and much more\" --> List too long. \nperhaps --> \"For example, you can see how much time your jobs spend downloading/uploading data, what data was processed or skipped, and which workers were given particular datums.\". remove \"in\". do we want the & to background this process again? . should be \"to reuse result\". I still find this progress notation weird because my mind does order of operations and see 3 plus 2/5. Not that this doc will change this, but perhaps above just be clearer that \"Below is an example of\n a job that has 5 datums total. It processed 3 datums and skipped 2.\". what is the \"N\" here? Typo?. dashboard should be deployed here too. Stats is basically not useful without the dashboard and we want to push people to that as the primary product interface. . The dashboard can be running, but you can still active via the CLI, right?. this is redundant with the above. It's true you dont need the dashboard deployed to activate , but you might as well have it and it shortens this guide. typo. \"collection\". Link somewhere here to the \"datum\" docs. Many people looking at this will still be new to Pachyderm and won't understand datums yet. . awkward wording \"persisted stats are persisted minimal in size in comparison\". \nThis statement also isn't completely true if you datum size is small. Stats can end up being a meaningful amount of data. The part about pachyderm deduping everything is definitely true and great to keep mentioned here.\n@derekchiang In a scenario where a user wants to reclaim space, how easy would it be for them to do delete-commit on the stats branch after successful datums and only keep around failed ones? This is probably something we'll need to implement automatically later if users actually want it. . Are the troubleshooting docs not actually helpful wrt to the tutorial?. Do we want to break this into 2 sections: Logging in via the CLI, logging in via the Enterprise UI/Dashboard?. @msteffen Is there a clear hierarchy with auth settings? Owner>writer>reader? If so, might makes to mention that here @dwhitena \nI ask because if I do pachctl auth set JoeyZwicker writer myrepo, I assume that means I'm also a reader? Or are those separate things?. Change \"blank slate\" or define it -- everyone will have access to all repos. \nIf Auth is deactivated, that also removes all notions of users from the system, right? So there isn't even a way to log in anymore, etc? \nLastly, it says that \"all ACLs are deleted\", but this is a really big deal (especially since ever ACL has to be manually added in our current MVP auth release) so perhaps break into a bullet list or something that emphasizes it even stronger.. we're on rc4? does this need to be manually updated for every release? I thought we had this managed automatically?. @gabrielgrant might be able to comment as well. Need a create-repo step in here?. instead of making users cd, you could also just name the file (pc put-file repo master file-name -c -f file). Whichever you think is easier for users to follow. all the cd so you dont get directories in pfs seems odd. yeah more cd. see previous comment. . Broad question: Do we want to include the UI or any screenshots in examples like this? Especially this one since many bioinformatics users are potential enterprise customers AND they often like the UI interface. \nIt still makes sense for all the commands to be through the CLI I think, but perhaps for the get/list commands above, you can also add screenshots of the UI in addition? What do you think?. I think I mentioned this in person too, but I think the best solution here may to go with just emphasizing which options are better for which setups instead of changing the order. . We should add pachctl as a prereq. Users often forget that. . Is there a 1-2 sentence conclusion we can make here?\nFor example, \"if we look at the final output of the select stage, we can see the modelXYZ using these parameter was actually the best model.\"\nI would also add a sentence or two on generalizing this. e.g.T\"he DAG pattern we've built here is very easy to generalize for any sort of exploration of a parameter space. As long as you break up your parameters into individual files as we've shown, you can test the whole parameter space in a massively distributed way and simply pick out the best results.\". I see in a lot of your pipeline specs, that you actually have a decent amount of code in stdin. Is this the correct usage pattern we want to encourage users to do? or should some of this logic (such as the if-statement below) be in the container directly or somewhere else? \nI'm asking more for my own education purposes, but it seems a bit odd to me once you start getting more logic contained in the pipeline spec over the code itself. Overall, it's mostly just setting up directories and naming, so maybe that's fine. One of the variables set above (line 85) is AWS_AVAILABILITY_ZONE, do we actually use this anywhere in our docs? I don't see if used anywhere. \n. typo \"your\". typo succeeded. typo: \"your\". can't actually delete any commit in the DAG. Only delete things at the top. . \"child\" isn't a term we really use in general?  Can that be changed to something more meaningful?. Not sure if this is quite right, but something like file at \\\"%s\\\" is a regular-file, but the \\\"%s\\\" already exists would be even clearer. what does regular-file mean or imply? Is there another type of file? If not, just \"file\" is probably description enough, no?. I'll rephrase, what is \"child\" in this context? I don't know what that's referring to. It's totally possible for this, and my other comments, that I'm just not familiar with normal file system terms, but figured I'd ask. oh that \"the\" was a typo by me. Sorry.. That explains it. I was just trying to understand. basically \"node\" was meaning \"file or directory\" and now you're making it explicit with directory and regular-file. That seems fine. I've never referred to a dir as a file, but I understand why those are sort of interchangable in terms of paths. Keeping regular-file is fine -- i just wanted to understand it better. We could always just write out \"file or directory\" -- I'm not a perfect example audience in every case about which terms are meaningful to me. @dwhitena probably is the best judge. . Do we need to use commitIDs for some reason here or can we just use branch name master . Do you want to set some context for what GATK is and why is useful?. I dont think this DS store file should be there. if I remember correctly, DS_store is some weird mac figment that it seems like got pushed into the repo along with the data. I dont think it'll break anything. Or maybe I'm totally wrong and that's a real part of the demo. typo. Don't use \"I\". This are docs, be prescriptive. . same as above.. Do we need EOF on all of these?. Are you going to add images for these?. I dont understand what this sentence is saying. Can you simplify the language and make it clearer?. Is all of this below just PG auto stuff of things the user needs to run. I know nothing about PG so wasnt sure what a user should be expecting to see here. Can you add more comments here to explain what's really going on?  I'm quite familiar with put-file syntax and I'm still confused. -c flag is deprecated. Does this work with the new non -c semantics or do they need to use start/finish commit?. I assume the pachctl put-header --help docs are in the other PR. I'm going to review that one piece as well. I'm lost trying to follow along with this whole example frankly. Please add more comments or break it the code block with text that describes the arguments and pachctl commend being made, what it's doing, and what they're seeing in the result. The alice/bob example was simple enough, but this needs more explanation.. It should be mentioned up front that there are put-header and put-footer commands. . This idea that the header is embedded into the dir, such that you can get-file on the dir and get back the header is very subtle. It need to be explained explicitly. Especially since list-file doesn't show that you have a header embedded.\nAlso, please add an example of deleting a header/footer. there isn't a delete-header command so that should also be explicitly shown in an example here. Out of curiosity, how does header/footers work if you have nested dirs? Does it apply to files recursively or break if you try to get-file on a dir with or without a header embedded?. Would it make sense to have a header/footer field in list-file on a dir that is a boolean to tell you if the dir has something embedded?. Make sure the pachctl docs are also updated (including the TOC tree) to include put-header and that this section links to those docs.. Do we just magically detect .pgdump files or is there some flag that user sets to specify this? Either way, let's call that out explicitly and make sure the limitations are clear (e.g. only .pgdump files, but we'll be adding more formats later?). -c deprecated. You don't mention --split sql until directly in the code block. So this went from too much cluttered info with all the SQL headers/footers to too little. Don't assume someone is going to read the CSV splitting docs above and then apply that logic to the sql splitting docs section. These sections should basically be standalone and walk through the same steps:\n\nusing --split\nget-file without header\nadd header (or maybe its automatic when you use --split sql? It's unclear to me so that indicates the docs are missing something)\nwalk through example. I get that we dont want to print the whole PG header 4 times because that clutters, but you can cull that smartly and still have a complete step-by-step walkthrough. . Typo. Should be \"connect to those outside services\". typo: \"processing might be retried\"?. How does this work if there are multiple files in pfs/foo? . How does processing being retried make it hard to reason about? Perhaps explain with simple example?. Did you decide not to offer the entire job-info? That's fine, just curious.. Had a user ask if input commit could also be an Env Var. Of course, he know he can get it from the output commit + prov, but thought having it directly would be more convenient. . Minor typo on my part: should be \"staging\". minor typo: \"to\". In the Next Steps section of this doc, we tell users they can access the dashboard. But for that to work, they need to manually port-forward, right? If so, we should add a sentence mentioning that in this section. \n\nSimilarly, we should add a mention of manual port-forward in our enterprise activation section: http://docs.pachyderm.io/en/latest/enterprise/deployment.html#activate-via-the-dashboard.\nside quest: the \"this guide\" link seems broken. Might as well fix at the same time (or just remove link if not needed). Can we have -n also work for --namespace like k8s does?. Make a note that's something like the below in this section and then we can merge these docs. \nNote: ADDRESS was renamed to PACHD_ADDRESS in 1.8.3. If you are using an older version of Pachyderm, use the ADDRESS env var instead. . Just removing from the index without removing the actual file? that\u2019s probably fine for now although that does mean it\u2019s googleable long term and we\u2019ll want to fix that. Perhaps add a note at the top that says \u201cmigration docs have been migrated to a new location: LINK\u201d. Considering how long this document is, we should at a table of context with some basic anchor links to below sections. I think even having a depth-2 ToC would be good:\n- Backup procedures\n    - 1...\n    - 2...\nMigration Procedures\n    - 1...\n. Numbering messed up?. Numbering?. Misspelled Pachyderm. Link to cross/union in in other docs that introduce the concepts. Keep in mind that this document is something that a pachyderm beginner may likely get point towards so there are many Pachyderm terms they might not familiar with already. . Output repositories dont merge data. This should read:\nPipelines must merge output data from what may be multiple containers running the same pipeline code, at the same time.. run-on sentence. \nThis document will take you through some of the advanced details, best practices, and conventions for:\n\nLoading data into Pachyderm (anchor link) (to learn the basics, see \"getting data into pachyderm-- link)\nUsing glob patterns in pipeline to consume data (anchor link)\nHaving pipeline write output data (anchor link). My wording is cludgy (mind is still mushy from travel), so reword as needed, but you get the gist. folder --> directory IMO. no comma between command and above. may want to mention this will be bad not just because doubled, but also because there will be intermediate header row. . This is great. Highlight this point somehow? Bold? \"TLDR\", etc?. mention that dedup happens at the per-file level as it's added to pachyderm. We dont do intra-file deduplication because is built to work with any file type.. I think this opening section needs to contextualize the problem better, not just rely on the section title. Why does split even exist? It's also missing a few details. Here's my draft of the wording.\n\nSplitting large files in Pachyderm\nUnlike a system like Git which expects 95+% of your files to be text, Pachyderm does not do intra-file diffing because we work with any file types -- text, JSON, images, binary, etc. Pachyderm diffs content at the per-file level. Therefore, if one bit in content of a file changes, Pachyderm sees that as a new file. Similarly, Pachyderm can only distribute computation at the file-level so if your data is only one large file, it can only be processed by a single worker. \nBecause of these reasons, it's pretty common to break up large files into smaller chunks. For simple data types Pachyderm provides the --split flag to put-file to automatically do this for you. For more complex splitting patterns (e.g. avro or other binary formats), you'll need to manually split your data either at ingest or with a Pachyderm pipeline. \nSplit and target-file flags\nFor common file types such as JSON, text or CSV, Pachyderm includes the --split, --target-file-bytes and --target-file-datums flags to deal with those files.\n--split  will divide those files into chunks based on what a \"record\" is. In line-delimited files, it's a line.  In json files, it's an object. Pachyderm also support splitting SQL data (specifically pgdump). --split takes one argument: line, json, sql.  This argument tells Pachyderm how you want to file delimited. For example, if you use --split line, Pachyderm will only divide your file on newline boundaries, never in the middle of a line. Along with the --split flag it's common to use additional \"target\" flags to get better control over the details of the split.. Add a sentence:\nEach split-file will automatically be given a 16-character filename, left-padded with 0. Split does not allow you to define more sophisticated file names right now, but this is a feature we plan to add in the future. . Not sure if this is accurate or if we actually support this right now. lets chat with eng. . Upon reviewing this doc, I think the lifecycle of a datum and everything below here might make sense as a separate doc. Append.overwrite and split is one doc -- then we have the distributed computing doc with is like Datums 101 and this doc is like Datum 201. . >  using pfs to assign datums to containers and merge results in the pipeline's output repo.\nIs this accurate @jdoliner? Or is the assigning/merging stuff in pps's purview. . Don't use a -i example here. IMO it complicates things to talk about having a txt file that has you put other files. Use put-file -r to add local files in a directory into pfs -- you can still put them at the top level.. Why are you starting from scratch here completely defining datums from the very basic ideas? our Distributed computing doc I think already does this? I figured this \"life cycle of a datum\" section would be like datums \"201\" (as opposed to 101) and focus on the more complicated ideas around datum in --> datum out and how that interacts with overwrite versus adding new one. Much of this opening section should just be a link to \"if you'd like to learn about the very basics of datums go to LINK. Then come back to this section to learn more about the intricacies of planning pipeline and using datums.\". This section needs some diagrams. I'm visualizing three graphics, one-to-one (e.g. 10 datums in to 10 datums out such as Edges), many-to-one (e.g. 10 datums in to 1 datum out such as Montage), and many-to-many (10 datums in to N datums out such as word count). . This probably deserves some addition wording around each of those examples as well. I think highlighting those 3 cases are the crux of the datum lifecycle. . change the file names/number to be 1 thru 6 (e.g. 1,2,3 in repo A and 4,5,6 in repo B) instead of 1,2,3 in both repos. That prevents the confusion in this section of the comments #from A, etc. . I see why you kept the files named the same above in union for this cross example. IMO, I still think different numbers files is better. One way to unify all of this might be to make your repos called \"foo\" and \"bar\" and then your files be called A, B, C in foo and a,b,c in bar or something like that. I don't feel strongly about this, but it's just a thought.. Can we highlight this note better? RTD offers a notation like\n::NOTE\nor something like that which will hight this. Maybe just bold or something could work too if you prefer. Why did the type start being italicized here and for the rest of the doc?. Or did the font change?. link to split section above?. > Not as long as each input file--tts.txt and md.txt--is being treated as a separate datum.\nI know what you're going for, but this wording is not totally accurate. Technically if your glob pattern was / meaning md.txt and tts.txt were both in the same datum, you would still get the same output result. The only difference now is that tts.txt will get reprocessed and technically you might not have an order guarantee -- e.g. it -> 10\\n5\\n could maybe also be it -> 5\\n10\\n. I'm not saying you need to explain the nuance I'm saying above to the user, just that your statement that having tts and md as separate datums isn't the reason the data isn't getting overwritten. You phrasing implies that if I use a glob of / then I'd somehow lose the tts output. that's not accurate, you'd just reprocess it. Anyway, i'm not sure exactly what I'd want this to say differently, but just trying to be as accurate as possible.. I'm feeling like there could be a good graphic here, but I'm not a graphic designer. Graphic like this could be used throughout this section IMO.\n\n. My terrible sketching....\n\n\n\n. These can effectively be summarized as:\nYour output datums should always reflect the state of processing all the input datums in your HEAD commit, independent of whether those input datums were added in separate commits or all added at once.. These two docs are really good. I feel like the one concept that isn't fully woven together is the idea of combining them. That requires a \"cookbook\" example (as a separate doc to come later) where you have a CSV that is --split with overwrite and perhaps doing time-windowing or aggregations similar to one our big finance customers' use cases and conversations in the user channel. Basically we want to take a real-world example and ties these \"fundamentals\" together. Just maing a note of this here to add to our burning questions doc. fair point. I'm ok with that as long as it doesn't go so far as you'll lose the reader with things they already know. . ",
    "KrishnaPG": "Looks interesting. It would be great if compatibility is kept between git's http access syntax and the proposed file access syntax. For example, git's way of accessing the ReadMe.md at master and some old version is\nhttps://github.com/pachyderm/pfs/blob/master/README.md\n    https://github.com/pachyderm/pfs/blob/90fcb0f0d8ab233daf2449c1125c5e6f7bee0d2d/README.md\nEssentially it is following the <root>/<branch>/<filename> pattern.\nSince the syntax we select for the access API is going to effect the performance and other factors it may be better to evaluate all the options w.r.to some of the below factors.\nFew important factors to consider for evaluation are:\n-    subscribers or listeners for any modifications at any level of the file system tree.\n-    security / access control\n-    Throttling / Rate limiting\n-    Caching / invalidating\nPlease add more if I missed something.\nOn the other hand, Most of these above are common for all file-systems since old ages - what is the need of hour and expected desperately from new file systems is:\n-    ability to join and remove additional file systems on the fly (e.g. files mapped on smart phones' SD card being able to join and remove as user walks across wifi zones). The key is: being able to serve files from the near-by smart phone's SD card on wifi, rather than going through central server from remote location  (similar to bit-torrent), and fall back to central server when the phone is not available.\nIt would be great if new systems such as pfs can at least take one step towards that direction, if not full solve them.\n. Thanks @jdoliner Will look into the referred issues (such as https://github.com/pachyderm/pfs/issues/58) to get more info on this. Will get back in case of any questions. \n. ",
    "neojski": "Apparently, even the error isn't logged. I wanted to play with pachyderm so I ran\njson\n {\n   \"type\": \"map\",\n   \"input\": \"identity\",\n   \"image\": \"ubuntu\",\n   \"command\" : [\"/bin/echo\"]\n }\nHoping to see some output.  But log only shows me:\nmapreduce.go:478: Running job:\nidentity\nmapreduce.go:493: Job: {map identity ubuntu [/bin/echo] 0 0 0}\nmapreduce.go:46: spinupContainer ubuntu [/bin/echo]\nand nothing more. There's no result anywhere and no error.\n. I think it'd be useful to have some blocking call that would stream the logs to my local machine. I'd like to know when a job is finished. I would like to have timestamps for logged messages.\nHow is running the job on multiple machines implemented? I think, depending on that the logging could be also complicated.\n. It'd be also very useful to add to How do I hack on pfs? section. In particular, I imagine it's possible to git clone pachyderm in some special container, modify go code, recompile and restart server and keep using curl for communication.\nRight now I just tried cloning this repo, docker build . and then modifying launch script with the just built image. Highly inefficient.\n. Ok. Please let me know when it's fixed.\n. There's a Dockerfile in both map and reduce. I'll try to test it later.\n. I updated my map-reduce and now it calculated the total length of all files in data directory.\n. Feel free to reference it :-)\nIt's resolved but please mention next branch on \"hacking pfs\" section.\nThanks for fixing this.\n. I forgot to add that I have btrfs-progs v3.19 on my machine. So the above didn't pass because my thing was too new.\nIt wasn't very difficult to get it on archlinux but I understand that this could be annoying if you have to take care of that kernel dependency on all of your dockerized machines. Thanks for explaining that.\n. ",
    "versae": "One question I have, is it working with privately-run Docker registries. If so, why not having Pachyderm to run their own registry at least in one node? I see a registry.service in the 1Node, but not sure how to use it.\nAlso, that raises more questions, like should the registry work in high availability mode in the cluster? If not, maybe a Paxos-based approach might work. Or maybe by pulling the docker files from git could remove the need for the docker registry?\nSorry if what I'am saying is nonsense, the project just looks really amazing and I got excited :)\n. Just can't express my feelings with words\n\n. ",
    "philips": "I agree with your overview. The send/receive thing is something that is left to userspace so you will have to figure out some way to address the contents of lower filesystems (hash or something) to ensure you are talking about the same thing across RPC boundaries.\noverlayfs should work fine with a commit workload of minute granularity although I haven't measured it myself. Essentially you would want to set some reasonable policy after measuring of when to flatten the commits again.\nOf course the other option would be to just continue using btrfs even on systems that don't have a btrfs root filesystem by using a loopback block device.\n. ",
    "sjezewski": "We are now using FUSE in lieu of btrfs, which suits our needs for now\n. This is a great ask, but is stale. As we see more requests for this, we'll re-evaluate. Closed / folded into Core Features List\n. We're considering a different discovery scheme w a refactor to our pfs module. So - closing this issue.\n. Is this still relevant @jdoliner ?\n. Right now there is an error, but not as descriptive as what we see from the fuse debug logs:\nshell\n$ echo 'hai' > foo\n2016-06-15T23:13:00Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"tensor_ptb\"},\"id\":\"3ab30fdc57c248ab863235671434ce2d\"}},\"shard\":{\"fileModulus\":\"1\",\"blockModulus\":\"1\"},\"modified\":\"2016-06-15T17:46:01.487673847Z\"},\"name\":\"foo\",\"err\":\"no such file or directory\"}\n2016-06-15T23:13:00Z ERROR fuse.DirectoryCreate {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"tensor_ptb\"},\"id\":\"3ab30fdc57c248ab863235671434ce2d\"}},\"shard\":{\"fileModulus\":\"1\",\"blockModulus\":\"1\"},\"modified\":\"2016-06-15T17:46:01.487673847Z\"},\"error\":\"commit tensor_ptb/3ab30fdc57c248ab863235671434ce2d has already been finished\"}\n-bash: foo: Input/output error\n. Yea, could use some cleanup\nshell\n$ pachctl create-repo foo\n$ pachctl create-repo foo\nrepo foo exists\n$ pachctl create-pipeline -f examples/fruit_stand/pipeline.json\nError from CreatePipeline: rpc error: code = 2 desc = Repo data not found\n$ pachctl create-repo data\n$ pachctl create-pipeline -f examples/fruit_stand/pipeline.json\n$ pachctl create-pipeline -f examples/fruit_stand/pipeline.json\nError from CreatePipeline: rpc error: code = 2 desc = repo filter exists\n. Fix is merged\n. I think we have this support?\n``` shell\npachyderm$ ls -alh ~/pfs/tensor_ptb/\ntotal 0\ndr-xr-xr-x 1 root wheel 0 Jun 15 10:46 3ab30fdc57c248ab863235671434ce2d\npachyderm$ du -h ~/pfs/tensor_ptb/3ab30fdc57c248ab863235671434ce2d\n17M /Users/sjezewski/pfs/tensor_ptb/3ab30fdc57c248ab863235671434ce2d\n```\nRe-open if this issue is asking for something else\n. Not able to reproduce this behavior.\nI setup a local firewall using pf and while I validated that I was blocking all outgoing traffic (curl didn't work / intitializing portforwarding didn't work), I still couldn't get it to hang. I think we must have added a timeout on the context already. Here's the error I see:\n```\n$ pachctl version\nCOMPONENT           VERSION                                        \npachctl             1.0.1-766b799e20ce22f1cf63e4e6ee2c36a1816d27b6 \npachd               (version unknown) : error connecting to pachd server at address (0.0.0.0:30650): context deadline exceeded\nplease make sure pachd is up (kubectl get all) and portforwarding is enabled\n```\nI'm closing this issue. If we have a user exhibit different results behind a firewall, we'll debug from there.\n. Is this still relevant @jdoliner ?\n. We did remove this in the refactor\n. @derekchiang I think you just saw (and perhaps resolved) this -- tar was modifying permissions in something you were working on\n. Is this a different issue from #203 ?\n. This is merged, and we ask users to use 1.2+ specifically in the setup instructions.\n. We have a good amount of this covered now on the live site:\nhttp://pachyderm.io/pfs.html\nhttp://pachyderm.io/pps.html\nFor the next level of granularity we'll be figuring out where to host/put technical docs. Since we've covered most of what's described here, I'll close this issue so that any new doc issues can be specific.\n. We've added some benchmarks to master now. We will open new issues w specifics on new things we want to benchmarks.\n. @jdoliner @derekchiang -- is this still relevant after your recent changes to the parallelism flags?\n. Also -- I think this only happens for OSX fuse\n. Ok, learned some interesting things. What I did:\n- instrumented fuse (in Read / ReadDirAll methods)\n- wrote two versions of the test w the following steps (TestCachingViaWalk and TestCachingViaShell):\n  - mount fuse\n  - create a repo\n  - look at files on the mount\n  - create another repo\n  - look at files on the mount\n  - expect to see both files\n- the two versions of the test differ in how they view the files\n- when I use filepath.Walk, it succeeds every time\n- when I use 'ls' it succeeds sometimes and fails sometimes\nDoing this 'manually' via pachctl differs a tiny bit in that we create the first repo before we mount. It could be that doing it this way produces a more consistent failure, but for now we have an interesting failure.\nWhen testing this on linux, both tests consistently pass. Only on mac does the TestCachingViaShell test fail intermittently.\nI guess not surprisingly, in the failing case, we never see a ListRepo / read request the second time we try to view the mount. What's unclear is what conditions mac os x decides to cache / not. \nPlaying w touching files doesn't invalidate the cache.\nRespecting the entryValidTime of 1m (waiting 1m+ before the second ls) also has no positive effect.\nI've attached a tar of the mac logs for these tests.\n@tv42 Can you take a look and help us understand this more? \nWe originally expected to never see the read requests come in, but the fact that this is intermittent gives us some hope that we can understand/exploit the conditions when os x caches.\nAnd of course, if there's more instrumentation you'd like, just let me know. Take a look at the 'issue_205' branch to see exactly where I instrumented for the attached logs.\n. mac-test-logs.tar.zip\n. Turned on debug level logging, and uploading the failing logs. We can see that after creating the second repo we never get a request to ReadDirAll\ncache_shell_failure.log.txt\n. TV has a potential workaround (that I've verified works) but he's trying to understand the impact of the change.\nWe might have to opt in to use fuse in an 'unsafe' mode to enable this behavior specifically on mac.\n. Fixed by: https://github.com/pachyderm/pachyderm/pull/343\n. Ok, diving in on this. Making a formula is relatively straightforward once you tag the GH repo w proper versions (GH calls them 'releases'). \nThe trick is making sure the version we tag is also reported by the binary. Industry standard here is to have the 'micro' version be the build number. So we'll do that. To do so, we'll have to read from an environment variable -- no way around this since the alternative is to modify the code / re-commit. This is fine (I've also seen it done by passing around a VERSIONS text file that contains the full version that the binary just reads and spits out).\nSo the important part of this task will be establishing versioning. We should also take this opportunity to do the same for pachd. So to be clear, the interface would be:\n```\ncloned & compiled locally:\npachctl version\n<= 1.0.0\ninstalled via homebrew:\npachctl version\n<= 1.0.345\ncloned & pachd generated from local files:\npachd version\n<= 1.0.0\npachd deployed via dockerhub:\npachd version\n<= 1.0.345\n```\nFor the sake of clarity I presented the interface this way -- I know I'm oversimplifying the interface (pachctl reports both versions).\nVersioning this way will enable users to write very easy bug reports & more importantly for us to easily track down the exact commit in question.\n. While we don't explicitly support ceph, we now support azure storage, GCS, S3, and anything that minio can interface with (glusterFS, etc). So I'm closing this issue for now. If you still need ceph support, let's open a separate issue for that.. The integration run command has changed, and the connection error is expressed in a different ticket. Closing.\n. WRT 1) -- while its AWS specific, it would be interesting to see if cloudfront (their CDN) improves performance. Since the obj's are immutable, that could provide us an easy win. Especially if this is combined with #223 and the client is directly requesting the data. (Then we really gain from the distributed edge caching of cloudfront)\n(FYI - Even within the same aws region, downloading from a bucket in the same region, I've only seen ~4MB/s). The first one is now represented by this issue: https://github.com/pachyderm/pachyderm/issues/257\nThe second one is described in several issues that are talking about egress pipelines / services / persistent services.. That first link is now broken (k8s cares not about links working). \nWhat is the desired outcome?\n\nIf its monitoring, we now have a make task to use the k8s monitoring output and aggregate/present/graph it\nIf its auto-scaling / healing, I think that depends on the cpu/mem requests, and that has been merged upstream\n\nSo I think this issue can be closed? @jdoliner \n. I think the ask here is adding app stats for prometheus to consume. This is absolutely something we want. This came up again recently.. A user made a plugin that scrapes pachyderm to populate prometheus. We'd want to add application stats as well, but this is a start:\nhttps://github.com/button/pachyderm_exporter. Woah cool - I love the tests simplification!\nThe rest looks good too.\n. This issue has been open for over a year. I'm closing.\nFYI - We still do want to add more docs about our processing model. . Ah, good catch. Thanks for the fix Jon!\n. duplicate of https://github.com/pachyderm/pachyderm/issues/252. Hey Erik!\nYup thats exactly whats happening. In short, removing it from the server vendor directory should work O.K. but is not ideal - I'd like to figure out a better solution here.\nI've run across a similar error myself. We've recently been working hard to make vendorization work for us and our users. It's challenging. Please make sure you're using the latest code as this is something we've been working the kinks out of.\nThe src/server/vendor folder exists to allow us to lock versions of the dependencies we need to build and run pachd.\nBut any code that is only using the pachyderm client (anything under src/client/...) should not require the server vendorization. \nBasically -- we did a refactor to enable our users to vendorize pachyderm client.\nIn fact - I've been playing w some sample apps to make sure that we allow our users to only import the client and build successfully. You can look at github.com/pachyderm/sandbox to see how I'm using pachyderm/pachyderm as a 'user'. In that app, I'm vendoring as well, and I'm actually vendoring golang.org/x/net/trace under my sandbox. So - Its not yet clear to me why I'm able to vendor it under an app but you're getting the conflict.\nIt would help me to know some sanity checks:\n- your version of golang\n- any value of GO15VENDOREXPERIMENT in your env\nBut beyond that, it would help if I could see pull_data.go and the repo that contains it. Otherwise, its possible that my version of pachyderm that I've vendored under sandbox is out of date, and this is an issue w the latest code. I can look into that as well.\n. Yea, I expect client to import shards which in turn imports other things. We chose to leave that import there because that avenue of code requires fuse and that logic should stay in server. But locally it seemed to be working.\n(I wasn't familiar w gopkg.in, but love the idea, I can't wait for golang to adopt some mature versioning tools. In our case though, I think what would be required is each dependency of ours to be versioned in the way gopkg.in requires, which isn't really under our control. Also - its unclear to me what happens in the import paths using gopkg.in --- if you can only specify the version during go get, I'm not sure how we'd make it enforceable ... but I'll do more reading on gopkg.in ... thanks for that lead)\nI'll play w your code this morning to see what I learn.\n. I can reproduce the error locally. Makes sense.\nWhen I vendorize under the app (see my code ) the import conflict goes away. Also makes sense.\nWhat I didn't realize is that our method of vendorization basically requires our users to vendorize as well. We want to enable our users to vendorize, but I don't really like the idea of making it a hard requirement. \nI'm thinking about a few ways we could try to keep our vendorization for server (which I do think is a requirement for us) while not requiring vendorization for our users. I'm going to play w one of those ideas, but I'm not super confident there is a way to do both. In which case, we'll have a harder design decision to make here.\n. Ok ... I do have a working version locally that allows me to vendor the server and provide the pachyderm repo without those files so that a new user won't have the vendored server files and so won't have the vendorization requirement.\nBut ... this has some consequences for our internal development process. I'll discuss w the team before we choose an approach here. Will provide more updates once we make a decision.\nThis has been amazingly helpful! Thanks for the bug report.\n. Turns out it was easier (and far more elegant) to eliminate all imports to src/server\nFor some reason I thought that the shard import path took a bunch of server components with it, but turns out it was manageable.\n. Should be all set if you pull master now ... it'll play nice vendoring or not :-)\n. This can be accomplished today w glob patterns. E.g. if at my root I have a bunch of images, I can specify a glob like 11*.tif to get a consistent subset.. I believe we can support this workflow w update-pipeline --reprocess and use of the --from-commit flag. This seems stale, so I'm closing. If there are more details / new requests for this we can re-open.. I like this idea a lot.\nI'm happy to do it, though I think any of us could tackle it\n\nOn Mar 30, 2016, at 5:33 PM, Derek Chiang (Enchi Jiang) notifications@github.com wrote:\nThis container should do exactly what make proto does today. The advantage of using a container is that the user doesn't have to install the protoc tool chain, or risk using the wrong version of the tool chain (which seems to be what's happening in my case).\nSo make proto simply becomes:\ndocker run pachyderm/proto .\n@sjezewski are you the best person to do this?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\n. In our discussion yesterday, we agreed that this issue should just cover the simple fix (JD describes). If we want more advanced updating mechanisms, we'll make a new issue.\n. We kind of have this as a result of the 'no-delimiter' support via PutFile() --- if you're putting binary data you implicitly also set the block size. \n\nBut I assume you want this set independently of delimiter type.\n. Haven't heard a request for this in a long time. Now that we have files as our finest granularity over blocks I think this is no longer meaningful. In addition to fixing that one call we should add some test coverage to make sure we have this fixed more generally.\n. Per discussion w JD:\n- he's going to update the handle coalescing serialization (to allow reads during commit I think?)\n- I'm going to write a negative control test case (or as close as I can) to show that collated PutFile writes would produce bad output ... and the current branch fixes this problem\n. I think @jdoliner implemented this as a command, so I think we can close this.. This post suggests there's some tooling for this:\nhttp://stackoverflow.com/questions/27097005/how-to-package-go-project-for-homebrew\nBut the link moved, and I only see the explicit list of resources :-/\n. Aha, here it is:\nhttps://github.com/samertm/homebrew-go-resources\n. Dupe of https://github.com/pachyderm/pachyderm/issues/598. Awesome!\n. Thanks for the feedback!\nRight now there is a bug w the deployment. Just running the normal setup causes a loop. I'll be looking into the sandbox again next week. For now, we're heads down on some other bugs.\n. This is defunct.. Comments on internal issue. All done!\n. Hmmm I've seen that pattern in several places before. I think I see it most when the server errors. If it recovers, those errors will go away once the client reconnects. Otherwise, if its thrashing, you'll continue to see them.\n. We have some make helpers for this ( make deps) that should do the fetching properly. We probably should just make that part of the make install job.\nThe other issue is making this 'go get'able ... have to think about this a bit ... since we did the client reorg I'm not sure the top level will be go gettable, but I think go get github.com/pachyderm/pachyderm/src/client would work. We should think a bit about what 'installation vector' we expect people to be using, then update the readme accordingly. \n. We now have a deploy script (leaning heavily on kops) for AWS that can be run to stand everything up. Not a 'button', but as close as we're going to get from the terminal. Stale.. Rethink is dead! etcd gives us multi-document transactions! \n(\u256f\u00b0\u25a1\u00b0\uff09\u256f\ufe35 \u253b\u2501\u253b. This is no longer valid because of the refactor, so just make sure that the test is not being skipped.\n. Validated test is not skipped on pfs-refactor branch\n. Bandaid for now -- exposed our bash helpers under contributing/bash_helpers\n. Huh, I guess it didn't occur to me that its not easy to run docker natively on mac, but that makes sense.\nAnother idea would be to have a dedicated kube cluster setup to basically 'build proto files As A Service'. That may be a bit silly ... but if we could automate this, I think it would ultimately help us.\nThat interface would be something like:\n- local task zips up current code into tarball  ... posts it to image\n- image accepts tarball, unpacks it, and runs proto compilation steps (probably just using whats already specified in our makefile)\n- then the service sends a tarball back\n- and locally unpack the new proto files\n. Looking at this now / making a failing test case (TestFileOverwrite). Logs for osx/linux attached - these are the lines just when trying to overwrite the file.\nIn each case we see a:\nshell\nWriteRequest: Goodbye, world\n@0\nBut I think osx always reports writes @ 0 -- so we may not have a way of differentiating an append vs overwrite. Will add a negative test case for append next.\nlinux-test-failure.log.txt\nmacosx-test-failure.log.txt\n. This issue has a lot in common with #355 \nThere are some big design decisions in here. We've been discussing this a lot. I'll summarize our thoughts below. Our discussion centered around the following questions:\n1) Do the Unix semantics make sense? Do they make sense in the world of parallelism?\n2) Are we enabling common use cases (e.g. new user / tries to write a file) ?\n3) What is a user trying to accomplish?\n\n1) Do the unix semantics make sense?\nKind of.\nFor a single node it makes enough sense. But what about for many nodes? Let's say we have 10 nodes. Then what would you expect this output to be?\nshell\necho 'hi' > foo\nYou probably want it to:\na) overwrite the existing foo (basically delete the file)\nb) and append 'hi' once per node\nc) so that 'foo' contains 10 'hi's \nBut that's not really what the > operator normally expresses. So even here, we're not really overwriting, we're deleting and appending.\n2) Are we enabling common use cases?\nMaybe. Maybe not.\nIf we support the difference between > and >>, we need to do so by instrumenting the FUSE callbacks. This would be done by watching for some FUSE callbacks. Unclear if this is possible. If there is a unique SetAttr we may be able to make this work. If not, we'd have to watch for a WriteRequest@0, in which case doing normal new user things like writing a new file (which would also trigger a WriteRequest@0) in a job that you run more than once would have the side effect of overwriting (not appending). This is surprising. \n3) What is a user trying to accomplish with this feature?\nConsidering the parallel use case above and using the > operator ... it became clear that the user probably wanted to delete the file ... and then append some more things. So, we'd prefer to be explicit about this desired behavior.\n\nSo, we're proposing the following. Instead of:\nshell\necho 'hi' > foo\nWe'd expect a user to do:\nshell\nrm foo\necho 'hi' > foo # OR\necho 'hi' >> foo\nAnd if a user did:\nshell\necho 'hi' > foo\nIt would work the same as it does today --- simply append to the file.\nThe idea here is that removing / overwriting a file is probably a less common case. And deleting a file is something you should be explicit about (especially since this will often involve deleting a large section of data).\n. Yes, those changes make sense to me. While the difference between > and >> could be surprising ... it makes sense in the parallel case.\nYou make a good point about the order of operations of remove. That is surprising. While deleting is probably a rare use case, overwriting is probably not? We'll have to make sure this is clear in the docs (filed under deleting and overwriting).\n. This has been merged:\nhttps://github.com/pachyderm/pachyderm/pull/366\n. This has been fixed by the changes in issue #270. The fuse tests (all server tests) are now running succeeded on travis\nClosing since this is now redundant\n. Turns out we are already explicit about kubectl version in SETUP.md but we should probably mention it before the one-liner (the first thing you see on readme.md) ?\n. And we need to update how we run / mount FUSE. I'm doing it w sudo, but our docs don't use sudo. Again, see #327 \n. Writing this down so we don't forget --- we also discussed going a 'Pachyderm Design Principles' or goals page, where we'd discuss:\n- Reproducibility\n- Incrementality\n- Collaboration\n- Data Provenance\n. We should also add coverage for the places that we support exponential backoff. Specifically in PutBlock and DeleteBlock\n. We have a recent need for testing of this nature:\nhttps://github.com/pachyderm/pachyderm/issues/2675\nThis will be a critical path for a customer's deployment.. Looks good to me. I like that we're using a consistent error object instead of strings everywhere. \nBased on what Derek described, the logic changes make sense, so I think we're good to merge. \nThe only remaining open question is why some nodes don't know about all repos. Derek says this happens only after some load. @derekchiang will open a separate issue for this specifically.\n. Hard to reproduce\n. Since we don't have a way to reproduce, closing after the pfs-refactor\n. Oh, and this fixes #208 \n. Based on JD's feedback, changed the format of the version string. Now the output looks like:\nshell\n$ pachctl version\nCOMPONENT           VERSION             \npachctl             1.0.0(530)          \npachd               1.0.0(538)\nSo its Major.Minor.Micro(Build), and we'll increment the Micro as needed for any patches.\n. ;-)\n. I used JD's new make proto task ... it worked like a charm!\nNow just have pedge's library that needs updating:\nshell\n../../../go.pedge.io/proto/version/protoversion.pb.go:98: cannot use _API_GetVersion_Handler (type func(interface {}, context.Context, func(interface {}) error) (interface {}, error)) as type grpc.methodHandler in field value\nSo I'll make a PR for him\n. To be clear -- until this is resolved, this blocks travisCI for all of us.\nSo if we don't get a PR merged soon from pedge, we'll just fork and patch\n. Made a PR:\nhttps://github.com/peter-edge/proto-go/pull/2\n. Fair question.\nI haven't seen an exact need for this yet, but I'm sure we will run across one soon. Pretty standard faire to ask your logs to tell you the place the log originated from. \nIf we want to wait until we're debugging an issue that is intermittent enough to happen on a single pod at a time, we can do that.\n. Oh and to answer your question of difficulty - I don't think its hard - assuming that k8s exposes an index somehow within a pod that represents its pod number among its parallel peers.\n. We now have the ability to stand up a logging service. In v1.4 we also add the metadata we can (jobid, etc). Thanks for the detailed notes!\nA few questions / comments so far:\n1) That url for the manifest we did update recently, but needed to double check. I can reproduce that error and will fix that bug\n2) That error message (and warning) when you use the local manifest is new. Can you run kubectl version and gcloud version and share the output with me? \n3) FUSE mounting \nSo did you do an apt-get for FUSE ? Or did you already have it installed? Again, a version number would be helpful. (I think there was a recent change that requires the config flag you mention).\nYou can see how we handle setting that flag for our CI or heres another post that has some suggestions on how to handle setting and using fuse.conf.\nAs far as permissions go for mounting/unmounting fuse ... I do think that I run sudo -E ... when running those commands. Perhaps I'm running it wrong, or perhaps we need to update the documentation.\n. Looking into the manifest from the URL, it should be identical to the local copy:\nwww$ diff manifest.json ../pachyderm/etc/kube/pachyderm.json\n(Returns nothing)\nBut we definitely added another top level object to that file. I wonder if the difference is in how its served / how kubectl is treating it based on mime type?\n. gcloud is not explicitly a dependency, but happens to be how I install / upgrade kubectl\n. Looking at your versions, it does seem like you have an older version of kubectl (the client version). So I would recommend updating that before you retry building.\n. Ah interesting, updating my version of kubectl client helped w the first error.\nThis is what it was before:\nshell\nClient Version: version.Info{Major:\"1\", Minor:\"1\", GitVersion:\"v1.1.7\", GitCommit:\"e4e6878293a339e4087dae684647c9e53f1cf9f0\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.0\", GitCommit:\"5cb86ee022267586db386f62781338b0483733b3\", GitTreeState:\"clean\"}\nAnd after updating (via gcloud components update kubectl)\nshell\nClient Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.2\", GitCommit:\"528f879e7d3790ea4287687ef0ab3f2a01cc2718\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.0\", GitCommit:\"5cb86ee022267586db386f62781338b0483733b3\", GitTreeState:\"clean\"}\nAnd now running the kubectl command using the URL works just fine. I'll update the docs to specify a new minimum version\n. Ok I think the remaining issues here are just documentation related (which I also updated in #309). I'll leave this issue open though until\na) dwhitena validates its working for him, and \nb) when the documentation branch lands\n. I think the only remaining todo is to see if mkdir causes a consistent fuse crash and how we handle that error. But I'll make that into a separate issue.\n. This is redundant w #364, closing\n. Totes.\nGot 'em on the phone right now and I'm reading our FAQ to them line by line to see what's causing the error.\nThey're based in Germany so this is a long distance call (3$/minute). Don't worry, I'll expense it.\n. Cool. Much better\n. No one but me has run into this. Closing.. K8s has support in theory. Need to test this though. Required for talk at Red Hat Summit\n. Based on repro steps, the following seems like expected behavior to me.\nshell\npachyderm$ pachctl mount\nmkdir /pfs: permission denied\npachyderm$ sudo mkdir /pfs\npachyderm$ pachctl mount\n2016/04/28 11:25:55 mount helper error: mount_osxfusefs: failed to mount /pfs@/dev/osxfuse13: Operation not permitted\nmount_osxfusefs: exit status 71\npachyderm$ sudo chown sjezewski /pfs\npachyderm$ pachctl mount\n(The final mount succeeds)\n. We agree this looks normal\n. Good suggestion. That would also help cleanup some confusion around this.\nRight now you can specify a mount point w the -p flag, but instead I'll modify the command to:\n- require a positional argument - which specifies the directory to mount in\n- the command will not try to create the directory\nAnd this will change our quickstart docs to read:\nshell\nmkdir $HOME/pfs\npachctl mount $HOME/pfs &\n. Ok, output now adheres to the behavior described above:\n``` shell\npachyderm$ pachctl mount\nUsage:\n  pachctl mount path/to/mount/point [flags]\nFlags:\n  -n, --block-modulus int   modulus of block shard (default 1)\n  -b, --block-shard int     block shard to read\n  -m, --file-modulus int    modulus of file shard (default 1)\n  -s, --file-shard int      file shard to read\n  -h, --help                help for mount\npachyderm$ pachctl mount zzz\nmountpoint does not exist: /Users/sjezewski/pachyderm/workspace/src/github.com/pachyderm/pachyderm/zzz\npachyderm$ mkdir $HOME/pfs\npachyderm$ pachctl mount $HOME/pfs &\n[1] 78914\npachyderm$ ls $HOME/pfs\nTestDuplicatedJob.data.5050ee298cce  TestPipelineWithTooMuchParallelism.data.b2017fe69db0  job-a79ec30149114052a60b23b7cec42da5  pipeline.47085d404ca3\nTestDuplicatedJob.pipeline.3c2d98f037d8  TestSharding.85addb2fe79d                 job-b5f8f72cfb7344f7b959714a30183663  pipeline.885a74e027f2\nTestFromCommit.be25ce2d149c      TestSimple.4d89cc2b6466                   job-b6f7846b5e594e9386dede69fcb4d3c7  pipeline.9fe896cefcf7\nTestGrep.data.c976dda71afb       iyvwfktyhl                        job-d6969a241cff413d98aa9a53831123f4  pipeline.cc1111eea3ac\nTestJob.data.964ab05706d8        job-5672c1f33eb246f0abbbe85bd92ace8c              job-dedd469b4b0243aeb6500dd4f09873b0\nTestJob.data.f642ab10118f        job-6bdb3379dd5c4b78a0e70e9670a87c9b              pipeline.0ad7135da904\nTestPipeline.data.49b70803f6bc       job-71a8a6aa82114df68bdadf4c14226501              pipeline.0bb21bcbcb56\n```\n. On travis, its getting compile time errors w files missing ... but a fresh local clone has those ... odd\n. Merged in master / rebuilding\n. Popped up on travis too:\nhttps://travis-ci.org/pachyderm/pachyderm/builds/125140723\n. stale. This is really old, and I don't think we have intermittent timeout/failures w the FUSE test suite anymore.. I can't reproduce this issue. Some googling suggests that it may be a credentials issue?\nIf we see this popup again, I'd like to see your kubeconfig file\n. Yea I'll review now\n. The only thing I see missing is a test. \nDo we test the pachctl CLI interface anywhere? We probably have some test coverage of those utils somewhere else? As long as there's a test that covers the usage that we feel good about, LGTM\n. Doesn't apply to 1.4 processing model. I like this plan. Additionally, when we start including versions in the manifest, I'll update the release instructions for pachd to include updating the manifest\n. Yup that should work. We're also going to make detecting/reporting this a lot clearer to the user -- see issue #358\n. Also related to #390 \n. Fixed in https://github.com/pachyderm/pachyderm/pull/510\n. Fixed in https://github.com/pachyderm/pachyderm/pull/510\n. What do you use docker-build-fruitstand for? \nOther than that, changes LGTM\n. Got it - I see the new image specified in pipeline.json\nLGTM then\n. Hi Erik - sorry you ran into this.\nFrom my understanding this is not expected behavior. We want to enable composing pipelines together in many ways, and performing a reduce should not prohibit performing any maps downstream.\nWe'll look into this.\n. Updated the title to reflect the action needed. Was formerly 'Map job performed on a repo produced by a reduce cannot be parallelized'\n. This is pending the fix needed in #385 \n. Now that the fuse fixes have landed, took another look at this. \nThe readme has changed enough ... particularly the order of the commands ... that I don't think this makes sense anymore. If we want to make sure we have coverage of the 'first five minutes', we can make an explicit test for that instead.\n. Since we're using pedge.io helpers for this, where do you want the solution to live?\nMake a PR for those helpers? Write our own?\n. Reminder for docs for new ENV variables for user job environments:\nhttps://github.com/pachyderm/pachyderm/pull/409\nBasically this, but w real names:\nshell\nPACH_COMMIT_OUTPUT_ID\nPACH_COMMIT_INPUT_REPO_NAME_1_ID\nPACH_COMMIT_INPUT_REPO_NAME_1_ID\n. That specific item (env vars) was added to the docs. The rest of these look like todos for the corp site. I assume this gets pushed to 1.2?\n. @msteffen and @JoeyZwicker are doing a docs overhaul, but the things listed here I'm pretty sure are not releveant. So I'm closing this issue\n. A few broad questions / comments:\n1) Will we be hosting the docs anywhere? It would be nice to do something like what TVs fuse library does: https://godoc.org/bazil.org/fuse. Probably out of scope for this PR, but something I think we should consider. Not a dealbreaker for this PR\n2) The generated docs have a lot of noise ... \nshell\n$ godoc ./src/client/pfs | grep func | wc -l\n268\nThere are a bunch of boilerplate functions that are generated from the proto API interfaces. For these docs to be helpful, I think we need to restructure the package a bit? Or maybe there's a way to flag certain files to be omitted from godoc?\nGodoc seems to sort the functions w descriptions towards the top, but there is a large table of contents to scroll past (again ... most of those types I think we want to omit). The command line version of the doc is a bit better ... (only 93 lines to scroll through and abridged ToC) ... but the html version requires a lot of scrolling.\nSo - my request for this PR is to see if there's an easy way to omit files from the output. That would make this documentation a TON clearer.\n. Also - In a lot of the descriptions you're breaking down the arguments which is great. In other doc formats there are specific formatting mechanisms for arguments. I'm not sure if godocs has those, but if they do we should use them\n. If omitting files is easy / after that this LGTM\n. Fixes #372 \n. Ok, LGTM !\n. LGTM\n. Ship it!\n. Yup, good call. Building now:\nhttps://travis-ci.org/pachyderm/pachyderm/builds/127673513\n. Great. What I'll do as part of the automation scripts is put that deb file on the corp site so we can make the one liner a bit nicer:\nshell\n$ curl -o /tmp/pachctl.deb -L https://pachyderm.io/pachctl.deb && dpkg -i /tmp/pachctl.deb\n. We're going to run w the one-liner solution for now. Renaming this ticket to reflect this.\nI'll open a new ticket for upgrading both brew/deb installations to use the official repositories.\n. we can do this with pachctl put-file repo branch -c -f file. Yes, definitely part of the spec will be considering the consequences for parallel operations. \nAnd writing it down will force us to be explicit about the workflows we support. \n. Some updates.\n\nThe Spec\nIs on the fuse-spec branch, and will be merged into the fix for this issue fix-mac-fuse before merging to master.\nGoals of the spec:\n1) Represent our state of the world (what actions we support/dont support/are undefined)\nTL;DR -- Expectations are specified within a test, but not quite at the syscall level\nThere are many future todos to improve the spec (those are listed below). But we needed to start somewhere. To guarantee that what was reported was actually the behavior we produce, I needed to define the actions at the granularity that we specify expectations. This means the actions are within a single test, but based on how the test is written, not necessarily a single syscall. But, our goal should be to write tests at the syscall level of granularity in the future.\n2) Be an easy reference\nThe spec is generated every time the fuse test package runs. The output is checked against the committed version and the tests fail if these don't match. This is to prevent the spec from getting out of sync w tests running on CI.\nThe actions are separated into 4 broad types of behavior:\n- Root Level Directory\n- Repo Level Directories\n- Data level:\n  - Open Commit\n  - Closed Commit\nAnd refer to different parts of the mount, and different states of the commit when accessing data level files/directories.\nThe reports live at src/server/pfs/fuse/spec/reports and exist for linux and mac, since there are differences. We should refer to these in the course of debugging any future fuse issues. If we find different views / combinations more helpful, we can update the output format.\nTo see the existing spec html docs, look here ... but to view the files you have to clone and open them locally.\n\nThe actual issue\n(fuse cache differences on os x)\nRemaining plan of attack:\n- create a few syscall level tests for new issue here\n  - they should fail initially\n  - test for write behavior\n  - and attr/readdir interface methods\n- then add piggyback logic for commit state on inspect\n  - test for this new feature\n- make tests pass\n  - make sure new tests are added to spec\n- add cache busting logic in new places (for Create, Mkdir, Symlink, Link, Mknod) ... (or at least createDir and mkDir)\n  - tests for each\n  - and make sure its hooked into the spec\n\nFuture Todos for the Spec:\n- Add test harness to run fuse tests on single mount (user mode) and in parallel (as used by PPS)\n  - the behavior for some of the actions may be surprising in different cases, and we want to make sure this is well tested\n- add 'user tests' spec which allows running commands like rm /data/foo && echo 'bar' > data/foo vs echo 'bar' > data/foo && rm /data/foo (which should have the same results)\n  - we'll find this very useful when fielding user reported issues\n- add syscall level tests\n  - we should get in the habit of writing tests this way\n  - but there is an argument to be made to test both ends --- e.g. the way a 'normal user' would write the code to read a file (an all in one read file helper) AND the syscall behaviors we expect under the hood\n. Update - \nThis got hairy, so we're opting for as simple fix as needed. We also decided to change some behavior around deletion (in the scraper branch). Once that lands, I'll retest this issue, but I think it will be fixed by those changes.\n. This tool has proved ineffective for organizing our todos. This mechanism for organization has proved ineffective. Closing.. Fixed in https://github.com/pachyderm/pachyderm/pull/510\n. LGTM\n. Hi there!\nSorry you ran into this. Yes - we need to either update the docs or provide that image on dockerhub as you suggest.\nIf you run make docker-build-fruitstand it'll build the image locally. However, if your cluster already has some repos created, it may be easiest to reset your cluster by doing the following:\nshell\nmake clean-launch\nmake clean-launch-kube\nmake launch-kube\nmake launch\n. Newer versions of k8s have support for LB w different cloud providers. We have an issue to document this further: https://github.com/pachyderm/pachyderm/issues/2684. Ok - made the changes per comments - uses pflag and manifest update is specified in assets.go (and regenerated w the make task to update pachyderm.json)\n. Fixes #406 \n. Good call @derekchiang  ... although I'm not sure exactly where to put this documentation. It feels like this might be a good instance of something to go on a dev docs site.\nOtherwise ... maybe go api docs ... on the createjob / createpipeline functions?\n. The existing link in the Dockerfile under etc/user-job/Dockerfile is:\nhttps://storage.googleapis.com/golang/go1.6.0.linux-amd64.tar.gz\nWhich provides an error for me. Is that what you see?\n. LGTM\nIf nothing else, we'll have more descriptive (and quicker) errors on CI to help us refine this further\n. This is stale. This is covered by the fix I did for #428 \nI have a test in place for ListCommits that reports the error you describe.\n. Hmmm I can't reproduce that error. Can you help me by providing a bit more information? I'd like to know:\n- the output of ls $GOPATH/src/github.com/spf13\n- the output of ls $GOPATH/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13\nThanks!\n. Another case we want to cover -- make sure handle and unsafe are optional in DeleteFile (and PutFile)\n. LGTM\nSome of the docs reflect things that I don't think are in master yet (e.g. provenance), but since this should address a user issue, we should run w it anyway\n. @derekchiang - where exactly do you expect the cleaner error message? I assume you mean a better error message for CLI users. (So this would correspond to the pachctl binary).\nWe probably still want the full grpc error even at the client level, because some of the grpc codes tell a client when its appropriate/not to retry.\n. Yea, I was using that function already in the CLI to only print the error description. Having a friendly message like that for users of the CLI makes sense. I'll be trying to standardize that too.\nBut grpc errors (in particular the codes) tell a client when its appropriate to retry and when its not. So we def want to use grpc errors where we can. \nI'm fine w scrubbing the error messages that our client returns. That makes sense to me. And that will be pretty easy to do across the board in the client code.\n. This fix has been merged\n. Looks good - except for inline comments (just that one test we might want to add), and:\n1) The fruit stand demo needs updating I think, right?\nSince we now treat redirection properly, I think the commands in the demo need updating, but that also means it needs to work on mac, and ...\n2) Running the tests locally (make test) on mac I do get a fuse failure:\nshell\n--- FAIL: Test296Appends (0.72s)\n    require.go:131: /Users/sjezewski/go/src/github.com/pachyderm/pachyderm/src/server/pfs/fuse/filesystem_test.go:309\n    require.go:133: Not equal: \"1\\n2\\n3\\n\" (expected)\n                != \"1\\n\" (actual)\nIt's likely a difference for osx fuse. Maybe there's a way to workaround this for the fruit stand demo? If we can't make osx behave consistently - then generally for an osx specific fuse issues, I've been duplicating the test out into separate targets for linux/mac so we test our expectations for each, whatever they may be. \n. BTW - this morning I applied the cache busting changes (we were playing w at the fuse layer before) to see if it would fix this issue - no dice.\n. The new changes LGTM except the above comment about the fruit stand demo / fuse tests passing on mac.\nLocally when I run the fuse tests I get the following log:\nshell\n$ CGOENABLED=0 GO15VENDOREXPERIMENT=1 go test -cover -v $(go list ./src/server/... | grep -v '/src/server/vendor/' | grep '/src/server/pfs/fuse')\n=== RUN   TestSeekRead\n2016-06-08T19:26:52Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.localBlockAPIServer\",\"method\":\"CreateDiff\",\"request\":\"diff:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e \\u003e \\u003e finished:\\u003cseconds:1465414012 nanos:657607015 \\u003e \",\"duration\":{\"nanos\":982097}}\n2016-06-08T19:26:52Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"CreateRepo\",\"request\":\"repo:\\u003cname:\\\"test\\\" \\u003e created:\\u003cseconds:1465414012 nanos:657607015 \\u003e \",\"duration\":{\"nanos\":7066647}}\n2016-06-08T19:26:52Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"CreateRepo\",\"request\":\"repo:\\u003cname:\\\"test\\\" \\u003e created:\\u003cseconds:1465414012 nanos:657607015 \\u003e \",\"duration\":{\"nanos\":28402014}}\n2016-06-08T19:26:52Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"StartCommit\",\"request\":\"repo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" started:\\u003cseconds:1465414012 nanos:704853209 \\u003e \",\"duration\":{\"nanos\":57396}}\n2016-06-08T19:26:52Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"StartCommit\",\"request\":\"repo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" started:\\u003cseconds:1465414012 nanos:704853209 \\u003e \",\"response\":\"repo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \",\"duration\":{\"nanos\":1760047}}\n2016-06-08T19:26:52Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"InspectRepo\",\"request\":\"repo:\\u003cname:\\\"test\\\" \\u003e \",\"response\":\"repo:\\u003cname:\\\"test\\\" \\u003e created:\\u003cseconds:1465414012 nanos:657607015 \\u003e \",\"duration\":{\"nanos\":11274}}\n2016-06-08T19:26:52Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"InspectRepo\",\"request\":\"repo:\\u003cname:\\\"test\\\" \\u003e \",\"response\":\"repo:\\u003cname:\\\"test\\\" \\u003e created:\\u003cseconds:1465414012 nanos:657607015 \\u003e \",\"duration\":{\"nanos\":1112560}}\n2016-06-08T19:26:52Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"InspectCommit\",\"request\":\"commit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e \\u003e \",\"response\":\"commit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e \\u003e commit_type:COMMIT_TYPE_READ finished:\\u003cseconds:1465414012 nanos:657607015 \\u003e \",\"duration\":{\"nanos\":13283}}\n2016-06-08T19:26:52Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"InspectCommit\",\"request\":\"commit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e \\u003e \",\"response\":\"commit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e \\u003e commit_type:COMMIT_TYPE_READ finished:\\u003cseconds:1465414012 nanos:657607015 \\u003e \",\"duration\":{\"nanos\":1329317}}\n2016-06-08T19:26:52Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"InspectCommit\",\"request\":\"commit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e \",\"response\":\"commit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e commit_type:COMMIT_TYPE_WRITE started:\\u003cseconds:1465414012 nanos:704853209 \\u003e \",\"duration\":{\"nanos\":102601}}\n2016-06-08T19:26:52Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"InspectCommit\",\"request\":\"commit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e \",\"response\":\"commit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e commit_type:COMMIT_TYPE_WRITE started:\\u003cseconds:1465414012 nanos:704853209 \\u003e \",\"duration\":{\"nanos\":11028960}}\n2016-06-08T19:26:52Z ERROR protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"InspectFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e unsafe:true handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"response\":\"\\u003cnil\\u003e\",\"error\":\"File file not found in repo test at commit 8538ae4685414abc86bb36784ec0642c\",\"duration\":{\"nanos\":46247}}\n2016-06-08T19:26:52Z ERROR protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"InspectFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e unsafe:true handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"response\":\"\\u003cnil\\u003e\",\"error\":\"rpc error: code = 2 desc = File file not found in repo test at commit 8538ae4685414abc86bb36784ec0642c\",\"duration\":{\"nanos\":1870930}}\n2016-06-08T19:26:52Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"test\"},\"id\":\"8538ae4685414abc86bb36784ec0642c\"}},\"write\":true},\"name\":\"file\",\"err\":\"no such file or directory\"}\n2016-06-08T19:26:52Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.localBlockAPIServer\",\"method\":\"PutBlock\",\"response\":\"block_ref:\\u003cblock:\\u003chash:\\\"z4PhNX7vuL3xVChQ1m2AB9Yg5AULVxXcg_SpIdNs6c5H0NE8XYXysP-DGNKHfuwvY7kxvUdBeoGlODJ6-SfaPg==\\\" \\u003e range:\\u003c\\u003e \\u003e \",\"duration\":{\"nanos\":435994}}\n2016-06-08T19:26:52Z NONE  append: 0xc8201c08c0 handles:<key:\"29d732760b2c446893cad8731c591ba5\" value:<block_ref:<block:<hash:\"z4PhNX7vuL3xVChQ1m2AB9Yg5AULVxXcg_SpIdNs6c5H0NE8XYXysP-DGNKHfuwvY7kxvUdBeoGlODJ6-SfaPg==\" > range:<> > > >\n2016-06-08T19:26:52Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"PutFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e file_type:FILE_TYPE_REGULAR handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"duration\":{\"nanos\":6689063}}\n2016-06-08T19:26:52Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"PutFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e file_type:FILE_TYPE_REGULAR handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"duration\":{\"nanos\":8035220}}\n2016-06-08T19:26:52Z NONE  inspect append: 0xc8201c08c0 handles:<key:\"29d732760b2c446893cad8731c591ba5\" value:<block_ref:<block:<hash:\"z4PhNX7vuL3xVChQ1m2AB9Yg5AULVxXcg_SpIdNs6c5H0NE8XYXysP-DGNKHfuwvY7kxvUdBeoGlODJ6-SfaPg==\" > range:<> > > >\n2016-06-08T19:26:52Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"InspectFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e unsafe:true handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"response\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e file_type:FILE_TYPE_REGULAR commit_modified:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e \",\"duration\":{\"nanos\":282387}}\n2016-06-08T19:26:52Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"InspectFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e unsafe:true handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"response\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e file_type:FILE_TYPE_REGULAR commit_modified:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e \",\"duration\":{\"nanos\":2636774}}\n2016-06-08T19:26:52Z ERROR protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"InspectFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"._file\\\" \\u003e unsafe:true handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"response\":\"\\u003cnil\\u003e\",\"error\":\"File ._file not found in repo test at commit 8538ae4685414abc86bb36784ec0642c\",\"duration\":{\"nanos\":38550}}\n2016-06-08T19:26:53Z ERROR protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"InspectFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"._file\\\" \\u003e unsafe:true handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"response\":\"\\u003cnil\\u003e\",\"error\":\"rpc error: code = 2 desc = File ._file not found in repo test at commit 8538ae4685414abc86bb36784ec0642c\",\"duration\":{\"nanos\":2108462}}\n2016-06-08T19:26:53Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"test\"},\"id\":\"8538ae4685414abc86bb36784ec0642c\"}},\"write\":true},\"name\":\"._file\",\"err\":\"no such file or directory\"}\n2016-06-08T19:26:53Z NONE  inspect append: 0xc8201c08c0 handles:<key:\"29d732760b2c446893cad8731c591ba5\" value:<block_ref:<block:<hash:\"z4PhNX7vuL3xVChQ1m2AB9Yg5AULVxXcg_SpIdNs6c5H0NE8XYXysP-DGNKHfuwvY7kxvUdBeoGlODJ6-SfaPg==\" > range:<> > > >\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"InspectFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e unsafe:true handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"response\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e file_type:FILE_TYPE_REGULAR commit_modified:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e \",\"duration\":{\"nanos\":137368}}\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"InspectFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e unsafe:true handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"response\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e file_type:FILE_TYPE_REGULAR commit_modified:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e \",\"duration\":{\"nanos\":1967810}}\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.localBlockAPIServer\",\"method\":\"PutBlock\",\"response\":\"block_ref:\\u003cblock:\\u003chash:\\\"yzd8ELD1piyANiWnmdnpCL5F52f10UfUdEkHywVZeqTt0ymgrxR63Qz0GB7TKPoeeZQmWCaz7T1-9vBnypkYWg==\\\" \\u003e range:\\u003cupper:9 \\u003e \\u003e \",\"duration\":{\"nanos\":2972981}}\n2016-06-08T19:26:53Z NONE  append: 0xc8201c08c0 handles:<key:\"29d732760b2c446893cad8731c591ba5\" value:<block_ref:<block:<hash:\"z4PhNX7vuL3xVChQ1m2AB9Yg5AULVxXcg_SpIdNs6c5H0NE8XYXysP-DGNKHfuwvY7kxvUdBeoGlODJ6-SfaPg==\" > range:<> > block_ref:<block:<hash:\"yzd8ELD1piyANiWnmdnpCL5F52f10UfUdEkHywVZeqTt0ymgrxR63Qz0GB7TKPoeeZQmWCaz7T1-9vBnypkYWg==\" > range:<upper:9 > > > >\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"PutFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e file_type:FILE_TYPE_REGULAR handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"duration\":{\"nanos\":5845724}}\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"PutFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e file_type:FILE_TYPE_REGULAR handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"duration\":{\"nanos\":7327460}}\n2016-06-08T19:26:53Z NONE  inspect append: 0xc8201c08c0 handles:<key:\"29d732760b2c446893cad8731c591ba5\" value:<block_ref:<block:<hash:\"z4PhNX7vuL3xVChQ1m2AB9Yg5AULVxXcg_SpIdNs6c5H0NE8XYXysP-DGNKHfuwvY7kxvUdBeoGlODJ6-SfaPg==\" > range:<> > block_ref:<block:<hash:\"yzd8ELD1piyANiWnmdnpCL5F52f10UfUdEkHywVZeqTt0ymgrxR63Qz0GB7TKPoeeZQmWCaz7T1-9vBnypkYWg==\" > range:<upper:9 > > > >\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"InspectFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e unsafe:true handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"response\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e file_type:FILE_TYPE_REGULAR size_bytes:9 commit_modified:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e \",\"duration\":{\"nanos\":267833}}\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"InspectFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e unsafe:true handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"response\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e file_type:FILE_TYPE_REGULAR size_bytes:9 commit_modified:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e \",\"duration\":{\"nanos\":10599723}}\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.localBlockAPIServer\",\"method\":\"CreateDiff\",\"request\":\"diff:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e \\u003e started:\\u003cseconds:1465414012 nanos:704853209 \\u003e finished:\\u003cseconds:1465414013 nanos:38739218 \\u003e appends:\\u003ckey:\\\".\\\" value:\\u003cchildren:\\u003ckey:\\\"file\\\" value:true \\u003e \\u003e \\u003e appends:\\u003ckey:\\\"file\\\" value:\\u003cblock_refs:\\u003cblock:\\u003chash:\\\"z4PhNX7vuL3xVChQ1m2AB9Yg5AULVxXcg_SpIdNs6c5H0NE8XYXysP-DGNKHfuwvY7kxvUdBeoGlODJ6-SfaPg==\\\" \\u003e range:\\u003c\\u003e \\u003e block_refs:\\u003cblock:\\u003chash:\\\"yzd8ELD1piyANiWnmdnpCL5F52f10UfUdEkHywVZeqTt0ymgrxR63Qz0GB7TKPoeeZQmWCaz7T1-9vBnypkYWg==\\\" \\u003e range:\\u003cupper:9 \\u003e \\u003e \\u003e \\u003e size_bytes:9 \",\"duration\":{\"nanos\":953832}}\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"FinishCommit\",\"request\":\"commit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e finished:\\u003cseconds:1465414013 nanos:38739218 \\u003e \",\"duration\":{\"nanos\":2974422}}\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"FinishCommit\",\"request\":\"commit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e finished:\\u003cseconds:1465414013 nanos:38739218 \\u003e \",\"duration\":{\"nanos\":5724124}}\n==== Finished commit\n2016-06-08T19:26:53Z NONE  inspect append: 0xc8201c08c0 block_refs:<block:<hash:\"z4PhNX7vuL3xVChQ1m2AB9Yg5AULVxXcg_SpIdNs6c5H0NE8XYXysP-DGNKHfuwvY7kxvUdBeoGlODJ6-SfaPg==\" > range:<> > block_refs:<block:<hash:\"yzd8ELD1piyANiWnmdnpCL5F52f10UfUdEkHywVZeqTt0ymgrxR63Qz0GB7TKPoeeZQmWCaz7T1-9vBnypkYWg==\" > range:<upper:9 > >\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"InspectFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e unsafe:true handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"response\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e file_type:FILE_TYPE_REGULAR size_bytes:9 modified:\\u003cseconds:1465414013 nanos:38739218 \\u003e commit_modified:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e \",\"duration\":{\"nanos\":168831}}\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"InspectFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e unsafe:true handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"response\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e file_type:FILE_TYPE_REGULAR size_bytes:9 modified:\\u003cseconds:1465414013 nanos:38739218 \\u003e commit_modified:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e \",\"duration\":{\"nanos\":2220743}}\n2016-06-08T19:26:53Z NONE  inspect append: 0xc8201c08c0 block_refs:<block:<hash:\"z4PhNX7vuL3xVChQ1m2AB9Yg5AULVxXcg_SpIdNs6c5H0NE8XYXysP-DGNKHfuwvY7kxvUdBeoGlODJ6-SfaPg==\" > range:<> > block_refs:<block:<hash:\"yzd8ELD1piyANiWnmdnpCL5F52f10UfUdEkHywVZeqTt0ymgrxR63Qz0GB7TKPoeeZQmWCaz7T1-9vBnypkYWg==\" > range:<upper:9 > >\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.localBlockAPIServer\",\"method\":\"GetBlock\",\"request\":\"block:\\u003chash:\\\"z4PhNX7vuL3xVChQ1m2AB9Yg5AULVxXcg_SpIdNs6c5H0NE8XYXysP-DGNKHfuwvY7kxvUdBeoGlODJ6-SfaPg==\\\" \\u003e size_bytes:3 \",\"duration\":{\"nanos\":61393}}\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.localBlockAPIServer\",\"method\":\"GetBlock\",\"request\":\"block:\\u003chash:\\\"yzd8ELD1piyANiWnmdnpCL5F52f10UfUdEkHywVZeqTt0ymgrxR63Qz0GB7TKPoeeZQmWCaz7T1-9vBnypkYWg==\\\" \\u003e size_bytes:3 \",\"duration\":{\"nanos\":112066}}\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"GetFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e size_bytes:3 unsafe:true handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"duration\":{\"nanos\":1784727}}\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"GetFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e size_bytes:3 unsafe:true handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"duration\":{\"nanos\":3322674}}\n==== 2016-06-08 12:26:53.057065205 -0700 PDT - Read word len 3 : foo\n==== 2016-06-08 12:26:53.057091591 -0700 PDT - err (<nil>)\n==== 2016-06-08 12:26:53.057098994 -0700 PDT - offset (6)\n==== Seeked to 6\n2016-06-08T19:26:53Z NONE  inspect append: 0xc8201c08c0 block_refs:<block:<hash:\"z4PhNX7vuL3xVChQ1m2AB9Yg5AULVxXcg_SpIdNs6c5H0NE8XYXysP-DGNKHfuwvY7kxvUdBeoGlODJ6-SfaPg==\" > range:<> > block_refs:<block:<hash:\"yzd8ELD1piyANiWnmdnpCL5F52f10UfUdEkHywVZeqTt0ymgrxR63Qz0GB7TKPoeeZQmWCaz7T1-9vBnypkYWg==\" > range:<upper:9 > >\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.localBlockAPIServer\",\"method\":\"GetBlock\",\"request\":\"block:\\u003chash:\\\"yzd8ELD1piyANiWnmdnpCL5F52f10UfUdEkHywVZeqTt0ymgrxR63Qz0GB7TKPoeeZQmWCaz7T1-9vBnypkYWg==\\\" \\u003e offset_bytes:6 size_bytes:3 \",\"duration\":{\"nanos\":150567}}\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"GetFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e offset_bytes:6 size_bytes:3 unsafe:true handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"duration\":{\"nanos\":1028970}}\n2016-06-08T19:26:53Z ERROR protorpclog.Call {\"service\":\"pachyderm.pfsserver.InternalAPI\",\"method\":\"InspectRepo\",\"request\":\"repo:\\u003cname:\\\"._.\\\" \\u003e \",\"response\":\"\\u003cnil\\u003e\",\"error\":\"Repo ._. not found\",\"duration\":{\"nanos\":12541}}\n2016-06-08T19:26:53Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"GetFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"test\\\" \\u003e id:\\\"8538ae4685414abc86bb36784ec0642c\\\" \\u003e path:\\\"file\\\" \\u003e offset_bytes:6 size_bytes:3 unsafe:true handle:\\\"29d732760b2c446893cad8731c591ba5\\\" \",\"duration\":{\"nanos\":2015313}}\n2016-06-08T19:26:53Z ERROR protorpclog.Call {\"service\":\"pachyderm.pfsserver.API\",\"method\":\"InspectRepo\",\"request\":\"repo:\\u003cname:\\\"._.\\\" \\u003e \",\"response\":\"\\u003cnil\\u003e\",\"error\":\"rpc error: code = 2 desc = Repo ._. not found\",\"duration\":{\"nanos\":1233561}}\n2016-06-08T19:26:53Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\"._.\",\"err\":\"Repo ._. not found\"}\n==== Read word len 3 : baz\n2016/06/08 12:26:54 grpc: Conn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp 127.0.0.1:54678: getsockopt: connection refused\"; Reconnecting to \"127.0.0.1:54678\"\n2016/06/08 12:26:55 grpc: Conn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp 127.0.0.1:54678: getsockopt: connection refused\"; Reconnecting to \"127.0.0.1:54678\"\n2016/06/08 12:26:58 grpc: Conn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp 127.0.0.1:54678: getsockopt: connection refused\"; Reconnecting to \"127.0.0.1:54678\"\n2016/06/08 12:27:02 grpc: Conn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp 127.0.0.1:54678: getsockopt: connection refused\"; Reconnecting to \"127.0.0.1:54678\"\n2016/06/08 12:27:09 grpc: Conn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp 127.0.0.1:54678: getsockopt: connection refused\"; Reconnecting to \"127.0.0.1:54678\"\n2016/06/08 12:27:20 grpc: Conn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp 127.0.0.1:54678: getsockopt: connection refused\"; Reconnecting to \"127.0.0.1:54678\"\n2016/06/08 12:27:34 grpc: Conn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp 127.0.0.1:54678: getsockopt: connection refused\"; Reconnecting to \"127.0.0.1:54678\"\n2016/06/08 12:27:57 grpc: Conn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp 127.0.0.1:54678: getsockopt: connection refused\"; Reconnecting to \"127.0.0.1:54678\"\n2016/06/08 12:28:43 grpc: Conn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp 127.0.0.1:54678: getsockopt: connection refused\"; Reconnecting to \"127.0.0.1:54678\"\n2016/06/08 12:29:43 grpc: Conn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp 127.0.0.1:54678: getsockopt: connection refused\"; Reconnecting to \"127.0.0.1:54678\n. Well ... it seems like its better than what's on master. Would like to see what @tv42 can say about it too\n\n. Now we have finer grain processing of data via 'datums', so it should get more evenly distributed in general.\nWhile a single datum could take much longer than others to process, it seems like 1.4 services this concern pretty well.\n@derekchiang - are there other todo's here?. Can you give an example of the output that this will produce? This is good for posterity and to be clear of your intent.\nIn particular, its unclear it you're running w @jdoliner's suggestion of keeping inspect-* commands return long output and list-* commands w a line-per-object output\n. Ok. LGTM\n. Fixes #429 \n. Also realized this fixed #416 \n. LGTM\n. Good ideas. Yea, we should do those things. That makes these non repeatable.\nAlso want to add -- I want to remove the debug lines from filesystem_test.go -- I was hoping that would help us catch other intermittent fuse errors ... but I don't think thats the case.\n. Great! Thanks!\nOk, so remaining todos are:\n- removing debug lines from testFuse() helper\n- updating the filesystem_seek_mac_test.go to compile\n. Hi there, sorry you're running into all these snags, but thanks for all the debugging diligence you're doing.\nThere seem to be two issues here:\n1) Your go environment variables aren't set\nNot sure why these are missing. It's standard practice to set these in your shell init script (e.g. ~/bash_profile). Are you setting them there?\nYou'll definitely need these set. You've identified that they're missing, which is great, but I want to make sure that these are always present, or you'll run into a lot of issues along the way. \n2) You're missing all the dependencies for compiling the pachyderm source\nWhat version of go are you using? (Please run go version and let us know what it reports). You should be using go 1.6\nYour errors around building suggest that the compiler isn't using the vendored copies of those dependencies from within our repo. You shouldn't have to manually grab each of them. The reason we vendor our dependencies is to lock them to specific versions that we know the pachyderm code is compatible with. Only in go 1.5/1.6 did support for vendoring dependencies arrive, so I want to make sure you're using the latest go, or this will just not compile.\n. Closing. Please re-open if you didn't get the installation working\n. @jdoliner - I think you mentioned you have a fix for this somewhere? If you're working on it, you should assign this to yourself\n. This fixes #410 \n. LGTM\n. I think @derekchiang may have found the issue?\nWe weren't closing our grpc connections (this happened when we did the fix for the change in grpc when grpc broke the build).\nHas the fix (closing grpc) connections landed in master yet?\n. Looks like it just landed\nI'll merge and re-run this failing test to see if this was the fix\n. And I'll just leave this here:\n\n. So playing w it locally -- doing 10k putfiles takes ~5min, so I halved that number in the test and made sure its parallel. Rebuilding now on CI.\n. But it does seem to work (and not run out of files)\n. So this will allow pachctl to support a pipeline manifest that doesn't specify a method (which will default to a map).\nLGTM\n. LGTM\n. Forgot my meme\n\n. I broke release tagging ... need to fix that before this is ready\n. Ok. Fixed tagging. PTAL\n. Ok made the changes @derekchiang PTAL\n. Ok, got that last uneeded comment\n. LGTM \n\n. @sambooo this would be a good one to tackle next\nI'll be revamping (maybe w more of your help) our release / automation process. The broad goal is to have our releases be completely automated. This is a part of that work. \nThere are a few steps to this, so I'll add some more details:\n1) Add make task for cross compilation\n    - make cross-compile should create a binary for the last two versions of os x\n    -  and whatever's reasonable for ubuntu\n2) Update the 'pachyderm/homebrew-tap` repo to use these binaries\n    - Right now the formula clones and builds pachyderm from source\n    - This has caused some bugs related to go versions ... and its slow\n    - Please update the formula to use pre-compiled binaries instead\nThose will be a PR on this repo and pachyderm/homebrew-tap respectively. Once we have each of those ready, we can hook these into #512 to automate all of this.\n. @sambooo's PR was merged, but I'm leaving this open until I have it fully hooked into the new automated release scripts\n. LGTM\n\n. Hi there -\nThat's helpful. FYI - when we refer to server version we're asking for the pachd version string reported from the first command you ran. That command reports the version of pachctl (the client) and pachd (the server).\nSince you're using a build of the client from source, its a bit hard to tell if its mismatched. Please try the following:\n- pull the latest pachyderm code (git pull origin master)\n- rebuild the client (make install)\n- retry making the pipeline\n- if that doesn't work, report the current commit you're on (git log | head -n 1)\n. Also, todo:\n- update the release instructions for the manifest (update local copy / then deploy hosted version)\n. +999999\n. I had to pre-release the pachd:v1.0.0-1397 so that the new manifest could refer to a valid image (that was also compatible w the new manifest).\nOnce this PR is merged, I'll finalize that incremental release by doing the pachctl part of the release.\n. OK - so we'd:\n- omit the last string altogether\n- and every time we release, we have to (at least) bump the point number\n  - if we automate the release process (see #512) this is easy to enforce\n. I think we're closing in on something much like semantic versioning. But for now, we're doing what makes sense timing wise when we release (not worrying too much about strict rules at the moment).\nWe'll see what our build/tagging system allows for this point release this week. That'll also inform how we move towards a standard.\n. Relies on #490 and #381 as well\n. Put the basic structure we want in place on the automate_release branch\n. Renamed to be more descriptive. We won't worry about baking into CI right now, but all of the release steps should be runnable by a single make task.\n. Oh, and about the Flush() question -\nI didn't write the flush test because while writing it, I got a failure (doing an os.Open and then f.Write() this was #521 - turns out I was just opening the file incorrectly.\nOnce that gets merged, we'll have that test.\n. This fix landed on master\n. Accounted for by refactor\n. For now we're relying on exit codes to denote success or failure.\nIf you do:\nshell\npachctl delete-repo something\necho $?\nAnd the result is 0, that means it succeeded. If its non-zero that means it erred.\n. Hi there -\nWhen you run pachctl version it reports the client version immediately, and requests the version information from the server. That error tells me that pachctl cannot connect to your pachd server. \nWe should make that error message more descriptive / helpful to the user. We can provide a link / steps to try next if they're having trouble connecting to their server.\nIn your case, please try the following:\n1) what is the output of kubectl get all ? Please report that here\n2) If you see something like this:\nshell\n$ kubectl get all\nNAME                   CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\nkubernetes             10.0.0.1     <none>        443/TCP    39d\nNAME                   READY        STATUS        RESTARTS   AGE\nk8s-etcd-127.0.0.1     1/1          Running       3          39d\nk8s-master-127.0.0.1   4/4          Running       14         39d\nk8s-proxy-127.0.0.1    1/1          Running       5          39d\nThat tells me that kubernetes is running, but pachyderm is not. In this case run make launch\n3) If you see pachd from the output of kubectl get all, e.g. like this:\nshell\n$ kubectl get all\nNAME                   DESIRED      CURRENT       AGE\netcd                   1            1             42s\npachd                  2            2             32s\nrethink                1            1             41s\nNAME                   CLUSTER-IP   EXTERNAL-IP   PORT(S)                        AGE\netcd                   10.0.0.124   <none>        2379/TCP,2380/TCP              42s\nkubernetes             10.0.0.1     <none>        443/TCP                        39d\npachd                  10.0.0.47    nodes         650/TCP                        33s\nrethink                10.0.0.224   nodes         8080/TCP,28015/TCP,29015/TCP   41s\nNAME                   READY        STATUS        RESTARTS                       AGE\netcd-f5vkb             1/1          Running       0                              42s\nk8s-etcd-127.0.0.1     1/1          Running       3                              39d\nk8s-master-127.0.0.1   4/4          Running       14                             39d\nk8s-proxy-127.0.0.1    1/1          Running       5                              39d\npachd-mnnvn            1/1          Running       0                              32s\npachd-p3oy6            1/1          Running       0                              32s\nrethink-dmq0f          1/1          Running       0                              41s\nThen I'm not sure why you cannot connect. In that case I'd try to do make clean-launch and make launch\n. I'm going to re-name this issue as a todo for us to make the error reported more descriptive / helpful.\n. Yup.\nBefore we get to that - when you paste log lines please wrap them in triple tick quotes ... like I edited your comment above. That makes the logs way more readable.\nIt'll take about 1-2minutes for the launch to succeed. So when you run make launch make sure that you let it go for at least that long.\nLet's see if your cluster is working. To test, run:\nshell\npachctl list-repo\nIf that returns at all, that means the cluster is ready. If not (that shouldn't take a long time to complete the command), then lets find out what the state of the cluster is:\nshell\nkubectl get all\nI'm looking specifically for any errors reported. If we see a certain pod reporting errors we'll do something like:\nshell\nkubectl logs pachd-33546\nFor the pod in question.\n. What is the output of:\nkubectl logs pachd-smta1\n. Actually ... lets move your debugging into a separate issue ... #540\nThis issue is only to improve the error message logging.\nTo that end, #147 is related.\n. fix is merged\n. Cool, will do\n. LGTM\n\n. Related to #223 - but I think #223 requires the client change you mention\n. I'm good w that. It's a dangerous op - probably should have a prompt / force flag.\n. LGTM\n\n. Related to #333 \n. This hasn't come up in quite a while. Now that we no longer user FUSE in the distributed processing step, I doubt user's will really want this short term.\nClosing. If we see requests for this, we'll re-open.. 1) Issue #520 is related to starting up the cluster, not runtime errors\nI think you're asking about logging. We leave it to the users to run with k8s tools to do their logging. How you report errors is up to your application logic.\nIf you're asking how to see logs per pod, do kubectl logs thenameofthepod\n2) We just support JSON\n3) I'm not sure what you're asking. Can you be specific about what you're trying to do?\nIf you're still blocked on #520, then creating a pipeline definitely won't work. I think you need to make sure that pachctl list-repo returns successfully. That shows us that the cluster is working. It's not fruitful to try the create-pipeline methods until your cluster is working.\n. Related to #512 \n. Ok how about:\nkubectl logs pachd-smta1 --previous \nAnd I'd also like to see kubectl get all again to make sure we're still seeing an error of some kind w that pod\n. Additionally ... going to update fuse to not report the FileNotFound errors to stdout ... those are confusing to users.\n. LGTM\n\n. (going to start listing questions we've seen more than once here so we remember what to add to these docs)\nWe've fielded a few questions for PullPolicy settings for custom manifests. Lets ID the symptoms for when this is set wrong (e.g. when you have old features bcz the new default is Always when you're trying to use newly compiled versions so you should set it to IfNotPresent)\n. The fixes from the error-cleanup branch silence the fuse errors, so we don't have to document that one anymore ;-)\n. +1\nYea this was striking looking at the Tensor Flow docs - you can assign / parallelize work as needed (within processes / across networks). Looking at some of their examples, it seemed like it would require a lot of overhead to setup the analogous computation in PPS.\n. If we see this running/passing on CI on pfs-refactor branch, we should close this\n. CI is passing on pfs-refactor branch, which runs it and it passes. So closing this until we have repro steps\n. And if we do that ... we should probably bake it into the release script to make sure its up to date.\n. We no longer use the fruit stand image, so this is not an issue\n. The new migration feature performs this ask. And API level details (we've fielded some questions about unsafe / handle params). This should be covered in API doc strings, but we should make those docs more prevalent.\n. via @JoeyZwicker -- we should up level the docs on how to make a new container that uses the job-shim container\n. the example image for that lives here: https://github.com/pachyderm/pachyderm/blob/master/etc/user-job/Dockerfile\n. Unfortunately, I think 'aggregate' issues for docs doesn't really work. I'll document the following specifics:\n- new environment variables\n- how to construct / use a new image using pachyderm\n- json / bin delimiters\nBeyond that ... we should just get into the habit of documenting in the moment. This will get easier once we have a place for docs!\n. Fix is merged\n. LGTM (other than the symlink which shouldnt be changed)\n\nMy bad\n. I think 'etcd state' as a source of truth is true w our current PPS implementation.. Discussed the design for this w @jdoliner  -- Here's where we landed:\nConstraints\n1) fuse (the client in this case) needs the move operation to be atomic\n2) there may be multiple pps containers doing the move, so we need to make sure concurrent requests to move are atomic within our API\nDesign\n1) pfs.InternalApiServer will add support for DeleteFile returning the list of blockrefs from the file\n    - This allows us to grab the blockrefs, and delete atomically wrt pfs.ApiServer\n2) pfs.InternalApiServer will add support for PutFile (or a variant) to accept blockrefs instead of bytes\n3) pfs.ApiServer will expose a new MoveFile() method, which will\n    - call pfs.InternalApi.DeleteFile and grab the blockrefs, then\n    - call pfs.InternalApi.PutFile w these blockrefs\n    - doing both these calls should guarantee the move is atomic wrt the (fuse) client\nTo be clear, the new DiffInfos Append will have a key for the new filepath, and the value will be all of the cumulative blockrefs from the old file path. This basically squashes the history of the old file into a single commit for the new file.\n. Ah, this is a known issue. Sorry you ran into this.\nRight now our FUSE later doesn't have rename support. We should be able to address this soon (after the Pfs refactor lands).\nBut for now as a workaround I'll refer you to the transform supplied with the example. It does a few peculiar things to workaround this issue. Basically it makes sure to run the model w inputs that are copied from the Fuse mount and copies the results back to the output mount. This gets around the python program trying to rename things within Fuse.\n\nOn Sep 5, 2016, at 5:46 AM, soshiant1992 notifications@github.com wrote:\nmy tensorflow code works fine just when i try to save the model i get this err\nE tensorflow/core/util/tensor_slice_writer.cc:121] Failed to rename file /home/xxxxxx/pytest/.tempstate16554456158648005478 to /home/xxxxxx/pytest/\nE tensorflow/core/client/tensor_c_api.cc:485] /home/xxxxxx/pytest/.tempstate16554456158648005478\n\u2014\nYou are receiving this because you were assigned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Bumping this to 1.4\n\nAlso since our FUSE usage pattern has changed quite a bit, we may want to re-evaluate which FUSE features we want to spend time on / support.. We no longer need to handle FUSE support for anything running in PPS. I'm not convinced users utilize the FUSE mount locally that often, especially now that we've moved away from FUSE. Closing this issue.. If our guess is right and its a seek issue, this will be addressed by #529 \n. BTW - The workaround is to copy the files out of the mount before reading them. This works fine.\n. We no longer user FUSE in PPS. LGTM\n\nI ran through the steps locally to validate they work. This created a dummy release on GH which should suffice for testing / updating the brew formula.\n. Very interesting.\nMaybe this is too tricky to test, but I wonder if this behavior changed recently w the grpc update. IIRC before the recent update, we were able to re-use grpc connections. This may have prevented the grpc connection count from growing and triggering GC. If so, it sounds like solving #588 would help mitigate this issue. Although it feels like it might mitigate, but not solve it, if the FUSE buffer pool is sensitive to GC at a certain rate.\n. Other than minor comment, LGTM\n\n. LGTM\n\n. Makes sense how it prevents the panic. \nWhere did this bug report come from? It's too bad we can't add a failing test -- next time would be good to try and get a reproduction. Something to keep in mind when we see issues come in. But fixing panic === good\nLGTM\n\n. \nLGTM\n. Hmm I'm seeing a different bug that seems related.\nI ran through the fruit-stand, and everything looked normal. Output commits from pipeline worked fine.\nThen I ran the generate script:\nshell\n$go run examples/fruit_stand/generate/generate.go\nAnd I see a bunch of commits on the data repo:\nshell\n$ pachctl list-commit data\nBRANCH                             ID                                 PARENT                             STARTED             FINISHED            SIZE                \nmaster                             35d7476b4f8540a48323a01a6a79802c   2175158356f84421b1b460e950ce5b7b   11 seconds ago      10 seconds ago      863 B               \nmaster                             2175158356f84421b1b460e950ce5b7b   28d07f999a864bddb553fcf7167d0015   11 seconds ago      11 seconds ago      863 B               \nmaster                             28d07f999a864bddb553fcf7167d0015   cb0a94fa5a0b493599886230c01ff581   11 seconds ago      11 seconds ago      872 B               \nmaster                             cb0a94fa5a0b493599886230c01ff581   9719744873d34067bf0de78141fc64a0   11 seconds ago      11 seconds ago      866 B               \nmaster                             9719744873d34067bf0de78141fc64a0   37035da60a5a43169869945437f6b960   12 seconds ago      12 seconds ago      866 B               \nmaster                             37035da60a5a43169869945437f6b960   8fb102b7b7ff4927a7ef656de8a4a5df   12 seconds ago      12 seconds ago      868 B               \nmaster                             8fb102b7b7ff4927a7ef656de8a4a5df   ee7fc238e26d4312b5b22546aaa6e889   12 seconds ago      12 seconds ago      870 B               \nmaster                             ee7fc238e26d4312b5b22546aaa6e889   f9af7b21342147e383363c0d040c5a2f   13 seconds ago      13 seconds ago      875 B               \nmaster                             f9af7b21342147e383363c0d040c5a2f   fcd208e19e9d4dc2812c9565cd6967b9   13 seconds ago      13 seconds ago      865 B               \nmaster                             fcd208e19e9d4dc2812c9565cd6967b9   6a8d09045bf24eae99183a6128ec73b2   13 seconds ago      13 seconds ago      867 B\nBut no new ones on the sum repo:\nshell\n$ pachctl list-commit sum\nBRANCH                             ID                                 PARENT                             STARTED             FINISHED            SIZE                \n17570ca4af614a869d45f9210c998c4d   a39d43799aea440b9a60052902a1891f   91a7f4d930884beb90b140c641ddf56c   9 minutes ago       9 minutes ago       12 B                \n17570ca4af614a869d45f9210c998c4d   91a7f4d930884beb90b140c641ddf56c   <none>                             21 minutes ago      21 minutes ago      12 B\n. That seems reasonable. As the person who tried to do the Fancy Thing and parse the fruitstand walkthrough to run the commands ... thats more trouble than its worth. If we add a test we should just have an integration test that does the steps in the walkthrough. We'll just have to keep that in sync.\n. Another data point\nWhen trying to reproduce via CLI by doing pachctl start-commit data master\nI see a commit in the data repo, but no corresponding commits in filter or sum\n. If we had this, it would also enable users to version their use of pachyderm in their dockerfiles.\nAs opposed to doing this where we base off a third party image and manually install job-shim ... which means we get whatever version is on head ... we could do something like:\napt-get pachyderm-job-shim -v 1.2.0-RC1\nThe reason you may need to version job-shim is that we've had breaking changes in protos. While running through the TF example post pfs-refactor, I'm getting this error message:\nrpc error: code = 13 desc = grpc: failed to unmarshal the received message proto: bad wiretype for field pps.Transform.Debug: got wiretype 2, want 0\nFrom the pod that's trying to run the transform.\n. Haha. Yup. Todos from testing on JD's machine:\n1) Update setup.md to include linux one liner\n2) Fail (and stop) if goxc not configured (missing GH token)\n3) Fail if sha256sum not found\n4) Update www script to upload single file\n5) Make sure we can supply a blank string (for no additional version)\n6) Remove commented lines in release task ... and output message to ask user to push to complete release\n. Ok, did each of the things above.\nThe one exception is that uploading a single file doesn't work for larger files. I've opened an issue on the www repo to track this.\nAlso ... it made me very happy to remove most of the docs under the release instructions.\nOnce the builds pass again, I'd like you to run the script locally again @jdoliner , but this time test a point release, so:\n1) increment the micro version in src/client/version/version.go\n2) run make point-release\nThen we'll have v1.0.2 released. If you'd rather test a pre-release, I'm ok w that too. In that case, no code changes, just:\nmake VERSION_ADDITIONAL=RCblah release\n. And fixed #600 while I was in there. Now we have true separation between pachctl installs (without go!) and pachyderm cluster deployment!\n. Hold on a sec ... I think we removed pachctl from the polling mechanism. Stay tuned.\n. Yea, its no longer needed.\nRipped it out as part of the automate release PR\n. Merged in PR #599 \n. Ran into a similar use case when running through @dwhitena's go compilation pipeline. Basically ... I had parallelism of 64, and doing the rough math I expected the job to complete in about 80 minutes.\nHowever, the input data (list of repos) size is not proportional to computation time. Compiling is CPU bound and some repos seem to take a lot longer. (So the processing is really lumpy)\n90 minutes into the job, I have 22 pods left. I'd love to be able to re-distribute the work to the other 42 'idling' VMs, so that we recover some of the total processing time. To do that, we'd have to detect what files have been read as you describe above.\n. I agree its tricky. This is a type of optimization we wouldn't consider for a while, because it probably only yields benefits in niche cases (like the one I listed above).\nI do think its possible if the users have the right expectations. Those expectations are basically to re-query to see what files are available between reads.\nThis prevents the pod that has files 'taken away' from it, from erring when it tries to read a file it may have 'seen' earlier when listing files in a directory.\nIt also allows pods to handle new files that might appear after initially listing files in a directory.\nEven though this may be theoretically possible ... doesn't mean I think its a good idea. We'll see if we have more demand for this type of niche.\n\nA more broad way to frame this problem is: What recourse we have for 'runaway jobs' ? ... jobs whose processing times are huge outliers. \nOne bit of recourse - w the PFS refactor, we'll treat each pod's changes as a separate commit, so if some subset of the pods are long running (as they were above), you should be at least able to stop them, and see which inputs were causing the problem.\n. There's a separate issue open for straggler detection: https://github.com/pachyderm/pachyderm/issues/442\nMeanwhile, in v1.4 the data is distributed at a much finer grain (via datums). I believe that would have helped mitigate the issue I was seeing. Also - PPS is smart enough not to reprocess any datums.\nI don't think there is any other todo's for this issue (since we have a separate straggler thread). @derekchiang - care to verify?. I don't remember seeing these until recently, so I don't think there's a reason they need to go to stdout/stderr\n. In 1.1.0 we added a verbose flag to see debug output. By default, fuse logs are not shown.\n. This is stale. Since we cannot reproduce, closing this issue\n. No known reliable way to reproduce this + addressed by refactor, so closing\n. LGTM\n\n. I think this is already done?\n. I believe this is on the doc-sidebar branch\n. No longer use rethink. We have etcd deployable as a petset in 1.4. Double closed!. Ooop ... not mergable ... I'll pull in master again and push.\n. I believe we have some semblance of this per datum in 1.4 ... @derekchiang can you weigh in?. This seems stale, and hasn't been requested in a while.. Just to restate the user's use case from #914 :\nThey ran out storage on their local disk (which prevented them from running pipelines). They did pachctl delete-all and were surprised that all of the raw files weren't also deleted. This seems like a reasonable thing to be surprised about.\nI'm not sure what the implementation would be to service this (I think there are several), but want to make sure we understand this use case when considering our design. \n. \n. This may be a tricky one ... doing awfully similar steps produces 'good' output:\n``` shell\nGoT$ pc create-repo test\nGoT$ pc start-commit test > commitID\nGoT$ mkdir ~/pfs\nmkdir: cannot create directory \u2018/Users/sjezewski/pfs\u2019: File exists\nGoT$ umount ~/pfs\numount: /Users/sjezewski/pfs: not currently mounted\nGoT$ rm -rf ~/pfs\nGoT$ mkdir ~/pfs\nGoT$ pachctl mount ~/pfs &\n[2] 28812\nGoT$ 2016-07-06T23:59:42Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\"..\",\"err\":\"repo .. not found\"}\n2016-07-06T23:59:42Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\"..\",\"err\":\"repo .. not found\"}\n2016-07-06T23:59:42Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\"..\",\"err\":\"repo .. not found\"}\nGoT$ 2016-07-06T23:59:43Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\"DCIM\",\"err\":\"repo DCIM not found\"}\n2016-07-06T23:59:43Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\"..\",\"err\":\"repo .. not found\"}\nGoT$ 2016-07-06T23:59:43Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".Spotlight-V100\",\"err\":\"repo .Spotlight-V100 not found\"}\nGoT$ \nGoT$ 2016-07-06T23:59:43Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index\",\"err\":\"repo .metadata_never_index not found\"}\nGoT$ \nGoT$ 2016-07-06T23:59:43Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index_unless_rootfs\",\"err\":\"repo .metadata_never_index_unless_rootfs not found\"}\nGoT$ \nGoT$ 2016-07-06T23:59:43Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\"..\",\"err\":\"repo .. not found\"}\nGoT$ \nGoT$ 2016-07-06T23:59:43Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\"..\",\"err\":\"repo .. not found\"}\nGoT$ \nGoT$ 2016-07-06T23:59:43Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index\",\"err\":\"repo .metadata_never_index not found\"}\n2016-07-06T23:59:44Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index_unless_rootfs\",\"err\":\"repo .metadata_never_index_unless_rootfs not found\"}\n2016-07-06T23:59:44Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\"..\",\"err\":\"repo .. not found\"}\nGoT$ \nGoT$ \nGoT$ \nGoT$ cp data/all.txt 2016-07-07T00:00:00Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index\",\"err\":\"repo .metadata_never_index not found\"}\n2016-07-07T00:00:00Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index_unless_rootfs\",\"err\":\"repo .metadata_never_index_unless_rootfs not found\"}\n~/pfs/test/318cdbc807e047568f731b5e302b5b61/\n2016-07-07T00:00:05Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"test\"},\"id\":\"318cdbc807e047568f731b5e302b5b61\"}},\"write\":true,\"shard\":{\"fileModulus\":\"1\",\"blockModulus\":\"1\"}},\"name\":\"all.txt\",\"err\":\"no such file or directory\"}\n2016-07-07T00:00:06Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"test\"},\"id\":\"318cdbc807e047568f731b5e302b5b61\"}},\"write\":true,\"shard\":{\"fileModulus\":\"1\",\"blockModulus\":\"1\"}},\"name\":\"all.txt\",\"err\":\"no such file or directory\"}\n2016-07-07T00:00:06Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"test\"},\"id\":\"318cdbc807e047568f731b5e302b5b61\"}},\"write\":true,\"shard\":{\"fileModulus\":\"1\",\"blockModulus\":\"1\"}},\"name\":\"._all.txt\",\"err\":\"no such file or directory\"}\nGoT$ \nGoT$ \nGoT$ \nGoT$ pc finish-commit test cat commitID\nGoT$ pc get-file test cat commitID all.txt > testfile.txt\ncat testGoT$ cat testfile.txt 2016-07-07T00:00:33Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index\",\"err\":\"repo .metadata_never_index not found\"}\n|2016-07-07T00:00:33Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index_unless_rootfs\",\"err\":\"repo .metadata_never_index_unless_rootfs not found\"}\n wc -l\n12733\nGoT$ 2016-07-07T00:01:39Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index\",\"err\":\"repo .metadata_never_index not found\"}\n2016-07-07T00:01:39Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index_unless_rootfs\",\"err\":\"repo .metadata_never_index_unless_rootfs not found\"}\nGoT$ \nGoT$ du -h testfile.txt \n1.4M    testfile.txt\nGoT$ \n```\n. Haven't seen this since. Generally we only see people using FUSE for read only stuff these days.. Fixes #562 \n. Merged\n. I think this was a sharing issue, so should be fixed by pfs refactor\n. Interesting - I wasn't sure if it would refresh when hit, so I double checked. At the bottom of their page they have:\nPackage client imports 10 packages (graph). Updated a day ago. Refresh now. Tools for package owners.\nThe presence of the button tells me that it doesn't refresh when a user hits it. But as long as it can't get older than a day, then yea I say no need to automate. If its longer than that, it would be nice to hit 'refresh' so that they stay up to date.\n. Ok, after the 1.2 release it seemed to digest the changes properly.\n. Hi there!\n1) You're correct, that first error:\n2016-07-11T04:23:52Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"data\"},\"id\":\"b11fc97a24e14d5aa106bdcd39980cc4\"}},\"write\":true,\"shard\":{\"fileModulus\":\"1\",\"blockModulus\":\"1\"}},\"name\":\"sales\",\"err\":\"no such file or directory\"}\nIs totally normal. You can safely ignore it.\n2) The first command you used to create the pipeline should have worked. When creating the pipeline, Pachyderm will also create the corresponding filter and sum repos.\nThe error you see here: \nError from InspectJob: grpc: failed to unmarshal the received message proto: bad wiretype for field google_protobuf.Timestamp.Seconds: got wiretype 2, want 0\nHappens when your pachyderm client (pachctl) and pachyderm server (pachd) are running incompatible versions. We recently made an update that requires you to use the latest version.\nSo please, make sure you have the latest code on master, then do:\n- reinstall pachctl (from source / homebrew / deb package ... however you did it the first time)\n- make clean-launch and make launch to update your Pachyderm server\nThen pachctl version should report the latest versions for both client and server:\n$ pachctl version\nCOMPONENT           VERSION             \npachctl             1.1.0               \npachd               1.1.0\nOnce you have reached this state, running the example should work just fine.\n. We discussed / reviewed the design today. In summary:\nMotivation\n\nWe want read/writes to be as fast as possible to enable scale\nIn particular, InspectFile  basically gets called for every operation\nWe also want to eliminate the complexity / state in the pfs API Server / Internal API Server / Driver Layers\n\nDesign\nWe'll trade the statefulness and hashed routing of PFS for:\n- a DB containing the state of branches / commits / etc\n- which enables dumb routing to PFS API Servers (any pachd server can service any request)\n- FUSE mounts on jobs can construct history directly from the database\n- job pods will each run their code in a separate commit, and merge the results afterwards\nPlan of Attack\n1) Update Driver In place\n- Create the new database schema\n- Implement the History method\n- Update all the file access methods in driver to use the new branch clocks / commits / diff objects\n2) Collapse PFS API/Internal API/Driver\n- Should eliminate everything but API Server\n- API Server should be very thin, and rely on\n- A File System Library that actually performs the operations\n3) Update job-shim / merge commit\n- Job shim should start a new commit per pod\n- Finish commit should perform the merge\n4) FUSE constructs history directly\n- Fuse no longer connects to API Server\n- instead uses the File System Library to directly access DB\n. shell\n$git checkout master\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\n$grep -R \"func Test\" ./src/* | grep -v vendor | grep -v \"func TestMain\" | wc -l\n135\n$git checkout pfs-refactor\nSwitched to branch 'pfs-refactor'\nYour branch is up-to-date with 'origin/pfs-refactor'.\n$grep -R \"func Test\" ./src/* | grep -v vendor | grep -v \"func TestMain\" | wc -l\n178\n43 more tests!\n. A few other things that @derekchiang and I noticed when doing a pass ourselves:\n- clock.go probably needs another pass ... a lot isn't used\n- get rid of shards const in server_test.go\n- Why are pod commits (used in pps api_server) a map not an array? (SJ: I think there was a reason for this, have to go back and look at why)\n- commitMount -- why did we see / need to handle the empty case?\n. Sorry you ran into this. Since this issue is quite old, I'm closing it. But feel free to re-open if you're still experiencing it.. LGMT\nIdeally we also have a test for the 'real world' use case hitting S3. But since we haven't been able to reproduce so far, and we want to validate if this fix works, lets make a separate issue for the S3 testing. (We may already have one)\n\n. Deployed\n. duplicate. referenced in aggregation issue. The live URL is here: pachyderm.readthedocs.io\n(That's basically empty because it refreshes off of master)\nWhich means that we have to merge the PR to see what they'll look like. Might need to fix a few broken links / make sure it all works as expected.\n. Fixes #633 \n. This may have been caused by the other PFS hang. Since we don't have a way to reproduce this, I'm closing for now.\n. Ideally should include:\n- 'dump' case ... where you input all the data in a table into a single commit\n- 'update' case ... where additionally on subsequent runs, it dumps the things in the DB that were added since the last commit (since we don't support line-diffs we can't semantically support rows that were updated, though you could always re-dump everything)\n. Might be helpful for having egress go to AWS, even when running code locally. I think it's just a matter of propagating the secrets through (either manually or via flags).\nSince no else is asking for this, and I don't remember why I needed it, I'm closing.. This is really old. Plus we have a bunch of other ones. Closing. LGTM\n\n. Also, I think we regard a session as a single pachd run. Would be nice if we could tag a user to understand development patterns.\n. To be clear - right now a session is a start of the pachd cluster (not a single pachd node)\n. Is there a way we can test the 'Secrets' feature as well?\nAlso - now that we have a place for docs, we need to make sure to document any new features (or they basically don't exist for users). This would probably go here I think\n. Minor comment in the test, but the coverage looks good.\nWe still need to add these features to the pipeline spec documentation though.\n. \n. Fixing #670 will make most of this better. The confusing thing is viewing the docs on GH - those docs won't work. So we need to remove the readme under /docs so that people don't do that. We should probably update the readme under docs to point users at read the docs. \nSo, I'm going to re-appropriate the title from \"Broken links on GH docs\" to \"Restructure Docs\". I'm sure they could use more organization :-)\n. \nLGTM\nWhat was the feature / reason we need the new version?\n. Fixes #670 \n. \n. Oh, so this one was detached and not waiting on. Makes sense\n\n. duplicate\n. There's baked in support for GA on the RTD dashboard for our project\nI'm not sure if we can inject our own JS and use their existing 'theme' (the thing that provides the basic styles / search / etc):\nhttps://github.com/rtfd/readthedocs.org/issues/728\nBut it's something we can look into if we don't want to add another analytics service (which is a very reasonable thing to want). \n. Ok, its hooked in now. Signin to GA through your gapps account to see traffic\n. LGTM\n\n. I loooooove this feature, and can't wait to start using it.\nMy comments center around the CLI API and test coverage.\n\nCLI\nShould we expose the new client functions to the CLI (ArchiveCommit() / StartPipeline() / StopPipeline()) ?\nI think we're in the habit of doing so. I could understand how these new functions are mostly used in the case provided by calling CreatePipeline() to update a pipeline, but I could see users wanting to start / stop a pipeline manually / etc.\n\nTest Coverage\nI like the tests you've written -- those are some real thorough tests! But I'm not sure we have coverage for everything that is affected by the change.\nThere's an archived field on both CommitInfo and DiffInfo ... which means anything that reports results based on commits / diffs can be affected, so should be tested. \nSome of these I do think we have coverage for already based on your tests (e.g. FlushCommit(), GetFile(), PutFile()). And some of the coverage I'm asking for is a bit redundant (quite a lot of things depend on the behavior of inspectFile() for instance) ... but I think still make for good end-end tests:\n- Commits\n  - InspectCommit() should report the right size (it should include only the diffs that are not archived)\n  - ListCommit() should list only non archived commits when the 'normal' type is specified (you test the 'all' case but I didn't see the 'normal' case ... maybe I missed it)\n- File\n  - InspectFile() should only list files that are created by non archived commits, and report the right size\n  - ListFile() should only list files from non archived commits\n- Repo\n  - ListRepo() should report correct sizes omitting the sizes from archived commits\n  - same for InspectRepo()\n\nConcerns\nIf we do expose these helper functions (ArchiveCommit() in particular), there are cases in which a user could shoot themselves in the foot. Mostly because I believe inspectFile() traverses back on commits and looks for delete flags and doesn't check for the archived flag. \nIf user's stay on the 'happy path' and only use CreatePipeline() to archive commits this can't happen ... since by restarting pipelines, we start basically a fresh branch that contains no archived commits as parents in the history. That's really the assumption a user could violate w the manual tools.\nBut this is something to consider. Especially in light of the PFS refactor. We've put some thought into how we detect deletions efficiently w our queries. As long as a whole 'branch' gets archived at once I think we're ok in the new paradigm. But again, if a user decides to archive commits that are ancestors of non archived commits, that's bad, and would have undefined behavior as is in the refactor. \nPerhaps as a compromise near term (esp since we want this to land short term) we:\na) don't expose these manual tools to the user, and\nb) consider how we'd handle this in the pfs refactor. Maybe either:\n- detecting 'bad' usage of ArchiveCommit() (and disallow it unless you archive a whole branch)\n- or update our commit querying logic to handle archiving (but this seems complicated and bad)\nHopefully my descriptions make sense. We can discuss more in slack if thats helpful.\n. Ah, ok. I can see how my comments were confusing. And you're right, there was some conflating of Archive and Delete. \nI was assuming we would expose the helper functions via CLI, and in that case I think I could construct a state where the results are confusing. I realize now the tests I was asking for are mostly addressing these sorts of problematic states. Below I describe what I mean in more detail for GetFile()\nIt makes sense what you said about repo sizes, (though if I was debugging something and only wanted to report the size of the non-archived commits it would be nice to have a flag for that).\nAnyway, as long as we don't expose the CLI helpers to the user (which we aren't currently), then this PR LGTM\n\n\nJust as an example of a confusing state the user could construct w access to ArchiveCommit via the CLI, consider the commit structure:\nA --> B --> C --> D\nWhere each commit appends to the same file. After having created this commit structure, a user decides to Archive commits A and B. (Not saying this corresponds to a clear use case, just is something that they could do).\nNow what happens when you call GetFile() on commit D with no From commit? Normally we'd expect it to traverse all the commits to construct the content. That is what will happen. But is that what we'd expect? Probably not? But I'm really not sure. That just seems confusing.\n. Oooo I like this feature\nAside from inline comment ...\n\n. This looks like it fixes #565 \n. We don't have coverage for this sort of code, so I opened #710\nOther than that and my comment, LGTM\n\n. merged. merged. Good tests, and we have an existing test for the default behavior\nLGTM\n\n. Makes sense.\nI'll run w the backoff code for deletion and adding support for archive-all\nAnd I agree about caching ... that's a fine solution for the right problem, but I'd want to make sure we've isolated it to that type of problem\n. Some updates! We now have:\n- the delete backoff code (in place on master)\n- the new getfile cache (solves a similar but different issue) ... about to land\n- and the logging update (in place on master) to help us identify the bug next time this happens\n. More updates:\nWe also added some profiling tools, and between profiling and looking at logs we identified a cascading bug.\nThis bug would be triggered on restart. \nIn particular, we were indexing a blank commit, which caused an endless loop of CreateJob()s to be called that wouldn't finish (because the commit ID was blank), resulting in a new commit that would be started and never finished on each output repo. The jobs would try and restart using backoff, but the pipeline's input repo watcher would also create new jobs, making the effect compound. This resulted in about 1 new CreateJob / output commit per second. After a day of this, we see about 100k commits (that were never finished).\nWe believe this is the source of the 'hanging' symptom. This cascade could happen anytime there was a restart ... from either normal k8s rescheduling or if we truly were doing too many GetFile()s making a pod unresponsive, docker would kill the process/container and k8s would reschedule.\nWe have a fix we're testing now.\nBetween this fix and the caching for GetFile()s, we think the issue should go away. But I'll leave this open so that we can validate that once its deployed.\n. This fix has been live about a week without reproducing the issue, so we're counting this as closed\n. Similarly, we might want to expose the number of current connections per pod. (So increment/decrement an atomic number when we start serving a request)\n. Once the block groupcache lands, we should have metrics reporting on:\n- cache hits\n- cache misses\n- cache utilization (hit/miss ratio)\n- per pod\n- also mem consumption per pod?\n. Master has the fix for this\n. And one last thing. Once we have a flag to enable/disable, this seems like a great example to write a benchmark to validate the behavior.\nAfter that, LGTM\n\n. Yea, thats fair. We have an open issue ( #648 ) about having test coverage for an obj store ... once we have that in place, this gets a lot easier.\nLet's make a new issue for this benchmark and refer to it in #648 \n. That first one is a good catch from the linter.\nLGTM\n\n. TestBigListFile is running and passing on pfs-refactor branch\n. Ok, think I addressed all your inline comments. Just need to make sure that it plays nice w CI\n. The only naming conflict I see is Block for the API and Block for the individual object. Then again, its probably pretty clear from what you're doing w that symbol which one it is.\n. Haven't reached for this tool in a long time. Closing.. Ah, I think those make tasks got removed when we automated the release process. In particular, I think I didn't want anyone 'foot-shooting' by regenerating the versioned manifest locally. But it makes sense why you'd want to have the dev version as a make task.\n. Reading through the changes it makes sense why you'd want to update the versioned manifest as well. However, if we merge this PR as-is, the versioned manifest thats available locally in the repo is different from the one available on the website. \nThat means the behavior of make launch and the command kubectl create -f https://pachyderm.io/manifest.json will have different results.\nTo keep those in sync, we can run the release script again. But in this case, that's what I was hoping to avoid by exposing make manifest. (Running the release task will update the manifest as part of the task).\nI think we should have two different versions of the stable manifest released. That seems like it'll lead to some confusion about which manifest is being used (esp when we're helping someone debug). Both versions will contain the same version strings for the images, so will be hard to distinguish.\nI guess it depends on how we see this feature being used. \nDo we:\nA) Assume they have to use make dev-launch to get pre-release features?\nIf so, I'd recommend removing the 'make manifest' task / undoing the changes to the versioned manifest\nOR\nB) Assume they'll only ever run make launch or kubectl create -f https://pachyderm.io/manifest.json\nIn which case, I would still remove the make manifest task, but run the release automation script (which itself should generate the versioned manifest)\n\nOther than my nitpicking about versions, LGTM\n. \n. Haha, I made it ;-)\nI mostly use: https://imgflip.com/memetemplates\nPretty soon our code reviews will involve reviewing the memes!\n. I'm still figuring out how these components fit together, but the basics are:\n- We spin up a fluentd pod per node (VM) that is provisioned. These gather logs from each node and send the logs to the elasticsearch-logging pods\n- The elasticsearch-logging pods, which store the logs and expose them via a REST API\n- The kibana pod provides a web UI for viewing the logs\n. The above is probably a valid path if we want to vendor in some logging tools w a given pachyderm cluster deployment. But! GCE and AWS seem to have builtin cluster wide logging already.\nIn particular, GCE provides monitoring (of CPU/MEM) via stackdriver, and logs (w search and past a few days) via their dashboard.\n. \n. Thanks for the benchmark. We've ripped out FUSE and a lot has changed in our processing model. I don't see any more todo's here, so I'm closing the issue. Please reopen if you disagree.. Ok, made those changes. Lmk what you think about the make task name\n. \n. Minor bash feedback, otherwise\n\nAlso this is perfect timing, as this would've fixed a confusing pattern of CI failures I was just wrestling with.\n. @jdoliner thinks that these don't test any code that is used anymore\nIf so, we can just remove these\n. Oo, this is a nice batch of improvments.\nI love the port-forwarding command, thats super helpful\n. Minor issue-tracking todo, otherwise\n\n. We've broken down this implementation into a few steps:\n\n'job' analog of a service (mounts the data ... but data never gets refreshed)\nupdating pachctl CLI to have a delete-job method (and implement deletion\n'pipeline' analog of a service (mounts the data and does a rolling update each time a new commit is available)\n\nWe expect at least the first part to land for 1.3\n. The 'job' analog + the delete-job CLI landed in 1.3\nMoving this issue to 1.4 for the remaining pipeline analog feature. Fixed by : https://github.com/pachyderm/pachyderm/pull/799\n. Would also be helpful for nested objects in logs:\n2016-09-01T23:18:56Z INFO  protorpclog.Call \n{\n    \"duration\": \"0.000s\",\n    \"method\": \"CreatePipeline\",\n    \"request\": \"pipeline:<name:\\\"pipeline22893edd24b05\\\" > transform:<cmd:\\\"cp\\\" cmd:\\\"/pfs/pipelinea160108d7723/file\\\" cmd:\\\"/pfs/out/file\\\" > parallelism:1 inputs:<repo:<name:\\\"pipelinea160108d7723\\\" > > \",\n    \"service\": \"pachyderm.ppsclient.API\"\n}\n. Thanks for the detailed debugging.\nFUSE has been ripped out of PPS, and the job processing model has been updated in 1.4\n. Thanks for the load testing @ShengjieLuo. I don't think there's any more todo's here so I'm closing this issue. Feel free to re-open if that's not the case.\nWe're now starting to add load tests of our own to the repo so that we can compare performance across release boundaries. Soon you'll be able to pull those and run them yourself.. This is done on the pfs-refactor branch\n. The only comments remaining are those that highlight changes for the new API. We already have an issue for this, so closing this one.\n. This issue is quite old, so I'm going to close it. Feel free to re-open if you're still experiencing this issue.. Starting up rethink seems to timeout. We were asking for trouble by killing/starting test rethink per test suite (if there is n% likelihood it wouldn't startup within the 20s sleep window, then we were tripling that n% by doing it for a few suites).\nFor now, this seems to work. A longer term fix is described in #801 \n. \n. Ooof, that k8s image pull change must've been nasty. Looks like now when spinning up k8s jobs we now have the local tag.\nLGTM\n\n. Starting to make a list of such changes.\n1) Update PutFile() and GetFile()\nRefer to the comment at src/server/pfs/server/server_test.go#L1421\nWe want to update PutFile() and GetFile() to only accept commit not repo (since commit's have a repo), because this makes validating document existence easier (since we don't have to do the extra hop for repo).\n2) Remove redundant code for handling ambiguous commit IDs\n. Yea, thats a good point. However we want to make sure make test-fuse and make test-local also launch the cluster to make sure they have what they need for the tests. So, I'll:\n- Make each suite that needs it launch the cluster in its make task (make test, make test-fuse, make test-local)\n- Update make launch to do nothing if it detects that the cluster is already up (so that its idempotent)\nThis way we'll only launch the cluster once on CI, not three times.\n\nThen, I'll go ahead and add the readiness probe to the manifest for rethinkdb. That should make pachd startup cleaner (and less confusing to users). I'll be using their own script for this\n. Ok, this is all done on the pfs-refactor branch (just using the launch-dev task for spinning up DB, updating rethinkDB in manifest to use readiness probe).\nOne note -- if you're using docker-machine now, you'll need to forward the 28015 port to be able to run the tests locally.\n. I got this partly working on pfs-refactor branch, but couldn't get local tests suites connecting to the rethink instance (which I'm sure is some k8s networking config).\nHowever, I left the readiness probe in place, but there seem to be a few issues with it.\nA) For a user on GCE, it wasn't working at all\nThey see the following logs:\nshell\n  54m        1s        655    {kubelet gke-pach-cluster-default-pool-26852678-alpi}    spec.containers{rethink}    Warning        Unhealthy    Readiness probe failed: Get http://localhost:8080/: dial tcp 127.0.0.1:8080: getsockopt: connection refused\nI'd expect maybe a few failures in the course of spinning this up, but it really shouldn't take that long (54 minutes and not up)\nB) They reported that they saw the following when executing the curl manually:\nThey did curl localhost:8080 and got the loading message below\nhtml\n    <body>\n        <div class=\"alert global_loading\">Loading...</div>\n    </body>\nWhich I think is the message you see when the dashboard isn't connected to the backend? In which case, the server responding w any payload and a 200 gives us the wrong signal for a readiness check. In other words .... it'll respond before the backend is truly ready.\n. Oh one thing that comes to mind to explain the first effect. \nI used the defaults I saw elsewhere to specify the total number of times it'll retry the readiness probe. If they're deploying k8s on a smaller VM, its possible it'll take rethink longer to startup, but by that time we'll have spent all of our readiness checks. We should consider updating that number (or removing it if it'll just try indefinitely)\n. Yea, I'll remove it for now. If we find we want it later we'll need to vet these issues to make sure it works.\nRight now the integration tests implicitly rely on the cluster being up, so rely on the readiness check. But as you said, the pachd startup mechanism was working before without this check.\n. Ok, removed it\nPutting that link here for posterity for when we do look at this again.\n. Rethink is no more. Closing.. Just to add to jd's comments ... I'll explain how this might look once 1.2 lands.\nMost often users are going to want to start a new commit at the HEAD of a branch. This workflow looks like this:\nshell\n$pachctl create-repo test\n$pachctl start-commit test master\ne8b9e6bc428547aa9bd0286068eee3e1\n$pachctl finish-commit test master\n$pachctl list-commit test\nBRANCH              ID                                 PARENT              STARTED             FINISHED            SIZE                \nmaster              e8b9e6bc428547aa9bd0286068eee3e1   <none>              10 seconds ago      4 seconds ago       0 B                 \n$pachctl start-commit test master\n46e286b0c88e439c88937a9370b01366\n$pachctl finish-commit test master\n$pachctl list-commit test\nBRANCH              ID                                 PARENT                             STARTED              FINISHED             SIZE                \nmaster              46e286b0c88e439c88937a9370b01366   e8b9e6bc428547aa9bd0286068eee3e1   23 seconds ago       17 seconds ago       0 B                 \nmaster              e8b9e6bc428547aa9bd0286068eee3e1   <none>                             About a minute ago   About a minute ago   0 B\nIn the other case (where you want to start a new branch off of a commit) you specify both the branch and the commitID as you did above:\nshell\n$pachctl start-commit test dev -p 46e286b0c88e439c88937a9370b01366\n49309f5820e3439f81e21c2dad30571f\n$pachctl finish-commit test dev\n$pachctl list-commit test\nBRANCH              ID                                 PARENT                             STARTED             FINISHED            SIZE                \nmaster              46e286b0c88e439c88937a9370b01366   e8b9e6bc428547aa9bd0286068eee3e1   6 minutes ago       6 minutes ago       0 B                 \nmaster              e8b9e6bc428547aa9bd0286068eee3e1   <none>                             7 minutes ago       7 minutes ago       0 B                 \ndev                 49309f5820e3439f81e21c2dad30571f   46e286b0c88e439c88937a9370b01366   11 seconds ago      5 seconds ago       0 B\n. Refactor is merged onto master now, and the new behavior is supported\n. @JoeyZwicker discussed this very issue today. There are some good methods for this in v1.4\nWe opened an issue about this here: https://github.com/pachyderm/pachyderm/issues/1558. Looks like we're referencing the same document in a few places.\nI'm betting they did the naive jquery thing to add a click listener, e.g. something like:\njavascript\n$(\".creating-analysis-pipelines\").click(\nfunction() {\n   $(\".creating-analysis-pipelines div\").toggle()\n}\n)\n. I was close\nBut basically that function returns too many things ... not the actual content you want to drill down on\n. Ok, updated the doc-sidebar branch to handle that option.\n@joey if you do:\nshell\npip uninstall sphinx_rtd_theme\npip install -r requirements.txt\nThen you should have this option working for your local setup. We may hit a snag when we deploy to RTD, but we'll see when we cross that bridge\n. Yup, that looks like it should work. I'll try it out\n. That was part of it. The other part was updating our conf.py file. Pr incoming\n. This is live\n. In particular, TestWriteManyFiles in the fuse test suite is a stress test. In particular when doing 10k the CI build time is ~30 minutes. At 1k files its 17 minutes. We want to tighten the CI feedback loop, so I'm all for having a lower count for that test.\nOne thing we can do is run it as a post-job task. This gives us shorter test times but allows us to run benchmarks on CI\n. duplicate. stale. I assume you tested the health checking / reconnection manually?\nI didn't see tests for the umount CLI or the reconnection code, but that's not too surprising since we don't have test harnesses for these.\nLet's make a note to test each of these on the issues that we have for generating the test harnesses we need for each:\n- the CLI test harness for umount coverage\n- the fault injector for reconnection testing\nOther than that, LGTM\n\n. fixed. Hi there. In v1.4 we have 'datums' which means you can explicitly control how your data is parallelized w file glob patterns, e.g. \"/*\"\nIf you're still having this issue, please upgrade to 1.4 and try out the new semantics.. This issue https://github.com/pachyderm/pachyderm/issues/1515\nis where we're tracking the docs for this feature, FYI. Ah, as part of this PR we decided to remove support for the manifest on the live URL. \nBut I hadn't removed that file from www yet, so its in a funny state. You're right @jbowles - there is a typo. That was from debugging that PR.\nOnce the PR lands, the new recommended way of launching a pachyderm cluster with the latest manifest is simply:\npachctl deploy\nWhich uses your version of pachctl to generate the manifest directly. So you'll need to make sure you have the latest version of pachctl.\nIf you want to generate the actual manifest file (we do recommend checking it into your codebase), you can do:\npachctl deploy --dry-run > somefile.json\nWhich is what make launch uses under the hood to deploy the manifest from pachctl\nLet me know (or feel free to comment on that PR) if this new behavior doesn't suit your needs.\n. In the meantime (till that PR lands), I've fixed the manifest\n. Looks like @JoeyZwicker got this\n. Looks like the badge should work just fine. Once we have the domain hooked in you can use whichever form you want.\nThe static image one is the one I'd put on github. If you have a link to slack on RTD you might be able to use the fancier one? Not sure if you can embed scripts w rst. I haven't seen a way yet, but there may be a way.\n. http://slack.pachyderm.io/ is live now\nSounds like you got the image badge working w the other address. This works now w the new address:\n\nOr in markdown:\n[![Slack Status](http://slack.pachyderm.io/badge.svg)](http://pachyderm.io)\n. So running the test model works fine. Run the small model doesn't. The latter generates a much bigger model file, and something about this makes FUSE unhappy.\nHere's what I know.\nA) The copy command (specified in the pipeline ... to get around a different FUSE error) causes a read error. I can see this from the job logs:\nshell\npachyderm:0$pachctl get-logs 9948c4710851160d90ca78017dbdc571\n0 | 2016-09-16T18:17:59Z ERROR fuse.FileRead {\"file\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"GoT_train\"},\"id\":\"4a226f03060e43ea80cc12b96c0e0050\"},\"path\":\"/ptb.ckpt\"},\"shard\":{}},\"error\":\"read more than we need; this is likely a bug\"}\n0 | cp: error reading '/pfs/GoT_train/ptb.ckpt': Input/output error\n0 | cp: failed to extend '/training_data/ptb.ckpt': Input/output error\n0 | W tensorflow/core/util/tensor_slice_reader.cc:93] Could not open /training_data/ptb.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n0 | W tensorflow/core/util/tensor_slice_reader.cc:93] Could not open /training_data/ptb.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n0 | W tensorflow/core/framework/op_kernel.cc:900] Data loss: Unable to open table file /training_data/ptb.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n0 | W tensorflow/core/util/tensor_slice_reader.cc:93] Could not open /training_data/ptb.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n... \n(much more of the similar error type)\nB) If I use pachctl CLI, I can do the same operation just fine:\nshell\n$pachctl get-file GoT_train 4a226f03060e43ea80cc12b96c0e0050 ptb.ckpt > ptb.ckpt.cli\n2016-09-16T19:43:11Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index\",\"err\":\"repo .metadata_never_index not found\"}\n2016-09-16T19:43:13Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index_unless_rootfs\",\"err\":\"repo .metadata_never_index_unless_rootfs not found\"}\npachyderm:0$du -h ptb.ckpt.cli \n19M  ptb.ckpt.cli\nC) If I try and use a local FUSE mount, it fails:\nshell\n$cp ~/pfs/GoT_train/4a226f03060e43ea80cc12b96c0e0050/ptb.ckpt .\n2016-09-16T19:11:25Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index\",\"err\":\"repo .metadata_never_index not found\"}\n2016-09-16T19:11:25Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index_unless_rootfs\",\"err\":\"repo .metadata_never_index_unless_rootfs not found\"}\n2016-09-16T19:11:31Z ERROR fuse.FileRead {\"file\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"GoT_train\"},\"id\":\"4a226f03060e43ea80cc12b96c0e0050\"},\"path\":\"/ptb.ckpt\"},\"shard\":{\"fileModulus\":\"1\",\"blockModulus\":\"1\"}},\"error\":\"read more than we need; this is likely a bug\"}\ncp: error reading '/Users/sjezewski/pfs/GoT_train/4a226f03060e43ea80cc12b96c0e0050/ptb.ckpt': Input/output error\nWhich points pretty conclusively at FUSE. Will dig further\n. Some updates. I dug further into FUSE, and I believe the difference between the CLI and FUSE mount is that FUSE will make separate calls to GetFile to buffer the file.\nSpecifically:\n- FUSE does a bunch of reads (via separate calls to GetFile() of size ~1MB\n- on the 9th such read we cross a block boundary\n- on that 9th call the first call to Read() on our reader returns a small amount of data (the remnants of the first block)\n- then the next call to Read() on our reader fetches a new internal block reader and starts to read that data\n  - the problem is that the new internal block reader is created w a size equal to that of the initial (1MB) read size \u2026 not taking into account the small amount of data that was read from the remainder of the previous block\n  - so we always \u2018over-read\u2019 by exactly the size of the remaining data in the first block \u2026 causing an error\n- what\u2019s odd is that the code is pretty similar to the old driver\n- so while I could add a field onto the reader to fix this problem \u2026 id rather have a simpler solution \u2026 so am trying to understand how we deviate from the old driver\n. Now that 1.2 has landed, the best way to launch is how @jdoliner described it above. I'm closing this issue, but please re-open if you're still having trouble\n. We saw a similar user bug report in the users channel, and derek tried to reproduce and couldn't, so we think it's been fixed on master. But we need to validate this.\n. Hmm. Now I'm seeing the opposite. CLI works and fuse mount doesn't:\nshell\n$GOPATH/bin/pachctl list-file GoT_train 98a783dc4ee84df295f995d9dd42c680/0\nNAME                TYPE                MODIFIED            LAST_COMMIT_MODIFIED   SIZE                \n/id_to_word.json    file                10 minutes ago      -                      19.57 KiB           \n/ptb.ckpt           file                10 minutes ago      -                      24.42 KiB           \n/ptb.ckpt.meta      file                10 minutes ago      -                      118.2 KiB           \n/word_to_id.json    file                10 minutes ago      -                      17.22 KiB           \npachyderm:0$ls ~/pfs/GoT_train/\n2016-09-23T16:39:58Z ERROR fuse.DirectoryReadDirAll {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"GoT_train\"}}},\"shard\":{\"fileModulus\":\"1\",\"blockModulus\":\"1\"},\"modified\":\"2016-09-23T16:28:58.479605893Z\"},\"error\":\"invalid header field value \\\"gorethink: Index `CommitBranchIndex` was not found on table `pachyderm_pfs.Commits`. in: \\\\nr.DB(\\\\\\\"pachyderm_pfs\\\\\\\").Table(\\\\\\\"Commits\\\\\\\").Distinct(index=\\\\\\\"CommitBranchIndex\\\\\\\").Filter(func(var_382 r.Term) r.Term { return r.Row.Nth(0).Eq(\\\\\\\"GoT_train\\\\\\\") }).OrderBy(func(var_383 r.Term) r.Term { return r.Row.Nth(1) }).Map(func(var_384 r.Term) r.Term { return r.Row.Nth(1) })\\\"\"}\nls: reading directory '/Users/sjezewski/pfs/GoT_train/': Input/output error\npachyderm:2$$GOPATH/bin/pachctl version\nCOMPONENT           VERSION                                          \npachctl             1.2.0-4479b2162946b9a88a09c8faf1e0bf181471859f   \npachd               1.2.0-4479b2162946b9a88a09c8faf1e0bf181471859f   ```\n. Oh, no its definitely a new bug. But found it while trying to reproduce the old one.\n. Oops thought I had:\nshell\n$ls ~/pfs/foo\n2016-09-23T17:10:22Z ERROR fuse.DirectoryReadDirAll {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"foo\"}}},\"shard\":{\"fileModulus\":\"1\",\"blockModulus\":\"1\"},\"modified\":\"2016-09-23T17:06:59.498612904Z\"},\"error\":\"invalid header field value \\\"gorethink: Index `CommitBranchIndex` was not found on table `pachyderm_pfs.Commits`. in: \\\\nr.DB(\\\\\\\"pachyderm_pfs\\\\\\\").Table(\\\\\\\"Commits\\\\\\\").Distinct(index=\\\\\\\"CommitBranchIndex\\\\\\\").Filter(func(var_222509 r.Term) r.Term { return r.Row.Nth(0).Eq(\\\\\\\"foo\\\\\\\") }).OrderBy(func(var_222510 r.Term) r.Term { return r.Row.Nth(1) }).Map(func(var_222511 r.Term) r.Term { return r.Row.Nth(1) })\\\"\"}\nls: reading directory '/Users/sjezewski/pfs/foo': Input/output error```\n. (Even a simple repo w a single file and commit will trigger this behavior)\n. Will try that now.\nIt's erring in list branch ... but that's pretty weird since we have tests for that\n. Ok blowing away fixed it. And the original issue is no longer present.\nWhat's weird is that I was re-launching before. So I'm not sure what state is persisting across launches ... but I'd like to get to the bottom of that. For now, some of the magic of rm /tmp/pach has returned ... and I don't like magic\n. Hi there @willguxy - sorry you ran into this.\nThis looks awfully similar to the bug we found in #833. (My bet is that all but one of your files is larger than a single block)\nI'm looking into issue #833 as we speak, and we should have a fix shortly.\n. We have a PR up now\n. stale. Also updated the pipeline.json spec to use new parallelism_spec field so that the example will run w whats on master\n. We should also generate a new integration test corresponding to the example\n. This is done and merged\n. We should also generate a new integration test corresponding to the example\n. We should also generate a new integration test corresponding to the example\n. This has been done and the test merged\n. stale. Validated that the example works (using the code from this PR)\nHave a PR open for adding a new unit test for the TF example\n. Opencv has been run through by Matt and Joey. Test still pending\n. Doc sidebar has been merged, and make asset has been re-run to make tests pass. I believe this covers this issue.\n. Yea I noticed that, but forgot to get the mac tests (they're easy to miss!). Should all be updated now.\n. \n. \n. \n. We consolidated the way we load the pipeline JSON, so this shouldn't be an issue anymore\n. Should just be a matter of updating the settings on:\nhttps://readthedocs.org/dashboard/pachyderm/versions/\nBut it'll help if the redirection is working, so we don't have to hardcode in a version in the links\n. \n. I all of this now works -- we have a 'version' available for each version we deploy, in addition to 'stable' (which should point to the latest versioned tag that is released) and 'latest' (which corresponds to HEAD). Yea, this seems like a valid concern to me. We could also have a flag like --insecure if a user really wants to use filenames\n. \n. Is this still in active development / whats the status?\nWe've been talking a lot lately about how we want to keep PRs short and frequent as opposed to long running. Maybe this is blocked on something else?\n. See https://github.com/rtfd/readthedocs.org/issues/2422\nI'm not sure its possible. We'll see if they respond. If not, we'll deal, and have to update the links once more when we implement #871 \n. This is done. Docs can now be reached at:\nhttp://docs.pachyderm.io/en/latest/\n. \n. That first one should be done already w the latest fruit-stand PR\nhttps://github.com/pachyderm/pachyderm/pull/883#discussion_r80266206\n. The easiest thing to do to consolidate the pipeline would be only have pipeline.json and have the example reference the file. That'll require making the readme (e.g. wordcount) a little less 'hand-hold-y' (instead of supplying  a command to copy paste that includes the meat of the pipeline ... it includes the filename). \n. Yup! After our slack discussion this makes sense\n. I think \n- the CLI test harness\n- and the object store test harness\nwill take some real big bites out of what you're looking for.\nSo lets figure out what remains to be tested that we'd like to see. It sounds like what you want to see is some real amount of load on the system. \nI think the simplest approach would be getting in the habit of writing tests like this one\n- As written, as a 'failing test' it's scaled in such a way that it overloads the server's capabilities. Specifically, so much so that we cause a test to timeout.\n- Once fixed however, the system can handle the amount of scale supplied in the test with no problems.\nWhat's interesting though, is how that scale changes. For this test (I think ... it may have been TestBigWrite or TestATonOfPuts) we ended up throttling the test down so that the refactor wouldn't hang. That information isn't really captured anywhere!\nThis starts to matter once we care about the degree of scale, and the tradeoffs we're making. I can see how it would be helpful to have a benchmark suite to test many aspects of the system (e.g. Read speeds, Write speeds, Deletion speeds, best / average / worst case workloads, jobs that require resources that are correlated to data input size, jobs that don't, etc) to be able to get a snapshot of the system.\nBut I'm wary of building that without clear goals. I think that would be a great tool for us to:\n- understand the system's ability to scale and the 'shape' of that scale\n- internally make design decisions about scaling and tradeoffs\n- be rigorous about our performance at scale release over release\n. duplicate. We're using k8s 1.4 at this point. I didn't see anything specific in that link about making deployment easier.\nWe use kops + pachctl deploy now which gets us pretty far. Is there something more specific you had in mind @derekchiang ?. That sounds good, but I have a request:\nI want a way to record the results of the benchmark\nSo that in the future when we say \"We ran a benchmark testing heavy writes ... and it performed well\" we know exactly what was tested, and what 'well' means, so that it's repeatable. It would be super great if we could use this information to configure and re-run a benchmark. But at the very least, we should record enough information so that we can manually re-create it. Maybe this 'benchmark report' isn't done every time you run a benchmark. Maybe its a totally manual process. But I think getting most of this info is pretty easy to automate and would make the benchmark results salient in the short term and long term.\nI think the following information is necessary:\n- the images being used (great that you're already setting up a registry / private DH acct ... can just be an image hash)\n- the pipeline(s) manifests (if any) being used ... and if they're not ... any pachyderm specific config (e.g. # nodes, size of storage for rethink, etc)\n- if pipeline(s) aren't used directly, then a reference to the exact code being run (maybe its an image, maybe just a commit hash and golang benchmark name a la BenchmarkManyWrites (which refers to the BenchmarkManyWrites(b *testing.B){} function in the code)\n- the state and scale of the cluster\n  - AWS vs GCE\n  - k8s version\n  - docker version\n  - number of VMs\n  - shape of VMs (esp if they're heterogeneous)\n  - any other relevant config (e.g. k8s auto-scaling flag)\n- the results of the benchmark\n  - total run time?\n  - the metric may depend on the thing we're measuring. E.g. for heavy writes we probably care about writes per second\n  - we may also want to record other health stats (as soon as we have them) about performance. Things like:\n    - CPU used per VM (cpu graph over the timespan of the benchmark)\n    - mem usage (again over timespan of benchmark)\n    - I/O (local disk and network)\n    - other pachyderm specific stats we're going to add\n. That makes sense, and I think text and git are adequate output and storage mediums.\nI'm assuming in that case the amount that we scale the VMs in the cluster we generate is hardcoded into the script (and you commit each change you make). I think there will be cases where we want different size and shaped clusters. But we can cross that bridge once we get there.\n. stale. stale. This PR should have coverage once we have support for obj stores:\nhttps://github.com/pachyderm/pachyderm/pull/1215\n. duplicate. referenced in aggregation issue. As we see more customer implementations / ingress automation, having proper exit codes is important for scripting.. Usually I'd say at the time of PR it should be in a non-scripting language. But it sounds like you want to run w this to try and use it to run actual benchmarks before finalizing it. I'm ok w that in the spirit of 'more frequent PRs' that we've discussed. I'd just request that we make a new GH issue in 1.3 to remember to get to this.\nOther than that, LGTM if @jdoliner is good w the PullPolicy stuff\n\n. I asked @derekchiang - I don't think we use this at all anymore. We can remove this file. \n. The short answer is just my OCD :-P\nOrganizationally, I think it makes sense to keep the tests for examples alongside the examples.\nBut doing this and trying some of the obvious things for vendoring it wasn't super straightforward. Also I think there are some edge cases w running all tests w vendoring / symlinks / and the Makefile. \nSooo .... I don't think its worth the trouble.. Haha nice\n. This PR addresses the first part\nFor a plan of attack for identifying unique users:\n- We'll do so via pachctl only (not via the client code)\n- We'll provide an opt out mechanism\n- We'll store/create the unique at ~/.pachyderm/config.json (which will be backed by a protobuf)\n- We'll pass this value as part of the context for all requests to the cluster\n- Which will allow the cluster to report the metrics (and associate user actions w a cluster)\n. Also - we agreed to write up a 'semantic conventions' doc for contributors outlining when to put a field on a context vs a request proto object.\nGenerally, anything needed for a 'session' should go in context, whereas functional things (or things that can affect execution patterns) should go into a proto obj\n. This is a great doc to reference in our writeup: https://medium.com/@cep21/how-to-correctly-use-context-context-in-go-1-7-8f2c0fafdf39#.77fbkc67x\nSpecifically talks about how context informs, but doesn't change behavior\n. Also - we'll report only one metric in pachctl and that'll be when a user calls pachctl deploy ... in which case we can look at the --no-metrics flag for that call to Do The Right Thing\nThat'll help us see how many users are trying to startup clusters but never succeed.\n. In the case that the client is older than the server, we can handle this case server side. In this situation, the server will have metrics enabled, but will not receive a UUID from the client, so shouldn't report metrics.\nIn general, we don't have an upgrade policy around client/servers. If we're ok with requiring users to upgrade, we could instead return an error message if we don't receive the user data. But that seems heavy handed for this use case.\n. The PR landed, so this issue is complete. Yea, this is some outstanding tech debt. We haven't gotten to it, but it would up level our debug tools.. Cool, thanks for the extra notes\nAlso, metrics can have overlapping but different goals. Those are A) a debugging tool and B) a tool to understand how our software is used and the limits at which it breaks\nUsing segment definitely should satisfy B and could in theory also satisfy A. In contrast, if we bundled a promtheus/grafana configuration as part of our k8s manifest, it would be a good debug tool (A) ... when we could get access to it ... but would not provide any help at all for B.\nIf we had to choose, I think we value B (understanding usage) over A (pure debugging tool), but in theory segment gives us both.\nIf we learn along the way that we won't be able to send everything to segment, we could always double report / report to different services (one for metrics and usage, one for stats for debugging).\n. I'm picking this up again. To re-state the use case:\n\"As a user, I want to see visual statistics that inform me how my jobs are running and where they're hitting walls\"\nSome specific stats we want to measure + graph:\n\ngraph of datum download / run / upload times (\"oh ... look ... that one datum out of my ten total had a spike in run time ... let me grab it's ID so I can debug further\")\ngraph of job run times (\"oh yesterdays job was fine ... but today's job took 10x longer\")\n\nIteration 0\n\nstandup prometheus + grafana deployment to scrape/store/display these app stats\nreport a single type of app stat so that we can see it displayed in grafana\n\nIteration 1\n\ninstrument code to fill out other places where we want app stats reported\nexamples include\ndetailed stats per datum and per job\nstats per API call on the PFS side cars (maybe # of api calls / load times per call ... so we can see if a particular side car is getting too much load ... which could happen depending on the datum shape)\n\n\n\nFuture Iterations\n\nAdd mechanism for users to report custom app stats (See #228 )\nWe'd probably want to report/scrape app stats in flight\nEither we expect them to write prometheus gauges in their user code ... OR perhaps\nWe have something like /pfs/stats which is a pipe so that we can listen for stats, and send them along via the prometheus client\n\n\nMake pachyderm a first class grafana plugin\nWhich would allow us to bypass prometheus, but more interestingly to users ...\nWould allow output from a pipeline to be graphed by grafana ... which would be very handy for users trying to create reports / dashboards. BTW ... good trade off btw graphite / prometheus for the non UI parts of this stack:\n\n\n\nhttps://www.loomsystems.com/blog/single-post/2017/06/07/prometheus-vs-grafana-vs-graphite-a-feature-comparison\n. OK a bunch of stats stuff has landed. Now we:\n\nrecord application stats for pipelines for all worker pods (if enterprise is enabled)\nrecord all API calls for the pachd instance\nreport the groupcache stats\n\nRemaining iterations / todos:\n\nsupport pachd sidecar API calls\nid have to have more than one prometheus HTTP server on this pod, which requires reconfiguring how prometheus looks to discover services to scrape\nprovide deployment via helm chart\nright now we just have a make task to do deployment\nafter much discussion, we dont want to support this via pachctl deploy ...\nthe right way to support this is to add / configure it as part of our helm chart\nallow hook for custom user metric reporting\n. L\nG\nT\nM                                                                      \n. This change landed. Also - for posterity -- note that I pushed to CI w just the failing test ... and it failed. Especially when we have a test for a regression, I want us to get in the habit of doing this. This way, we're sure that the test is doing the right thing.\n. A few more things we might want to add:\n\n1) PRs must come w docs updates for functionality that is updated, removed, or added\n2) Did we land on a convention for trailing whitespace? (Specifically this came up w docs for @msteffen recently) ... and having a convention lets us have cleaner diffs\n. I agree its apparent, but I think this could still cause problems. Specifically for docs.\nThis is how (we think) it came up in the PR that Matt was working on:\n1) Someone edits docs, their editor may or may not have added trailing whitespace, but it was added to many/most lines in a file\n2) When that PR was submitted, this was net new content, and so was very hard to tell if there was trailing whitespace (I'm not sure the GH diff view shows trailing whitespace)\n3) Matt's branch had those changes in its commit history\n4) Matt happened to need to update that documentation file ... and his editor does the Right Thing and removes trailing whitespace\n4) The PR that Matt submitted then was really noisy around that doc file ... it was hard to tell which lines were really modified since the diff showed all lines being modified\nThis makes it really hard to actually validate that doc updates look good. And because we don't enforce it, it could easily happen again if someone adds net-new docs w an editor that has trailing whitespace.\nSo - this is specifically an issue around docs. I'm pretty sure gofmt does the right thing.\nIt should be easy to update make lint to check the docs files for trailing whitespace. That way if someone introduces it, the build will error.\nEither way, sounds like this is outside the scope of this PR\n\n. This landed recently. But is not documented :-/ (but there's an open issue for the docs). we now rely on the vet/linting from go1.10. ah, just read the title. go1.10 won't change anything in docs. But this issue is stale, so im keeping it closed. Great! I'm glad you got a workaround.\nSo to make this clearer next time this happens, we want to:\n- report a clearer error message\n- create a startup suite that issues any requests pachd might make to the k8s client\n- so that if we don't have access, we fail on startup\n. stale. An update from our triaging late last week --\n@willguxy was able to show me some logs during which time he was not initiating any requests to the cluster.\nThose logs indicated that pipelines were trying to create jobs, failing, and backing off. The other API calls (e.g. ListCommit and InspectRepo) were symptoms of this pattern. The pipelines will back off when attempting to start a job, but eventually this would fill the disk.\nWe do see an error a la:\nerror from InspectJob: gorethink: dial tcp 10.0.0.148:28015: getsockopt: connection refused\nWhich suggests that we cannot connect to rethinkdb. At this point, we suspect this as the root cause. If we cannot connect, job creation will never work, and logs will pile up and accumulate. This will exhaust resources in this deployment.\nIn general, that shouldn't be the case. I know k8s / GCP integrate nicely to do log rotation, aggregation, etc to report to a UI on the GCP console. I'm not as familiar w the AWS config, but I'd bet that they have some similar configuration options.\n. We now have a 'make launch-loggingtask that will spin up the EFK stack (fluentd aggregator, elastic search backend + index + serving, kibana frontend) for log aggregation. If your logs are taking up a lot of space, we recommend spinning up that service.. Oh weird ... it seems like the commits were created, but were archived? I noticed this when trying to do this operation manually, and getting back commit idmaster/7`:\nshell\n$pachctl start-commit foo master\nmaster/7\n$pachctl list-commit foo -a\nBRANCH              REPO/ID             PARENT              STARTED             FINISHED            SIZE                \nmaster              foo/master/0        <none>              6 minutes ago       6 minutes ago       0 B                 \nmaster              foo/master/1        master/0            5 minutes ago       5 minutes ago       0 B                 \nmaster              foo/master/2        master/1            5 minutes ago       5 minutes ago       0 B                 \nmaster              foo/master/3        master/2            3 minutes ago       3 minutes ago       4 B                 \nmaster              foo/master/4        master/3            2 minutes ago       2 minutes ago       4 B                 \nmaster              foo/master/5        master/4            2 minutes ago       2 minutes ago       4 B                 \nmaster              foo/master/6        master/5            2 minutes ago       2 minutes ago       4 B                 \nmaster              foo/master/7        master/6            10 seconds ago                          0 B                 \n$pachctl list-commit foo \nBRANCH              REPO/ID             PARENT              STARTED             FINISHED            SIZE                \nmaster              foo/master/7        master/6            25 seconds ago                          0 B\n. Running with versions:\nshell\n$pachctl version\nCOMPONENT           VERSION                                          \npachctl             1.2.1-a8a6a98f4cb074014ebce6d2cdb03877bc398a92   \npachd               1.2.1-a8a6a98f4cb074014ebce6d2cdb03877bc398a92\n. Hmmm if I blow away the repo, and run the command it works:\nshell\n$echo 'foo' | pachctl put-file foo master -c /blah -f -\n$pachctl list-commit foo \nBRANCH              REPO/ID             PARENT              STARTED             FINISHED            SIZE                \nmaster              foo/master/0        <none>              1 seconds ago       1 seconds ago       4 B\nBut I was fiddling w that putfile command. If I blow foo away again and use one of my malformed ones, we see the error:\nshell\n$echo 'foo' | pachctl put-file foo master -c -f blah -\nopen blah: no such file or directory\ndag:1$pachctl list-commit foo\nBRANCH              REPO/ID             PARENT              STARTED             FINISHED            SIZE                \ndag:0$pachctl list-commit foo -a\nBRANCH              REPO/ID             PARENT              STARTED             FINISHED            SIZE                \nmaster              foo/master/0        <none>              10 seconds ago      10 seconds ago      0 B\n. So ... while it makes sense that if an individual put file fails the commit that was created will be finished and archived ... in the broader flow (a user such as myself trying to get the putfile command working) this can be confusing ... because the 'parent' commits on master will be archived ... and so my commit will also be archived.\n. You're correct, they're cancelled.\nTo summarize:\n- If a user incorrectly does a PutFile() it can create a cancelled commit\n- If a user then does a correct PutFile() w the branch shorthand, it'll create a commit whose parent is cancelled\n- So the commit won't show up with a normal ListFile\nWe agree this behavior is confusing, but this behaves as expected w the current implementation. Soon we may remove the idea of 'cancelled commit's and this issue will go away.\n. This is way cool!\nI'd like us to get us in the habit of framing PRs from a user perspective (when applicable of course). If I was to take a swing at this one, I'd say something like:\n\"As a user ... having a private registry that pachctl updates when I create/update a pipeline allows me to seamlessly update my pipelines when my code is under active development\"\nIs that about right?\n. That makes sense. Especially since we want to get PRs to be in as small / digestible chunks as we can. And those won't always correspond to 'user facing things' in and of themselves.\nOne nice side effect of framing it for a user is that it highlights how much work we're doing is directly user facing vs technical debt etc.\nAnd by 'design phase' you mean an outline/discussion on the GH issue right? I know sometimes we use gdocs which makes sense for a discussion, but since our code (and GH) is our interface to our users, I think it's worth getting in the habit of framing things here on GH (even if only in summary).\n. We have cpu/mem requests in place now, so I think this accomplishes what we wanted here.. There are a few more changes I want to make before this is ready for final review:\n- I added the flags for pachctl deploy and pachctl version to opt out of metrics\n- But I think we discussed having pachctl deploy --no-metrics also deploy a manifest that has them disabled ... that isn't hooked in yet\n- I'm cleaning up the timing info reported via the stats\n- And I want to include a mixpanel screenshot of some things we'll see from these stats ... as a final step that its reporting and useful as designed\nWhen its ready, I'll update the PR w a synopsis of the changes too\n. Oh, and we agreed that any metrics errors should be non fatal. I need to go through and make sure that's true. In those cases (at least on the server side) we should log the error message\n. Tests are in. Just waiting on CI to pass now.\n. Good discussion w Matt + Joey\nInspecting a commit shows you the provenance. There may be a way to include the image name/hash per commit. Not necessarily the image is reported for each of the commtis included under 'provenance' but at least for the commit that is being inspected.\nIf we showed the image name + hash for inspect commit, a user could construct a complete snapshot as needed.\n. Ok, so we can provide basically all of this functionality today, just not in a single command. But that's ok. As long as you can re-create the state (and all of the processing), the rest is just better tooling. \n\nUse Cases\nAs an example, I've run the fruit stand demo to show 2 use cases:\n1) Given a complete set of input commits, return all of the output commits + jobs\n```\nFirst get all of the output commits of the input commit(s)\n$pachctl flush-commit data/master/0\nBRANCH                             REPO/ID                                     PARENT              STARTED             FINISHED            SIZE              \nmaster                             data/master/0                                             About an hour ago   About an hour ago   874 B             \n4fcdd66a9cb240d49580e65e9ae71f3a   sum/4fcdd66a9cb240d49580e65e9ae71f3a/0                    About an hour ago   About an hour ago   12 B              \n5581dee7f0d94c44a1d82e75270ba8bc   filter/5581dee7f0d94c44a1d82e75270ba8bc/0                 About an hour ago   About an hour ago   200 B            \nThen for each of the output commits, look for the job ID that created the commit:\n$pachctl list-job\nID                                 OUTPUT                                      STARTED             DURATION            STATE             \nadb86255ce181d7e7719efda3f203951   sum/4fcdd66a9cb240d49580e65e9ae71f3a/1      About an hour ago   3 seconds           success  \n609440a680caf705c3f3b451d70b732c   sum/4fcdd66a9cb240d49580e65e9ae71f3a/0      About an hour ago   3 seconds           success  \n4ab2d81bf69bd012096aa7d95cc57467   filter/5581dee7f0d94c44a1d82e75270ba8bc/1   About an hour ago   6 seconds           success  \n67c30d70ba9d2179aa133255f5dc81db   filter/5581dee7f0d94c44a1d82e75270ba8bc/0   About an hour ago   7 seconds           success    \nAnd using that job ID inspect the job to get the all of the transform info for the job\n$pachctl inspect-job 609440a680caf705c3f3b451d70b732c\nID: 609440a680caf705c3f3b451d70b732c \nStarted: About an hour ago \nDuration: 3 seconds \nState: success\nParallelismSpec: constant:1 \nInputs:\nNAME                COMMIT                               PARTITION           INCREMENTAL       \nfilter              5581dee7f0d94c44a1d82e75270ba8bc/0   FILE                DIFF              \nTransform:\n{\n  \"cmd\": [\n    \"sh\"\n  ],\n  \"stdin\": [\n    \"for fruit in apple orange banana; do\",\n    \"   { cat /pfs/prev/$fruit || echo 0; cat /pfs/filter/$fruit; } | awk '{s+=$1} END {print s}' > /pfs/out/$fruit\",\n    \"done\"\n  ]\n}\n```\nThis is manual, but allows you to completely reconstruct all input/output commits, and the image is referenced in the output of the inspect-job call. However, we don't currently report the hash of the image, and that is something we probably do want to support. (If you've only specified your image w names or name:tag, we wont necessarily be able to tell exactly which version of the image you're using. On the flip side, if you do need that level of granularity you can refer to an image by its hash and this will work. But we want to make this better. This is listed in the todo section below).\n2) Given a Pipeline's output commit somewhere on the DAG, provide a list of all the commits + jobs upstream of this commit\n```\nFirst find the commit you care about. In this case we'll choose the latest on the sum repo\n$pachctl list-commit sum\nBRANCH                             REPO/ID                                  PARENT                               STARTED             FINISHED            SIZE              \n4fcdd66a9cb240d49580e65e9ae71f3a   sum/4fcdd66a9cb240d49580e65e9ae71f3a/0                                  About an hour ago   About an hour ago   12 B              \n4fcdd66a9cb240d49580e65e9ae71f3a   sum/4fcdd66a9cb240d49580e65e9ae71f3a/1   4fcdd66a9cb240d49580e65e9ae71f3a/0   About an hour ago   About an hour ago   12 B                \nInspect that commit to get all commits in its provenance\n$pachctl inspect-commit sum 4fcdd66a9cb240d49580e65e9ae71f3a/1\nCommit: sum/4fcdd66a9cb240d49580e65e9ae71f3a/1\nParent: 4fcdd66a9cb240d49580e65e9ae71f3a/0\nBranch: 4fcdd66a9cb240d49580e65e9ae71f3a \nStarted: About an hour ago\nFinished: About an hour ago \nSize: 12 B\nProvenance:  filter/5581dee7f0d94c44a1d82e75270ba8bc/1  data/master/1  \nFor each of the commits listed that are the output of a pipeline, find the corresponding job\n$pachctl list-job | grep filter/5581dee7f0d94c44a1d82e75270ba8bc/1\n4ab2d81bf69bd012096aa7d95cc57467   filter/5581dee7f0d94c44a1d82e75270ba8bc/1   About an hour ago   6 seconds           success             \nInspect the job to get the job/image info\n$pachctl inspect-job 4ab2d81bf69bd012096aa7d95cc57467\nID: 4ab2d81bf69bd012096aa7d95cc57467 \nParent: 67c30d70ba9d2179aa133255f5dc81db \nStarted: About an hour ago \nDuration: 6 seconds \nState: success\nParallelismSpec: constant:1 \nInputs:\nNAME                COMMIT              PARTITION           INCREMENTAL       \ndata                master/1            BLOCK               DIFF              \nTransform:\n{\n  \"cmd\": [\n    \"sh\"\n  ],\n  \"stdin\": [\n    \"for fruit in apple orange banana; do\",\n    \"   grep $fruit /pfs/data/sales | awk '{print $2}' >>/pfs/out/$fruit\",\n    \"done\"\n  ]\n}\n```\n\nTodo\n1) Update the docs to provide these common debugging workflows as examples\n2) Update JobInfo to store the hash of the image (we'll need to see if we can get the hash by querying the docker registry somehow ... seems like we should be able to get it)\nIn the future, I think this will make a great tool for our web UI. There is also a case to be made that you might want a command to take a 'snapshot' as input and re-run something. But that is loosely defined right now, so I'll defer that decision until that use case becomes clearer.\n. We discussed this again last week. That discussion centered around the hash of a docker image and whether or not we could easily expose this information.\nThe goal here is for a user to be able to reproduce all of the data and the exact version of the code that was running to create the output data. \n@jdoliner keep me honest - but from what I remember I think our conclusion was:\n1) Support exposing the image hash per job\n\nwe think we can get the image hash from the k8s job API\nif this is easy, we could expose this information in pachctl inspect-job\n\n2) Recommend conventions around versioning images and development\n\nIn development, use pachctl update-pipeline ... --push-images which will automatically create a unique tag for your image each time you do an update\nIn production, you'll care a lot more about which version of the code was deployed. In this case, use semantic versions + docker tags\n\nBoth of these cases should be documented\n. The new migration feature marshalls a whole cluster into a file, thereby snapshotting it.. There's a broader issue open for defining different errors for the go client.. This looks like it was fixed:\n$GOPATH/bin/pachctl list-file bar master\nNAME                TYPE                MODIFIED            LAST_COMMIT_MODIFIED   SIZE                \n/Dockerfile         file                2 minutes ago       master/1               903 B               \n/Makefile           file                3 minutes ago       master/0               13.33 KiB. PPS only finishes a commit after all pods have finished, which means they have also finished any put-files, so the only way to trigger this is as a user doing 'manual' put-files.\nSo we think this is an edge case, and we can de-prioritize.. I believe this is solved w the upcoming 1.7 model. flush-commit blocks until commits are finished, and even failed jobs will finish commits.. stale. I'm also seeing a lot of lines like:\n2016-11-01T20:34:20Z INFO  protorpclog.Call {\"service\":\"health.Health\",\"method\":\"Health\",\"duration\":\"0.000001291s\"}\nI see about 20 of those per second, and those messages are about 4KB, so that means we have roughly 6GB of logs / day just from this log type\n6gb / day = 4KB * 20 messages/s * 60 s/min * 60 min/hr * 24hr/day   /   1024 kb/mb   /   1024 mb/gb\n\nEdit: I realized these keep alive / health checks are only coming from a FUSE mount. So this message type would log at that rate only while a user FUSE mount is running. Still may want to tune that down\n. Additionally, we discussed how the UI would consume logs either via the pachctl get-logs API or a logging service.. stale. @derekchiang -- can you update this issue in light of 1.4?. We also got a PR for submitting our brew formula's to the core tap:\nhttps://github.com/pachyderm/pachyderm/issues/598\nAnd if we're overhauling releasing, it would be a good time to look at supporting the other changes needed for that PR\n. The script we have now is good enough. And now I think most of the team has used it to push. So I'm closing this.. stale. Yea it definitely used to be. I think we broke it w the refactor. Taking a look now\n. \n. That error smells like something changed under our feet. We're using just tensorflow/tensorflow as the image ... when we probably should use a specific version (or better yet re-host the version we like).\nThis thread corroborates this theory: https://github.com/sherjilozair/char-rnn-tensorflow/issues/47\n. Yup that all sounds right.\nMatt asked me if he could disable it temporarily. Since I didn't have a working fix yet, that's ok with me.\nI was getting blocked by dockerhub DDoS DNS issues today ... all I needed was that silly version number ... but I had to wait/hit DH a bunch to get it. I have a build pending on CI now w the fix. (Works locally). So if its not urgent, its probably easier to wait for that to land. Sorry for the racy-development @msteffen \n. This is what make dev-launch does, no?\nThis is stale, so closing.. I notice that we removed all the log statements that report timing at the end of the request. What is the reason for that?\nIn #987 I understood the request to eliminate the redundancy between services pachyderm.pfsserver.API and pfs.API for instance (whose messages are otherwise redundant as far as I can tell other than that service name)\nIf you also eliminate the timing info you'll reduce the size by a factor of 4 instead of 2, so I could understand that desire.\nIf we are removing that in lieu of stats support for request timing (specifically the prometheus/grafana stats we haven't quite built yet), I'm ok w that ... but it does make me uncomfortable. I want to be clear about some utility we get from those logs.\nI have found it useful to be able to see this information in the logs when debugging. Even if it was also reported in stats ... its really helpful to be able to correlate debug logs w certain statistics (bcz logs inevitably have more information about the nature of the request being made). Perhaps one alternative to meet this requirement (coordinating stats and logs) would be to report a unique request ID to both ... but normally stats services don't report individual requests that well?\nBut - as long as we agree we want to correlate them in the future, I'm good w this.\n\n. Small comment, otherwise\n\n. Looks like we have to set region and a constraint:\n$aws s3api create-bucket --bucket fwqgahds --region us-west-2 --create-bucket-configuration LocationConstraint=us-west-2\n{\n    \"Location\": \"http://fwqgahds.s3.amazonaws.com/\"\n}\n$aws s3api get-bucket-location --bucket fwqgahds\n{\n    \"LocationConstraint\": \"us-west-2\"\n}\n. @msteffen and @JoeyZwicker I know you looked at AWS deployment after I ran through it. Have we made these updates to the docs?. @msteffen and @JoeyZwicker I know you looked at AWS deployment after I ran through it. Have we made these updates to the docs?. Is there an open issue for getting rid of port-forward? I remember we talked about it, but I can't seem to find it.\nI'd like to reference this issue in that one (the one about removing port forwarding), and push this issue out of 1.3. Closing this, as our AWS deployment method + docs have all changed. Any doc updates from the port forwarding update can be captured in #1169 \n. \n. Hmmm we went from about 14m pre refactor to above 20 post refactor. 30+ is starting to get pretty long for a test suite. This PR is fine to unblock us, but let's definitely open an issue to address the speed.\n. I think this is all implemented. Very cool.\nHaving compatibility w certain libc versions isn't unreasonable. And if a use case comes up where a user needs a binary for an alternative distro, we can cross compile a new target.\n. \n. This PR doesn't add all the things mentioned in #710 but definitely creates a framework to build out the rest of the tests in #710 \nSo in the spirit of smaller PRs, if this is passing let's get a review + merge it, even if #710 isn't completed. Its more than just that prefix. The links right now point to somecommand.md when they should point to somecommand.html\nThere is a problem with how we're rendering markdown on RTD.\nThis was working in the past because we had a manually updated doc/pachctl/pachctl.rst file that mimicked the auto-generated one.\nI've opened an issue on RTD so we'll see what they say.  Hopefully I'm just doing something silly. If not, we'll probably just run with the manual curation of the rst file.\n. rethink is no more. no longer relevant ... all integration tests are run in sequence now. Hmm but wouldn't we need those PRNG to be persistent across databases? (Or keystores, if we replace 'rethink' w 'etcd')\nThis would be helpful to make CI a bit less jittery, but only if it persists across data stores I think.. We made integration tests run in sequence, which has made them relatively more stable. Since this seems to ameliorate the issue, and this issue is stale, I'm closing it.. @jdoliner 's new migration PR does this. I assume this issue is still relevant if we s/job\\-shim/worker/g\nIs that right? @jdoliner @derekchiang . nice to have + stale = closed. Yup!\n. we now use pachyderm/etcd:v3.2.7. This PR is just some vendoring upgrades, but they make it so protobuf compilation works again... which it does not now\n. \n. Yea, that library looks like the right approach. He has a great blog post about why\n. To support YAML we need to:\nUnmarshal :\n\nfor pachctl create-job\nfor pachctl create-pipeline\n\nMarshal : \n\nfor pachctl deploy ...\n\nHowever, since we're using backing these structures with protobuf objects, and since the stdlib json encoder/decoder don't work w protobuf objects I don't think that library will work as is. I don't think it would be too hard to extend that library to use the jsonpb codec instead of the stdlibrary one, but I'm not sure\nI opened an issue to ask: https://github.com/ghodss/yaml/issues/17\nI also took a look at how k8s handles json and yaml. They use the ghodss library, but there is a lot of abstraction. I'm mostly leaving these breadcrumbs here for myself for future reference:\n\nTheir create command uses a builder abstraction which seems to handle json or yaml ... not sure about this step\nThey have some helpers for yaml decoders ... but those use the above library which means they use the stdlib json decoder\n\nSo I don't think their approach is compatible with ours.\n( Maybe this is why k8s has those crazy types e.g. IntOrStr https://github.com/kubernetes/kubernetes/blob/8fd414537b5143ab039cb910590237cabf4af783/pkg/util/intstr/intstr_test.go)\nThis wasn't the slam dunk we thought it would be. So we'll wait to hear back on the jsonpb support, or hear more requests for this feature.\n. Good points all.\nThis code evolved from playing w rethink in different ways to see if there was another call (e.g. if we could read the table list from the db) we could use as a proxy for 'database readiness', but there's no substitute for the real thing. \nWill make these changes.. Ok, ready for another look @jdoliner . Yea, that was a very silly mistake. Of course the slice changes over loop iterations.\nIt was working by coincidence.\nhttps://play.golang.org/p/VFFsEJdyfD\nNotice that on the second and third iteration I get the same value for val. Then it would error because that table didn't exist, and the backoff function would get called again w just [Commits] for the value for tablesToInitialize\nI'm not sure tbh why the loop behaves that way. It makes sense to me why the second value would be '3', but not why the 3rd value would be '3'. . I don't think we planned on having / building finder support for fuse mounts.\nFYI - this is likely to do w how we handle the weird metadata files that os x wants (the spotflight and '.' files). I believe this was a bug. This is working now.. \n. stale. This could be part of https://github.com/pachyderm/pachyderm/issues/2687. I'm happy to make the interval longer.\nAnd I agree -- we don't want to adversely effect memory, esp since that will be a bottleneck for other reasons.\nBut - the point of metrics is to learn what the state of the system is at scale. So specifically for big workloads, I want as much information as I can to understand how the system is being used and is creaking.\nI also think that understanding file counts and diff counts could be valuable. It may be interesting to see that ratio and how that ratio differs across users' clusters.\nFor now, if this solves an issue, I'm ok w the fix. But please open a new issue to add these metrics back in a more efficient manner.\n\n. Yea, we could do that.\nThat would be an interesting measure, since multiple users could be touching a single cluster. The pipelines that were run would report the user that last updated the pipelines.\nIf there are lots of updates, that's a great indicator of usage. If the DAG is a 'long running' production DAG, it may be less useful. But ... we care about understanding the dev process right now, so it sounds reasonable to me. I'm sure we could also tag them to differentiate 'manual' user usage from 'automated' user usage.\n. Oh, yea that's really easy to enable. Is that what you meant @jdoliner?\nInstead of doing client.NewFromAddress() for pachd and jobshim main.go,\nwe'd basically just do client.NewMetricsClientFromAddress()\nStill ... we'd probably want some way of denoting that its a cluster not a\nuser issuing the calls.\nOn Mon, Nov 28, 2016 at 5:51 PM, Derek Chiang (Enchi Jiang) \nnotifications@github.com wrote:\n\nI think by \"user\" JD just meant the cluster. Like if you run a lot of jobs\nin a cluster, you want the metrics to go to the cluster's user ID.\nWould that be easy to do?\nIf we don't do that, then wouldn't we only be collecting metrics for\ncommand line operations? Like only when someone manually put a file, etc.?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/pull/1106#issuecomment-263452973,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAkfwbi1kFJUi0KL6rtTHVFqgAt09Dz-ks5rC4UbgaJpZM4K-aiF\n.\n. Opened an issue (#1113) to capture the extended feature request.\n. Hey @derekchiang can I get a review on this PR?\n\nI think we all agree the extended features are good to have. If we think they're necessary, that just means we'll prioritize that work near term. . Ok this is ready for review @jdoliner . For some reason I was fixated on not passing the argument in the negative form (passing metrics instead  of noMetrics as an argument). But I tossed the baby out with the bathwater.\nClosing this in lieu of:\nhttps://github.com/pachyderm/pachyderm/pull/1116\n. Metrics today collects user information based on commands issued from the CLI + cluster information.\nRight now any calls from jobshim (including fuse mounts created by jobshim) are not tracked at all. Same for pachd (any requests to ListPipeline or ListRepo that are called periodically).\nWhat we have today w the user information is enough to establish some of the roadblocks in getting pachyderm up and running (e.g. we track pachctl version and pachctl deploy from the client so that if the server isn't even up ... we can see where users are struggling). Once the server is up, and API calls will be tracked/sent from the server. This will include things like pachctl update-pipeline... so while we won't see exactly how the pipelines are using PFS, we do get a good idea of the 'user development funnel' in broad strokes.\nThat said, of course we want to track more!\nOh, and if a user does pachctl mount ... any calls that the local fuse mount makes is tagged to that user. Which may be another interesting data point for a 'deployment' funnel.. Closing, job-shim is gone. Also -- I need to switch out the image I use in the tests (for a version we host). Our default image doesn't seem to have netcat installed\n. Good call. Docs are in a separate PR : https://github.com/pachyderm/pachyderm/pull/1156\n@jdoliner got a passing build, can I get another look? Basically just upgraded the test.. Good call. That will definitely generate a port conflict error. Re-assigning to me until I have this updated.. Interesting. Can you expand on cases where this might happen.\nIs there a reason you couldn't do something like:\nsudo mkdir /pfs\nsudo chown `whoami` /pfs. Stale\nWe have a few new issues around local user workflows that have an eye towards easier development.. There are a lot of cases to cover here. Feedback like this is going to be required if long term we expect our users to never touch k8s. \n. There is an analogous use case here w database dumps. A periodic dump can accumulate in size quickly. For database adapters we recommend using a db cursor. These are recommendations in theory. We haven't built one of these ourselves.\nMarking as stale until we a use case from a paying user.. I believe this is outdated since the cross/union refactor. We've seen this in a few instances. This is a very concrete example (non job data leaking, etc).\nIn cases where we have cleaned the disk I have seen clusters recover. Sometimes if k8s was thrashing enough (creating/evicting many pods) it takes a while ... or you can delete the evicted pods and it goes faster.\nEither way, I do want better tooling for this for the user for cases we can't anticipate. This issue I think is the most pragmatic method: https://github.com/pachyderm/pachyderm/issues/1389. Hey @dahankzter - thanks for the heads up.\nSince that was a custom release, it didn't include some of the build artifacts by default, but we provide the release binaries through homebrew / a public URL for the deb package. I've updated the release notes, but basically your options are:\n```\nFor OSX:\n$ brew tap pachyderm/tap && brew install pachctl\nFor Linux (64 bit):\n$ curl -o /tmp/pachctl.deb -L https://pachyderm.io/pachctl.deb && sudo dpkg -i /tmp/pachctl.deb\n```\n. You're right ... the binary URL referenced in the homebrew formula was broken. This should be fixed now.\nFWIW, to install the latest pachctl, I usually do the following:\nbrew untap pachyderm/tap\nbrew tap pachyderm/tap\nbrew unlink pachctl # if you have an existing version\nbrew install pachctl\nThat should make sure you have the latest formula, and will get the latest version. If you have a better workflow, I've love to hear it!\nPlease let me know if homebrew is working for you.. Cool. Made those changes. Yup @gg waiting on a +1 from you before I click the merge button.\n. Oh man, I came here to make that same comment. You're a step ahead! Nice!. Ok. Updated. Please take another look. stale. \n. rethink is dead! long live etcd!. That's correct. Yea we didn't have to delete it, but removed it for clarity. I guess next time we auto-gen it'll show up again.\nBut next time we want to improve any of this ... I suggest we generate the rst file automatically otherwise we'll always be fighting the automation we have.. The deploy flag --static-etcd-volume / --dynamic-etcd-nodes int flags allow you to deploy w a PV, which solves this issue.. Fixes #927 . This landed and the latest version now looks good. Fixes #1151 . Cool. I'll go ahead and remove those debug versions from RTDs, then once CI passes, merge. \n. stale. Hmmm the diff is messy because this is based off the simple_service branch. Once that lands (pending last 1/2 of CI) this is a lot easier to read.. Closing this version since the diff is weird. Will try opening a new one.. \n. One minor thing - that can be a separate PR since we want this to land this am. Ok, made those changes, so please take another look.. Looks great, but if we have one way of doing migrations, we should apply it to the existing migration we have (1.2.2 -> 1.2.3) as well.\nOn that note do we have support for 1.2.3 -> 1.2.4 so that the chain is unbroken? Or do we only specify migrations on versions with breaking changes?\nThese changes can be a separate PR that fully addresses #1055 \nI think this PR meets our needs of having a migration to 1.3\n\n. Yea, that PR I don't think would work as intended. We've learned a lot more about networking recently and different customer's needs. We definitely treat port forwarding as a second class citizen (not for production traffic / real users connecting to pachyderm), but it has served as a stop gap in some deployments until they can provision the right LBs / security rules.. stale. I believe we have streaming + pagination for a handful of these API calls. If further upgrades are needed, we'll create new issues.. fastest LGTM in the west! :hot_pepper: . stale. Done. Should be fixed by 1.7 / commit invariance. stale. We're in the process of transitioning a 3rd party python client for official support. That'll inform how we support other languages going forward.. I believe @jdoliner's recent migration PR does something like this. It marshals all of a clusters state into a file, and you can create a cluster from that file. Saving it into obj storage is left to the user, but can be readily done. Chunks? Leases? Oh my!\nThese are gone in 1.4. We had a user w a specific case where this would have helped:\na user (accidentally) deleted a k8s job that corresponded to a running pachyderm job (part of a pipeline), which made the pipeline unrecoverable\nThis should be easy enough to add as a test case for this feature.\n. @JoeyZwicker you literally just asked about this case ... but I didn't hear the conclusion. Care to chime in?. stale. We have the make logging task which stands up the logging service. This aggregates / stores as needed.. This is done now in the new worker model. LGTM\n\nMay be a good opportunity to address \nhttps://github.com/pachyderm/pachyderm/issues/1196\nBut in the spirit of small PRs, that can be done separately.. No longer valid ... we have workers and so k8s scheduling has a different effect now. Fixed in #1441 . stale. This is now done. I just did this yesterday when vetting EC2 iam role stuff. Haven't seen this since. Closing.. Generally looks good.\nI would request that we display the output field when doing pachctl inspect-job or pachctl inspect-pipeline\nAfter that, LGTM\n\nOh, and I linked to this PR for the obj store test harness issue. stale. Or, since some users don't have access to setting up EC2 container service (ECR), instructions for setting up credentials to a private dockerhub repo:\nhttp://stackoverflow.com/questions/36232906/how-to-access-private-docker-hub-repository-from-kubernetes-on-vagrant. This will become a blocker for at least one customer's production deployments soon. For now using dockerhub is OK, but they'd prefer to use ECR. Since we'll have to support this endeavor either way, we'll make sure to document it.. This is a requirement for a paying customer.. No more rethink! . We have cloudfront in alpha now, so if that's the ask here, I think we can close this issue. @jdoliner ?. stale\n(if we see another request, we can re-open). Stale :-/. Rethink is gone in 1.4\nI'm closing this issue, but feel free to re-open or hit us up in slack if you're still having this issue.. stale\n(and invalid I think ... ive seen the cache stuff in the logs and it doesn't seem like that much). stale. Oh, very cool. I like the meta data for proto field names, which eliminates the need for our one-off tool to fix this. So, this PR should also delete https://github.com/pachyderm/pachyderm/tree/master/src/server/cmd/protofix. Hey @roylee17 -- there are a lot of good changes in here and we really need the GRPC upgrade (we ran into another issue this week that we think is related). So I'm going to run w this PR so we can get this work merged in. I'll pull from your remote and setup a new branch on this repo and go from there. \nI'll ping you as I have any questions. Thanks!. One thing we can try -- next time this happens, if you use kubectl exec on the pod to try and ping the pachd node that will tell us if this is a networking issue, or perhaps something we're doing in our code.. Also - you can set the Debug field in the transform in the pipeline so that we can see the debug logs from job-shim as well.\nhttps://github.com/pachyderm/pachyderm/blob/master/src/server/cmd/job-shim/main.go#L127. I see. Good find!\nSo it sounds like this is something you can configure around (using their 'magic' env var). Once the workaround is confirmed, we can close this issue.\n. stale. delimiters are gone in 1.4. stale. Since there are partial fixes, and this is old, I'm gonna close it. We can open new issues for specific asks.. LGTM\n\n. I'm clicking the merge button since we want this in for a point release now. stale. Another one I see intermittently:\n--- FAIL: TestFailedJobReadData (30.09s)\n    require.go:153: /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/pachyderm_test.go:2361\n    require.go:155: No error is expected but got rpc error: code = 4 desc = context deadline exceeded\n. stale. I believe we rate limit the getfiles (and @jdoliner is working on rate limiting the putfiles) for 1.3\nI'm not sure if we retain the sync package code path in 1.4 -- if so, I believe this will be fixed once JD's PR lands.. Oh ... and because some of the logging stuff got swapped out ... our server logs look different now. \nIf its all the same, I'd like to just get this PR landed, then make a separate PR to fix up the logs. Hrrmmm the UI isn't showing me the comment? The diff is huge but a CTRL+F for 'jdoliner' has no results. And scrolling through / skimming it visually I don't see it. Do you remember which file you were commenting on?. Grrr CI won't pass ... so I'm going to wrestle w CI and look at #1273 . Our aws.sh script is now in the docs. Short term:\nIf the path we're writing to is always the same, couldn't we ask the user to just run sudo -E pachctl completion ? (which we could make output directly to that folder)\nLong term:\nI think that command should additionally make sure the user owns that folder so that we could update the file as needed as each pachctl command completes.. No ... we still list an empty token \" \" in the pachctl deploy amazon ... command:\nhttp://docs.pachyderm.io/en/latest/deployment/amazon_web_services.html. This is fixed. We use named args now.. Made a few comments.\nAlso -- if there is a consistent case where trying to start a persistent job before the data is ready causes issues, we should make that a separate bug report. That shouldn't take down the cluster or bork port forwarding.\nThe changes you're making in this PR will help work around any issues, but I also want to make sure we fix any underlying bugs if there are any.. \n. TestJobGC should be fixed from this PR: https://github.com/pachyderm/pachyderm/pull/1332. Darn ... that PR landed and I still got one of those JSON errors:\nhttps://travis-ci.org/pachyderm/pachyderm/builds/207212089\n--- FAIL: TestJSONSyntaxErrorsReportedCreatePipelineFromStdin (0.02s)\n    require.go:153: /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/pps/cmds/cmds_test.go:172\n    require.go:155: Not equal: \"Reading from stdin.\\nSyntax Error on line 5:\\n\\n    \\\"c\\\": {a\\n          ^\\ninvalid character 'a' looking for beginning of object key string\\n\" (expected)\n                != \"Reading from stdin.\\n2017/03/03 00:49:09 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \\\"transport: dial tcp 0.0.0.0:30650: getsockopt: connection refused\\\"; Reconnecting to {0.0.0.0:30650 <nil>}\\nSyntax Error on line 5:\\n\\n    \\\"c\\\": {a\\n          ^\\ninvalid character 'a' looking for beginning of object key string\\n\" (actual)\nFAIL\nmaybe we need to up the backoff interval (backoff more aggressively)\n(and i double checked that branch had the PR w the backoff upgrade)\n. stale. stale. stale. I believe our requirement for the wrapper is no longer valid. I believe there was discussion (and maybe an issue if I can find it) about removing this requirement in the pipeline spec.. \nI noticed you added metrics to GetFile - we should have them on PutFile too. But we can make that a separate issue.\n. Breaking up the scaling issues I found into a few PRs. This is the first. Want to get this on CI to make sure its passing. I also need to add a new test to address the big binary file case. Ok. Added a test case. Technically this is now ready for review. Except I needed the fixes from PR #1328, so the diff is weird. So I'll regenerate this PR once that other one lands so the diff is normal. stale. stale. Oh and I made sure that the sizes reported from 1.3.4 -> 1.3.6 get updated properly:\n17-02-17[13:08:16]:~:0$pachctl version\nCOMPONENT           VERSION                                          \npachctl             1.3.6-76538082a2729fbf026d8f573163f085ae410614   \npachd               1.3.6-76538082a2729fbf026d8f573163f085ae410614   \n17-02-17[13:08:19]:~:0$pachctl list-repo\nNAME                CREATED             SIZE                \ndata                4 minutes ago       0 B                 \nfilter              3 minutes ago       0 B                 \nsum                 3 minutes ago       0 B                 \n17-02-17[13:08:23]:~:0$pachctl list-repo\nNAME                CREATED             SIZE                \ndata                5 minutes ago       1.696 KiB           \nfilter              5 minutes ago       800 B               \nsum                 5 minutes ago       48 B. stale. stale. stale. I'm curious what the use case is here? It seems like you want to change the value of the max concurrency after you've initialized a client.\nI did run across something similar when updating the tests when the throttling sem PR landed. In this case I would just do something like:\nc, err := client.NewFromAddressWithConcurrency(addr, 200)\nc.PfsAPIClient.PutFile(...)\nAnd I'm curious why this pattern is insufficient. It seems like it has something to do with changing the channel's capacity 'in flight.\n. LGTM\nMuch simpler! I like it.. We've simplified the AWS credentials input, and I don't think we have a big need for this atm.. Validated this fix works on an empty s3 file, e.g. pachctl put-file debug aaa -f s3://testinvalidrange/blah. stale. Ok the new logs look like:\n2017-02-27T21:30:57Z INFO  protorpclog.Call \n{\n    \"duration\": \"0.000s\",\n    \"method\": \"putFileObj\",\n    \"request\": \"file:<path:\\\"test/1876.txt\\\" > file_type:FILE_TYPE_REGULAR url:\\\"test/1876.txt\\\" recursive:true \",\n    \"service\": \"pfs.API\"\n}\nAnd re-use the rpclog helpers.. It seems like for a while travis wasn't kicking off builds. It's at least trying to build this PR ... (its created the job) so we'll see if it works\nhttps://travis-ci.org/pachyderm/pachyderm/builds/206405818. Yea, we can omit this step, but we do need to make sure we're pushing to the right remote. In fact we should push to both ... pachyderm/www because that's the source of truth, and pachyderm/pachyderm.github.io for the GH site. Yea, we could just do that. We prob want to delete the old repo to prevent confusion. There are a few outstanding issues on the www repo, but maybe there's a way to port them over? Most of them are pretty old, so its not the end of the world if we lose them.\nThe other optimization I want to do is upload the .deb files someplace (like s3, where we could host a dl.pachyderm.io domain) just because ... right now we overwrite the pachctl.deb file w a large binary and it clutters the history. I'm sure there's a way to take a scalpel to the git history to flatten this file's history but right now everytime we release we put a 10-20Mb bin file in there.\n(But that's beyond the scope of this PR of course :-P)\n. Also, right now the release process doesn't actually push to the pages remote ... so the deb package doesn't get updated automatically. \n(We hadn't made the change to update the release script since we switched over to gh pages). For posterity, I do also want to record exactly what version of everything I used when running this, in case we need to upgrade these manifests / images in the future\n\n```\n17-03-02[16:28:01]:pachyderm:0$kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"5\", GitVersion:\"v1.5.2\", GitCommit:\"08e099554f3c31f6e6f07b448ab3ed78d0520507\", GitTreeState:\"clean\", BuildDate:\"2017-01-12T04:57:25Z\", GoVersion:\"go1.7.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"4\", GitVersion:\"v1.4.7\", GitCommit:\"92b4f971662de9d8770f8dcd2ee01ec226a6f6c0\", GitTreeState:\"clean\", BuildDate:\"2016-12-10T04:43:42Z\", GoVersion:\"go1.6.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n17-03-02[16:41:52]:pachyderm:0$kubectl cluster-info\nKubernetes master is running at https://api.bpachydermcluster.kubernetes.com\nHeapster is running at https://api.bpachydermcluster.kubernetes.com/api/v1/proxy/namespaces/kube-system/services/heapster\nKubeDNS is running at https://api.bpachydermcluster.kubernetes.com/api/v1/proxy/namespaces/kube-system/services/kube-dns\nmonitoring-grafana is running at https://api.bpachydermcluster.kubernetes.com/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana\nmonitoring-influxdb is running at https://api.bpachydermcluster.kubernetes.com/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n17-03-02[16:42:02]:pachyderm:0$kubectl cluster-info dump \n{\n    \"metadata\": {\n        \"selfLink\": \"/api/v1/nodes\",\n        \"resourceVersion\": \"5111707\"\n    },\n    \"items\": [\n        {\n            \"metadata\": {\n                \"name\": \"ip-172-20-41-204.ec2.internal\",\n                \"selfLink\": \"/api/v1/nodes/ip-172-20-41-204.ec2.internal\",\n                \"uid\": \"a3632eb1-f31d-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"5111696\",\n                \"creationTimestamp\": \"2017-02-15T01:25:30Z\",\n                \"labels\": {\n                    \"beta.kubernetes.io/arch\": \"amd64\",\n                    \"beta.kubernetes.io/instance-type\": \"r4.xlarge\",\n                    \"beta.kubernetes.io/os\": \"linux\",\n                    \"failure-domain.beta.kubernetes.io/region\": \"us-east-1\",\n                    \"failure-domain.beta.kubernetes.io/zone\": \"us-east-1a\",\n                    \"kubernetes.io/hostname\": \"ip-172-20-41-204.ec2.internal\"\n                },\n                \"annotations\": {\n                    \"volumes.kubernetes.io/controller-managed-attach-detach\": \"true\"\n                }\n            },\n            \"spec\": {\n                \"podCIDR\": \"100.96.2.0/24\",\n                \"externalID\": \"i-04a8a850295b017e4\",\n                \"providerID\": \"aws:///us-east-1a/i-04a8a850295b017e4\"\n            },\n            \"status\": {\n                \"capacity\": {\n                    \"alpha.kubernetes.io/nvidia-gpu\": \"0\",\n                    \"cpu\": \"4\",\n                    \"memory\": \"31402436Ki\",\n                    \"pods\": \"110\"\n                },\n                \"allocatable\": {\n                    \"alpha.kubernetes.io/nvidia-gpu\": \"0\",\n                    \"cpu\": \"4\",\n                    \"memory\": \"31402436Ki\",\n                    \"pods\": \"110\"\n                },\n                \"conditions\": [\n                    {\n                        \"type\": \"OutOfDisk\",\n                        \"status\": \"False\",\n                        \"lastHeartbeatTime\": \"2017-03-03T00:42:01Z\",\n                        \"lastTransitionTime\": \"2017-02-15T01:25:30Z\",\n                        \"reason\": \"KubeletHasSufficientDisk\",\n                        \"message\": \"kubelet has sufficient disk space available\"\n                    },\n                    {\n                        \"type\": \"MemoryPressure\",\n                        \"status\": \"False\",\n                        \"lastHeartbeatTime\": \"2017-03-03T00:42:01Z\",\n                        \"lastTransitionTime\": \"2017-02-15T01:25:30Z\",\n                        \"reason\": \"KubeletHasSufficientMemory\",\n                        \"message\": \"kubelet has sufficient memory available\"\n                    },\n                    {\n                        \"type\": \"DiskPressure\",\n                        \"status\": \"False\",\n                        \"lastHeartbeatTime\": \"2017-03-03T00:42:01Z\",\n                        \"lastTransitionTime\": \"2017-02-15T01:25:30Z\",\n                        \"reason\": \"KubeletHasNoDiskPressure\",\n                        \"message\": \"kubelet has no disk pressure\"\n                    },\n                    {\n                        \"type\": \"Ready\",\n                        \"status\": \"True\",\n                        \"lastHeartbeatTime\": \"2017-03-03T00:42:01Z\",\n                        \"lastTransitionTime\": \"2017-02-15T01:26:00Z\",\n                        \"reason\": \"KubeletReady\",\n                        \"message\": \"kubelet is posting ready status\"\n                    },\n                    {\n                        \"type\": \"NetworkUnavailable\",\n                        \"status\": \"False\",\n                        \"lastHeartbeatTime\": \"2017-03-03T00:42:02Z\",\n                        \"lastTransitionTime\": \"2017-03-03T00:42:02Z\",\n                        \"reason\": \"RouteCreated\",\n                        \"message\": \"RouteController created a route\"\n                    }\n                ],\n                \"addresses\": [\n                    {\n                        \"type\": \"InternalIP\",\n                        \"address\": \"172.20.41.204\"\n                    },\n                    {\n                        \"type\": \"LegacyHostIP\",\n                        \"address\": \"172.20.41.204\"\n                    },\n                    {\n                        \"type\": \"ExternalIP\",\n                        \"address\": \"52.55.243.169\"\n                    }\n                ],\n                \"daemonEndpoints\": {\n                    \"kubeletEndpoint\": {\n                        \"Port\": 10250\n                    }\n                },\n                \"nodeInfo\": {\n                    \"machineID\": \"8c0bb548bcc343f488e80ae5f4776a73\",\n                    \"systemUUID\": \"EC283080-63A4-5B39-96F7-DA8930D9960F\",\n                    \"bootID\": \"23822fd8-6f06-4008-b67a-d0c0c6bb9c45\",\n                    \"kernelVersion\": \"4.4.26-k8s\",\n                    \"osImage\": \"Debian GNU/Linux 8 (jessie)\",\n                    \"containerRuntimeVersion\": \"docker://1.11.2\",\n                    \"kubeletVersion\": \"v1.4.7\",\n                    \"kubeProxyVersion\": \"v1.4.7\",\n                    \"operatingSystem\": \"linux\",\n                    \"architecture\": \"amd64\"\n                },\n                \"images\": [\n                    {\n                        \"names\": [\n                            \"pachyderm/job-shim:1.3.5-bd608dc9e04b237c1006901033eb16989918ed2a\"\n                        ],\n                        \"sizeBytes\": 331972321\n                    },\n                    {\n                        \"names\": [\n                            \"protokube:1.5.0-alpha4\"\n                        ],\n                        \"sizeBytes\": 289612653\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/kube-proxy:v1.4.7\"\n                        ],\n                        \"sizeBytes\": 202281808\n                    },\n                    {\n                        \"names\": [\n                            \"cburki/influxdb-shell:latest\"\n                        ],\n                        \"sizeBytes\": 196017385\n                    },\n                    {\n                        \"names\": [\n                            \"rethinkdb:2.3.2\"\n                        ],\n                        \"sizeBytes\": 183753850\n                    },\n                    {\n                        \"names\": [\n                            \"cburki/etcdctl:latest\"\n                        ],\n                        \"sizeBytes\": 162397534\n                    },\n                    {\n                        \"names\": [\n                            \"ubuntu:16.04\"\n                        ],\n                        \"sizeBytes\": 129972069\n                    },\n                    {\n                        \"names\": [\n                            \"ubuntu:latest\"\n                        ],\n                        \"sizeBytes\": 129465861\n                    },\n                    {\n                        \"names\": [\n                            \"pachyderm/pachd:1.3.5-36175107cd1df868630c551a5b7a98186012bf13\"\n                        ],\n                        \"sizeBytes\": 62499364\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/kubedns-amd64:1.9\"\n                        ],\n                        \"sizeBytes\": 46998769\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/etcd:2.0.12\"\n                        ],\n                        \"sizeBytes\": 15265152\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/dnsmasq-metrics-amd64:1.0\"\n                        ],\n                        \"sizeBytes\": 13998769\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\"\n                        ],\n                        \"sizeBytes\": 11585302\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/exechealthz-amd64:1.2\"\n                        ],\n                        \"sizeBytes\": 8374840\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/kube-dnsmasq-amd64:1.4\"\n                        ],\n                        \"sizeBytes\": 5126001\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/pause-amd64:3.0\"\n                        ],\n                        \"sizeBytes\": 746888\n                    }\n                ],\n                \"volumesInUse\": [\n                    \"kubernetes.io/aws-ebs/aws://us-east-1a/vol-01423474318abee9d\"\n                ],\n                \"volumesAttached\": [\n                    {\n                        \"name\": \"kubernetes.io/aws-ebs/aws://us-east-1a/vol-01423474318abee9d\",\n                        \"devicePath\": \"/dev/xvdbb\"\n                    }\n                ]\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"ip-172-20-57-18.ec2.internal\",\n                \"selfLink\": \"/api/v1/nodes/ip-172-20-57-18.ec2.internal\",\n                \"uid\": \"4915ce86-f31d-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"5111704\",\n                \"creationTimestamp\": \"2017-02-15T01:22:58Z\",\n                \"labels\": {\n                    \"beta.kubernetes.io/arch\": \"amd64\",\n                    \"beta.kubernetes.io/instance-type\": \"r4.xlarge\",\n                    \"beta.kubernetes.io/os\": \"linux\",\n                    \"failure-domain.beta.kubernetes.io/region\": \"us-east-1\",\n                    \"failure-domain.beta.kubernetes.io/zone\": \"us-east-1a\",\n                    \"kubernetes.io/hostname\": \"ip-172-20-57-18.ec2.internal\",\n                    \"kubernetes.io/role\": \"master\"\n                },\n                \"annotations\": {\n                    \"scheduler.alpha.kubernetes.io/taints\": \"[{\\\"key\\\":\\\"dedicated\\\",\\\"value\\\":\\\"master\\\",\\\"effect\\\":\\\"NoSchedule\\\"}]\",\n                    \"volumes.kubernetes.io/controller-managed-attach-detach\": \"true\"\n                }\n            },\n            \"spec\": {\n                \"podCIDR\": \"100.96.0.0/24\",\n                \"externalID\": \"i-0f4da5892ff3e6f57\",\n                \"providerID\": \"aws:///us-east-1a/i-0f4da5892ff3e6f57\"\n            },\n            \"status\": {\n                \"capacity\": {\n                    \"alpha.kubernetes.io/nvidia-gpu\": \"0\",\n                    \"cpu\": \"4\",\n                    \"memory\": \"31402436Ki\",\n                    \"pods\": \"110\"\n                },\n                \"allocatable\": {\n                    \"alpha.kubernetes.io/nvidia-gpu\": \"0\",\n                    \"cpu\": \"4\",\n                    \"memory\": \"31402436Ki\",\n                    \"pods\": \"110\"\n                },\n                \"conditions\": [\n                    {\n                        \"type\": \"OutOfDisk\",\n                        \"status\": \"False\",\n                        \"lastHeartbeatTime\": \"2017-03-03T00:42:05Z\",\n                        \"lastTransitionTime\": \"2017-02-15T01:22:58Z\",\n                        \"reason\": \"KubeletHasSufficientDisk\",\n                        \"message\": \"kubelet has sufficient disk space available\"\n                    },\n                    {\n                        \"type\": \"MemoryPressure\",\n                        \"status\": \"False\",\n                        \"lastHeartbeatTime\": \"2017-03-03T00:42:05Z\",\n                        \"lastTransitionTime\": \"2017-02-15T01:22:58Z\",\n                        \"reason\": \"KubeletHasSufficientMemory\",\n                        \"message\": \"kubelet has sufficient memory available\"\n                    },\n                    {\n                        \"type\": \"DiskPressure\",\n                        \"status\": \"False\",\n                        \"lastHeartbeatTime\": \"2017-03-03T00:42:05Z\",\n                        \"lastTransitionTime\": \"2017-02-15T01:22:58Z\",\n                        \"reason\": \"KubeletHasNoDiskPressure\",\n                        \"message\": \"kubelet has no disk pressure\"\n                    },\n                    {\n                        \"type\": \"Ready\",\n                        \"status\": \"True\",\n                        \"lastHeartbeatTime\": \"2017-03-03T00:42:05Z\",\n                        \"lastTransitionTime\": \"2017-02-15T01:22:58Z\",\n                        \"reason\": \"KubeletReady\",\n                        \"message\": \"kubelet is posting ready status\"\n                    },\n                    {\n                        \"type\": \"NetworkUnavailable\",\n                        \"status\": \"False\",\n                        \"lastHeartbeatTime\": \"2017-03-03T00:42:02Z\",\n                        \"lastTransitionTime\": \"2017-03-03T00:42:02Z\",\n                        \"reason\": \"RouteCreated\",\n                        \"message\": \"RouteController created a route\"\n                    }\n                ],\n                \"addresses\": [\n                    {\n                        \"type\": \"InternalIP\",\n                        \"address\": \"172.20.57.18\"\n                    },\n                    {\n                        \"type\": \"LegacyHostIP\",\n                        \"address\": \"172.20.57.18\"\n                    },\n                    {\n                        \"type\": \"ExternalIP\",\n                        \"address\": \"52.200.153.133\"\n                    }\n                ],\n                \"daemonEndpoints\": {\n                    \"kubeletEndpoint\": {\n                        \"Port\": 10250\n                    }\n                },\n                \"nodeInfo\": {\n                    \"machineID\": \"791102aed3f94ac18c1292ffffa48df2\",\n                    \"systemUUID\": \"EC24A648-2A37-E289-E340-7BBC43DBADF1\",\n                    \"bootID\": \"d7446581-951c-47f6-9d11-55024250babb\",\n                    \"kernelVersion\": \"4.4.26-k8s\",\n                    \"osImage\": \"Debian GNU/Linux 8 (jessie)\",\n                    \"containerRuntimeVersion\": \"docker://1.11.2\",\n                    \"kubeletVersion\": \"v1.4.7\",\n                    \"kubeProxyVersion\": \"v1.4.7\",\n                    \"operatingSystem\": \"linux\",\n                    \"architecture\": \"amd64\"\n                },\n                \"images\": [\n                    {\n                        \"names\": [\n                            \"protokube:1.5.0-alpha4\"\n                        ],\n                        \"sizeBytes\": 289612653\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/kube-proxy:v1.4.7\"\n                        ],\n                        \"sizeBytes\": 202281808\n                    },\n                    {\n                        \"names\": [\n                            \"kope/dns-controller:1.5.1\"\n                        ],\n                        \"sizeBytes\": 188032597\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/kube-apiserver:v1.4.7\"\n                        ],\n                        \"sizeBytes\": 152093596\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/kube-controller-manager:v1.4.7\"\n                        ],\n                        \"sizeBytes\": 142134764\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/kube-scheduler:v1.4.7\"\n                        ],\n                        \"sizeBytes\": 81299564\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/etcd:2.2.1\"\n                        ],\n                        \"sizeBytes\": 28191895\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/pause-amd64:3.0\"\n                        ],\n                        \"sizeBytes\": 746888\n                    }\n                ]\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"ip-172-20-58-236.ec2.internal\",\n                \"selfLink\": \"/api/v1/nodes/ip-172-20-58-236.ec2.internal\",\n                \"uid\": \"9d83a5fa-f31d-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"5111702\",\n                \"creationTimestamp\": \"2017-02-15T01:25:20Z\",\n                \"labels\": {\n                    \"beta.kubernetes.io/arch\": \"amd64\",\n                    \"beta.kubernetes.io/instance-type\": \"r4.xlarge\",\n                    \"beta.kubernetes.io/os\": \"linux\",\n                    \"failure-domain.beta.kubernetes.io/region\": \"us-east-1\",\n                    \"failure-domain.beta.kubernetes.io/zone\": \"us-east-1a\",\n                    \"kubernetes.io/hostname\": \"ip-172-20-58-236.ec2.internal\"\n                },\n                \"annotations\": {\n                    \"volumes.kubernetes.io/controller-managed-attach-detach\": \"true\"\n                }\n            },\n            \"spec\": {\n                \"podCIDR\": \"100.96.1.0/24\",\n                \"externalID\": \"i-0a1a5b6b021adbbad\",\n                \"providerID\": \"aws:///us-east-1a/i-0a1a5b6b021adbbad\"\n            },\n            \"status\": {\n                \"capacity\": {\n                    \"alpha.kubernetes.io/nvidia-gpu\": \"0\",\n                    \"cpu\": \"4\",\n                    \"memory\": \"31402436Ki\",\n                    \"pods\": \"110\"\n                },\n                \"allocatable\": {\n                    \"alpha.kubernetes.io/nvidia-gpu\": \"0\",\n                    \"cpu\": \"4\",\n                    \"memory\": \"31402436Ki\",\n                    \"pods\": \"110\"\n                },\n                \"conditions\": [\n                    {\n                        \"type\": \"OutOfDisk\",\n                        \"status\": \"False\",\n                        \"lastHeartbeatTime\": \"2017-03-03T00:42:04Z\",\n                        \"lastTransitionTime\": \"2017-02-15T01:25:20Z\",\n                        \"reason\": \"KubeletHasSufficientDisk\",\n                        \"message\": \"kubelet has sufficient disk space available\"\n                    },\n                    {\n                        \"type\": \"MemoryPressure\",\n                        \"status\": \"False\",\n                        \"lastHeartbeatTime\": \"2017-03-03T00:42:04Z\",\n                        \"lastTransitionTime\": \"2017-02-15T01:25:20Z\",\n                        \"reason\": \"KubeletHasSufficientMemory\",\n                        \"message\": \"kubelet has sufficient memory available\"\n                    },\n                    {\n                        \"type\": \"DiskPressure\",\n                        \"status\": \"False\",\n                        \"lastHeartbeatTime\": \"2017-03-03T00:42:04Z\",\n                        \"lastTransitionTime\": \"2017-02-15T01:25:20Z\",\n                        \"reason\": \"KubeletHasNoDiskPressure\",\n                        \"message\": \"kubelet has no disk pressure\"\n                    },\n                    {\n                        \"type\": \"Ready\",\n                        \"status\": \"True\",\n                        \"lastHeartbeatTime\": \"2017-03-03T00:42:04Z\",\n                        \"lastTransitionTime\": \"2017-02-15T01:25:50Z\",\n                        \"reason\": \"KubeletReady\",\n                        \"message\": \"kubelet is posting ready status\"\n                    },\n                    {\n                        \"type\": \"NetworkUnavailable\",\n                        \"status\": \"False\",\n                        \"lastHeartbeatTime\": \"2017-03-03T00:42:02Z\",\n                        \"lastTransitionTime\": \"2017-03-03T00:42:02Z\",\n                        \"reason\": \"RouteCreated\",\n                        \"message\": \"RouteController created a route\"\n                    }\n                ],\n                \"addresses\": [\n                    {\n                        \"type\": \"InternalIP\",\n                        \"address\": \"172.20.58.236\"\n                    },\n                    {\n                        \"type\": \"LegacyHostIP\",\n                        \"address\": \"172.20.58.236\"\n                    },\n                    {\n                        \"type\": \"ExternalIP\",\n                        \"address\": \"174.129.160.224\"\n                    }\n                ],\n                \"daemonEndpoints\": {\n                    \"kubeletEndpoint\": {\n                        \"Port\": 10250\n                    }\n                },\n                \"nodeInfo\": {\n                    \"machineID\": \"a5925ee1d6df4c0a8b102a45bced5d58\",\n                    \"systemUUID\": \"EC265EC2-6B4A-3F6D-5CC5-59534304AB07\",\n                    \"bootID\": \"13f2b31c-5c2d-4374-8377-c47376e3c80b\",\n                    \"kernelVersion\": \"4.4.26-k8s\",\n                    \"osImage\": \"Debian GNU/Linux 8 (jessie)\",\n                    \"containerRuntimeVersion\": \"docker://1.11.2\",\n                    \"kubeletVersion\": \"v1.4.7\",\n                    \"kubeProxyVersion\": \"v1.4.7\",\n                    \"operatingSystem\": \"linux\",\n                    \"architecture\": \"amd64\"\n                },\n                \"images\": [\n                    {\n                        \"names\": [\n                            \"pachyderm/job-shim:1.3.5-bd608dc9e04b237c1006901033eb16989918ed2a\"\n                        ],\n                        \"sizeBytes\": 331972321\n                    },\n                    {\n                        \"names\": [\n                            \"protokube:1.5.0-alpha4\"\n                        ],\n                        \"sizeBytes\": 289612653\n                    },\n                    {\n                        \"names\": [\n                            \"sjezewski/etcdctl:latest\"\n                        ],\n                        \"sizeBytes\": 225237010\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/kube-proxy:v1.4.7\"\n                        ],\n                        \"sizeBytes\": 202281808\n                    },\n                    {\n                        \"names\": [\n                            \"cburki/influxdb-shell:v1.2.0\"\n                        ],\n                        \"sizeBytes\": 196017385\n                    },\n                    {\n                        \"names\": [\n                            \"cburki/influxdb-shell:latest\"\n                        ],\n                        \"sizeBytes\": 196017385\n                    },\n                    {\n                        \"names\": [\n                            \"rethinkdb:2.3.2\"\n                        ],\n                        \"sizeBytes\": 183753850\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/heapster-grafana-amd64:v4.0.2\"\n                        ],\n                        \"sizeBytes\": 131461556\n                    },\n                    {\n                        \"names\": [\n                            \"ubuntu:16.04\"\n                        ],\n                        \"sizeBytes\": 129972069\n                    },\n                    {\n                        \"names\": [\n                            \"ubuntu:latest\"\n                        ],\n                        \"sizeBytes\": 129465861\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1\"\n                        ],\n                        \"sizeBytes\": 101256311\n                    },\n                    {\n                        \"names\": [\n                            \"pachyderm/pachd:1.3.10-5a92afad9b9d5497529e3ecfa4201b99d4ab6a52\"\n                        ],\n                        \"sizeBytes\": 65076702\n                    },\n                    {\n                        \"names\": [\n                            \"pachyderm/pachd:1.3.5-36175107cd1df868630c551a5b7a98186012bf13\"\n                        ],\n                        \"sizeBytes\": 62499364\n                    },\n                    {\n                        \"names\": [\n                            \"pachyderm/pachd:1.3.5-bd608dc9e04b237c1006901033eb16989918ed2a\"\n                        ],\n                        \"sizeBytes\": 62491832\n                    },\n                    {\n                        \"names\": [\n                            \"pachyderm/pachd:1.3.5\"\n                        ],\n                        \"sizeBytes\": 62482583\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.0.0\"\n                        ],\n                        \"sizeBytes\": 48155586\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/etcd:2.0.12\"\n                        ],\n                        \"sizeBytes\": 15265152\n                    },\n                    {\n                        \"names\": [\n                            \"gcr.io/google_containers/pause-amd64:3.0\"\n                        ],\n                        \"sizeBytes\": 746888\n                    }\n                ]\n            }\n        }\n    ]\n}\n{\n    \"metadata\": {\n        \"selfLink\": \"/api/v1/namespaces/kube-system/events\",\n        \"resourceVersion\": \"35465\"\n    },\n    \"items\": [\n        {\n            \"metadata\": {\n                \"name\": \"heapster-564189836-dfalg.14a837a7748ce22f\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/heapster-564189836-dfalg.14a837a7748ce22f\",\n                \"uid\": \"3b12e7ee-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35453\",\n                \"creationTimestamp\": \"2017-03-03T00:27:49Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Pod\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"heapster-564189836-dfalg\",\n                \"uid\": \"3b0f7ec8-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"v1\",\n                \"resourceVersion\": \"5110299\"\n            },\n            \"reason\": \"Scheduled\",\n            \"message\": \"Successfully assigned heapster-564189836-dfalg to ip-172-20-58-236.ec2.internal\",\n            \"source\": {\n                \"component\": \"default-scheduler\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:49Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:49Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"heapster-564189836-dfalg.14a837a7b05b1021\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/heapster-564189836-dfalg.14a837a7b05b1021\",\n                \"uid\": \"3bab84e7-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35458\",\n                \"creationTimestamp\": \"2017-03-03T00:27:50Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Pod\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"heapster-564189836-dfalg\",\n                \"uid\": \"3b0f7ec8-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"v1\",\n                \"resourceVersion\": \"5110302\",\n                \"fieldPath\": \"spec.containers{heapster}\"\n            },\n            \"reason\": \"Pulled\",\n            \"message\": \"Container image \\\"gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1\\\" already present on machine\",\n            \"source\": {\n                \"component\": \"kubelet\",\n                \"host\": \"ip-172-20-58-236.ec2.internal\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:50Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:50Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"heapster-564189836-dfalg.14a837a7da372ca2\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/heapster-564189836-dfalg.14a837a7da372ca2\",\n                \"uid\": \"3c163a1c-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35463\",\n                \"creationTimestamp\": \"2017-03-03T00:27:50Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Pod\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"heapster-564189836-dfalg\",\n                \"uid\": \"3b0f7ec8-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"v1\",\n                \"resourceVersion\": \"5110302\",\n                \"fieldPath\": \"spec.containers{heapster}\"\n            },\n            \"reason\": \"Created\",\n            \"message\": \"Created container with docker id b216c964ad5b; Security:[seccomp=unconfined]\",\n            \"source\": {\n                \"component\": \"kubelet\",\n                \"host\": \"ip-172-20-58-236.ec2.internal\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:50Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:50Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"heapster-564189836-dfalg.14a837a7e0dee4d1\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/heapster-564189836-dfalg.14a837a7e0dee4d1\",\n                \"uid\": \"3c274959-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35465\",\n                \"creationTimestamp\": \"2017-03-03T00:27:51Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Pod\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"heapster-564189836-dfalg\",\n                \"uid\": \"3b0f7ec8-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"v1\",\n                \"resourceVersion\": \"5110302\",\n                \"fieldPath\": \"spec.containers{heapster}\"\n            },\n            \"reason\": \"Started\",\n            \"message\": \"Started container with docker id b216c964ad5b\",\n            \"source\": {\n                \"component\": \"kubelet\",\n                \"host\": \"ip-172-20-58-236.ec2.internal\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:51Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:51Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"heapster-564189836-i51x7.14a83795f7619149\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/heapster-564189836-i51x7.14a83795f7619149\",\n                \"uid\": \"0e4c7793-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35444\",\n                \"creationTimestamp\": \"2017-03-03T00:26:34Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Pod\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"heapster-564189836-i51x7\",\n                \"uid\": \"fb4f9870-fe13-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"v1\",\n                \"resourceVersion\": \"4832562\",\n                \"fieldPath\": \"spec.containers{heapster}\"\n            },\n            \"reason\": \"Killing\",\n            \"message\": \"Killing container with docker id 8c931ac698b3: Need to kill pod.\",\n            \"source\": {\n                \"component\": \"kubelet\",\n                \"host\": \"ip-172-20-58-236.ec2.internal\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:26:34Z\",\n            \"lastTimestamp\": \"2017-03-03T00:26:34Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"heapster-564189836.14a83795efd74ce8\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/heapster-564189836.14a83795efd74ce8\",\n                \"uid\": \"0e395c54-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35443\",\n                \"creationTimestamp\": \"2017-03-03T00:26:33Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"ReplicaSet\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"heapster-564189836\",\n                \"uid\": \"fb4f2d72-fe13-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"extensions\",\n                \"resourceVersion\": \"5110133\"\n            },\n            \"reason\": \"SuccessfulDelete\",\n            \"message\": \"Deleted pod: heapster-564189836-i51x7\",\n            \"source\": {\n                \"component\": \"replicaset-controller\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:26:33Z\",\n            \"lastTimestamp\": \"2017-03-03T00:26:33Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"heapster-564189836.14a837a773cc23e5\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/heapster-564189836.14a837a773cc23e5\",\n                \"uid\": \"3b103208-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35452\",\n                \"creationTimestamp\": \"2017-03-03T00:27:49Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"ReplicaSet\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"heapster-564189836\",\n                \"uid\": \"3b0efc17-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"extensions\",\n                \"resourceVersion\": \"5110297\"\n            },\n            \"reason\": \"SuccessfulCreate\",\n            \"message\": \"Created pod: heapster-564189836-dfalg\",\n            \"source\": {\n                \"component\": \"replicaset-controller\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:49Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:49Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"heapster.14a83795ef850fe7\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/heapster.14a83795ef850fe7\",\n                \"uid\": \"0e38827b-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35442\",\n                \"creationTimestamp\": \"2017-03-03T00:26:33Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Deployment\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"heapster\",\n                \"uid\": \"fb4d52f2-fe13-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"extensions\",\n                \"resourceVersion\": \"5110132\"\n            },\n            \"reason\": \"ScalingReplicaSet\",\n            \"message\": \"Scaled down replica set heapster-564189836 to 0\",\n            \"source\": {\n                \"component\": \"deployment-controller\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:26:33Z\",\n            \"lastTimestamp\": \"2017-03-03T00:26:33Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"heapster.14a837a77373b4c8\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/heapster.14a837a77373b4c8\",\n                \"uid\": \"3b0f4c49-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35451\",\n                \"creationTimestamp\": \"2017-03-03T00:27:49Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Deployment\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"heapster\",\n                \"uid\": \"3b0e8a69-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"extensions\",\n                \"resourceVersion\": \"5110296\"\n            },\n            \"reason\": \"ScalingReplicaSet\",\n            \"message\": \"Scaled up replica set heapster-564189836 to 1\",\n            \"source\": {\n                \"component\": \"deployment-controller\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:49Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:49Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-grafana-3344903701-kd95t.14a837a763ef13b1\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-grafana-3344903701-kd95t.14a837a763ef13b1\",\n                \"uid\": \"3ae7a652-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35450\",\n                \"creationTimestamp\": \"2017-03-03T00:27:48Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Pod\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-grafana-3344903701-kd95t\",\n                \"uid\": \"3ae5d9c0-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"v1\",\n                \"resourceVersion\": \"5110285\"\n            },\n            \"reason\": \"Scheduled\",\n            \"message\": \"Successfully assigned monitoring-grafana-3344903701-kd95t to ip-172-20-58-236.ec2.internal\",\n            \"source\": {\n                \"component\": \"default-scheduler\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:48Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:48Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-grafana-3344903701-kd95t.14a837a7b046920e\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-grafana-3344903701-kd95t.14a837a7b046920e\",\n                \"uid\": \"3baafef5-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35457\",\n                \"creationTimestamp\": \"2017-03-03T00:27:50Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Pod\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-grafana-3344903701-kd95t\",\n                \"uid\": \"3ae5d9c0-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"v1\",\n                \"resourceVersion\": \"5110288\",\n                \"fieldPath\": \"spec.containers{grafana}\"\n            },\n            \"reason\": \"Pulled\",\n            \"message\": \"Container image \\\"gcr.io/google_containers/heapster-grafana-amd64:v4.0.2\\\" already present on machine\",\n            \"source\": {\n                \"component\": \"kubelet\",\n                \"host\": \"ip-172-20-58-236.ec2.internal\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:50Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:50Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-grafana-3344903701-kd95t.14a837a7c75f6ba5\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-grafana-3344903701-kd95t.14a837a7c75f6ba5\",\n                \"uid\": \"3be5ffef-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35460\",\n                \"creationTimestamp\": \"2017-03-03T00:27:50Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Pod\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-grafana-3344903701-kd95t\",\n                \"uid\": \"3ae5d9c0-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"v1\",\n                \"resourceVersion\": \"5110288\",\n                \"fieldPath\": \"spec.containers{grafana}\"\n            },\n            \"reason\": \"Created\",\n            \"message\": \"Created container with docker id 174110ad9cfe; Security:[seccomp=unconfined]\",\n            \"source\": {\n                \"component\": \"kubelet\",\n                \"host\": \"ip-172-20-58-236.ec2.internal\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:50Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:50Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-grafana-3344903701-kd95t.14a837a7d4cb6234\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-grafana-3344903701-kd95t.14a837a7d4cb6234\",\n                \"uid\": \"3c085b0c-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35461\",\n                \"creationTimestamp\": \"2017-03-03T00:27:50Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Pod\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-grafana-3344903701-kd95t\",\n                \"uid\": \"3ae5d9c0-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"v1\",\n                \"resourceVersion\": \"5110288\",\n                \"fieldPath\": \"spec.containers{grafana}\"\n            },\n            \"reason\": \"Started\",\n            \"message\": \"Started container with docker id 174110ad9cfe\",\n            \"source\": {\n                \"component\": \"kubelet\",\n                \"host\": \"ip-172-20-58-236.ec2.internal\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:50Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:50Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-grafana-3344903701-vtknq.14a83794f1d68477\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-grafana-3344903701-vtknq.14a83794f1d68477\",\n                \"uid\": \"0baedecd-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35441\",\n                \"creationTimestamp\": \"2017-03-03T00:26:29Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Pod\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-grafana-3344903701-vtknq\",\n                \"uid\": \"fb1e31af-fe13-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"v1\",\n                \"resourceVersion\": \"4832550\",\n                \"fieldPath\": \"spec.containers{grafana}\"\n            },\n            \"reason\": \"Killing\",\n            \"message\": \"Killing container with docker id 86be6cef4f4b: Need to kill pod.\",\n            \"source\": {\n                \"component\": \"kubelet\",\n                \"host\": \"ip-172-20-58-236.ec2.internal\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:26:29Z\",\n            \"lastTimestamp\": \"2017-03-03T00:26:29Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-grafana-3344903701.14a83794e9fa39a0\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-grafana-3344903701.14a83794e9fa39a0\",\n                \"uid\": \"0b9b2469-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35440\",\n                \"creationTimestamp\": \"2017-03-03T00:26:29Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"ReplicaSet\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-grafana-3344903701\",\n                \"uid\": \"fb1cdb14-fe13-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"extensions\",\n                \"resourceVersion\": \"5110108\"\n            },\n            \"reason\": \"SuccessfulDelete\",\n            \"message\": \"Deleted pod: monitoring-grafana-3344903701-vtknq\",\n            \"source\": {\n                \"component\": \"replicaset-controller\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:26:29Z\",\n            \"lastTimestamp\": \"2017-03-03T00:26:29Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-grafana-3344903701.14a837a763794b50\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-grafana-3344903701.14a837a763794b50\",\n                \"uid\": \"3ae6725b-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35449\",\n                \"creationTimestamp\": \"2017-03-03T00:27:48Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"ReplicaSet\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-grafana-3344903701\",\n                \"uid\": \"3ae4a19b-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"extensions\",\n                \"resourceVersion\": \"5110283\"\n            },\n            \"reason\": \"SuccessfulCreate\",\n            \"message\": \"Created pod: monitoring-grafana-3344903701-kd95t\",\n            \"source\": {\n                \"component\": \"replicaset-controller\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:48Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:48Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-grafana.14a83794e9a5a3f4\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-grafana.14a83794e9a5a3f4\",\n                \"uid\": \"0b9a1bac-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35439\",\n                \"creationTimestamp\": \"2017-03-03T00:26:29Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Deployment\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-grafana\",\n                \"uid\": \"fb1c3ce7-fe13-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"extensions\",\n                \"resourceVersion\": \"5110107\"\n            },\n            \"reason\": \"ScalingReplicaSet\",\n            \"message\": \"Scaled down replica set monitoring-grafana-3344903701 to 0\",\n            \"source\": {\n                \"component\": \"deployment-controller\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:26:29Z\",\n            \"lastTimestamp\": \"2017-03-03T00:26:29Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-grafana.14a837a7631b1e82\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-grafana.14a837a7631b1e82\",\n                \"uid\": \"3ae58280-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35448\",\n                \"creationTimestamp\": \"2017-03-03T00:27:48Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Deployment\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-grafana\",\n                \"uid\": \"3ae4155a-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"extensions\",\n                \"resourceVersion\": \"5110282\"\n            },\n            \"reason\": \"ScalingReplicaSet\",\n            \"message\": \"Scaled up replica set monitoring-grafana-3344903701 to 1\",\n            \"source\": {\n                \"component\": \"deployment-controller\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:48Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:48Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-influxdb-421024531-2pvmk.14a8379769825568\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-influxdb-421024531-2pvmk.14a8379769825568\",\n                \"uid\": \"11ffc105-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35447\",\n                \"creationTimestamp\": \"2017-03-03T00:26:40Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Pod\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-influxdb-421024531-2pvmk\",\n                \"uid\": \"fb85682f-fe13-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"v1\",\n                \"resourceVersion\": \"4832575\",\n                \"fieldPath\": \"spec.containers{influxdb}\"\n            },\n            \"reason\": \"Killing\",\n            \"message\": \"Killing container with docker id d389fac70ecc: Need to kill pod.\",\n            \"source\": {\n                \"component\": \"kubelet\",\n                \"host\": \"ip-172-20-41-204.ec2.internal\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:26:40Z\",\n            \"lastTimestamp\": \"2017-03-03T00:26:40Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-influxdb-421024531-vwd0o.14a837a7916bb14d\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-influxdb-421024531-vwd0o.14a837a7916bb14d\",\n                \"uid\": \"3b5c056c-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35456\",\n                \"creationTimestamp\": \"2017-03-03T00:27:49Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Pod\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-influxdb-421024531-vwd0o\",\n                \"uid\": \"3b589343-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"v1\",\n                \"resourceVersion\": \"5110312\"\n            },\n            \"reason\": \"Scheduled\",\n            \"message\": \"Successfully assigned monitoring-influxdb-421024531-vwd0o to ip-172-20-41-204.ec2.internal\",\n            \"source\": {\n                \"component\": \"default-scheduler\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:49Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:49Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-influxdb-421024531-vwd0o.14a837a7c5e350eb\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-influxdb-421024531-vwd0o.14a837a7c5e350eb\",\n                \"uid\": \"3be205d6-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35459\",\n                \"creationTimestamp\": \"2017-03-03T00:27:50Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Pod\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-influxdb-421024531-vwd0o\",\n                \"uid\": \"3b589343-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"v1\",\n                \"resourceVersion\": \"5110314\",\n                \"fieldPath\": \"spec.containers{influxdb}\"\n            },\n            \"reason\": \"Pulled\",\n            \"message\": \"Container image \\\"gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\\\" already present on machine\",\n            \"source\": {\n                \"component\": \"kubelet\",\n                \"host\": \"ip-172-20-41-204.ec2.internal\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:50Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:50Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-influxdb-421024531-vwd0o.14a837a7d63fac51\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-influxdb-421024531-vwd0o.14a837a7d63fac51\",\n                \"uid\": \"3c0bd0c9-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35462\",\n                \"creationTimestamp\": \"2017-03-03T00:27:50Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Pod\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-influxdb-421024531-vwd0o\",\n                \"uid\": \"3b589343-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"v1\",\n                \"resourceVersion\": \"5110314\",\n                \"fieldPath\": \"spec.containers{influxdb}\"\n            },\n            \"reason\": \"Created\",\n            \"message\": \"Created container with docker id 03c52d1ae3be; Security:[seccomp=unconfined]\",\n            \"source\": {\n                \"component\": \"kubelet\",\n                \"host\": \"ip-172-20-41-204.ec2.internal\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:50Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:50Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-influxdb-421024531-vwd0o.14a837a7dbf4a98d\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-influxdb-421024531-vwd0o.14a837a7dbf4a98d\",\n                \"uid\": \"3c1a6eb2-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35464\",\n                \"creationTimestamp\": \"2017-03-03T00:27:50Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Pod\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-influxdb-421024531-vwd0o\",\n                \"uid\": \"3b589343-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"v1\",\n                \"resourceVersion\": \"5110314\",\n                \"fieldPath\": \"spec.containers{influxdb}\"\n            },\n            \"reason\": \"Started\",\n            \"message\": \"Started container with docker id 03c52d1ae3be\",\n            \"source\": {\n                \"component\": \"kubelet\",\n                \"host\": \"ip-172-20-41-204.ec2.internal\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:50Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:50Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-influxdb-421024531.14a83796f29fe2f3\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-influxdb-421024531.14a83796f29fe2f3\",\n                \"uid\": \"10cfb567-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35446\",\n                \"creationTimestamp\": \"2017-03-03T00:26:38Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"ReplicaSet\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-influxdb-421024531\",\n                \"uid\": \"fb84df83-fe13-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"extensions\",\n                \"resourceVersion\": \"5110154\"\n            },\n            \"reason\": \"SuccessfulDelete\",\n            \"message\": \"Deleted pod: monitoring-influxdb-421024531-2pvmk\",\n            \"source\": {\n                \"component\": \"replicaset-controller\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:26:38Z\",\n            \"lastTimestamp\": \"2017-03-03T00:26:38Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-influxdb-421024531.14a837a790623c24\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-influxdb-421024531.14a837a790623c24\",\n                \"uid\": \"3b595a1e-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35455\",\n                \"creationTimestamp\": \"2017-03-03T00:27:49Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"ReplicaSet\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-influxdb-421024531\",\n                \"uid\": \"3b580ba4-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"extensions\",\n                \"resourceVersion\": \"5110310\"\n            },\n            \"reason\": \"SuccessfulCreate\",\n            \"message\": \"Created pod: monitoring-influxdb-421024531-vwd0o\",\n            \"source\": {\n                \"component\": \"replicaset-controller\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:49Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:49Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-influxdb.14a83796f2562e5e\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-influxdb.14a83796f2562e5e\",\n                \"uid\": \"10cf099a-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35445\",\n                \"creationTimestamp\": \"2017-03-03T00:26:38Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Deployment\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-influxdb\",\n                \"uid\": \"fb84674b-fe13-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"extensions\",\n                \"resourceVersion\": \"5110153\"\n            },\n            \"reason\": \"ScalingReplicaSet\",\n            \"message\": \"Scaled down replica set monitoring-influxdb-421024531 to 0\",\n            \"source\": {\n                \"component\": \"deployment-controller\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:26:38Z\",\n            \"lastTimestamp\": \"2017-03-03T00:26:38Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-influxdb.14a837a7900016fc\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/events/monitoring-influxdb.14a837a7900016fc\",\n                \"uid\": \"3b5871ed-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"35454\",\n                \"creationTimestamp\": \"2017-03-03T00:27:49Z\"\n            },\n            \"involvedObject\": {\n                \"kind\": \"Deployment\",\n                \"namespace\": \"kube-system\",\n                \"name\": \"monitoring-influxdb\",\n                \"uid\": \"3b578452-ffa8-11e6-963a-12cd63c7b19c\",\n                \"apiVersion\": \"extensions\",\n                \"resourceVersion\": \"5110309\"\n            },\n            \"reason\": \"ScalingReplicaSet\",\n            \"message\": \"Scaled up replica set monitoring-influxdb-421024531 to 1\",\n            \"source\": {\n                \"component\": \"deployment-controller\"\n            },\n            \"firstTimestamp\": \"2017-03-03T00:27:49Z\",\n            \"lastTimestamp\": \"2017-03-03T00:27:49Z\",\n            \"count\": 1,\n            \"type\": \"Normal\"\n        }\n    ]\n}\n{\n    \"metadata\": {\n        \"selfLink\": \"/api/v1/namespaces/kube-system/replicationcontrollers\",\n        \"resourceVersion\": \"5111708\"\n    },\n    \"items\": []\n}\n{\n    \"metadata\": {\n        \"selfLink\": \"/api/v1/namespaces/kube-system/services\",\n        \"resourceVersion\": \"5111708\"\n    },\n    \"items\": [\n        {\n            \"metadata\": {\n                \"name\": \"heapster\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/services/heapster\",\n                \"uid\": \"3b276502-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"5110307\",\n                \"creationTimestamp\": \"2017-03-03T00:27:49Z\",\n                \"labels\": {\n                    \"kubernetes.io/cluster-service\": \"true\",\n                    \"kubernetes.io/name\": \"Heapster\",\n                    \"task\": \"monitoring\"\n                }\n            },\n            \"spec\": {\n                \"type\": \"ClusterIP\",\n                \"ports\": [\n                    {\n                        \"name\": \"\",\n                        \"protocol\": \"TCP\",\n                        \"port\": 80,\n                        \"targetPort\": 8082,\n                        \"nodePort\": 0\n                    }\n                ],\n                \"selector\": {\n                    \"k8s-app\": \"heapster\"\n                },\n                \"clusterIP\": \"100.70.12.237\",\n                \"ExternalName\": \"\",\n                \"sessionAffinity\": \"None\"\n            },\n            \"status\": {\n                \"loadBalancer\": {}\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"kube-dns\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/services/kube-dns\",\n                \"uid\": \"10b79cc2-e3e5-11e6-8dc2-12021b6d724c\",\n                \"resourceVersion\": \"102\",\n                \"creationTimestamp\": \"2017-01-26T16:32:44Z\",\n                \"labels\": {\n                    \"k8s-addon\": \"kube-dns.addons.k8s.io\",\n                    \"k8s-app\": \"kube-dns\",\n                    \"kubernetes.io/cluster-service\": \"true\",\n                    \"kubernetes.io/name\": \"KubeDNS\"\n                },\n                \"annotations\": {\n                    \"kubectl.kubernetes.io/last-applied-configuration\": \"{\\\"kind\\\":\\\"Service\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"metadata\\\":{\\\"name\\\":\\\"kube-dns\\\",\\\"namespace\\\":\\\"kube-system\\\",\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"k8s-addon\\\":\\\"kube-dns.addons.k8s.io\\\",\\\"k8s-app\\\":\\\"kube-dns\\\",\\\"kubernetes.io/cluster-service\\\":\\\"true\\\",\\\"kubernetes.io/name\\\":\\\"KubeDNS\\\"}},\\\"spec\\\":{\\\"ports\\\":[{\\\"name\\\":\\\"dns\\\",\\\"protocol\\\":\\\"UDP\\\",\\\"port\\\":53,\\\"targetPort\\\":0},{\\\"name\\\":\\\"dns-tcp\\\",\\\"protocol\\\":\\\"TCP\\\",\\\"port\\\":53,\\\"targetPort\\\":0}],\\\"selector\\\":{\\\"k8s-app\\\":\\\"kube-dns\\\"},\\\"clusterIP\\\":\\\"100.64.0.10\\\"},\\\"status\\\":{\\\"loadBalancer\\\":{}}}\"\n                }\n            },\n            \"spec\": {\n                \"type\": \"ClusterIP\",\n                \"ports\": [\n                    {\n                        \"name\": \"dns\",\n                        \"protocol\": \"UDP\",\n                        \"port\": 53,\n                        \"targetPort\": 53,\n                        \"nodePort\": 0\n                    },\n                    {\n                        \"name\": \"dns-tcp\",\n                        \"protocol\": \"TCP\",\n                        \"port\": 53,\n                        \"targetPort\": 53,\n                        \"nodePort\": 0\n                    }\n                ],\n                \"selector\": {\n                    \"k8s-app\": \"kube-dns\"\n                },\n                \"clusterIP\": \"100.64.0.10\",\n                \"ExternalName\": \"\",\n                \"sessionAffinity\": \"None\"\n            },\n            \"status\": {\n                \"loadBalancer\": {}\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-grafana\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/services/monitoring-grafana\",\n                \"uid\": \"3afc37c3-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"5110293\",\n                \"creationTimestamp\": \"2017-03-03T00:27:49Z\",\n                \"labels\": {\n                    \"kubernetes.io/cluster-service\": \"true\",\n                    \"kubernetes.io/name\": \"monitoring-grafana\"\n                }\n            },\n            \"spec\": {\n                \"type\": \"ClusterIP\",\n                \"ports\": [\n                    {\n                        \"name\": \"\",\n                        \"protocol\": \"TCP\",\n                        \"port\": 80,\n                        \"targetPort\": 3000,\n                        \"nodePort\": 0\n                    }\n                ],\n                \"selector\": {\n                    \"k8s-app\": \"grafana\"\n                },\n                \"clusterIP\": \"100.68.206.101\",\n                \"ExternalName\": \"\",\n                \"sessionAffinity\": \"None\"\n            },\n            \"status\": {\n                \"loadBalancer\": {}\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-influxdb\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/services/monitoring-influxdb\",\n                \"uid\": \"3b6f18e1-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"5110321\",\n                \"creationTimestamp\": \"2017-03-03T00:27:49Z\",\n                \"labels\": {\n                    \"kubernetes.io/cluster-service\": \"true\",\n                    \"kubernetes.io/name\": \"monitoring-influxdb\",\n                    \"task\": \"monitoring\"\n                }\n            },\n            \"spec\": {\n                \"type\": \"ClusterIP\",\n                \"ports\": [\n                    {\n                        \"name\": \"\",\n                        \"protocol\": \"TCP\",\n                        \"port\": 8086,\n                        \"targetPort\": 8086,\n                        \"nodePort\": 0\n                    }\n                ],\n                \"selector\": {\n                    \"k8s-app\": \"influxdb\"\n                },\n                \"clusterIP\": \"100.67.200.183\",\n                \"ExternalName\": \"\",\n                \"sessionAffinity\": \"None\"\n            },\n            \"status\": {\n                \"loadBalancer\": {}\n            }\n        }\n    ]\n}\n{\n    \"metadata\": {\n        \"selfLink\": \"/apis/extensions/v1beta1/namespaces/kube-system/daemonsets\",\n        \"resourceVersion\": \"5111708\"\n    },\n    \"items\": []\n}\n{\n    \"metadata\": {\n        \"selfLink\": \"/apis/extensions/v1beta1/namespaces/kube-system/deployments\",\n        \"resourceVersion\": \"5111708\"\n    },\n    \"items\": [\n        {\n            \"metadata\": {\n                \"name\": \"dns-controller\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/apis/extensions/v1beta1/namespaces/kube-system/deployments/dns-controller\",\n                \"uid\": \"0ff938bf-e3e5-11e6-8dc2-12021b6d724c\",\n                \"resourceVersion\": \"2906082\",\n                \"generation\": 2,\n                \"creationTimestamp\": \"2017-01-26T16:32:43Z\",\n                \"labels\": {\n                    \"k8s-addon\": \"dns-controller.addons.k8s.io\",\n                    \"k8s-app\": \"dns-controller\",\n                    \"version\": \"v1.5.1\"\n                },\n                \"annotations\": {\n                    \"deployment.kubernetes.io/revision\": \"1\",\n                    \"kubectl.kubernetes.io/last-applied-configuration\": \"{\\\"kind\\\":\\\"Deployment\\\",\\\"apiVersion\\\":\\\"extensions/v1beta1\\\",\\\"metadata\\\":{\\\"name\\\":\\\"dns-controller\\\",\\\"namespace\\\":\\\"kube-system\\\",\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"k8s-addon\\\":\\\"dns-controller.addons.k8s.io\\\",\\\"k8s-app\\\":\\\"dns-controller\\\",\\\"version\\\":\\\"v1.5.1\\\"}},\\\"spec\\\":{\\\"replicas\\\":1,\\\"selector\\\":{\\\"matchLabels\\\":{\\\"k8s-app\\\":\\\"dns-controller\\\"}},\\\"template\\\":{\\\"metadata\\\":{\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"k8s-addon\\\":\\\"dns-controller.addons.k8s.io\\\",\\\"k8s-app\\\":\\\"dns-controller\\\",\\\"version\\\":\\\"v1.5.1\\\"},\\\"annotations\\\":{\\\"scheduler.alpha.kubernetes.io/critical-pod\\\":\\\"\\\",\\\"scheduler.alpha.kubernetes.io/tolerations\\\":\\\"[{\\\\\"key\\\\\": \\\\\"dedicated\\\\\", \\\\\"value\\\\\": \\\\\"master\\\\\"}]\\\"}},\\\"spec\\\":{\\\"containers\\\":[{\\\"name\\\":\\\"dns-controller\\\",\\\"image\\\":\\\"kope/dns-controller:1.5.1\\\",\\\"command\\\":[\\\"/usr/bin/dns-controller\\\",\\\"--watch-ingress=false\\\",\\\"--dns=aws-route53\\\",\\\"--zone=kubernetes.com\\\",\\\"--zone=/\\\",\\\"-v=2\\\"],\\\"resources\\\":{\\\"requests\\\":{\\\"cpu\\\":\\\"50m\\\",\\\"memory\\\":\\\"50Mi\\\"}}}],\\\"dnsPolicy\\\":\\\"Default\\\",\\\"nodeSelector\\\":{\\\"kubernetes.io/role\\\":\\\"master\\\"},\\\"hostNetwork\\\":true}},\\\"strategy\\\":{}},\\\"status\\\":{}}\"\n                }\n            },\n            \"spec\": {\n                \"replicas\": 1,\n                \"selector\": {\n                    \"matchLabels\": {\n                        \"k8s-app\": \"dns-controller\"\n                    }\n                },\n                \"template\": {\n                    \"metadata\": {\n                        \"creationTimestamp\": null,\n                        \"labels\": {\n                            \"k8s-addon\": \"dns-controller.addons.k8s.io\",\n                            \"k8s-app\": \"dns-controller\",\n                            \"version\": \"v1.5.1\"\n                        },\n                        \"annotations\": {\n                            \"scheduler.alpha.kubernetes.io/critical-pod\": \"\",\n                            \"scheduler.alpha.kubernetes.io/tolerations\": \"[{\\\"key\\\": \\\"dedicated\\\", \\\"value\\\": \\\"master\\\"}]\"\n                        }\n                    },\n                    \"spec\": {\n                        \"volumes\": null,\n                        \"containers\": [\n                            {\n                                \"name\": \"dns-controller\",\n                                \"image\": \"kope/dns-controller:1.5.1\",\n                                \"command\": [\n                                    \"/usr/bin/dns-controller\",\n                                    \"--watch-ingress=false\",\n                                    \"--dns=aws-route53\",\n                                    \"--zone=kubernetes.com\",\n                                    \"--zone=/\",\n                                    \"-v=2\"\n                                ],\n                                \"resources\": {\n                                    \"requests\": {\n                                        \"cpu\": \"50m\",\n                                        \"memory\": \"50Mi\"\n                                    }\n                                },\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            }\n                        ],\n                        \"restartPolicy\": \"Always\",\n                        \"terminationGracePeriodSeconds\": 30,\n                        \"dnsPolicy\": \"Default\",\n                        \"nodeSelector\": {\n                            \"kubernetes.io/role\": \"master\"\n                        },\n                        \"serviceAccountName\": \"\",\n                        \"securityContext\": {\n                            \"hostNetwork\": true\n                        }\n                    }\n                },\n                \"strategy\": {\n                    \"type\": \"RollingUpdate\",\n                    \"rollingUpdate\": {\n                        \"maxUnavailable\": 1,\n                        \"maxSurge\": 1\n                    }\n                }\n            },\n            \"status\": {\n                \"observedGeneration\": 2,\n                \"replicas\": 1,\n                \"updatedReplicas\": 1,\n                \"availableReplicas\": 1\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"heapster\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/apis/extensions/v1beta1/namespaces/kube-system/deployments/heapster\",\n                \"uid\": \"3b0e8a69-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"5110332\",\n                \"generation\": 2,\n                \"creationTimestamp\": \"2017-03-03T00:27:49Z\",\n                \"labels\": {\n                    \"k8s-app\": \"heapster\",\n                    \"task\": \"monitoring\"\n                },\n                \"annotations\": {\n                    \"deployment.kubernetes.io/revision\": \"1\"\n                }\n            },\n            \"spec\": {\n                \"replicas\": 1,\n                \"selector\": {\n                    \"matchLabels\": {\n                        \"k8s-app\": \"heapster\",\n                        \"task\": \"monitoring\"\n                    }\n                },\n                \"template\": {\n                    \"metadata\": {\n                        \"creationTimestamp\": null,\n                        \"labels\": {\n                            \"k8s-app\": \"heapster\",\n                            \"task\": \"monitoring\"\n                        }\n                    },\n                    \"spec\": {\n                        \"volumes\": null,\n                        \"containers\": [\n                            {\n                                \"name\": \"heapster\",\n                                \"image\": \"gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1\",\n                                \"command\": [\n                                    \"/heapster\",\n                                    \"--source=kubernetes:https://kubernetes.default\",\n                                    \"--sink=influxdb:http://monitoring-influxdb:8086\"\n                                ],\n                                \"resources\": {},\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            }\n                        ],\n                        \"restartPolicy\": \"Always\",\n                        \"terminationGracePeriodSeconds\": 30,\n                        \"dnsPolicy\": \"ClusterFirst\",\n                        \"serviceAccountName\": \"\",\n                        \"securityContext\": {}\n                    }\n                },\n                \"strategy\": {\n                    \"type\": \"RollingUpdate\",\n                    \"rollingUpdate\": {\n                        \"maxUnavailable\": 1,\n                        \"maxSurge\": 1\n                    }\n                }\n            },\n            \"status\": {\n                \"observedGeneration\": 2,\n                \"replicas\": 1,\n                \"updatedReplicas\": 1,\n                \"availableReplicas\": 1\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"kube-dns\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/apis/extensions/v1beta1/namespaces/kube-system/deployments/kube-dns\",\n                \"uid\": \"10ae67f6-e3e5-11e6-8dc2-12021b6d724c\",\n                \"resourceVersion\": \"2906324\",\n                \"generation\": 2,\n                \"creationTimestamp\": \"2017-01-26T16:32:44Z\",\n                \"labels\": {\n                    \"k8s-addon\": \"kube-dns.addons.k8s.io\",\n                    \"k8s-app\": \"kube-dns\",\n                    \"kubernetes.io/cluster-service\": \"true\"\n                },\n                \"annotations\": {\n                    \"deployment.kubernetes.io/revision\": \"1\",\n                    \"kubectl.kubernetes.io/last-applied-configuration\": \"{\\\"kind\\\":\\\"Deployment\\\",\\\"apiVersion\\\":\\\"extensions/v1beta1\\\",\\\"metadata\\\":{\\\"name\\\":\\\"kube-dns\\\",\\\"namespace\\\":\\\"kube-system\\\",\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"k8s-addon\\\":\\\"kube-dns.addons.k8s.io\\\",\\\"k8s-app\\\":\\\"kube-dns\\\",\\\"kubernetes.io/cluster-service\\\":\\\"true\\\"}},\\\"spec\\\":{\\\"selector\\\":{\\\"matchLabels\\\":{\\\"k8s-app\\\":\\\"kube-dns\\\"}},\\\"template\\\":{\\\"metadata\\\":{\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"k8s-app\\\":\\\"kube-dns\\\"},\\\"annotations\\\":{\\\"scheduler.alpha.kubernetes.io/critical-pod\\\":\\\"\\\",\\\"scheduler.alpha.kubernetes.io/tolerations\\\":\\\"[{\\\\\"key\\\\\":\\\\\"CriticalAddonsOnly\\\\\", \\\\\"operator\\\\\":\\\\\"Exists\\\\\"}]\\\"}},\\\"spec\\\":{\\\"containers\\\":[{\\\"name\\\":\\\"kubedns\\\",\\\"image\\\":\\\"gcr.io/google_containers/kubedns-amd64:1.9\\\",\\\"args\\\":[\\\"--domain=cluster.local.\\\",\\\"--dns-port=10053\\\",\\\"--config-map=kube-dns\\\",\\\"--v=2\\\"],\\\"ports\\\":[{\\\"name\\\":\\\"dns-local\\\",\\\"containerPort\\\":10053,\\\"protocol\\\":\\\"UDP\\\"},{\\\"name\\\":\\\"dns-tcp-local\\\",\\\"containerPort\\\":10053,\\\"protocol\\\":\\\"TCP\\\"},{\\\"name\\\":\\\"metrics\\\",\\\"containerPort\\\":10055,\\\"protocol\\\":\\\"TCP\\\"}],\\\"env\\\":[{\\\"name\\\":\\\"PROMETHEUS_PORT\\\",\\\"value\\\":\\\"10055\\\"}],\\\"resources\\\":{\\\"limits\\\":{\\\"memory\\\":\\\"170Mi\\\"},\\\"requests\\\":{\\\"cpu\\\":\\\"100m\\\",\\\"memory\\\":\\\"70Mi\\\"}},\\\"livenessProbe\\\":{\\\"httpGet\\\":{\\\"path\\\":\\\"/healthz-kubedns\\\",\\\"port\\\":8080,\\\"scheme\\\":\\\"HTTP\\\"},\\\"initialDelaySeconds\\\":60,\\\"timeoutSeconds\\\":5,\\\"successThreshold\\\":1,\\\"failureThreshold\\\":5},\\\"readinessProbe\\\":{\\\"httpGet\\\":{\\\"path\\\":\\\"/readiness\\\",\\\"port\\\":8081,\\\"scheme\\\":\\\"HTTP\\\"},\\\"initialDelaySeconds\\\":3,\\\"timeoutSeconds\\\":5}},{\\\"name\\\":\\\"dnsmasq\\\",\\\"image\\\":\\\"gcr.io/google_containers/kube-dnsmasq-amd64:1.4\\\",\\\"args\\\":[\\\"--cache-size=1000\\\",\\\"--no-resolv\\\",\\\"--server=127.0.0.1#10053\\\",\\\"--log-facility=-\\\"],\\\"ports\\\":[{\\\"name\\\":\\\"dns\\\",\\\"containerPort\\\":53,\\\"protocol\\\":\\\"UDP\\\"},{\\\"name\\\":\\\"dns-tcp\\\",\\\"containerPort\\\":53,\\\"protocol\\\":\\\"TCP\\\"}],\\\"resources\\\":{\\\"requests\\\":{\\\"cpu\\\":\\\"150m\\\",\\\"memory\\\":\\\"10Mi\\\"}},\\\"livenessProbe\\\":{\\\"httpGet\\\":{\\\"path\\\":\\\"/healthz-dnsmasq\\\",\\\"port\\\":8080,\\\"scheme\\\":\\\"HTTP\\\"},\\\"initialDelaySeconds\\\":60,\\\"timeoutSeconds\\\":5,\\\"successThreshold\\\":1,\\\"failureThreshold\\\":5}},{\\\"name\\\":\\\"dnsmasq-metrics\\\",\\\"image\\\":\\\"gcr.io/google_containers/dnsmasq-metrics-amd64:1.0\\\",\\\"args\\\":[\\\"--v=2\\\",\\\"--logtostderr\\\"],\\\"ports\\\":[{\\\"name\\\":\\\"metrics\\\",\\\"containerPort\\\":10054,\\\"protocol\\\":\\\"TCP\\\"}],\\\"resources\\\":{\\\"requests\\\":{\\\"memory\\\":\\\"10Mi\\\"}},\\\"livenessProbe\\\":{\\\"httpGet\\\":{\\\"path\\\":\\\"/metrics\\\",\\\"port\\\":10054,\\\"scheme\\\":\\\"HTTP\\\"},\\\"initialDelaySeconds\\\":60,\\\"timeoutSeconds\\\":5,\\\"successThreshold\\\":1,\\\"failureThreshold\\\":5}},{\\\"name\\\":\\\"healthz\\\",\\\"image\\\":\\\"gcr.io/google_containers/exechealthz-amd64:1.2\\\",\\\"args\\\":[\\\"--cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 \\u003e/dev/null\\\",\\\"--url=/healthz-dnsmasq\\\",\\\"--cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 \\u003e/dev/null\\\",\\\"--url=/healthz-kubedns\\\",\\\"--port=8080\\\",\\\"--quiet\\\"],\\\"ports\\\":[{\\\"containerPort\\\":8080,\\\"protocol\\\":\\\"TCP\\\"}],\\\"resources\\\":{\\\"limits\\\":{\\\"memory\\\":\\\"50Mi\\\"},\\\"requests\\\":{\\\"cpu\\\":\\\"10m\\\",\\\"memory\\\":\\\"50Mi\\\"}}}],\\\"dnsPolicy\\\":\\\"Default\\\"}},\\\"strategy\\\":{\\\"rollingUpdate\\\":{\\\"maxUnavailable\\\":0,\\\"maxSurge\\\":\\\"10%\\\"}}},\\\"status\\\":{}}\"\n                }\n            },\n            \"spec\": {\n                \"replicas\": 1,\n                \"selector\": {\n                    \"matchLabels\": {\n                        \"k8s-app\": \"kube-dns\"\n                    }\n                },\n                \"template\": {\n                    \"metadata\": {\n                        \"creationTimestamp\": null,\n                        \"labels\": {\n                            \"k8s-app\": \"kube-dns\"\n                        },\n                        \"annotations\": {\n                            \"scheduler.alpha.kubernetes.io/critical-pod\": \"\",\n                            \"scheduler.alpha.kubernetes.io/tolerations\": \"[{\\\"key\\\":\\\"CriticalAddonsOnly\\\", \\\"operator\\\":\\\"Exists\\\"}]\"\n                        }\n                    },\n                    \"spec\": {\n                        \"volumes\": null,\n                        \"containers\": [\n                            {\n                                \"name\": \"kubedns\",\n                                \"image\": \"gcr.io/google_containers/kubedns-amd64:1.9\",\n                                \"args\": [\n                                    \"--domain=cluster.local.\",\n                                    \"--dns-port=10053\",\n                                    \"--config-map=kube-dns\",\n                                    \"--v=2\"\n                                ],\n                                \"ports\": [\n                                    {\n                                        \"name\": \"dns-local\",\n                                        \"containerPort\": 10053,\n                                        \"protocol\": \"UDP\"\n                                    },\n                                    {\n                                        \"name\": \"dns-tcp-local\",\n                                        \"containerPort\": 10053,\n                                        \"protocol\": \"TCP\"\n                                    },\n                                    {\n                                        \"name\": \"metrics\",\n                                        \"containerPort\": 10055,\n                                        \"protocol\": \"TCP\"\n                                    }\n                                ],\n                                \"env\": [\n                                    {\n                                        \"name\": \"PROMETHEUS_PORT\",\n                                        \"value\": \"10055\"\n                                    }\n                                ],\n                                \"resources\": {\n                                    \"limits\": {\n                                        \"memory\": \"170Mi\"\n                                    },\n                                    \"requests\": {\n                                        \"cpu\": \"100m\",\n                                        \"memory\": \"70Mi\"\n                                    }\n                                },\n                                \"livenessProbe\": {\n                                    \"httpGet\": {\n                                        \"path\": \"/healthz-kubedns\",\n                                        \"port\": 8080,\n                                        \"scheme\": \"HTTP\"\n                                    },\n                                    \"initialDelaySeconds\": 60,\n                                    \"timeoutSeconds\": 5,\n                                    \"periodSeconds\": 10,\n                                    \"successThreshold\": 1,\n                                    \"failureThreshold\": 5\n                                },\n                                \"readinessProbe\": {\n                                    \"httpGet\": {\n                                        \"path\": \"/readiness\",\n                                        \"port\": 8081,\n                                        \"scheme\": \"HTTP\"\n                                    },\n                                    \"initialDelaySeconds\": 3,\n                                    \"timeoutSeconds\": 5,\n                                    \"periodSeconds\": 10,\n                                    \"successThreshold\": 1,\n                                    \"failureThreshold\": 3\n                                },\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            },\n                            {\n                                \"name\": \"dnsmasq\",\n                                \"image\": \"gcr.io/google_containers/kube-dnsmasq-amd64:1.4\",\n                                \"args\": [\n                                    \"--cache-size=1000\",\n                                    \"--no-resolv\",\n                                    \"--server=127.0.0.1#10053\",\n                                    \"--log-facility=-\"\n                                ],\n                                \"ports\": [\n                                    {\n                                        \"name\": \"dns\",\n                                        \"containerPort\": 53,\n                                        \"protocol\": \"UDP\"\n                                    },\n                                    {\n                                        \"name\": \"dns-tcp\",\n                                        \"containerPort\": 53,\n                                        \"protocol\": \"TCP\"\n                                    }\n                                ],\n                                \"resources\": {\n                                    \"requests\": {\n                                        \"cpu\": \"150m\",\n                                        \"memory\": \"10Mi\"\n                                    }\n                                },\n                                \"livenessProbe\": {\n                                    \"httpGet\": {\n                                        \"path\": \"/healthz-dnsmasq\",\n                                        \"port\": 8080,\n                                        \"scheme\": \"HTTP\"\n                                    },\n                                    \"initialDelaySeconds\": 60,\n                                    \"timeoutSeconds\": 5,\n                                    \"periodSeconds\": 10,\n                                    \"successThreshold\": 1,\n                                    \"failureThreshold\": 5\n                                },\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            },\n                            {\n                                \"name\": \"dnsmasq-metrics\",\n                                \"image\": \"gcr.io/google_containers/dnsmasq-metrics-amd64:1.0\",\n                                \"args\": [\n                                    \"--v=2\",\n                                    \"--logtostderr\"\n                                ],\n                                \"ports\": [\n                                    {\n                                        \"name\": \"metrics\",\n                                        \"containerPort\": 10054,\n                                        \"protocol\": \"TCP\"\n                                    }\n                                ],\n                                \"resources\": {\n                                    \"requests\": {\n                                        \"memory\": \"10Mi\"\n                                    }\n                                },\n                                \"livenessProbe\": {\n                                    \"httpGet\": {\n                                        \"path\": \"/metrics\",\n                                        \"port\": 10054,\n                                        \"scheme\": \"HTTP\"\n                                    },\n                                    \"initialDelaySeconds\": 60,\n                                    \"timeoutSeconds\": 5,\n                                    \"periodSeconds\": 10,\n                                    \"successThreshold\": 1,\n                                    \"failureThreshold\": 5\n                                },\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            },\n                            {\n                                \"name\": \"healthz\",\n                                \"image\": \"gcr.io/google_containers/exechealthz-amd64:1.2\",\n                                \"args\": [\n                                    \"--cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 \\u003e/dev/null\",\n                                    \"--url=/healthz-dnsmasq\",\n                                    \"--cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 \\u003e/dev/null\",\n                                    \"--url=/healthz-kubedns\",\n                                    \"--port=8080\",\n                                    \"--quiet\"\n                                ],\n                                \"ports\": [\n                                    {\n                                        \"containerPort\": 8080,\n                                        \"protocol\": \"TCP\"\n                                    }\n                                ],\n                                \"resources\": {\n                                    \"limits\": {\n                                        \"memory\": \"50Mi\"\n                                    },\n                                    \"requests\": {\n                                        \"cpu\": \"10m\",\n                                        \"memory\": \"50Mi\"\n                                    }\n                                },\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            }\n                        ],\n                        \"restartPolicy\": \"Always\",\n                        \"terminationGracePeriodSeconds\": 30,\n                        \"dnsPolicy\": \"Default\",\n                        \"serviceAccountName\": \"\",\n                        \"securityContext\": {}\n                    }\n                },\n                \"strategy\": {\n                    \"type\": \"RollingUpdate\",\n                    \"rollingUpdate\": {\n                        \"maxUnavailable\": 0,\n                        \"maxSurge\": \"10%\"\n                    }\n                }\n            },\n            \"status\": {\n                \"observedGeneration\": 2,\n                \"replicas\": 1,\n                \"updatedReplicas\": 1,\n                \"availableReplicas\": 1\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"kube-dns-autoscaler\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/apis/extensions/v1beta1/namespaces/kube-system/deployments/kube-dns-autoscaler\",\n                \"uid\": \"10a8b789-e3e5-11e6-8dc2-12021b6d724c\",\n                \"resourceVersion\": \"2906265\",\n                \"generation\": 2,\n                \"creationTimestamp\": \"2017-01-26T16:32:44Z\",\n                \"labels\": {\n                    \"k8s-addon\": \"kube-dns.addons.k8s.io\",\n                    \"k8s-app\": \"kube-dns-autoscaler\",\n                    \"kubernetes.io/cluster-service\": \"true\"\n                },\n                \"annotations\": {\n                    \"deployment.kubernetes.io/revision\": \"1\",\n                    \"kubectl.kubernetes.io/last-applied-configuration\": \"{\\\"kind\\\":\\\"Deployment\\\",\\\"apiVersion\\\":\\\"extensions/v1beta1\\\",\\\"metadata\\\":{\\\"name\\\":\\\"kube-dns-autoscaler\\\",\\\"namespace\\\":\\\"kube-system\\\",\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"k8s-addon\\\":\\\"kube-dns.addons.k8s.io\\\",\\\"k8s-app\\\":\\\"kube-dns-autoscaler\\\",\\\"kubernetes.io/cluster-service\\\":\\\"true\\\"}},\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"creationTimestamp\\\":null,\\\"labels\\\":{\\\"k8s-app\\\":\\\"kube-dns-autoscaler\\\"},\\\"annotations\\\":{\\\"scheduler.alpha.kubernetes.io/critical-pod\\\":\\\"\\\",\\\"scheduler.alpha.kubernetes.io/tolerations\\\":\\\"[{\\\\\"key\\\\\":\\\\\"CriticalAddonsOnly\\\\\", \\\\\"operator\\\\\":\\\\\"Exists\\\\\"}]\\\"}},\\\"spec\\\":{\\\"containers\\\":[{\\\"name\\\":\\\"autoscaler\\\",\\\"image\\\":\\\"gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.0.0\\\",\\\"command\\\":[\\\"/cluster-proportional-autoscaler\\\",\\\"--namespace=kube-system\\\",\\\"--configmap=kube-dns-autoscaler\\\",\\\"--mode=linear\\\",\\\"--target=Deployment/kube-dns\\\",\\\"--default-params={\\\\\"linear\\\\\":{\\\\\"coresPerReplica\\\\\":256,\\\\\"nodesPerReplica\\\\\":16,\\\\\"min\\\\\":1}}\\\",\\\"--logtostderr=true\\\",\\\"--v=2\\\"],\\\"resources\\\":{\\\"requests\\\":{\\\"cpu\\\":\\\"20m\\\",\\\"memory\\\":\\\"10Mi\\\"}}}]}},\\\"strategy\\\":{}},\\\"status\\\":{}}\"\n                }\n            },\n            \"spec\": {\n                \"replicas\": 1,\n                \"selector\": {\n                    \"matchLabels\": {\n                        \"k8s-app\": \"kube-dns-autoscaler\"\n                    }\n                },\n                \"template\": {\n                    \"metadata\": {\n                        \"creationTimestamp\": null,\n                        \"labels\": {\n                            \"k8s-app\": \"kube-dns-autoscaler\"\n                        },\n                        \"annotations\": {\n                            \"scheduler.alpha.kubernetes.io/critical-pod\": \"\",\n                            \"scheduler.alpha.kubernetes.io/tolerations\": \"[{\\\"key\\\":\\\"CriticalAddonsOnly\\\", \\\"operator\\\":\\\"Exists\\\"}]\"\n                        }\n                    },\n                    \"spec\": {\n                        \"volumes\": null,\n                        \"containers\": [\n                            {\n                                \"name\": \"autoscaler\",\n                                \"image\": \"gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.0.0\",\n                                \"command\": [\n                                    \"/cluster-proportional-autoscaler\",\n                                    \"--namespace=kube-system\",\n                                    \"--configmap=kube-dns-autoscaler\",\n                                    \"--mode=linear\",\n                                    \"--target=Deployment/kube-dns\",\n                                    \"--default-params={\\\"linear\\\":{\\\"coresPerReplica\\\":256,\\\"nodesPerReplica\\\":16,\\\"min\\\":1}}\",\n                                    \"--logtostderr=true\",\n                                    \"--v=2\"\n                                ],\n                                \"resources\": {\n                                    \"requests\": {\n                                        \"cpu\": \"20m\",\n                                        \"memory\": \"10Mi\"\n                                    }\n                                },\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            }\n                        ],\n                        \"restartPolicy\": \"Always\",\n                        \"terminationGracePeriodSeconds\": 30,\n                        \"dnsPolicy\": \"ClusterFirst\",\n                        \"serviceAccountName\": \"\",\n                        \"securityContext\": {}\n                    }\n                },\n                \"strategy\": {\n                    \"type\": \"RollingUpdate\",\n                    \"rollingUpdate\": {\n                        \"maxUnavailable\": 1,\n                        \"maxSurge\": 1\n                    }\n                }\n            },\n            \"status\": {\n                \"observedGeneration\": 2,\n                \"replicas\": 1,\n                \"updatedReplicas\": 1,\n                \"availableReplicas\": 1\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-grafana\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/apis/extensions/v1beta1/namespaces/kube-system/deployments/monitoring-grafana\",\n                \"uid\": \"3ae4155a-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"5110328\",\n                \"generation\": 2,\n                \"creationTimestamp\": \"2017-03-03T00:27:48Z\",\n                \"labels\": {\n                    \"k8s-app\": \"grafana\",\n                    \"task\": \"monitoring\"\n                },\n                \"annotations\": {\n                    \"deployment.kubernetes.io/revision\": \"1\"\n                }\n            },\n            \"spec\": {\n                \"replicas\": 1,\n                \"selector\": {\n                    \"matchLabels\": {\n                        \"k8s-app\": \"grafana\",\n                        \"task\": \"monitoring\"\n                    }\n                },\n                \"template\": {\n                    \"metadata\": {\n                        \"creationTimestamp\": null,\n                        \"labels\": {\n                            \"k8s-app\": \"grafana\",\n                            \"task\": \"monitoring\"\n                        }\n                    },\n                    \"spec\": {\n                        \"volumes\": [\n                            {\n                                \"name\": \"grafana-storage\",\n                                \"emptyDir\": {}\n                            }\n                        ],\n                        \"containers\": [\n                            {\n                                \"name\": \"grafana\",\n                                \"image\": \"gcr.io/google_containers/heapster-grafana-amd64:v4.0.2\",\n                                \"ports\": [\n                                    {\n                                        \"containerPort\": 3000,\n                                        \"protocol\": \"TCP\"\n                                    }\n                                ],\n                                \"env\": [\n                                    {\n                                        \"name\": \"INFLUXDB_HOST\",\n                                        \"value\": \"monitoring-influxdb\"\n                                    },\n                                    {\n                                        \"name\": \"GRAFANA_PORT\",\n                                        \"value\": \"3000\"\n                                    },\n                                    {\n                                        \"name\": \"GF_AUTH_BASIC_ENABLED\",\n                                        \"value\": \"false\"\n                                    },\n                                    {\n                                        \"name\": \"GF_AUTH_ANONYMOUS_ENABLED\",\n                                        \"value\": \"true\"\n                                    },\n                                    {\n                                        \"name\": \"GF_AUTH_ANONYMOUS_ORG_ROLE\",\n                                        \"value\": \"Admin\"\n                                    },\n                                    {\n                                        \"name\": \"GF_SERVER_ROOT_URL\",\n                                        \"value\": \"/\"\n                                    }\n                                ],\n                                \"resources\": {},\n                                \"volumeMounts\": [\n                                    {\n                                        \"name\": \"grafana-storage\",\n                                        \"mountPath\": \"/var\"\n                                    }\n                                ],\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            }\n                        ],\n                        \"restartPolicy\": \"Always\",\n                        \"terminationGracePeriodSeconds\": 30,\n                        \"dnsPolicy\": \"ClusterFirst\",\n                        \"serviceAccountName\": \"\",\n                        \"securityContext\": {}\n                    }\n                },\n                \"strategy\": {\n                    \"type\": \"RollingUpdate\",\n                    \"rollingUpdate\": {\n                        \"maxUnavailable\": 1,\n                        \"maxSurge\": 1\n                    }\n                }\n            },\n            \"status\": {\n                \"observedGeneration\": 2,\n                \"replicas\": 1,\n                \"updatedReplicas\": 1,\n                \"availableReplicas\": 1\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-influxdb\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/apis/extensions/v1beta1/namespaces/kube-system/deployments/monitoring-influxdb\",\n                \"uid\": \"3b578452-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"5110336\",\n                \"generation\": 2,\n                \"creationTimestamp\": \"2017-03-03T00:27:49Z\",\n                \"labels\": {\n                    \"k8s-app\": \"influxdb\",\n                    \"task\": \"monitoring\"\n                },\n                \"annotations\": {\n                    \"deployment.kubernetes.io/revision\": \"1\"\n                }\n            },\n            \"spec\": {\n                \"replicas\": 1,\n                \"selector\": {\n                    \"matchLabels\": {\n                        \"k8s-app\": \"influxdb\",\n                        \"task\": \"monitoring\"\n                    }\n                },\n                \"template\": {\n                    \"metadata\": {\n                        \"creationTimestamp\": null,\n                        \"labels\": {\n                            \"k8s-app\": \"influxdb\",\n                            \"task\": \"monitoring\"\n                        }\n                    },\n                    \"spec\": {\n                        \"volumes\": [\n                            {\n                                \"name\": \"influxdb-storage\",\n                                \"emptyDir\": {}\n                            }\n                        ],\n                        \"containers\": [\n                            {\n                                \"name\": \"influxdb\",\n                                \"image\": \"gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\",\n                                \"resources\": {},\n                                \"volumeMounts\": [\n                                    {\n                                        \"name\": \"influxdb-storage\",\n                                        \"mountPath\": \"/data\"\n                                    }\n                                ],\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            }\n                        ],\n                        \"restartPolicy\": \"Always\",\n                        \"terminationGracePeriodSeconds\": 30,\n                        \"dnsPolicy\": \"ClusterFirst\",\n                        \"serviceAccountName\": \"\",\n                        \"securityContext\": {}\n                    }\n                },\n                \"strategy\": {\n                    \"type\": \"RollingUpdate\",\n                    \"rollingUpdate\": {\n                        \"maxUnavailable\": 1,\n                        \"maxSurge\": 1\n                    }\n                }\n            },\n            \"status\": {\n                \"observedGeneration\": 2,\n                \"replicas\": 1,\n                \"updatedReplicas\": 1,\n                \"availableReplicas\": 1\n            }\n        }\n    ]\n}\n{\n    \"metadata\": {\n        \"selfLink\": \"/apis/extensions/v1beta1/namespaces/kube-system/replicasets\",\n        \"resourceVersion\": \"5111708\"\n    },\n    \"items\": [\n        {\n            \"metadata\": {\n                \"name\": \"dns-controller-4128279831\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/apis/extensions/v1beta1/namespaces/kube-system/replicasets/dns-controller-4128279831\",\n                \"uid\": \"0ffa1251-e3e5-11e6-8dc2-12021b6d724c\",\n                \"resourceVersion\": \"2906081\",\n                \"generation\": 1,\n                \"creationTimestamp\": \"2017-01-26T16:32:43Z\",\n                \"labels\": {\n                    \"k8s-addon\": \"dns-controller.addons.k8s.io\",\n                    \"k8s-app\": \"dns-controller\",\n                    \"pod-template-hash\": \"4128279831\",\n                    \"version\": \"v1.5.1\"\n                },\n                \"annotations\": {\n                    \"deployment.kubernetes.io/desired-replicas\": \"1\",\n                    \"deployment.kubernetes.io/max-replicas\": \"2\",\n                    \"deployment.kubernetes.io/revision\": \"1\"\n                }\n            },\n            \"spec\": {\n                \"replicas\": 1,\n                \"selector\": {\n                    \"matchLabels\": {\n                        \"k8s-app\": \"dns-controller\",\n                        \"pod-template-hash\": \"4128279831\"\n                    }\n                },\n                \"template\": {\n                    \"metadata\": {\n                        \"creationTimestamp\": null,\n                        \"labels\": {\n                            \"k8s-addon\": \"dns-controller.addons.k8s.io\",\n                            \"k8s-app\": \"dns-controller\",\n                            \"pod-template-hash\": \"4128279831\",\n                            \"version\": \"v1.5.1\"\n                        },\n                        \"annotations\": {\n                            \"scheduler.alpha.kubernetes.io/critical-pod\": \"\",\n                            \"scheduler.alpha.kubernetes.io/tolerations\": \"[{\\\"key\\\": \\\"dedicated\\\", \\\"value\\\": \\\"master\\\"}]\"\n                        }\n                    },\n                    \"spec\": {\n                        \"volumes\": null,\n                        \"containers\": [\n                            {\n                                \"name\": \"dns-controller\",\n                                \"image\": \"kope/dns-controller:1.5.1\",\n                                \"command\": [\n                                    \"/usr/bin/dns-controller\",\n                                    \"--watch-ingress=false\",\n                                    \"--dns=aws-route53\",\n                                    \"--zone=kubernetes.com\",\n                                    \"--zone=/\",\n                                    \"-v=2\"\n                                ],\n                                \"resources\": {\n                                    \"requests\": {\n                                        \"cpu\": \"50m\",\n                                        \"memory\": \"50Mi\"\n                                    }\n                                },\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            }\n                        ],\n                        \"restartPolicy\": \"Always\",\n                        \"terminationGracePeriodSeconds\": 30,\n                        \"dnsPolicy\": \"Default\",\n                        \"nodeSelector\": {\n                            \"kubernetes.io/role\": \"master\"\n                        },\n                        \"serviceAccountName\": \"\",\n                        \"securityContext\": {\n                            \"hostNetwork\": true\n                        }\n                    }\n                }\n            },\n            \"status\": {\n                \"replicas\": 1,\n                \"fullyLabeledReplicas\": 1,\n                \"readyReplicas\": 1,\n                \"observedGeneration\": 1\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"heapster-564189836\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/apis/extensions/v1beta1/namespaces/kube-system/replicasets/heapster-564189836\",\n                \"uid\": \"3b0efc17-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"5110330\",\n                \"generation\": 1,\n                \"creationTimestamp\": \"2017-03-03T00:27:49Z\",\n                \"labels\": {\n                    \"k8s-app\": \"heapster\",\n                    \"pod-template-hash\": \"564189836\",\n                    \"task\": \"monitoring\"\n                },\n                \"annotations\": {\n                    \"deployment.kubernetes.io/desired-replicas\": \"1\",\n                    \"deployment.kubernetes.io/max-replicas\": \"2\",\n                    \"deployment.kubernetes.io/revision\": \"1\"\n                }\n            },\n            \"spec\": {\n                \"replicas\": 1,\n                \"selector\": {\n                    \"matchLabels\": {\n                        \"k8s-app\": \"heapster\",\n                        \"pod-template-hash\": \"564189836\",\n                        \"task\": \"monitoring\"\n                    }\n                },\n                \"template\": {\n                    \"metadata\": {\n                        \"creationTimestamp\": null,\n                        \"labels\": {\n                            \"k8s-app\": \"heapster\",\n                            \"pod-template-hash\": \"564189836\",\n                            \"task\": \"monitoring\"\n                        }\n                    },\n                    \"spec\": {\n                        \"volumes\": null,\n                        \"containers\": [\n                            {\n                                \"name\": \"heapster\",\n                                \"image\": \"gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1\",\n                                \"command\": [\n                                    \"/heapster\",\n                                    \"--source=kubernetes:https://kubernetes.default\",\n                                    \"--sink=influxdb:http://monitoring-influxdb:8086\"\n                                ],\n                                \"resources\": {},\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            }\n                        ],\n                        \"restartPolicy\": \"Always\",\n                        \"terminationGracePeriodSeconds\": 30,\n                        \"dnsPolicy\": \"ClusterFirst\",\n                        \"serviceAccountName\": \"\",\n                        \"securityContext\": {}\n                    }\n                }\n            },\n            \"status\": {\n                \"replicas\": 1,\n                \"fullyLabeledReplicas\": 1,\n                \"readyReplicas\": 1,\n                \"observedGeneration\": 1\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"kube-dns-782804071\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/apis/extensions/v1beta1/namespaces/kube-system/replicasets/kube-dns-782804071\",\n                \"uid\": \"10af6dcf-e3e5-11e6-8dc2-12021b6d724c\",\n                \"resourceVersion\": \"2906322\",\n                \"generation\": 1,\n                \"creationTimestamp\": \"2017-01-26T16:32:44Z\",\n                \"labels\": {\n                    \"k8s-app\": \"kube-dns\",\n                    \"pod-template-hash\": \"782804071\"\n                },\n                \"annotations\": {\n                    \"deployment.kubernetes.io/desired-replicas\": \"1\",\n                    \"deployment.kubernetes.io/max-replicas\": \"2\",\n                    \"deployment.kubernetes.io/revision\": \"1\"\n                }\n            },\n            \"spec\": {\n                \"replicas\": 1,\n                \"selector\": {\n                    \"matchLabels\": {\n                        \"k8s-app\": \"kube-dns\",\n                        \"pod-template-hash\": \"782804071\"\n                    }\n                },\n                \"template\": {\n                    \"metadata\": {\n                        \"creationTimestamp\": null,\n                        \"labels\": {\n                            \"k8s-app\": \"kube-dns\",\n                            \"pod-template-hash\": \"782804071\"\n                        },\n                        \"annotations\": {\n                            \"scheduler.alpha.kubernetes.io/critical-pod\": \"\",\n                            \"scheduler.alpha.kubernetes.io/tolerations\": \"[{\\\"key\\\":\\\"CriticalAddonsOnly\\\", \\\"operator\\\":\\\"Exists\\\"}]\"\n                        }\n                    },\n                    \"spec\": {\n                        \"volumes\": null,\n                        \"containers\": [\n                            {\n                                \"name\": \"kubedns\",\n                                \"image\": \"gcr.io/google_containers/kubedns-amd64:1.9\",\n                                \"args\": [\n                                    \"--domain=cluster.local.\",\n                                    \"--dns-port=10053\",\n                                    \"--config-map=kube-dns\",\n                                    \"--v=2\"\n                                ],\n                                \"ports\": [\n                                    {\n                                        \"name\": \"dns-local\",\n                                        \"containerPort\": 10053,\n                                        \"protocol\": \"UDP\"\n                                    },\n                                    {\n                                        \"name\": \"dns-tcp-local\",\n                                        \"containerPort\": 10053,\n                                        \"protocol\": \"TCP\"\n                                    },\n                                    {\n                                        \"name\": \"metrics\",\n                                        \"containerPort\": 10055,\n                                        \"protocol\": \"TCP\"\n                                    }\n                                ],\n                                \"env\": [\n                                    {\n                                        \"name\": \"PROMETHEUS_PORT\",\n                                        \"value\": \"10055\"\n                                    }\n                                ],\n                                \"resources\": {\n                                    \"limits\": {\n                                        \"memory\": \"170Mi\"\n                                    },\n                                    \"requests\": {\n                                        \"cpu\": \"100m\",\n                                        \"memory\": \"70Mi\"\n                                    }\n                                },\n                                \"livenessProbe\": {\n                                    \"httpGet\": {\n                                        \"path\": \"/healthz-kubedns\",\n                                        \"port\": 8080,\n                                        \"scheme\": \"HTTP\"\n                                    },\n                                    \"initialDelaySeconds\": 60,\n                                    \"timeoutSeconds\": 5,\n                                    \"periodSeconds\": 10,\n                                    \"successThreshold\": 1,\n                                    \"failureThreshold\": 5\n                                },\n                                \"readinessProbe\": {\n                                    \"httpGet\": {\n                                        \"path\": \"/readiness\",\n                                        \"port\": 8081,\n                                        \"scheme\": \"HTTP\"\n                                    },\n                                    \"initialDelaySeconds\": 3,\n                                    \"timeoutSeconds\": 5,\n                                    \"periodSeconds\": 10,\n                                    \"successThreshold\": 1,\n                                    \"failureThreshold\": 3\n                                },\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            },\n                            {\n                                \"name\": \"dnsmasq\",\n                                \"image\": \"gcr.io/google_containers/kube-dnsmasq-amd64:1.4\",\n                                \"args\": [\n                                    \"--cache-size=1000\",\n                                    \"--no-resolv\",\n                                    \"--server=127.0.0.1#10053\",\n                                    \"--log-facility=-\"\n                                ],\n                                \"ports\": [\n                                    {\n                                        \"name\": \"dns\",\n                                        \"containerPort\": 53,\n                                        \"protocol\": \"UDP\"\n                                    },\n                                    {\n                                        \"name\": \"dns-tcp\",\n                                        \"containerPort\": 53,\n                                        \"protocol\": \"TCP\"\n                                    }\n                                ],\n                                \"resources\": {\n                                    \"requests\": {\n                                        \"cpu\": \"150m\",\n                                        \"memory\": \"10Mi\"\n                                    }\n                                },\n                                \"livenessProbe\": {\n                                    \"httpGet\": {\n                                        \"path\": \"/healthz-dnsmasq\",\n                                        \"port\": 8080,\n                                        \"scheme\": \"HTTP\"\n                                    },\n                                    \"initialDelaySeconds\": 60,\n                                    \"timeoutSeconds\": 5,\n                                    \"periodSeconds\": 10,\n                                    \"successThreshold\": 1,\n                                    \"failureThreshold\": 5\n                                },\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            },\n                            {\n                                \"name\": \"dnsmasq-metrics\",\n                                \"image\": \"gcr.io/google_containers/dnsmasq-metrics-amd64:1.0\",\n                                \"args\": [\n                                    \"--v=2\",\n                                    \"--logtostderr\"\n                                ],\n                                \"ports\": [\n                                    {\n                                        \"name\": \"metrics\",\n                                        \"containerPort\": 10054,\n                                        \"protocol\": \"TCP\"\n                                    }\n                                ],\n                                \"resources\": {\n                                    \"requests\": {\n                                        \"memory\": \"10Mi\"\n                                    }\n                                },\n                                \"livenessProbe\": {\n                                    \"httpGet\": {\n                                        \"path\": \"/metrics\",\n                                        \"port\": 10054,\n                                        \"scheme\": \"HTTP\"\n                                    },\n                                    \"initialDelaySeconds\": 60,\n                                    \"timeoutSeconds\": 5,\n                                    \"periodSeconds\": 10,\n                                    \"successThreshold\": 1,\n                                    \"failureThreshold\": 5\n                                },\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            },\n                            {\n                                \"name\": \"healthz\",\n                                \"image\": \"gcr.io/google_containers/exechealthz-amd64:1.2\",\n                                \"args\": [\n                                    \"--cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 \\u003e/dev/null\",\n                                    \"--url=/healthz-dnsmasq\",\n                                    \"--cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 \\u003e/dev/null\",\n                                    \"--url=/healthz-kubedns\",\n                                    \"--port=8080\",\n                                    \"--quiet\"\n                                ],\n                                \"ports\": [\n                                    {\n                                        \"containerPort\": 8080,\n                                        \"protocol\": \"TCP\"\n                                    }\n                                ],\n                                \"resources\": {\n                                    \"limits\": {\n                                        \"memory\": \"50Mi\"\n                                    },\n                                    \"requests\": {\n                                        \"cpu\": \"10m\",\n                                        \"memory\": \"50Mi\"\n                                    }\n                                },\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            }\n                        ],\n                        \"restartPolicy\": \"Always\",\n                        \"terminationGracePeriodSeconds\": 30,\n                        \"dnsPolicy\": \"Default\",\n                        \"serviceAccountName\": \"\",\n                        \"securityContext\": {}\n                    }\n                }\n            },\n            \"status\": {\n                \"replicas\": 1,\n                \"fullyLabeledReplicas\": 1,\n                \"readyReplicas\": 1,\n                \"observedGeneration\": 1\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"kube-dns-autoscaler-2715466192\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/apis/extensions/v1beta1/namespaces/kube-system/replicasets/kube-dns-autoscaler-2715466192\",\n                \"uid\": \"10a93ff8-e3e5-11e6-8dc2-12021b6d724c\",\n                \"resourceVersion\": \"2906264\",\n                \"generation\": 1,\n                \"creationTimestamp\": \"2017-01-26T16:32:44Z\",\n                \"labels\": {\n                    \"k8s-app\": \"kube-dns-autoscaler\",\n                    \"pod-template-hash\": \"2715466192\"\n                },\n                \"annotations\": {\n                    \"deployment.kubernetes.io/desired-replicas\": \"1\",\n                    \"deployment.kubernetes.io/max-replicas\": \"2\",\n                    \"deployment.kubernetes.io/revision\": \"1\"\n                }\n            },\n            \"spec\": {\n                \"replicas\": 1,\n                \"selector\": {\n                    \"matchLabels\": {\n                        \"k8s-app\": \"kube-dns-autoscaler\",\n                        \"pod-template-hash\": \"2715466192\"\n                    }\n                },\n                \"template\": {\n                    \"metadata\": {\n                        \"creationTimestamp\": null,\n                        \"labels\": {\n                            \"k8s-app\": \"kube-dns-autoscaler\",\n                            \"pod-template-hash\": \"2715466192\"\n                        },\n                        \"annotations\": {\n                            \"scheduler.alpha.kubernetes.io/critical-pod\": \"\",\n                            \"scheduler.alpha.kubernetes.io/tolerations\": \"[{\\\"key\\\":\\\"CriticalAddonsOnly\\\", \\\"operator\\\":\\\"Exists\\\"}]\"\n                        }\n                    },\n                    \"spec\": {\n                        \"volumes\": null,\n                        \"containers\": [\n                            {\n                                \"name\": \"autoscaler\",\n                                \"image\": \"gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.0.0\",\n                                \"command\": [\n                                    \"/cluster-proportional-autoscaler\",\n                                    \"--namespace=kube-system\",\n                                    \"--configmap=kube-dns-autoscaler\",\n                                    \"--mode=linear\",\n                                    \"--target=Deployment/kube-dns\",\n                                    \"--default-params={\\\"linear\\\":{\\\"coresPerReplica\\\":256,\\\"nodesPerReplica\\\":16,\\\"min\\\":1}}\",\n                                    \"--logtostderr=true\",\n                                    \"--v=2\"\n                                ],\n                                \"resources\": {\n                                    \"requests\": {\n                                        \"cpu\": \"20m\",\n                                        \"memory\": \"10Mi\"\n                                    }\n                                },\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            }\n                        ],\n                        \"restartPolicy\": \"Always\",\n                        \"terminationGracePeriodSeconds\": 30,\n                        \"dnsPolicy\": \"ClusterFirst\",\n                        \"serviceAccountName\": \"\",\n                        \"securityContext\": {}\n                    }\n                }\n            },\n            \"status\": {\n                \"replicas\": 1,\n                \"fullyLabeledReplicas\": 1,\n                \"readyReplicas\": 1,\n                \"observedGeneration\": 1\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-grafana-3344903701\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/apis/extensions/v1beta1/namespaces/kube-system/replicasets/monitoring-grafana-3344903701\",\n                \"uid\": \"3ae4a19b-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"5110326\",\n                \"generation\": 1,\n                \"creationTimestamp\": \"2017-03-03T00:27:48Z\",\n                \"labels\": {\n                    \"k8s-app\": \"grafana\",\n                    \"pod-template-hash\": \"3344903701\",\n                    \"task\": \"monitoring\"\n                },\n                \"annotations\": {\n                    \"deployment.kubernetes.io/desired-replicas\": \"1\",\n                    \"deployment.kubernetes.io/max-replicas\": \"2\",\n                    \"deployment.kubernetes.io/revision\": \"1\"\n                }\n            },\n            \"spec\": {\n                \"replicas\": 1,\n                \"selector\": {\n                    \"matchLabels\": {\n                        \"k8s-app\": \"grafana\",\n                        \"pod-template-hash\": \"3344903701\",\n                        \"task\": \"monitoring\"\n                    }\n                },\n                \"template\": {\n                    \"metadata\": {\n                        \"creationTimestamp\": null,\n                        \"labels\": {\n                            \"k8s-app\": \"grafana\",\n                            \"pod-template-hash\": \"3344903701\",\n                            \"task\": \"monitoring\"\n                        }\n                    },\n                    \"spec\": {\n                        \"volumes\": [\n                            {\n                                \"name\": \"grafana-storage\",\n                                \"emptyDir\": {}\n                            }\n                        ],\n                        \"containers\": [\n                            {\n                                \"name\": \"grafana\",\n                                \"image\": \"gcr.io/google_containers/heapster-grafana-amd64:v4.0.2\",\n                                \"ports\": [\n                                    {\n                                        \"containerPort\": 3000,\n                                        \"protocol\": \"TCP\"\n                                    }\n                                ],\n                                \"env\": [\n                                    {\n                                        \"name\": \"INFLUXDB_HOST\",\n                                        \"value\": \"monitoring-influxdb\"\n                                    },\n                                    {\n                                        \"name\": \"GRAFANA_PORT\",\n                                        \"value\": \"3000\"\n                                    },\n                                    {\n                                        \"name\": \"GF_AUTH_BASIC_ENABLED\",\n                                        \"value\": \"false\"\n                                    },\n                                    {\n                                        \"name\": \"GF_AUTH_ANONYMOUS_ENABLED\",\n                                        \"value\": \"true\"\n                                    },\n                                    {\n                                        \"name\": \"GF_AUTH_ANONYMOUS_ORG_ROLE\",\n                                        \"value\": \"Admin\"\n                                    },\n                                    {\n                                        \"name\": \"GF_SERVER_ROOT_URL\",\n                                        \"value\": \"/\"\n                                    }\n                                ],\n                                \"resources\": {},\n                                \"volumeMounts\": [\n                                    {\n                                        \"name\": \"grafana-storage\",\n                                        \"mountPath\": \"/var\"\n                                    }\n                                ],\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            }\n                        ],\n                        \"restartPolicy\": \"Always\",\n                        \"terminationGracePeriodSeconds\": 30,\n                        \"dnsPolicy\": \"ClusterFirst\",\n                        \"serviceAccountName\": \"\",\n                        \"securityContext\": {}\n                    }\n                }\n            },\n            \"status\": {\n                \"replicas\": 1,\n                \"fullyLabeledReplicas\": 1,\n                \"readyReplicas\": 1,\n                \"observedGeneration\": 1\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-influxdb-421024531\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/apis/extensions/v1beta1/namespaces/kube-system/replicasets/monitoring-influxdb-421024531\",\n                \"uid\": \"3b580ba4-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"5110334\",\n                \"generation\": 1,\n                \"creationTimestamp\": \"2017-03-03T00:27:49Z\",\n                \"labels\": {\n                    \"k8s-app\": \"influxdb\",\n                    \"pod-template-hash\": \"421024531\",\n                    \"task\": \"monitoring\"\n                },\n                \"annotations\": {\n                    \"deployment.kubernetes.io/desired-replicas\": \"1\",\n                    \"deployment.kubernetes.io/max-replicas\": \"2\",\n                    \"deployment.kubernetes.io/revision\": \"1\"\n                }\n            },\n            \"spec\": {\n                \"replicas\": 1,\n                \"selector\": {\n                    \"matchLabels\": {\n                        \"k8s-app\": \"influxdb\",\n                        \"pod-template-hash\": \"421024531\",\n                        \"task\": \"monitoring\"\n                    }\n                },\n                \"template\": {\n                    \"metadata\": {\n                        \"creationTimestamp\": null,\n                        \"labels\": {\n                            \"k8s-app\": \"influxdb\",\n                            \"pod-template-hash\": \"421024531\",\n                            \"task\": \"monitoring\"\n                        }\n                    },\n                    \"spec\": {\n                        \"volumes\": [\n                            {\n                                \"name\": \"influxdb-storage\",\n                                \"emptyDir\": {}\n                            }\n                        ],\n                        \"containers\": [\n                            {\n                                \"name\": \"influxdb\",\n                                \"image\": \"gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\",\n                                \"resources\": {},\n                                \"volumeMounts\": [\n                                    {\n                                        \"name\": \"influxdb-storage\",\n                                        \"mountPath\": \"/data\"\n                                    }\n                                ],\n                                \"terminationMessagePath\": \"/dev/termination-log\",\n                                \"imagePullPolicy\": \"IfNotPresent\"\n                            }\n                        ],\n                        \"restartPolicy\": \"Always\",\n                        \"terminationGracePeriodSeconds\": 30,\n                        \"dnsPolicy\": \"ClusterFirst\",\n                        \"serviceAccountName\": \"\",\n                        \"securityContext\": {}\n                    }\n                }\n            },\n            \"status\": {\n                \"replicas\": 1,\n                \"fullyLabeledReplicas\": 1,\n                \"readyReplicas\": 1,\n                \"observedGeneration\": 1\n            }\n        }\n    ]\n}\n{\n    \"metadata\": {\n        \"selfLink\": \"/api/v1/namespaces/kube-system/pods\",\n        \"resourceVersion\": \"5111708\"\n    },\n    \"items\": [\n        {\n            \"metadata\": {\n                \"name\": \"dns-controller-4128279831-5u7dk\",\n                \"generateName\": \"dns-controller-4128279831-\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/pods/dns-controller-4128279831-5u7dk\",\n                \"uid\": \"731cf0ef-f31d-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"2906080\",\n                \"creationTimestamp\": \"2017-02-15T01:24:09Z\",\n                \"labels\": {\n                    \"k8s-addon\": \"dns-controller.addons.k8s.io\",\n                    \"k8s-app\": \"dns-controller\",\n                    \"pod-template-hash\": \"4128279831\",\n                    \"version\": \"v1.5.1\"\n                },\n                \"annotations\": {\n                    \"kubernetes.io/created-by\": \"{\\\"kind\\\":\\\"SerializedReference\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"reference\\\":{\\\"kind\\\":\\\"ReplicaSet\\\",\\\"namespace\\\":\\\"kube-system\\\",\\\"name\\\":\\\"dns-controller-4128279831\\\",\\\"uid\\\":\\\"0ffa1251-e3e5-11e6-8dc2-12021b6d724c\\\",\\\"apiVersion\\\":\\\"extensions\\\",\\\"resourceVersion\\\":\\\"2906032\\\"}}\\n\",\n                    \"scheduler.alpha.kubernetes.io/critical-pod\": \"\",\n                    \"scheduler.alpha.kubernetes.io/tolerations\": \"[{\\\"key\\\": \\\"dedicated\\\", \\\"value\\\": \\\"master\\\"}]\"\n                },\n                \"ownerReferences\": [\n                    {\n                        \"apiVersion\": \"extensions/v1beta1\",\n                        \"kind\": \"ReplicaSet\",\n                        \"name\": \"dns-controller-4128279831\",\n                        \"uid\": \"0ffa1251-e3e5-11e6-8dc2-12021b6d724c\",\n                        \"controller\": true\n                    }\n                ]\n            },\n            \"spec\": {\n                \"volumes\": [\n                    {\n                        \"name\": \"default-token-37qxp\",\n                        \"secret\": {\n                            \"secretName\": \"default-token-37qxp\",\n                            \"defaultMode\": 420\n                        }\n                    }\n                ],\n                \"containers\": [\n                    {\n                        \"name\": \"dns-controller\",\n                        \"image\": \"kope/dns-controller:1.5.1\",\n                        \"command\": [\n                            \"/usr/bin/dns-controller\",\n                            \"--watch-ingress=false\",\n                            \"--dns=aws-route53\",\n                            \"--zone=kubernetes.com\",\n                            \"--zone=/\",\n                            \"-v=2\"\n                        ],\n                        \"resources\": {\n                            \"requests\": {\n                                \"cpu\": \"50m\",\n                                \"memory\": \"50Mi\"\n                            }\n                        },\n                        \"volumeMounts\": [\n                            {\n                                \"name\": \"default-token-37qxp\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\"\n                            }\n                        ],\n                        \"terminationMessagePath\": \"/dev/termination-log\",\n                        \"imagePullPolicy\": \"IfNotPresent\"\n                    }\n                ],\n                \"restartPolicy\": \"Always\",\n                \"terminationGracePeriodSeconds\": 30,\n                \"dnsPolicy\": \"Default\",\n                \"nodeSelector\": {\n                    \"kubernetes.io/role\": \"master\"\n                },\n                \"serviceAccountName\": \"default\",\n                \"nodeName\": \"ip-172-20-57-18.ec2.internal\",\n                \"securityContext\": {\n                    \"hostNetwork\": true\n                }\n            },\n            \"status\": {\n                \"phase\": \"Running\",\n                \"conditions\": [\n                    {\n                        \"type\": \"Initialized\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:24:09Z\"\n                    },\n                    {\n                        \"type\": \"Ready\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:24:20Z\"\n                    },\n                    {\n                        \"type\": \"PodScheduled\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:24:09Z\"\n                    }\n                ],\n                \"hostIP\": \"172.20.57.18\",\n                \"podIP\": \"172.20.57.18\",\n                \"startTime\": \"2017-02-15T01:24:09Z\",\n                \"containerStatuses\": [\n                    {\n                        \"name\": \"dns-controller\",\n                        \"state\": {\n                            \"running\": {\n                                \"startedAt\": \"2017-02-15T01:24:20Z\"\n                            }\n                        },\n                        \"lastState\": {},\n                        \"ready\": true,\n                        \"restartCount\": 0,\n                        \"image\": \"kope/dns-controller:1.5.1\",\n                        \"imageID\": \"docker://sha256:0160bb323c1cc7d5dc4a04c31e985c25a3c4335379c4d583489e03e31d435a82\",\n                        \"containerID\": \"docker://09b83ef6a19c502d8f87f9d46cfd4e77ca30de0bed5f79040fc1a9c150867957\"\n                    }\n                ]\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"etcd-server-events-ip-172-20-57-18.ec2.internal\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/pods/etcd-server-events-ip-172-20-57-18.ec2.internal\",\n                \"uid\": \"7329f5cc-f31d-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"2906068\",\n                \"creationTimestamp\": \"2017-02-15T01:24:09Z\",\n                \"labels\": {\n                    \"k8s-app\": \"etcd-server-events\"\n                },\n                \"annotations\": {\n                    \"kubernetes.io/config.hash\": \"36d0ef1494a441f3d131efed09786634\",\n                    \"kubernetes.io/config.mirror\": \"36d0ef1494a441f3d131efed09786634\",\n                    \"kubernetes.io/config.seen\": \"2017-02-15T01:22:31.286632674Z\",\n                    \"kubernetes.io/config.source\": \"file\"\n                }\n            },\n            \"spec\": {\n                \"volumes\": [\n                    {\n                        \"name\": \"varetcddata\",\n                        \"hostPath\": {\n                            \"path\": \"/mnt/master-vol-0375c572ca6a7fde1/var/etcd/data-events\"\n                        }\n                    },\n                    {\n                        \"name\": \"varlogetcd\",\n                        \"hostPath\": {\n                            \"path\": \"/var/log/etcd-events.log\"\n                        }\n                    }\n                ],\n                \"containers\": [\n                    {\n                        \"name\": \"etcd-container\",\n                        \"image\": \"gcr.io/google_containers/etcd:2.2.1\",\n                        \"command\": [\n                            \"/bin/sh\",\n                            \"-c\",\n                            \"/usr/local/bin/etcd 1\\u003e\\u003e/var/log/etcd.log 2\\u003e\\u00261\"\n                        ],\n                        \"ports\": [\n                            {\n                                \"name\": \"serverport\",\n                                \"hostPort\": 2381,\n                                \"containerPort\": 2381,\n                                \"protocol\": \"TCP\"\n                            },\n                            {\n                                \"name\": \"clientport\",\n                                \"hostPort\": 4002,\n                                \"containerPort\": 4002,\n                                \"protocol\": \"TCP\"\n                            }\n                        ],\n                        \"env\": [\n                            {\n                                \"name\": \"ETCD_NAME\",\n                                \"value\": \"etcd-events-us-east-1a\"\n                            },\n                            {\n                                \"name\": \"ETCD_DATA_DIR\",\n                                \"value\": \"/var/etcd/data-events\"\n                            },\n                            {\n                                \"name\": \"ETCD_LISTEN_PEER_URLS\",\n                                \"value\": \"http://0.0.0.0:2381\"\n                            },\n                            {\n                                \"name\": \"ETCD_LISTEN_CLIENT_URLS\",\n                                \"value\": \"http://0.0.0.0:4002\"\n                            },\n                            {\n                                \"name\": \"ETCD_ADVERTISE_CLIENT_URLS\",\n                                \"value\": \"http://etcd-events-us-east-1a.internal.bpachydermcluster.kubernetes.com:4002\"\n                            },\n                            {\n                                \"name\": \"ETCD_INITIAL_ADVERTISE_PEER_URLS\",\n                                \"value\": \"http://etcd-events-us-east-1a.internal.bpachydermcluster.kubernetes.com:2381\"\n                            },\n                            {\n                                \"name\": \"ETCD_INITIAL_CLUSTER_STATE\",\n                                \"value\": \"new\"\n                            },\n                            {\n                                \"name\": \"ETCD_INITIAL_CLUSTER_TOKEN\",\n                                \"value\": \"etcd-cluster-token-etcd-events\"\n                            },\n                            {\n                                \"name\": \"ETCD_INITIAL_CLUSTER\",\n                                \"value\": \"etcd-events-us-east-1a=http://etcd-events-us-east-1a.internal.bpachydermcluster.kubernetes.com:2381\"\n                            }\n                        ],\n                        \"resources\": {\n                            \"requests\": {\n                                \"cpu\": \"100m\"\n                            }\n                        },\n                        \"volumeMounts\": [\n                            {\n                                \"name\": \"varetcddata\",\n                                \"mountPath\": \"/var/etcd/data-events\"\n                            },\n                            {\n                                \"name\": \"varlogetcd\",\n                                \"mountPath\": \"/var/log/etcd.log\"\n                            }\n                        ],\n                        \"livenessProbe\": {\n                            \"httpGet\": {\n                                \"path\": \"/health\",\n                                \"port\": 4002,\n                                \"host\": \"127.0.0.1\",\n                                \"scheme\": \"HTTP\"\n                            },\n                            \"initialDelaySeconds\": 600,\n                            \"timeoutSeconds\": 15,\n                            \"periodSeconds\": 10,\n                            \"successThreshold\": 1,\n                            \"failureThreshold\": 3\n                        },\n                        \"terminationMessagePath\": \"/dev/termination-log\",\n                        \"imagePullPolicy\": \"IfNotPresent\"\n                    }\n                ],\n                \"restartPolicy\": \"Always\",\n                \"terminationGracePeriodSeconds\": 30,\n                \"dnsPolicy\": \"ClusterFirst\",\n                \"serviceAccountName\": \"\",\n                \"nodeName\": \"ip-172-20-57-18.ec2.internal\",\n                \"securityContext\": {\n                    \"hostNetwork\": true\n                }\n            },\n            \"status\": {\n                \"phase\": \"Running\",\n                \"conditions\": [\n                    {\n                        \"type\": \"Initialized\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:22:36Z\"\n                    },\n                    {\n                        \"type\": \"Ready\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:22:47Z\"\n                    },\n                    {\n                        \"type\": \"PodScheduled\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:22:36Z\"\n                    }\n                ],\n                \"hostIP\": \"172.20.57.18\",\n                \"podIP\": \"172.20.57.18\",\n                \"startTime\": \"2017-02-15T01:22:36Z\",\n                \"containerStatuses\": [\n                    {\n                        \"name\": \"etcd-container\",\n                        \"state\": {\n                            \"running\": {\n                                \"startedAt\": \"2017-02-15T01:22:46Z\"\n                            }\n                        },\n                        \"lastState\": {},\n                        \"ready\": true,\n                        \"restartCount\": 0,\n                        \"image\": \"gcr.io/google_containers/etcd:2.2.1\",\n                        \"imageID\": \"docker://sha256:ef5842ca5c4276cdf3e7818b624526f908104b1aa7cbc0d70649beaf4a8d0e14\",\n                        \"containerID\": \"docker://dacbfc146f36e94b57dbec1679a9e61b298009d8e1251761566196ec6c767d7a\"\n                    }\n                ]\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"etcd-server-ip-172-20-57-18.ec2.internal\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/pods/etcd-server-ip-172-20-57-18.ec2.internal\",\n                \"uid\": \"6a398904-f31d-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"2905938\",\n                \"creationTimestamp\": \"2017-02-15T01:23:54Z\",\n                \"labels\": {\n                    \"k8s-app\": \"etcd-server\"\n                },\n                \"annotations\": {\n                    \"kubernetes.io/config.hash\": \"98fad7014025e6ec64ad34c5a4f5aa74\",\n                    \"kubernetes.io/config.mirror\": \"98fad7014025e6ec64ad34c5a4f5aa74\",\n                    \"kubernetes.io/config.seen\": \"2017-02-15T01:22:31.286645804Z\",\n                    \"kubernetes.io/config.source\": \"file\"\n                }\n            },\n            \"spec\": {\n                \"volumes\": [\n                    {\n                        \"name\": \"varetcddata\",\n                        \"hostPath\": {\n                            \"path\": \"/mnt/master-vol-04ccc2185cdc104c9/var/etcd/data\"\n                        }\n                    },\n                    {\n                        \"name\": \"varlogetcd\",\n                        \"hostPath\": {\n                            \"path\": \"/var/log/etcd.log\"\n                        }\n                    }\n                ],\n                \"containers\": [\n                    {\n                        \"name\": \"etcd-container\",\n                        \"image\": \"gcr.io/google_containers/etcd:2.2.1\",\n                        \"command\": [\n                            \"/bin/sh\",\n                            \"-c\",\n                            \"/usr/local/bin/etcd 1\\u003e\\u003e/var/log/etcd.log 2\\u003e\\u00261\"\n                        ],\n                        \"ports\": [\n                            {\n                                \"name\": \"serverport\",\n                                \"hostPort\": 2380,\n                                \"containerPort\": 2380,\n                                \"protocol\": \"TCP\"\n                            },\n                            {\n                                \"name\": \"clientport\",\n                                \"hostPort\": 4001,\n                                \"containerPort\": 4001,\n                                \"protocol\": \"TCP\"\n                            }\n                        ],\n                        \"env\": [\n                            {\n                                \"name\": \"ETCD_NAME\",\n                                \"value\": \"etcd-us-east-1a\"\n                            },\n                            {\n                                \"name\": \"ETCD_DATA_DIR\",\n                                \"value\": \"/var/etcd/data\"\n                            },\n                            {\n                                \"name\": \"ETCD_LISTEN_PEER_URLS\",\n                                \"value\": \"http://0.0.0.0:2380\"\n                            },\n                            {\n                                \"name\": \"ETCD_LISTEN_CLIENT_URLS\",\n                                \"value\": \"http://0.0.0.0:4001\"\n                            },\n                            {\n                                \"name\": \"ETCD_ADVERTISE_CLIENT_URLS\",\n                                \"value\": \"http://etcd-us-east-1a.internal.bpachydermcluster.kubernetes.com:4001\"\n                            },\n                            {\n                                \"name\": \"ETCD_INITIAL_ADVERTISE_PEER_URLS\",\n                                \"value\": \"http://etcd-us-east-1a.internal.bpachydermcluster.kubernetes.com:2380\"\n                            },\n                            {\n                                \"name\": \"ETCD_INITIAL_CLUSTER_STATE\",\n                                \"value\": \"new\"\n                            },\n                            {\n                                \"name\": \"ETCD_INITIAL_CLUSTER_TOKEN\",\n                                \"value\": \"etcd-cluster-token-etcd\"\n                            },\n                            {\n                                \"name\": \"ETCD_INITIAL_CLUSTER\",\n                                \"value\": \"etcd-us-east-1a=http://etcd-us-east-1a.internal.bpachydermcluster.kubernetes.com:2380\"\n                            }\n                        ],\n                        \"resources\": {\n                            \"requests\": {\n                                \"cpu\": \"150m\"\n                            }\n                        },\n                        \"volumeMounts\": [\n                            {\n                                \"name\": \"varetcddata\",\n                                \"mountPath\": \"/var/etcd/data\"\n                            },\n                            {\n                                \"name\": \"varlogetcd\",\n                                \"mountPath\": \"/var/log/etcd.log\"\n                            }\n                        ],\n                        \"livenessProbe\": {\n                            \"httpGet\": {\n                                \"path\": \"/health\",\n                                \"port\": 4001,\n                                \"host\": \"127.0.0.1\",\n                                \"scheme\": \"HTTP\"\n                            },\n                            \"initialDelaySeconds\": 600,\n                            \"timeoutSeconds\": 15,\n                            \"periodSeconds\": 10,\n                            \"successThreshold\": 1,\n                            \"failureThreshold\": 3\n                        },\n                        \"terminationMessagePath\": \"/dev/termination-log\",\n                        \"imagePullPolicy\": \"IfNotPresent\"\n                    }\n                ],\n                \"restartPolicy\": \"Always\",\n                \"terminationGracePeriodSeconds\": 30,\n                \"dnsPolicy\": \"ClusterFirst\",\n                \"serviceAccountName\": \"\",\n                \"nodeName\": \"ip-172-20-57-18.ec2.internal\",\n                \"securityContext\": {\n                    \"hostNetwork\": true\n                }\n            },\n            \"status\": {\n                \"phase\": \"Running\",\n                \"conditions\": [\n                    {\n                        \"type\": \"Initialized\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:22:36Z\"\n                    },\n                    {\n                        \"type\": \"Ready\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:22:48Z\"\n                    },\n                    {\n                        \"type\": \"PodScheduled\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:22:36Z\"\n                    }\n                ],\n                \"hostIP\": \"172.20.57.18\",\n                \"podIP\": \"172.20.57.18\",\n                \"startTime\": \"2017-02-15T01:22:36Z\",\n                \"containerStatuses\": [\n                    {\n                        \"name\": \"etcd-container\",\n                        \"state\": {\n                            \"running\": {\n                                \"startedAt\": \"2017-02-15T01:22:47Z\"\n                            }\n                        },\n                        \"lastState\": {},\n                        \"ready\": true,\n                        \"restartCount\": 0,\n                        \"image\": \"gcr.io/google_containers/etcd:2.2.1\",\n                        \"imageID\": \"docker://sha256:ef5842ca5c4276cdf3e7818b624526f908104b1aa7cbc0d70649beaf4a8d0e14\",\n                        \"containerID\": \"docker://9acf0a19e5a3e72ceff0647c0f6c2d193c71d44d2df8b2311c1123de2dda6731\"\n                    }\n                ]\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"heapster-564189836-dfalg\",\n                \"generateName\": \"heapster-564189836-\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/pods/heapster-564189836-dfalg\",\n                \"uid\": \"3b0f7ec8-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"5110327\",\n                \"creationTimestamp\": \"2017-03-03T00:27:49Z\",\n                \"labels\": {\n                    \"k8s-app\": \"heapster\",\n                    \"pod-template-hash\": \"564189836\",\n                    \"task\": \"monitoring\"\n                },\n                \"annotations\": {\n                    \"kubernetes.io/created-by\": \"{\\\"kind\\\":\\\"SerializedReference\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"reference\\\":{\\\"kind\\\":\\\"ReplicaSet\\\",\\\"namespace\\\":\\\"kube-system\\\",\\\"name\\\":\\\"heapster-564189836\\\",\\\"uid\\\":\\\"3b0efc17-ffa8-11e6-963a-12cd63c7b19c\\\",\\\"apiVersion\\\":\\\"extensions\\\",\\\"resourceVersion\\\":\\\"5110297\\\"}}\\n\"\n                },\n                \"ownerReferences\": [\n                    {\n                        \"apiVersion\": \"extensions/v1beta1\",\n                        \"kind\": \"ReplicaSet\",\n                        \"name\": \"heapster-564189836\",\n                        \"uid\": \"3b0efc17-ffa8-11e6-963a-12cd63c7b19c\",\n                        \"controller\": true\n                    }\n                ]\n            },\n            \"spec\": {\n                \"volumes\": [\n                    {\n                        \"name\": \"default-token-37qxp\",\n                        \"secret\": {\n                            \"secretName\": \"default-token-37qxp\",\n                            \"defaultMode\": 420\n                        }\n                    }\n                ],\n                \"containers\": [\n                    {\n                        \"name\": \"heapster\",\n                        \"image\": \"gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1\",\n                        \"command\": [\n                            \"/heapster\",\n                            \"--source=kubernetes:https://kubernetes.default\",\n                            \"--sink=influxdb:http://monitoring-influxdb:8086\"\n                        ],\n                        \"resources\": {},\n                        \"volumeMounts\": [\n                            {\n                                \"name\": \"default-token-37qxp\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\"\n                            }\n                        ],\n                        \"terminationMessagePath\": \"/dev/termination-log\",\n                        \"imagePullPolicy\": \"IfNotPresent\"\n                    }\n                ],\n                \"restartPolicy\": \"Always\",\n                \"terminationGracePeriodSeconds\": 30,\n                \"dnsPolicy\": \"ClusterFirst\",\n                \"serviceAccountName\": \"default\",\n                \"nodeName\": \"ip-172-20-58-236.ec2.internal\",\n                \"securityContext\": {}\n            },\n            \"status\": {\n                \"phase\": \"Running\",\n                \"conditions\": [\n                    {\n                        \"type\": \"Initialized\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-03-03T00:27:49Z\"\n                    },\n                    {\n                        \"type\": \"Ready\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-03-03T00:27:51Z\"\n                    },\n                    {\n                        \"type\": \"PodScheduled\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-03-03T00:27:49Z\"\n                    }\n                ],\n                \"hostIP\": \"172.20.58.236\",\n                \"podIP\": \"100.96.1.63\",\n                \"startTime\": \"2017-03-03T00:27:49Z\",\n                \"containerStatuses\": [\n                    {\n                        \"name\": \"heapster\",\n                        \"state\": {\n                            \"running\": {\n                                \"startedAt\": \"2017-03-03T00:27:51Z\"\n                            }\n                        },\n                        \"lastState\": {},\n                        \"ready\": true,\n                        \"restartCount\": 0,\n                        \"image\": \"gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1\",\n                        \"imageID\": \"docker://sha256:4ff6ad0ca64c7a065cd595a557b692a92b8ef4fe56bffd7c104eabed4c5a6d98\",\n                        \"containerID\": \"docker://b216c964ad5b7c1be2783b8d0f21054f372d98ce02fc53605fc3530705db105e\"\n                    }\n                ]\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"kube-apiserver-ip-172-20-57-18.ec2.internal\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/pods/kube-apiserver-ip-172-20-57-18.ec2.internal\",\n                \"uid\": \"46c45569-f31d-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"2905859\",\n                \"creationTimestamp\": \"2017-02-15T01:22:54Z\",\n                \"labels\": {\n                    \"k8s-app\": \"kube-apiserver\"\n                },\n                \"annotations\": {\n                    \"dns.alpha.kubernetes.io/external\": \"api.bpachydermcluster.kubernetes.com\",\n                    \"dns.alpha.kubernetes.io/internal\": \"api.internal.bpachydermcluster.kubernetes.com\",\n                    \"kubernetes.io/config.hash\": \"9aad1532de8d0e12eb61ea31ccf71e15\",\n                    \"kubernetes.io/config.mirror\": \"9aad1532de8d0e12eb61ea31ccf71e15\",\n                    \"kubernetes.io/config.seen\": \"2017-02-15T01:22:31.286651491Z\",\n                    \"kubernetes.io/config.source\": \"file\"\n                }\n            },\n            \"spec\": {\n                \"volumes\": [\n                    {\n                        \"name\": \"usrsharessl\",\n                        \"hostPath\": {\n                            \"path\": \"/usr/share/ssl\"\n                        }\n                    },\n                    {\n                        \"name\": \"usrssl\",\n                        \"hostPath\": {\n                            \"path\": \"/usr/ssl\"\n                        }\n                    },\n                    {\n                        \"name\": \"usrlibssl\",\n                        \"hostPath\": {\n                            \"path\": \"/usr/lib/ssl\"\n                        }\n                    },\n                    {\n                        \"name\": \"usrlocalopenssl\",\n                        \"hostPath\": {\n                            \"path\": \"/usr/local/openssl\"\n                        }\n                    },\n                    {\n                        \"name\": \"srvkube\",\n                        \"hostPath\": {\n                            \"path\": \"/srv/kubernetes\"\n                        }\n                    },\n                    {\n                        \"name\": \"logfile\",\n                        \"hostPath\": {\n                            \"path\": \"/var/log/kube-apiserver.log\"\n                        }\n                    },\n                    {\n                        \"name\": \"etcssl\",\n                        \"hostPath\": {\n                            \"path\": \"/etc/ssl\"\n                        }\n                    },\n                    {\n                        \"name\": \"varssl\",\n                        \"hostPath\": {\n                            \"path\": \"/var/ssl\"\n                        }\n                    },\n                    {\n                        \"name\": \"etcopenssl\",\n                        \"hostPath\": {\n                            \"path\": \"/etc/openssl\"\n                        }\n                    },\n                    {\n                        \"name\": \"etcpkitls\",\n                        \"hostPath\": {\n                            \"path\": \"/etc/pki/tls\"\n                        }\n                    },\n                    {\n                        \"name\": \"etcpkicatrust\",\n                        \"hostPath\": {\n                            \"path\": \"/etc/pki/ca-trust\"\n                        }\n                    },\n                    {\n                        \"name\": \"srvsshproxy\",\n                        \"hostPath\": {\n                            \"path\": \"/srv/sshproxy\"\n                        }\n                    }\n                ],\n                \"containers\": [\n                    {\n                        \"name\": \"kube-apiserver\",\n                        \"image\": \"gcr.io/google_containers/kube-apiserver:v1.4.7\",\n                        \"command\": [\n                            \"/bin/sh\",\n                            \"-c\",\n                            \"/usr/local/bin/kube-apiserver --address=127.0.0.1 --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota --allow-privileged=true --apiserver-count=1 --basic-auth-file=/srv/kubernetes/basic_auth.csv --client-ca-file=/srv/kubernetes/ca.crt --cloud-provider=aws --etcd-servers-overrides=/events#http://127.0.0.1:4002 --etcd-servers=http://127.0.0.1:4001 --secure-port=443 --service-cluster-ip-range=100.64.0.0/13 --storage-backend=etcd2 --tls-cert-file=/srv/kubernetes/server.cert --tls-private-key-file=/srv/kubernetes/server.key --token-auth-file=/srv/kubernetes/known_tokens.csv --v=2 1\\u003e\\u003e/var/log/kube-apiserver.log 2\\u003e\\u00261\"\n                        ],\n                        \"ports\": [\n                            {\n                                \"name\": \"https\",\n                                \"hostPort\": 443,\n                                \"containerPort\": 443,\n                                \"protocol\": \"TCP\"\n                            },\n                            {\n                                \"name\": \"local\",\n                                \"hostPort\": 8080,\n                                \"containerPort\": 8080,\n                                \"protocol\": \"TCP\"\n                            }\n                        ],\n                        \"resources\": {\n                            \"requests\": {\n                                \"cpu\": \"150m\"\n                            }\n                        },\n                        \"volumeMounts\": [\n                            {\n                                \"name\": \"usrsharessl\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/usr/share/ssl\"\n                            },\n                            {\n                                \"name\": \"usrssl\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/usr/ssl\"\n                            },\n                            {\n                                \"name\": \"usrlibssl\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/usr/lib/ssl\"\n                            },\n                            {\n                                \"name\": \"usrlocalopenssl\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/usr/local/openssl\"\n                            },\n                            {\n                                \"name\": \"srvkube\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/srv/kubernetes\"\n                            },\n                            {\n                                \"name\": \"logfile\",\n                                \"mountPath\": \"/var/log/kube-apiserver.log\"\n                            },\n                            {\n                                \"name\": \"etcssl\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/etc/ssl\"\n                            },\n                            {\n                                \"name\": \"varssl\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/var/ssl\"\n                            },\n                            {\n                                \"name\": \"etcopenssl\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/etc/openssl\"\n                            },\n                            {\n                                \"name\": \"etcpkitls\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/etc/pki/tls\"\n                            },\n                            {\n                                \"name\": \"etcpkicatrust\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/etc/pki/ca-trust\"\n                            },\n                            {\n                                \"name\": \"srvsshproxy\",\n                                \"mountPath\": \"/srv/sshproxy\"\n                            }\n                        ],\n                        \"livenessProbe\": {\n                            \"httpGet\": {\n                                \"path\": \"/healthz\",\n                                \"port\": 8080,\n                                \"host\": \"127.0.0.1\",\n                                \"scheme\": \"HTTP\"\n                            },\n                            \"initialDelaySeconds\": 15,\n                            \"timeoutSeconds\": 15,\n                            \"periodSeconds\": 10,\n                            \"successThreshold\": 1,\n                            \"failureThreshold\": 3\n                        },\n                        \"terminationMessagePath\": \"/dev/termination-log\",\n                        \"imagePullPolicy\": \"IfNotPresent\"\n                    }\n                ],\n                \"restartPolicy\": \"Always\",\n                \"terminationGracePeriodSeconds\": 30,\n                \"dnsPolicy\": \"ClusterFirst\",\n                \"serviceAccountName\": \"\",\n                \"nodeName\": \"ip-172-20-57-18.ec2.internal\",\n                \"securityContext\": {\n                    \"hostNetwork\": true\n                }\n            },\n            \"status\": {\n                \"phase\": \"Running\",\n                \"conditions\": [\n                    {\n                        \"type\": \"Initialized\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:22:36Z\"\n                    },\n                    {\n                        \"type\": \"Ready\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:22:54Z\"\n                    },\n                    {\n                        \"type\": \"PodScheduled\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:22:36Z\"\n                    }\n                ],\n                \"hostIP\": \"172.20.57.18\",\n                \"podIP\": \"172.20.57.18\",\n                \"startTime\": \"2017-02-15T01:22:36Z\",\n                \"containerStatuses\": [\n                    {\n                        \"name\": \"kube-apiserver\",\n                        \"state\": {\n                            \"running\": {\n                                \"startedAt\": \"2017-02-15T01:22:53Z\"\n                            }\n                        },\n                        \"lastState\": {},\n                        \"ready\": true,\n                        \"restartCount\": 0,\n                        \"image\": \"gcr.io/google_containers/kube-apiserver:v1.4.7\",\n                        \"imageID\": \"docker://sha256:00bdd1ed85dca56e1833c44e015a2dce32404faa496a9a54d6b5ed45eea03024\",\n                        \"containerID\": \"docker://6dd6d8801e932962df83121c1ed941431cbe353958655e3cc2310ad2e6fe9304\"\n                    }\n                ]\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"kube-controller-manager-ip-172-20-57-18.ec2.internal\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/pods/kube-controller-manager-ip-172-20-57-18.ec2.internal\",\n                \"uid\": \"49ec9877-f31d-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"2905860\",\n                \"creationTimestamp\": \"2017-02-15T01:23:00Z\",\n                \"labels\": {\n                    \"k8s-app\": \"kube-controller-manager\"\n                },\n                \"annotations\": {\n                    \"kubernetes.io/config.hash\": \"2ad2337314bbbf4abf6b99d1d293a82b\",\n                    \"kubernetes.io/config.mirror\": \"2ad2337314bbbf4abf6b99d1d293a82b\",\n                    \"kubernetes.io/config.seen\": \"2017-02-15T01:22:31.286659019Z\",\n                    \"kubernetes.io/config.source\": \"file\"\n                }\n            },\n            \"spec\": {\n                \"volumes\": [\n                    {\n                        \"name\": \"usrsharessl\",\n                        \"hostPath\": {\n                            \"path\": \"/usr/share/ssl\"\n                        }\n                    },\n                    {\n                        \"name\": \"usrssl\",\n                        \"hostPath\": {\n                            \"path\": \"/usr/ssl\"\n                        }\n                    },\n                    {\n                        \"name\": \"usrlibssl\",\n                        \"hostPath\": {\n                            \"path\": \"/usr/lib/ssl\"\n                        }\n                    },\n                    {\n                        \"name\": \"usrlocalopenssl\",\n                        \"hostPath\": {\n                            \"path\": \"/usr/local/openssl\"\n                        }\n                    },\n                    {\n                        \"name\": \"srvkube\",\n                        \"hostPath\": {\n                            \"path\": \"/srv/kubernetes\"\n                        }\n                    },\n                    {\n                        \"name\": \"logfile\",\n                        \"hostPath\": {\n                            \"path\": \"/var/log/kube-controller-manager.log\"\n                        }\n                    },\n                    {\n                        \"name\": \"etcssl\",\n                        \"hostPath\": {\n                            \"path\": \"/etc/ssl\"\n                        }\n                    },\n                    {\n                        \"name\": \"varssl\",\n                        \"hostPath\": {\n                            \"path\": \"/var/ssl\"\n                        }\n                    },\n                    {\n                        \"name\": \"etcopenssl\",\n                        \"hostPath\": {\n                            \"path\": \"/etc/openssl\"\n                        }\n                    },\n                    {\n                        \"name\": \"etcpkitls\",\n                        \"hostPath\": {\n                            \"path\": \"/etc/pki/tls\"\n                        }\n                    },\n                    {\n                        \"name\": \"etcpkicatrust\",\n                        \"hostPath\": {\n                            \"path\": \"/etc/pki/ca-trust\"\n                        }\n                    }\n                ],\n                \"containers\": [\n                    {\n                        \"name\": \"kube-controller-manager\",\n                        \"image\": \"gcr.io/google_containers/kube-controller-manager:v1.4.7\",\n                        \"command\": [\n                            \"/bin/sh\",\n                            \"-c\",\n                            \"/usr/local/bin/kube-controller-manager --allocate-node-cidrs=true --cloud-provider=aws --cluster-cidr=100.96.0.0/11 --cluster-name=bpachydermcluster.kubernetes.com --configure-cloud-routes=true --leader-elect=true --master=127.0.0.1:8080 --root-ca-file=/srv/kubernetes/ca.crt --service-account-private-key-file=/srv/kubernetes/server.key --v=2 1\\u003e\\u003e/var/log/kube-controller-manager.log 2\\u003e\\u00261\"\n                        ],\n                        \"resources\": {\n                            \"requests\": {\n                                \"cpu\": \"100m\"\n                            }\n                        },\n                        \"volumeMounts\": [\n                            {\n                                \"name\": \"usrsharessl\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/usr/share/ssl\"\n                            },\n                            {\n                                \"name\": \"usrssl\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/usr/ssl\"\n                            },\n                            {\n                                \"name\": \"usrlibssl\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/usr/lib/ssl\"\n                            },\n                            {\n                                \"name\": \"usrlocalopenssl\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/usr/local/openssl\"\n                            },\n                            {\n                                \"name\": \"srvkube\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/srv/kubernetes\"\n                            },\n                            {\n                                \"name\": \"logfile\",\n                                \"mountPath\": \"/var/log/kube-controller-manager.log\"\n                            },\n                            {\n                                \"name\": \"etcssl\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/etc/ssl\"\n                            },\n                            {\n                                \"name\": \"varssl\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/var/ssl\"\n                            },\n                            {\n                                \"name\": \"etcopenssl\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/etc/openssl\"\n                            },\n                            {\n                                \"name\": \"etcpkitls\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/etc/pki/tls\"\n                            },\n                            {\n                                \"name\": \"etcpkicatrust\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/etc/pki/ca-trust\"\n                            }\n                        ],\n                        \"livenessProbe\": {\n                            \"httpGet\": {\n                                \"path\": \"/healthz\",\n                                \"port\": 10252,\n                                \"host\": \"127.0.0.1\",\n                                \"scheme\": \"HTTP\"\n                            },\n                            \"initialDelaySeconds\": 15,\n                            \"timeoutSeconds\": 15,\n                            \"periodSeconds\": 10,\n                            \"successThreshold\": 1,\n                            \"failureThreshold\": 3\n                        },\n                        \"terminationMessagePath\": \"/dev/termination-log\",\n                        \"imagePullPolicy\": \"IfNotPresent\"\n                    }\n                ],\n                \"restartPolicy\": \"Always\",\n                \"terminationGracePeriodSeconds\": 30,\n                \"dnsPolicy\": \"ClusterFirst\",\n                \"serviceAccountName\": \"\",\n                \"nodeName\": \"ip-172-20-57-18.ec2.internal\",\n                \"securityContext\": {\n                    \"hostNetwork\": true\n                }\n            },\n            \"status\": {\n                \"phase\": \"Running\",\n                \"conditions\": [\n                    {\n                        \"type\": \"Initialized\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:22:36Z\"\n                    },\n                    {\n                        \"type\": \"Ready\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:23:00Z\"\n                    },\n                    {\n                        \"type\": \"PodScheduled\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:22:36Z\"\n                    }\n                ],\n                \"hostIP\": \"172.20.57.18\",\n                \"podIP\": \"172.20.57.18\",\n                \"startTime\": \"2017-02-15T01:22:36Z\",\n                \"containerStatuses\": [\n                    {\n                        \"name\": \"kube-controller-manager\",\n                        \"state\": {\n                            \"running\": {\n                                \"startedAt\": \"2017-02-15T01:22:59Z\"\n                            }\n                        },\n                        \"lastState\": {},\n                        \"ready\": true,\n                        \"restartCount\": 0,\n                        \"image\": \"gcr.io/google_containers/kube-controller-manager:v1.4.7\",\n                        \"imageID\": \"docker://sha256:e2f7cdc2389371a096bd182aa10aae7138c41adcfd4ff2cb92e99f263b704978\",\n                        \"containerID\": \"docker://1b20d6503281be38aab72a9dd72499288cf103dd92360d01f589c13e296f4a4b\"\n                    }\n                ]\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"kube-dns-782804071-0l14u\",\n                \"generateName\": \"kube-dns-782804071-\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/pods/kube-dns-782804071-0l14u\",\n                \"uid\": \"72b27112-f31d-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"2906321\",\n                \"creationTimestamp\": \"2017-02-15T01:24:08Z\",\n                \"labels\": {\n                    \"k8s-app\": \"kube-dns\",\n                    \"pod-template-hash\": \"782804071\"\n                },\n                \"annotations\": {\n                    \"kubernetes.io/created-by\": \"{\\\"kind\\\":\\\"SerializedReference\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"reference\\\":{\\\"kind\\\":\\\"ReplicaSet\\\",\\\"namespace\\\":\\\"kube-system\\\",\\\"name\\\":\\\"kube-dns-782804071\\\",\\\"uid\\\":\\\"10af6dcf-e3e5-11e6-8dc2-12021b6d724c\\\",\\\"apiVersion\\\":\\\"extensions\\\",\\\"resourceVersion\\\":\\\"2905975\\\"}}\\n\",\n                    \"scheduler.alpha.kubernetes.io/critical-pod\": \"\",\n                    \"scheduler.alpha.kubernetes.io/tolerations\": \"[{\\\"key\\\":\\\"CriticalAddonsOnly\\\", \\\"operator\\\":\\\"Exists\\\"}]\"\n                },\n                \"ownerReferences\": [\n                    {\n                        \"apiVersion\": \"extensions/v1beta1\",\n                        \"kind\": \"ReplicaSet\",\n                        \"name\": \"kube-dns-782804071\",\n                        \"uid\": \"10af6dcf-e3e5-11e6-8dc2-12021b6d724c\",\n                        \"controller\": true\n                    }\n                ]\n            },\n            \"spec\": {\n                \"volumes\": [\n                    {\n                        \"name\": \"default-token-37qxp\",\n                        \"secret\": {\n                            \"secretName\": \"default-token-37qxp\",\n                            \"defaultMode\": 420\n                        }\n                    }\n                ],\n                \"containers\": [\n                    {\n                        \"name\": \"kubedns\",\n                        \"image\": \"gcr.io/google_containers/kubedns-amd64:1.9\",\n                        \"args\": [\n                            \"--domain=cluster.local.\",\n                            \"--dns-port=10053\",\n                            \"--config-map=kube-dns\",\n                            \"--v=2\"\n                        ],\n                        \"ports\": [\n                            {\n                                \"name\": \"dns-local\",\n                                \"containerPort\": 10053,\n                                \"protocol\": \"UDP\"\n                            },\n                            {\n                                \"name\": \"dns-tcp-local\",\n                                \"containerPort\": 10053,\n                                \"protocol\": \"TCP\"\n                            },\n                            {\n                                \"name\": \"metrics\",\n                                \"containerPort\": 10055,\n                                \"protocol\": \"TCP\"\n                            }\n                        ],\n                        \"env\": [\n                            {\n                                \"name\": \"PROMETHEUS_PORT\",\n                                \"value\": \"10055\"\n                            }\n                        ],\n                        \"resources\": {\n                            \"limits\": {\n                                \"memory\": \"170Mi\"\n                            },\n                            \"requests\": {\n                                \"cpu\": \"100m\",\n                                \"memory\": \"70Mi\"\n                            }\n                        },\n                        \"volumeMounts\": [\n                            {\n                                \"name\": \"default-token-37qxp\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\"\n                            }\n                        ],\n                        \"livenessProbe\": {\n                            \"httpGet\": {\n                                \"path\": \"/healthz-kubedns\",\n                                \"port\": 8080,\n                                \"scheme\": \"HTTP\"\n                            },\n                            \"initialDelaySeconds\": 60,\n                            \"timeoutSeconds\": 5,\n                            \"periodSeconds\": 10,\n                            \"successThreshold\": 1,\n                            \"failureThreshold\": 5\n                        },\n                        \"readinessProbe\": {\n                            \"httpGet\": {\n                                \"path\": \"/readiness\",\n                                \"port\": 8081,\n                                \"scheme\": \"HTTP\"\n                            },\n                            \"initialDelaySeconds\": 3,\n                            \"timeoutSeconds\": 5,\n                            \"periodSeconds\": 10,\n                            \"successThreshold\": 1,\n                            \"failureThreshold\": 3\n                        },\n                        \"terminationMessagePath\": \"/dev/termination-log\",\n                        \"imagePullPolicy\": \"IfNotPresent\"\n                    },\n                    {\n                        \"name\": \"dnsmasq\",\n                        \"image\": \"gcr.io/google_containers/kube-dnsmasq-amd64:1.4\",\n                        \"args\": [\n                            \"--cache-size=1000\",\n                            \"--no-resolv\",\n                            \"--server=127.0.0.1#10053\",\n                            \"--log-facility=-\"\n                        ],\n                        \"ports\": [\n                            {\n                                \"name\": \"dns\",\n                                \"containerPort\": 53,\n                                \"protocol\": \"UDP\"\n                            },\n                            {\n                                \"name\": \"dns-tcp\",\n                                \"containerPort\": 53,\n                                \"protocol\": \"TCP\"\n                            }\n                        ],\n                        \"resources\": {\n                            \"requests\": {\n                                \"cpu\": \"150m\",\n                                \"memory\": \"10Mi\"\n                            }\n                        },\n                        \"volumeMounts\": [\n                            {\n                                \"name\": \"default-token-37qxp\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\"\n                            }\n                        ],\n                        \"livenessProbe\": {\n                            \"httpGet\": {\n                                \"path\": \"/healthz-dnsmasq\",\n                                \"port\": 8080,\n                                \"scheme\": \"HTTP\"\n                            },\n                            \"initialDelaySeconds\": 60,\n                            \"timeoutSeconds\": 5,\n                            \"periodSeconds\": 10,\n                            \"successThreshold\": 1,\n                            \"failureThreshold\": 5\n                        },\n                        \"terminationMessagePath\": \"/dev/termination-log\",\n                        \"imagePullPolicy\": \"IfNotPresent\"\n                    },\n                    {\n                        \"name\": \"dnsmasq-metrics\",\n                        \"image\": \"gcr.io/google_containers/dnsmasq-metrics-amd64:1.0\",\n                        \"args\": [\n                            \"--v=2\",\n                            \"--logtostderr\"\n                        ],\n                        \"ports\": [\n                            {\n                                \"name\": \"metrics\",\n                                \"containerPort\": 10054,\n                                \"protocol\": \"TCP\"\n                            }\n                        ],\n                        \"resources\": {\n                            \"requests\": {\n                                \"memory\": \"10Mi\"\n                            }\n                        },\n                        \"volumeMounts\": [\n                            {\n                                \"name\": \"default-token-37qxp\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\"\n                            }\n                        ],\n                        \"livenessProbe\": {\n                            \"httpGet\": {\n                                \"path\": \"/metrics\",\n                                \"port\": 10054,\n                                \"scheme\": \"HTTP\"\n                            },\n                            \"initialDelaySeconds\": 60,\n                            \"timeoutSeconds\": 5,\n                            \"periodSeconds\": 10,\n                            \"successThreshold\": 1,\n                            \"failureThreshold\": 5\n                        },\n                        \"terminationMessagePath\": \"/dev/termination-log\",\n                        \"imagePullPolicy\": \"IfNotPresent\"\n                    },\n                    {\n                        \"name\": \"healthz\",\n                        \"image\": \"gcr.io/google_containers/exechealthz-amd64:1.2\",\n                        \"args\": [\n                            \"--cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 \\u003e/dev/null\",\n                            \"--url=/healthz-dnsmasq\",\n                            \"--cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 \\u003e/dev/null\",\n                            \"--url=/healthz-kubedns\",\n                            \"--port=8080\",\n                            \"--quiet\"\n                        ],\n                        \"ports\": [\n                            {\n                                \"containerPort\": 8080,\n                                \"protocol\": \"TCP\"\n                            }\n                        ],\n                        \"resources\": {\n                            \"limits\": {\n                                \"memory\": \"50Mi\"\n                            },\n                            \"requests\": {\n                                \"cpu\": \"10m\",\n                                \"memory\": \"50Mi\"\n                            }\n                        },\n                        \"volumeMounts\": [\n                            {\n                                \"name\": \"default-token-37qxp\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\"\n                            }\n                        ],\n                        \"terminationMessagePath\": \"/dev/termination-log\",\n                        \"imagePullPolicy\": \"IfNotPresent\"\n                    }\n                ],\n                \"restartPolicy\": \"Always\",\n                \"terminationGracePeriodSeconds\": 30,\n                \"dnsPolicy\": \"Default\",\n                \"serviceAccountName\": \"default\",\n                \"nodeName\": \"ip-172-20-41-204.ec2.internal\",\n                \"securityContext\": {}\n            },\n            \"status\": {\n                \"phase\": \"Running\",\n                \"conditions\": [\n                    {\n                        \"type\": \"Initialized\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:26:11Z\"\n                    },\n                    {\n                        \"type\": \"Ready\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:26:41Z\"\n                    },\n                    {\n                        \"type\": \"PodScheduled\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:26:11Z\"\n                    }\n                ],\n                \"hostIP\": \"172.20.41.204\",\n                \"podIP\": \"100.96.2.4\",\n                \"startTime\": \"2017-02-15T01:26:11Z\",\n                \"containerStatuses\": [\n                    {\n                        \"name\": \"dnsmasq\",\n                        \"state\": {\n                            \"running\": {\n                                \"startedAt\": \"2017-02-15T01:26:32Z\"\n                            }\n                        },\n                        \"lastState\": {},\n                        \"ready\": true,\n                        \"restartCount\": 0,\n                        \"image\": \"gcr.io/google_containers/kube-dnsmasq-amd64:1.4\",\n                        \"imageID\": \"docker://sha256:3ec65756a89b70b4095e43a340a6e2d5696cac7a93a29619ff5c4b6be9af2773\",\n                        \"containerID\": \"docker://ef7c515fee6c51641e2393afbce69f7a396773e1f694d479cebe87a68284585d\"\n                    },\n                    {\n                        \"name\": \"dnsmasq-metrics\",\n                        \"state\": {\n                            \"running\": {\n                                \"startedAt\": \"2017-02-15T01:26:35Z\"\n                            }\n                        },\n                        \"lastState\": {},\n                        \"ready\": true,\n                        \"restartCount\": 0,\n                        \"image\": \"gcr.io/google_containers/dnsmasq-metrics-amd64:1.0\",\n                        \"imageID\": \"docker://sha256:5271aabced07deae353277e2b8bd5b2e30ddb0b4a5884a5940115881ea8753ef\",\n                        \"containerID\": \"docker://ccc864febf71662d572ae48e336edf1d44a0966e5101960b7c2ed3b1ca71404a\"\n                    },\n                    {\n                        \"name\": \"healthz\",\n                        \"state\": {\n                            \"running\": {\n                                \"startedAt\": \"2017-02-15T01:26:37Z\"\n                            }\n                        },\n                        \"lastState\": {},\n                        \"ready\": true,\n                        \"restartCount\": 0,\n                        \"image\": \"gcr.io/google_containers/exechealthz-amd64:1.2\",\n                        \"imageID\": \"docker://sha256:93a43bfb39bfe9795e76ccd75d7a0e6d40e2ae8563456a2a77c1b4cfc3bbd967\",\n                        \"containerID\": \"docker://ce077fea0e847a556f2ccbd3bdadc61b0c14c8d56faf9331538c69337412b939\"\n                    },\n                    {\n                        \"name\": \"kubedns\",\n                        \"state\": {\n                            \"running\": {\n                                \"startedAt\": \"2017-02-15T01:26:29Z\"\n                            }\n                        },\n                        \"lastState\": {},\n                        \"ready\": true,\n                        \"restartCount\": 0,\n                        \"image\": \"gcr.io/google_containers/kubedns-amd64:1.9\",\n                        \"imageID\": \"docker://sha256:26cf1ed9b14486b93acd70c060a17fea13620393d3aa8e76036b773197c47a05\",\n                        \"containerID\": \"docker://1a4c9df5a795b82847e25b9772e18c1272bc5b9ed854740c8ec0c79dc10b6a69\"\n                    }\n                ]\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"kube-dns-autoscaler-2715466192-3u1s8\",\n                \"generateName\": \"kube-dns-autoscaler-2715466192-\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/pods/kube-dns-autoscaler-2715466192-3u1s8\",\n                \"uid\": \"72706066-f31d-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"2906263\",\n                \"creationTimestamp\": \"2017-02-15T01:24:08Z\",\n                \"labels\": {\n                    \"k8s-app\": \"kube-dns-autoscaler\",\n                    \"pod-template-hash\": \"2715466192\"\n                },\n                \"annotations\": {\n                    \"kubernetes.io/created-by\": \"{\\\"kind\\\":\\\"SerializedReference\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"reference\\\":{\\\"kind\\\":\\\"ReplicaSet\\\",\\\"namespace\\\":\\\"kube-system\\\",\\\"name\\\":\\\"kube-dns-autoscaler-2715466192\\\",\\\"uid\\\":\\\"10a93ff8-e3e5-11e6-8dc2-12021b6d724c\\\",\\\"apiVersion\\\":\\\"extensions\\\",\\\"resourceVersion\\\":\\\"2905958\\\"}}\\n\",\n                    \"scheduler.alpha.kubernetes.io/critical-pod\": \"\",\n                    \"scheduler.alpha.kubernetes.io/tolerations\": \"[{\\\"key\\\":\\\"CriticalAddonsOnly\\\", \\\"operator\\\":\\\"Exists\\\"}]\"\n                },\n                \"ownerReferences\": [\n                    {\n                        \"apiVersion\": \"extensions/v1beta1\",\n                        \"kind\": \"ReplicaSet\",\n                        \"name\": \"kube-dns-autoscaler-2715466192\",\n                        \"uid\": \"10a93ff8-e3e5-11e6-8dc2-12021b6d724c\",\n                        \"controller\": true\n                    }\n                ]\n            },\n            \"spec\": {\n                \"volumes\": [\n                    {\n                        \"name\": \"default-token-37qxp\",\n                        \"secret\": {\n                            \"secretName\": \"default-token-37qxp\",\n                            \"defaultMode\": 420\n                        }\n                    }\n                ],\n                \"containers\": [\n                    {\n                        \"name\": \"autoscaler\",\n                        \"image\": \"gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.0.0\",\n                        \"command\": [\n                            \"/cluster-proportional-autoscaler\",\n                            \"--namespace=kube-system\",\n                            \"--configmap=kube-dns-autoscaler\",\n                            \"--mode=linear\",\n                            \"--target=Deployment/kube-dns\",\n                            \"--default-params={\\\"linear\\\":{\\\"coresPerReplica\\\":256,\\\"nodesPerReplica\\\":16,\\\"min\\\":1}}\",\n                            \"--logtostderr=true\",\n                            \"--v=2\"\n                        ],\n                        \"resources\": {\n                            \"requests\": {\n                                \"cpu\": \"20m\",\n                                \"memory\": \"10Mi\"\n                            }\n                        },\n                        \"volumeMounts\": [\n                            {\n                                \"name\": \"default-token-37qxp\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\"\n                            }\n                        ],\n                        \"terminationMessagePath\": \"/dev/termination-log\",\n                        \"imagePullPolicy\": \"IfNotPresent\"\n                    }\n                ],\n                \"restartPolicy\": \"Always\",\n                \"terminationGracePeriodSeconds\": 30,\n                \"dnsPolicy\": \"ClusterFirst\",\n                \"serviceAccountName\": \"default\",\n                \"nodeName\": \"ip-172-20-58-236.ec2.internal\",\n                \"securityContext\": {}\n            },\n            \"status\": {\n                \"phase\": \"Running\",\n                \"conditions\": [\n                    {\n                        \"type\": \"Initialized\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:26:11Z\"\n                    },\n                    {\n                        \"type\": \"Ready\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:26:18Z\"\n                    },\n                    {\n                        \"type\": \"PodScheduled\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:26:11Z\"\n                    }\n                ],\n                \"hostIP\": \"172.20.58.236\",\n                \"podIP\": \"100.96.1.2\",\n                \"startTime\": \"2017-02-15T01:26:11Z\",\n                \"containerStatuses\": [\n                    {\n                        \"name\": \"autoscaler\",\n                        \"state\": {\n                            \"running\": {\n                                \"startedAt\": \"2017-02-15T01:26:18Z\"\n                            }\n                        },\n                        \"lastState\": {},\n                        \"ready\": true,\n                        \"restartCount\": 0,\n                        \"image\": \"gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.0.0\",\n                        \"imageID\": \"docker://sha256:e183460c484d3ddda51797d29f23e2212f646f40a830b173cbeef81d6f27afed\",\n                        \"containerID\": \"docker://3e0374c4c99c1b6a7f2fa115e1e355737872d2d8ddd91a0b5c14d14d30d9af7c\"\n                    }\n                ]\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"kube-proxy-ip-172-20-41-204.ec2.internal\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/pods/kube-proxy-ip-172-20-41-204.ec2.internal\",\n                \"uid\": \"b8305e00-f31d-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"2906271\",\n                \"creationTimestamp\": \"2017-02-15T01:26:05Z\",\n                \"labels\": {\n                    \"k8s-app\": \"kube-proxy\"\n                },\n                \"annotations\": {\n                    \"kubernetes.io/config.hash\": \"28077438d41902c09fe3d2e626c973d0\",\n                    \"kubernetes.io/config.mirror\": \"28077438d41902c09fe3d2e626c973d0\",\n                    \"kubernetes.io/config.seen\": \"2017-02-15T01:25:30.080213172Z\",\n                    \"kubernetes.io/config.source\": \"file\"\n                }\n            },\n            \"spec\": {\n                \"volumes\": [\n                    {\n                        \"name\": \"ssl-certs-host\",\n                        \"hostPath\": {\n                            \"path\": \"/usr/share/ca-certificates\"\n                        }\n                    },\n                    {\n                        \"name\": \"kubeconfig\",\n                        \"hostPath\": {\n                            \"path\": \"/var/lib/kube-proxy/kubeconfig\"\n                        }\n                    },\n                    {\n                        \"name\": \"varlog\",\n                        \"hostPath\": {\n                            \"path\": \"/var/log\"\n                        }\n                    }\n                ],\n                \"containers\": [\n                    {\n                        \"name\": \"kube-proxy\",\n                        \"image\": \"gcr.io/google_containers/kube-proxy:v1.4.7\",\n                        \"command\": [\n                            \"/bin/sh\",\n                            \"-c\",\n                            \"kube-proxy --kubeconfig=/var/lib/kube-proxy/kubeconfig --resource-container=\\\"\\\" --master=https://api.internal.bpachydermcluster.kubernetes.com --v=2 1\\u003e\\u003e/var/log/kube-proxy.log 2\\u003e\\u00261\"\n                        ],\n                        \"resources\": {\n                            \"requests\": {\n                                \"cpu\": \"100m\"\n                            }\n                        },\n                        \"volumeMounts\": [\n                            {\n                                \"name\": \"ssl-certs-host\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/etc/ssl/certs\"\n                            },\n                            {\n                                \"name\": \"varlog\",\n                                \"mountPath\": \"/var/log\"\n                            },\n                            {\n                                \"name\": \"kubeconfig\",\n                                \"mountPath\": \"/var/lib/kube-proxy/kubeconfig\"\n                            }\n                        ],\n                        \"terminationMessagePath\": \"/dev/termination-log\",\n                        \"imagePullPolicy\": \"IfNotPresent\",\n                        \"securityContext\": {\n                            \"privileged\": true\n                        }\n                    }\n                ],\n                \"restartPolicy\": \"Always\",\n                \"terminationGracePeriodSeconds\": 30,\n                \"dnsPolicy\": \"ClusterFirst\",\n                \"serviceAccountName\": \"\",\n                \"nodeName\": \"ip-172-20-41-204.ec2.internal\",\n                \"securityContext\": {\n                    \"hostNetwork\": true\n                }\n            },\n            \"status\": {\n                \"phase\": \"Running\",\n                \"conditions\": [\n                    {\n                        \"type\": \"Initialized\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:26:05Z\"\n                    },\n                    {\n                        \"type\": \"Ready\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:26:20Z\"\n                    },\n                    {\n                        \"type\": \"PodScheduled\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:26:05Z\"\n                    }\n                ],\n                \"hostIP\": \"172.20.41.204\",\n                \"podIP\": \"172.20.41.204\",\n                \"startTime\": \"2017-02-15T01:26:05Z\",\n                \"containerStatuses\": [\n                    {\n                        \"name\": \"kube-proxy\",\n                        \"state\": {\n                            \"running\": {\n                                \"startedAt\": \"2017-02-15T01:26:19Z\"\n                            }\n                        },\n                        \"lastState\": {},\n                        \"ready\": true,\n                        \"restartCount\": 0,\n                        \"image\": \"gcr.io/google_containers/kube-proxy:v1.4.7\",\n                        \"imageID\": \"docker://sha256:9e4978d03b60564d576142232a4ea144c7b08be2b3fce8a026b7814587d87ef4\",\n                        \"containerID\": \"docker://f093a7bce91a9f327fa7193fef808c015cfb79d72ca0bd3b805a6fe65847a248\"\n                    }\n                ]\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"kube-proxy-ip-172-20-57-18.ec2.internal\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/pods/kube-proxy-ip-172-20-57-18.ec2.internal\",\n                \"uid\": \"4ff5c189-f31d-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"2905863\",\n                \"creationTimestamp\": \"2017-02-15T01:23:10Z\",\n                \"labels\": {\n                    \"k8s-app\": \"kube-proxy\"\n                },\n                \"annotations\": {\n                    \"kubernetes.io/config.hash\": \"f0cc5d2293c36667d80bad6a4192c6bf\",\n                    \"kubernetes.io/config.mirror\": \"f0cc5d2293c36667d80bad6a4192c6bf\",\n                    \"kubernetes.io/config.seen\": \"2017-02-15T01:22:31.286663228Z\",\n                    \"kubernetes.io/config.source\": \"file\"\n                }\n            },\n            \"spec\": {\n                \"volumes\": [\n                    {\n                        \"name\": \"ssl-certs-host\",\n                        \"hostPath\": {\n                            \"path\": \"/usr/share/ca-certificates\"\n                        }\n                    },\n                    {\n                        \"name\": \"kubeconfig\",\n                        \"hostPath\": {\n                            \"path\": \"/var/lib/kube-proxy/kubeconfig\"\n                        }\n                    },\n                    {\n                        \"name\": \"varlog\",\n                        \"hostPath\": {\n                            \"path\": \"/var/log\"\n                        }\n                    }\n                ],\n                \"containers\": [\n                    {\n                        \"name\": \"kube-proxy\",\n                        \"image\": \"gcr.io/google_containers/kube-proxy:v1.4.7\",\n                        \"command\": [\n                            \"/bin/sh\",\n                            \"-c\",\n                            \"kube-proxy --kubeconfig=/var/lib/kube-proxy/kubeconfig --resource-container=\\\"\\\" --master=http://127.0.0.1:8080 --v=2 1\\u003e\\u003e/var/log/kube-proxy.log 2\\u003e\\u00261\"\n                        ],\n                        \"resources\": {\n                            \"requests\": {\n                                \"cpu\": \"100m\"\n                            }\n                        },\n                        \"volumeMounts\": [\n                            {\n                                \"name\": \"ssl-certs-host\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/etc/ssl/certs\"\n                            },\n                            {\n                                \"name\": \"varlog\",\n                                \"mountPath\": \"/var/log\"\n                            },\n                            {\n                                \"name\": \"kubeconfig\",\n                                \"mountPath\": \"/var/lib/kube-proxy/kubeconfig\"\n                            }\n                        ],\n                        \"terminationMessagePath\": \"/dev/termination-log\",\n                        \"imagePullPolicy\": \"IfNotPresent\",\n                        \"securityContext\": {\n                            \"privileged\": true\n                        }\n                    }\n                ],\n                \"restartPolicy\": \"Always\",\n                \"terminationGracePeriodSeconds\": 30,\n                \"dnsPolicy\": \"ClusterFirst\",\n                \"serviceAccountName\": \"\",\n                \"nodeName\": \"ip-172-20-57-18.ec2.internal\",\n                \"securityContext\": {\n                    \"hostNetwork\": true\n                }\n            },\n            \"status\": {\n                \"phase\": \"Running\",\n                \"conditions\": [\n                    {\n                        \"type\": \"Initialized\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:22:36Z\"\n                    },\n                    {\n                        \"type\": \"Ready\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:23:10Z\"\n                    },\n                    {\n                        \"type\": \"PodScheduled\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:22:36Z\"\n                    }\n                ],\n                \"hostIP\": \"172.20.57.18\",\n                \"podIP\": \"172.20.57.18\",\n                \"startTime\": \"2017-02-15T01:22:36Z\",\n                \"containerStatuses\": [\n                    {\n                        \"name\": \"kube-proxy\",\n                        \"state\": {\n                            \"running\": {\n                                \"startedAt\": \"2017-02-15T01:23:09Z\"\n                            }\n                        },\n                        \"lastState\": {},\n                        \"ready\": true,\n                        \"restartCount\": 0,\n                        \"image\": \"gcr.io/google_containers/kube-proxy:v1.4.7\",\n                        \"imageID\": \"docker://sha256:9e4978d03b60564d576142232a4ea144c7b08be2b3fce8a026b7814587d87ef4\",\n                        \"containerID\": \"docker://83213a643705766eff67cf986baa0e4831438619519d9d2015af878f3db4142e\"\n                    }\n                ]\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"kube-proxy-ip-172-20-58-236.ec2.internal\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/pods/kube-proxy-ip-172-20-58-236.ec2.internal\",\n                \"uid\": \"aece1be4-f31d-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"2906225\",\n                \"creationTimestamp\": \"2017-02-15T01:25:49Z\",\n                \"labels\": {\n                    \"k8s-app\": \"kube-proxy\"\n                },\n                \"annotations\": {\n                    \"kubernetes.io/config.hash\": \"28077438d41902c09fe3d2e626c973d0\",\n                    \"kubernetes.io/config.mirror\": \"28077438d41902c09fe3d2e626c973d0\",\n                    \"kubernetes.io/config.seen\": \"2017-02-15T01:22:19.343111055Z\",\n                    \"kubernetes.io/config.source\": \"file\"\n                }\n            },\n            \"spec\": {\n                \"volumes\": [\n                    {\n                        \"name\": \"ssl-certs-host\",\n                        \"hostPath\": {\n                            \"path\": \"/usr/share/ca-certificates\"\n                        }\n                    },\n                    {\n                        \"name\": \"kubeconfig\",\n                        \"hostPath\": {\n                            \"path\": \"/var/lib/kube-proxy/kubeconfig\"\n                        }\n                    },\n                    {\n                        \"name\": \"varlog\",\n                        \"hostPath\": {\n                            \"path\": \"/var/log\"\n                        }\n                    }\n                ],\n                \"containers\": [\n                    {\n                        \"name\": \"kube-proxy\",\n                        \"image\": \"gcr.io/google_containers/kube-proxy:v1.4.7\",\n                        \"command\": [\n                            \"/bin/sh\",\n                            \"-c\",\n                            \"kube-proxy --kubeconfig=/var/lib/kube-proxy/kubeconfig --resource-container=\\\"\\\" --master=https://api.internal.bpachydermcluster.kubernetes.com --v=2 1\\u003e\\u003e/var/log/kube-proxy.log 2\\u003e\\u00261\"\n                        ],\n                        \"resources\": {\n                            \"requests\": {\n                                \"cpu\": \"100m\"\n                            }\n                        },\n                        \"volumeMounts\": [\n                            {\n                                \"name\": \"ssl-certs-host\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/etc/ssl/certs\"\n                            },\n                            {\n                                \"name\": \"varlog\",\n                                \"mountPath\": \"/var/log\"\n                            },\n                            {\n                                \"name\": \"kubeconfig\",\n                                \"mountPath\": \"/var/lib/kube-proxy/kubeconfig\"\n                            }\n                        ],\n                        \"terminationMessagePath\": \"/dev/termination-log\",\n                        \"imagePullPolicy\": \"IfNotPresent\",\n                        \"securityContext\": {\n                            \"privileged\": true\n                        }\n                    }\n                ],\n                \"restartPolicy\": \"Always\",\n                \"terminationGracePeriodSeconds\": 30,\n                \"dnsPolicy\": \"ClusterFirst\",\n                \"serviceAccountName\": \"\",\n                \"nodeName\": \"ip-172-20-58-236.ec2.internal\",\n                \"securityContext\": {\n                    \"hostNetwork\": true\n                }\n            },\n            \"status\": {\n                \"phase\": \"Running\",\n                \"conditions\": [\n                    {\n                        \"type\": \"Initialized\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:25:49Z\"\n                    },\n                    {\n                        \"type\": \"Ready\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:26:02Z\"\n                    },\n                    {\n                        \"type\": \"PodScheduled\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:25:49Z\"\n                    }\n                ],\n                \"hostIP\": \"172.20.58.236\",\n                \"podIP\": \"172.20.58.236\",\n                \"startTime\": \"2017-02-15T01:25:49Z\",\n                \"containerStatuses\": [\n                    {\n                        \"name\": \"kube-proxy\",\n                        \"state\": {\n                            \"running\": {\n                                \"startedAt\": \"2017-02-15T01:26:02Z\"\n                            }\n                        },\n                        \"lastState\": {},\n                        \"ready\": true,\n                        \"restartCount\": 0,\n                        \"image\": \"gcr.io/google_containers/kube-proxy:v1.4.7\",\n                        \"imageID\": \"docker://sha256:9e4978d03b60564d576142232a4ea144c7b08be2b3fce8a026b7814587d87ef4\",\n                        \"containerID\": \"docker://ae5d6f95751b746e9126114705db371854cc198ecd3d4a20c0b44ac849971c5e\"\n                    }\n                ]\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"kube-scheduler-ip-172-20-57-18.ec2.internal\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/pods/kube-scheduler-ip-172-20-57-18.ec2.internal\",\n                \"uid\": \"5259da9c-f31d-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"2905869\",\n                \"creationTimestamp\": \"2017-02-15T01:23:14Z\",\n                \"labels\": {\n                    \"k8s-app\": \"kube-scheduler\"\n                },\n                \"annotations\": {\n                    \"kubernetes.io/config.hash\": \"9f0383a38b3adbc180075eebb530e421\",\n                    \"kubernetes.io/config.mirror\": \"9f0383a38b3adbc180075eebb530e421\",\n                    \"kubernetes.io/config.seen\": \"2017-02-15T01:22:31.286666739Z\",\n                    \"kubernetes.io/config.source\": \"file\"\n                }\n            },\n            \"spec\": {\n                \"volumes\": [\n                    {\n                        \"name\": \"logfile\",\n                        \"hostPath\": {\n                            \"path\": \"/var/log/kube-scheduler.log\"\n                        }\n                    }\n                ],\n                \"containers\": [\n                    {\n                        \"name\": \"kube-scheduler\",\n                        \"image\": \"gcr.io/google_containers/kube-scheduler:v1.4.7\",\n                        \"command\": [\n                            \"/bin/sh\",\n                            \"-c\",\n                            \"/usr/local/bin/kube-scheduler --leader-elect=true --master=127.0.0.1:8080 --v=2 1\\u003e\\u003e/var/log/kube-scheduler.log 2\\u003e\\u00261\"\n                        ],\n                        \"resources\": {\n                            \"requests\": {\n                                \"cpu\": \"100m\"\n                            }\n                        },\n                        \"volumeMounts\": [\n                            {\n                                \"name\": \"logfile\",\n                                \"mountPath\": \"/var/log/kube-scheduler.log\"\n                            }\n                        ],\n                        \"livenessProbe\": {\n                            \"httpGet\": {\n                                \"path\": \"/healthz\",\n                                \"port\": 10251,\n                                \"host\": \"127.0.0.1\",\n                                \"scheme\": \"HTTP\"\n                            },\n                            \"initialDelaySeconds\": 15,\n                            \"timeoutSeconds\": 15,\n                            \"periodSeconds\": 10,\n                            \"successThreshold\": 1,\n                            \"failureThreshold\": 3\n                        },\n                        \"terminationMessagePath\": \"/dev/termination-log\",\n                        \"imagePullPolicy\": \"IfNotPresent\"\n                    }\n                ],\n                \"restartPolicy\": \"Always\",\n                \"terminationGracePeriodSeconds\": 30,\n                \"dnsPolicy\": \"ClusterFirst\",\n                \"serviceAccountName\": \"\",\n                \"nodeName\": \"ip-172-20-57-18.ec2.internal\",\n                \"securityContext\": {\n                    \"hostNetwork\": true\n                }\n            },\n            \"status\": {\n                \"phase\": \"Running\",\n                \"conditions\": [\n                    {\n                        \"type\": \"Initialized\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:22:36Z\"\n                    },\n                    {\n                        \"type\": \"Ready\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:23:14Z\"\n                    },\n                    {\n                        \"type\": \"PodScheduled\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-02-15T01:22:36Z\"\n                    }\n                ],\n                \"hostIP\": \"172.20.57.18\",\n                \"podIP\": \"172.20.57.18\",\n                \"startTime\": \"2017-02-15T01:22:36Z\",\n                \"containerStatuses\": [\n                    {\n                        \"name\": \"kube-scheduler\",\n                        \"state\": {\n                            \"running\": {\n                                \"startedAt\": \"2017-02-15T01:23:13Z\"\n                            }\n                        },\n                        \"lastState\": {},\n                        \"ready\": true,\n                        \"restartCount\": 0,\n                        \"image\": \"gcr.io/google_containers/kube-scheduler:v1.4.7\",\n                        \"imageID\": \"docker://sha256:d38dbdb13b771397cf2357ec15e083cb78f768dc21809a22636a672215260771\",\n                        \"containerID\": \"docker://ffb841b6bc556e06fe549fe0cf91154eee4b84ff128ed124389aceb7ac414239\"\n                    }\n                ]\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-grafana-3344903701-kd95t\",\n                \"generateName\": \"monitoring-grafana-3344903701-\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/pods/monitoring-grafana-3344903701-kd95t\",\n                \"uid\": \"3ae5d9c0-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"5110325\",\n                \"creationTimestamp\": \"2017-03-03T00:27:48Z\",\n                \"labels\": {\n                    \"k8s-app\": \"grafana\",\n                    \"pod-template-hash\": \"3344903701\",\n                    \"task\": \"monitoring\"\n                },\n                \"annotations\": {\n                    \"kubernetes.io/created-by\": \"{\\\"kind\\\":\\\"SerializedReference\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"reference\\\":{\\\"kind\\\":\\\"ReplicaSet\\\",\\\"namespace\\\":\\\"kube-system\\\",\\\"name\\\":\\\"monitoring-grafana-3344903701\\\",\\\"uid\\\":\\\"3ae4a19b-ffa8-11e6-963a-12cd63c7b19c\\\",\\\"apiVersion\\\":\\\"extensions\\\",\\\"resourceVersion\\\":\\\"5110283\\\"}}\\n\"\n                },\n                \"ownerReferences\": [\n                    {\n                        \"apiVersion\": \"extensions/v1beta1\",\n                        \"kind\": \"ReplicaSet\",\n                        \"name\": \"monitoring-grafana-3344903701\",\n                        \"uid\": \"3ae4a19b-ffa8-11e6-963a-12cd63c7b19c\",\n                        \"controller\": true\n                    }\n                ]\n            },\n            \"spec\": {\n                \"volumes\": [\n                    {\n                        \"name\": \"grafana-storage\",\n                        \"emptyDir\": {}\n                    },\n                    {\n                        \"name\": \"default-token-37qxp\",\n                        \"secret\": {\n                            \"secretName\": \"default-token-37qxp\",\n                            \"defaultMode\": 420\n                        }\n                    }\n                ],\n                \"containers\": [\n                    {\n                        \"name\": \"grafana\",\n                        \"image\": \"gcr.io/google_containers/heapster-grafana-amd64:v4.0.2\",\n                        \"ports\": [\n                            {\n                                \"containerPort\": 3000,\n                                \"protocol\": \"TCP\"\n                            }\n                        ],\n                        \"env\": [\n                            {\n                                \"name\": \"INFLUXDB_HOST\",\n                                \"value\": \"monitoring-influxdb\"\n                            },\n                            {\n                                \"name\": \"GRAFANA_PORT\",\n                                \"value\": \"3000\"\n                            },\n                            {\n                                \"name\": \"GF_AUTH_BASIC_ENABLED\",\n                                \"value\": \"false\"\n                            },\n                            {\n                                \"name\": \"GF_AUTH_ANONYMOUS_ENABLED\",\n                                \"value\": \"true\"\n                            },\n                            {\n                                \"name\": \"GF_AUTH_ANONYMOUS_ORG_ROLE\",\n                                \"value\": \"Admin\"\n                            },\n                            {\n                                \"name\": \"GF_SERVER_ROOT_URL\",\n                                \"value\": \"/\"\n                            }\n                        ],\n                        \"resources\": {},\n                        \"volumeMounts\": [\n                            {\n                                \"name\": \"grafana-storage\",\n                                \"mountPath\": \"/var\"\n                            },\n                            {\n                                \"name\": \"default-token-37qxp\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\"\n                            }\n                        ],\n                        \"terminationMessagePath\": \"/dev/termination-log\",\n                        \"imagePullPolicy\": \"IfNotPresent\"\n                    }\n                ],\n                \"restartPolicy\": \"Always\",\n                \"terminationGracePeriodSeconds\": 30,\n                \"dnsPolicy\": \"ClusterFirst\",\n                \"serviceAccountName\": \"default\",\n                \"nodeName\": \"ip-172-20-58-236.ec2.internal\",\n                \"securityContext\": {}\n            },\n            \"status\": {\n                \"phase\": \"Running\",\n                \"conditions\": [\n                    {\n                        \"type\": \"Initialized\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-03-03T00:27:48Z\"\n                    },\n                    {\n                        \"type\": \"Ready\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-03-03T00:27:51Z\"\n                    },\n                    {\n                        \"type\": \"PodScheduled\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-03-03T00:27:48Z\"\n                    }\n                ],\n                \"hostIP\": \"172.20.58.236\",\n                \"podIP\": \"100.96.1.62\",\n                \"startTime\": \"2017-03-03T00:27:48Z\",\n                \"containerStatuses\": [\n                    {\n                        \"name\": \"grafana\",\n                        \"state\": {\n                            \"running\": {\n                                \"startedAt\": \"2017-03-03T00:27:50Z\"\n                            }\n                        },\n                        \"lastState\": {},\n                        \"ready\": true,\n                        \"restartCount\": 0,\n                        \"image\": \"gcr.io/google_containers/heapster-grafana-amd64:v4.0.2\",\n                        \"imageID\": \"docker://sha256:a1956d2a1a1643ea7a27fbb614804e59885b8197a32a0ac0b2e065eb34f3c766\",\n                        \"containerID\": \"docker://174110ad9cfef878847bb0ee29f3e41a9f1bb8643b931f0801f2638ea028903b\"\n                    }\n                ]\n            }\n        },\n        {\n            \"metadata\": {\n                \"name\": \"monitoring-influxdb-421024531-vwd0o\",\n                \"generateName\": \"monitoring-influxdb-421024531-\",\n                \"namespace\": \"kube-system\",\n                \"selfLink\": \"/api/v1/namespaces/kube-system/pods/monitoring-influxdb-421024531-vwd0o\",\n                \"uid\": \"3b589343-ffa8-11e6-963a-12cd63c7b19c\",\n                \"resourceVersion\": \"5110333\",\n                \"creationTimestamp\": \"2017-03-03T00:27:49Z\",\n                \"labels\": {\n                    \"k8s-app\": \"influxdb\",\n                    \"pod-template-hash\": \"421024531\",\n                    \"task\": \"monitoring\"\n                },\n                \"annotations\": {\n                    \"kubernetes.io/created-by\": \"{\\\"kind\\\":\\\"SerializedReference\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"reference\\\":{\\\"kind\\\":\\\"ReplicaSet\\\",\\\"namespace\\\":\\\"kube-system\\\",\\\"name\\\":\\\"monitoring-influxdb-421024531\\\",\\\"uid\\\":\\\"3b580ba4-ffa8-11e6-963a-12cd63c7b19c\\\",\\\"apiVersion\\\":\\\"extensions\\\",\\\"resourceVersion\\\":\\\"5110310\\\"}}\\n\"\n                },\n                \"ownerReferences\": [\n                    {\n                        \"apiVersion\": \"extensions/v1beta1\",\n                        \"kind\": \"ReplicaSet\",\n                        \"name\": \"monitoring-influxdb-421024531\",\n                        \"uid\": \"3b580ba4-ffa8-11e6-963a-12cd63c7b19c\",\n                        \"controller\": true\n                    }\n                ]\n            },\n            \"spec\": {\n                \"volumes\": [\n                    {\n                        \"name\": \"influxdb-storage\",\n                        \"emptyDir\": {}\n                    },\n                    {\n                        \"name\": \"default-token-37qxp\",\n                        \"secret\": {\n                            \"secretName\": \"default-token-37qxp\",\n                            \"defaultMode\": 420\n                        }\n                    }\n                ],\n                \"containers\": [\n                    {\n                        \"name\": \"influxdb\",\n                        \"image\": \"gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\",\n                        \"resources\": {},\n                        \"volumeMounts\": [\n                            {\n                                \"name\": \"influxdb-storage\",\n                                \"mountPath\": \"/data\"\n                            },\n                            {\n                                \"name\": \"default-token-37qxp\",\n                                \"readOnly\": true,\n                                \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\"\n                            }\n                        ],\n                        \"terminationMessagePath\": \"/dev/termination-log\",\n                        \"imagePullPolicy\": \"IfNotPresent\"\n                    }\n                ],\n                \"restartPolicy\": \"Always\",\n                \"terminationGracePeriodSeconds\": 30,\n                \"dnsPolicy\": \"ClusterFirst\",\n                \"serviceAccountName\": \"default\",\n                \"nodeName\": \"ip-172-20-41-204.ec2.internal\",\n                \"securityContext\": {}\n            },\n            \"status\": {\n                \"phase\": \"Running\",\n                \"conditions\": [\n                    {\n                        \"type\": \"Initialized\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-03-03T00:27:49Z\"\n                    },\n                    {\n                        \"type\": \"Ready\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-03-03T00:27:51Z\"\n                    },\n                    {\n                        \"type\": \"PodScheduled\",\n                        \"status\": \"True\",\n                        \"lastProbeTime\": null,\n                        \"lastTransitionTime\": \"2017-03-03T00:27:49Z\"\n                    }\n                ],\n                \"hostIP\": \"172.20.41.204\",\n                \"podIP\": \"100.96.2.28\",\n                \"startTime\": \"2017-03-03T00:27:49Z\",\n                \"containerStatuses\": [\n                    {\n                        \"name\": \"influxdb\",\n                        \"state\": {\n                            \"running\": {\n                                \"startedAt\": \"2017-03-03T00:27:50Z\"\n                            }\n                        },\n                        \"lastState\": {},\n                        \"ready\": true,\n                        \"restartCount\": 0,\n                        \"image\": \"gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1\",\n                        \"imageID\": \"docker://sha256:d3fccbedd1804ec28362bfa27dee04d10d3a2d04f4c6774bc18578f71134f8ab\",\n                        \"containerID\": \"docker://03c52d1ae3bef979753b24e1ef7fba140c460b9275601c05e7a69a2d787a271a\"\n                    }\n                ]\n            }\n        }\n    ]\n}\n==== START logs for kube-system/dns-controller-4128279831-5u7dk ====\nI0303 00:19:22.676385       1 nodecontroller.go:103] node watch channel closed\nW0303 00:19:22.676412       1 nodecontroller.go:70] querying without field filter\nI0303 00:19:22.678855       1 nodecontroller.go:79] node: ip-172-20-41-204.ec2.internal\nI0303 00:19:22.678881       1 nodecontroller.go:79] node: ip-172-20-57-18.ec2.internal\nI0303 00:19:22.678891       1 nodecontroller.go:79] node: ip-172-20-58-236.ec2.internal\nW0303 00:19:22.678900       1 nodecontroller.go:86] querying without field filter\nI0303 00:35:13.663182       1 podcontroller.go:101] pod watch channel closed\nW0303 00:35:13.663211       1 podcontroller.go:68] querying without label filter\nW0303 00:35:13.663216       1 podcontroller.go:70] querying without field filter\nW0303 00:35:13.674885       1 podcontroller.go:83] querying without label filter\nW0303 00:35:13.674902       1 podcontroller.go:85] querying without field filter\n==== END logs for kube-system/dns-controller-4128279831-5u7dk ====\n==== START logs for kube-system/etcd-server-events-ip-172-20-57-18.ec2.internal ====\n==== END logs for kube-system/etcd-server-events-ip-172-20-57-18.ec2.internal ====\n==== START logs for kube-system/etcd-server-ip-172-20-57-18.ec2.internal ====\n==== END logs for kube-system/etcd-server-ip-172-20-57-18.ec2.internal ====\n==== START logs for kube-system/heapster-564189836-dfalg ====\nI0303 00:27:51.180868       1 heapster.go:71] /heapster --source=kubernetes:https://kubernetes.default --sink=influxdb:http://monitoring-influxdb:8086\nI0303 00:27:51.180994       1 heapster.go:72] Heapster version v1.3.0-beta.1\nI0303 00:27:51.181288       1 configs.go:61] Using Kubernetes client with master \"https://kubernetes.default\" and version v1\nI0303 00:27:51.181308       1 configs.go:62] Using kubelet port 10255\nE0303 00:29:58.553267       1 influxdb.go:238] issues while creating an InfluxDB sink: failed to ping InfluxDB server at \"monitoring-influxdb:8086\" - Get http://monitoring-influxdb:8086/ping: dial tcp 100.67.200.183:8086: getsockopt: connection timed out, will retry on use\nI0303 00:29:58.553292       1 influxdb.go:252] created influxdb sink with options: host:monitoring-influxdb:8086 user:root db:k8s\nI0303 00:29:58.553311       1 heapster.go:193] Starting with InfluxDB Sink\nI0303 00:29:58.553315       1 heapster.go:193] Starting with Metric Sink\nI0303 00:29:58.566688       1 heapster.go:105] Starting heapster on port 8082\nI0303 00:30:05.051082       1 influxdb.go:215] Created database \"k8s\" on influxDB server at \"monitoring-influxdb:8086\"\n==== END logs for kube-system/heapster-564189836-dfalg ====\n==== START logs for kube-system/kube-apiserver-ip-172-20-57-18.ec2.internal ====\n==== END logs for kube-system/kube-apiserver-ip-172-20-57-18.ec2.internal ====\n==== START logs for kube-system/kube-controller-manager-ip-172-20-57-18.ec2.internal ====\n==== END logs for kube-system/kube-controller-manager-ip-172-20-57-18.ec2.internal ====\n==== START logs for kube-system/kube-dns-782804071-0l14u ====\nError from server (BadRequest): the server rejected our request for an unknown reason (get pods kube-dns-782804071-0l14u)\n```. Added the blocking logic. Had to add an arg for the namespace as well. The 5s timeout is due to the fact that a remote cluster sometimes takes a bit to respond (1s didn't work for me w my aws cluster).. Ok, I've made the needed changes, now I need a review from you @msteffen . Oh, huh, whoops. I thought if I clicked 'approved' bcz of the changes I requested/made I could then request a review from you. Looks like no. So let's do it the old fashioned way and just use comments and a 'LGTM'. We recently ran into a new case.\nIt seems like a bug snuck in around how we provision volumes for rethink in the non stateful set deployment. (Part of this is k8s changing the default behavior w 1.5). If we had a test that deployed to AWS, created some jobs/output, tore down the cluster, redeployed pach cluster to check that the state persists, we would've caught this error. Now that we have a bead on the issue, we can write a simpler test, but it would still need to be run on a cloud deployment.\n. Here's another case we should have coverage for:\nhttps://github.com/pachyderm/pachyderm/issues/1423. We also want to have coverage of our object stores as described in #906, #648 \n. One thing that came up recently is the go s3 client does some fancy things like read the EC2 metadata to get the IAM role of the instance the code is running on and using those credentials for s3 access. This has become an important use case for customer installations. I'm not sure if the minio client does that.\nThat plus the timestamp on this issue, I'm marking it as stale.. stale. Matt and I discussed implementation a bit. This should be pretty straightforward.\nThe worker already captures the stdout of the user's code.\nTo hook in a k8s log aggregator, we need to write the stdout for the pod.\nSo, we'll basically be a Tee writer (one output to block store, one to pod stdout), and the one that gets written to stdout will write in the format the log aggregator expects (e.g. statsd wants json) with the metadata fields we want (timestamp, level, workerID, datumID or hash, message/user std out line).\nThis also has the advantage that we can output more data to the log aggregator that would be helpful for debugging. E.g. While debugging its often helpful to see exactly what part of the input is provided for this run ... so in this case print the datum ... the list of input files.\n. stale. \nAnd here's a screenshot\n. Hi there!\nThis is actually something we're actively looking into. I'll provide an update here once we have a PR that addresses this feature.\nThis will be configured in a pachyderm pipeline manifest, but will rely on the underlying k8s features that enable this. K8s just added support for this in 1.6.0, which is still in beta.\n. Nvidia GPUs + Tensorflow support is in alpha ... you can read more about it here:\nhttp://docs.pachyderm.io/en/latest/cookbook/tensorflow_gpu.html?highlight=nvidia\nNote ... it's only been tested on a deployment using kops on AWS.. \nLGTM. I'm not sure when two CreateJobs would be called for the same job. Can you give an example of a scenario where this might happen?\nI want to understand the race, but to do that, I think I need to have that context.\nBased on the code you updated, I'm guessing that the race is in the jobManager wait loop ... and if its called twice for the same job, one manager could see two creation changes and think it needs two chunks. Instead you determine the # of chunks not from the change feed, making it not racy.\nStill, I want to understand the situation where the job is created twice.\n. Cool. LGTM. This may have been because of the fuse mount. We migrated away from using FUSE within workers.. stale. stale. This is a dupe of another issue I just closed. A handful of the list operations are streaming/paginated now. If theres a specific request to make another one streaming/paginated we can add that as a new issue.. That makes sense. But for now, our low grade solution (simple semaphore fixed size per container) seems sufficient.\n. Cloudfront is in alpha support. I think we want to get that to the 'beta' level for the 1.5 release. This was fixed in: https://github.com/pachyderm/pachyderm/pull/1903. stale. stale. stale. If its the 'rst' vs 'md' thing, this will be the 3rd time that it's accidentally been done incorrectly / not done manually. In which case, I think its time to automate that step.. The montage addition was submitted in https://github.com/pachyderm/pachyderm/pull/1850\nSo I think this can be closed? @JoeyZwicker ?. Yea, I have something similar, where it kills the old port forward command, issues a new one, and then issues whatever api call you're making. \nIdeally we bake this into the go client though.. This is being addressed as part of the 1.4 upgrade here:\nhttps://github.com/pachyderm/pachyderm/pull/1460\nBasically, after discussion, we're going to make the manually provisioned volume the default.\nThis should guarantee that you can completely wipe your pachyderm and k8s cluster, spin them back up, and your data can be re-used (from the old volume).\nWhen we deploy etcd as a stateful set, right now you can only dynamically provision (w a 'delete' retention polict), so this guarantee is not in place. \nWe will update the docs to make this clear.\nAlso - this will probably prioritize the feature we've discussed many times of being able to marshall all the state into and out of the obj store ... so that's all we need to be truly persistent across cluster lifecycles.\n. Silly me. Didn't export the variable. stale. stale. stale. @jonandernovella -- if you look at the pachd logs (e.g. kubectl logs pachd-erg45) and look for putfile logs ... you'll see a line per file that is getting put (if its coming from a separate obj store). That might be handy. If that doesn't suit your need, please open a new issue to track put-file progress\nMeanwhile, 1.3 gets no new features, so I'm closing this issue.. For brew -- this is in place per Major.Minor release. So you can do brew install pachctl13 or brew install pachctl14\nFor deb packages, the version is fully resolved in the URL. Going forward, we will be able to provide this easily to stable major.minor releases.\nToday, you can accomplish this for any release via the release artifacts on github. It's not the prettiest thing, but it's doable. To date, I haven't had a request to go back more than a single version, so I think our current release mechanisms are 'good enough'.\nAlso, we're seeing great adoption of the latest release, so right now this is not an issue. In the future it might be, but if we have users on legacy versions, we have the build artifacts so we can provide some install mechanism as needed.. Is this something that we need for 1.5? Having the release boundary might be a good time to test out the behavior we want longer term. @gabrielgrant . closing, as this is stale\nin practice, having user's upgrade their client is usually fine. We have glob-file which lists files, so you could do something like:\npachctl glob-file repo master /*/*.json | while read file; do mkdir -p $(dirname $file); pachctl get-file repo master $file > $file; done\nWhich would create the file structure locally, but wouldn't provide the output concatenated. It sounds like there's a specific use case you have in mind there. In that case you could do:\npachctl glob-file repo master /*/*.json | while read file; do pachctl get-file repo master $file >> dump; done\nThis seems to enable what users might want (though it requires some bash-fu). Do you think these cases should be first class pachctl functions @jdoliner ?\n. I believe this is being implemented as part of 1.7 / commit invariance\n@msteffen @jdoliner - can you add this to the right PR / issue to make sure we close this once the feature lands?. Recently w customer installations we've seen more usage of the go CLI for scripting. It's important for these commands to use meaningful exit codes.. We also had a user get very confused about our use of verb-noun syntax in our subcommands as opposed to verb noun that kubectl uses. It is confusing to new users (usually of both k8s and pachyderm) to learn which one is which.\ne.g. pachctl list-repo\nvs kubectl get pods\n.  Different user posted related issue about that here: https://github.com/pachyderm/pachyderm/issues/1534. low priority + stale = closed. Also paired w the sibling PR on the homebrew repo:\nhttps://github.com/pachyderm/homebrew-tap/pull/3. Going to merge since that vastly simplifies testing this. Other suites that we're not running are:\n\npretest (linter, go vet)\ntest-client (coverage for users of our API)\ntest-local (which does a go list under /src/server omitting fuse and vendoring, but getting all other packages) ... so we might be missing some there. stale. I think this is done?\n\nIt's also stale, so I'm closing.. This has been discussed a few times. I believe our standard answer these days is to recommend convention --- use tagged images, so that when you inspect a job, you can see what version of your code was used.\nI think this ask is to record this information more explicitly somewhere in pachyderm. While I think that's interesting (and probably useful to users), we haven't seen requests for this. So I'm marking it as stale.. May be related, may be separate implementation ...\n@gabrielgrant noted that debugging some stats/datum stuff was hard bcz we see so many expected FNF errors from inspect files under the hood. Maybe there is a way to denote these 'expected' errors have a different log level?. stale. stale. stale. stale. I believe this will be addressed in the 1.7 commit invariance PR, right @msteffen ?\n. stale. Not merging into master, and one of these passed, so merging into pfs-v2 branch. stale. stale. Also -- I need to make the check that validates your commit is on origin better. If you haven't pushed to origin, then the goxc bug will cause the new tag to be pointed at whatever is HEAD of master on origin, not the commit you have locally.. See the comment on #1515 about wrong download URLs. This needs updating too. To address the needs of the UI deploy, we'll need to increment the defaultDashImage value in src/server/pkg/deploy/cmds/cmds.go whenever we cut a release.. Closing as this is mostly stale.\nThe goxc commit checking is still an issue, but lately our release documentation has been smart enough that it hasnt been an issue.. Links on this page are broken:\nhttp://pachyderm.readthedocs.io/en/stable/reference/pachyderm_pipeline_system.html\nLooks like we need to add '../' instead of './' to those links. The 'Deploying Pachyderm' section in the side bar has the 'Deploying Pachyderm' as a prefix to all the links.. This page contains comments in the shell sections, but they're rendered as titles:\nhttp://docs.pachyderm.io/en/stable/pachctl/pachctl_flush-commit.html#return-commits-caused-by-foo-xxx-and-bar-yyy\nThis can also be seen on the page 'above' it:\nhttp://docs.pachyderm.io/en/stable/pachctl/pachctl.html. @gabrielgrant noted that old doc links are borked:\nhttp://docs.pachyderm.io/en/v1.3.18/deployment/amazon_web_services.html\n404s\nI'll look into this / it seems like a RTDs issue. Oh, looks like the RTDs finally realized it needed to rebuild 1.3.18 (I had opened an issue about this) ... so its currently building RN. That should resolve itself.. Beg tutorial has empty boxes: http://pachyderm.readthedocs.io/en/stable/getting_started/beginner_tutorial.html. @gabrielgrant's point means I need to update the release script to update those docs as well. I've updated #1514 to include that todo. stale. There is a confirmation for delete-all but not the other two.\nThis is stale, so I'm closing.. LGTM\nI see you've based this off the 1.3 release branch - that'll be helpful to cut a new 1.3 as needed for this.\nIs 1.4 prone to the same issue? I'm not sure if it uses the same sync/upload code path.. We now use deployments. stale. We have the DefaultMaxConcurrentStreams value defined in /src/client/client.go to be 100. It's a bit odd that this is different. We have been seeing some throttling issues from AWS as well. Maybe we'll need to start setting limits per cloud provider. Just thinking aloud. Nothing to change here really.\nLGTM. I believe there is a way to filter the results. @derekchiang can weigh in. fixed. Dupe of #1547 afaict. Implementation method described here: https://github.com/pachyderm/pachyderm/issues/1549. agreed. stale. I closed it because it was born of the same discussion that generated #1545 (which I linked to this issue).. Totally - we want to avoid that as much as possible.\nI think s3 has an rsync like feature (which only uploads new things). I'm hoping each client has something similar.. Yea you're right.\nWe agreed the easy tactical thing to do here is either:\nA) upload under a prefix (e.g. the commit ID)\nAdvantage: super simple to implement, and gets us 'correctness' for what a user wants\nDisadvantage: a user needs to lookup the latest commit (which doesn't really suit the use case ... this is intended for users who are not techincal and 'just want to see a lsit of files' and may not even have access to the p7m cluster)\nB) delete the contents, then upload\nAdvantage: gets us correctness + ease of use for all users\nDisadvantage: deleting obj from a bucket can be really time consuming (as I'm finding out when trying to do this cleanup for our AWS buckets)\n\nI think we'll go w B for now. It might be slower ... but it gets us closer to the finish line we want ... and is probably step1 in rolling our own rsync like feature.. stale. GPU issue is #1401 . And to make it clear what the consequences of not having this ... a user reported that without the selector, k8s scheduled too many worker pods, and other apps got evicted. Not good.. This is a duplicate of #1547 . #1547 came out of the same discussion in #users \nI agree ... GPU implementation could be related, but is likely to not be since its getting first class support.. This is helpful / necessary for some common debugging workflows.\nIn particular, we need a way to know which worker pod processed which datum. This is critical when debugging scaling issues (or other breaking builds). I need to know which files (by way of a datum) was processed by which pod, so I can look at the resource usage for that pod and see what happened. \n. You're right ... it got pulled out. It was on master before 1.4 landed. Unclear if that was from a funny merge or intentional. . This is related https://github.com/pachyderm/pachyderm/issues/1113. I need to revisit this to see if it's still valid.. Thanks to @msteffen 's deployment upgrades, the tests now run, but there is a panic that is a bit odd. We're closer to getting the nightly test suite in place, so I'll leave that issue as the one 'required' for 1.5. We had AWS integration tests running for every PR, but we disabled it. So we have the ability to do this, but at this time are choosing not to.. Yea, for a customer POC we saw egress fail. Unfortunately I don't have more info than that.. stale. stale. stale. I think this is a duplicate of #922 \nThough doing what's described here may be a simpler initial tool than using an off the shelf tracing library. TBD. . stale. Duplicate of https://github.com/pachyderm/pachyderm/issues/1486\nClosing this one since the other has a bit more detail. stale. We've also seen individual jobs emit 1TB of net new data. (Another data point for testing considerations).. stale. we have support for ^ and ~\nClosing this. If there are other requests, they can be filed as new issues.. stale. stale. stale. stale\nIn general, I would like to have better support, or clearer conventions about k8s API version support. Generally speaking, we support the latest release, and the penultimate release. . Generally, we moved away from using k8s primitives to define pipelines because of the overhead it incurred. Spinning up a pod to process a datum was too slow. So we switched to a worker model.\nThat said, we'd love to use the k8s objects more directly. We'd love to have a pachyderm pipeline really just be a k8s manifest definition. (We were just discussing this again yesterday in the context of re-using other k8s components).\nThat said, this is something we have an eye on, but we don't have a tactical plan to get there in the near term. So I'm closing this issue. . @msteffen 's cluster management script that we're using for testing is a big help here ... but only if you use that script to deploy clusters. It means that we'll be in pretty good shape for any clusters that CI spins up ...  but any clusters that we spin up without that script can get leaked.. I still want this, but its stale. Actually, this is still a WIP.\nI'm testing / validating it works w the dash images that we've pushed. And we probably want to include the pachctl --dash-image flag as part of this PR as well. I did not include any of the new features (dash images flag, or portforwarding), but just fixed a bug in the grpc image name and merged in / resolved master. Could probably use another review.. stale. stale. stale. Because of the recent changes to the ~/.pachyderm/config file ... I think we'd need that code updated (as well as any code that checks for env vars?)\nSee this comment: https://github.com/pachyderm/pachyderm/pull/2005/files#diff-396f93197d0a15fc1d7c95feedc343f5R116. stale. stale. stale. stale. Just as a data point -- the job that I saw was run, marked successful, and so we couldn't run it.\nThis was a relic of the user code not 'erring out' when it ran out of disk. In this case gzip did emit an 'out of disk' message (but not an error?). In the general case we'll need users to write code that errs (and exits!) when they run out of disk. It's probably not clear to a user how to 'test for that' with their code ... and their only recourse is to 'exit on all errors'. That works for bash (w the magic incantation) and golang (bcz of the err model), but might not be suitable for other languages (R, python, etc). At the very least, if we required that, we should make that reallllly clear in our docs.\nAs far as the difference from #1654, I want to make sure we address the broader class of errors that this represents. \nI'd define that class as 'stateful things that corrupt job output that arent captured in the hash'. In other words, things that arent the transform or input data changing that can also effect the user's output.\nSome examples include: out of disk errors, OOM errors, transient network issues.\nIn each of these examples, ideally the user's code errs and exits properly. But we're making a development platform. We can't assume that's the case the first time they run the code. Ideally their workflow is something like:\n1) try to run the job, it runs out of disk\n2) ideally ... pachctl logs report it ran out of disk ... and that their job should fail if this happens ... and that they need to update their code and redeploy\n3) they fix their user code and redeploy\nStep #2 is the hard part ... we may be able to define and report specific instances. But whitelisting each way it can fail seems hard. The point I'm making is that there are cases when a user may need to 'force' the job to re-run, and we could include a flag for that.\n. stale. Oh weird, I definitely clicked it to base off the ui_deploy branch ... will remake the PR.. Huh ... git is very confused about this. Its still messy when I try to base off ui_deploy, so I'll wait for that to land before I submit this PR. @JoeyZwicker is going to run w this. Also I accidentally based this not off master ... instead please use the production_docs branch\n. stale. stale. stale. stale. stale. stale. Yup. And I want us to have good tooling around that if we can provide it. If your datum > disk size (or disk remaining) if we could output a message to that effect that would help a lot.. I think a modified version of put-file is what you want.\nIf you do pachctl put-file -h you'll see some examples of advanced cases. I believe the one you want is:\n```\nPut the data from a URL as repo/branch/path:\npachctl put-file repo branch path -f http://host/path\n```. stale. I believe this is covered w update-pipeline / reprocess flag / from-commit flag. stale. the release process now handles this. I believe I asked this already, but as a sanity check. If you mount the input repo, what is the total number of files from the input repo that you're marking as lazy?\nYou could check by doing find ./* | wc -l for instance. stale. stale. stale. stale. Yup, added it back.. We kept the pachctl command without changing the name, so the functionality is there.. stale. Hmmm ... another segment data point looks like it reports it ...\nclient.Track(&analytics.Track{\n  UserId: \"c4dbbe76fe0844ac8b88c3e1205a9ebb\",\n  Event: \"user.usage\",\n  Properties: map[string]interface{}{\n    \"ClusterID\": \"8cab5ebdc47a402bba58a5b491829c05\",\n    \"ListCommitStarted\": 1,\n  },\n})\nbut in mixpanel i dont see it ... still investigating. Actually it seems like we are reporting it ... I can generate reports on it. However, many times we are not reporting it. And we can see 90% of the user metrics don't have it set:\n\nAnd this has been happening for a while ...\n\nSo it doesn't correspond directly to the 1.4 release (though testing / usage may account for some of this data)\n. stale. stale. stale. As of now ... repo sizes are pretty accurate again, but we don't have a good way to report # of files (or # of datums ... though that's kind of a vague measurement).\nBut we should be able to report job/pipeline/bytes/commits. stale. stale. stale. stale. stale. stale. I think this is clearer now that we have logs per datum. True - that works for low volume log cases.\nIt's not easy to do that when there are many many log messages ... because of the logs the user code generates there are huge logs. The kibana UI does provide some methods to refine by time windows, but it would be a whole lot easier if I could just apply a filter to get all the logs from a single run.. Now we have logs per datum on the UI. stale. stale. Looks like the PR build fails because the path to etc/build/travis.sh moved ... I'll take a look. stale. Actually, I'll save the docs updates and the updates to aws.sh until I have the security stuff working.. stale. stale. stale. stale. This is something we've added w stats support. You can browse the state of the input/output directories on the UI, or there is a way to do this via pachctl as well. This will officially land in 1.6, and I believe stats/UI are going to be paywalled.\nIf you're interested in trying this out, you can sign up on the UI for a free trial. Here are some instructions on how to deploy the dash (e.g. use --dashboard-only if you have an existing pach cluster). stale. stale. stale. stale. stale. stale. stale. stale. I'm realizing I'd like to write a test for this.\nI think if I make a job that sleeps for 5 seconds we should see the issue.\n. No that won't work ... because the client that is trying to KeepConnected is between the worker and the sidecar pachd/pfs\nSo it needs to be during the file download phase.\nWe run the integration tests on travis CI using the filesystem as the obj store ... so we'd need to write a datum that takes more than 5s to read from disk. Googling HD read speeds ... thats ~100MB/s ... so we'd need to write a file > 500MB, prob about 1G to be sure. Not sure this is the best use of an integration test, but could be a good load test?\nWhat do you think @msteffen ?\n. Discussing w Matt I think we'll have good coverage for this w the load test harness.. stale. stale. stale. stale. Would love to do this, but it's not been a priority\nstale. stale (and /ironic). stale. We've done a few revs of the dashboard, and haven't been able to reproduce. Please try upgrading and let us know if you're still having the error.. stale. stale. Addressed in https://github.com/pachyderm/pachyderm/pull/1970. stale. stale. stale. This is done. We now have datum state (success/fail/skipped) as part of the stats UI upgrade. Woah, this is great ... this gives us job stats. That's big.\nThe proposed solution would be great at answering a bunch of questions around job stats:\n\nhistogram of datum size (e.g. cases of extremely heterogeneous datum sizes ... this came up for a customer w 400 datums where the last 2 accounted for 50% of the processing time)\nhistogram of job run times over commits\nI think this would give us an easy way to record 'data written per commit' ... but I don't think quite gives us repo size? (maybe there's something clever we can do here to make that calculation easy too)\nthe ability to list datums per job (which we don't have a first class API for)\nthe ability to record where a datum was run (for debugging purposes)\nand perhaps most importantly ... a pretty easy way to add more stats to be measured/collected\n\nWhich are things we don't have today but I've felt the need for.\nBroadly, this bites a huge chunk out of what I'm looking for w the stats service. It gives us job stats. And this would help us dogfood our own tech, which is great.\nThe two things I see missing / we'd need to address are:\n1) App stats for pachd\nI want us to be able to answer questions like:\n\nHow much of each API call are we seeing? (e.g. ListFile, GetFile, PutFile ...)\nWhere is the bottleneck in the DAG / where are the API calls slowing down? (we could see where in the DAG this was slowing down by looking at a graph of GetFile / PutFile frequencies or times to start)\nHow does pachd handle load? sidecars vs master? Where is the scaling bottleneck for pachd? When do we recommend scaling up pachd nodes? \n\nAnd to answer those questions I still think we need to record stats for all the pachd API calls. Again, I haven't looked too much into this, there's a chance we can use what we already have reported in the logs. But I want to be able to search / filter / graph the stats data.\n2) The rest of the 'stats service' stack\nUsing grafana / elastic search / statsd (or a similar stack)  gives us quite a lot of things we need. The ability to store (and rotate) data, search, filter, and graph the data. And present the data in a way that we can use to display stuff in our own UI.\nWhat we're talking about here is basically replacing some of the stats collection (statsd or fluentd) and the indexing/hosting of the data (elastic search). I think we get a lot for free w pfs, but it won't be parity with those services\nThat's not necessarily a bad thing. We'll start with what works. But I just want to make sure we don't try to build all the features of those other things. Because that's probably not our core intention for our tech. And for the things that it would align with (e.g. data volume, data ingest rates) dogfooding it will only make our tech better. Awesome.\nI do think we'd need some graphing utils. I'm sure some of this we'll consume and present in our own UI. But I don't want to reinvent the wheel for UI tools that already exist. So to me the question is how to integrate this with something like grafana. It could be that we have an 'ETL' pipeline running on this repo that massages it into the JSON format that grafana expects (and elastic search serves).\nThis all feels like we may want to start thinking about namespacing repos/pipelines. This all feels like it would go under the 'pach-system' namespace and user's wouldn't see by default. (And prob shouldn't have write access to directly). Adding 'origin access identities' to this list. A single account can have 100 ... so we won't hit the limit right away ... but we will need to GC these.. This is really for our own consumption. Managing this state in pachyderm doesn't make any sense. As such, marking as stale. Tested the fruit stand example ... seemed to work fine for the filter step ... the sum step reported an error ... but seems like its related to the transform ... seems like a bug w fruit stand?\n\"transform\": {\n    \"cmd\": [ \"sh\" ],\n    \"stdin\": [\n        \"for fruit in apple orange banana; do\",\n        \"   [[ -e /pfs/filter/$fruit ]] || continue\",\n        \"   cat /pfs/filter/$fruit | awk '{s+=$1} END {print s}' >/pfs/out/$fruit\",\n        \"done\"\n    ]\n  },\nthe error is:\n```\n17-06-04[18:16:11]:pachyderm:0$pc get-logs --job b9bd2040-bdda-40bc-9297-82ace8372775\nsh: 2: [[: not found\nsh: 2: [[: not found\nsh: 2: [[: not found\nsh: 2: [[: not found\nsh: 2: [[: not found\nsh: 2: [[: not found\nsh: 2: [[: not found\nsh: 2: [[: not found\nsh: 2: [[: not found\n``. Oh and I should mention that I tested./etc/testing/deploy/aws.shwith and without the--use-cloudfront` flag and it worked fine.\nOnce cloudfront becomes the default (in the next PR) this deployment code path will be hooked into the nightly test suite.. The checks actually succeeded. I'm guessing this is a relic of the GH outage this afternoon.\nMerging now.. stale. Huh. Logs suggest that the putfile completed (~5000s or ~1hr 20m) but I don't see a log line for the finish commit ... actually I don't see a starting log line for finish commit. Which way did you run this ... w putfile -c or a separate finish-commit call?. We saw something similar a week ago with the Process() api call (before the PPS refactor to use a master model).\nBasically the Process() call completed on the worker, but the debug line we put right after the Process() call on pps never printed. Somehow the grpc cxn was lost. In each case ... the deferred log printed suggesting that the server (pachd or worker in each case) thinks it successfully wrote the response.\nI suspect an underlying grpc issue here. Somehow on long running requests we see a server send a response, but the client never receive it.. This is an awesome list.\nAs we expect users to use more of the UI (and have less reliance on k8s), this list is how we'll make that happen.. stale. stale. stale. Would love to do this optimization!\nBut it hasn't been a priority. Marking as stale. Tested this on master ... seems to be fine now.. stale. One thing that is a bit inconsistent is sometimes I use template files and sed to do the replacement, then later on I figured out how to do it w jq\nWith sed I can do it in place (and multiple substitutions at once), whereas with jq I have to make a copy of the file each time. But I need jq to do some of the fancier things when updating bucket policy/distributions.\nSo ... I'd rather do it one way than two ... so I'll update the scripts to only use jq for this purpose.. ok @dwhitena please take another look. This is really old, and I don't think we've seen this in a while. Closing.. Oh I also want to add a few items to debug disk pressure issues. Most of them are now mitigated since we use emptyDir but there is still at least one failure pattern.. The recipes make sense to go under best practices. What about the troubleshooting guide? Where should that go?. OK. So you're good to merge this as is @dwhitena ?\nI do think the stuff I added under 'troubleshooting' should live in a separate doc. I tried hard to keep it in the 'symptom' and 'recourse' format. And there are things in common between some of those examples.. Fixes #1870 . I think the 1.5 changelog should be all the changes since 1.4\nI do think there is higher fidelity in what is reported under the /releases path, and I'd be ok w that level of detail. Here's an example of another project that solely uses the /releases feature to report changes\nI like that they have the PRs / issues linked right in there. That makes it easy for me to evaluate if a newer release contains a relevant feature/bugfix I'm looking for.. Ah, on second thought I see what you're getting at Derek. If we only report changes on /releases ... then I guess it's fair to have the 1.5.0 CL only be since 1.4.8\nThough in that case ... we miss the 1.4->1.5 change summary.\nWhich ... makes me think I've re-derived the need for the changelog :-P. Once this PR lands (https://github.com/pachyderm/pachyderm/pull/1944)\nYou can merge PRs like this that only bump the version once CI passes.. stale. We have a limit on the # of files we can support using lazy mode (~25k) which corresponds to the OS thread limit I think. FUSE would allow us to support a greater number of files. However, FUSE performance isn't great, so there would likely be a tradeoff.\nMarking this as stale, as we haven't had requests for this.. And to be clear - I don't mean add a log statement after every line in jobManager ... but after every action (e.g. making a call to inspectCommit, or the call to BuildCommit) would be helpful. stale. stale. stale. In the past we've cut custom images, which usually satisfies a customer's needs. However, we've run into this a few times recently -- the customer needs an RC and/or a custom install. Yes, they could download the binaries from GH, but mac users prefer using homebrew.\nSeems like we can support a homebrew formula at 0.0 and include the sha in the additional version slot. Which would give us the ability to host custom releases in a way that doesn't occlude the latest release.. stale. Fixes #1950 . Ok ... thanks for the feedback, this looks way better now. Basically I took the helpers under Process() and standardized how they report logs. I also log the start/finish of egress.\nI think the only contentious bit might be the logs around the outside of the Process() call on the worker master. I'm seeing an intermittent issue exactly between the rpc boundary here ... so it would be helpful for me to have these. If there is a bug here (w day long running RPCs) it's intermittent, so it would be helpful to have this log. And also, if we need to address a bug here, this would require a big model change (no more long running / blocking Process() grpc call). So these logs are a canary to see if we do need to make this change.\nBut, if we want that done separately, I can pull that out too.. I think I will pull those logs out as well. The customer who was having the issue got their jobs working w master (which changed the jobmanager to the master model). We don't have a theory why exactly this change had an effect, but it seems to work.. Reading the code ... the sidecars (as well as any pachd replicas) will report in addition to the pachd master. While I don't think this will totally mess up our stats (since they're just reporting the current # of repos/pipelines/etc ... not net new in the last 5m), it would be good to remove the duplicate calls to lower our mixpanel data API usage.\n. stale. Fixed in https://github.com/pachyderm/pachyderm/pull/1989. We were seeing some rate limiting on GCS ... ~42 files for a total of 640GB ... and we'd get to about 39/42 files before failing.\nOn GCS we usually see ~1GB/m speeds.\nNow with this change it works, and takes ~1:10m. stale. So thinking about what I'd like to see as a user, I'd find the following stats / debug info useful. Most of it is pretty basic. Most of the 'logic' is just 'rolling it up' at the right levels.\nSome of this is clearly stuff that would be integrated / presented at the UI layer. But I'm leaving this list as-is so that the bigger picture is more clear.\nPer Datum:\nRun Info:\n\njob ID\nworker / pod ID\nresources requested\nimage\n\nStats:\n\ninput/output data size\ntotaled (to understand disk usage)\n\n\ntotal time (not just sum of the next three ... but truly total ... this is a great way to catch bugs / understand any overhead)\ndownload time\ncode run time\nupload time\negress time\n\nPer Job:\nRun Info:\n\nnumber of datums\nnumber of workers (in actuality, not just what parallelism asks for)\n\nStats:\n\ntotals / avgs / bar chart --- across DATUMs\ntotal time\ndownload time\ncode run time\nupload time\negress time\ninput/output data size\n\n\ntotal time per job (incl restarts)\n\nPer Pipeline\nRun Info:\n\nstatus\nnumber of jobs run\n\nStats:\n\ntotals / avgs / bar chart -- across JOBs\ntotal time\ncode run time\n...\n\n\n\nDAG\nStats\nhighlighting:\n\nhighlight data output sizes on DAG\nhighlight data proc times on DAG\n\ntotals:\n\ntotal size of all repos (to understand obj store usage)\ntotal resource capacity (mem/cpu/gpu) -- k8s reports this per node\ntotal resources allocated -- we can construct this\ntotal mem usage (workers + RRs)\ntotal cpu usage (workers + RRs)\ntotal GPU usage (workers + RRs)\n\n\n\n(note: not real time usage ... just what is allocated x number of workers)\n. This is still a bug, we should report this better. Discussed this a bit.\nThe change is that it'll set the ID on the config if the config is present but the ID isn't set. In the old paradigm only if the config wasn't present would the id be set.\nBut for aws.sh the Id in config should be there either way ... since pachctl deploy will try and read the config, and if it's not present, set it.. stale. Duplicate of #1935 . JD and I talked through the interface we have to do operations like 'list-datum' and 'inspect-datum'\nBecause the implementation is a bit tricky (and may be subject to change if the stats file structure changes), we'll make this a first class API.\nThis will look something like:\n1) List Datum\npachctl list-datum <pipelineName> <jobID>\nWhich should return a list of datums + state (success/fail/skipped) + total time\n2) Inspect Datum\npachctl inspect-datum <pipelineName> <datumID>\nWhich will return:\n\nthe detailed timing stats\nthe list of files in the input?\ncommit / filepaths to logs / pfs debug info?\n\n. Iteration 0 is satisfied. Each of the four user stories above is completed.\nYou can see a list of datums sorted so failed ones are first, and after sorting by state, they're sorted by time (so slowest successful ones are first).\nDrilling into a specific datum, you can see the timing stats, the logs for that datum, and a link to browse the files (input/output) for that datum.\n\n\nWe're in the process of defining Iteraion 1 now ... the goal of which will be a thing we can ship w 1.6. This iteration is completd. The old logs look like:\n2017-06-12T21:59:58Z INFO  protorpclog.Call {\"service\":\"pps.API\",\"method\":\"InspectJob\",\"request\":\"job:\\u003cid:\\\"aa39e035-8fa0-4d87-8de8-99651c40958a\\\" \\u003e \",\"duration\":\"0.000s\"}\nI tried to make the new log format similar to the old, the new log lines now look like:\n2017-07-13T21:19:52Z INFO pfs.API.ListRepo {\"duration\":0,\"error\":null,\"request\":{},\"response\":null} \n2017-07-13T21:19:52Z INFO pfs.API.ListRepo {\"duration\":0.00038794,\"error\":null,\"request\":{},\"response\":{\"repo_info\":[{\"repo\":{\"name\":\"ttt\"},\"created\":{\"seconds\":1499906398,\"nanos\":434489310},\"size_bytes\":12826738}]}}\nBut have less redundant info (that protorpclog.Call bit), put duration / error fields towards the beginning which I think makes scanning visually a bit easier), and don't double escape quotes ... so the fields are proper JSON, and so a bit easier to read.. One other thing to note ...\nThis commit shows the manual modification of vendored code I had to do.\nAs the commit message suggests, I think this is OK in this instance. There is a known issue w the upper/lower case versions of the log library here, and we can tell what deps we've modified manually by doing govendor status. We're starting to see more and more cluster maintenance type of requests like this. This seems like a very reasonable use case we need to support.. stale. Interesting suggestion. We haven't seen a lot of interest in this, so I'm marking it as stale. But if we see more interest, this could be something we could support. We have to make sure we do this streaming, but looks like the underlying libraries support this:\nhttps://bitbucket.org/kardianos/staticserv/src/5a536ebb8016d795187138ad99881533e14a59ef/main.go?at=default#main.go-232\n. This has not been implemented. I'm marking it as stale. stale. stale. Example pachctl output\n17-07-27[13:05:29]:pachyderm:0$pc list-datum 486cbb55-3ce0-4940-b0ca-b08bb54fa2ca\nID                                                                 STATUS              TIME                \neec3814a51f0a14e00b3744e568f5d0cfa06c165907831a1183fe017d3c6d0cc   success    Less than a second\n17-07-27[13:05:48]:pachyderm:1$pc inspect-datum 486cbb55-3ce0-4940-b0ca-b08bb54fa2ca eec3814a51f0a14e00b3744e568f5d0cfa06c165907831a1183fe017d3c6d0cc\nID                eec3814a51f0a14e00b3744e568f5d0cfa06c165907831a1183fe017d3c6d0cc\nState             SUCCESS\nData Downloaded   1.696 KiB\nData Uploaded     400 B\nTotal Time        53.346428ms\nDownload Time     44.673543ms\nProcess Time      3.033248ms\nUpload Time       5.639637ms\nPFS State:\n  REPO     COMMIT                             PATH                                                                                                         \n  filter   0b397f42164c4f8b9ba955f58f4cd0c0   /486cbb55-3ce0-4940-b0ca-b08bb54fa2ca/eec3814a51f0a14e00b3744e568f5d0cfa06c165907831a1183fe017d3c6d0cc/pfs. Fixed in https://github.com/pachyderm/pachyderm/pull/2117. Yea, I think having the message value be proper JSON would be the easiest option for now.\nGoing forward, we will see more ppl use custom logging stacks, and we'll probably add the pipeline flag you describe. The reason why I don't want to do that first is that we'd have to consider the consequences for the dashboard's log consumption. The interaction of those components is still getting fleshed out.. Additionally ... it seems like it often errors bcz there is no output for 10m when kops tries to delete the cluster ...\nsecurity-group:sg-004df067  ok\nroute-table:rtb-224e3146    ok\nvpc:vpc-8c5018e8    ok\ndhcp-options:dopt-19ce077d  ok\nW0801 22:39:17.608481   28165 delete_cluster.go:209] error removing kube config: error loading kubeconfig: Error loading config file \"/home/travis/.kube/config\": open /home/travis/.kube/config: permission denied\nDeleted cluster: \"c0a1e096-pachydermcluster.kubernetes.com\"\n++cat .bucket\n+aws s3 rb --region sa-east-1 --force s3://24421-pachyderm-store\nNo output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.\nCheck the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#Build-times-out-because-no-output-was-received\nThe build has been terminated\nIf we put this in the after_script travis section, I think we can use the travis_wait trick so that it'll run our cleanup script and not err this way.. We've disabled CI benchmarking for now, so this is moot. You're talking about build # 8291? sometimes we hit the log travis bug, but if you refresh the page you should see it. fixes https://github.com/pachyderm/pachyderm/issues/2121. stale. stale. A few updates ...\nThe long running stats PR is landing now, which addresses the 'code cleanup' todo for this iteraion.\nThe backend now supports pagination / is fast enough to be consumed by the UI #2151 \nWhat remains is the UI update to consume the datums in a paginated way.. The paginated UI update landed, and works well. (Infinite scroll)\n. Additionally profiling the mem used on the UI side:\n100 datums: + 28MB\n500 datums: + 55MB\n5k datums: + 590MB\nSo I have some concerns about a streaming solution. Such a solution would provide the UI the data sooner ... but the latency from the pachd call isn't really the issue ... most of the time spent here is in JS land (parsing / prepping). So while a streaming call would provide the data sooner, we'd have to make it so that the JS caches the data that was not displayed. If we're designing this to work for on the order of 10k datums, that means we'll have ~1GB of mem used by the UI. That seems like too much.\n\nSo I see two other options.\nA) Paginate\nThis would require making the ListFile api call truly paginate. Basically you'd provide an offset, and it would return a batch of files w that offset from the total count.\nThis would require some tricky stuff on the backend. Perhaps the 'heavy write' that @msteffen discussed yesterday (when writing a hashtree ... also write sub-hashtrees of standard chunk sizes so that we can load those directly from obj store ... and perhaps we only do this for stats hashtrees not normal files).\nBut this seems tricky.\nB) Make list-datum fast enough it just doesn't matter\nRight now the longest part of list-datum is the part where we diff the files / gather each datum. This accounts for 90% of the time spent in list-datum. If we could optimize this to be an order of magnitude or two faster, then we could do dumb pagination (generate all the results ... only return the slice that matters).\nThis has the advantage that we want to fix this codepath anyway. @jdoliner pointed out a correctness bug with this part of the code (where we incorrectly mark a datum as new when it is in fact skipped). The solution would be instead of doing diff-file using an inspect-file to determine datum status. If the hashtree for this commit is in the cache (which it would be after checking the first datum), we would avoid any trips to fetch objects from obj storage. (I think diff-file gets 2 hashtrees (one for parent commit, one for current commit) and doesn't fetch any objs from obj storage ... so this may not be a huge perf win).\nBut, since we want to make this change anyway, I think it makes sense to start there first, and also see if we can squeeze any performance juice out there as well.\n. Offline discussion\n\n1) We're going to proceed first w the correctness fix\nThis will write a skipped file to the stats tree if the datum was skipped. This will be done on the worker master\n2) We can optimize the code in list-datum \nAfter we read all the files (to get all the datums), we can already get the state (bcz each fileinfo is a dir w a children field ... we can already test for the existence of the failed/skipped files) and sort before doing the expensive operation of getting each datum\n. This is fixed in #2171 . I think we have this handled since @msteffen's refactor. Now make doc and make point-release dont use commit hashes / trailing release suffixes (e.g. RC1), and we have analogous make doc-custom and make release-custom tasks for truly one off releases.. I think I've addressed all the feedback, please take another look @jdoliner . stale. Dupe of https://github.com/pachyderm/pachyderm/issues/2673. This was when hitting the 'waiting for nodes to come online' issue that we've seen w k8s 1.7.0 (but was resolved w an update to the aws.sh script afaict).\nFWIW, I have kubectl installed via gcloud and validated that kops 1.7.0 and kubectl 1.7.2 work ok for me and the aws.sh works fine.. Yes, an ubuntu image should work for now.\nThe probably 'todo' from this issue will be to create a make task so that its easy to roll your own. You're welcome to to that of course, but we'd want to make it easier by putting that into a task for you.\nI am curious -- it seems like you're trying to use pachctl within an alpine container. Can I ask why?. stale. It should be easier to add the paths and add them onto LD_LIBRARY_PATH colon delimited ... e.g.\nLD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/rootfs/usr/lib/...\"\nInstead of actually copying the files.. Yea I remember you mentioning that when we were debugging this.\nI believe you, but I don't see why that should be true. If we're going to recommend a change, first we should try and reproduce and understand why that is. We went to a considerable amount of effort to make this work in a way so that users don't have to copy drivers over. \nIf that is required in this case for some reason, I'd like to understand why.\nOur goal here was to make this easy for users. I'd like your use case to be handled just as easily.. I believe our recommended method of using GPUs has changed, so I'm calling this stale and closing the issue.. I'm not sure if this was implemented or not. Do we still need this @gabrielgrant ?. stale. Fixed in https://github.com/pachyderm/pachyderm/pull/2208. Also, a note ... for some reason hitting that URL from my browser doesn't work ... but substituting 127.0.0.1 for localhost works ... I think this is just a DNS issue / difference btw my browser and the shell.. Closing as 1.6 is done. Dan is going to transfer over the checklist for 1.7. stale. stale. I changed my mind about this. I don't think we want to support any shells. Bash is fine. Let's just update the docs to make this clear.. Just the deploy docs. We don't know all the ways it might err in different shells, so its best just to say that we expect bash.. stale. stale. I'm not sure what the second half means. @gabrielgrant do you mean you want to remove the enterprise token?. It's a configuration bug / edge case w k8s. It happens when you run\ndockerized k8s locally and are also trying to use enterprise auth features.\nSo I think our team is probably the only folks in the intersection of those\ngroups. (This shouldn't be an issue for any cloud deployments w ent auth\nlogin).\nI've hit and Matt has hit it. I usually opt for the 8.8.8.8 workaround.\nI'm not sure how many users run dockerized k8s locally. And if they do, I'm\nnot sure how they run it (they might use our make task or not).\nI think we can close this.\nOn Fri, Oct 20, 2017 at 5:37 PM, Joey Zwicker notifications@github.com\nwrote:\n\nWhat's the status here?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/2279#issuecomment-338351462,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAkfwYeNxylSwVD86oMVKD5PgZ9rz97zks5suTzjgaJpZM4PVJFb\n.\n. Leaving this here as a breadcrumb ... I'm now seeing this on k8s 1.7 w minikube running on my local machine w --vm set to none.\n\nThe solution posted here\nBasically ...\nsudo systemctl stop systemd-resolved\nsudo systemctl disable systemd-resolved\nand edited /etc/resolv.conf to contain only:\nnameserver 8.8.8.8\nWorked for me. (I did have to restart minikube).. I don't have a perfect solution yet ... I'm doing it as needed (I don't reboot that often), though you could add it to a startup script somewhere.. This is tech debt. It's importnat.\nIt can be a nasty gotcha if you don't see / adhere to the convention. I\nbelieve I ran into it recently but asked Matt about it and so I figured it\nout. Generally we don't like conventions in the codebase that have bad side\neffects ... as its easy to lose track of those, and shoot ourselves in the\nfoot.\nOn Fri, Oct 20, 2017 at 5:34 PM, Joey Zwicker notifications@github.com\nwrote:\n\nAny updates to this issue? How important is this?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/2295#issuecomment-338351192,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAkfwTYX9JPv_Z__F55lgM1ei2C9w1Kjks5suTwsgaJpZM4PZAc-\n.\n. As we start to have more usage by bigger teams, we'll probably see more requests for namespacing repos / pipelines as well as defining teams that have access to specific groups.\n\nBut for now, we haven't seen explicit requests for this, so I'm marking this as stale. stale. I should note ... this makes the change as requested, but changes the default behavior ... so we may not want to merge until the dash has an analogous PR. Since this will fix CI and I need to get a different PR merged for an RC, I'm gonna click the big green button myself.. Hahaha. Hmmm I'm not super familiar w the aws metadata services. I'm not sure if/how that plays nice with containers. If running any AWS api tools on a 'raw' ec2 instance Does The Right Thing by querying the metadata API, we could probably figure out how to make this work on containers ... but we may need to poke the right things through from the host to the container.\nWe'll have to look into this.. Ah, good to know.\nIn this case, it's really just a matter of specifying a different deployment option for aws. That's pretty easy for us to support.. I just had a user run into the same thing. I believe we were trying to put file from an s3 url to a path /, but I don't quite recall. Closing in favor of \n2388 and  #2371\nWhich are more specific. stale. #2345 was closed ... so I think we're good? But unclear if the fix also checks case sensitivity (e.g. if validating the user exists against GH will also serve to invalidate spellings of usernames w the wrong cases)\nCan you weigh in here @msteffen ?. stale. Tested, this is fixed now. This will be complete once this PR lands. Leaving this open. I closed a dupe of this that was very old.. This has come up many times w customers.. Our bash completion got even better (it populates values too). We should either document installation or auto install it.. I believe the 2nd issue isn't DNS ... its a limit on the VPCs in that region ... you have to delete a few.. Well in this specific case, list-datum already has a pagination api. It's\njust that pachctl doesn't expose this via flags. So we could implement\naccess relatively easily just by stringing the flags through.\nHowever, the UX might be better if you could truly return all the datums\nfrom a single call. If I have many many datums, chances are I'm going to be\ngrepping through them looking for something ... so it would be handy to\nhave them all together and not have to wrestle w pagination.\nOn Fri, Oct 20, 2017 at 5:13 PM, Joey Zwicker notifications@github.com\nwrote:\n\n@msteffen https://github.com/msteffen we should make list-datum,\nlist-commit, and maybe list-repo streaming just like we did for list-file\nin #2418 https://github.com/pachyderm/pachyderm/pull/2418\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/2380#issuecomment-338349250,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAkfwZVg8iy-8w7YZaw36MAR0wJ-92zOks5suTckgaJpZM4PviFC\n.\n. stale. Thanks @brycemcanally , this landed:\n\nhttps://github.com/pachyderm/pachyderm/pull/2625. In fact a paying user does want this.. Ah, I did pachctl deploy local --dashboard-only even though I was connected to an AWS cluster, and it worked fine.\nI did see a warning about pachyderm-storage-secret already exists but that meant it didn't overwrite it, and I don't seem to see any side effects.\nBut yea, if we can omit the storage secret, so much the better.. stale. stale. The cloudfront request is already wrapped in retry logic:\nhttps://github.com/pachyderm/pachyderm/blob/master/src/server/pkg/obj/amazon_client.go#L129\nI'm pretty confident this isn't related to #2438 since the max backoff elapsed time is 15m (so it shouldn't be possible for us to generate a signed URL that expires before the backoff completed).\nI don't think there is a todo here, so I'm closing the issue.\n. Another user reports a similar use case (tolerances + affinity)\nRight, I did that to the resources that `pachctl deploy --dry-run` generates and you can see in the `kubectl get pods` output that all pods but one are on the right node. The misbehaving pod was created with the command `pachctl create-pipeline -f edges.json` that doesn\u2019t have a `--dry-run` option to spit out the resources it would create and allow me to set `affinity` and `tolerations` to achieve what you are suggesting.\nThe default node scope for a google backed k8s cluster is `storage-ro` so I believe this would impact anyone trying to install  Pachyderm on an existing google cluster.. stale. This is awesome! I like that approach a lot.\nOne thing I love is that it doesn't matter if you're using this command against a local cluster or shared cluster. That really bridges cloud/local dev gap elegantly. That's awesome.\nA few other ideas for future iterations / tools:\n1) Explicit state\n\nthere are a lot of other languages / frameworkes (ruby (rvm) / python (virtual env) / etc) that hack your local environment to be a sandbox for development\nusually this state is set by commands (e.g. rvm use gemset foo) or by an RC file specified at your home / user / or project directory\nthe benefit of having this config file / CLI options around is that it makes the state explicitly available\na lot of developers who use the above tools use it to sandbox development\nand it makes it easy to contextualize development and set it down and pick it back up again\n\n\n\nAs a future iteration we could provide something like this for your pachctl test-pipeline ... command. Using a config file you could specify the pipeline name / input branch or commits / etc. Then using pachctl test-pipeline would run the transform w the right env vars, and use the config specified.\n2) Datum Discovery\nSince the user needs to specify the datum ID, it would be helpful if they could easily discover which datums saw which files. Maybe something like pachctl find-datum <repo> <commit> <path-to-file>\nI think as described your API works well for the majority of cases. Chances are you're debugging a failed datum and those are easy enough to find via list-datum. But there could be a bunch of failures. Or you could want to see the results / debug a specific part of the input data.. @dwhitena you identified a good piece of what's missing here, which is the cloud deployment /collaboration. @jdoliner is in the process of writing up his proposal on just that on this issue.\nBroadly, we'll be able to specify a github repo as an input, so that users can write transforms that use the code from there. \nThe jupyter integration is described here afaict.\n. Something to consider in the new model:\nhttps://github.com/pachyderm/pachyderm/issues/2412\nWe may want to be able to 'tag' the job or something w the git hash that was checked out to be run.. Love it. Let's do it.\nAs an implementation detail, I think the easiest way to enable users to do this is as a github integration (a la travis), as opposed to requiring that they specify a GH ssh key for a machine user (which requires a fair amount of setup).. Some discussion on implementation method / iteration:\nIteration 0\nGoal\nAs a user, I can specify a github repository as a pipeline input, that gets triggered by new pushes (NOT every new commit) to that repository.\nAs a user, I should only have to add some config to my GH repo for integration ... I should not have to create a new GH machine user / SSH key.\nAs a user, this service should work for public or private repositories.\nImplementation\nNote: This will only be supported on cloud based deployments\n\nWhen a user specifies a GH repo as an input we:\nProvide a URL for them to copy/paste into their repo's config for a GH integration\nCreate a pachyderm repo w the GH repo name\nThis repo will have a field that marks it as a GH repository\n\n\nCreate a github service on the pachd node that accepts a POST\nThis is the integration point w GH's webhook API so that we're notified on new commits being pushed\nWe'll use the push event\n\n\nUpon receiving a new push notification, we'll\nAdd a new commit to that repository containing the commit SHA of HEAD (the new commit we will want to clone)\n\n\nThen, when a worker processes a datum that contains an input repo that is marked as a GH repo:\nInstead of mounting the files (in this case just the commit SHA), instead we ...\nClone the contents of that GH repo at the commit SHA provided on the latest commit \nThen, we proceed processing the job as normal\n\n\n\nOpen Questions\n\nExactly how we can integrate w GH to authenticate to enable cloning private repos (should be do-able ... travis does this)\n\nIteration 1\nNice to haves:\n\nSpecify a branch name as well as a repository\nMake our pipeline creation smart enough to automatically provision a cloud load balancer + DNS / etc so that we can provide the user w a public domain name to copy/paste into GH\nUpdate the UI to support this feature during pipeline creation\nUpdate UI to link back to the GH repo at the SHA in question when navigating to a GH input type. Note ... if we do implement that 'load balancer / DNS / firewall' nice to have ... it'll also buy us a proper URL to use for the dashboard (no more port forwarding).. Breadcrumb for myself here ... this project looks to have what we need in terms of an external DNS setup. Right now (v0.4) it almost has kops support too, which would be great (so we don't have to do our DNS hacking on our aws.sh deploy).. And this is what we have today via k8s 1.7 to expose a public IP address. Doing some more reading on the GH integration options (a great tradeoff writeup is here ) I'm not entirely clear which option we want.\n\nOn the one hand a GH app gets clone access right away ... on the other hand ... it would be nice to have any user click a button to enable this support. However, then I'd have to deal with / hook in oauth integration (and we'd need to support that on the UI as well).\nSo for now to keep things simple ... the first iteration will only include the ability to clone from a public repo. We'll just use the webhook feature directly.\n. Just an update on what we have merged so far that'll make the 1.7 release:\n\nwe support git inputs (specify a repo url / branch)\nwe don't yet support cloning at a SHA (which means we clone then checkout ... which incurs overhead for large repos)\nwe don't yet support private GH repositories. I've described what will land for 1.7\n\nBeyond that, we'll create separate issues for further improvments.. I believe this issue is resolved.\nWe added resource limit support:\nhttps://github.com/pachyderm/pachyderm/pull/2507\nAnd upgraded to k8s 1.8:\nhttps://github.com/pachyderm/pachyderm/pull/2514. stale. We ran across dex which may solve a bunch of this for us.. How would this work?\nPachd would terminate using TLS, and would use a self signed cert, but one that the dashboard trusts?\nBut if a snooper has access to the k8s network, I think they'd have k8s access? (Maybe this is my faulty assumption). In which case they could see the certs, and decrypt the traffic.\n. Right now it's tied to the obj client, which gets initialized when the worker starts up. So cycling the pod may help (killing the pod and waiting for a restart).\n. That's not quite right.\nIt looks like a new amazon Reader is created every time we get an underlying object from the obj store. (This could happen 1 to many times per file).\nSo it's weird that you're getting an expired URL ... unless perhaps this is a rather large file that takes longer than 1 hour to download? What is the size of that file?. I don't think that's quite right.\nThe worker master client makes the GRPC Process() call to an individual worker. That worker then downloads the input data, which in turn calles GetFile which calls GetObject(). For each call of GetObject() a new amazon client reader is created, and at that time a new signature and new expiry is generated.\nSo. The fact that the log line's timestamp is just after the expiry makes sense. If you make a call w an expired url, of course the error will bubble up just a bit afterwards.\nWhat is unclear is still exactly why/how the URL could've expired. We generate the signed URL right before we attempt to get that URL. I do think a rather large download could do it. (I would still like to know what the max file size is for all the inputs to this pipeline)\nBut I have a new hypothesis. @homme -- this workload involves ~1M files, correct?\nIt's possible that we've hit a rate limit w cloudfront. That seems like it shouldn't happen ... cloudfront is a CDN and should be designed for this. However, this is a large workload, and it could be that they measure the workload differently per key rather than across keys and degrade differently.\nThis would also account for the small end to end time we see in the log message (~400ms). There's no way it could hang for an hour on a large file if thats the total run time of that call. And this also supports the idea that we've hit a limit, and this just happens to be a symptom of what happens after we hit that limit. We saw similar things once we hit the original S3 rate limiting.\nI would like to confirm that this is a large # of files @homme . It would be great if we could get a file count for an individual datum. But barring that, your total file count and glob patterns will suffice.\nThen this becomes a matter of confirming this theory and engineering the workaround.\nI believe we already have a semaphore for rate limiting our downloads from an obj store. I can try lowering that bandwidth and seeing if that helps. It will make it so that downloads take a bit longer ... but they will eventually succeed if we're right.\n@homme -- I'd also like to get access to the cluster (you can ping me the kubeconfig file on slack) so that I can poke around a bit. In particular I'm interested in seeing the logs / existing download timing for other datums.\nWe'll go from there.\nIdeally, I don't like solutions that slow us down. One thing I'll be thinking on is if we can configure cloudfront differently to get past this rate limiting. 1M files per datum is a use case we want to support. And we'll get it working. But I'd also like it to be as fast as possible.. Thanks for the detailed reply @homme \nAt the end of the day, we'll look at this more as we can reproduce it. So if you have a case that does, let us know.\n\nBut, some thoughts:\nI'm still not sure how the URL could be expiring in that code. Looking at the max elapsed time in the retry loop, its set at 15m, so that URL shouldn't expire. (Another reason I'd like to see it reproduced in the wild).\nIf we are in fact hitting cloudfront rate limiting, we have some options to scale appropriately. We might want to specify the semaphore/bandwidth as an advanced pipeline setting. That may allow the job to complete, though at a slower rate. It's a bit tricky to be 'smart' about that setting ... since:\na) its unclear how dynamic/static the throttling is\nb) its hard to know over different job runs how many files are in a datum ... and distribute that info to the other workers\nAnother idea is to be smarter about our distribution of objects on s3. \n(AWS has certain recommendations about key distribution that we could use. At this point, I'm providing breadcrumbs for myself here ... but that basically means distributing the hashes at the top level of the key, e.g. /asdfdf objects instead of /pach/obj/asdfdf ... the former will route to different s3 app servers and get more evenly distributed and the rate limiting goes into effect per app server for s3).\nIf we are seeing rate limiting, we would prefer a solution that gets past it, and doesn't slow us down. Thanks for sharing those scale numbers, that's helpful for us to know when designing.. stale. stale. This has been fixed. This happened to me yesterday on a production installation. The cluster eventually did 'recover' since those timeouts were low, and the pods got scaled back down.\nAdditionally, a failed job that happened before the restart was re-attempted. I see the job multiple times in list job, but the input commits are different ... and come from an upstream failed job that was re-attempted and failed.. I think this is not surprising. Idk what was deleted in s3, but I know there is at least one level of dereference happening there. So if you delete the underlying content but not the reference, I could see that error happening.\nMore broadly, I don't think we support users modifying the underlying obj store.\nMarking as stale. If this happens again / is something we want to support we can re-open.. A number of users have stumbled on errors like this. Our audience doesn't know what which ... does so we need to be prescriptive.. Looks like this gist does a great job listing what is required of a kops admin:\nhttps://gist.github.com/chrislovecnm/7c5b57e675e41fefc172f5dc8e9287c9\nI just wish that lived in a place that was a bit more long lived / maintained than a gist. But it's a start.\n. stale. Subsumed by https://github.com/pachyderm/pachyderm/issues/2684. I think if you supplement the secret/pachyderm-storage-secret w the google values this will work.\nMarking as stale until we see more requests for this.. Ah, according to\nhttps://github.com/pachyderm/pachyderm/issues/2578\nWe don't currently support google secrets, only the metadata API. But theres a new lib that would allow us to use secrets. Keeping this closed until we see more requests for this.. FYI, if you need this for debugging, you can port forward etcd, then use the client (v2) to read the key. I just had to do this recently, and it would've been handy to be able to poll directly.. This would also come in handy for users doing more advanced deployments, e.g. using k8s namespaces: \nhttps://github.com/pachyderm/pachyderm/issues/2479\nIf we had the pachctl dump-cluster feature ... we could easily see how this user deployed their cluster (within a namespace). And the pachctl update command would modify that in place to work for their deployment.\n. Interesting point. Why is that awful? It seems like that's what k8s does for their components. It sounds like your objection is to the automation here (the undeploy / redeploy), not actually using k8s as a source of truth.\nAnd to that point - I agree - it's not automatable. You're going to need a human to do any sort of 'merge conflict' on our k8s deployment. So scratch that first user story.\nHowever, I do think it would be helpful to be able to 'read' the state of the cluster. (User story 2). This enables me as a user to do two things:\na) debugability\nThis would give a human the tool they need to make doing this sort of 'merge conflict' resolution possible. This sort of semantic is analogous to the tooling that kubectl get ... or kops edit enables. It's so easy to forget what you modify in the course of development/testing. Personally I've found that tooling invaluable - for debugging and for development. And I agree ... the best practice should always be storing this info in version control. But that's not how folks develop. I don't check in a k8s manifest every time I'm debugging a cluster or testing a deployment. Especially while standing something up for the first time.\nThis will make our user's lives easier. And this will make it much easier to support our users when they come to us w an issue. I could ask for the output of pachctl dump-cluster to see exactly what's going on. There are a bunch of use cases that are not custom deployments that this would help debug (e.g. debugging that they setup their obj store secret correctly)\nb) manage custom deployments\nWhile this use case (user's manually modifying the manifest) has been rare in the past ... we've seen this come up more recently. And since we'll never want to support all of the k8s api surface via pachctl, I don't see this 'manual modifications' use case going away or lessening anytime soon. \nSome recent examples include:\n\nA user adding custom RBAC config\nA user adding custom namespacing\n\nAgain - an alternative would be to support each of these things through our API. But I doubt we want to do that. K8s is great at providing options for deployments. There are a bunch of k8s tools that promote good practices for true production grade clusters. That's not our core value proposition. And we don't want a bunch of flag/options inflating our deployment commands. . Yea, either way -- either a user makes the call for k8s to dump the right stuff ... or we have a helper that does it. \nIt seems reasonable to me to have a helper. Or at least, for us to apply a label to the Right Things so that a very simple kubectl command gets the info we need. The command you list would also dump the worker pod specs, which while that might be interesting, was not the goal here. The goal here would be to get the equivalent of the k8s objects you'd see in the manifest output by pachctl deploy --dry-run ..., so we'd see the deployments / RCs, but not the worker pod specs. The worker pod specs should be wholly determined by our pipeline spec format. Which is why we'd want a dump command for our pipelines. \n(Though I think I heard talk of some sort of --dry-run support for debugging worker pod specs? I'm not sure what the use case was there. Perhaps users need to apply custom k8s options on those specs as well, e.g. namespacing. I'm not sure.)\nI don't think it doesn't help upgrading at all. I think it does help. Yes, in these cases, there will always be manual editing. But the manual case is what we're trying to make easier. If a user has made extensive changes, it might be easier to start w the manifest from their dump, and add the new objects from the output of pachctl. In other cases, it will work as you describe. They'll have to look through their dump to see what's different, and add that again to the new manifest. But without seeing the state of what they have, they can omit configurations that they need. Without the ability to see this state, they can easily shoot themselves in the foot. \nTo that end, as a user, if the objects weren't reported in the same order (pachctl deploy --dry-run vs kubectl get all ... -o json), I'd additionally want to run them through some normalization (lets say ... sort the objects) so that I could easily diff those two results to see if there were any adjustments I'd need to make manually. \nSo AFAICT, you could get close to this result today w some kubectl commands and bash-fu. But to me this is a UX thing. It seems pretty reasonable to make it easy for users to do this. The more often they can do this sort of things themselves, the further they will get w the product.\nSo to be clear, at this point I'm proposing:\n\na pachctl command that issues queries via kubectl for the right k8s objects in the right order,\nand outputs the result as JSON\n\nOtherwise, we're falling back on recommending a convention to users. (Using a kubectl command, etc). But that hasn't been working for us so far (telling users to version control their manifests). I get that we want users to be better at k8s. This is tricky. Ultimately we don't want to be in the business of training data engineers, we want folks to use our hosted version. Till then, we just want folks to use our tech.\n. This is superseded by\n2683 . This is a requirement for customers w multi tenant k8s clusters.. Fixed by https://github.com/pachyderm/pachyderm/pull/2714/files. I just tested this, and it works. User code that emits to stderr gets recorded and is seen via get-logs. I believe this is to accommodate larger datum leases, but I'm not sure what the use case is?\nMarking as stale until we have a concrete ask.. Ah, I was wondering where that folder kept coming from. . Just to capture what we chatted about --- I planned on doing docs after the next iteration. The reason being ... the docs I'd have to write now are much longer than what I'd write after we have a first class k8s config that exposes this service on a public IP. Once we have that, I'll add docs, and they'll be more concise.. Good to know. It didn't seem like the git client we're using here supported\nit, but maybe I missed a config option somewhere. As we hit bottlenecks,\nthis will be a good thing to keep in mind.\nOn Tue, Nov 14, 2017 at 6:57 AM, Omer Katz notifications@github.com wrote:\n\nIt is possible to clone just one commit with Git>2.5 (which is pretty old).\nSee https://stackoverflow.com/a/30701724/85140\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/pull/2488#issuecomment-344284547,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAkfwaBbV-HtjSZGLkLJCyFiJYwuz_L-ks5s2apZgaJpZM4QNnN0\n.\n. Additionally, I've tested this on k8s 1.8 and it gets us past the GPU error we were seeing.. stale. dupe. It seems like the recent additions of the github input tests adds ~5m+ to the test suite. This is mostly because we're cloning pachyderm as part of the tests (which is a big repo). We should switch that out to a smaller public repo. This is a pretty easy fix, and will knock off a fair bit of time.. The immediate need was fixed. I pointed the git tests at a tiny repo and got builds back down to around 20m. However ... passing builds seem to be around 25m again ... so at some point we're going to want to look into the caching to speed up builds.. This is generic, I'm closing it.\n\nI recently did another pass (https://github.com/pachyderm/pachyderm/pull/2623) that caches what we can. Further improvements lie in specific issues such as: https://github.com/pachyderm/pachyderm/issues/2632\n. Yea, so what's happening here is that pachd will use the ec2's instance IAM role by default.\n(This is true no matter what you've set the --iam-role flag to, which is why our docs need updating and this is confusing: https://github.com/pachyderm/pachyderm/issues/2680)\nThat error message happens when that EC2 iam role does not have access to the s3 bucket specified. I saw this error when explicitly testing this out yesterday. I expect that error when the role doesn't have access. Once I added s3 access to that role, and restarted pachd, the error went away.\nHopefully that makes this a bit clearer. We have some updating to do on our docs to make this more transparent.. It's been a bit since I dug into this. There needs to be an API change to support this: https://github.com/src-d/go-git/issues/691\nIt's a bit unclear how hard that is.\nAn alternative is to use tarballs (most hosted git solns provide these -- bitbucket, gituhb, gitlab), but we were hoping the git client would just support it as is, as the alternative requires supporting each git hosting provider's API.. Seems fixed now. This is nifty.\nThe concern I have is that this adds a node install to CI. We're a bit sensitive to adding to CI build times lately. I guess when we get around to optimizing build times, we can cache the node download.\nIdk. What do you think @jdoliner ?\n. Yea ... looking into it ... (travis is nice and annotates each command's elapsed time) it took the before_install.sh from ~1s to ~20s.\nNot as bad as I would've thought. But not good either.. So we discussed this, and are good w the actual spell check changes, and even the new make task, but don't want it running on CI. Can you rip out that part?. I mean, if you look at the diff, the line I mentioned was added back, but w different whitespace, so now it's a different whitespace difference.\nBut since this PR process incurs so much overhead (we have to build the PR, etc) and its just whitespace, yea lets go ahead and just merge it.. I'm not sure what happens to stats commits in the new commits invariants re-model. @msteffen - can you shed some light?. Fixes #2503 \n. Yup :+1: \nWe exit the retry loop if the context is cancelled so that we don't retry (even though its a zero backoff loop).. Looks like an intermittent network issue when trying to install kubectl:\n\nI restarted the build.. Hrmm, I'm guessing there was a bad merge in here somewhere. Those compile errors on CI are because you're missing the import line:\n\"github.com/fsouza/go-dockerclient\"\nin src/server/pps/cmds/cmds.go, but that line is on master. Can you try merging again?. Yea, this is the place that CI was getting stuck. It was fixed in this PR\nSo merge in master again and try once more.. I want to double check manually that the pps master restarts the service successfully. Otherwise, this is ready for review.. Ok, double checked that case, it seems to work.. I put a reminder for docs here: https://github.com/pachyderm/pachyderm/issues/2597. Looks like you don't have uuid installed.\nThis issue is subsumed by this one which will make these lines more prescriptive for users.. I was able to reproduce this issue. I'm looking into it now.. So the push build is getting suck on launching minikube. I'm noticing that its trying to launch:\nsudo CHANGE_MINIKUBE_NONE_USER=true minikube start --vm-driver=none --kubernetes-version=v1.7.5\nWhereas on master it uses 1.8.0 ... do you mind merging in master to see what happens?. So this approach may be flawed. I'm going to write down what we've tried, what we know, what we suspect, and how we'll proceed\nOld Model\nIn the old model, we'd put a single document per putfile w a unique prefix. This would contain the list of content added, or a tombstone denoting a deletion.\nSomething like this\n_, err = d.etcdClient.Put(ctx, path.Join(prefix, uuid.NewWithoutDashes()), tombstone)\nAnd then at FinishCommit time, we'd read through these documents sorted in order by creation time to construct the final HashTree.\nHowever, to solve #2575 we need to paginate the etcd query results (when getting / constructing the hashtree). The two mechanisms we have to do that are providing a prefix for the initial query, and then using the from-key or range for subsequent pages. However. We cannot specify a from-key and also sort by creation time. Observe:\n```\n18-01-03[11:29:33]:~:0$etcdctl --endpoints=http://127.0.0.1:32379 get --prefix pachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/ --keys-only --limit=10 --sort-by=\"CREATE\"\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-0/e16a5029c8c8491b882c26622e769645\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-1/adfbc5fc61b54d4fb4a661d343e18257\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-10/9a4ff9960a984af49f7cb2b8688cc871\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-100/31433b535c1f4a0296f991cbc256c2cd\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-101/aa0c5647643a45b6972402d8e74554ac\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-102/320ca1e96a0a4c0ebb48f7ffb50d5120\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-103/a9e52350c2a0459986ffb6b5884b74f8\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-104/0be81542c1f0491b9485e728a4a2c141\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-105/c56c43571bef40ceb6b55d8ce22f3f54\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-106/c21b7ddb808b4b50a1ac88f034dbb63f\n18-01-03[11:29:39]:~:0$etcdctl --endpoints=http://127.0.0.1:32379 get --from-key pachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-106/c21b7ddb808b4b50a1ac88f034dbb63f --keys-only --limit=10 --sort-by=\"CREATE\"\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-11/7c132ca39d1449ef9be376cd5ee7abb7\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-106/c21b7ddb808b4b50a1ac88f034dbb63f\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-107/6c6ebe8bb90549c28ff30503b3d51515\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-108/1730f1369b0341248355a59323c32f34\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-109/1b5f7fc9589c40499760801c346f70a7\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-110/8fd6d1d205a7443181a6e92222fb6c60\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-111/34643bb879514180ba7d5d31d9c9f0ee\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-112/78564947e27240728abe79c28a40a6c6\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-113/f8c8bad758e745bfa52157b31f5e8300\npachyderm_pfs/scratch/TestUpdatePipelineStats_dataf5752fde9766/5e62973836af466a80ab997fc1b8125c/file-114/b5e00e60ec534f5eb31c75fc1cb98bf6\n```\nWe should be seeing files 0 through 10 in the first batch, and 10 through 19 in the second. But we get lexigraphically sorted results. The sort happens after the filtering.\nNew Model\nSo, the new model we opted for was instead of doing an insert of a document for each put, actually use etcd to do a transaction on the document (upsert) as needed.\nSo instead of the put above, we're doing something like:\n```\n    _, err = col.NewSTM(ctx, d.etcdClient, func(stm col.STM) error {\n        recordsCol := d.putFileRecords.ReadWrite(stm)\n    var existingRecords pfs.PutFileRecords\n    err := recordsCol.Get(prefix, &existingRecords)\n    if err != nil && !col.IsErrNotFound(err) {\n        return err\n    }\n    if newRecords.Tombstone {\n        existingRecords.Tombstone = true\n        existingRecords.Records = nil\n    } else {\n        existingRecords.Split = newRecords.Split\n        existingRecords.Records = append(existingRecords.Records, newRecords.Records...)\n    }\n    // Now put the new data\n    recordsCol.Put(prefix, &existingRecords)\n    return nil\n})\n\n```\nWhat we see\nThis mostly works (the integration test suite mostly passed), and the newly added TestManyFilesSingleCommit passes, but the pfs test suite fails miserably.\nIn particular, the memory on my machine (32GB) gets eaten by etcd, at which point I see an error like:\n```\n2018-01-03T10:49:10-08:00 ERROR pfs.API.PutFile {\"duration\":267.096380606,\"error\":\"etcdserver: request timed out\",\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"test\"},\"id\":\"1a5956636a7945249a6af7c6e564f728\"},\"path\":\"foo\"}}} \n2018-01-03T10:49:10-08:00 ERROR pfs.API.PutFile {\"duration\":23.390383328,\"error\":\"etcdserver: request timed out\",\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"test\"},\"id\":\"1a5956636a7945249a6af7c6e564f728\"},\"path\":\"foo\"}}} \npanic: etcdserver: request timed out\ngoroutine 43394 [running]:\ngithub.com/pachyderm/pachyderm/src/server/pfs/server.TestATonOfPuts.func1(0x1388, 0x64, 0xc4306b32e0, 0xc426178340, 0x1000992, 0x4, 0xc426beace0, 0x1039d02, 0x53, 0xc4270c7780)\n    /home/sjezewski/go/src/github.com/pachyderm/pachyderm/src/server/pfs/server/server_test.go:2087 +0x249\ncreated by github.com/pachyderm/pachyderm/src/server/pfs/server.TestATonOfPuts\n    /home/sjezewski/go/src/github.com/pachyderm/pachyderm/src/server/pfs/server/server_test.go:2083 +0x2ed\nFAIL    github.com/pachyderm/pachyderm/src/server/pfs/server    435.119s\nMakefile:335: recipe for target 'test-pfs' failed\nmake: *** [test-pfs] Error 1\n```\nAnd the etcd logs:\n2018-01-03 18:49:10.108882 W | etcdserver: apply entries took too long [6.849775633s for 16 entries]\n2018-01-03 18:49:10.121605 W | etcdserver: avoid queries with large range/delete range!\n2018-01-03 18:49:10.152136 I | etcdserver/api/v3rpc: grpc: Server.processUnaryRPC failed to write status: stream error: code = Canceled desc = \"context canceled\"\n2018-01-03 18:49:10.154867 I | etcdserver/api/v3rpc: grpc: Server.processUnaryRPC failed to write status: connection error: desc = \"transport is closing\"\n(there are many more transport closing messages)\nNow I've also modified the page size (the number of results etcd returns). While having it at 10k makes the performance of FinishCommit much better (~3s) in the TestManyFilesSingleCommit test (as opposed to 100, which makes that operation take 3m!). I tuned it back down to 100 to see if that helped the memory load on etcd. It made its memory footprint grow more slowly, but it did eventually consume all the memory and then fail as above.\nWhat we suspect\nSome combination of:\n\nusing the STM instead of the simple put creates too much overhead\nwe're doing too many transactions within etcd at once\n\nIs causing the memory usage to blow up on etcd. \nHow we'll proceed\n\nI'm going to instrument the old method of putting vs the new STM\nI'm also going to tune the client semaphore down to see if that helps this test TestATonOfPuts (a single test that makes the memory blow up) succeed at all (even if it takes longer). In this case we suspect that etcd doesn't handle many concurrent transactions well ... in which case we might want to implement a semaphore at the STM layer to meter etcd transactions\nI'll also try upgrading our image of etcd to see if a newer version ameliorates this issue\n\nBased on these results, we may throw out this model and try a different one. We have shied away from doing upsert on file infos in the past, and this performance bottleneck may be why. If so, it's important to understand this going forward. This is my attempt to record this so we remember 'why' we made this choice.\nDifferent Model\ne.g we could instead write many documents to etcd, but use a counter and include that in their key so that we don't need to sort the results ... the lexigraphically sorted results are also the results sorted by creation time\n. We're still using the upsert model, and tracked down some of the mystery outlined above.\nBasically, I took etcd's benchmark tool, modified it to:\n\nuse our collection library\nadd a new putFileRecord each time it was run\n\nAnd this way I was able to reproduce the very low rate we were seeing (~12req/s).\nWith this tool I was also able to reproduce the results from this etcd blog post which outlines what we should see in cases where there is high and low key contention.\nBasically, our slowdown was caused by larger and larger payloads being written. If we modify the key N times and each time add a record, we're basically transferring O(N^2) size in total (sum of N things ... is N(N+1)/2). Using our collection lib alone in their benchmark tool didn't replicate our slow request rate, but adding something to the record each time matched our rate pretty closely.\nSo, the result was that we tuned down that TestATonOfPuts test so that it runs in a reasonable amount of time. The test still goes beyond the collection.QueryPaginationLimit, so we verify that we can handle larger numbers of files without making the test take forever. This resolves issue #2575 \nAdditionally, I added a few benchmarks (since that TestATonOfPuts was reporting timing info for putting the files and finishing the commit) so that its easier to analyze the performance of those components in the future.\nAlso note -- etcd can still gobble up memory ... but it makes sense because of the data being transferred. The fact that the mem (according to the OS) was still being used by etcd is a function of go's GC, not etcd's active usage. (Still a bit annoying if you're running these locally, like me, but not a dealbreaker). But if we see etcd evictions / OOMs, this may be a pattern as to why. (Too much load too quickly).. Here are the results of the benchmarks as well:\n18-01-08[17:27:52]:pachyderm:1$1$go test -v ./src/server/ -bench=BenchmarkPutManyFiles -run=BenchmarkPutManyFilesSingleCommit -timeout=3000s -benchtime 100ms\ngoos: linux\ngoarch: amd64\npkg: github.com/pachyderm/pachyderm/src/server\nBenchmarkPutManyFilesSingleCommitFinishCommit1-8                2     51069982 ns/op\nBenchmarkPutManyFilesSingleCommitFinishCommit10-8              1     184704487 ns/op\nBenchmarkPutManyFilesSingleCommitFinishCommit100-8             1    1917883609 ns/op\nBenchmarkPutManyFilesSingleCommit1-8                           1    8932577530 ns/op\nBenchmarkPutManyFilesSingleCommit10-8                          1    96257950347 ns/op\nBenchmarkPutManyFilesSingleCommit100-8                         1    965105909386 ns/op\nPASS\nok      github.com/pachyderm/pachyderm/src/server   2136.295s\nTranslating that into human durations ...\nFinish Commit:\n| # of Files | time/op |\n| ---| ---|\n| 1000 | 51ms/op|\n|10000|184ms/op|\n|100000|1.91s/op|\nPutting Many Files:\n| # of Files | time/op|\n|---|---|\n| 1000 | 8.93s/op|\n| 10000 | 1m36s/op|\n|100000|16m5s/op|\n. Ok ... I made all the changes above and tests are passing. @jdoliner if you want to take another look.. We're waiting on CI to get unstuck, but in the meantime, here's a summary of the build improvements we'll see to the timing of make docker-build\n\nBasically, we remove:\n\nneeding to upload anything as part of the Docker build context\nthe need to rebuild the pachyderm_compile image (cutting out all the docker build time)\n\nAnd for the go build cache, we test three cases:\n\nno cache\nsemi cached (cache is there, but a small thing was changed ... e.g. I changed a const in src/server/pfs/server/driver.go)\nfully cached\n\nIn the fully cached case make docker-build clocks in at ~9s. And for the semi-cached case we see ~27s (vs 43), or a 37% speed improvement (YMMV of course depending on how much changed).\nAnd compared to master ... build times are way faster (43s vs 163s) in the uncached code cases.\n. Hrmmmm seems like pike veto'd single line ignore directives\nhttps://github.com/golang/go/issues/17058\nBut I bet I can silence this class of error w a flag. That'll disable all such errors though.. So two practical things to try:\n\nuse non ubuntu base to keep the base image small\npre-build the layers that don't include the compilation stuff (all the apt-get etc), publish that on dockerhub, so that CI just has to pull that small base\n\nIn fact ... now that we use a hostmount for the actual code (because of the go build cache update), we may be able to cache/pre-build the whole compilation image.. Hit this during a customer install. The workaround was hacky :-(. Ok :+1: \nHuh, I thought that worked too. Maybe its lower case fixes ?. Yea, that's it. If flush commit reports failures, this gets a lot simpler. We can close this.. Just did this again :-/\nThis time I used 2GB instead of 2G. Updated issue lives here: https://github.com/pachyderm/pachyderm/issues/2680\nBasically what we have today is sufficient for node based IAM role usage. Practically speaking, to do that, do a normal aws.sh deployment, and then delete the amazon-id, amazon-secret and amazon-token fields from the k8s pachyderm-storage-secret.\nThen, the s3 go client by default polls the ec2 metadata API to get the IAM role it should use (which will be the role assigned to the ec2 instance). In a kops deployment thats something like node.nameofcluster.com\nI tested this on a fresh AWS deployment and it works fine.\nHowever if a customer needs container based IAM roles, we'll need to add documentation around deploying the daemonset (in addition to using the --iam-role deploy flag). That is captured in the issue above.. The ask here is creating the vault plugin that will call the right pachyderm auth API calls to allow vault to login/renew user's pachyderm tokens.\nI have their example working, but it's a bit unclear exactly how we would expect users to provide the necessary admin token. I'm going to reach out to hashicorp and ask / read more on this.. The actual Pachyderm auth API updates are captured in #2700 . Dupe of #2700 . Discussed this a bit w @jdoliner and @msteffen \nWe want to draw a distinction between control flow (and perhaps notifications) versus monitoring.\nThe latter should be done by a system that is independent of the system being monitored.. added. Dupe of #2700 \n. Closed in favor of:\nhttps://github.com/pachyderm/pachyderm/pull/2734/files. Naming changes / other changes in place. Except for the version rename, as discussed in slack. Ok. Yea I figured if we're ok passing the version in as a value, the cluster ID would be ok as well. \nThe alternative would be to get the cluster ID using the etcd v3 api (which means setting it that way too) since this is one of the few remaining places we use v2. Might be worth doing just for that reason. Which shouldn't really require a migration -- we'd keep the logic that if it wasn't set we'd generate a new one. That means we might 'lose track' of certain clusters w specific IDs, but that's really not a big deal. I'm ok making that change.\n. We may also want to add the namespace to the ~/.pachyderm/config.json file ... since we include details that tell pachctl how to connect to a cluster in there (right now the pachd address, user token).. No, the release script should update this file whenever we cut a new release.\nWhat happened for 1.6.9 was it was cut from a custom branch 1.6.x that stayed long lived and so the file never landed on master. I've added that file manually on master for this edge case.\nI think it's ok to leave as an error. But I don't have a strong opinion. @jdoliner what do you think?. User reports the following when trying to run on alpine:\n/ # sh /usr/local/bin/pachctl\n/usr/local/bin/pachctl: line 1: \u007fELF: not found\n/usr/local/bin/pachctl: line 2: syntax error: unexpected \")\". What's odd is that the pachd image is based off of scratch. So perhaps there is a difference in the dependencies for pachctl? . Perhaps JSON stream decoders handle this case gracefully?\nA user who wasn't using a stream decoder lib ran into this.. The diff is big bcz I had to add vendored deps, but most of the changes are in:\nsrc/server/worker/stats.go\nsrc/server/worker/api_server.go\n. I added a test suite, which was very helpful in vetting some of these queries, and will also serve as a good handoff to @gabrielgrant to hook this into the UI. The tests are a little funky (have timeouts in a few places) but are designed to generate data that is meaningful to look at.\nHere are some example graphs that we can generate.\nUpload time across jobs:\n\nUpload time per job:\n\nAvg runtime across jobs (by state ... finished  / errored):\n\n. So our current understanding supports the 'horizontal line' at the end of the graph. In particular since most of the things we're looking at now are functions as applied to counters ... counters report their state even if it hasn't changed, and so that line is expected.. Ok, I've integrated all the feedback, this is ready for another review @jdoliner . So I removed the k8s 1.7 compatibility test. This was consistently failing for me on CI. Locally it works most of the time, but sometimes doesn't work w kubectl 1.8.2\nK8s is now at 1.10 so we're ok dropping this test.. Ok. I pulled all those things out into helpers. Ready for a final review @jdoliner . This is implemented! We now have builds that run in about 15m wall time!. Fair enough. If you need to keep it disabled to get CI to pass (and we want it to be disabled on master), let's just open an issue to fix that suite. Fixes https://github.com/pachyderm/pachyderm/issues/2632. Ok, made those changes, and bumped to latest (go1.10.2). Ok made that change, and responded to comments. Ready for another review.. I added a few more changes. Basically I namespaced the metric names so that delete_all wouldn't create a registration conflict (since it exists in both pfs and pps). Probably needs another review.. Haha :-P\n. Thanks for the fix!\nWe actually do have some notes on how to update vendor.json:\nhttps://github.com/pachyderm/pachyderm/tree/master/src/server\nWithout that part, it's hard to tell which SHA the new version came from. So while it's awesome you updated to a version that fixes this issue, it's hard to tell which version you used to do that.\nCan you resubmit w the vendor.json changes? Once you know which SHA you grabbed (or if its latest), I'd recommend checking out (undo'ing) these changes and letting go vendor pull in the files. That'll update vendor.json and update the packages you need.\n. So poking around here a bit.\nIt seems that this function call ... uuid.NewV4() returns 2 args at the version we have vendored (36e9d2ebbde5e3f13ab2e25625fd453271d6522e) but not at release 1.2.0 (f58768cc1a7a7e77a3bd49e98cdd21419399b6a3) which seems to be the version that dep uses as its the latest release. Unfortunately the API at version 1.1.0 doesn't seem compatible either.\nSo there is a bit of impedance mismatch between these vendoring tools.\nSince there is a way to specify an override using dep ... I've added this to my Gopkg.toml file:\n[[override]]\n   name = \"github.com/satori/go.uuid\"\n   revision=\"36e9d2ebbde5e3f13ab2e25625fd453271d6522e\"\nAnd that did the trick.\n. Discussed this. It's intentional.\nBasically for variable width calls (esp list-job) the output can get pretty hairy without reprinting the header. Needs a review first.. Both good points.\nI'm not sure we document that config file anywhere, so that should be a separate issue/ticket to document that feature, probably after we address those issues listed.\nIn terms of ports/hostnames for dash access, I'm not sure how we're going to handle this short of:\n\nenabling CORS the right way so that cross domain is fine, or\nhaving a single reverse proxy for all the services we need to expose\n\nI don't think there's a way to have k8s provision a single load balancer for multiple services ... short of doing the reverse proxying ourselves (nginx / etc).\nAs it stands, this is our current best practice. Since I've been asked this a few times just this week, I'd like to get the docs up. But yea, not being able to login is going to be a dealbreaker for sure.. It seems like the recommended k8s approach is to setup / config an ingress controller\nhttps://blog.getambassador.io/kubernetes-ingress-nodeport-load-balancers-and-ingress-controllers-6e29f1c44f2d\nWhich is basically what we described above. This should be a new feature request. And this will probably become a dealbreaker for enterprise deployments.. Some example usage:\n```\n18-08-08[15:25:54]:pachyderm:0$pachctl put-file foo master tableA -f pgdump --split sql\nwarning: PFS destination \"tableA\" looks like a URL; did you mean -f tableA?\n18-08-08[15:26:15]:pachyderm:0$pachctl list-file foo master\nNAME   TYPE SIZE   \ntableA dir  1.354KiB \n18-08-08[15:26:30]:pachyderm:0$pachctl list-file foo master /tableA\nNAME                     TYPE SIZE   \n/tableA/0000000000000000 file 1.21KiB\n/tableA/0000000000000001 file 1.212KiB \n/tableA/0000000000000002 file 1.213KiB \n18-08-08[15:26:34]:pachyderm:0$pachctl get-file foo master /tableA/0000000000000000\n-- PostgreSQL database dump\n--\n-- Dumped from database version 10.4 (Debian 10.4-2.pgdg90+1)\n-- Dumped by pg_dump version 10.4 (Debian 10.4-2.pgdg90+1)\nSET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nSET client_encoding = 'UTF8';\nSET standard_conforming_strings = on;\nSELECT pg_catalog.set_config('search_path', '', false);\nSET check_function_bodies = false;\nSET client_min_messages = warning;\nSET row_security = off;\nSET default_tablespace = '';\nSET default_with_oids = false;\n--\n-- Name: company; Type: TABLE; Schema: public; Owner: postgres\n--\nCREATE TABLE public.company (\n    id integer NOT NULL,\n    name text NOT NULL,\n    age integer NOT NULL,\n    address character(50),\n    salary real\n);\nALTER TABLE public.company OWNER TO postgres;\n--\n-- Data for Name: company; Type: TABLE DATA; Schema: public; Owner: postgres\n--\nCOPY public.company (id, name, age, address, salary) FROM stdin;\n1   alice   100 1234 acme st                                        1000000\n.\n--\n-- Name: company company_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres\n--\nALTER TABLE ONLY public.company\n    ADD CONSTRAINT company_pkey PRIMARY KEY (id);\n--\n-- PostgreSQL database dump complete\n--\n18-08-08[15:26:45]:pachyderm:0$pachctl get-file foo master /tableA\n-- PostgreSQL database dump\n--\n-- Dumped from database version 10.4 (Debian 10.4-2.pgdg90+1)\n-- Dumped by pg_dump version 10.4 (Debian 10.4-2.pgdg90+1)\nSET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nSET client_encoding = 'UTF8';\nSET standard_conforming_strings = on;\nSELECT pg_catalog.set_config('search_path', '', false);\nSET check_function_bodies = false;\nSET client_min_messages = warning;\nSET row_security = off;\nSET default_tablespace = '';\nSET default_with_oids = false;\n--\n-- Name: company; Type: TABLE; Schema: public; Owner: postgres\n--\nCREATE TABLE public.company (\n    id integer NOT NULL,\n    name text NOT NULL,\n    age integer NOT NULL,\n    address character(50),\n    salary real\n);\nALTER TABLE public.company OWNER TO postgres;\n--\n-- Data for Name: company; Type: TABLE DATA; Schema: public; Owner: postgres\n--\nCOPY public.company (id, name, age, address, salary) FROM stdin;\n.\n--\n-- Name: company company_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres\n--\nALTER TABLE ONLY public.company\n    ADD CONSTRAINT company_pkey PRIMARY KEY (id);\n--\n-- PostgreSQL database dump complete\n--\n```\nAnd note how you'd delete/reset the header/footer values. In particular, since we validate the pgdump header/footer format ... you need to use the line delimiter to actually delete them.\n```\n18-08-08[15:26:57]:pachyderm:0$touch empty; echo '' | pachctl put-file foo master tableA --header empty\nwarning: PFS destination \"tableA\" looks like a URL; did you mean -f tableA?\ncannot specify header without also setting --split flag\n18-08-08[15:28:06]:pachyderm:1$touch empty; echo '' | pachctl put-file foo master tableA --header empty --split sql\nwarning: PFS destination \"tableA\" looks like a URL; did you mean -f tableA?\nReading from stdin.\ninvalid header - missing row inserts\n18-08-08[15:28:12]:pachyderm:1$touch empty; echo '' | pachctl put-file foo master tableA --header empty --split line\nwarning: PFS destination \"tableA\" looks like a URL; did you mean -f tableA?\nReading from stdin.\n18-08-08[15:28:26]:pachyderm:0$pachctl get-file foo master /tableA\n.\n--\n-- Name: company company_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres\n--\nALTER TABLE ONLY public.company\n    ADD CONSTRAINT company_pkey PRIMARY KEY (id);\n--\n-- PostgreSQL database dump complete\n--\n18-08-08[15:28:33]:pachyderm:0$touch empty; echo '' | pachctl put-file foo master tableA --footer empty --split line\nwarning: PFS destination \"tableA\" looks like a URL; did you mean -f tableA?\nReading from stdin.\n18-08-08[15:28:42]:pachyderm:0$pachctl get-file foo master /tableA\ncannot read directory, no header or footer\n18-08-08[15:28:44]:pachyderm:1$\n```. Ok cool. Good to see it wasn't a regression.\nI have had customers explicitly ask if it's ok to use a newer client than server version. (This happens easily as their team members start using pachyderm long after the cluster has been deployed). Generally I think we say its fine. Maybe we need to change our response, or perhaps in the changelog list when there are breaking changes? That can be tricky to estimate / catch in the changelog though.. A starting point is to look at the make logging task\nIt worked at some point, but I'm not sure its still working.\nOn Fri, Sep 14, 2018 at 7:41 AM, Nick Harvey notifications@github.com\nwrote:\n\nWhat is the ask?\nCreate documentation for how to configure an ELK stack for centralized\nlogging of the pachyderm cluster.\nWhat is the goal / desired outcome?\nComplete instructions on how to configure centralized logging of the\npachyderm cluster so that information isn't lost due to log rotation.\nSo that I can accomplish my goal ...\nIf there is a way to accomplish this today via workaround, what does that\nrequire?\nEnvironment?:\n\nKubernetes version (use kubectl version):\nPachyderm CLI version (use pachctl version):\nCloud provider (e.g. aws, azure, gke) or local deployment (e.g.\n   minikube vs dockerized k8s):\nOS (e.g. from /etc/os-release):\nOthers:\n\nWhat happened?:\nWhat you expected to happen?:\nHow to reproduce it (as minimally and precisely as possible)?:\nAnything else we need to know?:\nEnvironment?:\n\nKubernetes version (use kubectl version):\nPachyderm CLI version (use pachctl version):\nCloud provider (e.g. aws, azure, gke) or local deployment (e.g.\n   minikube vs dockerized k8s):\nOS (e.g. from /etc/os-release):\nOthers:\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/3152, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAkfwTW0Zm0qZ_2bCdfuwvM0Qy2oLl4Hks5ua8AdgaJpZM4WpZkZ\n.\n. Yup. Done and pushed.\n. Yea I'm not sure. I just installed whatever was easiest for protoc / proto-gen-go / grpc / etc until error messages went away. Maybe we need some hooks in the build script to lock down versions of some or all of those things?\n. Just checking to see if you'd read the code ;-)\n. Yup. I was making an exception to leaving commented code in in case in the future we need to do anything else. But it really is vestigial.\n. I agree\n. Yup, good call\n. Discussed w JD. He's not worried about any changes if the integration tests pass. They do locally! So we'll merge this in.\n. And to be clear - these changes are JUST from me running 'make proto' before I made any changes to how protobufs were compiled. So any changes should just be from my version of protoc/etc/etc.\n. Yea, that was the idea. But I think thats ok? We seem to be using lion mostly for printing lines like this:\n\n2016-04-18T19:22:21Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfsclient.localBlockAPIServer\",\"method\":\"ListDiff\",\"request\":\"shard:10 \",\"duration\":{\"nanos\":49364}}\nWhich aren't very helpful, and makes reading the output of CI (even when failing) hard to read. Every travis run contains ~6k of these log lines. Thats a lot to sift through.\nSince we have tests passing more often than failing, this makes it easier to see the output of travis under more frequent conditions. \nSo, I figured silence them, and if you're debugging something and really need them, pass the verbose flag (either locally, or enable on your branch for travis).\n. This was my own doing ... but I think this line is redundant given the lines below\n. I dislike verbose flags\n. Why the name?\n. Why are we skipping these?\n. Yea, I like naming it 'TestIssue296' - thats probably enough since github\nURLs are semantic (easy to copy paste the number)\nOn Tue, Apr 19, 2016 at 6:43 PM, Joe Doliner notifications@github.com\nwrote:\n\nIn src/server/pfs/fuse/filesystem_test.go\nhttps://github.com/pachyderm/pachyderm/pull/275#discussion_r60340088:\n\n\nif err := pfsclient.FinishCommit(apiClient, repoName, commit.ID); err != nil {\nt.Fatalf(\"FinishCommit: %v\", err)\n  +func Test296(t *testing.T) {\n\n\nOh this test is recreating the situation described in #296\nhttps://github.com/pachyderm/pachyderm/issues/296 . I kind of like\nadding regression tests for specific issues, what about TestIssue296, and\nmaybe a comment with a link to the original issue?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\nhttps://github.com/pachyderm/pachyderm/pull/275/files/1aa62f715c1c21e9ebd91ba885650dc6950cf739#r60340088\n. Yea. Mostly it grew while I was figuring it out, because each time I'd understand a part of it. Now that I think I understand the quirks I can definitely cut it down.\n. It was intentional, but happy to discuss. In the commit message for those changes I mention the reasons.\n\nBasically for #346 there are several ways it seems to fail. Having those lines in the error output does differentiate a few of the error patterns. Since #346 is difficult to reproduce, and you only see that output when a fuse test fails (and you may not if you re-run it), i thought it might be helpful to leave them in so that as we see this issue again, we can add the log to the issue #346. \n. Yes. I'll update the comment\n. Ah, I missed that helper. I thought I had to call c.Usage() manually, and so ripped out the other one. I'll use this instead.\n. Using that helper results in this output:\nshell\n$ pachctl mount\nExpected 1 args, got 0\nWhich is a lot less helpful than what it was before by using c.Usage : \n``` shell\n$ pachctl mount\nUsage:\n  pachctl mount path/to/mount/point [flags]\nFlags:\n  -n, --block-modulus int   modulus of block shard (default 1)\n  -b, --block-shard int     block shard to read\n  -m, --file-modulus int    modulus of file shard (default 1)\n  -s, --file-shard int      file shard to read\n  -h, --help                help for mount\n```\nThe os exit repetition is unfortunate, but thats what the helper is doing under the hood. I'd prefer to leave it as is since c.Usage is more descriptive.\n. This is a sentence fragment / just needs some rephrasing\n. Redundant sentence here - I think you only want one of those clauses\n. 'guaranteed to wind up'\n. I understand the reason you want to 'hoist' the consts, but client.NONE is a bit vague given how much is in the client.* namespace now. Maybe COMMIT_NONE?\n. I like that\n. Calls like this should fall through the the right embedded client, right? (Just like in the c.CreateRepo(...) call a few lines above?)\nWhatever pattern we expect users to use we should also use in the tests. I think thats the simpler c.CreateJob(...)\n. Same thing applies here / in tests in general\n. I like that we're using the provided creation helper, not an internal helper.\n. Ah, this is the trade we make for the consolidated interface for users. Not a bad trade.\n. Makes way more sense when reading the new version of the code.\n. That makes sense. Should we create a new ticket for this?\n. Oh, right. Yup. Will add it there.\n. So the info we're losing is repo / commit ID / shard. When invoking GetFile() you provide the repo and commit ID, so it seemed redundant to provide this in the error message. Presumably the caller has enough information to debug.\nAs long as the other places we return 'commit not found' also have those inputs, I think it's fine to omit. \nBut - is the shard number meaningful to report? Maybe for debugging? For the CLI, I'd say no. For the error reported to the client, I think its fine to include.\nI will go through the other locations we want to return 'commit not found' and use the canonical err.\n. Ok talking w this more, we will report all the info in all the cases.\nI'll use this opportunity to make the error reporting more consistent across the board. We still have some error strings in lieu of using the preset errors. And we don't have errors for missing job / etc.\n. Yup will do\n. Yup, logical error there.\nI think the name is more descriptive, but thats just a style thing. I'll change it.\n. This seems like debug output - it should be removed.\n. In the original version I know you had several domains. Do we want that again for the final demo?\n. Done\n. Done and renamed in all cases\n. Does this mean the default behavior of putfile is replacing not appending? Am I reading this test correctly?\n. And what is the mechanism to append?\nAlso, we don't have to worry about API stability yet, but it would be good to start making note of the side effects of the functions in the inline comments for the API docs.\n. That makes sense. I'm all for rewriting tests based on intent.\nThose PutFiles threw me off. Reading the test closer, it looks like cp command causes the change in the output, which suggests its making a call to SetAttr. That makes sense. If I cp a file someplace I'm overwriting it, which we now support.\nThanks for the detour on behalf of me learning :-)\n. What do we expect if I try to get a file thats really a directory? I'm guessing we return an empty file? \nWe should have a test for whatever behavior we support.\n. Hooray for much more debugging!\n. I'm trying to remember the case why we needed to touch the file, and cant. If you can, seems like a good place for a comment\n. Ooop, I remember now, its when we see the truncate as a SetAttr w size 0\n. Yup, the retErr setting should be nested\n. Sure, easy change. This is in a few places in the tests\n. Yes, I like that idea. I do think as-is its non expressive\n. Originally that's what I was doing ... but in this case decoder gets destroyed across calls to putOneBlock which means that the decoder loses track of how much its read. This results in json content past the first block getting mangled.\n. Ooop yup. Missed this change. Let me get that real quick.\n. Yup! I think that's more descriptive - our suffixes describe the type of data we're storing.\n. I dont think this comment doesn't apply anymore\n. Yes, this will silence the annoying 'file not found' errors that would popup from normal usage via fuse mount.\nThe PersistentPreRun makes sure that this function runs before any of the child command's Run callbacks are called. So - we make sure we set the log levels before the 'real' code runs.\n. Yup! Since I changed the behavior above and specified a new error string, I wanted a test for these changes.\n. I like that, I'll run w that\n. The motivation is we wanted the error to be more descriptive. Before the change if you try to create the same pipeline, you'll get an error like repo foo exists, which isn't descriptive. \nWith the change, we check to see if the PipelineInfo matching the request already exists, and return a descriptive message (e.g. pipeline xxx already exists) if it does.\nThe race case is interesting. It won't race more than what happens today -- as you say the CreatePipelineInfo prevents that. But you're right - ideally we handle it here so that even in the race condition we return the nice error message.\n. @jdoliner pointed out a way to simplify this code a bunch. Just moved the repo creation to after creating the PipelineInfo ... now the race is prevented and the descriptive error is present before the repo is created. I'll just make sure to destroy the PipelineInfo if the repo creation fails.\n. Ah, yea.\nThe match sting 'has already been finished' is not the most descriptive thing, so I was hoping to make the code around the matching more descriptive ... but I can see how that's not really the case right now either.\nI'll inline as you suggested.\n. Haha ... nope :-P\n. Fair enough - and I do very much like that it teaches the user about logs. I'll make the change.\nFWIW - I did it this way because it felt like the perplexity was part of the result, and should be stored w the resulting model. Since the get logs command uses k8s pod logs, they could go away if you restarted the cluster (right?) ... but this is more than adequate for the sake of an example.\n. Ah, one side effect of using logs is we'll see both stdout and stderr. And since we don't silence the FUSE logs when mounting via the job-shim, we'll see a bunch of innocuous FUSE errors:\n$ pachctl get-logs 8225e745ef8e3d0c4dcf550c895634e3\n0 | Epoch: 1 Learning rate: 1.000\n0 | 0.002 perplexity: 8526.820 speed: 1558 wps\n0 | 0.102 perplexity: 880.494 speed: 1598 wps\n0 | 0.202 perplexity: 674.214 speed: 1604 wps\n0 | 0.302 perplexity: 597.037 speed: 1604 wps\n0 | 0.402 perplexity: 561.110 speed: 1604 wps\n0 | 0.501 perplexity: 535.682 speed: 1599 wps\n0 | 0.601 perplexity: 518.105 speed: 1601 wps\n0 | 0.701 perplexity: 504.523 speed: 1600 wps\n0 | 0.801 perplexity: 498.718 speed: 1601 wps\n0 | 0.901 perplexity: 491.138 speed: 1604 wps\n0 | Epoch: 1 Train Perplexity: 486.294\n0 | Epoch: 1 Valid Perplexity: 458.257\n0 | Test Perplexity: 433.468\n0 | 2016-07-07T21:56:34Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"GoT_train\"},\"id\":\"dd2c024a5da041cb89e12e7984c81359\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"ptb.ckpt\",\"err\":\"no such file or directory\"}\n0 | 2016-07-07T21:56:34Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"GoT_train\"},\"id\":\"dd2c024a5da041cb89e12e7984c81359\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"ptb.ckpt\",\"err\":\"no such file or directory\"}\n0 | 2016-07-07T21:56:34Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"GoT_train\"},\"id\":\"dd2c024a5da041cb89e12e7984c81359\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"ptb.ckpt\",\"err\":\"no such file or directory\"}\n0 | 2016-07-07T21:56:34Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"GoT_train\"},\"id\":\"dd2c024a5da041cb89e12e7984c81359\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"ptb.ckpt.meta\",\"err\":\"no such file or directory\"}\n0 | 2016-07-07T21:56:34Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"GoT_train\"},\"id\":\"dd2c024a5da041cb89e12e7984c81359\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"ptb.ckpt.meta\",\"err\":\"no such file or directory\"}\n0 | 2016-07-07T21:56:34Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"GoT_train\"},\"id\":\"dd2c024a5da041cb89e12e7984c81359\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"ptb.ckpt.meta\",\"err\":\"no such file or directory\"}\n0 | 2016-07-07T21:56:34Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"GoT_train\"},\"id\":\"dd2c024a5da041cb89e12e7984c81359\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"id_to_word.json\",\"err\":\"no such file or directory\"}\n0 | 2016-07-07T21:56:34Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"GoT_train\"},\"id\":\"dd2c024a5da041cb89e12e7984c81359\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"id_to_word.json\",\"err\":\"no such file or directory\"}\n0 | 2016-07-07T21:56:34Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"GoT_train\"},\"id\":\"dd2c024a5da041cb89e12e7984c81359\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"id_to_word.json\",\"err\":\"no such file or directory\"}\n0 | 2016-07-07T21:56:34Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"GoT_train\"},\"id\":\"dd2c024a5da041cb89e12e7984c81359\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"word_to_id.json\",\"err\":\"no such file or directory\"}\n0 | 2016-07-07T21:56:34Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"GoT_train\"},\"id\":\"dd2c024a5da041cb89e12e7984c81359\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"word_to_id.json\",\"err\":\"no such file or directory\"}\n0 | 2016-07-07T21:56:34Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"GoT_train\"},\"id\":\"dd2c024a5da041cb89e12e7984c81359\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"word_to_id.json\",\"err\":\"no such file or directory\"}\nWhat do you think? Keep using the log method, but disclaim the errors in the readme?\n. Thats what I ran with (ommitting the errors from the example output, but mentioning that you can ignore FUSE errors)\n. I think this was a line for debugging the test. I don't think we need this anymore for the purposes of what we're testing.\n. While we won't return it, we may want to know if we failed to recover. If the user knows its in an undefined state, we can instruct them to at least check the state.\n. This comment looks like a copy/paste typo\n. I do know you like to have strings of the same length :-P but if so, then I guess you don't need the '2'\n. Nitpick - should use consistent whitespace. Looks like we use 2 spaces mostly in the file.\n. If we can't recover / the state is undefined, it might be useful to report this to the user. \n. Looks like a debug line. Do we want to keep this in here?\n. I think you just want to return this value. Otherwise if it errs, we don't report it\n. Do we want to follow the new paradigm where we also provide a log statement at the beginning of the function?\nIf I understand the get logic correctly, if its a cache miss the getter could try to read from obj storage and hang. So, it may be useful to instrument here as well w a log line at the beginning of the function.\n. We should provide a way to enable/disable. How about (per your comments :-P) provide size as a config flag?\n. I don't think you need the backtics here\n. I think this flag makes it so that if any line returns non zero the script exits. Thats a little funny since you're handling the first command's return code explicitly, and the second one (on line 10) you probably don't want to return immediately (because you want to print the compile error on line 11).\n. Would like to see some tests for this (e.g. what happens if kubectl DNE or cannot connect?), but thats out of scope for now. Lets add these cases to this issue so that we revisit it once we have the test harness: https://github.com/pachyderm/pachyderm/issues/710\n. Ok, thats a good point about verbs, but make pachd-binary sounds too generic for the use thats intended here ... it seems like I should use that make task somehow when building pachd images.\nHow about make pachd-profiling-binary instead?\n. Yup, good points all, I'll make those changes\n. We should just get rid of those comments altogether. \nFWIW - \nThose comments were way back from the cleint/server reorg during the vendoring refactor. The comment alludes to the fact that pfsclient.APIServer is a bit silly ... we have to namespace all the server itnerfaces under the client namespace for vendoring purposes ... but to someone just reading that code ... its not the clearest thing.\n. Good question. It is a bit wonky. I remember we discussed this a fair bit at the time, but I can't remember why we went w interface over structs. I'll convert to just structs.\n. Looks like you did this\n. Yup! I'll make that change\n. Hmmm ... this seems a bit overkill on second consideration. We have good coverage for list-commit already ... so there's no reason to deal w stdout parsing for list-commit. I'll change this to use the API directly\n. Ok I fixed this.\n. Oh, I branched off that branch (before it landed) because I needed the TF example working before I could make a test for it. I need to merge in master on this branch, and then the diff will reflect the right things.\n. Hahaha agreed\n. Ah, forgot about these. They're gone now\n. I could read it, but I'd have to parse out the JSON. Additionally, I do actually need to change the pipeline a bit (to point it at HN not wikipedia).\nWe discussed adding another layer of checks to the test in the future where it behaves similar to the assets.go test. It would check that the JSON strings used here match what we see in this file, and otherwise fails the test. We'd have to handle the case where the JSON deviates on purpose (probably parse the JSON in the test, change the field, emit the JSON, then compare?).\nThis is a bit tricky. So for now, this seems adequate.\n. Aha! Yea this felt weird. I'll make the change\n. Ok changed, and passing locally\n. True\nI'll detect the current commit ID and interpolate that into the URL\n. Eh, wasn't that hard. Added it while I was waiting on a different build.\n. I see. We can make that change if you'd like. (You're right it is a bit annoying but only if you're modifying this test, or the corresponding JSON file. In this case CI will do the right thing / or you can push to your branch as you develop if you want to run tests locally )\nMy goal with this approach was to make the test as similar to what a user will actually type. We don't currently have CLI test coverage and so I don't think we have coverage for PutFile from a file or URL.\nBut we do have an issue for CLI coverage.\nSo, I'll leave it up to you. Would you like me to change it to reading the file locally?\n. Cool. I'll make the change\n. This doesn't seem to be used to repeat the benchmark.\n. I like we're printing this to stdout. If we want to record results we can just pipe to a file and also have the info about how the benchmark was run.\n. This is a nitpick, but does affect readability of our PRs.\nIt looks like your editor removed trailing whitespace from a bunch of lines. That makes this diff quite a bit more noisy. Can you re-submit without this change? \nMaybe its time for us as a team to have some conventions around a few editor settings (wrt whitespace / tabs v spaces).\n. Nice, like the helper. \n. I like that you're validating that this test stays in sync w the documentation. However, after discussing w JD I went another route to guarantee this for the other tests. Instead of having any manifest in a few places (test, readme, separate file), we opted for just a separate file. I also had to update the readme so that it told the user where to look for that file.\n. If you want to check that it was written correctly, I'd also check the value of the buffer\n. Even if you're not using the returned commitInfos, may as well make sure FlushCommit didn't err\n. Seems reasonable. I think the doc should really inform us how to configure our editors so even when editing english prose we don't end up w noisy diffs like this.\n. Yup! Much better. Done\n. Yea ... forgot to add this. Done\n. Maybe goes without saying ... but lock statements should be followed directly by a deferred unlock statement ... unless the structure of the function prohibits this.\n. Wording is funny ... I think you mean:\n\"All new packages and most new significant functionality must come with test coverage\nAvoid waiting for a short amount of time for an asynchronous thing to happen (e.g. wait for 1 seconds and expect a Pod to be running). Wait and retry instead.\"\n. Yup!\n. Cool, change made\n. Ah, right because we defaults are zero not nil.\n. Those concerns makes sense\na) it would be pretty straightforward to use a goroutine for the code that's actually making the request to segment\nb) since I'm resetting the counts per user (not collecting cumulative call counts), I think I do need a channel just for resetting the batch of objects ... otherwise there could easily be overcounting\n. Yes there is -- its a bit below -- client.Track()\nThis code isn't hooked into anything yet, but in the code where we see a user for the first time we would call this. I think its most likely fine if we call this too aggressively (if they've already been reported), which might make the code simpler\n. This is because of how the tracking request works. The code we use to report the cluster metrics also provides an AnonymousId field. We need to provide an ID of some sort. In the cluster metrics case this is a cluster ID. \nFor the users, we use the same client.Track() method, but provide a user id. \nThis will allow us on mixpanel to keep these sets of properties associated to the right objects. With a user, I'd expect to see their counts of API calls. With a cluster, I'd expect to see the total usage stats. \nAs far as I can tell, there's no way to report multiple users in a single tracking request, so we'd have to associate the cluster statistics w a single user. That feel's pretty wrong. We should however, definitely report the cluster ID as one of the user's properties (this is unimplemented yet) so that we can associate user behavior w cluster statistics.\n. We discussed this offline, but to capture that discussion. \n1) In this model (batching the requests ourselves):\na) we'd want to batch the statistics (you normally report thing X happens Y times within a static interval) which isn't a good use case for a DB\nb) we'd have to do an extra insert into a useraction table for every single action a user could perform. This would add some complexity\n2) Channels work well for this, and I was able to simplify my implementation based on the feedback\n3) But I found segment's library will do the batch reporting for you (asynchronously w channels), so I can make the code muuuch simpler and rip out the little channel code I did have\n. Actually, interface{} defaults to nil, so you do need to initialize it. But in any case, this code is ripped out now (segment does the batching)\n. I was able to simplify this so that I just had a channel for incrementing. You de-facto get the 'lock' on the map based on how the channel select was constructed.\nBut again, this code is gone now.\n. I simplified the name to just identifyUser() ... which can be called as many times as we like, but does need to get called before any Track() calls\n. Yea, I like that\n. Yea! Now its just two. This should be changed everywhere\n. Good question. A few reasons:\n1) I need a way to tell if metrics are enabled\nWe have the appEnv.Metrics value in pachd's main function, but we need to check if metrics are enabled when the tracking methods are called from each API function.\n2) We want to re-use the segment client (since this is what does the batching for us) \nWhat I can do is break those out into two global objects so the rest of the 'reporter' isn't global. That's easy enough (and the client itself used to be a global). While thats technically more global objects, its much more concise about what we need to be global, which I think is where some of your concern is coming from.\n. Hmmm ... I do have a 'metrics' flag on client as well now. Let me take a look and see if there's a good way to consolidate those\n. No, I think we need both of those metrics flags.\nWe need the flag in the client -- because as a user who specifies the '--no-metrics' flag, the client needs to know when to send the metrics data or not.\nOn the server we need it ... to know when to send things to segment. And there is a world where the person who deploy's the cluster enables metrics, but an individual user connecting to the cluster chooses not to report their metrics.\n\nAnyway, so I'll make that client + that flag the global items instead of the whole Reporter object\n. I see, yea I can pass the reporter through to the pfs/pps servers.\nThe other place we had the metrics flag is within the client, here: https://github.com/pachyderm/pachyderm/blob/metrics_unique_by_user/src/client/client.go#L43\n. Nice catch. Every bit simpler helps\n. Nitpick -- If we only use $VERSION in the commit string ... I don't think we need it. If someone fails to set it the message reads 'Update version and run make doc for  point release' which is not good. \nBut if we make the example string \"Update version and run make doc for VERSION point release\" it is at least obvious that they forgot to add it and can run git --amend\n. Yea, I'll make this update. At the moment, it won't run in parallel (my understanding is that only happens if its flagged that way .... e.g. see here ) but I agree in general we should design our tests to run in parallel\n. The returned value is used in the defer function. I can re-name it reportFinalMetrics to make that clearer\n. Good point. Added lion support.\nSeems like they're all equivalent:\nhttps://play.golang.org/p/zDsEx-cTLr\nSo I just went with %v (since that's my goto). This is repeated in the job() helper ... I could break out the PodSpec definition into its own helper as well.. Well since its a requirement for make test how about I list it there (after docker-build) as a sub task?. I think I had it that way, but then it conflicts w the jobOptions struct I define a few lines above. In this case what's the best convention?. Cool, added that and rebuilt protos to propagate the changes to the pb file.. Cool, makes sense. Yes! That would be unfriendly to say the least.\nIn that vein ...\nDo we want to allow someone to delete a job that belongs to a pipeline? Under what conditions is that helpful? What happens if you delete a job belonging to a pipeline while its running?\nWouldn't the pipeline reschedule the job? That might be an interesting way to force a job to re-run (if its laggy ... or perhaps you scaled up your VMs because it was being slow and want to run the job again so that its distributed across more VMs). Yea! \nI think there's more we can get rid of. It seems like k8s has an API to delete dependent pods when you're deleting a higher level object. For example here look at how I delete the ReplicationController in deleteJob() ... I think we can use the same thing for k8s 'Job' deletion instead of the pod deletion code below that. But I'll play w that in a separate PR. Ah, yea doing the job restart only once a VM gets cycled is goofy. I like that error pattern. I'll implement that plus a test.\n. Looks like we can remove this comment too. Instead of 'in Pachyderm's data versioning' can we say something like 'and manipulating versioned data sets using Pachyderm'. Maybe this blurs the line between example/blog post. But I'd really love a visual of the DAG ... then where the jupyter service 'attaches' to the DAG. Also ... I think it would be helpful to have a screenshot / example graph here w the dip circled. Again .. this blurs the line between blog post / example. Might be worth stressing this a bit more. \"We used a scipy notebook ... but you can attach any libraries you need by using whatever image you're already using\". Maybe make a note here on how to connect / troubleshoot the connection. Could link to another doc even. But this may be different (or may break) if you haven't forwarded ports, or if you want to connect directly to the node via an IP address.. I would lead them further, even if its just an example.\nMaybe you dont need to hand hold for every line of the notebook, but I think the punchline is equally important here as in a blog post.\nWe want to answer the question \"Why did we have a dip in sales?\" and the follow ups, \"Was there a cause? or was it an anomaly?\"\nUsing the weather data, we can answer that question. Right now the example leaves it dangling. We need to show people you can answer historic data / debugging questions like this with this setup. By proving that, we are promising them that the time they invest in setting up pachyderm will pay off. Without that promise, its unclear why they should spend the time.. Typo ... I think you meant 'Pipelines and jbos that have ...'. Yea --- those lines had to swap out 'protolion' for the new logger 'log' and that was part of a merge conflict fix when merging in master.. These changes should be equivalent, right?\nI don't mind the change, but all things equal, I'd prefer a cleaner diff.. Why does this need to point to the copy in your docker account? Generally I'd prefer people be able to build it and use the local docker image. If for some reason we think they need an external image reference (e.g. we think building the docker image is too hard), then it should be under the pachyderm account.. It sounds like there is a race here. By removing this and letting the user run the make launch-jupyter task we haven't fixed the race. It sounds like your working theory is that there was not any data available. Can we add a flush-commit command in here so that it blocks until the data is ready, then create the jupyter job?. Ah, got it. I agree - let's not automate this. It's unclear what caused your port forwarding issue but the succinct bug report for the invalid service is helpful. Yup that's correct. :-P computers are hard. Fair question.\nOriginally I set it higher than a block for two reasons:\n1) the MaxMsgSize includes all of the payload of the grpc request including headers, etc (so if you set it to exactly the max size of a block and you tried to put/get a block of this size grpc would hangup)\n2) we don't strictly enforce the block maximum. After the amount of data written to a block exceeds this size, we cut if off. Each 'write' is determined by the delimiter type. So for instance we could have a pathological case where the last line written itself was dozens of MBs. I created an issue for this case in #1280 \nHowever, the change that just landed (where we buffer our writes over grpc) should guarantee that we never put more than MaxMsgSize/2 data in a putfile request. And skimming the server's GetFile code (and verifying this by testing), we write to the StreamingBytesServer in chunks of 4MB (bufio's default buffer size).\nSo. If we want ... we could really make this any value over 4MB that leaves 'headroom' for the message headers/metadata.\nI can make that change in a separate PR if you'd like. (Also I think that the pathological case above - #1280 - is invalid now that we buffer our writes). I'd like having it decoupled now that it's truly not related.\n. So before that buffer change, we realized that in some cases we were writing a whole block at once. This had to do with the fact that the implementation of io.Copy uses some tricks and doesn't actually copy under the hood if it doesn't have to.\nSo in the case of test cases where you were using a bytes.NewReader() for instance, io.Copy would do a single write() call w the whole buffer. This amounts to writing the whole block. If the reader was an os.File I believe it was written in smaller chunks. But it depended on the reader, not the writer. So the buffered write fix enforces that we write in small chunks all the time now.\nHopefully that clears that part up?\nSo before this buffered write change, a single block could be written all at once. (At least in a common way we were writing tests) This is why blockSize was conflated with MaxMsgSize. I agree they shouldn't be. And with the code we have today, they are no longer functionally coupled at all.\nI've created an issue to decouple these values\n. Cool. Updated this. Good point! Updated this and added another 'sugar' constructor so less code changes. Ok, done. Oh interesting. That's good to know. I spent some time looking through our GetFile code to see why the same problem wasn't happening on the other end. The short of that is it looks like we're using bufio, so we're probably writing in 4MB chunks again. I figured that's why it was working. Good to know that the setting only applies to incoming though.\nI had already made this into a separate PR. I think that's ok ... since really this change should've gone along w the buffered PutFile PR. And we can make sure that PR lands before this one does.. Grabbed this from Andrew Gerrand's rec way of doing this:\nhttps://talks.golang.org/2014/testing.slide#23\nWe also use this pattern to test some failure patterns of the deploy command here:\nhttps://github.com/pachyderm/pachyderm/blob/master/src/server/pps/cmds/cmds_test.go\nThe reason that I need to have this level of handler here is that the deploy command exits 1 if the callback we define on the deploy command returns an error. It seemed easier to use this type of harness than abstract those functions in a way that I could call them for testing.. Turns out no -- this was a different bug. One that happened manually (by human) and via a bug in my deploy script. So I'm closing this PR.. Just supplying the struct results in the following log line:\n2017-02-27T18:06:47Z INFO  &{pfs.API putFileObjHelper file:<commit:<repo:<name:\"debug\" > id:\"j\" > path:\"test\" > file_type:FILE_TYPE_REGULAR url:\"s3://tests3recursiveputs/test\" recursive:true  529.741697ms <nil> test/29388.txt test/29388.txt}\nI think the protolion package automatically does the marshaling but then I'd have to back the struct w a proto object. I almost could've used their rpc error method except for this new log type I needed a few new fields -- the filePath and objPath` info.\n. Ah, cool. Yea I like that. I don't think this will pass the linter (and will fail CI) - I think any exported const's need a comment above explaining them. comment typo, i think you meant 'by concurrent'. After discussing, its simpler to not have it safe to modify the capacity in flight.. Cool. Yea I almost did just that, but glad it exists somewhere. I like that, I'll add it.. It seems like we are handling errors / defaults differently here for mem/cpu - is there a reason for that?. Personally I think this name is confusing ... I'd just name it getResourceRequest and its ok if it includes default behavior. But that's just me. This is a nitpick, not a dealbreaker.. Yea I don't think so (https://github.com/kubernetes/kubernetes/issues/1899). The only thing I could think of is maybe doing the equivalent of kubectl logs job/someuuid -f and block until input is received? But I'm not sure that'll work either. This seems ok for now.. Cool. Yea we want this to be shown for inspect pipeline and inspect job. So I think the pachctl output commands need to be updated to print this information as well.. Looks like we upgraded to a newer go image. Let's make sure we keep referencing the image by a version tag, even if its the latest tag.. Cool. I like that this is explicit and separate.. +1 --- thanks for distilling the k8s spelunking down to a comment. We could. But I like the convenience offered here.\nShort term this is sugar to make it more convenient for us to use / test out the latest UI. The UI is still in alpha so there could definitely be breaking changes ... buts its alpha ... so I don't see that as an issue. Nor do I think that we have a short term need to support the UI on an out of date version of pachctl ... we aggressively ask users to upgrade anytime they see a bug.\nLonger term (once we're out of alpha/beta), we'd probably want to use the /stable path in lieu of /latest. I agree once we're out of alpha/beta we need to agree and adhere to versioning practices that will allow a user w a given version not to accidentally install a UI that has breaking changes. The preliminary idea here was to make breaking changes per minor release (which is why we use the major/minor version in the url).. Yup, will do. I disagree, but perhaps our preferences or goals are different.\nI want a tool that stays out of my way in the most commonly used cases (in this case, upgrading). And I want a tool that is easy for our less technical users (who are the ones that need the UI the most) to use.\nI think it's a pretty common practice to host something as part of an install. I see rvm, brew, gsutil, and gcloud all have one liners that curl a thing and pipe it to bash. I'm pretty certain I've seen the URL-just-to-get-the-latest version pattern as well.\nWe could provide the one-liner you suggest in the docs which would be easy for anyone to copy/paste. However, if we find ourselves using (and recommending) this method the only real difference is putting it in the code or making it a convention (having it in the docs).\nI argue that having it in the docs is less convenient to novice users, less convenient for me, and is no more robust than having it in the code.\nI get how this couples us to another service (github.io). I don't think the site being down is a common enough case for us to worry much about it. In this case the code defaults to the value hardcoded. If we provide the one liner via the docs (and that's the recommended method for users), it doesn't make it work better if github.io is down. In fact it'd probably return a confusing error to the user.\nAnyway. That's my argument for the feature as-is. The choice is document the one liner in the docs or keep it in the code\n\nOR\nI realized we have a third option. If I update the release process on the UI side to always additionally tag and push an image like pachyderm/dash:1.3-latest (and we only update pachyderm/dash:1.3-stable when doing a point release) then we can just keep the flag (for debugging) and avoid the URLs altogether. \nI like that best because then I don't have to upgrade the release script to be smart and try to update those hardcoded image values. And that is a really sane and easy default for novice users, both for a standard deployment and upgrading their UI.. Hmmm ... I like that third option but don't like the label name. That'll be a bit confusing as we have a totally separate semantic version of the ui. Maybe something like pachyderm/dash:pachctl-1.3-latest ?\nThat's a little goofy.. Ok, since we want this to be practical short term, I'll just run w the flag for today. Beyond that we need to think on it.. Interesting ... it seems like you're running this as a docker command and poking the command through to the root filesystem to get at the script above. Seems like this docker run ... one liner is getting complicated ... would it be easier to just make a docker image that contains the script to startup k8s?. Neat!. We may think about starting a style guide for shell scripts. If we're combing through this, let's remove this line. We need to handle the location constraint properly if we're allowing the region to be specified as a flag. Also think we need the location constraint applied here based on region. I think we need the location constraint logic here too. Nope, was just using what other files in this pkg were using. Updated this and obj.go to use lion as well. done. Yup, I forgot to return connErr outside the retry functions. Beyond that, this version of retry doesn't use the notify function as you describe, but I've made a separate issue to standardize our usage of retry / notify: https://github.com/pachyderm/pachyderm/issues/1780. done. If these are running concurrently, a global kubeconfig file won't work. I think this should still be NUM_NODES so that we define that in a single place (its defined above). I just ran into this myself actually ... I changed NUM_NODES but here in the script the ready function expected the wrong number.. Seems like you just moved this to the bottom of the file?. Nice! That one was bugging me locally a bit yesterday. So to be clear -- it is our intent to run only the stuff directly under src/server ... basically the integration tests? I think that makes sense because the integration tests are the ones most likely to use / test the cloud deployment. I just want to be clear on our intent.. Now that I've read up on getopts I realize no-metrics shouldn't have a trailing colon. Will fix.. I believe this was a typo. It does work, but I'll make it consistent. Because of the incantation, this line will err instead of continuing. I'll just add some print statements that will serve as emptiness checks. I chose to leave this sed in place because there are a bunch of substitutions within that file. Yup! That landed in the last PR, so we're good. . I like that. Oh haha. Yea I wasn't sure but found a different link that was .md so ran with that. Now I'll have to find that link and fix that one too. Sure. I'm not sure what you mean?\n. There are two subtle things here that are different. We need them to pass an additional flag (--use-cloudfront) and we need them to save the output (for a future step), hence the redirection. So I think it's ok to keep it here.. I haven't tested this pathway myself. I think I saw some stuff that suggested that the nvidia-docker stack didn't play nice with k8s for some reason. There's not a lot of data, so those GH issues I'm thinking of could be out of date. I guess it's up to us how confident we are in recommending things here. Maybe we mention this is untested or YMMV? Idk.. This line seems repetitive. Maybe we say something like 'Create GPU nodes by using kops to create a new instance group...'. Also. This wasn't in the original docs, but I've realized since. If you upped your rootVolumeSize (and set the rootVolumeType in your other instance group, you should do the same here. In the absence of GPU jobs, normal jobs could get scheduled on this node, in which case you'll have the same disk requirements as the rest of your cluster. We have no way of setting 'disk' resource requests, so we have to use a convention instead.. Note - if this fails w a message that tells you the 'fields were not recognized' it means you need to upgrade kops (unless there was a typo). We should probably add this troubleshooting tip here.. You may want to link to the new section in my troubleshooting guide which covers 'I dont see my GPU node' ... but maybe it makes more sense to add that link in that PR. Whichever one lands second.. I'd put the command to get the node here kubectl get node/thenodename as well to be explicit. Oh haha ... there it is a line below. I want to get specific here since there's a few similar hurdles here. There's the one where the node never appears. That's probably the instance limit thing that I mention in the troubleshooting docs.\nThen there's this one. You see the node in the AWS UI, but not via kubectl get nodes ... then the recourse here is applicable. Let's make it a bit more specific what the symptom is here.. Yea, I was looking for a unique identifier to provide in the logs so that you could tie it to a job/datum ... but now that I think about it that's kind of the whole point of the logging interface we have setup (it adds that metadata). I'll remove the environ / inputs from the calls.. I'll remove those. Relic of code paste / how it's done in pfs/pps api land.. Yea ... but that kind of makes sense. The getTaggedLogger() takes a processRequest object ... and those logs get metadata associated with running that exact job/datum.\nThe Status() call contains the datum in the response ... so tying it w the metadata for get-logs is redundant I think.\nI guess the question is would a user want to see these logs (via get-logs). I think they might want to see that it's Cancelled. I'm not sure they need to see any Status() calls ... I think that means you might see more log lines anytime the status is requested.\nI opted for the empty logger because it seemed like the simplest change. If we want Cancel() to be discoverable by get-logs I'll need to decorate the server struct w the datum info from processRequest. I think in that case I'll want to put a sync around setting/getting those fields as well (since the worker could get a new datum to process at anytime).. Yup! But it's not present anywhere in the logs right? The goal here was to make it easier to read the logs and notice restart boundaries. This way I could even search the logs for a certain run of the datum.. Yup, I agree there's redundant stuff in here. I'll go through and remove those bits. I should've done that before asking for feedback. My bad.\nHowever, I do think there are some net new things in there that are helpful:\n\nhaving the timing logged for the downloadInput / runUserCode / uploadInput helpers I think is helpful for us. Yes we could look at logs and look at timestamps ... but wrestling w log interfaces (I'm looking at you GKE) makes this tedious and difficult\nalso in my experience having individual timing (for helpers) and a total timing (for the overall call .. in our case Process()) is invaluable for detecting bugs introduced in other parts of the code / the overhead of the larger call ... I've seen this pattern a lot and like it\nI agree ... having egress logging / timing is helpful\nhaving the log pattern for an API call started / ended can be very helpful ... this would've saved me a day or so debugging w a current customer (just to realize that the request is being dropped in between the worker Process() call ... and the jobManager waiting for the response. I would've seen that the single run of jobmanager for this datum ... on this retry ... had a starting log line but not an ending log line)\n\nMore broadly ... I agree some of this is mostly just going to help us debugging. I'm not sure why we're concerned about making the logs too big. In cases where I'm debugging now it's usually backed by a logging service. If we're worried about the noise ... then it's time to use log levels. I'm happy to add some of these in on the `debug' level only.\nIt's important to make it easier for us to debug. I think we've all discussed how we want to decrease turn around time for customer engagements (pocs, issues, or otherwise). Better debug tools for us makes that possible.\nI clearly need to do another pass at this PR before it's ready for review. I'll ping you again when it's ready. But I want to hear your take on the above.\n. typo here 'connecto' . typo here (completse). You'll only see this if you're using kops. So I think you can remove the bit after the 'or' clause. Note: if you're using a log aggregator service (e.g. the default in GKE) you won't see any logs this way, but will need to look at your logs UI (e.g. in GKE's case the stackdriver console). Nitpick ... but I'd change the ',' to a ':'. This advice should probably go in a section of the docs that's proactive in addition to here (where we're reacting to the problem). Maybe under the data management section? Basically they should plan to deploy their cluster with enough root disk size for all inputs/outputs of all simultaneous datums that will be running. (And we can provide some math there ... e.g. 10 pipelines x 20 datums each x 1GB per datum => 200GB+ volume size is needed ... and other examples for lazy pipelines / etc). If you want to do that as a separate PR, that's cool, but let's make a GH issue so we don't forget.. There's also the possibility that your credentials have expired. This just happened to me. In the case where you're using GKE and gcloud ... to renew the credentials do:\n17-06-21[15:23:15]:stuck-job:130$kc get all\nUnable to connect to the server: x509: certificate signed by unknown authority\n17-06-21[15:23:29]:stuck-job:0$gcloud container clusters get-credentials my-cluster-name-dev\nFetching cluster endpoint and auth data.\nkubeconfig entry generated for my-cluster-name-dev.\n17-06-21[15:23:45]:stuck-job:0$kubectl config current-context\ngke_my-org_us-east1-b_my-cluster-name-dev. I'd like to keep the 'started' and 'finished' log lines. I find this helpful for debugging. And for instance here we'd see any calls made when the worker was busy.\nIf you prefer, we can move these statements below the worker busy message where the Received request log line is and replace that log line w this pair.. Ok, seems simplest to remove them both.. I know this one seems a bit redundant w the logs that are within the Process() command ... but I'm seeing this issue w a customer where the worker Process() call completes ... but we don't get the response. These logs will help me identify when this happens (it's intermittent).. :+1: . This is a start ... but could still lead you to set the rootVolumeSize too low ... e.g. if you have 10 datums getting processed at once (within this pipeline or across pipelines) you'd run out of space. This is kinda tricky. We don't have a great way for users to measure this right now (but this kicked off an interesting discussion about how we could present this on the UI) ... so it's a bit of trial and error. \nI think the todo here is just to make them aware that this is the total processing bandwidth for all their jobs that are running at any point in time. So do the calculation for each type of job, then do something like:\nfileStorageNeededPerDatum x numberOfDatumsRunningSimultaneously / numberOfNodes\nNote - the numberOfDatumsRunningSimultaneously could be within/across pipelines.\nThat's crude, but an ok place to start. It's reasonable to start w a value and up it later as your needs increase.\n. Function comment string needs to be updated. Ooops, yup. ok. makes sense. Works for me. And then pachd/main.go will specify the port to run the server on as well. One question about ports --- if the container port is 80 do we want the nodeport to be 30080 ? Because the dash UI already uses that node port.. I think that's still valid? This is only serving the get file API call over http. If we still want the tracing access we still need that port open.. Yup! Fixed now.. I double checked, and no ... the pprof API won't be served from the service we're adding. We do still need the separate ListenAndServe call. Bcz the dash uses 30080, we kept this port as-is. There's an existing datum message, so I can use that, but it has a hash which I don't think means the same thing as the hash that's generated by the hash of all files. I think the existing datum message is used in the logging codepath. I'll readup to see how I can resolve these primitives. We could also present total in addition to those 3, and upload/download as well.\nThis seems like the question of where to present the data, a discussion which we have often in the API. In general, I think we choose to let the user specify what level of specificity they need by using the API. Those fields are printed in the inspect-datum view right now, so I think that's sufficient.. I think in this case we should print a warning, but don't want to err, right? Otherwise, you'll never be able to run port forward bcz the process that it's trying to kill has already exited.. Generally I think we're putting any files we need for state under ~/.pachyderm ... as defined in the /src/client/pkg/config package. We should probably be consistent w that code and do something like ~/.pachyderm/port-forward.pid. Ok, this should be done now. The existing Datum message was renamed InputFile. I left this as-is since the inspect-datum command displays more info.. Ah, yup, that handles the case I mentioned too.. Ah, yup. Will add that.. Offline discussion -- the example output below though it only has 3 columns is quite wide. The hash in particular is long. It really should be shorter, but we've made that into a separate issue: https://github.com/pachyderm/pachyderm/issues/2113. :+1: . Makes sense. Fixed, per our discussion, and added the TestPipelineWithStatsAcrossJobs test. fixed. I'm not sure we can do that ... bcz non initialized ints are 0 right? (And the pages are zero indexed) So I can't tell the difference between non initialized page and page=0 (the first page).\nBut ... this did make me realize it needs to be an unsigned int. Hmm ... not sure what you mean. I need to remove some of the file infos before I sort them, and that is done whether or not the response is paginated.. Ah, I just saw this. I actually made the change and it wans't any additional casts really\nBut if you want int64, I can do that. I will have to add some error checking though (right now if you specify a negative page, I believe we'll panic). Using uint64 seems like an easier way to deal to validate the input consistently.. Look at that! The test was just waiting for you to add this case :+1: . Ok, after offline disc going w int64's ... but also we're using pageSize (a new param) w a non zero value as the measure if we should paginate. . Ah, you're asking to use the composite if statement style ... ok done. Oops, need to remove this debug statement. Yup, good call. Looks like there are a few debug statements leftover here. Debug statement. This is a bit weird. What is the case where there are multiple auth configs and we want to push images to each? I'm not sure what that means really. . What is the purpose of this make task? Seems like its good for testing, but it also seems redundant w the new pachctl deploy import/export commands. I think you meant to remove this. This doesn't do anything, right?. Typo here, 'valud' -> 'valid'. Same thing for pulling the images. As far as I can tell the authconfigs are separate username/passwords? (authenticating across all users seems bad). But if there's more than one for the same person ... presumably both logins won't work (they might be for different registries?) ... in which case this code would always perform the action (push), but since one fails return an error.\nOr maybe I'm missing something.. I see. Yea, I think this is fine. This was a small unrelated change. Basically this omits the large output from make proto so that you can see errors. I've been bitten by this a few times, so it seemed handy to include. If we want to omit it, that's ok.. This change doesn't need to be here ... let me fix these.. This seems like a weird merge gone wrong?\nAFAICT, on master it looks like the right hand side. See here. The library I used to handle the webhooks supports bitbucket and gitlab but for us to support these we'd need to add those handlers to the githook server. Since there was no ask for those, I didn't add them. \nThat said, I can change the name.. I'll change the name.\nThis helper is used in a bunch of places in this PR. Yea, the caller could check, but this seemed DRYer. Also to note ... it seems like it could've taken the input.Git as the argument ... however this is also used on the worker side where there is the worker input type where this wouldn't work.. Ah oops, thats a leftover from where I made my own recursive walker before I found the VisitInput function. . No, it works, because I was careful in not using inline if / error statements, so it wasn't shadowed. Here's an example of how the code is written as-is: https://play.golang.org/p/XcgwDbm39W\nThat said, it does feel brittle. I felt gross in particular writing the bits around line 111+\nIt's not often in golang, much less our codebase, that we have functions that don't return an error and that we want to capture any internal errors for logging. So yes, I agree it's a novelty.\nTo be clear on your suggestion -- you're saying wrap the entire function body in an inline function that reports an error so we can more idiomatically return errors? I like that. Though inlining it seems a bit silly (I don't like nesting if I can help it) ... I'll probably make a non public helper function but do the same thing.. Yea it's an odd quirk for sure. And so bad to rely on. I'm glad we're cleaning this up.\nI like that method of inlining, so I ran w that.. Seems like this is a debug statement left in here?. Ok ... I was trying to figure out what was wrong ... didn't realize GH was smart enough to do syntax linting on json code snippets ... thats nifty. This was a bit more idiomatic ... we want this info when tests fail. Was helpful during my debugging.. Ah, yup. Good catch.. Is this line redundant? It seems like this operation is done on line 244 either way (after the alpha/beta suffix is removed).. Typo here, will fix. I split this up to ameliorate this issue\nThere's also a known issue w cmd.Run where if you're doing IO not to a file descriptor (among other edge cases) cancelling the context won't halt command execution. So for now, I put the command in a separate channel so that the context timeout is respected. The zombie killing solution mentioned above will be added as another iteration.. Yea, dumb mistake. So what I recall from testing is that without code like this, cancelling the context (or putting a deadline on the context ... which calls cancel at the right time) doesn't kill the command.  This is what is described in this issue So if we just called blocking Run() the timeout feature wouldn't work. That was the impetus for this restructure. We wait for the execution to finish even if the deadline is exceeded.\nWhat we can do is in the select case where the context is finished, call cmd.Process.Kill() which does its best to kill the process, and would make that goro complete. I'll double check this is the case. I thought as written it does kill the process ... but I'll double check.\nI can make a job that keeps appending to a randomly named file in /pfs/out ... then using kubectl exec ... ps or just catting those files ... I should be able to verify that the process is dead.\nThis won't handle zombies, but that is already the case on master.. Do you mind putting this line back? Just makes the diff a bit nicer.. Cool makes sense. Nifty example.. :+1: ok done. should be inspect-pipeline. Ah, nope.\nThis is the k8s object that is a k8s svc that exposes the 650 (grpc port), 651 (go trace port), 30652 (the http get file port), and finally 30999 (the githook port). So it's being added to the right k8s object.\nThe change made in this PR is to use the githook.NodePort() helper since I wanted to make all definitions of the git ports in one place so they don't get out of sync (insted of using the loose convention of just adding 30k to the target port).. So when a pipeline is created we create the service if it doesn't already exist. On pps master, we do the same thing, which will cover cases of a pipeline getting updated or a cluster getting restarted, in which case it'll make sure the service gets started.\nI thought we had a reason for putting it in both places, but now that I think about it, I think you're right ... we don't do k8s ops in pps server, just the master. I'll update this.. Well ... there's also the case where the loadbalancer hasn't finished being provisioned, in which case k8s won't report the ingress URL yet, so in that case it really is pending. Really ... if we had a way to detect that you're on a local deployment (not a cloud deployment) we should not even try to provision the githook service. \nAs is, we don't really have a way to distinguish the local deployment from cloud deployments.. Yup, will do. So let me make sure I understand this bit. You're putting the mu.Lock() here outside the goro so that the subsequent pod will block on the lock to get its logs. This guarantees that the logs come back in a particular order. But we want to keep the goro around because of a) the follow case and b) convenience / use of the err group. Ah, I see you're referring to the comment. Yea, that's not a great description. It is not a 'pachyderm service'. It is a k8s object that will allow us to get a public IP/domain for the service on cloud deployments.\nI'll update the comment.. Yup :+1: makes sense. Why this change? It seems like the ScrubGRPC function should leave the error alone if it's not a grpc error w an error code. Do we want to display grpc error codes here specifically?. Nice ... I think that's the last instance of this lib reference. That's correct. I will update the comment. Actually while I'm at it ... the existing scratch prefix puts some redundant namespacing in there (something like /pachyderm_pfs/scratch) that we don't need now that these keys are namespaced because of the collection.. Sounds good :+1: . Close. If you look at line 2010, I'm actually removing a level of indentation. That's because a single putFileRecords can have a tombstone and have data to be written (whereas before it was an if/else). But yea ... didn't really change anything in here except for the indentation.. Sounds good :+1: . Yea, so the way the fromKey works is that when you specify that in a query, it returns that key as well. So if the previous 'page' ended on the key that is actually the last key, the subsequent query will return 1 result.. Similar deal ... because etcd returns the fromKey in the results (and the previous page has already provided that data through the iterator) we want to move past that key in the results we return via the iterator.. I did not. I think this is analogous to the previous incantation of the code, seen here\nBut I can double check this.. Realistically, I think we're ok.\nSince go uses UTF-8 the maximum value would be the \\xff value (which is a non printable character).\nAs long as users name their files sane things (within the ASCII range), even if they use the max printable ASCII character (which is ~ or \\x7e), incrementing this value is a valid byte that will serve as a valid endKey.\nThere is however, the matter of what a user chooses to name a file. Right now we only make sure the null character isn't in the file path. So in theory a user could use non printable UTF-8 bytes in filenames, and if they supplied \\xff then yea, this would overflow and cause a problem. We should probably just validate file paths to be printable ASCII characters\n. Capturing our discussion offline ...\nReasoning about this code is tricky. From manual testing w etcdctl its hard to check a revision of a key. So instead we opted to check that the value here is non empty and the commit ID matches the current commit. Really that's the simplest check we can think of to guarantee the value exists (and isn't an empty struct unmarshalled from an empty string).. The new vetting as part of go build complained / err'd out at these lines. Basically it said something like 'mismatch types string vs indexCol` or something like that. One of the statements was missing the output value.. We're not cleaning it out so much as specifying everything as something that docker should ignore. Since the code is mounted in as a host volume, we no longer want to upload anything to the build context. (This nets us at least a handful of seconds ... and in cases where your local directory gets cluttered, can save a fair bit of time).. Noted. Issue here: https://github.com/pachyderm/pachyderm/issues/2669. I think this is a copy/paste typo in the error message.. Makes sense. \nI'm not sure what the status of this issue is on the invariants branch, but if it is implemented...\nWhat do you think of basing this PR off of the commit invariants branch so that we can add a test that FlushJob (and really the underlying FlushCommit) returns the expected results when a job fails?\nI think if there were a 5 stage pipeline (as in this test) and the 3rd job failed, I'd expect FlushCommit to return after that job failed w 2 successful and 3 failed commits. I'd expect FlushJob to return (with an error) after that 3rd job failed w 2 successful jobs and 1 failed job. It would be nice to capture these expectations in a test. Especially since the recent customer use cases would use this API in particular to detect failed jobs.. Agreed, that is weird. Probably was a copy/paste error. Thanks. So it seems like this now tests the case where you just get back the pachd logs, right?. What was happening here before? Were we getting double newlines?. This is the bug fix, yea? Otherwise you'd specify the worker component and get no logs.. Makes sense. Do we have test coverage for these cases?. Yup, I removed this\n. That is true. I'm not sure what a good default here would be. Read the first value only and print a warning?. That sounds like the right thing to do here. Can we add that in? There's the notFound(...) helper below.. It seems reasonable that we expect pipelineInfo.Pipeline.Name to match what was actually provided in the request path (serviceName), but I'd use the thing that was actually provided. More of a style thing though.. Since you're in here, we may as well remove this debug route. I haven't seen the need to use it since we got the login stuff working, so it can be removed (as well as the actually handler below).. This suggests pipeline tokens need to be renewed. I assume pps master handles this? The lines below suggest only pipelineTokenInfos can be issued by admins. So we're issuing pipelineTokenInfos to users of the vault plugin? In the world where non machine users need access to pachyderm via vault, this may need to change.. I think these cases are flipped around. Why a panic and not a return here?. Since that condition in the comment is true, I'd omit the question mark. Why do we omit the return errors now?. What are the '*'s for? I guess to make the comment bold? Not sure if we do that by convention. I'd omit them. You're worried about the index values that have been written up to this point? I think it's ok. Since they're attached to a lease, and a lease has a max TTL, they'll get GC'd eventually.. Got it. Makes sense :+1: \n. Ah, I didn't realize we were caching the error. Makes sense. Not sure if this port is exported, but if so I'd prefer we use that and not hardcode any ports in the tests.. This seems like debug output that we can remove. I'm not sure where an s3 read happens in here. Per discussion it sounds like you're going to move this backoff someplace else.. Hrmm, at some point I was getting a funny import error. But removing it and it doesn't happen. So I've removed this line (and the corresponding cleanup line).. Alright. If we do change them, the tests will fail, which seems sufficient.. Is this debug output? Or something we want the user to see?. I doubt you meant to remove all this migration stuff. I think you just need a merge. So when I do kubectl edit ... and I don't make any changes, nothing gets updated. It seems in this code, even if you make no changes, we'll update (and perhaps reprocess) the pipeline depending on what flags are set. That might be a handy way to reprocess a pipeline, but it does deviate from the conventions I've seen elsewhere (kubectl and kops).\nWhat do you think?. I think you need to update the error string from pps to enterprise server or something.. Huh. I just read the code for the enterprise state. I see its cached, which makes sense ... you wouldn't want to issue an etcd query anytime the server needs to check for access. But that really only happens in the multi-pachd pod world, right? Couldn't you grab the lock / edit the value for this server, and that would be sufficient to update this pachd's enterprise state?\nI'm not sure if this is something we need to change. In theory we do want to support multiple pachd pod deployments. On the other hand, this method is self described as a testing only method, and our tests currently only use a single pachd pod.. As discussed, this will be removed. . Seems like this could be made into a helper like idempotentEnterpriseActivate(). Typo ... missing end paren. I don't think there's a test case for this behavior included. We should have a test for this change.. It seems like we're covering an edge case here, but I'm not sure exactly what. Is it that before the spec repo would've used non su creds to create the internal spec repo? Which could fail if ... ?\nIs it worth writing a test case for this?. I like that argument. I'm going w exportStats as we use the word metrics to denote mixpanel reporting.. Not sure how these annotations got added. Were these packages added but the metadata not recorded? If they're added, the metadata should be here. If they're removed, we can remove these stubs as well.. Agreed. This is an embarrassing mistake, and should not have made it to code review.. Yea, I made the change, and it made the diff smaller :+1: . Capturing an offline discussion.\nHistograms are the recommended metric type for stats like runtime. Histograms will automatically populate 2 metrics ... foo_count and foo_sum which also exposes the count as you describe.\nBut our current understanding of histograms is that they do have a max age / time window. So ... that count metric may not be true cumulative count over time. So I did go ahead and add counters for upload/proc/download times as well as download/upload bytes.. I agree. Chaining should've been a red flag. The code has been refactored to explicitly handle the errors (just emit them to stdout as warnings, since metrics is non critical code).. Ah ok. Yea I can do that. As a general practice I like to keep values that are used to initialize a struct all in one place when we create the new struct literal. Idk ... that just seems cleaner since that keeps the state defined all in one place. And so I didn't want to modify server after the fact. \nBut, obviously exportStats is not something that really has any side effects other than the ones I want ... so nothing really to worry about in this case.. Using the old timeout ... the tests don't complete for me locally, so upping to match CI.. It's certainly recommended to use snake_case, but it does seem to accept camel case ones. However, that means our names will be inconsistent ... e.g. pachd_GetFile_count since prometheus prepends the namespace and in some cases (e.g. histograms) appends the metric type (count, sum, or bucket). \nAlso ... it'd be inconsistent between this and other stats. The worker stats we report are all snake case.\nI get how adding a new dependency in here for handling camel case isn't ideal, but it seems worth it to have our metric names / queries be more consistent.. Yea :-/ ... I'll put in a mutex to fix that. Actually, seems like this is one of the 2 recommended use cases for the new conccurent map\nSo I'll use that instead, unless we have a preference against that new primitive\n. Just went w a mutex, seemed easier. Yea I was wondering how we handled goro's that we spawn and forget. I like this.. So, I don't initialize the value of reportMetricGauge unless the registration of that gauge w prometheus succeeds. If it doesn't, it will be nil. That seemed like the easiest way to degrade in case registration errs. Yea I like that. Yea it doesn't need to be a pointer, but interesting - you can't omit a field from a struct literal if you're not using named fields ... so actually relying on the zero value for mutex initialization makes the cacheStats initialization code take a few more lines. But that's fine, it's a bit easier to read that way anyway.. Nope. You're right it should be a field.\nI did leave it as a pointer because we create the groupcache in the obj server, and pass along the stats field to the registration function here. In general I avoid passing by value if I can help it.. Why is this in a backoff? Is it because it takes a bit to actually stop the pipeline?. I like the format you used on the next line, but either way, these are easier to read if they're consistent.. Not sure how to comment on a line that's not here ... but the comment on line 4267 I think is no longer valid. The initial garbage collection call + storing the original counts may still be useful though ... since when developing I may run this test as a one off (e.g. go test -v ./src/server -run=TestGarbageCollection) and expect it to work regardless of what is in my pachd cluster.. We should load in the etcdprefix from the default here instead of hardcoding. I think this comment was copy/pasted ... I don't think it really makes sense here.. So using this label selector matches all the k8s objects across pipeline versions and pipelines that are services? That's a lot simpler. We only pull in what we need for compilation (reducing the uncompressed image size by ~500MB). The docker install? It adds ~15MB and looks to be just the docker binary:\nroot@e309e844f3f9:/# du -h /bin/docker \n15M /bin/docker\nWhich we use in the actual compilation. So I don't think there's more juice to squeeze there\n. Hrmm yea good call. We could have a version file that the Makefile cats and the Dockerfile adds and then cats inline to get the download URL. Something like ./etc/compile/GO_VERSION. :+1: . Grepping the code base, it looks like we're providing this address in a bunch of test suites, while it's defined in the assets package, and the driver hardcodes the port (and assumes its there) as well. Seems ripe for a const definition someplace to keep this DRYer, unless I'm missing something.. I think this will fail the lint check. This is where we'll specify the number of buckets to split the PPS test suite into. We also have to explicitly enumerate the corresponding environment variables in .travis.yml\nI'm open to ways to keep this DRYer, but this seemed like the simplest implementation method. I know I was the one who put this in there ... and while I think its funny that a real GH user exists called daffyduck ... we should reign in any assumptions in our tests ... at least we could make the username one of us.. So per offline discussion - response LeaseDuration is the remaining time left on the lease. That's why we have bounds in other tests below. So here to keep this non flaky maybe we should check its <=2 and >0. Makes sense we want to measure a bound, but we don't expect 68 hours to have passed during this test ... so I think we can tighten up this value a bit.. I think we want to err if its greater than 10, right?. Same thing here ... I think the max we expect is 10. Where does 60 come from? If that's a default when no ttl param or config is specified, where is it defined? We should use that const in the tests if possible.. Nifty tricks! After much discussion though, we decided to leave it as is for better readability.. Good point, done. Yea this may be something we think about. And more generally ... maybe we want to start a bash style guide? I read something recently that stated something like 'when in doubt ... double quote in bash' and that will rarely be 'wrong'. Since none of us are bash masters, I kind of like keeping quotes in place, because it's more consistent (and there are cases where not having quotes are a gotcha) even if its a bit more verbose than needed in some cases.. :+1: . Why does this need to run in a subshell?. Is there a convention we can supply here to further mark it private? Maybe name it _pachClient ?. So what is the desired recourse? If on startup the driver cannot successfully dial we panic and force a restart? Is there a reason this isn't in a backoff (or at least backoff pending the network is retryable pattern)? . I'm missing something. Why is this no longer needed? . After creating a helper to mark the pipeline as stopped, we only used this helper (updatePipelineState) here to mark a pipeline as running, so I simplified the helper and named it as such.. This seems like the easiest way to report the PAUSED state ... since I'm not sure how to enforce an invariant on the etcPipelineInfo's state at write time.. :+1: \n. This arg wasn't getting used anywhere, so I cleaned up the function.. If we export the default, seems like we could re-use it here. I don't see the harm in exporting it for this reason. I believe we do this for other constants. It's a bit DRYer. I'm not sure why this declaration is needed ... it's about as handy as the underlying call? Also ... this method doesn't seem to be called anywhere.. It might be nice to differentiate btw the pachd pod vs worker sidecars. And if we could populate the pod name for pachd sidecars that may be helpful too. Unless I'm missing something that you'd get from the debug output that would ID the pod/container source? \nI could easily see a case where you have many pipelines, and you want to see which container/pod a pachd sidecar issue happened on. Without this info, you wouldn't even know which pipeline's sidecar had the issue (much less which worker in particular if parallelism is used). Most of the time we see pachd sidecar issues it has something to do w the usage pattern from a pipeline, so this seems helpful. That said ... I'm not sure you'll have the appEnv.PodName as you do in the worker. But I think we can string that through.. Reading more of the code ... it looks like you have some support for dumping by pipelines (via the pipelineRcName param to the Clients() method), but it doesn't seem to used by the Dump() method at all.\nIn any case, I think it'd be helpful to provide the pod name. Even within a pipeline knowing which pod is running which goro's would be helpful. I think it's likely we'd want to know which datums the worker was processing to see if we could reproduce the issue. By knowing the worker/podname you could get the logs to get the datum list. . Not sure if you want to ever provide the pipeline RC name here. That might be a useful flag for debugging.. Not DRY ... this is repeated here:\nhttps://github.com/pachyderm/pachyderm/blob/bd3b6c5bb57e98a9d327c360ab555ac98a3f232f/src/server/cmd/worker/main.go#L203. Fair enough. If we're not adding pachd sidecars, this is less helpful.. If there's a way to filter the output based on pipeline, then yea I don't think we need it. I assumed there wasn't since you the pipeline name you emit in the output comes from this value.. This is kind of goofy, but the alternative (incrementing for the other two delimiter types after doing the same error checking) is even less DRY I think.. A bit on the fence about this signature / name. Adding 3 more args seems heavy - we could just create a struct for metadata that includes the header/footer/size ... and then call this method PutFileWithMetadata . Discussed offline, but for posterity. I tried that lib as well as https://github.com/araddon/qlbridge but neither supported the SQL syntax we need. (Both barfed on COPY statements ... and qlbridge didn't like the pgdump format for CREATE TABLE ... either).\nOpened an issue upstream for this:\nhttps://github.com/xwb1989/sqlparser/issues/32\nSo for now, we're leaving our very dumb parser in place.. Ah yea, very good point. We definitely need to handle those cases as well.. What's the use case? For now I'm assuming that the input is a whole pgdump file that contains a single table's inserts. Right now we require both a header and footer, since that's what's needed to construct the table, etc. (Footer less so ... its just easier to require it for parsing reasons).. Makes sense. Fixed.. Makes sense. Fixed.. Ah, somehow I missed that. Yes, this is much much simpler now.. Indeed it did. This is now a test case (not sql, but the generic header/footer content case).. Why up the chunksize? Also the comment needs updating. Looks like opts cannot be nil, as the comment suggests. Not sure the scope / meaning of this todo ... is this something we want done in this PR?. So standard permis for file / dir are 0644 / 0755 [ 1 ]\nI think execute permission is needed to cd into a directory [ 2 ], so here we should at least specify 0755\nBut ... none of these are actually writeable right? So then shouldn't they be 0444 and 0555 ?. Woah, this is cool. I think this was a requirement for some python libs ... I forget if it was jupyter / tensorflow.. Bummer :-/ I remember this part of FUSE ... it's very frustrating that mounting doesn't block.. I think the linter may complain about this since it's exported w/o comment. You may also want to put it in the testing.go file ... which would be a tad DRYer for localBlockServerCacheBytes. Ah, I didn't know you could have a nil receiver. Good to know.. Sounds good. Made separate put-header and put-footer commands. I put it there to raise its scope -- we use the limiter below when putting the objects for the header/footer ... around line 2012. I thought about omitting the use of a limiter there altogether since its just 2 additional puts per putfile ... but figured it was easy enough to use the limiter that was already there.. Huh. Yea, it definitely does. I'd seen output w rows out of order during testing, but that must've been a bug in the way I was testing, because it's definitely ordered in the output. \nAnyway, this is much simpler now.. Yup, easy change. . So, as-is, the code handles the case where we traverse a directory w a header and no footer. If I only add the footer if its non nil, then this check needs to change as well. We'd still need to check if its non nil to make sure we handle the case where the stack is empty (otherwise we'd panic when traversing a directory only w a header). But it would save some space on the stack. So I'll go ahead and add this test case and make these changes.. Yup. Debug relic. Fixed.. Yup. Fixed.. Yes this is too specific. Basically we just need to make sure that the list of paths we walk over (w the stack approach) includes the directory nodes for any files listed, so that we can accumulate the header/footer objects appropriately. . So ... if the file was created w a call to newFile() but FUSE never receives any reads (presumably calls to this Read function), then we never report the error? Does it make sense to have a logger emit this error right when it happens (after the GetFile?). Is this a debug line?. Just curious about the GB threshold ... I'm guessing you wanted to test something just past 1GB and the 17 is an arbitrary small number to get past 1GB. :+1: . Fair enough. I can see how it'd generate noise before a user actually attempts a read.. Grammar typo here (missing a 'for' I think). Nifty!. Made this change. Ok, updated this so that we construct the objects array in a single pass, to support a streaming get file operation.. It's not the same scope. If I leave the limiter declaration as-is it generates a compiler error. So you want me to declare two limiters? One for the put objects for the content and the other just for the header/footer?. Per offline discussion, we decided this was out of scope. However, we don't have a good answer today on how we want to handle incremental SQL data ingestion. Issue here. Yup yup. Did a find+replace so should have them all now.. Ok added. ",
    "brndnmtthws": "Pachyderm is likely a great candidate to become a first-class Mesos framework. It can be run in containers (via Docker or App Container) on CoreOS, much like you're already doing.  For service discovery, you can use Mesos-DNS.\n. ",
    "kunthar": "@brndnmtthws  :+1:    as a reminder :)\n. ",
    "chicagobuss": "Any updates here?  Really looking forward to trying this but we're definitely moving towards mesos.\n. Our problem with Kubernetes is how slow it is to schedule containers.  We tested it pretty heavily and were left feeling it was nowhere near production ready.\nI feel like regardless of whether you get it sponsored or not, it'd be very good to make Pachyderm work on platforms other than Kubernetes.\n. ",
    "peter-edge": "src/pfs/discovery :)\n. Can we close this since this is the default on master now?\n. Hey @cemeyer, we're redoing the PFS API with protocol buffers v3 and grpc, the current API is here https://github.com/pachyderm/pachyderm/blob/master/src/pfs/pfs.proto, there will be generated HTTP endpoints as well. Would this cover your concerns? Should one of us look into generating documentation based on this if not?\n. I think we will just close this issue for now (I'm all about the\nunachievable 0 issues 0 PRs active) and when things are more settled, we\nmay decide to document this further with some documentation generation\ntool. Thanks for the input :)\nOn Monday, July 20, 2015, Conrad Meyer notifications@github.com wrote:\n\nThat looks ok. It's really up to you, though :).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/38#issuecomment-122745539.\n. I like the general direction of this, but it would need to be resolved, @Mistobaan are you still interested in doing that?\n. Closing for now, @Mistobaan if you want to pick this back up, feel free to re-open. Thanks!\n. I definitely default to golang:1.4.2 in general for these scenarios, but given that pachyderm does have some reliance on the host, i think being explicit about the OS might make sense? @jdoliner \n. Ya so my hope in the next few months is we don't have a reliance on anything in the actual image other than built binaries, but this will take a lot of work. The official golang images are based on debian I believe. I think for now it's six one way, half dozen the other. @Mistobaan thoughts?\n. We did this in the next branch, closing for now. Thanks for contributing/this was a good thought!\n. @bitwiseman @jdoliner should we close this and concentrate on https://github.com/pachyderm/pfs/pull/69? I'm all about 0 issues, 0 PRs :)\n. I got through most of the scripts, still a few left. What mostly inspired this was difficulty getting a dev envinronment up and running, and wanting more clarity as an external user as to what was happening - I think this a decent start on that :) I would run through the commands yourself, however, to make sure I did not have any typos. Ie I do have typos and am still not having the easiest time running all of them :)\n. Hmm, I've tried both Chrome and Safari and CLAHub and both are returning an error. Can you see on your side if it actually went through?\n\n\n. It's alright. The PR isn't really complete anyways I'd say, if you're\ncomfortable with the direction of it I'll finish it and we will figure this\nout in the mean time.\nOn Sunday, June 7, 2015, Joey Zwicker notifications@github.com wrote:\n\nHmm, https://www.clahub.com/agreements/pachyderm/pfs works just fine for\nme. We'll figure it out.\nOn Sun, Jun 7, 2015 at 2:32 AM, Peter Edge notifications@github.com\n<javascript:_e(%7B%7D,'cvml','notifications@github.com');> wrote:\n\nHmm, I've tried both Chrome and Safari and CLAHub and both are returning\nan error. Can you see on your side if it actually went through?\n[image: image]\n<\nhttps://cloud.githubusercontent.com/assets/4228796/8023337/d760441e-0d08-11e5-82e4-02c5fb4b684d.png\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pachyderm/pfs/pull/69#issuecomment-109729341.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pachyderm/pfs/pull/69#issuecomment-109777177.\n. Lol\n\nOn Sunday, June 7, 2015, Joe Doliner notifications@github.com wrote:\n\nYeah turns out CLAHub is now defunct. We'll figure something else out.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pachyderm/pfs/pull/69#issuecomment-109791052.\n. lol got it. you should update the link in CONTRIBUTING.md :)\n\nthis is generally at the state where I'd want you to verify stuff works, also covers https://github.com/pachyderm/pfs/issues/70\nalso see https://github.com/pachyderm/pfs/issues/81\noverall, ya this is a start, but definitely would want to do a PR after this that moves a lot of stuff to makefiles, and moves the -wrapper scripts somewhere else (to denote they are for use interally only).\n. note this also takes care of https://github.com/pachyderm/pfs/pull/67 but in a very different/more complicated way\n. @bitwiseman can you create PRs for those two changes against peter-edge/pfs, so i can merge them in?  @jdoliner: using . instead of source should solve this problem, i'm used to doing things in a bashy way, which is a bad habit for docker world\n. dasdvkjasdvkbasdvkbjasdv well at least we'll know all our scripts will be compatible with both bash and zsh hahaha :)\n. merged :)\n. @bitwiseman I'm having a lot of trouble with this, this is not a technique I have used before, I may just patch the whole thing into a new PR, what do you think?\n. @jdoliner what do you think about this now?\n. Closing for now, after the submitted fixes already and since we're refactoring the other stuff\n. Only thing you need to take care of is lib/btrfs/btrfs.go\nThe functions in there shell out to btrfs and sync. sync you can handle, but btrfs goes to /bin/btrfs/ which is chroot /host brtrfs, and a minimal docker container with the scratch base image won't have chroot (or anything else). You can do this with syscall package, but...well read bout it haha. And it changes the whole go process which is obviously not ok.\nBut ya, there's solutions to this. If you don't care about absolute minimal binary, you could use either the busybox or gliderlabs/alpine images instead, which are still super small, and have chroot. What you think?\nOnly other super common thing to worry about (haven't gone through the code yet) is if you do any HTTP calls anywhere, or plan to, root certs aren't available. So you can add them straight up (just copy them from CoreOS), or I put together this https://github.com/peter-edge/go-cacerts for this exact reason. Be careful to cache any objects you use from cacerts though, a ton of memory is allocated for each call to cacerts.NewX509CertPool().\n. Done\n. https://github.com/pachyderm/pachyderm/commit/f6dc3b9dc1a073c4f61db4f4822965f058aa12ec\n. Done\n. Fixes for everything here should be complete. Thanks for the help!\n. $ ag job\netc/certs/ca-certificates.crt:660:F7mGnjixlAxYjOBVqjtjbZqJYLhkKpLGN/R+Q0O3c+gB53+XD9fyexn9GtePyfqF\netc/certs/ca-certificates.crt:1345:vIOlCsRnKPZzFBQ9RnbDhxSJITRNrw9FDKZJobq7nMWxM4MphQIDAQABo0IwQDAP\netc/certs/ca-certificates.crt:1734:9BcjGlZ+W988bDjkcbd4kdS8odhM+KhDtgPpTSEHCIjaWC9mOSm9BXiLnTjoBbdq\netc/certs/ca-certificates.crt:1844:teGYO8A3ZNY9lO4L4fUorgtWv3GLIylBjobFS1J72HGrH4oVpjuDWtdYAVHGTEHZ\netc/certs/ca-certificates.crt:3241:6bM8K8vzARO/Ws/BtQpgvd21mWRTuKCWs2/iJneRjOBiEAKfNA+k1ZIzUd6+jbqE\nsrc/btrfs/btrfs.go:596:     // inode 6683 file offset 0 len 107 disk start 0 offset 0 gen 909 flags INLINE jobs/rPqZxsaspy\nREADME.md:42:WE'RE HIRING! Love Docker, Go and distributed systems? Learn more about [our team](http://www.pachyderm.io/jobs.html) and email us at jobs@pachyderm.io.\nREADME.md:77:Rather than thinking in terms of map or reduce jobs, pps thinks in terms of pipelines expressed within a container. A pipeline is a generic way expressing computation over large datasets and it\u2019s containerized to make it easily portable, isolated, and easy to monitor. In Pachyderm, all analysis runs in containers. You can write them in any language you want and include any libraries.\nREADME.md:297:We're hiring! If you like ambitious distributed systems problems and think there should be a better alternative to Hadoop, please reach out. Email jobs@pachyderm.io.\nLol. Fixed with https://github.com/pachyderm/pachyderm/commit/f77e4d3dcf502b1d40e74724e3874e1d19d4d959\n. Hey, should we change this issue around to maybe be a feature (s3 import), or maybe close it for now, since we're doing pfs as the base?\n. Hey, mines ready too! :) haha\nOn Wednesday, June 24, 2015, Joe Doliner notifications@github.com wrote:\n\nMerged #93 https://github.com/pachyderm/pfs/pull/93.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pachyderm/pfs/pull/93#event-338366935.\n. LGTM for now :) Thanks for contributing!\n. I don't know if we need this at all:\n- standard is just to use built-in, no helpers, or if you really need a t.Fatal helper, use testify https://github.com/stretchr/testify\n- I don't know the answer to this, but I think that if a file does not end in _test.go, then dependences will be compiled into a main binary, which would mean a main binary would rely on the testing package\n. Ya so I would just get rid of check\n\nIn general, I may be in the minority but I actually prefer:\nif err != nil {\n  t.Fatal(...)\n}\nto using wrapper functions. It encourages more fine-grained control over tests (ie Error vs Fatal), and I use Errorf/Fatalf a lot for better messages (you can have check and checkf too, but again ya).\nIf we still wanted this, then using testify is pretty standard and gets rid of needing a special testing package in pfs.\nI don't think this one function deserves making a special package in pfs for\n. Hey man,\nSuper appreciate all this. So right now, we're changing around a bunch of stuff with how we're branching, and the merge will be super painful, plus there's a couple little things here - we feel bad but we're going to close this for now, and let's discuss it more in https://github.com/pachyderm/pfs/issues/72? I feel super bad about closing this but I think you'll like what we're doing and agree it'll be easier for you and for us :)\n. This one won't be the easiest since it's a command line tool with a big external dep (fleetctl, etc). I think later on, an integration test would be good...?\n. Closing for now\n. Hey @plar, thanks so much for the detailed issue and filing this - apologies it took so long to get back to you. We are redoing all the btrfs code in src/pfs right now, so I had put this on the backburner. I promise next time I'll check your issues out within 17 hours if I can, not 17 days!\nSo I looked into this and it turns out that from the time you filed this issue, to the time I just investigated it, we (I) inadvertently moved to something even worse - we read the entire diff into a byte buffer, and then send it over. You can imagine how this would blow up memory.\nI agree with the io.Pipe() idea, and put that into the code at https://github.com/pachyderm/pachyderm/commit/407804f922ebb66a5b4a1abafb3686063f81363e. Care to review? It's already committed but if you don't like something, file a PR or just ping me and I'll fix it. Also @jdoliner \n. Hey @plar, closing this for now, feel free to re-open if you come by :)\n. Can you let me know the make test failure? All the development commands\nmust be run in a Linux environment with btrfs >= 3.14\nOn Tuesday, July 21, 2015, Pavel Larkin notifications@github.com wrote:\n\nHi @peter-edge https://github.com/peter-edge, it looks good but I still\ncan't test it.\nAs I see you've refactored Makefile and I do not see container-shell\ntarget anymore.\nI'm trying to run make test but it fails (dependencies issue)\nLet me try to run it via vagrant.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/108#issuecomment-123366320\n.\n. perfect, thanks for that, changing now :)\n. you seem to be running an old version of the code, the above is referencing container-shell along with using an old docker image?\n. I'm not completely sure. You should be fine, there is nothing special about\nvagrant specifically. Did you add the -v to go get?\n\nOn Tue, Jul 21, 2015 at 8:01 PM, Pavel Larkin notifications@github.com\nwrote:\n\nOops, you're right. It was an old image.\nI'm trying to run it on my local side. My steps:\npavel@devel:~/workspace$ git clone git@github.com:pachyderm/pachyderm.git\nCloning into 'pachyderm'...\nremote: Counting objects: 6745, done.\nremote: Compressing objects: 100% (40/40), done.\nremote: Total 6745 (delta 21), reused 0 (delta 0), pack-reused 6705\nReceiving objects: 100% (6745/6745), 4.66 MiB | 1006.00 KiB/s, done.\nResolving deltas: 100% (3987/3987), done.\nChecking connectivity... done.\npavel@devel:~/workspace$ cd pachyderm\npavel@devel:~/workspace/pachyderm(master)$ export GOPATH=pwd  # don't want to pollute  global GOPATH namespace\npavel@devel:~/workspace/pachyderm(master)$ make test-deps\ngo get -d -v -t ./...github.com/aws/aws-sdk-go (download)github.com/vaughan0/go-ini (download)github.com/go-fsnotify/fsnotify (download)github.com/pachyderm/pachyderm (download)github.com/satori/go.uuid (download)github.com/stretchr/testify (download)github.com/golang/protobuf (download)github.com/peter-edge/go-google-protobuf (download)\nFetching https://golang.org/x/net/context?go-get=1\nParsing meta tags from https://golang.org/x/net/context?go-get=1 (status code 200)\nget \"golang.org/x/net/context\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/context?go-get=1\nget \"golang.org/x/net/context\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net?go-get=1\nParsing meta tags from https://golang.org/x/net?go-get=1 (status code 200)golang.org/x/net (download)\nFetching https://google.golang.org/grpc?go-get=1\nParsing meta tags from https://google.golang.org/grpc?go-get=1 (status code 200)\nget \"google.golang.org/grpc\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc?go-get=1google.golang.org/grpc (download)github.com/bradfitz/http2 (download)\nFetching https://golang.org/x/oauth2?go-get=1\nParsing meta tags from https://golang.org/x/oauth2?go-get=1 (status code 200)\nget \"golang.org/x/oauth2\": found meta tag main.metaImport{Prefix:\"golang.org/x/oauth2\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/oauth2\"} at https://golang.org/x/oauth2?go-get=1golang.org/x/oauth2 (download)\nFetching https://golang.org/x/oauth2/google?go-get=1\nParsing meta tags from https://golang.org/x/oauth2/google?go-get=1 (status code 200)\nget \"golang.org/x/oauth2/google\": found meta tag main.metaImport{Prefix:\"golang.org/x/oauth2\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/oauth2\"} at https://golang.org/x/oauth2/google?go-get=1\nget \"golang.org/x/oauth2/google\": verifying non-authoritative meta tag\nFetching https://golang.org/x/oauth2?go-get=1\nParsing meta tags from https://golang.org/x/oauth2?go-get=1 (status code 200)\nFetching https://golang.org/x/oauth2/jwt?go-get=1\nParsing meta tags from https://golang.org/x/oauth2/jwt?go-get=1 (status code 200)\nget \"golang.org/x/oauth2/jwt\": found meta tag main.metaImport{Prefix:\"golang.org/x/oauth2\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/oauth2\"} at https://golang.org/x/oauth2/jwt?go-get=1\nget \"golang.org/x/oauth2/jwt\": verifying non-authoritative meta tag\nFetching https://golang.org/x/oauth2?go-get=1\nParsing meta tags from https://golang.org/x/oauth2?go-get=1 (status code 200)\nFetching https://google.golang.org/cloud/compute/metadata?go-get=1\nParsing meta tags from https://google.golang.org/cloud/compute/metadata?go-get=1 (status code 200)\nget \"google.golang.org/cloud/compute/metadata\": found meta tag main.metaImport{Prefix:\"google.golang.org/cloud\", VCS:\"git\", RepoRoot:\"https://code.googlesource.com/gocloud\"} at https://google.golang.org/cloud/compute/metadata?go-get=1\nget \"google.golang.org/cloud/compute/metadata\": verifying non-authoritative meta tag\nFetching https://google.golang.org/cloud?go-get=1\nParsing meta tags from https://google.golang.org/cloud?go-get=1 (status code 200)google.golang.org/cloud (download)github.com/peter-edge/go-env (download)github.com/spf13/cobra (download)github.com/inconshreveable/mousetrap (download)github.com/spf13/pflag (download)github.com/coreos/go-etcd (download)github.com/ugorji/go (download)github.com/fsouza/go-dockerclient (download)\npavel@devel:~/workspace/pachyderm(master)$ make test\nbin/run bin/wrap bin/test -test.short ./...\nbtrfs-progs v3.19.1\nSee http://btrfs.wiki.kernel.org for more information.\nTurning ON incompat feature 'extref': increased hardlink limit per file to 65536\nTurning ON incompat feature 'skinny-metadata': reduced-size metadata extent refs\nERROR: device scan failed '/var/lib/pfs/btrfs.img' - Block device required\nfs created label (null) on /var/lib/pfs/btrfs.img\n    nodesize 16384 leafsize 16384 sectorsize 4096 size 10.00GiB\nSending build context to Docker daemon 58.37 MB\nSending build context to Docker daemon\nStep 0 : FROM ubuntu:15.04\n ---> 6be21d1e5d1e\nStep 1 : RUN apt-get update -yq &&   apt-get install -yq --no-install-recommends     btrfs-tools     build-essential     ca-certificates     curl     git\n ---> Using cache\n ---> fa9268db6aee\nStep 2 : RUN curl -sSL https://storage.googleapis.com/golang/go1.4.2.linux-amd64.tar.gz | tar -C /usr/local -xz &&   mkdir -p /go/bin\n ---> Using cache\n ---> f9291b08e71a\nStep 3 : ENV PATH /go/bin:/usr/local/go/bin:$PATH\n ---> Using cache\n ---> 485f5b67d2a3\nStep 4 : ENV GOPATH /go\n ---> Using cache\n ---> d23e6ec1af6b\nStep 5 : RUN go get -v golang.org/x/tools/cmd/vet &&   go get -v github.com/kisielk/errcheck &&   go get -v github.com/golang/lint/golint\n ---> Using cache\n ---> 1cdb7eb61b0f\nStep 6 : RUN mkdir -p /go/src/github.com/pachyderm/pachyderm\n ---> Using cache\n ---> eafdaa691d98\nStep 7 : WORKDIR /go/src/github.com/pachyderm/pachyderm\n ---> Using cache\n ---> 05592ee3b2a3\nStep 8 : RUN mkdir -p /go/src/github.com/pachyderm/pachyderm/etc/deps\n ---> Using cache\n ---> 1ca1b34f2915\nStep 9 : ADD etc/deps/deps.list /go/src/github.com/pachyderm/pachyderm/etc/deps/\n ---> Using cache\n ---> bf7f853060f6\nStep 10 : RUN cat etc/deps/deps.list | xargs go get -v\n ---> Running in 2b4035e2e613github.com/aws/aws-sdk-go (download)github.com/vaughan0/go-ini (download)github.com/bradfitz/http2 (download)github.com/coreos/go-etcd (download)github.com/ugorji/go (download)github.com/fsouza/go-dockerclient (download)github.com/go-fsnotify/fsnotify (download)github.com/golang/protobuf (download)github.com/inconshreveable/mousetrap (download)github.com/peter-edge/go-env (download)github.com/peter-edge/go-google-protobuf (download)github.com/satori/go.uuid (download)github.com/spf13/cobra (download)github.com/spf13/pflag (download)github.com/stretchr/testify (download)\nFetching https://golang.org/x/net/context?go-get=1\nParsing meta tags from https://golang.org/x/net/context?go-get=1 (status code 200)\nget \"golang.org/x/net/context\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/context?go-get=1\nget \"golang.org/x/net/context\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net?go-get=1\nParsing meta tags from https://golang.org/x/net?go-get=1 (status code 200)golang.org/x/net (download)\nFetching https://golang.org/x/oauth2?go-get=1\nParsing meta tags from https://golang.org/x/oauth2?go-get=1 (status code 200)\nget \"golang.org/x/oauth2\": found meta tag main.metaImport{Prefix:\"golang.org/x/oauth2\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/oauth2\"} at https://golang.org/x/oauth2?go-get=1golang.org/x/oauth2 (download)\nFetching https://google.golang.org/cloud/compute/metadata?go-get=1\nParsing meta tags from https://google.golang.org/cloud/compute/metadata?go-get=1 (status code 200)\nget \"google.golang.org/cloud/compute/metadata\": found meta tag main.metaImport{Prefix:\"google.golang.org/cloud\", VCS:\"git\", RepoRoot:\"https://code.googlesource.com/gocloud\"} at https://google.golang.org/cloud/compute/metadata?go-get=1\nget \"google.golang.org/cloud/compute/metadata\": verifying non-authoritative meta tag\nFetching https://google.golang.org/cloud?go-get=1\nParsing meta tags from https://google.golang.org/cloud?go-get=1 (status code 200)google.golang.org/cloud (download)\nFetching https://google.golang.org/grpc?go-get=1\nParsing meta tags from https://google.golang.org/grpc?go-get=1 (status code 200)\nget \"google.golang.org/grpc\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc?go-get=1google.golang.org/grpc (download)github.com/aws/aws-sdk-go/aws/awserrgithub.com/aws/aws-sdk-go/aws/awsutilgithub.com/vaughan0/go-inigithub.com/aws/aws-sdk-go/internal/endpointsgithub.com/aws/aws-sdk-go/internal/protocol/query/queryutilgithub.com/aws/aws-sdk-go/internal/protocol/xml/xmlutilgithub.com/bradfitz/http2/hpackgithub.com/ugorji/go/codecgithub.com/fsouza/go-dockerclient/vendor/github.com/docker/libcontainer/usergithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/parsersgithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/ulimitgithub.com/fsouza/go-dockerclient/vendor/github.com/Sirupsen/logrusgithub.com/aws/aws-sdk-go/aws/credentialsgithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/ioutilsgithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/promisegithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/unitsgithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/homedirgithub.com/go-fsnotify/fsnotifygithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/mflaggithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/systemgithub.com/bradfitz/http2github.com/aws/aws-sdk-go/awsgithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/poolsgithub.com/golang/protobuf/protogithub.com/inconshreveable/mousetrapgithub.com/peter-edge/go-envgithub.com/satori/go.uuidgithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/fileutilsgithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/archivegithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/optsgithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/stdcopygithub.com/spf13/pflaggithub.com/stretchr/testify/assertgithub.com/aws/aws-sdk-go/internal/protocol/querygithub.com/aws/aws-sdk-go/internal/protocol/restgolang.org/x/net/contextgolang.org/x/net/internal/timeseriesgolang.org/x/net/tracegithub.com/aws/aws-sdk-go/internal/protocol/restxmlgithub.com/aws/aws-sdk-go/internal/signer/v4github.com/spf13/cobragolang.org/x/oauth2/internalgolang.org/x/oauth2/jwsgithub.com/peter-edge/go-google-protobufgoogle.golang.org/cloud/internalgoogle.golang.org/cloud/compute/metadatagoogle.golang.org/grpc/codesgoogle.golang.org/grpc/grpcloggoogle.golang.org/grpc/metadatagolang.org/x/oauth2golang.org/x/oauth2/jwtgolang.org/x/oauth2/googlegoogle.golang.org/grpc/credentialsgoogle.golang.org/grpc/transportgithub.com/aws/aws-sdk-go/service/s3github.com/stretchr/testify/requiregoogle.golang.org/grpcgithub.com/stretchr/testify/suitegithub.com/fsouza/go-dockerclientgithub.com/aws/aws-sdk-go/service/s3/s3managergithub.com/coreos/go-etcd/etcd\n ---> d62b5f4be8fc\nRemoving intermediate container 2b4035e2e613\nStep 11 : RUN go get github.com/coreos/go-etcd/etcd &&   cd /go/src/github.com/coreos/go-etcd &&   git checkout release-0.4\n ---> Running in 0b6c0cb0fe6e\nSwitched to a new branch 'release-0.4'\nBranch release-0.4 set up to track remote branch release-0.4 from origin.\n ---> 0fa08b1ccc49\nRemoving intermediate container 0b6c0cb0fe6e\nStep 12 : ADD . /go/src/github.com/pachyderm/pachyderm/\n ---> 9ae543f8eb70\nRemoving intermediate container b924b7a74adf\nSuccessfully built 9ae543f8eb70\nPFS_DIR=/var/lib/pfs/btrfs\nPFS_HOST_VOLUME=/var/lib/pfs/btrfs/wrap-1\nPFS_LOCAL_VOLUME=/var/lib/pfs/btrfs/wrap-1\nPFS_BTRFS_ROOT=/var/lib/pfs/btrfs/wrap-1\n../../aws/aws-sdk-go/internal/features/shared/shared.go:15:2: cannot find package \"github.com/lsegal/gucumber\" in any of:\n    /usr/local/go/src/github.com/lsegal/gucumber (from $GOROOT)\n    /go/src/github.com/lsegal/gucumber (from $GOPATH)\nsrc/github.com/bradfitz/http2/h2i/h2i.go:41:2: cannot find package \"golang.org/x/crypto/ssh/terminal\" in any of:\n    /usr/local/go/src/golang.org/x/crypto/ssh/terminal (from $GOROOT)\n    /go/src/golang.org/x/crypto/ssh/terminal (from $GOPATH)\nsrc/github.com/peter-edge/go-tools/common/aws/commonaws.go:6:2: cannot find package \"github.com/peter-edge/go-cacerts\" in any of:\n    /usr/local/go/src/github.com/peter-edge/go-cacerts (from $GOROOT)\n    /go/src/github.com/peter-edge/go-cacerts (from $GOPATH)\nsrc/github.com/peter-edge/go-tools/common/aws/commonaws.go:8:2: cannot find package \"github.com/peter-edge/go-tools/common\" in any of:\n    /usr/local/go/src/github.com/peter-edge/go-tools/common (from $GOROOT)\n    /go/src/github.com/peter-edge/go-tools/common (from $GOPATH)\nsrc/github.com/peter-edge/go-tools/ecs-list-ips/main.go:11:2: cannot find package \"github.com/peter-edge/go-tools/common/aws\" in any of:\n    /usr/local/go/src/github.com/peter-edge/go-tools/common/aws (from $GOROOT)\n    /go/src/github.com/peter-edge/go-tools/common/aws (from $GOPATH)\n../../stretchr/testify/mock/mock.go:11:2: cannot find package \"github.com/stretchr/objx\" in any of:\n    /usr/local/go/src/github.com/stretchr/objx (from $GOROOT)\n    /go/src/github.com/stretchr/objx (from $GOPATH)\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/context: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/context expects import \"golang.org/x/net/context\"can't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/dict: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/dict expects import \"golang.org/x/net/dict\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/html: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/html expects import \"golang.org/x/net/html\"can't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/html/atom: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/html/atom expects import \"golang.org/x/net/html/atom\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/html/charset: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/html/charset expects import \"golang.org/x/net/html/charset\"can't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/icmp: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/icmp expects import \"golang.org/x/net/icmp\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/idna: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/idna expects import \"golang.org/x/net/idna\"can't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/internal/iana: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/internal/iana expects import \"golang.org/x/net/internal/iana\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/internal/nettest: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/internal/nettest expects import \"golang.org/x/net/internal/nettest\"can't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/internal/timeseries: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/internal/timeseries expects import \"golang.org/x/net/internal/timeseries\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/ipv4: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/ipv4 expects import \"golang.org/x/net/ipv4\"can't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/ipv6: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/ipv6 expects import \"golang.org/x/net/ipv6\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/netutil: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/netutil expects import \"golang.org/x/net/netutil\"can't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/proxy: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/proxy expects import \"golang.org/x/net/proxy\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/publicsuffix: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/publicsuffix expects import \"golang.org/x/net/publicsuffix\"can't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/trace: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/trace expects import \"golang.org/x/net/trace\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/webdav: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/webdav expects import \"golang.org/x/net/webdav\"can't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/websocket: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/websocket expects import \"golang.org/x/net/websocket\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2 expects import \"golang.org/x/oauth2\"can't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/clientcredentials: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/clientcredentials expects import \"golang.org/x/oauth2/clientcredentials\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/facebook: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/facebook expects import \"golang.org/x/oauth2/facebook\"can't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/github: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/github expects import \"golang.org/x/oauth2/github\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/google: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/google expects import \"golang.org/x/oauth2/google\"can't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/jws: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/jws expects import \"golang.org/x/oauth2/jws\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/linkedin: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/linkedin expects import \"golang.org/x/oauth2/linkedin\"can't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/odnoklassniki: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/odnoklassniki expects import \"golang.org/x/oauth2/odnoklassniki\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/paypal: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/paypal expects import \"golang.org/x/oauth2/paypal\"can't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/vk: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/vk expects import \"golang.org/x/oauth2/vk\"\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/cloud: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/cloud expects import \"google.golang.org/cloud\"can't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/cloud/bigquery: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/cloud/bigquery expects import \"google.golang.org/cloud/bigquery\"\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/cloud/bigtable: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/cloud/bigtable expects import \"google.golang.org/cloud/bigtable\"can't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/cloud/bigtable/bttest: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/cloud/bigtable/bttest expects import \"google.golang.org/cloud/bigtable/bttest\"\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/cloud/container: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/cloud/container expects import \"google.golang.org/cloud/container\"can't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/cloud/datastore: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/cloud/datastore expects import \"google.golang.org/cloud/datastore\"\n/go/src/google.golang.org/cloud/bigquery/bigquery.go:24:2: cannot find package \"google.golang.org/api/bigquery/v2\" in any of:\n    /usr/local/go/src/google.golang.org/api/bigquery/v2 (from $GOROOT)\n    /go/src/google.golang.org/api/bigquery/v2 (from $GOPATH)\nsrc/google.golang.org/cloud/examples/bigtable/bigtable-hello/helloworld.go:21:2: cannot find package \"google.golang.org/appengine\" in any of:\n    /usr/local/go/src/google.golang.org/appengine (from $GOROOT)\n    /go/src/google.golang.org/appengine (from $GOPATH)\nsrc/google.golang.org/cloud/examples/bigtable/bigtable-hello/helloworld.go:22:2: cannot find package \"google.golang.org/appengine/log\" in any of:\n    /usr/local/go/src/google.golang.org/appengine/log (from $GOROOT)\n    /go/src/google.golang.org/appengine/log (from $GOPATH)\nsrc/google.golang.org/cloud/examples/bigtable/bigtable-hello/helloworld.go:23:2: cannot find package \"google.golang.org/appengine/user\" in any of:\n    /usr/local/go/src/google.golang.org/appengine/user (from $GOROOT)\n    /go/src/google.golang.org/appengine/user (from $GOPATH)\n/go/src/google.golang.org/cloud/pubsub/pubsub.go:33:2: cannot find package \"google.golang.org/api/googleapi\" in any of:\n    /usr/local/go/src/google.golang.org/api/googleapi (from $GOROOT)\n    /go/src/google.golang.org/api/googleapi (from $GOPATH)\n/go/src/google.golang.org/cloud/pubsub/pubsub.go:34:2: cannot find package \"google.golang.org/api/pubsub/v1beta2\" in any of:\n    /usr/local/go/src/google.golang.org/api/pubsub/v1beta2 (from $GOROOT)\n    /go/src/google.golang.org/api/pubsub/v1beta2 (from $GOPATH)\n/go/src/google.golang.org/cloud/storage/acl.go:21:2: cannot find package \"google.golang.org/api/storage/v1\" in any of:\n    /usr/local/go/src/google.golang.org/api/storage/v1 (from $GOROOT)\n    /go/src/google.golang.org/api/storage/v1 (from $GOPATH)\nsrc/google.golang.org/cloud/examples/storage/appengine/app.go:30:2: cannot find package \"google.golang.org/appengine/file\" in any of:\n    /usr/local/go/src/google.golang.org/appengine/file (from $GOROOT)\n    /go/src/google.golang.org/appengine/file (from $GOPATH)\nsrc/google.golang.org/cloud/examples/storage/appengine/app.go:32:2: cannot find package \"google.golang.org/appengine/urlfetch\" in any of:\n    /usr/local/go/src/google.golang.org/appengine/urlfetch (from $GOROOT)\n    /go/src/google.golang.org/appengine/urlfetch (from $GOPATH)\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/cloud/pubsub: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/cloud/pubsub expects import \"google.golang.org/cloud/pubsub\"can't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/cloud/storage: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/cloud/storage expects import \"google.golang.org/cloud/storage\"\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/grpc: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/grpc expects import \"google.golang.org/grpc\"can't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/grpc/codes: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/grpc/codes expects import \"google.golang.org/grpc/codes\"\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/grpc/credentials: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/grpc/credentials expects import \"google.golang.org/grpc/credentials\"can't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/grpc/grpclog: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/grpc/grpclog expects import \"google.golang.org/grpc/grpclog\"\nsrc/google.golang.org/grpc/grpclog/glogger/glogger.go:40:2: cannot find package \"github.com/golang/glog\" in any of:\n    /usr/local/go/src/github.com/golang/glog (from $GOROOT)\n    /go/src/github.com/golang/glog (from $GOPATH)\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/grpc/metadata: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/grpc/metadata expects import \"google.golang.org/grpc/metadata\"can't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/grpc/transport: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/grpc/transport expects import \"google.golang.org/grpc/transport\"\nmake: *** [test] Error 1\nWhat am I doing wrong? or maybe the tests must be run only through\nvagrant?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/108#issuecomment-123416856\n.\n. Yep, and I run into problems even sooner, because setting GOPATH to the checkout has conflicts, namely the repository has both src and bin folders. Can you try this with a more normal setup and the checked-in Dockerfile? I've had no problems recently.\n. Hey,\n\nNo problem, thank YOU for your patience! Let me know if you need any more help. I would obviously prefer a path for development that worked on 14.04 that I could easily document as this is the LTS version, so let me know if you can help out with that.\n. LGTM\n. LGTM\n. Hey, super thanks :)\nThere's a couple things, and I think in general we should re-think the s3utils package, but this is a great start. For testing, if you have an aws account handy (I promise this requirement will go away :) ) you can just export aws creds per normal environment variables, and make test should do the trick. \n. Ya actually for now, this is fine and an improvement over what we have. I made a couple comments but otherwise, thank you so much!\n. ignore the travis stuff\n. Can you change the Dockerfile and remove the goamz stuff and add go get github.com/aws/aws-sdk-go?\n. also go get github.com/vaughan0/go-ini\n. also i get these errors with make build:\n```\n$ make build\ngo get -d -v ./...\ngo build ./...\ngithub.com/pachyderm/pfs/src/btrfs\nsrc/btrfs/replica.go:72: undefined: s3utils.BucketOwnerFull\nsrc/btrfs/replica.go:72: cannot use r.uri (type string) as type *s3.Bucket in argument to s3utils.PutMulti\nsrc/btrfs/replica.go:76: undefined: s3\nsrc/btrfs/replica.go:78: undefined: err\nsrc/btrfs/replica.go:79: undefined: s3\nsrc/btrfs/replica.go:104: undefined: err\nsrc/btrfs/replica.go:105: undefined: err\ngithub.com/pachyderm/pfs/src/pipeline\nsrc/pipeline/pipeline.go:106: undefined: s3\nsrc/pipeline/pipeline.go:143: undefined: s3\nMakefile:48: recipe for target 'build' failed\nmake: *** [build] Error 2\n. Passes! Woot!\n. This is done for the new PFS api, we will probably not do this for src/btrfs for now.\n.\n$ docker images\nREPOSITORY           TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\npachyderm/pfsshard   latest              92ad6a776b36        4 minutes ago       10.31 MB\npachyderm/pfs        latest              8b3c1537a93f        4 minutes ago       634.9 MB\npachyderm/pfspfsd    latest              b1cd5e08a1c0        8 minutes ago       9.817 MB\n```\n. LGTM++, just one comment to fix\n. LGTM\n. Ya you want to attempt to pull it to get updates, and if the pull fails,\nthen check if it is local. I'm not sure if pull returns a specific error if\nthe image does not exist, but I can check post-merge.\nOn Sunday, July 19, 2015, Pierre Massat notifications@github.com wrote:\n\nNote that I only try to pull the image if the image is not local now. The\nway I understand if it makes more sense but maybe we want to pull the image\nanyway?\nI also deleted the extra lines.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pachyderm/pachyderm/pull/124#issuecomment-122689582.\n. LGTM\n. https://github.com/pachyderm/pachyderm/pull/128\n. Thanks!\n. It's on our end sort of, I was double-closing a *grpc.ClientConn. They shouldn't have a panic if that happens but separate issue, I'll try to talk to them about it soon. Our code is fixed though.\n. I was looking at https://godoc.org/github.com/docker/libkv/store#Store and I was thinking we should just replace the discovery Client with this soon - our fake implementation can be done with their boltdb implementation. The biggest changes I can see would be GetAll would be replaced with List, CheckAndSet and CheckAndDelete would be replaced with AtomicPut and AtomicDelete, and I'm not sure of the equivalent for the Hold function. But this would mean we could delete the discovery package, which would be really nice I think.\n. @mjpan let me know if you want to fix the above, otherwise I'll just keep this closed - thanks as it is :)\n. github.com/docker/docker/pkg/units needs to be moved to github.com/docker/go-units\n\nsrc/pfs/pretty/pretty.go:11:2: cannot find package \"github.com/docker/docker/pkg/units\" in any of:\n    /go/src/github.com/pachyderm/pachyderm/vendor/github.com/docker/docker/pkg/units (vendor tree)\n    /usr/local/go/src/github.com/docker/docker/pkg/units (from $GOROOT)\n    /go/src/github.com/docker/docker/pkg/units (from $GOPATH)\n. Ping - with the new commits, this has gotten out of date, sending over this stuff as a courtesy, can you check it out?\n. Oh, that's an interesting one, I'll fix it tomorrow, I think I know the\nissue\nOn Wed, Jan 27, 2016 at 1:57 AM, Joe Doliner notifications@github.com\nwrote:\n\nProtolion is breaking for us, seems like there's some new assumptions\nabout what types of data are logged.\ngithub.com/pachyderm/pachyderm/vendor/go.pedge.io/lion.(_errorHandler).Handle(0x1266e00, 0x7f3006701050, 0xc8203c65a0)\n        /go/src/github.com/pachyderm/pachyderm/vendor/go.pedge.io/lion/defaults.go:37 +0x78github.com/pachyderm/pachyderm/vendor/go.pedge.io/lion.(_logger).print(0xc82018a070, 0x2, 0xc8203d8300, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /go/src/github.com/pachyderm/pachyderm/vendor/go.pedge.io/lion/logger.go:221 +0xc9github.com/pachyderm/pachyderm/vendor/go.pedge.io/lion.(_logger).LogEntryMessage(0xc82018a070, 0xc800000002, 0xc8203d8300)\n        /go/src/github.com/pachyderm/pachyderm/vendor/go.pedge.io/lion/logger.go:216 +0x50github.com/pachyderm/pachyderm/vendor/go.pedge.io/lion/proto.(_logger).Info(0xc820115650, 0x7f3015880550, 0xc8203d82e0)\n        /go/src/github.com/pachyderm/pachyderm/vendor/go.pedge.io/lion/proto/logger.go:38 +0xeagithub.com/pachyderm/pachyderm/vendor/go.pedge.io/lion/proto.Info(0x7f3015880550, 0xc8203d82e0)\n        /go/src/github.com/pachyderm/pachyderm/vendor/go.pedge.io/lion/proto/protolion.go:201 +0x42github.com/pachyderm/pachyderm/src/pkg/shard.(_sharder).fillRoles.func1(0xc82029e0c0, 0x0, 0x0)\n        /go/src/github.com/pachyderm/pachyderm/src/pkg/shard/sharder.go:915 +0x103bgithub.com/pachyderm/pachyderm/src/pkg/discovery.(_etcdClient).watchAllWithoutRetry(0xc8200b6290, 0xc82029e060, 0x2e, 0xc8200b0ba0, 0xc82029e090, 0x0, 0x0)\n        /go/src/github.com/pachyderm/pachyderm/src/pkg/discovery/etcd_client.go:217 +0x3e6github.com/pachyderm/pachyderm/src/pkg/discovery.(_etcdClient).WatchAll(0xc8200b6290, 0xc82029e060, 0x2e, 0xc8200b0ba0, 0xc82029e090, 0x0, 0x0)\n        /go/src/github.com/pachyderm/pachyderm/src/pkg/discovery/etcd_client.go:61 +0x5cgithub.com/pachyderm/pachyderm/src/pkg/shard.(_sharder).fillRoles(0xc820119c20, 0xc8201e0fc0, 0xe, 0x7f3015886e18, 0xc8200b09c0, 0xc8200b0b40, 0xc8200b0ba0, 0x0, 0x0)\n        /go/src/github.com/pachyderm/pachyderm/src/pkg/shard/sharder.go:952 +0x18cgithub.com/pachyderm/pachyderm/src/pkg/shard.(_sharder).Register.func3(0xc8201e1200, 0xc820119c20, 0xc8201e0fc0, 0xe, 0x7f3015886e18, 0xc8200b09c0, 0xc8200b0b40, 0xc8200b0ba0, 0xc8201e11f0, 0xc8201e1140)\n        /go/src/github.com/pachyderm/pachyderm/src/pkg/shard/sharder.go:125 +0xa7\ncreated by github.com/pachyderm/pachyderm/src/pkg/shard.(_sharder).Register\n        /go/src/github.com/pachyderm/pachyderm/src/pkg/shard/sharder.go:131 +0x2bf\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pachyderm/pachyderm/pull/185#issuecomment-175318630.\n. OK fixed.\n\nThe issue was that map[uint64]bool is not a valid JSON type, and I was directly calling json.Marshal on the message. uint64s are not valid JSON keys, only strings. The jsonpb package, however, handles this issue, so I added protobuf-specific code to handle this when the data type is proto.Message.\nThe issue actually was probably in protolog too, but would not have appeared on the first level of a message (ie if you had a nested message of this type, it would probably fail).\nLet me know if there are any other issues.\n. Also note that once this is merged, I'm going to merge github.com/peter-edge/lion-go into github.com/peter-edge/protolog-go, so if you have lion-go checked out, and then do make update-test-deps etc, it may fail because the remote path will change. The solution if/when this happens is just to delete go/src/go.pedge.io/lion and re-clone.\n. There's a couple new features you might want to use:\n- envlion package - replaces go.pedge.io/pkg/log, sets up logging based on environment variables\n- protolion.LogDebug(), protolion.LogInfo() - if the log level is above LevelDebug or LevelInfo, this returns a discard logger, including for WithField/WithFields/WithContext. This improves performance when you have situations like protolion.WithField(\"key1\", \"value1\").WithField(\"key2\", \"value2\").Debugln(), which normally is going to do copy operations on the WithField calls\nI'm adding more stuff, and will document this eventually.\n. Is this just make test? Maybe I can reproduce\nOn Wed, Jan 27, 2016 at 11:54 PM, Joe Doliner notifications@github.com\nwrote:\n\nHmm, just tried this. I'm still seeing the same error :(\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pachyderm/pachyderm/pull/185#issuecomment-175901754.\n. make launch-kube fails:\n\n$ make launch-kube\netc/kube/start-kube-docker.sh\n0637a4785c7a1068cb06611e481d2e6c4c3a33895c8161183c3965e72bd10b49\nError response from daemon: Cannot start container 4fa9b63bbe98f74018ab7575e1d181345733334ec0c9fb7da2c7d88f77ae9b50: [8] System error: stat /var/lib/docker/aufs/mnt/4fa9b63bbe98f74018ab7575e1d181345733334ec0c9fb7da2c7d88f77ae9b50/sys/fs/cgroup/cpuset/docker/4fa9b63bbe98f74018ab7575e1d181345733334ec0c9fb7da2c7d88f77ae9b50: no such file or directory\nError: failed to start containers: [4fa9b63bbe98f74018ab7575e1d181345733334ec0c9fb7da2c7d88f77ae9b50]\nmake: *** [launch-kube] Error 1\n$ dc # my docker cleanup alias, deletes all containers\nError response from daemon: Driver aufs failed to remove root filesystem 4fa9b63bbe98f74018ab7575e1d181345733334ec0c9fb7da2c7d88f77ae9b50: rename /var/lib/docker/aufs/mnt/4fa9b63bbe98f74018ab7575e1d181345733334ec0c9fb7da2c7d88f77ae9b50 /var/lib/docker/aufs/mnt/4fa9b63bbe98f74018ab7575e1d181345733334ec0c9fb7da2c7d88f77ae9b50-removing: device or resource busy\nError response from daemon: Driver aufs failed to remove root filesystem 0637a4785c7a1068cb06611e481d2e6c4c3a33895c8161183c3965e72bd10b49: rename /var/lib/docker/aufs/mnt/0637a4785c7a1068cb06611e481d2e6c4c3a33895c8161183c3965e72bd10b49 /var/lib/docker/aufs/mnt/0637a4785c7a1068cb06611e481d2e6c4c3a33895c8161183c3965e72bd10b49-removing: device or resource busy\nError: failed to remove containers: [4fa9b63bbe98 0637a4785c7a]\n. OK it should be fixed now - the ordering of global initialization affected which json marshaller was used.\n. Also, mentioned this before but just in case: you might be interested in https://github.com/peter-edge/lion-go/blob/master/env/env.go\n. ping @jdoliner, I can delete this PR if it is not needed.\n. Just happened to come over here to see what's been up with Pachyderm - man so much new stuff, good work haha. Just note, feel free to ping me if you need something, ie gRPC being upgraded in lion. Also note, I would recommend moving to github.com/uber-go/zap, it's (IMO) the best logging lib out there, and I thought lion was before :-). all the hardcoding of the DOCKER_HOST makes this untestable on other OS's, or remotely\n. This is a hackish version I wrote up a while ago https://github.com/peter-edge/go-dockerutils/blob/master/dockerutils.go#L55 but I wouldn't recommend directly using it, I may refactor it or delete it soon.\n. Hey,\nFor the record, I think DOCKER_HOST is actually the better option here. DOCKER_HOST, DOCKER_TLS_VERIFY, and DOCKER_CERT_PATH are environment variables not only recognized natively by a docker client, but other docker-based processes (including docker-compose). My problem here would be the lack of differentiating between docker daemons running with TLS and those which do not, which also needs to be handled.\n(Bad, ie do not use this) example: https://github.com/peter-edge/go-dockerutils/blob/master/dockerutils.go#L55\n. just mentioning again, this won't work for tls-enabled docker daemons :)\n. the only requirement is that DOCKER_TLS_VERIFY is set, ie os.Getenv(\"DOCKER_TLS_VERIFY\") != \"\"\n. should do some checks, i do something similar to all this at https://github.com/peter-edge/go-dockerutils/blob/master/dockerutils.go#L84, it checks to make sure the files exist, etc\n. should probably just combine this into the above function\n. this feels slightly weird, can you give a quick explanation?\n. put below const\n. no newline\n. no newlines between statement and err check (do it for below too)\n. same thing from before (feels weird but maybe ok)\n. sgtm\n. sgtm\n. trailing space in quotes\n. or is that on purpose?\n. i think if this is -1, this will break right?\n. you don't use this\n. ah whatever, it bugs me, dont want guaranteed overflow situations committed, but i think we'll clean this up anyways and if it works, all good\n. Blank if?\n. Language :)\n. There is a function at https://github.com/fsouza/go-dockerclient/blob/master/misc.go#L50 that does this can you use that?\n. This should return (bool, error), so you can return errors from the docker.Client\n. Nit: not a fan of the extra newlines, can you remove them :)\n. this doesn't actually work, here's a line of output from ListImages:\n{ID:d2a0ecffe6fa4ef3de9646a75cc629bbd9da7eead7f767cb810f9808d6b3ecb6 RepoTags:[ubuntu:latest ubuntu:trusty] Created:1436470114 Size:0 VirtualSize:188357544 ParentID:29460ac934423a55802fcad24856827050697b4a9f33550bd93c82762fb6db8f RepoDigests:[] Labels:map[]}\n. I think you want the tag, see below\n. I think what you want is:\nexpectedRepoTag := fmt.Sprintf(\"%s:%s\", repository, tag)\nfor _, repoTag := range image.RepoTags {\n  if repoTag == expectedRepoTag {\n    return true, nil\n  }\n}\n. I think it should be if len(split) == 2 {\n. !=\n. ah ok, then check if != 2 or != 3\n. if tag == \"\", tag \"latest\"\n. can move this outside the top loop\n. check args length, otherwise it will panic\n. s/c/conn/g\n. func Mount(...) (retErr error) {\n...\ndefer func() {\n  if err := conn.Close(); err != nil && retErr == nil {\n    retErr = err\n  }\n}()\n...\n}\n. is this something we want to be dynamic? i have no idea\n. NONE types in the protos should be considered an error case - you have to always start enums with a 0 value, and a 0 value in src/pfs means that it was not set - if this field is expected to be set, then pfs.FieType_FILE_TYPE_NONE should fall through to the default statement\n. fmt.Errorf(\"unrecognized pfs.FileType: %v\", fileInfo.FileType)\nAlso we should agree on error messages - I think you do \"start with upper case, end with .\", I do \"start with lower case, end with no .\"\n. error case\n. I've been directly accessing the structs in the rest of the code - in golang, it makes no difference for protos:\nfunc (m *GetFileInfoResponse) GetFileInfo() *FileInfo {\n    if m != nil {\n        return m.FileInfo\n    }\n    return nil\n}\nIs the same thing as:\nm.FileInfo\nLol. I don't know why they are generating things like that, but I think it's a holdover from Java. I could get behind it if they had a corresponding SetFileInfo and made the struct fields private, but you can't do JSON marshalling with private struct fields so ya.\n. You can do:\ngo\nresult := make([]fuse.Dirent, len(response.FileInfo))\nfor i, fileInfo := range response.FileInfo {\n...\nresult[i] = RESULT_FOR_FILE_INFO\n}\nIt's slightly more efficient\n. with the rest of the code in src/pfs, the style I have is:\nsrc/path/to/something/something.go # all public methods, nothing else\nsrc/path/to/something/filesystem.go # filesystem struct, private constructor, methods\nWe can talk about it but this works REALLY well especially with onboarding new people into the codebase\n. see note below about private constructor\n. do you need the pointer back to the filesystem?\n. perfectttttt\n. do we need to make this (and the fields below) dynamic? i don't know enough about the fuse stuff yet\n. i would make them constants otherwise at the top\n. SGTM :) or just do whatever, this was more just for thought\n. YOUR_FORK isn't something implicitly defined anywhere heh, I think the former was less confusing\n. this is adding a specific users' GOPATH to the global path - adding this somewhere below in ${HOME}/.profile makes more sense to me\n. ",
    "ianmiell": "Hi Joe,\nThanks for the great response! The project does have \"Alpha\" plastered\nfirmly all over it so don't stress about newb issues :)\nUnfortunately setting up /etc/hosts like so (and the other way round as you\nsuggested, which I assume was a mistake).\nSHUTIT_TMP#GusY7I2m>cat /etc/hosts\n10.132.128.22 coreos-1\n10.132.129.103 coreos-2\nI can only assume coreos has its own way of doing name resolution?\nWas my send_sample call correct btw? If so, I can update the chess example\ndocumentation.  Tho it returned an error code 6, possibly because of the\nlookup issues.\nI'm not too familiar with coreos, so am without all my usual debugging\ntools :(\nDon't worry about setting something up for me. I can try the others myself;\nit's really a DO automation that I was after.\nThanks again.\nIan\nOn Sat, Jan 24, 2015 at 10:25 PM, Joe Doliner notifications@github.com\nwrote:\n\nHi, really sorry you ran in to this. I think I have an idea what's going\non here.\ntl;dr: I think it's a problem with dns.\nPfs uses etcd to announce its services, this is handled by the\nannounce-master--.service services. Those services do etcdctl set\n/pfs/master/0-1 {HOST_NAME}:port. The router service then uses this to\nfigure out how to contact the master. However this only works if the router\ncan do a dns lookup. How did you setup the CoreOS instances? I remember\nrunning in to this problem when I tried to get pfs setup on Vagrant.\nThere are a few easy ways I can think of to fix this for you short term:\n- echo \"coreos-1 10.132.128.22\" >/etc/hosts is a quick hack that\n  should make things start working. This would also be a good way to confirm\n  the DNS theory.\n- GCE and EC2 both have DNS by default, shoot me an email at\n  jd@pachyderm.io and I'll setup a hosted instance for you to play\n  around with.\n\nAction items for pfs:\n-  If DNS is indeed the problem this error message should explicitly\n  mention that as a likely cause.\n-  Docs should do a better job of describing this in the \"getting\n  started\" section.\n-  We should look in to making pfs not depend on DNS. I think this\n  will be doable via flannel or something similar.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pachyderm/pfs/issues/32#issuecomment-71340613.\n. \n",
    "Dal-Papa": "Hey Joe\nThanks for the quick answer. I have to admit I didn't know about this journal part of fleetctl. I'm now able to see more log and a lot is coming from my Go server so I think I'll investigate first there and it might work itself out !\nI will let you know for sure of my progress.\nThanks again.\n. ",
    "cemeyer": "Even a really rough sketch of verbs, endpoints, and that the protocol uses HTTP would be great. It doesn't have to be the best documentation you've ever seen. Thanks!\n. That looks ok. It's really up to you, though :).\n. ",
    "damnMeddlingKid": "hehe , you also need to fix this on docker hub https://registry.hub.docker.com/u/pachyderm/pfs/\n. ",
    "DimShadoWWW": "Pull request:  #67 \n. ",
    "brendanashworth": "I installed it on 1.6.0 (Ubuntu 14.04) by removing the check. You can apply the patch I just submitted if you'd like, as 1.6.0 is supposed to be supported (README states >= 1.5.0).\n. We could use something like cloudflare/semver_bash, or we can parse it manually. I can try out parsing later today.\n. Thanks!\n. ",
    "waitingkuo": "thanks~\n. it any better way to compare the version? in case docker update to 1.7.0, we need to update this again\n. ",
    "alexed1": "ok, will try that.\n. ",
    "bitwiseman": "I don't this this test's what you think it does.\nAt minimum you want, 3\\.1[4-9]+|[4-9]\\.[0-9] not [3\\.1[4-9]+|[4-9]\\.[0-9]].\nBut that's not it either...\n@jdoliner, what is the minimum version number you want to allow here? \n. Assuming you want > 3.14, this should do it: \n3\\.((1[4-9])|([2-9][0-9])|([1-9][0-9][0-9]+))(\\.[0-9]+)?\n. Closable. \n. @peter-edge,\nIf you fetch my fork, I've done the merge and added a commit for the comments I made. \nhttps://github.com/bitwiseman/pfs/commit/691da176be93a9d906b862cea117e1671cfa1cb0\n. @jdoliner - what shell/version are you running? That error message seems to say that the source command wasn't found...\nOh... take a look at this: \nhttp://stackoverflow.com/questions/4732200/replacement-for-source-in-sh\nAnother case of sh != bash ?  . will work in bash and sh, source is not present in sh?\nI can't repro the problem locally, but if you checkout https://github.com/bitwiseman/pfs/commit/a6a1a4168b40c14d456a56ff8d0d568f86b8db8b and that fixes it, great. :smile:\n. @peter-edge  - https://github.com/peter-edge/pfs/pull/1\n. @peter-edge - do you think you could compress (rebase -i) your changes into a meaningful set of commits?   \"more cleanup\" is not a great commit history. \n. You don't have to go to a whole new PR, you can just force push into this same one... \nThis is also just my suggestion, so ... You're not required to do it. :smile: \n. Oh, I see #69 is pretty extensive and covers this.  Your choice as to which to take. \n. Support version strings that don't contain micro:\n([0-9]+)\\.([0-9]+)(\\.([0-9]+))?\n. error: Found ${executable} version ${version}. Major version must be \"${major}\", minor must be version \"${minor}\" or greater.\n. See #93 maybe merge these two if's into one. \n. ",
    "phacops": "Hi,\nI'll make the changes!\nJust a few questions:\n- is signing the CLA required for the changes to be merged?\n- do you develop on a private repo and publish changes once in a while or is github the primary repository?\n. Fixed.\n. Is anyone working on this yet? If not, I'll start working on it.\n. @peter-edge just a reminder because the comment was hid : I didn't do checks for files on purpose, as go-dockerclient will make some and return the proper error. Do you think I should still check for the files?\n. I usually try to not have a package where I put random convenient functions like this because it can get messy pretty quickly (we may put very unrelated functions in there). A better solution might be to specialize the package so it has a specific purpose (compared to a \"good-for-everything\" package).\nAlso, I want to rename check (to check a test) with CheckTest. Might be easier to not mistake this function for a builtin functions. What do you think?\nI'll take a look to lib/shard/test_helpers.go.\n. TL;DR: get rid of test utils or make a test utils package.\ntestify looks cool, but wouldn't it just more test helpers instead of just the one we have here? If we separate test utils and regular utils, I think it'll be identical to have our own check function. We shouldn't use any helper then.\nConcerning the 2nd bullet point, in this state, you're right, testing will be compiled in the binary. If we separate into our own testing package, this package would only be included by files ending by _test.go and this wouldn't be a problem.\n. How do you think we should change it? And I'll create a AWS account to test, this won't be a real problem.\n. I took care of the errors (sorry about that). I still need a complete make test to run but it's on its way.\nAbout the s3utils errors, it's just because we're referring to the package on github, which doesn't have the new code yet, but I changed the path and it builds properly.\nAlso, why do you want me to include go get github.com/vaughan0/go-ini in this PR?\n. Note that I only try to pull the image if the image is not local now. The way I understand if it makes more sense but maybe we want to pull the image anyway?\nI also deleted the extra lines.\n. Looking at the doc I don't think there's such an error type. It returns either a 200 or 500 http code [0] with an cryptic error message.\n[0] https://docs.docker.com/reference/api/docker_remote_api_v1.14/#create-an-image\n. I'll make another pull request to fix this then.\n. I didn't do checks for files on purpose, as go-dockerclient will make some and return the proper error.\nSetting the path as ~/.docker is interesting, I'll start by this and if you really want me to do the file checks, I'll do them.\n. True.\n. It's kind of weird indeed. It's just New takes a pointer to a struct to override the default configuration and we don't need to override the default config.\nI'll replace it with a NewClient function in s3utils in case we have to override the default config but other than that, it's sounds fine to me.\n. Hum actually, I'm going to leave it like that. No point of making a function out of this.\nAny thoughts about it?\n. Test gone wrong :(.\n. A version can be 4.1.2 for example. We're splitting on . so we would have a len(split) of 3 if I'm correct. So the only case where we want to return an error is when they're less than a major and minor version, no?\n. ",
    "DazWilkin": "+1\n. That worked!\nAlright, now to try the Wordcount example.\nThank you very much.\n. Thank you for being so supportive and apologies for my stupid questions.\nProblem 1: OK -- That I guessed correctly at least.\nProblem 2: Ack.\nProblem 3: I've grabbed the logs and will mail to you.\nProblem 4: Ack.\nProblem 5: I did not. I suspect I'm creating the job incorrectly. I'll ship my pachfile to you too.\ndocker ps -a\nCONTAINER ID        IMAGE                  COMMAND                CREATED             STATUS              PORTS                   NAMES\nc6b460a38609        pachyderm/pfs:latest   \"/go/bin/router 3\"     10 hours ago        Up 10 hours         0.0.0.0:80->80/tcp      router              \ne9ab62c7ede4        pachyderm/pfs:latest   \"/go/bin/shard 2-3 p   10 hours ago        Up 10 hours         0.0.0.0:49161->80/tcp   shard-2-3           \n9195ad2e5859        pachyderm/pfs:latest   \"/go/bin/shard 0-3 p   10 hours ago        Up 10 hours         0.0.0.0:49153->80/tcp   shard-0-3           \na1d6d2c15c56        pachyderm/pfs:latest   \"/go/bin/shard 1-3 p   10 hours ago        Up 10 hours         0.0.0.0:49158->80/tcp   shard-1-3\n. ",
    "plar": "In my opinion it should be implemented via io.Pipe\n. Hi @peter-edge, it looks good but I still can't test it. \nAs I see you've refactored Makefile and I do not see container-shell target anymore. \nI'm trying to run make test but it fails (dependencies issue)\nLet me try to run it via vagrant.\n. Good news, it works under vagrant but there is an error in the documentation here: https://github.com/pachyderm/pachyderm#environment-setup\n\nOnce in the vagrant box, set everything up and verify that it works:\n\ngo get github.com/pachyderm/pachyderm\ncd ~/go/src/github.com/pachyderm/pachyderm\nmake test\nShould be (3 points)\ngo get github.com/pachyderm/pachyderm/...\n. Dead lock again on the same test:\n``` bash\ndevel:~/workspace/pachyderm/src/github.com/pachyderm/pachyderm(master)$ make container-shell\ndocker build -t pachyderm/pfs .\nSending build context to Docker daemon   254 kB\nSending build context to Docker daemon \nStep 0 : FROM golang:1.4.2\n ---> ebd45caf377c\nStep 1 : EXPOSE 80\n ---> Using cache\n ---> 2278acff9b7e\nStep 2 : RUN mkdir -p /go/src/github.com/pachyderm/pachyderm\n ---> Using cache\n ---> b5594cad3d91\nStep 3 : WORKDIR /go/src/github.com/pachyderm/pachyderm\n ---> Using cache\n ---> 7ec52b88b70b\nStep 4 : RUN mkdir -p /go/src/github.com/pachyderm/pachyderm/etc/bin\n ---> Using cache\n ---> a43d86d8e46d\nStep 5 : RUN go get -v golang.org/x/tools/cmd/vet &&    go get -v github.com/kisielk/errcheck &&    go get -v github.com/golang/lint/golint &&  go get -v golang.org/x/tools/cmd/vet &&     go get -v github.com/stretchr/testify\n ---> Using cache\n ---> 5cf95ac45b77\nStep 6 : RUN go get github.com/coreos/go-etcd/etcd &&   cd /go/src/github.com/coreos/go-etcd &&   git checkout release-0.4\n ---> Using cache\n ---> 3af9e17733a6\nStep 7 : RUN go get github.com/satori/go.uuid &&   go get github.com/fsouza/go-dockerclient &&   go get github.com/mitchellh/goamz/aws &&   go get github.com/mitchellh/goamz/s3 &&   go get github.com/go-fsnotify/fsnotify\n ---> Using cache\n ---> 51cdc4ddc0ec\nStep 8 : ADD etc/bin /go/src/github.com/pachyderm/pachyderm/etc/bin/\n ---> Using cache\n ---> 5249c5030a99\nStep 9 : RUN ln /go/src/github.com/pachyderm/pachyderm/etc/bin/btrfs-wrapper /bin/btrfs\n ---> Using cache\n ---> 11ed49b6de15\nStep 10 : RUN ln /go/src/github.com/pachyderm/pachyderm/etc/bin/fleetctl-wrapper /bin/fleetctl\n ---> Using cache\n ---> 07d4e96ac211\nStep 11 : ADD . /go/src/github.com/pachyderm/pachyderm/\n ---> Using cache\n ---> a63fcaaff64c\nStep 12 : RUN go install github.com/pachyderm/pachyderm/...\n ---> Using cache\n ---> bf4a1ecd145b\nSuccessfully built bf4a1ecd145b\nsudo -E bash -c 'bin/shell'\nsuccess: btrfs version 3.19.1 ok\nsuccess: docker version 1.7.0 ok\nbtrfs-progs v3.19.1\nSee http://btrfs.wiki.kernel.org for more information.\nTurning ON incompat feature 'extref': increased hardlink limit per file to 65536\nTurning ON incompat feature 'skinny-metadata': reduced-size metadata extent refs\nERROR: device scan failed '/var/lib/pfs/data.img' - Block device required\nfs created label (null) on /var/lib/pfs/data.img\n    nodesize 16384 leafsize 16384 sectorsize 4096 size 10.00GiB\nroot@8c50ab2203a4:/go/src/github.com/pachyderm/pachyderm# cd src/btrfs/; go test -test.short -test.v=true -test.timeout=1m\n=== RUN TestOsOps\n--- PASS: TestOsOps (0.00s)\n=== RUN TestGit\n2015/07/21 16:44:56 log.go:36:        btrfs.go:782 -> btrfs subvolume create /var/lib/pfs/vol/repo_TestGit\n2015/07/21 16:44:56 log.go:36:        btrfs.go:782 -> btrfs subvolume create /var/lib/pfs/vol/repo_TestGit/master\n2015/07/21 16:44:57 log.go:36:        btrfs.go:804 -> btrfs subvolume snapshot -r /var/lib/pfs/vol/repo_TestGit/master /var/lib/pfs/vol/repo_TestGit/commit1\n2015/07/21 16:44:57 log.go:36:        btrfs.go:334 -> btrfs property get -t s /var/lib/pfs/vol/repo_TestGit/commit1\n2015/07/21 16:44:57 log.go:36:        btrfs.go:807 -> btrfs subvolume snapshot /var/lib/pfs/vol/repo_TestGit/commit1 /var/lib/pfs/vol/repo_TestGit/branch\n2015/07/21 16:44:57 log.go:36:        btrfs.go:804 -> btrfs subvolume snapshot -r /var/lib/pfs/vol/repo_TestGit/branch /var/lib/pfs/vol/repo_TestGit/commit2\n2015/07/21 16:44:57 log.go:36:        btrfs.go:489 -> btrfs subvolume list -o -c -u -q --sort -ogen /var/lib/pfs/vol/repo_TestGit\nID 261 gen 10 cgen 10 top level 257 parent_uuid ebe26729-1b9a-564c-a2df-bd52fa0e6beb uuid 72c315d0-6020-4f4d-b436-9c7661c421e2 path repo_TestGit/commit2\nID 260 gen 10 cgen 9 top level 257 parent_uuid bb2c7517-8621-3246-be64-545bee74d37b uuid ebe26729-1b9a-564c-a2df-bd52fa0e6beb path repo_TestGit/branch\nID 259 gen 9 cgen 8 top level 257 parent_uuid 604c65c9-df18-cb40-a4a6-e154d0227ed6 uuid bb2c7517-8621-3246-be64-545bee74d37b path repo_TestGit/commit1\nID 258 gen 9 cgen 7 top level 257 parent_uuid - uuid 604c65c9-df18-cb40-a4a6-e154d0227ed6 path repo_TestGit/master\n--- PASS: TestGit (0.97s)\n=== RUN TestNewRepoIsEmpty\n2015/07/21 16:44:57 log.go:36:        btrfs.go:782 -> btrfs subvolume create /var/lib/pfs/vol/repo_TestNewRepoIsEmpty\n2015/07/21 16:44:57 log.go:36:        btrfs.go:782 -> btrfs subvolume create /var/lib/pfs/vol/repo_TestNewRepoIsEmpty/master\n--- PASS: TestNewRepoIsEmpty (0.38s)\n=== RUN TestCommitsAreReadOnly\n2015/07/21 16:44:58 log.go:36:        btrfs.go:782 -> btrfs subvolume create /var/lib/pfs/vol/repo_TestCommitsAreReadOnly\n2015/07/21 16:44:58 log.go:36:        btrfs.go:782 -> btrfs subvolume create /var/lib/pfs/vol/repo_TestCommitsAreReadOnly/master\n2015/07/21 16:44:58 log.go:36:        btrfs.go:804 -> btrfs subvolume snapshot -r /var/lib/pfs/vol/repo_TestCommitsAreReadOnly/master /var/lib/pfs/vol/repo_TestCommitsAreReadOnly/commit1\n--- PASS: TestCommitsAreReadOnly (0.56s)\n=== RUN TestBranchesAreReadWrite\n2015/07/21 16:44:58 log.go:36:        btrfs.go:782 -> btrfs subvolume create /var/lib/pfs/vol/repo_TestBranchesAreReadWrite\n2015/07/21 16:44:58 log.go:36:        btrfs.go:782 -> btrfs subvolume create /var/lib/pfs/vol/repo_TestBranchesAreReadWrite/master\n2015/07/21 16:44:59 log.go:36:        btrfs.go:804 -> btrfs subvolume snapshot -r /var/lib/pfs/vol/repo_TestBranchesAreReadWrite/master /var/lib/pfs/vol/repo_TestBranchesAreReadWrite/my_commit\n2015/07/21 16:44:59 log.go:36:        btrfs.go:334 -> btrfs property get -t s /var/lib/pfs/vol/repo_TestBranchesAreReadWrite/my_commit\n2015/07/21 16:44:59 log.go:36:        btrfs.go:807 -> btrfs subvolume snapshot /var/lib/pfs/vol/repo_TestBranchesAreReadWrite/my_commit /var/lib/pfs/vol/repo_TestBranchesAreReadWrite/my_branch\n--- PASS: TestBranchesAreReadWrite (0.78s)\n=== RUN TestReplication\n--- SKIP: TestReplication (0.00s)\n    btrfs_test.go:148: implement this\n=== RUN TestSendRecv\n2015/07/21 16:44:59 log.go:36:        btrfs.go:782 -> btrfs subvolume create /var/lib/pfs/vol/repo_TestSendRecv_src\n2015/07/21 16:44:59 log.go:36:        btrfs.go:782 -> btrfs subvolume create /var/lib/pfs/vol/repo_TestSendRecv_src/master\n2015/07/21 16:45:01 log.go:36:        btrfs.go:804 -> btrfs subvolume snapshot -r /var/lib/pfs/vol/repo_TestSendRecv_src/master /var/lib/pfs/vol/repo_TestSendRecv_src/mycommit1\n2015/07/21 16:45:02 log.go:36:        btrfs.go:804 -> btrfs subvolume snapshot -r /var/lib/pfs/vol/repo_TestSendRecv_src/master /var/lib/pfs/vol/repo_TestSendRecv_src/mycommit2\n2015/07/21 16:45:03 log.go:36:        btrfs.go:782 -> btrfs subvolume create /var/lib/pfs/vol/repo_TestSendRecv_dst\n2015/07/21 16:45:03 log.go:36:        btrfs.go:782 -> btrfs subvolume create /var/lib/pfs/vol/repo_TestSendRecv_dst/master\n2015/07/21 16:45:03 log.go:36:        btrfs.go:753 -> btrfs send /var/lib/pfs/vol/repo_TestSendRecv_src/mycommit1\n2015/07/21 16:45:03 log.go:36:        btrfs.go:713 -> btrfs receive /var/lib/pfs/vol/repo_TestSendRecv_dst\npanic: test timed out after 1m0s\ngoroutine 15 [running]:\ntesting.func\u00b7008()\n    /usr/src/go/src/testing/testing.go:681 +0x12f\ncreated by time.goFunc\n    /usr/src/go/src/time/sleep.go:129 +0x4b\ngoroutine 1 [chan receive]:\ntesting.RunTests(0x839fe8, 0x947260, 0x12, 0x12, 0x1)\n    /usr/src/go/src/testing/testing.go:556 +0xad6\ntesting.(*M).Run(0xc208032370, 0x951e20)\n    /usr/src/go/src/testing/testing.go:485 +0x6c\nmain.main()\n    github.com/pachyderm/pachyderm/src/btrfs/_test/_testmain.go:86 +0x1d5\ngoroutine 12 [syscall]:\nsyscall.Syscall(0x1, 0x7, 0xc208082000, 0x8000, 0x200000003, 0x1, 0x4350a0)\n    /usr/src/go/src/syscall/asm_linux_amd64.s:21 +0x5\nsyscall.write(0x7, 0xc208082000, 0x8000, 0x8000, 0x0, 0x0, 0x0)\n    /usr/src/go/src/syscall/zsyscall_linux_amd64.go:1263 +0x6e\nsyscall.Write(0x7, 0xc208082000, 0x8000, 0x8000, 0x47fc38, 0x0, 0x0)\n    /usr/src/go/src/syscall/syscall_unix.go:152 +0x58\nos.(File).write(0xc208030168, 0xc208082000, 0x8000, 0x8000, 0x0, 0x0, 0x0)\n    /usr/src/go/src/os/file_unix.go:212 +0xb7\nos.(File).Write(0xc208030168, 0xc208082000, 0x8000, 0x8000, 0x0, 0x0, 0x0)\n    /usr/src/go/src/os/file.go:139 +0x91\nio.Copy(0x7f5a45ead510, 0xc2080704e0, 0x7f5a45eabe30, 0xc208030120, 0xc567, 0x0, 0x0)\n    /usr/src/go/src/io/io.go:364 +0x278\ngithub.com/pachyderm/pachyderm/src/btrfs.recv(0x7d2bb0, 0x15, 0x7f5a45eabe30, 0xc208030120, 0x0, 0x0)\n    /go/src/github.com/pachyderm/pachyderm/src/btrfs/btrfs.go:727 +0x4ad\ngithub.com/pachyderm/pachyderm/src/btrfs.func\u00b7015(0x7f5a45eabe30, 0xc208030120, 0x0, 0x0)\n    /go/src/github.com/pachyderm/pachyderm/src/btrfs/btrfs_test.go:182 +0x54\ngithub.com/pachyderm/pachyderm/src/shell.CallCont(0xc2080783c0, 0xc208058ef8, 0x0, 0x0)\n    /go/src/github.com/pachyderm/pachyderm/src/shell/shell.go:48 +0x3b4\ngithub.com/pachyderm/pachyderm/src/btrfs.send(0x7d2bf0, 0x15, 0x7b0810, 0x9, 0xc208058ef8, 0x0, 0x0)\n    /go/src/github.com/pachyderm/pachyderm/src/btrfs/btrfs.go:753 +0x2cc\ngithub.com/pachyderm/pachyderm/src/btrfs.TestSendRecv(0xc20808a3f0)\n    /go/src/github.com/pachyderm/pachyderm/src/btrfs/btrfs_test.go:190 +0x9ac\ntesting.tRunner(0xc20808a3f0, 0x9472f0)\n    /usr/src/go/src/testing/testing.go:447 +0xbf\ncreated by testing.RunTests\n    /usr/src/go/src/testing/testing.go:555 +0xa8b\nexit status 2\nFAIL    github.com/pachyderm/pachyderm/src/btrfs    60.008s\nroot@8c50ab2203a4:/go/src/github.com/pachyderm/pachyderm/src/btrfs#\n```\nI still have the same environment:\n- Ubuntu 14.04 (x64)\n- Go 1.4.2\n- Docker 1.6.0\n- btrfs 3.19.1\nWhich version of btrfs do you use?\n. Oops, you're right. It was an old image. \nI'm trying to run it on my local side. My steps:\n``` bash\npavel@devel:~/workspace$ git clone git@github.com:pachyderm/pachyderm.git\nCloning into 'pachyderm'...\nremote: Counting objects: 6745, done.\nremote: Compressing objects: 100% (40/40), done.\nremote: Total 6745 (delta 21), reused 0 (delta 0), pack-reused 6705\nReceiving objects: 100% (6745/6745), 4.66 MiB | 1006.00 KiB/s, done.\nResolving deltas: 100% (3987/3987), done.\nChecking connectivity... done.\npavel@devel:~/workspace$ cd pachyderm\npavel@devel:~/workspace/pachyderm(master)$ export GOPATH=pwd  # don't want to pollute  global GOPATH namespace\npavel@devel:~/workspace/pachyderm(master)$ make test-deps\ngo get -d -v -t ./...\ngithub.com/aws/aws-sdk-go (download)\ngithub.com/vaughan0/go-ini (download)\ngithub.com/go-fsnotify/fsnotify (download)\ngithub.com/pachyderm/pachyderm (download)\ngithub.com/satori/go.uuid (download)\ngithub.com/stretchr/testify (download)\ngithub.com/golang/protobuf (download)\ngithub.com/peter-edge/go-google-protobuf (download)\nFetching https://golang.org/x/net/context?go-get=1\nParsing meta tags from https://golang.org/x/net/context?go-get=1 (status code 200)\nget \"golang.org/x/net/context\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/context?go-get=1\nget \"golang.org/x/net/context\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net?go-get=1\nParsing meta tags from https://golang.org/x/net?go-get=1 (status code 200)\ngolang.org/x/net (download)\nFetching https://google.golang.org/grpc?go-get=1\nParsing meta tags from https://google.golang.org/grpc?go-get=1 (status code 200)\nget \"google.golang.org/grpc\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc?go-get=1\ngoogle.golang.org/grpc (download)\ngithub.com/bradfitz/http2 (download)\nFetching https://golang.org/x/oauth2?go-get=1\nParsing meta tags from https://golang.org/x/oauth2?go-get=1 (status code 200)\nget \"golang.org/x/oauth2\": found meta tag main.metaImport{Prefix:\"golang.org/x/oauth2\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/oauth2\"} at https://golang.org/x/oauth2?go-get=1\ngolang.org/x/oauth2 (download)\nFetching https://golang.org/x/oauth2/google?go-get=1\nParsing meta tags from https://golang.org/x/oauth2/google?go-get=1 (status code 200)\nget \"golang.org/x/oauth2/google\": found meta tag main.metaImport{Prefix:\"golang.org/x/oauth2\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/oauth2\"} at https://golang.org/x/oauth2/google?go-get=1\nget \"golang.org/x/oauth2/google\": verifying non-authoritative meta tag\nFetching https://golang.org/x/oauth2?go-get=1\nParsing meta tags from https://golang.org/x/oauth2?go-get=1 (status code 200)\nFetching https://golang.org/x/oauth2/jwt?go-get=1\nParsing meta tags from https://golang.org/x/oauth2/jwt?go-get=1 (status code 200)\nget \"golang.org/x/oauth2/jwt\": found meta tag main.metaImport{Prefix:\"golang.org/x/oauth2\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/oauth2\"} at https://golang.org/x/oauth2/jwt?go-get=1\nget \"golang.org/x/oauth2/jwt\": verifying non-authoritative meta tag\nFetching https://golang.org/x/oauth2?go-get=1\nParsing meta tags from https://golang.org/x/oauth2?go-get=1 (status code 200)\nFetching https://google.golang.org/cloud/compute/metadata?go-get=1\nParsing meta tags from https://google.golang.org/cloud/compute/metadata?go-get=1 (status code 200)\nget \"google.golang.org/cloud/compute/metadata\": found meta tag main.metaImport{Prefix:\"google.golang.org/cloud\", VCS:\"git\", RepoRoot:\"https://code.googlesource.com/gocloud\"} at https://google.golang.org/cloud/compute/metadata?go-get=1\nget \"google.golang.org/cloud/compute/metadata\": verifying non-authoritative meta tag\nFetching https://google.golang.org/cloud?go-get=1\nParsing meta tags from https://google.golang.org/cloud?go-get=1 (status code 200)\ngoogle.golang.org/cloud (download)\ngithub.com/peter-edge/go-env (download)\ngithub.com/spf13/cobra (download)\ngithub.com/inconshreveable/mousetrap (download)\ngithub.com/spf13/pflag (download)\ngithub.com/coreos/go-etcd (download)\ngithub.com/ugorji/go (download)\ngithub.com/fsouza/go-dockerclient (download)\npavel@devel:~/workspace/pachyderm(master)$ make test\nbin/run bin/wrap bin/test -test.short ./...\nbtrfs-progs v3.19.1\nSee http://btrfs.wiki.kernel.org for more information.\nTurning ON incompat feature 'extref': increased hardlink limit per file to 65536\nTurning ON incompat feature 'skinny-metadata': reduced-size metadata extent refs\nERROR: device scan failed '/var/lib/pfs/btrfs.img' - Block device required\nfs created label (null) on /var/lib/pfs/btrfs.img\n    nodesize 16384 leafsize 16384 sectorsize 4096 size 10.00GiB\nSending build context to Docker daemon 58.37 MB\nSending build context to Docker daemon \nStep 0 : FROM ubuntu:15.04\n ---> 6be21d1e5d1e\nStep 1 : RUN apt-get update -yq &&   apt-get install -yq --no-install-recommends     btrfs-tools     build-essential     ca-certificates     curl     git\n ---> Using cache\n ---> fa9268db6aee\nStep 2 : RUN curl -sSL https://storage.googleapis.com/golang/go1.4.2.linux-amd64.tar.gz | tar -C /usr/local -xz &&   mkdir -p /go/bin\n ---> Using cache\n ---> f9291b08e71a\nStep 3 : ENV PATH /go/bin:/usr/local/go/bin:$PATH\n ---> Using cache\n ---> 485f5b67d2a3\nStep 4 : ENV GOPATH /go\n ---> Using cache\n ---> d23e6ec1af6b\nStep 5 : RUN go get -v golang.org/x/tools/cmd/vet &&   go get -v github.com/kisielk/errcheck &&   go get -v github.com/golang/lint/golint\n ---> Using cache\n ---> 1cdb7eb61b0f\nStep 6 : RUN mkdir -p /go/src/github.com/pachyderm/pachyderm\n ---> Using cache\n ---> eafdaa691d98\nStep 7 : WORKDIR /go/src/github.com/pachyderm/pachyderm\n ---> Using cache\n ---> 05592ee3b2a3\nStep 8 : RUN mkdir -p /go/src/github.com/pachyderm/pachyderm/etc/deps\n ---> Using cache\n ---> 1ca1b34f2915\nStep 9 : ADD etc/deps/deps.list /go/src/github.com/pachyderm/pachyderm/etc/deps/\n ---> Using cache\n ---> bf7f853060f6\nStep 10 : RUN cat etc/deps/deps.list | xargs go get -v\n ---> Running in 2b4035e2e613\ngithub.com/aws/aws-sdk-go (download)\ngithub.com/vaughan0/go-ini (download)\ngithub.com/bradfitz/http2 (download)\ngithub.com/coreos/go-etcd (download)\ngithub.com/ugorji/go (download)\ngithub.com/fsouza/go-dockerclient (download)\ngithub.com/go-fsnotify/fsnotify (download)\ngithub.com/golang/protobuf (download)\ngithub.com/inconshreveable/mousetrap (download)\ngithub.com/peter-edge/go-env (download)\ngithub.com/peter-edge/go-google-protobuf (download)\ngithub.com/satori/go.uuid (download)\ngithub.com/spf13/cobra (download)\ngithub.com/spf13/pflag (download)\ngithub.com/stretchr/testify (download)\nFetching https://golang.org/x/net/context?go-get=1\nParsing meta tags from https://golang.org/x/net/context?go-get=1 (status code 200)\nget \"golang.org/x/net/context\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/context?go-get=1\nget \"golang.org/x/net/context\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net?go-get=1\nParsing meta tags from https://golang.org/x/net?go-get=1 (status code 200)\ngolang.org/x/net (download)\nFetching https://golang.org/x/oauth2?go-get=1\nParsing meta tags from https://golang.org/x/oauth2?go-get=1 (status code 200)\nget \"golang.org/x/oauth2\": found meta tag main.metaImport{Prefix:\"golang.org/x/oauth2\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/oauth2\"} at https://golang.org/x/oauth2?go-get=1\ngolang.org/x/oauth2 (download)\nFetching https://google.golang.org/cloud/compute/metadata?go-get=1\nParsing meta tags from https://google.golang.org/cloud/compute/metadata?go-get=1 (status code 200)\nget \"google.golang.org/cloud/compute/metadata\": found meta tag main.metaImport{Prefix:\"google.golang.org/cloud\", VCS:\"git\", RepoRoot:\"https://code.googlesource.com/gocloud\"} at https://google.golang.org/cloud/compute/metadata?go-get=1\nget \"google.golang.org/cloud/compute/metadata\": verifying non-authoritative meta tag\nFetching https://google.golang.org/cloud?go-get=1\nParsing meta tags from https://google.golang.org/cloud?go-get=1 (status code 200)\ngoogle.golang.org/cloud (download)\nFetching https://google.golang.org/grpc?go-get=1\nParsing meta tags from https://google.golang.org/grpc?go-get=1 (status code 200)\nget \"google.golang.org/grpc\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc?go-get=1\ngoogle.golang.org/grpc (download)\ngithub.com/aws/aws-sdk-go/aws/awserr\ngithub.com/aws/aws-sdk-go/aws/awsutil\ngithub.com/vaughan0/go-ini\ngithub.com/aws/aws-sdk-go/internal/endpoints\ngithub.com/aws/aws-sdk-go/internal/protocol/query/queryutil\ngithub.com/aws/aws-sdk-go/internal/protocol/xml/xmlutil\ngithub.com/bradfitz/http2/hpack\ngithub.com/ugorji/go/codec\ngithub.com/fsouza/go-dockerclient/vendor/github.com/docker/libcontainer/user\ngithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/parsers\ngithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/ulimit\ngithub.com/fsouza/go-dockerclient/vendor/github.com/Sirupsen/logrus\ngithub.com/aws/aws-sdk-go/aws/credentials\ngithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/ioutils\ngithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/promise\ngithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/units\ngithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/homedir\ngithub.com/go-fsnotify/fsnotify\ngithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/mflag\ngithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/system\ngithub.com/bradfitz/http2\ngithub.com/aws/aws-sdk-go/aws\ngithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/pools\ngithub.com/golang/protobuf/proto\ngithub.com/inconshreveable/mousetrap\ngithub.com/peter-edge/go-env\ngithub.com/satori/go.uuid\ngithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/fileutils\ngithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/archive\ngithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/opts\ngithub.com/fsouza/go-dockerclient/vendor/github.com/docker/docker/pkg/stdcopy\ngithub.com/spf13/pflag\ngithub.com/stretchr/testify/assert\ngithub.com/aws/aws-sdk-go/internal/protocol/query\ngithub.com/aws/aws-sdk-go/internal/protocol/rest\ngolang.org/x/net/context\ngolang.org/x/net/internal/timeseries\ngolang.org/x/net/trace\ngithub.com/aws/aws-sdk-go/internal/protocol/restxml\ngithub.com/aws/aws-sdk-go/internal/signer/v4\ngithub.com/spf13/cobra\ngolang.org/x/oauth2/internal\ngolang.org/x/oauth2/jws\ngithub.com/peter-edge/go-google-protobuf\ngoogle.golang.org/cloud/internal\ngoogle.golang.org/cloud/compute/metadata\ngoogle.golang.org/grpc/codes\ngoogle.golang.org/grpc/grpclog\ngoogle.golang.org/grpc/metadata\ngolang.org/x/oauth2\ngolang.org/x/oauth2/jwt\ngolang.org/x/oauth2/google\ngoogle.golang.org/grpc/credentials\ngoogle.golang.org/grpc/transport\ngithub.com/aws/aws-sdk-go/service/s3\ngithub.com/stretchr/testify/require\ngoogle.golang.org/grpc\ngithub.com/stretchr/testify/suite\ngithub.com/fsouza/go-dockerclient\ngithub.com/aws/aws-sdk-go/service/s3/s3manager\ngithub.com/coreos/go-etcd/etcd\n ---> d62b5f4be8fc\nRemoving intermediate container 2b4035e2e613\nStep 11 : RUN go get github.com/coreos/go-etcd/etcd &&   cd /go/src/github.com/coreos/go-etcd &&   git checkout release-0.4\n ---> Running in 0b6c0cb0fe6e\nSwitched to a new branch 'release-0.4'\nBranch release-0.4 set up to track remote branch release-0.4 from origin.\n ---> 0fa08b1ccc49\nRemoving intermediate container 0b6c0cb0fe6e\nStep 12 : ADD . /go/src/github.com/pachyderm/pachyderm/\n ---> 9ae543f8eb70\nRemoving intermediate container b924b7a74adf\nSuccessfully built 9ae543f8eb70\nPFS_DIR=/var/lib/pfs/btrfs\nPFS_HOST_VOLUME=/var/lib/pfs/btrfs/wrap-1\nPFS_LOCAL_VOLUME=/var/lib/pfs/btrfs/wrap-1\nPFS_BTRFS_ROOT=/var/lib/pfs/btrfs/wrap-1\n../../aws/aws-sdk-go/internal/features/shared/shared.go:15:2: cannot find package \"github.com/lsegal/gucumber\" in any of:\n    /usr/local/go/src/github.com/lsegal/gucumber (from $GOROOT)\n    /go/src/github.com/lsegal/gucumber (from $GOPATH)\nsrc/github.com/bradfitz/http2/h2i/h2i.go:41:2: cannot find package \"golang.org/x/crypto/ssh/terminal\" in any of:\n    /usr/local/go/src/golang.org/x/crypto/ssh/terminal (from $GOROOT)\n    /go/src/golang.org/x/crypto/ssh/terminal (from $GOPATH)\nsrc/github.com/peter-edge/go-tools/common/aws/commonaws.go:6:2: cannot find package \"github.com/peter-edge/go-cacerts\" in any of:\n    /usr/local/go/src/github.com/peter-edge/go-cacerts (from $GOROOT)\n    /go/src/github.com/peter-edge/go-cacerts (from $GOPATH)\nsrc/github.com/peter-edge/go-tools/common/aws/commonaws.go:8:2: cannot find package \"github.com/peter-edge/go-tools/common\" in any of:\n    /usr/local/go/src/github.com/peter-edge/go-tools/common (from $GOROOT)\n    /go/src/github.com/peter-edge/go-tools/common (from $GOPATH)\nsrc/github.com/peter-edge/go-tools/ecs-list-ips/main.go:11:2: cannot find package \"github.com/peter-edge/go-tools/common/aws\" in any of:\n    /usr/local/go/src/github.com/peter-edge/go-tools/common/aws (from $GOROOT)\n    /go/src/github.com/peter-edge/go-tools/common/aws (from $GOPATH)\n../../stretchr/testify/mock/mock.go:11:2: cannot find package \"github.com/stretchr/objx\" in any of:\n    /usr/local/go/src/github.com/stretchr/objx (from $GOROOT)\n    /go/src/github.com/stretchr/objx (from $GOPATH)\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/context: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/context expects import \"golang.org/x/net/context\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/dict: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/dict expects import \"golang.org/x/net/dict\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/html: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/html expects import \"golang.org/x/net/html\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/html/atom: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/html/atom expects import \"golang.org/x/net/html/atom\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/html/charset: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/html/charset expects import \"golang.org/x/net/html/charset\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/icmp: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/icmp expects import \"golang.org/x/net/icmp\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/idna: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/idna expects import \"golang.org/x/net/idna\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/internal/iana: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/internal/iana expects import \"golang.org/x/net/internal/iana\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/internal/nettest: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/internal/nettest expects import \"golang.org/x/net/internal/nettest\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/internal/timeseries: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/internal/timeseries expects import \"golang.org/x/net/internal/timeseries\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/ipv4: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/ipv4 expects import \"golang.org/x/net/ipv4\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/ipv6: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/ipv6 expects import \"golang.org/x/net/ipv6\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/netutil: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/netutil expects import \"golang.org/x/net/netutil\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/proxy: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/proxy expects import \"golang.org/x/net/proxy\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/publicsuffix: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/publicsuffix expects import \"golang.org/x/net/publicsuffix\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/trace: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/trace expects import \"golang.org/x/net/trace\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/webdav: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/webdav expects import \"golang.org/x/net/webdav\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/net/websocket: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/net/websocket expects import \"golang.org/x/net/websocket\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2 expects import \"golang.org/x/oauth2\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/clientcredentials: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/clientcredentials expects import \"golang.org/x/oauth2/clientcredentials\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/facebook: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/facebook expects import \"golang.org/x/oauth2/facebook\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/github: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/github expects import \"golang.org/x/oauth2/github\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/google: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/google expects import \"golang.org/x/oauth2/google\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/jws: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/jws expects import \"golang.org/x/oauth2/jws\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/linkedin: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/linkedin expects import \"golang.org/x/oauth2/linkedin\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/odnoklassniki: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/odnoklassniki expects import \"golang.org/x/oauth2/odnoklassniki\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/paypal: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/paypal expects import \"golang.org/x/oauth2/paypal\"\ncan't load package: package github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/vk: code in directory /go/src/github.com/pachyderm/pachyderm/src/golang.org/x/oauth2/vk expects import \"golang.org/x/oauth2/vk\"\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/cloud: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/cloud expects import \"google.golang.org/cloud\"\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/cloud/bigquery: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/cloud/bigquery expects import \"google.golang.org/cloud/bigquery\"\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/cloud/bigtable: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/cloud/bigtable expects import \"google.golang.org/cloud/bigtable\"\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/cloud/bigtable/bttest: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/cloud/bigtable/bttest expects import \"google.golang.org/cloud/bigtable/bttest\"\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/cloud/container: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/cloud/container expects import \"google.golang.org/cloud/container\"\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/cloud/datastore: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/cloud/datastore expects import \"google.golang.org/cloud/datastore\"\n/go/src/google.golang.org/cloud/bigquery/bigquery.go:24:2: cannot find package \"google.golang.org/api/bigquery/v2\" in any of:\n    /usr/local/go/src/google.golang.org/api/bigquery/v2 (from $GOROOT)\n    /go/src/google.golang.org/api/bigquery/v2 (from $GOPATH)\nsrc/google.golang.org/cloud/examples/bigtable/bigtable-hello/helloworld.go:21:2: cannot find package \"google.golang.org/appengine\" in any of:\n    /usr/local/go/src/google.golang.org/appengine (from $GOROOT)\n    /go/src/google.golang.org/appengine (from $GOPATH)\nsrc/google.golang.org/cloud/examples/bigtable/bigtable-hello/helloworld.go:22:2: cannot find package \"google.golang.org/appengine/log\" in any of:\n    /usr/local/go/src/google.golang.org/appengine/log (from $GOROOT)\n    /go/src/google.golang.org/appengine/log (from $GOPATH)\nsrc/google.golang.org/cloud/examples/bigtable/bigtable-hello/helloworld.go:23:2: cannot find package \"google.golang.org/appengine/user\" in any of:\n    /usr/local/go/src/google.golang.org/appengine/user (from $GOROOT)\n    /go/src/google.golang.org/appengine/user (from $GOPATH)\n/go/src/google.golang.org/cloud/pubsub/pubsub.go:33:2: cannot find package \"google.golang.org/api/googleapi\" in any of:\n    /usr/local/go/src/google.golang.org/api/googleapi (from $GOROOT)\n    /go/src/google.golang.org/api/googleapi (from $GOPATH)\n/go/src/google.golang.org/cloud/pubsub/pubsub.go:34:2: cannot find package \"google.golang.org/api/pubsub/v1beta2\" in any of:\n    /usr/local/go/src/google.golang.org/api/pubsub/v1beta2 (from $GOROOT)\n    /go/src/google.golang.org/api/pubsub/v1beta2 (from $GOPATH)\n/go/src/google.golang.org/cloud/storage/acl.go:21:2: cannot find package \"google.golang.org/api/storage/v1\" in any of:\n    /usr/local/go/src/google.golang.org/api/storage/v1 (from $GOROOT)\n    /go/src/google.golang.org/api/storage/v1 (from $GOPATH)\nsrc/google.golang.org/cloud/examples/storage/appengine/app.go:30:2: cannot find package \"google.golang.org/appengine/file\" in any of:\n    /usr/local/go/src/google.golang.org/appengine/file (from $GOROOT)\n    /go/src/google.golang.org/appengine/file (from $GOPATH)\nsrc/google.golang.org/cloud/examples/storage/appengine/app.go:32:2: cannot find package \"google.golang.org/appengine/urlfetch\" in any of:\n    /usr/local/go/src/google.golang.org/appengine/urlfetch (from $GOROOT)\n    /go/src/google.golang.org/appengine/urlfetch (from $GOPATH)\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/cloud/pubsub: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/cloud/pubsub expects import \"google.golang.org/cloud/pubsub\"\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/cloud/storage: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/cloud/storage expects import \"google.golang.org/cloud/storage\"\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/grpc: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/grpc expects import \"google.golang.org/grpc\"\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/grpc/codes: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/grpc/codes expects import \"google.golang.org/grpc/codes\"\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/grpc/credentials: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/grpc/credentials expects import \"google.golang.org/grpc/credentials\"\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/grpc/grpclog: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/grpc/grpclog expects import \"google.golang.org/grpc/grpclog\"\nsrc/google.golang.org/grpc/grpclog/glogger/glogger.go:40:2: cannot find package \"github.com/golang/glog\" in any of:\n    /usr/local/go/src/github.com/golang/glog (from $GOROOT)\n    /go/src/github.com/golang/glog (from $GOPATH)\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/grpc/metadata: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/grpc/metadata expects import \"google.golang.org/grpc/metadata\"\ncan't load package: package github.com/pachyderm/pachyderm/src/google.golang.org/grpc/transport: code in directory /go/src/github.com/pachyderm/pachyderm/src/google.golang.org/grpc/transport expects import \"google.golang.org/grpc/transport\"\nmake: *** [test] Error 1\n```\nWhat am I doing wrong? or maybe the tests must be run only through vagrant? \n. yeap, I did. Did you try to run my sequence of steps?\n. Peter, I got to run everything in Vagrant/Virtualbox, but locally I still have problems with the tests. \nI thought that the problem could be in the version of btrfs. I have btrfs v3.19.1 and Ubuntu 15 has btrfs v3.17. I installed v3.19.1 on Ubuntu 15 and passed all the tests again, so I guess that's not about btrfs. :)\nMaybe because I am using Ubuntu 14 (Vagrant uses 15), I will try to reproduce the situation in Virtualbox.\nI think the issue can be closed. If I find a problem with Ubuntu 14.04 then it is probably better to create another issue.\nThank you for your patience.\n. ",
    "mattnenterprise": "@jdoliner @peter-edge Are you still looking to switch to using libkv? If you are I would like to have a go at it.\n. This doesn't crash pfsd for me running everything locally in docker. I get\n\nbash: /pfs/foo/file: Input/output error\n. This currently works for me:\n$ pachctl create-repo foo\n$ pachctl finish-commit foo bar\nrpc error: code = 2 desc = \"commit foo/bar not found\"\n$ pachctl start-commit foo\naf44ba6e7aba4e399a563ca5397f3586\n. This also happens to me on linux to. I got it to work by mounting the etc/kube directory to the kubelet container. I created a pull-request for this here. I tested it on both Mac OSX and Ubuntu Linux.\n. So changing the storage driver to devicemapper works. I will delete my pull request and close this issue, as you have already added the fix to the documentation.\n. I believe I have it in the correct place now.\n. This looks like it hasn't been merged.\n. This relates to #157\n. \n",
    "Manikandan-Selvaganesh": "Why do you need to remove pfsutil?\nCould you please describe more!!\n. ",
    "padioca": "Any update on this? I'm in the exact same boat...\n. No worries, thanks for the update!\n. ",
    "cxxly": "Have this updated? I haven't found a example, and the document is not clear.\n. Ok, I will make some tests about this step by step, and talk about here.\n. ",
    "brinman2002": "Upgraded to latest docker and it still didn't start, although the error message was just-\nvagrant@vagrant-ubuntu-vivid-64:~/go/src/github.com/pachyderm/pachyderm$ docker logs pachyderm_ppsd_1 \nKUBERNETES_PORT_443_TCP_ADDR not set\nThe documentation seems to state that Kubernetes is not necessary to run Pachyderm for testing/dev purposes.\n. Adding this environment under ppsd in the docker-compose.yml seems to fix it but as a complete noob to Pachyderm and docker, I'm not sure if it's exactly the fix needed.\nenvironment:\n    - KUBERNETES_PORT_443_TCP_ADDR=127.0.0.1\n. @jdoliner good information, thanks.  I was having trouble getting the etc/kube/start-kube-docker.sh script to execute properly as well.  I should have some time today to reproduce that and I'll log another issue if I'm not able to get it running.\n. Couple things I noted; I don't see anything in the script starting the rethinkdb container so I've done that manually (docker run -d rethinkdb:2.0.4).  The other is that the kubelet container just doesn't start hyperkube.  If I commit the container and run it with bash, then execute the same command in the script, it seems to start but can't seem to connect to rethinkdb on 8080.  Still learning my way around docker though; I'll keep playing with it.\nedit: sorry for the comment spam, but I did realize that 8080 is one of the kube ports and I was misreading the quickstart \"to check if it worked\" bit.\n. Here is the output from running hyperkube manually. \nhyperkube_fromcmdline.txt\nOne problem with the container is that the volumes don't appear to be linking to the host properly.\nroot@65f570570abc:/# ls /var/run/   \nkubernetes  lock  utmp\nvagrant@vagrant-ubuntu-vivid-64:~/go/src/github.com/pachyderm/pachyderm$ ls /var/run\nacpid.socket  cloud-init    dhclient.eth0.pid  initctl    motd.dynamic  plymouth    rpcbind       rsyslogd.pid     sshd       thermald    utmp\natd.pid       crond.pid     docker             initramfs  mount         pppconfig   rpcbind.lock  screen           sshd.pid   tmpfiles.d  uuidd\nblkid         crond.reboot  docker.pid         lock       network       puppet      rpcbind.sock  sendsigs.omit.d  sysconfig  udev\nchef          dbus          docker.sock        log        pcscd         resolvconf  rpc_pipefs    shm              systemd    user\n. Unfortunately I didn't really keep good track of everything I tried, but it does seem like Docker is the problem or at least the symptom of another problem.  I've gotten kubernetes to start and pachctl to at least connect to it by dropping back to earlier versions of everything, but then the enhancements that seem to be needed by Pachyderm aren't there.  I've gotten the \"kubelet\" container to start by committing the container created by the script and running /bin/bash and then running hyperkube manually, but that doesn't start up the pod. \nNot directly related, but the experimental Docker build also can't stop containers due to it complaining about permissions.  Based on doc for Kubernetes, I added cgroup_enable=memory swapaccount=1 to the Kernel parameters for the VM but I wonder if there is some other Kernel setup needed?\n. > So I'm a little confused on the status of this issue. You have Kubernetes up and running but it's the wrong version? What happens when you try to start the correct version?\nSorry for the confusion.  At one point Kubernetes would start when I referenced an earlier version.  I probably wasn't using the Pachyderm master.json and Pachyderm didn't work correctly.  I imagine that this would be expected.  The correct version as referenced by the start kube script does not start correctly.\n\nAlso could you try this without Vagrant? All you need is Docker and Golang which I think makes Vagrant sort of unneeded. I think we should consider just removing the Vagrant file.\n\nVagrant makes it possible to blow away bad experiments and start over.  Is Docker or any of the other technologies used known to not play well in a VM?  I do understand if you don't want to maintain an official Vagrantfile.\nThanks\n. Ok, I do have to correct myself.  I am able to get a Kubernetes cluster working by using the instructions on their site substituting the 1.1.2 version instead. But, that leaves me with this error-\n```\nvagrant@ubuntu-14:~/go/src/github.com/pachyderm/pachyderm$ $HOME/go/bin/pachctl create-cluster\nReplicationController \"pfsd-rc\" is invalid: spec.template.spec.containers[0].securityContext.privileged: forbidden '<*>(0xc2092af1f8)true'\nvagrant@ubuntu-14:~/go/src/github.com/pachyderm/pachyderm$ $HOME/go/bin/kubectl get svc\nNAME         LABELS                                    SELECTOR      IP(S)        PORT(S)\netcd         app=etcd,suite=pachyderm                  app=etcd      10.0.0.173   2379/TCP\n                                                                                  2380/TCP\nkubernetes   component=apiserver,provider=kubernetes           10.0.0.1     443/TCP\nrethink      app=rethink,suite=pachyderm               app=rethink   10.0.0.194   8080/TCP\n                                                                                  28015/TCP\n                                                                                  29015/TCP\n```\nIn this setup the custom master.json doesn't get copied to the Kubernetes image, so presumably the configuration in it is a part of the problem?\nAlso forgot to mention that I switched to a Phusion based image (phusion/ubuntu-14.04-amd64) instead of the default Ubuntu as they are supposed to be more Docker friendly.  I think Pachyderm is a great idea and I'm hoping I can get this working.\n. Much to my surprise, Docker doesn't work correctly in Vagrant (with VirtualBox as the backend anyway) but it is working (better) on bare metal.  I can only assume it is doing lower level virtualization that isn't supported by VirtualBox.\nI'm still not completely working but I'm past all of the issues documented here.\n. I just tried to update and now make install doesn't work.  The launch goal triggers install so it fails as well.  When I made the previous comment, I was using the /etc/kube script as that was what was suggested on issue 160.\n```\n~/go/src/github.com/pachyderm/pachyderm$ make install\nGO15VENDOREXPERIMENT=1 go install ./src/cmd/pachctl\ngo.pedge.io/pkg/sync\n../../../go.pedge.io/pkg/sync/lazy_loader.go:21: undefined: atomic.Value\n../../../go.pedge.io/pkg/sync/lazy_loader.go:22: undefined: atomic.Value\ngolang.org/x/crypto/ssh\n../../../golang.org/x/crypto/ssh/keys.go:492: undefined: crypto.Signer\ngithub.com/pachyderm/pachyderm/src/pkg/shard\nsrc/pkg/shard/shard.pb.log.go:10: undefined: protolog.Register\nsrc/pkg/shard/shard.pb.log.go:10: undefined: protolog.MessageType_MESSAGE_TYPE_EVENT\nsrc/pkg/shard/shard.pb.log.go:10: undefined: protolog.Message\nsrc/pkg/shard/shard.pb.log.go:11: undefined: protolog.Register\nsrc/pkg/shard/shard.pb.log.go:11: undefined: protolog.MessageType_MESSAGE_TYPE_EVENT\nsrc/pkg/shard/shard.pb.log.go:11: undefined: protolog.Message\nsrc/pkg/shard/shard.pb.log.go:12: undefined: protolog.Register\nsrc/pkg/shard/shard.pb.log.go:12: undefined: protolog.MessageType_MESSAGE_TYPE_EVENT\nsrc/pkg/shard/shard.pb.log.go:12: undefined: protolog.Message\nsrc/pkg/shard/shard.pb.log.go:13: undefined: protolog.Register\nsrc/pkg/shard/shard.pb.log.go:13: too many errors\nMakefile:55: recipe for target 'install' failed\nmake: *** [install] Error 2\n```\n. The previous error was from not having my machine set up correctly.  I think I have it set up right now but get is still having issues:\n```\n~$ go get github.com/pachyderm/pachyderm/...\npackage github.com/pachyderm/pachyderm/vendor/github.com/emicklei/go-restful/examples/google_app_engine\n    imports google.golang.com/appengine: unrecognized import path \"google.golang.com/appengine\"\npackage github.com/pachyderm/pachyderm/vendor/github.com/emicklei/go-restful/examples/google_app_engine\n    imports google.golang.com/appengine/memcache: unrecognized import path \"google.golang.com/appengine/memcache\"\npackage github.com/pachyderm/pachyderm/vendor/github.com/emicklei/go-restful/examples/google_app_engine/datastore\n    imports google.golang.com/appengine/datastore: unrecognized import path \"google.golang.com/appengine/datastore\"\npackage github.com/pachyderm/pachyderm/vendor/github.com/emicklei/go-restful/examples/google_app_engine/datastore\n    imports google.golang.com/appengine/user: unrecognized import path \"google.golang.com/appengine/user\"\npackage github.com/pachyderm/pachyderm/vendor/go.pedge.io/protolog/cmd/protoc-gen-protolog\n    imports go.pedge.io/proto/plugin: cannot find package \"go.pedge.io/proto/plugin\" in any of:\n    /opt/go/src/go.pedge.io/proto/plugin (from $GOROOT)\n    /home/brandon/go/src/go.pedge.io/proto/plugin (from $GOPATH)\n```\n. Yeah I'm not sure what changed but make install is working now. \nDo you recommend make launch or etc/kube/start-kube-docker.sh to run Pachyderm? make launch still uses Docker Compose, which you said wasn't \"a viable way to deploy\".\n. Running etc/kube/start-kube-docker.sh seems to be working now.  Thanks!\n. Not completely working after all.  I can create repos with pachctl but it doesn't create commits.\n~/go/src/github.com/pachyderm/pachyderm$ sudo  $(which pachctl) start-commit foo\nrpc error: code = 2 desc = \"btrfs subvolume create /pfs/btrfs/repo/foo/120d16b28a28451c918fe4762cd8ac24: exit status 1\\n\\tERROR: can't access to '/pfs/btrfs/repo/foo'\\n\"\n~/go/src/github.com/pachyderm/pachyderm$ ls /pfs/foo\nls: cannot access /pfs/foo: Input/output error\n~/go/src/github.com/pachyderm/pachyderm$ ls /pfs\ndata  output\nAlso, if you do the echo in the quickstart to demonstrate that you can't write to the data directory, it throws things into a bad state.  The echo command never exits and the pachctl mount command continuously throws errors like these-\n2015/12/05 21:36:32 transport: http2Client.notifyError got notified that the client transport was broken read tcp 127.0.0.1:40089->127.0.0.1:650: read: connection reset by peer.\n2015/12/05 21:36:32 transport: http2Client.notifyError got notified that the client transport was broken read tcp 127.0.0.1:40091->127.0.0.1:650: read: connection reset by peer.\n2015/12/05 21:36:32 transport: http2Client.notifyError got notified that the client transport was broken read tcp 127.0.0.1:40093->127.0.0.1:650: read: connection reset by peer.\n2015/12/05 21:36:32 transport: http2Client.notifyError got notified that the client transport was broken read tcp 127.0.0.1:40095->127.0.0.1:650: read: connection reset by peer.\n2015/12/05 21:36:32 transport: http2Client.notifyError got notified that the client transport was broken read tcp 127.0.0.1:40097->127.0.0.1:650: read: connection reset by peer.\n2015/12/05 21:36:32 transport: http2Client.notifyError got notified that the client transport was broken read tcp 127.0.0.1:40099->127.0.0.1:650: read: connection reset by peer.\n2015/12/05 21:36:32 transport: http2Client.notifyError got notified that the client transport was broken read tcp 127.0.0.1:40101->127.0.0.1:650: read: connection reset by peer.\nI've had to reboot to make to things work again.\n. On the quickstart, is using \"foo\" just a documentation error?\nbrandon@tamami:~$ pachctl create-repo data\nbrandon@tamami:~$ pachctl create-repo output\nbrandon@tamami:~$ ls /pfs\ndata  output\nbrandon@tamami:~$ pachctl start-commit data\n7ea12a9dac744eec817c634e4d486bd0\nbrandon@tamami:~$ echo \"Hello world\" > /pfs/data/7ea12a9dac744eec817c634e4d486bd0/hello.txt\nbrandon@tamami:~$ pachctl finish-commit data 7ea12a9dac744eec817c634e4d486bd0\nbrandon@tamami:~$ cat /pfs/data/7ea12a9dac744eec817c634e4d486bd0/hello.txt \nHello world\n. According to the quickstart, this shouldn't work either since it hasn't committed yet.\nbrandon@tamami:~$ pachctl start-commit data\n559e851fc6f64086be60740d0a890540\nbrandon@tamami:~$ echo \"Hello world\" > /pfs/data/559e851fc6f64086be60740d0a890540/hello2.txt\nbrandon@tamami:~$ cat /pfs/data/559e851fc6f64086be60740d0a890540/hello2.txt \nHello world\nI know you mentioned the doc is a little stale (and as a developer, I know how that happens :D ), but I thought I'd point this out because it seems like it could be an issue in the code as well.\n. Another hang\n```\nbrandon@tamami:~$ pachctl list-commit data\nID                                 PARENT              STATUS              STARTED             FINISHED            TOTAL_SIZE          DIFF_SIZE         \n559e851fc6f64086be60740d0a890540                 writeable           45 years ago                            28 B                12 B              \n7ea12a9dac744eec817c634e4d486bd0                 read-only           45 years ago        15 minutes ago      26 B                12 B              \nbrandon@tamami:~$ pachctl finish-commit 559e851fc6f64086be60740d0a890540\nExpected 2 args, got 1\nbrandon@tamami:~$ pachctl finish-commit 559e851fc6f64086be60740d0a890540 data\nrpc error: code = 2 desc = \"commit 559e851fc6f64086be60740d0a890540/data not found\"\nbrandon@tamami:~$ pachctl finish-commit data 559e851fc6f64086be60740d0a890540 \nhung on this command\n```\n. Great, thanks.  Is there an IRC/Slack chat/mailing list for more informal questions?\n. ",
    "jaybennett89": "@brinman2002, Docker is very similar to Vagrant in it's functions in that it also manages the automated deployment of virtual machines. So why run a virtualization Inception? Try docker on its own!\n. ",
    "teodor-pripoae": "Hi,\nI'm running a default vagrant cluster with 3 k8s nodes to simulate a production cluster.  pachyderm/pfsd is crashing every time. All other containers seem to work fine.\nCluster started with: KUBERNETES_MINION_MEMORY=2048 NUM_MINIONS=3 KUBERNETES_PROVIDER=vagrant ./kube-up.sh\n```\n[vagrant@kubernetes-minion-2 ~]$ sudo docker logs -f 4ff34f7e27da\nTurning ON incompat feature 'extref': increased hardlink limit per file to 65536\nWARNING! - Btrfs v3.12 IS EXPERIMENTAL\nWARNING! - see http://btrfs.wiki.kernel.org before using\nfs created label (null) on /pfs-img/btrfs.img\n    nodesize 16384 leafsize 16384 sectorsize 4096 size 10.00GiB\nBtrfs v3.12\nmount: could not find any free loop device\n```\n. Hi,\nI'm getting this error when I'm doing make launch. It seems on older versions kubernetes was accepting a bool pointer for privileged, but no it just takes the value. I'm running kubernetes 1.1.3\nThe ReplicationController \"ppsd\" is invalid.\nspec.template.spec.containers[0].securityContext.privileged: forbidden '<*>(0xc20b22c4b0)true'\nEdit: it seems kubernetes resources are created using kubectl, I'm running hyperkube and I think allow_privileged is disabled. I will investigate.\nEdit2: fixed by creating a docker-machine with device-mapper and using make launch-dev. It seems my kubernetes instalation based on hyperkube was missing privileged=true somewhere.\n. Done, I signed it.\nI remember I made another contribution in the past, does this automatically apply to that one ? I'm just checking so you are 100% covered by the agreement.\n. I had the repo created in /tmp/pfs which was mounted using pachctl mount -p /tmp/pfs. \nI'm working right now to setup a vagrant file with everything needed for working with pfs. Is this something I can contribute ?\nThere were a few problems:\n- I can't mount by default to /pfs without beeing root\n- If running on osx I need to install osx-fuse from homebrew which looks buggy.\n- Port forwarding from docker-machine does not work well: In the quickstart it is recommented to port forward host port 650 to port 30650 running inside the vm. I can't bind to this port using ssh without beeing root. If I run it as root, ssh will not work since I don't have the public/private key set up.\n- After binding on another port and setting PFS_ADDRESS and PPS_ADDRESS to localhost:30650 and 30651, ssh port forward breaks gRPC connection becouse it is tcp over tcp. I had this problem while port forwarding other gRPC services too, for example youtube vitess or my own services.\n- if I set PFS_ADDRESS=192.168.99.101:30650 and export PPS_ADDRESS=192.168.99.101:30651 in my zshrc it works and I can succesfully mount the fuse filesystem, but it does not correctly get replicated. I lost my energy at this point :disappointed: \nI'm thinking of the following solution which will help other people to get a quick start using pfs:\n- Create a ubuntu machine with docker running using device-mapper\n- Start kubernetes cluster automatically using upstart so it can start on machine boot\n- Persist etcd, so after vm restart kubernetes starts the pods again. Currently this pods need to be created every time if cluster is running under docker-machine.\n- Mount /pfs automatically: here I need to find a way to delay this mount on machine boot since kubernetes pods take a while to start\nWhat do you think ?\nPS. Also, after created the pipeline which silently failed, pachctl list-pipelines was throwing a go panic, and pachctl delete-pipeline was exiting cleanely but the pipeline was not deleted. Here I suspect another silent error.\n. Yes, I'd prefer using docker-machine too, I couldn't make docker work with devicemapper in vagrant without any bugs.\nIt looks very convenient to mount to /pfs when running inside docker and I think you can let like this by default but maybe there should be a message if unable to mount to this path:\n```\n$ pachctl mount\nmkdir /pfs: permission denied\n$ # proposed:\n$ pachctl mount\nmkdir /pfs: permission denied\nIf you are not running as root you can use pachctl mount -p /path/to/mount/pfs to mount it to a different directory.\n```\nCurrently as far as I understand, pachctl create-repo works by sending gRPC requests to pachctl mount daemon so I don't need the PFS_ADDRESS here. Somehow, writing multiple times to a file inside a commit duplicates the file content:\n$ ls /tmp/pfs      # ignore this files since I'm creating a new repo:\ndata                  example               pipeline-grep-example\n$ pachctl create-repo testrepo\n$ pachctl list-repo\nNAME                    CREATED             SIZE\nexample                 46 years ago        0 B\ntestrepo                46 years ago        0 B\ndata                    46 years ago        0 B\npipeline-grep-example   46 years ago        0 B\n$ ls /tmp/pfs\ndata                  example               pipeline-grep-example testrepo\n$ commit=$(pachctl start-commit testrepo)\n$ echo $commit\n7c06c56e3a544a968060415679b6da5d\n$ echo foo > /tmp/pfs/testrepo/$commit/file\n$ echo bar > /tmp/pfs/testrepo/$commit/file\n$ echo baz > /tmp/pfs/testrepo/$commit/file\n$ pachctl finish-commit testrepo $commit\n$ cat /tmp/pfs/testrepo/$commit/file # bar gets duplicated\nfoo\nbar\nbaz\nbar\n$ pachctl create-pipeline -f grep-example/grep-example-pipeline.json # changed pipeline name and repo\nppsd log:\n2016-01-13T06:57:44Z ERROR protorpclog.Call {\"service\":\"pachyderm.pps.persist.API\",\"method\":\"CreatePipelineInfo\",\"request\":\"pipeline_name:\\\"grep-example\\\" transform:\\u003cimage:\\\"pachyderm/grep-example\\\" cmd:\\\"/bin/grep-example.sh\\\" \\u003e shards:1 input_repo:\\u003cname:\\\"\\\\n\\\\010testrepo\\\" \\u003e created_at:\\u003cseconds:1452668264 nanos:279957078 \\u003e \",\"response\":\"\\u003cnil\\u003e\",\"error\":\"Duplicate primary key `PipelineName`:\\n{\\n\\t\\\"CreatedAt\\\":\\t{\\n\\t\\t\\\"Nanos\\\":\\t179209495,\\n\\t\\t\\\"Seconds\\\":\\t1452666254\\n\\t},\\n\\t\\\"InputRepo\\\":\\t[\\n\\t\\t{\\n\\t\\t\\t\\\"Name\\\":\\t\\\"\\\\n\\\\u0004data\\\"\\n\\t\\t}\\n\\t],\\n\\t\\\"PipelineName\\\":\\t\\\"grep-example\\\",\\n\\t\\\"Shards\\\":\\t1,\\n\\t\\\"Transform\\\":\\t{\\n\\t\\t\\\"Cmd\\\":\\t[\\n\\t\\t\\t\\\"/bin/grep-example.sh\\\"\\n\\t\\t],\\n\\t\\t\\\"Image\\\":\\t\\\"pachyderm/grep-example\\\",\\n\\t\\t\\\"Stdin\\\":\\t\\\"\\\"\\n\\t}\\n}\\n{\\n\\t\\\"CreatedAt\\\":\\t{\\n\\t\\t\\\"Nanos\\\":\\t279957078,\\n\\t\\t\\\"Seconds\\\":\\t1452668264\\n\\t},\\n\\t\\\"InputRepo\\\":\\t[\\n\\t\\t{\\n\\t\\t\\t\\\"Name\\\":\\t\\\"\\\\n\\\\btestrepo\\\"\\n\\t\\t}\\n\\t],\\n\\t\\\"PipelineName\\\":\\t\\\"grep-example\\\",\\n\\t\\\"Shards\\\":\\t1,\\n\\t\\\"Transform\\\":\\t{\\n\\t\\t\\\"Cmd\\\":\\t[\\n\\t\\t\\t\\\"/bin/grep-example.sh\\\"\\n\\t\\t],\\n\\t\\t\\\"Image\\\":\\t\\\"pachyderm/grep-example\\\",\\n\\t\\t\\\"Stdin\\\":\\t\\\"\\\"\\n\\t}\\n}\",\"duration\":{\"seconds\":\"0\",\"nanos\":8113960}}\n2016-01-13T06:57:44Z ERROR protorpclog.Call {\"service\":\"pachyderm.pps.PipelineAPI\",\"method\":\"CreatePipeline\",\"request\":\"pipeline:\\u003cname:\\\"grep-example\\\" \\u003e transform:\\u003cimage:\\\"pachyderm/grep-example\\\" cmd:\\\"/bin/grep-example.sh\\\" \\u003e shards:1 input_repo:\\u003cname:\\\"\\\\n\\\\010testrepo\\\" \\u003e \",\"response\":\"\\u003cnil\\u003e\",\"error\":\"Duplicate primary key `PipelineName`:\\n{\\n\\t\\\"CreatedAt\\\":\\t{\\n\\t\\t\\\"Nanos\\\":\\t179209495,\\n\\t\\t\\\"Seconds\\\":\\t1452666254\\n\\t},\\n\\t\\\"InputRepo\\\":\\t[\\n\\t\\t{\\n\\t\\t\\t\\\"Name\\\":\\t\\\"\\\\n\\\\u0004data\\\"\\n\\t\\t}\\n\\t],\\n\\t\\\"PipelineName\\\":\\t\\\"grep-example\\\",\\n\\t\\\"Shards\\\":\\t1,\\n\\t\\\"Transform\\\":\\t{\\n\\t\\t\\\"Cmd\\\":\\t[\\n\\t\\t\\t\\\"/bin/grep-example.sh\\\"\\n\\t\\t],\\n\\t\\t\\\"Image\\\":\\t\\\"pachyderm/grep-example\\\",\\n\\t\\t\\\"Stdin\\\":\\t\\\"\\\"\\n\\t}\\n}\\n{\\n\\t\\\"CreatedAt\\\":\\t{\\n\\t\\t\\\"Nanos\\\":\\t279957078,\\n\\t\\t\\\"Seconds\\\":\\t1452668264\\n\\t},\\n\\t\\\"InputRepo\\\":\\t[\\n\\t\\t{\\n\\t\\t\\t\\\"Name\\\":\\t\\\"\\\\n\\\\btestrepo\\\"\\n\\t\\t}\\n\\t],\\n\\t\\\"PipelineName\\\":\\t\\\"grep-example\\\",\\n\\t\\\"Shards\\\":\\t1,\\n\\t\\\"Transform\\\":\\t{\\n\\t\\t\\\"Cmd\\\":\\t[\\n\\t\\t\\t\\\"/bin/grep-example.sh\\\"\\n\\t\\t],\\n\\t\\t\\\"Image\\\":\\t\\\"pachyderm/grep-example\\\",\\n\\t\\t\\\"Stdin\\\":\\t\\\"\\\"\\n\\t}\\n}\",\"duration\":{\"seconds\":\"0\",\"nanos\":8414681}}\n2016-01-13T06:57:58Z INFO  protorpclog.Call {\"service\":\"pachyderm.pps.persist.API\",\"method\":\"CreatePipelineInfo\",\"request\":\"pipeline_name:\\\"grep-example2\\\" transform:\\u003cimage:\\\"pachyderm/grep-example\\\" cmd:\\\"/bin/grep-example.sh\\\" \\u003e shards:1 input_repo:\\u003cname:\\\"\\\\n\\\\010testrepo\\\" \\u003e created_at:\\u003cseconds:1452668278 nanos:719612052 \\u003e \",\"response\":\"pipeline_name:\\\"grep-example2\\\" transform:\\u003cimage:\\\"pachyderm/grep-example\\\" cmd:\\\"/bin/grep-example.sh\\\" \\u003e shards:1 input_repo:\\u003cname:\\\"\\\\n\\\\010testrepo\\\" \\u003e created_at:\\u003cseconds:1452668278 nanos:719612052 \\u003e \",\"error\":\"\",\"duration\":{\"seconds\":\"0\",\"nanos\":3852347}}\n2016-01-13T06:57:58Z INFO  protorpclog.Call {\"service\":\"pachyderm.pps.PipelineAPI\",\"method\":\"CreatePipeline\",\"request\":\"pipeline:\\u003cname:\\\"grep-example2\\\" \\u003e transform:\\u003cimage:\\\"pachyderm/grep-example\\\" cmd:\\\"/bin/grep-example.sh\\\" \\u003e shards:1 input_repo:\\u003cname:\\\"\\\\n\\\\010testrepo\\\" \\u003e \",\"response\":\"\",\"error\":\"\",\"duration\":{\"seconds\":\"0\",\"nanos\":11903499}}\n2016-01-13T06:57:58Z INFO  pipeline errored: rpc error: code = 2 desc = \"repo \\n\\btestrepo not found\"\npfsd log\n2016-01-13T06:56:59Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfs.InternalAPI\",\"method\":\"FinishCommit\",\"request\":\"commit:\\u003crepo:\\u003cname:\\\"testrepo\\\" \\u003e id:\\\"b123fa7293f84173adc92b9ca4fd7177\\\" \\u003e finished:\\u003cseconds:1452668219 nanos:596205987 \\u003e \",\"response\":\"\",\"error\":\"\",\"duration\":{\"seconds\":\"0\",\"nanos\":32633490}}\n2016-01-13T06:56:59Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfs.API\",\"method\":\"FinishCommit\",\"request\":\"commit:\\u003crepo:\\u003cname:\\\"testrepo\\\" \\u003e id:\\\"b123fa7293f84173adc92b9ca4fd7177\\\" \\u003e finished:\\u003cseconds:1452668219 nanos:596205987 \\u003e \",\"response\":\"\",\"error\":\"\",\"duration\":{\"seconds\":\"0\",\"nanos\":34046873}}\n2016/01/13 06:57:14 Appends: map[.:children:<key:\"file\" value:true >  file:block_refs:<block:<hash:\"DPkYCnZKuoY6Z7bXLwkYvBMcZ3JkLLLc5aNPCnAvlHDdwr8SXBIZixmVwjPDS0r9NGxUojNMNQqUilG26LTmtg==\" > range:<upper:4 > > block_refs:<block:<hash:\"zAaAjLvuBRAzGql5dBMujcKWrreVviKdBkuueEsKh6XPQoHYLoyZJxt12yFI8IoCbBpg7Zyr24ysbSQkLaxAYw==\" > range:<upper:4 > > block_refs:<block:<hash:\"yoCXzzisVHEYV9lrMk2vhW5Y240E9X1X_rppOvdIIn-_-u2rWqzp96YQ2XOJS9C_hLtaljbbDNhrSFzsfoE7Kw==\" > range:<upper:8 > > ]\n2016/01/13 06:57:14 using: block_refs:<block:<hash:\"DPkYCnZKuoY6Z7bXLwkYvBMcZ3JkLLLc5aNPCnAvlHDdwr8SXBIZixmVwjPDS0r9NGxUojNMNQqUilG26LTmtg==\" > range:<upper:4 > > block_refs:<block:<hash:\"zAaAjLvuBRAzGql5dBMujcKWrreVviKdBkuueEsKh6XPQoHYLoyZJxt12yFI8IoCbBpg7Zyr24ysbSQkLaxAYw==\" > range:<upper:4 > > block_refs:<block:<hash:\"yoCXzzisVHEYV9lrMk2vhW5Y240E9X1X_rppOvdIIn-_-u2rWqzp96YQ2XOJS9C_hLtaljbbDNhrSFzsfoE7Kw==\" > range:<upper:8 > >\n2016/01/13 06:57:14 returning: file:<commit:<repo:<name:\"testrepo\" > id:\"b123fa7293f84173adc92b9ca4fd7177\" > path:\"file\" > file_type:FILE_TYPE_REGULAR size_bytes:16 modified:<seconds:1452668219 nanos:596205987 > commit_modified:<repo:<name:\"testrepo\" > id:\"b123fa7293f84173adc92b9ca4fd7177\" >\n2016-01-13T06:57:14Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfs.InternalAPI\",\"method\":\"GetFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"testrepo\\\" \\u003e id:\\\"b123fa7293f84173adc92b9ca4fd7177\\\" \\u003e path:\\\"file\\\" \\u003e size_bytes:16 \",\"response\":\"\",\"error\":\"\",\"duration\":{\"seconds\":\"0\",\"nanos\":7933837}}\n2016-01-13T06:57:14Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfs.API\",\"method\":\"GetFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"testrepo\\\" \\u003e id:\\\"b123fa7293f84173adc92b9ca4fd7177\\\" \\u003e path:\\\"file\\\" \\u003e size_bytes:16 \",\"response\":\"\",\"error\":\"\",\"duration\":{\"seconds\":\"0\",\"nanos\":8808770}}\n2016-01-13T06:57:58Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfs.InternalAPI\",\"method\":\"CreateRepo\",\"request\":\"repo:\\u003cname:\\\"pipeline-grep-example2\\\" \\u003e \",\"response\":\"\",\"error\":\"\",\"duration\":{\"seconds\":\"0\",\"nanos\":5120}}\n2016-01-13T06:57:58Z INFO  protorpclog.Call {\"service\":\"pachyderm.pfs.API\",\"method\":\"CreateRepo\",\"request\":\"repo:\\u003cname:\\\"pipeline-grep-example2\\\" \\u003e \",\"response\":\"\",\"error\":\"\",\"duration\":{\"seconds\":\"0\",\"nanos\":1989055}}\n2016-01-13T06:57:58Z ERROR protorpclog.Call {\"service\":\"pachyderm.pfs.InternalAPI\",\"method\":\"ListCommit\",\"request\":\"repo:\\u003cname:\\\"\\\\n\\\\010testrepo\\\" \\u003e commit_type:COMMIT_TYPE_READ block:true \",\"response\":\"\\u003cnil\\u003e\",\"error\":\"repo \\n\\u0008testrepo not found\",\"duration\":{\"seconds\":\"0\",\"nanos\":59402}}\n2016-01-13T06:57:58Z ERROR protorpclog.Call {\"service\":\"pachyderm.pfs.API\",\"method\":\"ListCommit\",\"request\":\"repo:\\u003cname:\\\"\\\\n\\\\010testrepo\\\" \\u003e commit_type:COMMIT_TYPE_READ block:true \",\"response\":\"\\u003cnil\\u003e\",\"error\":\"rpc error: code = 2 desc = \\\"repo \\\\n\\\\btestrepo not found\\\"\",\"duration\":{\"seconds\":\"0\",\"nanos\":2517764}}\nRegarding tcp over tcp, I tried again now and it looks to work for pachctl mount and maybe I was missing something last night, I can't reproduce anymore. Nevermind about tcp over tcp, I think this is happening only if I tunnel the traffic over a ssh socks socket.\n. pachctl list-pipeline is returning a panic, see #180\nEdit: Removed stacktrace\n. ",
    "JonathanFraser": "I can run the ubuntu container and the etcd container comes up fine, its the created container that is having difficulty.\n. realized I didn't escape the < braces in the original ticket, there is a subvolumeID in that path string.\n. It's the kubelet, the script doesn't reach the proxy. \n. I've tried manually starting the created container outside the scrip and I receive the same error. \n. swapping the storage manager seems to have fixed the problem. It might have something to do with my root file system is also running btrfs. \n. For this yeah, I'll upstream the issue. \n. Version of kubernetes is the one that comes up when you run launch-kube so 1.1.2\ndocker version: 1.9.1\nas per the previous ticket the storage backend was switched to device mapper (although the underlying file system is still btrfs)\nall three of those containers seem to just want an empty directory mount but I can't seem to find any errors that are more useful.\n. Seems like the mount errors are a issue in kubernetes, they are resolved in version 1.2. Request a version bump of the start script to kubernetes v1.2.\n. Seems easy enough, although I don't see any concept of locality in that interface. I presume its not  exposed currently?\n. Doing some leg work for the Ceph backend in particular.\nThere are two ways to access ceph as a object store, via the radosgw which provides a s3 like interface or via the librados library. \nThe radosgw has a bug which was just recently patched preventing the go-aws-sdk from communicating with it properly. Specifically radosgw did not support absolute links which the sdk was sending. This was just recently merged into ceph (12 days ago) so we are likely not to see it in a stable version for a while. \nThe second option is via librados, which is a C library and is required to be installed to build. There already exist go bindings for it, and integration would be relatively painless, however, it would need to be integrated with some build tag foo to prevent all users from requiring ceph to be installed. \n. @jdoliner there would still need to be some credential monkeying to get ceph to work via radosgw, as it appears to have no concept of tokens. \nAs for the ceph cycle, they do a development release once a month or so, this then gets stabilized into a production release some 3 to 4 months later. If you want an LTS release, that might be 8 months to a year out. \nThe go wrapper on librados, seemed to work out of the box. While I have yet to integrate it as a pachyderm backend it, I was able to get it up and listing objects relatively quickly. Moreover, the functions more or less line up with the interface. \nIf we need to use build tags then I would suggest going  with a method similar to how the stdlib handles sql backends. A scheme where an init function registers it with a frame work which then gets looked up at runtime. This would mean either using a connection string (not ideal), or putting the credential lookup in the same place as the object store code (probably reasonable). \n. I could be useful if the obj.Client interface had a few comments on it, or if there are some docs, where can I find them. For example, when we grab a writer, what happens if there is a pre-existing object (error, open for truncate, other). Likewise, what is the name parameter in Walk. I assume it's a prefix but what if the backend does not have explicit support for prefixes. \n. Its declared as a global in the kubernetes code. This means that if we import that package or import a package which imports it that flag will show up. \n. As pachyderm is a versioned system, it is write once read many. A simpler change would be to provide the functionality to retrieve the storage backend key from pachyderm to allow for direct download from the backend. \n. would it make more sense to split the block storage into a separate gateway process, perhaps running as a daemonset.  . I suspect the apiserver is not coming up correctly, seems like they dropped the cluster-name parameter, or at least its not available. \nThere is also a deprecation warning on portal-net and address flags, they seem to have be superceeded by service-cluster-ip-range and insecure-bind-address instead\n. I was having some issues with this locally myself, it seemed like pachd could not contact the rethinkdb database. I kept trying to do that on a subnet that didn't seem to exist anywhere in the container. \n. This is actually starting to bump into our stuff. JSON is a natural serialization format for a lot of our data, however, some of our files can contain NaNs. JSON does not support NaNs and so we are left with looking for a different format. most of the other options are binary formats but of course new lines don't make sense for binary files.  This leaves us with the wonderful option of base64 encoding the binary files to make them work with new lines. This would work, but it's a little clunky and it would be nice if there was a way to split things up a little more nicely. \n. I presume with CHUNKING_NONE then the shard could grow up to the full object size limit of the backing store? For performance reasons you probably don't want to but it would be nice if that was an option, also how would one decide the chunking behaviour in the fuse layer. Maybe its a property of the entire mount?\n. It's funny, I was thinking about this on the way to work this morning. The solution, I think, is that code is data and data is code. I see little difference between merging data from two seperate repos and merging code with data. If you were to version your code in pachyderm, then you would get the behaviour you want implicitly I think.  I mean really, what you guys have built here is a distributed build system for data. \n. clap clap clap now go get drunk\n. @derekchiang while this is not the current behaviour, one use for an empty commit is overall providence tracking.\nLets take a system that has some number of source repos and then it fans out through the processing DAG. No say I want to gather together all the results of a given commit to the source repos. How would that be done if the result of that commit only made it partially down the DAG. I would either have to:\na) wait until a commit comes along which propagates to the end of the DAG\nb) poll pachyderm periodically for the state and extract the data\nc) inject code into each pipeline step to report results\nIf however all pipeline steps executed, but with some containing no new data (or zero data). Then it is easy. I simply set up a stage at the end of the pipeline which then scans over it. In this case it will trigger every time there is a new input commit.\nThis also starts to collapse things and make the commits more one to one. \n. If the testing framework can't be bumped to 1.10, then maybe the requirements should just be listed as 1.9? I was hoping for 1.10 because of the annoying /dev/pts bug but it's not a bit deal. \n. The container was job-shim (f7b5c3e1894f) with the custom executable added in. \nhere is the pipeline: \n{\n  \"pipeline\": {\n    \"name\": \"tokamak_fmt\"\n  },\n  \"transform\": {\n    \"image\":\"refmt\",\n    \"cmd\": [ \"sh\" ],\n    \"stdin\": [\n        \"./refmt -S=/pfs/tokamak -D=/pfs/out > /pfs/out/log_file 2> /pfs/out/err_file\",\n        \"cp /pfs/tokamak/shotnumber /pfs/out/shotnumber\"\n    ]\n  },\n  \"shards\": \"1\",\n  \"inputs\": [\n    {\n      \"repo\": {\n        \"name\": \"tokamak\"\n      }\n    }\n  ]\n}\n. do you one better, I have the entire container on dockerhub jonathanfraser/refmt\n. we've sent a message out of band about access to our data. For the time being I was able to mock it up with some quick scripts and such. I've put everything up here.\nYou should be able to go get it and go install the gencsv command.\nmk-source-repo creates a repo and fills it with dummy data after which you can attach the pipeline. You can either create the container your self or pull the one on dockerhub.\n. On another note I'm watching my free memory slowly climb down so I suspect there is a memory leak in there somewhere.\n. Ok it looks like there are a few issues here that are falling over each other. looking at the pachd logs I'm geting alot of bufio.Scanner: token too long when trying to putFile into the output pipeline. This makes sense as it looks like you guys are using a scanner to read the bytes line by line from from the protostream. I presume this is so blocks contain only full lines of text. This of course fails if a single line exceeds 64kB. The second issue, which I have yet to track down is why this isn't triggering an error in the job. That error is getting swallowed somewhere and being exposed as just pipeline that stalls out. \n. Is there any reason that scanner can't be changed to a bufio.Reader the readline function would probably do what you want. just keep reading until you've reached your block_size and is_prefix is false. It would also save the step of having to append a newline explicitly\n. Yeah it sits there in a running state without erroring out in anyway. \nA few jobs get some of the output being produced, but I presume that is because it is succeeding a few times before it hits a line that is too long.\n. Setting shards to something other than 1 makes it go nuts.\n. and it just go weirder, I did a cat again on the file, without any additional commits and got a larger output\njfraser@ubunutVirt:/pfs/test$ cat a01f9e808b41461c9516fb29637a3f49/a\n1\n2\n3\n2\n4\n2\n3\n2\n5\n2\n3\n2\n4\n2\n3\n2\nadditional dumps looks like they have stabilized onto this value.\n. @jdoliner same output\n. @jdoliner sure, but for the containers running partial jobs how does that work? \n. I'm also getting this error is a decrease the pachd replicas down to 1.\n. @tv42 I'm pretty sure this is in fuse and has to do with the reusing of the message buffers, these strange values are being handed directly to the files Write method.\n. This could be a misinterpretation of what I understand should happen but I put a log entry followed by a panic into the Write which is called directly by the fuse bindings\ndata\nThis report is in response to echo 5 > file\nAlthough its entirely possible I might be missing something. \n. Looking into fuse.go when generating the Write request there is an explicit check that the buffer is not less than in.Size but it doesn't seem to care that it might be larger.\n. The kernel holding on to it also seems weird because from the perspective of the file-system I'm writing to a new distinct file each time (as seen here).\n. @jdoliner does it also fix it for test script.\n. @jdoliner huh, ok because at no point was I writing to same file twice and yet DirectIO fixed it. Glad it's fixed but damned if I understand why.\n. I don't know, I'm think if a case where a pipe line step is monitoring one or more repos and producing a summary. The summary is nearly the same all the time except a few lines may be different and when those lines are different kick off the next stage of the pipeline dealing with those lines.\n@jdoliner that's basically what I was thinking as well. Also make sure the latter pipelines are smart enough to detect that the commit has the same data and either not run or run as a NO-OP. \n. Correct me if I'm wrong but it appears the reductivness affects the view of the input repos and does not apply to the pipeline itself.\n. I guess what I'm saying is that the reduce flags should affect only the input repos and the option whether or not to reduce in the pipeline itself is decided if the code overwrites (>) or appends(>>). In this way a reducing pipeline has it's full commit history for free and indeed you could choose to reduce on a per file basis.\n. @jdoliner by this bug I presume you mean #296 and #270. I can appreciate lumping similar bugs together but additional feature maybe we keep separate.\n. It is unusual for a mount command to create a directory. Usually it should mount to a prexisting directory and the first parameter of the command should be the directory to mount to. This is the typical mount procedure in a linux system. \n. That's now more or less in lines with what a regular command does.\n. @erikreppel and I were discussing this offline a bit. The need for this may be related to the unavailability of more typical map/reduce semantics. In a conventional map-reduce you assign your keys in the map phase and then the reduce operation can be shared per key. In the case of pachyderm, there is only one shard level in reduce (all).  One way to get more typical map-reduce sematics would be to allow for finer sharding in the reduce stage, say per file. That way one can shard their reduce while still having an atomicity guarantee. \n. @jdoliner ahh, this could just be misunderstanding of what pachyderm is doing under the hood.  If you are saying reduces are parallelized on a per file basis already then ignore my tom foolery. But yes having a flag which allows for a complete view of a repo is useful.\n. funny if I copy paste that into my browser it works fine?\n. yes, that returns just and xml page for me.\n. The issue I see with the second one is it doesn't allow for a differential behaviour between when there is new data and when there is not. This case matters when you have more than one input repo.\n. ahh, i was under the impression that it would display it's values as dictated by the partition flag\n. actually, then what does partition do? if incremental means you only see the increment on new data and you see the entire repo when there isn't what does partition actually specify?\n. So in this instance I was changing the container image each run, I suspect it would have been find if that was also included in the job id.\n. @jdoliner sure here is the manifest file\nas you can see it's executing cp as the very first command\n{\n  \"pipeline\": {\n    \"name\": \"prospector_preprocessing7\"\n  },\n  \"transform\": {\n    \"image\": \"generalfusion/fushim:new_base\",\n    \"cmd\": [\n      \"sh\"\n    ],\n    \"stdin\": [\n      \"export PACH_REPO=prospector_preprocessing\",\n      \"mkdir /inputs\",\n      \"cp -r /pfs/prospector_input /inputs\",\n      \"repo/gfpy/digitizer/preprocess.py /temp_output /inputs || exit 1\",\n      \"data-replicator -bucket-name=generalfusion-temp /temp_output/ /pfs/out/ || exit 1\"\n    ]\n  },\n  \"parallelism\": 0,\n  \"inputs\": [\n    {\n      \"repo\": {\n        \"name\": \"prospector_input\"\n      },\n      \"method\": {\n        \"partition\": \"file\",\n        \"incremental\": true\n      }\n    }\n  ]\n}\n. @derekchiang \nnot completely, 1d5e2da13cf8164094eefec8c7df1caae361d940 \"fixed\" this by producing empty commits, however, the empty commits are un-parented even if they should be.\n. At some point going down this road you are simply trading off a filesystem tree for a datum tree.\n. @jdoliner Right, so what I'm about to suggest might well be heresy but please bear with me. This pattern of you dumping a database to a directory where each file refers to a table and each line a row exactly maps to the case where each table is a directory and each file corresponds to a row. This would be a perfectly valid way to dump a database and has the convenient property that the user gets to set the granularity with external tools. So rather than bake all of this into pachyderm, build it on top with additional tools. The only thing to make this work is for reduces directories need to be able to be presented atomically.\n. @jdoliner  so I think there is a very fundamental difference between Hadoop and pachyderm and this hits it on the head. Hadoop is based on injecting user code as plugins into every aspect of it, including how it parses and breaks up files. Unless pachyderm starts to include a similar plugin based API there is no way the core can hope to cover all possible ways of breaking up files. After all, there are as many ways to breakup files as there are users. You can specify delimiters into the blocks but then how do users influence that. do we get to choose a set stock delimiters or do I inject parsing code somehow. If it's a set of stock delimiters? Is it only one type per repo/pipeline or can I combine them? How can I inject my own delimiters, by modifying pachyderm?\nIn the current system there is two ways to inject behaviour, via various config options (so long as pachyderm supports it) and via a user supplied docker container. If we want to support the ability for end users to inject delimiters without modifying pachyderm itself then there needs to be a way to do that interacting only with the files and a filesystem. There is a solution to this and that's writing out to individual files. The only difference between this and a single large file is coming up with a name.\nI guess I kind of feel like, because of the way pachyderm works with containers and exposing things via a filesystem,  that large sharded files are almost an anti-pattern. So instead of asking the question \"Given that I have a large quantity of coarsely broken data, how do I finely content address it?\" it changes into \"Given that I have a large quantity of finely addressed data, how do I store it coarsely?\". I Suppose this is the same problem but it is perhaps slightly easier to think about. \nIf was to solve this problem, I would batch up writes to files in a given commit into one blob. The meta data for a file diff then contains the blob where the file is stored, the offset into the blob and the length of the file. S3 supports partial reads so this is ok, I don't know about other stores. This also means most of a diff is stored together and so that when a pipeline needs to operate on it then it will likely use most of the diff and we can read the blob in its entirety. Ultimately I just think batching is an easier problem to solve than slicing. \n. I'm a little confused why this is a thing, kubernetes has the concept of a load-balancer for this reason. Why are we re-inventing the wheel here?. I agree with removing the port-forward from pachctl, seemed a bit redundant since kubectl offered that anyway. So this patch just addresses automagically locating kubernetes provided that it is reachable from your host and only if ADDRESS is not set. In which case, that makes sense. \nRe the TLS comment, the only way to connect to an unsecure docker repo is to set a flag on the client side daemon and restart the service. This is not really ideal. You can create self signed certificates but that gets messy. Whats the reason for the registry deployment as opposed to just using offering like those from the cloud providers or docker hub. . I'm going to jump in here and say that it might make sense to separate errors which occur in the pipeline from errors that occur during the flush. Leave the error value for GRPC for cases when the flush call itself fails due to network issues or the commit not existing....etc. Have the actual response contain two sets of data one of successful commits and another of failed commits and their associated errors.  This allows for differentiated errors with pipeline and other errors as well as provides a mechanism for displaying more that one error in the pipelines. \n. I think it makes more sense (and be more inline with existing tooling) if the image is passed as an opaque string. \n. you can simply create an aws account with an access key/ secret pair and leave the token string blank. . I don't think the additional overhead of a seperate view pipeline was that big a cross to bear.  Moreover, with state you start to introduce some ambiguity because, PREV was read only but now STATE needs to be Read-Write. It begs the question if I write to state and then read from it what do I see. Additionally, do I read the state written in a separate job and if I do there is a race condition. Where we previously had a fold operation we now have this strange fold_and_map operation.\nRe the optimization: you can do that with prev just as much as you can do it with state, nothing changes.  . There is another improvement that can be done if there is no state. The commits following a failed commits do not need to be cancelled. This allows the opportunity to fix data that was previously incorrect.. This especially important because it creates deployment issues with upgrades. Not only do you need to update the pachd node but all of your deployed pipeline images as well. If your deployment code tracks changes to images naively, this mean re-running all the pipelines, not ideal.. We've decided to manage this simply by specify tags on the images, the tag on the image name both forces the hash to change, as well as ensures that you don't have any ambiguity on the what an image name is referencing. . my argument is whether or not it's a bug. It comes down to whether update pipeline should be an idempotent operation. If it is idempotent then every thing must be specified in the manifest, including code versions vis a vis docker tags. If you don't care about idempotence and want the pipeline to be redeployed on every single operation, then ok this need to be changed. I'm more in the idempotence camp. However, if you would like to have a value which forces redeploy but doesn't fiddle with docker tags then perhaps add a  'version' field to the manifest file and include it in the hash. . @dwhitena versioning your data is pointless if you aren't versioning your code. I agree leaving humans to name things generally works out poorly, just look at any typical code base :confounded:  .\nOur build system automatically tags images with the git commit so in this case tags do uniquely identify a docker image and you don't have to worry about those pesky humans.\nHowever, if you have a tight iterate,build,test loop that does not involve a git commit then I can see the issues at play.  A specific version field in the pipeline manifest, with a conveinence flag on pachctl  to auto increment version would probably allow for the desired cli semantics without removing the idempotent nature of the underlying api request. \nAs an aside, I think it would be neat to try and put a build system into a pachyderm pipeline with your code being revisioned controlled in a pachyderm repo. That way committing too the repo re-runs your build and automatically re-runs affected pipelines.. In order match up any two things (such as a pachyderm job and an docker image)  together you need a unique identifier. AFAICT, there are two identifiers for a docker image, the repo:tag and the image ID. Image ID provided the fundamental ground truth because as of, I believe, 1.10 the image ID is a hash of the contents which provides a unique signature of the files. The way I see it the image ID is analogous to a git commit id, while the docker tag is analogous to a git tag, with latest being a rough approximation of master. \nThere's a problem here though, in git you can checkout a version based on its commit id so that's sufficient to track but that's not true for docker. There is no way retrieve a docker image by its ID AFAIK. In docker you must pull based on docker name:tag. So if you don't treat that pair as unique you can wind up in some sticky race conditions (which is true not just for pachyderm).\nLets say you have a set of pipelines deployed and they have run some commits, then you push a new image do your docker repo with a tag you used previously. You then push new data and for some reason your node(s) need to re-pull the image (you added a new node or the job didn't run on that node previously, etc). Then you have this new data running with new code and you didn't perform an update-pipeline. Obviously this is not what we want.\nSo scratch what I previously said about adding a version field, it won't fix the scenario described above.  The way I see it, there are three possible options to avoid this.\n1) Pipelines are deployed following best practices using tags which do uniquely identify the image either via commit tags or version numbers (v1.2.4). \n2) your code exists as a pachyderm repo and you never really bump the container for the pipeline. You initiate a code redeploy by making a commit to a pachyderm repo. \n3) Pachyderm manages its own docker repository (which it kind of is, but now it has to be backed up for recovery purposes) and when a user updates a pipeline(s) pachyderm pulls the image(s) down from the remote repo and into its repository. When it does so it name mangles the image to give it a unique name:tag. Then any jobs issues by pachyderm go back to this internal repository using the mangled/unique name. \nThree seems like alot of fuddling and the increased complexity would probably introduce more bugs than it solves. Moreover the added repository starts to create additional maintenance concerns.  Two probably sounds better than it would work in practice given how pachyderm shards and manages files. One, is a known good solution which is already the convention for using docker. \nEdit:\nDocker does allow pulling via digest  which provides a non-human fungible way of referencing docker images in repos which means we can pull by a unique ID instead of relying on tags. Pachyderm just needs to cache the digest and submit the jobs with the digest instead of the user provided container tag. . > It gets a bit hairy though because you might have an image that can only pulled using some pull secrets that pachd doesn't have access to. \nThis lack of pulling can happen now with new pipelines. I don't see how this is unique to ensuring pipeline have a dedicated provenance. \n. 100% this, git has 3  objects (Commits, Trees, and Files) which work together in a massive distributed hash table. The hash table ensures that you have de-duplication and only extra storage you need is the relavant changes. It will mean that you need to perform multiple queries to traverse the file tree but not to traverse a single file.  . Lets say your data repo is 100TB and your models repo is small like 1GB. Each model maps to quite a bit of data. Now lets say you have a pipeline which is applying your models to the data (2 input pipeline) and you make a change to your model. Via the cross product rules, you have the change appearing in the model repo, but you have every single shard of data appearing in the data repo. If you cluster is medium sized, say 10 nodes. Hell lets be bold and say 100 nodes you will need to pull down 1TB per node which is not great. Even more so, lets say that a model doesn't apply to all the data and  you can tell which data to apply the model too by its path/filename. In which case you don't need to download all of the data which matches, but you won't be able to tell that until the job is running. . @derekchiang @jdoliner This is probably been summarized in a different issue, but what about the Fuse layer makes it so slow. Is it the trips into userspace or something else?. All of our code takes the folders to process as inputs and we expand them out in the pipeline manifest. This makes it easy to quickly test code without mounkying with absolute paths.\nAlso please don't enforce how a users program gets called, leave that up to the manifest fixing a positional pattern can seriously hamper how a user passes flags and other information into their processing. . For us, because we put our job output into a temporary folder in the container we managed to fill up the drives in under a day. . pachctl port-forward is actually only a thin wrapper around kubectl port-forward and it can be used directly for use cases such as this. . so the service IP is stable in kubernetes. If you perform a dns lookup in the kube for anything thats not a headless service you get the service IP and the kubernetes fabric load balances automatically in the backend. The IP address of pachd should not change in the kubernetes cluster so long as everything is set up correctly. . It would be good to pre-compute the datum diff so that the second job is only processing that which could potentially generate new data. Then processing in parallel would not be a problem.. @derekchiang There are two separate issues here. a race condition resulting in double datum processing and job scheduling priority. pre-computing the diffs removes the race condition. On the scheduling side preferring earlier commits is probably a good heuristic. . This is probably related to #223. A secondary optimization can be performed here in the case of lazy pipelines. In that case, if one of the input pipes is copied into /pfs/out the worker can elide all of the data transfer.. @emk  You can glob on a prefix. This lets you use directories to specify how many files you want available as an atomic unit. . @jdoliner True....So when you have relatively small incremental commits but your initial commit is large then this will be a problem. . ",
    "HackToday": "@jdoliner  if I understand correct, that means pachyderm not support docker volume related function, is it correct ?\n. ",
    "yellow1912": "I'm also interested in knowing why this is better than Spark or Storm? \n. ",
    "tadhunt": "This still wasn't sufficient for 'make test' to successfully pass, so I'll try again.\n. Looks like there's been tons of refactoring lately so it makes sense that the tests are unstable.  Even with the vendor extension and the govendor tool, it's disappointing that managing dependencies on a large project like this is so fraught with peril.\n. ",
    "tv42": "There's a little bit of undesired repeating of things on the mount code path, e.g. the tests are not going through the Mounter abstraction and aren't necessarily using the same mount options. On the plus side, using the fstestutil library makes tests easier to write, and we can work on unifying the code paths later.\n. Redid the tests to avoid the overly pedantic \"captured protocol conversation\" style, now all tests run against localhost API instances (managed as part of the test). Please take a look, should be good to merge.\n. foo missing from ls /pfs there is not a FUSE thing I'd be aware of. I bet your ReadDirAll method for the /pfs dir fails to include foo in the results the second time its called, or the Lookup for foo fails.\n. Shell echo a >f is just a Setattr for truncating and then Write at offfset 0. Seekability doesn't really enter that picture.. perhaps you're not handling the truncate right?\nThat you're getting a write error there means your fuse handlers are returning errors. What, why? Is this because of the write-only nature of your open commits? Is it actually write-once?\nThe data being corrupted, especially the repeating 2 in 1232, almost must be something in the storage backend (the handling of the sharded blocks?).\nYou can see tons more debug by enabling debug in https://godoc.org/bazil.org/fuse/fs#Config or https://godoc.org/bazil.org/fuse#Debug\n. @JonathanFraser Show me.\n. @jdoliner Looks like the file wasn't truncated, only overwritten. The 3\\n goes on top of the 1\\n.\n. And the 2\\n goes after the 1\\n, not overwriting, because at that point the kernel still knows the file contents; the FinishCommit thing probably flushes that out.\n. Okay now there's multiple scenarios in flight, this gets confusing.\n. Here are the facts:\n- you are using page cache backed files (no fuse.OpenDirectIO visible); that means kernel writes a page at a time\n- file.Setattr is not implemented, hence truncation does not do anything\n- your file.Write does not pass request.Offset to the pfsclient.Putfile, hence the storage backend thinks all writes are appends\nThis combination results in the kernel page cache contents differing from what your storage backend thinks they are.\n. resp is nil in your Open handler? I find that very hard to believe. Show me.\nHere's an example using fuse.OpenDirectIO:\ngo\nfunc (f *myFile) Open(ctx context.Context, req *fuse.OpenRequest, resp *fuse.OpenResponse) (fs.Handle, error) {\n        resp.Flags |= fuse.OpenDirectIO\n        ...\n}\n. I feel like there's still confusion in the air. For example, my definition of Right Time is when a commit is finalized, not when a (now-finalized) commit is looked at via the gRPC API. That is, if an open commit is being mutated on host A, and someone else calls FinishCommit on host B, either one of two things should happen:\n1. all changes to files in that commit attempted on host A start returning errors (e.g. EROFS)\n2. FinishCommit refuses to proceed and returns an error\nAnything else will likely just hide bugs, and sounds like a data loss scenario. Disabling caches decreases the sizes of race windows, does not remove them.\nSo, I would recommend, in that spec you're proposing, to think of not just the local mounted FUSE filesystem and a pretend-worldview where there is instantaneous global state, but how the distributed system as a whole will operate.\n. The write-up above seems aligned with my current understanding.\nLatest bazil.org/fuse should fix this (by decreasing the buffer size on Linux): https://github.com/bazil/fuse/commit/9e8e65293a037ce998ba0df102710c49313aa513\nYou still need to vendor in the new version. I gave @derekchiang https://github.com/tv42/pachyderm/tree/fuse-memory-leak-revendored & https://github.com/tv42/pachyderm/commit/a2c5eb509f75e87b038bf9248d504ba24b823005 that's on top of his then-current branch.\nLet me know if you need something more.\n. Not sure why you closed this. gRPC is supposed to send multiple RPCs over one connection, even concurrently.\nA quick tcpdump says that a TestWriteManyFiles run causes 40040 connections to be opened. That's clearly way, way, too many.\n. A lot of code seems to look like\ngo\n        clientConn, err := a.getClientConnForFile(request.File, a.version)\n        if err != nil {\n            return err\n        }\n        defer clientConn.Close()\nas clarified on slack, apparently grpc connection reuse was explicitly taken out (for now) because grpc had trouble recovering from a hung TCP connection. Well, too many connections is the other end of the spectrum :(\nExpect random test & benchmark failures, expect weird drops in sustained throughput.\n. It seems https://godoc.org/google.golang.org/grpc#WithDialer with a dialer that sets TCP keepalive with https://golang.org/pkg/net/#TCPConn.SetKeepAlive would help the problem of hung gRPC connections.\n. It's triggered by go generate, and that shell script can even go away once the bug referenced below is fixed.\n. ",
    "joeblew99": "Cool. Check out minio then. Might be useful to you\n. It's golang based s3 thingy. \n. ",
    "rusenask": "pulling current master and updating kubectl solved this\n. ",
    "jnevin": "Thanks jd\nProblem: on Mac, a simple make install does not generate the pachctl CLI. I could only get make install to work from within a Docker.\n. For instance:\n$ make install\n$ whereis pachctl\n$\n. hmmm... go/bin is already on my path\nso, for example:\n$ go version outputs:\ngo version go1.5.3 darwin/amd64\nbut a direct invocation of the make command fails to create pachctl locally:\nJamess-MBP:pachyderm jnevin$ go install ./src/cmd/pachctl ./src/cmd/pachctl-doc\nJamess-MBP:pachyderm jnevin$ pachctl\nbash: pachctl: command not found\n. OK, addiing $GOPATH/bin to my PATH now exposes pachctl as executable -- now back to seeing if I can get pachctl to do mounts and other good stuff!\n. I'm seeing some advice here that might help:\nhttps://forums.docker.com/t/getting-mounts-denied-on-previously-working-container-after-upgrading-to-v1-12-0-beta18-3-gec40b14/17546/5\n. I was excited about the Pachyderm 1.3 release (still am). \nThis issue, however, breaks the current Beginner Tutorial documentation.\nIt took some (useful learning) hours to figure out a temporary workaround (Mac OS X 10). There's probably an easier way, but this worked for me:\nMake sure you can start minikube:\n$ minikube start\nStarting local Kubernetes cluster...\nKubectl is now configured to use the cluster.\nCreate a pachctl manifest you can edit and use with kubectl for a fixed deployment:\n$ pachctl deploy local --dry-run > pachctl-manifest-old.yaml\nThen edit the pachctl-manifest-old.yaml file you created above and replace the lines:\n{\n \"apiVersion\": \"apps/v1alpha1\",\n \"kind\": \"PetSet\",\nwith (per instructions here)\n{\n  \"apiVersion\": \"apps/v1beta1\",\n  \"kind\": \"StatefulSet\",\nand save the file as, say, pachctl-manifest-new.yaml\nThen, instead of: \npachctl deploy local\ndo:\n$ kubectl create -f pachctl-manifest.yaml`\nserviceaccount \"pachyderm\" created\npersistentvolume \"rethink-volume-0\" created\nreplicationcontroller \"etcd\" created\nservice \"etcd\" created\nservice \"rethink\" created\nstatefulset \"rethink\" created\nservice \"rethink-headless\" created\njob \"pachd-init\" created\nservice \"pachd\" created\nreplicationcontroller \"pachd\" created\nThen you can do your port forwarding:\n```\n$ pachctl port-forward &\n[1] 6291\nJamess-MBP-2:$ Port forwarded, CTRL-C to exit.\n```\nand see that things are in order:\n$ pachctl version\nCOMPONENT           VERSION\npachctl             1.3.1\npachd               1.3.1. Thanks @JoeyZwicker\nBut when I use the --deploy-rethink-as-rc option, I get these errors:\n```\n$ pachctl deploy local --deploy-rethink-as-rc\npanic: RethinkDB can only be managed by a ReplicationController as a single instance, but recieved 0 volumes\ngoroutine 1 [running]:\npanic(0x1135340, 0xc4207ae210)\n    /usr/local/go/src/runtime/panic.go:500 +0x1a1\ngithub.com/pachyderm/pachyderm/src/server/pkg/deploy/assets.WriteAssets(0x1d7ff80, 0xc420292150, 0xc4204534c0, 0x0, 0x0, 0x0, 0x0, 0x1, 0x13912c0, 0xe)\n    /home/sjezewski/go/src/github.com/pachyderm/pachyderm/src/server/pkg/deploy/assets/assets.go:856 +0xf2b\ngithub.com/pachyderm/pachyderm/src/server/pkg/deploy/assets.WriteLocalAssets(0x1d7ff80, 0xc420292150, 0xc4204534c0, 0x13912c0, 0xe)\n    /home/sjezewski/go/src/github.com/pachyderm/pachyderm/src/server/pkg/deploy/assets/assets.go:878 +0x83\ngithub.com/pachyderm/pachyderm/src/server/pkg/deploy/cmds.DeployCmd.func1(0xc4204e5100, 0x0, 0x1, 0x0, 0x0)\n    /home/sjezewski/go/src/github.com/pachyderm/pachyderm/src/server/pkg/deploy/cmds/cmds.go:60 +0x115\ngithub.com/pachyderm/pachyderm/src/server/vendor/go.pedge.io/pkg/cobra.RunBoundedArgs.func1(0xc4203e8d80, 0xc4204e5100, 0x0, 0x1)\n    /home/sjezewski/go/src/github.com/pachyderm/pachyderm/src/server/vendor/go.pedge.io/pkg/cobra/pkgcobra.go:26 +0x9d\ngithub.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra.(Command).execute(0xc4203e8d80, 0xc4204e50a0, 0x1, 0x1, 0xc4203e8d80, 0xc4204e50a0)\n    /home/sjezewski/go/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra/command.go:572 +0x439\ngithub.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra.(Command).ExecuteC(0xc42044eb40, 0xc42011d720, 0xc4204e4ff0, 0xc42011d721)\n    /home/sjezewski/go/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra/command.go:658 +0x367\ngithub.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra.(*Command).Execute(0xc42044eb40, 0xd, 0xc42044eb40)\n    /home/sjezewski/go/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra/command.go:617 +0x2b\nmain.do(0x10f8d40, 0xc42011d700, 0x0, 0x0)\n    /home/sjezewski/go/src/github.com/pachyderm/pachyderm/src/server/cmd/pachctl/main.go:26 +0x119\ngithub.com/pachyderm/pachyderm/src/server/vendor/go.pedge.io/env.Main(0x148a328, 0x10f8d40, 0xc42011d700, 0x0, 0x0, 0x0)\n    /home/sjezewski/go/src/github.com/pachyderm/pachyderm/src/server/vendor/go.pedge.io/env/env.go:46 +0x83\nmain.main()\n    /home/sjezewski/go/src/github.com/pachyderm/pachyderm/src/server/cmd/pachctl/main.go:16 +0x7d\n```\nNote that I'm seeing this with kBs 1.5.1\nAny clue how I'd get the --deploy-rethink-as-rc option to work now?. Hey @jdoliner \n1.3.2 release plays nicely now with kB8s 1.5.1 -- thanks! Have not tested the Tutorials yet to see if they work. \nHere's good news:\n$ pachctl deploy local\nserviceaccount \"pachyderm\" created\npersistentvolume \"rethink-volume-0\" created\nreplicationcontroller \"etcd\" created\nservice \"etcd\" created\nservice \"rethink\" created\npersistentvolumeclaim \"rethink-volume-claim\" created\nreplicationcontroller \"rethink\" created\njob \"pachd-init\" created\nservice \"pachd\" created\nreplicationcontroller \"pachd\" created\n```\n$ kubectl get all\nNAME               READY     STATUS    RESTARTS   AGE\npo/etcd-t8vjc      1/1       Running   0          5m\npo/pachd-kn2r6     1/1       Running   5          5m\npo/rethink-5x5nx   1/1       Running   0          5m\nNAME         DESIRED   CURRENT   READY     AGE\nrc/etcd      1         1         1         5m\nrc/pachd     1         1         1         5m\nrc/rethink   1         1         1         5m\nNAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)                                          AGE\nsvc/etcd         10.0.0.157           2379/TCP,2380/TCP                                5m\nsvc/kubernetes   10.0.0.1             443/TCP                                          5m\nsvc/pachd        10.0.0.148          650:30650/TCP,651:30651/TCP                      5m\nsvc/rethink      10.0.0.225          8080:32080/TCP,28015:32081/TCP,29015:32119/TCP\n```\n$ pachctl version\nCOMPONENT           VERSION\npachctl             1.3.2\npachd               1.3.2. Note: you do have to wait a couple of minutes before the pachd pod runs successfully.. ",
    "bderooms": "Is there a possibility to get some insights on the results of these benchmarks for users? Mainly interested in number crunching benchmarks. ",
    "jdelfino": "+1 for exposing prometheus metrics to monitor the state of pipelines. We're using https://github.com/button/pachyderm_exporter also, but the scraping is slow and takes minutes. Would be nice to have more performant support.. Hi, I've also run into this problem. I have a pipeline that processes about 1700 input items, and produces ~2k small (~150 bytes) output items for each input. Job inspection below.\nI understand we can work around this by grouping the items into fewer, smaller files, but this is not great for a few reasons:\n- It forces us to tune the aggregation ourselves, and the sweet spot will change every time we change our cluster size.\n- Aggregating will make downstream datums larger, and therefore more likely to fail while processing.\n- It makes this data less re-usable by other, future pipelines, which might not want the data aggregated in the same way.\nIs this on your roadmap? Just wondering what kind of timeline there might be for a fix. Thanks! \n```\n-- 22:17 $ pachctl inspect-job d5b02dc101144a7795c05081c9ec2811\nID: d5b02dc101144a7795c05081c9ec2811\nPipeline: fetch_tracks\nStarted: 3 hours ago\nDuration: 2 hours\nState: success\nReason:\nProcessed: 1681\nFailed: 0\nSkipped: 8\nTotal: 1689\nData Downloaded: 95.48KiB\nData Uploaded: 65.55MiB\nDownload Time: 8 minutes\nProcess Time: 3 hours\nUpload Time: 14 hours\nDatum Timeout: (duration: nil Duration)\nJob Timeout: (duration: nil Duration)\nWorker Status:\nWORKER              JOB                 DATUM               STARTED             QUEUE\nRestarts: 1\nParallelismSpec: coefficient:4\nResourceRequests:\n    CPU: 0\n    Memory: 64M\nInput:\n{\n  \"atom\": {\n    \"name\": \"fetch_intro_playlists\",\n    \"repo\": \"fetch_intro_playlists\",\n    \"branch\": \"master\",\n    \"commit\": \"2437ac4ff0b445d1b41c98c89cca9c58\",\n    \"glob\": \"/*\"\n  }\n}\nTransform:\n{\n  \"image\": \"gcr.io/canopy-196615/fetch-tracks:v13\",\n  \"cmd\": [\n    \"/app/pipelines/genre_intro_playlists/pachyderm/fetch_tracks.binary\"\n  ]\n}\nOutput Commit: 5cbc7a6518fe4a79bb077daacfc7d0e4\n``. Is there any more information on this bug - relative priority, eta for a fix, workarounds? We were planning to use this for a web crawling pipeline that doesn't discard old data. This is blocking us right now, and we'd like to know if we need to come up with a different solution. Thanks!. Great, thanks @jdoliner!. @gabrielgrant I haven't tried with tags, and unfortunately it's not trivial for me to switch things around - digests are fairly baked into how we push images & pipelines right now.. @gabrielgrant it's possible, but I've kicked off a number ofreprocess`es, so it's definitely not happening every time.\nIs there a suggested workaround for this? It would be nice to get these jobs back to running once per hour.. Hi @gabrielgrant, thanks for the suggestion. While extract-pipeline does have less \"extra\" stuff than inspect-pipeline --raw, it still has all the same comparison difficulties - differing key names, different text formats, inclusion of default values, etc.. All the input keys are snake_case, and all the output keys are camelCase. e.g. parallelism_spec vs parallelismSpec.. This bit us too, for 2 reasons: the addition of .scratch, and the fact that files under /pfs seem to have changed to symlinks. We use golang's filepath.Walk to iterate through input files, and that function does not follow symlinks. \nI think this should at least be noted prominently in the release notes. Ideally this sort of change also wouldn't show up in a point release, since it is not fully backward compatible.. ",
    "gabrielgrant": "@jdelfino sean has actually has landed support for reporting a bunch of pachyderm stats already: https://github.com/pachyderm/pachyderm/issues/923#issuecomment-388977845 If there are other specific stats you'd like to see included, it would be great if you could add them to that issue (or try out what's there)\n@sjezewski AFAICT the original ask in this issue is about exposing custom metrics to K8S for the purpose of autoscaling, rather than just basing scaling on CPU/mem. Specifically seems @jdoliner was referring to this: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics (originally introduced in 1.2, but has since changed again in 1.6)\nNot sure about the original 1.2 version, but at least as of 1.6 I don't think that has anything to do with prometheus -- k8s seems to be incubating their own metrics-server\n@sjezewski two questions:\n\ndoes this comment (linked above) still reflect the current state of stats collection?\nis what you're talking about in this issue allowing users to easily expose their own custom stats that are specific to (and generated by) their pipeline code, rather than the generic pachyderm stats that you're exposing for all pipelines in the system? (edit: whether or not that's what you're referring to here, it seems useful, and is something I think we've talked about before, so I've opened an issue for it: #3067)\n\n@jdoliner what specific stats were you thinking we'd want to expose? Number of queued jobs/datums would be my first inclination as to what people should be autoscaling based on, but not sure if you had something else in mind?. @JoeyZwicker This pattern of creating ticker commits that trigger an ingress pipeline (I think that's the currently-recommended approach, right @jdoliner ?) seems ripe for inclusion in our Cookbook docs\ncc: @dwhitena . Related issues:\n\nSpout/ingestion Pipelines #2958\nSQL Support #3066. Now that basic support for spout pipelines have landed in #3531 this should probably be revisited. @JoeyZwicker Is the ask here just to poll a DB with cron? (which could already be done) Or to have records pushed into pach from a spout pipeline on some regular cadence?\n\nIn general, these \"db connectors\" seems like a good candidate for figuring out what a pachyderm \"standard lib\" would look like: #2242. The use-case of having a number of commits added to a single repo before triggering a downstream pipeline is reasonably well-addressed by having another branch that gets manually updated at a slower cadence the branch to which the rapidly-received data is being added. This pattern is described in more detail in #2296 and the docs on deferred processing\n. The directory structure should probably mirror that of the HTTP API (to be documented in #2660 ). work on this is finally underway in #3411  :tada: . This seems like an extremely reasonable (as in, maybe the core?) use case of having provenance, from a user's point of view.\nTalked with Bryce about how to best get this info, both in terms of provenance (ie \"what files went into creating this file?\") and subvenance (ie \"what files resulted from this file?\")\nThe short answer is that the info to answer both of these technically already exists in the system. How to actually get at that info is...the longer answer :P\nThe reason this is complicated to do in a generic way, is that one file can contribute to (and/or have been contributed to by) many upstream files.[1] What Pachyderm stores are the \"from-a-datum\" indexes -- for a given datum, you can easily find:\n- Which files make up that datum\n- Which files were output from that datum\nbut, given a file, we don't currently store:\n- Which datums that file is in\n- Which datums contributed to that file\nWhich means that in order to go in the other (\"from-a-file\") direction, the index needs to be reversed.\nSo, to get provenant files:\n- get directly-provenant commits\n- for each commit, get the corresponding stats commit\n- filter the list of datums by whether the given file appears in it's output (glob-file /*/pfs/out/<path-to-file>)\n- union the list of input files that make up those datums\nSimilarly, to get subvenant files:\n- get directly-subvenant commits\n- for each commit, get the corresponding stats commit\n- filter the list of datums by whether the given file appears in it's input (glob-file /*/pfs/<input-repo>/<path-to-file>)\n- union the list of output files that came from those datums\nNot the prettiest, and not particularly efficient computationally, but certainly possible.\nWe should talk about:\n- whether this is actually the best way to get this info today\n- what facilities could be added to make this easier\n- how this is affected (and improved) by the reverse index storage that @brycemcanally is planning to work on soon (and how to best leverage/expose that info for this use-case)\n\n[1]: With commits, when we talk about provenance we're generally talking about \"full provenance\" (ie the commits in all repos upstream in the DAG ancestors). For files, I'm talking about just getting the direct ancestors. I'm figuring this is probably a good first step (and obviously if tracing further upstream is required, that can be done recursively)\n. One way I think this could possibly be done more easily/efficiently (with some minimal support from pachyderm), is through use of the datum factory to build a list of datums by (input) file, rather than building that list by querying and re-organizing info in the stats branch. We have some SQL integration work underway here: #3066\nalso, it's kinda old, but started documenting some possible patterns for how to integrate with postgres here: https://github.com/gabrielgrant/pach-postgres-demo. There are actually some great ideas in here. Some of this seems like it's probably no longer relevant, but we should be giving this kind of feedback to help figure out how some of the more detailed tuning fields of a pipeline spec should be used. Specifically talking about resource_spec requests (and, soon, limits), parallelism_spec, scale_down_threshold, and cache_size\nIn general, we do an alright job of describing what these are in the pipline spec docs, but give basically zero guidance as to how/why to use them\n@sjezewski I'm not sure which of the specific items listed in this issue are still relevant today. could you please create a meta-issue on the dash repo that migrates over the useful info (and link it back here for posterity). I certainly agree what you're asking for would be useful, but, unless i'm missing something, it doesn't sound to me like either of the two use-case examples (datum and job runtimes) are really the type of time-series metric data that any of these tools (grafana, prometheus, graphite...or collecd/RRDtool et al) are made for. My understanding is that, fundamentally, these plot a metric's value over time (you implement a collector that gets called on a regular interval and reports a value, the tool drops that datapoint on a graph, and potentially does rollups to compress historical data by summing, or averaging, or whatever aggregation)\nTraditionally, these are used for things like monitoring memory or CPU usage. Things that are \"poll-able\". If we wanted to, say, show the number of jobs running at a given moment over time, then this would be the tool, but i'm not sure how (and whether) to make use of them to collect these more event-based data forms you're looking for. Because they run sequentially, you could maybe poll the runtime of the latest job (though this would be mostly re-collecting redundant data, you'd still have to figure out how/whether to handle multiple jobs run within one polling interval, and this wouldn't give you any insight into which job corresponded to a segment of the chart if you noticed a spike), but for datums, which run in parallel, i'm really having a hard time imagining how this would work...\nFWIW, (as @sjezewski pointed out in #228), Button have open sourced a Prometheus exporter for pachyderm. The list of metrics captured by that button collector are again things that can be checked at a point in time by polling, such as: total number of jobs in various states; total number of datums processed; total bytes downloaded and uploaded by all jobs, etc.\nWhile interesting (particularly, if aggregated across many users, for getting an overview of how people are using pach in general), these seem less useful overall (and certainly less useful for debugging problems with specific jobs or datums) than the types of specific event-based info that (seems to be) motivating this issue\nI think something like a heatmap, burn-down chart, cummulative distribution (\"burn-up\") plot, or set of stacked bars view over our already-collected data in the stats branch and in job history, that let you drill down to the individual item level would better address what you're trying to achieve. Yes, if I wasn't clear enough about that: it's very likely I am misunderstanding parts of what these tools do/how they work :P Regardless, I certainly do think having some of this time-series info is useful, just want to make sure that we're clear about what we are and aren't getting from them. Specifically: this gives us aggregate trends over time, but not a direct way to identify and drill into outliers (especially at the datum level). Spent a little time today sketching out some ideas of how we might want to present some of that more detailed/connected info in the dash. Maybe chat quickly about whether some options and what we think would be useful tomorrow?. Is this actually still an issue? There have been some big model changes and a lot of refactoring since this was opened. PharmBio has a Helm chart they are using for deployment here: https://github.com/pharmbio/pbcharts/tree/master/pachyderm. That chart doesn't seem to have any support for the dashboard or HTTP interface to PFS. Unfortunately it seems to have been taken offline, but I believe the PharBio chart did have dash support (though it required using a custom domain IIRC), but also lacked the PFS HTTP interface.. @jonandernovella oh, sweet, it's back! that repo was offline for a while, wasn't it?. @jonandernovella no worries, just wanted to make sure i wasn't going crazy :P thanks for putting it back up!\nHoping we can merge your helm chart design with what i've been working on with @naztyroc (ie based on the official helm chart) to get a best-in-class recommendation for how dash deployment should work with helm. IIRC, we worked on getting the dash working with your helm chart sometime last summer. To confirm is that still working for you today? One feature i don't believe it supports is exposing the PFS-over-HTTP API, which is used for file download. Not sure if there are other things missing? . @LaurentGoderre the helm chart hasn't been updated to 1.8 yet (tracked in #3438), in part because no one has gotten around to it yet (so if you wanted to submit a PR that would be greatly appreciated! :wink:  ) but also in part because it isn't clear how migrations from earlier versions should be handled with helm (tracked in #3439). could the metadata explosion be avoided by processing a bunch of datum-tuples locally, and then committing the whole batch at once? From the users' perspective, the effect would be mostly the same: their code operates on a single datum-tuple. Presumably the job shim would just shuttle each set of datums into and out of /pfs before/after each run, and buffer it locally somewhere until it was ready to commit. This loses per-datum provenance, but saves a (potentially-very-large?) amount of network traffic/metadata storage. As for passing tuples of datums via stdin, not sure how that would work in practice. Encoding them in JSON objects or something seems fine in smaller cases, but if entire (potentially-large) files are being passed in, it seems likely there'd be cases where processing them on-disk would be preferable (or, if they're too big to fully buffer in memory, the only option). Yes, sorry for the confusion @dahankzter , you're right, it does seem the missing binaries are breaking homebrew installs. Reopening.. @dahankzter if you want to try the 1.2.4 release, that should get you going for now\nI'm actually on linux, so i'm not able to test the specific commands to run, but i believe you should be able to list the versions available with \ufeff\u2060\u2060\u2060\u2060brew search pachyderm\ufeff\u2060\u2060\u2060\u2060\nThen you should be able to \ufeff\u2060\u2060\u2060\u2060brew install <specific-version>\ufeff\u2060\u2060\u2060\u2060\n(I \ufeff\u2060\u2060think\ufeff\u2060\u2060 \ufeff\u2060\u2060\u2060\u2060\ufeff\u2060\u2060\u2060\u2060 should be something like \ufeff\u2060\u2060\u2060\u2060pachderm/pachyderm_1.2.4\ufeff\u2060\u2060\u2060\u2060 but that's what i'm not 100% sure of). LGTM \n\n. This should be updated to include instructions for generating binary build artifacts once #1134 is fixed. Ok, now it LGTM as well as Trump. He's not much of a detail guy ;). I believe this needs to be done as part of #1226 , but people do seem to have uses for --dry-run beyond just deploying from a private registry\nOmer Katz and Homme discussed this recently in slack: https://pachyderm-users.slack.com/archives/C3UN0S0Q0/p1508146446000362\n. related: #1833 (posted the above-mentioned slack convo in that issue). Reopening, since spark integration has recently come to the forefront again as an important roadmap feature\nThis is related to #1623 but the sense i get is that we need some integration specific to spark, rather than just a general approach to integrating other distributed frameworks (though we do need to refine that too) -- specifically, @jdoliner said he thought the general pattern of loading data into spark from within a generic pach pipeline container was insufficient: at best it's inefficient, and contrary to the standard spark paradigm of pulling data from sources. In many cases it seems it is impossible, since there are often more files than can reasonably be loaded through one machine, and AFAIK there is no way of ingressing to a single spark job from files on multiple machines. (something like #2902 may help to make it possible, but it will still be slow). @shukla-deepesh to clarify: are you loading that full 500+GB into spark and processing it as a batch, or are you hoping to stream it through?. The transform section of the pipeline spec docs explains how to set image_pull_secrets so pach can pull user images from a private registry, but AFAICS we still don't have any docs on how to actually deploy pachyderm itself from a private registry (which is what this issue seems to be about). Nope, it looked to me like that was the issue for making it possible (ie the code), and this was the issue for adding the docs. But if you want to track it all there, that works too :). Hey, just talked with @JoeyZwicker about adding a \"joins\" page under cookbook that just links to the blog post: https://medium.com/pachyderm-data/easy-distributed-joins-with-pachyderm-8307bab8a761. For anyone coming across this issue now, you can find details on using GPUs with Pachyderm here: http://docs.pachyderm.io/en/latest/cookbook/gpus.html. Ah, ok. Usage for put-file implies the path/to/file/in/pfs is a mandatory field (meaning it should fail with master too):\nUsage:\n  pachctl put-file repo-name commit-id path/to/file/in/pfs [flags]\nBut seems like that's just a doc bug. Filed as #1429. @jdoliner can you explain a bit more about how ListCommit is paginated at the API layer? From what I can see, both the current (1.3) and upcoming (1.4) versions of the command return a CommitInfos message, that contains an array of CommitInfo messages, so it doesn't look to be using streaming.\nAre you talking about the from/to (and number) params in the new ListCommitRequest message? Can I just pass any one or two (or zero to get all?) of the three of them? Which chronological direction are these referring to?. Interestingly, it seems that child commit id also references the commit in which the file was last modified, not the same commit as that of the parent file. I'm currently doing this in the dash by traversing the commit history and finding the first commit where a particular file hash appears, but this is pretty horribly inefficient, requires code that i shouldn't really have to maintain, and it seems silly that this info isn't available from the CLI\nfrom discussion with @msteffen : this is doable, but will require a migration to generate this info for all past files in all past commits. re-requested in https://github.com/pachyderm/pachyderm/issues/2065#issuecomment-397159072. we're going to solve this use-case by adding a --history [n] flag to ListFile. More details in https://github.com/pachyderm/pachyderm/pull/3277#issuecomment-446751226. Copying cat and rm messages/behavior seems reasonable (fail if file doesn't exist; rm  has a --force flag)\n```\n$ cat lsdf\ncat: lsdf: No such file or directory\n$ rm sfdsf\nrm: cannot remove \u2018sfdsf\u2019: No such file or directory\n$ rm --help\nUsage: rm [OPTION]... FILE...\nRemove (unlink) the FILE(s).\n-f, --force           ignore nonexistent files and arguments, never prompt\n``. not sure how we deal with recursive deletes or directories, though (but likely differently thanrm). @rawc0der yup, you're right -- Pachyderm's API uses GRPC (which in turn use protobufs), so all that's needed to interact with Pachyderm from Java is to use [the Java-native GRPC library](https://github.com/grpc/grpc-java). It looks like you should just be able to include the GRPC lib (whether by downloading the.jars or using the gradle/maven config in that README) and drop the Pachyderm .proto definition files for [the PPS API](https://github.com/pachyderm/pachyderm/blob/master/src/client/pps/pps.proto) and [the PFS API](https://github.com/pachyderm/pachyderm/blob/master/src/client/pfs/pfs.proto) into yoursrc/main/proto` dir. If you aren't using gradle or maven, it looks like you may also need to compile the .proto files\nThere's a full example of using a GRPC client from Java in the GRPC docs: http://www.grpc.io/docs/tutorials/basic/java.html#creating-the-client\n. JD and i were talking about this recently. We're far from sure that we actually want to do this, but seemed like it could open some interesting use-cases (and potentially allow elimination of some shuffle pipelines; unclear what the performance implications of this would be, though)\nThat conversation also led to the discussion about ** globs (#3335), and it does seem like a number of the use-cases we were originally thinking regexs would solve can actually be done with our extended glob syntax (matching against a set of extensions, for example). These non-obvious cases should probably be documented.\nThere are still some cases, though (like matching a character range; potentially useful for dates, say) that AFAIK can only be achieved with regexes (or, today, with a shuffle step). I wouldn't exactly call this doc \"easily discoverable by someone looking for \"joins\"\" -- it doesn't show up anywhere on the first page of google search results for \"pachyderm join\" (whereas the blog post with the broken link is the first result), and it doesn't jump out at me if I'm looking for info on joins in the index or sidebar. I've submitted a PR that changes the page title to hopefully make this more discoverable, but given the blog post is still the first result at the moment, could someone please update the link to point at this docs page? (I don't have access to edit the blog)\n\n. @derekchiang @jdoliner is this addressed by the Union inputs PR? https://github.com/pachyderm/pachyderm/issues/1665. a few of us discussed this last week after a request in users. It seems this got closed because a commit containing Partially resolve #1497 closes #1497 (since it contains the substring resolve #1497; thanks github :+1: ) \nReopening with a few specific proposals:\n\ndocument the actual requirements for pipeline and repo names (took a crack at this in #3006, but i imagine i'm missing some details, and that is only for pipelines -- not sure if the same rules apply for repos, and where the repo naming info should go in the docs)\nremove the pipeline-name character length limit. this is currently limited because we use pipeline names as k8s resource names, which have a 64-char-max-length limit. seems this could be worked around by having a short-hash version of longer names. for the sake of being able to find the k8s resources related to a particular pipeline, i'd recommend:\nonly using a hash when a name actually exceeds the maximum length\nusing something like <first 20 characters of pipeline name>-<10 char shorthash of name>-<last 20 characters of pipeline name> (or, if . is allowed, maybe ... instead of -?)\nexpand the range of acceptable characters by url-encoding in places where plain alphanumeric strings are required (and create explicit checks for the restrictions we do want to enforce). in talking with @sjezewski and @msteffen we think moving to using an opaque (UU?)ID in k8s resource names and storing pipeline name in a label (pipeline?) is probably a better approach than some ugly name mangling. This isn't ideal in that a basic kubectl get all would no longer show useful info, but kubectl get all -L pipeline's output makes navigating that extra level of redirection fairly straightforward\n. On the specific cloud provider docs pages, there is no version specified for the pachctl install commands. As a result, 1.3.16 is installed (at least via deb; not sure about brew):\n\nhttp://docs.pachyderm.io/en/stable/deployment/google_cloud_platform.html#install-pachctl\nhttp://docs.pachyderm.io/en/stable/deployment/amazon_web_services.html#install-pachctl\nhttp://docs.pachyderm.io/en/stable/deployment/azure.html#install-pachctl\nWe really should explicitly specify a version every time we have an install command (even if we think that's the default install at the time, it will inevitably change in the future). Pin all the versions! :P\n. clarify uuid dependency: it was listed in the aws deploy docs  in stable but is gone in latest\nIt's needed for the deploy script, but not the manual deploy, so should be added back in at the top of the script section\nShould also include an explanation of what's actually needed (sudo apt-get install uuid and some brew command, i suppose?). http://docs.pachyderm.io/en/latest/fundamentals/getting_data_out_of_pachyderm.html?highlight=service#examining-file-provenance-with-flush-commit has a broken link to a \u201cHow to leverage provenance\u201d Guide. Not sure where this should be pointing to (we used to have an empty placeholder provenance page, but that seems to be gone from v1.4). Googling for \"pachyderm service\" brings up http://docs.pachyderm.io/en/stable/deployment/serving_data_from_pachyderm.html which is currently a broken link. Did we figure out how to add redirects? I think that should be pointed here: http://docs.pachyderm.io/en/latest/fundamentals/getting_data_out_of_pachyderm.html. the link to glob patterns is broken in https://github.com/pachyderm/pachyderm/blob/master/migration/1.3.x-1.4.x/README.md#inputs\n(it's missing a .md)\nis it supposed be broken on GH to make it work on readthedocs or something? Afaics, we only actually link to this doc on GH, not RTD. Not clear to me why this should just be a docs change? Seems fairly straightforward to make this change in a backwards-compatible way: since we don't have any nouns named \"get\" \"create\" \"list\" etc, we can have both forms work for a couple releases while we deprecate the old forms and slowly migrate over our documentation\nI find myself tripping on the mismatch in command naming conventions between kubernetes and pachyderm (is it inspect, get, or describe??) practically every time I'm poking around a cluster. Having user code process a batch of datums (#1662) is one approach to solving this. Moving shuttling datums in and out of the user-visible /pfs dir is another (discussed here and here ). @dwhitena  when you say \"distributed ad hoc queries\" and/or \"online, streaming operations\" are you talking about live, exploratory analysis?\nbecause we are more production-oriented than many (most?) other tools, doing quick, exploratory (possibly throw-away) analysis has more friction than i'd like right now. hoping this may be something that can at least partially be addressed by integrating jupyter with the UI, but in the simplest implementation (using something like the 1.3 service model), we still don't really solve the problem of wanting a sandbox backed by distributed computation, since the service would still be running on a single instance. I'm not sure I totally understand what kind of operations are enabled by exporting pach metadata to an external DB... is this to somehow allow ad-hoc querying of their data? or to look into their repos'/pipelines' history or something?. This is maybe something to discuss again for 1.7 now that services are making a comeback (see: #2270 ). This should also probably be renamed \"input_commits\" (plural), since it is a repeated field. Not sure whether this should be an AND vs OR operation, and whether all inputs should have to be specified. My inclination is that exact matching makes the most sense (return jobs with exactly this list of input commits). @jdoliner sorry, that was a typo -- i meant \"input_commit(s)\". Fixed it.. We had some discussion a while ago about ways to stream multiple datums to a process as an optimization for this case of significant user-process startup time being the dominant factor in some pipelines. This requires some sort of signalling between the user's process and pachyderm, so I don't think we came to a conclusion. One pretty obvious option that I'd discussed a bit with @msteffen would be to have a signalling socket that pachyderm writes to once it's shuttled the current datum's files into place and that the user code writes to when it's done with a datum to alert pachyderm that it should move the next datum's file into place (this could operate over stdin/stdout, but that would conflict with the current behavior of treating stdout as log output)\nSo long as the datum's PFS structure was pre-prepared and moved into the user-visible location as a single atomic move, the user's code shouldn't ever see inconsistent state, but I imagine this could potentially have issues with users' code maintaining references to old datums' files.\n. Re-reading this issue, it seems this is more about pachyderm doing batching behind the scenes, as compared to #1592 which is more about allowing a user to process multiple datums within a single run of their code. My understanding is that the former (ie this issue) has been implemented. Is that correct, @jdoliner ?. Yea, agreed that those are the only two that clearly should be the same, though Jobs also seem like they could have a \"creation\" time (even if that's not, perhaps, what's being recorded in this field)\nFWIW, I think my pref for created_at, updated_at etc. comes from that being widely used in the web world (thanks originally, i think, to it being the default in Rails). But obviously doesn't matter, so long as it's consistent.. According to @jdoliner the possible values are s3, gcs, and as (with gs and wasb as aliases for gcs and as respectively). re-requested in #3406. As discussed with @sjezewski the ports internal and external ports are purposefully in different ranges to avoid confusing them, so just the one needs to be changed in #1754. Added error swallowing as #1767 . This PR fixes #1753 . Any idea what would be entailed in making this work? I'm going to be in a car for a number of hours with no internet again next weekend, and if it's not too hard, might be nice to be able to get some work done. Relevant slack convo: https://pachyderm-users.slack.com/archives/C3UN0S0Q0/p1508146446000362\n\nHomme Zwaagstra\n2:18 AM\nHi all,  does anyone know of a way to set tolerances for pipeline workers?  Our situation is that we have an instance group composed of high performance (read expensive!) nodes that scales down to 0 when there is no k8s pod pressure.   We only want pipeline workers to run on this instance group and not any of the other services in the cluster. This is usually the case and there aren't any problems, but occasionally a service will be evicted and end up re-deploying on this worker instance group when a job is running.  This prevents the instance group from scaling down to 0 and costs :moneybag: over the longer term. We would be like any taints that we have set on nodes in the IG to be tolerated by Pachyderm workers but can't figure out how to set these tolerances.  Should this be in the pipeline spec, is there another way to set tolerances, or is there a completely different approach that gets us to where we want to be? Thanks :slightly_smiling_face: (edited)\nOmer Katz\n2:34 AM\n@Homme I strongly suggest to avoid pipeline.spec for anything other the initial generation of the relevant YAML files.\n2:35\nThe workflow should be pipeline.spec -> Draft (for dev https://github.com/Azure/draft) -> Helm (for deployment)\nGitHub\nAzure/draft\ndraft - A tool for developers to create cloud-native applications on Kubernetes.\n2:36\nYou can generate a yaml file to work with by using the --dry-run option of pachctl create-pipeline (edited)\n2:38\nAny non-trivial deployment should use the k8s primitives and not the abstractions pachyderm provides for data scientists imo\nHomme Zwaagstra\n2:39 AM\nHi @the_drow and thanks for the reply and suggestion.  Being able to edit the pipeline YAML would be a perfect workaround to not being able to specify tolerances in the spec, and I did previously have a look at the create-pipeline command to just get YAML output, but couldn't find anything useful: I still don't see the --dry-run option for that command (I'm running 1.6.1).\nOmer Katz\n2:39 AM\nI might be confusing with some other option name but there is such option\nHomme Zwaagstra\n2:40 AM\nAny non-trivial deployment should use the k8s primitives and not the abstractions pachyderm provides for data scientists imo\nThat makes sense to me, and I already use some other services running alongside our pipeline to egress/ingress data to good effect.\n2:42\n@the_drow I can see the --dry-run option for deploy, but nothing for create-pipeline, unless I'm missing something?\n```\npachctl create-pipeline -h\nCreate a new pipeline from a Pipeline Specification\nUsage:\n  pachctl create-pipeline -f pipeline.json [flags]\nFlags:\n  -f, --file string       The file containing the pipeline, it can be a url or local file. - reads from stdin. (default \"-\")\n      --password string   Your password for the registry being pushed to.\n  -p, --push-images       If true, push local docker images into the cluster registry.\n  -r, --registry string   The registry to push images to. (default \"docker.io\")\n  -u, --username string   The username to push images as, defaults to your OS username.\nGlobal Flags:\n      --no-metrics   Don't report user metrics for this command\n  -v, --verbose      Output verbose logs\n```\nOmer Katz\n2:43 AM\nYeh you are right. It is missing\nHomme Zwaagstra\n2:44 AM\nThat would be very useful functionality to have, though - nice idea for an enhancement.\n2:44\nupdate-pipeline doesn't have it either.\nOmer Katz\n2:45 AM\nSee https://github.com/pachyderm/pachyderm/issues/1833\nHomme Zwaagstra\n2:49 AM\nCool, although that issue would only be useful in this situation if it could be used to update k8s specs that aren't in the pipeline spec.  I like the --dry-run option better as it seems more flexible.\nOmer Katz\n2:52 AM\nWhat you should have is the option to generate a helm chart or a draft from a pipeline spec. --dry-run is very basic\nHomme Zwaagstra\n2:55 AM\nAuto generation of charts or drafts from a pipeline spec would be handy, but a lower level --dry-run option would be very useful too.  The former might be able to build on the latter, but either way manipulating the k8s spec before it is applied to the cluster would be great.\nOmer Katz\n2:57 AM\nIn the meantime you can extract the yaml template from the code\nHomme Zwaagstra\n3:00 AM\nYes, that's an option. I was thinking it may be easier to do something like kubectl get -o yaml rc/pipeline-my-pipeline-v1 | my-update-script | kubectl apply -f -.. this mechanism could also be useful for creating pipelines based on a standard template and for creating multiple, slightly altered versions of a pipeline (ie DAG branching). It looks to me like this issue shouldn't actually be an issue due to this block: \n\nhttps://github.com/pachyderm/pachyderm/blob/462145baade0514ffbfa47637711945e1c3917ea/src/server/worker/api_server.go#L1729-L1736\nIs that not doing what I think it's doing, or is there something else missing?\nIt does look like that may require the user to be explicitly specified as part of the pipeline spec, rather than inheriting from the docker image as a fallback (as I believe we do elsewhere). I'm not sure it's actually possible, but it feels a bit like it should be able to treat this like a special case of #1592 (if we were to implement that using pipes to shuffle data in and out). Not sure about how signalling multiple files down a pipe would actually work, though -- can you just write each with and EOF between?. We were discussing this again just last week (possibly in the context of the new pipeline API?), so gonna re-open . @kalugny I haven't seen that error before, so thanks for letting us know about this. Which version of pachyderm are you using? And do you know if you deployed a specific version of the dashboard?\nAlso, what's currently in your cluster in terms of pipelines & repos?. From https://github.com/pachyderm/pachyderm/issues/1885#issuecomment-305388705 : \n\"Append semantics are not documented anywhere and there is no list of formats that support mutation in Pachyderm\". #1894  Add a cookbook/example/best practice for incremental pipelines. I believe @jdoliner and @derekchiang decided that these naming changes would be deferred until a later release when we have some other reason to require users to run a migration, right?. We get something similar to item number 3 (\"Save the output of a job that failed\") from the stats commit on a per-datum basis, but we don't do merging of datum outputs. Would it maybe make sense to create a single, merged output (ie what would have happened as the last step in creating the output commit if the job had been successful) in the stats commit for failed jobs?. @suneeta-mall i certainly agree this could be more user-friendly. I believe the issue is that, by default, a k8s cluster may have no route in to the pachd pod from the outside world (unless manually exposed), so port-forward is needed for us to piggy-back on the hole that k8s does have by default for it's own API server\nIf/when a user has manually opened a route to their pachd pod (or, in the case of local deploy, where it is available by default), the user needs some way of telling pachctl where it should be trying to connect. This is where ADDRESS comes in. I believe this can also be specified in the pachyderm config file ($HOME/.pachyderm/config.json), though I'm not sure exactly how off the top of my head\nI totally agree that this is pretty subtle/tricky to understand (esp. for newer k8s users). If you wanted to submit a PR with improved docs about this that would have helped you avoid getting stuck (or, better yet, suggest an improved way of connecting that would require less explanation) I'd really appreciate it!. @suneeta-mall fyi: submitted a feature request related to your suggestion about ADDRESS autodiscovery here: #3089  (not sure if you ever got around to submitting a PR addressing this? i didn't see one, but definitely could have missed it). This should be addressed/clarified as part of #1869 . Re: data de-duplication -- if this is storing output files from each input datum that then later get concatenated into larger files, will those be deduped? Are we storing a block per concatenated chunk, or doing the concat and then storing the whole file together?. Should be completed as part of #1869 . Fixed in 3a3bffebb9c305b0640dfecb117e12578a29b16c : http://docs.pachyderm.io/en/latest/fundamentals/incrementality.html. Looks like the patch in #1909 just adds the prefix onto that message (malformed pipeline spec:), but leaves the rest of the original source error message details intact too. \nWere you running into this due to the same invalid pipeline spec (stdin as a string rather than a list)?. @suneeta-mall yea, it would definitely be nice to provide better error messages. Opened an issue here: #3072. I think we could still use a bit more clarity around this -- what we call a datum is the result of combining the files from multiple atom inputs using crosses and joins (ie it is multiple files). To me, that doc makes it sound like each of the individual files matched by a glob pattern is a datum.\nthese exact semantics are really quite complex to communicate, and there's a fair bit of subtlety that i think a lot of users miss. we probably need some diagrams and some specific examples. Note that just having commit_modified for a file (which disappeared in 1.4 -- see #1457) would help get closer to this: it wouldn't give all the input datums that went into creating a given output, but at least would allow the user to start exploring the logs for the right job (which, assuming they know the structure of the computation, is likely all they need). I think I may be missing something...I thought appending output only happened within a single commit, not between multiple commits?. @msteffen Any idea what that TestGetLogs failure is about?. I definitely agree that not all fields should have flags, but there is a --description flag on create-pipeline, and (at least in the release notes ) that's the recommended way to set the description, so it seems a bit strange to never be able to change that description. Do you think description should both be initially set and updated by adding it directly to the pipeline spec?. The other reason it might make sense to special-case the description field is that it's the only field that doesn't actually affect processing in any way, so it'd be nice to be able to update it without triggering reprocessing (which i think will happen when the version gets updated by issuing an update-pipeline, right?). Fixed by #1981. related: auto-merging multiple inputs sourced from the same repo #2441. Having \"Troubleshooting\" be a new top-level subsection within \"Reference\" makes the most sense to me. Maybe I'm missing something from the discussion on #1036, but reading that it sounds like a lot bigger feature request than what i'm asking for here (and I'm also not sure that it would actually address this issue). Whereas #1036 is asking about how to have multiple versions of a pipeline used/developed in parallel, what I'm asking about is (I believe) a pretty simple tweak to the way things work today: basically where we currently increment the pipeline version number when a new pipeline spec is deployed, I'm just asking to store a snapshot of the whole previous PipelineInfo message.\nI think this could be as simple as just adding repeated PipelineInfo pipeline_history = 23; or whatever into the PipelineInfo proto message, then having whatever code currently increments the version number just move the history list over from the previously-stored PipelineInfo and append that previous revision whenever a new pipeline spec is submitted. Maybe there's more complexity I'm not seeing, though?. I'm certainly not tied to any particular API for this functionality, but I'm still not seeing how #1036 proposes to provide history of the pipeline versions that output to a single branch\n(to answer your specific question, though, it doesn't seem that crazy to just have a history list be populated on the top-level PipelineInfo message, does it?). It makes sense to me that the commit be created at the beginning of the job. If files can be added incrementally as the individual datums' processing completes, then once @jdoliner 's change to allow reading uncommitted files lands, this could be really useful for inspecting in-flight results, particularly for long-running jobs (I'm pretty sure this isn't nearly as straightforward as I'm making it sound given how output blocks & files are added to PFS, but a guy can dream, right? :)). Sounds like this addresses #1738. Also, re: pipeline versions, it would be helpful if old pipeline specs were stored (maybe just as a list within the PipelineInfo document?) before being replaced, to make it easy to access previous versions (right now I need to search through old jobs to find the transform spec)\nSee #1940 . Thanks for getting around to these, @derekchiang \nThe one other description-related problem I'd mentioned (no way to edit repo descriptions) didn't seem to have a GH issue, so I opened one: #1995 . Ah, I thought you'd asked for it to be beneath files (or something similar) to allow other API endpoints to be added at the top level later.\nShouldn't commit ids be namespaced to repos? (ie pfs/<repo-name>/<commit-id>)\nThough really, a \"proper\" RESTful endpoint structure usually follows a <object-type-noun>/<object-identifer> structure, meaning it should be something more like:\nGET http://pachd:658/pfs/repos/<repo-name>/commits/<commit-id>/file/<...path/to/file...>\nWhere \nGET http://pachd:658/pfs/repos/\nwould return a list of repos and\nGET http://pachd:658/pfs/repos/<repo-name>/commits/\nreturns a list of commits within a repo.\nThis lets you have other nouns without creating conflicts, like\nGET http://pachd:658/pfs/repos/<repo-name>/tags/. This is implemented for files in #2043 \nAs discussed with @sjezewski we would like to also be able to download directoiries and entire repos as as a single .zip or .tar.gz file\nEdit: added as #2062 . Is this implementing batching from the user's POV, or just in the sending of datums to workers (ie with no API change)?. @sjezewski is this still an issue, or did this get implemented along with raw/non-download file output?. I still have an open issue on the dash that is blocked on this. this seems more relevant now that branches (and thus BranchInfo objects) are more prominent in the system post 1.7. IIUC Technically those definitions aren't necessarily accurate for manually updated branches, but they're probably sufficient for the use case I'm describing, since they should hold for pach-maintained master/stats branches (right?). @JoeyZwicker Sounds like https://github.com/pachyderm/pachyderm/issues/1457 is basically what you're looking for?. This came up again recently in the context of discussing/trying to clarify the behavior of stop-job vs delete-job. Any chance we can make these changes for 1.9?. added this as an edit, but for clarity, i'll make it a new comment:\nBasically, if the JobID is part of the \"composite primary key\" of a datum (which it currently is -- you can't fetch a datum without a JobID), then the Datum message should have a Job field. offline discussion with JD: although a \"Datum\" as a group of files is not specific to a Job, the message we currently return from the Datum API contains state that is relevant only to the specific Job for which that datum was requested. This is one place where the distinction between \"Datums\" (a group of files) and \"runs\" (the processing of that group of files within a given job) becomes pertinent.\nConclusion: for the moment, the DatumInfo reply should contain the Job to which it applies. Later, once we expose Runs as a first-class object, that field (and the job-specific state info) should probably be removed from Datum, and put into a Run message\nAlso discussed that \"run\" may be the wrong term to use, since many (most?) runs are not actually executing (\"running\") the user's code, and in those cases the only really important info is the fact that a run is skipped. AFAIK this has not been done, and still should be. I don't think it should take long for someone who has a better understanding of how this deploy/undeploy code works and how we group pachyderm parts of the system (not sure if there's anything other than suite=pachyderm?). It may just be a matter of adding the right tag in the right place in the deploy manifest. Just ran into weirdness from this again:\n```\n$ make clean-launch-dev\ncheck that kubectl is installed\nwhich kubectl\n/home/gabriel/.local/opt/kubernetes/bin/kubectl\npachctl deploy local -d --dry-run | kubectl  delete --ignore-not-found -f -\nkubectl  delete rc -l suite=pachyderm\nNo resources found\nkubectl  delete svc -l suite=pachyderm\nNo resources found\n$ kubectl delete all -l suite=pachyderm  # manually delete remaining pods\npod \"dash-3498131396-mjg3h\" deleted\ndeployment \"dash\" deleted\n``\n@jdoliner is this something that's easy to fix? Is there something that you're doing with the other pach pods that makes them go away that I could be doing with this dash pod?. For backwards compat, should probably accept bothADDRESSandSOMETHING_ADDRESSfor some time, change the docs, and possibly start issuing a deprecation warning. @ysimonson any chance you want to finally get around to this at some point too, since you seem to have become our port master?. This separation also makes it clear what info is available for all jobs (a list of Datums) and what is only available for jobs that have stats enabled (the Run info). This is important, because at the moment we've lost the ability to view logs in the UI for jobs that don't have stats enabled \n. was just talking about this with @brycemcanally @jdoliner and @msteffen as part of the datum improvements we want to tackle in 2019 . Fixes pachyderm/dash#99 (which is part of #2153). I might be misunderstanding what you're trying to achieve, but filtering by input file(s) is possible directly fromget-logs` today:\npachctl get-logs --job=<job-id> --inputs /path/to/your/input/file\nAre you trying to do something more complex?. Somewhat. At some point (maybe in December?) this was blocked on clearance from legal. i think we came to a conclusion on that (though not sure what it was)\nBut there's stuff to be added both to the front- and back-end (specifically, right now we log usage stats in a number of places in the backend, and we'd like those logs to include the enterprise activation code when one has been installed). This is related to some of the extra hackery I've been having to get the info I need for stats (as discussed a bit yesterday with @jdoliner ). We agreed we're probably going to want to move some of that upstream. I believe this assumption that a job can only have a single failed datum is no longer valid -- my understanding is that as of 1.7 a job will try all datums (was just talking with @sjezewski about this). FWIW, rather than a single standard lib that we maintain, we probably (eventually) want to have some sort of index/ecosystem of contributed pipeline transforms (likely with some \"officially-blessed\" items, \u00e0 la DockerHub). To start, this could probably just be a centralized list of pach-pipeline-repo links. Removing the token with etcdctl doesn't seem to work:\nThe key gets removed from etcd\n$ ETCDCTL_API=3 etcdctl --debug --endpoints=http://localhost:32379 del pachyderm_enterprise/token\n1\n$ ETCDCTL_API=3 etcdctl --debug --endpoints=http://localhost:32379 get pachyderm_enterprise/token\n$\n...but the API still returns the state as ACTIVE\n@msteffen believes this may be because of a caching issue. Nope, between deactivating by setting an old timestamp and deleting with etcd, I think that solves my use cases. Thanks!. Weird that our date tools would be producing different formats, but it's also weird that Go clearly does recognize the format and translates it into what it seems to want: parsing time \"2017-09-11T14:51:02-0700\" as \"2006-01-02T15:04:05Z07:00\"\n...but then still for some reason refuses to accept it -_-. this should both be implemented in pachctl (delete whatever file is persisting the pach token locally) and in pachd (end the client's \"session\" by deleting the etcd entry for the client's current pach-token in the auth token list). \n\n. Discoverability permissions specifically is tracked in #2434. Is that top pipeline that pulls data from S3 cron-triggered on a short interval? Or is it running as a spout process outside of pachyderm?\nI can think of several options for how this would work, but none of them are great.\nIf the spout is running outside of pachyderm, one option could be to only close the commit that it's writing into when you want the report generated.\nAnother option that would work regardless of where the S3 spout is running would be to have a cron job copy data from the S3 repo to an input repo that triggers the report pipeline. Because of pachyderm's data de-duplication this copy command should be quick, and won't result in extra storage costs. You will, however, lose provenance tracking between the report and the original input data.\nIf that provenance is important, another not-so-pretty option could be just keeping this reporting pipeline in a normally-paused state, and having another cron-triggered pipeline un-pause it occasionally to chug through the backlog. My understanding is that this will process each commit to the S3 repo as a separate job, though*.\nIf you're only adding data to the upstream repo and you want to effectively \"squash\" all commits from the past month together (to only trigger one job with all the data added), I think having the cron job add the S3 repo as an input to your report pipeline, call flush-commit on the HEAD commit in the S3 repo, then remove the S3 repo as an input would get what you're looking for, but feels preeeety hacky.\n*: to be clear, it won't reprocess your actual data, but if you're just adding new data every commit, it will repeatedly check that the old datums have been processed. Depending on how many commits and datums you have, this could start to be meaningful overhead. Image is here: https://hub.docker.com/r/pachyderm/grpc-proxy/tags/. See error details in  https://github.com/pachyderm/dash/issues/10#issuecomment-332972800 \nEdit: error details are in comment below. Ah, yea, sorry, that's a private repo. Here's what @dwhitena wrote over there:\nSo, I tried the following from an object store URL (s3://):\n   1. Uploading a directory recursively, and\n   2. Uploading a single file\nAnd the results are:\n\nThe recursive upload succeeded w/out error in the dashboard, but the data is no where to be found:\n\n\u279c  opencv git:(master) \u2717 pachctl list-file test2 master\nNAME                TYPE                SIZE\n\u279c  opencv git:(master) \u2717 pachctl list-commit test2\nREPO                ID                                 PARENT              STARTED             DURATION             SIZE\ntest2               ccd3205e1d934cbba4ce7b6f93624fad   <none>              31 seconds ago      Less than a second   0B\n\u279c  opencv git:(master) \u2717 pachctl list-file test2 ccd3205e1d934cbba4ce7b6f93624fad\nNAME                TYPE                SIZE\n\nThis actually gave me an error related to a new line character. I think someone else reported this right?  Anyway all the related logs are here:\n\n$ kubectl logs po/pachd-1036467475-m9bs8 | grep \"s3://\"\n2017-09-28T21:37:31Z INFO pfs.API.PutFile {\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"test2\"},\"id\":\"ccd3205e1d934cbba4ce7b6f93624fad\"},\"path\":\"blah\"},\"url\":\"s3://object-detection-model\",\"recursive\":true}}\n2017-09-28T21:37:31Z INFO pfs.API.PutFile {\"duration\":0.000436399,\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"test2\"},\"id\":\"ccd3205e1d934cbba4ce7b6f93624fad\"},\"path\":\"blah\"},\"url\":\"s3://object-detection-model\",\"recursive\":true}}\n2017-09-28T21:39:38Z INFO pfs.API.PutFile {\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"test2\"},\"id\":\"2ce91cb7ff6a4aebbf945815a0496c55\"},\"path\":\"airplane.jpg\"},\"url\":\"s3://object-detection-model/blah/airplane2.jpg\"}}\n2017-09-28T21:39:38Z ERROR pfs.API.PutFile {\"duration\":0.000994404,\"error\":\"InvalidEndpointURL: invalid endpoint uri\\ncaused by: parse https://s3.us-west-2\\n.amazonaws.com/{Bucket}/{Key+}: invalid character \\\"\\\\n\\\" in host name\",\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"test2\"},\"id\":\"2ce91cb7ff6a4aebbf945815a0496c55\"},\"path\":\"airplane.jpg\"},\"url\":\"s3://object-detection-model/blah/airplane2.jpg\"}}. This code is being touched in #2365 but not sure if that PR addresses this specific issue. @derekchiang ?. Should this not get installed as part of the deb setup (and presumably whatever the equivalent is for OS X installs)? Doesn't look like it is a.t.m., right?. nvm... user error -_-. I believe @derekchiang was working on this somewhat recently. Did that go anywhere?. Looks like work i was thinking of may be in #2293, which doesn't seem to include anything about progress reporting. do you know if the process is sent a SIGKILL, SIGINT or SIGTERM? (Wondering if there's a chance to do a graceful shutdown). To be clear, is the interface that got merged the same as what you mentioned in this comment: https://github.com/pachyderm/pachyderm/pull/3325#issuecomment-451545784 ?\ni'd still prefer if we do the right thing on linux, at least\nam i understanding correctly that the issue is that there's no standard location on OS X?\nit seems this just be part of what happens when you install the .deb.... clarification for those (like me) who didn't actually know how this all works under the covers -- bash-completion is a .deb package that adds extensible the completion interface to bash, having arbitrary programs define their own completion logic is not something built into bash by default.\nIt seems the ideal way this would work would be to do something like  command -v complete to detect if bash completion is installed, and if so auto-install our completions (also: set bash-completion as a suggested package in the pachyderm deb package).\nAre the completion locations different on different linux distros?  or just different on OS X vs linux?\nFWIW, auto-install of these definitely does happen on ubuntu, at least -- I don't believe I've ever manually put one of these into my /etc/bash_completion.d/ dir, and definitely haven't manually put the 23 that i'm seeing there right now. @homme thanks for the issue report\nSpecifically, pachctl should use the datum pagination that is already available through the list-datum GRPC API. Removed @derekchiang \n@msteffen i think you were saying you have windows installed on your machine, right? . @pappasilenus I think you were asking about this yesterday. Did you get this sorted out? . Thanks a lot for testing this out and updating us, @maedoc . Not sure why this was closed: the current behavior of hard-killing services any time new data is added to an upstream repo is still problematic (even more so now that running Jupyter notebooks as services is working again). This would definitely make the auto-completion more useful, and is something we've discussed building. Unfortunately getting potential values (eg. fetching the list of repos via an RPC call to pachd) is substantially more complicated than the simple/static completion we have today, so haven't gotten to it yet.\nIf any community members are interested in taking a stab at a first substantial contribution, this seems like it would be a good, well-isolated part of the code on which to do so. @JoeyZwicker do you particularly think this shouldn't be worked on? This bug wasted several hours of time for me, homme, matt and JD trying to debug these supposedly-failed jobs, and it seems like it should be a single-line (or close to it) fix. As discussed in slack, when the dashboard's CD system is fully operational, these files will contain a full list of dash versions that have been tested to be compatible with each version of pachyderm. \npachctl assumes they're listed in order from oldest to newest, and will use the last in the list (the latest) as the version to deploy by default\nFor now we're updating the list manually, so pachctl deploy stops complaining\nLGTM. Not sure if any of these are requirements/hard reasons to not proceed this way, but a few considerations we should take into account:\n\nDex's SAML connector doesn't support refresh tokens. Without that I think either people would have to either constantly re-login, or else we cache auth results locally, but that would mean we wouldn't reflect changes from the SAML server. According to their docs, this is a fundamental issue with the way dex works.\nit appears dex doesn't have support for RADIUS, kerberos, or CAS which afaiu are all pretty widely used. Not sure if there's any fundamental reason it couldn't interop with those, but it's not super encouraging that the radius issue has been open for a month with no comment (and the other don't appear to have ever been mentioned)\ndex provides authentication, but not authorization. At the moment, we hold all authz info internally, but if we did want to support using LDAP-based authz, or ABAC/XACML*, it seems having dex as an indirection layer may complicate doing so.\n\n*: This seems likely: afaiu ABAC was specifically designed with (\"big\") data security in mind/is supported by hadoop, and it's usage over RBAC is now officially recommended by NIST/mandated for federal US government organizations. Another issue is that apparently dex doesn't really work with OIDC groups (for example google's auth system): https://github.com/coreos/dex/issues/863 and https://github.com/coreos/dex/issues/1065\nI don't know whether we've actually had demand for this, but I'd imagine many orgs (Pachyderm included) are using google apps as an auth provider, so having that integration work well would be great\nThere are a couple PRs open to address (some of?) the issues, but they seem to not have been touched in months (and the repo as a whole hasn't seen a commit in about 8 weeks). Overall, dex looks super promising, but I am a little concerned about leaning too heavily (ie basing our auth model) on a fairly new project that is showing signs of already not being actively maintained :/. to clarify scope: are groups a flat namespace, or can i add a group to another group?. Just answered basically the same question from @homme and @bbajer (Bart) in slack this morning:\n\nWhat are the steps to migrate pachyderm between clusters (presevering etcd and the persistant volume and pointing it back to the same S3 bucket)?\nSimilarly how do we ensure that when we want to do a clean deploy that everything underneath pachd has been cleared out?\nGabriel Grant 9:08 AM\n@bbajer my understanding is that if you undeploy a cluster and redeploy with the same PV and object storage source, everything should come back up as expected\n(the flip side being that if you use a fresh PV and bucket that you should have a completely clean slate)\n\nUnclear exactly what he means by \"migrate\": are both clusters running on the same cloud, and have access to the same storage sources, or do you actually need to move data? (eg moving GCS to Amazon, for example. Or, probably even moving between Amazon regions)\nThe former should be pretty straightforward (though there's a bit of added complexity in finding the PV name (if it was auto-created by pach) and adding it to the pachctl deploy command on the new cluster). Not entirely sure of the steps in the second case, but I'd imagine copying all the bucket contents over, and tar-ing up the full PV, creating a new PV on the new provider, un-tarring there, and then deploying pointed at that should work, right?. relevant slack convo https://pachyderm-users.slack.com/archives/C3UN0S0Q0/p1508526704000170\n\nBart Bajer 12:11 PM\nKind of a Naive question\netcd stores everything in the persistent volume.\nThere are two ways to deploy pachyderm using static-etcd and using dynamic-etcd.\nUsing --static-etcd-volume you can probably point etcd at the volume\nbut how would you do that with dynamic-etcd-nodes? I imagine directly through kubectl and not pachctl?\nGabriel Grant 1:04 PM\n@bbajer sorry, i'm honestly not 100% clear on exactly how that works (that's why that part of the issue i filed is a bit hand-wavey). @jdoliner are you able to answer that? (edited)\n1:05\n(or maybe @dwhitena?)\nJoe Doliner 1:08 PM\n@bbajer I don't think you can use --static-etcd-volume with --dynamic-etcd-nodes\nBart Bajer 1:08 PM\nI'm sure you can't\n1:10\nI'm asking how do you 'migrate/move' a pv from a pachyderm having been deployed via dynamic-etcd-nodes\n1:11\nhow do you attach that old pv to a new pachyderm deployment on a different k8s cluster (edited)\nJoe Doliner 1:13 PM\nso I think the easiest way would be to just use --static-etcd-volume <volume-name> does that work for your purposes or does it miss something?\nBart Bajer 1:16 PM\nI think that only works if all you k8s masters are in the same availability zone.\nJoe Doliner 1:20 PM\n@bbajer hmm, because the k8s master can't mount the volume if it's in the wrong zone?\nBart Bajer 1:20 PM\nI believe so\n1:21\nI might be mixing something up but I'm pretty sure there was some issue where we had to use dynamic etcd\nJoe Doliner 1:25 PM\nhmm, so that seems like a bit of a problem. Since the volume you need is stored in 1 specific AZ, the etcd pod is going to have to be deployed in that same AZ for this to work\nJoey Zwicker 1:26 PM\nI imagine AWS has some way to snapshot a volume and then load that into a new volume in an AZ, but I havent played with that before. yea, seems we should handle refresh, rather than just relying on a longer fixed time window (esp in light of #2436). closing in favor of #2438 (more context). Discussion from #2437 : \n\n@sean: Looking at the amazon client code ... looks like the default expiry is set to an hour in the future. We should up that.\n@homme: Is the expiry something that can be updated as a job progresses in order to make sure it never gets triggered no matter how long a job runs for?\n@gabrielgrant: yea, seems we should handle refresh, rather than just relying on a longer fixed time window (esp in light of #2436)\n@homme: If I update a job using update-pipeline should that refresh the expiry?  Just trying to think of a practical workaround in the short term.... @homme i'm not 100% sure, but i'd be very surprised if the token were tied to a pipeline rather than a job, so that should work\n@sjezewski can you confirm?. I'd be surprised if this error occurred due to a single request that took a long time to download -- my understanding is that generally expiry is checked at the beginning of a request, and the request is allowed to complete regardless of how long it takes.\nIn fact, in this case, this almost certainly has to be a request that is initially made (or at least received) after the expiry time: the HTTP error code is returned at the very beginning of the response, so if the token was valid when the request began, but expired while the data was in flight, there's no way to change to 403 response, because the 200 would have already been sent. The only option would be to just abruptly stop the data stream (and even that assumes amazon is actually checking for this in-flight expiry case, which, as i said above, i seriously doubt they are). Looking at the log timestamp, we can see that this is just barely timed out -- the log line was recorded ~766ms after the signature expires. So what is possible is that the request was sent just before the expiry time (22:17:03.99, say). If the client did a simple check at that point, the signature would still appear to be valid. But in that case it likely wouldn't be received by amazon until a moment after it expired, due to network latency (say 20ms later, at 22:17:04.019)\nIf this signature is getting generated immediately before sending, with a 1 hour window, though, it's not clear to my why/how this could be occuring. @homme great, thanks -- I'll talk to @sjezewski tomorrow and see if it needs more debugging, or if we can easily identify the fix by just looking at the code. Yes, as i mentioned, I agree those are both issues. I think the issue of branches causing directory structure to change is probably more surprising, but also far less likely to occur -- it can't right now; and even if it could in the future, it's probably pretty rare. So it seems reasonable to just say we'll only do this auto-merging for the same branch on the same repo. Should be pretty easy to enforce that, since the condition of two different branches of a repo both not having names is something that could be easily detected and just result in a failure at pipeline creation time (ie the same error check we already do today).\nAs for files overlapping. Yes, that's how it would work. Doesn't seem all that surprising, that if I ask for the same file to be loaded into the same location, that that's what I get... \nThe specific use-case that I'm targeting is wanting, say, 5 specific files from directory in a repo as input. This is particularly relevant/likely in the context of doing exploratory analysis or debugging in a service. For example @homme had a case last week where he had some datums (each was just a single input file) that were failing, while seemingly-similar datums were succeeding. It would be useful to load up a workspace with both the failure cases and a few examples of datums that work as expected in order to step through and compare them. It feels pretty weird to have to explicitly name each (and then end up with these parallel, mostly-empty directory structures), rather than just effectively, giving me a subset of files from my repo. Copy could possibly work in that case, though I think there are (more?) often cases where what you're really looking for is a copy pipeline, rather than a copy operation (ie if the file changes upstream, you want to see the change downstream). Having sugar around creating auto-updating copies of a subset of files (filtered copy pipelines) would be great (though it still seems kinda inefficient and cumbersome to have to copy an entire repo, when I could just reference the original files).\nThere are also still cases where I think it really only make sense to merge the repos' directory trees. For example:\nI have data and metadata (say a CSV file split into one-file-per-line, and another file for the header row). This should be straightforward to consume as a cross of a glob with the metadata file: repo/row-* X repo/header; currently I can't get these passed to my code with the same directory structure as they appear in the input repo. That seems strange to me. discussed this with @sjezewski and @jdoliner yesterday and confirmed this is something we want to do for 1.8. Docs is a good stop-gap measure for right now, but if docker on OS X automatically puts it's config there, then we be checking there in addition to the standard linux location. Almost certainly too optimistic...we are dealing with computers, after all\n\nSeriously, though, am I understanding correctly that that lib isn't finding the default config for docker on mac? if so, I think we should certainly either fix it upstream or work around it ourselves\n. @bpb no worries, just updated the title to hopefully make it more clear I'm only taking about what triggers the first run(s) of a pipeline. Updated to lower case (\"Joining\")\nI don't care much about the case, but I do think it's important to have the word \"join\" both in the title, and in the page itself, just because that's what people are going to be looking for. Even if the first line is \"Joining data in pachyderm is a bit different that doing a JOIN in a traditional, relational database\". to enable tracking metrics from the dash. Related to the third option #2958 . Interesting. Yea, I though that at some point this was working, but it's not clear how. I see some dashboard code that handles a synthetic queued state, but looking at it again, I'm not sure how I, as a client, should be distinguishing between actually-failed datums, and these incomplete datums. Should these be returned in the STARTING state? Should there be a different pending state? Or is there some other field i should be looking at to make the distinction? . This seems to be the one issue left to fix for us to be able to work on OpenShift using their default permission model (ie no privileged access). Using this as the meta-issue to track a number of more specific issues related to this\n\nsidecar tries to create /pach dir; fails without root access (#3404)\nworker fails to exec user code when run as non-root user (#3405)\n(unclear if this really needs to be fixed) pipeline workers fail to run without root, due to cert error when connecting to sidecar (#3394)\nNot sure if this is still an issue, but \"Non-root users cannot access /pfs\" (#1845) has been open for a couple years. Note that the OpenShift model (worker itself runs as non-root) may not be quite the same as what (I imagine) was originally being asked for here: having pach worker running as root still, but having user code running as non-root. I think the latter is already supported today, though (is that true @jdoliner ?). @msteffen We've shipped (and have people using) a version of Robot Users as of a while ago. Is this all implemented? What's left to do? Is this documented? (if not, possibly something to add to @Nick-Harvey 's todo list?) Or can this issue be closed?. Should we open a new issue about a users table? or are we no longer intending to implement that?. It seems we should maybe have a \"retry policy\" which is both a number of times to retry, and a delay to wait between attempts (if we're assuming a retried job may succeed, then that presumably means the cause of the error was something transient, which would take some non-zero amount of time to correct itself)\n\neg:\n\n. @dwhitena @sjezewski probably knows this better than i do, but my understanding is that's not actually an error, it just means it's using the default image because i haven't published a new (non-preview) one since 1.6.9 was cut. i agree it's confusing, though, so just submitted #2750 to clarify\nIt certainly shouldn't be related to the later failure at all. @brokenjacobs the helm chart is still not (yet) an officially/fully supported deployment method for Pachyderm, as it hasn't been incorporated into the test suite or build/release process. That being said, it would be great if you could file each of those three questions as it's own issue. Hopefully someone on the pachyderm core team or a community member can get around to answering/adding each of those things soon.. @marcadella most likely what you'll want to do is run pachctl deploy custom --dry-run [YOUR_OPTIONS] > manifest.json using options that are as close as possible to what you want for your deployment. This will output a Kubernetes manifest into manifest.json that you can then edit to suit your particular environment.. @iswaverly Sorry for the confusion, Pachyderm uses the Minio client to communicate with all non-Amazon S3-compatible object stores (including Ceph). So it sounds like you've got it set up correctly. ah, sweet\n. The issue is that every time we do a deploy, someone needs to manually update the chart. So far @jonandernovella has been fairly diligent in doing so (and it does appear to be up to date with v1.7.3 at the moment -- thanks @jonandernovella !), but we should really be able to automate this, to save him from having to do so. @ztj this is super useful, thanks!\nI'm not sure i follow why you think the error about no user with UID 1000 is coming from a jupyter requirement of the host context? In my case, I'm trying to run the pach pipeline on a locally-deployed pach/k8s, cluster, so I believe it should be using the same docker environment as when i execute docker directly (where the container jupyter works fine, and where i do have a UID 1000). This leads me to believe it's some pach-specific problem.\nDo you think, when re-launching the notebook, that you'd want it to have the data from the commit you were originally working on loaded in, or the latest commit's data?. @jdoliner you were working on some fixes related to this. Did those get merged in? (quick glance through the PRs didn't turn up anything obviously related, but I don't recall exactly what the root causes of the issues that you fixed were, so may have missed it). To summarize current status, there seem to be three issues:\n\nUser doesn't get set correctly (user 1000 not found)\nnon-root owners can't access /pfs\nincorrect port mapping\n\nThe first issue is addressed by #3085, the others can be worked around (by chowning the dir, and by manually setting jupyter to bind to port exposed port with  --NotebookApp.port=30888), but are not yet solved. agreed, that seems like i reasonable ask. I believe Minio egress is something we're going to need for the on-prem OpenShift deployment we're currently working on. Another option would be letting SubscribeCommit()'s repo and/or branch param either be optional or a glob pattern. Then an external process could either do processing, or perform whatever pipeline adjustments are needed based on commits to the spec repo. In this case, doing so for FlushCommit() would probably make sense too. I really like the idea of in-lining small files. We obviously want to keep these fairly small, but is there any reason that the size needs to be limited strictly to exactly 64 bytes (the size of the hash)? It seems this would be a worthwhile optimization for (somewhat) larger files too\nIs the second optimization the same as what i was talking about in #2873 ?. Does this issue represent anything close to the final state of where this design landed?. Would it make sense to have deletes fail on errors by default (as they do today), and to require -f to kill it with fire? Thinking this would likely give users (and us) a better chance of figuring out why commits got into a corrupted state in the first place. @williballenthin Thanks a lot for the detailed report!\nTo summarize: the number of past commits/jobs has no bearing on processing time of future jobs (other than to potentially reduce the amount of work required, if some of the current job's datums have already been processed), but job runtime does increase linearly with the number of datums in that job (which, in your case, are one-per file).\nAt the moment, the added overhead of an additional skip-able datum is fairly small (check if it exists in the object store), but non-zero. There are certainly ways to reduce this time (worker-side bloom filter checks being the obvious answer). There are also, however, potentially design changes that could be made to effectively eliminate it (by, say, storing and only checking a diff per commit, rather than scanning all datums' metadata), but that gets complicated when files in an upstream commit don't map one-to-one with downstream datums, and would require (much) more significant changes.. @williballenthin When you say \"none of the datums are skippable\", do you mean you're modifying every existing file in every commit, as well as adding new files? If that's the case (ie you need to process an ever-increasing number of files every commit), it sounds like you're fundamentally performing an operation that scales linearly (regardless of any Pachyderm overhead). It seems like I may be misunderstanding your actual use-case, though. Mind explaining a bit more about specifically what you're trying to achieve?\n(including the pipeline spec for your above example would also help to make sure we're on the same page about what's going on there). Was thinking the file would have the name of it's input. (ie /pfs/input-name would be a file instead of a dir)\nAnd yes, the map-reduce case is exactly what I'm talking about -- do some processing on each datum individually, and somewhere in your dag you want to, say, compute some summary stats across all the datums/files. My understanding is that the standard way to \"watch\" in (postgre)SQL is either using\nLISTEN/NOTIFY or triggers, but AFAIU you're right that some notion of leases on keys would probably require client-side changes (unless you're just talking about something like SELECT FOR UPDATE?)\nRe: graph DBs -- probably a bit early to use this, but it looks pretty interesting: https://github.com/indradb/indradb (full disclosure -- project is spearheaded by a good friend of mine). That looks cool, and it seems to support a first-class notion of relationships, which is really a lot of what I'd want from making a shift away from etcd. FWIW, this can be worked around in development by checking for the PPS_PIPELINE_NAME env var: if it's present, read from /pfs; if it's absent, read from wherever your development/testing data is stored. Still would be nice to resuscitate $PFS though. Also, when this is fixed, should revisit documenting it: https://github.com/pachyderm/pachyderm/issues/2403. To clarify: the proposal is to change the implementation so it will automatically use the $PACHD_PORT_650_TCP_ADDR env var as the address to connect to (presumably only if ADDRESS isn't explicitly specified), meaning pachctl would just work within k8s by default. How does this interact/is this no longer needed due to auto-port-forwarding (#3089)? (can kubectl talk to k8s by default from within k8s?). edit: nvm, misread. this sounds great. Do we support bash-style set notation in glob patterns today? eg *.{txt,png}\nWhat about bash's ! for exclusion?. Interesting, didn't realize the support was so extensive. Haven't tested, but if I'm reading this grammar correctly*, it seems that should be valid, no? I'm not totally sure it whether it would have the same behavior as the new exclude pattern field you're proposing, though.\n*: How I'm reading that: foo, bar and !buzz are all valid terms, each term is a pattern and multiple patterns can be composed into a pattern-list, which, wrapped in{} is again a valid term/pattern. In addition to the use-cases above, just being able to see the history of a branch would be quite useful when they're being used in advanced ways to trigger processing (as documented in https://docs.pachyderm.io/en/latest/cookbook/deferred_processing.html ). That's what I would expect, but I don't believe it does. I don't have this running anymore, but the I'm quite sure the pipeline was in RUNNING state.. FWIW, it seems you're looking at an out-dated version of the docs (1.7.2 vs current is 1.7.3)\nI updated the latest docs in #3003 but yes, as i noted last week (#3002), would definitely be good to have this properly automated\n. Hmmm...weird, I thought I'd pulled from GH in the past few days, but it looks like i'm on a version from about a month ago:\n$ pc version\nCOMPONENT           VERSION                                          \npachctl             1.7.0-46599b737d58408f57d91be11ca8ce25598e6c61   \npachd               1.7.0-46599b737d58408f57d91be11ca8ce25598e6c61. Hmm, didn't know Go's json.Decoder will take undelimited JSON. I should have been more explicit -- by \"most JSON parsers\", I was referring to (at least) those shipped with Python, JavaScript, and Ruby, none of which accept this:\n```\n$ python\nPython 2.7.6 (default, Oct 26 2016, 20:30:19) \n[GCC 4.8.4] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport json\njson.loads('{\"hello\": \"goodbye\"}')\n{u'hello': u'goodbye'}\njson.loads('{\"hello\": \"goodbye\"}{\"goodbye\": \"hello\"}')\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/lib/python2.7/json/init.py\", line 338, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/lib/python2.7/json/decoder.py\", line 369, in decode\n    raise ValueError(errmsg(\"Extra data\", s, end, len(s)))\nValueError: Extra data: line 1 column 21 - line 1 column 41 (char 20 - 40)\n\n$ node\nJSON.parse('{\"hello\": \"goodbye\"}')\n{ hello: 'goodbye' }\nJSON.parse('{\"hello\": \"goodbye\"}{\"goodbye\": \"hello\"}')\nSyntaxError: Unexpected token { in JSON at position 20\n```\n\n\n\nruby 2.5.0p0 (2017-12-25 revision 61468) [x86_64-linux]\n   require 'json'\n=> false\n   JSON.parse('{\"hello\": \"goodbye\"}')\n=> {\"hello\"=>\"goodbye\"}\n   JSON.parse('{\"hello\": \"goodbye\"}{\"goodbye\": \"hello\"}')\n765: unexpected token at '{\"goodbye\": \"hello\"}'\n(repl):1:in `<main>'\n. ok, yea, @brycemcanally thinks the invalid character message is coming from his new validation code. I'll try to track that down with him/file another issue. filed as #2953. kinda related: https://github.com/pachyderm/pachyderm/issues/2892. the file it's trying to read appears to be there: \n$ pc list-file test stats 55e0ee664569f966aac859402879bafe23dd1bba2d1a1bffc2115a49e732e40c\nNAME                                                                                                   TYPE SIZE     \n/55e0ee664569f966aac859402879bafe23dd1bba2d1a1bffc2115a49e732e40c/index                                file 1B       \n/55e0ee664569f966aac859402879bafe23dd1bba2d1a1bffc2115a49e732e40c/job:cea66104a30644d2a7503e10d4edcb54 file 0B       \n/55e0ee664569f966aac859402879bafe23dd1bba2d1a1bffc2115a49e732e40c/logs                                 file 2.006KiB \n/55e0ee664569f966aac859402879bafe23dd1bba2d1a1bffc2115a49e732e40c/pfs                                  dir  12B      \n/55e0ee664569f966aac859402879bafe23dd1bba2d1a1bffc2115a49e732e40c/stats                                file 126B\nbut reading the content fails:\n$ pc get-file test stats 55e0ee664569f966aac859402879bafe23dd1bba2d1a1bffc2115a49e732e40c/job:cea66104a30644d2a7503e10d4edcb54\nno file(s) found that match 55e0ee664569f966aac859402879bafe23dd1bba2d1a1bffc2115a49e732e40c/job:cea66104a30644d2a7503e10d4edcb54\n(note ids are different from the original test case because i re-ran the repro steps). There are some more use-cases in #2242 (in general, these pipelines seem like the most likely candidates for sharing between users). @adelelopez do you think your implementation in #3531 is sufficient to call this issue resolved? if not, what's left to be done?. Hey @suneeta-mall thanks for following up, and sorry for taking a while to get back to this. Not sure if this failure with the enterprise key is something transient, or something that actually needs to be fixed. I believe the garbage collection failure, though, is known to be a bit flaky. I've just restarted both the CI builds. Ah, sorry, looks like that failure to pull the enterprise secret for testing requires me to copy this PR over to \"approve\" it. Recreated the PR here: #2994. I dunno...it's a pretty complex change ;)\n. @vasilak sorry for the delay. Yes, it sounds like you should be able to achieve what you're looking for with flush commit -- I believe it waits for all downstream commits by default. Also just tried with this slightly modified pipeline, to make sure it wasn't just something with my (mis)understanding of **-style glob patterns, but doesn't seem to make a difference (other than there being 6, instead of 7, datums):\npachctl create-repo input\necho '{\n  \"pipeline\": {\n    \"name\": \"test\"\n  },\n  \"transform\": {\n    \"cmd\": [ \"bash\", \"-c\", \"cp -r $(ls -d /pfs/*|grep -v /pfs/out) /pfs/out\" ]\n  },\n  \"input\": {\n    \"atom\": {\n      \"repo\": \"input\",\n      \"branch\": \"master\",\n      \"glob\": \"splitted/*\"\n    }\n  },\n  \"enable_stats\": true\n}' | pachctl create-pipeline. I certainly agree that getting that full list once seem like it should be (significantly) faster than checking each of those 12 billion files one-by-one in the object store. When i brought this up with @msteffen what i was trying to get at was how it might be possible to make job overhead scale relative to the number of files changed in a commit, rather than the total number contained. it seems like the difference you're talking about isn't so much 1-vs-2 trees, but the cost of downloading/processing of a potential \"diff\" tree (containing the often-relatively-small number of files that have been added) vs the tree(s) containing the full list of 12-billion (or whatever) files. I'd imagine that computing the diff to find that only one of the 12 billion files changed must still have pretty significant overhead, no?\nFrom what Matt has said to me/written here, it sounds like the tricky part seems to be modifying a parent tree to remove the old datums' output before adding in the new datums' output to generate the new child tree. Am I understanding correctly that this is basically because we don't have a mapping from (chunks of) output files back to the datums that created them?. Talked more with @msteffen about how removing a datum's output tree from a merged commit tree could work. At the moment we store each datum's output tree, so from that we can know which files in the commit tree will need objects removed. In most cases we can just remove the datum's contributed object, but in some cases a file may have the same object multiple times (if multiple datums output the same data to the same file). In these cases, we don't know which instance of the object came from which output tree. A few options to deal with this:\n\nstore the originating datum's ID along with the object pointer in the file\nconsistently order the objects in a file (based on, say, a lexicographic ordering of originating datums' IDs)\njust remove an arbitrary instance of the object\n\nMy inclination is that 3. is valid, since we don't guarantee object order in the first place. @msteffen expressed concern that, although the absolute ordering of objects in a file isn't guaranteed, with our current implementation the ordering of objects across different files will be \"consistent\": ie. if an object from datum A comes before an object from datum B in one file, we know it's objects will come before those of datum B in all files. Removing objects in an arbitrary order breaks that \"consistency\"\nEg imagine a tree with two files, each containing objects from three datums, where the contributions of datums 1 and 3 to one of those files were the same (the red lines in the second file):\n\nIf we wanted to remove datum 3's output from the commit tree, and did so by just arbitrarily removing the first instance of it's object in the second file, then the order of objects in the two files will have effectively \"reversed\", which is something that shouldn't ever happen in pachyderm today:\n\nDo we consider that \"consistent ordering\" to be \"defined behavior\"? Or just a consequence of our current implementation that people shouldn't be depending on? (to me this doesn't seem like something worth worrying about, since TBH i have trouble coming up with why people would be depending on that, but that could just be for lack of imagination). I think the details I'm talking about are relevant to cases of deletes and overwrites but also appending to files, right? That seems relatively less rare than the other two (though yes, probably still less common than simply adding new files in each commit)\n@msteffen was concerned about the added overhead of storing an extra datum ID with every object reference (perhaps especially so if this info is getting in-lined into the commit tree, which IIUC is happening with the current optimization pass?), but a nice side benefit of storing that info is that it makes it easy to get the actual processing logs for a given output file (#1914 ). This seems to be failing for silly reasons, but also apparently wasn't really needed in the first place, so closing. Relevant conversation: https://pachyderm-users.slack.com/archives/C3UN0S0Q0/p1528926671000463\n\nscreenshotted\n\n![image](https://user-images.githubusercontent.com/493968/41389207-4999393c-6f44-11e8-90f6-8d5a36a6d879.png)\n  \n\n\nsearchable version\nDrayton Munster [2:51 PM]\nAre there any examples/suggestions on building an optimization/analysis loop with Pachyderm?\n\nGabriel Grant [2:53 PM]\n@dwmunster pachyderm can certainly be useful for that sort of thing. mind explaining a bit more specifically about your use-case?\nare you talking specifically about implementing an iterative optimization algorithm? or just about performing optimization of, say, an NN's hyperparameters? (edited)\n\nDrayton Munster [2:55 PM]\nThe specific example I had in mind was using Pachyderm to handle the parallelization and execution for MCMC sampling.\nMy current thought was to have the \u201canalysis\u201d pipeline take, e.g., a UUID and the MCMC code. Each iteration would then be pushed to an \u201cexecution\u201d pipeline with branches keyed by ID and iteration number.\n\nDrayton Munster [3:09 PM]\nuploaded this image: example.png \n\nDrayton Munster [3:11 PM]\n[Apparently you can\u2019t edit comments on uploaded images, so reproduced here in comment form.]\nSpecifically, I\u2019d be interested in using Pachyderm to manage the execution/provenance of the colored block. Each P_i may have many hundreds/thousands of sample points, each of which can be evaluated independently.\n\nGabriel Grant [3:20 PM]\nok, full disclosure: i have fairly rudimentary understanding of MCMC (i've only used it a little in a non-parallelized context -- my understanding is that in the naive case it is an iterative algo where determining sample point n+1 depends on evaluation of sample point n), but if the samples can be independently computed, that sounds like a good candidate for parallelization (edited)\n\nDrayton Munster [3:22 PM]\nChoosing the next batch of sample points can be complicated, but that general idea (iteration batch n+1 depends on iteration batch n) is the important bit\nAnd is the same for MCMC or iterative optimization algorithms\n\nGabriel Grant [3:23 PM]\nbut each iteration of P_i is evaluating a whole set of independent points, right?\n\nDrayton Munster [3:24 PM]\nYes\n\nGabriel Grant [3:27 PM]\nok, so that \"pure map\" type function is certainly well within pachyderm's wheelhouse. the \"outer loop\" iteration of evaluating whether your set of evaluated points is sufficient to reach a termination condition, or if it should trigger another iteration, is a bit more tricky, but certainly doable (i'd probably just have an evaluation pipeline that conditionally creates a new commit to an upstream ingress repo if another iteration is needed) (edited)\n\nDrayton Munster [3:28 PM]\nThat's the idea I was trying to describe earlier, wanted to make sure I wasn't missing a better way.\n\nGabriel Grant [3:31 PM]\nah, ok -- sorry, just needed a bit more context to be able to follow what you were asking. i'm still not sure i totally follow what you mean about having branches keyed by ID and iteration number, though\nmy thinking was that each iteration would basically be triggered by a commit on an input repo of a list of points at which to sample\neach of those sample points would become a datum, which could be executed in parallel\nthen you'd presumably need some sort of a reducer to determine whether to iterate again\n\nDrayton Munster [3:34 PM]\nThe ID and iteration branches would just be a slightly more readable way to track the execution.\n\nGabriel Grant [3:34 PM]\n(and something to generate the next set of points to evaluate)\nok, yea, what i'm describing is certainly a pretty naive way of doing this\ni'm not sure i totally follow how the ID and iteration branches would work exactly, though. mind spelling that out a bit more for me?\n\nDrayton Munster [3:38 PM]\nThe final result may depend on the entire history (e.g. all the P_i and y_i). The way you described requires keeping a list of iterations to commit ids.\n\nGabriel Grant [3:38 PM]\nah, i see, so at the end you'd have to reduce over all the commits to get a final result?\n\nDrayton Munster [3:38 PM]\nYes\n\nGabriel Grant [3:38 PM]\ngotcha\n\nDrayton Munster [3:39 PM]\nSo ID plus iteration is just a human friendly way to track in case manual inspection is necessary\nFor an optimization example, consider making a convergence plot\n\nGabriel Grant [3:40 PM]\ni wonder whether it would make sense to just keep all the history in the head commit, then, and just add a new directory for each iteration's results?\n(meaning each commit would just append another directory, and your final evaluation would see all results in a single commit to reduce over)\ndo you have any idea roughly how many iterations we're talking about?\n\nDrayton Munster [3:44 PM]\nOptimization would like be small (<100), but MCMC could be many thousands of loop iterations\n\nGabriel Grant [3:45 PM]\nsorry, just to make sure i'm clear: you're saying the number of outer loop iterations would be <100, but the number of samples evaluated within each outer loop run (ie within each P_i step) would be many thousands?\n\nDrayton Munster [3:46 PM]\nBoth the number of independent points and the number of outer iterations could be large\n\nGabriel Grant [3:47 PM]\nah, ok\nand what does this reduce/looper step that determines whether to run another outer iteration (and computes which points need to be evaluated?) look like?\n\nDrayton Munster [3:49 PM]\nFor example: http://dfm.io/emcee/current/\n\nGabriel Grant [3:49 PM]\ndoes it need to see all the historical data every time it runs?\n\nDrayton Munster [3:49 PM]\nThe basic usage example has 100 independent points with 1000 outer iterations\nDetermining if another iteration is necessary could, in principle, require the full history\n\nGabriel Grant [3:52 PM]\nok, thanks, that's useful to have something concrete to look at (sorry again for the elementary MCMC questions)\n\nDrayton Munster [3:53 PM]\nBut in these examples, they just need the most recent and the only reduction at the end needs the full history\nFor example, we might want the most likely point (maximum) out of the 100 x 1000 evaluations\n\nGabriel Grant [3:56 PM]\nah, i see, looks like this multiple \"walker\" concept is what i was missing from my understanding of MCMC: http://dfm.io/emcee/current/user/faq/#what-are-walkers\ngotcha\n\nDrayton Munster [3:56 PM]\nI don't mind doing the iteration calculation and reduction in a single container, but the parallel walker eval is what I'm interested in\n\nGabriel Grant [3:57 PM]\nso, at some point you're going to need to see everything\n\nDrayton Munster [3:57 PM]\nNot all MCMC algorithms have the multiple walkers, but that one does. For an optimization example, consider evolutionary algorithms or simulated annealing\n\nGabriel Grant [4:06 PM]\nok, well yes, parallelizing over walkers sounds like it should certainly be doable. if your reduce step is going to run over all the results, my inclination would be to model this as an append-only workload where each datum is a point at which to evaluate, and subsequent iterations just get added as additional directories (mostly for the sake of making it easier to consume the results together). but having one branch per iteration would basically be equivalent (a bit more work to set up, but would save some overhead of having a ton of mostly-unused files in the commit tree) (edited)\n\nDrayton Munster [4:33 PM]\nHow expensive is that overhead for the unused files? Keeping the iterations together in different folders is tempting.\n\nGabriel Grant [4:41 PM]\nbasically the main cost is that there's some extra metadata transferred to workers when doing processing, but that should only be paid once per job (at most; i believe it should generally get cached). there also, historically, has been an upper bound on the total number of files in a commit because of the requirement that said metadata be loaded into memory when finalizing a commit. but there's work going on at this very moment that should be landing soon to remove that limitation: https://github.com/pachyderm/pachyderm/pull/2984 :slightly_smiling_face: (edited)\nGitHub\nBolt hashtree by jdoliner \u00b7 Pull Request #2984 \u00b7 pachyderm/pachyderm\n\nDrayton Munster [4:43 PM]\nNice, I\u2019ll see if I can put together a (fairly) minimal example. Thanks for the help\n\nGabriel Grant [7:33 PM]\n@dwmunster should have mentioned: there'd also be a slight cost to recognize that previous iterations don't need to be re-evaluated, but i believe the pattern we're describing could be implemented in such a way that that cost would be relative to the number of iterations (1000s), not the total number of evaluations (100s of 1000s), so shouldn't be too significant (it's basically one network request per check)\nwe do also have a proposal to remove that overhead completely: https://github.com/pachyderm/pachyderm/issues/2993\nGitHub\nPropagate list of changed files through PPS DAG \u00b7 Issue #2993 \u00b7 pachyderm/pachyderm\nBackground Right now, PPS jobs avoid the cost of reprocessing datums by this mechanism: Each worker claims some subset of the job's datums The worker hashes each datum and checks if the hash i...\nre: GAs -- there's a bit of a difference in that you're generally only concerned with the surviving population at a given iteration, so the working dataset doesn't continue to grow the way it sounds like your MCMC one will, but it is the same general iteration pattern, yea\nglad i could help confirm that you were basically on the right track. look forward to seeing what you come up with, and let us know if there's anything else we can do to help. wrote up a summary/proposal to make what we're talking about easier here: https://github.com/pachyderm/pachyderm/issues/2999\nGitHub\nBetter support/documentation for implementing iterative algorithms \u00b7 Issue #2999 \u00b7 pachyderm/pachyderm\n  \n\n. The issues are:\n1. the incrementality here spans multiple pipeline stages\n2. the triggering of subsequent iteration is caused by the output of a previous run, rather than by some externally-initiated commit\nneither of these are well-handled by our current notion of incremental processing. To expand on what I mentioned in standup today: one of the cases where this pattern is perhaps most relevant our core use-case is for hyper-parameter tuning of ML models. Right now we do a good job of supporting manual hyper-parameter tuning (where hyper-parameters live in an ingress repo and are manually updated) and dumb grid-search sweeps (where the various hyper-parameter options to test are predefined, they can just be put in an input repo and each become a datum that triggers a training run) but any more intelligent/automated hyper-parameter tuning requires the type of feedback loop that this issue discusses\nEspecially for something like random search, where (as the name suggests) the detailed decisions/performance of the final model will depend on the specific random numbers chosen at each iteration of the tuning, having provenance tracking seems like it could be particularly important.. Yea, I actually don't think the workaround I proposed is that bad, if implemented with the output of the last \"logical\" pipeline stage basically being (instructions for?) what gets inserted back at the top of the DAG, then having an \"egress pipeline\" that just does the CopyFile operation. (Note this pachyderm-internal \"copy\" will be a quick operation just shuffling some metadata, and doesn't actually result in any data duplication -- it's more like creating a hardlink)\nWhile some type of lazily computed notion of infinite provenance could probably be baked up to solve this, the type of first-class support I was thinking could most easily exist would basically be (logically) equivalent to dynamically extending the DAG downwards after each iteration, but without actually creating new k8s resources for the downstream pipelines (to avoid the excessive computation resource cost). My understanding is that basically would work today within our model, just wouldn't be recommended because:\n\nit's prohibitively expensive to run many instances of the same pipeline(s) (and maybe not even, with aggressive spin-down)\nit would be pretty ugly to look at all those extra DAG elements\nwe don't often see thousands of DAG elements, so we probably have some other unknown scaling constraints\n\nRe: 1 -- It seems like not recreating the duplicate downstream pipelines is a conceivable optimization to make (and one we've discussed in the context of pipeline/DAG branching (#1036) -- I think this would basically entail noticing that multiple pipelines use the same docker image and reusing the workers (see #3008)\nRe: 2 -- allowing those workers to be reused, and making it so (potentially large numbers) of related DAG elements don't pollute your view of the system is what we're after with the DAG organization work\nRe: 3 -- removing these limitations may not be a high priority, but doing so certainly wouldn't be fundamentally outside our model\nThe fundamentally new piece that would be needed to have first-class support would be some notion of branching: basically a way for a pipeline to tell pach that it's output should trigger the (logical) creation of a new \"iteration\" of a section of the DAG that consumes it's output. But that logic could also easily just be wrapped up in a \"branching condition pipeline\" that made the DAG modifications\nThat being said, there would obviously be a few benefits to having it baked in:\n\nthis could likely be stored more efficiently as some sort of \"instance count\", rather than actually duplicating all the elements in etcd, which would start to matter at some point\nhaving a notion of updating all the copied pipelines together\n\nBut those seem like they could pretty easily be worked around today, it likely doesn't make sense to bake it in that unless we see use of this workaround cropping up a lot and start having people run into these issues. @msteffen i think this is permanently fixed by #3014, right? (as well as the temp fix in #3003). Thanks for the report! Unfortunately/somewhat surprisingly I don't see an error from the original pipeline creation in the pachd logs. If you happen to still have the pachd logs available, could you please attach the full (probably long) log file here?. That's possible, but it certainly looks like the initial create-pipeline request succeeded from the console history you pasted at the top. Generally if a request from pachctl doesn't make it to pachd, it will either error immediately or hang for a bit and then return a timeout (usually something like \"Context Deadline Exceeded\")\nOne thing you could grep for would be pps.API.CreatePipeline -- if we think something went wrong with an earlier call to that particular endpoint, this would allow us to start piecing together what happened (I see two calls to it in the log you've provided, but the first one has an error saying the pipeline already exists, so presumably there are others that aren't shown)\n@jdoliner may have ideas for what else might be useful, but TBH, without being able to repro, I imagine this is gonna be pretty tough to track down. Have you run into this again since you first encountered it last week?. Oh, the other thing that I should have asked for before was the info/logs for the pipeline pod\nSpecifically output of:\n- kubectl get all\n- kubectl describe <pipeline-pod (from previous get all command)>\n- kubectl logs <pipeline-pod> user\n- kubectl logs <pipeline-pod> storage\nIf you've been using the cluster much since you saw the failure, it's quite possible these logs will have been deleted already, but you may be able to get them using the --previous flag to the kubectl logs command\n. Ah, sorry, looking back at slack logs, it seems you already did all that :P \nFor posterity:\n```\n$ kubectl get all\nNAME                          READY     STATUS    RESTARTS   AGE\npod/dash-5d974d8668-6k5mt     2/2       Running   6          1h\npod/etcd-66858555cd-mlsbj     1/1       Running   0          1h\npod/pachd-7b564794bc-pgdrl    1/1       Running   0          1h\npod/pipeline-edges-v1-lljn5   2/2       Running   0          57m\nNAME                                      DESIRED   CURRENT   READY     AGE\nreplicationcontroller/pipeline-edges-v1   1         1         1         57m\nNAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                   AGE\nservice/dash                NodePort    10.106.3.76              8080:30080/TCP,8081:30081/TCP                             1h\nservice/etcd                NodePort    10.106.5.52              2379:32379/TCP                                            1h\nservice/kubernetes          ClusterIP   10.96.0.1                443/TCP                                                   1h\nservice/pachd               NodePort    10.99.89.196             650:30650/TCP,651:30651/TCP,652:30652/TCP,999:30999/TCP   1h\nservice/pipeline-edges-v1   ClusterIP   10.110.141.223           80/TCP,9090/TCP                                           57m\nNAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/dash    1         1         1            1           1h\ndeployment.apps/etcd    1         1         1            1           1h\ndeployment.apps/pachd   1         1         1            1           1h\nNAME                               DESIRED   CURRENT   READY     AGE\nreplicaset.apps/dash-5d974d8668    1         1         1         1h\nreplicaset.apps/etcd-66858555cd    1         1         1         1h\n```\nnote that montage deployment/rc/pods are missing, despite being present in pach:\n$ pachctl list-pipeline\nNAME    INPUT                OUTPUT         CREATED           STATE            \nmontage (edges:/ \u2a2f images:/) montage/master 19 minutes ago    failure \nedges   images:/*            edges/master   About an hour ago running\n```\n$ pachctl inspect-pipeline montage\nName: montage\nCreated: 21 minutes ago\nState: failure\nReason: rpc error: code = Canceled desc = context canceled\nParallelism Spec: \nResourceRequests:\n    CPU: 0\n    Memory: 64M\nDatum Timeout: (duration: nil Duration)\nJob Timeout: (duration: nil Duration)\nInput:\n{\n \"cross\": [\n   {\n     \"atom\": {\n       \"name\": \"edges\",\n       \"repo\": \"edges\",\n       \"branch\": \"master\",\n       \"glob\": \"/\"\n     }\n   },\n   {\n     \"atom\": {\n       \"name\": \"images\",\n       \"repo\": \"images\",\n       \"branch\": \"master\",\n       \"glob\": \"/\"\n     }\n   }\n ]\n}\nOutput Branch: master\nTransform:\n{\n \"image\": \"v4tech/imagemagick\",\n \"cmd\": [\n   \"sh\"\n ],\n \"stdin\": [\n   \"montage -shadow -background SkyBlue -geometry 300x300+2+2 $(find /pfs -type f | sort) /pfs/out/montage.png\"\n ]\n}\nJob Counts:\nstarting: 0    running: 0    failure: 0    success: 0\n```\n$ pachctl create-pipeline -f https://raw.githubusercontent.com/pachyderm/pachyderm/master/doc/examples/opencv/montage.json\nrpc error: code = Unknown desc = pipeline montage already exists\n. @thedrow thanks! Seems like these should be getting run as part of CI -- @sjezewski wanna take a crack at making that happen?. cool, i think this line with the link needs to be changed too, right? https://github.com/pachyderm/pachyderm/blame/b5958b22a4f1d0a953760b3d8f0942fa5abe828c/doc/examples/readme.md#L19\nI'm assuming other links (like ToC) are generated automatically?. Thanks for the report! Haven't seen this particular error before, but your minio + helm setup is a lot less common than using pachctl to deploy directly to one of the cloud providers. We unfortunately haven't yet had a chance to adopt the helm chart as a first-class deployment path yet, so we don't have all the tooling set up to avoid these errors and make this easy to repro/debug (#3021). \n@jonandernovella has been the one most actively maintaining the helm chart -- is this an issue that you've run into?\nAlternatively, if it's not a helm issue, it may well be related to minio, which also doesn't seem to see as much use as the cloud-vendor-provided object store backends (though that we do have some testing for, so I'd imagine it'd be less likely to fail in this way). Any chance you have access to a cloud-provider object store you could test on to see if we can localize the problem to one of these two components?. Awesome, thanks a lot!. Thanks for the report -- that's quite strange. I was under the impression that the pipeline's transform field was snapshotted when a job is first created. @jdoliner Maybe this changed somehow with the addition of the __spec__ repo to track pipeline versions? \n@jdelfino To clarify: in the repro steps you mention that you're using digests instead of tags. Did you specifically try with tags and the issue didn't appear, or just didn't have the chance to try that?. IIRC this applies to leading underscores too, right?\nRelated PR documenting the requirements: #3006 and issue proposing to relax the req: #1497. My understanding is that we also support setting the address via ~/pachyderm/config.json -- should that maybe be included in these docs?\nRelated: #2138 and #2606. Also, IIUC this does open up the HTTP endpoint on the pachd service too, but it will be on a different hostname than the dash, and so won't actually work for logging in/downloading files (both because the dash has no notion of which address to use, and because of cross-domain cookie policy), right?. Split out the config file docs as #3036 \nshould we open another issue for the dash auth/HTTP download link problem, or do you want to do a bit more investigation to see if it can be supported within the structure of what you're proposing here?. Yes, just confirmed. With it not installed:\n$ pachctl <TAB><TAB>\nhas no effect. after installing:\n$ sudo env \"PATH=$PATH\" pachctl completion\nand opening a new terminal:\n$ pachctl g <TAB><TAB>\ngarbage-collect  get-logs         get-tag          \nget-file         get-object       glob-file\nseems to work as expected. I think the need for setting address in the pach config file is somewhat diminished if we can implement autodiscovery of the address in a wider variety of contexts: #3089 . Perhaps even better still would be to have this actual config info stored and versioned in Pach, so the provenance tracking meaningfully captures the whole state that is required to recreate the results. Thanks for the report, @jdelfino -- you're right that this does sound like a race. Could this be related to things getting out of sync following multiple pipeline updates that you filed in #3045 ? Wondering if each --reprocess is maybe re-adding a cron trigger?. If others are running into this problem: I believe the workaround that @jdelfino and @jdoliner came up with is restarting pachd, by deleting the running pachd pod (kubernetes will automatically schedule a new one)\nThat being said, looks like the better \"workabound\" is gonna be this PR fixing the issue #3054 ;) . I've also run into this a few times with the openshift stuff -- pach will list a pipeline as running, despite the storage container having failed to come up. Fairly annoying.. @mdaniel not sure i'm following where that error message is coming from? (though agree that sounds like it should be a failure, not starting). FWIW, while i certainly agree that this is a annoying error (and so filed the specific issue as #3412), if I'm understanding correctly, the broader problem is a little different than the first example I had -- specifically, this seems to be a case of us detecting that something went wrong in the creation of the underlying K8S resources for the pipeline (hence populating the reason field), but for some reason not transitioning into a failed state\nIf you have a minute, can you please share the output of kubectl get all and kubectl describe po/<bug3059-pod-name>?\nThanks!. This is awesome, thanks so much for sharing. Can you please sign our CLA?\nThanks again!. This seems like it might be a path towards solving (part of?) what @JoeyZwicker is asking for (or is at least related/would be needed to make this work): https://github.com/pachyderm/pachyderm/issues/228#issuecomment-406737480. Another config request: allowing export as the same PG dump format that we take for input, so that multiple postgres steps can be easily chained (corollary: allowing input format to be specified, so we can load a DB from CSV files). I do think dumping as CSV is rather important: making use of pg_dump files in any downstream pipelines that aren't pg-based is a big hassle. This impedance mismastch is mitigated if we also allow input format to be specified, so we can load a DB from CSV files too. The one kinda tricky issue is that, although widely used, CSV is a pretty crappy format, with a ton of different subtly-different implementations, none of which have nearly the level of typing support that a SQL DB does (technically every value in CSV is a string, so most implementations have different coercion logic). JSON isn't a ton better, but we may also want to support an object-per-row stream of JSON objects:\n{\"id\":1,\"column_1\":43,\"column_2\":\"asfasfasfd\"}\n{\"id\":2,\"column_1\":43,\"column_2\":\"asfasfasfd\"}\n...\n(this is what is output by pg's ROW_TO_JSON, and already supported with our put-file --split json). More generally, this seems like a pretty powerful pattern (would be super useful for the spark stuff I've been working on). Could we generalize this into a \"pipeline-data adapter API\", so the transform could take:\n\na sidecar image\nan input-data PFS-to-adapted-pipeline image\nan output-data adapted-pipeline-to-PFS image\n\nso that we have something to point people at, rather than getting ourselves into the black hole of having to build and bake in every possible adapter ourselves?\n(presumably we would want to build some of these, like postgres, ourselves with more official support/sugar, but having this seems like a good extension point around which to hopefully increase community contributions, and is an a good way to start building up towards #2242). Another consideration for the pg-specific case is that, for a multi-stage SQL pipeline, dumping and loading at every stage can be incredibly time consuming (especially if rebuilding secondary indices). This may be outside the scope of what we should do for a first pass at this feature, but I started documenting some other possible patterns for how integration with postgres could work here: https://github.com/gabrielgrant/pach-postgres-demo  (warning: this is pretty old; demo code is incomplete/almost-certainly broken)\nMy wishlist design for versioning a series of SQL transforms would probably be for pachyderm to coordinate something like Option 3.b\nFor full-production-db-dumps/non-incrementally-ingressed workloads:\n\nwhen a new pg_dump input commit arrives, spin up a new PG node with a read replica for each child pipeline\ningress the data into that DB\nfor each child pipeline:\nadd a read replica for each of it's children\nrun the pipeline's SQL\nrepeat\n\nwe could just permanently store the data at each stage in it's DB, but to do so we'd probably end up having to manage a ton of persistent volumes* (annoying and likely expensive), so we could also egress the data to PFS for long-term storage and spin down/reuse the DB node. This could be done as:\n\npg_dump text files\nthe raw PGDATA dir\na changelog as described in option 4.\nafaik WAL-E is kinda the tool for continuous archiving of PG nodes' WALs, so we probably wanna just use it? \nas I mentioned in option 4., afaiu pglogical seems to have a (potentially far) more efficient transfer/storage format than the raw WAL, so we may also want to look at that\nkinda related: uber recently open-sourced a potentially-useful tool for doing something similar: https://github.com/uber/queryparser \n\nFor incrementally-ingressed workloads, do basically the same thing, except reuse the input DB\nAlso, for bonus points, we could also probably come up with a more efficient way of ingressing data (direct, over-the-wire pg_dump, maintaining a streaming read replica of prod, etc). How we do this probably interacts with what we want to do about spout pipelines\n: unless we're able to store all the DB's backing files in directories on a single shared filesystem, like Google's new Cloud Filestore. Higher level question: in cases where people really want to distribute big SQL queries, wondering if we should maybe consider just recommending/integrating something like Citus (a PG extension -- with similar open-source+enterprise model as pach -- that handles sharding & parallelizing across multiple PG DB nodes) or possibly CockroachDB, rather than a kinda hybrid, pach+SQL approach? Integration would be:\n\nThe ingress from/egress to other pach repos/pipelines\nAdding versioning\n\nBut would mean people could (in theory) still run their legacy SQL query pipeline steps essentially unmodified, saving a bunch of engineering effort pach-ifying their joins, and probably end up having (substantially?) better performance. Seems like that combination could be a good way to each play to our strengths, and potentially be a pretty fruitful collaboration\n*:  Downsides of using CockroachDB:\n1. we lose the whole (substantial) PG tooling ecosystem to help with our integration\n2. seems they don't yet support streaming replication, so AFAIU we couldn't (easily) implement my replication-based pach-pipeline-data-storage/transfer proposal (above)\n3. I don't believe it's currently possible to stream from a PG node into Cockroach, so it wouldn't be possible to easily do my streaming ingress proposal (also above)\n. This is a request I've had in the dash too -- this issue sounds strongly related to (if not the same as?) #2468. Yea, I think you're probably right that a) the extra dir level isn't that big a deal and b) having /pfs/out be a single file is a better design than throwing away the file names. This came up again in conversation last week, and we just had this feature re-requested by one of our big enterprise users. From our convo, sounded like this shouldn't be too big a deal to implement. From the design side, i think what we'd briefly discussed was adding split_output field on pipelines that would contain an object describing how to apply the split. (might also makes sense to re-use this same SplitSpec proto for put-file?)\nThoughts on assigning some resources to make this happen for 1.9?. FWIW same thing seems to happen when a pod is pending (still gets listed as running in Pachyderm)\nIs starting state just not being used at all any more?. The first time i ran into this was with a service, but the one i just tested with was a normal pipeline. So both, I guess?\n$ pachctl list-pipeline\nNAME      INPUT                     OUTPUT           CREATED           STATE            \ndump_env  dump_env_input:/          dump_env/master  About an hour ago running \n$ kubectl get all\nNAME                                 READY     STATUS             RESTARTS   AGE\npod/dash-5d974d8668-rtqt9            2/2       Running            0          8h\npod/etcd-66858555cd-75v9s            1/1       Running            0          2d\npod/pachd-586cd85d4b-hcs98           1/1       Running            1          2d\npod/pipeline-dump-env-v1-s68lh       0/2       Pending            0          54m\n...\n. It's not 100% broken, though -- if a pod is in ErrImagePull status, it does properly get marked as failure in pachyderm. Which makes it a bit tricky to come up with concrete repro steps. Currently I've triggered it by overfilling my cluster, so pods are failing to schedule due to insufficient memory. It looks like watch supports a --color flag that will accept and use standard colorization codes. Wondering if there's there a way to have pachctl detect use of that flag and pass codes through? Guessing this is not straightforward (it probably just appears to be a non-interactive pipe either way), but also seems like the type of thing someone else must have tried to figure out...\nOne workaround would be to just have our own --color flag that causes output to retain color codes regardless of whether we think the output stream is interactive or not  (eg watch --color ls --color). @suneeta-mall thanks for the thorough explanation -- it does sound like this should work for a fair number of cloud-based clusters :+1: . I think this is mostly fixed by auto-port forwarding, but as @mdaniel put it, we should:\n\neither reserve an IANA port for pach, default to :650, or default to :30650, because this behavior is annoying\n```\n$ time env ADDRESS=10.250.1.242 pachctl list-pipeline\nerror connecting to pachd: could not connect (note: address \"10.250.1.242\" does not seem to be host:port): context deadline exceeded\n\nreal    0m30.042s\nuser    0m0.029s\nsys    0m0.027s\n``\n(running it with-vvvvshows theErr :connection error: desc = \"transport: Error while dialing dial tcp: address 10.250.1.242: missing port in address\". Reconnecting...` happens right away, and then it apparently just continues hammering until some 30 second expiry at which point the underlying error is finally presented to the user)\nIs there any good reason to not try those as defaults?. Sounds good, opened as #3417 . Are you meaning to include the whole 1400ish line output in here? Given you have the small sample section of the output in the README, it doesn't really seem necessary to include the whole thing (it's not that huge -- about 42kB -- so not a big deal if you do want to include it, but just wanted to make sure it was intentional, since I don't actually see it referenced anywhere). Not that this fully addresses your request, but you should be able to use pachctl extract-pipeline [pipeline-name] to get a pipeline's spec out. Note, however, that although this will be logically identical to what was passed in, it likely will not be textually identical:\n\nspacing/ordering may be different\na \"salt\" will be added (including this field avoids reprocessing if the pipeline is recreated from that dump)\nI believe default values that were not explicitly passed in will be included. @jdelfino yes, definitely not much easier. Curious, though -- which key names are different? That sounds like a bug that should be fixed. @jdoliner has this been fixed?\n\nedit: nope :/\n@msteffen wanna add this to your migration fixes todo list?. To clarify: is that in contrast to put-file? Meaning put-file still actually moves data even when going to/from locations within the same pach cluster?. Makes sense. Don't really think there's anything to do here, then :). The intention of --reprocess is not to re-process historical commits, but to short-circuit pachyderm's computation deduplication so user code will be run for all datums in the current head commit: normally any datums that have already been processed by a previous job will be skipped if they are unchanged in the latest commit, and the previous result will be reused in the current job's output commit. The --reprocess flag ensures that a job will not skip any of it's datums. \nHope that makes sense?. I haven't seen that Call dropped by load balancing policy message before, but it seems to be coming from GRPC. From what i can tell, it seems like the specific error message text may actually be a red-herring that is masking a deeper issue (due to a bug in GPRC that propagates error info incorrectly: https://github.com/grpc/grpc/issues/12506). It seems like the deeper issue may be a leak in the node GRPC implementation (node is used to proxy from a websocket connection from the browser to a GRPC connection to pachd): https://github.com/grpc/grpc/issues/11015\nAm i correct in understanding that you didn't touch anything on your end, but rather this seemed to just appear suddenly of it's own accord while you were in the middle of using the dash? (that would be consistent with the theory of a leak in the proxy)\nCould you please post the output of kubectl get all (i'm curious if the grpc proxy is having trouble), and upload the logs of the grpc-proxy container within the dash pod (if you've been using the dash for a while, those can get quite long, so probably best to dump to a file and attach it here)\nIt would also be useful to confirm whether the requests are getting to pachd at all (if my theory is right, I'd expect them to not be, but it's also possible the error is coming back from pachd). The easiest way to test this would be to look at the pachd logs with the dash open -- you should see regular requests being made (though also just confirming that pachctl commands are working while the dash is failing is a pretty good indication that the issue is on the dash side rather than within pachd)\nThe other thing to try would be to just restart the proxy container/dash pod. If it's just a leak, I'd expect that when it comes back up it would start working again.. Can you also confirm how you're accessing the dash? (pachctl port-forward, public IP, custom-configured domain, etc...). So to confirm:\n\nthe dashboard was working when first deployed\nyou closed it while running jobs within the cluster for a few hours\nyou re-opened it and saw the above errors\n\nIs that correct?\nAlso, when you say that the dash pod isn't getting any request, which container are you talking about? It certainly looks like the main web server container is serving the static assets, at least, and, judging by the fact that it's getting to the whoAmI() API call, it seems the the connection to the proxy container is at least getting initiated. @mdaniel yes, definitely agree that the default verbosity of the grpc proxy is waaaay excessive. this will be fixed in the upcoming 1.8 release which should be shipping in a few weeks (i should also be able to cut a one-off release of the proxy image with this fix that you can upgrade to manually if you'd like)\nCan you explain exactly what you mean when you say \"the dashboard app never boots up at all\"? Do you mean the pod itself never boots, or the dash never loads in the browser? if you can show screenshots of what you're seeing in the browser, the browser error console, and any errors being logged in kubernetes, that would be helpful. One thing to discuss is what we want to do about multiple auth providers. My understanding is that right now github/OAuth-based auth is always active, even if saml is being used. The dash currently supports displaying users from multiple auth providers, but does not have the ability to add new users from an auth system other than one through which the current user is authenticated.\nWe should probably decide if multiple active auth systems is something we want to support (my gut feeling is probably not right now), and decide how urgent it is to forcefully deactivate oauth, or if that's something that can wait until we properly migrate auth-based auth to the new auth APIs/flows (tracked in #3101). Also, not specifically related to SAML, but possibly part of this project in general, I think we're supposed to change permissions around discover-ability (#2434), right?. In talking with @msteffen we decided it doesn't really make sense to let users activate auth as a robot through the dashboard, so for testing I'll just do so directly through the API, then generate an OTP to put in the URL when redirecting to the otp-based autologin, and start the test from there.. This all sounds great, and will enable features we've wanted in the dash for a while.\nfor accessing historical versions from the CLI, I also prefer the commit-style syntax, rather than numerical indexing. Under the hood in the API, however, I'd be inclined to use a version field, rather than a previous field. I think this we'd want this to be 1-indexed (assuming we want the default value -- 0 -- to be the latest), and using negative numbers to walk backwards from the most recent. @jdoliner is this all implemented? (and documented? ;) )\nIIUC you want to implement the ability to retrieve a full list of historical versions, as well as an individual version. Do we have a design for that? Would you rather track that here, or in a new issue?. This sounds pretty interesting. To take a swipe at the bike shed, |} looks like a kinda cute \"reducing pipe\". Broader issue here is comprehensive docs/process/scripts defining what a production-grade Pachyderm deployment looks like. Seems every instance of pach should have this ability. Similar to/dependent on a few other auth and accounting issues:\n- Cluster Users #3157 \n- \"Accounting\"/Audit logs #2468\nAnd corresponding support in the dash:\n- \"recent changes\" https://github.com/pachyderm/dash/issues/237\n- \"cluster activity\" https://github.com/pachyderm/dash/issues/238\n- Users route improvements to show who has access to what: https://github.com/pachyderm/dash/issues/315. I don't think this is just a dash issue, it's an API-level change, and I think it's covered by #2434, right?  (that's where we've been tracking this feature in the context of other auth changes). This is super useful info to have written down\nWhy is set-branch being deprecated though? those examples seem like they'd be more readable with set (or update) rather than create. @jdoliner yes, to be clear -- i agree update-branch would be the most consistent and readable, but i think using create-branch when you're actually updating is more confusing than the inconsistency of having the API inconsistency of a lone set-* method\nedit: added as #3208. Do we really want to be encouraging people to do joins this way (massively-wasteful cross), rather than re-indexing files by authorId and bookId? I thought our learning from your recent attempts at doing this for the SQL workload (and as identified in the readme) was that this becomes untenably slow incredibly quickly? . The errors stacking up across the screen are definitely a problem, and we'll be shipping a fix in the upcoming release.\nAs for repeatedly attempting to reconnect: the overarching idea is to automatically recover from the all-to-frequent transient connection issues we've seen (they can obviously happen for a variety of reasons, but kubectl's port-forward being flaky is a common one). This still seems to me to be a reasonable goal, although it does seem we're maybe a bit overly aggressive in attempting to do so. My understanding is that this should be trying to re-establish each connection once per second, which on it's own doesn't seem like it should be blowing up your CPU usage. I'm wondering if those INSUFFICIENT_RESOURCES messages may be the result of us leaking connections over time -- do you see the CPU usage spike immediately after the connection fails?. @ysimonson i believe you recently fixed some problems with displayed repo size, right? would that have addressed this issue?. Related issues:\n\nDelete-all should force delete everything #3225\nsimilar problem (dupe?) #3025\ndocumenting the naming requirements: #3006\nproposal to relax the req: #1497. @anishvarghese hey, sorry for the slow reply. if you're running a \"local\" deployment of pachyderm on a digital ocean droplet, then yes, the most straightforward thing to do is just to access the dash via droplet's ip directly at http://:30080 -- this will automatically connect to the websocket port on the same IP.\n\nIf you have your local kubectl pointed at the kubernetes cluster into which pach is deployed, you can also use pachctl port-forward -- this will basically just set up the tunnels that it sounds like you've created manually, so you're able to access the dash via http://localhost:30080\nWhile using port-forward probably doesn't make sense in this case, it can be useful in cloud deployments where access to arbitrary ports/nodes is more tricky to set up. @discdiver thanks for the fixes! \nSorry about the benign Travis failures: the CI process uses some private secrets, which has the annoying side effect that it always fails for PRs from people outside the pachyderm github org (to avoid those secrets from being exposed). Would you mind filling out the CLA: https://pachyderm.wufoo.com/forms/pachyderm-contributor-license-agreement/\nThen someone on the team can get this through CI and merged in (@JoeyZwicker  ?)\nThanks again!. Sorry this has taken a while to get to, @Nick-Harvey is away at KubeCon at the moment.\n@Nick-Harvey can you please make sure to take a look at this when you have a minute?. Definitely seems like it would be good to have this info, though it is probably relevant to more than just docs contributors. @Nick-Harvey wanna take a look at where else this info should maybe go?\nAlso, it would be even better if we could figure out a better way to work around this issue. Could we have a limited subset of our test suite run for all PR submitters? or an easier way to approve PRs to run through the full suite?. I believe the technical side of migration shipped a while ago. Is there more to this issue than that (ie some additional docs?)\nOr is there still technical work to be done @msteffen ?. Instead of having each command take multiple operations, what about just allowing users to wrap multiple commands in a transaction to be applied atomically?\nNot 100% sure this is a good idea, but it feels like this could kind of be a nice evolution of manually calling start-/finish-commit. The semantics you'd want probably aren't identical (for instance transactions, more than commits, should probably user-scoped), but there's certainly some similarity in the idea of creating a set of operations to be applied atomically.\nOne thing I'm not totally sure of is how this would interact with provenance/branch-updating/commit propagation. My thinking is that new commits within a transaction would actually get created immediately so that we have somewhere to store the data, but the relevant branch pointer wouldn't get moved until calling commit\n```\n$ pc create-repo input1\n$ pc create-repo input2\n$ pc create-pipeline -f my-pipeline-that-takes-input1-and-input2.json\n$ pc begin-transaction\n76296847-ba29-4a29-b8ea-27ffaa687b12\n$ pc put-file input1 master some-file \nadded to transaction 76296847-ba29-4a29-b8ea-27ffaa687b12\n$ pc put-file input2 master some-other-file\nadded to transaction 76296847-ba29-4a29-b8ea-27ffaa687b12\n$ pc copy-file other-repo master some-third-file input1 master\nadded to transaction 76296847-ba29-4a29-b8ea-27ffaa687b12\n$ pc commit-transaction\ncommitting...\ncommitted successfully\n$ pc list-job --pipeline my-pipeline\nonly one job has been created (i think?)\n```\nOpen questions:\n- should transactions be implicitly tied to users/sessions? or should each operation allow a transaction to be speficied explicitly? (my inclination is that this should vary between how it's exposed in the API vs in the CLI -- end users shouldn't have to manually manage the context of which transaction they're currely interacting with, but that state should be stored client-side, not on the server)\n- how does isolation between simultaneous transactions work? my inclination is to keep this super simple: basically either just use last- or first-writer-wins (ie just bail if a later change has affected relevant objects)\n- should these be a client- or server-side queue of operations? it could be that a transaction gets created, assigned an ID, and that operations get added to it (this is kinda what my example above looks like). Alternatively, it could be that a client just sends a CommitTransaction() request with a Transaction message which contains a list of operations that are all applied atomically. This seems simpler to me than managing state on the server, but limits functionality around isolation/consistency\n- .. lots more i'm sure.... Specific request for atomic updates: https://github.com/pachyderm/pachyderm/issues/3287#issuecomment-454944627. Would it make sense to also re-introduce the file.commit_modified field that disappeared in 1.4?  #1457 Or is this a replacement? (meaning that issue should be closed). summarizing offline convo:\nright now if i get a list of files (without the new --history flag introduced by this PR), then FileInfo.File.Commit contains the commit passed into list-file. With the --history flag, I'll get a list of FileInfos, one for each commit at which the file was modified. In order to optimize this for the (common) case of just wanting to get the commit at which the file was most recently modified, @jdoliner proposed having the --history flag take an optional number parameter, defining how far to walk back in history. So calling list-file with --history 1 would return a list of files in the head commit, each linked to the commit at which they were last modified\n. @iswaverly Sorry for the confusion, Pachyderm uses the Minio client to communicate with all non-Amazon S3-compatible object stores (including Ceph). as mentioned in slack, presumably longer-term fix for this and other new features/breaking changes is to have the new feature in the docs with \"available as of v1.8.1\" and note on the deprecated feature as of what version it is deprecated and (if we have a timeline) when it is expected to be removed (in this case presumably \"deprecated as of 1.9\"; do we have a policy on timeline for removal of deprecated features?). Seems like a very reasonable ask. Internally not sure whether i makes more sense to shoe-horn this into the UpdatePipeline API call vs adding a new endpoint. @jdoliner ?. Thanks for the clarification, @mttwong -- having a separate pachctl reprocess pipeline command (or something similar) makes sense to me. I think the (probably more minor) part about having to manually fetch your pipeline specs can be worked around today by piping extract-pipeline to update-pipeline, but that does seem like a bit of a kludge and should probably be built in. As for the multi-update part of this request, would that be addressed by what we're talking about in #3267 @mttwong ?\nEdit: you can also use pachctl edit-pipeline <pipeline-name> to drop into an editor and change it. Not 100% sure, but I think making no actual changes, while still passing the --reprocess flag, should still kick off a reprocessing job.. @ktarplee thanks for your work on this, i'm also interested in giving microk8s a go. i believe you will continue to need the --no-expose-docker-socket flag if the docker socket is not located in the standard location. afaik the docker socket is mostly used by pach to introspect metadata from containers (getting the user to run as is what comes to mind, but there may be more use cases i'm forgetting). We'll need to follow the guidelines about being able to run as any user here: https://docs.openshift.com/container-platform/3.9/creating_images/guidelines.html#general-container-image-guidelines. looks like this may be the same issue as #2446?. Seems the root of this issue is that the go-dockerclient library doesn't support creds stored in oxskeychain (more details here, among other places)\nThis should probably be addressed with a PR against that lib (related to this issue: https://github.com/fsouza/go-dockerclient/issues/677), but can be worked around by embedding the docker creds in the ~/.docker/config.json file (as is done on Linux)\nby default, ~/.docker/config.json will look something like this:\n```\n{\n  \"stackOrchestrator\" : \"swarm\",\n  \"auths\" : {\n    \"https://index.docker.io/v1/\" : {\n}\n\n},\n  \"credSstore\" : \"osxkeychain\"\n}\n```\nyou can generate a string containing your dockerhub creds by running \necho -n \"username:password\" | base64\nand paste the resulting string into the config file's auths section:\n{\n  \"stackOrchestrator\" : \"swarm\",\n  \"auths\" : {\n    \"https://index.docker.io/v1/\" : {\n      \"auth\": \"<YOUR CREDS STRING HERE>\"\n    }\n  },\n  \"credSstore\" : \"osxkeychain\"\n}\n. I agree that using the docker (rather than system) user seems to be better behavior. But I'm inclined to say that expecting users to find this issue and manually edit their ~/.docker/config.json to implement this workaround isn't really sufficient to consider this issue \"resolved\". I think we should either fix/work around the upstream lib issue, or at least point the user in the right direction when using --push-images\nwhat do you think @ysimonson ?\n. well, given how broken it seems it was, i doubt it has been used much to date :P\nso yea, maybe just checking for the existence of ~/.docker/config.json's [\"auths\"][\"https://index.docker.io/v1/\"][\"auth\"] key and printing a warning message pointing to this workaroud if that key doesn't exist?. @Snapple49 i'm not sure i follow what you're trying to achieve with editing the addresses? afaiu most of the addresses we connect to around these ports are internal to kubernetes, so are assigned dynamically by k8s' network routing layer. would you mind opening a new issue explaining your use case a bit more plz? Thanks!\n. We've broken out a number of more-specific issues with port customization, but makes sense to keep this around as the meta-issue\nAFAIK there are two remaining related issues:\n\n\n3374 Port forwarding does not respect customized remote ports\n\n\n3365 pipeline services allocate port 80 as \"grpc-port\"; this needs to be configurable by env var\n\n\n@ysimonson are there others I'm missing?. nothing hard-blocked, no. though now that that is fixed, i should be able to use that in my script rather than manually getting pod names and running kubectl port-forward (i think?)\n. I believe all changes for this are in master/released in later 1.8.x builds, but afaik none of this has been documented yet, so converting this to a docs issue. Note the specific list of ports is in #3363 and #3333. Just re-kicked off the CI job which had flaked out, but don't think that's really relevant to this change. Does this look OK to you @JoeyZwicker ?. Cool, was fairly sure that was the case, but just wanted to make sure i wasn't missing some actual reason that this should be referencing lazy. i think it's a change needed in the deploy command to just remove this (seemingly) unused field, but if it used somewhere that i'm missing, then it certainly needs to be better documented. This LGTM, but I haven't actually tested all the examples. @JoeyZwicker @Nick-Harvey or @pappasilenus  do one of you want to test this out? Or are these examples exercised by the test suite?\nFWIW CI looks to be just failing because of non-team-member access to the testing enterprise token.. I think what we may want is to have two pod specs: one to override values in our generated spec (afaiu this is what we do today), and one to be recursively merged (so, for example, lists could be extended)\nIf we want this to be maximally flexible, we could just allow passing something like an entire jq filter (or list of filters) to apply. This would, say, allow modifying individual values in a list, which would be impossible with the override + merge scheme I've proposed above, (and would have made the recent OpenShift work way easier, for example). This may, however, be overkill for many use-cases. sorry, that comment was the conclusion of an in-person convo, but I should have added some context from the discussion:\nOverall, I totally get the use-case, and really like the general pattern of using a series of filters, it's just that I've had some bad experiences with JSON-Patch in practice in the past. Specifically, JSON-patch uses the fairly restrictive JSON Pointer (IETF RFC 6901) spec, which requires specifying array indices exactly (compared with something like JSON-path, or jq, both of which have some richer form of \"querying\")\nIn some more \"traditional\" uses of JSON[1], this isn't terrible, but in the way we/K8S (and many modern APIs) use JSON, this is problematic, because \"key-value\" pairs are often stored in a non-stably- (and non-imporantly-)ordered array (effectively being used as a set) of JSON objects where each contains some sort of \"key\" (name, generally, in K8S) and value. This means that it's effectively impossible to use JSON-Patch to, say, replace the value of an env var, since the position of the particular env var in the array may change, others may be added, etc.\nThis is the primary reason I've been using jq in the recent work I've been doing to manipulate pachctl deploy custom's output to work on OpenShift -- I can do things like: \n```\ndisable usage of the docker socket (in pachd container template, within pachd Deployment)\njq 'select(.kind == \"Deployment\" and .metadata.name == \"pachd\").spec.template.spec.containers[] |= (select(.name == \"pachd\").env[] |= (select(.name == \"NO_EXPOSE_DOCKER_SOCKET\").value = \"true\"))'\n```\nFar more powerful/less brittle, if somewhat opaque\nAs for how this could be implemented, jq is a fully-self-contained C implementation, so the options I see would be either to just ship the binary in the pachd image and shell out to it, or to use libjq through an FFI layer (probably just cgo). Pretty confident in saying re-implementing in Go is not on the table.\nAll that being said, I wouldn't be entirely opposed to having a JSON-Patch-based implementation if you think that better addresses the use-case than just having a single \"overwrite\" and \"merge\" tree (and especially if you're willing to contribute it!). I am curious, though, which specific cases the flexibility of JSON-Patch actually helps with, over a simpler replace-and-merge (totally believe there may be some, I just don't have examples coming to mind)\n[1]: By \"traditional\", I mean specifically where:\n1. mappings are always stored as JSON objects with keys as keys and values as values\n2. Array order is stable (and, ideally, relevant). Basically the point i was trying to make about JSON patch being broader than what's needed for these simpler use-cases is that i have trouble coming up with use-cases for move or copy operations. But, at the end of the day, I think you're probably right that using a standard is more important than minimizing the API surface. So thinking the best path forward is doing JSON patch for now (which iiuc JD has already started on), then, if we find we're running into cases for which richer querying is needed, we could add a jq (or similar) filter down the line (either as  pod_spec_jq_filters, or just by making pod_spec_filters be a list of generic Filter objects, each of which has a type)\nThe particular cases where I was seeing consistently inconsistent ordering of these key-value sets that get encoded as JSON arrays was with env vars, which I believe stems from them being stored in a Go map (which, IIUC, have their order deliberately randomized when items are extracted to reinforce that order shouldn't be relied upon). @ktarplee glad that worked!\nStill makes a me a bit uncomfortable that you have to know that pachyderm already supplies a list of volumes (and that there's no way to insulate yourself from this implementation detail, as afaik there's no way to append to a list if it exists, or create it if it does not). This is one of the issues with JSON patch, but I think it may be addressed by using a JSON merge patch, right? @jdoliner I am correct that the pod_spec field now assumes the passed value is a merge-patch by default? (as opposed to it's old behavior of just bluntly overwriting)\nEdit: according to the pod_spec docs, looks like it does indeed accept a merge patch now\nAs an aside: I have to admit I didn't know merge patches existed when I wrote my comments above about applying an \"override and a merge operation\" (as that seems to be pretty much a better-thought-out version of exactly what I was trying to describe), and had totally missed that @ktarplee had already linked to k8s' implementation in his comment above. Ah, no, I think I was actually confusing the IETF's RFC 7396 (ie the official, standard) JSON Merge Patch with a third spec, the \"Strategic Merge Patch\" (which appears to be a K8S-specific \"standard\"): https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/strategic-merge-patch.md (again, can't possibly imagine why :roll_eyes: )\nStrategic Merge Patch seems to be the one that is most similar to what I was describing earlier, where (simplifying a bit) there are separate directives for \"overrides\" (delete/replace) and \"merges\" (which allows \"merging\" arrays-used-as-sets by combining elements)\nThe benefit of that, over requiring the use of JSON Patch appends, is that it seems a JSON Patch append only works if the list already exists in the base document -- an append to a non-existent key doesn't work -- so the user's patch is dependent on Pachyderm's generated manifest (since there seems to be no way to guard against this by \"upserting\" a key to an array: https://stackoverflow.com/questions/47670366/jsonpatch-adding-element-to-array-and-creating-it-if-not-exist).\nWhat would you think of switching to using that \"spec\" (by just using the K8S implementation)? It's technically non-standard, but is K8S-world-standard (and does seem pretty much strictly better than the alternatives). Strategic Merge Patch seems to be what is actually implemented by the code @ktarplee linked above: https://github.com/kubernetes/kubernetes/blob/a3d30dc939c102be2eec3d7d5c7f33f63a1e9893/staging/src/k8s.io/apimachinery/pkg/util/strategicpatch/patch.go#L812. Answering your Q from #3293 -- yes, using the docker user certainly seems like more reasonable behavior than the system user. . Would it make sense to just have travis detect whether it can run these tests, and just skip them if it can't, rather than disabling them completely?. oh. well then... :). As just discussed in person: not all of these changes have landed in an official release yet, so we should probably wait to merge this until that has happened, since as of right now some of this still won't work. Pretty sure @brycemcanally is planning to cut a point release with the code this afternoon, though. @jdoliner is this something to do with a null/zero value being returned by your new file history API?. Thanks for filing this! To confirm: does this only occur 1) on output repos (not repos you manually create and add files to explicitly) that 2) have files only in subdirectories (files at the top level are counted towards the computed filesize?). related: #3295. To clarify, my understanding is that because of pachd running in worker pods, we still need to be able to run in cases where there is no matching container port exposed. So I think the change should be:\n\nuse the env var if explictly set (pachd does this today, but worker does not: #3362)\nuse the corresponding service port if it exists\nuse the current default values. Possibly relevant https://docs.openshift.com/container-platform/3.9/creating_images/guidelines.html#general-container-image-guidelines. related: #2386 glob file docs\n\nAlso, it's possible some of this is more useful if used in conjunction with exclude patterns (#2911)...have to think about that more. That does seem more useful, though I thought avoiding this type of lookahead matching was (a big?) part of what allowed glob patterns to be more performant than generic regexes. Not sure matching is actually ever a perf bottleneck for us, but potentially something to keep in mind?\nRelated: i can haz generic regex path matching? :) . Looks like we did have an issue for regex matching already: #1488. FWIW, I did update the hard-coded fallback value today so that new releases will stop referencing something ancient (but that's obviously not a long-term fix)\nI think the ideal fix for this issue is probably to add something to dash CI to automatically keep that fallback value up to date with the latest tested-to-be-compatible dash version. Once that's done, the current behavior of pulling that value to bootstrap the new compat file is probably a good approach. That being said, dash CI/CD is a quagmire that I've been putting off properly tackling. So until I (eventually) get around to that, this PR is probably the next best thing. since cron is inherently time-based, would it maybe make sense to have the file just be named after the (probably ISO 8601) timestamp?. Can a template contain multiple pipelines? (as a pipeline spec file can today)\nNot sure of all the implications of that, but if they could, I think it would at least be nice in that you'd be able to use the same \"organization\" primitive both to manage \"steps\" (containers?) of a pipeline, as well as the relationships between pipelines (further enforcing that the two concerns of \"logical representation/organization\" and \"system-level implementation/consistency guarantees/scheduling\" are orthogonal). Also seems like this is probably the time to revisit Pipeline versioning/DAG branching (#1036) and how that works with this design. We recently had a long convo with one of our two primary users at ((recently-kicked-off-OpenShift-PoC-company))  about how this feature set will be important to them in the future. Oh, looks like that has already been filed here: https://github.com/pachyderm/pachyderm/issues/2395\nNow that we're recommending this, though, may make sense to prioritize that change (tbh didn't even know those flags existed until now). nvm, this is the input repo. there is some other issue causing my job to not run, but this certainly isn't it  :man_facepalming:. This is required for openshift deployment to work (#3334)\nRelated port-customization issue: #3295 \nOnce this is done, then these ports should be usable enough that it makes sense to include them in the docs (also tracked in #3333)\n. yes, sorry, it's supposed to be 650 -- just edited to correct the original issue description (1650 is what i was using to test)\nThis issue is about setting the ports in the k8s manifest generated when a user deploys a pipeline: all should be propagated to the sidecar pachd binary running in the storage container; some (but likely not all?) should be propagated to the worker in the user container\n3362 is about the worker not making use of certain port env vars if they are set. afaict that PR (https://github.com/pachyderm/pachyderm/pull/3368)  handles the worker (which, yes, requires a subset of the ports), but the sidecar (storage container) is a full pachd process, so i think it will need all the ports propagated. @ysimonson once #3367 lands, I believe all ports will be configurable. Does that PR include propagation? (doesn't seem like it would make sense for it to not -- the port it's making configurable isn't even used on the pachd master afaict). In testing this out, it's looking like the storage container doesn't have most of the ports set (but the worker container does). The testing is using this custom release\nSpecifically, it looks like pachd is propagating two of the ports:\n- name: PORT\n              value: '1650'\n            - name: HTTP_PORT\n              value: '1652'\nbut none of the others, including 653 (which, iirc, is the one that the worker actually uses to connect to the sidecar)\nI think the storage container failing to bind on the privileged ports may actually be the root of the connection timeout issue, and that the SSL certs errors/warnings mentioned in that issue may just be a red herring\nI've added the missing ports manually:\n```\n        - name: PORT\n          value: '1650'\n        - name: PPROF_PORT\n          value: '1651'\n        - name: HTTP_PORT\n          value: '1652'\n        - name: PEER_PORT\n          value: '1653'\n        - name: PPS_WORKER_GRPC_PORT\n          value: '1680'\n\n``\n and that seems to have allow the container to progress. When #3367 lands, this will be set-able via thePPS_WORKER_GRPC_PORT, correct?. would it also make sense to change the default from80to, say680, or even just654(to be in keeping with the rest of our650-653` ports)?\nIf it were a webserver, this would kinda make sense, but it's not clear to me why a GRPC service would have been on 80 in the first place.... generally agreed, though my thinking was partly that changing the default would flush out the places where customization is broken (due to implicit use of 80). Is anything else needed to get this merged?. It seems that PR adds a flag to set which remote-port is forwarded (to?). Can you explain what needs to be done in order to use port-forward to attach to a cluster with custom ports?\n(thinking maybe that flag needs to be set to the same value as passed in the PORT env var?). also need to explicitly set the namepace for some commands to work (see #2745). Specific change needs to happen here https://github.com/pachyderm/pachyderm/blob/master/src/server/pps/server/worker_rc.go#L175\nThinking making this configurable with an env var? Turning this off is the easy part, though -- what's not totally clear to me (and potentially more tricky) is all the reasons we need this container to run as root (i thought we were already able to run without, but it seems that's just the main pachd, not the worker). Having the ability to configure pachd to only listen on ports >1024 is why we've been making all these port configuration fixes for the past week :)\nAFAIK we now allow all ports to be set to custom (>1024) values. Are you saying there are still places we're missing?. it's unclear whether the cert issue is actually blocking. afaict, it doesn't seem to be at the moment, since we're talking to the sidecar over a local port within the pod, which doesn't require a cert. it appears the timeout shown in the error above (error constructing pachClient: context deadline exceeded) is actually caused because of a real failure in the sidecar pachd running in the storage container. It appears to not be blocking. Still seems like setting this up as an emptydir mount could work in most cases, but does run the risk of issues if the base image already has something in /etc/ssl/certs/\nSo probably good to leave this for now. i don't believe any fix was needed for this -- my recollection is that the \"error\" turned out to actually be benign (and the real issue was actually some of the other port errors you fixed), so i didn't bother overwriting that dir with an emptydir to avoid the possibility of breaking things in the case that an image actually did have some certs in that dir already. I'm still not actually sure why that cert is needed at all, since i'm pretty sure the worker communicates with the sidecar pachd over a private port that doesn't require a cert to connect over. @msteffen do you know why this cert is being written in the first place? without the cert there, is something here going to break if TLS is enabled? any suggestions of possible approaches if we actually want to fix this?. This still doesn't seem to be working with the latest custom release, unfortunately: https://github.com/pachyderm/pachyderm/releases/tag/v1.8.2-3d440134bca6a690643f64911242930cca174a48\nThe vars are set on the master pachd: \n$ oc describe deployment.apps/pachd\nName:                   pachd\n[...]\n      PORT:                                           1650\n      PPROF_PORT:                                     1651\n      HTTP_PORT:                                      1652\n      PEER_PORT:                                      1653\n      PPS_WORKER_GRPC_PORT:                           1680\n[...]\nbut not on the resulting pipeline RCs:\n$ oc describe replicationcontroller/pipeline-edges-v1\nName:         pipeline-edges-v1\n[...]\nContainers:\n   user:\n    Image:      pachyderm/opencv\n    Port:       <none>\n    Host Port:  <none>\n    Command:\n      /pach-bin/worker\n    Requests:\n      cpu:     0\n      memory:  64M\n    Environment:\n      PPS_WORKER_IP:          (v1:status.podIP)\n      PPS_POD_NAME:           (v1:metadata.name)\n      PPS_ETCD_PREFIX:       pachyderm/1.7.0/pachyderm_pps\n      PPS_NAMESPACE:         pachyderm-3\n      PPS_SPEC_COMMIT:       65817ae7da884f2e8c5c6c490ac9bfc0\n      PPS_WORKER_GRPC_PORT:  1680\n      PPROF_PORT:            1651\n      PEER_PORT:             1653\n      PPS_PIPELINE_NAME:     edges\n      PACH_ROOT:             /pach\n      STORAGE_BACKEND:       MINIO\n[...]\n   storage:\n    Image:      pachyderm/pachd:1.8.2-3d440134bca6a690643f64911242930cca174a48\n    Port:       <none>\n    Host Port:  <none>\n    Command:\n      /pachd\n      --mode\n      sidecar\n    Requests:\n      cpu:     0\n      memory:  64M\n    Environment:\n      BLOCK_CACHE_BYTES:    64M\n      PFS_CACHE_SIZE:       16\n      PACH_ROOT:            /pach\n      STORAGE_BACKEND:      MINIO\n      PORT:                 1650\n      HTTP_PORT:            1652\n      STORAGE_BACKEND:      MINIO\n[...]\nNote that the storage container is missing PPROF_PORT and PEER_PORT, though they are present on the worker. (it also has STORAGE_BACKEND twice, for some reason...)\nFull output: \n\n```\n$ pachctl version\nCOMPONENT           VERSION                                          \npachctl             1.8.2-3d440134bca6a690643f64911242930cca174a48   \nERRO[0001] Implicit port forwarding was not enabled because of an error: No pods found for app pachd \npachd               1.8.2-3d440134bca6a690643f64911242930cca174a48 \n$ oc get all\noc describe NAME                            READY     STATUS             RESTARTS   AGE\npod/etcd-6485bcbd8d-rf4tc       1/1       Running            0          1m\npod/pachd-5db48cdc69-9gd6x      1/1       Running            0          1m\npod/pipeline-edges-v1-m9q4t     1/2       CrashLoopBackOff   4          1m\npod/pipeline-montage-v1-m2f2r   1/2       CrashLoopBackOff   4          1m\n\nNAME                                        DESIRED   CURRENT   READY     AGE\nreplicationcontroller/pipeline-edges-v1     1         1         0         1m\nreplicationcontroller/pipeline-montage-v1   1         1         0         1m\n\nNAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                                      AGE\nservice/etcd                  NodePort    172.30.102.112           2379:30338/TCP                                                               1m\nservice/pachd                 NodePort    172.30.185.147           1650:30950/TCP,1651:30951/TCP,1652:30952/TCP,1654:30954/TCP,1999:30999/TCP   1m\nservice/pipeline-edges-v1     ClusterIP   172.30.39.148            1680/TCP,9090/TCP                                                            1m\nservice/pipeline-montage-v1   ClusterIP   172.30.190.163           1680/TCP,9090/TCP                                                            1m\n\nNAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/etcd    1         1         1            1           1m\ndeployment.apps/pachd   1         1         1            1           1m\n\nNAME                               DESIRED   CURRENT   READY     AGE\nreplicaset.apps/etcd-6485bcbd8d    1         1         1         1m\nreplicaset.apps/pachd-5db48cdc69   1         1         1         1m\ngabriel@rexy2:~/repos/pachyderm/rbc-poc/deployment$ oc describe service/pipeline-edges-v1\nName:              pipeline-edges-v1\nNamespace:         pachyderm-3\nLabels:            app=pipeline-edges-v1\n                   component=worker\n                   pipelineName=edges\n                   suite=pachyderm\n                   version=1.8.2-3d440134bca6a690643f64911242930cca174a48\nAnnotations:       prometheus.io/port=9090\n                   prometheus.io/scrape=true\nSelector:          app=pipeline-edges-v1,component=worker,pipelineName=edges,suite=pachyderm,version=1.8.2-3d440134bca6a690643f64911242930cca174a48\nType:              ClusterIP\nIP:                172.30.39.148\nPort:              grpc-port  1680/TCP\nTargetPort:        1680/TCP\nEndpoints:         \nPort:              prometheus-metrics  9090/TCP\nTargetPort:        9090/TCP\nEndpoints:         \nSession Affinity:  None\nEvents:            \n$ oc describe replicationcontroller/pipeline-edges-v1\nName:         pipeline-edges-v1\nNamespace:    pachyderm-3\nSelector:     app=pipeline-edges-v1,component=worker,pipelineName=edges,suite=pachyderm,version=1.8.2-3d440134bca6a690643f64911242930cca174a48\nLabels:       app=pipeline-edges-v1\n              component=worker\n              pipelineName=edges\n              suite=pachyderm\n              version=1.8.2-3d440134bca6a690643f64911242930cca174a48\nAnnotations:  pipelineName=edges\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:       app=pipeline-edges-v1\n                component=worker\n                pipelineName=edges\n                suite=pachyderm\n                version=1.8.2-3d440134bca6a690643f64911242930cca174a48\n  Annotations:  pipelineName=edges\n  Init Containers:\n   init:\n    Image:      pachyderm/worker:1.8.2-3d440134bca6a690643f64911242930cca174a48\n    Port:       \n    Host Port:  \n    Command:\n      /pach/worker.sh\n    Environment:  \n    Mounts:\n      /pach-bin from pach-bin (rw)\n      /pfs from pachyderm-worker (rw)\n  Containers:\n   user:\n    Image:      pachyderm/opencv\n    Port:       \n    Host Port:  \n    Command:\n      /pach-bin/worker\n    Requests:\n      cpu:     0\n      memory:  64M\n    Environment:\n      PPS_WORKER_IP:          (v1:status.podIP)\n      PPS_POD_NAME:           (v1:metadata.name)\n      PPS_ETCD_PREFIX:       pachyderm/1.7.0/pachyderm_pps\n      PPS_NAMESPACE:         pachyderm-3\n      PPS_SPEC_COMMIT:       65817ae7da884f2e8c5c6c490ac9bfc0\n      PPS_WORKER_GRPC_PORT:  1680\n      PPROF_PORT:            1651\n      PEER_PORT:             1653\n      PPS_PIPELINE_NAME:     edges\n      PACH_ROOT:             /pach\n      STORAGE_BACKEND:       MINIO\n      MICROSOFT_ID:                   Optional: true\n      MICROSOFT_SECRET:           Optional: true\n      GOOGLE_CRED:                     Optional: true\n      MINIO_SECURE:                   Optional: true\n      MINIO_SIGNATURE:             Optional: true\n      AMAZON_BUCKET:                 Optional: true\n      AMAZON_SECRET:                 Optional: true\n      AMAZON_TOKEN:                   Optional: true\n      AMAZON_VAULT_ADDR:         Optional: true\n      AMAZON_VAULT_ROLE:         Optional: true\n      MICROSOFT_CONTAINER:     Optional: true\n      AMAZON_VAULT_TOKEN:       Optional: true\n      MINIO_ID:                           Optional: true\n      MINIO_SECRET:                   Optional: true\n      AMAZON_REGION:                 Optional: true\n      AMAZON_ID:                         Optional: true\n      GOOGLE_BUCKET:                 Optional: true\n      MINIO_ENDPOINT:               Optional: true\n      AMAZON_DISTRIBUTION:     Optional: true\n      MINIO_BUCKET:                   Optional: true\n    Mounts:\n      /pach-bin from pach-bin (rw)\n      /pachyderm-storage-secret from pachyderm-storage-secret (rw)\n      /pfs from pachyderm-worker (rw)\n   storage:\n    Image:      pachyderm/pachd:1.8.2-3d440134bca6a690643f64911242930cca174a48\n    Port:       \n    Host Port:  \n    Command:\n      /pachd\n      --mode\n      sidecar\n    Requests:\n      cpu:     0\n      memory:  64M\n    Environment:\n      BLOCK_CACHE_BYTES:    64M\n      PFS_CACHE_SIZE:       16\n      PACH_ROOT:            /pach\n      STORAGE_BACKEND:      MINIO\n      PORT:                 1650\n      HTTP_PORT:            1652\n      STORAGE_BACKEND:      MINIO\n      GOOGLE_CRED:                    Optional: true\n      MICROSOFT_ID:                  Optional: true\n      MICROSOFT_SECRET:          Optional: true\n      AMAZON_SECRET:                Optional: true\n      AMAZON_TOKEN:                  Optional: true\n      AMAZON_VAULT_ADDR:        Optional: true\n      AMAZON_VAULT_ROLE:        Optional: true\n      MICROSOFT_CONTAINER:    Optional: true\n      MINIO_SECURE:                  Optional: true\n      MINIO_SIGNATURE:            Optional: true\n      AMAZON_BUCKET:                Optional: true\n      AMAZON_VAULT_TOKEN:      Optional: true\n      AMAZON_ID:                        Optional: true\n      GOOGLE_BUCKET:                Optional: true\n      MINIO_ID:                          Optional: true\n      MINIO_SECRET:                  Optional: true\n      AMAZON_REGION:                Optional: true\n      MINIO_BUCKET:                  Optional: true\n      MINIO_ENDPOINT:              Optional: true\n      AMAZON_DISTRIBUTION:    Optional: true\n    Mounts:\n      /pachyderm-storage-secret from pachyderm-storage-secret (rw)\n  Volumes:\n   pach-bin:\n    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:  \n   pachyderm-worker:\n    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:  \n   pachyderm-storage-secret:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  pachyderm-storage-secret\n    Optional:    false\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1m    replication-controller  Created pod: pipeline-edges-v1-m9q4t\n$ oc describe deployment.apps/pachd\nName:                   pachd\nNamespace:              pachyderm-3\nCreationTimestamp:      Fri, 25 Jan 2019 16:56:48 -0800\nLabels:                 app=pachd\n                        suite=pachyderm\nAnnotations:            deployment.kubernetes.io/revision=1\nSelector:               app=pachd,suite=pachyderm\nReplicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:           app=pachd\n                    suite=pachyderm\n  Annotations:      iam.amazonaws.com/role=\n  Service Account:  pachyderm\n  Containers:\n   pachd:\n    Image:       pachyderm/pachd:1.8.2-3d440134bca6a690643f64911242930cca174a48\n    Ports:       1650/TCP, 1651/TCP, 1652/TCP, 1653/TCP, 1999/TCP, 1654/TCP\n    Host Ports:  0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP\n    Limits:\n      cpu:     1\n      memory:  3G\n    Requests:\n      cpu:      1\n      memory:   3G\n    Readiness:  exec [/pachd --readiness] delay=0s timeout=1s period=10s #success=1 #failure=3\n    Environment:\n      PACH_ROOT:                                      /pach\n      ETCD_PREFIX:                                    \n      NUM_SHARDS:                                     16\n      STORAGE_BACKEND:                                MINIO\n      STORAGE_HOST_PATH:                              \n      WORKER_IMAGE:                                   pachyderm/worker:1.8.2-3d440134bca6a690643f64911242930cca174a48\n      IMAGE_PULL_SECRET:                              \n      WORKER_SIDECAR_IMAGE:                           pachyderm/pachd:1.8.2-3d440134bca6a690643f64911242930cca174a48\n      WORKER_IMAGE_PULL_POLICY:                       IfNotPresent\n      PACHD_VERSION:                                  1.8.2-3d440134bca6a690643f64911242930cca174a48\n      METRICS:                                        true\n      LOG_LEVEL:                                      info\n      BLOCK_CACHE_BYTES:                              1G\n      IAM_ROLE:                                       \n      NO_EXPOSE_DOCKER_SOCKET:                        true\n      PACHYDERM_AUTHENTICATION_DISABLED_FOR_TESTING:  false\n      PACHD_POD_NAMESPACE:                             (v1:metadata.namespace)\n      PACHD_MEMORY_REQUEST:                           3000000000 (requests.memory)\n      EXPOSE_OBJECT_API:                              false\n      GOOGLE_CRED:                                              Optional: true\n      MINIO_SIGNATURE:                                      Optional: true\n      AMAZON_SECRET:                                          Optional: true\n      AMAZON_TOKEN:                                            Optional: true\n      AMAZON_VAULT_ROLE:                                  Optional: true\n      AMAZON_DISTRIBUTION:                              Optional: true\n      GOOGLE_BUCKET:                                          Optional: true\n      MICROSOFT_ID:                                            Optional: true\n      MINIO_ID:                                                    Optional: true\n      MINIO_SECURE:                                            Optional: true\n      AMAZON_VAULT_ADDR:                                  Optional: true\n      AMAZON_REGION:                                          Optional: true\n      AMAZON_BUCKET:                                          Optional: true\n      MICROSOFT_CONTAINER:                              Optional: true\n      MICROSOFT_SECRET:                                    Optional: true\n      MINIO_BUCKET:                                            Optional: true\n      MINIO_ENDPOINT:                                        Optional: true\n      MINIO_SECRET:                                            Optional: true\n      AMAZON_ID:                                                  Optional: true\n      AMAZON_VAULT_TOKEN:                                Optional: true\n      PORT:                                           1650\n      PPROF_PORT:                                     1651\n      HTTP_PORT:                                      1652\n      PEER_PORT:                                      1653\n      PPS_WORKER_GRPC_PORT:                           1680\n      WORKER_USES_ROOT:                               false\n    Mounts:\n      /pach from pach-disk (rw)\n      /pachyderm-storage-secret from pachyderm-storage-secret (rw)\n  Volumes:\n   pach-disk:\n    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:  \n   pachyderm-storage-secret:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  pachyderm-storage-secret\n    Optional:    false\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    NewReplicaSetAvailable\nOldReplicaSets:  \nNewReplicaSet:   pachd-5db48cdc69 (1/1 replicas created)\nEvents:\n  Type    Reason             Age   From                   Message\n  ----    ------             ----  ----                   -------\n  Normal  ScalingReplicaSet  8m    deployment-controller  Scaled up replica set pachd-5db48cdc69 to 1\n```\n\n. @ysimonson i tried it both on the custom release you cut that i linked in my comment above (which i believe is from the head of the openshift branch) and with my own image off my openshift-hacks branch (which adds a couple commits to your openshift branch). they both seemed to have the same result. > To make pachctl mount work, i have to install fuse first with \"apt-get install fuse\".\n\nBy default, /dev/fuse is \"crw-rw---- 1 root root 10, 229 Jan 24 21:38 /dev/fuse\"\nI just did \"chmod o+rw /dev/fuse\", and it works for normal user (edited) \nnot sure if this is the proper way to solve it tho\n\n\ufffc\nI thought the answer would be something like adding a fuse group, and adding users to it, but if /dev/fuse is owned by root group, not sure how that would have helped. So if this works... \u00af_(\u30c4)/\u00af \nuser was testing on \"jupyter/all-spark-notebook\" docker image\n. Do we want to set RunAsNonRoot, or just entirely skip setting a SecurityContext altogether? In order to get past this when manually editing the manifests, I just removed the whole SecurityContext declaration, and that _seemed to work (but I also haven't gotten all the way along, so not sure of all the implications of doing that)\nFrom a brief bit of googling, seems RunAsNonRoot may require a numeric user to be explicitly set in the Docker image? Is that what is happening by default if no SecurityContext is set?. This fix appears to be working with https://github.com/pachyderm/pachyderm/releases/tag/v1.8.2-3d440134bca6a690643f64911242930cca174a48 :champagne: . here are full examples of a manifest generated by pach (before.yaml) and one with the manual edits applied to make it work on OpenShift (ie this and the remaining port fix): https://gist.github.com/gabrielgrant/6868f813b16226776e66f51725b0e564\nThe key parts are:\n\nthe added volume mount (compared to before)\nthe added volume declaration (compared to before). Note that these are actually different pipelines (running concurrently in our test cluster), so if you just run a diff on the files, they'll show a bunch of things related to image name and stuff that aren't relevant. Specific error from the sidecar (storage container):\n\n2019-01-28T22:26:50Z INFO pfs.API.GetFile {\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"__spec__\"},\"id\":\"926485a265934415a16602a14d094484\"},\"path\":\"spec\"}}} []\n2019-01-28T22:26:50Z INFO authclient.API.WhoAmI {\"request\":{}} []\n2019-01-28T22:26:50Z INFO authclient.API.WhoAmI {\"request\":{}} []\n2019-01-28T22:26:50Z INFO authclient.API.WhoAmI {\"request\":{}} []\n2019-01-28T22:26:50Z INFO authclient.API.WhoAmI {\"request\":{}} []\n2019-01-28T22:26:50Z ERROR pfs.API.GetFile {\"duration\":0.003874548,\"error\":\"mkdir /pach: permission denied\",\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"__spec__\"},\"id\":\"926485a265934415a16602a14d094484\"},\"path\":\"spec\"}}} []\n(repeated three times)\nPropagated to the user container as error getting pipelineInfo: could not read existing PipelineInfo from PFS: mkdir /pach: permission denied. Confirmed: removing that section of code causes the job to run successfully to completion :tada: \nCan be tested by deploying the 1.8.2-2df33038a2e15e53e70b1269438b9681c03bf618 image that is built from the openshift-hacks branch: https://github.com/pachyderm/pachyderm/tree/openshift-hacks\nSpecifically, this is the key change: https://github.com/pachyderm/pachyderm/commit/fd6d5d27109975c84c5de37a3c95f519311e3ad2\nSo if we just add an env var flag to disable this user-switching code, I think we should be good. The pachctl binary for linux corresponding to that image version is here:\npachctl-1.8.2-2df33038a2e15e53e70b1269438b9681c03bf618.tar.gz\nThis is a self-contained go binary, so it should be able to just be un-tarred and placed on the PATH on any linux machine\n. transform.user controls which user the worker exec's the user code as, but it seems we need to disable explicitly exec-ing as a specific user altogether. To clarify, are you trying to run pachctl on the RPi, or run the whole of pachyderm (including kubernetes) there?\nLooks like we had an issue for pachctl support in #1734 which should be fairly straightforward\nSupport for deploying all of pachyderm onto arm, however, is probably going to be a bit more tricky. I have not personally re-visited running on RPi, but I think the best way to approach this would be to break down the steps:\n\nbuild the command line tool (make install)\nMake sure that works (run pachctl version --client-only)\nrun pachctl deploy local\ncheck the deploy with kubectl get all\nensure you can talk to the cluster (pachctl version)\nensure all functionality works, by following the intro tutorial: http://docs.pachyderm.io/en/latest/getting_started/beginner_tutorial.html. yes, sorry, i was totally wrong about that. just updated my comment to hopefully be more helpful. to be clear, are you trying to run the entire kubernetes cluster on the RPi? or just the pachctl client, talking to a remote kubernetes cluster?. @Mccpie01 to clarify, did you manage to get it to compile for arm?. ok, to get the whole of pach running on RPi will require rebuilding the docker images for ARM, in addition to  recompiling the command line client\n\nHave you got kubernetes running on RPi/ARM already? It seems that isn't something that is officially supported by k8s out of the box, though it does seem to be possible: https://github.com/luxas/kubernetes-on-arm. @jdoliner it does seem exec-ing as the randomly-assigned user within OpenShift executes the subprocess as the same user as the parent by default (which certainly makes sense, not sure what else the behavior would be?)\nIs it ever the case that a non-root user would be able to exec as another user? If not, just detecting whether we're root, and throwing an error (or warning?) if the pipeline spec/image has an explicit user set when we're not root might be a better experience in terms of helping pachyderm users figure out wtf is going on when they're debugging a pipeline. If it is possible for other, non-root users to exec as someone else, I imagine there's some way we can check if we have that ability? alternatively, just catching the \"not allowed\" error and surfacing it in a more helpful way (you set the user field in your pipeline spec, but pachyderm has insufficient priviledges to switch users when running pipelines <link to openshift docs talking about restrictions of running as non-root>?)  might be sufficient\nEdit: issue for those docs is here:  #3414. Oh, yes, of course, that makes sense. We just spent some time digging into the issue of having a reason field set, but not transitioning into an error state. Basically, it seems what's going on is that this error is reported in the same way as a number of transient k8s errors.\nOur default behavior is to retry any operation that could be a transient failure (ie that we haven't hard-coded to be fatal), rather than bailing on every network hiccup. What we've just discussed, though, is being more explicit about distinguishing states that are \"good/expected\" vs \"possibly good/attempting recovery\" (today those are mostly conflated). This should almost certainly be available as a config param (#3036) in addition to a flag -- having to add this to every command is a terrible user experience . :+1: filed as #3419. actually, this suggestion from @jdoliner  might be even better: https://github.com/pachyderm/pachyderm/issues/2745#issuecomment-371302004\nThat should still work with new auto-port-forwarding, right?. The list of required changes will be similar to what's needed to run in multiple namespaces (esp. with respect to port changes): #3375\nThe bulk of the changes are those port changes. I don't think that info is in any one place, so consolidating here.\nI believe the changes need to happen in three places\n\nNodePort declarations on services\nport declarations on containers: https://github.com/pachyderm/pachyderm/issues/3333#issue-396121230\nsetting env vars to tell the running pachd process which ports it should be using: https://github.com/pachyderm/pachyderm/issues/3363#issue-399077649\n\n(though i think that list of ports is missing PPS_WORKER_GRPC_PORT (default: 80))\n. :tada: \nIs your thinking with removing the docs changes so that we avoid people on old clients finding our new docs and getting confused when PACHD_ADDRESS doesn't work?\nThat makes sense, though we should really figure out some way to have the docs most people are looking at match released code, rather than HEAD -- in general, it seems like ensuring implementation PRs include their related docs is a lot more likely to ensure our docs and implementation stay in sync longer-term. thanks @setgree !\nI think there are some places in the docs where versions are auto-incremented; seems like this should one of those places. ooh, actually, this looks like it might be a better approach: https://github.com/pachyderm/pachyderm/issues/2745#issuecomment-371302004. does that mean that implicit port forwarding breaks when pach is deployed in a non-standard namespace? (but works if the steps in https://github.com/pachyderm/pachyderm/issues/2745#issuecomment-371302004 have been followed?). but if you've set up your default namespace with kubectl config set-context dev --namespace=development does auto-port-forward use that?. Am I correct that the original source of this requrest related to getting data out of an on-prem object store?\nIf so, kinda related to #2868 (this is the ingress side, that's the egress side). @jdoliner to be clear, you're suggesting adding \"s3api://\" as a new resource type accepted wherever other object store resource types are currently used? (in egress there, in put-file here). There have been a few requests for arm (#1734 ), but hasn't been tackled yet. I haven't done any cross compilation with go, so good to know it shouldn't be hard. @mdaniel to confirm, are these both input repos, or are you copying an output?. Yes, apparently the recent performance improvements to output repos have made it so files can't be copied out of them to input repos. Agree that either these copies should be allowed, or they should fail in a more obvious manner. Yes, dash is accessible with a local deployment by just visiting the relevant IP (usually 192.168.99.100 with minikube iirc), and that generally seems to work better than going through port-forward. My understanding is the problems with suggesting that are (as @ysimonson pointed out):\n1. It's not clear how we find what IP to point people at (in some cases this could maybe come from parsing the kubectl config?)\n2. Even if we find the address of the kubernetes cluster, if this is a non-local deploy, there is not necessarily a host we can point to that will have the relevant port open (I'm actually not sure if this is generally true -- the dash and pachd's grpc endpoint are exposed with NodePorts; if we can reach the k8s API node to do a port-forward, in practice, does this not expose the NodePorts as well as the k8s API endpoint?)\nWhere 2. is definitely a problem is if people have removed the NodePort declarations (since those are a cluster-global resource, this is likely in prod environments or where they're trying to run multiple instances of pach). But in those cases, people hopefully know what they're doing and can set up their own ingress path.\nAlso note, this is all what is hopefully going to be a lot easier once we have a gateway\n. There may be some more details around workflow that we want to include in this, but it seems the bulk of what's needed is docs on how to actually run multiple pachyderm instances: #3375. Yup, think we want to add info about all of those things :P\nFWIW I had filed a docs issue for the config file specifically earlier (#3036) but agree with you that makes a lot more sense in the context of a larger \"API ingress\" doc\nYusuf has some more useful info about auto-port-forward here: https://github.com/pachyderm/pachyderm/issues/3413#issuecomment-459502816\nThere are also some changes that will affect this (and hopefully make it better): https://github.com/pachyderm/pachyderm/issues/3419. Also, turned out the problem of the particular customer this content was originally written for was actually a proxy issue. So we should also document that \"if you're connecting with port-forward, you're using 0.0.0.0 -- if you're using a proxy, make sure it handles that appropriately\". @ysimonson with the new auto-port-forward, I assume pachctl will still fall back to using localhost:30650 if no k8s config is found, meaning if you want to set up some other sort of tunnel, that's where you should be binding it?\nedit:\npretty sure that's the case, since that's what we're doing with openshift: \n``\nPACHD_POD_NAME=oc get pod --output=json | jq -r '.items[] | select(.metadata.name|startswith(\"pachd\")).metadata.name'`  #  -r flag is needed to not get quotes in the output\noc port-forward pod/$PACHD_POD_NAME 30650:1650```. @ysimonson that seems kinda annoying if you want to always use some other tunneling method. did we come to a conclusion on having an env var/config setting for this?. yes, i think so. we may want to revisit the global config vs local config vs envvar question at some point, but that's outside the scope of this issue. Seems there's a bunch of relevant info here: http://docs.pachyderm.io/en/latest/deployment/amazon_web_services/connecting_to_your_cluster.html\nSome of that is actually directly related to AWS deployments, but much (most?) of it is not. @jrvanalstine I'm not sure where you're trying to do here is going to work: the pachd API port you're trying to expose is a grpc server, not a REST API. while grpc connections are technically using http under the covers, it's using advanced/non-standard features of http2 that are not appeared by a lot of web infrastructure (trailers, at least; possibly others).\nSo it's possible (though not guaranteed) that envoy can handle these connections, but I don't know of a way to set the base path of the connection (you're using /pachd/ in your example; both the client and server would need to know about this prefix) and it's certainly not going to work with http 1.1 (which is what the request/reply in the example seem to be using).\nIn general, my experience has been that it's best to treat grpc connections as you would any other opaque tcp-level protocol ie expose it on a dedicated port and don't assume you can use any layer-7/http-based infrastructure with it unless that infrastructure has explicit grpc support (I believe Traefik, for example, does). \nThis is why pachyderm defaults to accessing the API either via a nodeport or port-forwarding. Looks like this may have something to do with the new auto-port-forward feature...\nHave you upgraded the version of pach deployed in your k8s cluster, or just the local client?\nFirst, can you try adding the --no-port-forwarding flag?\nalso, if you've upgraded the deployed version, can you please try accessing it again with the old (<=1.8.2) client?\nThanks!. We do generally want CI to be green before merging, yes. I'm not familiar with this test, but just gave travis a kick to re-run in case it was just a transient network issue. @mdaniel poked. This is actually a bit of a tricky problem, since a commit can be part of multiple branches (or be part of no branch at all). We have an algorithm for this to pick a \"primary branch\" for a given commit in the dash, but it's not an exact science. One option would be to just show a list of all the branches that a commit is part of, another option (used by git) would be to show which branches current point at a commit (ie  branches for which this commit serves as HEAD), but not to show branch info for non-HEAD commits. do you think this should be changed/removed? i agree this is somewhat confusing, but i also think having it's behavior be inconsistent with the other commands' -v seems strange (and afaik we don't actually have  a more verbose version of server-side logs to show). @kevindelgado i think ISO datetime strings (the former) are generally a lot more useful. am i understanding correctly that is that what should be coming out of this, right? https://github.com/pachyderm/pachyderm/pull/3547/files#diff-412ea471cf093eeab2b3162c664da90cR183\n(according to https://golang.org/pkg/time/#Time.String ). I haven't heard any requests for that. I think if someone really needs epoch time, they can pull it out from --raw and convert it themselves with date or whatever. This sounds like it is one part of #2468 (and #3069, which is basically the same thing). Fixed by #3586 . Thanks @gpkc!  Sorry for the delay getting back to this, it LGTM\nThis basically addresses #3570. We may also want to point at the docs that explain how splitting of these other types works (especially given the weirdness with headers), but this is definitely a good first step. i think tags just being a very literal user-defined alias for a commit ID is sufficient (ie the provenance of a tag would just be the provenance of it's commit). So yea, initially I was thinking that using a simplified version of the branch implementation (or even just a \"static\" flag or something on branches) would make sense, but given that definition of a tag is so much simpler than a branch (particularly in pach, because of the provenance, even more so than git), i wonder if it does make sense for them to be implemented in the same way? Re #3582 I wonder if the shared part should just be a general notion of a \"Commit Reference\" that could be a commit ID, a tag name, a branch name, or any of those with various modifiers (^, ~, etc)?\nThe one way in which some notion of \"tag provenance\" could be useful would be to see the provenance of a tag, in terms of other tags (rather than opaque commit ids). But I think just being able to tell which tags point at a given commit (in addition to obviously wanting to know which commit is referenced by a given tag) would be a better way of doing that. I believe git generally shows all tags and branches that point at a commit (in parentheses) pretty much whenever a commit id is printed.. I think the close-bracket should be before the comma here, right? Also, might as well add a link:\nIf you're doing a custom release (off a branch that isn't master), skip to [the section at the bottom](#custom-release). add https:// to github.com/pachyderm/pachyderm/releases so autolinking works. Curious why it doesn't make sense for union inputs to be triggered before all atoms inputs have a commit? Couldn't the pipeline just run as soon as any datums are present?. Is there a reason union inputs couldn't be (optionally) named? It seems in a lot of cases it would be useful to have the files from any of the atom inputs show up in the same directory so my code doesn't have to manually check multiple places (unless I'm missing something, I don't think there could be file conflicts, right?). Do you mean to be doing this set -x here?. Ok, makes sense. I'm running maybeKcCreate() first (and capturing it's output) because it (or, actually, the k8s commands it runs) outputs several lines while doing the deploy, and I want the UI info output to follow that.\nCan definitely move the output code into maybeKcCreate() -- I thought you'd said to duplicate it in each deploy function because you wanted to keep maybeKcCreate() only doing the generation and not pass in any superfluous context\nFWIW, I think ideally the output would happen from the calling code -- where it is now -- but each of these DeployCmd functions would be refactored to be a single parameterized function. That seems like a fair bit of tweaking, though, and it seems Cobra wants these to be individual, so not going to bother for now -- I'll just polute maybeKcCreate() a bit and avoid adding any more code duplication. Maybe move the \"Cluster Created\" message up to the point where the cluster has actually been created, before outputting the address?. the dashboard address changes to localhost:30080 in the next release. the dash address changes to localhost:30080 in the next release (1.5.1; change is already merged on master). Scrape. This seems like it would be useful as a script, right? Maybe split this out?. Small nit: we should probably be prompting for password interactively by default, rather than asking people to input it as part of the command. aren't these equivalent?. apparently dash (ubuntu/debian's bash replacement) doesn't have source, so it fails. sigh.. wrapping the struct seems reasonable to me, but I'm far from the expert on these sorts of stylistic Go issues. Probably best to ask @jdoliner (or another systems team member) if you're really concerned. ",
    "brollb": "I am interested in this as well. I am currently streaming large amounts of input data and I am interested in accumulating it to update a batch clustering model periodically (every hour or so). . Here's another use case:\nSuppose in the tensorflow example, you are streaming training data in but also performing some data augmentation (like reflecting the images or random cropping or something) as well.\nTraining Data Stream -> Data Augmentation -> Training (Batch) Model\nWe may want the first two steps to stream the data as fast as it gets it in (so small commits) but the last step should be run only on larger amounts of data. Otherwise, we would end up training a new model one each combination of the data set (ie, trained model on n elements, trained model on n+1, trained on n+2, etc).\nI think the challenge is that since the data batch size (streaming = batch size 1) of the entire pipeline is dependent entirely on the size of the inputs. Normally this is no problem but if you want to accumulate data, this can be tough.. A similar but slightly different use case:\nSuppose we wanted to run it like an anytime algorithm and just stream massive amounts of data in but always have a model ready (maybe MLAAS or something). In this case, maybe the criteria for running the pipeline would be to start the next training cycle once the current one has finished. That is, always running one and only one training pipeline on the latest data (at least the latest commit when the pipeline started).. I like how completion is handled here (although that specific example is a cli framework for js).\nBasically, just print the completion info to stdout and have the user redirect it to wherever he/she wants. Then, in the docs, I would just direct it to the /etc/bash_completion.d directory. I like the flexibility and simplicity of this approach to shell completion - thoughts?. signed!. This is awesome - thanks!. sounds good - I have a tendency to just fix whatever I see right when I see it... I can probably stop using the easy \"Edit on Github\" button and just add it to my existing PR :). that makes sense. \nMaybe something stating that the job was not run? In my case, I was puzzled about not seeing any logs and had assumed that it ran since I had data in the repo (although I had no parallelizable datum)... It might be nice to have some indication to help people debug bad pipeline jsons..\nI wonder if being able to have a dry run for your pipeline specifications would be useful - maybe something that would tell you some metadata about the creating the given pipeline (such as number of datums, commits to be processed, size of the processed data).. I like being able to process multiple datums in a single job - especially in \"map\" jobs. \nThis is especially relevant when the job has an additional startup overhead. For example, most machine learning (esp. deep learning) predictions/conversions/etc require the loading of the trained model which can be quite large. If we only process a single piece of data at a time, this can add significant overhead to processing the pipeline.\n. this would probably just be running a service but it would be nice if we could pass the job id so we could investigate why the given job id failed. I think this would be especially relevant once we have created an effective pipeline and want to run in on a new dataset (input repo). Currently, you would have to change the repo name and potentially change it in the stdin as well.\nIt might be nice to go one step further and be able to create a new pipeline from an existing spec and just changing the input repos:\npachctl create-pipeline -f original_pipeline.json input_repo1 input_repo2\nOr perhaps more generally just overriding fields in the pipeline spec\npachctl create-pipeline -f original_pipeline.json --set inputs[0].repo.name=input_repo_1 --set pipeline.name=myNewPipeline. Yeah, I think that should work - thanks!\nGood point on the adjustments to the env vars - it is certainly nicer to be able to give them names referring to their role in the pipeline (rather than referring to them by index). It looks like there were two main motivating reasons for #1543: avoiding duplicate processing and being able to see the first couple finished jobs quickly.\nAvoiding duplicate processing should be able to be handled by the glob patterns - that is, we can figure out what the new data is in each commit by diffing the datum files (basically what @JonathanFraser said in #1543).\nI am assuming that seeing the finished jobs quickly is important for testing a pipeline (and iterating on it to make it work). For getting quick results of finished jobs, it might work to use things like the from field to get a quick sanity check for if the pipeline was effective. If you are not getting quick results to test a pipeline, I don't see the value of trading the perf of completing a single commit for the perf of completing the processing of an entire repo. Is there another use case for finishing jobs quickly that I am missing? \n. This seems to be closely related to the problem of representing computation dependencies. Prior to #1543, there was no dependency between computation of the dependencies. After, there were implied dependencies between every commit. I wonder if there is a better way to represent the dependencies. They should certainly be associated with a pipeline (as they certainly depend on the computation being performed - not the commits/data).\nCurrently, it seems that the only options for this are \"in order\" or \"not in order\" (for a given pipeline, of course) though I wonder if there is any other option that people may need.... ",
    "erikreppel": "Hey Sean,\nI'm working off a pull of the master branch from this morning with Go version 1.6, GO15VENDOREXPERIMENT was not set.\nI ran into this: https://github.com/pachyderm/pachyderm/blob/master/src/client/pfs/pfs.pb.go looks like the generated pfs file is calling shard from server. There might be a dependency of shard that calls the vendor version of trace.\nHere's the repo of pull_data. The method I used for connecting is the same, although I played with a few other things after I got the connection working.\nTotally understandable. Have you guys looked at something like gopkg.in for taking specific versions?\nFor now while prototyping I'll keep removing vendor until a better solution comes together.\n. Awesome, sounds good.\n. Hey @jdoliner,\nyes it was an instance of k8s that had Pachyderm running on it before.\nI restarted kubernetes and ran the tests (which failed), then tried again to launch a dev cluster, but after a minute or so in a crash loop it now seems to be stable. I'll try changing the pull policy if I run into this again, but for now it seems to be working (I think you guys build the docker images in the make test script somewhere right? Could be what fixed it)\n. That being said tests in make test still fails some of the tests\n. ```\nreplicationcontroller \"pachd\" created\nwait for the pachyderm to come up\nif we can call the list repo, that means that the cluster is ready to serve\nuntil timeout 5s /home/erik/dev/go/bin/pachctl list-repo 2>/dev/null >/dev/null; do sleep 5; done\nCGOENABLED=0 go test ./src/server -timeout 300s\n--- FAIL: TestLogs (24.11s)\n        require.go:121: /home/erik/dev/go/src/github.com/pachyderm/pachyderm/src/server/pachyderm_test.go:181\n        require.go:123: No error is expected but got rpc error: code = 2 desc = container \"user\" in pod \"fe4d3ad1635140d092004c84a9a5153f-55z8h\" is waiting to start: ContainerCreating\n--- FAIL: TestGrep (31.00s)\n        require.go:121: /home/erik/dev/go/src/github.com/pachyderm/pachyderm/src/server/pachyderm_test.go:233\n        require.go:123: No error is expected but got rpc error: code = 4 desc = context deadline exceeded\n--- FAIL: TestJob (31.21s)\n        require.go:121: /home/erik/dev/go/src/github.com/pachyderm/pachyderm/src/server/pachyderm_test.go:78\n        require.go:123: No error is expected but got rpc error: code = 4 desc = context deadline exceeded\nFAIL\nFAIL    github.com/pachyderm/pachyderm/src/server       32.912s\nmake: *** [integration-tests] Error 1\n```\nWas the output\n. Ahh gotchya, thanks for the help!\n. @dwhitena run make docker-build to re build the pachd to the latest version, that got it working again for me\n. Yeah I'm more surprised that my code wasn't working but pachctl inspect-file works. pachctl seems to also pass FilterShard with 0 for everything too: https://github.com/pachyderm/pachyderm/blob/master/src/server/pfs/cmds/cmds.go\n. good to know!\n. Yeah exactly.\nThe purpose of this was to see the case of a single number has been produced as the result of prior calculations and stored in a repo and we want to scale every point of data from a file in a different repo by that number since a really common thing we do.\nAny ideas on how to efficiently do this?\nAs an aside I noticed the other day that if you specify \"shard\": 3 on a reduce pipeline in the logs it appeared spin up 3 containers, all of them seeing files, but only one of the containers having files with actual data in them which kind of surprised me. Is that expected? I assumed it wouldn't happen since you don't spin up empty containers.\n. Its useful from an efficiency standpoint but those kinds of cross product operations are pretty common. For us at least I think cross product type of operations would be the most common thing we do from a processing standpoint.\nThe ideal behavior to me would be that I can set a flag for an input to a pipeline to be globally available so it appears in every container spun up by the pipeline.\n. @jdoliner ran make assets\n. Never mind, can't operate on closed commits, have to open a new commit then delete the file.\n. @jdoliner Yup! \nIt was actually an error caused by not vendoring grpc properly, I was getting panic: http: multiple registrations for /debug/requests here too, for some reason Godeps didnt like updating the vendored version of grpc but deleting my vendor folder and updating grpc on my GOPATH worked.\n. ",
    "lavvy": "+1. Please what is the status of this feature. Has looks like rethinkdb already got a driver to pachyderm? \n. Oh OK.  I just came across this project today and i think it almost\nsummarizes how data should be treated.\nAnd i feel that every data is a file somehow. The two major types of data\norganizations are filesystem and database.\nBigger files live in filesystem and take more time to pull but smaller\nfiles live in database and are faster to access.\nIf the two of them (fs and db) can dump their files in one platform. I\nthink it will be easier for us.\nI had been using mysql but been planning to migrate to a nosql cos i\nbelieve database should be treated as a file too. Until i saw this project\nwith pipelines and a document file database (rethinkdb ). I just felt the\nconvergence  could be achieved between these three.\nOn Aug 26, 2016 9:31 PM, \"Joe Doliner\" notifications@github.com wrote:\n\nHi @lavvy https://github.com/lavvy, Pachyderm uses rethink internally,\nbut there's no connector for it yet. As of now this feature hasn't been\nscheduled for a release, thank you for +1ing it helps us know to prioritize\nthis feature. One of the reasons we haven't scheduled this feature yet is\nthat we don't feel like we fully understand the use cases for it. If you\nwouldn't mind giving us a few more details about what you'd like to do with\nthis feature it'd be super helpful for designing it. In particular:\n- what database are you interested in connecting to?\n- what would you expect this data to look like in pachyderm's\n  filesystem?\n- what are you looking to do with the data once it's in there?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/243#issuecomment-242887588,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGsC43sg56PYusYBrCES7qNtPLnEP2GQks5qj5NVgaJpZM4H8Iqh\n.\n. @jdoliner from https://www.youtube.com/watch?v=kRjk_Xsf7t4 . it seems you can plug into pachyderm from rethinkdb. Is it still a hack or does it have official support ?\nOur use case is to be able to write a job that can crunch data from both database and file system and running within a particular commit or tag. \n\nif a database driver or pipeline can actually be serving from pfs will be so awesome, that means you can always keep  timeline versions of the full app ecosystem.\n. ",
    "lazuhrus": "@jdoliner Is there any update on this? Looking to do something similar with an oracle database, but wanted to know if this was in the works before we started rolling our own.. ",
    "stale[bot]": "This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. This issue has been automatically marked as stale because it has not had any recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n. ",
    "hsaputra": "It is from \"Deploying Pachyderm.\" link in the README.md file\n. ",
    "jpoon": "A suggestion is to add a completion command to pachctl similar to kubectl completion. The command can spit out a bash/zsh completion file that you can load. \nLooks like Cobra has a built-in ability to generate bash completions:  https://github.com/spf13/cobra/blob/master/bash_completions.md\n. Do the vendored dependencies also need to be updated to 1.4? https://github.com/pachyderm/pachyderm/blob/master/src/server/vendor/vendor.json#L1218, looks like it is pointing to somewhere between 1.2 and 1.3. \n(I'm a go-newb so sorry if this is a stupid question)\n. Awesome. Thanks @jdoliner! Coincidentally, updating the k8s vendored dependencies to 1.4 will be needed for the Azure work (https://github.com/pachyderm/pachyderm/issues/960) as data-disk volume support which is needed for the etcd+rethinkdb pods was introduced 1.4.\n. @jdoliner, ETA on getting this merged? It's needed for volume mounting portion of https://github.com/pachyderm/pachyderm/issues/960. \n. @jdoliner Done. \n. > data disk for etcd, should be identical to the situation for rethink\nis this separate of the etcd cluster needed for kubernetes?\n. How do I deploy my modified pachd image? \nI tried:\n1. make docker-build to build the necessary docker images on my local dev machine\n2. pachctl deploy microsoft (... and some other flags I added to the cmd line ...) --dev which makes it use a pachyderm/pachd:local image\nI assumed that there would be some magic to grab my local docker image and push it to my k8s cluster running on Azure, but the pods fail to pull said image :(. Do I need to setup my own docker registry and push that image up there? \nAlternatively, is there a better way to test my changes?\n. My code changes to add Azure storage support worked on the first try which makes me super suspicious. Are there automated tests I can run against my deployment?\n. With https://github.com/pachyderm/pachyderm/pull/974, that should be the last of it. Looking at the kubernetes manifest, the etcd services are using a hostpath volume so no further changes are needed there.\n. Closed with https://github.com/pachyderm/pachyderm/pull/974\n. @jdoliner Awesome possum. When should we expect the next patch release? As mentioned on the email, it'd be great to have one with these bits prior to Nov. 7 so no rush. \n. Travis failed with:\n--- FAIL: TestFlushCommitAfterCreatePipeline (6.74s)\n    require.go:153: /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/pachyderm_test.go:2006\n    require.go:155: No error is expected but got commit TestFlushCommitAfterCreatePipelinePipelinef2cf16a68ae4/TestFlushCommitAfterCreatePipelinePipelinef2cf16a68ae4:79b21857023448f58a058fab7db0a61d:9 was cancelled\nThis test passes on my k8s cluster running on Azure. I can't see how my changes could have affected a hyper-kube deployment. Pushing a random change to force another build. fingers crossed hoping the earlier failure was a transient one.\n. Nah, that it's for this PR so it's ready for review @jdoliner! I'm planning to split the work for #960 into several PRs: \n- this one (#969) adds support for writing/reading to Azure blob storage\n- separate PR for using an Azure data disk for rethinkdb/etcd which is dependent on k8s 1.4\n- documentation changes/make targets, etc\nLet me know if you want me to make a giant one instead. \n. > Do you think it'd be possible to add the Dockerfile you used to create the container to this PR under etc/microsoft? \nAbsolutely.\nShould be ready for review by tomorrow. \n. @jdoliner All good for a review. One additional thing that I want to do is reorganize the 'deploying on the cloud' documentation, there's a fair bit of duplication as it's currently organized by cloud provider.\nBefore:\n1. GCE\n   1. Install Prereqs\n   2. Deploy Kubernetes\n   3. Install Pachctl\n   4. ...\n2. AWS\n   1. Install Prereqs\n   2. (same as GCE list)\n3. Azure\n   1. (same as GCE list)\nAfter:\n1. Install Prereqs\n   1. Specific instructions for GCE/AWS/Azure\n2. Deploy Kubernetes\n   1. GCE/AWS/Azure instructions\n3. ...\n. On second thought, doing it that way might be hard to read. I'll leave it as it is unless you think there's a better way to organize it.\nI was initially thinking to organize it like how Kelsey Hightower did in his post: https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/README.md\n. @jdoliner merged in master and CI looks good!\n. Thanks @jdoliner. \n. Thanks for the quick merge! \ud83d\udc6f \n. The work entailed here should be pretty easy as it's all do-able through the CLI: https://docs.microsoft.com/en-us/cli/azure/disk?view=azure-cli-latest#az_disk_create\nI'll take a look at this next week.. Closed with https://github.com/pachyderm/pachyderm/pull/2499. We can make things easier by adding support for managed disks (ref: https://github.com/pachyderm/pachyderm/issues/2136). The work to do so is fairly trivial.\nAnother option is to push that particular image to DockerHub instead of asking folks to build it on their machines? . The azure instructions are specifying static-etcd-volume argument for pachctl deploy. We can simplify the instructions there to use the new CLI commands.\nI noticed there's a new dynamic-etcd-nodes flag. How are these disks provisioned? If it's through storage class/pv/pvc's, it should just work on Azure.. If you deploy a cluster through the latest AKS, you get storage classes by default:\n$ kubectl get storageclass\nNAME                PROVISIONER\ndefault (default)   kubernetes.io/azure-disk\nmanaged-premium     kubernetes.io/azure-disk\nmanaged-standard    kubernetes.io/azure-disk\nOtherwise, you can create them manually. Then if you deploy with the --dynamic-etcd-nodes it'll just work.\n```\n$ kubectl get pv\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                         STORAGECLASS   REASON    AGE\npvc-29de5018-c19c-11e7-9204-000d3a14317b   10Gi       RWO            Delete           Bound     default/etcd-storage-etcd-1   default                  1d\npvc-4d89bfdd-c19e-11e7-9204-000d3a14317b   10Gi       RWO            Delete           Bound     default/etcd-storage-etcd-2   default                  1d\npvc-ebb25a97-c19b-11e7-9204-000d3a14317b   10Gi       RWO            Delete           Bound     default/etcd-storage-etcd-0   default                  1d\n```. @dwhitena @jdoliner are those test failures my doing?. Previous link to https://github.com/pachyderm/pachyderm/blob/master/SETUP is dead. Couldn't find a similar looking \"SETUP\" file so deleting it all together\n. proto-compilation looked to be fixed and merged. Deleting this comment as it may no longer be relevant.\n. Oops. Sorry for the confusion. Yep, workaround until k8s 1.4 + Azure data-disk support (which is the next PR I'll work on)\nActually, not sure if this change is entirely necessary but it's purpose was to use the local host path for the time being. \n. ",
    "jbowles": "Thanks guys.\nI did this \nsh\ngo get github.com/pachyderm/pachyderm\ncd $GOPATH/src/github.com/pachyderm/pachyderm\nmake install\nAnd everything looked good, the binaries pach-deploy, pachctl, pachctl-doc installed and there were no error messages.\nI can also confirm that go get github.com/pachyderm/pachyderm/src/client returns no errors; and make deps-client returns no errors.\n. l\n. awesome, thanks\n. ",
    "dwhitena": "@sjezewski @derekchiang Thanks.  Here is the output of kubectl version:\nClient Version: version.Info{Major:\"1\", Minor:\"0\", GitVersion:\"v1.0.1\", GitCommit:\"6a5c06e3d1eb27a6310a09270e4a5fb1afa93e74\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.0\", GitCommit:\"5cb86ee022267586db386f62781338b0483733b3\", GitTreeState:\"clean\"}\nand gcloud version return:\ngcloud: command not found\nis gcloud a dependency?\n. I'm using the make launch-kube command to start kubernetes via docker.  Maybe I need to blow away the old images and retry.\n. @sjezewski Ok I will try later today and I will also apt-get FUSE.\n. @sjezewski One thing to note is that the make launch-kube command will start kubernetes as mentioned above.  That is:\nClient Version: version.Info{Major:\"1\", Minor:\"0\", GitVersion:\"v1.0.1\", GitCommit:\"6a5c06e3d1eb27a6310a09270e4a5fb1afa93e74\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.0\", GitCommit:\"5cb86ee022267586db386f62781338b0483733b3\", GitTreeState:\"clean\"}\nSo if this a problem for the quickstart, you may want to update start-kube-docker.sh.  Working on updating kubernetes now.\n. Ok, updated the kubectl binary.  Now I have:\nClient Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.0\", GitCommit:\"5cb86ee022267586db386f62781338b0483733b3\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.0\", GitCommit:\"5cb86ee022267586db386f62781338b0483733b3\", GitTreeState:\"clean\"}\nAnd the following seems to work:\nkubectl create -f http://pachyderm.io/manifest.json\nI also updated my fuse.conf based on the above, and now when I am trying to mount:\ndwhitena@dirac:pachyderm$ pachctl mount &\n[1] 16663\ndwhitena@dirac:pachyderm$ mkdir /pfs: file exists\n^C\n[1]+  Exit 1                  pachctl mount\ndwhitena@dirac:pachyderm$ ls /pfs\nls: cannot access /pfs: Transport endpoint is not connected\nSuggestions?\n. @jdoliner Thanks.  sudo pachctl mount returns:\nmkdir /pfs: file exists\nprobably because I manually created it and changed permissions earlier trying to get it to work.  If I try to rm -r or ls that directory I always get:\nrm: cannot remove \u2018/pfs\u2019: Transport endpoint is not connected\nor\nls: cannot access /pfs: Transport endpoint is not connected\nSo I did:\nfusermount -uz /pfs\ngot rid of /pfs.   And upon retrying sudo pachctl mount, it just hangs (assuming because I didn't run in the background).  But if I ls /pfs in another directory I can see foo.  \nSo success, I guess?\n. Sweet.  Thanks guys!\n. Sorry for the delay and confusion here @haps-basset. Looks like this issue slipped through the cracks a bit, but we are more than happy to help you work through this. If you do try again, please paste you result here and/or let us know what questions come up in our public Slack channel: http://slack.pachyderm.io/.\nRegarding your Openshift questions:\n\n\nRegarding users, you should be able to deploy Pachyderm from anywhere that you can access your cluster (i.e., via oc).\n\n\nRegarding the persistent disk, this would be a \"persistent volume\" is OS terms. More information here: https://docs.openshift.com/enterprise/3.1/dev_guide/persistent_volumes.html\n\n\nRegarding the object store, this could be any S3 compliant object store solution like Minio, Rook, Ceph, Swift, etc. or any one of the cloud provider's object store solutions like S3, GCS, or Azure blob storage.\n\n\nRegarding the object store args, this will depend on what object store you are using. Can you give a little more information about which object store you are using?\n\n\nNote, the custom deploy is meant out-of-the-box to support clusters deployed in the cloud, but with a non-cloud-provider object store (like Minio). As such, you will likely have to do what is mentioned here in terms of running the deploy with --dry-run and then modifying your manifest according if any adjustments need to be made for your particular k8s/OS deployment.\nLet us know which parts of this don't make sense, and we can definitely provide more details and help.. Hi @haps-basset, thanks for the update, and sorry for the issues with deployment. I am running through some OpenShift deployment stuff over the next few weeks, and I'm hoping to update the docs accordingly for future users. Please reach out any time if we can be of assistance with future deploys.. @jdoliner Attached is the output of kubectl logs k8s-master-127.0.0.1 apiserver, because just running kubectl logs k8s-master-127.0.0.1 returns:\nError from server: a container name must be specified for pod k8s-master-127.0.0.1, choose one of: [controller-manager apiserver scheduler setup]\nlog.txt\n. Having the same issue here:\nNAME                   DESIRED      CURRENT            AGE\netcd                   1            1                  21h\npachd                  2            2                  21h\nrethink                1            1                  21h\nNAME                   CLUSTER-IP   EXTERNAL-IP        PORT(S)                        AGE\netcd                   10.0.0.202   <none>             2379/TCP,2380/TCP              21h\nkubernetes             10.0.0.1     <none>             443/TCP                        1d\npachd                  10.0.0.53    nodes              650/TCP,750/TCP                21h\nrethink                10.0.0.47    <none>             8080/TCP,28015/TCP,29015/TCP   21h\nNAME                   READY        STATUS             RESTARTS                       AGE\netcd-w1qmp             1/1          Running            0                              21h\nk8s-etcd-127.0.0.1     1/1          Running            0                              1d\nk8s-master-127.0.0.1   4/4          Running            4                              1d\nk8s-proxy-127.0.0.1    1/1          Running            0                              1d\npachd-2yjyp            0/1          CrashLoopBackOff   14                             21h\npachd-m6sfg            0/1          CrashLoopBackOff   14                             21h\nrethink-5a0uf          1/1          Running            0                              21h\nWhere do I put the modified manifest file you mention above?  Where is the one it is currently using?\nThanks.\n. Irrelevant until PPS/PFS docs are brought back to life.  Tracked here: https://github.com/pachyderm/pachyderm/issues/1869. Yeah, I can revisit this release cycle @JoeyZwicker. . I'm changing this issue to track a SQL cookbook, as I think this is what @JoeyZwicker and I have discussed. Also, I'm moving this off of the 1.7 milestone, as I don't think it is high priority.. Already restructured.  New structure and updates tracked here: https://github.com/pachyderm/pachyderm/issues/1869. Not relevant at the time being until PPS and PFS docs are re-done.  Tracked here: https://github.com/pachyderm/pachyderm/issues/1869. @JoeyZwicker adding to the 1.5 check list.. Now being tracked here: https://github.com/pachyderm/pachyderm/issues/1869. +1 for this functionality.  A huge improvement to the workflow.  If no one gets to it before me, I'm going to look through this code one night.  Thanks.\n. Done. Closing.. Not in the API anymore, correct?  Closing.. @derekchiang This is awesome.  A few questions:\n- How does one know they committed \"erroneous data\"?  Just by the pipeline failing or by obtaining unexpected results?  Would it be nice to have some sort of head functionality to observe samples of the commits in/out of pipelines?  Something less than git-file and more than list-commit.\n- Is it true that pachctl update-pipeline --no-archive updates the pipeline images and waits for new commits of data?  This combined with pachctl update-pipeline address many of my workflow issues. \n. @derekchiang I think this is already enabled to some extent, because we can mount PFS.  I'm imagining that most data issues have something to do with weird formatting, missing data, mixed up types, etc. that are visible in a quick look at the data (e.g., via head).  And I'm imagining the case of tabular data more so than JSON data (which is so common in data science).\nget-file is nice, but pumps out the entire contents right?  It might be nice to look at only some of a file (just to get an idea about how it looks).  However, I guess we don't have to reinvent the wheel, as long as we can do unix-like stuff on the mounted files.\n. +1 from several users in users Slack.. Had a user recommend this as well:\n```\nuser [8:56 AM] \nCheck out Draft as well by Microsoft.\n[8:56] \nHelm is for releasing. Draft is for development :slightly_smiling_face:\n[8:58] \n@dwhitena See https://github.com/Azure/draft\n```. Yes, I think we can close this now @Vlaaaaaaad. Thanks for the reminder! This chart covers the major deployments, both cloud and local.. +1 for a way to re-run pipelines without having to explicitly specify tags, or at least having a way to do this without having to restart pachyderm.  This would naturally be something I would want as a data scientist iteratively improving/developing an analysis.  It's not so much that I couldn't use tags to solve the problem, but I have to exert extra effort in tagging, and keeping track of tags for something that I just want to rerun with an update to the image.  . I think there are two issues at play here:\n\nCreating truly reproducible analyses\nA workflow for developing an analysis\n\nOn the first point, @JonathanFraser no doubt code should be versioned.  However, in my opinion, \"tags\" for images are not sufficient for reproducible analysis.  Tagging is relying on a human to use good tags.  Similar to having to remember the naming of files like that_analysis_from_last_week_when_it_worked.py.  Further, tagging does not tie a specific image to a specific run of a pipeline.   For true reproducible analysis, images used during each run of a pipeline should be versioned or tracked with that pipeline run, but I don't think we should rely on humans to do that versioning. \nOn the second point, my thought is that, during development (i.e,. before production use), it is reasonable to want to re-run a pipeline with a newly created image, without having to constantly be creating new tags.  \nHowever, good points made by all here.  Just wanted to throw my thoughts into the mix, as this is something that has come up frequently as I work through examples.. @JonathanFraser Yes, that type of tagging system is very nice.  And, yes, the main case I'm thinking about is when you are just trying to get something working with a lot of tweaking to, e.g., a new model.\nMaybe @jdoliner @derekchiang could comment about the other ideas.  I think the issue of correlating a specific image with a specific run of a pipeline is important.  Once someone updates the pipeline spec, it seems that confusion could be created around which image was used on which runs of a pipeline.  I know I have talked with @jdoliner and others options for versioning images within Pachyderm, but there are open questions there.. Great comments @JonathanFraser.  Don't have much to add.  However, I did realize that the --update-images flag on create-pipeline or update-pipeline solves my development workflow issue.  So it seems that this is no longer a problem for me.. @JoeyZwicker No other mentions, as far as I can tell.  Closing.. Between that issue from @jdoliner and some debugging, I was able to determine that this had nothing to do with Pachyderm or pachctl.  I had changed some setting to my local Docker login, and we trying failed logins when pushing images.  . I experienced a similar logical blocker on a workflow.  I want to commit a new state of a JSON file (e.g., the output of an API) where certain fields change each day.  This then should trigger the update of some metrics in a corresponding output file in the output repo.  \nThis doesn't work (as expected) when you naively put-file the file, of the same name, because it simply appends.  My workaround flow for now is to:\n\nstart commit\ndelete the old file\nput the new file\nfinish commit.\n\nThis along with setting \"overwrite\": true in the pipeline spec producing the behavior I want, I think.  However, this wasn't necessarily intuitive and maybe I'm still doing things that have unfortunate consequences.  Still to be seen.. This is documented (at least for a start) in the pipeline spec docs.  Closing for now until we want to expand:\n\n. Ok @sjezewski, I added some expansions to the readme.md which I think expands on the story.  I'm going to read this through again in the morning and address any gaps, but let me know if you see anything else.  We will likely need to update this as well when we do the full blog post.. @JoeyZwicker is this still needed?  What is the use case?. @sjezewski note that supporting this https://github.com/pachyderm/pachyderm/issues/1883, would seemingly solve this problem.. This has all been updated.  Closing.  Correct me if I'm wrong.. @jdoliner any updates here? Had a couple users asking for this recently. . Is this still needed @sjezewski?. https://github.com/pachyderm/pachyderm/pull/1263 addresses the docs portion of this.  I opened an issue https://github.com/pachyderm/www/issues/14 on www to fix the web portion.. I think this is resolved or irrelevant.. @jdoliner Thanks.  Good to know.  I will assign someone from now on.  It should be ready to merge.  CI was passing, I think I just need to re-run it.. Addressed partially here: https://github.com/pachyderm/pachyderm/pull/1967. @JoeyZwicker Are any of these things high priority for?  Many of them have been addressed and/or standardized.. I'm going to close this for now.  I think that it's low priority and can already be deduced in many places.. Added in the pipeline specification for now:\n\n. Ok, I did the following based on this PR:\n\nbuild pachctl with make install\nbuild the docker images with make docker-build\nre-tagged the docker images with a custom tag and pushed them to my docker hub\ncreated a local minio instance via Docker.\nran ~/go/bin/pachctl deploy s3compatible docker <id> <secret> <end_point> --dry-run > test_minio.json to create a manifest for the deploy with minio.\nmodified the test_minio.json manifest to pull my custom docker images for pachd and job-shim\nminikube start\nkubectl create -f test_minio.json\n\nAfter this, it appears that the cluster is healthy:\n```\nNAME               READY     STATUS    RESTARTS   AGE\npo/etcd-41dk2      1/1       Running   0          12m\npo/pachd-j38gj     1/1       Running   2          12m\npo/rethink-shidc   1/1       Running   0          12m\nNAME         DESIRED   CURRENT   READY     AGE\nrc/etcd      1         1         1         12m\nrc/pachd     1         1         1         12m\nrc/rethink   1         1         1         12m\nNAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)                                          AGE\nsvc/etcd         10.0.0.49            2379/TCP,2380/TCP                                12m\nsvc/kubernetes   10.0.0.1             443/TCP                                          14m\nsvc/pachd        10.0.0.122          650:30650/TCP,651:30651/TCP                      12m\nsvc/rethink      10.0.0.18           8080:32080/TCP,28015:32081/TCP,29015:30838/TCP   12m\nNAME              DESIRED   SUCCESSFUL   AGE\njobs/pachd-init   1         1            12m\n```\nSo, then I tried creating a repo and committing some data:\n$ ~/go/bin/pachctl create-repo testminio\n$ ~/go/bin/pachctl list-repo\nNAME                CREATED             SIZE                \ntestminio           4 seconds ago       0 B \n$ ~/go/bin/pachctl put-file testminio master blah -c -f README.md \n$ pachctl list-repo\nNAME                CREATED              SIZE                \ntestminio           About a minute ago   9.922 KiB\nThat all looks good.  However, nothing shows up in Minio (even after a refresh):\n\nAnd, looking at the pachd logs with kubectl logs <pachd pod name> reveals the following:\n2017-01-20T21:04:21Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPIServer.Local\",\"method\":\"PutBlock\",\"duration\":\"0.000s\"}\n2017-01-20T21:04:21Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPIServer.Local\",\"method\":\"PutBlock\",\"response\":\"block_ref:\\u003cblock:\\u003chash:\\\"jWbSXl4TnwvJCf3hto9Ou0rECexi987etKKtIieYYQZy1yRyy9jT-iZHjZhY2KlSWpdRaVw8yYml6CgHrOHIDw==\\\" \\u003e range:\\u003cupper:5080 \\u003e \\u003e \",\"duration\":\"0.000496264s\"}\nPachyderm is still using the local filesystem, not minio.  So checking the manifest again (which I should have done to start) reveals, that indeed the deploy command didn't properly set the backend variable:\n432               {\n433                 \"name\": \"STORAGE_BACKEND\"\n434               },\nSo, I then manually set this as:\n{\n                \"name\": \"STORAGE_BACKEND\",\n        \"value\": \"S3COMPATIBLE\"\n              },\nThen, I restarted my k8s cluster, and repeated the above steps to deploy pachyderm with the new manually modified manifest.  This results in:\n```\nNAME               READY     STATUS             RESTARTS   AGE\npo/etcd-tdn6h      1/1       Running            0          4m\npo/pachd-zc3jg     0/1       CrashLoopBackOff   5          4m\npo/rethink-bq2w5   1/1       Running            0          4m\nNAME         DESIRED   CURRENT   READY     AGE\nrc/etcd      1         1         1         4m\nrc/pachd     1         1         0         4m\nrc/rethink   1         1         1         4m\nNAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)                                          AGE\nsvc/etcd         10.0.0.132           2379/TCP,2380/TCP                                4m\nsvc/kubernetes   10.0.0.1             443/TCP                                          5m\nsvc/pachd        10.0.0.112          650:30650/TCP,651:30651/TCP                      4m\nsvc/rethink      10.0.0.192          8080:32080/TCP,28015:32081/TCP,29015:32604/TCP   4m\nNAME              DESIRED   SUCCESSFUL   AGE\njobs/pachd-init   1         1            4m\n```\nChecking the logs we get:\n$ kubectl logs pachd-zc3jg\n2017-01-20T21:25:24Z INFO  shard.StartAssignRoles {}\nopen /amazon-secret/bucket: no such file or directory\nIt appears that it's not finding the bucket file.  However, the secret is definitely in the manifest:\n{\n  \"kind\": \"Secret\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"name\": \"amazon-secret\",\n    \"creationTimestamp\": null,\n    \"labels\": {\n      \"app\": \"amazon-secret\",\n      \"suite\": \"pachyderm\"\n    }\n  },\n  \"data\": {\n    \"secret\": \"cEFMTlFtZzh4N0JHbk14MkRFSWh2eVlZMjZDcVVwam9RV2F6NFVwYg==\",\n    \"endpoint\": \"aHR0cDovLzE3Mi4xNy4wLjM6OTAwMA==\",\n    \"secure\": \"MA==\",\n    \"bucket\": \"ZG9ja2Vy\",\n    \"id\": \"SDZVMUNSWFVKNUFYU0ZLREdPMlA=\"\n  }\n}\nAll in all, there seems to be two minor issues here (which I expect will be easy fixes):\n\nEnable pachctl deploy... to populate the correct backend variable in the manifest\nFix the passing of the minio secret to pachd. Yes, normally the deploy pulls the images corresponding to your release version.  However we are trying to do something custom.  You will need to put the images in some registry that pachyderm can pull from.  I will pushed mine up to my docker hub.  Then you can replace pachyderm/pachd and pachyderm/job-shim in the manifest (my test_minio.json) with your images.  In my case dwhitena/minio-pachd etc.  Is there an easy way to build and deploy custom pachd @jdoliner?. Ok @harshavardhana, we have Pachyderm running backed by Minio:\n\n\nAfter updating pachctl and the docker images to your latest and redeploying, I was able to successfully deploy the cluster locally with minikube and minio.  However, upon trying to commit data into Pachyderm, I was getting:\n$ ~/go/bin/pachctl put-file testminio master blah -c -f README.md \nGet http://127.0.0.1:9000/docker/?location=: dial tcp 127.0.0.1:9000: getsockopt: connection refused\nThe problem here was that minikube is running in a VM locally, and thus it was seeing 127.0.0.1 as the VM IP.  In order to fix this, I did our minio deploy with an endpoint of 10.0.2.2:9000, where 10.0.2.2 is the IP that virtualbox uses to access the actual localhost.  \nWhen I did that.  Boom!  Everything works great.  I can commit data in and it goes right into Minio.  Great work!\nWe need to resolve the following points before or after merging this:\n\nThis minio deploy works for a locally deployed pachyderm.  However, we really want to deploy our clusters to the cloud.  This brings up an interesting question, because we could have a minio deploy to AWS, Google, or Azure.  The deploy should be very similar, but we need to decide if we want 4 different commands (minio-local, minio-aws, etc.), or if we make subcommands etc.\nWe will need to add some quick docs to this, which I can tackle.. After talking with @jdoliner the plan for next things are:\n(1) merge this local version (when fully reviews and tests pass) \n(2) work on a PR to re-organize the commands to support all the combinations we have now (as mentioned by @harshavardhana above)\n(3) possibly replace the existing amazon SDK stuff with the minio client (as it is duplicate logic). This is fixed, right @sjezewski?. Definitely agree with your comments.  It seemed crazy that this race, if that's what it was, was taking things down and messing up my forwarding so much.  I can write a bug report.. err before creating an invalid service: https://github.com/pachyderm/pachyderm/issues/1291. Yes, @JoeyZwicker.  Corrected.. @jdoliner Great points.  I agree that --persistent-disk and --object-store are the best ways to describe the combinations.  Also, I agree that keeping the other defaults is useful.  I want to revise the options as:\n\ndefault local deploy (currently pachctl deploy local):\npachctl deploy local\nPure Google cloud deploy (currently pachctl deploy google):\npachctl deploy google\npachctl deploy --persistent-disk google-disk --object-store gcs\n(these would be equivalent)\nGoogle cloud persistent disk + S3 compatible object store (no current equivalent). This could be Minio, S3, or any other object store with an S3 compatible API:\npachctl deploy --persistent-disk google-disk --object-store s3 <id> <secret> ...\nPure AWS cloud deploy (currently pachctl deploy aws):\npachctl deploy aws\npachctl deploy --persistent-disk aws-ebs --object-store s3 <id> <secret> ...\nAWS cloud deploy + other S3 compatible object store (no current equivalent):\npachctl deploy --persistent-disk aws-ebs --object-store s3 <id> <secret> ...\nand etc.... Thinking that to keep the structure consistent and more readable we add custom:\ndefault local deploy (currently pachctl deploy local):\npachctl deploy local\nPure Google cloud deploy (currently pachctl deploy google):\npachctl deploy google\npachctl deploy custom --persistent-disk google-disk --object-store gcs\n(these would be equivalent)\nGoogle cloud persistent disk + S3 compatible object store (no current equivalent). This could be Minio, S3, or any other object store with an S3 compatible API:\npachctl deploy custom --persistent-disk google-disk --object-store s3 <id> <secret> ...\nPure AWS cloud deploy (currently pachctl deploy aws):\npachctl deploy aws\npachctl deploy custom --persistent-disk aws-ebs --object-store s3 <id> <secret> ...\nAWS cloud deploy + other S3 compatible object store (no current equivalent):\npachctl deploy custom --persistent-disk aws-ebs --object-store s3 <id> <secret> ...\nand etc.... This has been fixed by https://github.com/pachyderm/pachyderm/pull/1371. @JoeyZwicker Already fixed in the 1.4 docs branch. Closing.  Fixed.. I have tested this with Google + Minio.  Need to test with AWS + Minio and Azure + Minio.. Tested with AWS + Minio.  Just need Azure now.. After a bit of pain, this is now confirmed working with Azure.  Ready for review!. Ok @jdoliner and @sjezewski.  I added the specific tag.  Thanks!. @JoeyZwicker here are some structure suggestions based on my digging around in other people's docs:\nGetting Started\n\u251c\u2500\u2500 Local Installation\n\u2514\u2500\u2500 Beginner Tutorial\nPachyderm Fundamentals\n\u251c\u2500\u2500 Getting data into Pachyderm\n\u251c\u2500\u2500 Creating Analysis Pipelines\n\u251c\u2500\u2500 Getting data out of Pachyderm\n\u251c\u2500\u2500 Updating Pipelines\n\u2514\u2500\u2500 Serving data from Pachyderm\nFull Examples\n\u2514\u2500\u2500 Examples\nFAQ\n\u2514\u2500\u2500 FAQ\nDeploy Pachyderm\n\u251c\u2500\u2500 Google Cloud Platform\n\u251c\u2500\u2500 Amazon Web Services\n\u251c\u2500\u2500 Azure\n\u251c\u2500\u2500 OpenShift\n\u251c\u2500\u2500 Custom Object Stores\n\u251c\u2500\u2500 On Premises\n\u2514\u2500\u2500 Migrations\nPachyderm Cookbook\n\u251c\u2500\u2500 Tracking Analysis Provenance\n\u251c\u2500\u2500 Leveraging Incrementality\n\u251c\u2500\u2500 Composing Pipelines\n\u251c\u2500\u2500 Interacting with Databases\n\u251c\u2500\u2500 Pachyderm Clients\n\u2514\u2500\u2500 ML workflows\nScaling Pachyderm, Best Practices\n\u251c\u2500\u2500 Best Practices\n\u251c\u2500\u2500 Scaling pachd/etcd\n\u251c\u2500\u2500 Logging and Monitoring\n\u2514\u2500\u2500 Cluster Maintenance \nReference\n\u251c\u2500\u2500 Pipeline Specification\n\u251c\u2500\u2500 Data Versioning\n\u251c\u2500\u2500 Data Pipelining\n\u2514\u2500\u2500 Pachctl CLI Tool. @JoeyZwicker regarding best practices, do you have a good list of things that should be there.  These are the things I'm seeing frequently:\n\nscaling k8s volumes\ncorrecting diskpressure issues\nscaling pachd\ndealing with large files\ndealing with many small files\norganizing repositories\ncomposing pipelines\ndocker image size\n\nI'm sure there is more.  Just wanted to get these down somewhere.  Please add what you think needs to be there.. A new draft of our deployment docs is ready.  Note:\n\nI'm assuming @sjezewski's aws deploy script gets merged (we may have to adjust that link)\nI didn't really change the content on the migrations page.. @JoeyZwicker restructured the index here: https://github.com/pachyderm/pachyderm/blob/v1.4-docs/doc/index.rst\n\nNote that I added a bunch of placeholders as _PLACEHOLDER.  In all of those cases, the doc doesn't exist or needs to be moved, and, in a few cases, we need to create the respective folder.  I think this will help our organization a lot.  Let me know your thought and feel free to tweak as we go.  I will be pulling often from this branch.. @JoeyZwicker Yep, as we tackle each section we can make the folders. We shouldn't collide this way at least.. Yep, good call @JoeyZwicker.  Adding to the list.  I made some rearrangements today.  I'm going to work through the \"fundamentals\" section, then move on to the other things.  I'm still letting you take beginner tutorial and getting started, unless you say different.. Restructure is done, and remaining issues are tracked here: https://github.com/pachyderm/pachyderm/issues/1869. I can confirm that this works!! Thanks @harshavardhana \n```\n\u2794 \n\u2794 pachctl create-repo testminio\n\u2794 pachctl list-repo\nNAME                CREATED             SIZE              \ntestminio           9 seconds ago       0 B               \n\u2794 pachctl put-file testminio master blah -c -f README.md \n\u2794 pachctl list-repo\nNAME                CREATED             SIZE              \ntestminio           30 seconds ago      9.92 KiB          \n\u2794 pachctl get-file testminio master /blah\n \n\n\n\n\nPachyderm: A Containerized, Version-Controlled Data Lake\nPachyderm is:\n...\n``. @jdoliner yes LGTM!. @jdoliner do you know about @harshavardhana's question above?  Would that just be an append to a file and another put-file?. @JoeyZwicker Actually I encountered this recently in a pipeline.  I was outputting model checkpoint data to/pfs/outbut needed to modify some of that after training the model (to adjust directories etc.).  This seemed to work ok for me, because I was doing it in a pipeline and/pfs/out` was already mounted.  Here is the spec if you are interested: https://github.com/dwhitena/pach-pix2pix/blob/master/training_and_export.json\nI think it really depends on whether you are doing this from outside of Pachyderm or from inside.  From how you are approaching the above problem statement, it seems like you are committing from outside of Pachyderm.  I think in that case, I would recommend moving that web scraping, etc. inside of a pipeline stage where you can interact with /pfs/out more like a normal filesystem.\nAm I understanding correctly?  I don't really like the idea of being able to read from an open commit, unless you are restricted to a pipeline stage.  . @JoeyZwicker @sjezewski I fixed most of this in the 1.4 docs.  However, I will make a note to scrub the pachctl docs again here: https://github.com/pachyderm/pachyderm/issues/1869. I think this is addressed generally.  Closing now in favor of more specific pachctl doc issues.. Yep, that's great @JoeyZwicker.  I'll think about the mount.. Yes, @JoeyZwicker related to (3) above, I got feedback from a number of Google advocates that the demo could be made much stronger by having a very simple second stage.  This could be a crop or resize for example.  We could literally add and run this in a couple of second, but it would really demonstrate the power of pipelines in Pachyderm.  The one stage pipeline doesn't differentiate as much from other things like Domino, for example.. @jdoliner that's brilliant.  Great idea and I like how it is sort of a reduce after the map, and a different glob pattern.  We could also do it in simple bash if we wanted (which would make it multi-language).. @JoeyZwicker I have done the OpenCV demo quite a few times using display along with list-file and get-file.  It seems to work pretty smooth.  I'm not quite sure what the FUSE mount adds at this point, especially given that it is somewhat confusing to explain for many people (that is, explaining the mounting of the distributed file system seems to be a hurdle for many people that could be avoided by just listing files and getting them).  I guess you are showing multiple commits, but I think that is pretty clear when you list-commit and list-file.  The only piece that would be missing would be the timestamp.\nThese are just my opinions of course.  @JoeyZwicker, you have a ton of experience with the demo.\nAnd, yes, @sjezewski we need to add the docs for the montage, which is part of the 1.5 checklist.. @JoeyZwicker I'm unable to reproduce this on:\n\nPachyderm 1.3\nUbuntu 16.04\nminikube 0.17.1\n\nI can bring up port-forward and it stays good for 10+ minutes at least.  I haven't tested 1.4 yet.  However, it doesn't appear to be across the board with minikube 0.17.1.  \nI think we should definitely add a note to the docs however about manually setting ADDRESS.. Split file updated here: http://pachyderm.readthedocs.io/en/latest/cookbook/splitting.html.  Also, pachctl docs were scrubbed at the 1.5 release.  Closing for now in favor of more specific doc issues. . I think this is now a moot point as we removed assets.  Please re-open if I'm mistaken.. +1. @JoeyZwicker Actually our migration doc already points to this place generally (i.e., telling people where to look for specific migration instructions scripts).  However, it may be good to explicitly mention this one as an important migration to note.. It looks like CI references the pipeline spec doc for some reason.  Need to update the location or add a copy.. I think we need to add a note to the local deploy doc about how its best to start fresh every time with minikube (i.e., minikube + pachyderm doesn't respond well to restarts).  Just emphasize that minikube is good for testing, but it's not meant to be long running.. I think this has been removed from the docs. Closing.. @gabrielgrant do you think we need more than this currently: http://pachyderm.readthedocs.io/en/latest/cookbook/combining.html.  Maybe something paralleling certain types of joins (INNER, OUTER, etc.)?. I linked this blog post to the detailed combining/merging/joining docs. . Follow up on this issue from Slack:\n``\nBarnaby Keene [11:29 AM] \nthe errorsonly alphanumeric characters, underscores, and dashes are allowedandmay not contain underscore` are a bit weird, since a repo is automatically created for a pipeline and it's named after the pipeline, I think they should both be constrained to the same name requirements. right now, it seems like repos can use underscores but pipelines can not.\nJoey Zwicker [11:29 AM] \n@southclaws  we also opened an issue around pipeline chars. I didn't realize we changed this but I think etcd has some contraints on chars.\n[11:30]\nYeah we definitely will unify these\n[11:30]\nSorry for that confusion!\nBarnaby Keene [11:31 AM] \nalright great to hear thanks! for now I'll just have to smush our UUIDs and other things into one big mess of characters haha\nJoey Zwicker [11:31 AM] \nOr dash perhaps? Sorry again, we'll get this fixed right away\nBarnaby Keene [11:35 AM] \ndashes didn't work in 1.3 either, I originally replaced them with underscores but in 1.4 that stopped working\n. Need to add some details about volume sizes etc. on the AWS deploy doc page.  Write now those things are mostly hidden in the script..Alexandre Bourget [9:47 AM] \n@dwhitena http://pachyderm.readthedocs.io/en/stable/getting_started/getting_started.html  has broken links in the middle there..\n[9:47]\nwent looking for the docs but couldn't find them..\n```. The handle in \"Ping @pachyderm on Twitter\" isn't accurate. . Ok, consolidating all the things that need to be addressed asap here:\n\n[x] Enable readthedocs redirects for docs that existed in 1.3, but do not exist in 1.4 (e.g., deploying in the cloud, services, provenance, etc.)\n[x] Make sure that all references to Slack point to the correct invite link.\n[ ] Modify and add back in the PPS and PFS reference docs.  Also test/fix the included links.\n[x] Fix rendering on the pachctl doc pages (some comments are rendered as titles, pipeline spec shows up where it isn't supposed to, etc.)\n[x] Further update the AWS deploy doc to have a better flow and better integrate the aws deploy script as a starting point (including clarifying dependencies).\n[x] Check/fix/update broken links on the pipeline spec doc.\n[x] Fix beginner tutorial formatting issues.\n[x] Fix how the migrations page refers to automatic migration for 1.4, which doesn't exist.  Make sure the doc is clearly linked in the rendered site.\n[x] Make sure all references to installing pachctl include an explicit version number, and make a list of all references, so these can be automatically updated.\n[x] Add a glob/parallel processing section to fundamentals. \n[ ] Note about valid characters for pipeline/repo names.\n[x] Make sure all deploy commands have the necessary etcd flag.\n[x] clarify that v1.4, we spin up worker pods and leave them up instead of spinning up new pods at every commit. That's why you will see more pods.\n[x] add machine type to the GKE deploy, because the default machine type is not good enough.\n\nThen, these are things that need to be added/modified after the initial high priority stuff above:\n\n[ ] Links for the subsection of the Bill of Rights.\n[ ] Update the bill of rights to make sure everything rings true with 1.4.  Also correcting typos along the way.\n[ ] Add services doc back under fundamentals, update for 1.4\n[ ] Update Jupyter example for 1.4, add back into the docs and update any info in the blog post.\n[ ] Update fruit stand and scraper to 1.4\n[x] Better explanation for from options with update pipeline\n[ ] Better explanation of incrementality, when new data only is processed, etc.\n[x] Clearer flow for describing the different uses of put-file.  It can be confusing that sometimes you need a path, sometimes you don't, sometimes you have a commit-id, sometimes a branch, etc.\n[ ] Explain splitting files, delimiters, with an example.\n[ ] Add dynamic provisioning examples and test them.\n[ ] Make sure the we mention vendoring issues somewhere in relation to the Go doc (i.e., that you need a specific version of grpc)\n[ ] how do I handle failed put-file's with dangling commits.\n[x] format of image pull secrets?\n[x] increase default node size deployed when following our GKE docs\n[ ] Clients reference section\n\nThen, these are things that need to be added as we can:\n\n[ ] SQL example cookbook doc\n[x] Explicit ML workflow cookbook doc\n[ ] Jupyter integration cookbook doc\n[ ] Example OpenStack deploy integrated in the on-prem doc\n[ ] Best practices, scaling docs\n[ ] Update FAQ/troubleshooting and add them back in.\n[ ] Docs about branches\n[ ] Docs about deleting pipelines, repos, etc., why there isn't delete-commit, etc.\n[ ] Python client testing/example\n[ ] Whatever else has already been opened in individual issues.. @JoeyZwicker Adding to the checklist above.  It's really simple to change this.  Testing now actually.. Note @sjezewski @JoeyZwicker, I have created redirects for old pages that no longer exist.  However, as far as I can tell, we can't create redirects for new pages that didn't exist before (e.g., when you switch versions).  Not sure what the solution to that is.. Update pypachy to say it is compatible with 1.4 now.. I'm moving any remaining things that are tracked here to the corresponding 1.5 checklist here: https://github.com/pachyderm/pachyderm/issues/1869, and closing this issue.. More details about the above user's environment:\n\n\n\nThis was a fresh installation, new cluster and all. I actually deleted everything and tried again from scratch.\nThe list-file is in the examples above (same error as get-file). list-repo worked as expected (I don't have that deployment anymore so no screenshots).\nOne thing that may be relevant - when installing 1.4 I had the etcd pod fail due to a timeout, something to do with disk permissions (a shame I didn't record it). I deleted the pod and when it was re-created it worked so I thought nothing of it.. Another user experiencing this issue:\n\nHello again, I keep receiving the error message storage: object doesn't exist when a try to list or get the files from a directory. Anyone with the same problem?\n(Using pachyderm 1.4, deployed at Google Cloud). Ok, I have tried to reproduce this.  I deployed a GKE 1.4 cluster as specified below.  I did have one problem, which may or may not be related.  When I deployed pachyderm the first time, etcd seemed to not come up.  However, I did pachctl undeploy, then pachctl deploy ... again, and it came up fine.  \n\n```\n\u2794 CLUSTER_NAME=pach-test-bug\n\u2794 gcloud container clusters create ${CLUSTER_NAME} --scopes storage-rw\nCreating cluster pach-test-bug...done.                                                                                                                                                \nCreated [https://container.googleapis.com/v1/projects/dan-dev-155821/zones/us-west1-a/clusters/pach-test-bug].\nkubeconfig entry generated for pach-test-bug.\nNAME           ZONE        MASTER_VERSION  MASTER_IP       MACHINE_TYPE   NODE_VERSION  NUM_NODES  STATUS\npach-test-bug  us-west1-a  1.5.6           35.185.241.196  n1-standard-1  1.5.6         3          RUNNING\n\u2794 gcloud container clusters get-credentials ${CLUSTER_NAME}\nFetching cluster endpoint and auth data.\nkubeconfig entry generated for pach-test-bug.\n\u2794 STORAGE_SIZE=\"10\"\n\u2794 STORAGE_NAME=pach-bug-test\n\u2794 gcloud compute disks create --size=${STORAGE_SIZE}GB ${STORAGE_NAME}\nWARNING: You have selected a disk size of under [200GB]. This may result in poor I/O performance. For more information, see: https://developers.google.com/compute/docs/disks#pdperformance.\nCreated [https://www.googleapis.com/compute/v1/projects/dan-dev-155821/zones/us-west1-a/disks/pach-bug-test].\nNAME           ZONE        SIZE_GB  TYPE         STATUS\npach-bug-test  us-west1-a  10       pd-standard  READY\nNew disks are unformatted. You must format and mount a disk before it\ncan be used. You can find instructions on how to do this at:\nhttps://cloud.google.com/compute/docs/disks/add-persistent-disk#formatting\n\u2794 BUCKET_NAME=pach-bug-test\n\u2794 gsutil mb gs://${BUCKET_NAME}\nCreating gs://pach-bug-test/...\n\u2794 pachctl deploy google ${BUCKET_NAME} ${STORAGE_SIZE} --static-etcd-volume=${STORAGE_NAME}\nserviceaccount \"pachyderm\" created\npersistentvolume \"etcd-volume\" created\npersistentvolumeclaim \"etcd-storage\" created\nreplicationcontroller \"etcd\" created\nservice \"etcd\" created\nservice \"pachd\" created\nreplicationcontroller \"pachd\" created\nsecret \"google-secret\" created\n\u2794 \n(after undeploy and redeploy)\n\u2794 \n\u2794 kubectl get all\nNAME             READY     STATUS    RESTARTS   AGE\npo/etcd-mk7tm    1/1       Running   0          31s\npo/pachd-fq8zf   1/1       Running   2          30s\nNAME       DESIRED   CURRENT   READY     AGE\nrc/etcd    1         1         1         31s\nrc/pachd   1         1         1         30s\nNAME             CLUSTER-IP     EXTERNAL-IP   PORT(S)                       AGE\nsvc/etcd         10.3.240.152          2379:32464/TCP                31s\nsvc/kubernetes   10.3.240.1             443/TCP                       21m\nsvc/pachd        10.3.255.164          650:30650/TCP,651:30651/TCP   30s\n\u2794\n```\nThen I tried to put a text file, get the file, and list files.  Indeed I do reproduce the above error:\n\u2794 cat blah.txt\nblah\n\u2794 pachctl create-repo testbug\n\u2794 pachctl list-repo\nNAME                CREATED             SIZE                \ntestbug             3 seconds ago       0 B                 \n\u2794 pachctl put-file testbug master /blah.txt -c -f blah.txt\n\u2794 pachctl get-file testbug master /blah.txt\nstorage: object doesn't exist\n\u2794 pachctl list-file testbug master /\nstorage: object doesn't exist\n\u2794 pachctl list-repo\nNAME                CREATED              SIZE                \ntestbug             About a minute ago   5 B                 \n\u2794. Looks like some errors in the logs for pachd (from the same cluster referred to above):\n\u2794 kubectl logs po/pachd-fq8zf\ntime=\"2017-04-03T14:47:44Z\" level=info msg=\"address:\\\"10.0.0.5:650\\\" \" \ntime=\"2017-04-03T14:47:44Z\" level=info \ntime=\"2017-04-03T14:47:44Z\" level=info msg=\"serverRole:<address:\\\"10.0.0.5:650\\\" shards:<key:0 value:true > shards:<key:1 value:true > shards:<key:2 value:true > shards:<key:3 value:true > shards:<key:4 value:true > shards:<key:5 value:true > shards:<key:6 value:true > shards:<key:7 value:true > shards:<key:8 value:true > shards:<key:9 value:true > shards:<key:10 value:true > shards:<key:11 value:true > shards:<key:12 value:true > shards:<key:13 value:true > shards:<key:14 value:true > shards:<key:15 value:true > > \" \n2017-04-03T14:47:44Z INFO  adding shard 13\n2017-04-03T14:47:44Z INFO  adding shard 4\n2017-04-03T14:47:44Z INFO  adding shard 5\n2017-04-03T14:47:44Z INFO  adding shard 14\n2017-04-03T14:47:44Z INFO  adding shard 3\n2017-04-03T14:47:44Z INFO  adding shard 7\n2017-04-03T14:47:44Z INFO  adding shard 8\n2017-04-03T14:47:44Z INFO  adding shard 2\n2017-04-03T14:47:44Z INFO  adding shard 12\n2017-04-03T14:47:44Z INFO  adding shard 15\n2017-04-03T14:47:44Z INFO  adding shard 0\n2017-04-03T14:47:44Z INFO  adding shard 6\n2017-04-03T14:47:44Z INFO  adding shard 1\n2017-04-03T14:47:44Z INFO  adding shard 10\n2017-04-03T14:47:44Z INFO  adding shard 9\n2017-04-03T14:47:44Z INFO  adding shard 11\ntime=\"2017-04-03T14:47:44Z\" level=info msg=\"serverRole:<address:\\\"10.0.0.5:650\\\" shards:<key:0 value:true > shards:<key:1 value:true > shards:<key:2 value:true > shards:<key:3 value:true > shards:<key:4 value:true > shards:<key:5 value:true > shards:<key:6 value:true > shards:<key:7 value:true > shards:<key:8 value:true > shards:<key:9 value:true > shards:<key:10 value:true > shards:<key:11 value:true > shards:<key:12 value:true > shards:<key:13 value:true > shards:<key:14 value:true > shards:<key:15 value:true > > \" \ntime=\"2017-04-03T14:47:44Z\" level=info msg=\"addresses:<addresses:<key:0 value:\\\"10.0.0.5:650\\\" > addresses:<key:1 value:\\\"10.0.0.5:650\\\" > addresses:<key:2 value:\\\"10.0.0.5:650\\\" > addresses:<key:3 value:\\\"10.0.0.5:650\\\" > addresses:<key:4 value:\\\"10.0.0.5:650\\\" > addresses:<key:5 value:\\\"10.0.0.5:650\\\" > addresses:<key:6 value:\\\"10.0.0.5:650\\\" > addresses:<key:7 value:\\\"10.0.0.5:650\\\" > addresses:<key:8 value:\\\"10.0.0.5:650\\\" > addresses:<key:9 value:\\\"10.0.0.5:650\\\" > addresses:<key:10 value:\\\"10.0.0.5:650\\\" > addresses:<key:11 value:\\\"10.0.0.5:650\\\" > addresses:<key:12 value:\\\"10.0.0.5:650\\\" > addresses:<key:13 value:\\\"10.0.0.5:650\\\" > addresses:<key:14 value:\\\"10.0.0.5:650\\\" > addresses:<key:15 value:\\\"10.0.0.5:650\\\" > > \" \n2017-04-03T14:52:57Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"ListRepo\",\"duration\":\"0.000s\"}\n2017-04-03T14:52:57Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"ListRepo\",\"duration\":\"0.001277593s\"}\n2017-04-03T14:53:00Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"ListRepo\",\"duration\":\"0.000s\"}\n2017-04-03T14:53:00Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"ListRepo\",\"duration\":\"0.001704538s\"}\n2017-04-03T14:53:12Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"CreateRepo\",\"request\":\"repo:\\u003cname:\\\"testbug\\\" \\u003e \",\"duration\":\"0.000s\"}\n2017-04-03T14:53:12Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"CreateRepo\",\"request\":\"repo:\\u003cname:\\\"testbug\\\" \\u003e \",\"response\":\"\\u0026Empty{}\",\"duration\":\"0.002473597s\"}\n2017-04-03T14:53:15Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"ListRepo\",\"duration\":\"0.000s\"}\n2017-04-03T14:53:15Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"ListRepo\",\"response\":\"repo_info:\\u003crepo:\\u003cname:\\\"testbug\\\" \\u003e created:\\u003cseconds:1491231192 nanos:213705823 \\u003e \\u003e \",\"duration\":\"0.000978484s\"}\n2017-04-03T14:53:45Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"StartCommit\",\"request\":\"parent:\\u003crepo:\\u003cname:\\\"testbug\\\" \\u003e \\u003e branch:\\\"master\\\" \",\"duration\":\"0.000s\"}\n2017-04-03T14:53:45Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"StartCommit\",\"request\":\"parent:\\u003crepo:\\u003cname:\\\"testbug\\\" \\u003e \\u003e branch:\\\"master\\\" \",\"response\":\"repo:\\u003cname:\\\"testbug\\\" \\u003e id:\\\"a1d0ec39171f463a8c7ab2dc001e4e0f\\\" \",\"duration\":\"0.002434763s\"}\n2017-04-03T14:53:45Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"PutFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"testbug\\\" \\u003e id:\\\"master\\\" \\u003e path:\\\"/blah.txt\\\" \\u003e file_type:FILE \",\"duration\":\"0.000s\"}\n2017-04-03T14:53:45Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPI.Obj\",\"method\":\"PutObject\",\"duration\":\"0.000s\"}\n2017-04-03T14:53:46Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPI.Obj\",\"method\":\"InspectObject\",\"request\":\"hash:\\\"9895de267ca908c36ed0031c017ba9bf85b83c21ff2bf241766a4037be81f947c68841ee75f003eba3b4bddc524c0357d7bc9ebffe499f5b72f2da3507cb170d\\\" \",\"duration\":\"0.000s\"}\n2017-04-03T14:53:46Z ERROR protorpclog.Call {\"service\":\"pfs.BlockAPI.Obj\",\"method\":\"InspectObject\",\"request\":\"hash:\\\"9895de267ca908c36ed0031c017ba9bf85b83c21ff2bf241766a4037be81f947c68841ee75f003eba3b4bddc524c0357d7bc9ebffe499f5b72f2da3507cb170d\\\" \",\"response\":\"\\u003cnil\\u003e\",\"error\":\"storage: object doesn't exist\",\"duration\":\"0.246117535s\"}\n2017-04-03T14:53:46Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPI.Obj\",\"method\":\"PutObject\",\"duration\":\"0.947238014s\"}\n2017-04-03T14:53:46Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"PutFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"testbug\\\" \\u003e id:\\\"a1d0ec39171f463a8c7ab2dc001e4e0f\\\" \\u003e path:\\\"/blah.txt\\\" \\u003e file_type:FILE \",\"duration\":\"0.961372158s\"}\n2017-04-03T14:53:46Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"FinishCommit\",\"request\":\"commit:\\u003crepo:\\u003cname:\\\"testbug\\\" \\u003e id:\\\"master\\\" \\u003e \",\"duration\":\"0.000s\"}\n2017-04-03T14:53:46Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPI.Obj\",\"method\":\"PutObject\",\"duration\":\"0.000s\"}\n2017-04-03T14:53:47Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPI.Obj\",\"method\":\"InspectObject\",\"request\":\"hash:\\\"3f838c081daed8d6dcc95adeb367337613c2f22d97af800634fc4f28b716ad0951273b369dbf252019df8e0cbcd272b40f5017c124999410924093ec08eac8ed\\\" \",\"duration\":\"0.000s\"}\n2017-04-03T14:53:47Z ERROR protorpclog.Call {\"service\":\"pfs.BlockAPI.Obj\",\"method\":\"InspectObject\",\"request\":\"hash:\\\"3f838c081daed8d6dcc95adeb367337613c2f22d97af800634fc4f28b716ad0951273b369dbf252019df8e0cbcd272b40f5017c124999410924093ec08eac8ed\\\" \",\"response\":\"\\u003cnil\\u003e\",\"error\":\"storage: object doesn't exist\",\"duration\":\"0.117806811s\"}\n2017-04-03T14:53:47Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPI.Obj\",\"method\":\"PutObject\",\"duration\":\"0.673689749s\"}\n2017-04-03T14:53:47Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"FinishCommit\",\"request\":\"commit:\\u003crepo:\\u003cname:\\\"testbug\\\" \\u003e id:\\\"a1d0ec39171f463a8c7ab2dc001e4e0f\\\" \\u003e \",\"response\":\"\\u0026Empty{}\",\"duration\":\"0.688691495s\"}\n2017-04-03T14:54:05Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"GetFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"testbug\\\" \\u003e id:\\\"master\\\" \\u003e path:\\\"/blah.txt\\\" \\u003e \",\"duration\":\"0.000s\"}\n2017-04-03T14:54:05Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPI.Obj\",\"method\":\"GetObject\",\"request\":\"hash:\\\"3f838c081daed8d6dcc95adeb367337613c2f22d97af800634fc4f28b716ad0951273b369dbf252019df8e0cbcd272b40f5017c124999410924093ec08eac8ed\\\" \",\"duration\":\"0.000s\"}\n2017-04-03T14:54:05Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPI.Obj\",\"method\":\"InspectObject\",\"request\":\"hash:\\\"3f838c081daed8d6dcc95adeb367337613c2f22d97af800634fc4f28b716ad0951273b369dbf252019df8e0cbcd272b40f5017c124999410924093ec08eac8ed\\\" \",\"duration\":\"0.000s\"}\n2017-04-03T14:54:05Z ERROR protorpclog.Call {\"service\":\"pfs.BlockAPI.Obj\",\"method\":\"InspectObject\",\"request\":\"hash:\\\"3f838c081daed8d6dcc95adeb367337613c2f22d97af800634fc4f28b716ad0951273b369dbf252019df8e0cbcd272b40f5017c124999410924093ec08eac8ed\\\" \",\"response\":\"\\u003cnil\\u003e\",\"error\":\"storage: object doesn't exist\",\"duration\":\"0.133204735s\"}\n2017-04-03T14:54:05Z ERROR protorpclog.Call {\"service\":\"pfs.BlockAPI.Obj\",\"method\":\"GetObject\",\"request\":\"hash:\\\"3f838c081daed8d6dcc95adeb367337613c2f22d97af800634fc4f28b716ad0951273b369dbf252019df8e0cbcd272b40f5017c124999410924093ec08eac8ed\\\" \",\"error\":\"storage: object doesn't exist\",\"duration\":\"0.133604934s\"}\n2017-04-03T14:54:05Z ERROR protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"GetFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"testbug\\\" \\u003e id:\\\"a1d0ec39171f463a8c7ab2dc001e4e0f\\\" \\u003e path:\\\"/blah.txt\\\" \\u003e \",\"error\":\"storage: object doesn't exist\",\"duration\":\"0.137087699s\"}\n2017-04-03T14:54:20Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"ListFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"testbug\\\" \\u003e id:\\\"master\\\" \\u003e path:\\\"/\\\" \\u003e \",\"duration\":\"0.000s\"}\n2017-04-03T14:54:20Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPI.Obj\",\"method\":\"GetObject\",\"request\":\"hash:\\\"3f838c081daed8d6dcc95adeb367337613c2f22d97af800634fc4f28b716ad0951273b369dbf252019df8e0cbcd272b40f5017c124999410924093ec08eac8ed\\\" \",\"duration\":\"0.000s\"}\n2017-04-03T14:54:20Z INFO  protorpclog.Call {\"service\":\"pfs.BlockAPI.Obj\",\"method\":\"InspectObject\",\"request\":\"hash:\\\"3f838c081daed8d6dcc95adeb367337613c2f22d97af800634fc4f28b716ad0951273b369dbf252019df8e0cbcd272b40f5017c124999410924093ec08eac8ed\\\" \",\"duration\":\"0.000s\"}\n2017-04-03T14:54:20Z ERROR protorpclog.Call {\"service\":\"pfs.BlockAPI.Obj\",\"method\":\"InspectObject\",\"request\":\"hash:\\\"3f838c081daed8d6dcc95adeb367337613c2f22d97af800634fc4f28b716ad0951273b369dbf252019df8e0cbcd272b40f5017c124999410924093ec08eac8ed\\\" \",\"response\":\"\\u003cnil\\u003e\",\"error\":\"storage: object doesn't exist\",\"duration\":\"0.158253687s\"}\n2017-04-03T14:54:20Z ERROR protorpclog.Call {\"service\":\"pfs.BlockAPI.Obj\",\"method\":\"GetObject\",\"request\":\"hash:\\\"3f838c081daed8d6dcc95adeb367337613c2f22d97af800634fc4f28b716ad0951273b369dbf252019df8e0cbcd272b40f5017c124999410924093ec08eac8ed\\\" \",\"error\":\"storage: object doesn't exist\",\"duration\":\"0.158647672s\"}\n2017-04-03T14:54:20Z ERROR protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"ListFile\",\"request\":\"file:\\u003ccommit:\\u003crepo:\\u003cname:\\\"testbug\\\" \\u003e id:\\\"a1d0ec39171f463a8c7ab2dc001e4e0f\\\" \\u003e path:\\\"/\\\" \\u003e \",\"response\":\"\\u003cnil\\u003e\",\"error\":\"storage: object doesn't exist\",\"duration\":\"0.161919868s\"}\n2017-04-03T14:54:24Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"ListRepo\",\"duration\":\"0.000s\"}\n2017-04-03T14:54:24Z INFO  protorpclog.Call {\"service\":\"pfs.API\",\"method\":\"ListRepo\",\"response\":\"repo_info:\\u003crepo:\\u003cname:\\\"testbug\\\" \\u003e created:\\u003cseconds:1491231192 nanos:213705823 \\u003e size_bytes:5 \\u003e \",\"duration\":\"0.000862226s\"}. Another user hitting this issue:\n\nThis problem popped up when I upgraded to 1.4, it did this before using minio too but I haven't delved into it as much (just trying it without minio again to get the kube event log and see what the result is)\npachctl list-repo shows what I expect, input repo with a size > 0 and the pipeline output repos with sizes of 0\npachctl list-commit shows the commit I made\npachctl list-file with the repo/commit shows object not found for a google bucket or a similar error for minio\njust using google bucket on GKE\n\n@derekchiang were you able to find a path forward on this yesterday?. Fixed in 1.4.1.. @abourget Thanks for bringing this up.  I wasn't able to reproduce this with a 1.4 cluster in AWS:\n\u2794 echo '{}' >> lines.txt\n\u2794 echo '{}' >> lines.txt\n\u2794 echo '{}' >> lines.txt\n\u2794 cat lines.txt \n{}\n{}\n{}\n\u2794 pachctl create-repo testsplit\n\u2794 pachctl put-file testsplit master -c --split line -f lines.txt\n\u2794 pachctl list-file testsplit master\nNAME                TYPE                SIZE                \nlines.txt           dir                 9 B                 \n\u2794\nCould you provide a little more detail about anything you did in the deploy different from the default deploy outlines here: http://docs.pachyderm.io/en/latest/deployment/amazon_web_services.html?\nSpecifically, I think you renamed a couple of things maybe, etc.?  Could be related.  I'm not sure.. Also, given that users could potentially have many, many pods up and running but only be processing a small amount of data at a time (large parallelism but a small commit), it might be useful to provide some easy way to let the users/devs know which of the worker pods jobs run on.\nThis was brought up by a user in Slack.. Related? https://github.com/pachyderm/pachyderm/issues/1556. More info from the user:\n\nI asked @jdoliner yesterday. He said: I\u2019d try again with the constant field set to 32 and see if maybe that gets you a sane result, we\u2019re going to have to run some tests and see if we can reproduce what you\u2019re seeing here.\nIn that case, the same thing happened. 16 pods. Sorry, duplicate. @sjezewski @jdoliner is there any update on this or the related https://github.com/pachyderm/pachyderm/issues/1549?  Had some users asking today.. +1 . Ok, I was able to reproduce this @derekchiang and @jdoliner with 239Mb of images recursively uploading to a GKE cluster with Pachyderm 1.4.1.\n\n\u2794 pachctl put-file training master -c -r -f .\ntransport is closing\nI'm doing this remotely, so @derekchiang are you saying I should try from within the cluster?  Or maybe I upload to a bucket and then put-file from there?. I've tried a couple different things here as well.  I uploaded all my files to my object store, got URLs for each of the files, and tried to put-file with -i, thinking that would be better.  Also, I killed my port-forward and manually set ADDRESS, but still:\n\u2794 \n\u2794 pachctl create-repo training\n\u2794 pachctl put-file training master -c -i maps.txt \ntransport is closing\n\u2794 pachctl list-repo\nNAME                CREATED             SIZE                \ntraining            8 minutes ago       0 B                 \n\u2794\nSo, I guess we can rule out port-forward?. Ok, now I tried this from an instance in the same region as my cluster (because there are some weird settings on the instances in my GKE cluster, and I couldn't quickly get pachctl up and running).  But, alas, the same:\ndwhitena@put-file:~/maps/train$ pachctl create-repo training\ndwhitena@put-file:~/maps/train$ pachctl put-file training master -c -r -f .\ntransport is closing. This seems to be related to the actual number of files and/or how long it takes to upload them.  The above attempt was with 1097 images.  If I take out a sample of around 200 images:\ndwhitena@put-file:~/maps/train/sample$ pachctl put-file training master -c -r -f .\ndwhitena@put-file:~/maps/train/sample$\nSuccess!  So, this definitely seems to be reproducible as the number and/or size of the files increases.. Sweet.  Yeah, we will work this into the cookbook @JoeyZwicker.. Done: http://docs.pachyderm.io/en/latest/cookbook/time_windows.html. Duplicate with #1530.  Closing. @derekchiang I think giving access the /pfs and /tmp is enough.  We want to encourage people to be versioning via repos, not create a bunch of temporary intermediate states of the data.  That being said, at least having /tmp provides an avenue to manage data created during processes like ML training with TensorFlow.. It should be merged whenever CI passes @JoeyZwicker.  I just included the checklist to show what was included.  \nNot sure why CI isnt passing.. As you might have notice @jonandernovella is also experiencing this issue.  It appears that the breaking change was this check:\nhttps://github.com/pachyderm/pachyderm/commit/68ede0dd5070dc857fb0bf29dbd19be331e1fba4. Basically what was happening is that minio's GetObject doesn't return an error for a missing object until you start to read something. So we were getting nil, nil when we expected nil, error.. My setup to create this scenario was the following:\n\nPachyderm + Minio in GKE\nPachyderm 1.4.1\nk8s 1.5.6\nRunning this pipeline https://github.com/pachyderm/pachyderm/tree/master/doc/examples/ml/tensorflow\nOn the \"maps\" dataset that can be found here: https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/\n\nI'm going to try and reproduce w/out minio to exclude that as a variable as well.. @jdoliner, were you deploying with dynamic volumes every time?  I was using manual volumes every time and it happens every time for me. For me it can be reproduced by following our docs for Google deploys exactly.  That is what I have been doing.  etcd will just sit there is crash backoff until you undeploy and redeploy.  Then it works like a charm.. @jdoliner not really sure what I'm doing different then.  The only other variables are region, volume size, etc.. @jdoliner I am away from my laptop now, but I will try again on Monday and document the whole thing better.  . @jdoliner  and @JoeyZwicker, I tried this again with 1.4.3 and the cluster deployed on the first try.  I guess I will just bring this up again if/when I see it again.  Maybe 1.4.3 fixed this, maybe it is intermittent, or maybe @gabrielgrant has a way to reproduce?. @gabrielgrant By online/stream operations I meant distributed real-time processing, where all of you data does not fit into memory either by choice or necessity.  By ad hoc queries, I meant things like Spark SQL or distributed dataframe operations, so probably \"ad hoc\" wasn't good wording.  Basically, I meant the ability to execute arbitrary queries, select, subset, etc. operations without previously arranging the data in such a way to facilitate specific queries.\nI think Jupyter, etc. are a slightly different conversation, although maybe related in some ways as you have noted (Spark notebooks, zeppelin, Jupyter + Spark, etc.).. LGTM.  Did you render and check the links?  Just a reminder.. Just re-created this with another pipeline.  This seems to always happen after the first run fails. @derekchiang maybe related to the issues we were discussing on Slack?. I'm not sure about @jdoliner's suggestion above and how this is different.  However, I can also report that this \"... as it's already been processed\" error seems to also be popping up in the PharmBio pipeline, which is processing large files (~100MB) and outputting corresponding output files to /pfs/out/.  See https://github.com/minio/minio/issues/4077#issuecomment-293396028.  \nSpecifically, after a job fails (e.g., for some file corruption that is likely unrelated), any attempts at re-running the job, or even running delete-all and re-creating the job, end up with a job that constantly restarts. The error's in the job logs return:\n{\"pipelineName\":\"MultiPeakPickerHiRes\",\"pipelineId\":\"324d164714c1425db5b05d9067ca90c1\",\"jobId\":\"4cff0f80-f4f2-4f4e-b94d-26c47ed67d98\",\"workerId\":\"pipeline-multipeakpickerhires-v1-vrjr1\",\"data\":[{\"path\":\"/014_CRa_H9M5_M470_Pool_03_alternate_pos_low_mr.mzML\",\"hash\":\"OUxhSfR26Qne9ZaNqNQuS8hgCaQM9oQpLNvrxZg5XFU=\"}],\"ts\":\"2017-04-11T16:18:49.006889331Z\",\"message\":\"Received request\"}\n{\"pipelineName\":\"MultiPeakPickerHiRes\",\"pipelineId\":\"324d164714c1425db5b05d9067ca90c1\",\"jobId\":\"4cff0f80-f4f2-4f4e-b94d-26c47ed67d98\",\"workerId\":\"pipeline-multipeakpickerhires-v1-vrjr1\",\"data\":[{\"path\":\"/014_CRa_H9M5_M470_Pool_03_alternate_pos_low_mr.mzML\",\"hash\":\"OUxhSfR26Qne9ZaNqNQuS8hgCaQM9oQpLNvrxZg5XFU=\"}],\"ts\":\"2017-04-11T16:18:49.007925932Z\",\"message\":\"skipping input, as it's already been processed\"}\n{\"pipelineName\":\"MultiPeakPickerHiRes\",\"pipelineId\":\"324d164714c1425db5b05d9067ca90c1\",\"jobId\":\"4cff0f80-f4f2-4f4e-b94d-26c47ed67d98\",\"workerId\":\"pipeline-multipeakpickerhires-v1-vrjr1\",\"data\":[{\"path\":\"/014_CRa_H9M5_M470_Pool_03_alternate_pos_low_mr.mzML\",\"hash\":\"OUxhSfR26Qne9ZaNqNQuS8hgCaQM9oQpLNvrxZg5XFU=\"}],\"ts\":\"2017-04-11T16:19:10.349249468Z\",\"message\":\"Received request\"}\n{\"pipelineName\":\"MultiPeakPickerHiRes\",\"pipelineId\":\"324d164714c1425db5b05d9067ca90c1\",\"jobId\":\"4cff0f80-f4f2-4f4e-b94d-26c47ed67d98\",\"workerId\":\"pipeline-multipeakpickerhires-v1-vrjr1\",\"data\":[{\"path\":\"/014_CRa_H9M5_M470_Pool_03_alternate_pos_low_mr.mzML\",\"hash\":\"OUxhSfR26Qne9ZaNqNQuS8hgCaQM9oQpLNvrxZg5XFU=\"}],\"ts\":\"2017-04-11T16:19:10.351032440Z\",\"message\":\"skipping input, as it's already been processed\"}\n{\"pipelineName\":\"MultiPeakPickerHiRes\",\"pipelineId\":\"324d164714c1425db5b05d9067ca90c1\",\"jobId\":\"4cff0f80-f4f2-4f4e-b94d-26c47ed67d98\",\"workerId\":\"pipeline-multipeakpickerhires-v1-vrjr1\",\"data\":[{\"path\":\"/010_CRa_H9M5_M470_L03_K3_alternate_pos_low_mr.mzML\",\"hash\":\"Knoj3ZrWlc4sH7xCSaJrybKY0r5n6AZqVLgqylasSas=\"}],\"ts\":\"2017-04-11T16:19:10.360345396Z\",\"message\":\"Received request\"}\n{\"pipelineName\":\"MultiPeakPickerHiRes\",\"pipelineId\":\"324d164714c1425db5b05d9067ca90c1\",\"jobId\":\"4cff0f80-f4f2-4f4e-b94d-26c47ed67d98\",\"workerId\":\"pipeline-multipeakpickerhires-v1-vrjr1\",\"data\":[{\"path\":\"/010_CRa_H9M5_M470_L03_K3_alternate_pos_low_mr.mzML\",\"hash\":\"Knoj3ZrWlc4sH7xCSaJrybKY0r5n6AZqVLgqylasSas=\"}],\"ts\":\"2017-04-11T16:19:10.362667170Z\",\"message\":\"skipping input, as it's already been processed\"}\netc...\nNote, there may also be some Minio related issues happening here.  But, for whatever reason, when processing large files, it seems like they are also getting into this bad spot.  For them, I don't think update-pipeline even fixes this looping.. I just checked with Jon, and he ran a test.  This sort of restarting loop happens for him even when a job doesn't fail, or when there wasn't a previous job that failed on the data.  For him, it seems to be more related to the processing of larger files.\nIf you look at the non-raw logs for his job, the codes looks like it processed all the files just fine.  However, the job doesn't ever end.  It just starts looping:\n[{\"path\":\"/013_CRa_H9M5_M470_Blank_04_alternate_pos_low_mr.mzML\",\"hash\":\"5Hk/YSV799tZMfJKlB8mv7hiEUH4ttm/CPSx42HUkLI=\"}],\"ts\":\"2017-04-12T13:32:31.458187621Z\",\"message\":\"Received request\"}\n{\"pipelineName\":\"newpipeline\",\"pipelineId\":\"d2514a890d3a4a84b82a6a4eb015c92a\",\"jobId\":\"ee02de42-b24f-4aa4-892a-720de35f0d45\",\"workerId\":\"pipeline-newpipeline-v1-x12t7\",\"data\":[{\"path\":\"/013_CRa_H9M5_M470_Blank_04_alternate_pos_low_mr.mzML\",\"hash\":\"5Hk/YSV799tZMfJKlB8mv7hiEUH4ttm/CPSx42HUkLI=\"}],\"ts\":\"2017-04-12T13:32:31.461723545Z\",\"message\":\"skipping input, as it's already been processed\"}\n{\"pipelineName\":\"newpipeline\",\"pipelineId\":\"d2514a890d3a4a84b82a6a4eb015c92a\",\"jobId\":\"ee02de42-b24f-4aa4-892a-720de35f0d45\",\"workerId\":\"pipeline-newpipeline-v1-x12t7\",\"data\":[{\"path\":\"/019_CRa_H9M5_M470_C01_K3_alternate_pos_low_mr.mzML\",\"hash\":\"tSxgLrLiJFVwTrs+POCEJBXnYmH65O2P+/sHLEFEJdo=\"}],\"ts\":\"2017-04-12T13:32:31.466704655Z\",\"message\":\"Received request\"}\n{\"pipelineName\":\"newpipeline\",\"pipelineId\":\"d2514a890d3a4a84b82a6a4eb015c92a\",\"jobId\":\"ee02de42-b24f-4aa4-892a-720de35f0d45\",\"workerId\":\"pipeline-newpipeline-v1-x12t7\",\"data\":[{\"path\":\"/019_CRa_H9M5_M470_C01_K3_alternate_pos_low_mr.mzML\",\"hash\":\"tSxgLrLiJFVwTrs+POCEJBXnYmH65O2P+/sHLEFEJdo=\"}],\"ts\":\"2017-04-12T13:32:31.468799930Z\",\"message\":\"skipping input, as it's already been processed\"}\netc...\nSo maybe this needs to be addressed in another issue, not related to recovering from bad code.. @jdoliner I tried the above both with 1.4.3 and building from master and encountered the same behavior.  . @JoeyZwicker should be ready.  I added a figure.  Also, I added in a small change to the tensorflow example.. Yep, I like that @JoeyZwicker. Will do.. Yep, will do @JoeyZwicker.  Added here: https://github.com/pachyderm/pachyderm/issues/1869. Fixed here: http://pachyderm.readthedocs.io/en/latest/cookbook/splitting.html. Closing for now.. Fixed by https://github.com/pachyderm/pachyderm/pull/1887. LGTM. @jdoliner Yep, I'm on it.  I think the first step is to vendor in the new client and see if that fixes the issue.  That should be easy on our end.  But I will also confirm with the Minio guys and see if they can provide some guidance.. FYI, some clarification from users slack:\n\nDaniel Whitenack [10:09 AM]\n@dahankzter Also just to confirm, you were able to speed up Minio + Pachyderm with the latest client yeah?\n[10:10] \nIt's just a matter of vendoring, review issues.\nHenrik Johansson [10:50 AM] \n@dwhitena well only minio-mc client + same minio-server using new mc code (same lib as pachyderm)\n[10:51] \nWe actually never got a new pachd with the new minio-go lib deployed\n[10:51] \nWe may be able to do that, not sure\nDaniel Whitenack [10:55 AM] \n@dahankzter Thanks.  I will try as well.\nHenrik Johansson [10:56 AM] \nbut the difference with new mc vs HEAD mc was remarkable. @dahankzter This should be fixed by: https://github.com/pachyderm/pachyderm/pull/1867.  Feel free to try out the PR to see if it works for you, but it worked fine for me.  The speedup is indicated in the PR.. In addition, we need to address all of these in the docs, etc. to get things working in openshift: https://github.com/pachyderm/pachyderm/pull/1777. Ported to https://github.com/pachyderm/pachyderm/issues/2224.  Closing this.. Related doc changes tracked here: https://github.com/pachyderm/pachyderm/issues/1869. @radu-adrian-moldovan Thanks for reporting. It seems like some of what you are hitting is around port-forwarding and other things that you are hitting are related to the browser cache. After redeploying Pachyderm you should run port forward again and do a hard refresh in the browser for your dashboard. That should clear up any of those issues. Also, if you command line connection to Pachyderm is flaky, you can always manually point pachctl to the cluster by setting the ADDRESS variable:\n\nexport ADDRESS=<k8s master IP>:30650. Actually, it looks like the job spec hasn't been updated to the new atom/cross/union format:\nhttps://github.com/pachyderm/pachyderm/blob/master/src/client/pps/pps.pb.go#L1108\nand \nhttps://github.com/pachyderm/pachyderm/blob/master/src/client/pps/pps.pb.go#L445\nSo I'm updating the title of this issue to take care of this.. @gabrielgrant and @sjezewski is this sufficiently documented here for now: http://pachyderm.readthedocs.io/en/latest/fundamentals/distributed_computing.html#datums?. I might re-arrange a couple things as I'm going through the docs, but let's go ahead and get this in.  Thanks for the revisions.. I think the real issue here is related to mounting.  Because, I think that master will always give the latest in pipelines and using the CLI/client, right?. Perfect.  Thanks @jdoliner.\n. Thanks @jdoliner.  Makes total sense.. @sjezewski ok, this is ready for another review.  Thanks!. @sjezewski Thanks!  Some of these will go under a planned best practices section, which will also include some of Derek's recent additions.  I will be standardizing all of this for the 1.5 release and organizing it.  I suggest one of the following:\n\njust merging the *.md files here somewhere in docs without linking them, then I could organize them and standardize with the other things as I'm going through them, or\njust add these to another PR I'm working on for standardizing/adding best practices.  I'm happy to massage them.\n\nLet me know what you prefer.. I think the plan was to have a section that includes best practices, troubleshooting, etc., Not necessarily a section named \"best practices.\" Something like \"managing a Pachyderm cluster\" or something more concise.  We already have quite a few sections.. Yes, @sjezewski let's go ahead and merge and then I will re-arrange as I add the rest of the 1.5 docs.. @sjezewski Ready for another review.. Yeah, but everything is versioned here, so, if we needed to, we could pull an old version, right?  Composing pipelines and provenance are integrated, at least partially, in the pachyderm fundamentals section.. @JoeyZwicker I think we need to reformat any relevant content here into the new troubleshooting format.  The old troubleshooting docs were re-arranged and updated around the time this was submitted.\n. Some relevant bits pulled in, conflicts resolved, format updated, and additional typos corrected in: https://github.com/pachyderm/pachyderm/pull/2504. Some relevant bits pulled in, conflicts resolved, format updated, and additional typos corrected in: https://github.com/pachyderm/pachyderm/pull/2504. @peterjdolan Yeah, I will re-run.  There are some intermittent things.\nAlso, can you sign the CLA and get that to us?. Thanks @peterjdolan.  I restarted the build, which will hopefully take care of any issues.  Then I will get this merged.. @jdoliner @sjezewski do we have to run the CI differently for external contributors?  Seems like its failing right off the bat.. I should note that the above was with 1.5.0-RC1, for both example pipelines (1) and (2).. Added here: http://pachyderm.readthedocs.io/en/latest/fundamentals/incrementality.html. @JoeyZwicker Ok, I'm going to go ahead and merge this.  Let me know what your comment it and I will work it into further doc PRs.. @jdoliner @JoeyZwicker I added pictures here and massaged the wording a little bit.  If one of you could quickly skim this to see if anything seems off, that would be great.  Thanks.. Closing for now based on @jdoliner's comment.. @jdoliner any input here? I think use of gometalinter is pretty common, and I use it locally on saves.. Fixed in https://github.com/pachyderm/pachyderm/pull/2592. Yeah, this cropped up again today in Slack:\n\nstill a little confused as to the best practice in the following scenario. Have a user trying to commit changes into the output of a pipeline stage b/c:\nUser [1:47 PM] \nDang, okay.  We caught an error in one of the files that we wanted to update.  The previous pipeline stage takes ~30 hours to run, and the way the glob patterns work from the initial repos would cause all of that to run again.  We were hoping to just manually fix and commit the file downstream so we wouldn't have to wait for all of the upstream pipelines to run again. I think this looks ok for committing the same split file twice.  Is it just checking that the number of files is as expected though?  My questions were mostly around whether or not the datums generated by the split were the same and/or were replaced.  . Awesome.  LGTM @jdoliner!. This has been added.. Yes, I agree with @sjezewski here.  But maybe we should make this clearer in our docs. . So, I think I have reproduced the error here.  @bpb please let me know if there is any part of the following that is not consistent with your situation.  I'm also modifying the title of the issue to match the underlying problem.\n\nBehavior\nA pipeline that is subscribed to a repo, where the repo is the output of a cron triggered pipeline, will not run.  Actually, it seems like such a pipeline can't even be successfully created.  When you run create-pipeline to create such a pipeline, the worker pod get stucks in an endless CrashLoopBackoff.  \nHow to reproduce\nConsider the following pipeline spec:\n{\n  \"pipeline\": {\n    \"name\": \"copy\"\n  },\n  \"transform\": {\n    \"cmd\": [ \"/bin/bash\" ],\n    \"stdin\": [\n      \"cp /pfs/query/output.json /pfs/out/copied.json\"\n    ]\n  },\n  \"input\": {\n    \"atom\": {\n      \"repo\": \"query\",\n      \"glob\": \"/\"\n    }  \n  }\n}\nIf you manually create the query repo, put an output.json file into it, and create this pipeline, everything happens as expected:\n$ pachctl create-repo query\n$ pachctl create-pipeline -f copy.json\n$ pachctl put-file query master output.json -c -f README.md \n$ pachctl list-job\nID                                   OUTPUT COMMIT                         STARTED       DURATION           RESTART PROGRESS  DL       UL       STATE            \n3983a10a-0da3-4a91-b54c-cfd216b50f4a copy/039687fd7aa9446fa79ae96293cbfe78 2 seconds ago Less than a second 0       1 + 0 / 1 15.26KiB 15.26KiB success \n$ pachctl list-repo \nNAME                CREATED             SIZE                \ncopy                4 minutes ago       15.26KiB            \nquery               4 minutes ago       15.26KiB\nHowever, if you instead run the cron example (which creates the query repo as the output of a cron triggered pipeline) and then try to create the copy pipeline, the pipeline is not triggered:\n$ pachctl list-job\nID                                   OUTPUT COMMIT                          STARTED            DURATION  RESTART PROGRESS  DL  UL   STATE            \na6928098-2b5e-4305-8b45-e1af574949c1 query/ad509999b1cf48c99946e139fdf7ba08 24 seconds ago     1 second  0       1 + 0 / 1 26B 660B success \n1cd91b4f-ce88-4e0f-bf0a-3b42cd043230 query/2a46247e49d44407967d372fc59baaaf About a minute ago 1 second  0       1 + 0 / 1 26B 365B success \nccd96b26-f046-45e1-ab52-7205d8b516be query/320e796879a94436ad342e7175f2c097 About a minute ago 1 second  0       1 + 0 / 1 26B 750B success \nad2e8680-d174-4781-9e2e-9696df3a7267 query/7546e0e2ae274fc2b914ff1b7f6b401a About a minute ago 2 seconds 0       1 + 0 / 1 26B 746B success\nActually, the worker pod for the copy pipeline doesn't even come up in k8s.  It just gets stuck in a seemingly endless CrashLoopBackoff:\n```\n$ kubectl get all\nNAME                         READY     STATUS             RESTARTS   AGE\npo/etcd-3670196545-qxlcw     1/1       Running            0          20m\npo/pachd-1541096867-hr7cv    1/1       Running            3          20m\npo/pipeline-copy-v1-5cqbr    1/2       CrashLoopBackOff   3          1m\npo/pipeline-query-v1-k7bl3   2/2       Running            0          5m\nNAME                   DESIRED   CURRENT   READY     AGE\nrc/pipeline-copy-v1    1         1         0         1m\nrc/pipeline-query-v1   1         1         1         5m\nNAME                    CLUSTER-IP   EXTERNAL-IP   PORT(S)                                     AGE\nsvc/etcd                10.0.0.228          2379:32379/TCP                              20m\nsvc/kubernetes          10.0.0.1             443/TCP                                     22m\nsvc/pachd               10.0.0.11           650:30650/TCP,651:30651/TCP,652:30652/TCP   20m\nsvc/pipeline-copy-v1    10.0.0.205           80/TCP                                      1m\nsvc/pipeline-query-v1   10.0.0.186           80/TCP                                      5m\nNAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeploy/etcd    1         1         1            1           20m\ndeploy/pachd   1         1         1            1           20m\nNAME                  DESIRED   CURRENT   READY     AGE\nrs/etcd-3670196545    1         1         1         20m\nrs/pachd-1541096867   1         1         1         20m\n```\nDescribing the problem pod gives:\nkubectl describe po/pipeline-copy-v1-5cqbr\nName:           pipeline-copy-v1-5cqbr\nNamespace:      default\nNode:           minikube/192.168.99.100\nStart Time:     Tue, 29 Aug 2017 12:41:29 -0400\nLabels:         app=pipeline-copy-v1\n                suite=pachyderm\nAnnotations:    kubernetes.io/created-by={\"kind\":\"SerializedReference\",\"apiVersion\":\"v1\",\"reference\":{\"kind\":\"ReplicationController\",\"namespace\":\"default\",\"name\":\"pipeline-copy-v1\",\"uid\":\"e7e787f3-8cd8-11e7-9b42-0800...\nStatus:         Running\nIP:             172.17.0.6\nCreated By:     ReplicationController/pipeline-copy-v1\nControlled By:  ReplicationController/pipeline-copy-v1\nInit Containers:\n  init:\n    Container ID:       docker://90197e40c91aec4e75be082beb99980accb5ff4ed9ad962b02f2938e1d85df10\n    Image:              pachyderm/worker:1.5.3\n    Image ID:           docker-pullable://pachyderm/worker@sha256:fc1d207d5aa7820c82628cae179e174cb1a5b60fcc9afcadfef72e0c51f2192a\n    Port:               <none>\n    Command:\n      /pach/worker.sh\n    State:              Terminated\n      Reason:           Completed\n      Exit Code:        0\n      Started:          Tue, 29 Aug 2017 12:41:29 -0400\n      Finished:         Tue, 29 Aug 2017 12:41:29 -0400\n    Ready:              True\n    Restart Count:      0\n    Environment:\n      PPS_WORKER_IP:             (v1:status.podIP)\n      PPS_POD_NAME:             pipeline-copy-v1-5cqbr (v1:metadata.name)\n      PPS_ETCD_PREFIX:          pachyderm_pps\n      PPS_NAMESPACE:            default\n      PPS_PIPELINE_NAME:        copy\n    Mounts:\n      /pach-bin from pach-bin (rw)\n      /scratch from pachyderm-worker (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-c8nbj (ro)\nContainers:\n  user:\n    Container ID:       docker://74dd8342f922942a8cc0d8513e209637e9d0fe648e90641152661c3513b1475d\n    Image:              ubuntu:16.04\n    Image ID:           docker-pullable://ubuntu@sha256:34471448724419596ca4e890496d375801de21b0e67b81a77fd6155ce001edad\n    Port:               <none>\n    Command:\n      /pach-bin/guest.sh\n    State:              Waiting\n      Reason:           CrashLoopBackOff\n    Last State:         Terminated\n      Reason:           Error\n      Exit Code:        2\n      Started:          Tue, 29 Aug 2017 12:43:03 -0400\n      Finished:         Tue, 29 Aug 2017 12:43:03 -0400\n    Ready:              False\n    Restart Count:      4\n    Requests:\n      alpha.kubernetes.io/nvidia-gpu:   0\n      cpu:                              0\n      memory:                           64M\n    Environment:\n      PPS_WORKER_IP:             (v1:status.podIP)\n      PPS_POD_NAME:             pipeline-copy-v1-5cqbr (v1:metadata.name)\n      PPS_ETCD_PREFIX:          pachyderm_pps\n      PPS_NAMESPACE:            default\n      PPS_PIPELINE_NAME:        copy\n    Mounts:\n      /pach-bin from pach-bin (rw)\n      /scratch from pachyderm-worker (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-c8nbj (ro)\n  storage:\n    Container ID:       docker://f831a7c2f412861f0221c50387dad354b595214c6143bc75c82d72dbdeec1b77\n    Image:              pachyderm/pachd:1.5.3\n    Image ID:           docker-pullable://pachyderm/pachd@sha256:79ce19d7258752d196f632250949aa2c68f86ec55bfb578400cdd7067c9ec133\n    Port:               <none>\n    Command:\n      /pachd\n      --mode\n      sidecar\n    State:              Running\n      Started:          Tue, 29 Aug 2017 12:41:31 -0400\n    Ready:              True\n    Restart Count:      0\n    Environment:\n      BLOCK_CACHE_BYTES:        64M\n      PFS_CACHE_SIZE:           16\n      PACH_ROOT:                /pach\n      STORAGE_BACKEND:\n    Mounts:\n      /pach from pach-disk (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-c8nbj (ro)\nConditions:\n  Type          Status\n  Initialized   True \n  Ready         False \n  PodScheduled  True \nVolumes:\n  pach-bin:\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:\n  pachyderm-worker:\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:\n  pach-disk:\n    Type:       HostPath (bare host directory volume)\n    Path:       /var/pachyderm/pachd\n  default-token-c8nbj:\n    Type:       Secret (a volume populated by a Secret)\n    SecretName: default-token-c8nbj\n    Optional:   false\nQoS Class:      Burstable\nNode-Selectors: <none>\nTolerations:    <none>\nEvents:\n  FirstSeen     LastSeen        Count   From                    SubObjectPath                   Type            Reason                  Message\n  ---------     --------        -----   ----                    -------------                   --------        ------                  -------\n  2m            2m              1       kubelet, minikube       spec.initContainers{init}       Normal          Created                 Created container\n  2m            2m              1       kubelet, minikube                                       Normal          SuccessfulMountVolume   MountVolume.SetUp succeeded for volume \"pach-disk\" \n  2m            2m              1       kubelet, minikube                                       Normal          SuccessfulMountVolume   MountVolume.SetUp succeeded for volume \"pachyderm-worker\" \n  2m            2m              1       kubelet, minikube                                       Normal          SuccessfulMountVolume   MountVolume.SetUp succeeded for volume \"pach-bin\" \n  2m            2m              1       kubelet, minikube                                       Normal          SuccessfulMountVolume   MountVolume.SetUp succeeded for volume \"default-token-c8nbj\" \n  2m            2m              1       kubelet, minikube       spec.initContainers{init}       Normal          Pulled                  Container image \"pachyderm/worker:1.5.3\" already present on machine\n  2m            2m              1       kubelet, minikube       spec.initContainers{init}       Normal          Started                 Started container\n  2m            2m              1       default-scheduler                                       Normal          Scheduled               Successfully assigned pipeline-copy-v1-5cqbr to minikube\n  2m            2m              1       kubelet, minikube       spec.containers{storage}        Normal          Pulled                  Container image \"pachyderm/pachd:1.5.3\" already present on machine\n  2m            2m              1       kubelet, minikube       spec.containers{storage}        Normal          Started                 Started container\n  2m            2m              1       kubelet, minikube       spec.containers{storage}        Normal          Created                 Created container\n  2m            40s             5       kubelet, minikube       spec.containers{user}           Normal          Pulled                  Container image \"ubuntu:16.04\" already present on machine\n  2m            40s             5       kubelet, minikube       spec.containers{user}           Normal          Started                 Started container\n  2m            40s             5       kubelet, minikube       spec.containers{user}           Normal          Created                 Created container\n  2m            9s              10      kubelet, minikube       spec.containers{user}           Warning         BackOff                 Back-off restarting failed container\n  2m            9s              10      kubelet, minikube                                       Warning         FailedSync              Error syncing pod\n  . @joey, it is linked under examples: http://pachyderm.readthedocs.io/en/latest/examples/readme.html. Added @JoeyZwicker. Thanks.. Thanks for bringing this to our attention @jonalm.  Were you running the make docker-build-microsoft-vhd from the root of the pachyderm repo?  Mine executes when running it there.  \n(also, I'm going to be scrubbing the deploy docs with some updates soon so I will take this into account then as well). Also, added here so we don't forget to address any issues for 1.6: https://github.com/pachyderm/pachyderm/issues/2224. @jpoon @jdoliner Thanks for the comments. My understanding was that provisioning the volumes on Azure wasn't as straightforward and, thus, we could yet use dynamic volumes. However, my info may be out of date. . Thanks for the contribution @poopoothegorilla! CI passed in https://github.com/pachyderm/pachyderm/pull/2257.  . @JoeyZwicker ready to be reviewed again.\n. LGTM. Thanks @poopoothegorilla. CI passed in: https://github.com/pachyderm/pachyderm/pull/2288. Cool @JoeyZwicker, I actually removed the dashboard from the local install for now.  It is default on everything else.  We can consider adding it back in, but I also think leaving it out makes the deploy a lot faster for some people (depending on connection).  Anyway, will merge this soon, before 1.6.. @JoeyZwicker I agree that we should probably generate something new for beginner troubleshooting.  I was just meaning that if they are already struggling, and then they see stuff about grep and fruit stand, it might be worse than just removing them for now and revisiting.. Yes, a repo-level non-triggering flag of some sort.\nIn this scenario they wouldn't want anything to happen when the persisted model is updated.  They would just want the latest model to be utilize for new commits on the other input, whenever those new commits happen.\nThis is related to the discussions we have had about update-pipeline and models.  Sometimes when you update a model, you don't want to re-calculate results for previous input.  You just want it to be utilized for new inputs.  This is basically the parallel of that when, instead of changing your code, you change your training data, for example.\nIf there are multiple new versions of the PM, you would just run on the HEAD.  The idea is to always use the latest PM when you make predictions, but not update for old input whenever the PM changes.  . Interesting use case @pragmasoft-ua, and I definitely see what you are saying. @JoeyZwicker I wonder if this is maybe some combo of what we are talking about above with a non-triggering flag/field maybe with some sugar around from_commit and the new delete-commit model.  Anyway, I think the flag/field we are discussing above might be a good step in that direction.. Thanks for reporting this @emk. @derekchiang thoughts?. LGTM. Thanks!. @jdoliner @derekchiang @gabrielgrant just a little update and details here to help reproduce this:\nReproduce with minikube\nEnvironment\n$ minikube version\nminikube version: v0.22.2\n$ pachctl version\nCOMPONENT           VERSION\npachctl             1.6.0rc8\npachd               1.6.0rc8\nStart minikube\n$ minikube start\nStarting local Kubernetes v1.7.5 cluster...\nStarting VM...\nGetting VM IP address...\nMoving files into cluster...\nSetting up certs...\nConnecting to cluster...\nSetting up kubeconfig...\nStarting cluster components...\nKubectl is now configured to use the cluster.\nCreate the amazon-secret\nCreate a base64 encoded secret file with the id, secret, token, and region (us-west-2 in this case).\n$ cat secret.json\n{\n  \"kind\": \"Secret\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"name\": \"amazon-secret\"\n  },\n  \"data\": {\n    \"id\": \"<my encoded id>\",\n    \"secret\": \"<my encoded secret>\",\n    \"token\": \"\",\n    \"region\": \"dXMtd2VzdC0yCg==\"\n  }\n}\nAdd the secret to k8s:\n$ kubectl create -f secret.json\nsecret \"amazon-secret\" created\nCreate a Pachyderm local manifest w/ the secret\nRun deploy with --dry-run to get a local manifest:\n$ pachctl deploy local --dashboard --dry-run > manifest2.json\nUpdate the manifest so as to attached the amazon-secret to pachd:\nhttps://gist.github.com/dwhitena/2ef09c264dd133516c954f8fd940a751\nDeploy Pachyderm\n$ kubectl create -f manifest2.json\nserviceaccount \"pachyderm\" created\ndeployment \"etcd\" created\nservice \"etcd\" created\nservice \"pachd\" created\ndeployment \"pachd\" created\nservice \"dash\" created\ndeployment \"dash\" created\nAnd confirm that things are up and running:\n```\nkubectl get all\nNAME                        READY     STATUS    RESTARTS   AGE\npo/dash-1752167053-xgzmz    2/2       Running   0          23m\npo/etcd-2142892294-p0299    1/1       Running   0          23m\npo/pachd-1887717747-7cnjm   1/1       Running   0          23m\nNAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)                                     AGE\nsvc/dash         10.0.0.95           8080:30080/TCP,8081:30081/TCP               23m\nsvc/etcd         10.0.0.132          2379:32379/TCP                              23m\nsvc/kubernetes   10.0.0.1             443/TCP                                     31m\nsvc/pachd        10.0.0.187          650:30650/TCP,651:30651/TCP,652:30652/TCP   23m\nNAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeploy/dash    1         1         1            1           23m\ndeploy/etcd    1         1         1            1           23m\ndeploy/pachd   1         1         1            1           23m\nNAME                  DESIRED   CURRENT   READY     AGE\nrs/dash-1752167053    1         1         1         23m\nrs/etcd-2142892294    1         1         1         23m\nrs/pachd-1887717747   1         1         1         23m\n```\nTry putting a file from S3\n$ pachctl put-file test master -c -f s3://object-detection-model/ssd_mobilenet_v1_coco_11_06_2017.tar.gz\nInvalidEndpointURL: invalid endpoint uri\ncaused by: parse https://s3.us-west-2\n.amazonaws.com/{Bucket}/{Key+}: invalid character \"\\n\" in host name\nNote - I have confirmed that the base64 encoded region does not have a new line character at the end.\nReproduce on AWS\nI tried both the one shot script and manual deploy, and I had unrelated errors with both of the deploys (outside of this S3 issue).  The one shot error is a bug documented here: https://github.com/pachyderm/pachyderm/issues/2372, and the error I got on the manual deploy was DNS related (i.e., not related to Pachyderm).  I will try again later, but we definitely need to update the docs for the one shot script or fix that problem.. Ok, thanks to @sjezewski got the AWS deploy issues resolved and tried to reproduce the issue in AWS.  The S3 put-file works as long as it's a bucket in the same region as the cluster. This leads me to believe that it is something about the way I created the secret above. \nThat being said, the user that reported this wasn't using minikube.  They were in AWS, so maybe a recent change fixed this.  @sav are you able to check this with the latest RC and/or with 1.6.0 when it is released? \nFail to reproduce in AWS\nDeploy Pachyderm in AWS\nI used a slightly modified version of the one shot script (to fix issue from https://github.com/pachyderm/pachyderm/issues/2372), and ended up with the following cluster:\n```\n$ pachctl version\nCOMPONENT           VERSION\npachctl             1.6.0rc8\npachd               1.6.0rc8\n$ kubectl get all\nNAME                        READY     STATUS    RESTARTS   AGE\npo/etcd-0                   1/1       Running   0          3m\npo/etcd-1                   1/1       Running   0          3m\npo/etcd-2                   1/1       Running   0          3m\npo/pachd-1695804715-bpkcg   1/1       Running   0          3m\nNAME                CLUSTER-IP      EXTERNAL-IP   PORT(S)                                     AGE\nsvc/etcd            100.65.29.90           2379:31168/TCP                              3m\nsvc/etcd-headless   None                    2380/TCP                                    3m\nsvc/kubernetes      100.64.0.1              443/TCP                                     6m\nsvc/pachd           100.69.17.189          650:30650/TCP,651:30651/TCP,652:30652/TCP   3m\nNAME                DESIRED   CURRENT   AGE\nstatefulsets/etcd   3         3         3m\nNAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeploy/pachd   1         1         1            1           3m\nNAME                  DESIRED   CURRENT   READY     AGE\nrs/pachd-1695804715   1         1         1         3m\n```\nTry putting a file from S3\n$ pachctl put-file test master -c -f s3://newlinetest2/airplanes.jpg\n$ pachctl list-file test master\nNAME                TYPE                SIZE\nairplanes.jpg       file                38.72KiB. @sarovios Gotcha, well this should be fixed in the newest 1.6 release.  Basically, in these cases you need to create the amazon-secret manually if you want to connect to S3, and doing it the way that k8s recommends results in new line characters in the secret.  Now we tolerate and deal with those characters.. @sarovios The latest is v1.6.0, which will be officially released later this morning.  It should be fixed in that release.. @gabrielgrant I didn't know we had service docs. haha. I will look into this.. @jdoliner any update here or thoughts for @gauti038?. Also, it appears that there is a DNS related issue when I manually update the command to the latest version:\nW1003 10:59:13.800685   13140 executor.go:109] error running task \"DNSZone/kubernetes.com\" (9m59s remaining to succeed): error associating VPC with hosted zone \"kubernetes.com\": LimitsExceeded: Limits Exceeded: Cannot associate more VPCs to hosted zone.\n        status code: 400, request id: 6b39e841-a84b-11e7-a5d2-09d8d35a3b4f\nI1003 10:59:13.800787   13140 executor.go:91] Tasks: 43 done / 63 total; 16 can run\nI1003 10:59:19.453050   13140 launchconfiguration.go:320] waiting for IAM instance profile \"nodes.616db8c8-pachydermcluster.kubernetes.com\" to be ready\nI1003 10:59:19.461799   13140 launchconfiguration.go:320] waiting for IAM instance profile \"masters.616db8c8-pachydermcluster.kubernetes.com\" to be ready\nW1003 10:59:29.946099   13140 executor.go:109] error running task \"DNSZone/kubernetes.com\" (9m43s remaining to succeed): error associating VPC with hosted zone \"kubernetes.com\": LimitsExceeded: Limits Exceeded: Cannot associate more VPCs to hosted zone.\n        status code: 400, request id: 6be03a21-a84b-11e7-a516-d97526d8e4a7\nI1003 10:59:29.946172   13140 executor.go:91] Tasks: 58 done / 63 total; 3 can run\nW1003 10:59:30.450248   13140 executor.go:109] error running task \"DNSZone/kubernetes.com\" (9m42s remaining to succeed): error associating VPC with hosted zone \"kubernetes.com\": LimitsExceeded: Limits Exceeded: Cannot associate more VPCs to hosted zone.\n        status code: 400, request id: 753d3b56-a84b-11e7-a5d2-09d8d35a3b4f\nI1003 10:59:30.450360   13140 executor.go:91] Tasks: 60 done / 63 total; 1 can run\nW1003 10:59:30.807324   13140 executor.go:109] error running task \"DNSZone/kubernetes.com\" (9m42s remaining to succeed): error associating VPC with hosted zone \"kubernetes.com\": LimitsExceeded: Limits Exceeded: Cannot associate more VPCs to hosted zone.\n        status code: 400, request id: 7593bf38-a84b-11e7-a5d2-09d8d35a3b4f\nI1003 10:59:30.807430   13140 executor.go:124] No progress made, sleeping before retrying 1 failed task(s)\nI1003 10:59:40.807676   13140 executor.go:91] Tasks: 60 done / 63 total; 1 can run\nW1003 10:59:41.142839   13140 executor.go:109] error running task \"DNSZone/kubernetes.com\" (9m31s remaining to succeed): error associating VPC with hosted zone \"kubernetes.com\": LimitsExceeded: Limits Exceeded: Cannot associate more VPCs to hosted zone.\n        status code: 400, request id: 7bb93bb9-a84b-11e7-a516-d97526d8e4a7\nI1003 10:59:41.143043   13140 executor.go:124] No progress made, sleeping before retrying 1 failed task(s)\nI1003 10:59:51.143542   13140 executor.go:91] Tasks: 60 done / 63 total; 1 can run\nW1003 10:59:51.492270   13140 executor.go:109] error running task \"DNSZone/kubernetes.com\" (9m21s remaining to succeed): error associating VPC with hosted zone \"kubernetes.com\": LimitsExceeded: Limits Exceeded: Cannot associate more VPCs to hosted zone.\n        status code: 400, request id: 81e34c22-a84b-11e7-9f9f-d19c06ccfd46\nI1003 10:59:51.492358   13140 executor.go:124] No progress made, sleeping before retrying 1 failed task(s). Thanks @williballenthin!. @rmuellerjr and others. Currently, pachctl works in the WSL, and can be installed via the *.deb that is specified in the docs. The other issue is really minikube. However, now Docker for windows desktop apparently includes k8s, which should make that part easier. We just haven't tested it yet. I will hopefully do this for the Mac Desktop version soon, which should be similar.. Thanks for the suggestion. This would definitely be useful!. I think this is mainly for the retry logic @JoeyZwicker . Yep @JoeyZwicker. Just let me know when the CLA is signed @jiahao!. Fixed here: https://github.com/pachyderm/pachyderm/pull/2520. Closing.. @philipjhj Great questions, and appreciate the feedback. Currently the best practice would be to use unique tags for your Docker images, not latest.  This can be enabled in a couple of ways:\n\n\nHave some CI pipeline build new Docker images when you push to your code repo and then update the Pachyderm pipeline with update-pipeline and the new Docker image tag.\n\n\nUse the \"push images\" flag on pachctl update-pipeline .... This will basically do this unique tagging for you automatically after you build a Docker image locally.\n\n\nLet me know if that makes sense and/or if there are follow up questions.  In either one of these cases, you will be able to connect commits with the unique Docker tags and see them with inspect-pipeline. . @sjezewski and @jdoliner, I love the discussion here around the local dev environment and making the initial building of pipelines easier. I'm a big fan of the checkout-* family of commands, and I don't really have any other comments about that. I like how that has been presented above, without any changes.\nHowever, I do have some comments/questions about the actual workflows that users would use to run their workflow against checked out data (e.g., test-pipeline). My comments don't necessarily mean that I don't think test-pipeline would be useful, but I think we need to be very careful about how we implement and advertise this workflow.\nI'm approaching this from a non-CS, data scientist point of view, which is definitely the target audience here I think. From that point of view, you are absolutely right that:\n\nData scientists don't want to add a bunch of complexity to their workflows or generally change how they develop. There is a ton of evidence for this.\nData scientists aren't, in general, the most familiar with Docker (although this is gradually beginning to change).\n\nThat being said, we really need to be careful that we don't shoot ourselves in the foot here. By that I mean the following:\n\nWe don't want to introduce another type of workflow that is too unfamiliar to users. If we do this, we will just end up with two new workflows that data scientists struggle to master, rather than one.\nWe don't want to make the local development workflow so different from the cloud deployment workflow that (i) users can't see and learn the unique advantages of Pachyderm, and (ii) can't have reproduce the local behavior on a real cluster.  Both of these will end up with users hitting a wall and giving up (I think).\n\nI've been wrestling with these ideas ever since our planning yesterday, and I have come to some conclusions and a proposal (which might be an alternative for test-pipeline or something in addition to test-pipeline). These conclusions are based on what I have observed in the data science world with respect to systems that data scientists seem to latch onto very quickly (like Domino, Databricks, and others).\nWorkflow, Workbench, and Dev Environments that DS's Like\nIn my experience Data Scientists like to experiment with their data locally and iteratively develop code against samples of data locally.  They like to do this with their own tooling outside of whatever \"system\" or \"platform\" their company is using, be that Domino, Databricks, or whatever. Don't have much to say here, because our checkout-* family of commands satisfies this requirement.\nWhen it comes to putting these locally tested things into pipelines or testing them in a way that they will run them in the cloud, they would prefer to have:\n\n\nA finite choice of already configured environments with the things they need to use. Domino enables this, for example, with reusable environments (that are actually just Docker containers under the hood).  For example, they may be a Python base Data Science environment with scikit-learn, pandas, etc. and a base Tensorflow environment, etc. Obviously this is not great Docker-wise, because you end up with a bunch of bloated images, but this is what data scientists like to use.\n\n\nA way to run their code in the environment that they select. They want to spin up a Jupyter notebook in a Tensorflow environment or supply a python command/code to run in a Python base environment.  Once this runs, they want to interactively look at the results (e.g., via Jupyter).\n\n\nMy Proposal\nMy proposal for dealing with this local to deployed pipeline transition includes three components:\n\n\ncheckout-* commands - as discussed above for playing with sample data locally in your own environment.\n\n\nA \"code injection\" option in pipeline specs (maybe inject_code or add_code) - I will detail this below. It is closely related to some of the things @sjezewski has proposed, but with some slight modifications.\n\n\nOur planned Jupyter integration in the dashboard - This is going to bring soooo much value, but I won't comment on it in detail here.\n\n\nRegarding 2, I propose that via Git or otherwise, we provide a way for data scientists to: \n\nSpecify one of the many data science Docker images that are already publicly available and have basically anything they want: scikit-learn, pandas, tensorflow, etc.\nAn inject_code or add_code field in our pipeline spec that would allow users to specify a GH repo, tar ball, or zip with their code.\n\nWhen the pipeline is run, they would be able to then run their injected or added code in the environment that was already prepared for them, via the cmd and stdin.  They wouldn't have to build any docker images, have docker installed, etc.  \nAfter the code runs (and assuming we have Jupyter integration), they could then interactively analyze the results of their runs in Jupyter, without needing to checkout or pull the data down.\nAdvantages of add_code\n\nData scientists wouldn't have to touch Docker if they didn't want to.\nThey could run the exact same pipeline in a production cluster. This is hugely important, and I think it is the biggest different with test-pipeline. Because, unless I'm missing something with test-pipeline, I don't think we could guarantee that a pipeline that was successful with test-pipeline would work in an actual cluster, and there could still be friction when having to actually build a docker image or productionize what they test.\n\nDisadvantages of add_code\n\nData scientists would need to find a relevant Docker image, and there might be weirdness in third party images (like where dependencies are installed etc.).  We could definitely mitigate this by having a set of officially supported DS images with certain things (which we have talked about before), and/or just documenting good DS images and what they include.  Ideally, this should be as easy as saying, I want to run with scikit-learn, pandas, and matplotlib, find the relevant image, and write it in the pipeline spec.\nThe Docker images that would be used wouldn't be the most ideal. They would likely be large, bloated, and not so portable. Of course, if this becomes a problem, they can always make the leap to creating a custom image.\n\n. @sjezewski If this is the case:\n\nwe'll be able to specify a github repo as an input, so that users can write transforms that use the code from there\n\nI think that will be a huge win for local dev as well. Essentially, users could then have a workflow that completely avoids building custom Docker images. They could find a base image that has a bunch of stuff they need, and just inject their code.\nThis would let Data Scientists do that even when testing locally with minikube, etc., and I think we could have a really, really quick route from \"I'm a user with code\" to \"I'm running that code in Pachyderm,\" which is the overall goal here I think.\nMy concerns with test-pipeline are mostly around the fact that it isn't really a \"test\" of the pipeline. If I'm using my local environment, the pipeline could still behave differently when I actually build the images and deploy it. Am I misunderstanding @jdoliner?. > It seems weird to call this local dev, this is really just semantics but in my mind local dev means the code runs on the same machine the code is getting written on. \n@jdoliner  Yeah, I guess a better phrasing of this is \"local experimentation\" with something that you intend to deploy.  In my experience, data scientists like to do the following:\n\n\nUse what they are familiar with with local samples of data to figure out what they want to deploy (solved perfectly with your checkout-* family of commands).  In this stage, I have my doubts that they would want to do anything with Pachyderm (or any other platform they are using, like Domino or Databricks), outside of pulling sample data out of Pachyderm.\n\n\nTransition that to some \"environment\" where they can essentially do the same thing, but deploy it at some point. At this point, I think they would accept the fact that they need some config (i.e., the specification), but it would be nice if they didn't have to learn this along with Docker (at least in the beginning).\n\n\nIn my mind, using the git, zip, or tar ball injection or adding of code let's users do 2, without a need to learn docker, etc. They can essentially use some environment that is already prep'ed, and then transition super easy to something that runs the same in minikube or on a Pachyderm cluster.  If I'm developing things, I think that's what I would want to do.  . > make it possible to run the code in docker\n@jdoliner Regarding that point, is that different than injecting the code via GH, zip, or tar ball or maybe having a GH repo as input? In any event, I think something like that is needed.. fixed here: https://github.com/pachyderm/pachyderm/pull/2448. Yeah agreed @gabrielgrant. This is useful information that we should add.. This seems to be related: https://github.com/pachyderm/pachyderm/issues/2475, and maybe there is some significant overlap in these docs?. Should be covered in these docs:\n- http://pachyderm.readthedocs.io/en/latest/deployment/upgrading.html\n- http://pachyderm.readthedocs.io/en/latest/deployment/migrations.html. Changes LGTM @msteffen . @gabrielgrant Looks like this has maybe been taken care of. However, there is a confusing message displayed when port-forwarding, which is being addressed in a different issue https://github.com/pachyderm/pachyderm/issues/1728. Closing for now.\n```\n\u279c  pachyderm git:(jeffm14vt-docs-fix-typos) \u2717 pachctl deploy local\nserviceaccount \"pachyderm\" created\ndeployment \"etcd\" created\nservice \"etcd\" created\nservice \"pachd\" created\ndeployment \"pachd\" created\nsecret \"pachyderm-storage-secret\" created\nPachyderm is launching. Check its status with \"kubectl get all\"\n``. We should add a note in the docs. Tagging.. Fixes https://github.com/pachyderm/pachyderm/issues/2421. Yeah, I'm not sure about the specific motivation behindpachctl deploy storage ...`. Maybe @jdoliner or @derekchiang can comment. In any event, we have it as of now and people can do this if they like, I guess. \nI could be wrong, but for the built in ingress/egress, the secret needs to be added to the pachd pod (as opposed to the worker pods).  For other types of ingress (e.g., a database) you can just add the secret and then access it from a worker. I think that is the main difference and motivator for this functionality. . I think what we have added recently provides a general overview. Closing for now.. Maybe just leave it lowecase \"Joining,\" b/c we aren't actually doing an database JOIN officially. Otherwise, I'm fine with this, unless @JoeyZwicker doesn't want it. I know we had a previous conversation about leaving the \"Join\" terminology out for certain users.. Thanks @gabrielgrant. You can merge this any time. I do use \"joining\" in the cookbook, but I can revisit how we can make that more clear. However, we tried to expand the wording a bit beyond join on purpose, because people wanting to do not-so-traditional joining stuff were getting confused when we tried to tell them how to \"join\" in pachyderm. Hence, the combining/merging/joining language. . @JoeyZwicker When you say the common issues, can we list so here (b/c I might not be aware of all of them). Also maybe @jdoliner or @msteffen want to comment on this?  I think some of the issues are:\n\nYou used dynamic volumes in an initial deploy, now you want to reuse that volume(s) in the newly deployed cluster, so you have to use static volumes or do something else different.\nSometimes you seeing warnings when re-deploying, deploying the dashboard on a previous deployment etc., are these benign?\nReconnecting pachctl and kubectl etc. with the new cluster\n\nothers?. @jdoliner Any update here. Do you know if we can squeeze this into the next release as a couple users have been asking (cc: @JoeyZwicker ).. LGTM. CI passed here: https://github.com/pachyderm/pachyderm/pull/2493. Interesting. Thanks for reporting @jonandernovella. @jdoliner thoughts here?. @JoeyZwicker Updated. Also, at least partially, fixed some confusion from: https://github.com/pachyderm/pachyderm/issues/2254. Ah this is a great simplification! Love it. I will open a corresponding PR to run CI @JoeyZwicker.\n. @JoeyZwicker I get this question and request soooooo frequently. I think it's pretty reasonable and natural for us to support commit messages for commits into Pachyderm data repositories. It has a ton of use cases as well that people like:\n\n\nOnce you figure out provenance, you can quickly deduce the purpose/intent of the original changes.\n\nMakes actual collaboration of pipelines smoother in many ways\nProvides a nice changelog/history of changes to data, more so than a listing of commit ids\n\netc.... It appears that 1.7 fixes this, based on my tests with the RC. Closing for now.. Thanks for reporting this @amanderson! I think @JoeyZwicker is aware of this and looking into it. Can you confirm @JoeyZwicker?. CI passed here: https://github.com/pachyderm/pachyderm/pull/2524. Cool. Thanks for the details @scheidec! So this appears to be an apparent issue related to the way we are handling IAM roles. We will definitely try to reproduce.. @msteffen @sjezewski Does this have to do with some error we had for that release? Thanks for pointing this out @thedrow!. @sjezewski or @msteffen could you take a look here as this impacts Travis? I'm running the CI for this now, and will merge if that part looks ok to you.. Ping on this @sjezewski or @msteffen.. @thedrow @sjezewski new CI run here: https://github.com/pachyderm/pachyderm/pull/2579. @sjezewski CI passed. Is this ready to merge?. Thanks @thedrow. Let us know what you conclude here.. Running CI here: https://github.com/pachyderm/pachyderm/pull/2540. @jdoliner or @msteffen do you have an opinion here?. @atombender Thanks for the clarification and comment. I guess this is under documentation right now, because we need to:\n\n\nMake sure what is and isn't supported in this regard,\n\nBe clear about that in the documentation\n\nThen we can hopefully move on from there to expanding our support for these types of custom configuration.. @nikhilalmeida Still delayed here, as we are waiting on official preview access like many others. Hopefully we will be able to update this soon after we finish testing.. CI here: https://github.com/pachyderm/pachyderm/pull/2554. @sjezewski Do you know why the CI is failing for this (in PR https://github.com/pachyderm/pachyderm/pull/2554). I'm getting the following:\n```\ngithub.com/pachyderm/pachyderm/src/server/pps/cmds\nsrc/server/pps/cmds/cmds.go:784:17: undefined: docker\nsrc/server/pps/cmds/cmds.go:788:13: undefined: docker\nsrc/server/pps/cmds/cmds.go:800:35: undefined: docker\nsrc/server/pps/cmds/cmds.go:807:17: undefined: docker\nsrc/server/pps/cmds/cmds.go:809:16: undefined: docker\nsrc/server/pps/cmds/cmds.go:813:23: undefined: docker\nsrc/server/pps/cmds/cmds.go:827:3: undefined: docker\nmake: *** [install] Error 2\n```\nOnce we get that taken care of we can merge this I think.. @jeffm14vt was there a reason that github.com/fsouza/go-dockerclient was removed here. I think it's causing CI to fail. . Thanks @jeffm14vt. Rerunning CI.. Thanks! LGTM. . Thanks @poopoothegorilla. I will run CI in another PR.. CI here: https://github.com/pachyderm/pachyderm/pull/2555. @sjezewski @msteffen Any idea why CI might be failing here. Just a small docs change?. @JoeyZwicker no, but I created this to help me remember: https://github.com/pachyderm/pachyderm/issues/2562. Any idea why CI might be failing here @sjezewski @msteffen. I have restarted several times with the same error.. @sjezewski merged in master, but still issues:\n```\n+kubectl version\n+sleep 5\n+kubectl version\n+sleep 5\n+kubectl version\n+sleep 5\n+kubectl version\n+sleep 5\nThe job exceeded the maximum time limit for jobs, and has been terminated.\n```. Thanks for reporting this, and I agree it is confusing. We are currently working on fixing this as tracked by https://github.com/pachyderm/pachyderm/issues/2444 and https://github.com/pachyderm/pachyderm/issues/1728. This should be fixed soon, but closing this as a duplicate.. @olegserov Can you provide information about your version of Pachyderm and Kubernetes. We recently updated certain commands to streaming to deal with this issue, so it could be a version thing. If not, we should definitely investigate what's happening here. Thanks in advance!. Hi @olegserov. Thanks for the feedback. There are a couple of things that you can do here:\n\nSet ADDRESS directory instead of using port-forward, as forwarding includes some rate limiting.\nutilize the resumable upload functionality: https://github.com/pachyderm/pachyderm/pull/2293\nupload these from an object store or http link\n\nThose things should help in these scenarios. Let us know if you have other ideas, but closing for now as the resumable upload is the primary fix for this, and thanks again!. Thanks @olegserov! We are currently working on this: https://github.com/pachyderm/pachyderm/issues/2359, so I will close this as a duplicate.. Fixed by https://github.com/pachyderm/pachyderm/pull/2592. @anuragsoni9 Just to follow up here on our discussion in Slack, it would be great to add this information to all the places where pachctl installation is mentioned. Again, no pressure, but if you feel lead to add this, it's mentioned in the following:\nhttp://pachyderm.readthedocs.io/en/latest/deployment/google_cloud_platform.html\nhttp://pachyderm.readthedocs.io/en/latest/deployment/amazon_web_services.html\nhttp://pachyderm.readthedocs.io/en/latest/deployment/azure.html. Thanks @anuragsoni9! Sorry for the delay with the holidays. CI running here: https://github.com/pachyderm/pachyderm/pull/2604. Once that passes we will merge.. Thanks so much @frankhinek! CI running here: https://github.com/pachyderm/pachyderm/pull/2591. Looks like this might be a duplicate of what's in https://github.com/pachyderm/pachyderm/pull/2588. Closing for now, but correct me if I'm wrong.. @jdoliner I'm not sure why my environment wasn't printing out to stdout with the errgroup last time. Now I seem to not be able to reproduce that. Oh well. Simplified this.\n@JoeyZwicker Updated the messaging.. @jdoliner does this make more sense technically? If so, I can try to get CI passing and merge.. @jdoliner made the change you requested, although having trouble getting CI to pass. . Ah, darn, ok. Fixed. Not sure how that got through the linter in my IDE. I need to check that. \nThanks @jdoliner. . This is great @frankhinek, thank you so much. CI running here: https://github.com/pachyderm/pachyderm/pull/2628. Thanks @brycemcanally! I think you were discussing this with @jdoliner. @jdoliner do you have input here?. Thanks for the suggestions @brycemcanally. Any opinions on this @jdoliner or @msteffen?. CI is failing b/c of an intermittent problem I think, but merging b/c this a docs change. Thanks again!. Thank you very much @dionjwa. Have you signed the CLA? I'll kick off CI, which needs someone internal to Pachyderm to kick it off.. Thanks! merging.. @jdoliner @gabrielgrant Are you good with these changes? If so, I will merge in.. Hi @antjkennedy, I corrected a small merge conflict here. Also, I'm going to create a companion PR that updates the AWS, GCP, and beginner tutorial docs based on the dashboard being deployed by default. I need to have this in place when we merge for consistency. Working on that today.. @jdoliner CI passed here: https://github.com/pachyderm/pachyderm/pull/2710. Are you ok with the code changes here? If so, I will get @JoeyZwicker's ok on my companion doc changes and then merge both PRs.. Ok, CI is passing here: https://github.com/pachyderm/pachyderm/pull/2710. Merging.. More ideas here:\n\nOmer Katz [10:37 AM]\npachctl status could be useful for other stuff like alerting when etcd or pachyderm is out of disk space or other important operational aspects\nwe could use a pachctl status command and API. The API can be used to push to data to heapster/prometheous/whatever. Thank you for all the details @hchauvin, we will investigate this. Just to confirm are you able to retrieve test.txt as expected with pachctl get-file? \n\n@jdoliner or @msteffen any ideas here?. Thanks @hchauvin.\n@jdoliner has we considered making the number of retries configurable? I remember someone else asking me about this recently as well.. Thanks for the details here @hchauvin. I would have expected just the JSON to be printed as well, which I think is what happens when --raw is called on get-logs. @jdoliner or @msteffen maybe there is a bit of inconsistency here?. Ping @jdoliner or @JoeyZwicker here.. Thanks for reporting @hchauvin. We will look into this. @msteffen any initial ideas here?. Thanks @jdoliner. I'm reviewing this now, and I'm going to probably push some changes directly to this branch.. @jdoliner ok, revised these such that the deleting data and adding data docs are separate. I also went ahead and added in a section about adding data via the dashboard. . Looks like some linting errors here @jdoliner:\n$ make lint\n./src/server/pkg/backoff/example_test.go:32:6: should omit values from range; this loop is equivalent to `for range ...`\ngolint errors!. Hi @hchauvin, I actually wasn't aware of this restriction either, but I guess I can see both sides. I think there is probably a good compromise that we could figure out here, but I will defer that discussion to @jdoliner or @msteffen. Thanks again for the input!. Ah yeah, that makes sense. I think this will likely be confusing to many people. We can document it, but people still might make assumptions if they are used to normal cron. Can we utilize the normal cron without seconds for the \"*/1 * * * *\" format, and then maybe enable seconds for the every format?. Yeah, I think this would'nt be that bad to implement, because we already pass through things like resource limits and requests to k8s. However, we should decide what we want to expose to the Pachyderm user @jdoliner. As we could technically pass through many things, which might be a bit overwhelming. Although the above ask, or at least part of it, might be very useful.. @jdoliner not sure this is what you are looking for, as it doesn't really give much more info. Basically, the new 1.7 pachd never gets out of CrashLoopBackoff, and I tried to follow the logs as you suggested, but just got more of the same:\n```\n$ pachctl deploy local\nkWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply\nserviceaccount \"pachyderm\" configured\nWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply\nclusterrole \"pachyderm\" configured\nWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply\nclusterrolebinding \"pachyderm\" configured\nWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply\ndeployment \"etcd\" configured\nWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply\nservice \"etcd\" configured\nWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply\nservice \"pachd\" configured\nWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply\ndeployment \"pachd\" configured\nservice \"dash\" created\ndeployment \"dash\" created\nWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply\nsecret \"pachyderm-storage-secret\" configured\nPachyderm is launching. Check its status with \"kubectl get all\"\nOnce launched, access the dashboard by running \"pachctl port-forward\"\n$ kubectl get pods\nNAME                             READY     STATUS              RESTARTS   AGE\ndash-6c9dc97d9c-5bghl            0/2       ContainerCreating   0          3s\netcd-7dbb489f44-gs27j            1/1       Running             0          6m\npachd-784f7ffc47-ls5vw           1/1       Running             0          6m\npachd-86f57c689b-5kmzr           0/1       ContainerCreating   0          3s\npipeline-model-train-v1-zlgcc    2/2       Running             0          2m\npipeline-patient-rank-v1-nvtwk   2/2       Running             0          2m\npipeline-patient-sort-v1-42pnh   2/2       Running             0          2m\npipeline-pre-process-v1-7xsln    2/2       Running             0          2m\n$ kubectl logs -f pachd-86f57c689b-5kmzr\ntime=\"2018-03-06T15:33:13Z\" level=info msg=\"validating kubernetes access returned no errors\"\n2018-03-06T15:33:13Z INFO authclient.API.GetCapability {\"request\":{}}\n2018-03-06T15:33:13Z INFO authclient.API.GetCapability {\"duration\":0.091193843,\"request\":{},\"response\":{\"capability\":\"c7f5e3601f08419aaa668451696b564a\"}}\n2018/03/06 15:33:13 INFO: Listening on addr: :999 path: /v1/handle/push\n2018-03-06T15:33:13Z INFO admin.API.Restore {\"request\":null}\n2018-03-06T15:33:13Z INFO pfs.API.CreateRepo {\"request\":{\"repo\":{\"name\":\"raw_data\"}}}\n2018-03-06T15:33:13Z ERROR pfs.API.CreateRepo {\"duration\":0.294630956,\"error\":\"cannot create \\\"raw_data\\\" as it already exists\",\"request\":{\"repo\":{\"name\":\"raw_data\"}},\"response\":null}\n2018-03-06T15:33:13Z ERROR admin.API.Restore {\"duration\":0.29729892,\"error\":\"error creating repo: cannot create \\\"raw_data\\\" as it already exists\",\"request\":null}\nerror creating repo: cannot create \"raw_data\" as it already exists\n$ kubectl get pods\nNAME                             READY     STATUS             RESTARTS   AGE\ndash-6c9dc97d9c-5bghl            2/2       Running            0          24s\netcd-7dbb489f44-gs27j            1/1       Running            0          7m\npachd-784f7ffc47-ls5vw           1/1       Running            0          7m\npachd-86f57c689b-5kmzr           0/1       CrashLoopBackOff   1          24s\npipeline-model-train-v1-zlgcc    2/2       Running            0          3m\npipeline-patient-rank-v1-nvtwk   2/2       Running            0          3m\npipeline-patient-sort-v1-42pnh   2/2       Running            0          3m\npipeline-pre-process-v1-7xsln    2/2       Running            0          3m\n$ kubectl logs -f pachd-86f57c689b-5kmzr\ntime=\"2018-03-06T15:33:30Z\" level=info msg=\"validating kubernetes access returned no errors\"\n2018-03-06T15:33:30Z INFO authclient.API.GetCapability {\"request\":{}}\n2018-03-06T15:33:30Z INFO authclient.API.GetCapability {\"duration\":0.00277906,\"request\":{},\"response\":{\"capability\":\"5d0dce54c117441387bcabf9f29e7061\"}}\n2018/03/06 15:33:30 INFO: Listening on addr: :999 path: /v1/handle/push\n2018-03-06T15:33:30Z INFO admin.API.Restore {\"request\":null}\n2018-03-06T15:33:30Z INFO pfs.API.CreateRepo {\"request\":{\"repo\":{\"name\":\"raw_data\"}}}\n2018-03-06T15:33:30Z ERROR pfs.API.CreateRepo {\"duration\":0.308369507,\"error\":\"cannot create \\\"raw_data\\\" as it already exists\",\"request\":{\"repo\":{\"name\":\"raw_data\"}},\"response\":null}\n2018-03-06T15:33:30Z ERROR admin.API.Restore {\"duration\":0.40428309,\"error\":\"error creating repo: cannot create \\\"raw_data\\\" as it already exists\",\"request\":null}\nerror creating repo: cannot create \"raw_data\" as it already exists\n```. @jdoliner any insight here? I have yet to successfully migrate.\n@gabrielgrant any insight on the first dash error?. Woohoo! @jdoliner migration was successful for me after:\n\nDeploying from 1.6.x\nDeploying some pipelines\nApplying the manifest from master\n\nThanks!. Just hit this as well. Need to decide on the best solution. Then I can document it.. @jdoliner I followed all of the steps in the GCP docs except for the deploy command. I just added the namespace flag to that:\npachctl deploy google ${BUCKET_NAME} ${STORAGE_SIZE} --dynamic-etcd-nodes=1 --no-rbac --namespace pachyderm. Thanks for this info @jiaqi216. I seem to remember from Slack that you also tried this with 1.7.0rc, but that didn't solve the problem. Can you confirm?. @jiaqi216 Are you able to try this on 1.7.0rc2? I only mention this because the commit structure for jobs changed somewhat in 1.7, and this could be a non-issue. Just want to confirm. I was running 1.7.0rc2 when I tried to reproduce, and I didn't see the issue.. @jiaqi216 Both should match, so you would need to do the following (if possible, depending on what you need to save from the cluster etc.):\n\nundeploy your 1.6.x cluster with pachctl undeploy\nupgrade pachctl to 1.7.0rc2\ndeploy a 1.7.0rc2 cluster with the upgraded pachctl\nTry the pipeline again.\n\nYou will also be able to migrate automatically without the undeploy for the actual 1.7.0 release, but the above would be what you should do for now with the RC.. Thanks for you comments and contribution @naztyroc! This is great context for us as we improve the on prem docs. I think Jon Ander is reviewing your Helm chart PR, and we will move forward on the docs side here.. Thanks for the update @naztyroc. Will hopefully have these updated soon.\n@liuchenxjtu Thanks for reaching out. I would recommend using the Helm chart for your deploy: https://github.com/kubernetes/charts/tree/master/stable/pachyderm. You can use Minio, Rook, Ceph, Swift, etc. for your object store, and then use can use can any k8s PV for the volume: https://kubernetes.io/docs/concepts/storage/persistent-volumes/. @liuchenxjtu \nRegarding gluster FS, yes you can use gluster FS, which a Minio server as an S3 compatible layer between the two.\nRegarding the persistent disk, We are changing out on-prem default to use the Helm chart. That should clear up the confusion around the PVs. More details can be found here: https://github.com/kubernetes/charts/tree/master/stable/pachyderm. ping @JoeyZwicker . Note, that in the cases where Pachyderm fails to deploy and I get the above error, I have checked to ensure that the sa exists, and it does:\n$ kubectl get serviceaccounts\nNAME        SECRETS   AGE\ndefault     1         16m\npachyderm   1         26s. Thanks for the additional info @DSchmidtDev. Just to clarify for the notes here, I didn't deploy with a Helm chart. I used pachctl deploy ..., so I would imagine this is a problem with both routes to deployment @jdoliner.. @jdoliner or @msteffen input here?. Note, this was with 1.7.0rc2.. Thanks for the insight @rdefreitas!. @jdoliner or @msteffen can you take a quick look? . Hi @dyngts, I know GKE and Nvidia recently made updates to GPU support, and I know that Nvidia has patches to fix certain things with GKE. We will investigate this. However, you might also open an issue here to start some discussion: https://github.com/nvidia/k8s-device-plugin.\ncc @sjezewski @jdoliner . I added a quick note (which could definitely be expanded upon).  Feel free to add more here, if you think that some more specifics are needed before merging this.  I'm not sure I have a great grasp on all the edge cases.. Actually not, because it was uploading the directory structure to PFS as well (due to where the Makefile is located), and that was causing issues.  . Yes, I agree.  I mostly changed this to get it working. But I'm going to upload this to our Docker hub and change it.  . We can.  I will work on that.  I separated it out for now to at least force the separation of the two commands.. As we discussed in Slack, this is slightly more complicated because the user needs to flush commit and place the right commit id into the job spec.  This is documented in the readme, so I think shouldn't automate this for now.  Although, I guess we could use sed to replace it in the spec or something.. I'm assuming this was already vendored in?  Just making sure we weren't adding it for this purpose alone.. I think this is just meant to be a generic way of saying you can specify a binary that runs on data in PFS and writes out to PFS at the particular places.. Stole it from the pipeline specification doc.. Oh cool.  I will add.. Great question.  As far as I know, it would be required if you are pointing to a single file in S3.  However, it would be required if you are pointing to a directory (I think, we should confirm).  Thus the question for -r is really whether you are pointing to a directory or not.  Again, we should confirm.. Yeah, thanks.  I couldn't find the Scala client.  Is it even public or just WIP.  I can mention that they are being updated for 1.4 and create the issue (for Python at least).. Maybe link edge detection to https://en.wikipedia.org/wiki/Edge_detection.  Not guaranteed everyone will know right off the bat depending on background.. \"materialize\" can carry a bit of baggage.  Maybe just \"output\"?. Do we want  talk a lot about PFS here?  Maybe \"highest level data primitive in Pachyderm.  Like many things in Pachyderm, it shares it's name with primitives of Git...\". \"liberty.png\". Again, just a question of whether we want to use PFS/PPS language or just Pachyderm pipeline.  I've been trying to about PFS/PPS.  . Maybe use the image:\n\n. This is kind of cool, but I just wonder whether it's worth including here or maybe just including in the cookbook or somewhere.  There is the danger of them opening the directory structure and getting confused about how things are organized, duplicated, etc.  Not sure.  I trust your judgement either way.  Just provided an alternative point of view.. ../deployment/deploy_intro. ../fundamentals/getting_data_into_pachyderm. ../fundamentals/creating_analysis_pipelines. ../reference/pachyderm_file_system. ../reference/pachyderm_pipeline_system. ../deployment/deploy_intro. Hmmm.. I don't know.  It shows fine in my editor.. Yeah, we can update soon.  I just updated so it was 1.4 compliant.. Maybe \"big data or computationally intensive workflow\". I think if you just just use 1. and 2. here instead of 1) etc., it will be rendered as an actual numbered list.  Is that the same for 1) etc.?  . Is it enough to say \"spun up\"?  Will users know what that means?. Because in the next sentence you say \"pod,\" and this might be the first place in our docs where we are really talking about pods.. Ah, good here it is.  I would put this first before your \"spun up\" discussion.  Then it will make more sense.. Link to the pipeline spec?. +1, thanks.. This seems to be the first mention of \"sharding.\"  It looks like we are wanting this to be connected with question number (2) above.  Maybe mention sharding up there or rephrase this to more specifically connect it with the above question.  Also, you might consider: \n\ninternally linking each of the two questions above with the respective section below.\nactually adding the (1) and (2) numbers to the section titles.. maybe add a brief statement above this section about why we are going to talk about datums etc.  Just to guide the reader.. Maybe label this something like Defining datums via glob patterns?. \"desired\" instead of \"correct\". Combine with the sentence above.. I kind of feel like this should go under the (2) question along with datums (i.e., ## -> ###). What do you think?. Yep.  That is the plan, along with a few other things.  Just didn't have time yet. . Regarding including or not including JD's method (which in my understanding is (2)), wouldn't client side hashing be relevant to either (2) or (3).  That is, it seems like the client side hashing would be relevant to either of the rolling/moving time window examples, so I think including them both with the note would be fine.  Unless there is a super clear advantage of one over the other right now.. Definitely agree on the first two examples.  Actually, I don't know what I was totally thinking there.  Sorry.  Anyway, I will modify that.\n\nRegarding the map/reduce sort of language, I will emphasize that.  My only push back is that map/reduce language itself carries a lot of baggage, and in reality a user may have many different combinations of maps and reduces.  As such, I will add some statement in about aggregation within the time window etc, but will likely avoid too much distraction with map/reduce.. Honestly, I think it is out of our hands how the user defines now, to some degree.  We can make a note about time zones and UTC, but this is a problem that stretches far beyond the domain of Pachyderm.. Regarding (1), I think the standards (or at least what is popular) is without the /: http://stackoverflow.com/questions/19699059/representing-directory-file-structure-in-markdown-syntax\nRegarding (2), noted and will update.. Fixed.. Fixed. Thanks.. I did too. Unfortunately it doesn't render correctly in HTML.  You can check the current version to see or run make html from docs. See above.. Maybe this should go as a note near the top of this page (because it seems rather important)?  Maybe putting it right before the \"Prerequisites\" section like:\nNote - For production deployments..... Just some grammar stuff:\n\"...up AWS CloudFront.  AWS puts S3 rate limits in place that can limit the data throughput for your cluster, and CloudFront helps mitigate this issue.\". Do these instructions replace or add to the below instructions.  That doesn't seem so clear here.  Maybe \"After following one of the manual or one shot deployments below, follow these instructions to deploy CloudFront\". I think the link needs to be html rather than md.  This is confusing, but you can confirm by running make html under docs and confirming the link in a browser for the generated files.. \"To deploy a production ready AWS cluster with Cloudfront:\". Make these 1), 2), etc. sections? \n```\nFollow the instructions for a normal AWS deployment\n``. Should we just link to the \"one shot\" instructions in the normal AWS doc here?  Otherwise, we will have to maintain the same instructions in two different places.. I mean that you could make the things you have labeled as 1), 2), etc. actual sections similar to the other deploy docs.  Because the 1), 2), etc. don't look that great in markdown.. Yeah, we can mention that it is untested.  According to this it should work: http://blog.clarifai.com/how-to-scale-your-gpu-cloud-infrastructure-with-kubernetes/#.WUmHiXUrLkx. Yeah, I agree with this, but will rearrange as I pull in and organize your troubleshooting docs.. I have tried to clarify this and I will link to the respective section of troubleshooting and re-organize when I pull all that together.. I just went ahead and added a section to the \"data management\" doc.  I think that makes a lot of sense. . Comment added.. @JoeyZwicker I had some issues when running with the&again. I was going to ask @jdoliner when he was in, but the one without the&definitely worked for me (that is, it killed and restarted).. Somewhat vague.  More efficient than what and which \"things\" are reused.. Maybe list both types here and link to the sections.  This seems to work well in other docs.. Used for?. I think a picture would aid in this description.  I can work on one.. It would be great to show an actual example here.  The format of the \"progress\" bar isn't necessarily obvious.. \"not all computations can be done online, you have to enable...\" (comma). Period.. Again, I think adding some type of visual here would help.. @JoeyZwicker my understanding was that there would be no logging in via the UI/Dashboard in 1.6. That was what we discussed with @msteffen. Can you confirm @msteffen?. Ah, good point.  I will adjust.. fixed below.. @JoeyZwicker Not anymore.  They actually refer to fruit stand still andgrep.  . This is fixed now.. Yeah, I figured thecd` would be less confusing for multiple files, especially below where they have various names and are committed in the same commit. Can adjust later if I hear any confusion.. Yeah, I added a general view at the end. I think most of the examples should be CLI/client focused. But I definitely agree that adding some plugs for the dashboard would be useful.. We discussed this in the past and decided to make the manual deploy the first/preferred route in the docs. I'm not opposed to changing it, but we have received feedback both ways. There was a time that I had the one shot script first, but we got a bunch of feedback that people weren't able to change or customize what they wanted.  Also, people were complaining that the one shot script deployed both k8s and pachyderm, when they already had a k8s they wanted to use. \nAnyway, I can see both sides. Maybe @sjezewski has some opinions on this as well.. +1 Good catch.. Maybe an alternative here would be to better emphasize at the beginning of the docs who should use which deploys and why?. It's included explicitly here as part of the Pachyderm installation: https://github.com/pachyderm/pachyderm/blob/b593fc2c567d9dc4310e6cf6ea495ebf1b430db8/doc/deployment/amazon_web_services.md#install-pachctl. But I'm fine with moving it anywhere. If we move it up to the top, we should do that in all of the deploy docs and then maybe just have a common doc somewhere that talks about installing pachctl. . Yeah, I'm glad you caught this. There was actually supposed to be something else here, that I forgot to copy over. Moreover, when I tried to re-run this and recreate I got some weirdness, so dealing with that. I will update here shortly.. In my opinion/preference, I like to keep the code as simple as possible file in -> simple operation -> file out when possible, then wrap that with some logic about which files and that sets command line arguments. I think this is nice, because it allows you to \"version\" the different ways you have invoked that simple operation, based on command line args, etc. in previous jobs or versions of the pipeline spec. Also, it prevents hardcoding a bunch of things, and allows you to reuse Docker images more. Again, just my take on things.. Thanks! I think this could be a little more concise and combine with the Linux command. Maybe:\n```\nFor Linux (64 bit) or Window 10+ in WSL:\n$ curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v1.6.6/pachctl_1.6.6_amd64.deb && sudo dpkg -i /tmp/pachctl.deb\n``. Good question. I removed this, which now fixes: https://github.com/pachyderm/pachyderm/issues/2583. Looks like there are some extra lines here. Not a huge deal, but would be nice to remove.. I tried that @jdoliner, but I don't think that the error Group allows this. Or else, I was doing something else wrong. Things just ran in the background, because it's only in the case that all in the group returned that something happened. I could definitely be wrong here, because I'm new to error groups. But I did try that without success. Any pointers?. Maybe give an example of \"on disk objects\" so people have some context about what is being transformed. Maybe \"... some or all of the on disk objects, which persist Pachyderm's metadata for commits, jobs, etc.\". Formatting.pachctl deploy`. I think this is a little confusing. Does this mean that:\n\nI have a previous version of Pachyderm up and running.\nI update pachctl to the new version.\nI run my pachctl deploy ... command with the updated pachctl, thus deploying the new version side-by-side with the old version.\n\nIf so, it might be worth specifying that as steps, similar to above. Because some people won't get that from this quick statement I think.. \"While migration is running, you will see...\". \"During this time in which both pachd pods are running, the original pachd pod (deployed with the previous version of Pachyderm) will still...\". turned off? . Why don't you need to worry?. \"This is accomplished with the pachctl extract command. Running this command will generate a stream of API requests similar to the stream utilized by the above-mentioned migration. That stream can then be used to reconstruct your cluster by running pachctl restore.. \"To solve this...\" -> \"To bridge the gap to previous Pachyderm versions, we've made a final...\". \"1.6.9 requires no migration from 1.6.8. You...\". formatting.. After 1.6.9 is deployed you should make a backup using pachctl extract and then upgrade pachctl again to 1.7.0. Finally, you can pachctl deploy ... with pachctl 1.7.0 to trigger the migration.. \"starting to download data\". ",
    "munchee13": "Did you post the inquiry to the community list? That is likely the best\nplace to get some help.\nCheers,\nChris\nFrom: Joe Doliner notifications@github.com notifications@github.com\nReply: pachyderm/pachyderm\nreply@reply.github.com\nreply@reply.github.com\nDate: June 10, 2016 at 5:49:00 PM\nTo: pachyderm/pachyderm pachyderm@noreply.github.com\npachyderm@noreply.github.com\nCC: munchee13 cmorgan@redhat.com cmorgan@redhat.com, Mention\nmention@noreply.github.com mention@noreply.github.com\nSubject:  Re: [pachyderm/pachyderm] Get Pachyderm Working with OpenShift\n(#336)\n@munchee13 https://github.com/munchee13 first snag I seem to have hit. I\n\nsetup openshift using this guide:\nhttps://docs.openshift.org/latest/getting_started/administrators.html#running-in-a-docker-container\nand seem to be hitting and issue where none of the nodes become available\nand thus pods can't be scheduled. Any ideas on how to fix that?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/336#issuecomment-225304387,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/ABxhZRvj5L58B6xiYm1lSgUrlA781LVzks5qKdvMgaJpZM4IMoFk\n.\n. \n",
    "sambooo": "I've been able to get pachyderm running on openshift origin by doing the following:\n- Init the vagrant box: vagrant init thesteve0/openshift-origin\n- Forward port 30650 in Vagrantfile: config.vm.network \"forwarded_port\", guest: 30650, host: 30650\n- Start the vm: vagrant up\n- Login as admin: oc login, username and password are both admin\n- Enable privileged containers: oc edit scc and set allowPrivilegedContainer: true everywhere\n- Download the manifest: curl -O http://pachyderm.io/manifest.json\n- Search for hostPath in manifest.json and remove those three lines plus the previous comma.\n- Submit the manifest: oc create -f manifest.json\nNow pachctl should work, you can test it with pachctl version which should tell you your client and server versions.\n. An important note: Errors regarding timeouts on etcd/rethinkdb are probably unrelated to this issue. They were creating noise that made it hard to pin this down, and those errors are harmless unless they continue for a large number of restarts.\n. Working one-liner:\n$ curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v1.0.1-RC1/pachctl_1.0.1-RC1_amd64.deb && dpkg -i /tmp/pachctl.deb\n. Tests fixed in #469 \nHugo checks for linting errors in its Makefile in a way that will fail CI.\n. Compilation of that file was fixed in #469, no changes made to testFuse() though\n. Fixed some conflicts and removed the change that caused travis to fail so make lint can be merged. I'll open another PR with the failing travis command.\nI based this off of master so once it's closed I can reclaim my branch :)\n. In order to use github releases from travis, we'll need an encrypted access token which I personally can't get. See: https://docs.travis-ci.com/user/encryption-keys/ and https://gist.github.com/vinceallenvince/a7611b10f84e61eebdcc for an example .travis.yml\n. I moved the notes down to the bottom but haven't added anything about pushing to a registry yet.\nSince there's usually no reason to make an image built from head private, I think we can recommend pushing to the public hub as a simple solution and not take on the burden of documenting one of many ways to make a private registry.\n. @jdoliner clarification on revision needed here. If there's no issue with not explaining registries I can rewrite against master and update this.\n. Willing to tackle this myself but I'd like to know what you guys think first\n. I suppose then a simple redesign would be more like this:\nshell\nAvailable Commands:\n  create\n  delete\n  list\n  ...\nat which point it does become pretty much pointless, yeah. I had it backwards without thinking after too much gcloud.\nIn this new case the only change to the typed commands would be spaces instead of hyphens which isn't a change worth breaking it for. A shorter help text with the same command names would be best for the end-user but it's prone to getting out of sync with the code.\nCould be possible to rewrite commands into this new form and just alias the current names in code by replacing hyphens with spaces. A key benefit there would be that docs would all be tightly coupled to their commands, but at the cost of having some glue holding it together.\nDo you have an approach in mind that could let us decorate the command tree with info about list-, create-, delete-, inspect-, and the like without significantly changing the implementation of individual commands? Perhaps prefix matching on commands and storing help per-prefix in a map somewhere, then stitching it together at runtime would work too.\nThe easiest approach though is to do nothing for now, unless it breaks. EDIT: Slept on this and it makes no sense to me now\n. Is Semantic Versioning what you're after?\n. See #607 \n. @sjezewski without knowing how long each line/file/block will take to process ahead-of-time, there isn't really any way to know what is left to process at a given time.\nWithout enforcing some requirements on users regarding filesystem access, we can't just add more data to a running pod since it's entirely possible their code will enumerate files only once and never check for changes. I don't think there is any meaningful way to map file-access info to work-done, since again we don't know the significance of any given read.\nSo I believe that limits your options to either:\n1. Actively scheduling data to containers as they free up,\n2. Killing running containers, discarding their outputs entirely and redistributing work.\nThe first option sounds a lot like it's kubernetes' job which makes me hesitant to try it, but I don't see a reason the second couldn't work while being inefficient.\n. A good chunk of the comments could probably use more info but it's ready to merge and they can be fixed later :)\n. I'm pretty sure these errors are related to OSX:\n\u279c  pachyderm git:(master) pachctl mount ~/pfs\n2016-07-01T19:34:24Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\"DCIM\",\"err\":\"Repo DCIM not found\"}\n2016-07-01T19:34:24Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\"._.\",\"err\":\"Repo ._. not found\"}\n2016-07-01T19:34:24Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".Spotlight-V100\",\"err\":\"Repo .Spotlight-V100 not found\"}\n2016-07-01T19:34:24Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index\",\"err\":\"Repo .metadata_never_index not found\"}\n2016-07-01T19:34:24Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index_unless_rootfs\",\"err\":\"Repo .metadata_never_index_unless_rootfs not found\"}\n2016-07-01T19:34:24Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\"._.\",\"err\":\"Repo ._. not found\"}\n2016-07-01T19:34:24Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index\",\"err\":\"Repo .metadata_never_index not found\"}\n2016-07-01T19:34:24Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\".metadata_never_index_unless_rootfs\",\"err\":\"Repo .metadata_never_index_unless_rootfs not found\"}\n2016-07-01T19:34:24Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\"._.\",\"err\":\"Repo ._. not found\"}\n2016-07-01T19:34:24Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\"._.\",\"err\":\"Repo ._. not found\"}\nIt looks like once the volume is mounted, OSX checks for a bunch of metadata. In this case, DCIM would be camera data, .Spotlight-V100 is related to Spotlight, .metadata_never_index/_unless_rootfs are pretty self-explanatory and ._. relates to trash (I think). Afaik every one of these errors can be ignored safely.\nA possible fix to remove (most of) these errors is to prevent reading repos that begin with a dot before it ever reaches the fuse layer, which might already be the case. Then we can safely filter Repo .Spotlight-V100 not found just from the pachctl output since it's safe to ignore. Not sure about the best way to filter out DCIM since it could probably be a valid directory.\n. A little nitpick: examples/opencv_dominant_color_example could probably be examples/opencv_dominant_color instead\n. @rexmortus the source of that output shows that it's a hardcoded value that (probably) isn't meant to reflect the environment you're running in. It's not really clear from the output that it's just listing defaults, so maybe that's worth noting there.\n@jdoliner I can go through and see if there are any similar occurrences like this that could cause confusion and fix them. Worth doing or nah?\nAlso worth noting the irony of the typo here:\n\n1) There is a typo. It says \"Envronment\" when it should say \"Enivronment\".\n\n\ud83d\ude00\n. GetFile is failing here because of the changes in #663 I think\n. ",
    "haps-basset": "Hello \nI need your help. I am trying to evaluate capability of pachyderm in our operational context.  I would like to know how to integrate the pachyderm with openshift  baseline. The installation procedure is extremely laconic  thus any support in this matter highly appreciated. \n  pachctl deploy custom --persistent-disk  --object-store    [flags]\nWould you be able to explain what is it persistent disk with regards to OS definitions? Is it a president disk or volume? What is it an object store etc? I am must admit I am not OS admin just basic user and in this case I need  to know what I will  gain from investment in  pachyderm.\nSimple HOW-TO for dummies is my dream in this stage of engagement with this concept e.g.\n1)  What needs to be done before; shell I create an user pachyderm?\n2)  How to create in OS environment persistent disk?\n3)  How to create in OS environment object store?\n4)  What are those  persistent disk args  object store args  arguments?\nI have spoken with local OS experts and admins and they are also confused in term of terminology thus all  suggestions where not workable or rejected by pachctl.\nFurthermore, even pachyderm support was not able to explain what needs to be done so I am trying to find good soul  willing to help  me solve this mistery. \nHaps.\n. No I did not and we have decided to drop further study and evaluation of this product.\nWe are interesting in usage some product which should support our research then testing or verifying its suitability. \nPublished by link point to the same page as on yours main web page thus is just useless. . In our case we are only consider usage of private PaaS thus any suggestion to use non-private solution is just useless for us. In order to deploy private PaaS, we had considered OpenShift.  Due to the fact that deployment of Pachyderm on OpenShift was unsuccessful we had decided to drop further study of this product. Bear in mind, deployment OpenShift is not cheap.  To clarify, we thought  about service support from your company  as well but due to problems with evaluation risk with utilisation of the system was too high and   the program was stopped.  . ",
    "adrianog": "Hi @haps-basset \nDid you get to the bottom of this? Has anything changed in Openshift (or Pachyderm) since you last tried this?\nI notice an Openshift deployment page with a recent date here https://goo.gl/eg888a: is that relevant for your problems?\n. Hi\nCan we use native Containerized Red Hat Gluster Storage for the object store? (RE: \"this could be any S3 compliant object store solution like Minio, Rook, Ceph, Swift, etc. or any one of the cloud provider's object store solutions like S3, GCS, or Azure blob storage.\").\n. ",
    "jarcher": "Have you guys tested on 3.7 and 3.9 yet and been able to update your doc accordingly?. ",
    "pratheekrebala": "I was also facing the \"crash loop\" error with the latest build -- replacing the original manifest file with the one provided above did the trick for me!\n. ",
    "elsonrodriguez": "That did it.\nThanks!\n. sent in a PR for this.\n. From the client side there's no guaranteed way.\nSafest way is to get the namespace from a k8s object. The Pachd pod that is processing the request is a good bet.\n. The ELB that was provisioned is just raw TCP supposedly. Going from port 650 to 30650 on the nodes (the nodeport).\n. You've probably guessed my thoughts, but the fruit stand demo (and any future demos) should make as few assumptions as possible regarding somebody's k8s cluster. Especially in regards to local vs remote. Someone might be wanting immediate results on their 10k node cluster ;)\nThe concept of making your own container to process workloads is important to teach, but having a line in a json file is an extremely ninja way of exposing someone to that concept, and doesn't do it justice.\nPeople would be better served with a follow-up tutorial on how to make a container with a custom workload, after an initial tutorial that requires the least amount of thought.\n. Was able to finish the fruit-stand example!\nThe job-shim should be version pinned to prevent any future incompatibilities. The surest way to do this is probably to blank out the image field in pipeline.json, and then set this to include a tag string:\nhttps://github.com/pachyderm/pachyderm/blob/27329056b99dd0b7966815a11e1f5c05e2394769/src/server/pps/server/api_server.go#L941\nThat way pachd always uses the right version of job-shim by default. Pre-reqs for that are to make sure that CI is chucking an image to docker hub per pachd release/build.\nSome other notes are that I couldn't figure out what to delete to kill the job and re-create the pipeline. Once the pipeline errored out I just deleted pachyderm from k8s and started from scratch.\n. ",
    "Justin-Kuehn": "Hi Joe, thanks for the response.  Having CreateJob be idempotent makes sense.  In my case I was iterating on the pipeline (changing the docker image and such) and found the only way I could reprocess data was to make a new commit with the same data or rename my pipelines. I think a force flag sounds like a reasonable solution here.\n. ",
    "kevinemoore": "How do I tell? I'm not a go user, but I just tried go --version and go -v and neither worked.\n. According to brew, it's go-1.6.2.\n. go version\ngo version go1.6.2 darwin/amd64\n. The repo lives at:\n/Users/kmoore/toa/github/pachyderm\nAnd, for now, GOPATH=/Users/kmoore/go_workspace\n. OK. We got farther. Now, I'm seeing this:\nsrc/server/cmd/pachctl-doc/main.go:22: cannot use rootCmd (type \"github.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra\".Command) as type \"server/vendor/github.com/spf13/cobra\".Command in argument to doc.GenMarkdownTree\nmake: * [install] Error 2\n. ls $GOPATH/src/github.com/spf13\nls: /Users/kmoore/pachyderm/src/github.com/spf13: No such file or directory\nls $GOPATH/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13\ncobra   pflag\n. Client Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.2\", GitCommit:\"528f879e7d3790ea4287687ef0ab3f2a01cc2718\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.1+$Format:%h$\", GitCommit:\"$Format:%H$\", GitTreeState:\"not a git tree\"}\n. commit 4fd38cd7d4c33b379b2a122d8f69b943c190cfaf\nAuthor: Derek Chiang derekchiang93@gmail.com\nDate:   Fri May 20 18:41:05 2016 -0700\nMinor update to pipeline spec to get rid of unwanted highlighting\n. start-kube-docker.sh:\ndocker run \\\n    -d \\\n    --volume=/:/rootfs:ro \\\n    --volume=/sys:/sys:ro \\\n    --volume=/dev:/dev \\\n    --volume=/var/lib/docker/:/var/lib/docker:rw \\\n    --volume=/var/lib/kubelet/:/var/lib/kubelet:rw \\\n    --volume=/var/run:/var/run:rw \\\n    --net=host \\\n    --pid=host \\\n    --privileged=true \\\n    gcr.io/google_containers/hyperkube:v1.2.2 \\\n    /hyperkube kubelet \\\n        --containerized \\\n        --hostname-override=\"127.0.0.1\" \\\n        --address=\"0.0.0.0\" \\\n        --api-servers=http://localhost:8080 \\\n        --config=/etc/kubernetes/manifests \\\n        --allow-privileged=true\n. I see the --privileged=true and --allow-privileged=true options so I was surprised to see that error.\n. OK. I think I see the permission issue. I was getting an instance of \"localkube\" (from Redspread) running too. I think it was talking to kubectl. But, when it stops, I can't get hyperkube to talk to kubectl.\n. I'm using docker-machine and I have the port forwarding running. It seems to work for localkube, but not hyperkube.\n. Current status:\nair:kmoore>docker ps\nCONTAINER ID        IMAGE                                             COMMAND                  CREATED             STATUS              PORTS               NAMES\n8c3c513a9583        gcr.io/google_containers/hyperkube-amd64:v1.2.2   \"/hyperkube apiserver\"   5 minutes ago       Up 5 minutes                            k8s_apiserver.78ec1de_k8s-master-127.0.0.1_default_4c6ab43ac4ee970e1f563d76ab3d3ec9_e6b69f86\na3ffa9d5da5c        gcr.io/google_containers/hyperkube-amd64:v1.2.2   \"/setup-files.sh IP:1\"   5 minutes ago       Up 5 minutes                            k8s_setup.e5aa3216_k8s-master-127.0.0.1_default_4c6ab43ac4ee970e1f563d76ab3d3ec9_6b10c2e2\ne380ddf9f1b5        gcr.io/google_containers/hyperkube-amd64:v1.2.2   \"/hyperkube scheduler\"   5 minutes ago       Up 5 minutes                            k8s_scheduler.fc12fcbe_k8s-master-127.0.0.1_default_4c6ab43ac4ee970e1f563d76ab3d3ec9_d65b389a\n5cef63c89019        gcr.io/google_containers/hyperkube-amd64:v1.2.2   \"/hyperkube proxy --m\"   5 minutes ago       Up 5 minutes                            k8s_kube-proxy.9a9f4853_k8s-proxy-127.0.0.1_default_5e5303a9d49035e9fad52bfc4c88edc8_7b9baca2\n46ac3d82190d        gcr.io/google_containers/hyperkube-amd64:v1.2.2   \"/hyperkube controlle\"   5 minutes ago       Up 5 minutes                            k8s_controller-manager.70414b65_k8s-master-127.0.0.1_default_4c6ab43ac4ee970e1f563d76ab3d3ec9_d0b7eb9c\nae699fd5f845        gcr.io/google_containers/pause:2.0                \"/pause\"                 5 minutes ago       Up 5 minutes                            k8s_POD.6059dfa2_k8s-etcd-127.0.0.1_default_1df6a8b4d6e129d5ed8840e370203c11_bf50a671\n7dfca217bc92        gcr.io/google_containers/pause:2.0                \"/pause\"                 5 minutes ago       Up 5 minutes                            k8s_POD.6059dfa2_k8s-master-127.0.0.1_default_4c6ab43ac4ee970e1f563d76ab3d3ec9_ca8f6040\ne83db715cd4f        gcr.io/google_containers/pause:2.0                \"/pause\"                 5 minutes ago       Up 5 minutes                            k8s_POD.6059dfa2_k8s-proxy-127.0.0.1_default_5e5303a9d49035e9fad52bfc4c88edc8_dc7f2ccb\n1f21f182738f        gcr.io/google_containers/hyperkube:v1.2.2         \"/hyperkube kubelet -\"   5 minutes ago       Up 5 minutes                            distracted_panini\n83b10a82ee5e        weaveworks/weaveexec:1.4.5                        \"/home/weave/weavepro\"   5 minutes ago       Up 5 minutes                            weaveproxy\n9fefc52184d0        weaveworks/weave:1.4.5                            \"/home/weave/weaver -\"   5 minutes ago       Up 5 minutes                            weave\nb0d5227c0473        gcr.io/google_containers/etcd:2.0.9               \"/usr/local/bin/etcd \"   38 minutes ago      Up 38 minutes                           mad_albattani\nair:kmoore>ps -ef | grep ssh\n  501  1366     1   0 11:35AM ??         0:00.28 /usr/bin/ssh-agent -l\n  501 10967     1   0  5:39PM ??         0:00.00 /usr/bin/ssh -F /dev/null -o BatchMode=yes -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=quiet -o ConnectionAttempts=3 -o ConnectTimeout=10 -o ControlMaster=no -o ControlPath=none docker@127.0.0.1 -o IdentitiesOnly=yes -i /Users/kmoore/.docker/machine/machines/dev/id_rsa -p 52289 -fTNL 8080:localhost:8080 -L 30650:localhost:30650\n  501 11064   985   0  5:45PM ttys000    0:00.01 grep ssh\n. air:kmoore>kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.2\", GitCommit:\"528f879e7d3790ea4287687ef0ab3f2a01cc2718\", GitTreeState:\"clean\"}\nThe connection to the server 192.168.99.100:8080 was refused - did you specify the right host or port?\n. Looks like it's connecting to the client, but not the server?\n. ~/.kube/config could be the problem. Should I have anything in that file? \ncat ~/.kube/config\napiVersion: v1\nclusters:\n- cluster:\n  insecure-skip-tls-verify: true\n  server: 192.168.99.100:8080\n  name: localkube\n  contexts:\n- context:\n  cluster: localkube\n  user: \"\"\n  name: localkube\n  current-context: localkube\n  kind: Config\n  preferences: {}\n  users: []\n. ",
    "ShengjieLuo": "I have also met this problem. The situation can be listed here.\nNAME               READY     STATUS             RESTARTS   AGE       NODE\netcd-vox0k         1/1       Running            0          34m       gke-pachyderm-default-pool-788e5c85-7rhn\npachd-2icno        0/1       CrashLoopBackOff   11         34m       gke-pachyderm-default-pool-788e5c85-a794\npachd-g1csl        0/1       CrashLoopBackOff   11         34m       gke-pachyderm-default-pool-788e5c85-7rhn\npachd-init-sh6j8   0/1       CrashLoopBackOff   11         34m       gke-pachyderm-default-pool-788e5c85-7rhn\nrethink-u13ht      0/1       CrashLoopBackOff   11         34m       gke-pachyderm-default-pool-788e5c85-7rhn\nThe CrashLoopBackOff Problem is similar with author's original problem.\nThe debug information is list here:\nkubectl logs pachd-2icno --previous\ntime=\"2016-07-01T01:37:24Z\" level=warning msg=\"Error creating connection: gorethink: dial tcp 10.3.253.212:28015: i/o timeout\"  gorethink: dial tcp 10.3.253.212:28015: i/o timeout\nHow can it be done? The manifest used is strictly followed the official manual.\n. The log of the patch pod can be seen as following\nkubectl logs pachd-init-sh6j8 --previous\ntime=\"2016-07-01T01:37:47Z\" level=warning msg=\"Error creating connection: gorethink: dial tcp 10.3.253.212:28015: i/o timeout\" \ngorethink: dial tcp 10.3.253.212:28015: i/o timeout\nIt seems that the TCP port or rethink pod is error.\n. The log of the rethink can be seen as following\nkubectl logs rethink-6ust6 --previous\nRunning rethinkdb 2.3.3~0jessie (GCC 4.9.2)...\nRunning on Linux 3.16.0-4-amd64 x86_64\nLoading data from directory /var/rethinkdb/data\nVersion: rethinkdb 2.3.3~0jessie (GCC 4.9.2)\nerror: Inaccessible database file: \"/var/rethinkdb/data/metadata\": No such file or directory\nSome possible reasons:\n- the database file couldn't be created or opened for reading and writing\n- the user which was used to start the database is not an owner of the file\nIt is confused that the metadata file seems lose from the cluster.\n. Actually, there is no directory named '/var/rethink/data' either in Google Cloud Shell or Google VM instances. So the question is related to rethinkdb or anything else? @jdoliner \n. I do format the disk in GCE but not do rm -rf command. I had thought the new mount disk would be a empty disk and omitted this step. I would do this steps at once to see whether it works.\n. A question occurred to me that the rm -rf [path to disk]/*, what does the [path-to-disk] mean? Dose that mean /mnt/disks/[MNT_DIR] or /dev/disk/by-id/google-[DISK_NAME] ? See [https://cloud.google.com/compute/docs/disks/add-persistent-disk#formatting]\n@derekchiang \n. Tanks for you help. All of the pods are running. And the launch finished in 41 seconds. You do help me a lot for this problem.\n. The debug information can found here\npachctl version\nCOMPONENT           VERSION\npachctl             1.0.1-ab10ee8e15ad854f27813fad3ab7af1378960497\npachd               UNKNOWN: Error rpc error: code = 4 desc = context deadline exceeded\nThe address information:\necho $ADDRESS\n104.197.165.43:30650\nThe directory information\nls $GOPATH/bin\npachctl  pach-deploy\n. The firewall information is here\nNAME                                     NETWORK  SRC_RANGES         RULES                         SRC_TAGS  TARGET_TAGS\ndefault-allow-icmp                       default  0.0.0.0/0          icmp\ndefault-allow-internal                   default  10.128.0.0/9       tcp:0-65535,udp:0-65535,icmp\ndefault-allow-rdp                        default  0.0.0.0/0          tcp:3389\ndefault-allow-ssh                        default  0.0.0.0/0          tcp:22\ngke-pachyderm-2e98fcbb-all               default  10.0.0.0/14        udp,icmp,esp,ah,sctp,tcp\ngke-pachyderm-2e98fcbb-ssh               default  104.197.247.58/32  tcp:22                                  gke-pachyderm-2e98fcbb-node\ngke-pachyderm-2e98fcbb-vms               default  10.128.0.0/9       icmp,tcp:1-65535,udp:1-65535            gke-pachyderm-2e98fcbb-node\nmy-rule                                  default  0.0.0.0/0          tcp:32080\nmy-rule1                                 default  0.0.0.0/0          tcp:32081\nmy-rule2                                 default  0.0.0.0/0          tcp:32082\nmy-rule3                                 default  0.0.0.0/0          tcp:30650\npachd                                    default  0.0.0.0/0          tcp:30650\nI add several firewall rules to allow the traffic in 30650. It seems that dial to 30650 should be allowed in firewall rules.\n. Hence , the question is that what happened to pachctl? It does confuse me a lot. @jdoliner @derekchiang \n. Also I tested the node to see whether the port is open\nnetstat -ntpl\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\ntcp6       0            0       :::30650                     :::*                            LISTEN      -\nThe 30650 port is open for any server.\nI use telnet in Google Cloud Shell to see whether it is open. Unfortunately, the telnet told me the failure to connect to the port.\ntelnet 104.197.165.43 30650\nTrying 104.197.165.43...\ntelnet: Unable to connect to remote host: Connection timed out\nPing 104.197.165.43 is right.\n. @derekchiang The company has ran out its google account, hence, I may migrate my work to local server. And this problem cannot be solved actually. I think the possible reason to this problem maybe that I deploy the cluster and execute the command in Google Cloud Shell instead of ssh to the instances. Also, the port's ip address is tcp6 instead of tcp. While GCE does not allow IPv6. I am not sure it works.\n. I doubt that if there is error in reading input. Also I try to find the log of pachyderm, but fail to find the log file. Could anyone help me for this problem? Thanks. @derekchiang @jdoliner \n. Issue #605 share similiars problem with mine. I will try to match the version problem first.\n. i have checked my version again. The kubenetes version and kubectl version are both 1.3.0 following the guide here http://kubernetes.io/docs/getting-started-guides/docker/\nThe kubernetes used in the pachyderm source code is Kubernetes 1.2.2.\nIs it a matter to use v1.3.0?\nIf I use v1.3.0, I will not use the command make launch-kube, and directly execute command make launch. \nOr I try to use make launch-kube to install 1.2.2 version, the process lasts for a long time without any response, usually more than one hour, therefore I have to use the 1.3.0 version.\n. I have checked my pachyderm version. That's true I use the oldder version\nCOMPONENT           VERSION\npachctl             1.0.1-3ef7e74acf7074e38c350c8c19c71243f5e9ffc8\npachd               1.0.1-1433\nI will change it to the new version soon. Keep contact and see whether the new version works. \n. @sjezewski @jdoliner  Hi, the deployment problem has been solved. I update Pachyderm to version 1.1.0, and it works to the previous problem. The error message mentioned above doesn't occur this time. Thanks a lot. \n. See logs here:\n0 | 2016-07-15T04:40:53Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"scraper\"},\"id\":\"4cf15ae67e5d43d39177a590ea1ee876\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"www.google.com\",\"err\":\"no such file or directory\"}\n0 | 2016-07-15T04:40:53Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"scraper\"},\"id\":\"4cf15ae67e5d43d39177a590ea1ee876\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"www.google.com\",\"err\":\"no such file or directory\"}\n0 | 2016-07-15T04:40:53Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"scraper\"},\"id\":\"4cf15ae67e5d43d39177a590ea1ee876\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"www.google.com\",\"err\":\"no such file or directory\"}\n0 | 2016-07-15T04:40:53Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"scraper\"},\"id\":\"4cf15ae67e5d43d39177a590ea1ee876\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"www.google.com\",\"err\":\"no such file or directory\"}\n0 | 2016-07-15T04:40:53Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"scraper\"},\"id\":\"4cf15ae67e5d43d39177a590ea1ee876\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"www.google.com\",\"err\":\"no such file or directory\"}\n0 | 2016-07-15T04:40:53Z ERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"scraper\"},\"id\":\"4cf15ae67e5d43d39177a590ea1ee876\"}},\"repoAlias\":\"out\",\"write\":true},\"name\":\"www.google.com\",\"err\":\"no such file or directory\"}\n0 | --2016-07-15 04:40:53--  http://www.google.com/\n0 | Resolving www.google.com (www.google.com)... 216.58.221.100, 2404:6800:4005:803::2004\n0 | Connecting to www.google.com (www.google.com)|216.58.221.100|:80... failed: Connection timed out.\n0 | Connecting to www.google.com (www.google.com)|2404:6800:4005:803::2004|:80... failed: Network is unreachable.\nTwo problems are reported.\nFirst Problems is there is no directory named 'www.google.com'. Similar problems happen in other examples, I used to ignore it.\nSecond problem is the connection timeout. The google is blocked by company network configuration. Proxy server is needed in this situation.\n. To address this issue, I change the command in the pipeline.json as following,\n\"pipeline\": {\n\"name\": \"scraper_export_proxy\"\n},\n\"transform\": {\n\"cmd\": [\n\"export\",\"http_proxy=http://username:password@child-prc.intel.com:914\",\n\"export\",\"https_proxy=https://username:password@child-prc.intel.com:914\",\n\"echo\",\"$http_proxy\",\">\",\"/pfs/out/test.txt\",\n\"echo\",\"$https_proxy\",\">\",\"/pfs/out/test.txt\",\n\"wget\",\n\"--recursive\",\n\"--level\", \"1\",\n... ...\nUnfortunately, the change doesn't work.\nSee job status here,\nID                                 OUTPUT                                                  STARTED             DURATION            STATE\n9e172109fe3522527883473a01688ab9   scraper_export_proxy/6bada8e255f446cd8d60d6bb2ab04ec8   7 minutes ago       -                   pulling\nSee logs here,\npachctl get-logs 9e172109fe3522527883473a01688ab9\ncontainer \"user\" in pod \"9e172109fe3522527883473a01688ab9-6uqeu\" is waiting to start: ContainerCreating\nSee kubectl status here,\nNAME                                     READY        STATUS              RESTARTS                       AGE            IP           NODE\n9e172109fe3522527883473a01688ab9-6uqeu   0/1          ContainerCreating   0                              9m             <none>       127.0.0.1\nSo this method don't works for the proxy problem. \n. Also this problem is in the example Word Count. Besides the wget problem, in the Docker image, \nRUN apt-get update && apt-get install -y --no-install-recommends \\\ng++ \\\ngcc \\\nlibc6-dev \\\nmake \\\ncurl \\\n&& rm -rf /var/lib/apt/lists/*\nThe apt-get also need refine the apt-conf to affirm the proxy server.  The container cannot be established. \nSee pachyderm status here,\nID                                 OUTPUT                                                  STARTED             DURATION            STATE\n79b18e985de343bfbea9a2bea9a97f77   wordcount_map/fc7898973a964026bf88427269b0fdd4          45 hours ago        -                   pulling\nSee kubectl status here,\nNAME                                     READY        STATUS              RESTARTS                       AGE            IP           NODE\n79b18e985de343bfbea9a2bea9a97f77-tj8n4   0/1          ImagePullBackOff    0                              1d             172.17.0.6   127.0.0.1\n. Thanks for the tips.  I would try to establish the Env field in the Transform section instead of directly exporting it.  It's my first time to contact the docker and container project, so maybe some problems are to some extent naive. Keep contact and I will modify it in Next Monday.  \n. Hi @jdoliner  I have added ENV http_proxy  $my_proxy$ to the image and it can work now. I also mentioned that #668 planed to add the ENVM section in 'Transform'.  It works for this kind of problems, thanks.  You can close this issue.\n. Thanks. Hope to see the new version soon.\n. For example, if for the first time I submit 40GB data, the second time I submit 20GB data, and the third time I submit 40GB data. Totally, 100GB is stored in the disk.\nNow, I find one data item in the second submit need to be revised. Then is it necessary to come back to the first version, and then repeat the second and third submit? If I do need this, there will be 100GB(first branch) + 60GB(second branch) data in the disk. Is it any method to revise only one item? \n. To address this issue, I come back again to make the docker image.\ndocker build -t fruit_stand examples/fruit_stand\nSending build context to Docker daemon 23.04 kB\nStep 1 : FROM pachyderm/job-shim\n---> f8717cbf7d62\nSuccessfully built f8717cbf7d62\nSeems that the required image has been got. But when I do that again, the Pulling state still not be solved.\n. I speculate the job-shim image may be broken. So I continued to make docker-build to build all of the images.  It come back with a fatal image.\nStep 6 : ADD _tmp/job-shim /\n ---> f8717cbf7d62\nRemoving intermediate container 673eacee575d\nSuccessfully built f8717cbf7d62\nfatal: Not a git repository (or any of the parent directories): .git\nI am not sure whether the docker image has been built.\n. Finally, I check the docker list with docker ps\nThe latest image is about 8 days ago. That's the time I deploy Pachyderm on my local server.\ndocker ps\nCONTAINER ID        IMAGE                                                        COMMAND                  CREATED             STATUS              PORTS               NAMES\n111acbf7c05b        pachyderm/pachd:v1.1.0                                       \"/pachd\"                 8 days ago          Up 8 days                               k8s_pachd.6f6a0894_pachd-jxl0q_default_62571db4-47e6-11e6-9ac4-001e67548707_8c05733f\n59af82a25aa3        pachyderm/pachd:v1.1.0                                       \"/pachd\"                 8 days ago          Up 8 days                               k8s_pachd.6f6a0894_pachd-j1tcj_default_62571cbe-47e6-11e6-9ac4-001e67548707_bf48d6ce\n. So my problem can be concluded into 3 aspects:\n1  Does  command Delete-all delete the docker image?\n2  If it does delete the image or I accidentally  lost the image before this command, how can I recover the image and check the image state?\n3  If the image still exists in my system, is there any other thing leading the Pulling state problem?\n The sudden failure had prevented me from working for a whole day, and I can't find a method to address it.  Thanks a lot if someone can help me!\n. @montenegrodr  hi, this my second time met this question.  I suggest you first review the pachyderm version by pachctl version, make sure your pachd and pachctl are all 1.1.0 version. If you have a version contradiction, you will meet this problem.  Otherwise , can you post your debug information on github? \n. @montenegrodr  Can you try make docker-build in the installation file, the command consists of three commands  make docker-build-job-shim make docker-build-pachd, you can see them in the $GOPATH/src/github.com/pachyderm/Makefile. Try to see whether you can make it.\n. @montenegrodr Keep contact tomorrow to see whether it works. It has been deep into night in Shanghai,  I have to go sleeping. Probably @jdoliner is willing to solve the problems. He was so helpful. \n. @JoeyZwicker  So if I change my image to job-shim, pachyderm will download the image from dockerhub instead of local file?  I am not sure about it. \n. @montenegrodr  I appreciate you work correctly with the local build docker. And can you show me your docker ps situation to see whether there is new docker in the list? I run this command several times, however, this command occurs a fatal error reported in my third comment in this issue.\n. Thanks. It seems that the job-shim  won't be listed in the docker list even after it has been built.  So my problem may be changed to the fatal error occurred in my third comment.  Go further into the error fatal: Not a git repository (or any of the parent directories): .git, I realize a difference between my deployment and official deployment. Since my company blocks the git protocol for security reason then I use wget to download the zip directly. I am not sure whether this difference has a relation to the error.\n. @jdoliner I have added the /.git into the folder. Therefore, the docker image can be built.\nSee Dockerfile.job-shim file here\n```\nFROM ubuntu:14.04\nMAINTAINER jdoliner@pachyderm.io\nENV http_proxy \nENV https_proxy \nRUN \\\n  apt-get update -yq && \\\n  apt-get install -yq --no-install-recommends \\\n    ca-certificates \\\n    fuse \\\n    wget && \\\n  apt-get clean && \\\n  rm -rf /var/lib/apt\nADD _tmp/job-shim /\n```\nThe last command need do git operation. And since I wget master.zip the file directly, so I lost the ./.git folder. Now I do it again, and it can work.\nSee docker image list here,\nroot@kickseed:~/work/src/github.com/pachyderm# docker images\nREPOSITORY                                            TAG                 IMAGE ID            CREATED             SIZE\npachyderm/pachd                                       latest              e5bc3914812f        18 minutes ago      267.9 MB\npachyderm_pachd                                       latest              e5bc3914812f        18 minutes ago      267.9 MB\nfruit_stand                                           latest              f198d12c14c5        18 minutes ago      260.4 MB\npachyderm/job-shim                                    latest              f198d12c14c5        18 minutes ago      260.4 MB\npachyderm_job-shim                                    latest              f198d12c14c5        18 minutes ago      260.4 MB\npachyderm_compile                                     latest              9ad8079c388a        19 minutes ago      804.7 MB\nThe required docker has been built. \n. Unfortunately, I still cannot get rid of the Pulling state.\npachctl list-repo\nNAME                CREATED             SIZE\nsum                 10 minutes ago      0 B\nfilter              10 minutes ago      0 B\ndata                10 minutes ago      1.696 KiB\npachctl list-job\nID                                 OUTPUT                                    STARTED             DURATION            STATE\n965948e462c21505447838bd4b4d8f89   filter/e6e913dd2e5c457a82cd5efc096aaccf   10 minutes ago      -                   pulling\nfdc1f548472ead805c457dd82ebdf7c6   filter/603dd21753b048d3b390b6fa06d0d5b0   10 minutes ago      -                   pulling\npachctl list-pipeline\nNAME                INPUT               OUTPUT              STATE\nfilter              data                filter              running\nsum                 filter              sum                 running\nAlso please see kubectl debug information\nName:           965948e462c21505447838bd4b4d8f89-38ab9\nNamespace:      default\nNode:           127.0.0.1/127.0.0.1\nStart Time:     Thu, 21 Jul 2016 10:32:04 +0800\nLabels:         app=965948e462c21505447838bd4b4d8f89\n                suite=pachyderm\nStatus:         Pending\nIP:\nControllers:    Job/965948e462c21505447838bd4b4d8f89\nContainers:\n  user:\n    Container ID:\n    Image:              fruit_stand\n    Image ID:\n    Port:\n    Command:\n      /job-shim\n      965948e462c21505447838bd4b4d8f89\n    State:              Waiting\n      Reason:           ContainerCreating\n    Ready:              False\n    Restart Count:      0\n    Environment Variables:\n      PACH_OUTPUT_COMMIT_ID:    e6e913dd2e5c457a82cd5efc096aaccf\n      PACH_DATA_COMMIT_ID:      345949be84d041e7bb3dfcdecf640cec\nConditions:\n  Type          Status\n  Initialized   True\n  Ready         False\n  PodScheduled  True\nVolumes:\n  default-token-bm3z3:\n    Type:       Secret (a volume populated by a Secret)\n    SecretName: default-token-bm3z3\nQoS Tier:       BestEffort\nEvents:\n  FirstSeen     LastSeen        Count   From                    SubobjectPath   Type            Reason          Message\n  ---------     --------        -----   ----                    -------------   --------        ------          -------\n  6m            6m              1       {default-scheduler }                    Normal          Scheduled       Successfully assigned 965948e462c21505447838bd4b4d8f89-38ab9 to 127.0.0.1\n  4m            1m              2       {kubelet 127.0.0.1}                     Warning         FailedMount     Unable to mount volumes for pod \"965948e462c21505447838bd4b4d8f89-38ab9_default(4f8fa2df-4eeb-11e6-b885-001e67548707)\": timeout expired waiting for volumes to attach/mount for pod \"965948e462c21505447838bd4b4d8f89-38ab9\"/\"default\". list of unattached/unmounted volumes=[default-token-bm3z3]\n  4m            1m              2       {kubelet 127.0.0.1}                     Warning         FailedSync      Error syncing pod, skipping: timeout expired waiting for volumes to attach/mount for pod \"965948e462c21505447838bd4b4d8f89-38ab9\"/\"default\". list of unattached/unmounted volumes=[default-token-bm3z3]\nThe kubenetes status is Pending, which lasts for several hours. Although it can be done finally, I still think something negative happening.\n. I run it on ubuntu 14.04 system. The program has ran for all night yesterday and finally finish the task with a duration for 8 hours after restart mount for thousands of times. \nSee hardware situation here, \nroot@kickseed:~/work/src/github.com/pachyderm# df -h\nFilesystem      Size  Used Avail Use% Mounted on\nudev             63G  4.0K   63G   1% /dev\ntmpfs            13G  2.2M   13G   1% /run\n/dev/sda2       916G   19G  851G   3% /\nnone            4.0K     0  4.0K   0% /sys/fs/cgroup\nnone            5.0M     0  5.0M   0% /run/lock\nnone             63G  2.3M   63G   1% /run/shm\nnone            100M     0  100M   0% /run/user\n/dev/sda1       453M   38M  388M   9% /boot\ntmpfs            63G   12K   63G   1% /var/lib/kubelet/pods/2453c70c-4714-11e6-86ef-001e67548707/volumes/kubernetes.io~secret/default-token-wfbhe\ntmpfs            63G   12K   63G   1% /var/lib/kubelet/pods/2453c306-4714-11e6-86ef-001e67548707/volumes/kubernetes.io~secret/default-token-wfbhe\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/13be97c3-4719-11e6-9ec2-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/13f51a76-4719-11e6-9ec2-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/140d5e82-4719-11e6-9ec2-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/14372a58-4719-11e6-9ec2-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-y1yh4\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/14372e9d-4719-11e6-9ec2-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-y1yh4\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/4c29e35d-472f-11e6-afb6-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/4c615468-472f-11e6-afb6-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/4c6e7e36-472f-11e6-afb6-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/4c9ae5d8-472f-11e6-afb6-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-y2lb0\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/4c9ad9ba-472f-11e6-afb6-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-y2lb0\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/90e168cb-4735-11e6-afb6-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-yogc5\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/90e1dd96-4735-11e6-afb6-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-yogc5\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/90b4e3a8-4735-11e6-afb6-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/90c55ec9-4735-11e6-afb6-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/906fc018-4735-11e6-afb6-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/15f12704-4738-11e6-bf2f-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-fosiy\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/15f12c0e-4738-11e6-bf2f-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-fosiy\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/6936858e-47dc-11e6-8846-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-ztrto\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/693699de-47dc-11e6-8846-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-ztrto\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/690a935a-47dc-11e6-8846-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/68c8a1b2-47dc-11e6-8846-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/69152513-47dc-11e6-8846-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/dded55f2-47dc-11e6-9799-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-j36pm\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/dded4ebf-47dc-11e6-9799-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-j36pm\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/ddca812d-47dc-11e6-9799-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/dd803513-47dc-11e6-9799-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/e31f3904-47dd-11e6-aefb-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-w2bv5\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/e31f3e08-47dd-11e6-aefb-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-w2bv5\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/07858ddb-47de-11e6-aefb-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-s35t2\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/07858a92-47de-11e6-aefb-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-s35t2\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/076a04e9-47de-11e6-aefb-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/0761efed-47de-11e6-aefb-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/072cf965-47de-11e6-aefb-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/a7aaa613-47e4-11e6-9ac4-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-ffo16\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/a7aaa36b-47e4-11e6-9ac4-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-ffo16\ntmpfs            63G   12K   63G   1% /var/lib/kubelet/pods/62571db4-47e6-11e6-9ac4-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-8r5va\ntmpfs            63G   12K   63G   1% /var/lib/kubelet/pods/62571cbe-47e6-11e6-9ac4-001e67548707/volumes/kubernetes.io~secret/pachyderm-token-8r5va\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/b5bb78b9b313101aa364753af9efa4d28fd0e50537248c78ac7167aa2dbc14f1\nshm              64M     0   64M   0% /var/lib/docker/containers/8eda6975d44ff163d7157f379c936c3efba4a59455bccb48c4ffb7a59b15b73e/shm\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/2c3cb46ffe3cbeb46e78292813408ba3be23c1e53adc1f87550c4bdc839e405d\nshm              64M     0   64M   0% /var/lib/docker/containers/88106a447bb249230db8ef25dad58e18595e85ef7f522a44677616b5825fcb44/shm\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/3a147df463a749346d3402fee09c4c74e4f9466ff86c8cc006ba240c2f69f08d\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/aa1206e968c472d7b2963d7d30a28ff555610b797caff3293fc86f23bf0f2934\nshm              64M     0   64M   0% /var/lib/docker/containers/06819759aa16330f52b3f3c9d50bb94c1f686d927dcf524d90781e507aafb70e/shm\nshm              64M     0   64M   0% /var/lib/docker/containers/b17d5cdd07a5875be3d9214177625b1c4f85808fcc662b5fb43d2ef408260d2f/shm\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/6b2af5f2f059bf68e9fb0774d8d98138e6a9b0609e37ad726e010be412216219\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/181c5895715fb299b0c90ad62d97c4e2108f230f5f36db114bffedde9a525584\nshm              64M     0   64M   0% /var/lib/docker/containers/9653ac6bf4bde743014c60bed40a5101ea6e47fc0d4fea149ae141c6257dd75a/shm\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/0ea5889b10a2bab3924213011e97a849f0cf0de1d915c4ec20bfa756ab031559\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/a238835cf8e9afd4b32850f6c42bfaff6ea4d845511a938f125cdfca3f4cd92e\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/7bba68571f1d22bd58d337cc9e2f88106a803e3990c0833484daa7fb163821cf\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/e279ea4668d098727978291b737b78d1d9f4fd1df5712efd9c8a28dccd73069c\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/353f42237fd3a93f4b172c84b61d5dd8bccbbd6eb2ce20f384c4cd0cc784ad84\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/c34005049d1cc65864a64a1992f76fedf847f38d3b1204ad8ba5eca1eb15e6b2\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/59ca50a22d08a17aa309d0fcd214404ce654ea85a9fb92dcf7a534ed081f082c\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/6221efdc-47e6-11e6-9ac4-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/62300a79-47e6-11e6-9ac4-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/e6e958f30da143f341c649b00f71ff45a0259b1785d47eeb514ea122af0cae98\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/57b51c768070bcf07de816822feee61523cff21b903b2e2d0829dd54af19d0b7\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/e7383aa39fced360c4d1f27088bb035e754070e9af801f0acc971cdd277b0409\nshm              64M     0   64M   0% /var/lib/docker/containers/ccab5c6e9fc3c414528e920bfdbaed21de76b3c43470bb759d122c7c3d15a541/shm\nshm              64M     0   64M   0% /var/lib/docker/containers/b11d677f31d2ff73de1bc20e322ab36ce65173beb3e6a24eca3910bfa7136443/shm\nshm              64M     0   64M   0% /var/lib/docker/containers/95ddb493c4c32c718bb7e356953d7602ec64abbaad615b52b9d419a97ad7f87f/shm\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/cb954242d76a953d1e8d2631015a898ac2a1a304a1fa1d6386f8dccb0c0f808c\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/61e24021-47e6-11e6-9ac4-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\nshm              64M     0   64M   0% /var/lib/docker/containers/3a12814affccd71c625c74e56e61b548768a3117edc74b7cb15093c8753c5f48/shm\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/e8ede4205e41aebfb01f2dc1e3b8105571f00ebb74a592af664701894506eda1\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/2f054f795f8966ea01a1b01afd3bd4c4ea04651092dbc81c2eb56c7d5837de7a\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/e28829825aebbd7c4c431779cc3b30af7379d4d9e400726a54eb50c2a0edb317\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/1bb42b0e55f3995b9b848d0baa9e53a493562d8c8436dfb9ba673c1b6910516f\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/c302b8c76cda56a89c3f52a331f730c49eedabfd6648f559bcc26108d05c94f8\nshm              64M     0   64M   0% /var/lib/docker/containers/70a6e06116b10659a44014c466df5146c1a5c3aec97e7c28c21fadd4b6a54e09/shm\nshm              64M     0   64M   0% /var/lib/docker/containers/4cb005e4d280b067a84eb07eee2944780cf478895bfef2697ee3d193ad3e4e46/shm\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/eea8619bc64584cff1ea8d3d4f3705e5450df3b83d8cc6af0ddedcb68d369205\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/a8dbfd05351295a341ad1ff477b2a1f3ea553fe5635f5c697cecea62fb970f10\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/5ee56cdb6faa81e0100321fa62c0ee5ccf5f7f3387264f6363c88e77dde00fb5\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/6753e18f066f64a13fcd03d6c6de7197ceaee87da6179ba522efcbf02ee82f5e\nnone            916G   19G  851G   3% /var/lib/docker/aufs/mnt/2eb7cff799e019418318c452765bae94030245372da374517164de52f8abc1ec\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/c3b616b1-47fb-11e6-b885-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/c5072c88-47fb-11e6-b885-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/3afc0c3e-47fe-11e6-b885-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/3c46914b-47fe-11e6-b885-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/6d46ecac-48b7-11e6-b885-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/8f87d1c8-48b7-11e6-b885-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/2bd2ca8b-48b8-11e6-b885-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/69348d60-48b8-11e6-b885-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/82d98bb8-48be-11e6-b885-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/84449da5-48be-11e6-b885-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/272efcaa-48d6-11e6-b885-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/c194b820-48d6-11e6-b885-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/4f121dcb-4a46-11e6-b885-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/906f8bdd-4a4e-11e6-b885-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/5be54b0b-4e57-11e6-b885-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/5bc22173-4e57-11e6-b885-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\ntmpfs            63G     0   63G   0% /var/lib/kubelet/pods/aed47a9c-4e9a-11e6-b885-001e67548707/volumes/kubernetes.io~secret/default-token-bm3z3\nI am not sure about the disk storage situation. And the memory is totally enough.\n. see kubernetes issue here [https://github.com/kubernetes/kubernetes/issues/28709]. The problem seems similar after one pod had been deleted.\n. @derekchiang @jdoliner  I have checked the Kubernetes issue. The main idea of it is that if we have a great mount of pods, it will make device busy and the mount process would be repeated for several times. Sine Kubernetes use Exponential Backoff algorithm. So they will make a patch in Kubernetes 1.3.4 to improve the algorithm. However, I only mount two volumn, I am sure the solution mentioned doesn't match my problem.\n. hmm, it seems that nothing I can do to this error.  I have to deploy pachyderm on another server.   Hope someone can give me some notice about the mount problem.\n. @jdoliner  I recheck the file system. That's true that many mounts of pfs is in the file system. Even after I delete the pipeline and repo, it still exists. If I try to delete the mount point, it shoes that the device is busy. \n. @jdoliner You can close this issue too. This problem does not occur again when I use kubernetes1.2.2+Pachyderm 1.1.0, hope your guys can make pachyderm compatible with kubernetes1.3/1.4. Thanks for your patience and help! \n. @jdoliner I use the latest kubernetes stable version 1.3.2 and cause this problem.  I haven't tried to use kubernetes 1.2.2 before. Since the official guide of kubernetes has all upgraded to 1.3 version, therefore I can't find the method to deploy 1.2.2. However, I use two method to deploy it, but they all failed. \n. Method 1 I use docker to deploy kubernetes.\nexport K8S_VERSION=v1.2.2\nexport ARCH=amd64\ndocker run -d \\\n    --volume=/:/rootfs:ro \\\n    --volume=/sys:/sys:rw \\\n    --volume=/var/lib/docker/:/var/lib/docker:rw \\\n    --volume=/var/lib/kubelet/:/var/lib/kubelet:rw \\\n    --volume=/var/run:/var/run:rw \\\n    --net=host \\\n    --pid=host \\\n    --privileged \\\n    gcr.io/google_containers/hyperkube-${ARCH}:${K8S_VERSION} \\\n    /hyperkube kubelet \\\n        --containerized \\\n        --hostname-override=127.0.0.1 \\\n        --api-servers=http://localhost:8080 \\\n        --config=/etc/kubernetes/manifests \\\n        --cluster-dns=10.0.0.10 \\\n        --cluster-domain=cluster.local \\\n        --allow-privileged --v=2\nWhen I finish the deploying, the kubectl doesn't work. See here\nkubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.2\", GitCommit:\"528f879e7d3790ea4287687ef0ab3f2a01cc2718\", GitTreeState:\"clean\"}\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\nI think that the 8080 of api-server is blocked. But I am not sure.\n. Method2 I use make launch-kube in \\pavhyderm\\Makefile\nlaunch-kube: check-kubectl\n        etc/kube/start-kube-docker.sh\nSee the script here\n```\n!/bin/sh\nset -Ee\ndocker run \\\n    -d \\\n    --volume=/:/rootfs:ro \\\n    --volume=/sys:/sys:ro \\\n    --volume=/dev:/dev \\\n    --volume=/var/lib/docker/:/var/lib/docker:rw \\\n    --volume=/var/lib/kubelet/:/var/lib/kubelet:rw \\\n    --volume=/var/run:/var/run:rw \\\n    --net=host \\\n    --pid=host \\\n    --privileged=true \\\n    gcr.io/google_containers/hyperkube:v1.2.2 \\\n    /hyperkube kubelet \\\n        --containerized \\\n        --hostname-override=\"127.0.0.1\" \\\n        --address=\"0.0.0.0\" \\\n        --api-servers=http://localhost:8080 \\\n        --config=/etc/kubernetes/manifests \\\n        --allow-privileged=true\nuntil kubectl version 2>/dev/null >/dev/null; do sleep 5; done\n```\nUnfortunately, the same problem occur\n```\nkubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.2\", GitCommit:\"528f879e7d3790ea4287687ef0ab3f2a01cc2718\", GitTreeState:\"clean\"}\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n``\n. When I use kubernetes 1.3.0, no such problems. I can use kubernetes successfully. \n. See~/.kube/config` here\napiVersion: v1\nclusters:\n- cluster:\n    server: http://localhost:8080\n  name: test-doc\ncontexts:\n- context:\n    cluster: test-doc\n    user: \"\"\n  name: test-doc\ncurrent-context: test-doc\nkind: Config\npreferences: {}\nusers: []\n. It's my network situation here.\nThe server's IP : 172.16.6.55\nThe server use the http_proxy:http://<proxy_name>:914/\nThe server also uses https_proxy: https://<proxy_name>:914/\nI use ssh to connect the server.\nThe netstat information here:\nProto Recv-Q Send-Q Local Address           Foreign Address         State\ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:7001          0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:4001          0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:2380          0.0.0.0:*               LISTEN\ntcp6       0      0 :::22                   :::*                    LISTEN\ntcp6       0      0 :::4194                 :::*                    LISTEN\ntcp6       0      0 :::10250                :::*                    LISTEN\ntcp6       0      0 :::10251                :::*                    LISTEN\ntcp6       0      0 :::10255                :::*                    LISTEN\n8080 is not bind.\nSee a successfully deployed kubernetes 1.3.0\nProto Recv-Q Send-Q Local Address           Foreign Address         State\ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:7001          0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:4001          0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:2380          0.0.0.0:*               LISTEN\ntcp        0      0 127.0.0.1:8080          0.0.0.0:*               LISTEN\ntcp6       0      0 :::22                   :::*                    LISTEN\ntcp6       0      0 :::30650                :::*                    LISTEN\ntcp6       0      0 :::4194                 :::*                    LISTEN\ntcp6       0      0 :::10250                :::*                    LISTEN\ntcp6       0      0 :::10251                :::*                    LISTEN\ntcp6       0      0 :::6443                 :::*                    LISTEN\ntcp6       0      0 :::10252                :::*                    LISTEN\ntcp6       0      0 :::10255                :::*                    LISTEN\ntcp6       0      0 :::32080                :::*                    LISTEN\ntcp6       0      0 :::32081                :::*                    LISTEN\ntcp6       0      0 :::32082                :::*                    LISTEN\n8080 and 30650 is bind. Need I  disconnect the proxy server when deploying kubernetes? \n. Also see docker list here,\nCONTAINER ID        IMAGE                                             COMMAND                  CREATED             STATUS              PORTS               NAMES\na458370bc041        gcr.io/google_containers/hyperkube-amd64:v1.2.2   \"/hyperkube scheduler\"   About an hour ago   Up About an hour                        k8s_scheduler.fc12fcbe_k8s-master-127.0.0.1_default_4c6ab43ac4ee970e1f563d76ab3d3ec9_4aef43e8\nc27171089e72        gcr.io/google_containers/etcd:2.2.1               \"/usr/local/bin/etcd \"   About an hour ago   Up About an hour                        k8s_etcd.7e452b0b_k8s-etcd-127.0.0.1_default_1df6a8b4d6e129d5ed8840e370203c11_59ad41dc\ne84737652501        gcr.io/google_containers/hyperkube-amd64:v1.2.2   \"/hyperkube proxy --m\"   About an hour ago   Up About an hour                        k8s_kube-proxy.9a9f4853_k8s-proxy-127.0.0.1_default_5e5303a9d49035e9fad52bfc4c88edc8_1f8959d0\na5d601e99e00        gcr.io/google_containers/pause:2.0                \"/pause\"                 About an hour ago   Up About an hour                        k8s_POD.6059dfa2_k8s-etcd-127.0.0.1_default_1df6a8b4d6e129d5ed8840e370203c11_e66abc6a\n3be82a84ada9        gcr.io/google_containers/pause:2.0                \"/pause\"                 About an hour ago   Up About an hour                        k8s_POD.6059dfa2_k8s-master-127.0.0.1_default_4c6ab43ac4ee970e1f563d76ab3d3ec9_9157d23f\n6193e7999d3c        gcr.io/google_containers/pause:2.0                \"/pause\"                 About an hour ago   Up About an hour                        k8s_POD.6059dfa2_k8s-proxy-127.0.0.1_default_5e5303a9d49035e9fad52bfc4c88edc8_4ba5f669\n05d1fa8c86c2        gcr.io/google_containers/hyperkube:v1.2.2         \"/hyperkube kubelet -\"   About an hour ago   Up About an hour                        admiring_hamilton\n. I have worked on deploying Kubernetes v1.2.2 for a whole day, but do not solve this problem. The docker hyperkube cannot dial to 127.0.0.1:8080. Both of methods doesn't work for this task.\nI use nmap localhost to find the network port situation.\nIn a unsuccessfully deployed 1.2.2 system, it can be \n```\n nmap localhost\nStarting Nmap 6.40 ( http://nmap.org ) at 2016-07-22 16:38 CST\nNmap scan report for localhost (127.0.0.1)\nHost is up (0.000023s latency).\nOther addresses for localhost (not scanned): 127.0.0.1\nNot shown: 997 closed ports\nPORT     STATE SERVICE\n22/tcp   open  ssh\n4001/tcp open  newoak\n7001/tcp open  afs3-callback\nNmap done: 1 IP address (1 host up) scanned in 2.06 seconds\n```\nIn a successfully deployed kubernetes 1.3.0 it is \n```\n nmap localhost\nStarting Nmap 6.40 ( http://nmap.org ) at 2016-07-22 16:39 CST\nNmap scan report for localhost (127.0.0.1)\nHost is up (0.000023s latency).\nOther addresses for localhost (not scanned): 127.0.0.1\nNot shown: 996 closed ports\nPORT     STATE SERVICE\n22/tcp   open  ssh\n4001/tcp open  newoak\n7001/tcp open  afs3-callback\n8080/tcp open  http-proxy\nNmap done: 1 IP address (1 host up) scanned in 2.08 seconds\n```\nSee that 8080 is bind, and http-proxy use this port.\nSo I think the main problem in v1.2.2 is how to proxy the port 8080 and then it can works. Although I use similar methods to deploy v1.2.2 and v1.3.0, it seems that the proxy problem has been solved by the kubernetes upgrade.  But now I have to face the problem because of the old version. \nI search for the solution,\nsee here http://kubernetes.io/docs/getting-started-guides/docker/\n\nIf you are behind a proxy, you need to pass the proxy setup to curl in the containers to pull the certificates. Create a .curlrc under /root folder (because the containers are running as root) with the following line: proxy = :\n\nHowever, I insert the proxy in the /root/.curlrc of the server, but no effects. Also I use docker run -p and kubelet -proxy=<proxy>, no improvement at all.\n. Mentioned in trouble shooting of pachyderm,\n\nIf you see the following:\n$ kubectl get all\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\nYou probably have not enabled port forwarding. You can start port forwarding by running something like:\nssh <HOST> -fTNL 8080:localhost:8080 -L 30650:localhost:30650\nWhere  is one of the names of your docker hosts. You can see a list by running:\ndocker-machine ls\n\nBut I deploy the kubernetes on local server instead a docker machine. If I use docker-machine ls, the server note me it is an error command.\n. Note: \nI think maybe we can transform the way of thinking. I speculate that possibly the bug is because of the K8S_apiserver doesn't run !  \ndocker ps -a | grep api\na096a57c6395        gcr.io/google_containers/hyperkube-amd64:v1.2.2   \"/hyperkube apiserver\"   55 seconds ago       Exited (255) 53 seconds ago                           k8s_apiserver.78ec1de_k8s-master-127.0.0.1_default_4c6ab43ac4ee970e1f563d76ab3d3ec9_bf5d73be\n05bb8901a245        gcr.io/google_containers/hyperkube-amd64:v1.2.2   \"/hyperkube apiserver\"   2 minutes ago        Exited (255) 2 minutes ago                            k8s_apiserver.78ec1de_k8s-master-127.0.0.1_default_4c6ab43ac4ee970e1f563d76ab3d3ec9_8b14199a\ncc4aeb5414e2        gcr.io/google_containers/hyperkube-amd64:v1.2.2   \"/hyperkube apiserver\"   3 minutes ago        Exited (255) 3 minutes ago                            k8s_apiserver.78ec1de_k8s-master-127.0.0.1_default_4c6ab43ac4ee970e1f563d76ab3d3ec9_85583fc9\nSee the apiserver is exited each minute. \n. Hi @jdoliner The api-server is down in the deployment. \nSee apiserver logs here,\nsv: batch/v1\n  mv: extensions/__internal\nI0725 09:01:28.421642       1 genericapiserver.go:82] Adding storage destination for group batch\nW0725 09:01:28.421682       1 server.go:383] No RSA key provided, service account token authentication disabled\nF0725 09:01:28.421704       1 server.go:410] Invalid Authentication Config: open /srv/kubernetes/basic_auth.csv: no such file or directory\nI haven't get the solution for this problem. BTW, what do you mean by 'proxying port 8080', sorry I didn't catch it.\n. @jdoliner I have tested the deployment of Kubernetes and Pachyderm on four servers with different versions. \nThe conclusion go here that \n1 for kubernetes >= v.1.3.0 (including v1.3.0 v1.3.2 v1.3.3), the deployment of pachyderm dose not have problems. We can create and delete the repo. But the pipeline job is failure with the information the server has asked for the client to provide credentials (get pods) The new issues will be post soon to solve this problem by Pachyderm team.\n2 for kubenetes < v1.3.0 (including v1.2.0 v.1.2.2), the official guide http://kubernetes.io/docs/getting-started-guides/docker/ cannot be followed to deploy kubernetes, since the apiserver does not start. The information in the apiserver docker log can be seen here. I have join the kubernetes team in Slack to ask for help.\nI0725 09:01:28.421642       1 genericapiserver.go:82] Adding storage destination for group batch\nW0725 09:01:28.421682       1 server.go:383] No RSA key provided, service account token authentication disabled\nF0725 09:01:28.421704       1 server.go:410] Invalid Authentication Config: open /srv/kubernetes/basic_auth.csv: no such file or directory\nI attempt each version for several times on my servers to avoid the operation error. And I regard it as a common phenomenon instead of an accidental error. If pachyderm team has time to work on it, please re-verify the deployment process from the beginning and make sure it can work with a suitable version.   Thanks a lot if you can do it, I so appreciate your help in this topic.\n. @jdoliner I have contacted the Kubernetes team on Slack. They help me to check the logs. The reason for the 1.2.2 failure is the Setup Pod is not triggered. However, the remote check hasn't found the boot reason of the phenomenon. Also, they give a suggestion to change the OS system from ubuntu14.04 to other os. Would you mind verify the deployment process on the local machine ubuntu 14.04 behind the proxy server with Kubernetes 1.2.2? Thanks a lot if you can do this verification.\n. @jdoliner Thank you for your patience. Since my local servers cannot be used in deployment. I try to use AWS EC2 instead of the local server. The ubuntu14.04+kubernetes 1.2.2+Pachyderm1.1.0 works on AWS servers, and examples can run.\nFor the solution to this problem, I think the proxy port is not open for some application, since the only different difference between the EC2 and local server is the network configuration. However, it is just a speculation ,I am not sure about it. \nI would still work on Pachyderm. The idea of Pachyderm to use the docker as a job is really an exciting idea different from hadoop or spark. \nIf anyone meet similar problems in the future, this issue can be referenced. And you can close it now.  Thanks for your help.\n. Note: In the second experiment, i.e the local server experiment. I do not use the Pachyderm nor Hadoop. 1. launch a new server on AWS\n2. download the original data and install the runtime environment (GO environment)\n3. execute the map.go program to do map task \n4  execute the find /pfs/wordcount_map -name '*' | while read count; do cat $count | awk '{ sum+=$1} END {print sum}' >/tmp/count; mv /tmp/count /pfs/out/basename $count; done to do reduce task. \nAll these steps are manually fulfilled.  And a timer is set to calculate the processing time.\n. @jdoliner Thanks you for your advice. I use general SSD (gp2) disk for AWS VM, also the dick operation is regarded as the most possible bottleneck in our team. I still have some further problems. As you have noticed, Pachyderm will be optimized so that writing operation can be packed into a single S3, does it mean that currently Pachyderm uses more than one S3 volume to work even I only setup one docker container to fulfill the reduce task?\n@derekchiang Thanks, as a memory-computation data platform, Hadoop has an obvious velocity advantage over the disk-computation Pachyderm. I think the explanation can  give a convincing explanation to the velocity difference.\n. A table is given to show the performance test more clearly\n\n. @derekchiang For a single-node Hadoop, the operation is based on memory and local file system. However, let's focus on the difference of local server and Pachyderm first. It's important to give a detailed clarification here.\nThe aws has an automatic method to deploy Kubernetes cluster, and Pachyderm can be deployed on this five-node kubernetes cluster. In this case, S3 bucket is used to transform the data from one node to the other node.\nHowever in my experiment, I launched an instance on AWS. and launch the kubernetes single-node cluster on this instance. The AWS server plays a role the same with a local server.\nThe script used is \ndocker run \\\n    -d \\\n    --volume=/:/rootfs:ro \\\n    --volume=/sys:/sys:ro \\\n    --volume=/dev:/dev \\\n    --volume=/var/lib/docker/:/var/lib/docker:rw \\\n    --volume=/var/lib/kubelet/:/var/lib/kubelet:rw \\\n    --volume=/var/run:/var/run:rw \\\n    --net=host \\\n    --pid=host \\\n    --privileged=true \\\n    gcr.io/google_containers/hyperkube:v1.2.2 \\\n    /hyperkube kubelet \\\n        --containerized \\\n        --hostname-override=\"127.0.0.1\" \\\n        --address=\"0.0.0.0\" \\\n        --api-servers=http://localhost:8080 \\\n        --config=/etc/kubernetes/manifests \\\n        --allow-privileged=true\nuntil kubectl version 2>/dev/null >/dev/null; do sleep 5; done\nSee kubernetes node here,\n\nSee kubernetes work status here, \n\nAfter kubernetes has been launched, I deploy Pachyderm by,\ncd $GOPATH/src/github.com/pachyderm/pachyderm\nmake launch\nIn conclusion, the aws vm used in this experiment is just like a local server. Does S3 problem still exist in this step?\n. @derekchiang I would repeat the test as soon as possible to verify the test result. Thanks for your help and the two points you give about the experimental environment are right.\n. @derekchiang The experiment has been repeated today, no significant difference between the experiment yesterday.\n\n. @derekchiang  Sorry for response this issue late. It seems not work for the aws deployment.\nSee pachd pod work in two nodes,\nNAME            READY     STATUS    RESTARTS   AGE       NODE\netcd-qd1xn      1/1       Running   0          25m       ip-172-20-0-253.us-west-2.compute.internal\npachd-czle8     1/1       Running   5          25m       ip-172-20-0-252.us-west-2.compute.internal\npachd-gd4bk     1/1       Running   5          25m       ip-172-20-0-253.us-west-2.compute.internal\nrethink-g3isl   1/1       Running   0          25m       ip-172-20-0-253.us-west-2.compute.internal\nWhen we use the public IP address of pachd-czle8\n```\nexport ADDRESS=54.218.71.178:30650\npachctl version\nCOMPONENT           VERSION\npachctl             1.1.0-9a683a7fa30cf3215e846979e50a3f7991789672\npachd               (version unknown) : error connecting to pachd server at address (54.218.71.178:30650): context deadline exceeded\nplease make sure pachd is up (kubectl get all) and portforwarding is enabled\n```\nWhen we use the public IP address of pachd-gd4bk\n```\nexport ADDRESS=54.149.184.159:30650\npachctl version\nCOMPONENT           VERSION\npachctl             1.1.0-9a683a7fa30cf3215e846979e50a3f7991789672\npachd               (version unknown) : error connecting to pachd server at address (54.149.184.159:30650): context deadline exceeded\nplease make sure pachd is up (kubectl get all) and portforwarding is enabled\n```\nSo neither of the public addresses work. Could you give me a possible method to enable the communication between pachctl and pachd pods. Thanks for that. \n. The port forwarding problem has been solved.We need use the following command to do the port-forwarding.\nkubectl port-forward <pachyderm pods name> 30650:650 &\nFor example,\nkubectl port-forward  pachd-czle8 30650:650 &\n@jdoliner @derekchiang Do you think we need give the explanation on the document? Since it is not an apparent solution most times.\n. I am not sure whether the check failure is fatal. The failure happens when the travis-ci runs\nCGOENABLED=0 GO15VENDOREXPERIMENT=1 go test -cover -short $(go list ./src/server/... | grep -v '/src/server/vendor/' | grep -v '/src/server/pfs/fuse')\n--- FAIL: TestAssets (0.01s)\n    require.go:153: /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/pachyderm_test.go:1471\nAs the comment in the program  /src/server/pachyderm_test.go:1471\n```\n // This test fails if you updated some static assets (such as doc/pipeline_spec.md)\n// that are used in code but forgot to run:\n// $ make assets\nfunc TestAssets(t *testing.T) {\n        assetPaths := []string{\"doc/pipeline_spec.md\"}\n        for _, path := range assetPaths {\n                doc, err := ioutil.ReadFile(filepath.Join(os.Getenv(\"GOPATH\"), \"src/github.com/pachyderm/pachyderm/\", path))\n                if err != nil {\n                        t.Fatal(err)\n                }\n            asset, err := pachyderm.Asset(path)\n            if err != nil {\n                    t.Fatal(err)\n            }\n\n            require.Equal(t, doc, asset)\n    }\n\n}\n```\nNote: The pipeline_spec.md has just been updated yesterday, so seems that someone forgets to run make assets after the commit leading to the error.\nActually, I think my request has no relation with this test, since the test is about the local-deployment and my commit is about aws-deployment. Also, the main body of my request has just passed the online test yesterday before the commit on pipeline_spec.md , therefore I am not sure whether I should keep attention on this error, within the fact that the revised program can be executed correctly in my experiments.\nProgress: make assests to rebuild the assests.go, pass this test!\n. The conflict has been fixed! And CI failed with only one mistake in the last step of the integration-test. \nThe failure test is\n--- FAIL: TestPipelineThatOverwritesFile (60.17s)\n    require.go:153: /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/pachyderm_test.go:822\n    require.go:155: No error is expected but got rpc error: code = 4 desc = context deadline exceeded\ncommit repo0bfb4fe2bbcb_4/960d12754d4f4040a3b7b972c92cf199 was cancelled\nLookup the CI log for more information. As src/server/pachyderm_test.go , I think that the failure has no relation with my revise. Also, this step could be skipped in a short mode, but actually not. \nTherefore, I think the request probably can be merged now. \n. @derekchiang Could you give me some recommended applications that involving little file operations but large file size? I would work on it tomorrow.\n. @msteffen I also met this situation. Usually I terminate the Pachd pods manually, and then continue to the following work. \n. RE-test the Pachyderm with new master-version today. The version conflict and json problem has been solved. Thanks ! @jdoliner You can close the issue for it has been solved now.\n. RE-test the Pachyderm with new master-version today. The version conflict and json problem has been solved. Thanks ! @jdoliner You can close the issue for it has been solved now.\n. @derekchiang Yes , I am running the pipeline with a parallelism greater than 1.\nThe test pipeline is,\n{\n  \"pipeline\": {\n    \"name\": \"wordcount_reduce_p5\"\n  },\n  \"transform\": {\n    \"image\": \"pachyderm/job-shim:latest\",\n    \"cmd\": [\"sh\"],\n    \"stdin\": [\n        \"find /pfs/wordcount_map -name '*' | while read count; do cat $count | awk '{ sum+=$1} END {print sum}' >/tmp/count; mv /tmp/count /pfs/out/`basename $count`; done\"\n    ]\n  },\n  \"parallelism\": 5 ,\n  \"inputs\": [\n    {\n      \"repo\": {\n        \"name\": \"wordcount_map\"\n      },\n          \"method\": \"reduce\"\n    }\n  ]\n}\n. Retest the program, and it is found repeatedly.\nThe pipeline begins from 03:55 and all of the logs occurred after 03:55 is recorded.\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9301.764111] device veth84b51f3 entered promiscuous mode\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9301.764206] IPv6: ADDRCONF(NETDEV_UP): veth84b51f3: link is not ready\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9301.764210] docker0: port 3(veth84b51f3) entered forwarding state\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9301.764215] docker0: port 3(veth84b51f3) entered forwarding state\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9301.764495] docker0: port 3(veth84b51f3) entered disabled state\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9301.765576] device vethae8554e entered promiscuous mode\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9301.769374] IPv6: ADDRCONF(NETDEV_UP): vethae8554e: link is not ready\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9301.769378] docker0: port 6(vethae8554e) entered forwarding state\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9301.769383] docker0: port 6(vethae8554e) entered forwarding state\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9301.888712] device veth5e0d882 entered promiscuous mode\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9301.888798] IPv6: ADDRCONF(NETDEV_UP): veth5e0d882: link is not ready\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9301.888803] docker0: port 7(veth5e0d882) entered forwarding state\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9301.888807] docker0: port 7(veth5e0d882) entered forwarding state\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9301.901204] device veth60544b5 entered promiscuous mode\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9301.901288] IPv6: ADDRCONF(NETDEV_UP): veth60544b5: link is not ready\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9301.901293] docker0: port 8(veth60544b5) entered forwarding state\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9301.901297] docker0: port 8(veth60544b5) entered forwarding state\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9302.184349] IPv6: ADDRCONF(NETDEV_CHANGE): veth5e0d882: link becomes ready\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9302.184394] IPv6: ADDRCONF(NETDEV_CHANGE): veth84b51f3: link becomes ready\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9302.184416] docker0: port 3(veth84b51f3) entered forwarding state\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9302.184420] docker0: port 3(veth84b51f3) entered forwarding state\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9302.212860] IPv6: ADDRCONF(NETDEV_CHANGE): vethae8554e: link becomes ready\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9302.212886] IPv6: ADDRCONF(NETDEV_CHANGE): veth60544b5: link becomes ready\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9302.304133] net_ratelimit: 5 callbacks suppressed\nSep  7 03:55:38 ip-172-31-32-47 kernel: [ 9302.304137] IPv6: eth0: IPv6 duplicate address fe80::42:acff:fe11:4 detected!\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9302.629605] device veth91f8cb3 entered promiscuous mode\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9302.629724] IPv6: ADDRCONF(NETDEV_UP): veth91f8cb3: link is not ready\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9302.629728] docker0: port 9(veth91f8cb3) entered forwarding state\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9302.629733] docker0: port 9(veth91f8cb3) entered forwarding state\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9302.796310] docker0: port 9(veth91f8cb3) entered disabled state\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9302.800137] IPv6: eth0: IPv6 duplicate address fe80::42:acff:fe11:9 detected!\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9302.800780] IPv6: ADDRCONF(NETDEV_CHANGE): veth91f8cb3: link becomes ready\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9302.800818] docker0: port 9(veth91f8cb3) entered forwarding state\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9302.800823] docker0: port 9(veth91f8cb3) entered forwarding state\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9302.854309] device veth6372f68 entered promiscuous mode\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9302.854399] IPv6: ADDRCONF(NETDEV_UP): veth6372f68: link is not ready\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9302.854404] docker0: port 10(veth6372f68) entered forwarding state\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9302.854409] docker0: port 10(veth6372f68) entered forwarding state\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9302.864286] IPv6: eth0: IPv6 duplicate address fe80::42:acff:fe11:8 detected!\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9303.044870] IPv6: ADDRCONF(NETDEV_CHANGE): veth6372f68: link becomes ready\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9303.223746] device vethf71a46b entered promiscuous mode\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9303.223838] IPv6: ADDRCONF(NETDEV_UP): vethf71a46b: link is not ready\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9303.223843] docker0: port 11(vethf71a46b) entered forwarding state\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9303.223848] docker0: port 11(vethf71a46b) entered forwarding state\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9303.477619] IPv6: ADDRCONF(NETDEV_CHANGE): vethf71a46b: link becomes ready\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9303.481885] device veth3976e7a entered promiscuous mode\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9303.481982] IPv6: ADDRCONF(NETDEV_UP): veth3976e7a: link is not ready\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9303.481986] docker0: port 12(veth3976e7a) entered forwarding state\nSep  7 03:55:39 ip-172-31-32-47 kernel: [ 9303.481991] docker0: port 12(veth3976e7a) entered forwarding state\nSep  7 03:55:40 ip-172-31-32-47 kernel: [ 9303.644165] IPv6: eth0: IPv6 duplicate address fe80::42:acff:fe11:a detected!\nSep  7 03:55:40 ip-172-31-32-47 kernel: [ 9303.684990] IPv6: ADDRCONF(NETDEV_CHANGE): veth3976e7a: link becomes ready\nSep  7 03:55:40 ip-172-31-32-47 kernel: [ 9303.860176] IPv6: eth0: IPv6 duplicate address fe80::42:acff:fe11:b detected!\nSep  7 03:55:40 ip-172-31-32-47 kernel: [ 9304.220157] IPv6: eth0: IPv6 duplicate address fe80::42:acff:fe11:d detected!\nSep  7 03:55:40 ip-172-31-32-47 kernel: [ 9304.448147] IPv6: eth0: IPv6 duplicate address fe80::42:acff:fe11:c detected!\nSep  7 03:55:53 ip-172-31-32-47 kernel: [ 9316.796067] docker0: port 6(vethae8554e) entered forwarding state\nSep  7 03:55:53 ip-172-31-32-47 kernel: [ 9316.892068] docker0: port 7(veth5e0d882) entered forwarding state\nSep  7 03:55:53 ip-172-31-32-47 kernel: [ 9316.956072] docker0: port 8(veth60544b5) entered forwarding state\nSep  7 03:55:53 ip-172-31-32-47 kernel: [ 9317.212062] docker0: port 3(veth84b51f3) entered forwarding state\nSep  7 03:55:54 ip-172-31-32-47 kernel: [ 9317.852067] docker0: port 9(veth91f8cb3) entered forwarding state\nSep  7 03:55:54 ip-172-31-32-47 kernel: [ 9317.884068] docker0: port 10(veth6372f68) entered forwarding state\nSep  7 03:55:54 ip-172-31-32-47 kernel: [ 9318.236065] docker0: port 11(vethf71a46b) entered forwarding state\nSep  7 03:55:54 ip-172-31-32-47 kernel: [ 9318.492065] docker0: port 12(veth3976e7a) entered forwarding state\nNow time is 04:05, and Pachyderm has went into the Fake Run but no new logs.\nUnfortunately, also no OOM sign and no Error/Warning sign in the log.\n. @rexmortus You can try the Pachyderm on AWS or Google GCE if you have the account. Otherwise, a local  host (a server with 4CPU and 16GB memory) is enough to do some simple tests, which is my experiment environment. If you do not deploy it to a real commercial environment, it would certainly be okay.\n. @rexmortus Could you check the env | grep ADD to see whether the environment variable has been loaded in your system. Such function has been used up till now in my environment with no problems. Also, you can close #802 , since there is no content in this issue, thanks.\n. @rexmortus  Have no idea about it. Could you please show your result of the following command \n1 pachctl version . See the connection situation between client and server \n2 make logs in the installation file folder. See is there any error information in the logs\n3 kubectl get pods -o wide to see if there is a kubernetes connection and Pachd pods\nOr we need to ask for help from other members.\n. @rexmortus If you have pachctl version correct, it means that your pachctl can have a talk with pachd pods. If we analysis the code of Pachyderm, we will find that pachctl (client) need to make a connection to pachd(server), and pachd would tell pachctl the exact version you have. \nIn other word, if you do pachctl version right, actually you have got the correct address to connect the pachd server. So show me where you get this error information and what command you have done, also the log information is necessary for debug.\n. Glad you work on this issue. Maybe we should change the information showed in the command pachctl.  It's not a difficult work.\n. @willguxy I am not sure whether I understand your problems. Do you mean that you need the pipeline pods to deal with whole data, and then, add the results to one old repo? If you do this, you would get duplication data. Please give me an example about this issue to help me understand it. Thanks.\n. A little supplement for Jdoliner's statement. I think that @rexmortus wonders where the data file is stored, or where the file system is mount, in Pachyderm. I recommend you to read my issue #780 to help understand the structure of Pachyderm if it is necessary. A refactor would be finished in v1.2.0 to correct the problem.  A small advice that you can divide you issue into several parts, and each part can be related to one specific topic with some running results. It would be helpful for us to understand your problem.\n. See the amazon reference here http://docs.aws.amazon.com/cli/latest/reference/s3api/create-bucket.html\nThe --create-bucket-configuration LocationConstraint=$(AWS_REGION) is recommended by aws if we use the region besides the default region us-west-1. \n\nAlso the guide doesn't give an option named '--region '., even sometimes --region <regionName> still can be used.\n\nIn conclusion, I recommend to adapt the script to --create-bucket-configuration LocationConstraint=$(AWS_REGION)\n. I would correct the typo error soon. Thanks for the notice.\n. @jdoliner  For your worry, I do some additional tests to justify my request. I used a new aws account to do the following experiments with the default-region us-west-2.\nThe result is here, and a screenshot is shown to describe the situation.\n```\naws s3api create-bucket --bucket shengjieTest1 --region us-west-2\nAn error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation:\nThe unspecified location constraint is incompatible for the region specific endpoint this request was sent to.\naws s3api create-bucket --bucket shengjieTest1 --region us-east-1\n{\n    \"Location\": \"/shengjieTest1\"\n}\naws s3api create-bucket --bucket shengjietest2 --create-bucket-configuration LocationConstraint=us-west-2\n{\n    \"Location\": \"http://shengjietest2.s3.amazonaws.com/\"\n}\naws s3api create-bucket --bucket shengjietest5 --region us-west-2\n{\n    \"Location\": \"/shengjietest5\"\n}\naws s3api create-bucket --bucket shengjieTest4 --create-bucket-configuration LocationConstraint=us-west-2\nAn error occurred (InvalidBucketName) when calling the CreateBucket operation: The specified bucket is not valid.\naws s3api create-bucket --bucket shengjieTest4 --create-bucket-configuration LocationConstraint=us-east-1\nAn error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation: The us-east-1 location constraint is incompatible for the region specific endpoint this request was sent to.\n```\nIn conclusion, we summarize the error response here\n1.  The s3-bucket name is allowed to involve the uppercase letter only if the location of s3-bucket is us-east-1 with the create-bucket region --bucket <name> --region <region_name>\n2.  If we establish the s3-bucket with a name involving the uppercase lestter\n- 2.1 within the --create-bucket-configuration <default-region>, we have the error response as InvalidBucketName \n- 2.2 within the --create-bucket-configuration <none-default-region> we have the error response as IllegalLocationConstraintException\n- 2.3 within the --region <region_other _than_us-east-1> we have the error response as IllegalLocationConstraintException\n- 2.4 within the --region <us-east-1> , no error response, the bucket can be established.\nIf the bucket name follows the following points, than it can be used in both options.\n- Bucket names should not contain upper-case letters\n- Bucket names should not contain underscores (_)\n- Bucket names should not end with a dash\n- Bucket names should be between 3 and 63 characters long\n- Bucket names cannot contain dashes next to periods (e.g., my-.bucket.com and my.-bucket are invalid)\n- Bucket names cannot contain periods\nTherefore, from my opinion, a troubleshooting issue is better than revising the code. And the original code don't need to be revised. Please focus on the second part make amazon-clean. Thanks.\n. See the new commit. A shell script in the Makefile is used to provide a notice for the user when they delete the AWS pachyderm.\n. ",
    "ChaiBapchya": "kubectl get all\nbash: kubectl: command not found...\nInstall package 'kubernetes-client' to provide command 'kubectl'? [N/y] y\n- Waiting in queue... \n- Loading list of packages.... \n- Waiting in queue... \n- Waiting for authentication... \n- Waiting in queue... \n- Downloading packages... \n- Requesting data... \n- Testing changes... \n- Installing packages... \n  NAME                   CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\n  kubernetes             10.0.0.1             443/TCP    1h\n  NAME                   READY        STATUS        RESTARTS   AGE\n  k8s-etcd-127.0.0.1     1/1          Running       0          1h\n  k8s-master-127.0.0.1   4/4          Running       4          1h\n  k8s-proxy-127.0.0.1    1/1          Running       0          1h\n[root@localhost pachyderm]# make launch-kube\netc/kube/start-kube-docker.sh\nb7d67b55995f195f51761e8df6d1c42a605f9e1a3275f21507622343aa1ad516\n[root@localhost pachyderm]# make launch\nGOPATH/bin must be on your PATH to access these binaries:\nGO15VENDOREXPERIMENT=1 go install ./src/server/cmd/pachctl ./src/server/cmd/pach-deploy\nsrc/server/cmd/pachctl/main.go:5:2: cannot find package \"go.pedge.io/env\" in any of:\n    /usr/local/go/src/go.pedge.io/env (from $GOROOT)\n    /work/src/go.pedge.io/env (from $GOPATH)\nMakefile:63: recipe for target 'install' failed\nmake: *** [install] Error 1\necho $PATH\n/sbin:/bin:/usr/sbin:/usr/bin:/usr/local/go/bin\n\n. I downloaded go.pedge.io/env  in /usr/local/go/src/ \ngo get go.pedge.io/env\nThen when I try \"make launch\" in /pachyderm\nStill the error exists\n```\nGOPATH/bin must be on your PATH to access these binaries:\nGO15VENDOREXPERIMENT=1 go install ./src/server/cmd/pachctl ./src/server/cmd/pach-deploy\ngo install: no install location for directory /pachyderm/src/server/cmd/pachctl outside GOPATH\n    For more details see: go help gopath\ngo install: no install location for directory /pachyderm/src/server/cmd/pach-deploy outside GOPATH\n    For more details see: go help gopath\nMakefile:63: recipe for target 'install' failed\nmake: *** [install] Error 1\n```\n.  echo $GOPATH\n/work\nWork structure is\n/work/src/github.com/pachyderm/pachyderm\nThus according to\nhttps://github.com/pachyderm/pachyderm/issues/422\nOur repo expects to be located at $GOPATH/src/github.com/pachyderm. \nStill the error persists\n. GOPATH, GOROOT - environment variables seem to be set to null once I do go get github.com\nHence it gives me error that when I make install (it cant find it in the \n[chai@localhost /]$ export GOROOT=$HOME/go\n[chai@localhost /]$ export GOROOT=/usr/bin/go\n[chai@localhost /]$ export PATH=$PATH:$GOROOT/bin\n[chai@localhost /]$ export GOPATH=/work\n[chai@localhost /]$ cd work/\n[chai@localhost work]$ export GOROOT=/usr/local/go\n[chai@localhost work]$ go get github.com/pachyderm/pachyderm\ngithub.com/pachyderm/pachyderm\nar: u' modifier ignored sinceD' is the default (see `U')\n[chai@localhost work]$ sudo systemctl start docker\n[sudo] password for chai: \n[chai@localhost work]$ sudo su\n[root@localhost work]# ls\npkg  src\n[root@localhost work]# cd src/github.com/pachyderm/pachyderm/\n[root@localhost pachyderm]# make launch-kube\netc/kube/start-kube-docker.sh\n6b2bcc4644e9a004b6a27b915fc6cb67de7a00f4a8e1f5116a6d1aad66cc056f\n[root@localhost pachyderm]# make launch\nGOPATH/bin must be on your PATH to access these binaries:\nGO15VENDOREXPERIMENT=1 go install ./src/server/cmd/pachctl ./src/server/cmd/pach-deploy\nsrc/server/cmd/pachctl/main.go:4:2: cannot find package \"github.com/pachyderm/pachyderm/src/server/cmd/pachctl/cmd\" in any of:\n    /usr/src/github.com/pachyderm/pachyderm/src/server/cmd/pachctl/cmd (from $GOROOT)\n    ($GOPATH not set)\nsrc/server/cmd/pachctl/main.go:5:2: cannot find package \"go.pedge.io/env\" in any of:\n    /usr/src/go.pedge.io/env (from $GOROOT)\n    ($GOPATH not set)\nsrc/server/cmd/pach-deploy/main.go:6:2: cannot find package \"github.com/pachyderm/pachyderm/src/server/pkg/deploy/cmds\" in any of:\n    /usr/src/github.com/pachyderm/pachyderm/src/server/pkg/deploy/cmds (from $GOROOT)\n    ($GOPATH not set)\nMakefile:63: recipe for target 'install' failed\nmake: *** [install] Error 1\n[root@localhost pachyderm]# echo $GOPATH\n[root@localhost pachyderm]# echo $GOROOT\n[root@localhost pachyderm]# \nWhen I set GOROOT to point to location where it is installed\n[root@localhost pachyderm]# export GOROOT=/usr/local/go\nGOPATH to /work (where I did go get pachyderm)\nand try doing \nmake launch it says\n[root@localhost pachyderm]# make launch\nGOPATH/bin must be on your PATH to access these binaries:\nGO15VENDOREXPERIMENT=1 go install ./src/server/cmd/pachctl ./src/server/cmd/pach-deploy\nsrc/server/pfs/fuse/filesystem.go:15:2: cannot find package \"bazil.org/fuse\" in any of:\n    /usr/local/go/src/bazil.org/fuse (from $GOROOT)\n    /work/src/bazil.org/fuse (from $GOPATH)\nsrc/server/pfs/fuse/filesystem.go:16:2: cannot find package \"bazil.org/fuse/fs\" in any of:\n    /usr/local/go/src/bazil.org/fuse/fs (from $GOROOT)\n    /work/src/bazil.org/fuse/fs (from $GOPATH)\nsrc/server/pps/cmds/cmds.go:15:2: cannot find package \"github.com/Jeffail/gabs\" in any of:\n    /usr/local/go/src/github.com/Jeffail/gabs (from $GOROOT)\n    /work/src/github.com/Jeffail/gabs (from $GOPATH)\nsrc/client/pkg/discovery/etcd_client.go:6:2: cannot find package \"github.com/coreos/go-etcd/etcd\" in any of:\n    /usr/local/go/src/github.com/coreos/go-etcd/etcd (from $GOROOT)\n    /work/src/github.com/coreos/go-etcd/etcd (from $GOPATH)\nsrc/server/pfs/pretty/pretty.go:10:2: cannot find package \"github.com/docker/go-units\" in any of:\n    /usr/local/go/src/github.com/docker/go-units (from $GOROOT)\n    /work/src/github.com/docker/go-units (from $GOPATH)\nsrc/server/pps/pretty/pretty.go:9:2: cannot find package \"github.com/fatih/color\" in any of:\n    /usr/local/go/src/github.com/fatih/color (from $GOROOT)\n    /work/src/github.com/fatih/color (from $GOPATH)\nsrc/client/pfs/pfs.pb.go:60:8: cannot find package \"github.com/gengo/grpc-gateway/third_party/googleapis/google/api\" in any of:\n    /usr/local/go/src/github.com/gengo/grpc-gateway/third_party/googleapis/google/api (from $GOROOT)\n    /work/src/github.com/gengo/grpc-gateway/third_party/googleapis/google/api (from $GOPATH)\nsrc/client/pkg/shard/sharder.go:12:2: cannot find package \"github.com/golang/protobuf/jsonpb\" in any of:\n    /usr/local/go/src/github.com/golang/protobuf/jsonpb (from $GOROOT)\n    /work/src/github.com/golang/protobuf/jsonpb (from $GOPATH)\nsrc/client/pfs/pfs.pb.go:57:8: cannot find package \"github.com/golang/protobuf/proto\" in any of:\n    /usr/local/go/src/github.com/golang/protobuf/proto (from $GOROOT)\n    /work/src/github.com/golang/protobuf/proto (from $GOPATH)\nsrc/client/pkg/uuid/uuid.go:6:2: cannot find package \"github.com/satori/go.uuid\" in any of:\n    /usr/local/go/src/github.com/satori/go.uuid (from $GOROOT)\n    /work/src/github.com/satori/go.uuid (from $GOPATH)\nsrc/server/pkg/cmd/cmd.go:9:2: cannot find package \"github.com/spf13/cobra\" in any of:\n    /usr/local/go/src/github.com/spf13/cobra (from $GOROOT)\n    /work/src/github.com/spf13/cobra (from $GOPATH)\nsrc/server/cmd/pachctl/main.go:5:2: cannot find package \"go.pedge.io/env\" in any of:\n    /usr/local/go/src/go.pedge.io/env (from $GOROOT)\n    /work/src/go.pedge.io/env (from $GOPATH)\nsrc/server/pfs/fuse/mounter.go:11:2: cannot find package \"go.pedge.io/lion\" in any of:\n    /usr/local/go/src/go.pedge.io/lion (from $GOROOT)\n    /work/src/go.pedge.io/lion (from $GOPATH)\nsrc/client/pkg/shard/sharder.go:14:2: cannot find package \"go.pedge.io/lion/proto\" in any of:\n    /usr/local/go/src/go.pedge.io/lion/proto (from $GOROOT)\n    /work/src/go.pedge.io/lion/proto (from $GOPATH)\nsrc/client/pfs/pfs.pb.go:61:8: cannot find package \"go.pedge.io/pb/go/google/protobuf\" in any of:\n    /usr/local/go/src/go.pedge.io/pb/go/google/protobuf (from $GOROOT)\n    /work/src/go.pedge.io/pb/go/google/protobuf (from $GOPATH)\nsrc/server/pfs/cmds/cmds.go:17:2: cannot find package \"go.pedge.io/pkg/cobra\" in any of:\n    /usr/local/go/src/go.pedge.io/pkg/cobra (from $GOROOT)\n    /work/src/go.pedge.io/pkg/cobra (from $GOPATH)\nsrc/client/pfs.go:8:2: cannot find package \"go.pedge.io/proto/stream\" in any of:\n    /usr/local/go/src/go.pedge.io/proto/stream (from $GOROOT)\n    /work/src/go.pedge.io/proto/stream (from $GOPATH)\nsrc/server/pfs/fuse/filesystem.go:21:2: cannot find package \"go.pedge.io/proto/time\" in any of:\n    /usr/local/go/src/go.pedge.io/proto/time (from $GOROOT)\n    /work/src/go.pedge.io/proto/time (from $GOPATH)\nsrc/client/version/version.go:7:2: cannot find package \"go.pedge.io/proto/version\" in any of:\n    /usr/local/go/src/go.pedge.io/proto/version (from $GOROOT)\n    /work/src/go.pedge.io/proto/version (from $GOPATH)\nsrc/client/pfs/pfs.pb.go:67:2: cannot find package \"golang.org/x/net/context\" in any of:\n    /usr/local/go/src/golang.org/x/net/context (from $GOROOT)\n    /work/src/golang.org/x/net/context (from $GOPATH)\nsrc/client/pkg/grpcutil/dialer.go:4:2: cannot find package \"google.golang.org/grpc\" in any of:\n    /usr/local/go/src/google.golang.org/grpc (from $GOROOT)\n    /work/src/google.golang.org/grpc (from $GOPATH)\nsrc/server/pfs/fuse/filesystem.go:24:2: cannot find package \"google.golang.org/grpc/codes\" in any of:\n    /usr/local/go/src/google.golang.org/grpc/codes (from $GOROOT)\n    /work/src/google.golang.org/grpc/codes (from $GOPATH)\nsrc/server/pkg/obj/amazon_client.go:7:2: cannot find package \"github.com/aws/aws-sdk-go/aws\" in any of:\n    /usr/local/go/src/github.com/aws/aws-sdk-go/aws (from $GOROOT)\n    /work/src/github.com/aws/aws-sdk-go/aws (from $GOPATH)\nsrc/server/pkg/obj/amazon_client.go:8:2: cannot find package \"github.com/aws/aws-sdk-go/aws/credentials\" in any of:\n    /usr/local/go/src/github.com/aws/aws-sdk-go/aws/credentials (from $GOROOT)\n    /work/src/github.com/aws/aws-sdk-go/aws/credentials (from $GOPATH)\nsrc/server/pkg/obj/amazon_client.go:9:2: cannot find package \"github.com/aws/aws-sdk-go/aws/session\" in any of:\n    /usr/local/go/src/github.com/aws/aws-sdk-go/aws/session (from $GOROOT)\n    /work/src/github.com/aws/aws-sdk-go/aws/session (from $GOPATH)\nsrc/server/pkg/obj/amazon_client.go:10:2: cannot find package \"github.com/aws/aws-sdk-go/service/s3\" in any of:\n    /usr/local/go/src/github.com/aws/aws-sdk-go/service/s3 (from $GOROOT)\n    /work/src/github.com/aws/aws-sdk-go/service/s3 (from $GOPATH)\nsrc/server/pkg/obj/amazon_client.go:11:2: cannot find package \"github.com/aws/aws-sdk-go/service/s3/s3Manager\" in any of:\n    /usr/local/go/src/github.com/aws/aws-sdk-go/service/s3/s3Manager (from $GOROOT)\n    /work/src/github.com/aws/aws-sdk-go/service/s3/s3Manager (from $GOPATH)\nsrc/server/pkg/obj/obj.go:7:2: cannot find package \"github.com/cenkalti/backoff\" in any of:\n    /usr/local/go/src/github.com/cenkalti/backoff (from $GOROOT)\n    /work/src/github.com/cenkalti/backoff (from $GOPATH)\nsrc/server/pfs/server/obj_block_api_server.go:15:2: cannot find package \"github.com/gogo/protobuf/proto\" in any of:\n    /usr/local/go/src/github.com/gogo/protobuf/proto (from $GOROOT)\n    /work/src/github.com/gogo/protobuf/proto (from $GOPATH)\nsrc/server/pkg/metrics/segment.go:4:2: cannot find package \"github.com/segmentio/analytics-go\" in any of:\n    /usr/local/go/src/github.com/segmentio/analytics-go (from $GOROOT)\n    /work/src/github.com/segmentio/analytics-go (from $GOPATH)\nsrc/server/pkg/deploy/assets/assets.go:9:2: cannot find package \"github.com/ugorji/go/codec\" in any of:\n    /usr/local/go/src/github.com/ugorji/go/codec (from $GOROOT)\n    /work/src/github.com/ugorji/go/codec (from $GOPATH)\nsrc/server/pfs/server/api_server.go:19:2: cannot find package \"go.pedge.io/proto/rpclog\" in any of:\n    /usr/local/go/src/go.pedge.io/proto/rpclog (from $GOROOT)\n    /work/src/go.pedge.io/proto/rpclog (from $GOPATH)\nsrc/server/pkg/obj/google_client.go:7:2: cannot find package \"golang.org/x/oauth2/google\" in any of:\n    /usr/local/go/src/golang.org/x/oauth2/google (from $GOROOT)\n    /work/src/golang.org/x/oauth2/google (from $GOPATH)\nsrc/server/pkg/obj/google_client.go:8:2: cannot find package \"google.golang.org/cloud\" in any of:\n    /usr/local/go/src/google.golang.org/cloud (from $GOROOT)\n    /work/src/google.golang.org/cloud (from $GOPATH)\nsrc/server/pkg/obj/google_client.go:9:2: cannot find package \"google.golang.org/cloud/storage\" in any of:\n    /usr/local/go/src/google.golang.org/cloud/storage (from $GOROOT)\n    /work/src/google.golang.org/cloud/storage (from $GOPATH)\nsrc/server/pfs/server/api_server.go:24:2: cannot find package \"google.golang.org/grpc/metadata\" in any of:\n    /usr/local/go/src/google.golang.org/grpc/metadata (from $GOROOT)\n    /work/src/google.golang.org/grpc/metadata (from $GOPATH)\nsrc/server/pkg/metrics/external.go:6:2: cannot find package \"k8s.io/kubernetes/pkg/api\" in any of:\n    /usr/local/go/src/k8s.io/kubernetes/pkg/api (from $GOROOT)\n    /work/src/k8s.io/kubernetes/pkg/api (from $GOPATH)\nsrc/server/pkg/deploy/assets/assets.go:11:2: cannot find package \"k8s.io/kubernetes/pkg/api/resource\" in any of:\n    /usr/local/go/src/k8s.io/kubernetes/pkg/api/resource (from $GOROOT)\n    /work/src/k8s.io/kubernetes/pkg/api/resource (from $GOPATH)\nsrc/server/pkg/deploy/assets/assets.go:12:2: cannot find package \"k8s.io/kubernetes/pkg/api/unversioned\" in any of:\n    /usr/local/go/src/k8s.io/kubernetes/pkg/api/unversioned (from $GOROOT)\n    /work/src/k8s.io/kubernetes/pkg/api/unversioned (from $GOPATH)\nsrc/server/pkg/deploy/assets/assets.go:13:2: cannot find package \"k8s.io/kubernetes/pkg/apis/extensions\" in any of:\n    /usr/local/go/src/k8s.io/kubernetes/pkg/apis/extensions (from $GOROOT)\n    /work/src/k8s.io/kubernetes/pkg/apis/extensions (from $GOPATH)\nsrc/server/pkg/metrics/external.go:7:2: cannot find package \"k8s.io/kubernetes/pkg/client/unversioned\" in any of:\n    /usr/local/go/src/k8s.io/kubernetes/pkg/client/unversioned (from $GOROOT)\n    /work/src/k8s.io/kubernetes/pkg/client/unversioned (from $GOPATH)\nMakefile:63: recipe for target 'install' failed\nmake: *** [install] Error 1\n[root@localhost pachyderm]# \n. After go get of all the dependencies I got error in 4 dependencies related to k8s.io\nmake launch\nGOPATH/bin must be on your PATH to access these binaries:\nGO15VENDOREXPERIMENT=1 go install ./src/server/cmd/pachctl ./src/server/cmd/pach-deploy\nsrc/server/pkg/metrics/external.go:6:2: no buildable Go source files in /usr/local/go/src/k8s.io/kubernetes/pkg/api\nsrc/server/pkg/deploy/assets/assets.go:11:2: cannot find package \"k8s.io/kubernetes/pkg/api/resource\" in any of:\n    /usr/local/go/src/k8s.io/kubernetes/pkg/api/resource (from $GOROOT)\n    /work/src/k8s.io/kubernetes/pkg/api/resource (from $GOPATH)\nsrc/server/pkg/deploy/assets/assets.go:12:2: cannot find package \"k8s.io/kubernetes/pkg/api/unversioned\" in any of:\n    /usr/local/go/src/k8s.io/kubernetes/pkg/api/unversioned (from $GOROOT)\n    /work/src/k8s.io/kubernetes/pkg/api/unversioned (from $GOPATH)\nsrc/server/pkg/deploy/assets/assets.go:13:2: no buildable Go source files in /usr/local/go/src/k8s.io/kubernetes/pkg/apis/extensions\nsrc/server/pkg/metrics/external.go:7:2: cannot find package \"k8s.io/kubernetes/pkg/client/unversioned\" in any of:\n    /usr/local/go/src/k8s.io/kubernetes/pkg/client/unversioned (from $GOROOT)\n    /work/src/k8s.io/kubernetes/pkg/client/unversioned (from $GOPATH)\nMakefile:63: recipe for target 'install' failed\nmake: *** [install] Error 1\n. 1.I used to export $PATH, $GOPATH, $GOROOT and set the environment variables. However now I have set them in shell init script as told (although in few internet articles export also seemed to work, maybe temporarily)\n2.I have downloaded the tar file from https://golang.org/doc/install of latest version 1.6+\nin usr/local I have the extracted tar\nBut\n[root@localhost /]# go version\ngo version go1.4.2 gccgo (GCC) 5.3.1 20160406 (Red Hat 5.3.1-6) linux/amd64\nI again tried with\n[root@localhost /]# export PATH=$PATH:/usr/local/go/bin\n[root@localhost /]# go version\ngo version go1.4.2 gccgo (GCC) 5.3.1 20160406 (Red Hat 5.3.1-6) linux/amd64\n. Issue was resolved \nProblem\n2 versions of go installed\n1st GCC GO version 1.4 - already installed on my Fedora as a dev tool\n2nd Go version 1.6\nAccording to doc, 1.4 which was my default wasnt compatible with pachyderm and hence had to be removed, path changed and removed from Hash of the shell\nFinally right go version and path\n[root@localhost /]# go version\ngo version go1.6.2 linux/amd64\n[root@localhost /]# which go\n/usr/local/go/bin/go\nFollowing steps followed\n1. Get the git along with dependencies\n[root@localhost work]# go get github.com/pachyderm/pachyderm\n2.Starting docker\n[root@localhost work]# sudo systemctl start docker\n3.\n[root@localhost work]# cd ../workspace/\n[root@localhost workspace]# ls\npkg  src\n[root@localhost workspace]# cd src/github.com/pachyderm/pachyderm/\n[root@localhost pachyderm]# make launch-kube\netc/kube/start-kube-docker.sh\n62a213fadcd1cb4cc065414a020359259b7e0f617661c95460ddb3c53e663a37\nKubernete launched\n1. Pachyderm deployed on kubernete\n   [root@localhost pachyderm]# make launch\n   # GOPATH/bin must be on your PATH to access these binaries:\n   GO15VENDOREXPERIMENT=1 go install ./src/server/cmd/pachctl ./src/server/cmd/pach-deploy\n   kubectl  create -f etc/kube/pachyderm.json\n   serviceaccount \"pachyderm\" created\n   replicationcontroller \"etcd\" created\n   service \"etcd\" created\n   You have exposed your service on an external port on all nodes in your\n   cluster.  If you want to expose this service to the external internet, you may\n   need to set up firewall rules for the service port(s) (tcp:32080,tcp:32081,tcp:32082) to serve traffic.\nSee http://releases.k8s.io/release-1.2/docs/user-guide/services-firewalls.md for more details.\nservice \"rethink\" created\nreplicationcontroller \"rethink\" created\njob \"pachd-init\" created\nYou have exposed your service on an external port on all nodes in your\ncluster.  If you want to expose this service to the external internet, you may\nneed to set up firewall rules for the service port(s) (tcp:30650) to serve traffic.\nSee http://releases.k8s.io/release-1.2/docs/user-guide/services-firewalls.md for more details.\nservice \"pachd\" created\nreplicationcontroller \"pachd\" created\nwait for the pachyderm to come up\nuntil timeout 1s ./etc/kube/check_pachd_ready.sh; do sleep 1; done\nIssue is - nothing is happening after this \n. As a result of that, all other python libraries which used - twisted,awscli and other files (for e.g. scrapy,aws) became defunct..\nReason issue reported - Link - http://kubernetes.io/docs/getting-started-guides/aws/ - is mentioned in Pachyderm documentation as a procedure to deploy Kubernetes.\nAnyway, issue reported to Kubernetes\n. On the server side, at 52.37.153.65\nDocker image for ubuntu builds a container\nApache installed on the Ubuntu container\n(Basically htttp port 80 opened)\nAnd hence during \n[root@localhost pachyderm]# make MANIFEST=manifest launch\nGOPATH/bin must be on your PATH to access these binaries:\nGO15VENDOREXPERIMENT=1 go install ./src/server/cmd/pachctl ./src/server/cmd/pach-deploy ./src/server/cmd/pachctl-doc\nkubectl -s 52.34.196.247 create -f manifest\nUnable to connect to the server: dial tcp 52.34.196.247:80: i/o timeout\nMakefile:107: recipe for target 'launch' failed\nmake: *** [launch] Error 1\n. pachctl version\nCOMPONENT           VERSION             \npachctl             1.0.0(dirty)        \npachd               1.0.0(849)\npachctl version - returns version (of the server on which pachyderm is deployed right?)\nBy server version, do you refer to the pachyderm github repo version or kubectl version?\nMy kubectl version is \n```\n kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.4\", GitCommit:\"3eed1e3be6848b877ff80a93da3785d9034d0a4f\", GitTreeState:\"clean\"}\nServer Version: version.Info{Major:\"1\", Minor:\"2\", GitVersion:\"v1.2.4\", GitCommit:\"3eed1e3be6848b877ff80a93da3785d9034d0a4f\", GitTreeState:\"clean\"}\n```\n. Even I would like to help and contribute in resolving this. Since it would be more legible and comprehensible when re-arranged in that fashion..\n. Sir, even if we mount it in this fashion\n[root@chai /]# mkdir ~/pfs\nmkdir: cannot create directory \u2018/root/pfs\u2019: File exists\n[root@chai /]# pachctl mount ~/pfs &\n[2] 4240\nPachctl is mounted and Process id is output\n[root@chai /]# ls ~/pfs\nIt HANGS\nSo, despite not ending the process, it does not list the pfs\n. Realising that old mount is still present - Tried unmount the pfs\n[root@chai /]# pachctl umount ~/pfs\nbash: pachctl: command not found...\n[root@chai /]# umount ~/pfs\nHANGS\n@jdoliner \n. In my pc, \n1. pachctl list-repo lists all the repositories associated with GCS\n2.When I try to delete the repo using\npachctl delete-repo <repo-name>\n  *No error*\n1. pachctl list-repo - lists the repositories but the repositort still exists\nThis is possibly because our pipeline is running despite deleting it\n1. pachctl delete-pipeline filter102\n2. [root@chai /]# pachctl list-pipeline\nNAME                INPUT               OUTPUT              STATE               \nfilter102           data1               filter102           starting\n. Ya, I did rebuild the pachyderm\ngo get github.com/pachyderm/pachyderm\nmake install\n. ``` shell\nkubectl get all\nNAME                                     CLUSTER-IP    EXTERNAL-IP        PORT(S)    AGE\nkubernetes                               10.51.240.1                443/TCP    2d\nNAME                                     READY         STATUS             RESTARTS   AGE\n134afc472521a77b3fbd249d26f2bf78-30rb7   0/1           ImagePullBackOff   0          1d\n25ae86f3d24d272b6da236d6d98b9f96-67v09   0/1           ImagePullBackOff   0          1d\n2a8ea3dd00f533b8e85280ea4c3c3500-tzcps   0/1           ImagePullBackOff   0          1d\n2b16ed115e0230b81c475ca584f57171-sg5ud   0/1           ImagePullBackOff   0          1d\n427da1a27f3712994ac90816104a4a1e-egas4   0/1           ImagePullBackOff   0          1d\n548c37ff6f691a919a757f4b4c0e97e8-odgtk   0/1           ImagePullBackOff   0          1d\n8a0f6d5a4750656ef3da25294c41fbfe-7gz7c   0/1           ErrImagePull       0          1d\n8d6cb9bc6a70190a48b1d8cacca3fc0c-bul4w   1/1           Running            6          8h\n97d3b865088b1ff5c655fcdb9a678e5e-py0tt   0/1           ImagePullBackOff   0          12h\n9a1f951f238ba4c6d73d4bd6dd8af8bb-d86m3   0/1           ImagePullBackOff   0          12h\n9b7c13cee335de6e7b2b9952aaf0f609-x3x28   0/1           ImagePullBackOff   0          12h\n9bd2c05d0682771588fd74b80e8df6a2-t7vef   0/1           ImagePullBackOff   0          1d\n9f5b66319a6f2ecfd44c2558e1f7debf-9kcge   0/1           ImagePullBackOff   0          1d\nb4ef5d9e246d4ea9b7b137a100e941e0-awje5   0/1           ImagePullBackOff   0          1d\nc261c1e385e5c8b002967f0d6373c7f0-kles3   0/1           ImagePullBackOff   0          1d\nd1a9325c202913e3790886046b882ccc-tdxmc   0/1           ImagePullBackOff   0          1d\ne82a2a64287c5d05c99ed7656c5eb38d-xh4f1   0/1           ImagePullBackOff   0          1d\nf00ff025943ba8bec6434b0188266609-lkxf9   0/1           ImagePullBackOff   0          1d\nff63dadd8dd333269b6e0a064ea05eca-l14ia   0/1           ImagePullBackOff   0          1d\n[root@chai /]# cd pachy/src/github.com/pachyderm/pachyderm/\n[root@chai pachyderm]# make launch\ncheck that kubectl is installed\nwhich kubectl\n/google-cloud-sdk/bin/kubectl\nGOPATH/bin must be on your PATH to access these binaries:\nGO15VENDOREXPERIMENT=1 go install ./src/server/cmd/pachctl ./src/server/cmd/pach-deploy\nkubectl  create -f etc/kube/pachyderm.json\nserviceaccount \"pachyderm\" created\nreplicationcontroller \"etcd\" created\nservice \"etcd\" created\nYou have exposed your service on an external port on all nodes in your\ncluster.  If you want to expose this service to the external internet, you may\nneed to set up firewall rules for the service port(s) (tcp:32080,tcp:32081,tcp:32082) to serve traffic.\nSee http://releases.k8s.io/release-1.2/docs/user-guide/services-firewalls.md for more details.\nservice \"rethink\" created\nreplicationcontroller \"rethink\" created\njob \"pachd-init\" created\nYou have exposed your service on an external port on all nodes in your\ncluster.  If you want to expose this service to the external internet, you may\nneed to set up firewall rules for the service port(s) (tcp:30650) to serve traffic.\nSee http://releases.k8s.io/release-1.2/docs/user-guide/services-firewalls.md for more details.\nservice \"pachd\" created\nreplicationcontroller \"pachd\" created\nwait for the pachyderm to come up\nuntil timeout 1s ./etc/kube/check_pachd_ready.sh; do sleep 1; done\nNo pods found yet\nNo pods found yet\nNo pods found yet\nNo pods found yet\nNo pods found yet\nNo pods found yet\nNo pods found yet\nNo pods found yet\nNo pods found yet\nNo pods found yet\nNo pods found yet\nNo pods found yet\nNo pods found yet\nNo pods found yet\nNo pods found yet\nNo pods found yet\nNo pods found yet\nNo pods found yet\n^Z\n[1]+  Stopped                 make launch\n```\nThis has been the case for long time. Hence aborted. Any way forward? @sjezewski \n. ```\n pachctl list-repo\n^Z\n[2]+  Stopped                 pachctl list-repo\n[root@chai pachyderm]# kubectl get all\nNAME                                     DESIRED         CURRENT            AGE\netcd                                     1               1                  5m\npachd                                    2               2                  5m\nrethink                                  1               1                  5m\nNAME                                     CLUSTER-IP      EXTERNAL-IP        PORT(S)                        AGE\netcd                                     10.51.241.78                 2379/TCP,2380/TCP              5m\nkubernetes                               10.51.240.1                  443/TCP                        2d\npachd                                    10.51.252.52    nodes              650/TCP                        5m\nrethink                                  10.51.246.122   nodes              8080/TCP,28015/TCP,29015/TCP   5m\nNAME                                     READY           STATUS             RESTARTS                       AGE\n134afc472521a77b3fbd249d26f2bf78-30rb7   0/1             ImagePullBackOff   0                              1d\n25ae86f3d24d272b6da236d6d98b9f96-67v09   0/1             ImagePullBackOff   0                              1d\n2a8ea3dd00f533b8e85280ea4c3c3500-tzcps   0/1             ImagePullBackOff   0                              1d\n2b16ed115e0230b81c475ca584f57171-sg5ud   0/1             ImagePullBackOff   0                              1d\n427da1a27f3712994ac90816104a4a1e-egas4   0/1             ImagePullBackOff   0                              1d\n548c37ff6f691a919a757f4b4c0e97e8-odgtk   0/1             ImagePullBackOff   0                              1d\n8a0f6d5a4750656ef3da25294c41fbfe-7gz7c   0/1             ImagePullBackOff   0                              1d\n8d6cb9bc6a70190a48b1d8cacca3fc0c-bul4w   1/1             Running            6                              9h\n97d3b865088b1ff5c655fcdb9a678e5e-py0tt   0/1             ImagePullBackOff   0                              13h\n9a1f951f238ba4c6d73d4bd6dd8af8bb-d86m3   0/1             ImagePullBackOff   0                              13h\n9b7c13cee335de6e7b2b9952aaf0f609-x3x28   0/1             ImagePullBackOff   0                              13h\n9bd2c05d0682771588fd74b80e8df6a2-t7vef   0/1             ImagePullBackOff   0                              1d\n9f5b66319a6f2ecfd44c2558e1f7debf-9kcge   0/1             ImagePullBackOff   0                              1d\nb4ef5d9e246d4ea9b7b137a100e941e0-awje5   0/1             ImagePullBackOff   0                              1d\nc261c1e385e5c8b002967f0d6373c7f0-kles3   0/1             ImagePullBackOff   0                              1d\nd1a9325c202913e3790886046b882ccc-tdxmc   0/1             ImagePullBackOff   0                              1d\ne82a2a64287c5d05c99ed7656c5eb38d-xh4f1   0/1             ImagePullBackOff   0                              1d\netcd-5c6ua                               0/1             Pending            0                              5m\nf00ff025943ba8bec6434b0188266609-lkxf9   0/1             ImagePullBackOff   0                              1d\nff63dadd8dd333269b6e0a064ea05eca-l14ia   0/1             ImagePullBackOff   0                              1d\npachd-29px6                              0/1             Pending            0                              5m\npachd-init-0037i                         0/1             Pending            0                              5m\npachd-smta1                              0/1             Pending            0                              5m\nrethink-njbke                            0/1             Pending            0                              5m\n```\n3 states exist for the pods - Running, Pending, ImagePullBackOff\nCould we get further infomation on what actually happens, once the cluster is spinned - what functions called, which pods used, what purpose do they serve, and the meaning of the states, for better clarity (after we get rid of this error ofcourse)\n. The fact that the expected status doesn't match with the current status indicates a problem which is not resolved,  isnt it?...\nSince my output even after 8 hours is \n```\nkubectl get all\nNAME                                     DESIRED         CURRENT            AGE\netcd                                     1               1                  8h\npachd                                    2               2                  8h\nrethink                                  1               1                  8h\nNAME                                     CLUSTER-IP      EXTERNAL-IP        PORT(S)                        AGE\netcd                                     10.51.241.78                 2379/TCP,2380/TCP              8h\nkubernetes                               10.51.240.1                  443/TCP                        2d\npachd                                    10.51.252.52    nodes              650/TCP                        8h\nrethink                                  10.51.246.122   nodes              8080/TCP,28015/TCP,29015/TCP   8h\nNAME                                     READY           STATUS             RESTARTS                       AGE\n134afc472521a77b3fbd249d26f2bf78-30rb7   0/1             ImagePullBackOff   0                              1d\n25ae86f3d24d272b6da236d6d98b9f96-67v09   0/1             ImagePullBackOff   0                              1d\n2a8ea3dd00f533b8e85280ea4c3c3500-tzcps   0/1             ImagePullBackOff   0                              1d\n2b16ed115e0230b81c475ca584f57171-sg5ud   0/1             ImagePullBackOff   0                              1d\n427da1a27f3712994ac90816104a4a1e-egas4   0/1             ImagePullBackOff   0                              1d\n548c37ff6f691a919a757f4b4c0e97e8-odgtk   0/1             ErrImagePull       0                              1d\n8a0f6d5a4750656ef3da25294c41fbfe-7gz7c   0/1             ImagePullBackOff   0                              1d\n8d6cb9bc6a70190a48b1d8cacca3fc0c-bul4w   1/1             Running            6                              18h\n97d3b865088b1ff5c655fcdb9a678e5e-py0tt   0/1             ImagePullBackOff   0                              22h\n9a1f951f238ba4c6d73d4bd6dd8af8bb-d86m3   0/1             ImagePullBackOff   0                              22h\n9b7c13cee335de6e7b2b9952aaf0f609-x3x28   0/1             ImagePullBackOff   0                              22h\n9bd2c05d0682771588fd74b80e8df6a2-t7vef   0/1             ImagePullBackOff   0                              1d\n9f5b66319a6f2ecfd44c2558e1f7debf-9kcge   0/1             ImagePullBackOff   0                              1d\nb4ef5d9e246d4ea9b7b137a100e941e0-awje5   0/1             ErrImagePull       0                              1d\nc261c1e385e5c8b002967f0d6373c7f0-kles3   0/1             ImagePullBackOff   0                              1d\nd1a9325c202913e3790886046b882ccc-tdxmc   0/1             ImagePullBackOff   0                              1d\ne82a2a64287c5d05c99ed7656c5eb38d-xh4f1   0/1             ImagePullBackOff   0                              1d\netcd-5c6ua                               0/1             Pending            0                              8h\nf00ff025943ba8bec6434b0188266609-lkxf9   0/1             ImagePullBackOff   0                              1d\nff63dadd8dd333269b6e0a064ea05eca-l14ia   0/1             ImagePullBackOff   0                              1d\npachd-29px6                              0/1             Pending            0                              8h\npachd-init-0037i                         0/1             Pending            0                              8h\npachd-smta1                              0/1             Pending            0                              8h\nrethink-njbke                            0/1             Pending            0                              8h\n```\n@jdoliner @sjezewski \n. Awaiting assistance since we have hit the wall. @sjezewski @jdoliner \nsince pachctl is not working, we are not able to perform any actions related to pachyderm.\n. Yes, we do. However, in order to show that the currently running pipeline completed its work - We can add a state that associates itself with the same. So ya, Terminated is wrong in this context. Maybe, completed successfully(awaiting next input), would be right way to put it.\n. Doubt2 - Well, Pachyderm allows users to not be constrained by any specific language/script. Giving users flexibility to create pipelines, leverage the power of different scrips/languages for Warehousing or Designing the Analytics. Hence, the doubt. Hope we are able to use Python scripts for scraping somehow in the near future.\nYes. Doubt 3 was in our minds since some time. Even when our cluster was up and running and we faced issue with getting the output from the pipeline - that's when we wanted to know\n1. How CreatePipeline command works after invocation\n2. Control flow and underlying working that goes on, during the Execution of the command\nWe haven't got a direction as to how to circumvent the issue #520 . So awaiting assistance.\n. @derekchiang Thanks a lot for explaining us those methods of writing Python scripts for Pipeline. (which somehow got undocumented). Would surely try it out. However,  Our pachctl itself isnt working.since 3 days now\npachctl version\nCOMPONENT           VERSION             \npachctl             1.0.0(dirty)        \npachd               UNKNOWN: Error rpc error: code = 4 desc = context deadline exceeded\nRepeatedly stating this since many days. So despite having multiple ideas and keen on implementing them using pachyderm, we are held up by this impediment. Helplessly..Awaiting assistance...\n. Yes, definitely. Add me - chai.bapat@gmail.com\nAlways keen on learning,  helping, promoting and improving technology and such path-breaking features...\n. Awaiting the addition to slack channel! @derekchiang \n. In response to the Steps mentioned by @derekchiang to use Python scripts in our pipeline, we couldnot get the pipeline output. Since Pipeline once run, doesn't spit errors, we are unable to detect where we are going wrong.\nscap.zip\nDocker 1 - (Original Docker on 1 of the 3 instances of the Google Cloud Cluster)\ncontains Pachctl, Kubectl (PFS and PPS)\nDocker 2 - Newly created docker image with Python installed.\nSteps followed\n1. Created 3 files\na.py,Dockerfile,json file - In the scap.zip\n2.Build the docker image \ndocker build /home/ubuntu/\nSending build context to Docker daemon 5.781 MB\nStep 1 : FROM python\n ---> 0bb352ef17dd\nStep 2 : COPY . /src\n ---> f02b65a5194f\nRemoving intermediate container ede2381bd986\nStep 3 : CMD python /src/a.py\n ---> Running in 7f172300ac82\n ---> acf2a3ec273a\nRemoving intermediate container 7f172300ac82\nSuccessfully built acf2a3ec273a\ndocker images\nREPOSITORY                                     TAG                                IMAGE ID            CREATED             VIRTUAL SIZE\n<none>                                         <none>                             acf2a3ec273a        About an hour ago   703.4 MB\n<none>                                         <none>                             ebac1f32b4b5        About an hour ago   703.4 MB\n<none>                                         <none>                             1c99de1cd65c        About an hour ago   703.4 MB\n<none>                                         <none>                             b28979c334c1        2 hours ago         697.7 MB\n<none>                                         <none>                             ac38c1df5a88        2 hours ago         697.7 MB\npachyderm/job-shim                             latest                             fde3d80fa996        4 days ago          216.8 MB\npachyderm/pachd                                v1.0.1-1433                        699b1ac38bed        5 days ago          259.1 MB\npython                                         latest                             0bb352ef17dd        10 days ago         697.7 MB\nhello-world                                    latest                             6e162c693ab3        12 days ago         967 B\nrethinkdb                                      2.3.3                              b9c9ab45748c        2 weeks ago         183.8 MB\ngcr.io/google_containers/kube-proxy            c126c6dbe73c9e7db8b835f2dd6b8f8e   9f8e97123a2c        6 weeks ago         165.9 MB\nasia.gcr.io/google_containers/heapster         v1.0.2                             8b67b80f6263        11 weeks ago        96.22 MB\nasia.gcr.io/google_containers/addon-resizer    1.0                                186c40870d40        3 months ago        36.76 MB\nasia.gcr.io/google_containers/fluentd-gcp      1.18                               ed70ff4ab587        3 months ago        411.5 MB\nasia.gcr.io/google_containers/glbc             0.6.0                              6266f279a901        3 months ago        229.8 MB\nasia.gcr.io/google_containers/defaultbackend   1.0                                7f244066fca1        7 months ago        7.514 MB\ngcr.io/google_containers/pause                 2.0                                8950680a606c        8 months ago        350.2 kB\ngcr.io/google_containers/etcd                  2.0.12                             fafe47352699        12 months ago       15.27 MB\ntraining/webapp                                latest                             54bb4e8718e8        13 months ago       348.8 MB\ngcr.io/google_containers/pause                 0.8.0                              3e004fdaffa9        14 months ago       241.7 kB\nHowever while I check the running Dockers\ndocker ps\nCONTAINER ID        IMAGE                                                                  COMMAND                  CREATED             STATUS              PORTS         \n      NAMES\ne7243a2028c1        pachyderm/pachd:v1.0.1-1433                                            \"/pachd\"                 2 days ago          Up 2 days                         \n      k8s_pachd.691afb65_pachd-j8hnj_default_f23aa490-351a-11e6-acaa-42010af000de_56aeb54c\n3ef1693ffc0b        rethinkdb:2.3.3                                                        \"rethinkdb -d /var/re\"   2 days ago          Up 2 days                         \n      k8s_rethink.99fbdf27_rethink-k78er_default_f19eb059-351a-11e6-acaa-42010af000de_a07dada0\n4a4253344507        gcr.io/google_containers/pause:2.0                                     \"/pause\"                 2 days ago          Up 2 days                         \n      k8s_POD.a1892be5_pachd-j8hnj_default_f23aa490-351a-11e6-acaa-42010af000de_2b710e0a\n01da4819254d        gcr.io/google_containers/pause:2.0                                     \"/pause\"                 2 days ago          Up 2 days                         \n      k8s_POD.1bc25335_rethink-k78er_default_f19eb059-351a-11e6-acaa-42010af000de_a420e1cd\n77ba29bf5d76        gcr.io/google_containers/etcd:2.0.12                                   \"/usr/local/bin/etcd \"   2 days ago          Up 2 days                         \n      k8s_etcd.95adc135_etcd-2yqpo_default_f139ff78-351a-11e6-acaa-42010af000de_666542ad\n29aef23e9aaa        gcr.io/google_containers/pause:2.0                                     \"/pause\"                 2 days ago          Up 2 days                         \n      k8s_POD.54a62b2f_etcd-2yqpo_default_f139ff78-351a-11e6-acaa-42010af000de_6a529754\n013f0ce51b5b        asia.gcr.io/google_containers/glbc:0.6.0                               \"/glbc --default-back\"   2 days ago          Up 2 days                         \n      k8s_l7-lb-controller.de7870d_l7-lb-controller-v0.6.0-v3ntz_kube-system_02bb215a-3511-11e6-bf49-42010af000de_044bf9c0\n518c7b7bb150        asia.gcr.io/google_containers/defaultbackend:1.0                       \"/server\"                2 days ago          Up 2 days                         \n      k8s_default-http-backend.e201922e_l7-lb-controller-v0.6.0-v3ntz_kube-system_02bb215a-3511-11e6-bf49-42010af000de_0025c8e2\n28a20157950c        asia.gcr.io/google_containers/fluentd-gcp:1.18                         \"/bin/sh -c '/usr/sbi\"   2 days ago          Up 2 days                         \n      k8s_fluentd-cloud-logging.4c06df2f_fluentd-cloud-logging-gke-pachydermoyeok-default-pool-d7556851-3pm1_kube-system_e5290b52c3ae62325539949b6ee3dfdd_253a0a93\nb1fb63a92c43        gcr.io/google_containers/pause:2.0                                     \"/pause\"                 2 days ago          Up 2 days                         \n      k8s_POD.364e00d5_l7-lb-controller-v0.6.0-v3ntz_kube-system_02bb215a-3511-11e6-bf49-42010af000de_17fd78cb\n7ea8e665076e        gcr.io/google_containers/kube-proxy:c126c6dbe73c9e7db8b835f2dd6b8f8e   \"/bin/sh -c 'kube-pro\"   2 days ago          Up 2 days                         \n      k8s_kube-proxy.9068fc52_kube-proxy-gke-pachydermoyeok-default-pool-d7556851-3pm1_kube-system_06e8e2794ab6a9783c1c0852adfdbbb3_0081d349\n268f5f134797        gcr.io/google_containers/pause:2.0                                     \"/pause\"                 2 days ago          Up 2 days                         \n      k8s_POD.6059dfa2_kube-proxy-gke-pachydermoyeok-default-pool-d7556851-3pm1_kube-system_06e8e2794ab6a9783c1c0852adfdbbb3_be1da7b5\nb38cbfea5607        gcr.io/google_containers/pause:2.0                                     \"/pause\"                 2 days ago          Up 2 days                         \n      k8s_POD.6059dfa2_fluentd-cloud-logging-gke-pachydermoyeok-default-pool-d7556851-3pm1_kube-system_e5290b52c3ae62325539949b6ee3dfdd_5e576ca4\nBasically, Docker image of Python was spinned up and then we curl it into instance, so now the files reside on instance. When we try to create pipeline using\npachctl create-pipeline \n(from local host)\nPipeline isn't run.\nCould you guide us where we are going wrong.\n. kubectl logs pachd-smta1\n**No output**\n. Rename - Detecting the jobs that no longer exist.\nBtw, do you remove the job using Kubernetes via below command?\nkubectl delete <job-id>\n. Local deployment -\nWe install Docker and Kubernetes. Git Clone the Pachyderm repo.\nBy docker pulling the image, we can set up Local cluster of Pachyderm.\nInstall Kubectl, Pachctl (allows to talk with the Cluster - Local / Server based on the ADDRESS environment variable set to the corresponding I.P. address)\nServer side deployment - \nUsing Kubernetes, we can spin up a Cluster of Google Cloud Virtual machine / instances. Docker is pre-installed with them.\nPachyderm File and Pipeline System (PPS,PFS) sits on top of the Google Cluster so as to allow access to Pachyderm from entire cluster.\nQuestions\n1. How to access PFS from any instance (when we SSH into that instance - pfs cant be mount, pachctl doesnt work in order to mount pfs)\n2. We created a new Docker image (with a Python script we want to input to our pipeline). When we setup a container from this Docker image, it can't access the File system of the Instance on which it is running. Do we need to mount the PFS again into the Docker container, to be able to access the repos and files within the PFS? \n@jdoliner @JoeyZwicker @derekchiang @sambooo \n. Having gone through this detailed analysis of possible performance issue, I, as a user of Pachyderm, understand that Question is being raised not about FUSE file system per se\nBut the implementation of FUSE with the Unconventional I/O mechanism in Pachyderm File System\nOf the 2 suggested plans, from Network security and feasibility POV, I believe Plan B shouldn't be the encouraged due to Remote R/W. Instead the 3 tier structure seems more realistic. \n@ShengjieLuo Even I agree there exists a need to better understand Pachyderm implementation from the creators/contributors/users\n. ",
    "bedekarsarvesh": "@jdoliner tried apt-get install build-essential . But now some other error has popped up . $GOPATH/Bin must be on your binary . I am tagging both build-essential output as well as make launch . \nsarvesh@sarvesh-HP-Pavilion-g6-Notebook-PC ~/work/src/github.com/pachyderm/pachyderm $ sudo apt-get install build-essential\n[sudo] password for sarvesh: \nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following extra packages will be installed:\n  cpp-4.8 dpkg-dev g++ g++-4.8 gcc-4.8 gcc-4.8-base gcc-4.8-base:i386 libasan0\n  libatomic1 libc-dev-bin libc6-dev libdpkg-perl libgcc-4.8-dev libgfortran3\n  libgomp1 libitm1 libquadmath0 libstdc++-4.8-dev libstdc++6 libstdc++6:i386\n  libtsan0\nSuggested packages:\n  gcc-4.8-locales debian-keyring g++-multilib g++-4.8-multilib gcc-4.8-doc\n  libstdc++6-4.8-dbg gcc-4.8-multilib libgcc1-dbg libgomp1-dbg libitm1-dbg\n  libatomic1-dbg libasan0-dbg libtsan0-dbg libquadmath0-dbg glibc-doc\n  libstdc++-4.8-doc\nRecommended packages:\n  libalgorithm-merge-perl\nThe following NEW packages will be installed:\n  build-essential dpkg-dev g++ g++-4.8 libc-dev-bin libc6-dev\n  libstdc++-4.8-dev\nThe following packages will be upgraded:\n  cpp-4.8 gcc-4.8 gcc-4.8-base gcc-4.8-base:i386 libasan0 libatomic1\n  libdpkg-perl libgcc-4.8-dev libgfortran3 libgomp1 libitm1 libquadmath0\n  libstdc++6 libstdc++6:i386 libtsan0\n15 upgraded, 7 newly installed, 0 to remove and 284 not upgraded.\nNeed to get 34.6 MB of archives.\nAfter this operation, 54.6 MB of additional disk space will be used.\nDo you want to continue? [Y/n] y\nGet:1 http://archive.ubuntu.com/ubuntu/ trusty-updates/main gcc-4.8-base amd64 4.8.4-2ubuntu1~14.04.3 [16.2 kB]\nGet:2 http://archive.ubuntu.com/ubuntu/ trusty-updates/main gcc-4.8-base i386 4.8.4-2ubuntu1~14.04.3 [16.2 kB]\nGet:3 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libitm1 amd64 4.8.4-2ubuntu1~14.04.3 [28.5 kB]\nGet:4 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libgomp1 amd64 4.8.4-2ubuntu1~14.04.3 [23.1 kB]\nGet:5 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libgfortran3 amd64 4.8.4-2ubuntu1~14.04.3 [248 kB]\nGet:6 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libasan0 amd64 4.8.4-2ubuntu1~14.04.3 [63.1 kB]\nGet:7 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libatomic1 amd64 4.8.4-2ubuntu1~14.04.3 [8,636 B]\nGet:8 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libtsan0 amd64 4.8.4-2ubuntu1~14.04.3 [94.9 kB]\nGet:9 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libquadmath0 amd64 4.8.4-2ubuntu1~14.04.3 [126 kB]\nGet:10 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libgcc-4.8-dev amd64 4.8.4-2ubuntu1~14.04.3 [1,688 kB]\nGet:11 http://archive.ubuntu.com/ubuntu/ trusty-updates/main cpp-4.8 amd64 4.8.4-2ubuntu1~14.04.3 [4,595 kB]\nGet:12 http://archive.ubuntu.com/ubuntu/ trusty-updates/main gcc-4.8 amd64 4.8.4-2ubuntu1~14.04.3 [5,047 kB]\nGet:13 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libstdc++6 i386 4.8.4-2ubuntu1~14.04.3 [269 kB]\nGet:14 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libstdc++6 amd64 4.8.4-2ubuntu1~14.04.3 [259 kB]\nGet:15 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libc-dev-bin amd64 2.19-0ubuntu6.9 [69.0 kB]\nGet:16 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libc6-dev amd64 2.19-0ubuntu6.9 [1,910 kB]\nGet:17 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libstdc++-4.8-dev amd64 4.8.4-2ubuntu1~14.04.3 [1,053 kB]\nGet:18 http://archive.ubuntu.com/ubuntu/ trusty-updates/main g++-4.8 amd64 4.8.4-2ubuntu1~14.04.3 [18.1 MB]\nGet:19 http://archive.ubuntu.com/ubuntu/ trusty/main g++ amd64 4:4.8.2-1ubuntu6 [1,490 B]\nGet:20 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libdpkg-perl all 1.17.5ubuntu5.6 [179 kB]\nGet:21 http://archive.ubuntu.com/ubuntu/ trusty-updates/main dpkg-dev all 1.17.5ubuntu5.6 [726 kB]\nGet:22 http://archive.ubuntu.com/ubuntu/ trusty/main build-essential amd64 11.6ubuntu6 [4,838 B]\nFetched 34.6 MB in 1min 22s (420 kB/s)                                         \n(Reading database ... 171319 files and directories currently installed.)\nPreparing to unpack .../gcc-4.8-base_4.8.4-2ubuntu1~14.04.3_i386.deb ...\nDe-configuring gcc-4.8-base:amd64 (4.8.4-2ubuntu1~14.04) ...\nUnpacking gcc-4.8-base:i386 (4.8.4-2ubuntu1~14.04.3) over (4.8.4-2ubuntu1~14.04) ...\nPreparing to unpack .../gcc-4.8-base_4.8.4-2ubuntu1~14.04.3_amd64.deb ...\nUnpacking gcc-4.8-base:amd64 (4.8.4-2ubuntu1~14.04.3) over (4.8.4-2ubuntu1~14.04) ...\nSetting up gcc-4.8-base:amd64 (4.8.4-2ubuntu1~14.04.3) ...\nSetting up gcc-4.8-base:i386 (4.8.4-2ubuntu1~14.04.3) ...\n(Reading database ... 171319 files and directories currently installed.)\nPreparing to unpack .../libitm1_4.8.4-2ubuntu1~14.04.3_amd64.deb ...\nUnpacking libitm1:amd64 (4.8.4-2ubuntu1~14.04.3) over (4.8.4-2ubuntu1~14.04) ...\nPreparing to unpack .../libgomp1_4.8.4-2ubuntu1~14.04.3_amd64.deb ...\nUnpacking libgomp1:amd64 (4.8.4-2ubuntu1~14.04.3) over (4.8.4-2ubuntu1~14.04) ...\nPreparing to unpack .../libgfortran3_4.8.4-2ubuntu1~14.04.3_amd64.deb ...\nUnpacking libgfortran3:amd64 (4.8.4-2ubuntu1~14.04.3) over (4.8.4-2ubuntu1~14.04) ...\nPreparing to unpack .../libasan0_4.8.4-2ubuntu1~14.04.3_amd64.deb ...\nUnpacking libasan0:amd64 (4.8.4-2ubuntu1~14.04.3) over (4.8.4-2ubuntu1~14.04) ...\nPreparing to unpack .../libatomic1_4.8.4-2ubuntu1~14.04.3_amd64.deb ...\nUnpacking libatomic1:amd64 (4.8.4-2ubuntu1~14.04.3) over (4.8.4-2ubuntu1~14.04) ...\nPreparing to unpack .../libtsan0_4.8.4-2ubuntu1~14.04.3_amd64.deb ...\nUnpacking libtsan0:amd64 (4.8.4-2ubuntu1~14.04.3) over (4.8.4-2ubuntu1~14.04) ...\nPreparing to unpack .../libquadmath0_4.8.4-2ubuntu1~14.04.3_amd64.deb ...\nUnpacking libquadmath0:amd64 (4.8.4-2ubuntu1~14.04.3) over (4.8.4-2ubuntu1~14.04) ...\nPreparing to unpack .../libgcc-4.8-dev_4.8.4-2ubuntu1~14.04.3_amd64.deb ...\nUnpacking libgcc-4.8-dev:amd64 (4.8.4-2ubuntu1~14.04.3) over (4.8.4-2ubuntu1~14.04) ...\nPreparing to unpack .../cpp-4.8_4.8.4-2ubuntu1~14.04.3_amd64.deb ...\nUnpacking cpp-4.8 (4.8.4-2ubuntu1~14.04.3) over (4.8.4-2ubuntu1~14.04) ...\nPreparing to unpack .../gcc-4.8_4.8.4-2ubuntu1~14.04.3_amd64.deb ...\nUnpacking gcc-4.8 (4.8.4-2ubuntu1~14.04.3) over (4.8.4-2ubuntu1~14.04) ...\nPreparing to unpack .../libstdc++6_4.8.4-2ubuntu1~14.04.3_amd64.deb ...\nDe-configuring libstdc++6:i386 (4.8.4-2ubuntu1~14.04) ...\nUnpacking libstdc++6:amd64 (4.8.4-2ubuntu1~14.04.3) over (4.8.4-2ubuntu1~14.04) ...\nPreparing to unpack .../libstdc++6_4.8.4-2ubuntu1~14.04.3_i386.deb ...\nUnpacking libstdc++6:i386 (4.8.4-2ubuntu1~14.04.3) over (4.8.4-2ubuntu1~14.04) ...\nSelecting previously unselected package libc-dev-bin.\nPreparing to unpack .../libc-dev-bin_2.19-0ubuntu6.9_amd64.deb ...\nUnpacking libc-dev-bin (2.19-0ubuntu6.9) ...\nSelecting previously unselected package libc6-dev:amd64.\nPreparing to unpack .../libc6-dev_2.19-0ubuntu6.9_amd64.deb ...\nUnpacking libc6-dev:amd64 (2.19-0ubuntu6.9) ...\nSelecting previously unselected package libstdc++-4.8-dev:amd64.\nPreparing to unpack .../libstdc++-4.8-dev_4.8.4-2ubuntu1~14.04.3_amd64.deb ...\nUnpacking libstdc++-4.8-dev:amd64 (4.8.4-2ubuntu1~14.04.3) ...\nSelecting previously unselected package g++-4.8.\nPreparing to unpack .../g++-4.8_4.8.4-2ubuntu1~14.04.3_amd64.deb ...\nUnpacking g++-4.8 (4.8.4-2ubuntu1~14.04.3) ...\nSelecting previously unselected package g++.\nPreparing to unpack .../g++_4%3a4.8.2-1ubuntu6_amd64.deb ...\nUnpacking g++ (4:4.8.2-1ubuntu6) ...\nPreparing to unpack .../libdpkg-perl_1.17.5ubuntu5.6_all.deb ...\nUnpacking libdpkg-perl (1.17.5ubuntu5.6) over (1.17.5ubuntu5.4) ...\nSelecting previously unselected package dpkg-dev.\nPreparing to unpack .../dpkg-dev_1.17.5ubuntu5.6_all.deb ...\nUnpacking dpkg-dev (1.17.5ubuntu5.6) ...\nSelecting previously unselected package build-essential.\nPreparing to unpack .../build-essential_11.6ubuntu6_amd64.deb ...\nUnpacking build-essential (11.6ubuntu6) ...\nProcessing triggers for man-db (2.6.7.1-1ubuntu1) ...\nSetting up libitm1:amd64 (4.8.4-2ubuntu1~14.04.3) ...\nSetting up libgomp1:amd64 (4.8.4-2ubuntu1~14.04.3) ...\nSetting up libquadmath0:amd64 (4.8.4-2ubuntu1~14.04.3) ...\nSetting up libgfortran3:amd64 (4.8.4-2ubuntu1~14.04.3) ...\nSetting up libasan0:amd64 (4.8.4-2ubuntu1~14.04.3) ...\nSetting up libatomic1:amd64 (4.8.4-2ubuntu1~14.04.3) ...\nSetting up libtsan0:amd64 (4.8.4-2ubuntu1~14.04.3) ...\nSetting up libgcc-4.8-dev:amd64 (4.8.4-2ubuntu1~14.04.3) ...\nSetting up cpp-4.8 (4.8.4-2ubuntu1~14.04.3) ...\nSetting up gcc-4.8 (4.8.4-2ubuntu1~14.04.3) ...\nSetting up libstdc++6:i386 (4.8.4-2ubuntu1~14.04.3) ...\nSetting up libstdc++6:amd64 (4.8.4-2ubuntu1~14.04.3) ...\nSetting up libc-dev-bin (2.19-0ubuntu6.9) ...\nSetting up libc6-dev:amd64 (2.19-0ubuntu6.9) ...\nSetting up libstdc++-4.8-dev:amd64 (4.8.4-2ubuntu1~14.04.3) ...\nSetting up g++-4.8 (4.8.4-2ubuntu1~14.04.3) ...\nSetting up g++ (4:4.8.2-1ubuntu6) ...\nupdate-alternatives: using /usr/bin/g++ to provide /usr/bin/c++ (c++) in auto mode\nSetting up libdpkg-perl (1.17.5ubuntu5.6) ...\nSetting up dpkg-dev (1.17.5ubuntu5.6) ...\nSetting up build-essential (11.6ubuntu6) ...\nProcessing triggers for libc-bin (2.19-0ubuntu6.6) ...\n```\nsarvesh@sarvesh-HP-Pavilion-g6-Notebook-PC ~/work/src/github.com/pachyderm/pachyderm $ make launch\nGOPATH/bin must be on your PATH to access these binaries:\nGO15VENDOREXPERIMENT=1 go install ./src/server/cmd/pachctl ./src/server/cmd/pach-deploy\n/bin/sh: 1: go: not found\nmake: *** [install] Error 127\n```\nAs well as for reference here is my GOPATH path and my directory structure\n```\nsarvesh@sarvesh-HP-Pavilion-g6-Notebook-PC ~/work/src/github.com/pachyderm/pachyderm $ echo $GOPATH\n/home/sarvesh/work\n```\nwork folder does contain bin directory .\n. @derekchiang We had to restart a new cluster all together. Could you please elaborate on how to use python scripts in pipeline? Like we set cmd to \"python\" .. how to write python script in the stdin value . Any help appreciated ..\n. ",
    "SoheilSalehian": "Thanks a lot for the prompt response @jdoliner. That fixed it. And I have the simple toy tutorial running now.\n. looks like this issue was recently reported with #559, and a fix #574 is on its way! thanks guys.\n. Hey @derekchiang, thanks for the awesome quick response. It looks like I can just ignore these in my other terminal, the mounted directory thing was another issue. Does umount ~/pfs safe or do I have to run a specific pachctl command?\n. @jdoliner thanks, regarding the input images, this PR doesn't include the images.zip file for the same bloating reason you mention. I was thinking of adding a script that wgets the file from google drive or something like that. Feel free to let me know if there is a more solid way to do this so we don't bloat the repo itself. \n(I am leaving the responsibility to the reader to download the images and upload them to their choice of S3 bucket prior to starting to play with the main part of the example...not sure how clear that was when you read it through but please let me know if you find it a little confusing.)\n. @jdoliner you are right, I just realized that the C++ binary (dominantColor) is 11M. This is the binary that gets copied into the container via the Dockerfile and it's the algorithm that gets run. I added a make file to download the binary from the algorithm repo in the case that someone is building on their own and from the Dockerfile.\n. ",
    "anchal-agrawal": "I'll submit a PR after fixing the protobufs link.\n. ",
    "KshiteejK": "I am having the same problem, has there been any progress regarding this issue?. ",
    "aberoham": "CLA sent! \n. ",
    "Zanadar": "I slacked this, but I thought I would leave it as a comment on this issue to leave a paper trail.\n\nI drummed up a homebrew formula for pachctl to submit to homebrew-core to get the ball rolling.\nHere\u2019s the commit for said formula: \nhttps://github.com/Zanadar/homebrew-core/commit/d3f88048abafca534222fc6449a645f7371684a3\nLooking through the build scripts, it looks like there would be some necessary changes to the makefiles in order to support this, although it looks as if you could start using brew bump-formula-pr which could simplify sed usage a lot (eg https://github.com/rbenv/ruby-build/pull/996/files ).\nI haven\u2019t done anything on the apt-get side of things.  And I\u2019m not sure what the procedure would be on your end for submitting this as a PR to brew, although I could submit it initially as a PR in https://github.com/pachyderm/homebrew-tap.\nAnyway thoughts and feedback much appreciated!\n. ",
    "msteffen": "Strictly speaking, setup.md doesn't exist anymore, but the getting started page in our readthedocs guide is written using minikube\n. Minor updates:\n- PetSets don't let you use local disks, unfortunately, so we're still stuck with network volumes for the time being\n- PetSets work by creating N \"pets\", where each pet is a container + a persistent volume claim\n- This means that to create an N-node rethink cluster, users will have to create N persistent volumes (which means creating N EBS volumes/GCE persistent disks). Ideally we'll make that easy to do with pachctl\n. \n. CI is currently failing, but I believe this change is safe to merge anyway, as all of these changes are docs changes (.md and .rst) except for assets_test (in which only two strings are changed). I believe these failures are also already well-understood and are being fixed in https://github.com/pachyderm/pachyderm/pull/860\nBecause of this, we are planning to submit this change. If submission causes problems, I believe they will be easy to fix.\n\n. Sorry, I totally missed this, but LGTM (with one comment cmds.go)\nLooks like the test failure was just deadline_exceeded (so probably a flake), so I just restarted them\n. I'm not 100% sure this is exhaustive, but I think it gets most cases. If I see more I'll fix them.\n. JD and I agreed this looked good (modulo moving ParallelismSpec into its own message), so I'm going to close this PR and get our existing tests to pass with the new API.\n. Dank, this meme is\n. Welcome to the Dank side\n. \n. I'm closing this PR, since this feature was merged in https://github.com/pachyderm/pachyderm/pull/832\n. \n. \n. \n. Give a robot a fish, and you'll have a broken robot. Teach a robot to fish, and you'll have unstoppable global fish domination\n. This was disabled in the PFS refactor\n. \n. For what it's worth, I think in a totally perfect world the \"Don't list archived / cancelled branches\" change would be a separate branch, so that it could be reviewed separately. But it's a very small change.\n. What happens if two clients are committing concurrently? Can you start a commit while another client has a commit running? If they write lines after you start your commit, do those lines show up in your commit? Or is there some kind of dual-HEAD situation?\n. Updating after short discussion with @sjezewski:\n- To illustrate the kind of use-case I have in mind, consider several servers that are all logging and are all publishing their logs to pachyderm. Each server opens a commit and starts calling PutFile with its log files. Then one server fails and its commit is cancelled. What happens to the logs-processing pipeline that's watching these commits?\n- Re. my question above: Calling PutFile without a commit ID is sugar (AFAICT). StartCommit does return a commit ID, and you can call PutFile(\"foo\", \"master\", <commit-id>, \"file\") (or something equivalent) to write contents of file to the commit with the id commit-id. Only if you call PutFile with no commit ID does PFS put the contents of file into the most recent commit (i.e. what I've been calling HEAD)\n- In that vein, there's no actual pointer to HEAD. It's just the case that commits to a repo are given a monotonically increasing identifier, and calling PutFile with no commit ID (like I said) uses the highest-ID commit that's been created.\n- Therefore this change amounts to: when we create a commit with index n, and then it's cancelled, we need to rename commit n+1 to n, so that the cancelled commit is effectively replaced by its child in the commit history.\n- In fact, one stumbling block that Sean thought of is that when a commit n is cancelled, it might have several children, all of which will need to be renamed. In the context of the the logs use-case, suppose you have 10 servers, and they start commits at approximately the same time, and your repo looks like this:\nm/1 \u2192 m/2 \u2192 ... \u2192 m/10\nThen, the first server (m/1) fails. We'll need to rename all of m/2, ... , m/10 to be m/1, ..., m/9\n. such meme. wow.\n. @sjezewski  Whenever you get a chance\n. @jdoliner Whenever you get a chance\n. (LGTM)\n. \n. To add some context, Sean, Joey and I had another conversation about branching yesterday. Some of the conclusions, as I recall them, were:\n- Long-lived feature branches can cause work to pile up behind them, which is bad. We kind of ran into this with the PFS refactor[1] and with Joey's DocsSidebar branch[2]. A key point here is that big changes and refactoring are totally fine, as long as they're submitted quickly so that other engineers can start doing work on top of the new code.\n- One of the reasons we've been letting these feature branches sit for so long is that we're afraid to commit to master because a lot of users are using it, so we don't want e.g. docs to get out of sync, or features to be partially implemented. Usually users are using master because they can't use 1.1 for some reason. A solution to this that we discussed is to do more frequent point releases (e.g. every week or two). Then users will mostly use the most recent point release, and we can commit inconsistent or partially implemented stuff to master more freely, and don't have a reason to keep long-lived branches anymore.\n- In particular, the reason the DocsSidebar branch existed was so that users using master would have consistent docs and code. If we want to make sure users using point releases still have that, we need to have versioned code in ReadTheDocs, thus this issue\n[1] I was running into testing issues before it was merged, and was also getting nervous about merging the PFS refactor into my feature branch, to the point that I was considering working off of the refactor branch just so I wouldn't get squashed. I did eventually throw away the feature branch I was working on and created a new one after the PFS refactor merged, and my feature branch wasn't very big so it didn't take too long to recreate, but with more people or different circumstances, that kind of thing could get expensive\n[2] Similarly, while Joey had his DocsSidebar branch out, all of the docs changes that we were making were basically being sent to Joey over slack and manually committed by him into the branch. After we sat down and merged the DocsSidebar branch, we were all able to update docs independently again.\n. \n. @jdoliner Whenever you get a chance\n. Getting stuck on the tensorflow example test, I think. Trying to figure out what's wrong...\n. All fixed (I was returning EOF from the command instead of swallowing it)\n. SGTM. I mostly agree with JD's response, that ideally we shouldn't have to check in with each other before running benchmarks, and that keeping this output in git makes sense.\nOne thing that might be good (depending on how long the benchmark takes) would be to run the benchmarks several times and record a distribution of runtime. Benchmarks can often be very uneven, and knowing that e.g. our average benchmark time is X but 5% of the time is 10X (because e.g. fuse really cares about the order that certain events happen) seems potentially valuable me.\nWe should maybe also be aware of the cost of running these benchmarks? I don't know how expensive GKE CPUs are, but maybe that'll add up?\n. @sjezewski whenever you get a chance (or let me know if I should reassign)\n. Not sure if these will turn out to be useful, but we had a system at Google that was broadly used for similar kinds of tracing called Dapper. There are a few open-source systems that look like they're trying to do the same thing:\nhttp://opentracing.io/\nhttp://zipkin.io/  (just based on the screenshots, this looks very similar to dapper)\nhttp://htrace.incubator.apache.org/\nhttp://www.x-trace.net/wiki/doku.php\n. Initial solution to this implemented in https://github.com/pachyderm/pachyderm/pull/3541\nStill to do:\n- Trace HTTP calls (e.g. put-file <url>).\n  - Right now the time spent waiting for an HTTP response shows up as a huge unexplained stretch in the span\n- Tag RPCs\n  - Right now we have generic spans reported by a gRPC interceptor, but if we implemented tracing versions of each of our clients, then we could tag spans (e.g. GetFile and PutFile could include the name of the file being gotten/put).\n  - We've already done this in a limited way with src/server/pkg/obj/tracing.go. LGTM. Just to recap our verbal discussion, I think we tentatively concluded that segment/mixpanel would be a good place to keep sending this data. We also talked about metrics (cheap, lower-availability) vs monitoring (expensive, super duper high availability) and how we may eventually want to use different metrics and/or a different backend for monitoring.\n. I guess I do think we might want to be a little careful about relying on mixpanel for debugging, for two reasons:\n1) Primarily, if we lean on it too much, we may be incentivized to keep putting more and more granular data in there to understand a bad cluster's state, which could cause us to hit a scalability or cost wall at some point, and it might also be bad for our users' privacy. I think the scalability constraint in particular may mean that inevitably, some day, we won't be able to debug a broken cluster unless the customer gives us access to their monitoring in some way, and therefore I'm not sure that the benefit of circumventing that will be large or long-lived.\n2) Presuming Mixpanel's reliability is actually worse than Prometheus, we may end up in a state where our ability to debug a broken cluster depends on whether Mixpanel was available and actually collected all of the information we need when the cluster broke. If, instead, we come up with some more robust debugging infrastructure from the start, we may end up in a better place and more consistently able to debug stuff\nObviously we should start with what's easy, but these were just some thoughts.\n. @sjezewski whenever you get a chance\n. @ukd1 thanks for filing this. I've started looking into it, and just to get a little bit more context, are you running Kubernetes on your own servers, or are you using GCP or AWS (and running the pachctl tool inside a VM)? If you're running it on your own servers, do you know offhand what you're using for block storage?\nOr, just let me know if your setup is something else. Thank you!\n. @udk1 this is not expected, and I've reproduced the issue locally. We're still trying to identify the root cause (our current hypothesis is that it may something to do with the details of how that codepath uses RethinkDB, since we rewrote it for 1.2) but I'll keep updating this issue with progress as we learn more. Thanks for your patience!\n. @ukd1 thanks again for being patient with us! One possible workaround that you could try is to see if you can get the file onto the same host that's running pachd (by some means other than put-file, e.g. rsync) and then running pachctl put-file on that host with the now-local file. I haven't tried this on GCP yet, but once I do I'll post a set of commands that will do it. Obviously this isn't an ideal way to be running put-file, but our main goal is to understand the bug and get it fixed.\nJust to provide some transparency into what's going on: our initial hypothesis was that pachyderm was breaking up the file too much when sending it to pachd, and that pachd was being overwhelmed by the overhead cost of processing each small chunk. After reproducing the result though, we scanned the pachyderm logs and didn't see that behavior.\nOur next hypothesis was that pfs was encountering some bottleneck when storing this data to block storage. We did just rewrite PFS, so this might have explained why we didn't encounter this sooner. However, in one experiment, I created a kubernetes cluster out of a single machine and ran put-file on the same machine, and this actually ran much faster (100MB/s instead of 1MB/s). This is why I'd expect running put-file on the same machine as pachd to work.\nMy current hypothesis is that the bottleneck is related to the network connection between pachyderm and pachd, but I haven't been able to make more progress than that. I will try to keep updating this bug, though, and I'll let you know as soon as we've solved to problem, in case you want to try again.\nThanks again for reporting this, and for being patient while we work through it!\n. @ukd1 Hi Russell, sorry for the slow update, but it turns out we were looking in completely the wrong place. It looks like at least one potential cause for this slowness in the proxy created by pachctl port-forward. \nIs that how you're connecting to your pachyderm cluster? If so, you may be able to fix the slowness like so:\n``` sh\n\nNODE_IP=$(kc get nodes -o='jsonpath={$.items[0].status.addresses[?(@.type==\"LegacyHostIP\")].address}')\nexport ADDRESS=${NODE_IP}:30650\npachctl version  # should give a version for pachd if you've connected successfully\n```\n\nThis works by connecting to the first node in your kubernetes cluster listed by kc get nodes and then connecting to the pachd service on port 30650. In a quick test, I got much higher throughput with put-file after connecting to the node directly.\nIf you get a chance, let me know if this seems to work for you! Our current plan is to get rid of pachctl port-forward in future versions, bypassing this issue.\n. I'm closing this for now, just because I have no other remedies that I plan to try. However, @ukd1, if the ADDRESS fix above doesn't work for you, please re-open the issue so I can keep hunting for the problem\n. \n. LGTM. For some reason, I sometimes run out of file descriptors when I run the tests in minikube. But when I finally gave up on that, this runs fine when I run it in dockerized kubernetes\n. Oh, whoops, that makes sense. Thanks! LGTM\n. @jdoliner whenever you get a chance\n. To provide a little context, the -f flag was removed in 1.12: https://docs.docker.com/engine/deprecated/#/f-flag-on-docker-tag\n. @derekchiang whenever you get a chance\n. Meme lgtm\n. related: https://github.com/pachyderm/pachyderm/issues/847. Ideally, if a commit is cancelled, child commits also don't become invisible to ListCommit\n. Maybe down the line it would make sense to have a per-cluster config that points to the registry used for pipelines in that cluster? Then pachctl --push-image could just get that info from the cluster.\nProbably outside the scope of this PR, but just an idea\n. \n. Extra commits should be gone now...\n. It seems like my other PRs aren't passing CI because they're timing out. I think the example tests that Sean and I added may be responsible. We can work on shortening those if we think it's important, but it seems like we want to be able to get PRs through CI in the meantime\n. meme lgtm\n. Sorry about this, we're still working out some kinks in our release process. It should be available now, though!\n. This shouldn't be merged until after https://github.com/pachyderm/pachyderm/pull/962. This works if the only service that pachctl is talking to is pachd, but if pachctl may be talking to the docker registry as well, we'll have to make sure it uses the same IP\n. > So with this patch there shouldn't be any need to do port forwarding at all since pachctl will be smart and find k8s nodes to connect to. Is that accurate?\nYup, that's right\n. Just to update: This PR got a little bit stuck because #962 added a docker registry, so this PR needs to make sure that pachctl can talk to the docker registry even with port-forwarding turned off. Unfortunately, I haven't yet figured out how to talk to the docker registry remotely without TLS, so either we'll need to include a TLS certificate in our docker registry deployment, or I'll need to figure out how to override docker's default settings\n. I closed this PR on accident, but a) it's super old, and b) I'm not sure that it actually worked (it got the master node from kubectl and treated that as a regular cluster node by just sending traffic to the pachyderm NodePort port on that machine, and I'm not sure if that's legit). No worries. I'll get rid of this and submit https://github.com/pachyderm/pachyderm/pull/964 when your (@sjezewski) PR is in\n. LGTM!\n. > Oh man, I did not know that there was a Canonical option for encodings, this is awesome. This means no more flipping values in maps right?\nyup!\n. \n. LGTM\n. @jdoliner when you get a chance\n. \n. (Presuming CI passes)\n\n. @sjezewski whenever you get a chance\n. \n. We never updated the docs because I never committed the code that got rid of port-forward. At the time, that was because I couldn't easily connect to the docker registry we were running, and I didn't think I could push the change until that worked. (and, if you do use port-forward, you don't need the ingress rule, I believe)\nNow that we've gotten rid of the internal docker registry, though, I'll revisit that change. I agree that we need to mention the ingress rule in our docs if we want to get rid of port-forward. There was a PR but no issue. Just created one here: https://github.com/pachyderm/pachyderm/issues/1169. @jdoliner whenever you get a chance\n. Deleting branch because CI was broken, so I couldn't submit this, and then the merge got too complicated (maybe I should've been retrying CI more aggressively\u2014I'm not sure how so many PRs got submitted while CI was crashing, but in any case it seems to be healthier now that the time limit is extended)\n. CI keeps failing with the command travis_wait 30 make test\" exited with 137.. A little googling suggests that this may be due to timeouts, so I extended Travis's time to 60m from 30. If that fixes it, I guess we might want to see if there's any way to make our tests faster\n. LGTM\n. LGTM\n. LGTM\n. Since CI is broken without both this change (and is regularly failing without https://github.com/pachyderm/pachyderm/pull/1034), and it's a docs-only change (it breaks CI by breaking AssetsTest) I'm just going to submit it. I asked @JoeyZwicker to take a look at his convenience, and if this chance isn't what we want I'll revert and fix it\n. LGTM\nOne thought: would it make sense to keep the registry stuff (maybe in a side library) for testing? Just in the spirit that more hermetic tests are less likely to fail spuriously.\nAn alternative could also be to start a registry in our test but not from pachctl (i.e. have a docker run registry in the Makefile or something)\n. \n. LGTM\n. \n. I'm going to merge this now, because it passes tests on AWS and GKE (and locally), and that's as much testing as we've historically done. I do want to try and test this on Azure, but given that running a k8s cluster on Azure uses Kubernetes Anywhere[1], I don't think it'll be substantially different.\n(Similarly, PersistentVolumes and PersistentVolumeClaims abstract away most of the differences that might affect this change. If Azure worked with RC-controlled RethinkDB, which also used PVs and PVCs, it should work with this)\n[1] http://kubernetes.io/docs/getting-started-guides/azure/. LGTM. Sorry for the drive-by comment, but would it make sense to add a test for this? Maybe in a separate PR, if we need this in for tomorrow. LGTM, thanks for the contribution!. This (--deploy-rethink-as-rc not working with 'deploy local') is a particularly unfortunate bug in pachctl 1.3.1, especially with the PetSet change in k8s 1.5. Sorry you ran into it! It's already fixed in master, and will be fixed in 1.3.2, which should be released in a few days, if not tomorrow.. LGTM, once it passes CI. Note that the docs still need to be updated, so I'll run make doc before submitting. Only if you pass --deploy-rethink-as-stateful-set to pachctl deploy. One of the changes in this PR is that RethinkDB is managed by a ReplicationController by default now, meaning that by default, pachctl deploy will work with both pre- and post-1.5 Kubernetes. Nice! LGTM. LGTM, since this does seem like a strict improvement in the system and we need it for GF. That said: could this algorithm leave you with dangling commits?\nHere's my understanding of what was happening before:\n1) persistClient.CreateJobInfo() writes a JobInfo to RethinkDB\n2) a.kubeClient.Extensions().Jobs(a.namespace).Create(job) creates a commit and then fails\n3) the deferred func() detects that Create(job) returned an error, and writes JobState_JOB_FAILURE to RethinkDB\n4) The next job starts, sees the JobInfo in RethinkDB, checks that JobInfo's commit to see if it's finished, sees that the commit is not finished, and then waits indefinitely for it to finish\nThe difference is that now, instead of writing JobInfo to RethinkDB in (1), we wait until (2) has finished successfully. However, if (2) does not finish successfully, there will still be a dangling commit in the output repo.\n1) Will the next output commit depend on the unfinished commit? Will that cause problems inspecting the repo?\n2) Would it make sense in the medium/long run to detect if a job is in FAILURE and then delete its commit  if so? (presuming the job has an output commit, which I guess depends on how it failed). Same concerns as Derek, but I also agree that it's fine as long as it's temporary. LGTM. I do prefer Marshal and Unmarshal, but I ran into a naming conflict in https://github.com/pachyderm/pachyderm/pull/1338. I made HashTreeProto implement the (immutable) HashTree interface, which is also the interface that contains Marshal and Unmarshal. At a high level, I think that makes sense since the proto is what we want to persist. The problem is that then I'm defining Marshal and Unmarshal on HashTreeProto, which shadows the definition of Marshal and Unmarshal provided by the protobuf library. When I write:\nfunc (h *HashTreeProto) Marshal() ([]byte, error) {\n  return proto.Marshal(h)\n}\nI go into an infinite recursion (h.Marshal calls proto.Marshal calls h.Marshal and so on). Tests are currently failing because (I believe, but haven't confirmed) I'm giving pachd a 12GB memory request, and the k8s cluster we create in travis is too small to place pachd anywhere.  I think the ideal thing to do would be:\n- Make pachd cache size settable from the command line\n- Tie the pachd memory request to the size of the pachd cache (like rethink and its cache)\n- For pachctl deploy local, make all of these values small.\nAnother strategy might be:\n- Don't set resource requests at all in pachctl deploy local\nI'll try to figure out how much work this is, but it may make more sense to just set the request flags low in travis. Done\u2014pachyderm_test.go has a test now. I also added a default resource request (and test) for etcd, which will be necessary post-refactor. Per Sean and my discussion, this LGTM with Sean's changes. I think Sean is planning to maybe make a few more minor changes and submit it.. Okay, renamed --compute-backend back to --persistent-disk (and re-ran deploy --dry-run comparison to confirm that the output hasn't changed). I'll submit once this passes CI. IIUC, this should be rare, right? Basically, this just makes it such that all input repos must have at least one commit before the pipeline starts spawning jobs?. This is deliberate. You need to pass (IIRC) pachctl undeploy --all. The motivation is that when etcd disks are dynamically provisioned, they are also dynamically deleted when the PVC that has claimed them is deleted from Kubernetes. Therefore we don't delete the PVC without the --all flag to avoid users unexpectedly losing their data.. (feel free to reopen, obviously). (JD's theory, that it's the job rescheduling, seems most likely to be right to me). One way to do this (which I think would minimize the amount of redundant computation) would be to remove all hashtrees that were added by the failed job from the object store at failure time. It might be easier to simply ignore hashtrees that have already been computed when re-running a failed job\nAnother (essentially equivalent) possibility that occurs to me is that when we write HashTrees to the object store, we record the job ID. Then, in worker, when we check if a datum has been processed, we also look up the job ID for that HashTree, and if the job is FAILED we re-run.\nOne idea for dealing with disk usage problems (that might make pipelines more robust in general) is to use an emptyDir volume in the worker instead of a HostPath volume. Then, if a job fails, we restart all the pods (which will clear all of the full VM disks). I'll talk to Sean tomorrow morning to get some more details, but I'm having a hard time reproducing this (I've tried both v1.4.3 and master/HEAD). I tested this by creating a pipeline:\n(pipeline.json)\n{\n  \"pipeline\": {\n    \"name\": \"sum\"\n  },\n  \"transform\": {\n    \"image\": \"ubuntu:14.04\",\n    \"cmd\": [ \"sh\" ],\n    \"stdin\": [\n        \"ls /pfs/d | while read file; do\",\n        \"   wc -l /pfs/d/${file} >/pfs/out/${file}\",\n        \"done\"\n    ]\n  },\n  \"parallelism_spec\": {\n    \"strategy\": \"CONSTANT\",\n    \"constant\": 1\n  },\n  \"inputs\": [\n    {\n      \"name\": \"d\",\n      \"repo\": { \"name\": \"data\" },\n      \"branch\": \"master\",\n      \"glob\": \"/*\"\n    }\n  ]\n}\nand then modified transform:\n(pipeline2.json)\n...\n  \"transform\": {\n    \"image\": \"ubuntu:14.04\",\n    \"cmd\": [ \"sh\" ],\n    \"stdin\": [\n        \"echo \\\"This is new and different\\\"\",\n        \"ls /pfs/d | while read file; do\",\n        \"   wc -l /pfs/d/${file} >/pfs/out/${file}\",\n        \"done\"\n    ]\n  },\n...\nand when I ran pachctl create-pipeline -f pipeline.json and then pachctl create-pipeline pipeline2.json it reprocessed data appropriately. I printed out the transform it was using, and a hash of just the serialized transform's bytes, and they were different between pipeline 1 and pipeline 2.. I just talked to Sean, and we decided to close this bug, and re-open it if we see the behavior again.. One possibility would be to download many data to a side directory, and then copy them into /pfs/<input> one at a time (and run the user binary each time).\n(total aside, but I feel like ideally this side directory would be completely invisible. I've wondered in general whether it would be easy/beneficial to use other mounting tricks when running the user code, like making /pfs/<input> read-only). Thanks for the incredibly detailed bug! Right now, I think pachyderm will enter this loop any time a job fails for any reason. We restart jobs indefinitely, and each time we do, the job will try to reprocess all data in the input commit (including things that have already been processed successfully). Thus if CMykQv2SCTniKAJvLsM3H3SnFuk3Xlx4e1/1SaGqOXM= is processed successfully in the first run, you'll see the message above printed in a loop, until the job completes successfully.\nAs far as why the job is failing:\n- If the user's code is exiting with a non-zero (non-allowed) exit code, the job will be marked failed and retried\n- In this case, as @derekchiang pointed out, it may be unable to retrieve data it needs from the object store, and the error returned by the read may be causing the job to fail. Made both of the changes above. Let me know what you think. Hey, I hope I'm not stepping on your toes or anything, but I'm going to merge the pull request since I have no comments and it looks important. Nice catch!. Hey Derek, I opened a similar PR in https://github.com/pachyderm/pachyderm/pull/1727 that doesn't use keepalive and doesn't update vendor/grpc-go. My hope is that that will be easier to submit, so I'm going to close this PR to focus on that one, and then maybe we can come back to this if we realize keepalive is necessary (I think blocking until the connection is available is the most useful part of this change). Hi Nitish, thank you so much for your help on this issue! I had one quick question as well, if you get a chance.. At some point, I believe etc/build should be renamed to etc/release (I think having etc/build and etc/compile is confusing). Lol. For posterity: the travis docs on encrypting sensitive data are here: https://docs.travis-ci.com/user/encryption-keys/\nThe way it works is: travis generates a unique public/private key pair for each github repo (that uses travis, I guess) and then encrypts our creds with the pachyderm/pachyderm public key. When it runs our tests, it'll decrypt them with the pachyderm/pachyderm private key.\nIf a user tries to fork our repo (to say mallory/pachyderm and then modifies our test to print travis-batch-test...json and then sends us a PR to run their tests through CI, travis will try to use the private key for mallory/pachyderm instead of pachyderm/pachyderm to decrypt the creds and fail.\nThat's where the decryption key is stored, and is also how forks of the repo are unable to decrypt the data. btw, calling Walk on an empty hash tree will still cause you to traverse \"/\". That might be a good corner case to either handle or test.. \n. For posterity: It seems like there are two kubernetes clients: k8s.io/client-go and k8s.io/kubernetes/pkg/client (which we're using). I'm not clear on the difference, but I think to switch to k8s.io/client-go will require a refactor.. Nice! That's a huge improvement. we can certainly explicitly set the default resource request to 0. Two notes on that solution, though:\n- Explicitly setting GPU to 0 causes pods not to schedule (if the k8s node doesn't have any GPU. Turns out that requesting 0 of something that the node doesn't have is asking too much)\n- I saw this with kops. There's a k8s primitive called a Limit Range that can impose default resource reqeusts on pods (see https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-ram-container/ at the bottom: \"Default limits are applied according to a limit range for the default namespace\").\nGiven these, I think the default resource request will probably work (though it means that if any customers actually want to use a limit range, they can't).\n- Another thing that would probably work is using a non-default namespace.\n- A third thing that might work is kubectl deleteing the limit range that I imagine exists in new GKE clusters (this is what I did with Kops)\nFinally, I think it might be worth asking ourselves (if not now, at some point) whether resource requests of 0 are actually what we want. I imagine the reason Kops and GKE started creating these limit ranges is because pods do consume resources, and if you don't account them you probably do get bitten eventually. Better scale-down logic might be safer in the long run.. Re: the fruit stand, that's a bug (probably my fault). [[ is a bash-only enhancement of [ which I'm too in the habit of using.. (Just ammended the original issue to describe how I plan to protect paid ACL features). Here's an overview of how pachctl login is expected to work:\n--------------------+\n Pachctl or Web UI  |\n ----------------   |\n(pachctl) print url |\npointing at GitHub  |\nOAuth API           |\nor                  |\n(web UI) Display  a |\nlink                |\n--------------------+\n--------------------+\n     Browser        |                    +---------------+\n     -------        |                    |    Github     |\nuser pastes URL or  |                    |    ------     |\nfollows link        |  ----------------> | User Login    |\n                    |                    | and           |\n     ...            |  <----Redirect---- | auth approval |\n     ...            |                    +---------------+\n     ...            |                    +---------------+\n     ...            |                    | pachyderm.io  |                    +---------------+\nbrowser redirects   |                    | ------------  |                    |    Heroku     |\npachyderm.io/login- | ----authn-code---> | Forward authn |                    |    ------     |                    +---------------+\nhook                |                    | code to heroku| ----authn-code---> | Exchange authn|                    |    Github     |\n                    |                    |               |                    | code for      |                    |    ------     |\n                    |                    |               |                    | access token  | ----authn-code---> | Exchange authn|\n                    |                    |               |                    |               |                    | code for      |\nprint \"please paste | <--access-token--- | authn success | <--access-token--- | authn success | <--access-token--- | access token  |\nthis token into CLI |                    +---------------+                    +---------------+                    +---------------+\n/web UI: <access t>\"|\n--------------------+\n--------------------+\n Pachctl or Web UI  |\n ----------------   |                    +---------------+\nuser pastes token.  |                    |    PachAuth   |                    +---------------+\nSend Authorize      |                    |    (in k8s)   |                    |    Github     |\nrequest to PachAuth |                    |    --------   |                    |    ------     |\nservice             | ---access-token--> | Get user info | ---access-token--> | Get info for  |\n                    |                    |               |                    | access token's|\n                    |                    | authn success | <-github-username- | user          |\n                    |                    |     ...       |                    +---------------+\n                    |                    |     ...       |                    +---------------+\n                    |                    | generate new  |                    |     Etcd      |\n                    |                    | token (pach   |                    |     ----      |\n                    |                    | token)        | ----pach-token---> | write         |\n                    |                    |               |                    | \"GH_username  |\n                    |                    |               |                    | => pach token\"|\n                    |                    | session       | <----Success------ | with ttl=X hrs|\nstore pach token in | <----pach-token--- | created       |                    +---------------+\n.pachyderm/config   |                    +---------------+\n(pachctl) or browser|\ncookie (web UI)     |\n+-------------------+\nAlternatives Considered\n\nStarting a local server with pachd, and redirecting the user's browser there\npachctl could start a server at localhost:11111, and then when GitHub redirects the user's browser after they've authorized pachyderm, we could send them to localhost:11111.\nBecause the server is running on their machine, it could modify ~/.pachyderm/config without asking the user to copy/paste anything.\nThe problem with this approach is that it won't work with the web UI. Github needs to know in advance where it will redirect users after they authorize pachyderm, so if we pick localhost:11111 we can't decide, when a user logs in with the web UI, to send them to pachyderm.io instead\nHowever, because the web UI isn't running on the user's machine, we can't start a server at localhost:11111, so there wouldn't be any place for their browser to land\n\nFuture Enhancements\n\nRedirecting users to the pachyderm web UI:\nAs part of the payment/registration process, the user gives Pachyderm Inc. a URL at which they plan to run pachyderm (e.g. ec2-52-53-183-5.us-west-1.compute.amazonaws.com or some such)\nBefore pachctl prints the link to GitHub, it calls home to pachyderm.io\nIf the user running pachctl login is registered, we get the URL of their cluster and encrypt it (using a key that's secret to pachyderm.io)\nThen when GitHub redirects the user's browser to pachyderm.io with the access token, we include the encrypted URL as side-channel data in the redirect\npachyderm.io now decrypts the location of the cluster, and instead of returning a page that says please copy and paste this code into your terminal, it returns another redirect, this time to the user's web UI (encrypting the location of the user's web UI is necessary to prove that they won't be redirected, along with their secret access token, to a bad web UI that captures their token. We don't regard the location of the good web UI as secret)\nThe web UI captures the access token, and then sends the AuthenticateUser request to PachAuth, and the user is logged in.\nThe benefit of this is that there's no need to copy/paste code into the web UI, the user is logged in automatically.. Note: I'm updating the issue to reflect that we're no longer planning to implement ACLs as a separate service outside of Pachd. This is to make deployment simpler: we don't have to worry about forcing users to upgrade their ACL service every time they upgrade their Pachyderm cluster. If the ACL service is implemented inside pachd, then a user's Pachd implementation and ACL service implementation will always be in sync.. I just updated the design for pachctl login. We recently discovered that GitHub doesn't support the implicit OAuth grant type, which we were planning to use, so we have to use the slightly more complicated authorization code grant type.\n\nThis added an extra layer to our login flow, which is a Pachyderm-controlled Heroku app that exchanges a GitHub Authorization Code (a short lived token indicating that the user has granted Pachyderm access to their GitHub account, which I don't think can be forwarded to pachd) with an Access Token (a long-lived version of the same token which I do think can be forwarded to pachd). I just updated the top-level design to describe:\n What happens when the auth system is activated in a cluster that already has pipelines and repos (those resources have no owner)\n How resources with no owner behave. Renamed pipeline \"owner\" to pipeline \"RunAs user\". This clarifies that being the owner of a pipeline grants no special privileges, and simply exists to prevent users from using pipelines to access data they shouldn't be able to otherwise\nI also added that to update a pipeline, a user must have access to the pipeline's output repo, so that when the pipeline runs as them, it won't fail to write its output. I just updated the design to describe de-activation of the auth system (under Paid Feature)\nI also just created https://github.com/pachyderm/pachyderm/issues/2153 to describe features we hope to implement for 1.6. I just reorganized the text above, and incorporated the text describing what happens when a cluster's enterprise token expires. This design doc should be accurate for Pachyderm 1.6, which has now launched. I'm closing this issue to clean up our issue queue, but it should serve as a reference until subsequent designs supercede it (which should be noted in the comments here and at the top of the design doc). Per our conversation: hopefully etc/testing/deploy/aws.sh --delete will be upgraded to delete cluster VPCs for you (at which point this script will be unnecessary and can be deleted). Until then, this script might be helpful in making sure that the pachyderm clusters can be created. I think we may need to store the bucket name in s3 as well as locally, because if Travis tests fail then the VM will be cleaned up and the local files will be deleted (we may also want to record when the cluster was created, so that we can clean up old clusters, so that we don't accumulate clusters after failed test runs). Does this mean that dev@pachyderm.io will get an email every time CI fails on a PR? That seems like it may be a lot of email.\nAlso, I think this may just be the title of the PR, but just to clarify, there's actually already a cron job set up in Travis that runs the tests daily (that's something you set up in the Travis settings page). You can see an example of a run here: https://travis-ci.org/pachyderm/pachyderm/builds/252543448 (the \"cron\" label tells you how it was run).\nThat said, the daily test has been failing and hasn't been creating clusters, so the failure occurs at some point before that happens (the log includes Running daily benchmarks\nAWS Access Key ID [None]: AWS Secret Access Key [None]: Default region name [None]: Default output format [None]: so I think it's trying) but I think the next step here may be to investigate that. I stopped working on running our main tests in Travis when I switched to the Faraday test repo, which is why they're half-done :-P.. Per discussion with Derek: we'll see how much email this generates. If it turns out to be a lot we can always disable it.. I'm happy to wait. This was very easy. Basically just some vim macros. I'll discard this PR and recreate it later, since it sounds like the set of files that will need to be changed will be pretty different anyway. That segfault is because our configs are different. I had a cluster set in my config (meaning cfg.V1 was not null for me). I now make sure to set that field if it's null. For posterity: We've been struggling with a Travis bug (as confirmed by their support team) where our failed travis runs often have incomplete logs. According to Travis support, they internally buffer logs in memory before writing them to s3, and their system for buffering logs is encountering delays writing buffered logs to s3.\n@sjezewski pointed out that without travis_wait we could have interactive logs, which may mean they're not buffered. We're removing the travis_wait command to see if that's true, and if so, skipping Travis's internal buffering gets our logs into s3 faster so that they consistently seem to be complete.. One question, other than that LGTM. Updated backoffs in pachyderm_test and fixed the comment, let me know what you think. Gabriel and I worked together to check if this is possible, and we confirmed that it is. Fixed by https://github.com/pachyderm/pachyderm/pull/2144. Fixed by https://github.com/pachyderm/pachyderm/pull/2143. I believe the problem with TestUseMultipleWorkers was something I introduced when I added a backoff loop to pachyderm_test.go:2991. It's expecting to find the pipeline workers in a certain state in etcd, and by running that loop less frequently I reduced the likelihood that the test sees the state it's expecting. Update: another client has asked for this. One design issue I've encountered: If a user U owns a pipeline P, and U's access to P's input is revoked, what should happen?\nIt turns out that marking the next job \"failed\" is difficult. Writing a jobInfo to etcd is tough, since the jobInfo contains e.g. the parent commit, and if the pipeline's read access to the input repo has been revoked, then it doesn't have access to that info. Creating a stats commit for the failing job is difficult if U's write access to the output repo has also been revoked. \nFor now, pipelines will crashloop until their access to the input repo is restored.. Two notes:\n* For 1.6, depending on what we hear from the relevant customer, we think a \"good enough\" solution for preventing unauthorized users from accessing data directly in S3 with S3 credentials is: tell customers to only give k8s access to cluster admins.\n  * To do this, we need to make sure Pachyderm is usable without k8s access. In particular, we need to make sure that the dash is accessible without pachctl port-forward\n\nRight now, I've solved the Repo/ACL consistency problem (users may have repos with no ACLs) by changing how CreateRepo/CreatePipeline work. Now, those calls may fail if creating the ACL fails, but if the call creates a repo successfully, the new repo will have an appropriate ACL. We may leak ACLs with this solution, but the only cost associated with that is a few bytes of storage space in etcd. I'm closing this issue because 1.6 has launched. Some of the non-launch-blocking issues will be moved into their own GitHub issues. Fixed by https://github.com/pachyderm/pachyderm/pull/2252. Per our discussion:\n(a)  I'll leave out the ability to remove the auth token completely for now, because you're investigating ways to do that with etcdctl\n(b) The ability to make the enterprise token expire early already exists: just call Enterprise.Activate with ActivateRequest.expires set to some time in the past: https://github.com/pachyderm/pachyderm/blob/master/src/client/enterprise/enterprise.proto#L28. For (b): the ability to set the expiration time existed in the API but not in pachctl. It has since been added: https://github.com/pachyderm/pachyderm/pull/2252. https://github.com/pachyderm/pachyderm/pull/2253 should allow the enterprise token to be unset using etcdctl. I'm closing this issue because I don't have anything else I'm planning to do, but feel free to re-open if the current fixes aren't sufficient.. Per our discussion, both Activate and GetState will return a TokenInfo object that includes the token expiration time (as a boxed submessage). If no token has been provided, the submessage will be unset. This is fixed by https://github.com/pachyderm/pachyderm/pull/2259. @gabrielgrant . Fixes https://github.com/pachyderm/pachyderm/issues/2229. @gabrielgrant . Per our discussion, I added VERSION to .gitignore and removed the step from our guide asking the release engineer to delete it explicitly in a commit. Per slack, date -Iseconds works:\n$ pc enterprise activate $(cat activation-code.txt) --expires $(date -Iseconds)\nActivation succeeded. Your Pachyderm Enterprise token expires 2017-09-11 22:06:58 +0000 UTC\n\nAre you using date for tests? If so, does the fact that -Iseconds seems to work fix the issue?. Re-opening because I didn't read your comment carefully. Your second command, which used --iso-8601=seconds totally should've worked. Interestingly, our date tools appear to be different. When you set --iso-8601=seconds, it emits 2017-09-11T14:51:02-0700 (which doesn't have a colon in the time zone, which breaks the parser) whereas mine emits 2017-09-11T14:51:02-07:00. Both seem to be 8601-compliant.\nKind of a fumble on Go's part, IMO. I'll update pachd to accept both. This is actually an interesting feature of Go\u2014either totally brilliant or totally stupid depending on who you ask. The way Go does date format strings is by using example dates, rather than %Y %M %m etc tokens (what's the code for two-digit year again?).\nBasically, you format the date \"January 2, at 3:04 PM and 5 seconds, 2006, in GMT +7 in the format you want, and then Go will render/parse dates in that format. So the date format string \"2006-01-02T15:04:05Z07:00\" corresponds to YYYY-MM-DD\"T\"HH:MM:SS[timezone]. Because the timezone component of the string is \"Z07:00\" rather than \"Z0700\" it expects a colon, which date doesn't always seem to print.. Per our discussion: Auth.WhoAmI (https://github.com/pachyderm/pachyderm/blob/master/src/client/auth/auth.proto#L157) should work. It'll tell you:\n if auth isn't activated (\"the auth service is not activated\")\n if you're not logged in (\"auth token not found in context\")\n if your login has expired (\"token not found\")\n who you are, if auth is active and your authentication token is valid. Per our discussion: previously, this was because both calls had an IncludeAuth parameter that had to be set to activate that codepath. However, the current plan is to remove that parameter from the API and make it effectively always true.. The pachctl portion of this is fixed by https://github.com/pachyderm/pachyderm/pull/2283. This should be fixed by https://github.com/pachyderm/pachyderm/pull/2283. It turns out that this login issue is a symptom of a larger issue that we've been struggling with for a while: when we deploy kubernetes in docker on our local machines, it uses its own kuberenetes-internal DNS server, which can resolve domains inside the cluster (e.g. for StatefulSets) but can't resolve domains outside the cluster. One fix: set the --cluster_dns flag in etc/kube/internal.sh to 8.8.8.8 (Google's public DNS server). The line to modify is: https://github.com/pachyderm/pachyderm/blob/master/etc/kube/internal.sh#L10. One fix that worked for me at about a year ago (I've been using kubernetes in minikube since then, which seems to handle internal and external DNS resolution by default) is to comment out dns=dnsmasq in /etc/NetworkManager/NetworkManager.conf, and then restart network-manager (sudo service network-manager restart), kubernetes, and docker.\nI have no idea why dnsmasq would interfere with k8s DNS resolution, but it's in my notes.. Fix https://github.com/pachyderm/pachyderm/issues/2278. One thought: in our original discussion, we proposed that each object in Pachyderm have a vector of Scopes associated with it. Then, the dashboard will be able to get the Scopes associated with each object in the display and render it appropriately.\nIf, instead of an API that provided \"here is the list of operations that U can perform\", we narrowed the API to simply \"can U perform operation O?\" => (yes|no), we might be able to have a more fine-grained collection of operations that aren't necessarily orthogonal or additive(e.g. \"can the user update this pipeline?\" \"can the user update this pipeline with --rerun=true?\", etc), as well as offer an explanation (\"to update this pipeline with --rerun=true, you must be an READER of inrepo (true) and an OWNER of outrepo (false)\")\n(though this might be complicated and not particularly useful. Just a thought). To clarify this issue a little, I think we should build a role-based access control model similar to Kubernetes. See:\nhttps://en.wikipedia.org/wiki/Role-based_access_control\nhttps://stackoverflow.com/questions/4989063/what-is-the-meaning-of-subject-vs-user-vs-principal-in-a-security-context\nIn Pachyderm (for example):\nPrincipals (things that can be added to ACLs):\n\nGitHub users (authenticate via OAuth), LDAP users, Groups. Possibly capability tokens?\n\nSubjects (things that try to access data):\n\nHumans, Pipelines, Robot accounts (external, automated systems, rather than pipelines)\n\nPermissions:\n\nRepo\nCreateRepo/DeleteRepo\nInspectRepo/List (discovery -- does this repo show up in ListRepo for you?)\nGetFile/DiffFile\nListFile/GlobFile/ListCommit/InspectCommit/FlushCommit/SubscribeCommit/ListBranch\nStartCommit/FinishCommit/PutFile/CopyFile/DeleteFile\nSetBranch/DeleteBranch\nPipeline\nCreatePipeline/UpdatePipeline\nDeletePipeline\nRerun pipeline\nGet stats? Right now this is a property of the repo. May entail a stats redesign\nIt should be possible for the UI to read the complete set of permissions that a user has, so that it can use the information to render the page correctly\n\nRoles:\n\nRepo\nSpectator (can discover the repo -- mostly useful for making a repo publicly discoverable)\nReader\nWriter\nOwner\nPipeline\nSpectator\nWriter\nOwner\n\nResources\n\nCluster (i.e. the object of operations that are currently limited to \"admins\": Modifying the set of admins, deleting all data in a cluster, etc)\nPipelines\nRepos\nIdeally we'd build a small resource hierarchy, so that having \"writer\" access to the cluster implies \"writer\" access to any given repo in the cluster. We'd also like to add a Users table to Pachyderm's auth system\nThis will fix some silly Pachyderm semantics where certain calls (e.g. ListRepo) can only be called by logged-in users, but users may log in as someone totally unrelated to a given cluster and then start listing that cluster's repos (they'll see that they don't have access to anything, but still)\nThis will also help our UX (which will be able to filter repos/pipelines based on the user that is loading Pachyderm's dashboard) and affect our enterprise business model\nFinally, this might allow us to build a delegation system. Currently, there is no practical distinction between \"principals\" and \"subjects\", since a subject can't have more than one identity. With a users table, though, we might be able create delegation tokens that have access to an object iff the issuer also has access to the object, and associate those delegation tokens with the receiver. Other features:\nUltra-high priority (just so they're in here):\nSAML support (for e.g. okta)\nLDAP support (for authentication at least, and ideally for syncing groups)\nGroups in general\n\nMedium (I guess):\n List commands should allow the caller to only return resources to which they have access. This can't be the general form of these commands, as otherwise users have no way to request access to new things, but this might be helpful as far as limiting your view to a subset of your organization's DAG. Maybe we could distinguish between Listing e.g. repos, and searching for them?\n  * Also (or rather, in particular) how would this work for pipelines? You only see a pipeline if you have access to all adjacent repos? If you have access to some of the repos but not all of them, do the ones you can see appear as floating repos in the UI?\nOthers:\n* Display name != internal ID. This way a github user can change their username without losing their access to Pachyderm resources\n. Another change: Ideally PPS shouldn't have a superuser token. Instead, PPS should be its own subject in Pachyderm, and it should have explicit, limited access to the resources it uses (specifically, the spec repo). Things like adding every pipeline to its inputs ACLs should be handled by a more limited admin API. (Update: This is partially done. The PPS user is still a superuser, and handles adding pipelines to ACLs. However pipelines are no longer owned by the superuser by default)\nWe may also want to add a cluster-global lock, that lets calls ensure that no new pipelines are created while existing pipelines are being added to their inputs' ACLs (Update: This is also done, in a hacky way, as of 5/21/2018: When auth is activated, the PPS user is added as the cluster admin. Because you can't create repos or pipelines without logging in, and auth.Authenticate() doesn't work when the cluster is in this \"partially-activated\" state, no new repos or pipelines can be created until auth activation is finished). This is fixed by https://github.com/pachyderm/pachyderm/pull/2310. @gabrielgrant . Merging this PR because it'll unblock Gabriel. Per our discussion, make docker-build will now also run make enterprise-code-checkin-test to preemptively warn us against checking our code in. Merging even though CI is failing because the test failure is obviously not due to this PR and we'd like to get 1.6.0rc7 out tonight so that it can be tested tomorrow morning. Per my conversation with Sean, the comment above was the only issue, so I'm merging the PR. I accidentally merged this PR before I approved it, but LGTM, obviously :-P. One thing that occurs to me: with this change, if pachd goes over the limit, it'll crash (if I understand limits correctly). I think that with just a request, it's not an issue, since you're allowed to go over, so it's possible the reservation that's there now is too small. https://github.com/kubernetes/kubernetes/issues/53309 is crazy. Do they not test kubectl create?. Updated title to reflect that change that is being made. This was fixed by https://github.com/pachyderm/pachyderm/pull/2507. Future optimization: as the number of available datums goes down, the number of datums that workers try to process at once (i.e. k) goes down, to keep workers evenly loaded\nAnother: k depends on the size of the datums. The largest datums are processed alone, the smallest datums in large batches. Another future optimization: because workers no longer receive datums from a master (workers can always deduce any job's state from data persisted in etcd), pipelines may no longer need a master process, allowing an idle pipeline to consume no resources.\nCurrently, however a pipeline master's other job is to watch PFS for input commits. Removing pipeline masters therefore depends on the new commit/job model as well. Until that's submitted, and we're able to design masterless PPS, the responsibility for creating PPS jobs should remain with PPS pipeline masters. Updated the title to reflect that this affects how datums are distributed during jobs, rather than e.g. how jobs are triggered or anything else. Two differences between this design and its ultimate implementation (https://github.com/pachyderm/pachyderm/pull/2473):\n1. In this design, each chunk of datums (i.e. each \"run\" of k datums in the design above) is protected by a single lock in etcd (above, each datum has its own lock)\n2. the chunks are laid out by the pipeline master at the start of the job. Because the master defines the mapping from chunk\u2192datum in advance and writes that map to etcd, the chunks can be different sizes (workers don't have to infer the chunks\u2014they can just read each chunk directly). The master takes advange of this to lay out chunks that contain an approximately equal number of bytes. Currently, each chunk is the smallest run of datums that is >= some fixed size (>= is the condition so that every run contains at least one datum. It if were <=, a datum that was bigger than the fixed size wouldn't fit in any chunk). I tried to add a test for this, but it seems like you have to generate a fair amount of logs to trigger the bug, and when you do so, GetLogs gets really slow (to the point that TestGetLogs and TestGetLogsWithStats take ~5 minutes to run, each). I'm not sure why it gets so slow but I eventually just threw up my hands and submitted this. Done -- pps and worker are fixed. Future optimization: currently each PPS pipeline has a master process running in its own pod that watches the pipeline's upstream input repos. As part of this change, pipelines may not need a dedicated master to create PPS jobs in etcd, meaning that if both this issue and https://github.com/pachyderm/pachyderm/issues/2396 are finished, we might be able to remove pipeline masters completely.. I'm closing this issue because the design has been superseded by https://github.com/pachyderm/pachyderm/issues/2505. These are all really good concerns to keep on our radar. That said, Pachyderm doesn't support any kind of refresh flow with OAuth (once a user has authenticated, we discard their access token and then they authenticate via Pachyderm token), so the lack of a SAML refresh flow isn't a regression\u2014we could similarly discard their signed SAML credential and have users authenticate via Pachyderm token. The only downside is that group membership would be stale until the user reauthenticated, which we could force them to do daily (not great but per our last meeting, the client we're implementing this for would be okay with it). We also don't currently support OIDC in any capacity, so the lack of support for OIDC groups is a bridge we can cross later.\nMy perspective is that I haven't seen many SAML implementations in Go (there is https://github.com/crewjam/saml, but it seems to be maintained by one person, and it doesn't seem to support ID-provider-initiated auth), so improving on what we get from dex seems like a large project to me, and I'm eager to postpone it :-). (note that, as with ACLs, there's no difference between an empty group and a group that doesn't exist). Per https://github.com/pachyderm/pachyderm/issues/2432, we'll also have a few default magic groups, such as group:all, which implicitly includes all signed-in users. I'm adding @dwhitena for changes to the aws deploy doc, and @sjezewski for changes to aws.sh. If y'all are busy, just let me know if I should assign this to someone else. Rearranged the doc, let me know what you think. Sorry, I just checked back into this PR, but it's now merged. whoops, sorry, renamed the issue. I'm deleting this PR, since I replaced it with https://github.com/pachyderm/pachyderm/pull/2539. I'm adding a test in a followup PR. Unfortunately the Faraday tests ran into an error\nerror in waitJob failed to retrieve hashtree after processing for datum [file_info:<file:<commit:<repo:<name:\"opus_unpack_4\" > id:\"0ceb80ad7ce94b45b782890210b37bbd\" > path:\"/en\" > file_type:DIR size_bytes:1759936784 hash:\"\\201K\\354\\225\\226\\324@\\231\\373\\323~\\217\\333>D\\025\\377\\241N\\023$\\204\\325>>s\\310\\276\\365r\\177\\255\" > name:\"opus_unpack_4\" branch:\"master\" ]: tagGetter: tag 5e8dfc2585239a7a8f6defc0b90c7bec3e582a114631968305842e50d9c0dc0eaf75 not found, retrying in 22.128624283s\nI'm digging into it to see if I can figure out the problem. I'm going to try creating a second cluster and re-running the tests there, since I don't think this problem is unique to this PR, but it would be nice if we could figure it out. Interestingly, I went and re-checked the job from this test and it had completed successfully:\nID                                   OUTPUT COMMIT                                  STARTED     DURATION      RESTART PROGRESS      DL       UL       STATE\na06e6dd4-84f1-46bc-a793-6c72ab3c7586 opus_repack_4/c9a8499f30a446888c560862a8273fa2 4 hours ago 29 minutes    37      127 + 0 / 127 13.78GiB 14.32GiB success\nI'm going to re-run it, but I think a) it's reasonable to merge this PR at this point, and b) this object not found bug may be a cache invalidation bug: either S3 or our internal tree cache are caching the not found response and returning it. Just merged #2478, thanks for being patient with us!. We just observed a similar bug with a user in our users channel:\n- The user has a very similar DAG: two inputs feeding into one pipeline\n- The user updated their pipeline, which spawned a job, which the user killed (deleted? Unclear from their message)\n- The user then called update-pipeline --reprocess on the pipeline and it stopped \n- Once the user's cluster got into a broken state, new commits in either input branch didn't cause downstream pipelines to run\nEither deleting the job, --reprocess or some interaction between the two could be related. Another client ran into a similar bug: they called UpdatePipeline (without --reprocess) and lost the ability to trigger the pipeline.\nOne hypothesis: this bug is due to dangling JobInfos in etcd. When a pipeline master receives a new commit, it checks etcd to see if there any jobs with the same salt as the current pipeline, and if so, doesn't spawn anything new. If a PipelineInfo document for a dead job gets stuck in etcd, the pipeline master might stop spawning new pipelines.. I'm now convinced this is correct. The first fn (getTotalPages) I'll leave out because floor((a+b-1)/b) == ceil(a/b) is an old trick and easy to verify (just check the cases 1. b divides a and 2. b does not divide a)\nFor the second, there are three cases and we need to show equivalence:\n1. start > totalSize - 1 iff page > lastPage\n2. start < totalSize <= end iff page == lastPage\n3. end < totalSize iff page < lastPage\nProofs (note that for a in N and b in R, a >= b iff a >= ceil(b) and a < b iff a < ceil(b)) (verify by checking the cases b in N and b not in N):\n  1.\nstart >  totalSize-1\niff           start >= totalSize\niff page * pageSize >= totalSize                  (definition of 'start')\niff            page >= totalSize/pageSize         (real division, not computer division)\niff            page >= ceil(totalSize/pageSize)   (because 'page' is an integer)\niff            page >  ceil(totalSize/pageSize)-1\niff            page >  lastPage\n2.\nstart < totalSize         <= end\niff page * pageSize < totalSize         <= start + pageSize = (page+1)*pageSize\niff           page < totalSize/pageSize <= page+1\niff           page < ceil(totalSize/pageSize) <= page+1 (because 'page' is an integer)\niff           page <= ceil(totalSize/pageSize)-1 < page+1\niff           page == ceil(totalSize/pageSize)-1        (because `ceil(totalSize/pageSize)-1' is an integer)\niff           page == lastPage\n3.\nend < totalSize\niff (page+1)*pageSize < totalSize\niff            page+1 < totalSize/pageSize\niff            page+1 < ceil(totalSize/pageSize)\niff              page < ceil(totalSize/pageSize)-1\niff              page < lastPage. I think when I committed this change, I ran git commit -m\"port-forward writes to $HOME instead of ./~\" and bash expanded $HOME in the commit message, which was then picked up by github and put in the title.\nThen when I looked at this PR, I was like \"why did I tell everyone that pachctl would write to my personal home directory\"?. Update: when you create, start, or update a pipeline, the pipeline will only process the head commits of its inputs (rather than trying to traverse the commit history of its inputs in a stable order). This is simpler than trying to establish a stable order on all historical commits across all repos in PFS, and hopefully more useful, as users may not be interested in the result of processing old data.. Items remaining to be implemented:\n- [x] Currrently, you can't use multiple inputs from the same repo (PPS isn't using branch names to figure out which branch to use)\n  - Same repo/different branches works for non-incremental pipelines now, but not incremental ones\n- [x] Incremental jobs don't work at all\n- [x] We're not storing pipelineInfo or jobInfo in PFS (which would give us the property that output data is provenant on the right pipeline version)\n- [x] ~Indicate whether a branch is a PPS \"spec\" branch in its BranchInfo somehow~ We now put PipelineInfos in their own repo, rather than a branch in the pipeline's output repo.\n- [x] Failed jobs. Currently, when a job fails, we leave the job's output commit open. However, we'll need to create an empty output commit (with no hash tree). This will be both in src/server/worker/api_server.go:1150 and in StopPipeline (and perhaps separately in UpdatePipeline)\n- [x] ~Jobs should mark themselves killed if their output commit is finished while the JobInfo is still in RUNNING (happens if UpdatePipeline is called while a job is running)~. Job state should be inferred from the job's output commit, rather than from the JobInfo (propagating state changes from the output commit, which is modified inline in e.g. KillJob, to the JobInfo is currently unreliable)\n- [x] (#2615) Figure out how to compress commit subvenance. Currently, if you have a pipeline with two inputs where one input branch gets all the commits (e.g. a schema branch with almost no commits, and a data_dumps branch with very regular commits) then the subvenance of the head commit in the branch with few commits (e.g. schema) could get very large.\n- [x] (#2616) Rewrite PropagateCommit to use branch subvenance, instead of scanning all branches across all repos in PFS\n- [x] (#2624) We need to call propagateCommit in createBranch (currently we don't) in case we're updating a branch, need to create a new HEAD commit in that branch (due to new provenance) and that new HEAD commit should trigger commits in downstream branches. We should also write a test to expose this bug, where we have a 2-stage pipeline, we pause the first stage, commit to the input repo, and then unpause the first stage (which should create an output commit in the second stage's output)\n- [x] We'll need to fix job/datum timeouts. Currently worker masters are responsible for observing when a job or datum has exceeded its time limit and marking the job failed (and workers are responsible for observing this change and quitting). In this change, the worker master will need to close the output commit (and workers will still be responsible for observing this, though DeleteCommit and CancelJob also require this)\n- [ ]  Workers should kill any user processes running as part of a job in the following cases:\n    - [ ] The job's output commit is finished (specifically, the job is cancelled)\n    - [ ] The job's output is deleted,\n- [ ] (#2618) Implement DeleteCommit for closed commits\nNice-to-have (possibly implemented later)\n\n[ ] UpdatePipeline is not transactional. Currently, we rename the master branch to master-vN, then create a new PipelineInfo commit on the Spec branch, then create a new master branch, all in separate writes. If an upstream commit arrives between these steps, it will never be processed by the pipeline being updated\n[ ] Commit scratch spaces should be made into a collection, so that DeleteCommit can clear a commit's scratch space inside the same transaction that it uses to delete the commit. Quick note from a design discussion:\n--reprocess is changing significantly\nBefore, --reprocess meant \"Copy master to a new branch, and run the new pipeline code on all old commits, and put the results in the new master branch\"\nNow, we're moving away from processing old commits. Instead, every time a user calls UpdatePipeline, we archive the master branch and then process only the HEAD commit of the new pipeline's inputs (this is whether or not the user passes --reprocess)\n\nIf a user does pass --reprocess we change the pipeline's salt, meaning every datum is recomputed with the new code. If the user does not pass --reprocess, we'll re-use any cached results from the old version of the code\n\n\nmsteffen note: This is also philosophically consistent with another design idea we've had: Users can look at the results of a pipeline and mark them \"bad\" to remove them from the processed datum cache. Then if they update their pipeline, datums that were marked bad (and invalidated) will go through the new pipeline code again (and hopefully produce good output this time). In general, both of these features are easy to explain in the context of a computed results cache, which, incidentally, is how Pachyderm actually works. Another note from working on tests (as part of putting PipelineInfo in PFS):\n\nIn this design, pipelines often have one more job than you expect\nThe reason is that pipeline output branches are now provenant on pipeline spec branches (the spec branch is the branch in the output repo that holds the pipeline's PipelineInfos)\nFor example: if you create a pipeline with one input, then that'll create an output branch provenant on two other branches: the spec branch (containing the PipelineInfo) and the input branch\nIf there are no commits to the input branch, CreatePipeline commits in the pipeline spec branch, which creates an open commit in the output branch. Starting this commit causes PPS to spawn a job, which immediately realizes it has no datums to process and finishes\nThis is functionally equivalent to how Pachyderm works now\u2014the only significance of this is that when you run pachctl list-job you often see one more job than you expect. This might confuse customers initially, so I'm noting it here so that hopefully we'll be aware of it. Well, this is a design decision, but we've talked about wanting to create a new commit e.g. when a user calls UpdatePipeline and there are input commits. Basically, the new pipeline would process their data at HEAD.\n\n@JoeyZwicker  I actually think we want spec branches to trigger commits in general, we just might not want them to trigger if they're the only commit that's triggering the job. Consider that if PFS didn't propagate commits to the spec branch, then calling UpdatePipeline would have no effect until the next commit to an input branch. Another design note: because commits now precede jobs, If a job fails, that job will have an output commit. If you try to read data from that output commit, the read will fail. This includes calling e.g. pachctl get-file pipeline master /output/data when the most recent job from pipeline failed (that get-file will return with something like file /output/data not found in repo pipeline at commit abcd123456)\nUpdate: On revisiting this, we decided that e.g. GetFile should traverse \"killed\" commits (commits with Finished but no Tree) and give you information from the last commit with a Tree. And actually, the process for getting files from open commits and \"killed\" commits is the same: read the parent tree and add any scratch data in etcd to it. It's just that the process is vacuously correct for \"killed\" commits, as they have no scratch data in etcd. One other product change: Previously, if a user called update-pipeline --reprocess, we would move the commits on e.g. master over to an archive branch, e.g. master-v1. After this change, we won't create an archive branch. Updating a pipeline will simply leave the pipeline's old output commits dangling (and generally inaccessible, except by extracting them from the provenance/subvenance of other commits). When working on this change, we initially implemented the decision to store PipelineInfos in PFS by storing them in a branch in their pipeline's output repo. I.e. every output repo had a branch named spec with its pipeline's PipelineInfos in it.\nHowever, this decision interacted badly with Pachyderm's auth model. Currently, Pachyderm ACLs can only protect a repo. This means that if a user doesn't have at least READER access to a pipeline's output repo, they can't retrieve that pipeline's PipelineInfo. This means that ListPipeline won't be able to return most of a dag's pipelines to most of a cluster's users\u2014a significant departure from Pachyderm's current semantics.\nSo, we're returning to our original design: PipelineInfos will live in a dedicated PipelineInfos repo. All users will be able to read the PipelineInfos repo (so that ListPipeline retains its old semantics), and only PPS (which has a master auth token) will be able to write to it. PPS will apply its existing auth criteria to UpdatePipeline, meaning that Pachyderm's current auth semantics will be able to persist.. Potential improvement: PFS transactions\n- Currently, PFS propagates commits in StartCommit. This design has a few downsides:\n  - Once a user calls StartCommit(R, B), B points at an open commit. Note that calling StartCommit(R, B) fails if B already points to an open commit, so StartCommit(R, B) can't be called twice and essentially \"locks\" B for writing.\n  - All downstream commits triggered by StartCommit(R, B) are provenant on the new, open commit on B.\n  - Moreover, all future downstream commits triggered by calls to StartCommit(R', B'), where R/B and R'/B' have shared subvenance, will also be provenant on the open commit on B.\n  - In general, once there's an open commit on R/B, no downstream jobs can run until that commit closes. If this commit is deleted, all pending downstream jobs will disappear.\n  - One use-case where this would matter is if a user has an open commit and is streaming logs into it. No jobs will run until that user calls FinishCommit (which may be a long time), even if those jobs are triggered by commits to other repos:\n[      ]\n[ Logs ]-.\n[  --  ]  \\\n[ Open ]   \\\n[Commit]    \\\n[      ]     \\\n              \\\n                >---( P )-->[ Output ]---( P-2 )-->[ Output 2 ]\n              /\n[            ]\n[ Other data ]\n[            ]\n\nCommits to Other data will trigger jobs that won't be able to run until the commit to Logs is finished.\n\n\nThese semantics affect UpdatePipeline in the design above.\nCurrently, when a user calls UpdatePipeline(pipeline), PPS starts a PFS commit to hold the new PipelineInfo.\nIf PPS can't finish this commit (e.g. bad network), not only will pipeline never run any jobs, downstream jobs from other pipelines also won't be able to run. The open commit locks PPS until it's closed or deleted.\n\nAn alternative approach may be to make open PFS commits stateful. Specifically:\n- When I call c := StartCommit(R, B), no downstream commits are created. The open commit c \"remembers\" that it's on branch B. When I call FinishCommit(c), that's when B is moved and downstream commits are created.\n- If another user commits concurrently to other repos, downstream jobs will point at the closed commit at the head of B\n- If I abandon c, nothing is blocked. If I delete c before closing it, nothing else is deleted.\n- If another user calls StartCommit(R, B), and then finishes their commit before I finish mine, then when I call FinishCommit, I'll have to decide if I want to append my commit to their data or discard my commit.\n- One way to think about this change is that it makes PFS more tolerant of concurrent operations. In addition to the job-blocking problem described above, consider this example:\n  - I call StartCommit(\"repo\", \"master\") and then fall asleep (I may be a process). Some else calls StartCommit(\"repo\", \"master\"), receives an error (\"commit already open\"), and then says \"oh, msteffen abandoned this commit\" and calls FinishCommit(\"repo\", \"master\"); StartCommit(\"repo\", \"master\"). I then wake up and call PutFile(\"repo\", \"master\"). I'm now writing into someone else's commit, with no way to know what happened.\n  - Both of this example and the job-blocking problem are similar: with job-blocking, both I and another user are trying to \"write\" (i.e. pass data to) a downstream branch, and can't. In this example, both I and another user are trying to write to the same branch, and can't.\n- In this model, commits are more like transactions in a traditional MVCC ACID database, which admit multiple concurrent writers and and fit with the PFS transactions described above (in fact, rather than having both PFS commits and PFS transactions, we may be able to combine these into a single concept)\nSome downsides of this new model:\n- PutFile(repo, \"master\") and FinishCommit(repo, \"master\") would be difficult to make work. Either\n  - they wouldn't work at all\u2014\"master\" would always point to a closed commit\n  - we would have to keep a map from (branch/client -> open commit). This itself could be implemented in (at least) two ways:\n    1. in the client: every implementation of the Pachyderm client would have to support PutFile(repo, \"master\") --> PutFile(repo, abc123), or\n    2. in the server: after a user calls StartCommit(repo, \"master\"), the server either marks their GRPC channel or passes metadata back to the client, and PutFile(repo, \"master\") requests via the same TCP session or with the same side-channel metadata are applied to that user's open commit. Another design note: We currently store pipeline metadata in a dedicated repo (currently just called spec). We chose to have a dedicated repo so that the PPS data model would work well with our auth model:\n- all users are allowed to read from the spec repo, so that PPS.ListPipeline() can return information about all pipelines to all users, as it did before. If pipeline metadata were stored across several repos, users would have to get access to all of them for PPS.ListPipeline() to continue working this way.\n- only admins are allowed to write to spec, so that almost all writes to spec have to go through PPS. This prevents users from accidentally putting PPS in an invalid state by writing directly to spec in an unexpected way and from circumventing PPS's existing authorization logic.\nHowever, having a dedicated spec repo does interact badly with two other API calls: PFS.DeleteAll() and Auth.Deactivate(). Both of these delete all user data from etcd, but now PPS needs two pieces of data to exist at all times: the spec repo, and the PPS auth token (which it uses to write to the spec repo). To accommodate this, PFS.DeleteAll() has been modified to re-create the spec repo after deleting it, and the auth API no longer deletes capability tokens (which are held by pipelines) inside of Auth.Deactivate().\nThe former means that PFS has some code that it only runs for the benefit of PPS (arguably a breach of abstraction) and Auth has a collection of data that is impossible to delete. Neither of these design decisions seem optimal to me; I think it would be better if Pachyderm had some kind of initialize() function that re-initialized a cluster after DeleteAll(). Then users could inject their own initialization code if they want to set up additional software inside of Pachyderm.\nThese changes are simple, though, and will let us get this project finished. I'm just making a note so we can revisit them later.\n. Per conversation with Sean, I'm submitting this PR. If we think of a more robust way to do the version check, we'll fix it in a followup PR. @jdoliner Just rebased to fix merge conflicts -- I may have to make a few more changes (actually move CI to k8s 1.8, at least), but this should be approximately ready . On second though, because this PR includes the client refactor, it's going to be hard to review. Instead, I'm going to just commit directly to masterless because the merge is actually quite mechanical (just a few shared field numbers in proto files). @sjezewski Since I'm in New York, if this PR LGTY, could you also click the merge button?. Re: 1.7, that makes sense. Very exciting! A few comments on the auth portion of this:\n\nACLs are updated so that a user without read access also cannot see a given object\n\nI thought about doing this for the commit invariants project, but I decided if companies are going to use auth to separate teams, we probably want something a little more complicated. If you can't see repos that you can't read, then there's no way to ask for access to something that you don't already have access to.\nThere's an issue for discoverability in Pachyderm here: https://github.com/pachyderm/pachyderm/issues/2434, which I think would be a good place for design ideas\n\nDefine a custom k8s namespace for that user\n\nYes! This would be good anyway, so that users can e.g. get logs from their pipelines without being able to e.g. read directly from etcd. There's a related issue here: https://github.com/pachyderm/pachyderm/issues/2430, which again would be a good place to write out any ideas anyone has\n  . Closing this PR as it's obsolete. Since this isn't going straight into master, I'm merging the fixes that are in here and will then keep working on some of the auth tests. Again, because I'm not merging into master, I'll merge this into commit_invariants once CI passes. I'm merging this change for now, but auth_test.go#TestPipelineRevoke seems to be flaky, and may be indicating a race condition somewhere (it hangs on FlushCommit intermittently). As much as I want to get back to making forward progress, I think it's worth investigating since it may be a bug that's harder to fix once there's a ton of new code on top of it. Merging this pull request, with the same rationale as https://github.com/pachyderm/pachyderm/pull/2681. ~Update: Because we have two user types now, (github users and Pachyderm robots) Pachyderm's API will now insist that all usernames are preceeded by either 'github:' or 'pachyderm_robot:' to disambiguate usernames~\n~To be clear This change is user-visible, and might have consequences for the dash (@gabrielgrant), though I'm not sure what. When setting an ACL or a user's scope in a repo, the caller must prefix usernames with github:. If they don't, they'll get an error~\nThis is no longer being implemented. See comment below for new design.\nPer conversation with @JoeyZwicker  this morning: this is definitely awkward. A better solution would be to add a users table to Pachyderm, so that all requests only operate in terms of Pachyderm users, and Pachyderm internally tracks the relationship between user X and a specific GitHub account. However, I didn't think I could add a user's table (along with adding/deleting users) in time for the client that needs this change.\nMy understanding is that we'll embark on a larger rewrite of auth (with an improved design) soon, perhaps next week. Until this, it may be best to discourage use of auth as any callers will have to rewrite their auth clients once a better API is implemented. Second update: I also need to change how PPS gets capability tokens for its pipelines, as exposing ~GetCapability()~ GetToken() means that that call can't give out god tokens to pipelines before auth is activated, so those semantics need to change. Update: To avoid incompatibility with the dash, we're making the following changes to the auth API (which are partially visible):\nGet/Set requests that take principals now accept a robot: prefix\n\n(Set|Get)(Scope|ACL) check the Username field in requests. If Username contains no \":\" then it's assumed to refer to a GitHub user. If it contains the prefix robot: then it's assumed to refer to a Pachyderm robot. Any other form of request is an error\nThis is dangerous in the case of ModifyAdmins. If a user creates a github account with the same name as a robot account, and then an admin tries to add the robot account as a cluster admin but forgets the robot: prefix, the user will gain admin privileges over the cluster.\nIt might make more sense to disable modification of admins in the dashboard for now.\n\nGetACLResponse separates its entries\n\nGetAclResponse is gaining a new field: RobotEntries. This is a map<string,Scope>, and all keys in the map are robot principals on GetAclRequest.Repo's ACL\nThe existing field, GetACLResponse.Entries, will only contain GitHub user principals. This means that the dash won't encounter any unexpected prefixes, but also won't display the robot users on a repo's ACL.\nThis is also a risky design (as any user relying on the dash won't even know that a given robot user has access to a repo), so we should evaluate if it's absolutely necessary, but it will not break the dash\nAnother alternative would be to disable the display of ACLs in the dash completely, for now.. Note: Pachyderm auth allows tokens retrieved from GetAuthToken to be used after a user's enterprise license has expired (to avoid an expired token sending all of the user's pipelines into a crash loop). \n\nBecause robot tokens will also be retrieved via GetAuthToken, those auth tokens will also be valid after a user's enterprise license has expired (the user still won't be able to mint new robot tokens. Only existing tokens keep working). Note: As part of this design, we also need to be returning tokens (from GetAuthToken) that expire.\n In general, when vault authorizes a user to talk to Pachyderm and generates an Pachyderm token for them, that Pachyderm token should be short-lived. This will be a new feature of GetAuthToken\n As well, vault must be able to extend the lifetime of tokens that it has issued (on behalf of clients that are still using them). To support this, I'm adding a call ExtendAuthToken that only admins can call. It takes a short-lived token and new TTL for that token, and extends the lifetime of the token with the new TTL. Yeah, I'm happy to close this. The only part that isn't done is the users table, and the compromise we arrived at, which at this point is built into the code in a few places, is that a username with no prefix is assumed to be a GitHub user, while a username with a robot: or pipeline: prefix is assumed to be one of those\n(so the dash doesn't have awkward github: prefixes in the places where it's showing human users, but does have robot: and pipeline: prefixes, unless the dash is removing them :-) ). Per in-person discussion:\n ListCommit will detect errors due to deleted output commits, and \n  1. Delete the dangling EtcdJobInfos in a separate goroutine\n  2. Not return those jobs to the user (avoiding a breakdown of list-job)\n Stats commits (and stats branches) will be provenant on their pipeline's output branch. This fixes the bug you sent me in Slack yesterday. Done. Sean originally sent me https://github.com/pachyderm/pachyderm/pull/2711 for review. In the course of rebasing that PR (after submitting https://github.com/pachyderm/pachyderm/pull/2715) I've become quite familiar with the code. As both Sean and I are familiar with this and it's passing CI, I'm going to go ahead and merge it.. Got a verbal LGTM. I'll submit this when it passes CI. Quick update with this PR: the model where there's one pach client shared by the whole binary doesn't currently work with pfs/server/server_test.go, because that test creates many PFS servers, each connected to a separate object API server. Right now, all of those PFS servers are running in the same binary, so post-this change, they all trample each others' pachyderm client in the pachrpc library.\n~IMO the solution is to change pfs/server/server_test.go so that all tests share the same object API server and just clean up the object server's output directory at the end of each test.~\n~(Alternatively, I could try to wrap the pach client in some kind of pachEnv struct so that each PFS server can talk to separate object API servers, but I actually think it should be safe to assume that each pachyderm binary is participating in a single pachyderm cluster. The PFS test essentially sets up the test binary so that it's participating in dozens of \"logical\" pachyderm clusters\u2014one for each test)~\nPer our conversation, we decided to wrap the global pach client in an 'env' struct that we pass to the various API servers, which will let us keep the PFS test the way it is (and avoid a global variable, which is generally brittle). I agree\u2014once the 1.8 refactor is in (which touched a ton of stuff in PFS) I could take another look, but it's not worth messing with that.. If you're running a test cluster, pachd doesn't verify tokens with GitHub. Instead, it takes the token you give it and uses that token as your username. This is per https://github.com/pachyderm/pachyderm/pull/2759 (a feature that IIRC we wanted for dash testing? Though if it's not helping us test the dash we can/should do something else). Per our conversation, this is actually a bug in Activate\u2014currently, if you're running Pachyderm in Minikube (such that it can't talk to github.com due to minikube DNS issues) and you run pachctl auth activate and give it a real github token, Pachyderm auth gets stuck in an intermediate state. This was fixed by e7616feee3d13a222274ef440a62b06626ad1ded. pachctl auth activate now attempts to convert GitHub tokens to a username before putting auth in an intermediate state. Now, Pachctl auth should fail fast instead of getting stuck. Yes, this should prevent that from happening in general, and if it happens anyway (which would be due to etcd errors while Activate() is running) then it's easy to get out of (pachctl auth deactivate without authenticating works in this intermediate state). (changed base branch after merging start-auth-with-robot-admin into master). > Is it really just replacing make launch-kube with make launch-kube-vm?\nBasically, though I renamed launch-kube-vm to launch-dev-vm since it also launches Pachyderm. Since the one target starts minikube, pushes the images, and starts pachd, it's the only thing you'd call, rather than launch-kube + launch-dev. LGTM. Per our conversation:\nIt might help to have a benchmark suite that we can use to test the efficacy of this (and related) changes, which likely means finishing the work of setting up a benchmark suite from last year. I believe the rough pieces of that project are:\n- [x] Add a cron job (in aws lambda or something) that deletes clusters from etc/testing/deploy/aws.sh that are older than e.g. 8 hours\n  - Maybe we should have two kops s3 buckets, one for benchmarks and one for everything else, in case someone wants to use this tool to create a longer-lived cluster\n- [ ] Add something to the test Docker image that we already build that sends logs from the bench container (and perhaps the Pachyderm cluster)\nThat's 80% of what we need. When we want to run a benchmark, we run make aws-test and it'll create a cluster, run our tests, and email us the logs. The cluster will be cleaned up by the cron job\nOf the remaining 20%, 80% of it is:\n- [ ] Eventually etc/testing/deploy/aws.sh will stop working, because kops doesn't clean up all resources used by a cluster and aws.sh will hit a resource limit exceeded error. I think route53 records in particular, as well as the s3 buckets used by pachyderm in test clusters are the main offenders. The cron script that cleans up test clusters will also have to clean up these. I'm going to submit this PR and update the docs afterwards because this is green and I don't want to wait for CI to pass twice :-). As soon as it's in though, I'll send another PR in the same branch. Per our conversation, I'm submitting this PR since there wasn't any feedback other than the testing feedback. Hey Jesse, thanks for all of these contributions! I'm copying this PR (and the other two) into private branches to get them through CI. Tests passing in the internal branch: https://github.com/pachyderm/pachyderm/pull/2956\nCreated an issue to add a test for this: https://github.com/pachyderm/pachyderm/issues/2957. Thanks for the contributions!. @arinto The impact is that pipelines may not trust servers that are actually safe. The risk is that pipeline code might fail after refusing to talk to an (unnecessarily) untrusted server.\nThe logging warns users of the potential cause if the pipeline actually fails, and these are the kinds of errors that should only arise when the filesystem is in a bad state\u2014in other words, if your pipelines are healthy now, then I wouldn't expect this to change anything.. Hi Jesse, I haven't dug into it yet, but when I tried to copy this change into an internal branch to run our test suite, it failed and emitted this error:\n$ pachctl deploy local --no-guaranteed -d --dry-run  | kubectl  apply -f -\nserviceaccount \"pachyderm\" created\nclusterrole \"pachyderm\" created\nclusterrolebinding \"pachyderm\" created\ndeployment \"etcd\" created\nservice \"etcd\" created\nservice \"pachd\" created\nservice \"dash\" created\ndeployment \"dash\" created\nsecret \"pachyderm-storage-secret\" created\nThe Deployment \"pachd\" is invalid: \n* spec.template.annotations: Invalid value: \"\": name part must be non-empty\n* spec.template.annotations: Invalid value: \"\": name part must consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyName',  or 'my.name',  or '123-abc', regex used for validation is '([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9]')\nmake: *** [launch-dev] Error 1. Hi Jesse, sorry I let this languish for so long! On Friday I merged this into the test branch for this PR and unfortunately our internal CI failed because pachd couldn't come up; kubernetes gave this error:\nThe Deployment \"pachd\" is invalid: \n* spec.template.annotations: Invalid value: \"\": name part must be non-empty\n* spec.template.annotations: Invalid value: \"\": name part must consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyName',  or 'my.name',  or '123-abc', regex used for validation is '([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9]')\nmake: *** [launch-dev] Error 1\nI wanted to let you know just to give you a status update, and as far as next steps:\n- I can take a look at the PR and try to make sure no annotation is added if the field name is empty, though I have a few other projects I'm balancing at the moment\n- Basically, while some of our tests won't pass in this branch, because they test enterprise features and external PRs can't download our test enterprise key, you should be able to use these tests to get all non-enterprise tests passing here. In other words, if you're understandably tired of the slow feedback loop this with PR, another option to get unblocked is to get to the point where all non-enterprise tests pass, which will make the likelihood that our enterprise tests pass and we get this submitted much higher.\nWe're trying to make the process of getting external submissions merged smoother, so thanks for being so patient with us!. Ah, sorry, to clarify: under the comments on this page, there should be a box saying \"All checks have failed\" and then continuous-integration/travis-ci/pr ... Details. Click \"Details\" and you'll see how far Travis got through our test suite.\nRight now, all four test shards are failing, and if you click the first one one you're presented with a log from running the test. At the bottom of the log is:\nThe Deployment \"pachd\" is invalid: \n* spec.template.annotations: Invalid value: \"\": name part must be non-empty\n* spec.template.annotations: Invalid value: \"\": name part must consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyName',  or 'my.name',  or '123-abc', regex used for validation is '([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9]')\nmake: *** [launch-dev] Error 1\nshowing the problem.\nI think the last three shards of our test suite should be able to pass, and in the first shard, we'll know we hit an enterprise test problem because the logs will end with some kind of error message about failing to fetch the enterprise key.\nI hope this helps, and let me know if you have any other questions or anything!. Also, if you want to test locally, you'll have to install minikube, but the following commands, run inside the pachyderm repo, are a large subset of our overall test suite:\nexport ADDRESS=192.168.99.100:30650\nmake launch-dev-vm\ngo test -v ./src/server. Making a few travis changes in addition to the refactoring:\n- Initially, my tests couldn't pull kubectl, so I upgraded the version of kubectl that travis uses\n- Then, I believe we started hitting https://github.com/kubernetes/minikube/issues/2704, so I also modified etc/kube/start-minikube.sh to use localkube (which I gather doesn't require systemd, which is not available in travis VMs)\n- Finally, the new version of kubectl didn't understand the \"job\" resource type, so I removed it from pachctl undeploy. TestSkippedDatums fails on https://github.com/pachyderm/pachyderm/blob/2a43696a797cab6861a9aa42d9c7a3a5704aa4f2/src/server/pachyderm_test.go?#L5217\nI tried to go in and deflake it but couldn't actually see what would be going wrong\u2014the file being requested is copied by the pipeline into the output repo and should be there\nUpdate: TestSkippedDatums is fixed by cd986e48067ac93033149f348fbb68fa4a811159. With TestGarbageCollection, the line that fails is this one: https://github.com/pachyderm/pachyderm/blob/8f7d65e558f4e9b2bb43a3c530656316b240bc39/src/server/pachyderm_test.go#L4603\nI ran\n$ for i in $(seq 20); do go test -v ./src/server/ -run TestIncrementalFailure\\|TestGarbageCollection -count=1 || break; done\non my laptop, and ~all 20 runs of TestGarbageCollection passed. I'm not yet sure if that test just flakes infrequently or if there's some difference between my laptop and CI that's responsible for the flakes~ the test did eventually fail, so it just flakes infrequently. I think TestGarbageCollection may be failing due to ~a subtle and interesting behavior in PFS:~\n~ CreatePipeline calls PutFile (one-off) to put the pipeline spec in the __spec__ repo~\n PutFile calls PutObjectSplit\n PutObjectSplit calls io.CopyBuffer(<putFileReader>, <putObjectSplitWriteCloser>)\n The semantics of io.CopyBuffer are that it calls w.Write() after every successful call to r.Read(), even if the buffer it's using isn't full\n* That means that the number of objects created by CreatePipeline depends on the number of r.Read() calls it takes to read in the pipeline spec, which is depends on the details of sockets and network communication\n~Sometimes, the pipeline spec is written into the __spec__ repo as two objects (actually the common case, if my hypothesis is correct) and sometimes it's written as one. If It's written as two objects, both are deleted, and the call to GarbageCollect() deletes both of the them (3 objects deleted total: spec#1, spec#2, and bar), and if it's written as one object, GarbageCollect() deletes the one __spec__ object (2 objects deleted total: spec#1 and bar).~\nI think a lot of what I wrote above is too specific, but it does seem like the results of GarbageCollect() depend on the details of how objects are put. One piece of corroborating evidence is that if you print the objects in the cluster right before calling TestGarbageCollection you see 10 objects on a successful run and 9 objects on an unsuccessful run:\n```\nSuccess:\n\n\n\nobjectsBefore: [\nhash:\"08f71338a5f2f98d4b498116d3fa2e18034a6d1e4d07034b9a42dfcfbf0c6d5b1dfd833e8723aad2aa74c7b1f6f2f1627f0ac24ea675722149c3883595c29ed8\"  I believe this is \"bar\"\nhash:\"09f87bdbaee4ae7524aec75243807d7d1680be37d625e29f2b240d9265d10dff81c96cbf9a8e306f35b74c3804e7d0d0d403ff128a5d2eeb20e4ae3f7322f98a\"\nhash:\"508186d456c24c5b20fe9c49ea794e76a93f6f60a54b7d8e706e387e9da0c813095fa33c91393ba4a9270ace81136734179421140f23646d83c27617e72fc45c\"\nhash:\"5a8e0355f64fdc37eb6fa274b66ae8aae71c2468c0bc9eaa22acae35cc3e026cf408a9d72b5ca3c0ca75ea8b165a4e7781dd41b3d4f088a01bafc913129129b7\"\nhash:\"78001037516dc571ecab6873462fdd576219bc10a89103da3d888e5480d8af51220ff12ab0a7ec0cf105a5d6093c4f035cf053e21df6bb34c3fe1e5ca14c6e64\"  believe this is the hashtree of the input commit\nhash:\"9606fcaac7bcf00ac8ef0ca930a350bb575b6c99b154c43009aeb5ea67dabb9358ecf0986ff5aecbd2fab9a8d164e2d95e8719ad776c33f45bf83019d7a25ab7\"\nhash:\"d82c4eb5261cb9c8aa9855edd67d1bd10482f41529858d925094d173fa662aa91ff39bc5b188615273484021dfb16fd8284cf684ccf0fc795be3aa2fc1e6c181\"  \"bar\" in the input repo\nhash:\"eaa30b25658740dd90e9d98654569ccbc5844d01d2de0df9d642967d045c99bf46139817e4de6e17a89fc713822d31b99501e56f9de60f3557965ac39d83fc8f\"\nhash:\"f6e265bde6d2fd0bb7708aad99bf25be2ffe0bf79f99a918509e9bbe034ef4d4f9267ebbaacafd65465a6f9aad5499d164a11311d119f523ca2318ff46eb49f6\"\nhash:\"f7fbba6e0636f890e56fbbf3283e524c6fa3204ae298382d624741d0dc6638326e282c41be5e4254d8820772c5518a2c5a8c0c7f7eda19594a7eb539453e1ed7\"  \"foo\"?\n]\n\n\n\nfailure:\n\n\n\nobjectsBefore: [\nhash:\"03185ecf2c2d0b2cdb060c6db7ba98ca56059af1df016353a293b9a9698b13855251402d0705f64b84125f60037688eb53e6c9b24f99376c70350e029febc29a\"  I believe this is a hashtree (comes right after an InspectTag call)\nhash:\"08f71338a5f2f98d4b498116d3fa2e18034a6d1e4d07034b9a42dfcfbf0c6d5b1dfd833e8723aad2aa74c7b1f6f2f1627f0ac24ea675722149c3883595c29ed8\"  I believe this is \"bar\"\nhash:\"14077d89da106fcdc47b3411bba60a4769c72767b8231200188faa5e17289e70075f0c2a8af219bb4ccbd9c296876de48c1e69ddb319c9abe8bcda1ae4881435\"  pipeline spec commit (after stop)\nhash:\"78001037516dc571ecab6873462fdd576219bc10a89103da3d888e5480d8af51220ff12ab0a7ec0cf105a5d6093c4f035cf053e21df6bb34c3fe1e5ca14c6e64\"  believe this is \"foo\" (in both input and output)\nhash:\"7f8b82ece28ea31e9600fe5f95698d8ffdf15aff29f384775d86595b153bdca8008a82d08eb628c576be1b5d07aa0240ef9f969ec019c4987b29a0f710ab9090\"  Also pipeline spec after stop?!\nhash:\"886582ccd0602bf83d5b852a3c6de14ff42dbf59dd60c58035aec5a8464ff0b6c7dbafdc2bab00b63ca7625d9ade6ecf98670433ee5273cae28cdd88f5b0cecf\"  pipeline spec before stop?!\nhash:\"92b0108f39c0d0c9c7afe148431191e4af992f18751ef99e07358562c2bcec1198d3a6b4fb398ce5cb450b094733d73c952d73e75ac6cef350afd4d1f0fae647\"  also pipeline spec commit (both this and the above are created in createpipeline)\nhash:\"d82c4eb5261cb9c8aa9855edd67d1bd10482f41529858d925094d173fa662aa91ff39bc5b188615273484021dfb16fd8284cf684ccf0fc795be3aa2fc1e6c181\"\nhash:\"f7fbba6e0636f890e56fbbf3283e524c6fa3204ae298382d624741d0dc6638326e282c41be5e4254d8820772c5518a2c5a8c0c7f7eda19594a7eb539453e1ed7\" ]\n```. FWIW, I think a way to get the semantics we want from CopyBuffer (which I think Bryce is fixing in the new branch anyway) could be with bufio (though it doesn't use the buffer pool like Bryce's, so could lead to extra garbage/too much memory/slow writes, since writes have to finish before the next read can start). One more flake (I believe) that I just ran into: TestAcceptReturnCode\n\n\n\nEtcd couldn't find the record for a job that it expected to find (/pachyderm/1.7.0/.../jobs/<job id> not found). Update: I'm having a bit of a hard time getting make test-tls to pass in CI. It passes locally, but not in Travis, and Travis doesn't give much of an explanation of why\u2014just \"Deadline Exceeded\". I want to get this branch merged so it doesn't become a long-lived branch with lots of merge conflicts, and once this is in, I'll set up a VM similar to the Travis environment and get the test running for every PR. Closing due to staleness. Ran a simple test off of this branch in GKE (the split benchmark with a small amount of data) and it seemed to run fine. I'm going to merge this branch, as at least basic tests seem to show that it works. I didn't initially think this design could be implemented. Specifically, I thought that propagating missing files through the dag via deltas wouldn't work\u2014in pipelines, a file may be \"deleted\" not because any delete command was issued, but simply because no datum output it.\nHowever, the files present in a job's output commit are the union of the files present in the datum output commits, so the only way for a file to be deleted in an output commit is if its input datum was removed in the job's input commit. By removing files' old output trees, we handle the case where a file is removed because the input datum that generated it was removed. I'm starting to wonder if Travis networking is flaky. Two other examples of mysterious test flakes:\n=== RUN   TestListJobOutput\nNo config detected. Generating new config...\nNo UserID present in config. Generating new UserID and updating config at /home/travis/.pachyderm/config.json\n--- PASS: TestListJobOutput (17.13s)\n=== RUN   TestPipelineEnvVarAlias\n--- FAIL: TestPipelineEnvVarAlias (3.86s)\n    pachyderm_test.go:5534: No error is expected but got error deleting workers: Get https://10.96.0.1:443/api/v1/namespaces/default/services?labelSelector=pipelineName%3Dpipelineee259dbd4098: unexpected EOF\n    pachyderm_test.go:5534: current stack:\n        goroutine 14 [running]:\n        runtime/debug.Stack(0xc42010f950, 0x163d024, 0x1f)\n            /home/travis/.gimme/versions/go1.10beta1.linux.amd64/src/runtime/debug/stack.go:24 +0xa7\n        github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require.fatal(0x17c7b40, 0xc42010f950, 0x0, 0x0, 0x0, 0x163d024, 0x1f, 0xc4202ba680, 0x1, 0x1)\n            /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require/require.go:421 +0xc5\n        github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require.NoError(0x17c7b40, 0xc42010f950, 0x1790180, 0xc4202ba670, 0x0, 0x0, 0x0)\n            /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require/require.go:302 +0x13b\n        github.com/pachyderm/pachyderm/src/server.TestPipelineEnvVarAlias(0xc42010f950)\n            /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/pachyderm_test.go:5534 +0xd1\n        testing.tRunner(0xc42010f950, 0x16b4be0)\n            /home/travis/.gimme/versions/go1.10beta1.linux.amd64/src/testing/testing.go:774 +0xc4\n        created by testing.(*T).Run\n            /home/travis/.gimme/versions/go1.10beta1.linux.amd64/src/testing/testing.go:821 +0x2ca\n=== RUN   TestMaxQueueSize\n--- FAIL: TestMaxQueueSize (2.44s)\n    pachyderm_test.go:5579: No error is expected but got error deleting workers: Get https://10.96.0.1:443/api/v1/namespaces/default/services?labelSelector=pipelineName%3Dpipelineee259dbd4098: dial tcp 10.96.0.1:443: connect: connection refused\n    pachyderm_test.go:5579: current stack:\n        goroutine 15 [running]:\n        runtime/debug.Stack(0xc42010fb30, 0x163d024, 0x1f)\n            /home/travis/.gimme/versions/go1.10beta1.linux.amd64/src/runtime/debug/stack.go:24 +0xa7\n        github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require.fatal(0x17c7b40, 0xc42010fb30, 0x0, 0x0, 0x0, 0x163d024, 0x1f, 0xc4203500e0, 0x1, 0x1)\n            /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require/require.go:421 +0xc5\n        github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require.NoError(0x17c7b40, 0xc42010fb30, 0x1790180, 0xc4203500d0, 0x0, 0x0, 0x0)\n            /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require/require.go:302 +0x13b\n        github.com/pachyderm/pachyderm/src/server.TestMaxQueueSize(0xc42010fb30)\n            /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/pachyderm_test.go:5579 +0xd1\n        testing.tRunner(0xc42010fb30, 0x16b4b88)\n            /home/travis/.gimme/versions/go1.10beta1.linux.amd64/src/testing/testing.go:774 +0xc4\n        created by testing.(*T).Run\n            /home/travis/.gimme/versions/go1.10beta1.linux.amd64/src/testing/testing.go:821 +0x2ca\nand this from @gabrielgrant:\n=== RUN   TestPipelineBadImage\nNo config detected. Generating new config...\nNo UserID present in config. Generating new UserID and updating config at /home/travis/.pachyderm/config.json\n--- FAIL: TestPipelineBadImage (62.85s)\n    pachyderm_test.go:5389: No error is expected but got pipeline bad_pipeline_2_59e357ed13e9 should have failed\n    pachyderm_test.go:5389: current stack:\n        goroutine 10 [running]:\n        runtime/debug.Stack(0xc42049ba40, 0x164f924, 0x1f)\n            /home/travis/.gimme/versions/go1.10beta1.linux.amd64/src/runtime/debug/stack.go:24 +0xa7\n        github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require.fatal(0x17db880, 0xc42049ba40, 0x0, 0x0, 0x0, 0x164f924, 0x1f, 0xc4200b9010, 0x1, 0x1)\n            /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require/require.go:421 +0xc5\n        github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require.NoError(0x17db880, 0xc42049ba40, 0x17a3d20, 0xc4200b8fb0, 0x0, 0x0, 0x0)\n            /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require/require.go:302 +0x13b\n        github.com/pachyderm/pachyderm/src/server.TestPipelineBadImage(0xc42049ba40)\n            /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/pachyderm_test.go:5389 +0x5b5\n        testing.tRunner(0xc42049ba40, 0x16c77d8)\n            /home/travis/.gimme/versions/go1.10beta1.linux.amd64/src/testing/testing.go:774 +0xc4\n        created by testing.(*T).Run\n            /home/travis/.gimme/versions/go1.10beta1.linux.amd64/src/testing/testing.go:821 +0x2ca. For posterity: This fixes incremental jobs is by counting new files (line 1518 in the new commit) which approximately maps to the pipeline's direct provenance[1], rather than counting new commits when iterating through provenance (which is a transitive closure).\nIn a downstream incremental pipeline, there will always be more than one new commit in ci.Provenance, as the upstream pipeline will have both a new input and output commit. But if the incremental pipeline only consumes the upstream pipeline's output commit, then there will only be one new commit in the direct provenance and only one file (typically)\n[1]  (approximately the direct provenance, though a pipeline can have the same branch in two inputs\u2014e.g. crossing a branch with itself\u2014and in that somewhat pathological case the correspondence won't be bijective). As this is the same as https://github.com/pachyderm/pachyderm/pull/3050, which was already approved, I'm just going to merge this into master. With the imminent release of 1.8, I think this PR is obsolete, so I'm closing it. I think the current plan is to keep our current hashtree representation and shard by hash(path) across pipeline workers (for output repos. Input repos already have a different hashtree representation, which I gather would stay the same).\nThat said, I'm also intrigued by this idea (if the current plan turns out to be insufficient for some reason). Three advantages of this representation that appeal to me:\n1. Less data stored across a branch\nMany databases (cockroach and hbase) version individual rows1. This potentially allows for a more compact representation of many commits over a whole (linear) output branch. With the current serialized DB representation, each file's path is present in each commit's hashtree:\nCommit 1 ->\n/path/to/f1 -> [ block-1, block-2, block-3 ]\n/path/to/f2 -> [ block-4, block-5, block-6 ]\n...\nCommit 2 ->\n/path/to/f1 -> [ block-1, block-2, block-3 ]  // duplicate\n/path/to/f3 -> [ block-7, block-8, block-9 ]\nvs an example db representation:\n(/path/to/f1, version_start: 1, version_end: \u221e) -> [block-1, block-2, block-3] // only stored once\n(/path/to/f2, version_start: 1, version_end: 2) -> [block-4, block-5, block-6]\n(/path/to/f3, version_start: 2, version_end: \u221e) -> [block-7, block-8, block-9]\nAs we've learned, less data tends to yield better performance\n2. Faster jobs when |\u0394datums| is small\nA previously-considered goal of ours is to make FinishJob take time O(datums(HEAD - HEAD~1)) instead of its current O(datums(HEAD)) (due to the need to merge all datum outputs, even if the datums have already been processed). Like @gabrielgrant, I believe a db would make achieving this goal more tractable. Here is an example merge algorithm (in pseudocode):\nAssume the database schema for files in output commits (slightly modified from above):\n```\ndatum_ids: the ID of every datum that has been merged into (repo, branch, path)\nrepo, branch, path -> ([block_refs], [datum_ids])\nThen merge could look like the following:\nfind(haystack, needle): return first index of 'needle' in 'haystack'\nremove(l, idx): return 'l' where the element at 'idx' has been removed\nrepo: output repo, whose HEAD is being modified\n\nAlso, db.Query(datum_id, , ) would return a cursor over datum_id's output hashtree\n(scanning through each file and its block ref)\ndef add(datum_id):\n  for path_add, block_ref_add <- range(db.Query(datum_id, , )):\n    block_refs, datum_ids <- db.Query(repo, \"master\", path_add)\n    db.Write(repo, \"master\", path, block_refs + block_ref_add, datum_ids + datum_id)\ndef remove(datum_id): # currently difficult\n  for path_rm, _ <- range(db.Query(datum_id, , )):\n    block_refs, datum_ids <- db.Query(repo, \"master\", path)\n    idx <- find(datum_ids, datum_id)\n    db.Write(repo, \"master\", path, remove(block_refs, idx), remove(datum_ids, idx))\nfor change, datum_id <- \u0394datums:\n  if change == \"ADD\":\n    add(datum_id)\n  else:\n    remove(datum_id)\n``` \n3. We might be able to bring back object-level deduplication\nWe've tried two ways of doing this, neither of which work for large repos:\n  - Storing individual object hash -> block ref kv pairs in s3. This is infeasible due to the cost of issuing millions of writes (assuming millions of files, each of which is its own object) to s3\n  - Storing object hash -> block ref inline in each hashtree. This is infeasible due to the amount of extra data that every hashtree subsequently contains (slowing down operations such as GetFile, due to the need to download all of this extra data when downloading the hashtree).\nBy keeping this map in a database, though:\n  - We wouldn't pay a high overhead for single writes (avoiding the first issue)\n  - We wouldn't need to download this data when retrieving a hashtree (or even retrieve the whole hashtree for operations like GetFile)\n  - In line with 1. (less data stored across a branch), we would store less data per branch than we do when storing object hash -> block ref inline in every commit's hash tree, because when sequential commits contain the same object, the kv pair would still only be stored once:\n Commit 1 ->\n0x123 -> [ block-1, block-2, block-3 ]\n0x456 -> [ block-4, block-5, block-6 ]\n...\nCommit 2 ->\n0x123 -> [ block-1, block-2, block-3 ]  // duplicate\n0x789 -> [ block-7, block-8, block-9 ]\nvs an example db representation:\n0x123 -> [block-1, block-2, block-3] // only stored once\n0x456 -> [block-4, block-5, block-6]\n0x789 -> [block-7, block-8, block-9]\n\n1 I also think versioning could be done in a database that doesn't do per-row versioning automatically. To get the hashtree for repo/branch HEAD:\nSELECT * FROM file_meta WHERE repo = \"repo\", branch = \"branch\", version_end = INT64_MAX\nTo get the hashtree for commit #i (because version intervals don't overlap):\nSELECT * FROM file_meta WHERE repo = \"repo\", branch = \"branch\", version_start <= i, i < version_end. This looks the same at #2468 to me, and I think in general is a fairly common feature of auth systems. I think associating users with commits would be super useful, and I agree that we should do it, but my impression (based on my impression of similar features in other products) is that people will also want the ability to answer \"what are the actions that user U has taken, maybe just recently?\"\nI think that may require some additional data structures, otherwise users will have to sift through all recent commits in all repos to find out if any of them were created by U.. One hurdle to this that I recently encountered: connecting to pachd will work via pachctl port-forward but won't work via the master's IP if firewall settings (e.g. AWS Security Group) don't allow traffic on 30650 to the kubernetes nodes (and there's no other proxy, e.g. an ELB, to communicate with). This is the default case for new kubernetes clusters in any of our cloud providers.\nIn that light, pachctl's default behavior will be one of:\n1. Try to connect to the master's IP on port 30650. This will fail if the cluster hasn't been adjusted to allow traffic on this port yet. Pachd could ask users to check for firewall rules, but even so, the user running pachctl may not have permission to change them. I also believe we would still need to retain the ADDRESS variable so that, in this case, users could at least set ADDRESS=http://localhost:30650 and run port-forward (another option that we should perhaps suggest if the connection fails).\n2. Try to connect on localhost:30650, which is what we do now (we could also make pachctl ask about port-forward if it fails to connect, which it currently doesn't). An argument that I can think of for keeping this behavior is that it's a sensible default if we assume users haven't done any firewall configuration yet, and it means new users have fewer steps to follow when connecting to their cluster for the first time. @ysimonson Do you see a way around https://github.com/pachyderm/pachyderm/issues/3089#issuecomment-416727290?\nI actually didn't even realize this issue was still open, though if you see a solution then maybe it's fortuitous that we never closed it. Created a new issue at https://github.com/pachyderm/pachyderm/issues/3157. Closing this. I'm actually not sure if we want to do this. Currently, there's no way to re-run a non-deterministic pipeline (e.g. a scraper) or a pipeline that failed for transient reasons (e.g. database was down) other than to update the pipeline spec.\nThis, admittedly, is a bit of a hack, but it does have some nice properties: if you change the salt it re-runs everything, if you don't change the salt it just re-runs things that failed. If we did make this change, then we'd probably also need to add features that do the same thing as exporting+updating a pipeline with and without a new salt. Made some decisions about how this will work (which are particularly relevant to how this would work in a browser):\n SAMLRequest is sent to ID provider as a parameter in a GET request. This is safe because the request is signed by pachd (which also means both pachctl and the browser must get it from pachd) \n The connection between pachctl and pachd (1. in the diagram above) is a streaming connection, but pachd sends all of its responses at the beginning\nThe flow (again) with certain new details highlighted:\n  1. pachctl (or the dash) opens the streaming connection\n  2. pachd sends a response message containing 1) a pachyderm token that doesn't authenticate anyone yet, and 2) a signed SAML request, but does not close the connection\n  3. user pastes link in their browser (pachctl) or browser opens a new window (dash) with the destination http://okta?SAMLRequest=<base64-encoded signed SAML request\n  4. user is redirected to pachd's ACS endpoint and the ID in the SAMLResponse is connected to the pachyderm token indicated in the SAMLResponse's RelayState\n  5. pachd closes the streaming connection in 1. (with an error if auth was unsuccessful for some reason\u2014e.g. couldn't contact Okta's metadata service). One note: this won't work with IdP-initiated authentication. For that, we need either:\n1. Pachd's ACS endpoint is in the dash's domain, and returns the pachyderm token when queried\n2. Pachd's ACS endpoint returns a redirect to another endpoint that is in the dash domain, with either the pachyderm token as a request parameter, or a temporary code that can be exchanged for the pachyderm token by the browser (I think that in either case, the redirect would need to trigger a POST request from the browser, so that the authorization code isn't transmitted in plain text? In OAuth, browsers seem to send their authorization code in plain text to the client/service provider, though\u2014I'm not sure why that's okay yet). Update: I'm closing this issue because @gabrielgrant and I decided to take another approach (making this design obsolete). We're planning to implement IdP-initiated login only. I will update the top comment on this issue with a new, superseding issue once I've created it.. Sorry, I had a bunch of tabs open and thought this was one of my PRs.. This is similar to, but distinct from, the hashtree design we used for Pachyderm 1.8. Closing due to staleness.. ## Remaining work on this project:\nI'm breaking up the rest of this work into three pieces for planning: 1) demo (what needs to be done so that we can show a working product and get credit for it), 2) finishing (what needs to be done before this is effectively off our plates and we can move on to other work, and 3) stretch goals that may be worth the marginal effort but should not block the project.\nAlso included are approximate time estimates for each section (not rigorous, but if/when we're past them, we should re-evaluate how our time is being spent and update them)\nDemo\n2 business days\u2014finished by EOD 9/10\nFrontend\n\n[x] Users can add groups to ACLs if they're not GitHub users or pipeline users\nWhy: The main consumer of Pachyderm's SAML and groups features will likely be doing ACL management in the dash and will definitely have groups on ACLs (perhaps exclusively) so they will need to be able to add groups to ACLs in the dash. However, adding groups to ACLs should be disabled for GitHub-only users, since there are not GitHub-only groups that they could add. This heuristic satisfies both constraints without requiring any more integration between the dash and pachd (for time's sake)\nThis is a hack\u2014ultimately users should be able to add groups if the auth config includes a groups-enabled backend, but this is simpler and therefore faster\n[x] Cosmetic fixes\nOTP panel polls for a new OTP every second (but it should be static)\nSAML users (and groups) have GitHub icons but shouldn't\n\nBackend\n\n[x] It should be possible to have groups in the set of cluster admins\nWhy The client will probably use groups for all of their access control (because they already have a group management system that they like), so they'll probably want to have a group (perhaps only a group) for the cluster admins that they manage externally, rather than having a de-facto group in Pachyderm that they have to manage through Pachyderm\n[x] Respect SAML assertion expiration\nWhy This is particularly important because we're getting group membership info from SAML assertions. If a user is removed from a group, they won't lose access to the group's resources until they sign in again. It's expected that that will take as long as their last sign-in was valid for (something the client configures in their SAML ID provider) but unexpected that it would take longer than that.\n\nFinishing\n~4 additional business days (EOD on the 9/14)~\n- We demoed a custom release for TrueCar on 9/14, for which preparation consumed an unexpectedly large amount of time (probably 2-3 days)\n- In addition, setting up a test harness for SAML proved to be unexpectedly complicated. We can't run a saml IdP in the test suite because the IdP is scraped by pachd, which may be running in a VM or AWS and not able to reach the test suite. Instead, an IdP needs to be packaged in a docker image and deployed in the kubernetes cluster alongside pachd\n- Because of this, any group memberships or other info we want the IdP to send to pachd probably need to be configured beforehand (or we have to implement a special API in the test IdP, but that would probably be even more work)\nUpdated goal: EOD 9/21\nFrontend\n\n[ ] Testing (should the frontend use the test IdP?)\nWhy The frontend needs good test coverage for SAML features (since this is something that at least one user will rely on heavily). We need to figure out how to write these tests. In particular, is the easiest way to do that to use the same fake IdP that the backend uses for SAML testing? If so, we need to figure out how to get that test IdP deployed in the FE tests.\n[ ] It should be possible to activate auth as a robot user in the dash. Edit: removing this from the front end; this will be testing-API-only (see discussion below)\nWhy This is very useful for testing\n[ ] It should be possible to create new robot users (which likely implies giving admins the ability to get Pachyderm tokens for arbitrary users) in the dash\nWhy Testing\n[ ] Figure out why polling causes OTP to change every second\nWhy We believed that the 1-second polling that the dash does for the Pachyderm model shouldn't affect the display of low-level elements. For now, we turn polling off whenever the OTP panel is displayed, but we'd like to understand why that was necessary, and it may be necessary to come up with a more robust general solution since there are probably other polling bugs elsewhere in the dash.\n\nBackend\n\n[ ] Admins should be able to get OTPs for other users\nWhy Makes dash testing easier (Currently, you have to exchange a forged SAML assertion to get an OTP from Pachd, and the dash sign-in flow, which we wish to test starts by receiving an OTP from Pachyderm. With this feature, Gabriel can authenticate as an admin, directly generate an OTP for Alice, and then test the SAML sign-in flow without having to forge a SAML assertion)\n[x] Public documentation walking through SAML demo, showing off features and expected usage\nWhy In general, there's documentation describing how the auth system works (in GH issues) but little/none describing how it should be used. While that should be fixed in general, we definitely want such a doc here.\n[ ] Finish writing tests for features in feature branch\nWhy we want to check this code into master, and we won't know it'll continue working there without tests\n[ ] Config simplification\nWhy Users have to specify some information redundantly in their config. We should take this out of the config before anybody starts using it and we have to deal with config migration\n[ ] Config validation\nWhy We should reject invalid configs, but right now we don't\n[ ] Unify authentication code vs OTP naming (In code and docs, as well as --code and --otp flags)\nWhy The documentation for the flags says that one will be removed\n[ ] Remove debug_logging field from auth config\nWhy The documentation for the config says that it'll be removed\n\nNote: Former \"stretch\" goals have been moved into other issues (primarily https://github.com/pachyderm/pachyderm/issues/3157). > One thing to discuss is what we want to do about multiple auth providers. My understanding is that right now github/OAuth-based auth is always active, even if saml is being used.\nCorrect, and I believe that changing this should be a long-term goal (i.e. eventually it should be possible to turn off GitHub)\n\nWe should probably decide if multiple active auth systems is something we want to support (my gut feeling is probably not right now), and decide how urgent it is to forcefully deactivate oauth, or if that's something that can wait until we properly migrate auth-based auth to the new auth APIs/flows\n\nI agree (Long-term: definite goal. Short-term: not needed yet)\nRe: discoverability, in my opinion that seems like a separate project (at least, I haven't associated it in my mind with this one at all). ## Convo with Joey & JD\nConclusions:\n\nDoesn't make sense to hack a users table in the frontend before we have robust backend support. Shitty version of this isn't useful, so don't bother implementing it\nGabriel should work on other stuff while SAML is being finished, then when SAML is done, come up with a good backend implementation of a users table, at which point displaying this info in the frontend will be Gabriel's priority.\nWould be useful, before SAML is done, to designing the dash view of the users table. Would help us figure out what information we'll want to have available to display.\nDesign should reflect what Joey already kind of knows what he wants (deactivating users? what does a user have access to?)\n\nNotes\n\nVisibility of pipelines to certain users/groups, and tagging certain repos and pipelines as being related is below basic users table\nVisibility will, at some point, become urgent. But hasn't happened yet, and is probably a complicated feature, so postpone for now\nUsers table is highest priority among auth things until we have another contract riding on a missing feature.\nHow does that compare to other parts of Pachyderm? User profiles, for example, is less important. So is auto complete. The list above is in roughly the right order:\n\n\nList of users, 2. auto complete, 3. user profile page (i.e. admin wants to see what Alice has access to. When Alice gets fired, what do I have to revoke their access to), 4. OTP polling\n\n\nFor now: if auth isn't activated, there's no way to know how many users are using the system. We're fine with that. We don't need to audit users who don't have auth activated because we don't think that's common\nHackiest version of users table: call GetAcl on every repo and display a list of users on every ACL (doesn't work for groups though)\nRemoving a user from every ACL they're part of\n\n. Many of the \"stretch\" goals previously on this issue have been moved to https://github.com/pachyderm/pachyderm/issues/3157, which is likely the next auth project. I mean, somewhat, though to be honest I think having everything be so dependent on one key is unsustainable long-term. Hopefully we figure out some kind of phone-home mechanism, so that a leaked key is less of risk. Whoops, I thought I'd deleted this. I actually abandoned this PR for a few reasons:\n1) I don't think it's that important. It just fixes the error:\n/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning.\n2) I don't actually see how to fix it given the constraints of CI. AFAICT, to upgrade urllib inside of travis, I'd need to pip install urllib, which has cryptography as a dependency. Installing cryptography (which has C/C++ components) in turn requires an apt-get update/upgrade which doesn't finish inside the runtime constraint for Travis tests\nEven if I could figure out how to make this work, I think it would add a lot to our CI runtime (because all of these dependencies would have to be installed every time we ran CI), so I'm not sure it's worth it. I'm just going to delete the PR. LogReq does account for the slot it takes up in the call stack\u2014actually, if you look at e.g. pps/server/api_server.go, the GRPC handlers all wrap calls to Log in an anonymous function to similarly inject an extra frame into the call stack so that Log prints the right thing:\n```\nsrc/server/pps/server/api_server.go:762\n\nfunc (a apiServer) ListJobStream(request pps.ListJobRequest, resp pps.API_ListJobStreamServer) (retErr error) {\n    func() { a.Log(request, nil, nil, 0) }() / anonymous function adds stack frame /\n    ...\n}\n``\nat some point my plan was to replace all of those anonymous functions with LogReq, I just never got around to it.listJob()insrc/server/pps/server/api_server.gohandles auth slightly differently:\n1) Before there was no call toAuthorize()inlistJob()directly; it relied on auth checks inInspectCommit` which it does for every job's output commit. Now there's the block:\nif authIsActive && pipeline != nil {\n    // If 'pipeline is set, check that caller has access to the pipeline's\n    // output repo; currently, that's all that's required for ListJob.\n    ...\n}\n2) The inner function _f has a new else clause if it gets an error from inspectCommit():\nif err != nil\n    if isNotFoundErr(err) {\n        ...\n    } /* new */ else if auth.IsErrNotAuthorized(err) {\n        return nil // skip job--see note under 'authIsActive && pipeline != nil'\n    }. LGTM. It's a problem because NewInCluster() was calling NewFromAddress() with the naked IP (10.0.0.1) instead of e.g. 10.0.0.1:650. We never noticed because this is only used by tests when you run them inside kubernetes, but since I just started doing that I noticed it.\nI just switched the env vars while I was at it because combining the host and port seemed more robust than expecting the port-specific env-var. A common error I see from TestExtractRestoreObjects:\n=== RUN   TestExtractRestoreObjects\n--- FAIL: TestExtractRestoreObjects (18.75s)\n    admin_test.go:126: No error is expected but got transport: transport: the stream is done or WriteHeader was already called\n    admin_test.go:126: current stack:\n        goroutine 69 [running]:\n        runtime/debug.Stack(0xc000178700, 0x1806a5b, 0x1f)\n            /home/travis/.gimme/versions/go1.11.1.linux.amd64/src/runtime/debug/stack.go:24 +0xa7\n        github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require.fatal(0x19dcc00, 0xc000178700, 0x0, 0x0, 0x0, 0x1806a5b, 0x1f, 0xc000262040, 0x1, 0x1)\n            /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require/require.go:430 +0xc5\n        github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require.NoError(0x19dcc00, 0xc000178700, 0x19a2760, 0xc000262030, 0x0, 0x0, 0x0)\n            /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require/require.go:310 +0x136\n        github.com/pachyderm/pachyderm/src/server/admin/server.testExtractRestore(0xc000178700, 0xa2d76ae01)\n            /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/admin/server/admin_test.go:126 +0xebf\n        github.com/pachyderm/pachyderm/src/server/admin/server.TestExtractRestoreObjects(0xc000178700)\n            /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/admin/server/admin_test.go:211 +0x30\n        testing.tRunner(0xc000178700, 0x1885168)\n            /home/travis/.gimme/versions/go1.11.1.linux.amd64/src/testing/testing.go:827 +0xbf\n        created by testing.(*T).Run\n            /home/travis/.gimme/versions/go1.11.1.linux.amd64/src/testing/testing.go:878 +0x353. Per slack conversation with JD, I'm merging this into the put-object-improvements branch. @ysimonson I don't think so, I think the only reason it was in a function was so that it could be parallelized with xargs. Now that we're not using xargs, we can also inline the function\nLGTM. Update: A few months ago, golang/protobuf made a change that caused it to generate backwards-incompatible go structs: https://github.com/golang/protobuf/issues/607\nTo get this change to build, I've also had to update our code to handle the new generated structs. This was user error\u2014it just seems to sometimes take a moment for pipelines to stop and start. Update: govendor fetch golang.org/x/net/context may have solved this entire problem (corroborating [1] above\u2014updates coming soon)\n2nd update: updating golang.org/x/net/context was all we needed to do. Post go 1.9 there's no difference between the two imports (just aesthetics). This screen shows that the first travis build is in progress, but when I click through the link (to https://travis-ci.org/pachyderm/pachyderm/builds/466683207), Travis shows that the build is finished and succeeded. I suspect a message was dropped between Travis and GitHub, and as CI indicates that the build succeeded, I'm going to merge this PR. Note: for GPU resource requests, we convert N gpus (old-style GPU resource request) to\nGPUSpec {\n   Type: \"nvidia.com/gpu\",\n   Number: N,\n}\nThe choice of nvidia GPU in the kind of GPU that pipelines should request is arbitrary, but does fit the few customer uses of these feature that we've found. It's also easy for users to change after migration is fished.. Merging this PR because it's at a good stopping point, but two issues that I know still need to be addressed:\n1. extract/restore large object\n2. Confirm that extract to/restore from object storage works. Update: in general, it seems that the stats commit does finish. The only codepath we see in which it doesn't finish is the one in which one of the job's datums times out. Note: Russel was able to work around this issue by changing our datum retry timeout from 0s to exponential backoff with a 20s initial delay. Hey, sorry; taking a look at this now.\nOverall I agree with the direction. Right now, it looks like https://travis-ci.org/pachyderm/pachyderm/jobs/476637077 passed even though our PFS server test failed:\n```\ngo test -v ./src/server/pfs/server -timeout 3600s | grep -v \"$(date +^%FT)\"\ndocker: Error response from daemon: driver failed programming external connectivity on endpoint eloquent_tesla (71691e394c7ab9988f1db662ae59ec9825e63e6cb575c7082b5978524059f480): Error starting userland proxy: listen tcp 0.0.0.0:32379: bind: address already in use.\nERRO[0000] error waiting for container: context canceled \ngithub.com/pachyderm/pachyderm/src/server/pfs/server [github.com/pachyderm/pachyderm/src/server/pfs/server.test]\nsrc/server/pfs/server/server_test.go:2233:37: cannot use client (type client.APIClient) as type client.PutFileClient in argument to \"github.com/pachyderm/pachyderm/src/server/pkg/sync\".PushFile:\n    client.APIClient does not implement client.PutFileClient (wrong type for Close method)\n        have Close()\n        want Close() error\nsrc/server/pfs/server/server_test.go:2247:37: cannot use client (type client.APIClient) as type client.PutFileClient in argument to \"github.com/pachyderm/pachyderm/src/server/pkg/sync\".PushFile:\n    client.APIClient does not implement client.PutFileClient (wrong type for Close method)\n        have Close()\n        want Close() error\nsrc/server/pfs/server/server_test.go:2261:37: cannot use client (type client.APIClient) as type client.PutFileClient in argument to \"github.com/pachyderm/pachyderm/src/server/pkg/sync\".PushFile:\n    client.APIClient does not implement client.PutFileClient (wrong type for Close method)\n        have Close()\n        want Close() error\nFAIL    github.com/pachyderm/pachyderm/src/server/pfs/server [build failed]\n```\nI'm looking into it a little bit to try and understand what might be going on, but my only blocker as far as approving this is answering this and the open question of why the enterprise tests weren't run in both builds. For context: in testing, this came up because I had a pipeline with a cross input, and during migration, one of its repos would have its commits restored before the other. This meant that the cross pipeline would run with only one input commit, which DatumFactory treats as the pipeline not having any input datums (thus triggering this case). Flushing input commits to this pipeline didn't work (because of the stats commit not being finished) and a downstream pipeline also believed that its upstream pipeline had failed,  causing other issues. ### Forking Pipelines\n- Right now, the naive way to \"fork\" a pipeline is to get the pipeline spec for the pipeline you want to fork, alter the name, and create new pipelines with the new names\n- Among other problems, if you actually did this, Pachyderm would have to reprocess all of your \"new\" pipelines' data. Instead, we should implement \"dag forking\" or \"pipeline forking\" such that the forked pipeline re-uses its parents' processed datums  . > As an aside, is this note still relevant?\nI think it would be totally fine to kill that note. Reassigning to myself, since I basically just reviewed this, and I think we only need one review. LGTM pending a test, per slack conversation. Particular facets of this change I think we ought to test:\n- I think the issue of 0-datum jobs being marked failed I think still exists (though adding the stats commit means that flushCommit would now work, I think, so that's better. But see point 2 below)\n- I'm not actually sure whether it's valid to create a commit with no hashtrees\u2014it might be necessary to create and initialize an empty hashtree, write it to object store, and then reference it in the stats commit. A test would clarify this, though\n. I just went through the whole process of altering assets.go and then realized why we don't need to change it :joy: . Assets.go is part of pachctl deploy. It generates the manifest that gets sent to kubernetes when you're initially deploying pachyderm.\nI didn't need to change it for Gabriel because he's generating the manifest first and then changing the port (in other words, he generates a manifest that uses the hard-coded port in assets.go first, and then changes the environment variables it sets).\nI've modified AssetOpts and PachdDeployment, but I didn't alter pachctl deploy, so the ability to set the port isn't wired all the way through to pachctl, but its one step closer. For testing, I've tested this manually by bringing AWS clusters up and running our opencv test as part of merging https://github.com/pachyderm/pachyderm/pull/3376. Update: It turns out we never really removed the binaries from GitHub (and probably never will) because there are so many existing branches and tags that point to the old commit history. Fixing branches can be done (I've fixed a few) but fixing tags would be extremely difficult, if I understand correctly, because git's push semantics prevent me from overwriting tags in github via a push from my local repo. I'd have to delete all of our release tags in GitHub, and then recreate them in the new commit history, and then push them.\nOn the bright side, git clone does appear to be faster and create a smaller local repo. Closing due to staleness. Just tested this on my machine with the openCV demo, and it ran successfully. I haven't tested it on a machine that doesn't support --preserve-env=args, but since that construct isn't used anymore, I'd expect this to work. > The code looks good to me, but the tests seem to be failing legitimately.\n\nAs an aside, I'd imagine there's a performance gain in reducing the amount of work done in an STM, as you've done here. Is that correct?\n\nYeah, they were, though I think I just fixed the issue. As far as taking RPCs out of the STM, that's my hope (that it improves performance a little, at least in the case that the callback gets run more than once). I'm always a little nervous that one of the reads depends on a key that has to be read inside the txn, but I read through the code a few times and I'm pretty confident it's safe. Update: I had to put retrieving the parent tree back into the STM (but now there's a comment explaining why it's there). The issue is that we don't want to resolve branch outside the STM (in case another commit races with this one, and we need to re-run the callback with a different branch head), and we can't retrieve the parent commit until we resolve branch. Note that this also includes the changes in https://github.com/pachyderm/pachyderm/pull/3540. Just confirmed that tracing works on Minikube. ~Something about the serviceenv change is causing traces to be associated with the wrong parents. I'm not sure what's causing the change, but after digging into it for a few hours I think I'm going to have to investigate this further after I get back~\nUpdate: Addressed this issue (just needed to install the tracer before initializing the ServiceEnv). I've also updated https://github.com/pachyderm/pachyderm/issues/922 with a few more TODOs, but once CI passes I'm going to submit, as all comments have been addressed. The new command seems to work (@kevindelgado and I just tried it after running into the same issue you did). Updated the docs in https://github.com/pachyderm/pachyderm/commit/1b9a623d122a98ad43c4472b3da0cb31414ee44d#diff-b981c927fa5cbfdc99942ad4535e44a4\nI'm marking this fixed, but please feel free to re-open if it's not. Should we add a note about this (and possibly the filepath := scanner.Text() part below) to the PR description?\n. \"&\" is added\n. How come we're getting rid of this test? Are null characters still supported? \n. I'm not sure how much work this is to change, but some advice I've gotten in the past is not to use cryptographic hashes if you don't need irreversibility, because they incur a performance hit.\nAlso, do pachyderm repos include a version number? If so, we can change our hashes later by providing clients with a pipeline that migrates data from an old-version repo to a new-version repo, but if not, we may at some point be stuck with whatever hash function we pick (because a customer has a bunch data in an existing pachyderm cluster, keyed by a particular hash function, and they don't want to lose it). I think it might be worth double-checking with JD how he feels about that risk.\n. Interesting. Admittedly, the context where I heard this was for bloom filters, which a) use k hash functions, and b) are only used in performance-sensitive contexts. I guess it is hard for me to imagine that the cost of the hash is higher than the cost of fetching the file over the network.\nMainly, I am curious if we have any kind of versioning for repo formats, though (or if we think we'll add one before we get any big, inflexible users), because if the answer is yes we don't even really have to worry if this is exactly the right function or not.\nSecondarily: I think it is possible to turn two small hashes into one big hash by using double hashing to generate e.g. 4 distinct 64-bit hashes and then concatenating them.\n. Done, though I picked \"pipelineManifestReader\", let me know what you think.\n. Done\n. I agree that these spurious diff lines hurt readability and that we should have company policy for this stuff. The easiest solution here was #924, and once that's submitted these spurious diff lines will go away.\n. :+1: \n. Fixed. I just got rid of this section (I don't pass this string to pachyderm, I was just using the file that's already in this section)\n. Done.\n. Done.\n. That sounds good to me.\n. nit: Instead of numbering these 1-4, what if we put the parameters in the test name? e.g. TestPipelineInputMapMap, TestPipelineInputReduceMap, TestPipelineInputReduce_RepoDiff, TestPipelineInputReduceReduce? (or something like that). I think that might make these tests more comprehensible when you're scanning through the file.\n. Would you be okay with putting a comment at the top of this method? Doesn't have to be super long if you don't want, but just so people reading through the file recognize that this is called for a bunch of tests and maybe a little bit about what it does.\n. Just to aid my own understanding, are this and etc/kube/registry-rc.yaml the same? If so, why define them both?\n. I fixed this in src/server/cmd/cmd.go and regenerated the docs.\n. I don't actually know (as we discussed, I just got this PR by running make doc. I created https://github.com/pachyderm/pachyderm/issues/980 to track putting this into the documentation, though.\n. Sure. make doc seems to want to remove it (not sure why, but I don't know much about make doc), but I got around that by running make doc and then checking out this file from HEAD~1\n. I can't speak to the high-level question, really, though this doc won't have to be manually updated, as it's autogenerated by make doc (c.f. the comment ###### Auto generated by spf13/cobra on 12-Oct-2016 at the bottom of the file)\n. I'm not sure, to be honest. Created https://github.com/pachyderm/pachyderm/issues/981\n. It looks like what may have happened here is that we edited some auto-generated documentation. When I originally regenerated the docs, the added comments got squashed. Now that I've run make-docs again, at least some of this stuff appears to be back.\nI think (though will have to check with either Sean or JD, depending on who set it up) that the source for this is doc/development/pipeline_spec.md, so if there's stuff we want to change here, I gather we'd change it there and then re-run make doc\n. We discussed this f2f this morning. Basically, this is similar to create-branch + start-commit except that that combination creates a commit that's in two branches. Now, create-branch doesn't exist, and fork-commit satisfies the same use-case\n. Okay, this should be included now\n. Oh, ez. Fixed.\n. Fixed, I think. Let me know what you think. Same. Do we need one for statefulsets (in case anyone uses that)?. The inspiration came from grpc (https://godoc.org/google.golang.org/grpc/codes and https://godoc.org/google.golang.org/grpc#Code). If you wanted to do something similar with these errors, you could do:\nswitch Code(error) {\n// deal with codes\ncase Unknown:\n// not a hashTreeError\n}\nBut I can easily make Code an exported field. If you think that's better, let me know and I'll change it.. Nope, I don't think so. Simpler seems better, and they both already had the same implementation in deleteNode anyway. Fixed!. At a high level, no (for now I just wrapped the original impl in a for loop) though if we're merging once per datum, that seems like it could be a lot of merging, and it may make sense to to avoid all hashing until all the trees are merged (right now it rehashes once per tree, which is once per datum).\nIn pfs v2, is 1 datum == 1 file? Or will we be merging several times per file?. 1. it is not necessarily (though it is in e.g. DeleteFile)\n2. it makes sense to proceed because you might want to create a node at pnode\nThe reason visit works this way is for e.g. PutFile(\"/foo/bar/buzz\"), where \"/foo\" and \"/foo/bar\" both need to be created. The update fn in PutFile works by detecting the nil pnode and creating a directory.\nDoes that make sense?. Really good point. I limited this to 10 errors. I know that this pattern is what I used for ParallelismSpec, but it may be cleaner for users writing config if you omit ConnectorType and just detect which one of object_store and sql_db are set. Then the config will just look like:\n{\n   ...\n   \"output\": {\n      \"sql_db\": {\n         ...\n      }\n   }\n}\nI wish I'd done parallelism spec this way. . Ah sorry, I thought I replied to this. The answer to both is that this is used when you call PutFile(/path/to/file) and \"/path\" and \"/path/to\" don't exist yet. In that case, the update() function passed by PutFile to visit will create the missing directories.. * I think that returning an error is the right thing to do here (which I think means I agree...)\n\nNope, not really. I'll submit a PR to start ignoring the strategy field.. Thanks! :rocket: . Following up with our slack conversation, this has been fixed.\n\nFor posterity: I hadn't thought through how bad the performance would be with the naive approach, but it turns out it's really bad (like 1.5h to merge 100k trees), so I've added a benchmark to track the performance of merge, and made sure that it's a linear (or near-linear) operation and it now finishes in a few seconds. Do you want any kind of throttling for this? (like the semaphore in PullSQL)?. Why is this reading from sem deferred, instead of done at the beginning? If we're using sem to throttle, could you end up in a situation where you call eg.Go() on N objects, and all N functions passed to eg.Go() are run (flooding the object store), and then all of the functions compete to exit, but they've already done all of their reading so they're just taking up memory?. No you're right. I kept the interface the same in this PR so that I could refactor the code without changing the tests (the idea is that the tests have already been debugged and we trust them, so it's easier to trust that the new code is right). Next PR changes tests, but the code change is smaller because a lot of it was done here (i.e. canonicalize() already exists, and you can be pretty sure it's right because it's passing tests now).. Fixed.. Fixed, it now returns err inside the if block. Fixed this too.. Fixed as well. Fixed\n\nIs that whatever that's inside the b.N block?\n\nyup, that's it. Deserialize is exported, but it's not part of the HashTree interface (otherwise you'd have to create a finished HashTree somehow, and then call h.Deserialize(bytes) to replace this contents, instead of h := Deserialize(bytes). Per our discussion, I changed Serialize() to be package-level (now it's Serialize(h) instead of h.Serialize()). Yeah, that makes sense. This hasn't seemed too flaky so far, so yeah, hopefully it'll stay that way.. Sounds good, though per our discussion, you implemented this :-). How come PPSHostPath is in client? (vs. server, since it seems like we want to hide it from clients). These log messages are on purpose. The goal was for users to be able to see that pfs saw a datum and skipped it because it had already been processed (vs did not see a datum because it actually isn't in PFS/there's a bug in our code).. I just moved it back into pps/server/api_server.go. It still exists.. Not this one, sorry. This I was using for debugging, I think I just missed it. Removed.. Just added a few. Let me know what you think. 3 minor suggestions:\n1) you could write the struct like this\ntype Puller struct {\n  sync.Mutex\n  pipes map[string]bool\n  errCh chan error\n}\nthen later just do:\np := NewPuller()\np.Lock()\n2) Would you mind putting a comment on pipes? (e.g. // pipes[<path>] == true iff <path> was never read and a goroutine is blocked on it.)\n3) Would it make sense for pipes to be map[string]struct{}? It looks like you're just using it as a set, and IIUC, struct{} saves a little memory over bool. It's a little more awkward to set values, though, so I really don't feel strongly about this. I like this compromise (though I'll suggest we implement it by adding a json field rather than a prefix. E.g. \"source\": [\"pipeline_code\" or \"worker\"]. You might even choose to filter out pipeline_code logs if they're verbose and you really care about whether a worker is downloading data or not). Would it make sense to check jobs[0].JobState == STOPPED?. Would it make sense to factor this out? Like:\npvcTemplates = []interface{}{\n  ...\n}\nthen\nif backend == googleBackend || backend == amazonBackend {\n  pvcTemplates[0][\"metadata\"][\"annotations\"] = map[string]string{\n    \"volume.beta.kubernetes..\": etcdStorageClassName\n  }\n}\nAlso, if I understand this code, all it does is make sure that the volume.beta.kubernetes... label isn't set if you're not using those two backends, is that correct? Is the idea that you'll get dynamic provisioning in azure as long as you don't set that label?. Do we check that exactly one of --dynamic-etcd-nodes and --static-etcd-volume are set elsewhere? Just want to make sure that this code doesn't run if both are set (and that we emit an error instead).. rats :-P. My guess, just from reading this, is that when this value was long, it made tests fail because pachd would turn down pipeline RCs, and then turn them back up later, and the old (now invalid) IPs were still in etcd. Was that the motivation or was it something else?\nwould it make sense to leave a note here or something, so that if the issue appears in a customer's production cluster it's not too unexpected?. nit: how would you feel about a slightly different name, e.g. watchJobForCompletion. I just kept thinking that calling this function would signal that a job was completed immediately (in pipelineManager).\nDon't worry about it if you prefer the current name.. Just to make sure I understand this code: is this part of the scaling change, or just more general refactoring? (just because once I figured it was general refactoring, I didn't read it too closely). Yup, no problem. Done. Happy to change this. Yup, done. Fixed. Just trying to understand the change:\n- My understanding is that the goal is to make sure any Read() calls block until all pending writes are finished. Is that correct?\n- Is that accomplished here? It seems like this is more or less equivalent to what we had before (delegating to w.pipe.Write, instead of returning w.pipe so that users are calling Write() on it directly) but I'm not sure I understand the mechanism here.\nThanks again for your help!. I think close is similar, in that Close() also delegates to w.pipe.Close() (which is equivalent to returning the pipe and letting users call Close() on it directly). This version will propagate any errors that were encountered by the PutObject() call in Write(), which is a definite improvement, but I'm just trying to understand how that would affect concurrent Read() calls.. Got it; I misunderstood the explanation the first time, but now I'm all set. Thanks!!. Not really. I think in most places with timeouts we were waiting 5s, but that seemed a little short to me. But I'd be happy to use that.. Drive-by comment: Does k8s support GPU sharing? If so, should we make this a float?. Per f2f discussion, just log the error in pachd before continuing. Just to check: does this work for directories? I took a quick look at the docs for ModeType, and it looked like\nModeType = ModeDir | ModeSymlink | ModeNamedPipe | ModeSocket | ModeDevice\nso I just want to check that users won't get 'cannot upload special file' just because the output contains a directory. Do we have tests that output directories?. Got it, thanks. Somehow missed the return the first time.. Okay. I think I handled this correctly, though let me know if there's more I need to do. I'm going to try creating a kops cluster in us-east-1 and us-west-1 to test it.. Done. Yeah I agree. I'll write something short.. :-D. Done. Should we be using this library everywhere? Or is it just useful in this particular package for some reason?. That is the intent of this PR (and is currently all that make run-bench runs). Long-term, it might well make sense to run more, but I think limiting those to src/server is useful for now, since I want to get the nightly build to be useful asap. Yup. This will capture the case statement, but it'll also print out the command that's actually run, which is the goal. What would you think of using MustParse? You're parsing a constant, so it seems equally safe.. :+1: . Yup, it was a typo. Could you add a short comment to the top of create_s3_bucket indicating that it takes two arguments, and what the second argument means? (the first is the name of the s3 bucket being created). This is entirely a matter of aesthetics, but how you feel about:\ncmd=( pachctl deploy amazon ${BUCKET_NAME} \"${AWS_ID}\" \"${AWS_KEY}\" \" \" ${AWS_REGION} ${STORAGE_SIZE} --dynamic-etcd-nodes=3 --cloudfront-distribution ${CLOUDFRONT_DOMAIN})\nif [[ \"$USE_CLOUDFRONT\" == \"true\" ]]; then\n  cmd+=(\"--cloudfront-distribution ${CLOUDFRONT_DOMAIN}\")\nfi\nif [[ \"${METRICS_FLAG}\" == \"false\" ]]; then  # Already there...\n  cmd+=( \"--no-metrics\" )\nfi. My memory is that passing an empty flag is not the same thing as passing no flag, so if this doesn't work, you may need to rely on a pattern like\ncmd=(\"${aws_sh}\" --region=${REGION} --zone=${ZONE} --state=${STATE_STORE} --no-metrics)\nif [[ -n \"${CLOUDFRONT}\" ]]; then\n  cmd+=(\"${CLOUDFRONT}\")\nfi\nsimilar to above.... I believe the leading space is unnecessary. The way bash arrays work is the parsing is done before the array is created, so by appending this to the array using this syntax you don't need a separator (as opposed to e.g. cmd=\"${cmd[*]} ${CLOUDFRONT}\"). To expand the array here, I think you'll want to use:\nsudo \"${cmd[@]}\". That syntax expands cmd to all of its elements and quotes them, and I believe is generally the way people forward arguments. It's the same as \"${@}\" if you've ever used that idiom before). I apologize if this is done already and I just missed it, but does pachctl deploy have to be modified to add this extra data to the amazon secret?. Fixed. Yup, just added this check (and a test). Let me know what you think.. It's redundant. On line 475 we already have:\nif request.Pipeline == nil && request.Job == nil {\n  return fmt.Errorf(\"must set either pipeline or job filter in call to GetLogs\")\n}. I adjusted InitialInterval too, since IMO 500ms backoff is also too slow for interactive RPCs.\nI can move this out if you want, but there are a bunch of places where we don't retry and should, and I think we should be using this in most of them. I think I'll want to add this back at some point, if not in this PR.\nIf the name seems bad, I'd be happy to call it something else? NewInteractiveBackoff or something?. Done. Since the script already sets AWS_REGION, I think it might be simpler/safer to replace ${region} with ${AWS_REGION} everywhere and delete this line (really this is a bug in the original script). Nope, good catch. Fixed. Done. Good to know, thanks! I just added defer cancel(). The rationale is that it makes replacing \"configV1\" much less awkward (keep in mind that the name \"ConfigV1\" won't appear in peoples' configs. Just the field name \"v1\")\nBasically, suppose at some point in the future we want to change the structure of configs (what I had in mind was changing it so that a pachcfg had all of a user's clusters, and then a current-cluster that points at one of them, similar to kubecfg). We can't remove the existing fields from the proto, because that would cause peoples' existing configs to fail to parse (they'd have a bunch of unrecognized field names). So we'd have a bunch of fields in the top-level message with complicated relationships to each other (in this example, pachd_address, repeated pach_cluster and current_cluster where either the first is set or the last two are, and if either of the last two is set they must both be set)\nIn the long run, we'd have a lot of fields that don't make sense because they're archaic. By having explicit versions, we'd have several different versions of the config message, all of which are internally consistent, and exactly one of which is set.\n(that's the rationale, at least). done. I may be misreading the code, but I think we need to check parent scopes here. In other words, if acl.Entries[user.Username] == OWNER and req.Scope == READER, then authorized should return true.\nWe can leave this as a TODO if you want, but I'd like to at least leave a note or something. I think we need to look this up from etcd in NewAuthServer. Right now if pachd restarts (e.g. because it's evicted) after the user has enabled auth, the restarted node will think auth is disabled and that node won't be doing any auth checks.\nI think we can just check if the admin collection has any users in it?. I think it's cool that we include this in the User response, to save an etcd read. One risk though (maybe we just need to tell users, so they expect it?) is that if you revoke someone's admin access, they won't actually lose it until their token expires, which may be 24 hours later.. As discussed, we're leaving this problem alone for now. The idea was that only the owner of a repo could get the ACL for that repo, but any user could check if they had access to it. Sure, done. I actually forgot that I'd removed them.. Nope, this has been removed\nAlso, per discussion, https://github.com/pachyderm/pachyderm/pull/2070 means that there is no keepalive function in the client anymore, so we don't need to keep a special context we can cancel.. Done. Whoops. Fixed (and tested manually). Do you think it might make sense for the auth token to be owned by the worker APIServer rather than the client? It feels similar to the reason that we didn't want to re-use the same context for all requests in the client.\nIn particular, pipeline workers will always use the same auth token, and pachctl will always use the same auth token, but PFS and PPS won't (recall that e.g. PFS checks that the caller has READER access to the repo in GetFile, and PPS checks whether the caller has access to all pipeline inputs in CreatePipeline). PPS and PFS could call SetAuthToken before every client call, but at that point it seems like it should be a function parameter.. Per our conversation, the way the code is written now makes the most sense. All of the functions in src/client/pfs.go and src/client/pps.go don't take a context, so either we have to change all of those functions and all of their callers (which includes not just Pachyderm, but also external users of the Pachyderm client library), or we add SetAuthToken to client.APIClient to make those heavily-used functions continue to work with Pachyderm clusters that have the authorization system enabled.\nAlso, while pfs and pps make calls to the auth API and the object store, the only place where pfs and pps call each other (that I recall) is that pps.CreatePipeline() calls pfs.CreateRepo(). It's possible for pps to use the pfs grpc client directly instead of the convenience function in that context (incidentally, calling SetAuthToken() in that context actually won't work, because client.APIClient isn't thread safe), so rewriting all users of client.go just because CreatePipeline() can't use client/{pfs/pps.go,pps/pps.go} doesn't make sense.. Good catch, done. Done. Nope, done.. Done.. Done.. Done. Done. I added a new backoff type for tests and used it basically everywhere in pachyderm_test. Same change. Same change. so incoming -> outgoing is actually the main thing this does, and it's necessary because of how grpc-go is implemented. They way the RPC side-channel works in grpc-go is that the go context library already has a ctx.Value() method, and grpc-go has a special key that maps to data it needs to include in the grpc side channel: https://github.com/grpc/grpc-go/blob/master/metadata/metadata.go#L119\nWhat this method does is take auth info (which is in the image of mdIncomingKey{}) and puts it under mdOutgoingKey{}. The fact that metrics and stuff is cleared is incidental, but I wanted to warn people in the comments so nobody would be confused.. Fixed. for sure, fixed. This was another mistake\u2014I though col.NewSTM would retry if the function returned an error, but I read the code and realized that it only retries if the transaction fails to commit. So now it does just return an error.. Yup, revised these. After rereading it I realized that all I actually needed was !user.Admin && acl.Entries[user] < OWNER. I forgot that in Go you can access members without dereferencing. Fixed this too.. Fixed. It's kind of aesthetic, but if you pass a []string to a function that expects a []interface{} you get an error. You can't convert it either, so you have to make a new []interface{} and copy all of the elements. Makes sense, done.. Why change all of these? Should we just delete pfs.Repo altogether?. I'm not sure if we need this check--deleteRepo already checks if the caller is an owner (if we can get rid of it, we can also set includeAuth to false above). Per our discussion, it's to avoid a dependency cycle between pfs.proto and auth.proto. Feel free to ignore this.. Minor design suggestion: You might be able to get rid of --paginate. Basically, if --page=p then paginate and return page p. Otherwise, don't paginate. Minor suggestion: embed this in the if statement below?. Maybe open an issue for this, if there isn't one already?. Interesting. I'll revert it, since it's not relevant to the substance of this PR. I thought we did migrations using the pfs api now, though?. very minor suggestion: you could also write this as metadata.Pairs(auth.ContextTokenKey, cookie.Value) and it might be a little shorter. Oh, no I meant to use activate for the command too. This is fixed.\nThis was a change that I made at one point when the enterprise token RPCs were still in the auth API. But I eventually decided the distinction was confusing, and put enterprise token stuff in its own API. So now there's auth.Activate and enterprise.Activate, but I forgot to change pachctl back.. I think I picked enterpriseClientInfo because client is already a field, and I thought client.client might be weird. But it obviously doesn't matter\u2014fixed.. Better yet, I fixed the test :grinning: . fixed. Very minor suggestion: I think Walk(\"/\" func()) might be slightly easier to read (as in, \"/\" suggests that the argument is a path and you're starting at the beginning).  Totally your call on whether it's worth changing, though. Fixed. Fixed. Fixed. Fixed. Fixed. Fixed. There are a few places where it's still false, notably createRepo (https://github.com/pachyderm/pachyderm/blob/9aa9641f1d4279ed6664e54874463e4f37d5b739/src/server/pfs/server/driver.go#L310) and deleteAll (https://github.com/pachyderm/pachyderm/blob/9aa9641f1d4279ed6664e54874463e4f37d5b739/src/server/pfs/server/driver.go#L1902). This is fixed by https://github.com/pachyderm/pachyderm/pull/2289. How would you feel about adding a GetAddr() method to client.APIClient and using that, rather than 127.0.0.1:30650? The downside to having a hard-coded address here is just that it only works with pc port-forward, but doesn't work with ADDRESS (which incidentally means it doesn't quite work in my testing environment, though that's something I could easily fix).\nThe downside to GetAddr() is it widens the interface to client.APIClient\u2014your call. Per your observation: this is a leftover debug statement. nit: IIUC, go's HTTP library has a convenience function for this: https://golang.org/pkg/net/http/#Error (i.e. http.Error(w, \"malformed JSON\", http.StatusInternalServerError)). You actually will be able to log in via the UI/dashboard, that's one of the features we got in. It is a little awkward: you click a \"log in\" button, click \"allow\" in github to give us access to your profile, and then are redirected to a page where you have to copy/paste an access code back into the UI. But it does work. Yup, owner > writer > reader (having a role implies having the roles below it). Per discussion: you wanted to limit the use of flushWriter to GetFile (http.ResponseWriter normally buffers the response body in memory, which doesn't work for GetFile where the body might be very large). It's not really necessary in calls that only return a tiny amount of data). Note for posterity: Cookies set by /auth/login don't apply to /pfs/... unless you set path=/ in the response. One more nit: could you use http.Error(w, \"route not found\", http.StatusNotFound) here?. It looks like we preserve Cron repos if request.DeleteRepo == false. Just checking: is that deliberate?. Ah, whoops, thanks. Got rid of these. git diff origin/master | grep 'fmt.Print' didn't find any leftovers, so I think this should be good now. Fixed. No. It was kind of random, but I got rid of it (after reading back through the PR). That sounds good. I added a section at the top called \"options\" that explains the two paths. The reason I started editing this doc is that the customer I was working with missed this option, so I think some added emphasis will avoid that going forward. Yes, another solution could be to add --porcelain to pachctl version so it just spits out the client version and exits. On reflection, that does make it easier to look at an image tag and know how old it is (since you'd see pachyderm/pachd:1.4.5-abcdef in dockerhub, instead of just pachyderm/pachd:abcdef) so I'll go ahead and implement that.. I agree, and I did try to make this work with grep. The problem is that grep has no \"passthrough\" mode (where it echoes all text passed to it. I also learned that we're not the only people annoyed by this).\nag actually does (it has --passthrough) but I found that it was hard to debug these tests when I used ag that way because it gave no indication what the problem was (e.g. \"expected to find 'target text' but did not\"), so you don't really know what went wrong in a long block, just that something failed somewhere. I also considered writing match in bash using ag to give us this kind of output, but I figured that since I'd need to install ag in travis anyway, having something slightly nonstandard would be fine (and easy to change later in any case). Less than it looks like. \n  1. Most of the deleted tests are for internal functions that we don't need anymore (since thesese tests aren't calling the cobra command, but actually the pachctl binary itself. As well, all of the BE_CRASHER logic is gone) and job-related calls (e.g. CreateJob) which don't exist anymore.\n  2. Since this was calling job-related commands that don't exist, it actually didn't compile, so strictly speaking, we're not losing anything\n  2. One test that it makes sense to keep and fix rather than remove is TestPushImage but that just tests that pachctl update-pipeline --push-image, which I figured wasn't so urgent that we couldn't fix it later. Any chance we could add like\n// GetBlock blocks until the next write to 'key', and then returns the new value associated with 'key'. Does chunks not need to be restricted to the job? How does the worker avoid claiming other jobs' datums?. Could we add // Try to acquire a lease for chunk no. 'high'?. Also, just to confirm: this uses one lease for k documents, right? (where k == whatever the size of a chunk is.) If so I'll update the design doc. If I understand this code correctly, I think this might return an error even under relatively normal conditions: two workers trying to acquire the same chunk at the same time. I think if this fails, instead of returning an error from acquireDatums, we just need to go back and look for another unclaimed chunk (also it might make sense to add a small, random sleep to the beginning of this \"find unclaimed chunk\" cycle, to reduce the incidence of these kinds of collisions). would it make sense to add a goroutine here that renews the locks document until process returns, so that another worker doesn't claim this chunk while we're working on it?. Got it, thanks! Could we just make the comment for chunks be something like // chunks contains a document for each job indicating its progress?. Ah, just realized that this was our implementation of watch, not etcd's. (counterargument: if this worker is stuck for some reason, but its goroutine keeps updating the lease indefinitely, then the job will never finish). You're right\u2014I just checked the collection implementation and the only times PutTTL returns an error are:\n- if you set the keyCheck function in the collection (which we don't)\n- if you fail to create the lease (which isn't expected)\nIt doesn't include the random sleep, but that may not be so important anyway. any chance we could add a top-level comment that's just like // worker watches for new JobInfos, then 1) reads new jobs' chunks, 2) claims chunks withacquireDatums3) processes them withprocessDatums(passed toacquireDatums)?. I know none of this code is new, but I find it a little confusing that these blocks run bottom-to-top by virtue of being deferred. Probably something to fix later if ever, but still. I think breaking up runJob will make this much easier to read\n(clarification: this isn't a request\u2014it just looks like you broke up runJob). Should we delete all of this commented-out code?. This is actually the version check :-). Basically, I needed a way to tell if e.g. \"1.7.1\" was greater than \"1.6.9\" in bash, so I wrote hilarious, sad hack in which I strip the leading \"1.\" and then echo an expression to \"bc\". So this becomesecho \"7.1 > 6.9\" | bcwhich then prints \"0\" if it's false and \"1\" if it's true. Done, though I left the flag onfinish-commitas--messageand-m(to match Git). Let me know if you think I should change those as well. Done. Per our discussion:\n---messageand--descriptionare now both flags onstart-commitandfinish-commit`\n- Those flags are synonyms of each other. Do you still want to fix this?. Does \"pachd service\" mean Pachyderm service (as opposed to Pachyderm pipeline)?\nAFAICT, this isn't the usual code for creating a Pachyderm service in kubernetes, though, right (e.g. is there a PipelineInfo doc in etcd)? It looks like we have a special codepath for git hooks, distinct from other Pachyderm services, which I think that's totally fine, but I just want to make sure that a) that understanding is correct and b) that behavior is intentional. It looks like the PPS master creates the githook service as well\u2014does it make sense to try an do it here too? Particularly since IIUC kubernetes interaction doesn't generally happen in-line in PPS requests (like I think we discussed at one point). IIUC, if you're running Pachyderm locally, the Githook URL never appears, right? Would it make sense to call this state \"N/A\" or something like that?. Would you be okay with changing the error to something like \"Expected 1 githook service but found %d\"? It's just easy for me to imagine some user with a stray service in their cluster yelling \"what do you mean not found! It's right here!!\" ( that user might be me...soon). I know this is superficial, but would opts.RBAC be clearer than !opts.NoRBAC?. Similar to above, would BoolVar(&rbac, \"rbac\", true, ...) be clearer than BoolVar(&noRBAC, \"no-rbac\", false, ...)?. I know that booleans in general default to false, but since you can set the flag default to true, and you set the option directly from the flag either way, I think you can just flip both (and change false to true on this line) and it'll work.\nOTOH it does make the options struct a little weirder if you were to initialize it somewhere else (another possibility would be to change the flag from no-rbac to rbac and leave the options field the same), and like I said, this is all pretty cosmetic, so I'm on board with either. How come this block got moved farther down?. Just to update: I did write this test and it exposed a bug that I'm currently working on fixing. The bug is:\n1. deleteCommit calls createBranch (to update branches pointing at now-deleted commits)\n2. createBranch(branch) may inspect the HEAD commit of upstream branches (of branch) while deciding whether to create a new HEAD commit on branch\n3. Those upstream branches may not have been updated yet (i.e. may still point at deleted commits) causing createBranch to error\nI could sort branches by provenance before updating them, but since I actually don't think we want deleteCommit to ever cause any pipelines to run, I'm modifying deleteCommit so that it doesn't call createBranch. I'm still fixing a few bugs in this branch after this change, but it's nearly done.. Done. Done. I actually left this in on accident (when I was trying to debug a test failure that wasn't extracting the caller correctly because it was used inside a defer) but if you like it then sure, I'll get rid of the file/line log. Done. I think it's the size of the whole tree (driver.go:784) but yeah I agree that we could. I think it would be pretty easy\u2014should I make that change too?. I don't know if I totally understand this comment yet, but just as a quick note: I don't think PPS creates commits anymore (except in the spec repo), so I actually think deleting a commit with provenance should be stable, even if it's also conceptually invalid. One problem you might have is a branch with a HEAD commit where the branch provenance and the HEAD commit provenance don't match (e.g. branch has 3 inputs, HEAD commit has 2). But this can happen even if you prevent people from deleting commits with provenance. Cool, done. Yup, I think so. I originally thought of putting the list of branches in RepoInfo, so that you could retrieve the list without calling col.List(), but a) that might perform badly for the spec repo, which has a large number of branches, and b) doesn't help with rewriting ParentCommit.. I understand the commit ranges comment now. I haven't merged that PR yet (so that will add some work to the end of this PR) but I agree that it'll make the code simpler and less risky if we add this constraint, even if it ends up being easy to remove, so I'll add it. Makes sense. Just to update this thread, since it has to do with updating subvenance, I actually don't think there's a great way to avoid slightly gnarly subvenance code here. I was trying to figure out if two commits' subvenance always share a boundary (i.e. A.subvenance[i].upper == B.subvenance[j].lower), and one case I came up with that illustrates the need to rewrite subvenance (and which I'll add another test for) is this:\n```\nrepo_1: repo_2: repo_3:\n\nA       X       U\nB       Y       V\noutput:\nBVY\nBVX\nBUX\nAUX\n```\nNote that if I delete B, then I have to delete BUX through BVY, which means updating the subvenance of commit X, even though both X.subvenance[0].lower and X.subvenance[0].upper don't equal BUX or BVY. (One invariant I did come up with, which helps with drawing these diagrams: every output commit is the lower of a range in some upstream commit's subvenance.\nThis is because output commits are only created when new inputs commits are created, so the input commit that caused the new output commit will have the output commit as a lower somewhere). Leftover debug statement? We could make this e.g. \"could not inspect <repo>/<commit>: <err>\". Why iterate through upstream branches here? If the goal of this function is to create a new commit in downstream branches, it seems like we'd want to leave upstream branches alone?\nThere's also a comment below this (that I can't get GitHub to show for some reason) saying we should get rid of branchToCommit\u2014does this code affect that comment?. I'm afraid I don't understand this code\u2014it looks like it's essentially copying branchInfo.DirectProvenance into branchInfo.Provenance, but shouldn't it already be there?. would it make sense to put these casts in helper functions? e.g.\n```\nfunc directProvenance(branchInfo pfs.BranchInfo) branchSet {\n  return (*branchSet)(&branchInfo.DirectProvenance)\n}\nfunc provenance(branchInfo pfs.BranchInfo) branchSet {\n  return (*branchSet)(&branchInfo.Provenance)\n}\nfunc subvenance(branchInfo pfs.BranchInfo) branchSet {\n  return (*branchSet)(&branchInfo.Subvenance)\n}\n```\nThen\na) these casts might get shorter, e.g. directProvenance(branchInfo).add(...)\nb) You can't accidentally cast the wrong thing to a branchSet (which unlike generic branch slices, must be sorted). I'm still working on understanding \"pre-existing subvenance\" (and this code in general), but updating a branch shouldn't change its subvenance, right? This function takes a provenance argument (which, btw, it might make sense to call directProvenance now), so upstream branches' subvenance may change, but this branch's subvenance (and downstream branches' subvenance, which are the ones in toUpdate IIUC) should stay the same.. Leftover debug statement. would it make sense to add a little helper function (e.g. at the top of the STM) that's like\ncol.NewSTM(... func (stm STM) error {\n  getBranchInfo := func(b *pfs.Branch) (*pfs.BranchInfo, error) {\n    return d.branches(b.Repo.Name).ReadWrite(stm).Get(b.Name, provBranchInfo);\n  }\n})\nand then all of these would be\nif branchInfo, err := getBranchInfo(provBranch); err != nil {\n  // do stuff with branchInfo\n}. I'm still thinking through this code; is it possible that for a given oldProvBranch, there's still a path from oldProvBranch to toUpdate[i].Branch, but it just doesn't go through the edge oldProvBranch -> [branch from the argument]?\nIf so, it might make sense to add a test for this. I actually think we might need to write this part by putting all the branches in toUpdate into two data structures: a stack and a queue. You pull all the branches out of the queue to update their provenance, and then you pull all the branches out of the stack to update their subvenance. We can talk about this in person too, because I'm not 100% sure yet.. Do we need to call InspectCommit on these? I.e. if someone pc list-job -p mypipeline -i master should we try to convert master to an ID?. This is now implemented. Let me add a test for this. I'm actually not sure if there's a solution to this that doesn't involve making createBranch a batch call (i.e. update a set of branches, then create a new \"top layer\" of commits if there are unprocessed inputs). Nope, I meant to remove it in the other PR too, so I'll definitely kill it here. makes sense to me :+1:. Done. Done. Not directly, but if you delete an input commit, and then that causes a downstream output commit with multiple inputs to get deleted, the other input commit needs to be have its subvenance updated. Nit, but what would you think about panicking here, like in commit_invariants, so callers don't have to deal with an error?\n(for reference:\nfunc (a *apiServer) getPachClient() *client.APIClient {\n    a.pachClientOnce.Do(func() {\n        var err error\n        a.pachClient, err = client.NewFromAddress(a.address)\n        if err != nil {\n            panic(fmt.Sprintf(\"pps failed to initialize pach client: %v\", err))\n        }\n        // Initialize spec repo\n        if err := a.pachClient.CreateRepo(ppsconsts.SpecRepo); err != nil {\n            if !isAlreadyExistsErr(err) {\n                panic(fmt.Sprintf(\"could not create pipeline spec repo: %v\", err))\n            }\n        }\n    })\n    return a.pachClient\n}\n). Second question, actually: if passing a commit ID to BuildCommit lets you create commits with nonexistent parents, then why topologically sort commits when extracting them?. This comment should be updated. Per our conversation in person (but recorded here for posterity): instead of having version be an Op the plan is to have different Op types for each Pachyderm version, and a layer of indirection between Op and the various versioned Op1_xs.\nThe motivation is that different Pachyderm versions may have different APIs, and if we have different Ops for each, then we won't have to worry about deserializing 1.x ops into pachyderm 1.y API messages (which this PR already has to work around in a small way, in CreateBranch).\nFor now, the plan is:\n```\nmessage Op1_7{\n  pfs.PutObjectRequest object = 1;\n  pfs.TagObjectRequest tag = 2;\n  ...\n}\nmessage Op {\n  Op1_7 op1_7 = 1;  // always set by Pachd 1.6 and 1.7\n}\n```\nThen in Pachyderm 1.8, we'll move 1.7 protos into their own directory, and this would look something like:\n```\nmessage Op1_7{\n  pfs1_7.PutObjectRequest object = 1; // use archived message definition w/ same field numbers\n  pfs1_7.TagObjectRequest tag = 2;\n  ...\n}\nmessage Op1_8 {\n  pfs.PutObjectRequest object = 1;  // Use new versions of each proto\n  pfs.TagObjectRequest tag = 2;\n  ...\n}\nmessage Op {\n  Op1_7 op1_7 = 1;  // always set by Pachd 1.6 and 1.7\n  Op1_8 op1_8 = 2;  // will always be set by pachd 1.8\n}\n```\nThen when 1.8 deserializes a stream of ops, it can be sure to deserialize them correctly and then convert a stream of 1.7 ops to 1.8 ops if needed. I think this comment may need to be updated. What do you think of using underscore to disambiguate major/minor version (i.e. Op1_7)? I was going to confess that it was unconventional, but I actually think the go linter has an explicit exception for this kind of thing: https://github.com/golang/lint/issues/47 (which, despite the skeptical comment, is fixed by https://github.com/zimmski/lint/commit/2481c997ffa02332f21d9cac2e5e2b3907a51be0). Just checked, this is the only place. per our conversation (recorded for posterity):\n pipelineTokens contains non-pipeline tokens as well as pipeline tokens. Not all elements have TTLs (most don't\u2014particularly, all tokens requests by PPS, which are the tokens actually held by pipelines, don't)\n pipelineTokens is a huge hack to make sure that pipelines don't break when auth is disabled and then reenabled\n* The collection is going away in the other pre-1.7 auth PR. Ah yes, thanks. I actually rewrote this section of code so that the token and pipelineToken case run in parallel, so the distinction should hopefully be clearer, but I'll check that I didn't make this mistake twice. I started doing this to getXClient() methods in tests, after doing it in files that create a client once and then cache the error. When getXClient() simply caches an error and returns it in every subsequent invocation, there's no way to recover from the error, so returning it instead of panicking seemed pointless. I subsequently changed pachyderm_test.go and driver/server_test.go, because those tests never do anything with this error, and making it so that getXClient() doesn't return an error makes every test shorter (and means that you can do getEtcdClient().DoEtcdOperation() if you only need the client for one thing)\nIf you disagree, I'm not married to this opinion and I'm happy to change it back, certainly within this file. Whoops, leftover note to myself\u2014fixed. fixed. Ah, this is a little bit of a hack, but Get and fetch do the same thing. Basically, if you look at the implementation of runSTM, if one of these calls panics with an stmError, then runSTM recovers from the panic and passes the error to  the caller.\nBasically since we invoke transactions like this:\ncol.NewSTM(ctx, etcdClient, func(stm STM) error {\n  ...\n  myCol.TT(key) // might break such that we can't commit the txn, but how to communicate that?\n})\nthe way this communication happens is NewSTM calls this function, which calls myCol.TTL(key). Then myCol.TTL(key) panics, NewSTM calls recover(), and then returns that to the user without committing.. why delete this reference? Does it break start-vault?. I think so\u2014I moved it into a separate goro, and instead of returning an error, the goro panics if it can't initialize the PPS token within 60s. Let me know what you think. Ah, so the idea is that when users call auth.Activate(), it \"activates\" auth with one administrator: the magic user/pps user (it's impossible to log in as the magic user, but auth.isActivate() will return true). At that point, any calls that do auth checks will fail (repos have no ACLs, and there is only one cluster administrator), but pps will still be able to do things. If anything afterwards fails, you can just keep retrying auth.Activate() until pps.ActivateAuth() succeeds, at which point auth.Activate() makes the caller a cluster administrator and the cluster is fully auth-on. My main concern here is that I don't know if this'll work across RPC boundaries (which, at least for now, is part of what this library is supposed to do, and why it's in client).\nLike one example of how this works now is that all of these IsXYZError methods use strings.Contains() instead of strings.Equals(). Then, if the method gets a wrapped grpc errors (where Error() yield rpc error: code = Unknown desc = <actual description>) it'll just ignore the prefix and probably do the right thing.. Update: I did make this change for the various type FooBarError struct{} (which are now var ErrFooBar = errors.New(\"your foo is too bar\") (the go linter seems to require static errors to have names of the form ErrX). The functions of the form IsErrFooBar(err error) still have their old implementation, and I actually added tests for this which caught a few bugs in this library. Per our conversation, I wanted to preserve the behavior that these methods sleep even after updating their own cache, to give other pachd nodes time to catch up. I changed the implementation, though, so that these now have a backoff followed by a sleep.. Same as above. This now uses backoff + sleep.. Fixed. Per our conversation: there are a lot of other things we could do here. The only reason there's any waiting at all is that tests were flaking, but we could:\n1. update this server's cache, but still wait for other servers\n2. update this server's cache, but don't wait for other servers\n3. wait for this server's cache to update organically, but not wait for other servers\netc. We decided to leave this for now.. Fixed. IIUC, the comment is wrong. Basically:\n- this test runs outside kubernetes (typically), but uses etcd (as it runs a PFS server in a separate goroutine) so it has to talk to etcd on 32379.\n- We had a NewLocalDriver function in driver.go that basically existed for this test. Therefore driver.go, via NewLocalDriver, did expect an etcd server at 32379\n- However, NewDriver, which is what regular pachd uses, expects a server at whatever k8s env var says is the port.\n- Since NewLocalDriver was deleted in this PR, there's nothing in driver.go that expects an etcd server at 32379 anymore, so this comment should be deleted.. Good catch, thanks!. It can't. pachClientOnce was previously a mutex, and I forgot to update this part of the code. It was using a different library. One of the changes in this PR is that I replaced github.com/cenkalti/backoff with github.com/pachyderm/pachyderm/src/server/pkg/backoff(which is a fork of cenkalti). Good catch. I think it's safe the way it's written (updateLease returns before the goroutine starts, but since the lease fields are mutable, holding the mutex is a good call, I think). What would you think/how difficult would it be to consolidate AddUser and RemoveUser into e.g. ModifyMembers? (Similar to ModifyAdmins)?\nmedium-term, I think we might be able to get rid of the Admins API completely (cluster_admins would just be an initial group, similar to the __spec__ repo), but we definitely don't have to do that in this PR :-)\n(long-term it might even make sense to collapse SetScope into SetGroupsForUser and SetACL into some kind of SetMembers call\u2014each role on an ACL would just be a group). Nit: how would you feel about calling this /members?. nice! I was about to suggest testing this case and then saw it. If you wanted you could use go's t.Run() call to make go test print each of these as a separate test: https://blog.golang.org/subtests. It might make sense to do gh(username) (instead of matching the part after the suffix) to make sure that the usernames returned from Pachyderm are canonicalized.. I think that every value in this map is true right? That means you could make this\nif removeGroups.Groups[group] {\n...\n}\nif you wanted. Could we add a todo to only canonicalize subjects that we can't find (i.e. if the user gives us a username to remove and it's right there in the group, skip canonicalization and just remove it. Otherwise, canonicalize the username and then see if we still can't find it). Done. Fixed. Done\u2014added TestCreateRepoNotLoggedInError. The change here is actually sort of minor. Basically, before this PR, this code tried to create the __spec__ repo, and if it already existed, this code ignored the error and continued.\nThen I changed CreateRepo so that it returns an error at the beginning if you weren't logged in. After that, if you tried to create a repo without logging in, and the repo also already existed, you got auth token not found instead of already exists, which breaks this code even though __spec__ does already exist.\nWrapping this in sudo guarantees that if pachd starts after auth has been activated (which pretty much means that the spec repo has already been created, though there are no hard checks in the auth server that the spec repo exists) then pachd gets an already exists error here and happily ignores it, rather than an auth token not found error.\nI'll double-check, but I think this code is actually tested by virtue of the fact that there are auth tests that start pipelines, and each pipeline starts a sidecar which tries to create the spec repo. If this weren't wrapped in sudo, every sidecar that started after auth was activated would crash.. Done. I added a test for this and the other functions too. Done. Extra line. Nice\n. Just confirmed, this is covered by existing auth tests (when I revert this delta, they fail). Done. This is now in etc/kube/push-to-minikube.sh. would it make sense to make this EXPOSE_DOCKER_SOCKET with the default set to true? Then you could have if exposeDockerSocket rather than if !noExposeDockerSocket later on. leftover debug statement? Or do we want this?. You can actually get the pod name directly from kubectl using the -o option:\n`kubectl get pod -l suite=pachyderm,app=etcd -o jsonpath={.items[].metadata.name}'. I think they may be dependencies of the new client. Basically, the commands I ran were:\ngovendor fetch 'k8s.io/client-go/...@v7.0.0'\ngovendor fetch +m  # I saw packages were missing, so I assume this stuff was added here\ngovendor remove +u. Nit: how would you feel about breaking this out into a separate test (TestIsGlob or some such?).\nIn general, I think tests that do too much are harder to debug and more brittle when making changes (I don't think this really qualifies yet, so I'm happy to leave it if you feel that it should stay)\u2014the problem really arises if a test is using a complicated setup to cover lots of different behavior. Then small-seeming changes to the test can accidentally remove important coverage.. Could we add another test with escaped glob characters? It's not something new to this PR, but it is a corner case that we talked about, and I think having it covered now that we're thinking about it would be helpful. My preference would actually be to change this in hashtree.go (my opinion is that it should be an invariant of hashtree that it always has a root directory, so that its interface is always functional). Either way, I also think it would be useful to link to https://github.com/pachyderm/pachyderm/issues/2607.. It contains superUserClient. Basically I looked at factoring sudo into a library, thought it was too hard, and did this. I can rid of it, though I do like the idea of keeping superUserClient from getting used out of scope. Let me know what you think.. For control flow. I wanted to skip to the end either if pipelinePtr.AuthToken == nil or WhoAmI comes back with Not Activated. The alternatives are three nested if statements or a goto. I went with if statements, but let me know what you think. I removed this, but then I added it back so that it only returns if sudo returns an error. Done, let me know what you think. This is a very minor suggestion, but I think you could even leave the implementation of makeCommit the same if you put the code that creates the hashtree here:\nopenTree := parentTree.Open()\nfor i, record := range records {\n    if err := d.applyWrite(recordFiles[i], record, openTree); err != nil {\n        return err\n    }\n}\ntree, err = openTree.Finish()\nif err != nil {\n    return err\n}\ndata, err := hashtree.Serialize(tree)\nif err != nil {\n    return err\n}\ntreeRef, _, err = d.pachClient.WithCtx(ctx).PutObject(bytes.NewReader(data))\nbuildCommit(..., treeRef, ...). nit: I think this should be AmazonIAMSecret (since I think the convention in Go is that acronyms stay uppercased in names).\nAlso, this is up to you, but IMO this should include Role in the name, since IAM is a blanket term for general auth stuff (users, roles, policies, etc). The default lease duration is determined by vault. I removed this line, though, because it's a distraction. All we care about is that the token is valid (which is checked two lines down) and then, after we revoke it, that it's invalid. I changed this to 20 (because it's a lease that's been renewed: 10s + 10s. Technically the test has slept for 5s at this point, but setting this to 15s seems like it's relying on the semantics of sleep to me, which seems more flake-prone than doing nothing). Fixed. I just wanted to make sure it wasn't 30 days, but 10s is indeed the value we expect. Fixed. This is deliberate. A few lines above, this requests a token with a TTL > 700h (just over 29 days), and Pachyderm used to reject leases longer than 2 weeks, but it should accept leases up to 30d, which is the vault maximum. This test checks that if Pachyderm receives the vault maximum ttl, it won't choke (though I'll change this to define and use a constant). Fixed. I used jdoliner and JoeyZwicker. I think you can write this as if [[ $INDEX -ge $MIN ]] && [[ $INDEX -lt $MAX]]; then ... (though you may be able to get rid of this completely; see next comment). Extreme bash hack:\nYou could write write LIST as a bash array and replace this whole RUN loop with:\n``\nLIST=(go test -v  ./src/server/ -list \".\" | grep -v ok | grep -v Benchmark)\n(( BUCKET_SIZE += PART < NUM_BUCKETS ? 0 : COUNT % NUM_BUCKETS ))\nRUN=IFS=\\|; echo \"${LIST[]:MIN:BUCKET_SIZE}\"`\nNote: The inner quotes around \"${LIST[*]}\" are actually required for this to work\n``. Not sure if we want this to be our style, but in double parentheses (and double brackets, array indexes, and array offsets and lengths), I recently learned that you don't need dollar signs if the whole token is a variable name (e.g.Xvs${X}${Y}). LGTM (easy to change later, in any case). It's no longer called :-). Basically, when you callpachClient.WithCtx(ctx), it does its own transformation of the incoming ctx before copying it to the new client struct. All of Pachyderm's code now uses that abstraction instead.grpc.Dialhas a backoff loop built into it. We could extend the amount of time the backoff has (that's inclint.PachDialOptions()`) but in general if driver fails to connect, there is no recourse.\nIf you look at the old code, d.onceErr was initialized inside of d.pachConnOnce.Do() and then returned (somewhat uselessly) after every subsequent call, so the lack of recourse is a preexisting problem. Other implementations of getPachClient in our codebase also use this pattern (panic inside Once.Do()). Fixed. If walk encounters an error, it stops walking, and my logic was that if there's some reason we can't copy one cert we should still try to copy the others. Let me know what you think\n(added a comment explaining the code). minor suggestion: I generally prefer to call bash explicitly, since I'm sometimes surprised by what /bin/sh gives me (/bin/sh isn't bash in either ubuntu (dash) or alpine (ash), for example). Minor nit: would you be okay making the flag --etcd-storage-class rather than --etcd-storage-class-name? For brevity, and also so that it's less surprising (hopefully) when we create a new storage class if it's unset.. Just to avoid having to cd back to the current directory. I generally started doing this with the rationale that I saw two alternatives, One:\ncd ${GOPATH}/...\nmake plugin\ncd -\ndoes the wrong thing if the initial cd fails (cd - takes you to some random directory) and:\ncd ${GOPATH}/... && make plugin && cd -\nDoes the wrong thing if make plugin fails and we never cd back to the parent.\nIdeally I want something equivalent to cd ${GOPATH} && defer cd - but bash has no defer. Done. Done. This is slightly more complicated than you might think\u2014PFS doesn't define this constant anymore, because PFS doesn't create the tree cache. Instead, pachd/main.go defines this constant and creates the tree cache, and since it defines a main function, it can't be imported:\n```\n$ go test -c ./src/server/pfs/server/\ngithub.com/pachyderm/pachyderm/src/server/pfs/server\nsrc/server/pfs/server/server_test.go:28:2: import \"github.com/pachyderm/pachyderm/src/server/cmd/pachd\" is a program, not an importable package\n. Per our conversation, I'll leave the PR as-is.. It wasn't really a deliberate decision. I deleted ServerOptions and then later came back and added ServerSpec. I'm happy to rename it back to ServeOptions. Done. Done. Done. See slack for context, but I converted this to a warning. WDYT?. This PR might just not be ready yet, but does this map get added to anywhere?. I'm a little confused by the question\u2014the intent here was to add indentation with the goal of making the output more readable (the empty string in `MarshalIndent` is the prefix, which is text that would go before every line regardless of the indentation level. The two spaces are the indentation string). This was a total mistake, I meant to use NewExponentialBackoff (fixed!). We do for the command. `$ [any shell command] <<EOF` means \"pass the stuff starting at the next line, until you see `EOF` on a line by itself, to`[any shell command]` on standard in.. Fixed. Fixed. Fixed. Fixed. This is a somewhat minor nit (especially since I'm pretty sure the package worked this way before), but IIUC, I think `123^3` in git means `456` rather than `678`in this diagram:\n123\n \u251c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2510\n234 345 456\n \u2502\n567\n \u2502\n678\nI'm fine with leaving it, but I guess I wanted to note it. Is the idea that this blocks the pipeline from scheduling? Or does this label get applied to the nodes somewhere? Or am I misunderstanding how this feature works?. The way auth works now, I think we actually wouldn't get that much mileage out of that change because `deleteAll` deactivates auth, which invalidates old tokens. So these would have to look something like:\nadminClient := getClient()\ndeleteAll(t, adminClient)\nadminClient := getClient() // old adminClient token is invalid\n. Done. For the record, my preference would actually be to get rid of `lenientCanonicalizeSubject` completely and not assume any prefixes, but the dash isn't ready for that yet, and the way the code is laid out doesn't really affect that.. Done. Done. Done. The intention here is that it sort of is, in the form of AuthConfig\u2014canonicalConfig is an AuthConfig that's already been validated, and the urls are all converted to *url.URL and such. ToProto just converts back to an AuthConfig.\n.suggestion\n    // 8 GiB, the max for etcd backend bytes.\n```\n/trollface. I ended up pulling the docs stuff into a separate PR so they don't block submitting this. I applied this change there. For what it's worth, we might not want to advertise SAML support yet, at least until we've tested it more in the dash, if not until another user asks for it.. Applied in other PR. I want to push back on this a bit because I think it would make the code quite a bit more complicated, and I want to make sure it makes sense.\nThese config functions are only called in two places: the client library and the metrics library, so to use an env struct, we'd have to be initializing the env struct somewhere else and then pass the value of this environment variable into the client and metrics libraries, which would pass it here. In particular, we'd have to create an env struct in pachctl (which doesn't currently use one), and then everywhere that creates a client library (or sends cluster metrics) would have to be updated to set the config path argument. Everywhere a client is created in pachd, this field would be empty, but every library used by pachctl that itself uses the client library would also have to accept a config path so that it could wire the value through to the client library, and then all of those call sites would have to be updated too, etc.\nAs it is, I think this is actually cleaner (and it resembles other environment variables that the client library checks, like ADDRESS and PACH_CA_CERTS), and it's certainly less work. Even if we do want to change this, would it make sense to do it after this PR is submitted?. This change was accidental (was supposed to just be for the demo)\u2014I'm taking it out of the PR. lol. That sounds good to me; I wanted to unify these two elsewhere as well. Done.. My thoughts on this depend on what you have in mind.\nBasically, I like the idea of having separate data structures for configs and canonical configs, because I want to avoid migration problems (the AuthConfig proto is part of the SetConfig API, so it's also designed to be more flexible than what's actually supported, with multiple IDPs possible or an IDP with no SAML settings).\nThat said, if keeping canonicalConfig is okay, and the question is whether it's a proto or struct\u2014I'm fine with either, but it seems like either way I'll have to deal with conversion from canonicalConfig (which is internal and will change as we implement stuff) to AuthConfig (which is public and won't change much). Does making it a proto make the conversion less error prone? Even if I can use reflection to, by default, copy all fields from one to the other by name, that kind of makes me nervous since the point of having two data structures is that one must always be valid, so if I add a field and forget about it, I might be setting myself up to forget validation logic that I actually need.\nOne idea that I kind of like, but would probably do later is to write a simple fuzzer that generates random configs and then sets & gets them to make sure fields aren't dropped. :+1: Updated the flag description. Basically, if the flag is unset, it uses the default timeout in client.go (currently 30s), but if set to 0s, then it overrides the 30s with 0s, which means \"no timeout\".\nI basically used \"default\" as the value so that if we wanted to change the default timeout in client.go, we wouldn't have to remember to change it here too (in general I liked the idea that the flag would only do anything if it's explicitly set). We actually removed Merge from the HashTree interface, so these tests are for a function that doesn't exist anymore. I mean, in my view, it's hopefully permanent, though it ultimately depends on what customers ask for. I think supporting the operation where users convert non-header/footer directories into header/footer directories (or worse, supporting mixtures of header/non-header files or multiple headers) will be annoying and ultimately prohibitive.\nBasically, I see this as the narrowest interface that solves the problem of splitting files with headers, and supporting narrow interfaces is easier than supporting wide ones.. 1. Ultimately I plan to support both input and output repos with headers (I think Bryce hopes to ultimately make input and output repos be the same). I stuck with input repos only because I wanted to finish this PR in time for the release, and it's all we need for the basic version of this feature (SQL and hopefully CSV if I can finish it in time)\n\nI think supporting output repos in this PR would be hard (I do want to ultimately do it, but again, I'm trying to limit the scope here to finish the project). As for why I think it would be hard:\nwe'd have to give pipelines some way to indicate that their output has header/footer data, which isn't an interface that exists yet and we'd have to create.\nI think folding the logic into one function would be either hard or slow\u2014output repos don't have hashtrees that implement the hashtree interface (note the tree argument that this fn accepts). I could do it by having this function make a GetFile call, but I think that means downloading the hashtree again. By only supporting input repos, I can just pass the hashtree in.. We could, though if we did that then changing a directory's headers after putting some initial data would mean updating the Shared field of every child, which we currently get to avoid. I'm happy to change it, though to maybe add clarity to what it does, the idea is that all input repo nodes go through it (so mostly non-header nodes) so that the caller doesn't have to figure out or care whether the node has a header or not (it's just that output repos can't call it, and their nodes never have headers anyway, so they skip it). So one change that I'm considering adding to this PR (as part of csv support, which @Nick-Harvey asked for) is a flag --header-records=N. Then if you set --split with anything other than SQL (which has its own complicated header logic) it just grabs the first N records and makes them into a header. That would work for most record-based parsers (line, csv, json, etc). Thoughts?. Oh, so after our conversation yesterday, I made it legal to update the header. Basically, InspectFile et al just compute the file hash at read time (which I think was your idea), so if you change a header you don't have to worry about updating the child hashes or anything and pipelines and such work great. One of the tests I just added actually covers this case now\n\n(but basically, computing the hash at read time allowed me to avoid the change that I'd have to do if Shared were in the nodes, which is looking up all of a directory's children and modifying them if the directory's header is modified). Also, re: CSV parsing in particular, ask and ye shall receive: https://golang.org/pkg/encoding/csv/. Done\u2014this is now nodeToFileInfoHeaderFooter. Done. Removed :-). Done. I generally reworded this to talk about \"this example\" and \"the example app\" and \"example config\" and such. After make proto finishes running, the container that it was running in is still around, it's just stopped (and you need to delete the container to delete the pachyderm_proto image, and you need to delete the image for rebuilding the image to re-download the protobuf compiler).\nI couldn't figure out a way to delete only the pachyderm_proto container, but if you know how to do that (maybe with docker ps --filter, which I briefly tried to figure out but couldn't?) I agree that that would be better. Just figured it out, I think. Done (changed it to a month)\nIn general, my sense is that we don't rebuild protos very often. My goal was to make it so that if you're starting a project where you have to change the protos, you also have to update your container, but if you're iterating quickly within the project, you only have to update your container once at the beginning\nI'm not wedded to any of the particulars of how we accomplish that, though. Agreed. I added a TODO here (hopefully we rewrite this script in go someday soon anyway). This is a mistake. I added these while debugging the \"golang.org/x/net/context\" vs \"context\" issue (https://github.com/pachyderm/pachyderm/issues/3265) and forgot to remove them. They're gone now. I'll try to get something working with reflection. Just to update, reflection isn't working well for this as I'd hoped, because I keep having to convert types back and forth (convert.go is shorter, but admin/api_server.go is messier).\nI think one thing worth considering here is that this code is converting 1.7 protos, and those can't change anymore, so it's not like we'll have much maintenance to do here (and if we add new fields to the output protos that have to be set, automatic name-based conversion wouldn't work for the new fields anyway, and we'd still have to write custom code). Done. . do you happen to know if this method respects the KUBECONFIG environment variable? In other words, if I have a kubectl config at $HOME/.kube/config but also have the env var KUBECONFIG=project/kubeconfig will it take context and such from the latter config?. Doesn't matter much, but if it were me I think I'd put a suite selector on all of these (it probably won't come up, but if another software project decides to use the pachd label, it would disambiguate). This is purely for my own understanding, but given that these commands usually issue a single RPC and then end the process, is there anything that would be leaked if we didn't call close (a file or socket or something?). I think the answer is no, and that these are good hygiene but not strictly necessary, but I'm not really sure. Ah! Thanks!. Would you be on board with giving the return values names? (i.e. oneOff bool, repo string, branch string, err error)) I think it might help readability a little (and you could get rid of the oneOff, repo and branch declarations below). I'm not sure if this would break putFiles, but would it make sense to return false, \"\", \"\", err here (and in error cases below)? My impression is that it's more conventional in go to discard other return values when returning an error.. Would you be on board with adding a comment here like:\n// inspect the commit where we're adding files and figure out if this\n// is a one-off put-file. \n// - if 'commit' refers to an open commit                -> not oneOff\n// - otherwise (i.e. branch with closed HEAD or no HEAD) -> yes oneOff\n// Note that if commit is a specific commit ID, it must be open for\n// this call to succeed\nJust because the code here is doing a lot, but I think the question it's answering is pretty short. Should this return statement keep the old parameters? i.e. return oneOff, repo, branch, nil. Or is this an error  too?. I'm actually not 100% sure either (I'll have to check), but I think it might be, and that would explain why we have so few stats tests. If so, we should probably put this stuff in its own testing harness someday (not in this PR :-)). I know that this isn't fixed in this PR, but IMO we should add a TODO to check that the job succeeded (once we fix the known bug that currently makes that not happen). We might want to add these (release-helper and release-pachctl-custom) to the .PHONY section at the bottom as well. Per face-to-face conversation, we should make sure this works in the case where etc/compatibility/$VERSION doesn't exist (typical for a release). Just checking my understanding\u2014does this PR require that we remove this test? Or was this change unrelated?. In general I think this approach is great, though I wonder if it might make sense to put these tests in their own bash files, in case we want to run them locally. should this be -gcflags \"$(GC_FLAGS)\" as well?. If I'm reading this PR correctly, one of the consequences of this changes is that the CreateRepo(__spec__) call that all pachd pods (including e.g. sidecars) make on startup will now actually update the spec repo.\nI'm not sure if there's any problem with that with that (more writes to etcd, I guess?), but it seems notable enough to confirm?. Hmm, I mean if it's not a ton of work that definitely seems like a useful optimization. Maybe note that this can start a pachyderm cluster as well?. We actually do this for s3:// already, and I just added wasb://. Fixed; ran gofmt (which I just forgot to do before committing :sweat_smile:). Fixed. I think it should still work that way. The indentation is getting hard to read in GitHub (at least for me), but from my editor, this section is basically:\n545: // 1. Write 'newCommit' to 'openCommits' collection OR\n546: // 2. Finish 'newCommit' (if treeRef != nil or records != nil); see\n547: //    \"FinishCommit case\" above)\n548: if treeRef != nil || records != nil {\n549:     if records != nil {\n550:         ...\n570:     }\n571:     ...\n576:\n577:     // If we're updating the master branch, also update the repo size (see\n578:     // \"Update repoInfo\" below)\n579:     if branch == \"master\" {\n580:         repoInfo.SizeBytes = newCommitInfo.SizeBytes\n581:     }\n582: } else {\n583:     if err := d.openCommits.ReadWrite(stm).Put(newCommit.ID, newCommit); err != nil {\n584:         return err\n585:     }\n586: }\n(i.e. else dictates that records == nil && treeRef == nil). See below\u2014my understanding is that, before, we were only updating repoInfo if records or treeRef wasn't nil (i.e. if we're pushing a finished commit to the branch)\nThis is obscured by the GitHub code folding, but I believe this is inside of if treeRef != nil || records != nil { ... }, which I think are the cases where we'd be moving master to point at a finished commit, and where isStartCommit wasn't being updated before.. :+1: Updated this section. Done. This is the name pachd uses to identify itself when talking to Jaeger (i.e. in Jaeger, because this is set to pachd, you'll see traces for pachd.PutFile rather than jaeger.PutFile). I can rename the variable, I'm just not sure what would be a better name.. They're somewhat semantic changes\u2014I had to changed the obj.Client interface (in src/server/pkg/obj/obj.go to take a context in all of its calls, so that the various object clients (amazon_client, google_client, etc) could report spans to Jaeger for object store RPCs that were correctly associated with the surrounding Pachyderm RPCs. In other words, when you call PutFile, the PutOject calls associated with that show up underneath the original call in Jaeger because of the context propagation. But because of the interface change, I had to change all of the callers, which is all of these call sites are changed.\nIncidentally, I got rid of the ctx that the Google client constructor took, because it was just using that context to make RPCs, and now it uses the context that it recieves for each call (which I'm pretty sure has better cancellation semantics anyway) . Done. Done\u2014I went back and forth on this a bit, because I don't think users will want to trace every pachctl call they make over a period of time\u2014specifically, you probably don't usually want to run export PACH_ENABLE_TRACING=true, which was my rationale behind making it a flag. But, having thought about it, users who include the Pachyderm client library in their code (particularly anyone who uses the vault plugin) could take advantage of this tracing code, except that they might not be able to set a flag on the outer binary. So I changed this to use the environment variable PACH_ENABLE_TRACING. Clarified the comment for JaegerServiceName. Would it be possible to move this into its own test? I know TestProvenance is already pretty far gone, but in the past I've had bad experiences where we need to fix or remove complicated tests because we've changed our semantics, and we end up removing a bunch of auxiliary tests (like this one) that should still apply. If tests are smaller and more well-defined, then changing semantics without introducing regressions is easier. ",
    "Seth-Karlo": "This is also a requirement for us, commenting here so I can keep track of what's happening :). ",
    "jmbrunskill": "I also ran into this issue. I think it could be to do with the way mac's /var folder is actually a symlink to /private/var. My suspicion is that docker is comparing the configured shared path /private/var/lib against the requested path of /var/lib and they don't match. Their UI doesn't let you create a shared folder to the symlink  directly. I worked around this by changing the path in etc/kube/start-kube-docker.sh to a non-symlinked path, however I then ran into other problems with not being able to expose the kubernetes API port. In the end I installed kube-solo which worked perfectly. https://github.com/TheNewNormal/kube-solo-osx\n. Glad to be of service.\n. ",
    "montenegrodr": "Hey, @ShengjieLuo .\nFacing similar problem when trying to run pachyderm fruit stand example for the first time:\nID                                 OUTPUT                                    STARTED             DURATION            STATE               \nb1e57eb82cba7d1152e0db331668c4db   filter/5e748f14892d4fddade6e9cc7bbad1f3   14 minutes ago      -                   pulling\nDo you have any suggestion for that?\nThanks.\n. Thanks, @ShengjieLuo. Everything is good re to my versions:\nCOMPONENT           VERSION             \npachctl             1.1.0               \npachd               1.1.0\nAnd I got this from job-log:\n$ pachctl get-logs b1e57eb82cba7d1152e0db331668c4db\ncontainer \"user\" in pod \"b1e57eb82cba7d1152e0db331668c4db-nr4ou\" is waiting to start: trying and failing to pull image\nIt seems it is the missing image you'd mentioned. How can I pull this image?\n. Thanks @JoeyZwicker and @ShengjieLuo for the prompt answers.\n@ShengjieLuo , after make docker-build. I've got success for all pulling jobs. \n. @ShengjieLuo :\n$ sudo docker ps\nCONTAINER ID        IMAGE                                             COMMAND                  CREATED             STATUS              PORTS               NAMES\nd0a11e25b563        pachyderm/pachd:v1.1.0                            \"/pachd\"                 About an hour ago   Up About an hour                        k8s_pachd.134ae695_pachd-opx3s_default_df9b8741-4e82-11e6-8e73-000d3a143fbf_9f0910a9\ndd6cbe9f4f79        pachyderm/pachd:v1.1.0                            \"/pachd\"                 About an hour ago   Up About an hour                        k8s_pachd.134ae695_pachd-a2i7f_default_df9b2b3b-4e82-11e6-8e73-000d3a143fbf_e05d5725\n01e2bdcf0d95        rethinkdb:2.3.3                                   \"rethinkdb -d /var/re\"   About an hour ago   Up About an hour                        k8s_rethink.2a07aec2_rethink-9dryg_default_df650d0b-4e82-11e6-8e73-000d3a143fbf_07b14b5e\nfd844753169e        gcr.io/google_containers/pause:2.0                \"/pause\"                 About an hour ago   Up About an hour                        k8s_POD.a1892be5_pachd-a2i7f_default_df9b2b3b-4e82-11e6-8e73-000d3a143fbf_72a0b890\n88cb3f2cd03f        gcr.io/google_containers/pause:2.0                \"/pause\"                 About an hour ago   Up About an hour                        k8s_POD.a1892be5_pachd-opx3s_default_df9b8741-4e82-11e6-8e73-000d3a143fbf_8235570f\n05f8440ec1e5        gcr.io/google_containers/etcd:2.0.12              \"/usr/local/bin/etcd \"   About an hour ago   Up About an hour                        k8s_etcd.5b1b926a_etcd-xoiv5_default_df2aae46-4e82-11e6-8e73-000d3a143fbf_c29a4fba\n4ddb0bc64563        gcr.io/google_containers/pause:2.0                \"/pause\"                 About an hour ago   Up About an hour                        k8s_POD.1bc25335_rethink-9dryg_default_df650d0b-4e82-11e6-8e73-000d3a143fbf_84b32851\nd0c985393867        gcr.io/google_containers/pause:2.0                \"/pause\"                 About an hour ago   Up About an hour                        k8s_POD.54a62b2f_etcd-xoiv5_default_df2aae46-4e82-11e6-8e73-000d3a143fbf_5cbfdeb2\n99e4c62f7748        gcr.io/google_containers/hyperkube-amd64:v1.2.2   \"/hyperkube controlle\"   About an hour ago   Up About an hour                        k8s_controller-manager.70414b65_k8s-master-127.0.0.1_default_4c6ab43ac4ee970e1f563d76ab3d3ec9_0f2bd030\n87bd638b2942        gcr.io/google_containers/hyperkube-amd64:v1.2.2   \"/setup-files.sh IP:1\"   About an hour ago   Up About an hour                        k8s_setup.e5aa3216_k8s-master-127.0.0.1_default_4c6ab43ac4ee970e1f563d76ab3d3ec9_d8b37841\n932d7962cb49        gcr.io/google_containers/hyperkube-amd64:v1.2.2   \"/hyperkube scheduler\"   About an hour ago   Up About an hour                        k8s_scheduler.fc12fcbe_k8s-master-127.0.0.1_default_4c6ab43ac4ee970e1f563d76ab3d3ec9_d613d567\ne3acb7ea1818        gcr.io/google_containers/etcd:2.2.1               \"/usr/local/bin/etcd \"   About an hour ago   Up About an hour                        k8s_etcd.7e452b0b_k8s-etcd-127.0.0.1_default_1df6a8b4d6e129d5ed8840e370203c11_fc070564\n95eabbba3446        gcr.io/google_containers/hyperkube-amd64:v1.2.2   \"/hyperkube apiserver\"   About an hour ago   Up About an hour                        k8s_apiserver.78ec1de_k8s-master-127.0.0.1_default_4c6ab43ac4ee970e1f563d76ab3d3ec9_e4081cab\naf5372fa49ed        gcr.io/google_containers/hyperkube-amd64:v1.2.2   \"/hyperkube proxy --m\"   About an hour ago   Up About an hour                        k8s_kube-proxy.9a9f4853_k8s-proxy-127.0.0.1_default_5e5303a9d49035e9fad52bfc4c88edc8_5c6c1ebf\n93f4343fd870        gcr.io/google_containers/pause:2.0                \"/pause\"                 About an hour ago   Up About an hour                        k8s_POD.6059dfa2_k8s-etcd-127.0.0.1_default_1df6a8b4d6e129d5ed8840e370203c11_90afaf64\n28ef34f4eae7        gcr.io/google_containers/pause:2.0                \"/pause\"                 About an hour ago   Up About an hour                        k8s_POD.6059dfa2_k8s-master-127.0.0.1_default_4c6ab43ac4ee970e1f563d76ab3d3ec9_42158313\n58f49b94faf2        gcr.io/google_containers/pause:2.0                \"/pause\"                 About an hour ago   Up About an hour                        k8s_POD.6059dfa2_k8s-proxy-127.0.0.1_default_5e5303a9d49035e9fad52bfc4c88edc8_cf64276e\nce9b6ed49e8f        gcr.io/google_containers/hyperkube:v1.2.2         \"/hyperkube kubelet -\"   About an hour ago   Up About an hour                        lonely_mirzakhani\n. ",
    "zyluo": "Based on your first and second experiment, the Pachyderm's job shim seems to be the bottleneck.\n. bu -> by\npachyderm -> Pachyderm\n. Pachyderm\n. ",
    "alexsbromberg": "I think that's it. Didn't realize the version in master wasn't supposed to be backwards compatible with the current release. \nIf that's the case, it might make sense to update to the current canonical Dockerfile so that it pulls a specific version of the job-shim, or at least make some note about the compatibility issues.\n. As some additional info, when my job for repo C runs and it tries to copy the file it expects in repo B, i'm seeing the following error:\nERROR fuse.DirectoryLookup {\"directory\":{\"file\":{\"commit\":{\"repo\":{}}}},\"name\":\"B\",\"err\":\"operation not permitted\"}\n. Here's the output from pachctl list-job\nID                                 OUTPUT                                               STARTED             DURATION            STATE               \n5d51743c6f52cf4e18257a26aa7e8984   C/e68bffd9ed71483f9949ab5841ba1ede   10 seconds ago      3 seconds           success    \n055ba2f24ca92c1d56b110772039fdbc   B/6de392756ce04674a79b4b492164e434        20 seconds ago      9 seconds           success\nFrom pachctl inspect-job 5d51743c6f52cf4e18257a26aa7e8984\nID: 5d51743c6f52cf4e18257a26aa7e8984 \nStarted: 53 seconds ago \nDuration: 3 seconds \nState: success\nParallelism: 1\nInputs:\nNAME                 COMMIT                             PARTITION           INCREMENTAL              \nD   46d854308eab498abeec4d61321e0e1c   BLOCK               %!t(pps.Incremental=1)   \nTransform:\n{\n  \"image\": \"custom_image\",\n  \"cmd\": [\n    \"/usr/bin/R\",\n    \"--vanilla\"\n  ],\n \"stdin\": [\n  <code truncated>\n ]\n}\nFrom pachctl inspect-job 055ba2f24ca92c1d56b110772039fdbc\nID: 055ba2f24ca92c1d56b110772039fdbc \nStarted: 3 minutes ago \nDuration: 9 seconds \nState: success\nParallelism: 1\nInputs:\nNAME                COMMIT                             PARTITION           INCREMENTAL              \nA          f82b0531fac84cdc9159a5a2bc64f3f1   FILE                %!t(pps.Incremental=1)   \nTransform:\n{\n  \"image\": \"custom_image\",\n  \"cmd\": [\n    \"/usr/bin/R\",\n    \"--vanilla\"\n  ],\n  \"stdin\": [\n    <code truncated>\n  ]\n}\n. Cool. I'll check that it works on my end too once that refactor is merged in.\n. @jdoliner sounds good to me. Thanks for adding it.\n. Signed it.\n. ",
    "statik": "@jdoliner aha! makes perfect sense now. that worked great, thank you.\n. ",
    "cmars": "@jdoliner That fixed my issue, thanks!\n. I also had to specify v1.1.0 in etc/kube/pachyderm.json. I was getting pachd 1.0.1 otherwise, which didn't support the fruit-stand example when I tried working with pipelines, with a 1.1.0 client.\nHowever, everything's working now!\n. ",
    "rexmortus": "Thanks @ShengjieLuo :+1:\n. @JoeyZwicker fantastic news! I reluctantly gave Google yet another CC # but frankly I have to scratch the pachyderm itch. Looking forward to the minikube guide. :smile: \n. I was using go1.4. Installed gvm used go1.6 and everything works.  I assume go's vendoring system improved over two minor versions. Anyways everything works now \n. Thanks @JoeyZwicker and @jdoliner :+1: \n\nI deeply appreciate the support. I can't wait to get started! First project is to write a scraper for Melbourne Open Data.\n. Thanks @ShengjieLuo \nenv | grep ADD\nADDRESS=104.155.230.41:30650\nYep, the variable is there in the environment.\n. Thanks @ShengjieLuo I can tell that you care about the community!\n$ pachctl version\nCOMPONENT           VERSION             \npachctl             1.1.0               \npachd               1.1.0\n^ Looks good\nmake logs produces nothing out of the ordinary.\nget pods -o wide\nNAME            READY     STATUS    RESTARTS   AGE       IP          NODE\netcd-924uc      1/1       Running   0          5h        10.56.1.3   gke-pachyderm-default-pool-1037cff7-y8s2\npachd-h8fcn     1/1       Running   3          5h        10.56.1.5   gke-pachyderm-default-pool-1037cff7-y8s2\npachd-s109q     1/1       Running   3          5h        10.56.0.6   gke-pachyderm-default-pool-1037cff7-plg8\nrethink-pqufb   1/1       Running   0          5h        10.56.2.5   gke-pachyderm-default-pool-1037cff7-bw9x\nLooks like those are going OK, too?\n. Thanks again @ShengjieLuo.\nAs I suspected, it's just a cosmetic issue.\nWhen I have $ADDRESS set to an IP with pachd running, I get this result:\npachctl version\nCOMPONENT           VERSION                                          \npachctl             1.1.0-2a4d2ddd55192cf80aa7c9b162290489d1ab7290   \npachd               1.1.0\nHowever, when I run pachctl with no arguments and the list of commands appears:\n```\n$ pachctl\nAccess the Pachyderm API.\nEnvronment variables:\n  ADDRESS=0.0.0.0:30650, the server to connect to.\nUsage:\n  pachctl [command]\n...\n```\nTwo things to note about the above message.\n1) There is a typo. It says \"Envronment\" when it should say \"Enivronment\".\n2) It says that $ADDRESS is set to 0.0.0.0:30650\nWhich, as we've determined, it's not.\nIndeed, when I change ADDRESS to an IP where no pachd is running, pachctl version returns what I'd expect - an error!\n$ export ADDRESS=0.0.0.0:30650\n$ pachctl version\nCOMPONENT           VERSION                                          \npachctl             1.1.0-2a4d2ddd55192cf80aa7c9b162290489d1ab7290   \npachd               (version unknown) : error connecting to pachd server at address (0.0.0.0:30650): context deadline exceeded\nSo as I've discovered, it would seem that this is just a cosmetic issue, although it is more a than bit confusing.\n. I would like to take a moment and acknowledge the hilarity of me offering a correction to a typo, only to  offer a typo myself.\nGO ME! :boom:\n@jdoliner now that I understand the intent, it makes more sense. Maybe an explicit label would be helpful, as I (wrongly assumed) that line was reflecting the current environment's configuration.\n. Perhaps the most scale-able solution is that people take an LGTM selfie.\nhttps://github.com/hitode909/lgtmselfie <--- fkn :boom: m8 \n.  'person' just doesn't have the same alliterative ring to it.\nMaybe we need a gender neutral pronoun that starts with \"m\".\nIn Australia, we have the word \"mate\" but even that sounds stupid in context.\n. Thanks @jdoliner.\nI'm not sure I fully understand.\nWhat does /project correspond to? A git repository?\nIf yes, where would you subsequently mount the pachyderm filesystem?\nSo for example, if I have a project like this:\n/project\n   pipeline.json\n   Dockerfile\nWhere would you mount the pachyderm filesystem? In a different folder?\nAlso, would there be a one->one mapping of pachyderm repos to git repos containing the pipelines?\nOr would you have one git repo of pipeline files for n pachyderm repos?\nI guess it's totally arbitrary where I store the data on my local filesystem... just curious what the best practice is.\n. Also, a secondary :question:\nWhat would happen if you mounted pfs to multiple locations?\nEven if there wasn't a particular problem with it, should pachctl allow it (for reasons of it being ultimately un-necessary and potentially confusing)?\n. @jdoliner I like those suggestions! :+1: \nAdditionally, if you wanted to background the process perhaps you could do so with a flag --daemonize or some shit, which prints a message like:\nrunning in the background - use pachctl unmount to un-mount\nThanks for clarifying about multiple pfs mounts. I imagine that I'll need that before long. You can see that I still have a lot of exploring to do.\nAlso, while I've never hacked on a golang project before, I'm pretty sure that this one's got me hooked.\nReally excited to be part of this discussion.\n. Thanks @jdoliner I'll check it out a bit later on.\nQuite possible. But... it really shouldn't be returning 1. Locally this produces the desired outcome.\nIf my pfs looks like this:\n~/pfs/landmarks_and_places_of_interest/569c644fef224bef86fc81aedab4cfef/data\nAnd my job looks like this:\n\"grep \\\"Places of Worship\\\" /pfs/landmarks_and_places_of_interest/data > /pfs/out/places_of_worship\"\nAm I doing it right in terms of the file paths?\n. Excellent suggestions @jdoliner, about to step out for Saturday's shopping but let me say I appreciate the lengths you're going to.\nI'll report back later.\n. Another question.\nWhen a pipeline fails like this, what is your procedure for \"pushing a fix\"?\nRight now, I'm just deleting the pipeline and associated repo and starting again. Which seems kinda noob.\n. @jdoliner yep, it definitely a parsing issue. At some point I'll look into the code and see what's happening under the hood... I imagine that infracoder-type people are used to writing shell scripts inside JSON configs... but not me. :stuck_out_tongue: \nFWIW, this is what worked for me in the end:\n{\n  \"pipeline\": {\n    \"name\": \"places_of_worship\"\n  },\n  \"transform\": {\n    \"cmd\": [ \"sh\" ],\n    \"stdin\": [\"grep \\\"Place of Worship\\\" /pfs/landmarks_and_places_of_interest/data > /pfs/out/places_of_worship\"],\n    \"acceptReturnCode\": [1]\n  },\n  \"parallelism\": \"1\",\n  \"inputs\": [\n    {\n      \"repo\": {\n        \"name\": \"landmarks_and_places_of_interest\"\n      },\n      \"method\": \"map\"\n    }\n  ]\n}\nThe job succeeds and I can read the resultant data. :heart:  \nI just now realized that the idea is for my to define multiple transforms in a single pipeline file.\nNow that I've cracked writing pipeline configs, shit is about to get interesting. :raised_hands: \nGod this is exciting.\n. ",
    "pdeburen": "Had the same problem yesterday with kubectl v1.2.2 and pachctl v1.1.0 and custom docker images on docker hub. Switching to kubectl v1.3.6 and restarting solved this for me (not sure if due to the update of kubectl or the restart).\n. ",
    "willguxy": "@ShengjieLuo Thanks for your prompt reply and sorry for the confusion. To your question, yes and conceptually I could get duplicated data, but not necessarily. One simple example would be a job that takes the whole data set and compute the average when new data comes in. So the ideal output would be a time series consisting of timestamps and average up to each timestamp.\n. @jdoliner First of all, thanks for being so active in responding to questions and issues.\nRe your question,\n- My original thought was about some data transformation with a sliding time window, but in my example to @ShengjieLuo, I mentioned using the whole data history. I guess essentially the problem is similar, except the sliding window case has this extra layer of complexity if the requirement is for pachyderm to only show the job data that's part of the sliding window. In summary, the base case is access to the whole data history and we can subset within the actual code (although just exposing the data of certain time window would be more efficient in reading and writing files).\n- I think the outputs are relevant in the sense that they are results of the same data transformation method although are somewhat convoluted rather. Say computing the average stock price with 1 month lookback window everyday when new daily data comes in. The output series is consistent and does not contain duplicates. So I don't know if it makes sense make output data commit isolated/parentless.\nThis is more of a conceptual discussion here rather than requesting a feature since I haven't quite fully understood the use of parent/child relationship and how this could affect downstream jobs. Any explanations on my confusion is also very appreciated :)\n. @derekchiang Thanks for your prompt response. Would like to reopen this issue because in my case I would like to have the parallelization feature on small input data size.\nThe reason why I would need it is because I'm running a set of models in parallel. Each model transforms the same data set but has different model parameters. They have no overlap and the goal is to run these models in parallel. The output can be stacked as a table consisting of model parameters and corresponding model output.\nModel parameters are necessarily small in data size. So it wouldn't trigger the parallelization no matter what. Is there a way to force parallelization? \n. @derekchiang Yes it does make sense. The problem though, is that I might need to run hundreds or thousands of such models, and if each of them has a unique pipeline, it becomes much harder to manage the jobs and collect the results.\nTo better illustrate my use case, I have the following diagram\n\nand re your comment there isn't a way to run parallel jobs and have them all see the same set of input, do you mean if I have input 1 as raw data set and input 2 as a table of model parameters, I can't run the pipeline in parallel and let each process see ALL input 1 and PART of input 2? This is also highly relevant to what I was trying to do, although sounds like a separate issue.\nThe original question was where or not there's a way to force parallelization on input data of small sizes, which in my case, is the model parameters.\n. @derekchiang many thanks and yes, that's what I want. Does that mean the input data size issue you mentioned earlier is only related the block method? So in the above case if I separate the parameters into different files, it wouldn't restrain me from parallelizing the pipeline right?\n. @derekchiang Thank you for your detailed explanation. It was very clear. \nCan I request adding an option of switching on/off the feature of constraints on block data size to parallelize pipelines? I feel that it will be very useful not just in our use case, but in many others as well.\n. I'd use json or separate files for this purpose. So closed because it's not an issue.\n. @sjezewski Thank you very much. I pulled the latest code with your fix merged and tested my pipelines. Now everything works like a charm. I'm gonna close this issue for now.\n. After some digging, I realized that this issue is probably because k8s couldn't clean up the orphaned pods, and it just keeps trying infinitely. The log of hyderkube contains all the things like Failed cleaning pods and \nFailed to remove orphaned pod \"a99fc0eb-8699-11e6-b9ea-0e84a6c9a22d\" dir; err: remove /var/lib/kubelet/pods/a99fc0eb-8699-11e6-b9ea-0e84a6c9a22d/volumes/kubernetes.io~secret/default-token-64z4a: device or resource busy\nPreviously when parallelized pachyderm job fails partially, I deleted the pipeline and tried to delete the pods, too. I ran kubectl delete pod/<pod-id> but it didn't work. I added --grace-period=0 to it and it worked. But the job-shim <job-id> docker containers was still running, and I couldn't stop them. So I had to restart docker service and remove afterwards. \nI think at some point my behavior caused orphaned pods.\n. ",
    "fjukstad": "Great! I've signed the CLA. \n. Hi,\nAfter setting the parallelism_spec to constant the problem moved on to the pipeline running, but jobs failing with the error message: \nthe server does not allow access to the requested resource (get pods)\nThis error was caused by the pachyderm serviceaccount not having permission to get pods within the namespace I was using. We updated the permissions on the service account and the pipelines are now running as they should. \n. ",
    "maxknee": "Done and done.\n. Can this get merged?\n. ",
    "ukd1": "@JoeyZwicker...I would; I've no idea what half of it means and it'll take longer to understand than this patch.\nPlease just fix the links yourself; I came back here as there are many other errors in your docs; I was going to submit a patch for those too.\n. @jdoliner I'm doing it from another server (not laptop, wired in) with pachctl put-file with a 1000mbit connection on the same network....\nI will try http, but I can't see that being any better connection-wise (it's 100% not this box / it's connection).\nSuggestions based on that? \n. @msteffen GCP, pachyderm v1.2.1, fresh install yesterday, I configured with 500g of storage; pachctl is running direct on my local server.\n. ping? anything I can do or is this expected?\n. @msteffen ok - is there a work around? at the moment this renders pachyderm useless for me\n. ",
    "html5cat": "@JoeyZwicker done, cheers!\n. ",
    "mwaaas": "thanks\n. @jdoliner  will try to do one.\nThis diagram is not correct  (https://cdn-images-1.medium.com/max/2000/1*dQcpOVJtjQEx4NcccPucPA.png) the cluster management is not a complete separate layer. if it was it should be very easy to be swapped with another cluster management especially if you are running containers ( am assuming everything in pachyderm is a container even the Pachyderm jobs) \n. @jdoliner  thanks for the info. closing the ticket for now, when I get time will try implementing docker-swarm option. \nThe reason I want docker-swarm, it would be easy to host it in docker-cloud. \n. ",
    "AmundsenJunior": "Would it make sense at this point to update issue for updating to Ubuntu 18.04?. Is this issue resolved at this point by the merge of https://github.com/pachyderm/pachyderm/pull/2881, since subsequent builds have passed?. ",
    "bhueth": "I think so? Here's what I see:\npachctl create-repo data\npachctl put-file data master sales -c -f set1.txt\npachctl create-pipeline -f pipeline.json\npachctl list-job\nID                                 OUTPUT\n5eebd488fbd96dee3dc4d746a879b14c   sum/2a5de2d05f0c41b9abccee30cd6b4aaa/0\n67c30d70ba9d2179aa133255f5dc81db   filter/0fc94f46efb4401090ea09c22c948fca/0\npachctl list-repo\nNAME                CREATED              SIZE\ndata                About a minute ago   874 B\nfilter              22 seconds ago       400 B\nsum                 22 seconds ago       24 B\npachctl get-file sum sum/2a5de2d05f0c41b9abccee30cd6b4aaa/0 apple\ncommit sum/2a5de2d05f0c41b9abccee30cd6b4aaa/0 not found in repo sum\n. Sure does. Thanks.\n. ",
    "brendanacassidy": "Also, it should block until all consequences have finished: in the event of a combination of some successful commits and some errors leading to cancelled commits, it should not return immediately upon encountering an error but should block until everything has flushed.\n. For FlushCommit to error when there's a downstream error seems to make sense. However, if it's not a fatal error with regards to executing the flush (like a network error), then it shouldn't return with the error until all downstream consequencies have terminated, whether they are successful or not. In other words, if it can flush, it should flush.\nAs far as how to discover the failure(s), that could be through the same API (maybe with a flag that lets you either return only successful commits, or return all commits in order look through them to discover their statuses) or could be through a different API.\n. ",
    "moadben": "@jdoliner cool, thank you very much!\n. ",
    "russelhowe": "Oops, sorry, I didn't read the instructions. I am sorry to waste your time but I'm not interested in signing anything right now. I will close the pull request.\n. This is fixed in 1.8.1, the output is as expected.. ",
    "hunter": "Great! I got the dry-run output and started converting. One thing that I wasn't sure of was support for namespaces. Does pachctl (and pachd?) only support the default namespace?\nAlso, is there a reason for Replication Controllers instead of Deployments?\n. I've tried out a basic version of a deployment with Helm - https://github.com/AcalephStorage/charts/tree/pachyderm/incubator/pachyderm\nIt seems to work but found that I needed to hardcode resource names to ensure that pachd could find rethink. pachctl also seems to break when launched in a custom namespace so had to work around by querying for the pod name and port-forwarding through kubectl.. I was actually thinking about something similar yesterday. Would be great to be able to have a marketplace for pipelines. Templating like helm may be a first step?. ",
    "jonandernovella": "+1. It would be very handy. Hi @naztyroc. I also worked on this. Please find my implementation details here: https://github.com/pharmbio/pbcharts/tree/master/pachyderm/templates\nCheers!. @gabrielgrant Yes, it was. I am sorry for the inconveniences.. @gabrielgrant Hehe. I didn't test it for a wee while. But feel free to try it. I remember that I used to access the dash by navigating to: http://<dash-svc-name>/?host=<grpc-svc-name>&port=80.\nAnd yeah, I think we did not enable the PFS-over-HTTP API yet.\n. @JoeyZwicker Since it's an optional feature, I decided not to include it in the chart. Users can still deploy it through pachctl if they want the dash.. @thedrow Agree! Feel free to make a PR .. @LaurentGoderre Updating the chart to default 1.7.11 doesn't solve any problem. You can configure the chart to pull the version you want from the values manifest file.. @gabrielgrant There is some conflict with the ENV vars wrt the storage backend. Changing that should do the trick to make the chart work again on 1.8+. @LaurentGoderre Sure. Still I don't think that's enough to make a new release, given that you can easily change the version.... @LaurentGoderre I agree it's worth the experience gain, but this update process needs to be automated. It's not feasible manually updating the chart for every new version is released.. +1. And this should also include some command to check the progress of the put-file command. In case of uploading some big data-set it is hard to see whether it hangs or not. . +1. Tested it with 1.4 stable release. Now it seems to work better. However, instead of creating 32 (8vcpus*4worker nodes), i see that it created 48 pods (presumably 8 vcpus * 6 nodes including the 2 edge nodes ). Any ideas about this? I think it is desirable that they would only be scheduled in the worker nodes.. +1. @dwhitena @derekchiang yes, that definitely works.. #1547 is related to this.. +1. @dwhitena @jdoliner Indeed. This seems to coincide with my issue.. @derekchiang I have also experienced that deleting a pipeline does not fully delete all its jobs and their corresponding metadata.. @JoeyZwicker By changing the pipeline definition to include the new file, I mean that I change the name explicitly of the file in repo Y in the pipeline definition.\nExactly, it does seem to see the old ones but not the new one. New commits do not seem to help, and the above behavior seems to be consistent with both cross and union.. @JoeyZwicker Issue spotted. The thing was that the new file did not exist in the initial commit, so therefore it was not being processed.. +1. Issue reproduced. I can see the same behaviour. I tested it with 8 workers.. Solved. I was not aware about the parallelism. The total processing time in this case is the sum of all nodes.. @jdoliner Logs from the pipeline-rc: Error creating: pods \"pipeline-edges-v1-\" is forbidden: service account default/pachyderm was not found, retry after the service account is created \nI assume that the ServiceAccount name is hardcoded into Pachyderm. If this cannot be changed I should also hardcode it on the chart. What do you suggest?. @jdoliner Users are actually encouraged to deploy Pachyderm with the Helm chart on a custom namespace. I assume that the rc is looking for the sa on the default ns. That might also be an issue.. +1. I can reproduce this issue. Is there any workaround on this?. ",
    "Vlaaaaaaad": "Since https://github.com/kubernetes/charts/tree/master/stable/pachyderm exists, should this not be closed? Are there any other things missing from the chart?. ",
    "naztyroc": "Everyone,\nI've been working on adding Dashboard functionality to the official helm chart, via PR. There is one issue I presented to @gabrielgrant which was the dashboard needing http://:/?port=.. @jonandernovella thanks for the idea of using an ingress in the build, good stuff! . Hi Dan,\nWe were attempting to deploy an instance of pachyderm with our CEPH S3 gateway protocol as a replacement for a MINIO deployment, but ran into the error you explained in the intro of this issues report. This forced us to deploy using the helm chart provided in the official kubernetes charts  repo. This was a GREAT help! To make things even better for our use case, we were now able to deploy multiple instances of pachyderm for creating serval pipelines using different storage backends in the same K8 cluster. We encountered a few problems along the way  which required us to modify the helm chart pr 4159 This enabled us to deploy multiple instances of pachyderm using CEPH S3 gateway, MINIO, Amazon, etc.. Many thanks!!. Hi Dan,\nFor our use case, we are using CEPH as the storage backend for Kubernetes which connects as RBD volumes. We configured Kubernetes to communicate with CEPH by creating storage class methods. With that setup, we deployed Minio, with CEPH/RBD PVC, as the backend for Pachyderm which talks to it over S3 protocol. I hope that helps?\n-Rashian\nSent from my iPhone\n\nOn Apr 26, 2018, at 9:11 PM, liuchenxjtu notifications@github.com wrote:\nhi Dan,\nthanks your for your efforts. I am quite new to Pachyderm. now we want to deploy pachyderm on our own cluster. but most of the docs are about deploying on cloud provider, e.g., persistent-disk must be google, azure or aws. so for our case, could you advise what we should be for the local clusters? thank you!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "thedrow": "Since we do have a helm chart we should close this issue.. @jonandernovella It's easy enough to add to the Helm package.\nYou can use a conditional on a variable which defaults to false.. Exactly! You specify the file name(s) template in the configuration (or provide it as part of the message) and the Sink will commit it to the desired repository.\nThe Source will stream results from a repository. Since a pipeline always saves data in a new repository, creating a source from a repository is natural and will result in what you'd expect.. @jdoliner @derekchiang I think you guys are missing the point. Kafka has a standardized mechanism to move data to/from sources. \nIf we're going to use it, we do need to implement the Sink and Source in Java.. Just a note since I forgot to mention it. Kafka does not handle large messages very well. All producers must be able to partition the file in such a way that the connector will be able to reconstruct it.\nSee this presentation.. This is somewhat related to #1885.\nThe current file mutation behavior is not documented anywhere leading to confusion of users who haven't participated in our Slack channel or read some of the issues in this repository.\nWhile we resolve this issue, can we please document what happens when writing to a file?. Note that Alluxio has made a Go client available which should ease implementation.. FYI occasional development of this feature occurs in https://github.com/thedrow/pachyderm/tree/alluxio-client\nPRs are welcome.. I think this could be solved by transforming a pipeline spec into a Helm chart.\nWe can integrate pachctl with Helm pretty easily by talking to Tiller which is a GRPC service.\npachctl can be used as the following to create the pipeline:\npachctl create-pipeline --chart my-pipeline --values my-values.yaml. After a slack conversation we realized we don't have pachctl create-pipeline --dry-run so we don't have the basic building block for a reusable pipeline.. I tried using the --no-metrics option as well but it still seems like some other I/O happens.. No it wasn't. There shouldn't be an error in any case. . It seems that \"context deadline exceeded\" is printed whenever pachctl is not able to connect to the server when invoking any command that communicates with it. It's not a very clear error.. This should be fixed in 1.5 right?. #546 and this issue are similar but will be used in different use-cases.\nIn my use-case, the compressed files are only accessed once during their entire lifetime so theoretically #546 fits unless I'm experimenting. In this case, you don't want to decompress again and again.\nIn the future, multiple pipelines will use the same file which will also require repeated decompression if I use #546.\nThis is why TTL is a good addition.. But if foo is constantly updated appends will happen right?. I think that there are two issues here:\n\nBinary files are essentially immutable in Pachyderm.\nAppend semantics are not documented anywhere and there is no list of formats that support mutation in Pachyderm.. After a discussion with @JoeyZwicker we figured out that binary files are essentially immutable only between different workers of the same job.. If you upgrade to Go 1.8 you can provide more information. See https://github.com/golang/go/commit/b9fd510cd00b6aa26e2ea7001a07b90ebf97d2ed. You should check if Struct or Field is empty.\nSee https://github.com/golang/go/commit/b9fd510cd00b6aa26e2ea7001a07b90ebf97d2ed#diff-4299c3082a8bb7e546723132c992fa4cR120. FYI the go port of xxhash now implements the CRC algorithms. See https://github.com/OneOfOne/xxhash/blob/master/xxhash.go#L79. It does if you specify -a as well.. That works as a workaround. Didn't know that the yes commandline existed.\nShould we at least document this somewhere?. @jdoliner Feel free to close this if we shouldn't document this anywhere.\nOtherwise, it's probably best to add this to the documentation website which I can do.. For any production usage, your users need to use a provisioned IOPS drive for etcd pods.\nThis is true for any database but it is not common knowledge among data scientists.\nI don't think you should provision it yourself as more money is involved when provisioning such drive.\n\nFurthermore, for any high load usage where containers are created and destroyed a lot a provisioned IOPS drive is also required to avoid throttling the system.. The AWS deployment guide is a good place to document such recommendations.. Does this fix https://github.com/pachyderm/pachyderm/issues/1960?. See #2042 regarding using better hash algorithms. In this context we can use a a hash function that produces shorter hashes such as xxhash.\nAlso, #2031 is relevant to this issue.. So this will not happen?. No we do not pass the current linters which is why I made the linters to not fail the build.\nThere are a lot of issues that needs fixing if we were to merge this as is.\nI need some input on how to configure the linters to your satisfaction. . @jdoliner ping?. Given that there are a lot of linter errors I don't think it's fair to ask me to resolve all of them.\nI'm gonna close this for now.. Just noting that this isn't documented but should be. There's no other way of discovering the implementation of this feature other than exploration which some users won't do.. So this fixes #2089 in another way than what I suggested?\nCan we avoid committing in case the whole file has not been uploaded yet?. It is possible to clone just one commit with Git>2.5 (which is pretty old).\nSee https://stackoverflow.com/a/30701724/85140. Also, the releases page is missing the changes for each release for 1.6.4 and 1.6.5.. ~~Seems like travis refuses to run node related stuff in CI.\nWe'll figure this out later :)~~\nNevermind. It works.. Ping?. Sure. I can also try to activate one of the pre-installed nodejs programs somehow. I can separate it to a second PR. . @sjezewski done :). @sjezewski Ready for merge :). @dwhitena Ping?. Not sure if it's actually worth it. Let's check :). I'll need you to run the CI job so we'll be able to actually compare between the old build time and the new one.. Seems like it's not the bottleneck of the build.\nYour call.. 11s instead of 18s. Not bad :). If we had #1068 we could use https://github.com/mcuadros/terraform-provider-helm.. In that case, please deprecate all other deployment methods and migrate to using the Helm chart + Terraform.\nHelm is not a huge dependency to introduce and most if not all k8s clusters already have it installed.\nAlso, using Helm will concentrate our efforts to play nicely with the entire k8s ecosystem.\nTerraform is used in nearly every DevOps shop. You do not have to require it to deploy Pachyderm but recommending it is a good idea.. minikube's fault. Sorry for the noise.. I need to copy .minikube from another user and everything worked.. See #1482 for another use case.\nI have data in Kafka that I want to commit to pachyderm without doing so periodically.. Shouldn't this be configurable?\nIn fact, if ChunkSize is a constant it can waste some bandwidth if the remaining chunk is the last chunk and it is less than 16MB.\nIt'd be useful to have the chunk size as part of the protocol.. Why shouldn't we use CRCs here? See #2042 . I didn't notice that this code was simply moved from another file. I thought this was hashing specifically for resumable uploads. Sorry for the noise.  It's an issue for a separate PR.\nAs a side note, CRCs are not a replacement for proper hashes since they are not 100% accurate. They accompany hashes.\nCRCs don't have to be available but when they are, you are likely to speed up comparison between blocks.\nWhat I meant is to add an optional CRC field for Block and if it's not nil compare it to the other block.. That comment made me think that the code requires all chunks to be 16MB so if the last chunk is retransmitted it has to be exactly 16MB so some data is retransmitted and overwritten although it is already there.. You can non longer -> You can no longer. ",
    "LaurentGoderre": "@thedrow is there a way to use another version of pachydem with that helm chart. I am experiencing a lot of issues that have already been fixed in later v1.7.x versions.. @gabrielgrant I created a PR to update the chart to 1.7.11 which would solve a lot already. I'll take a look at helping with a 1.8 chart as well!\nPS I'm also working on a pachctl docker image. @jonandernovella yes but if you do the basic helm install from the README like I didn't you end up with an old version with bugs that have long been fixed.. IMHO, the developer experience gain are well worth it but at the minimum I think the doc should make it clearer about how to use more recent versions. @jonandernovella aaah! I get you on that. Would it be possible to merge my PR as we look into upgrading for 1.8 and automating this process?. @ysimonson can you explain your case 2. Running explicit port forwarding (which additionally allows you to set a namespace.)\nI did run explicit port forwarding but I was getting tons of errors on all pachtcl commands about implicit port forwarding not enabled because of an error.. @JoeyZwicker I would strongly recommend using FROM scratch and adopt the official image standard. Otherwise you will not get the full benefit of the images security scans and automated security updated you get from Docker Hub . @jdoliner for connecting to pachd from a container, wouldn't it simply be a matter of setting -e ADDRESS=[ip to pahcd]?. Here's the repo that creates the images: https://github.com/LaurentGoderre/docker-pachctl. /cc @tianon. @tianon, what about all the other achs? ALso, I have a preliminary working version here: https://github.com/LaurentGoderre/docker-pachctl. @gabrielgrant if we can create the binaries for all the arch using static linking, we can probably get this as an official image and all the different arch will be managed in docker hub.. Amazing @tianon!! Thanks for this!. ",
    "kav": "Adding a +1 for support here, shame it isn't a simple fix. It also looks like a no dice from Sam. I'll likely workaround with a pre-parse in the meantime. Looks like on OSX $DOCKER_CONFIG isn't defined on OSX, on my box at least (17.09.0-ce-mac35 (19611)) which is causing the upstream lib's check to pass on to later clauses, I'll file a bug over there or on Docker for OSX depending on whether the removal was intentional on Docker's part.\nClosing for now as this is upstream. Will update with upstream issue. For those googling in the meantime export DOCKER_CONFIG=~/.docker/ will fix the issue.. ",
    "homme": "I currently use y2j to convert our annotated YAML to a JSON spec for running in Pachyderm.  It works fine but it would be great to see native support in a similar vein to kubectl.. > After a slack conversation we realized we don't have pachctl create-pipeline --dry-run so we don't have the basic building block for a reusable pipeline.\nI guess this issue could be focussed on updating actual pipeline specs (which I agree would be useful) and we could have a separate issue for adding something like --dry-run to create-pipeline, update-pipeline and run-pipeline for tweaking the more general k8s deployment config.. I've just discussed a workflow on Slack with @dwhitena where it would be useful to be able to manually commit to a repo that is normally the output of a pipeline, and then have downstream pipelines depending on that repo be triggered to run against those manually committed datums.\nThe issue is that we have a downstream pipeline vrt-dataset-rasters run against an input repo with a large number of datums (say 50,000) that have been generated by the upstream pipeline list-dataset-contents.  This upstream pipeline fans out from one input datum.  The vrt-dataset-rasters pipeline runs successfully.  Subsequently, however, we discover that we need to re-run a few of those datums.    We obviously don't want to have to process all inputs to vrt-dataset-rasters again, and because the list-dataset-contents fans out we can't update its input to manipulate its output.  Therefore we copy the output datums that we're interested in reprocessing from list-dataset-contents and commit them to the master branch of that repo; we expect this to kick off  vrt-dataset-rasters to reprocess those new commits... only it doesn't.\nThe workaround I'm using after speaking to Daniel is to commit the datums for reprocessing to a new repo, and update the vrt-dataset-rasters pipeline to both point to that instead of list-dataset-contents and set the input.atom.name (so that we don't need to change the pipeline code).  This works, but seems a bit more unwieldy in terms of workflow than it needs to be.  Unless I'm missing something...?. Thanks for the update @jdoliner and your suggestion for having the pipeline take to inputs in a union is a very good one - I'll go down that route!. I've torn the cluster down and rebuilt, so please contact me if you need access to replicate the issue.. @dwhitena Thanks for following this up.  Yes, I agree it would be unsafe and would possibly forfeit any Pachyderm guarantees, but taking this into account it would be a useful 'power user' feature that would increase the flexibility of Pachyderm.\nThe workaround I'm going to look at goes something along these lines: kubectl get -o yaml rc/pipeline-my-pipeline-v1 | my-update-script | kubectl apply -f -.\nI would also add run-pipeline to the commands that could benefit from a --dry-run option.. @jdoliner Adding a tolerances section would solve the issue in this case, although the --dry-run option would be nice to have in the future.. @jdoliner no, pachd appears to be stable.  I've run the failing command a few times recently but thepachd pod has been running for 12 hours.. @jdoliner my bad, I wasn't looking at a UI showing restarts, just age: here is the log:\n```\nkubectl logs pachd-29183766-7zbm8 --previous\ntime=\"2017-10-16T21:47:31Z\" level=info msg=\"validating kubernetes access returned no errors\" \ntime=\"2017-10-16T21:47:31Z\" level=info msg=\"AWS deployed with cloudfront distribution at drjitb8uzax3c\n\" \ntime=\"2017-10-16T21:47:31Z\" level=info msg=\"Using cloudfront security credentials - keypair ID (APKAJ2SKC3QL6C6K6SHA) - to sign cloudfront URLs\" \ntime=\"2017-10-16T21:47:41Z\" level=info msg=\"Launching PPS master process\" \ntime=\"2017-10-16T21:47:41Z\" level=info msg=\"master: creating/updating workers for pipeline list-dataset-contents\" \n2017-10-16T21:47:41Z INFO enterprise.API.GetState {\"request\":{}} \n2017-10-16T21:47:41Z INFO enterprise.API.GetState {\"duration\":0.000001485,\"request\":{},\"response\":{}} \ntime=\"2017-10-16T21:47:41Z\" level=info msg=\"master: creating/updating workers for pipeline vrt-dataset-rasters\" \n2017-10-16T21:47:41Z INFO enterprise.API.GetState {\"request\":{}} \n2017-10-16T21:47:41Z INFO enterprise.API.GetState {\"duration\":0.000001367,\"request\":{},\"response\":{}} \ntime=\"2017-10-16T21:47:41Z\" level=info msg=\"master: creating/updating workers for pipeline tile-boundaries\" \n2017-10-16T21:47:41Z INFO enterprise.API.GetState {\"request\":{}} \n2017-10-16T21:47:41Z INFO enterprise.API.GetState {\"duration\":0.000001388,\"request\":{},\"response\":{}} \ntime=\"2017-10-16T21:47:41Z\" level=info msg=\"master: creating/updating workers for pipeline tile-creation\" \n2017-10-16T21:47:41Z INFO enterprise.API.GetState {\"request\":{}} \n2017-10-16T21:47:41Z INFO enterprise.API.GetState {\"duration\":0.000001303,\"request\":{},\"response\":{}} \ntime=\"2017-10-16T21:48:31Z\" level=info msg=\"objectCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2017-10-16T21:48:31Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2017-10-16T21:48:31Z\" level=info msg=\"objectInfoCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2017-10-16T21:49:31Z\" level=info msg=\"objectCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2017-10-16T21:49:31Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2017-10-16T21:49:31Z\" level=info msg=\"objectInfoCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2017-10-16T21:50:31Z\" level=info msg=\"objectCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2017-10-16T21:50:31Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2017-10-16T21:50:31Z\" level=info msg=\"objectInfoCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2017-10-16T21:51:31Z\" level=info msg=\"objectCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2017-10-16T21:51:31Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2017-10-16T21:51:31Z\" level=info msg=\"objectInfoCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \n2017-10-16T21:51:59Z INFO pfs.API.ListFile {\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"tile-boundaries\"},\"id\":\"fee4caefa03347f7a0e197da949b899c\"}}}} \n2017-10-16T21:51:59Z INFO authclient.API.Authorize {\"request\":{\"repo\":\"tile-boundaries\",\"scope\":1}} \n2017-10-16T21:51:59Z INFO authclient.API.Authorize {\"request\":{\"repo\":\"tile-boundaries\",\"scope\":1}} \n2017-10-16T21:51:59Z INFO authclient.API.Authorize {\"request\":{\"repo\":\"tile-boundaries\",\"scope\":1}} \n2017-10-16T21:51:59Z INFO pfs.BlockAPI.Obj.GetObject {\"request\":{\"hash\":\"400433ebdafa1d9ee97ff97efb54b240fe5adba6f29820cbd09a1aa5236dc518cb5c87ce415d32d25bbc3eb8fa93a3bae1ad4b37e2a585165d414f11d7acac09\"}} \n2017-10-16T21:51:59Z INFO pfs.BlockAPI.Obj.InspectObject {\"request\":{\"hash\":\"400433ebdafa1d9ee97ff97efb54b240fe5adba6f29820cbd09a1aa5236dc518cb5c87ce415d32d25bbc3eb8fa93a3bae1ad4b37e2a585165d414f11d7acac09\"}} \n2017-10-16T21:52:00Z INFO pfs.BlockAPI.Obj.InspectObject {\"duration\":0.072669047,\"request\":{\"hash\":\"400433ebdafa1d9ee97ff97efb54b240fe5adba6f29820cbd09a1aa5236dc518cb5c87ce415d32d25bbc3eb8fa93a3bae1ad4b37e2a585165d414f11d7acac09\"},\"response\":{\"object\":{\"hash\":\"400433ebdafa1d9ee97ff97efb54b240fe5adba6f29820cbd09a1aa5236dc518cb5c87ce415d32d25bbc3eb8fa93a3bae1ad4b37e2a585165d414f11d7acac09\"},\"block_ref\":{\"block\":{\"hash\":\"3b10865b4e2044f9a34f136afdbd9dd9\"},\"range\":{\"upper\":2522464138}}}} \n``. Is the expiry something that can be updated as a job progresses in order to make sure it never gets triggered no matter how long a job runs for?. If I update a job usingupdate-pipeline` should that refresh the expiry?  Just trying to think of a practical workaround in the short term.... @gabrielgrant @sjezewski just to let you know the Pachyderm team is always welcome to have access to our cluster to help in the debugging.... Thanks @gabrielgrant that's appreciated.  In the meantime I'm going to investigate options for setting up a workaround for that step by hooking up our pipeline docker images to a job queue for the I/O.. @sjezewski thank you for the detailed insight - that's really interesting. I'll try and reply to your questions:\n\nI would still like to know what the max file size is for all the inputs to this pipeline\n\nThe inputs are small JSON documents that are less than 1KB in size.  They actually reference much larger resources on CloudFront that we consume in the pipeline.\n\nthis workload involves ~1M files, correct?\n\nIt actually involves around 4.5 million files.\n\nIt's possible that we've hit a rate limit w cloudfront. That seems like it shouldn't happen ... cloudfront is a CDN and should be designed for this.\n\nI think it's very possible. I have been dealing with similar issues in the pipeline code (yesterday in testing we hit CloudFront with ~30 million requests). CloudFront doesn't like this, and will often return a 503 Service Unavailable response which I deal with successfully using simple retry logic (it's a bash pipeline ;)). \n\nIt would be great if we could get a file count for an individual datum. But barring that, your total file count and glob patterns will suffice.\n\nThe individual datums for the large pipelines (after fanning out) consist of the single JSON files I mentioned.  A typical glob pattern is /*/*/*/*.json.  In an effort to optimise I have set atoms to be lazy as I can get the information I need from the directory structure in these cases.  I haven't got an exact figure to hand but as I mentioned total file count is in the order of ~4.5M. \n\nI believe we already have a semaphore for rate limiting our downloads from an obj store. I can try lowering that bandwidth and seeing if that helps. It will make it so that downloads take a bit longer ... but they will eventually succeed if we're right.\n\nThat sounds good.  Perhaps some kind of exponential backoff that feeds into a variable that is global to a worker so that request rate just hovers on the edge of what CloudFront considers acceptable? Adding some randomness to that in terms of increasing downwards pressure on that limit could ensure that we're being as efficient as possible. The worker could even share that value around so that new workers are seeded with something useful.\n\nI'd also like to get access to the cluster\n\nI'll happily give you access to the cluster when I've got it up and running again - I tore it down and rebuilt it, but I need to create a new pachyderm bucket as @gabrielgrant and I think the existing one is corrupted.\n\n1M files per datum is a use case we want to support. \n\nWe're currently dealing with the baby dataset (~2.2TB) - the big one is 10TB :)  I haven't done a calculation of total files for that yet!. ",
    "dahankzter": "I still get this result:\n```bash\nbrew tap pachyderm/tap\nbrew install pachctl\n==> Installing pachctl from pachyderm/tap\n==> Downloading https://github.com/pachyderm/pachyderm/releases/download/v1.2.6/pachctl_1.2.6_darwin_amd64.zip\ncurl: (22) The requested URL returned error: 404 Not Found\nError: Failed to download resource \"pachctl\"\nDownload failed: https://github.com/pachyderm/pachyderm/releases/download/v1.2.6/pachctl_1.2.6_darwin_amd64.zip\n```\nPerhaps I misunderstood?. Hmm I think it is broken because I removed it before (brew complained a lot) and now I can't use switch or anything else that allows versions since it seems it has to be installed at least once first. :(\nIll look around to see if there is something I missed about homebrew details.. Works now thx!. There is a fairly large set of connectors from which ideas and perhaps corner cases with partition handling can be pulled in https://www.confluent.io/product/connectors/. I don't see how append could ever have been useful? There are versions available if needed why would appending to the file make sense?\nPerhaps I am too stuck in the git way but it seems that replacing the file in a new commit is the way to go?\nIssuing a delete first also requires finding the latest commit and then do a delete right?\nSeems a little counter intuitive and a bit cumbersome to code it. Do I have to traverse all the history and delete the file in every commit it has ever taken place?. Thanks @JoeyZwicker ! It was a bit of a confusion on my part and it is much clearer now. \nI still think the append by default is weird though. Other than a legacy behaviour is there any reason to maintain it? Having putfile behave as a \"replace\" would be conveniten but perhaps I have not considered enough ramifications. . It would be good if we could supply a mode to control if overwrite or\nappend should be used even in an open commit.\nOn Tue, May 9, 2017, 04:55 Derek Chiang (Enchi Jiang) \nnotifications@github.com wrote:\n\nChatted with @JoeyZwicker https://github.com/JoeyZwicker offline. One\nopen question that occurred to us was what the behavior should be when you\nare putting the same file twice in an open commit. Should one put overwrite\nanother? Or should they append? The former seems dangerous since you are\nlosing writes. The latter feels inconsistent with what we are proposing\nwhich is to have the default behavior of put-file being replace.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/1531#issuecomment-300047499,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAKcU-P-0ifttiAVy6dQEYO1i-etHxRIks5r39WcgaJpZM4Mupjj\n.\n. There seems to be breaking changes but perhaps it is just my lack of knowledge of govendor and how pachyderm is built that is lacking.. bash\nminio_client.go:135: invalid operation: errResp == sentinelErrResp (struct containing http.Header cannot be compared)\nThe ErrorResponse now contains http.Header which is a map so it cannot be compared.\nSimply removing the explicit sentinel compare at least compile.. Any news on this? Will it make it into the next RC?. Very nice! . Any eta on that RC? :). Works like a charm!. \n",
    "rrichardson": "I altered PACH_ROOT to be 'pach/dir' and got the same result. \nEnvironment Variables:\n      PACH_ROOT:                /pach/dir\n      NUM_SHARDS:               1\n      STORAGE_BACKEND:          AMAZON\n      PACHD_POD_NAMESPACE:      default (v1:metadata.namespace)\n      JOB_SHIM_IMAGE:           pachyderm/job-shim:1.3.0\n      JOB_IMAGE_PULL_POLICY:    IfNotPresent\n      PACHD_VERSION:            1.3.0\n      METRICS:                  true\n      LOG_LEVEL:                info. digging into a hung pod,  I found this interesting : \nroot@5bf9b8ab5024a6f4c68e4f39e5ad02d6-rh3y9:/go# ip addr                                                                                                                                                          \n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n3: eth0@if755: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc noqueue state UP group default \n    link/ether 7a:51:4e:c8:aa:2b brd ff:ff:ff:ff:ff:ff\n    inet 100.96.15.255/24 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::7851:4eff:fec8:aa2b/64 scope link \n       valid_lft forever preferred_lft forever\nIf that's doing what I think its doing, the container's IP is the broadcast IP..  which won't end well... It looks like there is a bug in my version of CNI. \nhttps://github.com/kubernetes/kops/issues/724. It's definitely a bug, and its in the networking.  Pinging pachd definitely won't work :). The workaround, as mentioned in the kops issue above, is to download a later CNI release and copy the bits from bin over your existing cni/bin.  In the case of kops, this is in  /opt/cni/bin  \nThe release I used was : \ncni-07a8a28637e97b22eb8dfe710eeae1344f69d16e.tar.gz. Assuming you treat your vms like cattle, and don't use a deployment tool (only a provisioning tool)\nparallel-scp and parallel-ssh makes this task rather easy. . It appears that this :  https://github.com/pachyderm/pachyderm/pull/1247  didn't fix the problem.  Might have to enumerate the pods and delete them directly.  :(. ",
    "teodimache": "I would very much like to see C# on that list if possible :)\nIt is a gRPC/Protobuf supported language after all. and it's gaining in popularity now with .NET Core being open source and multi-platform.. ",
    "abourget": "Yeah Pachyderm is just not useable on 1.5, because of this issue and this issue. Does this make the latest release incompatible with previous releases of Kubernetes ?. Ok, let's close this.. I'll re-open if I find a better way to repro.. Not a problem for me right now.. I'd also like to have pachctl logs job/job-id (with -f ?), that would mimic kubectl better too.. and it's a different verb.. get-logs mixes up things :). Could be from_commit, to clarify and ease the transition path.. we have some steps that write to an external DB.. and it doesn't make sense to write out of order..\nso it makes re-running this pipeline with from a no-go.. as it would go and write out of order... Perhaps the list-commit view could show the total size, and incremental size (what was added in this commit)..\nI'm not sure what the SIZE column in list-commit is  right now.. Upon commit, that could be saved somewhere, and summed up easily for list-repo ?\nI understand we could be tempted to list the size of the backing store, but most of the time, I'll want to know if my data is right, if I've been receiving the right amount of data, to assess if my pipelines are properly working, etc.. that's going to be day-to-day. Backing-storage will be a concern once in a while.. but I wouldn't put that first and foremost. Perhaps behind -o wide  ? :). I'm looking at the args passed to an sh and I can't find that information in the generated ReplicationController.  So it seems that create-pipeline stores some metadata not in the RC. This would need to be revisited if we were to allow definition of pipelines solely by means of kubectl create -f my-pipeline-deployment.yaml ..\nwe could have pachyderm.io/sh-arguments as an annotation, and that would be picked up by the job dispatcher somehow ?! :). hmm.. having issues logging into Wufoo.. will try later (I'm locked out now :P). Ok I signed the CLA.. Oh new conflicts! Let me fix that.\nWhen could we expect that? in the 1.4 series ?. Folks, how complex would it be to have env vars of the SOURCE commits?  maybe not the OUTPUT commits as we don't have it when we run user code. I'm looking for a way to get traceability after writing to a database. I'd write in my records something so I can find the source back.\nI think source commit would be fine already. and as there are multi possible inputs, we could simply uppercase the name, and replace - with _.. to something like PACH_INPUT_SESSION_SHARDS_COMMIT with a source repo of session-shards.\nWhat do you think ?. When using get-file and doing some --recursive, it seems the code uses filepath.Join() or something, as it uses backslashes even to pass file locations to etcd. I think those occurrences should be using path.Join() instead... It works for OS X and Linux, but some such uses will break under Windows.\nEx: \"no node at /raw-session\\2017-02-02\\asdflkajsdflkajsdf.json\". Ok, removing the comment.. ",
    "orangefuzzball": "These credentials are only used for the s3 bucket and the ebs store?. ",
    "binary-finary": "list-job returns:\nID                                 OUTPUT                                      STARTED             DURATION             STATE             \n67c30d70ba9d2179aa133255f5dc81db   filter/9d3816237ecc4e32b38fd3e577376a07/0   5 seconds ago       Less than a second   failure  \nI assume the first hash is the job id, but then I get:\n$ pachctl get-logs 67c30d70ba9d2179aa133255f5dc81db\njob 67c30d70ba9d2179aa133255f5dc81db not found\nSorry if I've misunderstood your instructions.\n. Seems like my pod was removed...\n$ kubectl get pod -a\nNAME               READY     STATUS      RESTARTS   AGE\netcd-srw0p         1/1       Running     0          1h\npachd-init-m5r6x   0/1       Completed   0          1h\npachd-m99l2        1/1       Running     1          1h\nrethink-zttvw      1/1       Running     0          1h\nI've looped through the demo script several times now (including deleting the minikube), and the result is always the same (mravi on your Slack channel has confirmed this as well).  Are there extra debugging options to track that down?  I could also just wait for your 1.3.2 release if that's easier for you.\nI'm running OSX 10.11.6 and VirtualBox 5.1.10.\nThanks\n. I started by deleting the minikube, creating the pods via the suggested manifest and reloading the data repo.  I then issued a create-pipeline and followed with:\n$ pachctl list-job\nID                                 OUTPUT                                      STARTED             DURATION             STATE             \n67c30d70ba9d2179aa133255f5dc81db   filter/8f4e6468bacd43818dfa878b751fabb3/0   2 seconds ago       Less than a second   failure    \nAnd just a couple seconds later issued a:\n$ pachctl get-logs 67c30d70ba9d2179aa133255f5dc81db\njob 67c30d70ba9d2179aa133255f5dc81db not found\nAnd a couple seconds after that:\n$ kubectl get pod -a\nNAME               READY     STATUS      RESTARTS   AGE\netcd-6m5j2         1/1       Running     0          6m\npachd-g46bp        1/1       Running     0          6m\npachd-init-4kdh9   0/1       Completed   0          6m\nrethink-c15s6      1/1       Running     0          6m\nAm I doing this correctly?  Any environment setting you want to know?  Happy to keep trying if this is worth your time.\nThanks\n. Yes, I verified K8s was free of P7m (did I coin that term?) after the minicube reboot.  The attached logs contain several error messages, so we're making progress.\npachd_logs.txt\n. Thanks for working through this.  I'll verify 1.3.2 fixes the problem and report back on this thread.. The core of our use case involves a rich model for timeseries events.  Using your fruitstand example, this first layer of time are just the recorded sales appended to the logs.  A second layer would be revisions to that history.  For example, consider the scenario where someone submitted 6 banana sales on Monday, but on Tuesday audited the receipt, realized the 5 was misread as a 6.  A third layer of time is when the updates were actually written to disk.  So  although the error was corrected on Tuesday, we blew away the database and underlying bits were set on Friday.\nReading your documentation on \"Incrementality\" leads me to believe you've done more thinking on this subject than most, but I'm not sure you are all the way there.  For example, when you say that 1.4 will pump the \"new\" data into Spark, to which version of the \"new\" data are you referring?  I understand modern streaming frameworks are based around the event log, but I think that's too complicated for a job author to wrap their head around.  They need data to be presented to Spark in such a way that they can ask \"Roll back the wall clock to Friday and tell me about my Monday banana sales as I believed on or before Tuesday.\"\nAlso, I'm worried about the latency of spinning up Spark clusters on demand.  I would rather have something where the Spark cluster is always on and simply references files (virtual or otherwise) by address.  For example, if they wanted the fully qualified address they might use Monday/Tuesday/Friday.txt, or if they just cared about the global latest they could use Monday.txt and the spectrum in-between.\nTo be clear, I wouldn't be expecting you folks to develop this rich timeseries view and Spark integration yourself.  We could contribute code, hire you folks to take on the feature work we are requesting or some combination of the two.  The main thing I'm trying to sniff out at this point is whether you are architecturally interested in our use cases and open to the level of collaboration required to achieve the goal.  Happy to take that discussion offline if that's your preference.\nHappy Holidays.\n. Thank Joey, I'll setup a call later this week.. ",
    "shukla-deepesh": "Just checking if there is an update on this issue. We are looking for a similar functionality as per the discussion above ? . Thanks for looking into this . This is a deal maker for pachyderm IMHO ! \nFeatures we are looking and AFAIK many client that we deal with are looking for as well:\nYou have a 500 GB or more workload that needs to be run on Spark cluster.What is the most optimized way to achieve this  give we want data versioning of the input to and the output from the Spark cluster.. Looking for a batch processing.\nIdea is to be validate how we will version \"big data\" sets using pachyderm\nfor Spark ML use case .  Hope it is making sense.\nOn Thu, Jul 12, 2018 at 7:49 AM, Gabriel Grant notifications@github.com\nwrote:\n\n@shukla-deepesh https://github.com/shukla-deepesh to clarify: are you\nloading that full 500+GB into spark and processing it as a batch, or are\nyou hoping to stream it through?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/1225#issuecomment-404368199,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AYN5E-ULiwre1BPw9qIRLGnPBQcxSzVLks5uFrI_gaJpZM4LSVis\n.\n. \n",
    "orenyomtov": "Hey @jdoliner,\nIt happened when running pachyderm on a 100 node cluster on AWS.\nBecause of the costs, I'm wary of attempting to reproduce.\nI would guess it will happen again, because it's not the first time I was encountering problems at this scale, just the first time I've documented and dug deeper.\nAs for whether they restarted and recovered or not - they got into a perpetual restarting loop.. ",
    "roylee17": "Okay. Signed the CLA.. I guessed grouping / sub command definitely have been thought of in the beginning. Just wondering has it been reconsidered as the number of command grows. The PR was more like a preview of the look and feel it might be.  I started with CLI, which I previously used, to only implement a subset of the commands. But it didn't take long to bring all command over.  Cobra seems to be more favored in some popular projects, though.  . Currently, some external packages altogether form a \"ecosystem\" on its own. It's rather difficult to change/update/remove any of them without hacking the rest. And when the dependencies involve protobuf, it get much more complicated.\nThis PR is a breakdown of a much larger WIP branch, which removes more external dependencies.\n(So itself is not vey clean for merge yet, you can tell from the lasts two dumb commits)\nBut before I \"clean up the cleanup\", I'd like to know the team is up to it. Glad the team is positive about it :-)\nIf we don't have a strong affinity to stick with specific package or implementation, we can take a further step to use independent alternative packages, or just port it over. (proto/stream, rpclog, version, etc)\nI can keep update the mergable changes on the branch tracked by this PR, so we can keep review it and feedback until we feel comfortable with all the changes.\nIn the meantime, don't worry about any conflict if you're actively working on large change set  or refactoring. I can keep rebasing accordingly along the way.\np.s. The CI failed due to some changes in the _test.go were left out in the last moment re-grouping before PR. I'll fix it in the next update.. @derekchiang didn't see your comment earlier. I just squashed similar change, and rebase to master, so the problem probably still exists. Ignore this push. I'll try to reproduce it and get back.\nKeep commenting any other issue/difference found between the changes, thx.. @derekchiang the last two commits should address the issues found. I went through the fruit_stand tutorial.. make example-tests passes.\nNot sure it's my environment or something, I still haven't got make test-fuse test-local working locally. make integration-tests can't finish as my minikube might be too mini...\nMight have to trouble travis  a few more times before I figure out what's missing on my set up.\n. Cool, thanks for putting the efforts to take this in.. It seems to be the preferred place maintained by open source. \nRefer to the last comment from @sethvargo in https://github.com/mitchellh/vagrant/issues/7155\n. Me, either. I thought it was the dev environment this morning, but then figured out the docker flow later on the slack. Will try the host environment later.. I guest it's just the existence. Tried host and docker first without luck in the first time. So turned to Vagrant, and realized it was just empty ubuntu + go.\nAdding a dev guide section in the documentation will definitely help.\nAnother thing might help is to break up the Makefile a little, as it now has more than 60+ targets. Or at least reorder the targets so recommended flow will be spotted more easily.\nFor example, this morning I tried the following\nmake\nmake build\nmake deps; make build\nmake proto\nThen I saw the Dockerfile(s) in the root. So\ndocker build -t pachyderm\ndocker run -it pachyderm\nmake deps; make build;\n...\nStill no luck. So I turned to slack for help.\nHowever, if I just keep scrolling down the Makefile a coupe of pages, I might have found the docker-build and found it earlier. \nmake docker-build. ",
    "willis7": "Appears similar to 439. @jdoliner \nUnfortunately, it still doesn't work. I restarted the kubelet with the --privileged=true flag, as suggested, but it failed with invalid flag. After some digging I found that the correct flag was --allow-privileged. Even with this flag set I get the same error.\nYou can see below the kubelet is started with allow privileged set.\n\u25cf kubelet.service - Kubernetes Kubelet\n   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: enabled)\n   Active: active (running) since Wed 2017-01-04 14:32:51 GMT; 19s ago\n     Docs: https://github.com/kubernetes/kubernetes\n Main PID: 3808 (hyperkube)\n    Tasks: 12\n   Memory: 36.7M\n      CPU: 621ms\n   CGroup: /system.slice/kubelet.service\n           \u251c\u25003808 /usr/bin/hyperkube kubelet --logtostderr=true --kubeconfig=/etc/kubeconfig.yml --require-kubeconfig --hostname-override=10.10.0.2 --address=0.0.0.0 --api-servers=http://localhost:8080 --cluster-dns=10.0.0.10 --cluster-domain=k8s.local --allow-privileged\n           \u2514\u25003829 journalctl -k -f\nUnfortunately, as I'm on a Windows machine, I cant go the Minikube route either because there's currently no pachctl install for windows.\n. ",
    "sashabaranov": "@jdoliner done. ",
    "davidgasquez": "I see! Thanks for pointing me to make assets. Just ran it and committed the new assets.go file. Let me know if I should do anything else.. Nice! One thing I've noticed is that there are two types of documentation files, .rst and .md. It's probably for a specific reason but I was thinking if moving everything to Markdown would improve consistency in the formatting.   . I think it should work (PFS documentation is written in Markdown), although perhaps RST has more formatting options. . Ugh, seems like the CI build is still failing! \ud83d\ude05 . Thanks for the info @JoeyZwicker! I just wanted to point this super small thing in case you missed. Makes sense to keep both formats then. . Seems that the check are still failing... \ud83e\udd14 Anything I can do?. Awesome! I'm deleting the branch then! \ud83d\ude09 . I'm sure you've already looked at it but just wanted to share in case that's helpful. Would be possible to do something like Kubernetes does with the  container commands? \nDisclaimer: Not sure how they do and if it's compatible with Pachyderm \ud83d\ude09 . Ups! Didn't noticed the change @JoeyZwicker! Sorry for the assumption and quick edit without checking. I think it's now reverted.. ",
    "kortschak": "That does not fix the problem. Do I need to blow the old instances out of the water (they're just tutorial data, so this is not a problem)?. ",
    "ferrouswheel": "A trivial example is currently documented here: http://pachyderm.readthedocs.io/en/latest/reference/pipeline_spec.html#multiple-inputs. Have signed the CLA - cheers @JoeyZwicker . Is the union meant to have fizz as the second entry?. ",
    "harshavardhana": "answers inline\n\nI'd prefer to just call this the Minio backend rather than s3-compatible. A few reasons for this 1) it's a bit weird to have an s3-compatible backend and an s3 backend. Users might find that confusing. Eventually this backend might subsume the s3 backend in which case we'll consider this. 2) Lots of users have asked about a way to run this on prem, when this PR lands we're going to start recommending Minio as the best way to do that so it'll be more discoverable if we make the names match. 3) you guys wrote the code so I feel like you should get the name recognition that comes with it :)\n\nCertainly, i thought to keep it more generic and say Minio.  Let me change it to minio.\n\nIn addition to the backend code for Minio we'll also need some code that creates a k8s secret with the Minio credentials for deployment. For an example checkout how we create AmazonSecrets. @dwhitena you've already done a bit of legwork on this yes?\n\nI think i already did that with this patch - https://github.com/kubernetes/charts/tree/master/stable/minio\n\nIt'd also be nice to have an example of how to deploy Minio on k8s that we could put in our docs so people have a complete guide for how to deploy an on prem Pachyderm cluster.\n\nSure would a helm chart be sufficient for you guys? \n\nLastly we'll need you to sign our CLA\n\nAlready signed CLA as well. \n. I tried this not sure what ErrImagePull means. \n$ kubectl get pods\nNAME               READY     STATUS             RESTARTS   AGE\netcd-5xhsh         1/1       Running            0          28s\npachd-init-sfbst   0/1       ImagePullBackOff   0          28s\npachd-zl31g        0/1       ErrImagePull       0          28s\nrethink-1k4g9      1/1       Running            0          28s . is this because i need to publish these? \n. $ kubectl logs pachd-init-bwcdk\ntime=\"2017-01-21T00:35:57Z\" level=warning msg=\"Error creating connection: gorethink: dial tcp 10.0.0.137:28015: getsockopt: connection refused\" \ngorethink: dial tcp 10.0.0.137:28015: getsockopt: connection refused\n$ kubectl logs pachd-kd0gv\ngorethink: Database `pachyderm_pps` does not exist. in: \nr.DB(\"pachyderm_pps\").Table(\"JobInfos\").Wait()\n$ kubectl logs pachd-kd0gv\ngorethink: Database `pachyderm_pps` does not exist. in: \nr.DB(\"pachyderm_pps\").Table(\"JobInfos\").Wait() \nAfter changing these to use y4m4/ . $ kubectl get pods\nNAME               READY     STATUS             RESTARTS   AGE\netcd-wpt8m         1/1       Running            0          3m\npachd-init-bwcdk   0/1       CrashLoopBackOff   4          3m\npachd-kd0gv        0/1       CrashLoopBackOff   4          3m\nrethink-2160n      1/1       Running            0          3m . Looks like pods are using wrong ips they don't have 10.x.x. assigned and wrongly using a separate ip range as well are you guys aware of this @dwhitena @jdoliner ?. > @harshavardhana I haven't seen any problems like that with wrong IPs being assigned. Looking at the logs it seems that pachd-init is failing to get in contact with rethinkdb while pachd is succeeding but not finding the tables it needs because pachd-init isn't creating them. I'm a little confused about what exactly is being assigned a wrong IP address here, is it the rethink pod?\nActually it worked fine but had do more changes.. Finishing them now there were few checks to disallow any backend other than localBackend to serve rethinkdb i just choose HostPath for minioBackend as well.  Unlike EBS for amazonBackend. \nDoes this sound right to you @jdoliner @dwhitena ?. > Yeah that sounds reasonable, we're likely going to need to make things a little bit more sophisticated now since people will probably want to be able to do minio with an ebs volume for amazon deploys or a PD for GCE deploys or what have you. But that's not necessary for the purposes of this PR.\nUnderstood.. . > This minio deploy works for a locally deployed pachyderm. However, we really want to deploy our clusters to the cloud. This brings up an interesting question, because we could have a minio deploy to AWS, Google, or Azure. The deploy should be very similar, but we need to decide if we want 4 different commands (minio-local, minio-aws, etc.), or if we make subcommands etc\nThe problem i see with choosing different sets of credentials is that the CLI should be simplified  and it can get really hard to specify many such options on command line. \nI am not sure if there is a work on this area already - to make it simple. Ideally it would be better to be in a single command so that documentation becomes easier. Sub-commands for minio choosing different cloud flavors is also an option. \nAny other thoughts? - once we finalize i can work this out and send another PR after this. . Oops looks like i force pushed it and lost @dwhitena commit, Can you send it again? sorry. . > LGTM after those comments\nAddressed all the comments.. Taking a look @dwhitena . // cc @dwhitena . > @jdoliner yes LGTM!\nIs there a way to test with offsets and specific length with \npachctl get-file testminio master /blah \n?? - once this is done it would cover all the cases of reading an object. \n// cc @dwhitena @jdoliner . Tested with \n```\npackage main\nimport (\n        \"bytes\"\n        \"log\"\n    \"github.com/pachyderm/pachyderm/src/client\"\n\n)\nfunc main() {\n        // Connect to Pachyderm.                                                                                                                                                                                                                                                                                                                                                           \n        c, err := client.NewFromAddress(\"REDACTED:30650\")\n        if err != nil {\n                log.Fatal(err)\n        }\n    var buffer bytes.Buffer\n\n    log.Println(\"Testing reading all file\")\n    if err = c.GetFile(\"testminio\", \"master\", \"/blah\", 0, 0, \"\", false, nil, &buffer); err != nil {\n            log.Fatal(err)\n    }\n    if buffer.Len() == 0 {\n            log.Fatalf(\"Expected all the data, nothing sent\")\n    }\n    log.Println(\"Success.\")\n    origDataLen := buffer.Len()\n    buffer.Reset()\n\n    log.Println(\"Testing length > 0 and offset == 0\")\n    if err = c.GetFile(\"testminio\", \"master\", \"/blah\", 0, 100, \"\", false, nil, &buffer); err != nil {\n            log.Fatal(err)\n    }\n    if buffer.Len() != 100 {\n            log.Fatalf(\"Expected buffer value 100, got %d\\n\", buffer.Len())\n    }\n    log.Println(\"Success.\")\n    buffer.Reset()\n\n    log.Println(\"Testing length as zero and offset > 0\")\n    if err = c.GetFile(\"testminio\", \"master\", \"/blah\", 100, 0, \"\", false, nil, &buffer); err != nil {\n            log.Fatal(err)\n    }\n\n    if buffer.Len() != origDataLen-100 {\n            log.Fatalf(\"Expected buffer value %d, got %d\\n\", origDataLen-100, buffer.Len())\n    }\n    log.Println(\"Success.\")\n    buffer.Reset()\n\n    log.Println(\"Testing length > 0 and offset > 0\")\n    if err = c.GetFile(\"testminio\", \"master\", \"/blah\", 100, 1, \"\", false, nil, &buffer); err != nil {\n            log.Fatal(err)\n    }\n    if buffer.Len() != 1 {\n            log.Fatalf(\"Expected buffer value 1, got %d\\n\", buffer.Len())\n    }\n    log.Println(\"Success.\")\n    buffer.Reset()\n\n}\n```\nWorks for all offsets and length combination. . This implementation is similar to how it is done in other pfs backends such as amazon_client.go - code borrowed. \nThis PR fixes a problem in the Close() to be synchronized properly with the PutObject() completion. It is similar in behavior to how the amazon_client.go Upload() call happens. \nBecause the PutObject() only completes when the Close() is called i.e PutObject() observes io.EOF  and the multipart session completes.  But since Close() is deferred and happens on function return. \nThe next subsequent Reader() races with a concurrently occurring PutObject(). \n. Oh no this is not necessary. . Sure. sure. You are right we need to fix this in amazon_client.go and other pfs implementations which use Pipe in this manner. . ",
    "Amit-PivotalLabs": "Hi Joe,\nThanks for your response.  Any time I've tried to exec commands like dig,\nor interactive shell sessions, inside the pachd container, I get errors\nlike this:\n$ kubectl --kubeconfig ~/.kube/config.dev exec pachd-1q744 env\nrpc error: code = 2 desc = \"oci runtime error: exec failed: exec: \\\"env\\\":\nexecutable file not found in $PATH\"\nThis is why I mentioned previously that debugging via kubectl exec was\ndifficult.  Am I missing something basic?\nOn Mon, Jan 23, 2017 at 12:03 PM, Joe Doliner notifications@github.com\nwrote:\n\nHi @Amit-PivotalLabs https://github.com/Amit-PivotalLabs, sorry you're\nhitting this.\nWhen I've seen issues like this most of the time it's been due to\nsomething being wonky about the cluster's DNS setup. In this case it looks\nlike either pods can't connect to the kube-dns server or when they do\nconnect it's unresponsive. However the fact that you're able to curl the\nfiles from other pods that are deployed suggests that it does work in some\ncases so maybe pachd is doing something different than curl. Could you try\nexecing into the container and using dig to check that the DNS server\nworks? You can do this with:\ndig raw.githubusercontent.com @100.64.0.10:53\nDepending on what that returns we'll have an idea of whether or not the\ncluster DNS is functioning correctly. You may be able to sidestep this\nissue by bypassing kube-dns and just using a public dns server. This can be\ndone by passing --cluster_dns=8.8.8.8 (or some other dns server you like)\nto the kubelet on startup.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/1286#issuecomment-274600856,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AC9t3hgxs-Iut-dyW1xMSDAt6rkVD7Veks5rVQetgaJpZM4Lq70Z\n.\n. Thanks @jdoliner \n\nI had actually noticed pachd was built FROM scratch and so tried in the rethinkDB container, but didn't have dig, and was having trouble apt-getting.  I got further now. I ran apt-get update several times, it got hung resolving DNS, resolved DNS locally, and echo \"IP HOST\" >> /etc/hosts a bunch until it worked.  For the record, here's what I needed:\n72.14.189.19 download.rethinkdb.com\n128.101.240.215 security.debian.org\n128.31.0.66 httpredir.debian.org\n128.180.2.105 debian.cc.lehigh.edu\n128.30.2.26 debian.csail.mit.edu\n69.90.151.252 deb.vanvps.com\n64.50.233.100 ftp-nyc.osuosl.org\n128.112.18.21 mirror.math.princeton.edu\nWas then able to apt-get install dnsutils and I can dig things just fine with @8.8.8.8 but not with the Kube DNS IP.  Will dig more into why not later.. kubectl logs, kubectl get all, ssh'ing and and running htop or free didn't lead to anything interesting.  I wasn't able to reproduce wget working, and\nkubectl run kill \\\n  --image=tutum/dnsutils \\\n  --attach  \\\n  --command -- dig +notcp pachd.dev.svc.cluster.local @100.64.0.10\ncouldn't even connect to the server.\nGave up and just kubectl scaled the Kube DNS RC down to 0 and back up to 2, and now things are working (i.e. I can put-file with a remote URL).\nThanks for your help!. ",
    "Southclaws": "Alright, that makes sense (I wasn't sure on this either but noticed the next argument was a pps.ParallelismSpec object so I just copied).\nI will simply replicate the arguments for a Transform in this function then. This is all because pretty much all of our pipeline images require Env and ImagePullSecrets and I wondered why these weren't available in the CreatePipeline function. I suppose an even better approach could be to un-marshal a JSON string directly into a Pipeline request like how pachctl create-pipeline works?. I actually investigated client.PpsAPIClient.CreatePipeline when looking at how APIClient.CreatePipeline did it, but I'm not entire sure what to do about the ctx argument.. Alright brilliant thanks! (as you can probably tell, I'm rather new to the Go language and it's patterns!). Ah, I actually completely forgot to use the NewFromAddress function, that will solve the problem.\nHowever, I was under the impression that since NewInCluster is designed to be used in a Kubernetes cluster, it should internally use the domain method since that's the more robust and reliable solution for when services go down and come back up. Is there any specific reason that NewInCluster uses a constant IP address instead of a flexible kube-DNS string?\nI didn't notice that KeepConnected method in the docs (my bad!) that looks like a super useful solution for this. I think in the future, as the documentation matures and more examples of how to work with the Go client API, things like this should be covered so others don't make the same mistakes I do!\nAnyway, that HealthCheck idea does sound useful and that would be very useful in future when I can imagine more parts of our system will be interacting with Pachyderm but what you've responded with is good for now and will definitely ensure stability in our system so I suppose you could go ahead and close this, or at least mark it as low priority. Thanks!. I've always wondered this and just assumed it's for backwards compatibility.\nStill, would be nice to see a migration to what's recommended as long as there are no downsides.. Some extra info in case it helps, this is happening on our GKE cluster with Kubernetes 1.5.4, Pachyderm 1.4 tried with both a Google storage bucket and a custom backend (temporary Minio server running inside Kubernetes). The cluster node-pool has \"storage-rw\" scope.\nA 1-file commit successfully went through (StartCommit, PutFileURL, FinishCommit) with no errors thrown.\nPipeline pods show up and describing shows:\n```\nName:           pipeline-jc2736f0e9b964200bf70f7cb047813adhelloworldgo-v1-17n0c\nNamespace:      spotlight\nNode:           gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw/10.132.0.5\nStart Time:     Tue, 04 Apr 2017 14:20:49 +0200\nLabels:         app=pipeline-jc2736f0e9b964200bf70f7cb047813adhelloworldgo-v1\n                suite=pachyderm\nStatus:         Running\nIP:             10.4.0.28\nControllers:    ReplicationController/pipeline-jc2736f0e9b964200bf70f7cb047813adhelloworldgo-v1\nInit Containers:\n  init:\n    Container ID:       docker://92d2a9d079acf844153817911443cb6bee4112ed2f8e0c296aa6d1b179ce5e02\n    Image:              pachyderm/worker:1.4.0\n    Image ID:           docker://sha256:c36ccffc2150b478467e4fc1ac8f2957563abd485edb138fc441c758882d1eca\n    Port:             \n    Command:\n      /pach/worker.sh\n    State:              Terminated\n      Reason:           Completed\n      Exit Code:        0\n      Started:          Tue, 04 Apr 2017 14:20:50 +0200\n      Finished:         Tue, 04 Apr 2017 14:20:50 +0200\n    Ready:              True\n    Restart Count:      0\n    Volume Mounts:\n      /pach-bin from pach-bin (rw)\n      /var/pachyderm_worker from pachyderm-worker (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-tnl33 (ro)\n    Environment Variables:\n      INPUT_DIRECTORY:          /pfs/jc2736f0e9b964200bf70f7cb047813adinput\n      OUTPUT_DIRECTORY:         /pfs/out\n      PPS_WORKER_IP:             (v1:status.podIP)\n      PPS_POD_NAME:             pipeline-jc2736f0e9b964200bf70f7cb047813adhelloworldgo-v1-17n0c (v1:metadata.name)\n      PPS_ETCD_PREFIX:          pachyderm_pps\n      PPS_PIPELINE_NAME:        jc2736f0e9b964200bf70f7cb047813adhelloworldgo\nContainers:\n  user:\n    Container ID:       docker://24792556c0ea352d78edbf5e8e91e24e0daa1b29e5ce9c42e9059930a65e550f\n    Image:              spotlightdata/worker_helloworld_go:latest\n    Image ID:           docker://sha256:ad88b46c20a2e5273bd22171ba75b3bb416c164aeee65227a8be072608249105\n    Port:             \n    Command:\n      /pach-bin/guest.sh\n    State:              Waiting\n      Reason:           CrashLoopBackOff\n    Last State:         Terminated\n      Reason:           ContainerCannotRun\n      Exit Code:        127\n      Started:          Tue, 04 Apr 2017 14:26:38 +0200\n      Finished:         Tue, 04 Apr 2017 14:26:38 +0200\n    Ready:              False\n    Restart Count:      6\n    Volume Mounts:\n      /pach-bin from pach-bin (rw)\n      /var/pachyderm_worker from pachyderm-worker (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-tnl33 (ro)\n    Environment Variables:\n      INPUT_DIRECTORY:          /pfs/jc2736f0e9b964200bf70f7cb047813adinput\n      OUTPUT_DIRECTORY:         /pfs/out\n      PPS_WORKER_IP:             (v1:status.podIP)\n      PPS_POD_NAME:             pipeline-jc2736f0e9b964200bf70f7cb047813adhelloworldgo-v1-17n0c (v1:metadata.name)\n      PPS_ETCD_PREFIX:          pachyderm_pps\n      PPS_PIPELINE_NAME:        jc2736f0e9b964200bf70f7cb047813adhelloworldgo\nConditions:\n  Type          Status\n  Initialized   True \n  Ready         False \n  PodScheduled  True \nVolumes:\n  pach-bin:\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:   \n  pachyderm-worker:\n    Type:       HostPath (bare host directory volume)\n    Path:       /var/pachyderm_worker\n  default-token-tnl33:\n    Type:       Secret (a volume populated by a Secret)\n    SecretName: default-token-tnl33\nQoS Class:      BestEffort\nTolerations:    \nEvents:\n  FirstSeen     LastSeen        Count   From                                                            SubObjectPath                   Type            Reason          Message\n  ---------     --------        -----   ----                                                            -------------                   --------        ------          -------\n  10m           10m             1       {default-scheduler }                                                                            Normal          Scheduled       Successfully assigned pipeline-jc2736f0e9b964200bf70f7cb047813adhelloworldgo-v1-17n0c to gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw\n  10m           10m             1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.initContainers{init}       Normal          Pulled          Container image \"pachyderm/worker:1.4.0\" already present on machine\n  10m           10m             1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.initContainers{init}       Normal          Created         Created container with docker id 92d2a9d079ac; Security:[seccomp=unconfined]\n  10m           10m             1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.initContainers{init}       Normal          Started         Started container with docker id 92d2a9d079ac\n  10m           10m             1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}           Normal          Pulling         pulling image \"spotlightdata/worker_helloworld_go:latest\"\n  10m           10m             1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}           Normal          Pulled          Successfully pulled image \"spotlightdata/worker_helloworld_go:latest\"\n  10m           10m             1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}           Normal          Created         Created container with docker id 53caae63c8aa; Security:[seccomp=unconfined]\n  10m           10m             1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}           Warning         Failed          Failed to start container with docker id 53caae63c8aa with error: Error response from daemon: Container command '/pach-bin/guest.sh' not found or does not exist.\n  10m           10m             1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}           Normal          Created         Created container with docker id b84052c18d7d; Security:[seccomp=unconfined]\n  10m           10m             1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}           Warning         Failed          Failed to start container with docker id b84052c18d7d with error: Error response from daemon: Container command '/pach-bin/guest.sh' not found or does not exist.\n  10m           10m             1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}           Normal          Created         Created container with docker id 515e292eced5; Security:[seccomp=unconfined]\n  10m           10m             1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}           Warning         Failed          Failed to start container with docker id 515e292eced5 with error: Error response from daemon: Container command '/pach-bin/guest.sh' not found or does not exist.\n  10m           10m             2       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}                                       Warning         FailedSync      Error syncing pod, skipping: failed to \"StartContainer\" for \"user\" with CrashLoopBackOff: \"Back-off 20s restarting failed container=user pod=pipeline-jc2736f0e9b964200bf70f7cb047813adhelloworldgo-v1-17n0c_spotlight(23910131-1931-11e7-b11f-42010a84000c)\"\n9m    9m      1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}   Warning Failed          Failed to start container with docker id c07ea39c038f with error: Error response from daemon: Container command '/pach-bin/guest.sh' not found or does not exist.\n  9m    9m      1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}   Normal  Created         Created container with docker id c07ea39c038f; Security:[seccomp=unconfined]\n  9m    9m      2       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}                               Warning FailedSync      Error syncing pod, skipping: failed to \"StartContainer\" for \"user\" with CrashLoopBackOff: \"Back-off 40s restarting failed container=user pod=pipeline-jc2736f0e9b964200bf70f7cb047813adhelloworldgo-v1-17n0c_spotlight(23910131-1931-11e7-b11f-42010a84000c)\"\n9m    9m      1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}   Warning Failed          Failed to start container with docker id 518f0de9c117 with error: Error response from daemon: Container command '/pach-bin/guest.sh' not found or does not exist.\n  9m    9m      1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}   Normal  Created         Created container with docker id 518f0de9c117; Security:[seccomp=unconfined]\n  9m    7m      7       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}                               Warning FailedSync      Error syncing pod, skipping: failed to \"StartContainer\" for \"user\" with CrashLoopBackOff: \"Back-off 1m20s restarting failed container=user pod=pipeline-jc2736f0e9b964200bf70f7cb047813adhelloworldgo-v1-17n0c_spotlight(23910131-1931-11e7-b11f-42010a84000c)\"\n7m    7m      1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}   Warning Failed          Failed to start container with docker id 09adc5112006 with error: Error response from daemon: Container command '/pach-bin/guest.sh' not found or does not exist.\n  7m    7m      1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}   Normal  Created         Created container with docker id 09adc5112006; Security:[seccomp=unconfined]\n  7m    4m      12      {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}                               Warning FailedSync      Error syncing pod, skipping: failed to \"StartContainer\" for \"user\" with CrashLoopBackOff: \"Back-off 2m40s restarting failed container=user pod=pipeline-jc2736f0e9b964200bf70f7cb047813adhelloworldgo-v1-17n0c_spotlight(23910131-1931-11e7-b11f-42010a84000c)\"\n4m    4m      1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}   Warning Failed          Failed to start container with docker id 24792556c0ea with error: Error response from daemon: Container command '/pach-bin/guest.sh' not found or does not exist.\n  10m   4m      6       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}   Normal  Pulled          Container image \"spotlightdata/worker_helloworld_go:latest\" already present on machine\n  10m   4m      7       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}                               Warning FailedSync      Error syncing pod, skipping: failed to \"StartContainer\" for \"user\" with RunContainerError: \"runContainer: Error response from daemon: Container command '/pach-bin/guest.sh' not found or does not exist.\"\n4m    4m      1       {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}   Normal  Created         Created container with docker id 24792556c0ea; Security:[seccomp=unconfined]\n  10m   4s      44      {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}       spec.containers{user}   Warning BackOff         Back-off restarting failed docker container\n  4m    4s      21      {kubelet gke-spotlight-cluster-node-pool-scaling-8b3313b5-0cbw}                               Warning FailedSync      Error syncing pod, skipping: failed to \"StartContainer\" for \"user\" with CrashLoopBackOff: \"Back-off 5m0s restarting failed container=user pod=pipeline-jc2736f0e9b964200bf70f7cb047813adhelloworldgo-v1-17n0c_spotlight(23910131-1931-11e7-b11f-42010a84000c)\"\n```\npachctl list-repo shows the repos and the input repo has bytes in it. pachctl list-commit with the input repo shows that something went in successfully.\nREPO                                     ID                                 PARENT              STARTED             DURATION            SIZE\nj9dd08b64955042acba8baf8e6fdf7377input   1b47631e4b584222b8a8533316c7ab66   <none>              42 seconds ago      2 seconds           59.38 KiB\nbut pachctl list-file just errors out with storage: object doesn't exist.. +1. Yeah port forward was on (I restarted it many times) kubectl worked too, pachctl version worked too I think. The pods were in running status (there were about 20-25 pods iirc) I didn't check throughput or PFS so I'm not sure if that was the issue.\nI've come across this a few times so if I see it again, I'll grab whatever extra information is needed! It's not a huge issue right now since we're running pachctl delete-all all the time since we're not in production but if this did happen in production I'm not sure what we could do other than deleting pachd and all associated data.. Great to hear, thanks! I think this form of logging is quite common in enterprise environments so it makes sense to support it going forward.. ",
    "svanharmelen": "Thanks @JoeyZwicker! Just signed the CLA \ud83d\udc4d . ",
    "josibake": "@jdoliner currently running v1.3.4. ",
    "mr-c": "FYI: http://www.commonwl.org\nChat @ https://gitter.im/common-workflow-language/common-workflow-language. Cool, glad to hear it.\n1) FYI: CWL is a standard, not a runtime. True, one could use any CWL implementation inside a container treating the entire workflow as a single step, but then you'd lose granularity.\n2) CWL implementations distribute their workload according to implementation specific heuristics as they may be running on a single node, high throughput computing cluster (grid), or one or more cloud providers. The CWL model is rich enough to provide them with lots of information to make these choices. Some implementations will implement CWL's scatter/gather across nodes, yes.\n3) I'll leave this for @gijzelaerr and others to answer, but from what I can see there should be enough information in CWL descriptions to convert into Pachyderm workflows.\nLet me know if you'd like a real time video chat, I'd be happy to go into more depth and share about how various projects have implemented CWL support so far.. ",
    "samuell": "Just to chime in with my 5c: Based on my experimentation with CWL and my breif reading of Pachyderm workflow examples, I get the impression that they are actually very similar. \nI would even think just a converter from CWL yaml/json to Pachyderm yaml, would not be very hard at all.\nCWL is also doing the definition of inputs / outputs (in-ports / out-ports in some contexts) in a more re-usable and composable way, as far as I can see, so I think pachy could probably learn a thing or two there as well, for any future updates of its format.\nBut all in all, the formats seem very similar.. ",
    "OliverEvans96": "Any further thought here? Has anyone attempted a CWL to Pachyderm yaml converter? Does it seem like there's sufficient feature parity for this to work well? I think it would make Pachyderm quite appealing to interface smoothly with the CWL standard.. ",
    "dannykwells": "Big Pachdyerm user here - we are using it throughout all of our pipelines. Are there any plans to support CWL soon? This language is becoming the lingua franca of computational genomics and support for it is extremely important for our continued use of Pachyderm.. ",
    "gijzelaerr": "well that would be a shame :). ",
    "philipithomas": "Oh man, I wasn't expecting so many failures. I'll start patching Go1.8 compatibility on this branch (then we can squash and merge) . The \"first path segment in URL cannot contain colon\" appears related to this issue filed for Kubernetes:\nhttps://github.com/kubernetes/kubernetes/issues/38380. I need to update Kubernetes to 1.5.3 to get their Go 1.8 fixes. I'm having trouble with govendor and will try to get it working tomorrow. . Just pushed the vendor change. Should close #1346 when tests pass.. Squashing and reopening. Reopening in a clean branch. It runs all of the tests in the CI environment in parallel for 1.7 and 1.8. I guess that, for some of the functional tests, it's using the 1.7 compiled version in Docker containers, even though the unit tests are being executed using the Go 1.8 binary.\nBecause these changes are NOOP on Go 1.7, I think they should be merged ASAP, and that you should keep the CI changes so that any incoming changes are Go 1.8-compatible. Then, when you're ready to officially upgrade to Go 1.8, you can update the Dockerfile and decide whether to drop 1.7 compatibility. (Relevant question here is - do customers/users use your binaries/images, or do they compile themselves? If it's the latter - you may want to test and maintain 1.7 compatibility for awhile.). ",
    "bpb": "Just some context as to how I'm using port-forward and mount that might shift paradigm of usage of these commands.\nI have the wrapper bash scripts called pachyderm_connect, pachyderm_disconnect, and a combination called pachyderm_reconnect.  \npachyderm_connect polls kubectl get all to ensure that etcd is up and running and pachd is up and running. If they are port-forward and mount get created with nohup and & pipeing the stdout to separate log files. Otherwise we wait until etcd and pachd are up.\npachyderm_disconnect deletes the pfs mount and forces a dismount if unsuccessful and deletes the mount path. It then kills all kubectl and pachctl portforwarding processes and deletes the log files. \npachyderm_reconnect basically runs pachyderm_disconnect followed by pachyderm_connect.\nThese wrapper scripts have effectively eliminated connectivity issues and concerns. I just run the reconnect script every time I open a new terminal or get a network disconnect.\nI can post these scripts here if you guys find this helpful.. Experiencing similar issue. Not sure if this is due some misconfiguration in setting up pachyderm on aws. Investigating.\nError below:\n`serviceaccount \"pachyderm\" created\ndeployment \"etcd\" created\nservice \"etcd\" created\nservice \"pachd\" created\ndeployment \"pachd\" created\nsecret \"amazon-secret\" created\nError from server (AlreadyExists): error when creating \"STDIN\": persistentvolumes \"etcd-volume\" already exists\nError from server (AlreadyExists): error when creating \"STDIN\": persistentvolumeclaims \"etcd-storage\" already exists\nPachyderm is launching. Check it's status with \"kubectl get all\"\nkubectl create -f -: exit status 1\nError from server (AlreadyExists): error when creating \"STDIN\": persistentvolumes \"etcd-volume\" already exists\nError from server (AlreadyExists): error when creating \"STDIN\": persistentvolumeclaims \"etcd-storage\" already exists. What is the suggestion for the time being? Use a ubuntu container or build my own Pachctl for alpine?. Bootstrapping pachyderm into a K8s AWS cluster using a docker container.. As a work around for the time being\nsimply grabbing agolang:alpinecontainer and running$ CGO_ENABLED=0;go build -ldflags '-extldflags \"-static\"' ./src/server/cmd/pachctl`\ndoes the trick\n. Strangely enough if you set up the automatically generated cron repo as input that triggers jobs no problem. But crossing them does not help.. My initial inclination would be to make this a simple pipeline spec field called group tags where you pass an array of strings.  The group tag would let you mark repos and pipelines as belong to a set of possibly overlapping groups.\nThen when you run  pachctl list-repo <group_name> and pachctl list-pipeline <group_name> pachctl would return results filtered on .. I think that should effectively solve it. \nA colleague of mine believed the following fork would solve it\nhttps://github.com/pachyderm/pachyderm/compare/master...fortytw2:master\nBut this set up a separate role for workers. Passing the same service account should do the trick. \nWe never actually deployed his fork. . Should this not also be true for cross input? A new datum input in Repo A should be crossed with all inputs of Repo B even if no new commit has occurred in repo B?. I must have miss understood. Some of the previous documentation made me think that crosses won't trigger unless both inputs have new commits. But the wording is that both inputs have at least one commit which means that no input is empty (aka has no commits).\nSorry for side tracking this ticket.. ",
    "hchauvin": "Hi, my two cents, while I'm evaluating Pachyderm :) For reasons I cannot understand, doing a port forward using kubectl is more flaky for me than doing it directly using k8s.io/client-go/tools/portforward, so I have a tool that does that directly.\nThis is also the approach taken by Helm: it needs a port forward to access the Tiller server, and it does so not through kubectl but directly in Go.  This is a good starting point.\nMoreover, it is possible not only to have a port forward in the background but to do as they do: just forward the remote port to a random local port each time the pachctl client is invoked, the user won't see the difference in terms of performance.. Hi,\nThanks for the reply.\nYes, pachctl get-file gives the correct result.\nTo be clearer:\nubuntu@ip-172-32-2-138:~/XXX$ pachctl get-file test master test.txt\ntest\nubuntu@ip-172-32-2-138:~/XXX$ ls ~/mount_point/test/master/test.txt\n/home/ubuntu/mount_point/test/master/test.txt\nubuntu@ip-172-32-2-138:~/XXX$ cat ~/mount_point/test/master/test.txt\ntest\ncat: /home/ubuntu/mount_point/test/master/test.txt: Input/output error\nAnd the corresponding logs in pachd:\n2018-02-27T18:37:34Z INFO pfs.BlockAPI.Obj.InspectObject {\"duration\":0.039537403,\"request\":{\"hash\":\"0e3e75234abc68f4378a86b3f4b32a198ba301845b0cd6e50106e874345700cc6663a86c1ea125dc5e92be17c98f9a0f85ca9d5f595db2012f7cc3571945c123\"},\"response\":{\"object\":{\"hash\":\"0e3e75234abc68f4378a86b3f4b32a198ba301845b0cd6e50106e874345700cc6663a86c1ea125dc5e92be17c98f9a0f85ca9d5f595db2012f7cc3571945c123\"},\"block_ref\":{\"block\":{\"hash\":\"f93c9578e288450d866da287d0f04852\"},\"range\":{\"upper\":5}}}} \n2018-02-27T18:37:34Z INFO pfs.BlockAPI.Obj.GetObjects {\"duration\":0.052090885,\"request\":{\"objects\":[{\"hash\":\"0e3e75234abc68f4378a86b3f4b32a198ba301845b0cd6e50106e874345700cc6663a86c1ea125dc5e92be17c98f9a0f85ca9d5f595db2012f7cc3571945c123\"}],\"size_bytes\":131072,\"total_size\":5}} \n2018-02-27T18:37:34Z INFO pfs.API.GetFile {\"duration\":0.055492296,\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"test\"},\"id\":\"3d0e3a875aa04ba7a1ff2616d1e05924\"},\"path\":\"test.txt\"},\"size_bytes\":131072}} \n2018-02-27T18:37:34Z INFO pfs.API.GetFile {\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"test\"},\"id\":\"master\"},\"path\":\"test.txt\"},\"offset_bytes\":5,\"size_bytes\":131072}} \n2018-02-27T18:37:34Z INFO authclient.API.Authorize {\"request\":{\"repo\":\"test\",\"scope\":1}} \n2018-02-27T18:37:34Z INFO authclient.API.Authorize {\"request\":{\"repo\":\"test\",\"scope\":1}} \n2018-02-27T18:37:34Z INFO pfs.BlockAPI.Obj.GetObjects {\"request\":{\"objects\":[{\"hash\":\"0e3e75234abc68f4378a86b3f4b32a198ba301845b0cd6e50106e874345700cc6663a86c1ea125dc5e92be17c98f9a0f85ca9d5f595db2012f7cc3571945c123\"}],\"offset_bytes\":5,\"size_bytes\":131072,\"total_size\":5}} \n2018-02-27T18:37:34Z INFO pfs.BlockAPI.Obj.InspectObject {\"request\":{\"hash\":\"0e3e75234abc68f4378a86b3f4b32a198ba301845b0cd6e50106e874345700cc6663a86c1ea125dc5e92be17c98f9a0f85ca9d5f595db2012f7cc3571945c123\"}} \n2018-02-27T18:37:34Z INFO pfs.BlockAPI.Obj.InspectObject {\"duration\":0.007720729,\"request\":{\"hash\":\"0e3e75234abc68f4378a86b3f4b32a198ba301845b0cd6e50106e874345700cc6663a86c1ea125dc5e92be17c98f9a0f85ca9d5f595db2012f7cc3571945c123\"},\"response\":{\"object\":{\"hash\":\"0e3e75234abc68f4378a86b3f4b32a198ba301845b0cd6e50106e874345700cc6663a86c1ea125dc5e92be17c98f9a0f85ca9d5f595db2012f7cc3571945c123\"},\"block_ref\":{\"block\":{\"hash\":\"f93c9578e288450d866da287d0f04852\"},\"range\":{\"upper\":5}}}} \n2018-02-27T18:37:34Z ERROR pfs.BlockAPI.Obj.GetObjects {\"duration\":0.016050744,\"error\":\"InvalidRange: The requested range is not satisfiable\\n\\tstatus code: 416, request id: 818E692BFCFDAD80, host id: dZXq5cmt9gLbSWTtR+meV2SuIVi/dRWlg0RpgX7WD46qBB7GAWRaFQpuOFsELYXIQOO9CvUlnco=\",\"request\":{\"objects\":[{\"hash\":\"0e3e75234abc68f4378a86b3f4b32a198ba301845b0cd6e50106e874345700cc6663a86c1ea125dc5e92be17c98f9a0f85ca9d5f595db2012f7cc3571945c123\"}],\"offset_bytes\":5,\"size_bytes\":131072,\"total_size\":5}} \n2018-02-27T18:37:34Z ERROR pfs.API.GetFile {\"duration\":0.018733142,\"error\":\"rpc error: code = Unknown desc = InvalidRange: The requested range is not satisfiable\\n\\tstatus code: 416, request id: 818E692BFCFDAD80, host id: dZXq5cmt9gLbSWTtR+meV2SuIVi/dRWlg0RpgX7WD46qBB7GAWRaFQpuOFsELYXIQOO9CvUlnco=\",\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"test\"},\"id\":\"3d0e3a875aa04ba7a1ff2616d1e05924\"},\"path\":\"test.txt\"},\"offset_bytes\":5,\"size_bytes\":131072}} \ntime=\"2018-02-27T18:38:07Z\" level=info msg=\"objectCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-02-27T18:38:07Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-02-27T18:38:07Z\" level=info msg=\"objectInfoCache stats: {Gets:19 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:19 LoadsDeduped:19 LocalLoads:19 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-02-27T18:39:07Z\" level=info msg=\"objectCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-02-27T18:39:07Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-02-27T18:39:07Z\" level=info msg=\"objectInfoCache stats: {Gets:19 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:19 LoadsDeduped:19 LocalLoads:19 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-02-27T18:40:07Z\" level=info msg=\"objectCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-02-27T18:40:07Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-02-27T18:40:07Z\" level=info msg=\"objectInfoCache stats: {Gets:19 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:19 LoadsDeduped:19 LocalLoads:19 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-02-27T18:41:07Z\" level=info msg=\"objectCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-02-27T18:41:07Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-02-27T18:41:07Z\" level=info msg=\"objectInfoCache stats: {Gets:19 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:19 LoadsDeduped:19 LocalLoads:19 LocalLoadErrs:0 ServerRequests:0}\". Thank you for that @jdoliner!  Yes, number of tries was what I had in mind.. Thank you!. done. I added it for completeness to be honest, because I wanted to split the RC name in two parts.  I'll remove it as that sets a precedent.. Well that was exactly my thought.  In the end, I don't think you want to fully duplicate all the options Kubernetes has to offer.  For instance, allowing to set affinity would require to greatly expand the API.  I thought about that and I think that past a certain point, if configuration is really necessary, then you could use a gotemplate for the RC spec that gets all the internals of Pachyderm as variables and outputs a YAML file.  This way, all the cases, present or future, are covered, including security, network policies, ...\nHowever, I thought this was an overkill if the only thing to do is selecting a node.  In the end, with resource requests/limits and a node selector, I think that worker scheduling is covered more than enough.. ",
    "ysimonson": "We follow the same approach as Helm now. Maybe this is good to close?. I think @gabrielgrant was working on this in December, right?. #3325 adds some flexibility to how bash completions are installed. Beyond that, I think auto-installing bash completions doesn't make sense because:\n\nThe bash completion tool isn't guaranteed to be installed.\n\nThe directory path that contains bash completions is not always the same.\n. Yes, the interface is the same as that comment. There's two gotchas to auto-installing the completions. These apply to linux, not just macOS:\n\n\nThere's no guarantee that bash completions exist on any given system. It may or may not be installed on ubuntu and debian by default, I'm not sure. It is an apt package.\n\nEven if bash completions are installed, there's no standard directory for putting the completions into.\n\nI'd prefer to go with the principle of least surprise and go with what most packages do. I did not do a thorough analysis, but from the few I took a look at, it seemed they did not-auto-install the completions.\n. Are you sure that most of the completions you see are not installed by bash-completion itself? It comes with a bunch out-of-the-box.\nAgain, the directory is not standardized on linux either. Apparently you can get at it programmatically, but it requires a bit of gymastics; see the FAQ here: https://github.com/scop/bash-completion/blob/master/README.md\n. @jdoliner it looks like nowadays we do rely on the docker config by default. That said, the default config built for macOS does not work, because the docker library we use doesn't support keychain access - so they need to rely on --password. . This is fixed with #3344. @mdaniel it's driving me crazy too. I'll dig in sometime soon.. cobra has support for zsh completions now: https://godoc.org/github.com/spf13/cobra#Command.GenZshCompletion. Not yet. I self-assigned to dig deeper at a later point. Happy to unassign if you think it's best, though.. A couple of points in favor of (1):\n\nIt's of course generally good to make the initial experience as seamless as possible, and I would guess most users kicking the tires would be using minikube.\nIt would default to not using the proxy, which is presumably slower.\n\nThat said, my first point is presumptuous. Plus I could see this change being really confusing for existing users who are accustomed to the current behavior.\nAs another alternative, would it be feasible to instead first try localhost:30650, and failing that, discover the host of the actively running cluster?. I like that. The only downside I can think of is that the user will default to going through the proxy, which will be slower (but still worthwhile, IMO, for its ergonomic gain.). I'm not aware of any reason we can't/shouldn't change those defaults, but that's a very different ticket from this. Maybe it makes sense to close this out and open a new one?. Did you run anything specific to trigger the panic? pachctl list-repo and pachctl list-job just print the warning, then fail to connect to the cluster.. Ah I see, there's some nil dereferencing going on in the code. What do you think erroring out instead of regenerating the config? I'm asking because it's not clear to me what should happen when the user specifies the config path via PACH_CONFIG. Should it still overwrite the file? What if the file is, e.g. an executable? What if it points to a directory that doesn't exist, should it create the directories as well?. I'm not sure if it's the right path or not, but here's a POC of what it'd look like if we removed the -i flag: https://github.com/pachyderm/pachyderm/compare/no-i-flag-for-put-file. @JoeyZwicker did you observe all of the files in set1 being put in correctly? I'm only seeing one file being put.\nHere's my set1:\n$ ls set1/\n1VqcWw9.jpg  9iIlokw.jpg  FsPcjlw.jpg  LV1Ap55.jpg  VnOxwyl.jpg  bXHb7sT.jpg  lgLYAVq.jpg  sqgLozf.jpg\n2GI70mb.jpg  Bx36V6G.jpg  GWYejkj.jpg  MVcakFW.jpg  VyAlcbq.jpg  foDqKls.jpg  lrkcRaq.jpg  statue.jpg\n3Kr6Mr6.jpg  C0cqLzA.jpg  JzO6aky.jpg  QAmyGmE.jpg  Wv84dI2.jpg  g9v1Q6N.jpg  nkC6fxH.jpg  tG4pKeg.jpg\n4Vp1OMR.jpg  F8FbGHL.jpg  JzYTdvw.jpg  RMknOd5.jpg  XrFx4gJ.jpg  iEGTqhO.jpg  sJtIFf0.jpg  uNJQvA4.jpg\n7mNRGwN.jpg  FdMlBz1.jpg  LKzpo4V.jpg  U76TTRm.jpg  ZEyfHJE.jpg  k0gFmm6.jpg  sTAMXEp.jpg  yt1olXP.jpg\n...vs what's in PFS after running pachctl put-file images master -r -f set1:\n$ pc list-file images master set1\nCOMMIT                           NAME              TYPE COMMITTED    SIZE     \n5ad7cf4c98ea4650b914c3fcd28dc466 /set1/4Vp1OMR.jpg file 49 years ago 218.6KiB\nFinishing this commit, starting a new open commit, and re-running put-file adds a second file, but still not the entire directory.. A couple more things:\n1) The committed timestamp for files is always the UNIX epoch (see above pc list-files output as an example.)\n2) I'm not sure where the error is coming from. Grep is not yielding anything obvious, and it's happening after the put-file cobra command completes.\n. Timestamp bug moved to a separate issue: https://github.com/pachyderm/pachyderm/issues/3328. The error is being produced here: https://github.com/pachyderm/pachyderm/blob/5e11c8dff04cbd297f9e39f69df3d40c3bfc2193/src/server/pfs/server/driver.go#L3010\nNot sure why yet.. #3294 might improve things a bit, since instead of spawning a bunch of subprocesses, we're using the port forwarding API, much like helm. Intuitively, that sounds more reliable, but I'm not sure how the API handles transient connection loss.. Yes, this should be fixed now. @zebesta would you be able to try this again against the repo that was bugging out?. Presently seeing an error similar to #3184: pachyderm/1.7.0/pachyderm_pfs/repos/ edges not found. Disregard my prior message; looks like it was some transient failure related to my specific setup. I'm now able to reproduce the original error, and it looks unrelated to #3184.. Analyzing TravisCI logs since mid-december, here are the 5 tests that've failed or timed out the most:\nTestService: 49\nTestFlushCommit: 23\nTestChainedPipelinesNoDelay: 18\nTestPipelineWithStatsAcrossJobs: 18\nTestCronPipeline: 18\nCurrent job failure rate by date:\n\nNote that these are job failures. Because builds have 5 jobs each, and any one job failure causes the build to fail, the build failure rate is higher.. Addressed in #3251 . As an alternative, I'll see how much work it is to disable tests in #3324.. @msteffen ready for another pass.\nIs get_images used anywhere? i.e. should it continue to be exported? I don't see anything in the repo grepping around, and if it's not used, I can simplify things further.. I'll close this - it only really helps for pachyderm employees who use macOS, which is presently just me :). Ah, that didn't pick up when grepping. Will fix now. If I can't find a workaround, I'll revert soon.. Presently working through a bug where atom inputs produce no datums. Bug fixed. No. Hoping to get some guidance on why copypasta'd code would work versus the prior implementation.. @jdoliner same tests are failing even with the performance improvements merged in. Given that, I think these changes are producing some sort of (non-deterministic) deadlock state, but I'm not sure how.. The only place I see no changes are in job state updates for services. My understanding is that there should be no merging state for services.. @jdoliner anything I should wait for, or good to merge at this point?. This is being caused by interactions with the path reversing logic for S3.\n\nThe Amazon client is configured to reverse by default.\nWhen reverse is true, the Walk method calls its input callback with the key not reversed.\nWhen reverse is true, the Reader method does reverse its input key.\n\n~~Should the Amazon client be configured to not reverse by default? Or should the Walk method call its callback with the key reversed?~~ Will move discussion to a PR.. @jdoliner the changes to Amazon client make sense too? Just want to double-check because, if there's any functionality that currently relies on walked paths being reversed, they will break with this change.. It's not required, but from our conversation, I think it might be a separate bug. I could open a separate PR for that change and have @brycemcanally review when he's back.. Moved to PR #3274. I opened a PR to fix broken links elsewhere in the docs. I can go ahead and just fix this one too.. FWIW, this does add a note regarding deprecation notices in the pipeline config docs. I figured it was too early to add it elsewhere, since PFS inputs aren't in any releases yet.. Well...that was a journey. I've tried two methods, neither of which really work:\n1) First, cobra's doc generation offers a URL rewriter. While this works, and we can use it to fix links on html-generated docs, it will then break links in rendered markdown pages (e.g. if the user is browsing the docs from github's markdown viewer.) We could resort to absolute URLs to docs.pachyderm.com, but then we risk further broken links whenever there's a divergence between what's on docs.pachyderm.com's latest build and the master branch.\n2) Second, I tried rewriting the URLs in sphinx/recommonmark. It's entirely broken with the versions of sphinx/recommonmark that we're using. Upgrading fixes that issue, but it only seems to rewrite a subset of the URLs, not including the see also sections.\nThat leaves two options:\n1) Get rid of the see also sections, as @Nick-Harvey advocated for. Unfortunately cobra doesn't have a built-in way to do arbitrary reformatting like this, so it would be a non-trivial build.\n2) Using rtd's redirect feature. I don't have access to this so can't check if it'd work.. Part\u00a0(but not all) of the problem seems to be that microk8s does not store its k8s config in the standard location. This can be fixed like so:\nmkdir -p ~/.kube\nmicrok8s.kubectl config view --raw > ~/.kube/config\nSource: https://github.com/docker/compose-on-kubernetes/issues/59#issuecomment-456347608. I managed to get a cluster up and running. The opencv example still fails, but I think the error is due to something with microk8s itself: pods can't reach the internet, even though I made the suggested firewall updates.\nHere's what I did:\n1) Made the config changes as noted in the previous comment.\n2) Rebuilt everything from scratch.\n3) Exported PACHD_ADDRESS=127.0.0.1:30650.\n. @ktarplee here's the script I used to rebuild everything from scratch. It destroys the cluster and rebuilds everything from scratch. Very much a work-in-progress (hence why it's not on master yet), but maybe helpful?. No, though that looks like something we just fixed: https://github.com/pachyderm/pachyderm/pull/3453. The reset script is probably the best bet, but first I need to commit push-to-microk8s.sh (sorry about that!) I'll get that in by end of day.. Fixed!. @ktarplee do you still need to set --no-expose-docker-socket? If so, would you happen to know why?. That make sense. I'm making some improvements to the reset script at the moment that'll make it a bit more resilient. These improvements will be in by EOD as well.\nI'd love to get your continued feedback on any issues you encounter with microk8s; at the moment I'm a bit limited in how much I can kick the tires with microk8s because of the aforementioned firewall issue.. Changes are in. I'm still hitting firewall issues, so haven't been able to check that it's working beyond compiling and starting the cluster successfully.. @ktarplee with 1.8.4 out, you should be able to run pachctl list-job on microk8s, without having to build from master or use the reset script. There seem to be two issues going on here.\nWithout any changes, I'm hitting the same issue as #2446.\nWith the config change, if I do pachctl update-pipeline -f edges.json --push-images, it fails, because it tries to push the image to [system user]/[repo] rather than [docker user]/[repo]. It appears that this is pachctl's default behavior when a user is not explicitly specified. Is this intended? Or should we instead default to the docker user?. I'll go ahead and open a PR with the fix. We can just not merge the PR if defaulting to system user is expected behavior.. @gabrielgrant confirmed that your fix, in tandem with #3322, allows you to push images in macOS.. Sure, that makes sense. Do we have an idea of how frequently --push-images is employed? That would help decide whether it makes sense to just print a friendly error message, or build a full workaround.. Yes, though #3374 shouldn't be a blocker for anything, correct?. The port forwarding changes are in master, but not in the openshift branch yet. I'm finding myself confused by --push-images as well. It seems to generate a new tag for the image, but not build a new version of the image. So I'm not sure what tag it's going off of. Is it the one in the pipeline spec? latest? Something else?\nOnce the pipeline is updated, presumably it uses the new tag. But then that means the pipeline has diverged from what's in the spec.\nMore broadly, if it's not rebuilding the image, is there much utility to this? I can get why a flag that would rebuild + retag + push + pull a docker image would be really helpful; that way, you could update a script and just run one command to see the changes reflected. But with this, you still need to run some docker commands before seeing script changes reflected.. Added support for rebuilding docker images (as outlined in the last comment) in #3370 . Maybe fixed by https://github.com/pachyderm/pachyderm/pull/3355 ?. Yes. The branch I have includes documentation changes as well. If we want to merge changes to examples first before changes to documentation, we could merge this PR first, followed by the one I have.. @JoeyZwicker I'm inclined to agree, but merging this PR first might make the transition a bit more seamless, because there won't be users (at least in the immediate future) that are reading through the 1.8.1 docs with pachyderm 1.8.0 or below, and confused why their PFS inputs aren't working.. @gabrielgrant soon to be fixed! #3324. @ktarplee thanks again for working on this! We're just going to go ahead and merge #3320 though, since it also includes the docs changes.. @jdelfino which of the strategies in the PR desc would you like to go with?. I knew that would happen sooner or later, sorry about that! @jdoliner . Is it not adjustable as an environment variable? Or is it that it should be adjustable via alternate means as well, such as in the pipeline spec?. These makefile targets are no longer run: test-vault test-auth test-enterprise test-worker. That is what it's doing. Tests are only disabled if the tests are being run from a forked repo. On this repo, all tests are running as per usual.. Tests are passing, though I don't really get how.\nThe way this is currently implemented is that it runs the full test suite if secret env vars are defined. My assumption is that secrets would not exist at all on forked repos, so that the partial tests would always run on forked repos, and the full tests would always run on the main repo.\nBut it looks it instead always runs the partial test suite for PR builds, and always runs the full test suite for push builds, regardless of whether it's a forked repo or not.\n@msteffen or @jdoliner: Is this ok? While it's not what I was expecting TravisCI to do, it still fixes tests so that they pass for forked repos, and still ensures the full test suite is run. \n. Okay, it looks like TravisCI is working as designed, I just didn't understand push vs PR builds. PR builds trigger for PRs open from remote forks, whereas push builds trigger only when you push to the main repo. Because I have this branch both on the main repo, and a forked repo, it was triggering both tests. Appropriately, the push builds had access to the secrets because the code was pushed to the main repo, but the PR builds did not.. The error is happening on master as well: https://travis-ci.org/pachyderm/pachyderm/jobs/477106402#L777. Fixed in #3348. I put it in a separate PR since the issue is unrelated.. Done\n\nTo print, you run pachctl completion\nTo install to /etc/bash_completion.d/pachctl, you run pachctl completion --install\nTo install to a custom path, you run pachctl completion --install --path foo/bar/pachctl\n. It happens for non-zero history values as well, though still could be related to the history changes . No longer seeing this issue. Blocked until #3294 is merged. @jdoliner this will not implicitly run port forwarding for PFS over HTTP; it will only run when the user explicitly calls pachctl port-forward. Is this what we want?\n\nImplicit port forwarding has not been added because, at the moment, the only use case I'm aware of is for functionality in the dash, which is also not implicitly port forwarded.\n. As an aside, is this note still relevant? The affected code here should have a negligible performance penalty, even when lots of small files are put, because commit ID dereferencing is only done once. I'm not sure if there's other parts that also carry a penalty though.\n. @David-Development the fix is in a PR now. Please note that this change doesn't prevent things from erroring out; rather, it ensures that the error happens in the pipeline that produced the non-utf-8-formatted filepath, so that it'd be easier to debug this in the future.. Passing in the username/password as arguments seems to fix this. I wonder if this issue was introduced by using this hack. Closing because this issue doesn't come up when setting the username/password instead of doing the hack fix. A PR to change the advice in the error message is open (#3354).. @JoeyZwicker it passed, AFAIK there's no testing of our docs. Why is there a separate ticket for PEER_PORT (#3362)? Is there something special about PEER_PORT that requires additional changes vs the other ports outlined in this ticket?. Port should be 650 rather than 1650, right? I'm not seeing any use of 1650 grepping around. . Do PORT and HTTP_PORT need to be propagated? I don't see them used in src/server/cmd/worker/main.go. Yes, it includes propagation . @gabrielgrant could you kick the tires with this branch: https://github.com/pachyderm/pachyderm/tree/fix-sidecar-port-propagation\nMy suspicion is that it's the same issue as we were seeing with #3367, i.e. that the sidecar API server requires those ports (though, if that's the case, it's strange that tests passed.) But this change shouldn't be merged unless it is the proper fix.\n. This doesn't include any changes to src/server/pkg/deploy/assets/assets.go, which still has port 653 hard-coded. Is that correct? I don't know what assets.go is for.. Note that this does not include changes to the port forwarder. I will tackle that along with port propagation in a separate PR.. Let's get port customization working first before changing defaults. Some tests are timing out, and the errors are reproducible locally, but I haven't been able to track down why yet. I'm running go test ./src/server -run TestFlushCommit locally, and the test times out after 10m. This is opposed to running the same test on master, which finishes after a little more than 2m.\nNotes:\n\nThe timeouts are happening here: https://github.com/pachyderm/pachyderm/blob/462145baade0514ffbfa47637711945e1c3917ea/src/server/pachyderm_test.go#L1091\npc list-job indicates all jobs finished successfully. However, it keeps spawning repo jobs until the tests time out. On master, only one pipeline job is spawned.\npc get-logs is not indicating any errors.\npc get-logs --job [job] --raw is not indicating any errors.\nkc logs [pod] user (i.e. kubernetes' logs for the various pipeline pods' user containers) is not showing any obvious errors.\nIn everywhere I've dropped debug messages, it is successfully defaulting to port 80. @jdoliner assigning you as reviewer since you're already helping a bunch with figuring out why it's broken. The problem was that, there's two constructors for the debug API server, and one of them wasn't modified to take in the port. As a result, API servers constructed with the second constructor had a port value of 0. The second constructor was used by sidecars, which would eventually time out or crash when they couldn't connect to a debug server on port 0.\n\nI'm not sure why this wasn't reflected in the logs. Maybe pachctl doesn't show sidecar logs, and I didn't think to check the k8s storage container logs?\n. Just need @jdoliner's review. Yes, you need to set the remote-port flag. #3402 includes this and other fixes to custom releases. Closing this one out.. pachd listens on ports < 1024, right? that alone would be reason to need root. No. I'm saying the manifest was originally configured to always run as root because the ports were originally configured to always run on ports below 1024. . Possibly fixed by https://github.com/pachyderm/pachyderm/tree/fix-sidecar-port-propagation. Holding on this until we verify whether it's blocking or not. If it does end up being blocking with the other fixes in place, let me know and I'll circle back.. Did emptydir end up fixing this?. @gabrielgrant ping. @JoeyZwicker adding a -n as shorthand for --namespace would break backwards compatibility for list-commit, which uses -n as shorthand for --number. It was introduced 2 years ago. Do we want that?\nIMO it's not worth it.. You ran that on the openshift branch, right?. OK, this likely isn't the right fix then. Closing and will re-investigate monday.. Will close this for now, and re-open when it's closer to feature-complete. That makes sense, I'll change it. Thanks, that's really helpful!. It looks like you should be able to set the user via the pipeline spec (see transform.user.) Have you tried that already?\n@jdoliner am I understanding the pipeline spec correctly? And if so, do we want to make any changes to further facilitate non-root process execution?. Oh right, they randomize the user id in openshift right?. I don't see a reason to hold at this point. @gabrielgrant it is possible for a non-root user to exec as another non-root user. This make sense: if a non-root user is able to exec as root (so long as they have the proper credentials), then it'd be strange if they couldn't execute as a non-root user (again, so long as they have the proper credentials.) You just don't see it much in practice.. Makes sense. I'll make that change, and just tackle noMetrics as well.. The test failure is strange. It's trying to execute a version of setup-vault.sh that I made in a separate PR. There's no changes to port forwarding or that script in this PR, as you can see in the diff. Did TravisCI leak cache or something?. @jdoliner I ended up going with embedding the branch name as part of the filepath/s3 key. This means that the user needs to be explicit about what branch they want to pull content from, but it's necessary because s3 bucket names are indeed validated, and the tight restrictions on valid bucket names is a subset of the restrictions we place on repo names (so there's no character I could use as a legal separator between repo and branch name in the bucket.) See the docstring for Server for a little more detail.. That's a larger and separate change that I won't be able to tackle any time soon; can you open a separate ticket?. Yeah that would work. The other option is to use explicit port forwarding with --namespace.\nTurns out this is not a bug. I thought a bunch of pachctl commands take --namespace, but the only ones that do are explicit port forwarding and deployment-related. All of the commands that use implicit port forwarding do not take a --namespace.. Implicit port forwarding is not meant to work for all situations, but rather streamline the default use case (i.e. using the default namespace, and no port customization.) Because of that, any of these methods will disable implicit port forwarding:\n1) Setting ~~ADDRESS~~ PACHD_ADDRESS\n2) Running explicit port forwarding (which additionally allows you to set a namespace): pachctl port-forward\n3) If you're on the bleeding edge, running a command with --no-port-forwarding\nDo you need implicit port forwarding? If so, have you tried setting the k8s context?. @LaurentGoderre were you getting a warning that looked something like this? Implicit port forwarding was not enabled because the pidfile could not be written to.\nI can update it to just log a debug message; warning seems too high of a level for something like that.. Addressed in #3494. Makes sense. I've also reverted docs changes for now.. @gabrielgrant yes. RTD does provide versioned docs, but from what I've seen most people read the latest docs regardless of what version of pach they're on.. @gabrielgrant do you think this would fit in somewhere in the existing docs, or do you see it as an entirely new page?. Going to merge in with #3466 . There's no --namespace argument, except for explicit port forwarding (pachctl port-forward). Yeah. You have to use explicit port forwarding for non-default namespaces. . I think so, but haven't checked. Beyond this PR and the last one, there's still 2 TODOs:\n1) docs\n2) dropping in the code to actually start the server (outside of tests.)\nShould I just update this PR with those items, or wait until this is done and open a third PR?. Another note; this is being tested against minio, but not any other S3 clients. It's not clear to me whether this includes minio-specific extensions that could somehow cause issues for more vanilla S3 clients. This'll be easier to check for once the server is exposed somewhere.. Should we only expose this through TLS? I'm inclined to say yes, but it does imply configuration overhead.. @JoeyZwicker I'm not familiar with how TravisCI allocates VMs, and I don't have sufficient permissions to dig around our settings and maybe see. My completely uninformed guess is that they have a fixed pool for OSS, and that adding examples does run the risk of slowing test runs down when they are overburdened.. I'm a fan of just piping from curl for any type of authentication. It feels more unix-ey, provides better separation of concerns, doesn't carry a massive performance tax, and doesn't have a security/privacy risk since the contents aren't persisted to disk.. Now that 1.8.3 is out, should we merge docs changes now? Or wait a little while for the latest release to distill to more people?. This needs a better message, but the reason it's showing up is because the dash is not accessible through implicit port forwarding, which only (transiently) port forwards for pachd and SAML ACS. Explicit port forwarding will enable dash access.. You haven't set PACHD_ADDRESS or ADDRESS env vars either?. Hmm, I'm actually not sure if we can improve this message much:\n\nI think it should show up regardless of whether --no-port-forwarding is set, because, even if implicit port forwarding is enabled, you still need to use pachctl port-forward in order to access the dash.\nWe could suggest that they can also connect directly, but we won't know the IP address.\n\n@gabrielgrant would love your thoughts here. Oh (2) is a good point. I'll dig around. It'd be awesome if we could tell the user the exact env var value to set to directly connect!. Okay, we could expose the cluster IP to directly connect to, but as you mentioned, for most cloud-deployed instances it won't be possible to directly connect. How do we want to proceed?. I'm going to close this for now, but @pappasilenus let me know if you disagree. @jdoliner the entire PR is the fix. I went back to the double negation, but the real bug was that I was dereferencing tht flags before they were parsed. Now I'm just passing the raw pointer to the flag value around, instead of just the value. \n@msteffen it's my bad! I should've tested more extensively. . @gabrielgrant correct. Because port forwarding reads the default k8s config, and microk8s does not put its config in the default location, microk8s users who do not want to set PACHD_ADDRESS will need to do the following: https://github.com/pachyderm/pachyderm/issues/3290#issuecomment-461632187. As of 1.8.4, users who want to disable implicit port forwarding can run any command with --no-port-forwarding. Other tunneling methods should be handled by setting PACHD_ADDRESS, right?. This build passed: https://travis-ci.org/pachyderm/pachyderm/builds/490701483?utm_source=github_status&utm_medium=notification\nEven though it shouldn't because there were test functions that called testDeploy which was removed: https://github.com/pachyderm/pachyderm/blob/a39c8539f5b110238016a4f0dfee257183fdf42c/src/server/cmd/pachctl/cmd/cmd_test.go#L16-L34\nIs cmd_test.go just not running at all?\n@jdoliner I know you hit issues on master when testDeploy was removed in the earlier cleanup PR. Did you come across that on a normal CI run, or something else?. In that case, it seems like the issue was transient, but I'm not sure why it happened.. @jdoliner this is ready for another pass. TravisCI says it's still running tests for the PR, but they passed hours ago - you can see if you click through.. @jdoliner quite a few tests are failing because they expect the root commit to not be returned from a FlushCommit call. I've been updating the tests, but it got me thinking: with this change, do we want to return the root commit? Or just block on it?. make test-auth alone runs in 20 minutes, so isolating it into one bucket and running all the other other misc tests in a separate bucket only saves at most 5 minutes, and carries the overhead of requiring another VM. As a result, in most cases this change will slow things down. I'll close this out.. Thanks! My .rgignore was disregarding vendored packages :). Thanks for all the details @brokenjacobs, I'll dig in today. @brokenjacobs are you still having issues building pachctl with make install? If not, could you see if this branch fixes it for you? https://github.com/pachyderm/pachyderm/pull/3504. It's probably worth waiting until CI is green first actually; pulling in these dependencies is requiring a bunch of upgrades to some other dependencies we have, which in turn may require some further changes.. Header-related changes addressed in #3501 . Blocked by #3497. Merged with #3486 . No, unfortunately I don't have a cluster handy to test this against. @brokenjacobs would you be up for kicking the tires? I can cut a custom release with this change if you're still seeing errors when building.. Here it is: https://github.com/pachyderm/pachyderm/releases/download/v1.8.5-3b15373481111ab338f03c171679c591965b3f3b/pachctl_1.8.5-3b15373481111ab338f03c171679c591965b3f3b_darwin_amd64.zip. @brokenjacobs Yeah, that's correct. That concern has been brought up before. I'm hoping that with all the tweaks of implicit port forwarding that will come out in the next release, it won't feel like it ever gets in the way - even if it doesn't solve all use cases, (including non-default namespaces) since we provide alternatives. Let's see how that shakes out first; I'm hoping to not have to add another global flag to all commands.\n. @res0nat0r if you're willing to build from source, this branch should fix it: https://github.com/pachyderm/pachyderm/pull/3504. @jdoliner let me know if any of the functionality I've targeted shouldn't be added, or, oppositely, if there's functionality minio supports that we should add: https://docs.minio.io/docs/golang-client-api-reference.html. I was hoping to nix multipart uploads (FPutObject) because it doesn't map well onto PFS semantics. Unfortunately, it appears that minio implicitly does a multipart upload when the contents are over 64mb in size. I think this will need to be implemented (even if done hackily), because otherwise we won't support ingress of files over 64mb from minio (and likely other clients as well.)\ncc @brycemcanally @jdoliner . Results of the first conformance test run: 94 passed, 785 errored, 132 failed. Ouch.. Errors are now standardized to mostly match S3's. 111 tests now passing. I haven't looked yet, but am hoping there's some low hanging fruit to get the pass count up.. Heads-up: I'm exploring going back to combining the repo and branch into the bucket name in a separate branch, since the current scheme is understandably causing a huge percentage of the conformance test failures. This comes with a separate set of downsides, but it might be worthwhile. TBD.. @jdoliner Here's the next iteration of improvements, under active development: https://github.com/pachyderm/pachyderm/compare/s3-gateway-exposed...s3-gateway-conformance-2?expand=1\nThe largest change this has is going back to branch names as part of the bucket name. ~~Bucket names are now repo-branch, with the shortcut repo referring to repo's master branch. I might tweak the separator to be something else, but overall it's a big improvement in the list bucket logic and conformance test passes.~~ Bucket names are now branch.repo, which overall yields a big improvement in list bucket logic and conformance test passes.\nIt's quite a big change, but it simplifies what's already on this branch -- so I think it makes sense to merge it into this branch. Are you cool with that?. @jdoliner I went ahead and merged those changes in, since I think it should make the overall review easier.\nAbout ~20% of conformance tests are passing. Most (but not all) of the failures are as expected, because it's testing functionality that we don't intend to support. I'll make continued improvements towards conformance test passes in a separate PR that I'll open after this gets merged.\nOnce pachd integration is completed, this will be ready to review.. I'm glad you asked! There's several options we could go with, but they all come with trade-offs. Here's the reasoning that led me to the current scheme, but I'm curious what you think after this wall of text:\nFirst, the general reason for using . as a separator is because it's the only character that's valid as part of a bucket name, and not valid as part of a repo name. I had somehow missed this the first go-round because I was looking at some bucket validation documentation that was even stricter than necessary. It's possible the use of . in a bucket name may cause issues for some S3 libs, but haven't come across that problem yet.\nSecond, the branch in front of the repo is for two reasons:\n1) The S3 spec is geared towards hostname-based bucket resolution, i.e. <bucket>.s3.amazonaws.com, rather than path-based. So, when an API is using path-based resolution like ours, the first part of the path is a subdomain in some sense, where branch.repo would make more sense than repo.branch.\n2) The conformance tests allow you to specify a prefix for bucket names, but not a suffix (see s3gateway.conf.) All sorts of tests are run, including fuzzing with randomly generated and very long bucket names. This setup allows me to keep the branch name fixed, but allow conformance tests to randomly generate repo names. I suppose I could flip this around, though, and just have the conformance tests generate a bunch of random branches.\nI see a few options, assuming you agree . is the right separator:\n1) We could keep branch.repo.\n2) We could switch to repo.branch, in which case conformance tests will randomly generate branches. It wouldn't be too much work for me to make this change and do a conformance test run pass if that still sounds good - unless you know of a reason why randomly generating branches might cause problems (e.g., are branch names more restrictive than repo names? Are there tighter length restrictions?)\n3) At one point, I experimented also with making the branch part optional, and defaulting to master; so /repo/ would resolve to the master branch of repo. I could go back to this, but it adds some complexity that I wasn't sure was worthwhile.. It turns out, when running minio, it'll gracefully degrade FPutObject/FGetObject to use non-multipart operations IFF multipart-related endpoints return a NotImplemented code. I didn't observe this before because I was instead returning a standard 404, which minio didn't gracefully degrade on.\nThis means I can rip out multipart code, and it'll still work (at least for minio.) I'm inclined to do this since that's the meatiest code here, but the trade-off is that other s3 clients may not be smart enough to gracefully degrade.\nI did also experiment with storing multipart contents on PFS on the s3-gateway-multipart-on-pfs branch, which I'll keep in case it's useful in the future. @jdoliner I know you suggested using the object store itself, but the ability to atomically commit multiple file changes was going to come in handy, so I tried that avenue first.. 41% of tests passing now. Most of gains come from just skipping tests that use unsupported features, but there's some real improvements too; most notably, support for HTTP ETags. This is done by storing a metadata JSON file for each object. This litters the repo with a lot of metadata files, but it's the simplest solution. I can explore something more thorough at a later point.. All non-ignored tests are now passing, though there's still one error that pops up in conformance tests, as boto3 tests rely on bucket versioning when tearing everything down.\npachd integration is also done. However, when running integration tests against a pachd instance with port forwarding, the port forward seems to inevitably break, and tests can't finish. I'm not sure if this is a problem with the port forwarder or the s3gateway server yet.. With #3573, --registry is a bit unintuitive. It's now used purely to look up which account to use for pushing the image. It does not specify where the images are pushed to, which is specified in the pipeline spec image.. I'm having trouble reproducing this issue. @JoeyZwicker I know you managed to, would you be able to add some details?. I have managed to repro this issue in #3508.\nIn that case at least, port forwarding breaks down after a few minutes of intense activity (conformance test runs.) This happens regardless of whether it's pachctl's port forwarder, or kubectl's, so it's not the implicit port forwarder (in fact that probably helps, since most implicit port forwarding activity is short-lived), however our increased employment of port forwarding probably makes this issue more obvious.\nIt's consistently reproducible in k8s on docker for mac. I think I've seen it on minikube/ubuntu, but need to do more test runs to see whether those were one-offs.. The code looks good to me, but the tests seem to be failing legitimately.\nAs an aside, I'd imagine there's a performance gain in reducing the amount of work done in an STM, as you've done here. Is that correct?. I don't have a proposed solution, just noting that this is weird . @David-Development has a POC of pachyderm running on k3s/dc here: https://github.com/David-Development/cluster-setup/tree/master/k3s-pachyderm. @jdoliner fwiw, the pachctl docs aren't linked off in the docs due to an issue: https://github.com/pachyderm/pachyderm/issues/3382. Maybe out of scope, but I'd love to see fuzztests for large pachyderm clusters, a la chaos monkey or jepsen. Related ticket: https://github.com/pachyderm/pachyderm/issues/310. It does not. One alternative I could pursue is to just hardcode in all of the excludes specified on .gitignore. I didn't go with this originally because:\n\nIt seems a bit brittle - subsequent changes to .gitignore would also have to be reflected here\nIn practice, this grep call runs pretty fast, especially once .git is ignored\n\nBut I could go that route if you think it's better. This is outside of the scope of this PR, but I found the reliance on previously exported variables somewhat confusing. Nitpick: capitalization of return is sometimes lowercase, sometimes uppercase in the above comment . Is it kosher go to use leading underscores to prevent variable shadowing?. It shows if --verbose is enabled. Whoa that is weird, not sure what happened. What about just choosing the first item instead? Random seems like it could cause confusing behavior.. @jdoliner Is this OK? Without this, case-insensitive filesystems (like APFS by default) will ignore any file/directory with version in it (including some of our deps.). Wait, nevermind. Looks like INFOs are printed without verbose. I'll change the level.. I changed this to debug. However, I'm not sure if it's even possible to see these messages; e.g. pachctl list-repo --verbose does not show it.. It uses logrus, but it's configured kinda weirdly because of etcd: https://github.com/pachyderm/pachyderm/blob/1ce30d389b28c6ea6a7e2989ae1cc5e572f362ac/src/server/cmd/pachctl/cmd/cmd.go#L167-L185. Yes, see here: https://github.com/kubernetes/client-go/blob/ee7a1ba5cdf1292b67a1fdf1fa28f90d2a7b0084/tools/clientcmd/loader.go#L137. AFAICS, the added port forwarder doesn't actually make any RPC calls, but it does open multiple sockets. These will be killed when either Close is explicitly called, or the process ends.\nSidenote: the client actually already had a Close method for killing the grpc connection, but Close was not explicitly called in most places since the process ends anyways. I conservatively added calls to Close wherever port forwarding may be used, rather than all of the places that it was missing from.. That makes sense. I've changed it to always use the pachyderm suite.. In the old implementation, we were peeking the first put file request and dereferencing its commit ID, but not doing so for subsequent requests. As a result, on an open commit to a branch, the first file in a put file request would go through (because the commit ID was dereferenced), but not subsequent requests.\nWith this change, the first put file request's commit ID is still dereferenced, but we now update subsequent requests to use the dereferenced commit ID.. that would explain why my test is failing, heh. Yeah, I was avoiding that because peerPort is a uint16, which means I'd need to coerce the value, then call FormatUint or Itoa. I can do that though.. This might be lack of familiarity with k8s, but if you specified a dockerfile, you'd still need to push an image to docker hub, no? Otherwise where would k8s fetch the image? I figured directly pushing the image from docker to k8s once it's built would be finicky.. This isn't possible since createPipeline is the name of the cobra command already. The current name does feel weighty though, I'll try a different name and lmk what you think.. I've removed the class in favor of just functions. It does look cleaner, albiet a little wordier since all the necessary parameters are passed around. Let me know what you think.. Is having multiple pipeline specs in a single directory atypical? It'd break those, including this opencv example (i.e. if you ran build on montage.). This is outdated, I checked and it is necessary. ~~Is there a better way to also check if the stats commit is currently open? The current use of FlushCommit doesn't seem ideal to me, but from what I could tell, ListCommit did not include the stats commit~~. Added the check. https://github.com/pachyderm/pachyderm/issues/3395. OK, made it optional. I'm not sure. Either way we hit an impedance mismatch - either between PFS and DNS (which most or all object stories rely on) or between PFS and ~case-insensitive~ case-sensitive filesystems. My gut is to remain case-sensitive, but don't have a good argument for it.. I agree. I have a separate branch that adds support for writing, and in that I also change hardcoded numbers to http constants. Is it OK to defer the change to when I open a PR for that branch?. Depends on the filesystem, but Linux filesystems are almost always case sensitive. This should be return b.Get().([]byte). This should be b.Put(buf). What does [:0] do? I've never seen that in go before. It's not creating an empty slice, right?. This block ID format looks strange. Could you add an explanation of what it's doing, and why azure needs it?. Why store the err on the struct here?. interfact -> interface. Sorry, I'm not following. I can't use a closure here because I need to implement an interface for logrus. As far as I can tell, closures can't implement interfaces: https://play.golang.org/p/6FUkQK52gx9. I'm actually not sure why this is publicly exported, since it's not used outside of this package. I'll swap it over. Would you like me to still add a comment?. \n. Changed it to a closure. They're both byte arrays, and tests would be failing if it didn't work since minio sets Content-MD5. bytes.Equal sounds like it would be better than reflect.DeepEqual though. http's builtin router only does literal prefix matching, which would help a bit but not much. I brought in mux, which has made things a lot cleaner.. I'll be adding that next! Updated the PR desc to reflect that.. nit: fix mixed tabbing. nit: fix mixed tabbing. it looks like we're no longer checking if records is nil before updating the repoInfo. Is this correct?. Likewise with the above comment; this used to only hit if records was nil. I think that gives me all the control I need, but I was concerned about boilerplate, since there's a lot of hardcoded values I put in to match S3's XML output. I'll experiment with encoding/xml, though, once multipart is wrapped up.. By streaming the data directly into object storage, do you mean PFS? If so, I'm not sure how we'd do that given the S3 API:\n\nYou upload parts of the file in individual requests, but they can come in out of order or in parallel - e.g. I could upload part 1, then part 3, then part 2.\nThe parts are combined/merged when you complete a multipart upload, but you can skip certain parts - e.g. I could combine just part 1 and 3.\n\nIf you don't mean PFS, that makes sense. Though that would introduce a great deal of overhead when PFS is deployed locally, because you're introducing a round trip to the cloud that didn't exist before.. Looks like the glob package we pull in actually has a way of escaping glob characters, so I'll use that to avoid another divergence from S3. What about just having a separate k8s persistent volume? That way we wouldn't need to implement anything new (well, above and beyond what's already here.). You can't mount the same persistent volume on more than one pod?. ^ TIL that's generally the case. \nWhat about a separate PFS repo for in-progress multipart content?\n. The only argument is that this is the more conservative choice, but given the symmetry, I think deleting repo makes sense.. Go's http server recovers from panics; I think this will just kill the goroutine servicing this specific client. notFoundError checks if the error message implies a 404, and if not, it falls back to a 500. Should I give it a better name?. I wasn't aware of %s**, and it doesn't seem to be supported by go's stdlib glob matcher. Is that an extension PFS supports? Does it recurse into subdirectories? If so, I could probably simplify this by nixing listRecursive.. That makes sense, but I'll have to substantially cut down the number of conformance tests run, since they specifically check against md5's. That's not great, but this is still better than no timestamp at all. We don't have a way of seeing when a specific file was changed, do we?. Confirmed that is the case. Do you still want me to change this?. should I update errutil.IsNotFoundError to use these new functions as well?. Because I was able to nix listRecursive, updateFileInfo is no longer needed. Done. There is no GlobFileF, but there is a low-level GlobFileStream. Should I use that instead?. Done. Removing meta files means only 200 conformance tests can be run (vs 275). I think removal still makes sense though because:\n\nAs you said, the s3 spec doesn't specify what algo is used, so arguably this is a bug in the conformance tests themselves\nThe overall architecture should not be driven by what gets conformance tests to pass anyways\n\nThat said, the number of conformance tests is pretty tiny at this point. ",
    "emk": "This would be a very nice ergonomic improvement.... Honestly, I always find \"stale\" bots to be a bit annoying. There are too many times when I've carefully reproduced a bug, reported it upstream, and heard back nothing, only to have it automatically marked \"stale.\" Not sure what the best solution is.. Oh, yeah, there's a workaround: Disable lazy and this code path should never be hit. But this comes at a performance cost, and data loss during processing is pretty bad in any case.\nOh, and a nice bit of confirmation that the numbers add up:\n\nWe saw the error from GetFile message 5 times.\n3 times, this resulted in an incomplete snappy frame read in szip. This will still produce output data, of course.\nWe saw exactly 2 files which appeared to contain 0 lines. szip would likely have assumed that 0 bytes of input means \"no compression frames\" means \"no output\".\n\nSo I think we can track all the odd behavior we're seeing here back to error from GetFile. But there are presumably two actual issues here:\n\nWhy doesn't Pachyderm notice error from GetFile and mark the job as failed?\nWhy is the unexpected EOF occuring in the first place on 5 out of 16 workers?\n. I think that this fix is the most important part: The underlying error seems to be fairly rare when the cloud isn't acting up, and as long as it gets detected and handled properly, we have no real worries. Thank you!. I'd like to add some details on our use case.\n\nImagine that we have four multi-stage pipelines, each of which does some fairly complex processing.\n\nsource_a \u2192 ... processed_a\nsource_b \u2192 ... processed_b\nsource_c \u2192 ... processed_c\nsource_d \u2192 ... processed_d\n\nIn each of the 4 processed_* top-level repos (not directories in a mega-repo), the data layout looks like:\nprocessed_a/\n  000/\n    5ff9e947-2065-4821-b1a7-5ef635545aa5.csv.gz\n    f1e3ca2b-7267-4e42-95a1-9b7d62fc40e8.csv.gz\n    7ea94526-aee2-44c4-82f6-524d45af5a15.csv.gz\n    ...\n  001/\n  002/\n  ...\n  999/\nprocessed_b/\n  000/\n    11f7e13e-5dc4-4671-9c57-9608527079dc.csv.gz\n    ...\n  001/\n  002/\n  ...\n  999/\nprocessed_c/\n  000/\n    2c554605-b564-4b37-a56e-301c35b0ff6e.csv.gz\n    ...\n  001/\n  002/\n  ...\n  999/\nprocessed_d/\n  000/\n    c7b85b69-e107-44f2-9522-9a372176eb8d.csv.gz\n    ...\n  001/\n  002/\n  ...\n  999/\nOur desired output looks like:\nout/\n  001.gz\n  002.gz\n  003.gz\n  ...\n  999.gz\nTo compute 001.gz, we need to combine all of:\n5ff9e947-2065-4821-b1a7-5ef635545aa5.csv.gz\n    f1e3ca2b-7267-4e42-95a1-9b7d62fc40e8.csv.gz\n    7ea94526-aee2-44c4-82f6-524d45af5a15.csv.gz\n    11f7e13e-5dc4-4671-9c57-9608527079dc.csv.gz\n    2c554605-b564-4b37-a56e-301c35b0ff6e.csv.gz\n    c7b85b69-e107-44f2-9522-9a372176eb8d.csv.gz\n...onto a single worker. Then the worker will do a very complex calculation to reconcile all those inputs into a single output file.\nSo essentially 000, 001, etc., are \"cross-repo join keys\" operating at the file level. This could be implemented in two steps:\n\n\"Smoosh\" processed_a, processed_b, processed_c, processed_d into a single unified namespace.\nApply the match glob /* to the unified namespace, so that all the 000/* files from each input repo wind up assigned to a single node.\n\nIt would be OK if this were somehow a two step process.\n@jdoliner Does this help explain our use case?. Thank you for looking into this feature!\nI've writen up a test case for union/\"smoosh\" joins, with sample input and output repositories, and a Python Pandas script that does a simulated join:\nhttps://github.com/faradayio/pachyderm_union_join_test\nObviously, my heart isn't set on exact syntax in the *.json file, or how the two inputs get combined for /pfs. But ideally, it should be possible to produce that exact output data using the supplied Python script and the two input repositories. (No duplicate CSV headers in the middle of files allowed! \ud83d\ude42 )\nAs mentioned in the README, the real join will involve far more records and more than 2 input repositories. But this should give you a more concrete idea about what we're trying to do.\nOnce again, thank you for looking into this, and please don't hesitate to ask questions!. I can confirm that \"coefficient\" counts the kops-created master as well as the worker nodes.. When operating on files, it would certainly be nice to have the option to get more than one file at a time, like we did in 1.3. This would allow us to do a partial reduce across several input files. I did some measurements and this reduces the size of one particular output repo by at least 30%. But we can work around the absence.. This also breaks delete-all.\nThe only workaround I've found is to delete everything manually, including the S3 bucket and underlying etcd volume.. Yeah, I spent about half a day running a recursive S3 delete to clear out all our Pachyderm FS and rebuild from scratch.. :+1: I'm glad to see that this was as simple to implement as I had hoped. This will make more complicated S3 egress setups much nicer, especially for any organization large enough to use multiple accounts for things like billing, per-department access control, etc.. The various AWS regions have different REST endpoints, so even if authentication is automatic, many clients may still need to be told where to find the AWS APIs. \nThere may be some clever way to default this by GETing the magic URL http://169.254.169.254/latest/meta-data and extracting the region from there, but that's probably too clever by half.. ",
    "rawc0der": "Hi @thedrow ,\nYou're idea sounds cool, but I'm not sure how you would define the functionality of a source/sink.\nIs this similar to what you are thinking about ?\n) Source  Pachyderm Pipeline that injects data to Kafka from the output repo.\n) Sink Extracts data from Kafka into the input repo.\nThanks.. Hi @jdoliner,\nIt's really cool that Pachyderm allows you to write language agnostic connectors by using the Pachyderm FS repositories ( assuming the Connector is run as a container in the Pipeline). \n@thedrow , here are some ideas about a truly powerful native kafka-connector I've been thinking about:\n * Source/Sink with topic & repository whitelisting\n * option to add a new commit to pachyderm on each event passing through the topic\nWhilst in general it seems that in Pachyderm you can connect with any client to a Kafka Cluster, but what about from a Kafka-Connect Connector (i.e. Sink/Source worker job) to a Pachyderm Cluster, so an important question follows:\n - What is the best option to interact with the pachd server from the context of a Kafka Connector ?\n. After a bit of reading, I see that the  protocol buffer API  can be used to build a client interface in java for the Kafka Connector to use.\n. ",
    "kevpie": "@thedrow it looks like the Beam Go SDK is starting to emerge.  https://github.com/apache/beam/tree/master/sdks/go\nHope this is helpful.. ",
    "rikonor": "Noticed a few places in the Beginner Tutorial where the text is badly formatted:\ncreate-a-repo towards the end of the section.\nadding-data-to-pachyderm mid and end of section.\nPossibly more places, just haven't read through the entire thing yet.\n. ",
    "mlhamel": "It might be something we would want to keep when you are manipulating files using the pachctl but it seems it is also how it is currently working when you are manipulating files inside a pipeline, which was something really weird at first for me.. i think that would be a great idea and would simplify a lot of things.. ",
    "westurner": "For online queries from various datastores:\n- Blaze translates Python code to {SQL, Spark, Pandas, NumPy,}\n  https://github.com/blaze/blaze \nDask Distributed is mentioned above. Dask-ml is fairly new: https://dask-ml.readthedocs.io/en/latest/\nFrom https://dask-ml.readthedocs.io/en/latest/joblib.html :\n\nFor large arguments that are used by multiple tasks, it may be more efficient to pre-scatter the data to every worker, rather than serializing it once for every task. This can be done using the scatter keyword argument, which takes an iterable of objects to send to each worker.\n\n```python\nSerialize the training data only once to each worker\nwith parallel_backend('dask.distributed',\n                      scheduler_host='localhost:8786',\n                      scatter=[digits.data, digits.target]):\n    search.fit(digits.data, digits.target)\n```\ndask-ml is now installed in the kaggle/docker-python image:\nhttps://github.com/Kaggle/docker-python/blob/master/Dockerfile. What are the advantages of pachyderm over e.g. dask-ml w/ joblib? How much k8s and provenance tracking does it do for me? W3C PROV?\nI notice that you have used protobuf; whereas parquet is built on thrift. \nhttps://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift. Would Apache Arrow make this data handoff potentially more efficient (as it does between Spark and Pandas dataframes)?. ",
    "nak3": "Sure, I have done.. Here is the current log message (without this patch):\n~~~\ntime=\"2017-05-03T11:14:48Z\" level=error msg=\"Error extracting userid metadata from context: context.Background.WithCancel.WithCancel.WithCancel\\n\"\n~~~. OK, I see. I close this PR, then.. In my opinion, the rc generated by pachyderm should have pachyderm service account by default.\nI wanted to avoid adding privileged to the default (entirely in pachyderm project), but yeah oadm policy add-scc-to-user privileged system:serviceaccount:pachyderm:default is more useful atm (since rc doesn't have pachyderm by default). . ",
    "savvopoulos": "Done. ",
    "rj3d": "That works for me, thanks for the input and help!. There's just one file in the repo that's being marked as lazy.\npachctl list-file kallisto_indices master\nNAME                                  TYPE                SIZE                \nHomo_sapiens.GRCh38.87.cdna.all.idx   file                2.206 GiB. Yeah, that will work for me, thanks! I didn't realize how that worked. . No worries, thanks for the quick reply! And thanks for replying on a Friday night too! gs://tesla-data/metadata/intervals is a directory. Based on your explanation it makes sense why that doesn't work. intervals.csv is a file that's present. If I try to load the file directly I get the storage: object doesn't exist error. However, if I copy the file into it's own folder and load the contents of that directory recursively it works fine. I guess it was a little confusing how I wrote the commands. To make it a bit more clear, this command also gives me an object doesn't exist error:\npachctl start-commit somatic_variant_aligner_input master && \\\npachctl put-file somatic_variant_aligner_input master /tesla -f gs://tesla-data/metadata/intervals/intervals.csv && \\\npachctl finish-commit somatic_variant_aligner_input master. Looking at the logs, I got this WARN:\n2017-06-03T02:22:07Z WARN ambiguous key metadata/intervals/, not creating a directory or putting this entry as a file. Thanks! I get where you're coming from, but the current behavior ended up loading an entire folder and set of files that I didn't specify anywhere in my commit. This behavior seems a little confusing, no?. @derekchiang Here are the logs from creating the repo to a little bit after when the load finished. I'm not sure if it was put-file or finish-commit that was hanging since I chained the commands, but I was assuming that it was put-file since finish-commit has run instantaneously and fine the first time every time after using Control+C to get out.. @derekchiang yup. They're the logs from the point that I created the repository to the point that the data finished uploading according to the logs. The command never exited until I used Control+C and then ran finish-commit again.. @derekchiang  @sjezewski The dataset I'm using is ~75 GB spread across 8 files. I get the hang when I run this with a separate start-commit and finish-commit call. The longest I've let this run before exiting with Control+C has been ~18 hours. I just found a workaround though, using put-file -c works, and completes in ~7 minutes.. I'm now seeing this issue with much larger commits even when using put-file with -c and using export ADDRESS instead of port forward. The command just hangs and never returns, and we can workaround if we allow command to run long enough that the PutFile finishes in the logs, Control+C out, and then manually run finish-commit.\nOne commit contains 91 files with a total size of 640.48 GiB and a maximum file size of 42.55 GiB and the other commit contains 22 files with a total size of 226.88 GiB and a maximum file size of 29.58 GiB.\nI've been debugging this with @sjezewski, and according to his reading of the logs we see the StartCommits in the logs, the PutFiles complete in the logs, but we don't see the FinishCommit api call in the logs.\n. ",
    "dwojdak": "hey @derekchiang, were you by chance trying to pass any extra arguments to kubectl using the -k option? this problem happened to me when trying to pass a namespace. turns out i needed to quote the argument like this pachctl port-forward -k '--namespace=donuts'. not sure if this was your issue, but just in case it helps.... ",
    "abhitopia": "@JoeyZwicker - Further, I think following section adds to the confusion\n\nIs this talking about from or from_commit, and if branch name ( such master is allowed ) then how would that work because I believe that master would point to the head commit-id, and perhaps, the required functionality is to run only on last commit, ie. including the HEAD commit. Which clearly is not from from_commit should do. \nCan someone explain, how from_commit works with branch name?. cc: @jdoliner . @jdoliner - Thanks. I think the possible solution is that whatever is mounted to the pipeline as /pfs/repo is actually an intersection of output of DiffFile and GlobFile.\nP.S. - I am not familiar with go language, but I am referring to #1832 and I couldn't find any intersection between glob and diff files.. @jdoliner - also, if you need help reproducing the issue, please let me know.. @jdoliner - I am confident that file was put correctly. Because when I check the contents of original repo hello, they are correct as seen using list-file and 'get-file`. Its just how its mounted inside the pipeline that changes.. You can reproduce the error by following instructions here -\nhttps://github.com/abhitopia/pachy_bug. @jdoliner - when will this be merged?. I can confirm that the problem is not limited to more than 2 inputs. I am having same issues with following pipeline as well -\n```bash\n 9ee906ca-24b3-4c61-9c84-9b8244cee403\nID: 9ee906ca-24b3-4c61-9c84-9b8244cee403\nPipeline: _test_workflow_a_times_x_squared\nStarted: 26 minutes ago\nState: running\nProgress: 0 / 1\nWorker Status:\nWORKER              JOB                 DATUM               STARTED\nRestarts: 0\nParallelismSpec: \nInput:\n{\n  \"cross\": [\n    {\n      \"atom\": {\n        \"name\": \"config\",\n        \"repo\": \"test_workflow\",\n        \"branch\": \"master\",\n        \"commit\": \"342da2c59605439088c7bbeabe95e353\",\n        \"glob\": \"/configurations/a_times_x_squared/*\"\n      }\n    },\n    {\n      \"atom\": {\n        \"name\": \"x_squared\",\n        \"repo\": \"_test_workflow_x_squared\",\n        \"branch\": \"master\",\n        \"commit\": \"7ed9bd89d3d84e9d8620441a6d92c10e\",\n        \"glob\": \"/\"\n      }\n    }\n  ]\n}\nTransform:\n{\n  \"image\": \"agispof/simpletest:master\",\n  \"cmd\": [\n    \"python\",\n    \"/root/service.py\",\n    \"--input_dir\",\n    \"/pfs/x_squared\",\n    \"-c\",\n    \"/pfs/config/configurations/a_times_x_squared/a_times_x_squared.json\",\n    \"--output_dir\",\n    \"/pfs/out\"\n  ]\n}\n```. ",
    "seamusabshere": ". hey @dwhitena do you know of anybody doing this yet?. ",
    "kalugny": "I'm the user who requested this. Maybe I'll explain my use case -\nI'm using an automated image build system (Google's Container Builder).\nThe latest image built is built with the \"latest\" tag.\nIf I put in the pipeline configuration the image with a \":latest\" tag, then I don't need to update the pipeline - it'll just d/l and use the new image the next time it is run.\nAs it stands now, I need to manually change the pipeline configuration file after each build (and there are quite a lot of those dependent on the same image \ud83d\ude26 ).\n@jdoliner do you have another solution for me to try?\nThanks!. Thank you for the suggestion. I'll try to make this work.. It's a GetFile request, though\nOn Fri, May 5, 2017, 22:19 Joe Doliner notifications@github.com wrote:\n\n@kalugny https://github.com/kalugny we've hit similar thing with the go\nclient. Normally I've seen this with sending messages that are too big. Is\nit possible that what you're seeing is from the server returning that error\nrather than the client? Also just to confirm this is happening on a call to\nPutFile, if so it seems unlikely to me that the problem is the size of\nthe response because PutFile has an empty response.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/1774#issuecomment-299553000,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADUd_QtU_NiG6eQWFThjF4xZqAEs-3gaks5r23Y8gaJpZM4NONiS\n.\n. From what I could gather, the limit is on the server side.. I'm using 1.4.7 RC1\n\nOn Tue, May 23, 2017, 04:13 Joey Zwicker notifications@github.com wrote:\n\nwhat version are you using? Just wanted to double check because the name\nof the field has changed a few times.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/1862#issuecomment-303262269,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADUd_QKukNRR7AsINKp5BWeHVOiQNQ6Jks5r8jK5gaJpZM4NhhAx\n.\n. This is happening both on 1.5.1 and 1.5.2. That's great.Close #2190  \n\nOn Tue, Aug 22, 2017 11:14 PM, Gabriel Grant notifications@github.com  wrote:\nI might be misunderstanding what you're trying to achieve, but filtering by\ninput file(s) is possible directly from get-logs  today:\npachctl get-logs --job= --inputs /path/to/your/input/file\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.. ",
    "sudhirpandey": "Instead of doing all the fixes for service account pachyderm to be able to run containers in pachyderm, I have tried to have this instead oadm policy add-scc-to-user privileged system:serviceaccount:pachyderm:default , This is default sa user of the rc  so we dont have to fix all  permission issue  (ie oc patch with service account) repeatedly when pipeline is updated . Thus the generated rc from pipeline works ootb.. having this oadm policy add-scc-to-user privileged system:serviceaccount:pachyderm:default seems to address most of the issue while launching worker pods, it we might also do not need to modify service account fix for json file obtained via runningpachctl deploy custom ... ",
    "vuc4": "2017-05-08T14:00:51Z INFO  launching job manager for job 30caf766-ca05-4b3a-bbb4-5a5877bd2650\n2017-05-08T14:00:51Z ERROR error running jobManager for job 30caf766-ca05-4b3a-bbb4-5a5877bd2650: Operation cannot be fulfilled on replicationcontrollers \"pipeline-load-power-db-v5\": the object has been modified; please apply your changes to the latest version and try again; retrying in 473.05082ms. https://gist.github.com/vuc4/90b54a0732aea426c251498578c5a64f. When I did list-pipeline it would show the load-power-db pipeline as \"restarting.\" There are errors in the job, but those occurred after I updated the pipeline and reverted to the deprecated inputs format and jobs were able to run.. That will work.. Deployed to GCS. No caching solution.\nOn Mon, May 22, 2017 at 1:46 AM, Derek Chiang (Enchi Jiang) \nnotifications@github.com wrote:\n\nFrom the pachd logs, it doesn't look like those two jobs were running\nconcurrently. @vuc4 https://github.com/vuc4 what cloud provider were\nyou using? Were you using any custom caching solution such as Cloudfront?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/1854#issuecomment-303004629,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACAih6qBrIFZL3sQSpLaG_LBpiFexHjlks5r8SE-gaJpZM4NeZAG\n.\n. \n",
    "swimablefish": "I checked the file's content. It was as same as the original file, not contain append content.. ",
    "ah-": "In addition it seems that the --no-metrics switch doesn't work properly in 1.4.6, it still tries to connect to api.segment.io and fails completely if it can't.\nTo reproduce: Run pachctl deploy --no-metrics on a machine with api.segment.io:443 routed to netcat on localhost. It still tries to connect and fails if it can't.. ",
    "paologf": "Hello, I'm in the same situation as well :\n\nCOMPONENT           VERSION\npachctl             1.4.8\ncontext deadline exceeded\n\nI was following some example and then it stopped working, so I try deleting all with pachctl delete-all and then I calling minikube delete && minikube start && pachctl port-forward &.\nSince I started yesterday to inspect Pachyderm, is there an easy way to do a sort of global reset (completely from scratch) ?\nThanks.. Hi @JoeyZwicker, \nthanks for the reply.\nI was redoing the steps before reply you and this time it works.\nI'm thinking that was only a matter of timing between pachctl deploy local and port-forward.\nIndeed the kubectl get all message changed from READY 0/1 to READY 1/1.\nThank you.. ",
    "merl-dev": "same problem:\n```\n$ kubectl get all\nNAME                        READY     STATUS    RESTARTS   AGE\npo/etcd-281005231-jrpxr     1/1       Running   0          37m\npo/pachd-1390770499-z44c9   1/1       Running   3          37m\nNAME             CLUSTER-IP      EXTERNAL-IP   PORT(S)                       AGE\nsvc/etcd         100.70.194.1           2379:30638/TCP                37m\nsvc/kubernetes   100.64.0.1              443/TCP                       1h\nsvc/pachd        100.64.20.148          650:30650/TCP,651:30651/TCP   37m\nNAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeploy/etcd    1         1         1            1           37m\ndeploy/pachd   1         1         1            1           37m\nNAME                  DESIRED   CURRENT   READY     AGE\nrs/etcd-281005231     1         1         1         37m\nrs/pachd-1390770499   1         1         1         37m\n```\nbut when I run:\n$ pachctl version\nCOMPONENT           VERSION             \npachctl             1.5.0               \ncontext deadline exceeded\nThis wasn't happening earlier (about 2-3 minutes after firing up the cluster). ",
    "radu-adrian-moldovan": "Hi guys, run into same issue today \"context deadline exceeded\"\n\nand also running\n\nstrange is the fact the dashboard seems to be active\n\nafter a minikube delete; minikube start; pachctl deploy local; pachctl port-forward&\n\nit seems that all cmd line commands are working \n\nalso\n\nBUT the dashboard is dead (chrome and safari)\n\nalso after 5 minutes i got this!\n\nAny thoughts?! thanks!. well, I had to take it again from scratch and clear browser cache to have it working, strange!. ",
    "suneeta-mall": "I am seeing this issue too .. Cant explain why though -v/verbose arg on patchctl  is simply being ignore as far as I can tell (version 1.7.3) kube 1.10.4 with auth mode alwaysallow (RBAC does not work but thats another problem).\nHow does the the patchctl client connects to server? have no issue hitting the web interface but the patchctl client can not talk to server .. \n```\n\u279c  pachyderm $ kubectl get all                                                                                                               \nNAME                           READY     STATUS    RESTARTS   AGE\npod/dash-5d974d8668-92p79      2/2       Running   0          14m\npod/etcd-0                     1/1       Running   0          14m\npod/pachd-75bfdb94c8-9ktr5     1/1       Running   0          14m\nNAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                   AGE\nservice/dash            NodePort    100.69.174.28            8080:30080/TCP,8081:30081/TCP                             14m\nservice/etcd            NodePort    100.67.145.145           2379:30985/TCP                                            14m\nservice/etcd-headless   ClusterIP   None                     2380/TCP                                                  14m\nservice/kubernetes      ClusterIP   100.64.0.1               443/TCP                                                   4d\nservice/pachd           NodePort    100.70.44.204            650:30650/TCP,651:30651/TCP,652:30652/TCP,999:30999/TCP   14m\nNAME                      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/dash      1         1         1            1           14m\ndeployment.apps/neltapi   2         2         2            2           2d\ndeployment.apps/pachd     1         1         1            1           14m\nNAME                                 DESIRED   CURRENT   READY     AGE\nreplicaset.apps/dash-5d974d8668      1         1         1         14m\nreplicaset.apps/neltapi-6b4b4977b8   0         0         0         2d\nreplicaset.apps/neltapi-77c5c47876   0         0         0         2d\nreplicaset.apps/neltapi-7c8fc78485   2         2         2         2d\nreplicaset.apps/neltapi-7db7d8dbf8   0         0         0         2d\nreplicaset.apps/neltapi-7ff8548cff   0         0         0         2d\nreplicaset.apps/neltapi-8455b8f9bb   0         0         0         2d\nreplicaset.apps/pachd-75bfdb94c8     1         1         1         14m\nNAME                    DESIRED   CURRENT   AGE\nstatefulset.apps/etcd   1         1         14m\n\u279c  pachyderm$ pachctl version --verbose                                                                                                     \nCOMPONENT           VERSION           \npachctl             1.7.3             \ncontext deadline exceeded\n```\nand I have already deployed it twice. \nI would think portforward is only convenience for us to access on local and the patchctl client would still use server master for communication. . I finally figured this error out! \n@JoeyZwicker Yes I agree .. this error is least explained. The documentation of having to specify the ADDRESS can be improved. I can raise a PR to spell that better if that makes sense but I am not sure why this value need to be set in the first place. Is not this already discoverable by kubeconfig/server host?\nThe choice of port-forward and variable seems to fiddly to me .. IMO, pachctl should not have to use port-forward or ADDRESS but it should be discovered by config ... is there is a reason for why this choice was made?. @gabrielgrant My apologies that I was not clear earlier. \nI was just indicating that instead of asking user to set the address variable pachctl should discover it itself using the current context. The ip of api server is discoverable @ dig +short api.<cluster-DNS>  and cluster DNS via kube context so it should be simpler and one less thing user need to do. \nso export ADDRESS=`dig +short api.$(kubectl config current-context)`:30650\nAgreed on port-forward need, but it should be users choice. \nEDIt: I will try and get to raising a PR later . I still see this problem on 1.7.4 .. see malformed pipeline spec: json: cannot unmarshal string into Go value of type []json.RawMessage. @gabrielgrant yeah had mixed syntaxes for image secret. Thanks. \nBut that message is so annoying - does not help at all in diagnosing the problem :( . +1 \nI would like to see this feature in pachyderm too. Whilst separating resources by namespace (borrowed from K8S is great), there is no reason why one would not want to isolate the boxes thats processing long running jobs from other critical  services running in cluster sharing same K8S resources. \nBesides I might want to run some pachyderm pipelines on spot instances or boxes of different configuration .. may be even GPU. Node selector will be helpful in deploying these pod at best boxes as users sees it. . I wonder if its enough to have --dry-run --o yaml|json sort of option on create/update pipeline which then users can take and modify as per their wish to add selector/affinity(anti-affinity) etc. This saves from adding all those abstractions on pipeline spec. It just means theres more than one way to deploy the pipeline which in a way is a good thing .. then ksonnet package for pachyderm could also be a thing :) . @jdoliner Thanks for the CLA I have signed it up now! . @jdoliner Can this be merged please ? Anything I need to do for the build errors ? . I think this one is addressed via this https://github.com/pachyderm/pachyderm/issues/3073. @jdoliner  @JoeyZwicker  I did add in this PR but still outstanding https://github.com/pachyderm/pachyderm/pull/3133 . @JoeyZwicker Creating path to leads to successful install, but the behaviour before and after install is same i.e. no command completion is performed. . This is what I see:\n```\n\npachctl completion                      \nopen /etc/bash_completion.d/pachctl: no such file or directory\nsudo  mkdir -p /etc/bash_completion.d/\nPassword:\npachctl completion                            \nPermission error installing completions, rerun this command with sudo:\n$ sudo env \"PATH=$PATH\" pachctl completion\nopen /etc/bash_completion.d/pachctl: permission denied\nsudo env \"PATH=$PATH\" pachctl completion\nBash completions installed in /etc/bash_completion.d/pachctl, you must restart bash to enable completions.\n```\nOn a new shell post install:\npachctl  \nhands me new files from current dir. \n\nI have zsh and I am on mac - perhaps thats whats different.. doh! I missed that bit. Thanks for pointing out .. would love to have zsh completion . @jdoliner Thanks for the explanation. Yes I agree that perhaps deploying with raw resource definitions will give me desired behaviour and its cool that --dry-run gives me this behaviour. \nI will close it. . I would really like to see this feature primarily because:\n- In most common scenarios, one would know the expected range of datum that will be processed but that range could reasonably be very vast. \n- Having a fixed parallelism does not work very well in this case because pipeline management is very manual. (It does not work for me .. I am not a fan of baby sitting my pipelines)\n- In my understanding it should work inline with pod autoscalers. So the pipeline scales based on matrix that indicates how much pipeline should scale .. based on % increase of that matrix or absolute. \n- This matrix should somehow be correlated to number of datums that needs processing. Not saying one to one mapping but at least proportional.\n- The pipeline pods should be able to scale on that with range limited by ASGs/IG limits.\nI need to understand why pipelines are by default pods and not deployments (dont have good internal understanding of pachyderm just yet). But IMO, if they were deployments then enduser could perhaps scale them better and have better control over how thats managed. Having them as pods leaves users with very less control over it. \nLastly, in my personal experience, I wont be worried of scale up and down as long as its proportional. K8S does fairly well with it. As for it being DevOps problem, I disagree, it will actually be much better for them with assumption they are orchestrated with appropriate isolation. Thats my view anyway. Happy to learn and be corrected :)  \nI appreciate that its more complex than I am putting it here but there is value in being able to scale the processing pipeline on demand and not having to babysit it .. specially given the powerful  automatic pipelining capability Pachyderm provides.. @jdoliner doh! I missed checking RCs .. The pods never scale incrementally (even if change in only on parallelism)  .. they always get replaced and my bad I did not check on RCs. \nWhen I babysit, I basically monitor how much datum it is yet to process and increase parallelism basing on that (if I want it faster which so far has always been the case -- who does not want faster - scale horizontally! :-D). I find it annoying that increasing parallelism (even though nothing on i/p or transform step has changed), all pods gets recreated - terminating what they were doing. Again, there might be justifiable reason for that, which I dont understand just yet. But as user its fairly bad experience. Having said that I agree another topic for another thread . \nAs for scale on matrix, yes I agree we cant use default matrix to scale on .. it will have to be something pachyderm exposes. Really dodgy hack of externally pulling datum and checking job status to decide how many datums are outstanding and use that custom matrix to scale might just work.. but it given it will be an ask (I think) it makes sense for Pachyderm to support it (under the realm of whats feasible and sensible of-course) . change -> https://github.com/pachyderm/pachyderm/pull/3074. closing as fix is merged . @jdoliner if it help ->  https://github.com/pachyderm/pachyderm/pull/3076. I am closing this given its merged. @gabrielgrant Sorry I missed your question earlier ..  \nLets start with background:  My view at the time of comment was biased with assumption that Kubernetes cluster will be shared with Pachyderm alongside of any other services/kube resources. I appreciate this wont always be true. \nLets assume that its true for a bit, then finding A-record for API server is really as simple as digging or 'discovering' IP of the API server hostname (via network tools/libraries). Since Pachyderm port is accessible from all nodes then going via any of the master node IP will be fair. So if we just dig the cluster api server for IPs and chose one - that will simply work with how you are proposing to setup ADDRESS already. \nThis is how I get past it for example:\ndig +short api.$(kubectl config current-context) | while read mip ; do \n        export ADDRESS=$mip:30650\n    echo Possible host $ADDRESS\ndone\nNow, this clearly safely assumes that your current kube context is where pachyderm is deployed. But this can be an option e.g. --context  or  --use-context etc. \nIn essence, I was proposing that instead of asking user to explicitly set this (which I find annoying), it would be fairly better user experience (something easily achievable) to  'discover' this value and set it internally.  \nSo then its just about using argument  `--context`` (to pachctl) to override default (current-context) for cases where users are using non current-context Kube cluster as Pachyderm environment. The save really is by default pachctl just works without having to explicitly set anything (or provide context for special case). \nTLDR Use context to 'discover' the ip of API server and by default set it to ADDRESS as you are already proposing. The context can default to current.  This leaves us with port-forward being an optional thing thats useful for dashboard or should user prefer. I hope that clarifies a bit. Happy to be corrected if I oversaw anything  \n . @jdoliner  Created this based on info I have got. Please let me know if anything else is required. . Thanks guys.. I will close this. @gabrielgrant I have a good new and a bad news. The bad news is that I had a incident on cluster and had to recreate everything so I have lost the situation that presented the problem. That means I cant get you any more logs. \nAs for kubectl get all - it was showing all pods were running okay. None of the pods were reporting any issues or were crashing. You are right in saying that pachd was not getting an action from dash at all. I did bounch back all pods (dash, etcd, pachd) everything else worked fine as before but dashboard still showed the problem. \nAs for using port-forward, I was using API Server's Public IP but I did try port-forward too in my curiosity. Both behaved just the same - no joys. If I understand it correctly, I will agree with your reasoning but I am not expert about node side of things. \nNow to good news, the dashboard is now working. I think I may know why but there is no certainty to it. So I had enterprise token that I activated it dashboard and then from CLI i activated auth. Dashboard (and everything else was happy). But auth was annoying me so I shut it off from CLI meaning I deactivated auth. But I did nothing with dashboard. Before the auth deactivation from CLI Is last I have known dashboard working. (I dont use dashboard as often whilst in active dev). From this point onward dashboard never worked.\nI hope you find some clues to the problem but unfortunately I can no longer provide logs. Having said this the cluster frenzy had nothing to do with dashboard. \n. \n@gabrielgrant The dashboard issue started happening again .. this time without doing anything to cluster .. nothing at all... I setup brand new cluster and deployed. Everything was good. Added Enterprise token .. still good .. all pipelines works fine.. \nLeft it churning for 3 hours .. all process works perfectly file no crash/hang/caught fire etc .. but dashboard has load balancer policy error now..\nHeres kube get all log \n```\nNAME                                     READY     STATUS    RESTARTS   AGE\npod/dash-6866f58f7f-vp84q                2/2       Running   0          6h\npod/etcd-0                               1/1       Running   0          6h\npod/pachd-67f659cc9c-s7z9z               1/1       Running   3          6h\nNAME                                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                   AGE\nservice/dash                           NodePort    100.68.235.75            8080:30080/TCP,8081:30081/TCP                             6h\nservice/etcd                           NodePort    100.69.223.176           2379:32385/TCP                                            6h\nservice/etcd-headless                  ClusterIP   None                     2380/TCP                                                  6h\nservice/pachd                          NodePort    100.67.108.48            650:30650/TCP,651:30651/TCP,652:30652/TCP,999:30999/TCP   6h\nNAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/dash    1         1         1            1           6h\ndeployment.apps/pachd   1         1         1            1           6h\nNAME                               DESIRED   CURRENT   READY     AGE\nreplicaset.apps/dash-6866f58f7f    1         1         1         6h\nreplicaset.apps/pachd-67f659cc9c   1         1         1         6h\nNAME                    DESIRED   CURRENT   AGE\nstatefulset.apps/etcd   1         1         6h\n``\nI have removed all pipeline pods from logs but they have been happy. The 3 restarts on pachd was because of 'seg fault on inspect-pipeline that is known issue and @jdoliner fixed it recently. These crashes were before I left it off churning ...\nAgain, same issue accessing with port-forward or publicIP. New fail screen same problem\n\nAlso same as before, neither pachd nor dash pod get any request from browser\n. logs-from-grpc-proxy-in-dash-6866f58f7f-vp84q.txt\nAh yea so user container was not getting requests but grpc proxy dumped these errors. Theres also logs\ncall_id: 1,\n     json_data: '{\"stack\":\"Error: Call dropped by load balancing policy\\\\n    at /usr/src/app/node_modules/grpc/src/node/src/client.js:554:15\",\"message\":\"Call dropped by load balancing policy\",\"code\":2,\"metadata\":{\"_internal_repr\":{}}}',\n     event: 'error' } }\nsending (post-stringify): %s\n'{\"call_event\":{\"service_id\":1,\"call_id\":1,\"json_data\":\"{\\\\\"stack\\\\\":\\\\\"Error: Call dropped by load balancing policy\\\\\\\\n    at /usr/src/app/node_modules/grpc/src/node/src/client.js:554:15\\\\\",\\\\\"message\\\\\":\\\\\"Call dropped by load balancing policy\\\\\",\\\\\"code\\\\\":2,\\\\\"metadata\\\\\":{\\\\\"_internal_repr\\\\\":{}}}\",\"event\":\"error\"}}'\nsending (pbMessage message): { service_create: null,\n  service_release: null,\n  call_create: null,\n  call_event: \n   { call_id: 1,\n     service_id: 1,\n     event: 'error',\n     json_data: '{\"stack\":\"Error: Call dropped by load balancing policy\\\\n    at\nThe static resources were browser cached I think. I dont get them in private mode or on cache clear.\nand yes, the dashboard was working .. I left it as it closed my machine. Came back after 6 hours .. everything else works but not dashboard.. The pipeline always had stats enabled from beginning - its only 5/6 days old pipeline on a fresh cluster with enterprise key enabled from start ..  . @jdoliner ah that makes sense. Thanks\nIs there a trick to getting subsequent pipeline triggered as well (whilst this pipeline is in sort of hung state)?  I suppose not (other than manually branching off) but keen to hear your thoughts. @jdelfino Yeah. It leads to crashd. pachd dont seem to log segfault though. Last log of crashed pachd is inspect (see in details). Thats the previous pod log. Heres logs from transport closing seen today:\nerror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceedederror in monitorPipeline: context deadline exceeded%\nLed to crash in pachd pod, reason\nLast State:     Terminated\n      Reason:       OOMKilled\n      Exit Code:    137. Ran with 34 GB memory limit, error was still reproducible .. \nSee same error or transport closing .. pachd crash .. no logs on pachd  - last log on previous pod pachd (crashed one) is\n2018-09-18T19:37:27Z INFO pfs.BlockAPI.Obj.InspectObject {\"duration\":0.000017423,\"request\":{\"hash\":\"159c1a320ba8ebd62ba0a2c426b26cd04e54586a0bb4fab701cf2d2907b75b8eb174f764e47954084c7b6fb2f5329aabe9918422f148df3453c9d96328bc6ec3\"},\"response\":{\"object\":{\"hash\":\"159c1a320ba8ebd62ba0a2c426b26cd04e54586a0bb4fab701cf2d2907b75b8eb174f764e47954084c7b6fb2f5329aabe9918422f148df3453c9d96328bc6ec3\"},\"block_ref\":{\"block\":{\"hash\":\"b0c3313581484636825787176cf0aecb\"},\"range\":{\"upper\":527}}}} []\n2018-09-18T19:37:27Z INFO pfs.BlockAPI.Obj.GetObjects {\"duration\":0.000151842,\"request\":{\"objects\":[{\"hash\":\"159c1a320ba8ebd62ba0a2c426b26cd04e54586a0bb4fab701cf2d2907b75b8eb174f764e47954084c7b6fb2f5329aabe9918422f148df3453c9d96328bc6ec3\"}],\"total_size\":527}} []\n2018-09-18T19:37:27Z INFO pfs.API.GetFile {\"duration\":0.09619293,\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"__spec__\"},\"id\":\"c8b14c0e23ec41d597b33eb78e288232\"},\"path\":\"spec\"}}} []\ntime=\"2018-09-18T19:37:27Z\" level=info msg=\"PPS master: deleting workers for pipeline XX (PIPELINE_PAUSED)\"\nreason for crash\nLast State:     Terminated\n      Reason:       Error\n      Exit Code:    137\n2nd attempt ran a bit longer but pachd stopped responding to kubelete it seems - looked possibly hung after 2 mins of doing something .. eventually was killed by kube\n2018-09-18T19:42:06Z INFO pfs.API.GetFile {\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"__spec__\"},\"id\":\"e56d2e92292949698ffb4d4975b0a3fb\"},\"path\":\"spec\"}}} []\n2018-09-18T19:42:06Z INFO authclient.API.WhoAmI {\"request\":{}} []\nrpc error: code = DeadlineExceeded desc = context deadline exceeded% \nTrimmed event trace is:\nContainer image \"pachyderm/pachd:1.7.5\" already present on machine\n2m          2m           3         pachd-5bfdbc649-rhrx9.15559640a3bf4a5f    Pod          spec.containers{pachd}   Warning   Unhealthy           kubelet,     Readiness probe failed: rpc error: code = 14 desc = grpc: the connection is unavailable\n1m        2m        4         pachd-5bfdbc649-rhrx9.155596435612b282   Pod                 Warning   FailedKillPod   kubelet,    error killing pod: [failed to \"KillContainer\" for \"pachd\" with KillContainerError: \"rpc error: code = Unknown desc = Error response from daemon: Cannot stop container 31bfd1ffe97dbf48c77d6a137e3d9f630e1bab4000396138625de977b937c951: Cannot kill container 31bfd1ffe97dbf48c77d6a137e3d9f630e1bab4000396138625de977b937c951: rpc error: code = 14 desc = grpc: the connection is unavailable\"\n, failed to \"KillPodSandbox\" for \"05c64ec7-bb7a-11e8-bf33-020dbee6c9cc\" with KillPodSandboxError: \"rpc error: code = Unknown desc = Error response from daemon: Cannot stop container cebd49b04c59f91887965c5acd821533dfd7f38ca7f4fd4b239d855ad89e5767: Cannot kill container cebd49b04c59f91887965c5acd821533dfd7f38ca7f4fd4b239d855ad89e5767: rpc error: code = 14 desc = grpc: the connection is unavailable\"\n]\n57s       2m        5         pachd-5bfdbc649-rhrx9.15559642d3546463   Pod       spec.containers{pachd}   Normal    Killing   kubelet,    Killing container with id docker://pachd:Need to kill Pod. @jdoliner Created this to put errors together. Looks like a could of different reasons for fail on egress. My guess is oom kill repercussion. Hey @jdoliner Seeing increased number of error on egress \u2026 it seem to be related to AWS SDK \u2026 in pachyderm code .. do you explicitly specify region on write to s3?  It seems to fix the issue in most cases .. The rate limited errors as I have seen before presents differently so I think its to do with region may be ? see https://github.com/aws/aws-cli/issues/634 and https://github.com/boto/boto/issues/2207. I know you are probably using go SDK and not boto but it may be same issue .. \nmay be related ..  https://github.com/aws/aws-sdk-go/issues/1763?. I tried with 100 MB file .. uploading to S3 first and killing the subsequent updates whilst still uploading/updating ..  I dont see the same behaviour in those isolated s3 tests. \n. Ah yeah sorry my bad .. I forgot namespace arg .. my bad. @jdelfino Nice! thanks for letting me know .. I will try .4 soon .. ta. 1.8.4 worked . ta . It would be nice if Cobra supported unit suffixed for byte measure .. somewhat like duration but alas... \nThe doc might need to be changed to indicate this value being only in bytes. Right now it suggests suffixed and sort of implies value in MB .. just my thoughts :) . Not sure if my approval counts \ud83d\ude01  but you have my votes!  :shipit: . ",
    "banchee": "@jdoliner Gonna give the two inputs and same repo a go. Wasn't aware you could use multiple inputs with same repo, but this is great, will solve my issue \ud83d\udc4d . @jdoliner Im using pachctl version 1.4.7 and get this error when trying it out \npachctl update-pipeline -f borrower_merge.json                                                                                                                                               \u2039ruby-2.3.1\u203a\nconflicting input names: platform_borrower\nShould I try the 1.4.8 pre release? \nMy pipeline spec: \n\"input\": {\n    \"cross\": [\n      {\n        \"atom\": {\n          \"repo\": \"platform_borrower\",\n          \"glob\": \"/profiles/borrower_all.jsonl\"\n        }\n      },\n      {\n        \"atom\": {\n          \"repo\": \"platform_borrower\",\n          \"glob\": \"/applications/borrower_all.jsonl\"\n        }\n      },\n      ...\n    ]\n}. Just a suggestion, not sure how hard something like this would be implement. But as a developer and having to setup pipeline and updating them. To have a single datum with multiple files from a repo I would like to do soemthing like this \n{\n        \"atom\": {\n          \"repo\": \"platform_borrower\",\n          \"glob\": [\"/profiles/borrower_all.jsonl\", \"/applications/borrower_all.jsonl\"]\n        }\n}. ",
    "mightyguava": "What has been implemented already from this issue? It looks like the default for update-pipeline is now to not reprocess old data, with a --reprocess flag.. Oh wow, I wasn't expecting a reply so quickly. @jdoliner applying the version changes worked for me!. Thanks for the detailed answer @jdoliner. I think we should be able to split the input file in most cases. Hopefully that'll be fast enough. Doing additional grouping is probably not the best idea for us since we'd much rather rely on pachyderm to handle sharding for us rather than adding another layer on top.\n\nI suspect you could also speed this up by switching to a different object store, we've observed s3 to be one of the less performant object stores (GCS has generally been the best). I think almost all of the overhead here is in uploading to the object store so it should have a big impact.\n\nWe are going to be running pachyderm on AWS for the foreseeable future. If we opted to use a GCS bucket, I assume there's going to be non-trivial network latency. Do you have intuition or concrete numbers on whether this will more performant for us, going across clouds?. Hey @jdoliner, here's the pod info. It was deployed with defaults using pachctl. Don't know how high memory usage got since this is a cobbled together cluster with no instrumentation, but given it was OOM killed I assume it got up to at least 3G.\nkubectl describe pod pachd-7487cddb56-lkxbt\nName:           pachd-7487cddb56-lkxbt\nNamespace:      default\nNode:           ip-172-20-51-105.us-west-2.compute.internal/172.20.51.105\nStart Time:     Sun, 14 Jan 2018 15:36:18 -0500\nLabels:         app=pachd\n                pod-template-hash=3043788612\n                suite=pachyderm\nAnnotations:    iam.amazonaws.com/role=\n                kubernetes.io/created-by={\"kind\":\"SerializedReference\",\"apiVersion\":\"v1\",\"reference\":{\"kind\":\"ReplicaSet\",\"namespace\":\"default\",\"name\":\"pachd-7487cddb56\",\"uid\":\"930bb8d5-f96a-11e7-8c85-06f62e79fb1e\",\"...\nStatus:         Running\nIP:             100.119.118.92\nControlled By:  ReplicaSet/pachd-7487cddb56\nContainers:\n  pachd:\n    Container ID:   docker://65b82da4207188aec37b36e13ad85c4be8c2247143668f0eade96ed363c69b7f\n    Image:          pachyderm/pachd:1.6.7\n    Image ID:       docker-pullable://pachyderm/pachd@sha256:2de2309b0814555156012ffb1850180417991844d3b20b6f61c58f6112f92e59\n    Ports:          650/TCP, 651/TCP, 652/TCP, 999/TCP\n    State:          Running\n      Started:      Mon, 22 Jan 2018 17:42:49 -0500\n    Last State:     Terminated\n      Reason:       OOMKilled\n      Exit Code:    137\n      Started:      Mon, 22 Jan 2018 17:41:41 -0500\n      Finished:     Mon, 22 Jan 2018 17:42:30 -0500\n    Ready:          True\n    Restart Count:  2\n    Limits:\n      cpu:     1\n      memory:  3G\n    Requests:\n      cpu:     1\n      memory:  3G\n    Environment:\n      PACH_ROOT:                                      /pach\n      NUM_SHARDS:                                     16\n      STORAGE_BACKEND:                                AMAZON\n      STORAGE_HOST_PATH:\n      PACHD_POD_NAMESPACE:                            default (v1:metadata.namespace)\n      WORKER_IMAGE:                                   pachyderm/worker:1.6.7\n      IMAGE_PULL_SECRET:\n      WORKER_SIDECAR_IMAGE:                           pachyderm/pachd:1.6.7\n      WORKER_IMAGE_PULL_POLICY:                       IfNotPresent\n      PACHD_VERSION:                                  1.6.7\n      METRICS:                                        true\n      LOG_LEVEL:                                      info\n      BLOCK_CACHE_BYTES:                              1G\n      IAM_ROLE:\n      PACHYDERM_AUTHENTICATION_DISABLED_FOR_TESTING:  false\n    Mounts:\n      /pach from pach-disk (rw)\n      /pachyderm-storage-secret from pachyderm-storage-secret (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from pachyderm-token-2plgd (ro)\nConditions:\n  Type           Status\n  Initialized    True\n  Ready          True\n  PodScheduled   True\nVolumes:\n  pach-disk:\n    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:\n  pachyderm-storage-secret:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  pachyderm-storage-secret\n    Optional:    false\n  pachyderm-token-2plgd:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  pachyderm-token-2plgd\n    Optional:    false\nQoS Class:       Guaranteed\nNode-Selectors:  <none>\nTolerations:     node.alpha.kubernetes.io/notReady:NoExecute for 300s\n                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s\nEvents:          <none>. ",
    "tobschmidt": "We are seeing this issue with our ML pachyderm setup where the first pipeline calls a script that downloads files from S3 and creates a particular directory structure, with some of these directories empty. The following pipeline runs through all directories and populates them according to some predefined rules. The second pipeline does not know about how the directory structure was created or which directories should be present. Therefore, when some of the empty directories are missing, the second pipeline does not populate all required directories / files.. Here a dummy pipeline scripts to reproduce the problem:\npipeline-with-empty-dirs.json.zip\n. ",
    "ssaamm": "Admittedly, I'm unfamiliar with this codebase, but the CI failure looks unrelated to this change. Please correct me if this breaks something!. #1984 is a superset of this PR. My bad. Signed!. ",
    "peterjdolan": "The build failure seems to be unrelated to the change.. Thanks @dwhitena . I've signed the CLA.. ",
    "DSchmidtDev": "@derekchiang It seems that I'm also hitting the Go runtime thread limit (all Subconns are in TransientFailure) when processing 10k-25k files but I explicitly set lazy to false. \nIs it possible that the file handling is independent of the lazy option?. This issue is not GKE specific. It's related to the Kubernetes version and RBAC settings.\nThe serviceaccount pachyderm is missing a role binding with enough permissions to list and create the resources. \nIn case of using the helm chart an additional clusterrole and binding resource with enough permissions is needed. I'm not sure. Thought it was officially introduced (stable) with Kubernetes v1.8 but it depends on the deployment args. GKE disabled the old authorization method with the change to v1.8 as default. So since then you have to configure your roles when needed. With v1.6 and v1.7 RBAC and the old authorization was possible in parallel.\nUnfortunately I haven't invested much time in general RBAC settings yet to tell you more..\nYour snippet does not cover all permissions. \nThe ClusterRole needs at least create, update and delete permissions to ReplicationControllers for the pipeline worker.. ",
    "najibninaba": "This is happening for me as well, version 1.5.2 on GCS.\n. I've tested with version 1.5.3 and I've successfully deployed 1.5.3 and ran through the beginner tutorial with no problems.\nRecommend to close this issue.. This is the workaround that worked for me with the current default version 1.8.8-gke.0 in GCP and Pachyderm version 1.7.0. I've also installed Pachyderm on a separate namespace. The installation uses the default RBAC setup as per the official documentation[1].\n[1] http://pachyderm.readthedocs.io/en/latest/deployment/google_cloud_platform.html\nThe workaround is as follows, after running the Pachyderm deployment steps:\n$ kubectl delete clusterrolebinding pachyderm\n$ kubectl create clusterrolebinding pachyderm --clusterrole=cluster-admin --serviceaccount=pachyderm:pachyderm --namespace=pachyderm --user=system:serviceaccount:default:pachyderm\n$ kubectl delete pods --all\nThe key thing is that the user is set to system:serviceaccount:default:pachyderm and given the cluster-admin role. Is this something that can be set for the serviceaccount settings somewhere?. ",
    "ritazh": "@jdoliner done! Not sure how to fix the CI...help?. Thanks @jdoliner \ud83c\udf89 . ",
    "ClaytonJY": "Does a manual commit to what is normally a pipeline-populated repository have to break assumptions around provenance? If one was looking for the provenance of something further downstream that depended on that manually-committed data, could the reporting/introspection tools be made to indicate that it traces back to a manual commit?\nI'm not saying that isn't a ton of work, and it might not be at all worth it given what your goals for this tool are, but as an outsider it seems provenance-tracking need not be incompatible with this sort of workflow. After all, every commit to a top-level repository is a manual one.. @jdoliner thanks for the explanation, I see what you mean now. Diamond dependency graphs strike again!. ",
    "kareemk": "adding paths to LD_LIBRARY_PATH doesn't work. Confirming that @dwhitena is describing our usecase accurately. Thank you.. Reading through this raises another issue, is it possible for a job to then write to this repo (as well as its default repo)?. ",
    "jtratner": "The fix in #2233 works for arbitrarily many data repos? (i.e., a different env var for each name?)\nonly issue here is you'd still need to use stdin because it's not giving you the subpath from the repo for the datum.\ni.e., stdin becomes something like:\ncmd: [\"/bin/bash\"]\nstdin: [\"code.py $input /pfs/out/${input##/.*/.*}\"]\nbut yeah basically handles what I'm looking for - would it only work in stdin?\n. Often the transform I want to do is:\ncode.py --bam=/pfs/input/FlowcellA/Sample1/somefile.bam\n--vcf=/pfs/input/FlowcellA/Sample1/something.vcf\n/pfs/out/FlowcellA/Sample1/outputfile.txt\nSo my assumption is this would look like:\ncode.py --bam=$input/somefile.bam --vcf=$input/something.vcf\n/pfs/out/${input#/./.}\nWhere the goal is to strip out \"/pfs/input\" and put \"/pfs/out\" instead.\nBut def much cleaner\nOn Thu, Aug 31, 2017 at 6:13 PM Joe Doliner notifications@github.com\nwrote:\n\nRealized I didn't answer your last question, the env vars should work\noutside stdin too. So you can just as easily have your python code access\nthem directly.\nAlso I'm not totally sure I answered your question 2nd question. When you\npass /pfs/out/${input##/./.} what is that communicating to your code?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/issues/2232#issuecomment-326460992,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABhjq9qxGQ2oDIHTZfijUmXv3mohGuCUks5sd1o2gaJpZM4PJg6o\n.\n. \n",
    "fortytw2": "I wonder how exactly subscribing to a Kafka topic / the output of other streaming systems could work within the restrictions of current pachyderm?  \nWe ( myself and @bpb ) are already working on some of these (for internal use, but to be open sourced), it could be really beneficial to coordinate our efforts so we don't build the same things twice or establish two separate \"standards\" for standardized operations. \nAlso related to https://github.com/pachyderm/pachyderm/issues/657. this could be simplified as a map[string]bool then return failReasons[reason], which returns false in the nonexistent case (default value of a bool is false)\nhttps://play.golang.org/p/krYiE4pWwW. ",
    "edhemphill": "@sjezewski  - How were you able to get that to work?  My minikube overwrites all settings on boot, including changes to systemd  (I'm on k8s 1.8 w/ a Hyper-v VM driver). ",
    "pragmasoft-ua": "We're considering integrating Pachyderm as a pipeline engine into our product, and we also need better control over what can trigger a pipeline. Ideally would be to have a possibility to disable automatic triggering at all, for example, if customer account does not have enough balance when customer uploads new data to the repo, and also possibility to trigger pipeline manually by some api call and command. It would then work on files committed since last time it was triggered. \n. ",
    "itssimon": "Has any progress been made on this suggestion? Such an option would be perfectly suited for our use cases as well.. Yeah, there probably is an obvious workaround for this, but I could use some guidance on how to best implement it. I've got a pipeline that pulls data from S3 and processes it continually. Only once a month I want to generate an aggregate report based on all the data of the previous month. If I could use a cron input crossed with a non-trigger atom input of the continually updated data that would be easy.. Thanks, that should do just fine!. I'm also facing the same issue on AWS. ",
    "dcjohnson": "It works fine while running on containers. I have used iam ec2 roles while running in kubernetes and, as an example, you can exec into a pod and run \ncurl http://169.254.169.254/latest/meta-data/instance-id/\nwhich will return the instance-id of the node the pod is running on. \n. ",
    "konstunn": "I've already managed to build and run docker on i686 Debian (though the cpu supports long mode, so its architecture is x86_64, but crossgrading the system from i686 to x86_64 is not an option right now). I've successfully run hello-world container. So it seems to work, right? Unfortunately, I can not reproduce all the build steps right now.\nWhat I failed to do so far is to build a debian package so that I could track and manage the installation in my system with a package manager.\nI hope that, as long as I have docker set up (though, manually) and running, things will get simpler to build all the remaining things inside docker. What do you suggest?. ",
    "sarovios": "@gabrielgrant The link is not working.. Any updates/workaround on that?. @dwhitena I am not sure if that's relevant, but I have a custom deployment using minio as object store. I am not deploying pachyderm to aws.. @dwhitena Which is the newest release? I am using 1.6.0rc7 at the moment and I encounter this issue.. ",
    "uzdry": "Well, pachctl itself does not have that flag and I do not know how to pass a flag from pachctl to kubectl.. I now also saved the --dry-run.\nWith minikube up and running following worked:\nkubectl create -f ./pachyderm-manifest.json --validate=false\nBut @refrigerator, I think it was clear enough.. ",
    "refrigerator": "I'm also having this issue. I tried what you suggested @JoeyZwicker - I saved the --dry-run output and ran kubectl create -f ./pachyderm-manifest.json --validate=false but am getting these errors:\nunable to recognize \"./pachyderm-manifest.json\": no matches for /, Kind=ServiceAccount\nunable to recognize \"./pachyderm-manifest.json\": no matches for extensions/, Kind=Deployment\nunable to recognize \"./pachyderm-manifest.json\": no matches for /, Kind=Service\nunable to recognize \"./pachyderm-manifest.json\": no matches for /, Kind=Service\nunable to recognize \"./pachyderm-manifest.json\": no matches for extensions/, Kind=Deployment\nany ideas? . Solved it - I didn't have minikube up and running - running minikube start before trying the pachyderm commands worked :) Perhaps we can make this a bit clearer in the docs?. ",
    "gauti038": "I am currently running client v1.8 and server is also at 1.8+.\nEven when I run this with the validate=false flag, I am still seeing the error \nerror: unable to recognize \"dc2.yaml\": no matches for /, Kind=Deployment\nCan you please let me know how can I proceed? \n. ",
    "williballenthin": "I kept notes as I figured out how to do a local deployment of pachyderm on Win10+Hyper-V+WSL. You can review them here: https://gist.github.com/williballenthin/8bbb7710ca8faa93d6eb57a44eda785b. There's also this error in the k8s logs:\n$ kubectl logs -l app=pachd | head -n1\ntime=\"2017-12-28T16:17:13Z\" level=error msg=\"unable to access kubernetes nodeslist, Pachyderm will continue to work but it will not be possible to use COEFFICIENT parallelism. error: nodes is forbidden: User \"system:serviceaccount:default:pachyderm\" cannot list nodes at the cluster scope\"\npotential fix:\nCreate the following cluster role for reading the node list (via: k8s role-examples):\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: nodes-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"get\", \"list\", \"watch\"]\nand create a clusterrolebinding for the default:pachyderm service account:\n```\n$ kubectl create -f nodes-reader.yaml\nclusterrole \"nodes-reader\" created\n$ kubectl create clusterrolebinding pach-nodes-reader --clusterrole=nodes-reader --serviceaccount=default:pachyderm\nclusterrolebinding \"pach-nodes-reader\" created\n```\nnow when deploying pachyderm, the first log message looks like:\n$ kubectl logs -l app=pachd | head -n1\ntime=\"2017-12-28T17:23:15Z\" level=info msg=\"validating kubernetes access returned no errors\"\nwhich seems to be an improvement.. I've reviewed the kubectl logs for app=pachd and suite=pachyderm and there are no other warning/error messages:\n$ kubectl logs -l suite=pachyderm | grep error\ntime=\"2017-12-28T17:23:15Z\" level=info msg=\"validating kubernetes access returned no errors\"\n$ kubectl logs -l suite=pachyderm | grep warn\n$ kubectl logs -l app=pachd | grep error\ntime=\"2017-12-28T17:23:15Z\" level=info msg=\"validating kubernetes access returned no errors\"\n2017-12-28T17:33:18Z ERROR pps.API.InspectPipeline\n$ kubectl logs -l app=pachd | grep warn\nHowever, jobs are not spawning as expected (this workflow has worked on minikube and gce before), so i wonder if there are some other errors that have been swallowed. I'm not sure where else to look right now.. Hey @JoeyZwicker and @jdoliner thanks for the update. Glad to hear that the potential fixes were on the right track.\nWhile there are no other warning/error messages in the logs, and I can upload and list files to repos, jobs are not being spawned. But I don't know where to look to triage and confirm it is/n't related to the RBAC settings. Have you been able to reproduce this? Is there any information I can collect to chase it down? \n(We can migrate to a separate issue if you'd like). Here's the first pipeline in my flow. It hashes and arranges input samples:\n```\n$ pachctl list-pipeline\nNAME           INPUT              OUTPUT                CREATED             STATE\nsample         samples:/*         sample/master         7 days ago          running\n$ pachctl inspect-pipeline sample\nName: sample\nCreated: 7 days ago\nState: running\nReason:\nParallelism Spec: \nResourceRequests:\n        CPU: 0\n        Memory: 64M\nInput:\n...\n...\nJob Counts:\nstarting: 0     running: 0      failure: 0      success: 0\n```\nNote the job count is 0, despite the fact I've added files to the input repository:\n```\n$ pachctl list-job\nID OUTPUT COMMIT STARTED DURATION RESTART PROGRESS DL UL STATE\n[empty]\n$ pachctl list-file samples master\nNAME                TYPE                SIZE\ncat                 file                50.86KiB\nls                  file                123.6KiB\n```\nHere is the k8s state. It seems there is a rc for the pipeline.\n```\n$ kubectl get all\nNAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeploy/pachd          1         1         1            1           7d\nNAME                         DESIRED   CURRENT   READY     AGE\nrs/pachd-64cb8cffb8          1         1         1         7d\nNAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeploy/pachd          1         1         1            1           7d\nNAME                DESIRED   CURRENT   AGE\nstatefulsets/etcd   1         1         7d\nNAME                         DESIRED   CURRENT   READY     AGE\nrs/pachd-64cb8cffb8          1         1         1         7d\nNAME                                        READY     STATUS    RESTARTS   AGE\npo/etcd-0                                   1/1       Running   0          7d\npo/pachd-64cb8cffb8-mrq85                   1/1       Running   0          1d\npo/pipeline-sample-v1-8xs66                 2/2       Running   0          7d\nNAME                                  DESIRED   CURRENT   READY     AGE\nrc/pipeline-sample-v1                 1         1         1         7d\nNAME                                   TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                                                   AGE\nsvc/etcd                               NodePort    10.3.0.179           2379:32096/TCP                                            7d\nsvc/etcd-headless                      ClusterIP   None                 2380/TCP                                                  7d\nsvc/kubernetes                         ClusterIP   10.3.0.1             443/TCP                                                   8d\nsvc/pachd                              NodePort    10.3.0.42            650:30650/TCP,651:30651/TCP,652:30652/TCP,999:30999/TCP   7d\nsvc/pipeline-sample-v1                 ClusterIP   10.3.0.239           80/TCP                                                    7d\n``. After a suggestion by @jdoliner, I found a bunch of error messages in the worker pod logs for theusercontainer. The log messages from thestoragepod were already in the output fromkubectl logs -l suite=pachyderm` (this is surprising to me, but am not sure if its expected, or a pach or k8s bug):\n$ kubectl logs pipeline-sample-v1-8xs66 user | head\n{\"pipelineName\":\"sample\",\"workerId\":\"pipeline-sample-v1-8xs66\",\"master\":true,\"ts\":\"2017-12-28T17:32:02.692277370Z\",\"message\":\"Launching worker master process\"}\n{\"pipelineName\":\"sample\",\"workerId\":\"pipeline-sample-v1-8xs66\",\"master\":true,\"ts\":\"2017-12-28T17:32:02.704355432Z\",\"message\":\"master: error running the master process: master: error constructing worker pool: unknown (get endpoints); retrying in %!!(MISSING)v(MISSING); retrying in 8.608714168s\"}\n{\"pipelineName\":\"sample\",\"workerId\":\"pipeline-sample-v1-8xs66\",\"master\":true,\"ts\":\"2017-12-28T17:32:11.316102829Z\",\"message\":\"Launching worker master process\"}\n{\"pipelineName\":\"sample\",\"workerId\":\"pipeline-sample-v1-8xs66\",\"master\":true,\"ts\":\"2017-12-28T17:32:11.320513340Z\",\"message\":\"master: error running the master process: master: error constructing worker pool: unknown (get endpoints); retrying in %!!(MISSING)v(MISSING); retrying in 16.060099141s\"}\n[...and 93,000 similar lines...]\nThe error messages seem to indicate that the worker nodes do not have permission to fetch the endpoint list.\npotential fix\nCreate the following cluster role for reading the endpoints list:\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: endpoints-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"endpoints\"]\n  verbs: [\"get\", \"list\", \"watch\"]\nand create a clusterrolebinding for the default service account:\n```\n$ kubectl create -f endpoints-reader.yaml\nclusterrole \"endpoints-reader\" created\n$ kubectl create clusterrolebinding default-endpoints-reader --clusterrole=endpoints-reade\nr --serviceaccount=default:default\nclusterrolebinding \"default-endpoints-reader\" created\n```\nNote that this gives the endpoints-reader role to the default service account, which is a bit more relaxed than best-practice. Binding the same role to the default:pachyderm service account did not affect situation. I'd recommend ensuring that workers are running as the default:pachyderm service account and binding the endpoints role to this service account, instead.\nAnyways, now jobs are created!\n$ pachctl list-job\nID                                   OUTPUT COMMIT                                         STARTED       DURATION           RESTART PROGRESS  DL       UL       STATE\n73e22045-1287-4532-a8ee-f95bc36ff97e sample/22188c84b7394c2bab8df75a02d3d853               4 minutes ago Less than a second 0       1 + 1 / 2 50.86KiB 50.86KiB success\n589ae918-b3fd-4d16-b065-fe7ef26b4c67 sample/7e501d82b63044bf9af773801b3d22c0               4 minutes ago Less than a second 0       1 + 0 / 1 123.6KiB 123.6KiB success. @dionjwa I had success manually creating RBAC roles and bindings to get Pachyderm working. See the original issue here: https://github.com/pachyderm/pachyderm/issues/2601\nThis was for an AWS deployment though, not for minikube where I would often tear down and rebuild. ymmv.. Hey @JoeyZwicker this makes sense to me. I keep realizing that my mental model of pachyderm is slightly off, so, this is my fault. Thanks for taking the time to clarify how pachyderm handles the files and jobs.\nI think that your correct that a reasonable solution is to always handle the inputs, and use output paths to indicate success/failure. . Other theories I've considered:\n\noverhead from outstanding job depth (creating commits faster than pipelines can process); however, after I stopped creating new input files, job duration continued to increase with no change.\ntoo many processes/threads/tasks running on worker nodes; however, via manual inspection (and CPU utilization charts above), CPU load has remained between 3 and 5 on four core nodes. This doesn't seem out of place.. in comparing job logs from early and recent jobs from the same pipeline, I noticed that the time before the first log entry (before process call started - request: ...) has increased dramatically. see cherry-picked example below:\n\nearly job\n130ms: process call started - request: job_id:\"ced77a2e-3f67-4402-81de-e0bb2ea2975e\" data:<file_info:<file:<commit:<repo:<name:\"idbs\" > id:\"cdc35bcc1ec243809dc8561248f30d20\" > path:\"/73665ca317de49300c8a373d180b2b619a8b865e913688120d97635c1c57f8e0\" > file_type:FILE size_bytes:4607861 hash:\"\\025\\335c\\263y\\256\\300\\342\\267\\271\\374R\\214!s\\266G\\037\\330\\005\\026p_E*8\\253\\020)j\\rw\" > name:\"idbs\" branch:\"master\" > enable_stats:true\n174ms: input has not been processed, downloading data\n325ms: input data download took (151.445356ms)\n325ms: beginning to run user code\n327ms: inputs:\n328ms: 73665ca317de49300c8a373d180b2b619a8b865e913688120d97635c1c57f8e0\n330ms: input:  73665ca317de49300c8a373d180b2b619a8b865e913688120d97635c1c57f8e0\n378ms: WARNING:idb.fileformat:section class not implemented: id2\n386ms: INFO:idb.netnode:resolved string netnode Root Node to ff00000000000002\n386ms: INFO:idb.netnode:resolved string netnode Root Node to ff00000000000002\n420ms: output:\n421ms: 00d6cfd5bdf6998f39ce1af16d7b4e6f.i64\n421ms: done\n422ms: finished running user code - took (97.095435ms) - with error (<nil>)\n438ms: starting to upload output\n746ms: finished uploading output - took 307.179174ms\nrecent job\n10948ms: process call started - request: job_id:\"8e8a712a-a917-40d3-8fc3-62d22c5abd73\" data:<file_info:<file:<commit:<repo:<name:\"idbs\" > id:\"1ba6e5d8590141d69a41cd5e5193c046\" > path:\"/e2cca7bed72a04647422b4d9adc214c9f69acd25bb5ceeacd4e633e7b2ec2c34\" > file_type:FILE size_bytes:1273955 hash:\"\\304\\341\\306p\\332_\\335S\\350\\222\\362z\\374\\303\\265\\032*mG&/21W\\230\\211\\270\\231n\\266\\257,\" > name:\"idbs\" branch:\"master\" > enable_stats:true\n10996ms: input has not been processed, downloading data\n11192ms: input data download took (195.252644ms)\n11192ms: beginning to run user code\n11195ms: inputs:\n11196ms: e2cca7bed72a04647422b4d9adc214c9f69acd25bb5ceeacd4e633e7b2ec2c34\n11198ms: input:  e2cca7bed72a04647422b4d9adc214c9f69acd25bb5ceeacd4e633e7b2ec2c34\n11262ms: WARNING:idb.fileformat:section class not implemented: id2\n11289ms: INFO:idb.netnode:resolved string netnode Root Node to ff000002\n11289ms: INFO:idb.netnode:resolved string netnode Root Node to ff000002\n11315ms: output:\n11316ms: 3442f17ac4cb2559d69fcefe79fce345.idb\n11318ms: done\n11319ms: finished running user code - took (126.904087ms) - with error (<nil>)\n11338ms: starting to upload output\n11825ms: finished uploading output - took 486.345898ms. in reviewing a slack conversation from earlier, i see this explanation:\n\n@jdoliner:  so basically what's going on here is that for each commit you create the system needs to figure out what all the datums are for that commit. Then for each datum either process, if it's new, or skip it if it's already been processed, and then merge the results together. Skipping datums is much faster than processing them, but there's still some overhead associated with it since you have to check if you have the right objects in the object store. Because you're committing one datum at a time Pachyderm needs to do n^2 checks for already processed datums to process n datums. One of the major things we're going to make faster is the check for already processed datums. But in the meantime you can make this a lot faster by committing things in batches.\n\nI won't self-close the issue yet to allow for some discussion. I find the existing behavior surprising (though, understanding after explanation), and definitely look forward future enhancements.. Hey @gabrielgrant thanks for adding to the discussion here!\nWould you mind clarifying a bit (so that my future pipelines are most efficient):\n\njob runtime does increase linearly with the number of datums in that job\n\nSo with one datum per job, then overhead is at its minimum?\n\nAt the moment, the added overhead of an additional skip-able datum is fairly small.\n\nCool, this makes sense to me. In my example above, none of the datums are skippable. Is there an overhead for non-skippable datums? \nDoes the increasing overhead reported here comes from having to check \"for the new datum, does it match any of the old datums so that i can skip it?\". If so, is the correct name for this github issue \"per-job duration increases linearly with the number processed datums\"? Ultimately, I'm trying to understand the crux of the issue and avoid linear performance degradation as I add commits.. Whoops, probably just getting tripped up on terminology. I had been using \"datum\" to refer to a group of files added in a commit that are grouped and processed together.\nIn the example \"user-idb\" pipeline from above (and whose contents are below), the inputs are unique files that can be processed independently. \n{\n    \"pipeline\": {\n        \"name\": \"user-idb\"\n    },\n    \"input\": {\n        \"atom\": {\n            \"repo\": \"idbs\",\n            \"glob\": \"/*\"\n        }\n    },\n    \"parallelism_spec\": {\n        \"constant\": 1\n    },\n    \"transform\": {\n        \"image\": \"...\",\n        \"image_pull_secrets\": [\"dockerhubkey\"],\n        \"cmd\": [\"/bin/bash\"],\n        \"stdin\": [\n            \"cd /pfs/idbs/;\",\n            \"FILE=$(ls -1 | head -n 1); \",\n            \"IDBMD5=$(python /opt/idb_md5.py $FILE);\",\n            \"FMD5=$(md5sum $FILE | cut -d ' ' -f 1);\",\n            \"mkdir -p /pfs/out/$IDBMD5/$FMD5/;\",\n            \"if [ $(head -c 4 $FILE) = 'IDA1' ]; then \",\n            \"  cp $FILE /pfs/out/$IDBMD5/$FMD5/$FMD5.idb ;\",\n            \"else \",\n            \"  cp $FILE /pfs/out/$IDBMD5/$FMD5/$FMD5.i64 ;\",\n            \"fi; \",\n            \"exit 0;\"\n        ]\n    },\n    \"enable_stats\": true\n}\n. Ah, I see that now:\n$ pachctl inspect-job 378cca63-622c-4575-b95f-f32fb413b955\nID: 378cca63-622c-4575-b95f-f32fb413b955\nPipeline: p1\nParent: cf6ccaac-5f64-403a-941d-96282e504b2d\nStarted: 2 minutes ago\nDuration: 5 seconds\nState: success\nReason:\nProcessed: 1\nSkipped: 151\nTotal: 152\nSo, as new files are added and processed independently, the other existing files are considered skipped. And there is a constant overhead for each other file that is skipped. So as more files are added to the repo, jobs will take longer and longer. Thanks for walking me through this behavior @gabrielgrant and @JoeyZwicker.\nSo, given the above, is there anything I can do to keep overhead down, when my use case is to process small subsets of a repo independently? I've heard a suggestion to batch commits to the input repo by @jdoliner, but I think this will only delay the problem, rather than solve it.\n. For future reference, here are scripts and a pipeline spec to reproduce the behavior described in this issue.\np1.json\n{\n    \"pipeline\": {\n        \"name\": \"p1\"\n    },\n    \"input\": {\n        \"atom\": {\n            \"repo\": \"inputs\",\n            \"glob\": \"/*\"\n        }\n    },\n    \"transform\": {\n        \"image\": \"alpine:3.7\",\n        \"cmd\": [\"/bin/sh\"],\n        \"stdin\": [\n            \"for F in /pfs/inputs/*;\",\n            \"do\",\n            \"  HASH=$(sha256sum ${F} | cut -d ' ' -f 1);\",\n            \"  mkdir -p /pfs/out/${HASH}/;\",\n            \"  ln -s ${F} /pfs/out/${HASH}/${HASH};\",\n            \"done;\"\n        ]\n    },\n    \"enable_stats\": true\n}\nsetup.sh\n```\n!/bin/bash\npachctl create-repo inputs\npachctl create-pipeline -f p1.json\n```\ntrigger.sh\n```\n!/bin/bash\nfor i in $(seq 1 1000); do\n    echo \"$i\";\n    echo \"$i\" > \"/tmp/${i}.txt\";\n    pachctl put-file inputs master \"$i\" -c -f \"/tmp/${i}.txt\";\n    rm \"/tmp/${i}.txt\";\ndone\n```\nreset.sh\n```\n!/bin/bash\npachctl delete-pipeline p1 --delete-jobs --delete-repo\npachctl delete-repo inputs\npachctl garbage-collect\n```\n. ",
    "rmuellerjr": "I am also interested in the progression of Pachyderm for local Windows deployment . Let me see if I can give a try on Windows!\n. ",
    "pappasilenus": "I posted to the user, waiting for response.\nJohn Karabaic\ngpg key available at hkp://keys.gnupg.net\n513.295.6365\n\nOn Dec 7, 2018, at 20:32, Gabriel Grant notifications@github.com wrote:\n@pappasilenus I think you were asking about this yesterday. Did you get this sorted out?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. To be clear, Good/Better/Best are additive.  Good is the minimum we should provide, Better would be nice to have in addition to Good, and Best would be icing on the cake.\n. @brokenjacobs Is there an issue number with EMC that we can reference here?. But I'm able to see the dash without explicit port forwarding. . Note: you may actually need to pull this and build in sphinx to see the animations for cross and union, since I used HTML markup to scale them and relative links for efficiency.  I've verified it builds correctly in my environment.. > @pappasilenus Hey, I believe the titles Overwriting files and Appending to files might be reversed?\n\nGood catch, @gpkc !  Fixed, added a transition.  Thanks!. That's just the way that markdown does numbered lists.  You always start them with 1. and md does the rest\n\nJust\nLike \nThis\n. Spent some time how to put an edit of another file in the same PR.  I removed the old content, since it's in the new file anyway, and left it with one line, below:\n\nThis content has been moved to the new section, Backup, Restore, and Migrate\n. Agree with @jdoliner 's comment.  . Good point, clarified and recommitted!. Links added.. I'm going to replace \"Pipelines\" with \"Pachyderm Pipeline System\" because it's more correct than \"pipelines\".. Reworked it a bit, but got the gist of the comment.. The comma is grammatically correct, as \"above\" is in apposition, but your point on style is taken. :-). Added to my internal style guide.  Hard to reconcile my casual, breezy style with jargon. :-). There's no \"sql\" option in 1.8.6.  Now I see that there's a docs issue on it.  I'll link to it here.. Split and added to this PR.. I removed this so we can ship this thing, and will add it back if we confirm.. Documentation doesn't follow the \"don't repeat yourself\" dictum.  It's best to start a section that goes into advanced topics with a short review of what's been covered, and not distract the reader by asking them to \"go here to learn this\".  It is worthwhile to include a link to the detailed 101 doc if users want to follow it, but it's good to make each one self-contained rather than having folks keep clicking to get what they want.. ",
    "maedoc": "I used Docker for Desktop's Kubernetes cluster, with the Linux pachctl binary on WSL successfully, along lines of\nbat\nREM generate manifest and apply\nwsl pachctl deploy local --no-guaranteed --dry-run > deploy-pach.json\nkubectl apply -f deploy-pach.json\ndel deploy-pach.json\nREM wait a bit, then contact pachd\nwsl pachctl create-repo images\nThis doesn't handle path mapping for local files, so maybe a Windows native binary would be nice at this point.. ",
    "brycemcanally": "I am going to be looking into this.. Yeah, this looks like it only gets the provenance from the previous two levels of the DAG. Since the notion of provenance is moving to branches in the new commit invariants design this should be getting removed/changed right?. Is anybody working on this? If not, I can do it.. Okay, cool. Yeah, I looked into this a little bit already and am thinking I should be making modifications to the checkPath function in the driver that checks for null characters.. Sounds good. Do you want it exported like the repo name validation?. Okay, I will do that as well.. No problem. This is a cool project that I would like to get involved with to learn more about distributed systems.. Yeah, I should have done that initially. Test cases have been added now.. Of course, that was why I left them out :smile:. Maybe I am overlooking something, but why would you not just treat the file name as if it were a glob and use the glob-file functionality then process the original command with the returned file infos? The performance overhead for a non-glob should be minimal, and this would be much more familiar to hadoop and unix/linux users.. As far as the first point is concerned, the globbing library you guys are using supports escape sequences for those special characters so that information could just be forwarded to the users.\nAs far as the second point is concerned, I guess I was just thinking about use cases with less than 10 million files. It looks like your globbing on the backend is just running the match on the keys in the fs map, which I could simulate using the globbing library you guys are using. When I did this on 100000 filenames with greater than 50 characters, it took less than 10ms for an exact name, which would mean that this should be less than a second for under 10 million files. I do agree that this is not a great solution, especially for large numbers of files, and it would be better to do some sort of check if the path is a glob first. There is a small javascript library (https://github.com/micromatch/is-glob) that does this very thing, and could probably be transliterated to meet this use case.. After looking at the regex they are using and testing the library myself, I am convinced it has some issues. A simple case that does not work is a single character match (path/to/fi?e.txt). I think this problem is a lot simpler than they/we are making it because we just need to check if the user is trying to use a glob pattern rather than check if it is valid (the globbing library will catch if the overall pattern is bad). I think this regex\n^(\\\\.|[^\\\\])*([*?]|\\[(\\\\.|[^\\\\\\]])+\\]|\\{(\\\\.|[^\\\\\\}])+\\})\nshould work for checking if the user is trying to glob while adhering to the globbing syntax you guys are using. This would always require escaping for *, ?, and \\, and only require escaping for [, ], {, and } when they enclose one or more characters. We could run this then just remove the extra escapes if they are not globbing. We could also be more rigorous and enforce escaping for all globbing characters which would make this very simple.\nThe globbing syntax pulled from the library:\n//    pattern:\n//        { term }\n//\n//    term:\n//        `*`         matches any sequence of non-separator characters\n//        `**`        matches any sequence of characters\n//        `?`         matches any single non-separator character\n//        `[` [ `!` ] { character-range } `]`\n//                    character class (must be non-empty)\n//        `{` pattern-list `}`\n//                    pattern alternatives\n//        c           matches character c (c != `*`, `**`, `?`, `\\`, `[`, `{`, `}`)\n//        `\\` c       matches character c\n//\n//    character-range:\n//        c           matches character c (c != `\\\\`, `-`, `]`)\n//        `\\` c       matches character c\n//        lo `-` hi   matches character c for lo <= c <= hi\n//\n//    pattern-list:\n//        pattern { `,` pattern }\n//                    comma-separated (without spaces) patterns\nAnd some test cases:\n```\nGlobs:\npath/to/file\npath/*/file\npath/to/f?le\npa[th]/to/file\npa{th}/to/file\n[pa[[a]/to/file\npat\\h/to/f{ile\npa\\[th]/to/file\npath[\\a]/to\npath[]ab]\n[]]\n[\\]\nNot Globs:\npath/to/f*le\npath/t\\?/file\npath/t[/fil]\npa{h/to/fi\\?e\npath/t[]o/file\n[]\n```\nI typically use https://regexr.com/ for quickly writing, testing, and analyzing regexes, so it might be useful if you want to quickly check these out.. I think I might know what the issue is here. I played around with profiling pachd for this particular use case and I noticed that an inordinate amount of memory was being used through the grpc utility package, specifically for the buffer pool. After digging in the driver for a bit, it looks like there is no concurrency limit when splitting the file, such that there is a goroutine running for each line (in the case of a line delimiter) which could result with a huge number of goroutines. Each of these goroutines will allocate 2MB of memory for a buffer if there are none available in the buffer pool. I imagine kicking off a bunch of goroutines and allocating buffers takes less time than completing a grpc call, which would mean that the memory allocated for the buffer pool would get very large. I was able to fix this by using the limiter package for controlling the number of goroutines that are running at a time, which eliminated the OOM deaths, but it looks like there is another issue downstream.\nI ran into an issue with the max message size for etcd grpc calls when writing the put file records, similar to #2599. Since there could be a ton of files when using this feature, the upsert can result with a lot of data being sent to etcd in a single message since each file will be getting its own put file record and will all be stored as a single key value pair. Etcd is not really intended for storing large values so I don't think there would be a way to write a value in chunks and it probably would not scale well. This might require some design changes for this particular use case, if my understanding of the way this is working is correct. Maybe making a separate key value pair for each file, rather than grouping them then decoupling them when you generate the tree.\n. Is input commit filtering implemented in the back end? It looks like only pipeline and output commit filtering are currently implemented because the InputCommit field is not being used (unless I am overlooking something). Also, since the input commits is now a flag, wouldn't it be better to change the use to \"list-job [-i commits]\" or get rid of the commits portion entirely?. This issue comes up if you use the --no-rbac flag when deploying to a version of GKE later than 1.8. Using this flag removes the rolebindings for the pachyderm service account from the manifest. The docs can be a little misleading in this regard, so we are going to update them to reflect this information.. This looks like you do not have the permissions to create roles in your cluster. You can make yourself cluster admin with this command:\nkubectl create clusterrolebinding cluster-admin-binding \\\n--clusterrole cluster-admin --user $(gcloud config get-value account)\nYou are probably going to want to stick to not using the --no-rbac flag.. Did you switch to using a 1.7 version of GKE? If you did, you would need the --no-rbac flag because role based access control is not the default for that version. The error message you were getting in your second deployment attempt was because the deployment was trying to create the pachyderm service account and grant it privileges that you did not have.. Okay. If you have already tried a clean deployment, you might want to jump into our slack users channel and layout your situation. Making yourself cluster admin should have gotten you past the issue you were having in the original post.. I decided not to because most users will be using >1.8 moving forward and I feel like it would be extra clutter to include in the deployment examples. I added the necessary information to the flag description so that people will see when they need to use that flag. I am fine with adding it to the examples though, I just think that adding edge cases like this would result with excessively verbose examples.. That makes sense. I am going to go ahead and close this pull request.. Hey Joey,\nThat command is only required if the user does not already have the permissions that are needed for the pachyderm service account (kubernetes is preventing privilege escalation). Also, that command relies on the user having the permissions needed to create a cluster role binding. So, the way this should be handled really depends on the cluster. For a user who is just spinning up their own cluster with a default configuration, this command should work fine. For a user who is deploying into a cluster their company owns, they will probably need to speak with their admins to decide how they will get the permissions they will need. It would probably be best to explain this in the RBAC doc we have. It would not be sufficient to just say run that command.. Here are the permissions that the pachyderm service account will need.\nRules: []rbacv1.PolicyRule{{\n    APIGroups: []string{\"\"},\n    Verbs:     []string{\"get\", \"list\", \"watch\"},\n    Resources: []string{\"nodes\", \"pods\", \"pods/log\", \"endpoints\"},\n}, {\n    APIGroups: []string{\"\"},\n    Verbs:     []string{\"get\", \"list\", \"watch\", \"create\", \"update\", \"delete\"},\n    Resources: []string{\"replicationcontrollers\", \"services\"},\n}, {\n    APIGroups:     []string{\"\"},\n    Verbs:         []string{\"get\", \"list\", \"watch\", \"create\", \"update\", \"delete\"},\n    Resources:     []string{\"secrets\"},\n    ResourceNames: []string{client.StorageSecretName},\n}},. @JoeyZwicker I don't think there is a straightforward way to handle this through our deploy process. The command mentioned just binds the user account to an admin role which I don't think is going to fly in most organizations that have a multi-tenant kubernetes cluster. The way this should probably look in the docs is that we should mention the permissions that the user account needs in order to deploy Pachyderm, but also include that command as a shortcut for the case where they are in a cluster where they can make themselves an admin.. Thanks for the detailed report. This is definitely a bug. We should be creating the secret when an IAM role is used without credentials. This will be fixed before the next release.. The sha512 hash is actually 64 bytes (512 bits). I used that number because it is the absolute minimum size we should use for in-lining, but it definitely makes sense to in-line larger files and we can discuss what number makes sense when we get to it.. @jdoliner So I think we may need to do a combination of 1 and 3 because we may be in a situation where we need more connections than we can support across all the machines in the cluster (e.g. millions of datums in a job). So, what I mean by the hierarchical merge is that a worker might merge in groups such as 1:[1-10000], 2:[10001-20000], 3:[20001-30000], ... then merge 1, 2, 3, ... and the master would read from this stream. The results from the merges for groups 1, 2, and 3 would need to be stored somewhere.. I'm fine with it, but this seems more like a question for Joe. I don't use ripgrep, but if somebody else does, I'm not sure if they would always want to ignore vendored files.. Update\nThis is an update on where we are in terms of the shuffle step implementation. The model that we decided to go with is a pull-based model rather than a push-based model, which was what we described in the original design. The pull-based model has much cleaner failure semantics and is a simpler implementation that meets our performance requirements. The set of operations that happen now look like this:\n\nWhile a worker is processing a chunk, it is writing the completed datum hashtrees to both object storage and a local datum cache. \nWhen a worker finishes processing a chunk, it merges the datum hashtrees in the local datum cache into a hashtree that it stores in the local chunk cache (which stores merged chunks). Then, it marks the chunk as complete with its address in the chunk state.\nAs chunks are completing, workers responsible for shards are requesting the chunk hashtrees from the workers that processed them, and thus have them cached locally. The requesting workers also send a shard identifier that the workers will use to filter the results to just the data relevant to the requester. If a request for a particular chunk hashtree fails, then these workers will fall back on the old model to pull and filter datum hashtrees from object storage. This fallback happens at a per-chunk granularity, so one worker going down does not cause us to fallback to object storage for the later chunks.\nAfter all processing is complete, the workers responsible for shards will merge the chunk hashtrees that are stored locally.\n\nNotes:\n- Something new added that was not really talked about in the original post is new shard claiming logic that persists across jobs. When a worker starts, it will attempt to claim one of the shards and keep it for as long as possible. The goal with this is to support caching across jobs (to avoid re-downloading data for subsequent jobs) in the future. If a worker goes down or fails to renew its claim on a shard, all of the workers without shards will attempt to acquire the newly available shard (which may include the worker who originally claimed the shard)\n- In the full re-merge case (re-processed datums) each worker will download the skipped datum hashtrees in the chunks they process and merge them into the chunk tree. This a simple to reason about model, particularly in terms of the actual implementation, since the workers doing a merge will only need to have logic for determining if it is using a parent or not in the merge.. Discussion on whether we really want the shuffle aspect of the work we have done so far rather than better object storage batching\nWe discovered a serious bug in the process of implementing the shuffle step that may have been the primary contributor to the unpredictable object storage behavior. The bug essentially was a busy loop in each worker that failed to claim a chunk/merge when they were waiting on the last set of chunks/merges to complete. This resulted with a ton of CPU usage and network activity (100% CPU usage and ~2.5 million packets per node in a 10 node cluster (20 workers / 5 mergers) with machine type of r4.xlarge in AWS on a simple 100,000 datum workload). We implemented a simple fix for this issue and ran some tests to confirm this excessive activity was caused only by the busy loop and that seems to be the case (CPU usage and network activity dropped to essentially zero for workers not doing a merge). After discovering this, we decided to think a bit more about whether we really want shuffling logic or if just more effective batching to object storage would meet our needs. We are having some trouble reproducing the issues some users have been seeing with the merge hanging, but it is worth considering whether continuing down the route of managing a shuffle makes sense from a complexity vs value standpoint. \nPros of shuffling:\n- A constant factor improvement in merge time because we remove the object storage hop to get the data to the correct merger (this will come into play more when we are merging the actual data).\n- Could be noticeably less expensive in the case where there are many chunks\nPros of just batching on top of object storage:\n- Less complexity\nI am leaning towards keeping the shuffling right now because the constant factor improvement could be very noticeable and the complexity does not seem excessive. In terms of complexity, the main concern is future complexity that will come with moving the actual output data between workers to be merged. I think for the most part the issues we would run into there (too much data for disk) will have to be addressed even if we are using object storage for the intermediary because we need to get all the data through disk and merged. There probably will be some check pointing during the merge in those particular cases.\n. @jdoliner Nick and I already took care of adding the HashtreeSpec to the pipeline spec doc in this PR #3351. This should be fixed now. You will need to delete and re-download the package.. Dump of internal discussions:\nWe decided to explore a storage model that will impact this change. This new model may provide the features and performance we are looking for with the new hash trees. This model is based on the idea of having two indexes into our sorted data, an index for objects and an index for file offsets. The index for objects is essentially multiple objects in object storage that the files are spread across in lexicographical order. This model also depends on the content based hashing mentioned in #3310 , which will determine the boundaries across which separate objects will exist. Having multiple indexes and content based hashing allows us to merge and batch content much more effectively than we currently do. \nThe basic idea is that the merge of content is actually just a merge of the new data into the objects that are impacted by the new content. Because there is a global ordering of content (order with respect to lexicographical ordering of files), we can index into the necessary objects based on the output files we are encountering during the merge. Here is a simple example of what this can look like:\n```\nCommit 0\nObjects: 5c, a7, d3\nFiles: {a: 5c(0-5)}, {b: 5c(6), a7(0-2)}, {d: a7(3-8)}, {e: d3(0-4)}\nCommit 1 (Middle insert of file c)\nObjects: 5c, b4, 73, d3\nFiles: {a: 5c(0-5)}, {b: 5c(6), b4(0-2)}, {c: b4(3-5), {d: 73(0-5)}, {e: d3(0-4)}\nCommit 2 (Removal of one byte from the middle of b)\nObjects: 5c, 8f, 73, d3\nFiles: {a: 5c(0-5)}, {b: 5c(6), 8f(0-1)}, {c: 8f(2-4)}, {d: 73(0-5)}, {e: d3(0-4)}\n```\nThis model provides several benefits: we get a consistent hash of a file regardless of the operations that led to its creation (hash is a combination of the hashes of the objects, plus the edges that don't take up a full object (beginning and end)), our data is stored in a format that lends itself well to batching, and we get an easy way to manage deduplication (at the level of an object that should be of some approximate size, tbd).\nSomething that is still on the table is whether we want to lazily merge the content. There are some mapreduce workloads where eagerly merging can mean re-storing a lot of data.\n. Another internal discussion dump:\nWe are continuing to explore the model discussed in the previous post. I started to prototype some of the new hash trees and encountered a pretty clear point of friction. Our metadata management (hashtree package) is not able to access or manipulate the actual data it is managing. This has one major problem, any place in Pachyderm where we read/write/modify data, we have to shuttle data through two abstractions (hashtree package and object storage api server). This has been okay up to this point, but is going to become more painful as we move towards the model previously discussed because of how much back and forth there will be between the metadata and data management layers. What seems to be a necessary change to effectively support the new storage model is to move both metadata and data management behind one abstraction that can manage hashtrees and the content the hashtrees reference (this layer would also be where we manage the caching of trees/data).. Approaches to implementing these changes:\n\nAttempt to make small steps that can be integrated with the main codebase behind feature flags.\nPros:\nOther developers can see the direction we are going at every step, which means they can proactively incorporate their changes\nCons:\nIntegration at every step, which could significantly slow down how fast we can iterate.\nBuild the system independently of the main codebase, and attempt to plug it in behind a feature flag when we are confident it is has a stable API.\nPros and cons are essentially the inverse of the first approach.\n\nI am leaning a bit towards the second approach because I think we can still accomplish the main benefit of approach 1 by effectively communicating to engineers not involved with this project that changes they make to the way we read/write/modify files will also need to be implemented within the new storage layer and will need to pass our test suite which would be independent of the main test suite.. Yeah, I noticed that issue when CI failed. I vendored the path that was given for getting the command line tool not the library. I figured it might be useful to have a general purpose archiving utility available vs hand coding using what go has available. I mean, it is only two lines of code to create the file vs how many are in that program.. This looks like it should be changed to a comment for the Once object rather than what I'm guessing was previously a mutex.. That number refers to the number of datums per chunk, so this spec would be 10 datums per chunk.. Sounds good. Will change.. It looks like there are a few places in the pfs proto where a string is being used instead of the pfs.Branch type. Is there a reason for this? I am commenting this here because not having a protobuff type for the branch threw me off here.. I believe this should be seen not sent.. This should probably be re-worded.. Sounds good. I will change that.. I could do that. That seems better.. It might make sense to eventually move the blockState type here to JobState similar to the way the commit interfacing functions use CommitState. . Is there a reason why you are not using NoErrorWithinTRetry here?. It would probably be more readable if the code following this switch was in the put event case because it would be clear upon first look that it corresponds to a put event. Also, you could get rid of the delete event case or leave it empty if you did this.. Is this supposed to be checking if the pipeline is in standby?. That is a good idea. I will add that.. Yeah, that is true. I tend to stick to the comma ok idiom for checking key existence, but since this is being used as a set it might be better to check that way.. Sure.. Okay, guess I won't be using my new OneOfMatches function lol.. Shouldn't we be returning the error if there is an error here? I noticed you have a todo below to find out why the statement below is running when there is an error.. Might want to change the comment since it is in monitor pipeline now.. Yes.. I was thinking about this as well. I was going to do this in a refactor commit once I have get file committed.. As a side note, it might be useful to have a commit range abstraction that holds the from, to, and number of commits.. Leftover debugging prints.. This is just a note about changing this idiom to returning a break/done boolean, rather than returning a break within an error.. Should this be named TestIteration instead?. This probably needs to be reworded.. It might be better to check against an error code rather than the message because I have seen this message be different between grpc versions. It looks like the error code for this is ResourceExhausted.. Nope. Will change it.. I agree, this is a mistake.. I will separate them.. Glob characters are not currently allowed in paths, so this should not come up. We decided to disallow glob characters in paths to prevent mistakes that can happen when a glob character is not properly escaped.. Okay, I am convinced. Although it is still kind of weird, the benefit of not having to wrap the sends and being more concise is probably worth it. . This is testing the case where a user does a put file with a split then deletes files in the split. We discussed that this should not be allowed and instead the user should do a split using the atomic put file you are working on. This test breaks if we apply changes in mod revision order because of the way split file records are stored in etcd. The two deletes would get applied before the put file split in this case.. This makes a lot of sense. I will go ahead and start migrating this over to a callback interface and make a mental note for when I add/change things in the future.. Sounds good. Will make these changes.. It would probably make sense to store this in a variable so that upgrading etcd versions will be a little easier.. This comment needs to be fixed.. Are the sorted children still useful to us in the directory node since our fs is now in sorted order?. Just a note that I will need some changes here to store the BlockRefs.. Also, it might make sense to change the function name a bit because it seems like it means to get the tag of a hash tree.. Does it make sense to repeat these comments for each implementation of PutFileClient?. So I think it might make sense to have this be deferred before we send so that we attempt to close the connection if an error occurs during the send, unless it would make more sense to have the caller retry.. Yeah, that is right. Makes sense.. Might want to defer this as well.. Is there not a way to do this without having to keep info in memory for every file we have seen? As far as the upsert is concerned, it seems like it might be as simple as calling the upsert after each putfile, but the one off stuff might be a bit more tricky.. Ahh, good point. I will fix that.. It might be useful to have some information about the string format or an example (30s is 30 seconds) here. A user might just pass a number for this flag assuming it will be seconds. Also, why use the string \"default\" rather than an empty string?. Not yet, the hashtree package tests are not updated yet.. Go big or go home. Jk, so that actually does make sense in the context of how it is being used. Maybe the field name could use some work though. This field is used for the memory limiter in the driver to prevent OOM kills when users are doing big put file splits. The value is pulled from the container memory request, and if that is not available, then we pretty much set no limit (thus the 1T).  . So I think we discussed this before, but I think there is actually a pretty good argument for having asynchronous writers in Go particularly because goroutines are so cheap, it is much cleaner and not really that error prone considering it is a reasonable expectation for a user of the writer to close the writer when they are done. I believe when we agreed on this pattern, it was mainly because this is how the gcs client implements its writers. There is a goroutine that is reading from a pipe that handles the writes. I think this code is bound to get changed/scrapped anyways when we pull out the sidecar, so I am not too inclined to spend too much time on it if there are not any bugs since it will probably be changed again. . Input repo commits will have tree set and output repo commits with have trees set. Whether tree gets deprecated depends on how we change input repos moving forward. I guess I can add a comment there that clarifies that.. We need to lower this because list datum still needs performance work. It makes a bunch of random file requests, which works fine when the full hashtree fits in memory, but not so much when you have to go to object storage. This was the reason I brought up batching file requests based on a range so that we could grab a subtree using the indexing and have all the metadata we need at a point in time.. This will be removed when I commit the removal of incremental.. So, I do not think that comment really applies anymore. It looks like the history of that is that there used to be a hardcoded limit on the etcd request size (10,000 kvs), but it is now something that gets, I think, dynamically resized with a potential upper bound of 262,144 kvs https://github.com/pachyderm/pachyderm/blob/f07bb15aed7cd137d0f5920b3c551f24ec66a45f/src/server/pkg/collection/collection.go#L24. So, I think the 1T solution is the cleanest because I do not need to special case everywhere I use the memory limiter. We could still special case on 0, but that means we are either setting the value read by the driver to a high value or each memory limiter call would need to check for this special case.. Sure. Sure, you mean making \"list datums\" faster right?. That is weird, and is not the behavior we want. We should return an error if the first request does not include the block. I will fix this.. Not sure I understand what you are saying here. The size field is for the requester to say how many bytes they want to have available to read.. Well, it could be in the driver, but the idea behind this is that it is a global limiter in the container. I have a feeling we are going to want to change this over time as well because preventing OOM kills while also using the available memory is very tricky and this is not the most elegant solution.. Fair, I can add a once to this even though we generally only have one driver.. Inspect datum is fine, and is much quicker on first access because it does not download the full hashtree. The access pattern where a datum fails then is inspected is much quicker in 1.8 for any reasonably sized filesystem.. K, I will change that.. Yeah, after thinking about it, I think I agree. And yeah, we should only have 1 driver at a time in a container.. Yeah, doing it.. That would be way too much right now. . I agree, I struggled trying to make this more readable, but did not have too much luck. There is probably a cleaner way to do this, but it will probably require more time to think it through. At a high level I think it is pretty straightforward in that I iterate until I find the lower bound, then iterate until I find the upper bound. The parts that are confusing are the multiple edge cases. I think the comments I have explain the edge cases, but I guess I can see if I can add a little bit more.. k. https://github.com/pachyderm/pachyderm/blob/f07bb15aed7cd137d0f5920b3c551f24ec66a45f/src/server/pkg/hashtree/db.go#L1195\nSo, I do not think append would work here because getting a node from the queue does not change the queue's slice size. The queue size change is reflected in the size field. I could make a change to the fill function to do that, but I think this is the more performant solution because I know how big the slice can get from the beginning.. It did.. Yeah, that makes sense. This piece of code was changed a few times. It used to do more than create a function. The name setupStats was used to indicate that it was setting up the callback for writing stats. I will change this to a single function that will be deferred.. I cannot for the life of me remember why I put this here lol. I vaguely remember discussing with you something that suggested to me that I should remove this a while ago, but looking at it recently I cannot remember why that made sense. Should I just remove the comment?. Okay, so we discussed this offline. The change for this is going to be to introduce a close function to the hashtree reader and pbutil reader, then use the pattern of remove then close to have the desirable semantics of getting back a reader and closing it when done.. So doing this is a bit more weird than I expected. It does not really make sense to generate hashtree package errors outside of the hashtree package and there is probably a better way to accomplish this functionality entirely in the hashtree package. I think I am going to leave it as is for now.. For consistency, we should probably change this to chunksCol or plansCol to plans.. Yeah, they seem fine.. Clustomer lol?. I would change \"only workload that should be on that node\" to \"only pod that should be on that node\". Also it looks like \"set to the match the\" should be \"set to match the\".. why is increase capitalized?. Something to note is that you cannot change the hashtree spec after pipeline creation. There is a technical reason why we need to enforce this.. :+1: . Sure.. I think the original idea I had with this was that we could have a range filter and the error would be used for breaking out of the iteration, but I don't know if that will be too useful with the indexing we have now. I will go ahead and remove the error return for now, and we can re-visit later if necessary.. Sure, I have a habit of defaulting to the map[string]struct{} data structure because of the small memory savings, but the readability might be more useful.. Sounds good.. I need to store it in a byte buffer so I can cache it. If I just wrote directly into w, with the current model, I would need to re-serialize it into the cache. I decided it would be better to store a single serialized hashtree in memory rather than serialize every datum hashtree twice. There is probably a better way to do this such as writing to two locations in the serialize function, but I went with this because it is easy to reason about and the extra memory (one serialized datum hashtree) should not be much in most cases.. Same as above.. Sure.. This should probably be removed.. Existent not existant. Minor nit pick lol.. Same as above.. I don't think it really makes sense to call a file in pfs static vs non-static because there is no notion of a non-static file in pfs.. Also, the static file stuff mentioned below.. We usually don't leave commented code around. It adds too much clutter.. So, I decided to go with a simple channel that communicates that a shard was lost because I think communicating this through a context might not be a good design pattern considering the code flow.. That looks like exactly what I need. I think I will switch over to that.. Okay, so there are a few things going on here. The first thing I am going to address is the uninitialized lostShard channel, that is clearly a stupid mistake and should be initialized to a buffered channel of size 1. Without it being initialized, the deadlock situation would be that a worker would never lose a shard due to taking too long to renew (which is only a deadlock in the sense that claim shard is trying to write to the channel, this would not impact jobs completing). I do not think there is a deadlock situation when it is initialized though because after failing to renew, we return from the claimShard function. The channel will only get written to once. But, I think it is probably going to make sense to have the worker capable of claiming a shard in the future even if it failed to renew a shard at some point, so there probably are still some changes needed.\nAs far as context vs channel, I still think I am leaning towards a channel simply because there does not seem to be a clear way to incorporate a context for a shard into the context hierarchy we have for the job. Since we want to have workers retain shards across jobs (to avoid re-downloading hashtrees and potentially data) we would need a context that is out-of-band with the context that we use for jobs because cancellation of the job context should not correspond to a lost shard. It seems like the only way to get this behavior would be to derive it from the job context or some other child context (which really doesn't make sense because that claim has nothing to do with a particular job) or create a separate context. If we go the separate context route, then what purpose does it serve other than to use the done channel and we would not even be able to use it in the APIs we use because they take one context and we are going to preference the job context over some shard context in that case. The merge datums operation is idempotent, so if in some cases, which I believe should be rare, we do some duplicate merging I think we should be fine. Unless there is some way to use contexts that I am unaware of that will address the issues I mentioned, then I think a simple channel is the most straightforward solution.\n. So, when I first started implementing this, I used CopyBuffer (as well as some other optimizations to have maximum concurrency and to do as much in memory as possible), but I eventually backed off because I feel like we could be adding too much complexity with pre-mature optimizations without any data to back it up. The CopyBuffer change does not seem like much complexity, but the problem with it (and something we have encountered before) is that the buffer pooling solution we have (in grpcutil) does not defend against bursty requests. So, for example, if we were to make 1024 concurrent requests that did disk IO, we would be using 2G of memory in buffers alone. There is of course a way around this which involves some adjustments to our buffer pool or maybe making the buffers smaller, but I sort of decided that a simple solution that is guaranteed to work in every situation (coarse grained locking and no chance of using too much memory) would be a good starting point for testing things out. Since we have coarse grained locking right now, the bursty requests are not an immediate problem, but I actually think I am going to do some testing today to independently change those two variables to see which has a bigger impact, if any. Right now, it is probably not a bottleneck because we are just dealing with metadata, but it will probably matter when we starting moving the actual data around.. I looked at that data structure when I was creating this and I am not sure it is what we would need here. Taking into account what I mentioned in the post above on pre-mature optimizations, I think it is unclear if we will fit into the two common use cases they list for that data structure. The case where it only grows is probably not applicable for eviction purposes (particularly when we start putting real data in here). The case where the accesses from multiple go routines is disjoint seems very likely untrue particularly when we would be using this for caching data in crosses. I think a lot of the optimizations we could make here need to be informed by more information as we get to where we need them. After some more testing, I should have a better idea what we really need, and we can continue to iterate on this as we gather more info.. Right, so I decided to do this to be explicit about which etcd options are supported by our watch package. Basically, these are wrappers for the options that use the type system to reasonably prevent  (you can still bypass it through a cast, but you would not accidentally do that) clients of the watch package from using options that don't play nicely with the functionality we add on top of etcd watchers.. I think this check was to see if it is at the end of the cursor, and to not seek if it is. I don't think it is necessary to check if d or d.c is nil if we use the NewChildCursor function.. We might want this in a retry because it could potentially be flaky, although one second should be enough in general.. We have a backoff package btw that has some retry utilities.. I am guessing there was more to comment here?. Looks like it is called collectMetadata now.. So, I don't think this is actually going to work when we have multiple of the same paths in the queue (which does not happen now because this operates on merged tree shards). During the traversal, we are going to break at the compare(i, next) when the next entry has the same path as well as when the next path is greater. I also think that this would not handle the case when both the left and right entry have the same path as the root. I think the \"fill\" operation is fundamentally different from what you want here which is essentially a level order traversal.. not. Yeah, that seems like a good idea. This is actually something I pulled straight from Matt's work, so I do not know the reason why a goto was used.. There is only one GetFiles call here, so we don't need to run this in a goroutine. We can get rid of the error group and limiter.. Is there ever a valid scenario where fileInfo is non-nil and fileInfo.File is nil? If not, then I think we should maybe think about refactoring some of this because we could let an error slip by unnoticed. I think it might make more sense to have this just check for fileInfo being nil and then handle the potential fileInfo.File being nil within the block. Also, I think it might be cleaner to have the make file callback be a no-op, and then take the copy out of the else block.. That seems fine.. I think the call chain \"attachHeaderFooterToMetadata -> collectReferences\" is a little misleading in that you have to go through attachHeaderFooterToMetadata even if there is no header or footer. It might be clearer to have a check for a header/footer that will be used to conditionally call a function that attaches the header/footer, then call collect references after the if block.. Shouldn't there be a check for the recursive flag here? I think you could be in a situation where if it was not set, you could parse the files incorrectly.. Same as above with the recursive check.. I don't think we would want to return this error type here. ErrBreak type is intended for breaking out of callback based iteration and should not be used for indicating an actual error. I would probably just use a fmt.Errorf error here.. This could be a little cleaner by not putting the first node (1) in ns during initialization and then remove the check for cur != 1 before adding a node.. ",
    "Czxck001": "@jdoliner I found this issue helps when I tried to walk through the official tutorials.\nIt seems currently released Pachyderm has not yet perfectly been compatible with k8s 1.8. Unfortunately in official tutorial I failed to see anything about this compatibility issue was mentioned. I followed the tutorial guide and get kubectl 1.8 and minikube v0.23.0 installed, and later failed.\nI suppose it would be better if the official tutorial & documentation could be updated with this issue mentioned. i.e. suggest using k8s 1.7.10 and minikube v0.22.3 (which works on my Macbook). @sjezewski Thanks!. ",
    "jkinkead": "@sjezewski someone with root access to the k8s cluster would be able to snoop the contents of memory, control the pods, etc; but a user running pods shouldn't be able to see anything, even if they could intercept the packets.\nIf the certs are stored in k8s secrets, then a user with access to the secrets could snoop the traffic - but in a cluster with RBAC enabled, this would not be allowed.. . . . or just limit the launching of privileged pods.\nThe problem here is that this effectively grants the pipeline root to the entire cluster.\nIf the init container instead were responsible for looking up the image info, and passing it on to the worker container, only it would need the docker port, and it would close the security hole nicely.. If you point me at linting instructions (or maybe add them to the contributing guide), I'd be happy to help with these formatting issues... :). I'll work on cleaning this up today! Is there a unit testing guide for contributors? I was lazy about tests, and didn't see a reference to what you typically run.. Yeah, I hit a couple of errors trying to get it working locally.\nThe linter now succeeds, but the Travis tests are failing due to something new - connecting to the kubectl port forwarder, possibly?. These aren't blocking us at the moment.\nMy goal here is to make sure that we have a documented, simple way to regenerate our k8s configs - and rolling it up into the base tool is the best (IMHO) way to do this.. Yes, this is an unrelated bug; the prometheus port isn't serializing correctly to YAML. Removing the prometheus annotations from the pachd Service should fix this.\nI'll open an issue and see if there's a simple fix.. I've added a commit to this branch to pass the annotation name through to other places it's needed.. What's the best way to run the non-enterprise tests? Or, similarly, what set of tests should complete successfully?. Thanks, that should help!. It looks like this can be fixed by upgrading the version of ghodss in vendor.\nPatch in a few.. Agreed this is a duplicate of #3073.. Did you figure out a set of conditions under which symlinks would fail?\nWould it make sense to make this a pipeline option (noprivileged flag or similar)?. No problem - Done!. I retained the Name suffix on all variable names, since some of them pre-dated this PR. I'm happy to swap these also, if you like, it just seemed a little noisy.. ",
    "SpenGietz": "I wanted to confirm that this implementation would include the web servers located on ports 651, 652, and 999 as well.. ",
    "philtweir": "Apologies if this is missing the failing functionality, but just as a datapoint, I'm using pachyderm 1.6.3 on Kubernetes 1.7.4 w RBAC (kubectl 1.8.2) and found that the approach mentioned by @msteffen with pachctl port-forward -k '--namespace NAMESPACE' seemed to let local commands work as normal. As an aside, I could be wrong, but I don't think a StorageClass has a namespace?. ",
    "poopoothegorilla": "nvm ... looks like it is handled here https://github.com/pachyderm/pachyderm/pull/2478. np \ud83d\udc4d \n. ",
    "cglewis": "@JoeyZwicker done!. ",
    "drewda": "@JoeyZwicker sure, done.. ",
    "scheidec": "I was running into this issue yesterday when trying the manual deploy with the IAM role, but I tried again this morning using the one-shot script (static credentials) without using an IAM role and it appears there are no pachd issues that way.. ",
    "atombender": "@dwhitena Your issue says this is about documentation, but does Pachyderm even support custom configuration so that you can add the necessary affinities to new pods?. Also, the official Google recommendation (see Securing containers with Kubernetes Engine 1.8) is to reduce the default permissions given to individual nodes:\n\nThe principle of least privilege helps to reduce the \"blast radius\" of a potential compromise, by granting each component only the minimum permissions required to perform its function. Should one component become compromised, least privilege makes it much more difficult to chain attacks together and escalate permissions.\nEach Kubernetes Engine node has a Service Account associated with it. You\u2019ll see the Service Account user listed in the IAM section of the Cloud Console as \u201cCompute Engine default service account.\u201d This account has broad access by default, making it useful to wide variety of applications, but has more permissions than you need to run your Kubernetes Engine cluster.\nWe recommend you create and use a minimally privileged service account to run your Kubernetes Engine Cluster instead of the Compute Engine default service account.\n\n(Emphasis mine.). As far as I can see, at least with the current (0.17) version of the Google SDK, you should instantiate client.NewClient without options. The current call is:\ngo\n    client, err := storage.NewClient(\n        ctx,\n        cloud.WithTokenSource(google.ComputeTokenSource(\"\")),\n        cloud.WithScopes(storage.ScopeFullControl),\n    )\nBut reading the code, it looks like the absence of a token source will cause the client to call google.FindDefaultCredentials, which goes through the standard Google SDK ways of finding credentials:\n//   1. A JSON file whose path is specified by the\n//      GOOGLE_APPLICATION_CREDENTIALS environment variable.\n//   2. A JSON file in a location known to the gcloud command-line tool.\n//      On Windows, this is %APPDATA%/gcloud/application_default_credentials.json.\n//      On other systems, $HOME/.config/gcloud/application_default_credentials.json.\n//   3. On Google App Engine it uses the appengine.AccessToken function.\n//   4. On Google Compute Engine and Google App Engine Managed VMs, it fetches\n//      credentials from the metadata server.\n//      (In this final case any provided scopes are ignored.)\nAs for WithScopes, the default scope is actually ScopeFullControl, so it can be dropped.\nIn other words, Pachyderm can just use google.DefaultClient(ctx) instead of what it does now. On GCE, this would use the metadata API to get the node's credentials (today's behaviour), but also allow Pachyderm to be deployed with other credentials (either OAuth 2.0 token or a service account, as described here).. ",
    "shebiki": "I'm a little confused by the various access methods but it looks to me like access scopes are now deprecated[1] and instead Pachyderm should encourage users to create a GCE service account and accept a configuration[2] of the credentials.json for that service account that is uses on any pods that need write access to Google Cloud Storage.\n1: https://cloud.google.com/compute/docs/access/service-accounts#accesscopesiam\n2: https://godoc.org/cloud.google.com/go#example-package--ServiceAccountFile. This ticket is related to https://github.com/pachyderm/pachyderm/issues/2578. ",
    "nikhilalmeida": "Hey Joey, Any updates on this?. ",
    "jgerber-suplari": "Any updates?. ",
    "Nick-Harvey": "Just appending to this for when we tackle this. After working with a user in slack, when deploying Pachyderm on EKS, additional security group steps might need to be taken to \"allow all communication between security groups for the cluster\". I don't have specific steps from the user, but if you're able to resolve etcd from outside the cluster but curl -v ... shows connection timed out error, allowing communication between security groups will fix it. \nApologies for the lack of detail, it's all I was able to get from our slack conversation.. We should update the docs with the steps need to ensure the gcp service account has the appropriate permissions to be deployed properly. . Adding the docs label to clarify our docs a bit better as requested by @David-Development  . Instructions added to RBAC Doc\nhttps://docs.pachyderm.io/en/latest/deployment/rbac.html. cla-assistant has been configured and added to the repo. . Looks good to me . closing this PR and opening a separate one with the requested changes. . Item #1 is fixed. I'm going to come back to the rest. . Context from Joey: \nI think what I referring to here is that \u201cdatum-path\u201d isn\u2019t a meaningful term that a user understands. So we should either not use that term if \u201cdatumID\u201d is actually more descriptive or define datumpath as part of the docs or in the datum_info object or whatever (edited)\nbasically where this came from is I was trying to learn how to use restart datum and it said \u201cuse the datum path\u201d and I was like \u201cwtf the is a datum path, that\u2019s not describe anywhere\u201d\nthere is the PATH section in inspect-datum, but there\u2019s likely/possible a bug here or something because I couldn\u2019t seem to get that work\nthis is a semi indepth docs issues to tackle because you\u2019ll need to first learn how to use the *-datum cmds, then trying to understand how they\u2019re \u201csupposed\u201d to work, then update the docs/help functions to actually explain how they DO work and/or submit GH issues for bugs that are found\nDoes that make sense?. Pull request created: https://github.com/pachyderm/pachyderm/pull/3351. Merged. Updated https://github.com/pachyderm/pachyderm/blob/master/doc/pachctl/pachctl_glob-file.md with a note to help users avoid the issue. . Fixed. You need to use the markdown syntax for RTD to create the image correctly i.e ![my gif image](path/to/image.gif). I went ahead and created redirects for these pages, but the bigger issue is still at large I believe. I'll take a look and see what we need to do to properly fix it (i.e old doc versions being properly retired) . Added \"Docker for Desktop\" Instructions to our documentation here: https://pachyderm.readthedocs.io/en/latest/getting_started/local_installation.html That should help get things started and should be enough to close this issue. Future DFD issues - docs or otherwise - should get a separate issue :) . Given the work Matt did with https://github.com/pachyderm/pachyderm/issues/3390, I went ahead and just applied these manually to avoid merge conflicts. @discdiver gets all the credit for these changes, thanks for the changes \ud83e\udd47 . Completed . LGTM. I'm pretty sure I got all the -c mentions in that last commit (other than cli help output) \n$ grep -rnw './' -e '-c'\nBinary file .//enterprise/dashboard.png matches\nBinary file .//enterprise/enterprise.png matches\n.//pachctl/pachctl_put-file.md:64:  -c, --commit                    DEPRECATED: Put file(s) in a new commit.\n.//pachctl/pachctl_put-file.md:69:  -m, --message string            A description of this commit's contents (only allowed with -c)\n.//pachctl/pachctl_mount.md:17:  -c, --commits []string   Commits to mount for repos, arguments should be of the form \"repo:commit\" (default [])\nBinary file .//cookbook/cron1.png matches\nBinary file .//custom_theme/static/fonts/fontawesome-webfont.eot matches\nBinary file .//getting_started/montage-screenshot.png matches\nBinary file .//getting_started/dashboard1.png matches\nBinary file .//getting_started/dashboard3.png matches\n. fixed the -r flag. . Did a final review/revision and everything lgtm. Fixed. \nI also did a broader search for pachctl deploy amazon and pachctl deploy storage aws to ensure that region was part of the instructions, and this looked to be the only instance where it wasn't. . Do to the work done by matt in https://github.com/pachyderm/pachyderm/issues/3390, I went ahead and just edited the file directly to avoid git merge issues. . The map.json was missing a trailing slash in the transform section. Before it was /pfs/scraper and now it's /pfs/scraper/. Thanks for bringing this to our attention. . LGTM\n. All are fixed . LGTM. This should be included for the Pachyderm Enterprise roadmap. This was reported in the users channel this morning so I haven't reproduced it myself. As a side-note, I think this could also effect minio deployments if I'm not mistaken.. ",
    "jeffm14vt": "Not sure why the build's failing. @dwhitena Sorry about that, not sure why it was omitted. . ",
    "bbhoss": "My initial thought was to update the code of the pipeline(s) downstream from the pipeline you're changing the schema from to handle both schemas first, then to update the pipeline that you're changing the schema of. This would also facilitate not having to reprocess all datums, since your downstream code can still handle both formats. Also, if you wanted to clean this up you could always force a reprocess and then remove the code for handling the old schema.. ",
    "olegserov": "I've used it locally just to upload the data into the repo. The version was the latest at that date. I've just finished the installation. Unfortunately, I don't have that version anymore.. ",
    "anuragsoni9": "Hello @dwhitena  , \nI think I am able to correctly add all the doc changes  you mentioned into this pull request.\nPlease verify.\n. @dwhitena  :  Could you approve the PR now?. Apologies. Closed by mistake.. removed the extra lines.\n@dwhitena . ",
    "dionjwa": "The issue this PR fixed also affected minikube (local) installs, so I'm having difficulty evaluating pachyderm. Is there a temporary workaround? If not, what is the approx time frame for the next release?. It's failing with this message:\ngo test -v ./src/server/auth/server -timeout 3600s\n=== RUN   TestAdminRWO\n--- FAIL: TestAdminRWO (0.00s)\npanic: cannot get test enterprise token from s3: NoCredentialProviders: no valid providers in chain. Deprecated.\n    For verbose messaging see aws.Config.CredentialsChainVerboseErrors [recovered]\n    panic: cannot get test enterprise token from s3: NoCredentialProviders: no valid providers in chain. Deprecated.\n    For verbose messaging see aws.Config.CredentialsChainVerboseErrors. @jdoliner . I just signed the CLA now.. I could really use a terraform based pachyderm deploy config. . ",
    "frankhinek": "Great, thanks @JoeyZwicker.. ",
    "raja1212": "yes... it's working. Thanks a lot! . ",
    "nysthee": "\u2705\nOn Thu, 25 Jan 2018 at 00:20, Joey Zwicker notifications@github.com wrote:\n\nThanks @nysthee http://email.thomasnys.com/c/eJxFjrtuwkAQRb9m3QXNzr4LF2AgEUIpQaHbJ7ZkJ7A2JP77LKTIaHSkW9yZE2rrJCRVdXWzfCO4WosC4DnN140lbD2I48m-wnHMP_v3fUa_22ybj-bQ7V5Oh3zN9_BNOAy26xfnJ_3XULW1dl4IKmwUOjkulXIMtVMQEhXcg6v6up2my0jYkuC27Lmb2pt7tEv4nMepjbHKdY6Xfn4YgQOtDZiokxHG2xQjINdWUaEjd0YbpsCKhMhZTGjQJ_gbSqWGIF0yaKk0PtKioLgKxft5fvH_-xeH_FDf! @dwhitena\nhttp://email.thomasnys.com/c/eJxFTsluwjAU_BrnVvS824ccaFgqhHoEldvzRiwlBUxayt_X0ENHo5HmMEto0SlIusltN38j7HUhq4Ao6X5ZIuGLUe4PuIb9tfxs37eF-c1y1X10u7x5OezKpXyHGxEwYh5mx6f609j0LWoFmnGqpKTBaw8GNWKikQsRgk7N0PbTdL4SPidsVXnMU__lHulqwq3PU_zEprQlnof74xI4MMaCjSZZaT2mGIGJWkulicJZY7kGlIkxwWNilvkEf6BUGQjKJcuQKusjpVJooUM9_qyf_Y__AtDFUS0 will review this (it's such a simple change\nso that'll only take 10 seconds) and get it merged in.\nCan you sign our CLA\nhttp://email.thomasnys.com/c/eJxNT7lug0AQ_RroQHsfBYWD7USWldJW3A3LLCBxeYE4_vuA02T09IqZ0TvKDApFvI6bLN99ROxtL1ciIvjn_QAR33fyeoN3cp3Cz_nzHJg7HY75V35pTsntEu7hu3xEgnTQtGn1Yjd0cZ0xYQG0FEp7BdRy6a20HDnVyivNWNxm9TyPU8R3ETuuGMHVzxJDlz4WPwybzLr1Q-im_9fEDf0cmmKZh5C0jcN-wgSqgNhhP6-fccgCju1za0EKYowlFs3m7sAjEiYMaCoNisIayzUB6RkTHD2zzHnyN5QqQ0pVeMuAKuuQUim00OXa9SWfVs1cL8UW8xeFvGOb\n?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttp://email.thomasnys.com/c/eJxlT7tuwzAM_Bp7S0CReg4eUidpEQQdEzSbJEuxAbt1ZLut_75OOnQocTiSwx3uqsI6CVHlTVFuXjJ82oqFgKc433Y2o20nzhf7DOchfR9fjwn9Ybcv38pTc1hdTumWPquvjENnm3Z9fbD_6PK6cFqRC9p5iQIUj9KT1cZGZJGc8jFvi3oc-yGjTYb7BddmrCd3Vy9Pb309VyH9u6e2XRZKogypGYYpLIIuvI8rkkCglKI8FSn07XyvAQ60NmCCjkYYb2MIgFxbxYQO3BltSIEVEZFTiGjQR_gdxqSGSrpo0DJpfGBMcMVVtZR92K__Av8AKI9hyg,\nor mute the thread\nhttp://email.thomasnys.com/c/eJxFUEFuwjAQfE1yA9mOHduHHEKAthGgthIgckG2YxOXAMFx2tLX19BDV6PRzkg70mydCZkCQ2ObFflzhCZTEghgZ27XmYiS6YlsK_EEtr37XqwWDqlyNi92xcaWo2rjru6z_oowOAnbjg8PVpdT3GQC1UgixmrBDZQYU5oSBmGdhE0ibOI2a7zv-ijJIzQPOFjfDPJ-HcT54q2xSnh7OfdBD-d-kL1yVuqRGHwTrHxRvBxfW3gTQ1esc111Iz_Jy6Vs97uPn14de-JX9Lo_iLKrlvi9e3tfxy5zumtv94ZAAsY44JoZTrgSRmuAMBMUEqax5IwnFAhiEMKJNogjZcDfQJgyUKfScCRgypWGkGCKaR3-8Igf_3f5BaXTbOI\n.\n. Signed!. \n",
    "threefoldo": "Thanks. I have read almost all of the document, but still don't understand how Pachyderm works. Is it possible to test/verify programs and docker image before creating a pipeline? For example, run \"worker\" manually with a fake '/pfs' directory. \nBelow is the logs.\n$ kubectl get pods\nNAME                        READY     STATUS             RESTARTS   AGE\netcd-7dbb489f44-ljjb4       1/1       Running            0          21h\npachd-96fdc4ff5-sjswv       1/1       Running            0          21h\n...\npipeline-test-v4-v1-f5jh6   1/2       CrashLoopBackOff   25         1h\n$ kubectl logs po/pachd-96fdc4ff5-sjswv\nsegment 2018/01/27 05:08:08 error sending request: Post https://api.segment.io/v1/batch: dial tcp: lookup api.segment.io on 10.43.0.10:53: dial udp 10.43.0.10:53: i/o timeout\ntime=\"2018-01-27T05:08:29Z\" level=info msg=\"objectCache stats: {Gets:14 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:08:29Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:08:29Z\" level=info msg=\"objectInfoCache stats: {Gets:26 CacheHits:14 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:09:29Z\" level=info msg=\"objectCache stats: {Gets:14 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:09:29Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:09:29Z\" level=info msg=\"objectInfoCache stats: {Gets:26 CacheHits:14 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:10:29Z\" level=info msg=\"objectCache stats: {Gets:14 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:10:29Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:10:29Z\" level=info msg=\"objectInfoCache stats: {Gets:26 CacheHits:14 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:11:29Z\" level=info msg=\"objectCache stats: {Gets:14 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:11:29Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:11:29Z\" level=info msg=\"objectInfoCache stats: {Gets:26 CacheHits:14 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:12:29Z\" level=info msg=\"objectCache stats: {Gets:14 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:12:29Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:12:29Z\" level=info msg=\"objectInfoCache stats: {Gets:26 CacheHits:14 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\" \nsegment 2018/01/27 05:13:08 error sending request: Post https://api.segment.io/v1/batch: dial tcp: lookup api.segment.io on 10.43.0.10:53: dial udp 10.43.0.10:53: i/o timeout\ntime=\"2018-01-27T05:13:29Z\" level=info msg=\"objectCache stats: {Gets:14 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:13:29Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:13:29Z\" level=info msg=\"objectInfoCache stats: {Gets:26 CacheHits:14 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:14:29Z\" level=info msg=\"objectCache stats: {Gets:14 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:14:29Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:14:29Z\" level=info msg=\"objectInfoCache stats: {Gets:26 CacheHits:14 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:15:29Z\" level=info msg=\"objectCache stats: {Gets:14 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:15:29Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\" \ntime=\"2018-01-27T05:15:29Z\" level=info msg=\"objectInfoCache stats: {Gets:26 CacheHits:14 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\". \n```\n2018-01-27T03:19:32Z INFO authclient.API.WhoAmI {\"request\":{}}\n2018-01-27T03:19:32Z INFO authclient.API.GetCapability {\"request\":{}}\n2018-01-27T03:19:32Z INFO authclient.API.GetCapability {\"duration\":0.000990326,\"request\":{},\"response\":{\"capability\":\"f802f8254a3d4eedb6dabeba4f26b20d\"}}\ntime=\"2018-01-27T03:19:32Z\" level=info msg=\"master: creating/updating workers for pipeline test-v4\"\n2018-01-27T03:19:32Z INFO pfs.API.CreateRepo {\"request\":{\"repo\":{\"name\":\"test-v4\"},\"provenance\":[{\"name\":\"webqa\"}]}}\n2018-01-27T03:19:32Z ERROR pfs.API.CreateRepo {\"duration\":0.000788056,\"error\":\"cannot create \\\"test-v4\\\" as it already exists\",\"request\":{\"repo\":{\"name\":\"test-v4\"},\"provenance\":[{\"name\":\"webqa\"}]},\"response\":null}\n2018-01-27T03:19:32Z INFO pps.API.CreatePipeline {\"duration\":0.05814208,\"request\":{\"pipeline\":{\"name\":\"test-v4\"},\"transform\":{\"image\":\"threefoldo/cutsents:v4\",\"cmd\":[\"python3\",\"/code/cut.py\",\"/pfs/webqa\",\"/pfs/out\"]},\"input\":{\"atom\":{\"name\":\"webqa\",\"repo\":\"webqa\",\"branch\":\"master\",\"glob\":\"/\"}}},\"response\":{}}\n2018-01-27T03:19:32Z INFO enterprise.API.GetState {\"request\":{}}\n2018-01-27T03:19:32Z INFO enterprise.API.GetState {\"duration\":0.000003562,\"request\":{},\"response\":{}}\ntime=\"2018-01-27T03:20:29Z\" level=info msg=\"objectCache stats: {Gets:14 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\"\ntime=\"2018-01-27T03:20:29Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\"\ntime=\"2018-01-27T03:20:29Z\" level=info msg=\"objectInfoCache stats: {Gets:26 CacheHits:14 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\"\n2018-01-27T03:21:27Z INFO pps.API.ListPipeline {\"request\":{}}\n2018-01-27T03:21:27Z INFO pps.API.ListPipeline {\"duration\":0.051597526,\"request\":{},\"response\":{\"pipeline_info\":[{\"pipeline\":{\"name\":\"test-v4\"},\"version\":1,\"transform\":{\"image\":\"threefoldo/cutsents:v4\",\"cmd\":[\"python3\",\"/code/cut.py\",\"/pfs/webqa\",\"/pfs/out\"]},\"created_at\":{\"seconds\":1517023172,\"nanos\":700049714},\"output_branch\":\"master\",\"resource_requests\":{\"memory\":\"64M\"},\"input\":{\"atom\":{\"name\":\"webqa\",\"repo\":\"webqa\",\"branch\":\"master\",\"glob\":\"/\"}},\"cache_size\":\"64M\",\"salt\":\"4a0b9cb9944c4da58cc03317093023ee\",\"capability\":\"f802f8254a3d4eedb6dabeba4f26b20d\",\"max_queue_size\":1},{\"pipeline\":{\"name\":\"test-v3\"},\"version\":1,\"transform\":{\"image\":\"threefoldo/cutsents:v3\",\"cmd\":[\"python3\",\"/code/cut.py\",\"/pfs/webqa/\",\"/pfs/out\"]},\"parallelism_spec\":{\"constant\":5},\"created_at\":{\"seconds\":1516979544,\"nanos\":499441338},\"state\":1,\"job_counts\":{\"0\":0,\"1\":0,\"3\":3},\"output_branch\":\"master\",\"resource_requests\":{\"memory\":\"64M\"},\"input\":{\"atom\":{\"name\":\"webqa\",\"repo\":\"webqa\",\"branch\":\"master\",\"glob\":\"/\"}},\"cache_size\":\"64M\",\"salt\":\"45bea5ae19a1460bb7e0220a2373fdeb\",\"capability\":\"cbed301e75404fdb8b8c942faf2aad44\",\"max_queue_size\":1},{\"pipeline\":{\"name\":\"edges\"},\"version\":2,\"transform\":{\"image\":\"pachyderm/opencv\",\"cmd\":[\"python3\",\"/edges.py\"],\"image_pull_secrets\":[\"regsecret\"]},\"parallelism_spec\":{\"constant\":2},\"created_at\":{\"seconds\":1516958988,\"nanos\":343908371},\"state\":1,\"output_branch\":\"master\",\"resource_requests\":{\"memory\":\"64M\"},\"input\":{\"atom\":{\"name\":\"images\",\"repo\":\"images\",\"branch\":\"master\",\"glob\":\"/\"}},\"cache_size\":\"64M\",\"salt\":\"73e3ecbd664f45cb99571ceffdffb9e2\",\"capability\":\"0dffe4fad4814e65b46547e51ea8c6f8\",\"max_queue_size\":1}]}}\ntime=\"2018-01-27T03:21:29Z\" level=info msg=\"objectCache stats: {Gets:14 CacheHits:2 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\"\ntime=\"2018-01-27T03:21:29Z\" level=info msg=\"tagCache stats: {Gets:0 CacheHits:0 PeerLoads:0 PeerErrors:0 Loads:0 LoadsDeduped:0 LocalLoads:0 LocalLoadErrs:0 ServerRequests:0}\"\ntime=\"2018-01-27T03:21:29Z\" level=info msg=\"objectInfoCache stats: {Gets:26 CacheHits:14 PeerLoads:0 PeerErrors:0 Loads:12 LoadsDeduped:12 LocalLoads:12 LocalLoadErrs:0 ServerRequests:0}\"\n2018-01-27T03:21:37Z INFO pps.API.InspectPipeline {\"request\":{\"pipeline\":{\"name\":\"test-v4\"}}}\n2018-01-27T03:21:37Z INFO pps.API.InspectPipeline {\"duration\":0.048978775,\"request\":{\"pipeline\":{\"name\":\"test-v4\"}},\"response\":{\"pipeline\":{\"name\":\"test-v4\"},\"version\":1,\"transform\":{\"image\":\"threefoldo/cutsents:v4\",\"cmd\":[\"python3\",\"/code/cut.py\",\"/pfs/webqa\",\"/pfs/out\"]},\"created_at\":{\"seconds\":1517023172,\"nanos\":700049714},\"output_branch\":\"master\",\"resource_requests\":{\"memory\":\"64M\"},\"input\":{\"atom\":{\"name\":\"webqa\",\"repo\":\"webqa\",\"branch\":\"master\",\"glob\":\"/\"}},\"cache_size\":\"64M\",\"salt\":\"4a0b9cb9944c4da58cc03317093023ee\",\"capability\":\"f802f8254a3d4eedb6dabeba4f26b20d\",\"max_queue_size\":1}}\n2018-01-27T03:21:42Z INFO pps.API.ListPipeline {\"request\":{}}\n2018-01-27T03:21:42Z INFO pps.API.ListPipeline {\"duration\":0.000969861,\"request\":{},\"response\":{\"pipeline_info\":[{\"pipeline\":{\"name\":\"test-v4\"},\"version\":1,\"transform\":{\"image\":\"threefoldo/cutsents:v4\",\"cmd\":[\"python3\",\"/code/cut.py\",\"/pfs/webqa\",\"/pfs/out\"]},\"created_at\":{\"seconds\":1517023172,\"nanos\":700049714},\"output_branch\":\"master\",\"resource_requests\":{\"memory\":\"64M\"},\"input\":{\"atom\":{\"name\":\"webqa\",\"repo\":\"webqa\",\"branch\":\"master\",\"glob\":\"/\"}},\"cache_size\":\"64M\",\"salt\":\"4a0b9cb9944c4da58cc03317093023ee\",\"capability\":\"f802f8254a3d4eedb6dabeba4f26b20d\",\"max_queue_size\":1},{\"pipeline\":{\"name\":\"test-v3\"},\"version\":1,\"transform\":{\"image\":\"threefoldo/cutsents:v3\",\"cmd\":[\"python3\",\"/code/cut.py\",\"/pfs/webqa/\",\"/pfs/out\"]},\"parallelism_spec\":{\"constant\":5},\"created_at\":{\"seconds\":1516979544,\"nanos\":499441338},\"state\":1,\"job_counts\":{\"0\":0,\"1\":0,\"3\":3},\"output_branch\":\"master\",\"resource_requests\":{\"memory\":\"64M\"},\"input\":{\"atom\":{\"name\":\"webqa\",\"repo\":\"webqa\",\"branch\":\"master\",\"glob\":\"/\"}},\"cache_size\":\"64M\",\"salt\":\"45bea5ae19a1460bb7e0220a2373fdeb\",\"capability\":\"cbed301e75404fdb8b8c942faf2aad44\",\"max_queue_size\":1},{\"pipeline\":{\"name\":\"edges\"},\"version\":2,\"transform\":{\"image\":\"pachyderm/opencv\",\"cmd\":[\"python3\",\"/edges.py\"],\"image_pull_secrets\":[\"regsecret\"]},\"parallelism_spec\":{\"constant\":2},\"created_at\":{\"seconds\":1516958988,\"nanos\":343908371},\"state\":1,\"output_branch\":\"master\",\"resource_requests\":{\"memory\":\"64M\"},\"input\":{\"atom\":{\"name\":\"images\",\"repo\":\"images\",\"branch\":\"master\",\"glob\":\"/\"}},\"cache_size\":\"64M\",\"salt\":\"73e3ecbd664f45cb99571ceffdffb9e2\",\"capability\":\"0dffe4fad4814e65b46547e51ea8c6f8\",\"max_queue_size\":1}]}}\n```. The problem has gone after I rebuild the cluster. I'm not sure the exact reason, but kube-dns was not working properly because of invalid certificates, a lot of dns lookup failure occurred in the cluster. Sorry for the trouble.\n. ",
    "gkumar7": "Deployment of pachyderm using terraform k8s provider: https://github.com/gkumar7/pachyderm-terraform\nHope this helps!. ",
    "ryanberckmans": "Similarly where this json is missing the spec.pipeline\npachctl create-pipeline -f missing-pipeline-section.json\npachctl output is rpc error: code = Unavailable desc = transport is closing and pachd panics\n```\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x1736e40]\ngoroutine 282 [running]:\ngithub.com/pachyderm/pachyderm/src/server/pps/server.(apiServer).validatePipeline(0xc420212c60, 0x7fd03ef1d870, 0xc42086ae70, 0xc420876140, 0xbe94344259954a3e, 0x6a2701e55)\n        /go/src/github.com/pachyderm/pachyderm/src/server/pps/server/api_server.go:1169 +0x60\ngithub.com/pachyderm/pachyderm/src/server/pps/server.(apiServer).CreatePipeline(0xc420212c60, 0x7fd03ef1d870, 0xc42086ae70, 0xc42060c840, 0x0, 0x0, 0x0)\n        /go/src/github.com/pachyderm/pachyderm/src/server/pps/server/api_server.go:1391 +0x4f5\ngithub.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pps._API_CreatePipeline_Handler(0x1bd04e0, 0xc420212c60, 0x7fd03ef1d870, 0xc42086ae70, 0xc4202c1090, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /go/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pps/pps.pb.go:3143 +0x276\n```\nWhich in 1.6.7 is https://github.com/pachyderm/pachyderm/blob/master/src/server/pps/server/api_server.go#L1200. ",
    "antjkennedy": "Hi @jdoliner I've signed the CLA, let me know if there is anything else you need.. ",
    "lrepolho": "@thedrow did you come up with a solution for that error? Cheers. ",
    "ryansmith23": "I was able to get this working for the helm chart on a private GKE cluster with an internal load balancer using a pre-defined static IP with:\n```\nkind: Service\napiVersion: v1\nmetadata:\n  name: pachd\n  labels:\n    app: pachd\n    chart: \"{{ .Chart.Name }}-{{ .Chart.Version }}\"\n    release: \"{{ .Release.Name }}\"\n    heritage: \"{{ .Release.Service }}\"\n    suite: pachyderm\n  annotations:\n    cloud.google.com/app-protocols: '{\"api-grpc-port\":\"HTTP2\"}'\n    cloud.google.com/load-balancer-type: \"Internal\"\nspec:\n  type: LoadBalancer\n  loadBalancerIP: \n  ports:\n    - name: api-grpc-port\n      port: 650\n      targetPort: 650\n      nodePort: 30650\n    - name: trace-port\n      port: 651\n      targetPort: 651\n      nodePort: 30651\n    - name: api-http-port\n      port: 652\n      targetPort: 652\n      nodePort: 30652\n  selector:\n    app: pachd\n```. ",
    "sillystring13": "Apologies for the lateness - the form is signed . ",
    "seeb0h": "I share the need with a mixed cpu/gpu cluster. \nI need: \n\nto prevent cpu pipelines to be run on gpu nodes. \nto prevent monothreaded cpu pipelines to be run on cpu nodes with many cpus \nto choose my gpu type for my gpu pipelines\n\nBeing able to use nodeSelector would be perfectly fine.. ",
    "jiaqi216": "I did not try this in 1.7.0rc. I encountered this issue with\nCOMPONENT           VERSION\npachctl             1.6.9\npachd               1.6.9\nand\nCOMPONENT           VERSION\npachctl             1.6.9\npachd               1.6.7. @dwhitena do you mean 1.7 for the pachd version or pachctl version?. ",
    "liuchenxjtu": "hi Dan,\n   thanks your for your efforts. I am quite new to Pachyderm. now we want to deploy pachyderm on our own cluster. but most of the docs are about deploying on cloud provider, e.g., persistent-disk must be google, azure or aws. so for our case, could you advise what we should be for the local clusters? thank you!. Hi Dan,\n   thanks for your reply. yeah I am working on that. but how can I deploy pachyderm on my local kubernete cluster? now it seems it put the data in a local folder, how should I configure to put it in Minio? sorry could not find many documents . Hi @dwhitena ,\n   sorry for the late reply. \n\nIt's also not really meant for production workloads where you'd need a real object store.\nso this means if I want to make it production level, I need to set up an object store? can I use glusterFS?\nWhen you say \"local clusters\" do you mean on-prem or do you mean a minikube or \"pachctl deploy local\" cluster?\nour local cluster is a 1 master,3 node kubernetes cluster, we built it on our own servers but not in the cloud. I guess I need the on-prem setting. but according to the document there, the persistent-disk parameter I can only use aws,google or azure. but for my case, I did not use any of them, how should I set?  thanks for your help!. hi @dwhitena , @JoeyZwicker \n   thanks for your reply. sorry for my poor knowledge. could you clarify whether I can build pachyderm directly on glusterFS?\nin addition,   I tried to set up Pachyderm with Minio by following command helm install  --name my-release1 --set credentials=s3,s3.accessKey=AKIAIOSFODNN7EXAMPLE,s3.secretKey=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY,s3.bucketName=test,s3.endpoint=\"http://172.16.41.154:9000\",etcd.persistence.enabled=false,etcd.persistence.accessMode=ReadWriteMany pachyderm\n\nwhen I tried to put file in one repo, e.g., pachctl put-file test master -c -f lsof.log\n\"The specified bucket does not exist\". if I ran it again, it shows \"rpc error: code = Unknown desc = parent commit 92d19698780b456d9f4fdeb734350e1e has not been finished\". but I do have the bucket in minio. it is also set up by helm charts. there is log from pod. \n2018-05-08T09:13:36Z ERROR pfs.API.GetFile {\"duration\":0.004858689,\"error\":\"objectInfoGetter: object c79f09e533b8de46b88eea1267f97842ed6e4d93104d8d4879306d1561bbb48a1b580f0bc2adeb20af24f6d2d4facb4e12c1ef3f6825cdd86cde0bae363d292c not found\",\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"spec\"},\"id\":\"f2541bff66c64ac48fa3541d4b72f9c4\"},\"path\":\"spec\"}}}\ntime=\"2018-05-08T09:13:36Z\" level=error msg=\"master: error running the master process: watch event had no pipelineInfo: could not read existing PipelineInfo from PFS: objectInfoGetter: object c79f09e533b8de46b88eea1267f97842ed6e4d93104d8d4879306d1561bbb48a1b580f0bc2adeb20af24f6d2d4facb4e12c1ef3f6825cdd86cde0bae363d292c not found; retrying in 12.919605042s\"\nthanks for your help!!\n. ",
    "marcadella": "I have the same issue: I'd like to use pachctl deploy local but with Minio instead of what local uses (probably some kind of local directory). The issue is that custom expects us to use provide --persistent-disk (with for only options google, aws or azure).. @David-Development I'm not a pachyderm expert but I think you got confused about the INTERNAL object store used by Pachyderm (which you set in the command you listed), and the EXTERNAL object store used to egress data (which is the object of this question).\n@JoeyZwicker I also would like to be able to set a custom minio egress destination because, even though it is easy to do ourselves, it means that we need to install minio client (mc) on our images which is a bit of an annoyance. So it would be awesome to have this feature implemented.\nCheers. To use an egress AWS storage you would do the following:\n1. Run pachctl deploy storage amazon <region> <id> <secret>\nThis creates a Kubernetes secret containing your amazon credentials\n2. In your pipeline  you add\n\"egress\": {\n    \"URL\": \"s3://bucket/dir\"\n}\nUnfortunately for us, when Pachyderm sees s3, it makes up an endpoint that suits AWS (something like s3.<location>.amazon.com). If we had a way to set-up the endpoint when we do pachctl deploy storage (by adding a backend s3 for instance that accepts an endpoint), we would be able to egress to Minio. But for now this is not possible.. Same error for me. Same error for me. ",
    "brokenjacobs": "Will the helm chart be updated to 1.8?\nCan you configure replicas for etcd with the helm chart?\nHow do you deploy the dashboard with the helm chart?. I was able to get it deployed using the custom option. I was scared off by the persistent disk option being required but it didn\u2019t seem to put anything into the output manifest.\nIt\u2019s not working properly though, dashboard won\u2019t come up. I suspect k8s 13.2 may be an issue. Will investigate on Monday.. This looks like an error being returned by our object storage system during a multi-part upload on the commit. Much past that I don't have more information.. It does indeed seem that pachyderm is pushing 0 byte multi-part, parts:\nPUT /dev-pachy-test/pach/block/806d2aa3eff145bc99a63ba4c7a747db-index?partNumber=1&uploadId=18786bb15cfd4243b22bed63b170a229 HTTP/1.1\\r\\n\n    Host: test-s3.*****\\r\\n\n    User-Agent: Minio (linux; amd64) minio-go/2.1.0\\r\\n\n    Content-Length: 0\\r\\n\n    Authorization: AWS dev-pachy:ZXwHyIdVPKSc/MsDnndXumMdfDg=\\r\\n\n    Content-Md5: 1B2M2Y8AsgTpgAmY7PhCfg==\\r\\n\n    Date: Wed, 06 Feb 2019 04:00:14 GMT\\r\\n\n    Accept-Encoding: gzip\\r\\n\n    X-Forwarded-Proto: https\\r\\n\n    X-Forwarded-For: 10.206.28.45\\r\\n\n    \\r\\n\n    [Full request URI: http://test-s3.****/dev-pachy-test/pach/block/806d2aa3eff145bc99a63ba4c7a747db-index?partNumber=1&uploadId=18786bb15cfd4243b22bed63b170a229]\n    [HTTP request 1/1]\n    [Response in frame: 1085]. This gets more and more odd. According to the minio api docs this shouldn't be happening:\nhttps://docs.minio.io/docs/golang-client-api-reference\nPutObject(bucketName, objectName string, reader io.Reader, objectSize int64,opts PutObjectOptions) (n int, err error)\nUploads objects that are less than 64MiB in a single PUT operation. For objects that are greater than 64MiB in size, PutObject seamlessly uploads the object as parts of 64MiB or more depending on the actual file size. The max upload size for an object is 5TB.. Is there a way I can turn debug logging up to figure out where in the code this call is being made? Something equivalent to -v=9 for k8s?. I have an open ticket with EMC and they have escalated this up. We have also isolated the issue to a missing Transfer-Encoding: chunked header. As far as I know that header \"should\" be in the multi-part upload request, so this may in fact be a pachyderm/minio issue. But apparently Amazon doesn't fail with it? I have tried to reproduce against aws minio. We are working on deploying minio here currently to see if reproduces with it.\nThe 0 byte multi-part is not a bug as far as I can tell. The 0 byte comes from the 'streaming' nature of uploading inside the pachyderm software. If you want to use Pipes/Streams with S3, you have to do multi-part uploads. There is no effective way to get a content-length on a stream. . To clarify... ECS doesn't break on the request if it includes Transfer-Encoding: chunked. That doesn't necessarily mean the S3 protocol requires it.. This is an issue with the pachctl client. port-foward.go needs to include that dependency or it can't talk to the api service when an oidc provider is in the kubeconfig.\npachd won't be affected because it doesn't use OIDC authentication.\nThe k8s client-go library is parsing the ~/.kube/config file and erroring out when it sees:\nauth-provider:\n  name: oidc\nUnder a user setting in the configuration. Looks like a common problem among client-go using projects from google, just a missing dependency.. FYI I can confirm for you that setting the \"PACHD_ADDRESS\" env (and not adding --no-port-forwarding at all) works against the v1.8.2 cluster with a v1.8.4 client:\n```\n$ export PACHD_ADDRESS=**:30650\n$ pachctl list-repo\nNAME       CREATED       SIZE\nimages     9 seconds ago 0B\nlevel-zero 29 hours ago  295.2KiB\n$ pachctl version\nCOMPONENT           VERSION\npachctl             1.8.4\npachd               1.8.2\n$ unset PACHD_ADDRESS\n$ pachctl version\nCOMPONENT           VERSION\npachctl             1.8.4\nERRO[0000] Implicit port forwarding was not enabled because the kubernetes config could not be read: No Auth Provider found for name \"oidc\". I would test adding the import but I'm having some issues compiling pachctl.. As requested, 1.8.2 works against 1.8.4 cluster:\nexport ADDRESS=**:30650\n$ pachctl version\nCOMPONENT           VERSION\npachctl             1.8.2\npachd               1.8.4\n$ unset ADDRESS\nsjacobs@neon-07071 bin $ pachctl.old port-forward --namespace pachyderm &\nCTRL-C to exit\nNOTE: kubernetes port-forward often outputs benign error messages, these should be ignored unless they seem to be impacting your ability to connect over the forwarded port.\nForwarding the dash (Pachyderm dashboard) UI port to http://localhost:30080 ...\nForwarding the pachd (Pachyderm daemon) port...\nForwarding the SAML ACS port...\n[1]+ pachctl.old port-forward --namespace pachyderm &\n$ pachctl.old version\nCOMPONENT           VERSION\npachctl             1.8.2\npachd               1.8.4\n. No problem! If you could make an OS X build I can test it tonight. Just let me know where to grab it.. It works, however there is no way to set a namespace now except on the port-forward command. So port-forward is the only command I can run for now. I would suggest adding the --namespace option to all commands to support this functionality. . ",
    "iswaverly": "\nHi Dan,\nWe were attempting to deploy an instance of pachyderm with our CEPH S3 gateway protocol as a replacement for a MINIO deployment, but ran into the error you explained in the intro of this issues report. This forced us to deploy using the helm chart provided in the official kubernetes charts  repo. This was a GREAT help! To make things even better for our use case, we were now able to deploy multiple instances of pachyderm for creating serval pipelines using different storage backends in the same K8 cluster. We encountered a few problems along the way which required us to modify the helm chart pr 4159 This enabled us to deploy multiple instances of pachyderm using CEPH S3 gateway, MINIO, Amazon, etc.. Many thanks!!\n\nHi, I am trying to use CEPH by s3 gateway. How can I config charts? I set s3 configuration in values.yaml, and It seems to use Minio as backend storage. It would be of great help if you could show me how you config charts.. I am trying to deploy CEPH as backend storage, and I set s3 in charts configuration (reference: https://github.com/helm/charts/tree/master/stable/pachyderm). It seems that it use minio, and I am not sure whether minio support CEPH despite that both of them use s3 as interface.. ",
    "isabella": "As described, I get the following error as described above with the --no-rbac option. \n$ pachctl deploy google ${BUCKET_NAME} ${STORAGE_SIZE} --dynamic-etcd-nodes=1 --no-rbac\ntime=\"2018-04-01T20:45:03Z\" level=error msg=\"unable to access kubernetes nodeslist, Pachyderm will continue to work but it will not be possible to use COEFFICIENT parallelism. error: nodes is forbidden: User \"system:serviceaccount:default:pachyderm\" cannot list nodes at the cluster scope: Unknown user \"system:serviceaccount:default:pachyderm\"\"\ntime=\"2018-04-01T20:45:03Z\" level=error msg=\"unable to access kubernetes pods, Pachyderm will continue to work but certain pipeline errors will result in pipelines being stuck indefinitely in \"starting\" state. error: unknown (get pods)\"\ntime=\"2018-04-01T20:45:03Z\" level=error msg=\"unable to access kubernetes pods, Pachyderm will continue to work but get-logs will not work. error: pods is forbidden: User \"system:serviceaccount:default:pachyderm\" cannot list pods in the namespace \"default\": Unknown user \"system:serviceaccount:default:pachyderm\"\"\ntime=\"2018-04-01T20:45:03Z\" level=error msg=\"unable to create kubernetes replication controllers, Pachyderm will not function properly until this is fixed. error: replicationcontrollers is forbidden: User \"system:serviceaccount:default:pachyderm\" cannot create replicationcontrollers in the namespace \"default\": Unknown user \"system:serviceaccount:default:pachyderm\"\"\ntime=\"2018-04-01T20:45:03Z\" level=error msg=\"unable to delete kubernetes replication controllers, Pachyderm function properly but pipeline cleanup will not work. error: replicationcontrollers \"df15acbfa5644dd49d684d1796fa1921\" is forbidden: User \"system:serviceaccount:default:pachyderm\" cannot delete replicationcontrollers in the namespace \"default\": Unknown user \"system:serviceaccount:default:pachyderm\"\"\n2018/04/01 20:45:03 INFO: Listening on addr: :999 path: /v1/handle/push\nendpoints \"pachd\" is forbidden: User \"system:serviceaccount:default:pachyderm\" cannot get endpoints in the namespace \"default\": Unknown user \"system:serviceaccount:default:pachyderm\"\nHowever, without the --no-rbac option,  I get the following error. \n$ pachctl deploy google ${BUCKET_NAME} ${STORAGE_SIZE} --dynamic-etcd-nodes=1\nError from server (Forbidden): error when creating \"STDIN\": clusterroles.rbac.authorization.k8s.io \"pachyderm\" is forbidden: attempt to grant extra privileges: [PolicyRule{Resources:[\"nodes\"], APIGroups:[\"\"], Verbs:[\"get\"]} PolicyRule{Resources:[\"nodes\"], APIGroups:[\"\"], Verbs:[\"list\"]} PolicyRule{Resources:[\"nodes\"], APIGroups:[\"\"], Verbs:[\"watch\"]} PolicyRule{Resources:[\"pods\"], APIGroups:[\"\"], Verbs:[\"get\"]} PolicyRule{Resources:[\"pods\"], APIGroups:[\"\"], Verbs:[\"list\"]} PolicyRule{Resources:[\"pods\"], APIGroups:[\"\"], Verbs:[\"watch\"]} PolicyRule{Resources:[\"pods/log\"], APIGroups:[\"\"], Verbs:[\"get\"]} PolicyRule{Resources:[\"pods/log\"], APIGroups:[\"\"], Verbs:[\"list\"]} PolicyRule{Resources:[\"pods/log\"], APIGroups:[\"\"], Verbs:[\"watch\"]} PolicyRule{Resources:[\"endpoints\"], APIGroups:[\"\"], Verbs:[\"get\"]} PolicyRule{Resources:[\"endpoints\"], APIGroups:[\"\"], Verbs:[\"list\"]} PolicyRule{Resources:[\"endpoints\"], APIGroups:[\"\"], Verbs:[\"watch\"]} PolicyRule{Resources:[\"replicationcontrollers\"], APIGroups:[\"\"], Verbs:[\"get\"]} PolicyRule{Resources:[\"replicationcontrollers\"], APIGroups:[\"\"], Verbs:[\"list\"]} PolicyRule{Resources:[\"replicationcontrollers\"], APIGroups:[\"\"], Verbs:[\"watch\"]} PolicyRule{Resources:[\"replicationcontrollers\"], APIGroups:[\"\"], Verbs:[\"create\"]} PolicyRule{Resources:[\"replicationcontrollers\"], APIGroups:[\"\"], Verbs:[\"update\"]} PolicyRule{Resources:[\"replicationcontrollers\"], APIGroups:[\"\"], Verbs:[\"delete\"]} PolicyRule{Resources:[\"services\"], APIGroups:[\"\"], Verbs:[\"get\"]} PolicyRule{Resources:[\"services\"], APIGroups:[\"\"], Verbs:[\"list\"]} PolicyRule{Resources:[\"services\"], APIGroups:[\"\"], Verbs:[\"watch\"]} PolicyRule{Resources:[\"services\"], APIGroups:[\"\"], Verbs:[\"create\"]} PolicyRule{Resources:[\"services\"], APIGroups:[\"\"], Verbs:[\"update\"]} PolicyRule{Resources:[\"services\"], APIGroups:[\"\"], Verbs:[\"delete\"]} PolicyRule{Resources:[\"secrets\"], ResourceNames:[\"pachyderm-storage-secret\"], APIGroups:[\"\"], Verbs:[\"get\"]} PolicyRule{Resources:[\"secrets\"], ResourceNames:[\"pachyderm-storage-secret\"], APIGroups:[\"\"], Verbs:[\"list\"]} PolicyRule{Resources:[\"secrets\"], ResourceNames:[\"pachyderm-storage-secret\"], APIGroups:[\"\"], Verbs:[\"watch\"]} PolicyRule{Resources:[\"secrets\"], ResourceNames:[\"pachyderm-storage-secret\"], APIGroups:[\"\"], Verbs:[\"create\"]} PolicyRule{Resources:[\"secrets\"], ResourceNames:[\"pachyderm-storage-secret\"], APIGroups:[\"\"], Verbs:[\"update\"]} PolicyRule{Resources:[\"secrets\"], ResourceNames:[\"pachyderm-storage-secret\"], APIGroups:[\"\"], Verbs:[\"delete\"]}] user=&{Dagnytaggartindustrialist@gmail.com  [system:authenticated] map[authenticator:[GKE]]} ownerrules=[PolicyRule{Resources:[\"selfsubjectaccessreviews\"], APIGroups:[\"authorization.k8s.io\"], Verbs:[\"create\"]} PolicyRule{NonResourceURLs:[\"/api\" \"/api/*\" \"/apis\" \"/apis/*\" \"/healthz\" \"/swagger-2.0.0.pb-v1\" \"/swagger.json\" \"/swaggerapi\" \"/swaggerapi/*\" \"/version\"], Verbs:[\"get\"]}] ruleResolutionErrors=[]\nkubectl apply -f - --validate=false: exit status 1\n$ pachctl version --client-only\n1.7.0\n$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"9\", GitVersion:\"v1.9.2\", GitCommit:\"5fa2db2bd46ac79e5e00a4e6ed24191080aa463b\", GitTreeState:\"clean\", BuildDate:\"2018-01-18T10:09:24Z\", GoVersion:\"go1.9.2\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"8+\", GitVersion:\"v1.8.8-gke.0\", GitCommit:\"6e5b33a290a99c067003632e0fd6be0ead48b233\", GitTreeState:\"clean\", BuildDate:\"2018-02-16T18:26:58Z\", GoVersion:\"go1.8.3b4\", Compiler:\"gc\", Platform:\"linux/amd64\"\nThen, when setting the cluster version for kubernetes to --cluster-version 1.7.14-gke.1\nError from server (BadRequest): error when creating \"STDIN\": ClusterRole in version \"v1\" cannot be handled as a ClusterRole: no kind \"ClusterRole\" is registered for version \"rbac.authorization.k8s.io/v1\"\nError from server (BadRequest): error when creating \"STDIN\": ClusterRoleBinding in version \"v1\" cannot be handled as a ClusterRoleBinding: no kind \"ClusterRoleBinding\" is registered for version \"rbac.authorization.k8s.io/v1\"\n. That didn't fix it either. \nThe logs from k get logs for pachd are: \nUnknown user \"system:serviceaccount:default:pachyderm\"\"\ntime=\"2018-04-02T04:08:04Z\" level=error msg=\"unable to access kubernetes pods,\nPachyderm will continue to work but certain pipeline errors will result in pipelines\nbeing stuck indefinitely in \"starting\" state. error: unknown (get pods)\"\n. I'm using the 1.8.8-gke.0 default version. I'm following these steps: http://docs.pachyderm.io/en/latest/deployment/google_cloud_platform.html\nWith the following kubectl create rolebinding pach-admin --clusterrole=cluster-admin --serviceaccount=default:pachyderm --namespace=default, I was able to get pachd to start. . ",
    "alanz": "See also https://github.com/pachyderm/pachyderm/issues/2787\nI resorted to starting against 1.7.14-gke.1. ",
    "stevef1uk": "I have got past this stage on GKE by following the above and now when following the tutorial can create a repo but when attempting to add the blah.txt file get:\n$pachctl put-file myrepo master -c -f blah.txt \ngoogleapi: Error 403: Insufficient Permission, insufficientPermissions\nFrom the Web GUI I have manually added a file to the bucket I used when deploying pachyderm\nHow do I update the permissions for the service account?\nHmm. This may be my problem as I configured the Kubernetes cluster from the GUI and didn't set any scopes :-(\nhttps://stackoverflow.com/questions/29837531/changing-permissions-of-google-container-engine-cluster\nThus I needed to delete the previous K8s cluster and created another one and I clicked on he More tab and set Full access to the Storage API and all ok :-). ",
    "rdefreitas": "The current master branch of the go sdk has the same behavior.  Unless I'm mistaken, unlike the .NET SDK, the go sdk appears to be a nothing but a simple wrapper around the Azure Blob Storage REST API.\nAs such, you're likely going to need to manually loop through the 4MB chunks when calling the API.. ",
    "bclermont": "it had been updated end of March, is this still a problem?. ",
    "kaktus42": "Same happened here.\nAny idea how to recover from that?. I was getting this after deleting a pipeline and then garbage-collect.\nIn my case there were no jobs running.\nI am using version 1.7.1\nI wanted to do this, because when I delete-pipeline and then create-pipeline with the same name, I see that the old commits of the pipeline-repo are still there.. new PR: #2869 . ah. perfect. thanks for putting it here!. I deployed 1.7.1 and also I can't reproduce anymore.. no merge?. ah. now I understand.\nI didn't get that I have to run the make doc ;)\nWill do.\n2018-05-09 18:47 GMT+02:00 Joe Doliner notifications@github.com:\n\nWe're happy to merge after make doc is run, we could even do that step\nfor you it's just a bit of a pain to commit it back to this PR since it's\nbased off of your personal branch, so I'd have to create a separate PR.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pachyderm/pachyderm/pull/2869#issuecomment-387802740,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABiMb4k9dhFE6HBhBvwGiQrUpXAcjpiHks5twx2mgaJpZM4ThqZv\n.\n. \n",
    "emmanuel": "Thanks @jdoliner.. @jdoliner TBH, it's not (technically) enforced today. The team responsible for cluster administration keeps tabs on pods that perform hostPath mounts on an ad-hoc basis, and at the moment uses moral suasion to encourage behavior change (i.e., the cluster admin team gives a firm wag of the finger to the team using a hostPath mount, especially firm if it's the Docker socket being mounted). \nThat said, in the not-too-distant future, I expect the cluster admin team will have Kubernetes reject pods with unauthorized hostPath mounts by way of PodSecurityPolicy.. The mechanism by which we limit the launching of privileged pods will likely also be PodSecurityPolicy. \nAn alternative would be a custom admission controller, but given that PSP is designed for exactly these use-cases (controlling pod access to hostPath mounts, pods with privileged: true, and other security controls) that's where I would bet that we start.. ",
    "jeffdeville": "Chiming in as a new user.  The first word of this message is 'error'. I think it's reasonable for anyone to assume that the put-file command would have failed.\nIf this isn't something with a known solution, perhaps including this specific error message in the tutorial (where I encountered it) and troubleshooting section (where I went next) of the docs would help.. ",
    "eclosson": "Some observations I've recently had that might be related to this...\n\nI created a pipeline with stats enabled.\nRan a few datums through it\nDeleted the pipeline pachctl delete-pipeline _name_\nRe-created the pipeline with the same name\nSent one datum to it and it processes succesfully\nJob enters infinite repeats\nKill job\nDisable stats on pipeline\nRedeploy pachctl update-pipeline -f /path/to/json --reprocess\nJob finished no infinite reprocesses\n\n[2018-04-18 18:14:05]:~/mdv/sponge-spikes[master*]$ pc list-job\nID                               OUTPUT COMMIT                          STARTED       DURATION   RESTART PROGRESS  DL       UL       STATE            \n0a11088b74894042ab34fc1efe0be04b join/bdb2ecdbe7cd42219edd8c3c4d053a71  7 seconds ago 6 seconds  0       0 + 1 / 1 0B       0B       success \nb24cca9b60244f4d866605a444c909be join/d6d248846df345ebb38b455eeec18672  6 minutes ago 5 minutes  26      1 + 0 / 1 1.33KiB  1.33KiB  killed  \ncf403b59fe0246579b68a5cfde927b28 join2/eb422385d71441129acce61a4127c73b 9 minutes ago 12 seconds 0       1 + 0 / 1 1.33KiB  1.33KiB  success \n47f1896048c44cafb98063c6c594267e split/f9815e267a55430a9393f12dd449f859 5 hours ago   14 seconds 0       1 + 0 / 1 4.329KiB 1.33KiB  success \n4ffeab5ffcf24cf58fea972f6f3078fb split/3e2900d040564980ad2878b0818406b4 5 hours ago   19 seconds 0       0 + 1 / 1 0B       0B       success \n171f31f15d6a4239b0a3655d0e8167a9 split/c7bd9ce9baeb4ed3a075c5a5556c18f8 7 hours ago   14 seconds 0       1 + 0 / 1 4.329KiB 1.567KiB success \ne97413ffff8848cca6bc210d7211df55 split/ae65038885554dc68fb6daefb4e5194f 7 hours ago   12 seconds 0       1 + 0 / 1 0B       0B       failure. ",
    "oskca": "Thank you @JoeyZwicker . ",
    "ztj": "Personally I\u2019d like to see /pfs/in as the prefix to input data rather than just /pfs. I don\u2019t think that\u2019s an unreasonable request, though, it wouldn\u2019t be symlinks or differ in any way from now except the base directory difference.\nHowever, what I think would be more realistic and actually fit the flexibility of pachyderm far better would simply be to enable specifying where the  input data appears just like we already do for secrets.. I\u2019ve worked around this issue and some others. My first issue is the same as the one you describe here. The output is identical. The solution is easy. Instead of directly referencing the jupyter/datascience-notebook container, make your own with a couple extra parameters. My Dockerfile looks like this:\n```\nFROM jupyter/datascience-notebook:9f9e5ca8fe5a\nUSER root\n```\nThe next problems that I ran into have to do with port mapping and permissions. Despite specifying the correct port values for internal_port and external_port (8888, and 30888 respectively) the mapping that appeared in k8s was 30888:30888 and as a result, nothing could be accessed. Furthermore the jovyan user can not access /pfs/** by default. \nI worked around both issues by using a pipeline definition like this one...\n{\n    \"pipeline\": {\n        \"name\" : \"jupyterdn\"\n    },\n    \"transform\": {\n        \"image\": \u201clocal/jupyter-ds-n:1.0.0\",\n        \"cmd\": [\"/bin/bash\"],\n        \"stdin\": [\n            \"chown -R 1000:100 /pfs\",\n            \"start-notebook.sh --NotebookApp.password='sha1:ddddddd:dd278ddddddddddddf926af9968' --NotebookApp.port=30888\"\n        ]\n    },\n    \"parallelism_spec\": {\n        \"constant\": 1\n    },\n    \"input\": {\n        \"atom\": {\n            \"repo\": \u201cmyinputrepo\",\n            \"glob\": \"/\"\n        }\n    },\n    \"service\": {\n        \"external_port\": 30888,\n        \"internal_port\": 30888\n    }\n}\nThis works out okay given the parameters I used in the Dockerfile above for the container image and the fact that start-notebook.sh will downgrade the running user to jovyan during initialization and prior to running the actual server. \nSo to reiterate for the sake of the issue, the problems I encountered were:\n\nThe jupyter container appears to expect a user with uid 1000 to exist in the host context but that can be overridden and is probably not a pachyderm bug specifically\nThe permissions of the /pfs directories aren\u2019t compatible with the runtime user of the jupyter notebook server which is again not a pachyderm flaw and I showed how to deal with it one way up above\nThe port mapping doesn\u2019t work correctly and this does appear to be a pachyderm bug\n\nBeyond the modifications I made, I\u2019m still missing a few things to make use of this in an effective manner. For example, it would be great if there was some way we could persist notebooks/assets created by the server into a repo as well as access past notebooks in future pipeline/service launches. If there is a way and it\u2019s not really awkward, I\u2019d be interested to know about it. It\u2019d be great to finish up a notebook, stop the service and have that as part of the data provenance graph as well as available to start from the next time the service launched.. @gabrielgrant So to be clear, I\u2019m preeeetty new to the container world and docker specifically, but, my understanding of how the isolation works, how the linux kernel works, and so on is such that I believe the UIDs are shared between the host running the containers and the containers themselves while the actual user name and its permissions at runtime within the container may differ from the host. At the very least, at container execution time, whatever UID is specified has to exist in the real kernel. That\u2019s my understanding, it could be wrong, but, it jives with everything I\u2019ve seen regarding this issue as well as things like file permissions on mounted volumes, etc.\nAs a result, if you try to run a container with USER 1000 specified in the Dockerfile (which is the case by default with jupyter/datascience-notebook) and the host doesn\u2019t even have a UID 1000, it will fail. And I guess this isn\u2019t normally an issue since most host systems generally have a user with uid 1000. Looking at it a bit more closely, it seems another solution might be to set the NB_UID and NB_GID environment variables to valid UID/GID values at runtime, but, I\u2019m not sure if that\u2019s something that can be done trying to use the image directly from Pachyderm, or if the actual host OS would be predictable enough or promise a particular UID would exist.\nSo I just took the simpler approach of running the container as UID 0 like most containers seem to be run.\nAs for the last question, in my one real use case so far, what I\u2019m actually doing now is creating the jupyter pipeline when I want to explore the data in pachyderm, waiting for it to start, uploading a notebook I had downloaded from the previous run and using that. Once I\u2019m done there, I download that notebook again and delete the pipeline until I need it again. It works okay. But it would certainly be more convenient, especially when actively working on/in a pipeline, if that series of manual actions could be skipped. Having the notebook as part of the data provenance would be an extra bonus as well, if it were possible.\nThat said, I suspect it\u2019s a non-trivial matter and that other use cases for notebooks might drive a desire for different behavior entirely. And it certainly isn\u2019t standing in my way for using pachyderm for its intended purpose.. ",
    "David-Development": "I would like to add that the current \"error\" message is kind of difficult to interpret. I'm running a On-Premise installation of Pachyderm (using Minio as an S3 Storage). Setting the egress output to \"URL\": \"s3://my-bucket/pachyderm-graph/\" results in a failing job. When inspecting the job I can see the following Reason: amazon-region not found. So if I understand this right, using minio as an output isn't supported right now? \nI'm using the following command to deploy Pachyderm (using minio):\nbash\npachctl deploy custom --namespace pachyderm --persistent-disk google --object-store s3 pachyderm-s3 10 pachyderm-bucket ${S3_STORAGE_USERNAME} ${S3_STORAGE_PASSWORD} ${S3_STORAGE_HOST} --dynamic-etcd-nodes 1 --dry-run > deployment.json. @marcadella Indeed, I guess I mixed them up a little. Thank you for the clarification! Is there any documentation on the difference between the two? I couldn't find any documentation on how to define an external object store. Therefore I assumed, that pachyderm uses the \"internal\" one by default. \n@JoeyZwicker Maybe it would be helpful to include this kind of information in the documentation? The current docs on egress only mentions that you can push results to an external provider (Link). However the pipeline specs only includes the bucket name and directory (see example below). Therefore I assumed, that by default it will use the internal s3 storage.\n\"egress\": {\n    \"URL\": \"s3://bucket/dir\"\n},\nI can only agree with @marcadella that it would be tremendously useful the use minio as an egress storage.. @ysimonson It looks like that some of our filenames contained non utf-8 characters. We were able to identify this by pulling a file-list of the input-repo by using pachctl list-file repo-name master > file_list.txt. After that, we were able to identify the problematic \"files\" by running the following grep command: grep -axv '.*' file_list.txt (ref: https://stackoverflow.com/a/41741313). @ysimonson Thank you for fixing this! \ud83d\udc4d . @ysimonson Just found the official code docker uses to parse the repository name. Maybe that's something you guys can use? This way you can parse the repository (if specified) and separate it from the image name. (https://stackoverflow.com/a/37867949)\nActual code: https://github.com/docker/distribution/blob/master/reference/normalize.go#L91. ",
    "dyngts": "@brycemcanally Thanks for the explanation. Can you elaborate more for following statement:\n\nThat command is only required if the user does not already have the permissions that are needed for the pachyderm service account (kubernetes is preventing privilege escalation).\n\nWhat exactly kind of permission that user need to get authorized?. ",
    "joshcc3": "Thanks for the quick reply!. ",
    "arriqaaq": "Thanks @jdoliner, looking forward to contribute more. The CI seems to fail based on some token issue whilst connecting to vault. ",
    "mdaniel": "So, as you correctly said the nvidia device plugin has chosen nvidia.com/gpu but I wanted to point that it is evidently the long-term path forward, since the kubernetes e2e tests use it. There are no more occurrences of alpha.kubernetes.io/nvidia-gpu in the kubernetes/kubernetes nor kubernetes/api repos. I also tried searching for nvidia-gpu to see if it had moved out of alpha, and no to that, also.\nWhat is your opinion about extending the message ResourceSpec to include a map<string, string> custom to allow side-stepping this? Now that device plugins are out-of-tree (from kubernetes's PoV), it is conceivable every device vendor can make up their own resource namespace -- we certainly wouldn't want to have to wait on a pachyderm release to adjust for them all.\nI'm aware that pod_spec exists but using it would be extremely annoying, especially since that field is a string(!) as well as currently undocumented. I can reproduce this with 1.8.5 from the Brew tap\nconsole\n$ pachctl version\nCOMPONENT           VERSION\npachctl             1.8.5\n(well, ours specifically is list-datum --raw but same-same). Should I interpret the lack of a \"milestone\" on this issue to mean it is not under consideration? These log lines contain both words that drive log monitoring crazy: WARNING and error. This also will put pipelines in an infinite starting state, too, and for what appears to be an amazingly silly reason:\n\nReason: failed to create workers: invalid character '{' after array element. json\n{\n  \"input\": {\n    \"pfs\": {\n      \"branch\": \"master\",\n      \"glob\": \"/\",\n      \"name\": \"something\",\n      \"repo\": \"repo3\"\n    }\n  },\n  \"parallelism_spec\": {\n    \"constant\": 1\n  },\n  \"pipeline\": {\n    \"name\": \"bug3059\"\n  },\n  \"pod_spec\": \"{\\\"volumes\\\": [{\\\"name\\\": \\\"nope\\\"}{\\\"name\\\": \\\"bogus\\\"}]}\\n\",\n  \"transform\": {\n    \"cmd\": [\n      \"/bin/true\"\n    ],\n    \"image\": \"busybox:latest\"\n  }\n}\nconsole\n$ cat bug3059.json | pachctl -vvvv create-pipeline\nReading from stdin.\n[etcd/grpc] INFO  2019/01/29 10:29:24 log.go:172: INFO: 2019/01/29 10:29:24 parsed scheme: \"\"\n[etcd/grpc] INFO  2019/01/29 10:29:24 log.go:172: INFO: 2019/01/29 10:29:24 scheme \"\" not registered, fallback to default scheme\n[etcd/grpc] INFO  2019/01/29 10:29:24 log.go:172: INFO: 2019/01/29 10:29:24 ccResolverWrapper: sending new addresses to cc: [{10.250.1.242:32763 0  <nil>}]\n[etcd/grpc] INFO  2019/01/29 10:29:24 log.go:172: INFO: 2019/01/29 10:29:24 ClientConn switching balancer to \"pick_first\"\n[etcd/grpc] INFO  2019/01/29 10:29:24 log.go:172: INFO: 2019/01/29 10:29:24 pickfirstBalancer: HandleSubConnStateChange: 0xc4204fb250, CONNECTING\n[etcd/grpc] INFO  2019/01/29 10:29:24 log.go:172: INFO: 2019/01/29 10:29:24 pickfirstBalancer: HandleSubConnStateChange: 0xc4204fb250, READY\nthen\n```console\n$ pachctl inspect-pipeline bug3059\nName: bug3059\nCreated: 2 minutes ago\nState: starting\nStopped: false\nReason: failed to create workers: invalid character '{' after array element\n\n-- snip -- 8<\n```. It cheerfully generates seemingly infinite error messages on the pod output, too:\nconsole\n$ k -n pach3 logs -c user pipeline-bug3059-v1-lf79v\n{\"pipelineName\":\"bug3059\",\"workerId\":\"pipeline-bug3059-v1-lf79v\",\"master\":true,\"ts\":\"2019-01-31T18:53:35.797259216Z\",\"message\":\"Launching worker master process\"}\n{\"pipelineName\":\"bug3059\",\"workerId\":\"pipeline-bug3059-v1-lf79v\",\"master\":true,\"ts\":\"2019-01-31T18:53:35.906952171Z\",\"message\":\"waitJob: 99a5bfc771044c5d87f61813e4f4c743\"}\n{\"pipelineName\":\"bug3059\",\"workerId\":\"pipeline-bug3059-v1-lf79v\",\"master\":true,\"ts\":\"2019-01-31T18:53:36.045082494Z\",\"message\":\"error in waitJob rpc error: code = Unknown desc = file \\\"/yankeedoodle\\\" not found, retrying in 351.593438ms\"}\n{\"pipelineName\":\"bug3059\",\"workerId\":\"pipeline-bug3059-v1-lf79v\",\"master\":true,\"ts\":\"2019-01-31T18:53:36.428705637Z\",\"message\":\"error in waitJob rpc error: code = Unknown desc = file \\\"/yankeedoodle\\\" not found, retrying in 645.653563ms\"}\n{\"pipelineName\":\"bug3059\",\"workerId\":\"pipeline-bug3059-v1-lf79v\",\"master\":true,\"ts\":\"2019-01-31T18:53:37.192060452Z\",\"message\":\"error in waitJob rpc error: code = Unknown desc = file \\\"/yankeedoodle\\\" not found, retrying in 1.204507436s\"}\n.... I don't know if adding the :+1: reaction registers with the issue owner, but I would also like this fixed. I am experiencing this behavior on a non-AWS cluster, poured into 3 virtualbox vms, only in my case the dashboard app never boots up at all\nSeparately, I was going to file an issue about the verbosity of grpc-proxy because wow that thing is chatty. I haven't seen a \"normal\" grpc-proxy to know how much of the verbosity is some kind of debug logging, versus BAU logging, but as someone who egresses all container logs into centralized logging, I am especially sensitive to oppressively chatty containers.. > Can you explain exactly what you mean when you say \"the dashboard app never boots up at all\"? Do you mean the pod itself never boots, or the dash never loads in the browser? \nThe latter of the \"or\": I see the two screenshots in the issue description upon first connection, and never even knew the dashboard was an Enterprise feature because the app never said anything to me except error messages. The chrome messages are approximately as seen in the screenshots above, and the only thing I've ever seen from the pachd Pod is api.segement.io timeouts doing DNS lookups. The irony of the environment variable LOG_LEVEL=debug for that Pod not saying anything is not lost on me.\nIts grpc-proxy friend actually says entirely too much to capture, but the log output attached in that logs-from-grpc-proxy-in-dash-6866f58f7f-vp84q.txt a few comments up is also pretty representative -- a wall of grpc silliness, with perhaps some meaningful error messages cleverly hidden therein. If there's a similar LOG_LEVEL knob for it, I'd very much welcome knowing what it is and what to set it to in order to produce sane output.. As per our call this morning, and the brief email with Michael, I know I still owe you an issue about that top-most \"Fetching repos failed\" and its [object Object] Log out friend, but I'd prefer to have one more data point in my hand before filing that issue\nedit my other experience seems to be this: #3113. > Do you see the CPU usage spike immediately after the connection fails?\nI'd have to study it to know for sure; I actually only remembered that I still had pachctl port-forward and the Chrome instance when I noticed the Chrome process going berserk (at which time I checked on pachctl and saw the E about losing connectivity with the pachd pod.\nThere is for sure something weirdo going on with the SDN in my local cluster, but I haven't pinned it down and since we need an in-AWS cluster to test some more relevant features, I was going to wait until that cluster was running before saying anything about dash weirdness. However, since this super aggressive retry seemed like a for-real bug, that's why I started with it.\nI was cheating a little bit with step 3 in the reproduction steps, since I can for sure drive the dashboard Chrome crazy by doing that, but it doesn't hammer the server at nearly the velocity I was experiencing in the screenshot. I suspect I'd have to leave things in their steady state for a few hours to get the actual repo steps, and regrettably I need that cluster to come down tomorrow. > If we want this to be maximally flexible, we could just allow passing something like an entire jq filter (or list of filters) to apply\nI was gravely hoping you would accept ktarplee's suggestion of using RFC 6902 since almost anyone using kubectl will already recognize that syntax, and at least it's an RFC and not \"jq syntax\" which, while awesome, would require reimplementing in Golang, right?\nI hope I don't regret saying this, but if pod_spec_json_patches sounded like something that the Pach folks would accept as a PR, I'd do what I can to implement it because (much like ktarplee) this is currently the highest pain-point I have when interacting with Pach. I want very much to use the word blocker but pragmatically speaking, I have a workaround in kubectl scale rc --replicas=0 && kubectl patch rc && kubectl scale rc --replicas=$whatever but wowzers is that both ugly and not very sustainable. > This means that it's effectively impossible to use JSON-Patch to, say, replace the value of an env var, since the position of the particular env var in the array may change, others may be added, etc.\nI hear that they may change, but has it been your experience that they do change? Toward that end, AIUI, that's the risk that {\"op\": \"test\"} is trying to address: bomb rather than applying a patch to the wrong place. I would also be super interested in knowing if pachd has influence over the stable ordering of those arrays, since it appears they are ordered at generation time and I would be stunned if kubernetes itself reorders them.\nedit I should also mention that my specific use-case, and the grave pain point, is adding items to arrays, which I concede does requires less JSON-Patch hoopjumpery, but from reading the description of ktarplee's case, that seems to be true for this overall issue, too. Perfect-enemy-good-etc-etc\n\nI am curious, though, which specific cases the flexibility of JSON-Patch actually helps with, over a simpler replace-and-merge (totally believe there may be some, I just don't have examples coming to mind)\n\nI don't know exactly what \"replace and merge\" means in your mind, but I would be perfectly fine if the merge keys system used by k8s was actually used, but the situation right now is that pod_spec: does not merge volumes[] even though it has name: merge keys on the existing objects, and (at least in my case) it always has merge keys on input. > If you have a minute, can you please share the output of kubectl get all and kubectl describe po/?\nI'm relocated that thread over here, since its seems more relevant to this issue\nAs you might suspect, there is no Pod created because the RC was never created because pachd is banging its head against the RC creation process:\n2019-01-29T23:34:29Z INFO pfs.API.GetFile {\"duration\":0.004561045,\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"__spec__\"},\"id\":\"a4fd2e26771e43c6b69ceb4ca62fc36b\"},\"path\":\"spec\"}}} []\ntime=\"2019-01-29T23:34:29Z\" level=info msg=\"PPS master: creating/updating workers for pipeline bug3059\"\n2019-01-29T23:34:29Z INFO enterprise.API.GetState {\"request\":{}} []\n2019-01-29T23:34:29Z INFO enterprise.API.GetState {\"duration\":0.000002325,\"request\":{},\"response\":{}} []\ntime=\"2019-01-29T23:34:29Z\" level=error msg=\"error creating workers for pipeline bug3059: invalid character '{' after array element; retrying in 530.666954ms\"\n2019-01-29T23:34:29Z INFO enterprise.API.GetState {\"request\":{}} []\n2019-01-29T23:34:29Z INFO enterprise.API.GetState {\"duration\":0.000002031,\"request\":{},\"response\":{}} []\ntime=\"2019-01-29T23:34:29Z\" level=error msg=\"error creating workers for pipeline bug3059: invalid character '{' after array element; retrying in 792.533877ms\"\n2019-01-29T23:34:30Z INFO enterprise.API.GetState {\"request\":{}} []\n2019-01-29T23:34:30Z INFO enterprise.API.GetState {\"duration\":0.000002088,\"request\":{},\"response\":{}} []\ntime=\"2019-01-29T23:34:30Z\" level=info msg=\"moving pipeline bug3059 to PIPELINE_STARTING\"\ntime=\"2019-01-29T23:34:30Z\" level=error msg=\"master: error running the master process: invalid character '{' after array element; retrying in 8.686887543s\". > All of the commands that use implicit port forwarding do not take a --namespace.\nYes, and it is the implicit part which makes that a bug. Through what mechanism would you expect pachctl version to learn that pachd is in a pach namespace, and not just dumped into default?\nI'm on-board with adding the namespace to ~/.pachyderm/config.json as #3419 proposes, but given that I had absolutely zero idea such a file existed before Wednesday, that might not be the best UX in the world. It also means that the user would need to swap config.json files around to interact with more than one pachd (or use $PACHCTL_CONFIG, which I also only recently learned about). It's a copy from an output back to an input(? I guess that's the terminology for a \"non-output\" repo?); the ingress-batch-files in the above output is a repo that was created by a pipeline job, and the atp in the above output was a \"root\" repo, created by create-repo\nIIRC Joey said in Slack that it is unsupported behavior to do that, but if that's true, then having pachctl exit non-zero and/or be chatty about the situation would be far, far better than what appeared to be a no-op. If I might point out, there's no reason for that method to do any string manipulation if registry is empty, so:\ngo\nfunc AddRegistry(registry string, imageName string) string {\n    if registry == \"\" {\n        return imageName\n    }\n}\nalthough reasonable people can differ about whether calling AddRegistry with a blank registry has any meaning, and thus it's the caller who is in error. No, but I'm not the one who is solving it. I haven't seen it in a while, but I also haven't been creating repos recently, so unknown if it went away or I just haven't been triggering the behavior. > I'm still finding one unformatted log line:\n\nThough not sure where it's coming from.\n\nhttps://github.com/pachyderm/pachyderm/blob/v1.8.4/src/server/vendor/gopkg.in/go-playground/webhooks.v3/webhooks.go#L65. > Only additional thing would be adding this test to our CI\nThat might be something worth adding to the contributing.md, and or have a checklist in that document of local tasks one should be expected to run. I admit that I was mostly relying on Travis to whine about something rather than figure out which of the 19 test make targets were safe to run (you'll see what I mean by \"safe\" in the panics below)\nWhen I tried make test-local, there were several panics that seemed to imply they were not \"local\" as I understand that word:\n```\n--- FAIL: TestValidateActivationCode (62.58s)\npanic: cannot get test enterprise token from s3: ExpiredToken: The provided token has expired.\n        status code: 400, request id: C97C8B6B142A073D, host id: 63DQKo45F0Lb0XhuCTbkov0Fkrtk0/5XT7DkdkldwNpaZ6fbdhAAewEQAGyASHY7vMGzA5ghlJI= [recovered]\n        panic: cannot get test enterprise token from s3: ExpiredToken: The provided token has expired.\n        status code: 400, request id: C97C8B6B142A073D, host id: 63DQKo45F0Lb0XhuCTbkov0Fkrtk0/5XT7DkdkldwNpaZ6fbdhAAewEQAGyASHY7vMGzA5ghlJI=\ngoroutine 10 [running]:\ntesting.tRunner.func1(0xc000169100)\n        /usr/local/Cellar/go/1.11.5/libexec/src/testing/testing.go:792 +0x387\npanic(0x228d760, 0xc0001f87e0)\n        /usr/local/Cellar/go/1.11.5/libexec/src/runtime/panic.go:513 +0x1b9\ngithub.com/pachyderm/pachyderm/src/server/pkg/testutil.GetTestEnterpriseCode.func1()\n        /Users/mdaniel/go/src/github.com/pachyderm/pachyderm/src/server/pkg/testutil/token.go:44 +0x467\nsync.(Once).Do(0x375ae20, 0x25d3408)\n        /usr/local/Cellar/go/1.11.5/libexec/src/sync/once.go:44 +0xb3\ngithub.com/pachyderm/pachyderm/src/server/pkg/testutil.GetTestEnterpriseCode(0x3732a168028f9a10, 0x5c65ad43)\n        /Users/mdaniel/go/src/github.com/pachyderm/pachyderm/src/server/pkg/testutil/token.go:24 +0x39\ngithub.com/pachyderm/pachyderm/src/server/enterprise/server.TestValidateActivationCode(0xc000169100)\n        /Users/mdaniel/go/src/github.com/pachyderm/pachyderm/src/server/enterprise/server/enterprise_test.go:45 +0x22\ntesting.tRunner(0xc000169100, 0x25d33f8)\n        /usr/local/Cellar/go/1.11.5/libexec/src/testing/testing.go:827 +0xbf\ncreated by testing.(T).Run\n        /usr/local/Cellar/go/1.11.5/libexec/src/testing/testing.go:878 +0x35c\n```\nand several other \"I'm trying to contact a network service\" style panics.. It looks like that Travis failure was unrelated to this change; is a green Travis a pre-req for this landing?\n=== RUN   TestExtractRestoreObjects\n--- FAIL: TestExtractRestoreObjects (16.78s)\n    admin_test.go:126: No error is expected but got transport: transport: the stream is done or WriteHeader was already called\n    admin_test.go:126: current stack:\n        goroutine 27 [running]:\n        runtime/debug.Stack(0xc0001ca700, 0x1805a5b, 0x1f)\n            /home/travis/.gimme/versions/go1.11.1.linux.amd64/src/runtime/debug/stack.go:24 +0xa7\n        github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require.fatal(0x19dbba0, 0xc0001ca700, 0x0, 0x0, 0x0, 0x1805a5b, 0x1f, 0xc0001b0070, 0x1, 0x1)\n            /home/travis/gopath/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require/require.go:430 +0xc5\n        github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pkg/require.NoError(0x19dbba0, 0xc0001ca700, 0x19a1700, 0xc0001b0030, 0x0, 0x0, 0x0). This is becoming comical\n\n=== RUN   TestService\nNo output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.\nCheck the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#Build-times-out-because-no-output-was-received\n\nwhich one can see here:\nhttps://travis-ci.org/pachyderm/pachyderm/jobs/493955052#L1177\n\nSeparately, are these error messages expected?\n https://travis-ci.org/pachyderm/pachyderm/jobs/493955047#L260\n https://travis-ci.org/pachyderm/pachyderm/jobs/493955047#L420. Given that CI did not pass for what I presume is just a flakey build, can you poke Travis to have it try again?. Isn't this a dupe of #3083?. ok, well, are those actually INFO messages? Given that they appear to have no arguments nor any \"duration\" in any of the replies -- so are they informative, or just chatty?. I can believe that, but my long-held complaint is that those messages don't help me to help you, and %sinvalid character is almost always wrong\nI would think a perfectly reasonable \"fix\" to this would be substantially improving the error messages to include where the thing found a p and under what circumstances did a p emerge from ... somewhere. Then, carefully considering whether INFO error from is a meaningful expression\nSo, feel free to close this if you don't think it's a meaningful bug report. ",
    "wardn": "I should be good... https://github.com/pachyderm/pachyderm/commit/072db45fbd83812bcd1c9f8c2c59953290086a82\nLet me know if you need another signature.. ",
    "arinto": "Face to the same issue with Pachyderm deployment in GKE. Here's the environment details\nEnvironment?:\n- Kubernetes version (use kubectl version): \nClient Version: version.Info{Major:\"1\", Minor:\"9\", GitVersion:\"v1.9.7\", GitCommit:\"dd5e1a2978fd0b97d9b78e1564398aeea7e7fe92\", GitTreeState:\"clean\", BuildDate:\"2018-04-19T00:05:56Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"8+\", GitVersion:\"v1.8.10-gke.0\", GitCommit:\"16ebd0de8e0ab2d1ef86d5b16ab1899b624a77cd\", GitTreeState:\"clean\", BuildDate:\"2018-03-20T20:21:01Z\", GoVersion:\"go1.8.3b4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n- Pachyderm CLI version (use pachctl version):\nCOMPONENT           VERSION             \npachctl             1.7.2               \npachd               1.7.2\n- Cloud provider (e.g. aws, azure, gke) or local deployment (e.g. minikube vs dockerized k8s): gke\n- OS (e.g. from /etc/os-release): macOS Sierra 10.12.6\n- Others:. After upgrading the cluster and client to v1.7.3, I can run the sample pipelines. Here's my current version:\nmbp-tvlk:opencv arinto$ pachctl version\nCOMPONENT           VERSION\npachctl             1.7.3\npachd               1.7.3. what is the impact in failing to copy the SSL cert? Seems like the current fix is changing the severity from panic to warn? \nIs my cluster going to be fine?. Allright cool! Thanks for the info! . ",
    "adelelopez": "I wouldn't feel comfortable saying that until I have the Kafka demo working... I've uncovered some issues with this implementation which I think need to be addressed first.. ## Proposal\nAdd a new field to the cron input of a pipeline spec. This will be a bool, and called overwrite. If input.cron.overwrite is true, then it will do the current behavior where it has a single datum which it overwrites on each tick. If false, then it will create a new datum on each tick; implementing the feature requested here.\ninput.cron.overwrite will be an optional field and default to false. This may break some existing pipelines, but I think it will be the least surprising behavior for most users. . Conversation about this issue: https://pachyderm.slack.com/archives/CDWDPKPMY/p1550850186073900. append works with a nil array, so the check for d.k is not needed. One thing I might want to change: we don't have a way to log which spawner this master process is using, which might be a good thing to log for debugging purposes. I think the simplest and most robust way would just be to add a spawnerName field to the master function. Trying to hacky things to get the function name directly from the spawner pointer might also be possible, but seems like it would add undue complexity here. . Just want to double check that this is doing the right thing here.. Yep, that's right. I think it might be a little less confusing to call this ORIGINAL BRANCH instead of BRANCH.. what if we omitted the branch unless it was something besides master? . The problem here IIRC was that commits were not always getting finished, which would then cause the whole spout to hang since it couldn't start new commits.. ",
    "delwaterman": "Any update on this?. ",
    "vasilak": "Hi @jdoliner and thanks for responding.\nAbout the 2nd option you describe: understanding when the pipeline is done (for a particular input) can still be accomplished using flush-commit, right? I am asking since in this case you basically have 2 different \"end\" steps.. ",
    "ajbouh": "Any update on this?. Getting the name of the pipeline out of the computed hash seems like an obviously good idea. How easy would it be to accomplish this in practice?. ",
    "szeitlin": "Yeah, I couldn't find it in there, which made me wonder if the thing failed because the request didn't go through (?). Anyway, the file is really big (100 MB) since I had the dashboard open, I guess. Any suggestions on which lines you think would be most useful? For example, I dropped a lot of ListRepo lines that seemed to be pointless (empty requests), but that didn't make much of a dent. . So it looks like when it created the edges pipeline, the next thing that happened was GetState. That wasn't what happened when it tried to create the montage pipeline and failed. That time it went immediately to InspectRepo (note that I don't know what's supposed to be happening here, this is just what I can see from the logs). \nedges: \n2018-06-15T21:29:58Z INFO pps.API.CreatePipeline {\"duration\":0.026986319,\"request\":{\"pipeline\":{\"name\":\"edges\"},\"transform\":{\"image\":\"pachyderm/opencv\",\"cmd\":[\"python3\",\"/edges.py\"]},\"input\":{\"atom\":{\"name\":\"images\",\"repo\":\"images\",\"branch\":\"master\",\"glob\":\"/*\"}},\"salt\":\"598f3f6421ad4c11857f2ac74f4274f8\"},\"response\":{}} []\n2018-06-15T21:29:58Z INFO enterprise.API.GetState {\"request\":{}} []\n2018-06-15T21:29:58Z INFO enterprise.API.GetState {\"duration\":0.000003479,\"request\":{},\"response\":{\"state\":1,\"info\":{\"expires\":{\"seconds\":1530307218,\"nanos\":57000000}}}} []\ntime=\"2018-06-15T21:29:58Z\" level=info msg=\"moving pipeline edges to PIPELINE_RUNNING\" \n2018-06-15T21:29:58Z INFO pfs.API.GetFile {\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"__spec__\"},\"id\":\"e28472717c9b4c3da9b09b12b87aea23\"},\"path\":\"spec\"}}} []\n2018-06-15T21:29:58Z INFO authclient.API.WhoAmI {\"request\":{}} []\nmontage silent failure:\n2018-06-15T22:10:04Z INFO pps.API.CreatePipeline {\"request\":{\"pipeline\":{\"name\":\"montage\"},\"transform\":{\"image\":\"v4tech/imagemagick\",\"cmd\":[\"sh\"],\"stdin\":[\"montage -shadow -background SkyBlue -geometry 300x300+2+2 $(find /pfs -type f | sort) /pfs/out/montage.png\"]},\"input\":{\"cross\":[{\"atom\":{\"repo\":\"images\",\"glob\":\"/\"}},{\"atom\":{\"repo\":\"edges\",\"glob\":\"/\"}}]}}} []\n2018-06-15T22:10:04Z INFO pfs.API.InspectRepo {\"request\":{\"repo\":{\"name\":\"images\"}}} []\n2018-06-15T22:10:04Z INFO authclient.API.WhoAmI {\"request\":{}} []\n2018-06-15T22:10:04Z INFO pfs.API.InspectRepo {\"duration\":0.000925746,\"request\":{\"repo\":{\"name\":\"images\"}},\"response\":{\"repo\":{\"name\":\"images\"},\"created\":{\"seconds\":1529097715,\"nanos\":629318422},\"size_bytes\":244068,\"branches\":[{\"repo\":{\"name\":\"images\"},\"name\":\"master\"}]}} []\n2018-06-15T22:10:04Z INFO pfs.API.InspectRepo {\"request\":{\"repo\":{\"name\":\"edges\"}}} []\n2018-06-15T22:10:04Z INFO authclient.API.WhoAmI {\"request\":{}} []\nmontage succeeds after deleting the previous failed one:\n2018-06-15T22:10:04Z INFO pps.API.CreatePipeline {\"duration\":0.064718405,\"request\":{\"pipeline\":{\"name\":\"montage\"},\"transform\":{\"image\":\"v4tech/imagemagick\",\"cmd\":[\"sh\"],\"stdin\":[\"montage -shadow -background SkyBlue -geometry 300x300+2+2 $(find /pfs -type f | sort) /pfs/out/montage.png\"]},\"input\":{\"cross\":[{\"atom\":{\"name\":\"edges\",\"repo\":\"edges\",\"branch\":\"master\",\"glob\":\"/\"}},{\"atom\":{\"name\":\"images\",\"repo\":\"images\",\"branch\":\"master\",\"glob\":\"/\"}}]},\"salt\":\"8926a452087b466680dd689abe155ea0\"},\"response\":{}} []\n2018-06-15T22:10:04Z INFO enterprise.API.GetState {\"request\":{}} []\n2018-06-15T22:10:04Z INFO enterprise.API.GetState {\"duration\":0.000003157,\"request\":{},\"response\":{\"state\":1,\"info\":{\"expires\":{\"seconds\":1530307218,\"nanos\":57000000}}}} []\ntime=\"2018-06-15T22:10:04Z\" level=info msg=\"moving pipeline montage to PIPELINE_RUNNING\" \n2018-06-15T22:10:04Z INFO pfs.API.GetFile {\"request\":{\"file\":{\"commit\":{\"repo\":{\"name\":\"__spec__\"},\"id\":\"8cb077357cc545e9a636294e51c93791\"},\"path\":\"spec\"}}} []\n2018-06-15T22:10:04Z INFO authclient.API.WhoAmI {\"request\":{}} []\nI haven't reproduced the problem but I haven't tried again or done anything else with pachyderm yet. TBH I'm not sure where to go yet in terms of building pipelines - we're very early stage here, so I'm in the process of building models at the same time as I'm researching infrastructure tooling. . ",
    "kevlar1818": "I'll try to install Pachyderm on a GKE k8s cluster via helm and report back.. @JoeyZwicker I've been a bit preoccupied. I will definitely try this next week, though.. ",
    "ryanmackenziewhite": "Likely, this applies to any character that is not alphanumeric, '.', '-', '_', leading and/or trailing.. ",
    "shayshahak": "I'm referring to the deployment on Azure:\nhttps://github.com/pachyderm/pachyderm/blob/master/doc/deployment/azure.md. hi Joe, thanks for your reply. moving our conversation to private emails. Hi Joe,\nBug has been fixed, I confirmed it in my internal lab.\nSorry for the hassle.\nShay\n. ",
    "aarondancer": "@JoeyZwicker I've already signed it :). ",
    "SkinyMonkey": "I jut got some logs from a pachd container : \ntime=\"2018-07-19T23:16:45Z\" level=warning msg=\"TLS enabled but could not stat public cert at /pachd-tls-cert/tls.crt: &amp;{%!e(string=stat) %!e(string=/pachd-tls-cert/tls.crt) %!e(syscall.Errno=2)}\" \ntime=\"2018-07-19T23:16:45Z\" level=warning msg=\"TLS enabled but could not stat private key at /pachd-tls-cert/tls.key: &amp;{%!e(string=stat) %!e(string=/pachd-tls-cert/tls.key) %!e(syscall.Errno=2)}\" \ntime=\"2018-07-19T23:17:15Z\" level=info msg=\"error starting githook server context deadline exceeded\n\" \npanic: could not intiailize Pachyderm client in driver: context deadline exceeded\n goroutine 61 [running]:\ngithub.com/pachyderm/pachyderm/src/server/pfs/server.(*driver).getPachClient.func1()\n    /go/src/github.com/pachyderm/pachyderm/src/server/pfs/server/driver.go:160 +0x12d\nsync.(*Once).Do(0xc420284520, 0xc42047bf88)\n    /usr/local/go/src/sync/once.go:44 +0xbe\ngithub.com/pachyderm/pachyderm/src/server/pfs/server.(*driver).getPachClient(0xc420284510, 0x1d98ce0, 0xc4200400b0, 0xc4201d03c0)\n    /go/src/github.com/pachyderm/pachyderm/src/server/pfs/server/driver.go:156 +0x50\ngithub.com/pachyderm/pachyderm/src/server/pfs/server.newDriver.func3(0xc420284510)\n    /go/src/github.com/pachyderm/pachyderm/src/server/pfs/server/driver.go:144 +0x43\ncreated by github.com/pachyderm/pachyderm/src/server/pfs/server.newDriver\n    /go/src/github.com/pachyderm/pachyderm/src/server/pfs/server/driver.go:144 +0x498. In case anyone has the same pb:\nAfter trying 5 times through the dashboard, this time it worked:\nBy recreating everything from scratch using the az command line tool this time.. ",
    "gregfriedland": "@jdoliner I would also like this feature. My use case is that I have pipeline step to train a model that can take 12-36h to finish. If these aren't all run in parallel, the whole process will take an unneeded/unreasonable amount of time. If I overestimate how many datums will come as you suggest, then I will have potentially lots of extra pods running which in my case equates to lots of instances which is expensive (b/c these are GPU-bound jobs and GPUs can't be shared in k8s).\nIf there's a workaround to accomplish this, I'm happy to hear about it. Basically, I can see why this type of parallelism doesn't make sense for all jobs but I don't see the harm in offering it as an option.. @JoeyZwicker @jdoliner Any update on adding this feature?. I just ran list-job 10 times in a row and got 10 errors.. ",
    "carlsonp": "Ahh bugger, duplicate: #3158. ",
    "kevindelgado": "@JoeyZwicker I believe this has been fixed by #3585. My suspicion is that by improperly computing provenance in the subset in the propagateCommit step this enabled commits to hang around after commits they were provenant on had been deleted.\nMy methodology for verifying this has been to  run\npachctl delete-all\npachctl create-repo images\npachctl put-file images master liberty.png -f http://imgur.com/46Q8nDz.png\npachctl create-pipeline -f stats_edges.json\npachctl create-pipeline -f https://raw.githubusercontent.com/pachyderm/pachyderm/master/examples/opencv/montage.json\npachctl put-file images master AT-AT.png -f http://imgur.com/8MN9Kg0.png\npachctl put-file images master kitten.png -f http://imgur.com/g2QnNqa.png\npachctl update-pipeline -f stats_edges.json\npachctl put-file images master dog1.png -f http://imgur.com/XgbZdeA.png\npachctl put-file images master dog2.png -f http://imgur.com/Qz4bU.png\npachctl update-pipeline -f stats_edges.json --reprocess\npachctl put-file images master dog3.png -f http://imgur.com/A8eQsll.png\npachctl put-file images master dog4.png -f http://imgur.com/fOKhA.png\nand pepper in various delete-commit calls and confirm that all commits in all repos (especially edges) don't contain any provenance to deleted commits.\nLet me know if there is anything else you'd like to try or would me to verify before closing this issue out. Yup, repro'd before and after, closing.. PTAL @brycemcanally . PR Update: We now parse the stream of bytes returned by driver.getFile() to add File metadata and return a reader that sends back GetFileResponse messages instead of bytes.\nPTAL @brycemcanally to review my implementation of our earlier discussion. Left some notes inline to clarify specifically what I would like reviewed and the one piece that is still TODO.. PTAL @jdoliner @brycemcanally, major updates are that we now use GetFiles() in sync's Puller.Pull() call.. Comments addressed, PTAL @brycemcanally . Comments addressed, PTAL @brycemcanally. @gabrielgrant do we want timestamp displayed as seconds since epoch or as a timestamp string(i.e. 2019-03-04T22:10:12.572638018Z), or a different flag for both?. Yes that is correct, figured the ISO string would be better, wasn't sure if there are any use cases where someone explicitly wants seconds from epoch.. CI fixed. PTAL for real now @gabrielgrant . Generalized the help message and pluralized the command. The reason the help message appears twice is once for pfs/cmds and once from pps/cmds (similar to rawFlag which I was pattern matching off of). Not too concerned about PrintDetailed* wrapping, @jdoliner can chime in if he knows a cleaner way of doing it, just wanted to make sure I wasn't doing anything too stupid.\nPTAL @gabrielgrant . Friendly ping @gabrielgrant, lemme know if there's anything else we need out of this.. PTAL @msteffen . Comments addressed. PTAL @msteffen . That's a great idea. Part of the goal here is to make it easy enough for anyone to add new benchmark/load tests, so enabling others to create tests, like the ones you've describe, is a good way to test the usability of this infrastructure. Will keep in mind this use case as we are designing what this new infra should look like.. Done.. Done.. Done.. Done.. ya sorry, was just trying to trigger the ci.. Not entirely sure what the use case of these two methods are and thus am unclear on whether they need to be implemented for the streaming use case right now. Since I've got this working without, I'm assuming it's best to just leave them out for now?. Am I on the right track implementing Recv() in the client and Send() in the server (see below)? It seems a little fishy that they would be implemented in different places and I don't see any precedent for this elsewhere, but it works as is.. Also not clear on what this limiter stuff is doing and whether it's needed since it works without it.. Is this kosher to pass the GetFileRequest when getting the client and calling Recv() without any args. I noticed on PutFile it does not require the request in the args, but these method signatures are auto-generated by protobuf, so I just left it to make it work. Lemme know if there's a better way.. I see. I was pattern matching off of TestPutFiles which does not use the context in the client and also uses the raw grpc, but wasn't sure about the Sending/Receiving mechanisms that for PutFiles generates a put-file client with no args and calls Send with a PutFileRequest as an arg but for PutFiles we generate a get-file client with PutFileRequest as an arg but then send no args when we actually call GetFiles.\nI was a bit confused by this inconsistency, but chalked it up to the assumption that because the PutFile rpc call takes a stream rather than a message, client generation occurs without any arguments but because GetFiles takes a message, it's client needs to be generated with the message as an arg, and once we get to the point where it's taking a stream rather than a message, the client generation with more closely resemble what PutFile looks like. Just wanted to run this assumption by you.\nI've modified TestGetFiles (and all other instances where context.Background is used)to use the context in the PachClient, but will leave the raw grpc calls as is for now.. I do think it makes sense to fold it into the GetFile, but probably best to wait until after we've reached the desired state of the signature where it both takes a stream of GetFileRequest and returns a stream of GetFileResponse.\nRenamed to GetFiles for now.. Nope, I don't think there is any reason why Send can't be implemented. I think I originally did it because it seemed kind of weird to have something called GetFilesServer implemented in the client and then imported and used by the server, but I agree it makes more sense to put it here because all our wrappers around protobuf generated types live in the client package.\nYou are correct that these exist in order to be used by grpcutil.. TODO: this is taking in request.SizeBytes to use to determine the file sizes. This is incorrect and should instead take a sizes based on what happens in driver.getFile(). This will slightly change the driver.getFile() signature in order to have it return info on what the size of each obj/block that will then be passed to driver.filesFromByteStream().. @brycemcanally mainly wanted you to look at this and see that it looks like what you expected. As we discussed, it reads out and buffers bytes from what we get back from the getBlocks and getObjects calls in driver.getFile, generates GetFileResponse messages, and then writes those to a separate reader that get used by the client to receive the stream of GetFileResponse messages.\nIf this looks good so far, I will add the last piece which is to properly use the file size determined in the driver rather than the request size to break out the bytes we get into discreet messages.. nit: It'd be nice to have a comment here on what HandlePush is for at a high level. My understanding is for pipelines with input type of git when pushes are made to the relevant branch, we commit the payload to the corresponding pfs repo. . Ah, I see. In that case, I think a comment would make more sense at the package declaration level. . I think the biggest open question I have currently is how we want to structure the driver functions that GetFiles() calls. I have currently taken a naive approach to where I just modify driver.getFile() to now also return a slice of  NodeProtos that is then used to parse the stream of bytes into individual files.\nThe assumption I am making is that the size of this is small enough to keep in memory and we can just iterate over it. Let me know if this assumption is false. \nThis will most likely all be re-worked for the next step which is to enable calling GetFiles on directories, so I did not want to spent too much time building out something here that would later be scrapped.. One thing I realized is that the file path we receive from the request obviously does not correspond to the file path of the actual file we are getting because the request only has one path (which could be a glob pattern or, in the future, directories + glob patterns). My question here is whether or not we can just assume the name on the NodeProto to be the file path? This works for the naive case, but am unclear on how it will work with directories.  \nI added the node hash to the get file response because that seemed like more canonical metadata about a file. I'd imagine we either want one or the other to represent the metadata of a file (File, Hash, or something else), but added both now as a reminder for discussion. Thoughts?. Modified it to use callback style iteration.. Not a huge fan of all this copy/pasting from getFile(). It's necessary here, because we need separate logic to handle input and output repos. Let me know if there is a better way to do this.. Ya looking at nodeToFIleInfoHeaderFooter I do see that it is just copying the Name over. Therefore I think it makes the most sense to just leave it as is, because trying to actually call nodeToFileInfo...() here gets ugly due to different behavior between input and output repos.. This whole file creation/closing mechanism feels a little janky. Right now we create a file for every new file we get streamed back from the GetFiles call and then just leave it open so that we can continue copying data to it if the following messages from GetFiles are continuations of the file.\nThis means we become responsible for closing the file outside of the makeFile call. This seems like a working solution but lemme know if there's a better way.. @gabrielgrant we needed to find a way to pass the flag information to these PrintDetailed* functions. Correct me if I'm wrong, but to the best of my knowledge, there is no way to pass extra info into go templates.\nThe solution here is to wrap the struct we are generating our template off with the flag data. Another option is to just convert all these PrintDetailed* functions into separate Fprintf statements instead of using templates, but that seemed a bit more unwieldy. Thoughts?. Nope, now that getFiles() generates the fileInfo using the nodeToFileInfo() method we are guaranteed to have a non-nil File for every non-nil fileInfo. This was leftover from before I was using nodeToFileInfo() to properly handle the header/footer cases. Fixed.. Done.. Done.. Fixed.. Actually I meant to rename it to collectReferences, but forgot to. Lemme know if you have a preference, but I think references makes more sense since object storage references are actually what we are collecting here.. If there was, I have since forgotten. Deleted.. Yup.. Ya, I was previously using the ErrBreak here to break out of maybeProgressReader, but it makes more sense to check for the right size upfront there as well. Fixed.. Done.. Done.. I definitely agree that it's misleading to call collectReferences from within attachHeaderFooter, I have pulled that out to have it called separately. \nI'm not sure we want to conditionally call an attachHeaderFooter function, because we are then littering the getFile function with multiple copies (both inside and outside the walk callback) of tree getting, parent-path, and previous footer logic that are really only used for the dealing with headers and footers. Also there are a few levels of conditionals (whether the parent path matches, whether the footer is nil or not from the last iteration, whether the current node has header/footer), and picking any one of them to be the one to trigger the call to separate function is kind of arbitrary so for the sake of duplicating as little code in the main body of getFile it seems cleanest to isolate all the header/footer logic into one single function. Happy to break it out if you disagree, but it seemed like the main confusion comes from going through the header/footer code to call collectReferences.. Done.. ",
    "HaraldNordgren": "Not sure what is happening here? Do the new build jobs require their own credentials?\nPossibly the main symptom:\n++pachctl enterprise get-state\n+[[ No Pachyderm Enterprise token was found = \\N\\o\\ \\P\\a\\c\\h\\y\\d\\e\\r\\m\\ \\E\\n\\t\\e\\r\\p\\r\\i\\s\\e\\ \\t\\o\\k\\e\\n\\ \\w\\a\\s\\ \\f\\o\\u\\n\\d ]]\n+set +x\ndownload failed: s3://pachyderm-engineering/test_enterprise_activation_code.txt to - Unable to locate credentials\nexpected 1 arguments, got 0\nUsage:\n  pachctl enterprise activate activation-code [flags]\nFlags:\n      --expires string   A timestamp indicating when the token provided above should expire (formatted as an RFC 3339/ISO 8601 datetime). This is only applied if it's earlier than the signed expiration time encoded in 'activation-code', and therefore is only useful for testing.\n  -h, --help             help for activate\nGlobal Flags:\n      --no-metrics   Don't report user metrics for this command\n  -v, --verbose      Output verbose logs\n+pachctl auth list-admins\nthe auth service is not activated\n+admin=admin\n+echo admin\n+pachctl auth activate\n(1) Please paste this link into a browser:\nhttps://github.com/login/oauth/authorize?client_id=d3481e92b4f09ea74ff8&redirect_uri=https%3A%2F%2Fpachyderm.io%2Flogin-hook%2Fdisplay-token.html\n(You will be directed to GitHub and asked to authorize Pachyderm's login app on GitHub. If you accept, you will be given a token to paste here, which will give you an externally verified account in this Pachyderm cluster)\n(2) Please paste the token you receive from GitHub here:\nRetrieving Pachyderm token...\nerror activating Pachyderm auth: Pachyderm Enterprise is not active in this cluster, and the Pachyderm auth API is an Enterprise-level feature\nmake: *** [test-vault] Error 1\nThe command \"etc/testing/travis.sh\" exited with 2.. @jdoliner Filled in the CLA. I guess the PR you want to merge is: https://github.com/pachyderm/pachyderm/pull/3197. ",
    "d1vanloon": "@JoeyZwicker Signed.. ",
    "anishvarghese": "Hi Joey,\nThank you for quick response.\nMy Pachyderm pods are working fine.\nFinally I found out the issue , I am trying to access the Pachyderm dashboard using the URL localhost: 30080 at the same time a call goes to a web-socket ws:localhost:30081\nI deployed my Pachyderm in a digital ocean environment and to access the above I have created two ssh tunnels to my local machine for port 30080 and 30081 respectively and get it working.\nLet me know if there any possibility to access the ports 30081 and 30080 in my Digital ocean's public IP instead of localhost or any possibility of access this over a domain using nginx ??. ",
    "shimpel": "Hi @JoeyZwicker. After the update to 1.8.0 this doesn't occur any longer. \nI will close this issue.. ",
    "discdiver": "Thanks @gabrielgrant. No sweat. I filled out the CLA a day or two ago. Please let me know if it didn't come through and I should fill it out again.. Done, so closing issue.. Same bug with pachctl inspect-repo repo_name  - 0B size shown when files are in subdirectories.\n\n. Sure thing, @gabrielgrant.\nConfirming this bug only occurs only on output repos.\nNeither files in the directory nor files in the subdirectories count toward the computed filesize.\n. ",
    "Tryneus": "Multi-update pachctl commands proposal\nThis is mostly a strawman as I don't have a ton of context here, so I'm not overly attached to any one approach here.  Let me know what you think and I'll get started.\npachctl changes\nAdds the following new commands:\n pachctl create-branches\n pachctl start-commits\n pachctl delete-commits\n pachctl put-files\n pachctl copy-files\n pachctl delete-files\n pachctl create-pipelines\n pachctl start-pipelines\n* pachctl stop-pipelines\nThese commands will take a set of operations\nComplications\n\nstart-commit <repo> [<branch>] [<flags>] - branch is optional meaning we can't just chain these together, there's no way to tell if an identifier is a branch name or a repo name\nproposal: change <repo> <branch> or <repo> <commit> to <repo>/<branch> <repo>/<commit> ~everywhere\nthis is probably a compat issue with existing user scripting\n\n\nShould create-pipelines accept a JSON array, or multiple -f directives?\nIf -f - is passed we would almost certainly need to accept a JSON array, and possibly newline-delimited JSON objects\nBecause the parameters to these commands are all positional, it is not visually distinctive where one operation ends and the next one begins\nThis is alleviated slightly by using <repo>/<branch> notation, but not fully\ncopy-files is especially egregious because it uses 6 positional parameters per operation\nSupporting transactions as in @gabrielgrant's suggestion would make this problem go away, but we would need a more general solution\n\nproto changes\nAdd the following RPCs.\nPFS\nproto\nrpc StartCommits(stream StartCommitRequest) returns (stream Commit) {}\nrpc DeleteCommits(stream DeleteCommitRequest) returns (google.protobuf.Empty) {}\nrpc CreateBranches(stream CreateBranchRequest) returns (google.protobuf.Empty) {}\nrpc CopyFiles(stream CopyFileRequest) returns (google.protobuf.Empty) {}\nrpc DeleteFiles(stream DeleteFileRequest) returns (google.protobuf.Empty) {}\nPPS\nproto\nrpc CreatePipelines(stream CreatePipelineRequest) returns (google.protobuf.Empty) {}\nrpc StartPipelines(stream StartPipelineRequest) returns (google.protobuf.Empty) {}\nrpc StopPipelines(stream StopPipelineRequest) returns (google.protobuf.Empty) {}\ndriver changes\nProvide an abstraction for STM transactions and driver.propagateCommit so that we can perform multiple operations within one atomic transaction.  Hoist the STM functionality out of driver functions such as createBranch and deleteCommit and add a helper driver function to make this composable.\napiServer changes\nAdd handlers for the above RPCs and modify existing handlers to adhere to the new driver interface.\nAlternatives\n@jdoliner mentioned that create-branch is the command which could most use this functionality.  For now, we could just add the create-branches RPC and implement a more general solution in the future to save on an unnecessary middle step (see 'Future work' below).\nFuture work\nI believe we should end up in a world where a client can send an arbitrary set of modification requests that will be executed transactionally through STM.  Making this available through pachctl (as @gabrielgrant suggests above) is a little more complicated, but it shouldn't be too hard to support this through client libraries.  The driver changes above should be enough of a framework to make it simple to add such an RPC to the apiServer.\nConsider deprecating the existing RPCs this proposal mirrors, although I'm not sure what the policy is for compatibility and how long we should support old clients.\n. One nit - doesn't matter for this current issue as it stands, but:\n\nrepo@branch/path/to/file\n\nUnfortunately branches can contain / characters, so this is a little ambiguous - repos cannot, however.  I'm totally on-board with a solution of this shape, though.\nAnother option: repo@branch:path/to/file. Yep, I'll close in favor of that (and set the milestone) - thanks!. This fixes #3083 . This link was broken and also hard-coded a branch.. I'm not positive about this because I installed this anyway, but @jdoliner said this should be provided by the container used for make proto.. Heh, I've already renamed this in the next PR - it's currently executeRequests, but I think executeTransaction/runTransaction should make it very clear - do either of those sound good?\nI like execute because it lines up with the Operation.execute method, especially now that validate has been rolled into execute (in the follow-up PR).. Also, I've renamed execute to executeInTransaction so that Requests can expose an execute method that runs in its own transaction and returns the result directly instead of through a serialized protobuf message (for ease of use/readability if a batch transaction is unnecessary).. This is a known issue (as a side-effect of having two flags for the same value) - I could fix it here, but the solution would be a little ugly and I don't think it's a very big issue, just thought I'd raise awareness.. ",
    "mttwong": "\nAtomic updates for multiple pipelines simultaneously -- this seems very reasonable\n\nI definitely want this.  Updating my pipelines if very slow because it is done sequentially.  I would really like to deploy all my pipelines at once.\n\nHaving update-pipeline not trigger any job. I'm not sure if we want to do this because it then requires multiple steps for a standard \"my code is wrong and I want to push a fix\" workflow\n\nNot exactly what I am after, I'll explain after point 3.\n\nA way to reprocess a pipeline without having to grab the json spec file. This only seems to make sense if the pipeline failed for transient reasons.\n\nThe fact that I have to get the json file is just inconvenient.  I want to deploy all of my pipelines at once, then reprocess the pipelines with the changed behavior.  I can do this now, I just have to download the json file even though I am not changing it at all (since I just updated all my pipelines).. ",
    "ktarplee": "Just reporting my experience...  I am trying a microk8s deployment with pachyderm.  I was able to get the snap installed for microk8s.  pachctl installed without issues.  pachctl deploy local also completes without errors.  The pods come up and pachctl version works as expected (though it hung twice).  The issue is when one tries to deploy workflow all the pipeline pods fail and keep restarting.  The same workflow worked on macOS.  I am new to k8s and pachyderm so I am still trying to track down the error.  pachctl port-forward also does not work.. Just to follow up... I did end up getting it working with microk8s.  I had an error in the worker containers.\nTurns out that /var/run/docker.sock was a directory (created by k8s when the hostPath did not exist) so the check in api_server.go os.Stat(\"/var/run/docker.sock\") returns without error but it is not a file as expected but rather a directory and then the InspectImage call (trying to use the docker daemon) fails with error: \"error inspecting image ....\"\nOn the host I had to manually correct the docker.sock as root:\nrm -rf /var/run/docker.sock\nln -s /var/snap/microk8s/current/docker.sock /var/run/docker.sock\nWhich needs to be re-run at every startup.\nAlternatively disable docker access with pachctl deploy local --no-expose-docker-socket.  This will prevent binding /var/run/docker.sock into the container and then the os.Stat() should do the correct thing.  This approach changes slightly the way pipelines are run so it might cause issues.  Exposing the docker socket is probably a bad idea anyway so this might be the best bet.. I should add that micro k8s has been really nice to work with.  So officially supporting it at some point is probably a good idea for pachyderm.. Current error with microk8s and pachyderm 1.8.3 is \n```\nexport PACHD_ADDRESS=localhost:30650\nkmtarplee@gpuserver:~$ pachctl version\nCOMPONENT           VERSION           \npachctl             1.8.3             \npachd               (version unknown) : error connecting to pachd server at address (localhost:30650): context deadline exceeded\nplease make sure pachd is up (kubectl get all) and portforwarding is enabled\n.\nNAME                         READY   STATUS    RESTARTS   AGE\npod/dash-655844658b-gk2vm    2/2     Running   0          10m\npod/etcd-6b866844bd-zrmsp    1/1     Running   0          10m\npod/pachd-7cc99f4946-ncnc9   1/1     Running   0          10m\nNAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                                 AGE\nservice/dash         NodePort    10.152.183.103           8080:30080/TCP,8081:30081/TCP                                           10m\nservice/etcd         NodePort    10.152.183.91            2379:32379/TCP                                                          10m\nservice/kubernetes   ClusterIP   10.152.183.1             443/TCP                                                                 35d\nservice/pachd        NodePort    10.152.183.167           650:30650/TCP,651:30651/TCP,652:30652/TCP,654:30654/TCP,999:30999/TCP   10m\nNAME                    READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/dash    1/1     1            1           10m\ndeployment.apps/etcd    1/1     1            1           10m\ndeployment.apps/pachd   1/1     1            1           10m\nNAME                               DESIRED   CURRENT   READY   AGE\nreplicaset.apps/dash-655844658b    1         1         1       10m\nreplicaset.apps/etcd-6b866844bd    1         1         1       10m\nreplicaset.apps/pachd-7cc99f4946   1         1         1       10m\n```. @ysimonson I took a look at your script and I do not see the \"./etc/kube/push-to-microk8s.sh\" script references by the reset.py script in the repo/branch.  Did you forget to commit and push that file?\nIt looks like your script is primarily focused on development of pachyderm which is great.  However for a deployer/user of pachyderm I would still like to know what steps are required to get it working on microk8s.  I had it working on 1.8.1 just fine and then upgraded to 1.8.3 (this was my first upgrade) and it got really flaky.. I was able to get pachyderm 1.8.3 up on microk8s.  I have to do \n```\nmkdir -p ~/.kube\nmicrok8s.kubectl config view --raw > ~/.kube/config\npachctl deploy local --no-expose-docker-socket\nsilence errors\nexport PACHD_ADDRESS=127.0.0.1:30650\n```\nI had to reinstall (or maybe just a microk8s.reset would have worked) to get it working.  I was able to deploy pipelines, repos, and see some work being done.\nI there is a still a major issue but I think this has nothing to do with microk8s but more likely a regression in 1.8.3.  When I run pachctl list-job I get an error:\n```\npanic: runtime error: slice bounds out of range\ngoroutine 1 [running]:\ngithub.com/pachyderm/pachyderm/src/server/pps/pretty.PrintJobInfo(0x1ba8b00, 0xc0003bb1a0, 0xc00018cf00)\n        /home/jdoliner/go/src/github.com/pachyderm/pachyderm/src/server/pps/pretty/pretty.go:55 +0x8cd\ngithub.com/pachyderm/pachyderm/src/server/pps/cmds.Cmds.func4.2(0xc00018cf00, 0xc00018cf00, 0x0)\n        /home/jdoliner/go/src/github.com/pachyderm/pachyderm/src/server/pps/cmds/cmds.go:158 +0x40\ngithub.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client.APIClient.ListJobF(0x7f08db0895a8, 0xc0004ee038, 0x7f08db0896a8, 0xc0004ee040, 0x7f08db089770, 0xc0004ee048, 0x7f08db089808, 0xc0004ee050, 0x7f08db0898c8, 0xc0004ee060, ...)\n        /home/jdoliner/go/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/pachyderm/pachyderm/src/client/pps.go:276 +0x19c\ngithub.com/pachyderm/pachyderm/src/server/pps/cmds.Cmds.func4(0x2c32d88, 0x0, 0x0, 0x0, 0x0)\n        /home/jdoliner/go/src/github.com/pachyderm/pachyderm/src/server/pps/cmds/cmds.go:157 +0x3ee\ngithub.com/pachyderm/pachyderm/src/server/pkg/cmdutil.RunFixedArgs.func1(0xc000291d40, 0x2c32d88, 0x0, 0x0)\n        /home/jdoliner/go/src/github.com/pachyderm/pachyderm/src/server/pkg/cmdutil/cobra.go:20 +0x117\ngithub.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra.(Command).execute(0xc000291d40, 0x2c32d88, 0x0, 0x0, 0xc000291d40, 0x2c32d88)\n        /home/jdoliner/go/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra/command.go:572 +0x24a\ngithub.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra.(Command).ExecuteC(0xc000262d80, 0xc000519106, 0x16172e0, 0xc000518d8b)\n        /home/jdoliner/go/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra/command.go:658 +0x2cc\ngithub.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra.(*Command).Execute(0xc000262d80, 0x0, 0x0)\n        /home/jdoliner/go/src/github.com/pachyderm/pachyderm/src/server/vendor/github.com/spf13/cobra/command.go:617 +0x2b\nmain.main.func1(0x19761c0, 0xc0001f8700)\n        /home/jdoliner/go/src/github.com/pachyderm/pachyderm/src/server/cmd/pachctl/main.go:21 +0x4a\nmain.main()\n        /home/jdoliner/go/src/github.com/pachyderm/pachyderm/src/server/cmd/pachctl/main.go:22 +0xc3\n``\n@ysimonson Do you see the same thing on your microk8s setup?\n. Awesome.  What is the easiest way to try out master?  Is there a nightly build of pachyderm I could use?. @ysimonson  The reason I had to set that flag was because microk8s does not create a /var/run/docker.sock.\nPachyderm (at least in version 1.8.1) bind mounted /var/run/docker.sock to the container.  That flag prevents it from happening.  Apparently it is not needed anyway.  Maybe that behavior has changed in pachyderm because I am not sure if it is needed anymore.. I just re-tested with 1.8.4 and the flag inpachctl deploy local --no-expose-docker-socketis still required.pachctl list-job` also works now.. @JoeyZwicker I signed the CLA.. The volumes element is not the only one that contains a list.  There are many things that someone might want to change in the k8s pod spec.\nI would suggest using a more general patching policy.  For example we could use json-patch then pod_spec becomes a formal JSON patch per RFC 6902.  Then we would cover every case. This is how kubectl patch pod valid-pod --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/containers/0/image\", \"value\":\"new image\"}]'.  The -p argument is the JSON patch.  You can add, delete, replace, etc. with a JSON patch.\nAnother approach would be to use code from k8s and support ALL the types of merging and patching that kubernetes supports.  I think that would be overkill at this point.. I should mention that my use case is two fold.\n1. I want to be able to increase shared memory (k8s does not support this directly) with this hack to allow my pytorch GPU code to work properly.\n2. Add large (\"never\" changing) mounts to one of the pipelines so that I do not have to take the performance hit of downloading it every time (and unpacking it).  There are around 1 million files that I want to use this for so I tar them up (for now).  I realize this breaks encapsulation, but the use cases are numerous,  such as NLTK data (large, never changing, and should not go in the container).\nThis is a show stopper for me.  I cannot use my two GPUs with pachyderm until pod_spec is functional in pipelines for adding modules.  I have exhausted all other known options.. Just for future reference.  I was able to use pod_patch to increase the shared memory.  The formatted patch is\n[\n    {\n        \"op\": \"add\",\n        \"path\": \"/volumes/-\",\n        \"value\": {\n            \"name\": \"dshm\",\n            \"emptyDir\": {\n                \"medium\": \"Memory\"\n            }\n        }\n    },\n    {\n        \"op\": \"add\",\n        \"path\": \"/containers/0/volumeMounts/-\",\n        \"value\": {\n            \"mountPath\": \"/dev/shm\",\n            \"name\": \"dshm\"\n        }\n    }\n]\nThis can be applied to the pipeline file by adding the line.\n\"pod_patch\": \"[{\\\"op\\\": \\\"add\\\", \\\"path\\\": \\\"/volumes/-\\\", \\\"value\\\": {\\\"name\\\": \\\"dshm\\\", \\\"emptyDir\\\": {\\\"medium\\\": \\\"Memory\\\"}}}, {\\\"op\\\": \\\"add\\\", \\\"path\\\": \\\"/containers/0/volumeMounts/-\\\", \\\"value\\\": {\\\"mountPath\\\": \\\"/dev/shm\\\", \\\"name\\\": \\\"dshm\\\"}}]\". ",
    "Snapple49": "I would like to propose also being able to edit the addresses in addition to the ports, this way one could leverage wildcard dns functionality when exposing cluster ports is a limited option. . ",
    "jmcgill": "I do not believe (in 1.7.5) that it is possible to configure this or a hard memory limit on the storage pods via the pipeline spec. \nWe can manually set these values via kubectl on each pod, or on the replication controller to affect all newly created pods, however every deploy will wipe these values out.. ",
    "benmanns": "This would be useful to me. I want to fetch a dataset on a schedule, parse, and store historical versions within Pachyderm. This would enable me to compute deltas across fetches, and determine when records were inserted/updated/deleted.\n\nTimestamp as filename would work for me\nFile content wouldn't be needed anymore\n\nTo accomplish this today, I am using:\nsh\ntime=\"$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\"\npachctl create-repo myrepo_tick\necho | pachctl put-file myrepo_tick master \"${time}\" -f -\nAnd referencing myrepo_tick as a standard pfs input (instead of cron).. ",
    "CLAassistant": " All committers have signed the CLA..  All committers have signed the CLA..  All committers have signed the CLA..  All committers have signed the CLA..  All committers have signed the CLA..  All committers have signed the CLA..  All committers have signed the CLA..  All committers have signed the CLA..  All committers have signed the CLA..  All committers have signed the CLA..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you all sign our Contributor License Agreement before we can accept your contribution.8 out of 9 committers have signed the CLA.:white_check_mark: ysimonson:white_check_mark: jdoliner:white_check_mark: brycemcanally:white_check_mark: pappasilenus:white_check_mark: msteffen:white_check_mark: Tryneus:white_check_mark: gpkc:white_check_mark: kevindelgado:x: gabrielgrantYou have signed the CLA already but the status is still pending? Let us recheck it..  All committers have signed the CLA.. ",
    "Mccpie01": "I want to run pachyderm on my Pi. I cannot get it to compile.\nI saw that you tried it in the past. Were you ever able to compile it on the Pi?. I see this in the make file \nlaunch: install check-kubectl\n    $(eval STARTTIME := $(shell date +%s))\n    pachctl deploy local --dry-run | kubectl $(KUBECTLFLAGS) apply -f -\n    # wait for the pachyderm to come up\n    until timeout 1s ./etc/kube/check_ready.sh app=pachd; do sleep 1; done\n@echo \"pachd launch took $$(($$(date +%s) - $(STARTTIME))) seconds\"\nline 356 of https://github.com/pachyderm/pachyderm/blob/master/Makefile\n. I found the issue. It installs in my local user and not in /usr/local/bin\nand in the user local bin is a x86 bin. (Probable for my first steps in Linux )\nSo i have to close the issue.\nThanks for pointing me in the right direction\n. ``. Hi,\nYes everything is on the RPI.\nKubernetes\nDocker\nPachctl\nI am not sure i need minicube. I have recompile that one also\n\nDe : Gabriel Grant notifications@github.com\nEnvoy\u00e9 : 28 janvier 2019 23:27\n\u00c0 : pachyderm/pachyderm\nCc : Mccpie01; Mention\nObjet : Re: [pachyderm/pachyderm] Cannot get to compile on the PI (#3406)\nto be clear, are you trying to run the entire kubernetes cluster on the RPi? or just the pachctl client, talking to a remote kubernetes cluster?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpachyderm%2Fpachyderm%2Fissues%2F3406%23issuecomment-458343967&data=02%7C01%7C%7C99ada325805c427da86308d685782b78%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636843148495129368&sdata=VIe4CQVaKIRPXlLR0ZVzsteLuBVSmvdi2jEhmSYiIP4%3D&reserved=0, or mute the threadhttps://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAoc1EMR9UUmkdaDR-a-GjaIlm6ZrN8kdks5vH4dggaJpZM4aUzjb&data=02%7C01%7C%7C99ada325805c427da86308d685782b78%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636843148495139367&sdata=RBbLFQvJ5NBlCDvNhDvxMdBQQaW3Moiblyb7HX97m3M%3D&reserved=0.\n. Yes\n\nDe : Gabriel Grant notifications@github.com\nEnvoy\u00e9 : 29 janvier 2019 01:03\n\u00c0 : pachyderm/pachyderm\nCc : Mccpie01; Mention\nObjet : Re: [pachyderm/pachyderm] Cannot get to compile on the PI (#3406)\n@Mccpie01https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FMccpie01&data=02%7C01%7C%7Cea87d1d7f7624faab70a08d685859876%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636843206159448543&sdata=Le29b7ssQ3IfoLyJq3Lr9QHzcyjuphNIwAsoZQHN%2F%2Fs%3D&reserved=0 to clarify, did you manage to get it to compile for arm?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpachyderm%2Fpachyderm%2Fissues%2F3406%23issuecomment-458365307&data=02%7C01%7C%7Cea87d1d7f7624faab70a08d685859876%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636843206159458548&sdata=yduMstpOa%2F1Zx0owra%2FFA7GopUmk6YJeBZItAqTJQkk%3D&reserved=0, or mute the threadhttps://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAoc1EHyxzdIVCzWYvClWaR8hRltFI6BKks5vH53lgaJpZM4aUzjb&data=02%7C01%7C%7Cea87d1d7f7624faab70a08d685859876%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636843206159468565&sdata=t7uTxbCiGBR8wle4MQOu008%2Bsykw6yTTdaqRRRDGduE%3D&reserved=0.\n. Ye si got kubernetes running ok, I also have docker running but have not tested it yet\n\nDe : Gabriel Grant notifications@github.com\nEnvoy\u00e9 : 30 janvier 2019 09:39\n\u00c0 : pachyderm/pachyderm\nCc : Mccpie01; Mention\nObjet : Re: [pachyderm/pachyderm] Cannot get to compile on the PI (#3406)\nok, to get the whole of pach running on RPi will require rebuilding the docker images for ARM, in addition to recompiling the command line client\nHave you got kubernetes running on RPi/ARM already? It seems that isn't something that is officially supported by k8s out of the box, though it does seem to be possible: https://github.com/luxas/kubernetes-on-armhttps://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fluxas%2Fkubernetes-on-arm&data=02%7C01%7C%7C4026bef6bb4a4ac16ed408d68696d66d%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636844379726049736&sdata=aGmS%2ByzHzKBOwexCe0fnximSOK0yHbi5kysXcldcB3w%3D&reserved=0\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpachyderm%2Fpachyderm%2Fissues%2F3406%23issuecomment-458877022&data=02%7C01%7C%7C4026bef6bb4a4ac16ed408d68696d66d%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636844379726059741&sdata=mqR5H0wQ0tnpTZvp%2FwieoJyH6wqtdbxHsrlx%2B%2BGVLgc%3D&reserved=0, or mute the threadhttps://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAoc1EL6sd7QdKioTVXkFI-m4RrbRhX2lks5vIWhRgaJpZM4aUzjb&data=02%7C01%7C%7C4026bef6bb4a4ac16ed408d68696d66d%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636844379726069752&sdata=GNLvLq9ZNXiC6KuJzdCKodmtBKL87Z7N1VcB1b2XYhc%3D&reserved=0.\n. ",
    "setgree": "@JoeyZwicker just did, thanks. . ",
    "tianon": "Given that there's an officially-published binary release of pachctl, I'd honestly recommend not going for FROM scratch and instead targeting FROM alpine:3.9.  It's a ~4MB difference (which is pretty negligible when compared to even the compressed ~14MB of pachctl on https://github.com/pachyderm/pachyderm/releases/tag/v1.8.5).  Then the image can ideally consume those official release binaries directly, something like this:\ndockerfile\nFROM alpine:3.9\nENV PACHCTL_VERSION 1.8.5\nRUN set -eux; \\\n    wget -O pachctl.tgz \"https://github.com/pachyderm/pachyderm/releases/download/v$PACHCTL_VERSION/pachctl_${PACHCTL_VERSION}_linux_amd64.tar.gz\"; \\\n    tar -xvf pachctl.tgz --strip-components=1 -C /usr/local/bin/; \\\n    rm pachctl.tgz\n(Note that this doesn't currently actually work because the release binaries are compiled dynamically instead of statically, so they're expecting to find glibc, but that's a reasonably easy problem to solve and IMO worthwhile for published binary releases.). I mean, it's Go, so releasing official binaries for every architecture the\nproject wants to officially support is a pretty easy task too. \ud83d\ude05\n(Go's cross-compilation support is legendary \ud83d\ude03)\n. Still no idea what pachctl is, but I took https://github.com/tianon/gosu/blob/f87df69c868e19f7258b4facb7c2472d76d98dda/Dockerfile (which I use to release gosu binaries for a pretty large number of architectures with the smallest possible binaries I could get Go to create), and adapted it to build the latest release of pachctl:\n```dockerfile\nFROM golang:1.12-alpine3.9\nRUN apk add --no-cache file\nENV PACHYDERM_VERSION 1.8.5\nWORKDIR /go/src/github.com/pachyderm/pachyderm\nRUN set -eux; \\\n    wget -O pachyderm.tgz \"https://github.com/pachyderm/pachyderm/archive/v${PACHYDERM_VERSION}.tar.gz\"; \\\n    tar -xvf pachyderm.tgz --strip-components=1; \\\n    rm pachyderm.tgz\nWORKDIR ./src/server/cmd/pachctl\ndisable CGO for ALL THE THINGS (to help ensure no libc)\nENV CGO_ENABLED 0\nENV BUILD_FLAGS=\"-v -ldflags '-d -s -w'\"\n/go/bin/pachctl-$(dpkg --print-architecture)\nRUN set -eux; \\\n    eval \"GOARCH=amd64 go build $BUILD_FLAGS -o /go/bin/pachctl-amd64\"; \\\n    file /go/bin/pachctl-amd64; \\\n    /go/bin/pachctl-amd64 --help\n../../pkg/deploy/assets/assets.go:82:37: constant 8589934592 overflows int\nRUN set -eux; \\\neval \"GOARCH=386 go build $BUILD_FLAGS -o /go/bin/pachctl-i386\"; \\\nfile /go/bin/pachctl-i386; \\\n/go/bin/pachctl-i386 --help\n../../pkg/deploy/assets/assets.go:82:37: constant 8589934592 overflows int\nRUN set -eux; \\\neval \"GOARCH=arm GOARM=5 go build $BUILD_FLAGS -o /go/bin/pachctl-armel\"; \\\nfile /go/bin/pachctl-armel\nRUN set -eux; \\\neval \"GOARCH=arm GOARM=6 go build $BUILD_FLAGS -o /go/bin/pachctl-armhf\"; \\\nfile /go/bin/pachctl-armhf\nboo Raspberry Pi, making life hard\nRUN set -eux; \\\neval \"GOARCH=arm GOARM=7 go build $BUILD_FLAGS -o /go/bin/pachctl-armhf\"; \\\nfile /go/bin/pachctl-armhf\nRUN set -eux; \\\n    eval \"GOARCH=arm64 go build $BUILD_FLAGS -o /go/bin/pachctl-arm64\"; \\\n    file /go/bin/pachctl-arm64\nRUN set -eux; \\\n    eval \"GOARCH=ppc64 go build $BUILD_FLAGS -o /go/bin/pachctl-ppc64\"; \\\n    file /go/bin/pachctl-ppc64\nRUN set -eux; \\\n    eval \"GOARCH=ppc64le go build $BUILD_FLAGS -o /go/bin/pachctl-ppc64el\"; \\\n    file /go/bin/pachctl-ppc64el\nRUN set -eux; \\\n    eval \"GOARCH=s390x go build $BUILD_FLAGS -o /go/bin/pachctl-s390x\"; \\\n    file /go/bin/pachctl-s390x\nRUN file /go/bin/pachctl-*\n```\nI've had to comment out all 32-bit platforms since there's some constant in here somewhere that overflows an int, but otherwise it appears reasonably OK:\n/go/bin/pachctl-amd64:   ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, Go BuildID=Qa0N5tgrqh9Ws0hmAjMX/CEcy2VNZ5newmwm-9eEN/lz6JT17Cb3v33NZRNA-k/LYWF4FzHeFQwmsE9zDpl, stripped\n/go/bin/pachctl-arm64:   ELF 64-bit LSB executable, ARM aarch64, version 1 (SYSV), statically linked, Go BuildID=ObIrbZrDC3xbeByO3fx-/ErjTZZbyXg_t-vOeOuW1/eqgptHcmaTinyAFtYn5f/Hg1WybSpenoIqPbmKUjR, stripped\n/go/bin/pachctl-ppc64:   ELF 64-bit MSB executable, 64-bit PowerPC or cisco 7500, version 1 (SYSV), statically linked, Go BuildID=69hid6CLNHdtMJ7EoQnI/V9kt7uUKIWT1IdlEIr7-/rxITTV8y3BaBlYgjJ7ga/ovG2muJF0dlgWwqtaYgH, stripped\n/go/bin/pachctl-ppc64el: ELF 64-bit LSB executable, 64-bit PowerPC or cisco 7500, version 1 (SYSV), statically linked, Go BuildID=8C976BozPzHj0VgHiMU7/ZPhDqujGUzSZy2gu77K6/_h8Ya2tJA44Mt6AArntS/n8v4HgLqAKqs_bnMNrxL, stripped\n/go/bin/pachctl-s390x:   ELF 64-bit MSB executable, IBM S/390, version 1 (SYSV), statically linked, Go BuildID=a-CIo0Wry9oMWQh-yk22/R-RZZEv4QAEhIJ7k1Xqm/cn6TkXDy2viLE2QW8UBn/iUbdfe827iokF_AJm_I_, stripped\n-rwxr-xr-x    1 root     root       42.4M Feb 28 23:43 pachctl-amd64*\n-rwxr-xr-x    1 root     root       40.6M Feb 28 23:45 pachctl-arm64*\n-rwxr-xr-x    1 root     root       41.1M Feb 28 23:45 pachctl-ppc64*\n-rwxr-xr-x    1 root     root       41.0M Feb 28 23:46 pachctl-ppc64el*\n-rwxr-xr-x    1 root     root       42.4M Feb 28 23:46 pachctl-s390x*. ",
    "Bucca1992": "@mdaniel Did you solve this problem?. ",
    "jrvanalstine": "I'm attempting to put pachd behind an ambassador/envoy api-gateway with the rest of our services. It doesn't seem like I can really use pachd as a NodePort service here. So i'm attempting to have pachd just be type: ClusterIP and have envoy forward foo.bar.com/pachd/ to pachd port 650.\nHere's the pachd service with ambassador annotation:\napiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    getambassador.io/config: |\n      ---\n      apiVersion: ambassador/v1\n      kind: Mapping\n      name: pachd-dev-mapping\n      prefix: /pachd/\n      host: foo.bar.com\n      service: pachd.pachyderm-dev:650\n    prometheus.io/port: \"9091\"\n    prometheus.io/scrape: \"true\"\n  creationTimestamp: null\n  labels:\n    app: pachd\n    suite: pachyderm\n  name: pachd\n  namespace: pachyderm-dev\nspec:\n  ports:\n  - name: api-grpc-port\n    port: 650\n    targetPort: 0\n  - name: trace-port\n    port: 651\n    targetPort: 0\n  - name: api-http-port\n    port: 652\n    targetPort: 0\n  - name: saml-port\n    port: 654\n    targetPort: 0\n  - name: api-git-port\n    port: 999\n    targetPort: 0\n  selector:\n    app: pachd\n  type: ClusterIP\nstatus:\n  loadBalancer: {}\nI have not quite set this up correctly because when I curl -v https://foo.bar.com/pachd/ I get back:\n```\n   Trying ip.blah.blah.blah...\n TCP_NODELAY set\n Connected to foo.bar.com (ip.blah.blah.blah) port 443 (#0)\n ALPN, offering h2\n ALPN, offering http/1.1\n successfully set certificate verify locations:\n   CAfile: /etc/ssl/certs/ca-certificates.crt\n  CApath: /etc/ssl/certs\n TLSv1.2 (OUT), TLS handshake, Client hello (1):\n TLSv1.2 (IN), TLS handshake, Server hello (2):\n TLSv1.2 (IN), TLS handshake, Certificate (11):\n TLSv1.2 (IN), TLS handshake, Server key exchange (12):\n TLSv1.2 (IN), TLS handshake, Server finished (14):\n TLSv1.2 (OUT), TLS handshake, Client key exchange (16):\n TLSv1.2 (OUT), TLS change cipher, Client hello (1):\n TLSv1.2 (OUT), TLS handshake, Finished (20):\n TLSv1.2 (IN), TLS handshake, Finished (20):\n SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256\n ALPN, server did not agree to a protocol\n Server certificate:\n  subject: CN=foo.bar.com\n  start date: Mar  4 00:00:00 2019 GMT\n  expire date: Apr  4 12:00:00 2020 GMT\n  subjectAltName: host \"foo.bar.com\" matched cert's \"foo.bar.com\"\n  issuer: C=US; O=Amazon; OU=Server CA 1B; CN=Amazon\n*  SSL certificate verify ok.\n\nGET /pachd/ HTTP/1.1\nHost: foo.bar.com\nUser-Agent: curl/7.58.0\nAccept: /\n< HTTP/1.1 503 Service Unavailable\n< content-length: 57\n< content-type: text/plain\n< date: Thu, 07 Mar 2019 01:21:46 GMT\n< server: envoy\n< \n* Connection #0 to host foo.bar.com left intact\nupstream connect error or disconnect/reset before headers\n```\n\nYou can see the ambassador loadbalancer settings here:\n```\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    service: ambassador\n  name: ambassador\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"[MY CERT ARN]\"\n    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \"443\"\n    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \"tcp\"\n    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: \"true\"\n    service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: \"*\"\n    getambassador.io/config: |\n      ---\n      apiVersion: ambassador/v1\n      kind:  Module\n      name:  ambassador\n      config:\n        use_remote_address: true\n        use_proxy_proto: true\n      ---\n      apiVersion: ambassador/v1\n      kind: Module\n      name: tls\n      config:\n        server:\n          enabled: true\n          redirect_cleartext_from: 80\nspec:\n  type: LoadBalancer\n  ports:\n  - name: http\n    port: 80\n    targetPort: 80\n    protocol: TCP\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: 443\n  selector:\n    service: ambassador\n```\nI am not totally sure why i'm getting the 503 back, but I figured i'd post this in case someone sees something obviously wrong.. @gabrielgrant I hear you. At some point I will read about http2 protocol. \nAmbassador uses envoy and envoy does have gRPC support. I enabled the grpc configuration in the getambassador.io/config annotation, and I was at least able to get an ok response from pachd. I wasn't able to configure the client to work though.\nYou're right about the L7 point. The aws classic load balancer  doesn't support websockets or http2. I was able to get around the websocket issue by running ambassador service in L4 mode and just changing it to type LoadBalancer. It seems like a similar workaround is possible for http2, but I couldn't get the client to work. \nIt seems like the pieces are there, but I don't know anything about http2 so I might just deal with the extra load balancers for the time being.\nThanks =p. There's a lot more work to get this to work than I had realized as pointed out here. Will circle back to this at some point.. ",
    "res0nat0r": "@ysimonson Ah thanks, I can just keep an eye on that issue and follow from there.. ",
    "gpkc": "@pappasilenus Hey, I believe the titles Overwriting files and Appending to files might be reversed?. ",
    "0rax": "This was the broken link in question\n. "
}
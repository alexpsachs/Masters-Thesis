{
    "astrada": "See astrada/gapi-ocaml#4\n. Did you add them to your Drive? (https://support.google.com/drive/answer/2375057?p=swm_ww&rd=1)\nIf you don't, you won't be able to see those files in the mounted directory.\n. No, the 30-day period is only related to the Google App Engine proxy. The page you see is displayed because in the proxy, the method I use to request the access token is annotated with @login_required (see https://github.com/astrada/gd-ocaml-auth/blob/master/main.py#L157). The refresh token will last until you revoke it from this page https://accounts.google.com/b/0/IssuedAuthSubTokens?hl=en\n. You can find more detailed installation instructions here: https://github.com/astrada/google-drive-ocamlfuse/wiki/Installation\nI've prepared binary packages for Ubuntu 12.10 (although I don't know if they work on Ubuntu 13.04, but I plan to upgrade soon and to provide binary packages for that version too). I think that the easiest way to install from sources is using OPAM (https://github.com/astrada/google-drive-ocamlfuse/wiki/Installation#installing-from-source). Please let me know if you need any further assistance.\n. I've uploaded the binary package for Ubuntu 13.04: you can find it here https://forge.ocamlcore.org/frs/download.php/1177/google-drive-ocamlfuse-0.3.2-bin-ubuntu13.04-64bit.tar.gz\n. I'm going to close this issue. If you have any problem, please let me know.\n. The easiest way to install from sources is using OPAM. Please follow this link https://github.com/astrada/google-drive-ocamlfuse/wiki/Installation#installing-from-source to find detailed instructions. If you have any problem, let me know.\n. Unfortunately it's not possible to specify the success code in a configuration file. But there is a workaround. If you have curl installed on the target machine, you can issue this command:\ncurl https://accounts.google.com/o/oauth2/token -d \"code={success_code}&client_id={id}&client_secret={secret}&redirect_uri={redirect_uri}&grant_type=authorization_code\"\nWhere redirect_uri is the same URI configured in your Google API console. You should obtain a response like this:\njson\n{\n  \"access_token\" : \"ya29.ABCDEFG...\",\n  \"token_type\" : \"Bearer\",\n  \"expires_in\" : 3600,\n  \"refresh_token\" : \"1/abcdefghijkl...\"\n}\nThen you should copy the refresh_token and put it in the state file (in ~/.gdfuse/default). And then you can use google-drive-ocamlfuse normally. Let me know if this works for you, or if you have any problem.\n. I need to do a couple of tests to double check the OAuth flow (and I think I need a couple of days, until I can work on this). In the meantime you should check if the config file contains your client_id and client_secret. If it is so, you should use the -debug command line option to turn on verbose logging. And you should check if the curl.log file contains the correct client_id, client_secret, and refresh_token.\n. I've found a bug in version 0.3.2: client_id and client_secret from the config file are ignored. The only way to specify them is through the command line. So you always have to add -id and -secret options on the command line. I've fixed this bug in version 0.3.3, you can find the binary package for Ubuntu 13.04 here: https://forge.ocamlcore.org/frs/?group_id=305 \n. I've added verification_code to the config file in version 0.3.4. I'm closing this issue. Please let me know if it does not work for you.\n. I never thought about this scenario. But I think you could start the executable google-drive-ocamlfuse from your front-end, the same way you could mount some kind of filesystem. Do you already have an idea of how to use my application?\nYou should also consider integrating directly via the Drive API (https://developers.google.com/drive/v2/reference/) that is a RESTful JSON API very simple and well documented.\n. I don't think I will implement it, because there are no symlinks on Drive. One could think of using symlinks to implement the feature of multi-parent files (same file in different folders), but the problem is that on Google Drive side, there is just one file and multiple paths to it. There is no distinction between the master copy and the links. So, this distinction must be made client-side and must be persisted, but I don't want to keep important info on client-side, because otherwise you would not be able to reset the application cache anytime you want and restart from scratch.\n. Please have a look at this wiki page: https://github.com/astrada/google-drive-ocamlfuse/wiki/Installation. There you can find binary images for Ubuntu and a simpler way to install from sources (if you don't have Ubuntu). Let me know if you need further help.\n. Yes, the error message is quite obscure (I should make it more readable), but your problem here is that mountpoint must be an existent directory. And you may call it as you like. E.g.:\n$ mkdir gdrive\n$ google-drive-ocamlfuse gdrive\n. If you don't have permission to create a new folder, you can use an existing one. Choose an empty directory that you have access to, and use it as the argument of the command. E.g.:\n$ google-drive-ocamlfuse /mnt\n. If you mean that you cannot edit Docs, Spreadsheets, and Slides (https://support.google.com/drive/answer/49008?hl=en), that's by design. The reason for this behavior is that this kind of documents cannot be downloaded directly in their native format, but have to be exported in another format (LibreOffice format for example). I've tried to edit this kind of documents and re-import them on Drive, but usually the document layout gets messed, so I've decided to keep these file read-only to avoid unexpected modifications.\n. Thanks for your feedback.\n. You can mail me at alessandro (dot) strada (at) gmail (dot) com (replacing (dot) with . and (at) with @). All folders should be writable. Can you give me an example of what you are trying to do?\n. It would be nice, but it isn't an easy task and unfortunately I don't think I have enough spare time to dedicate to this task. Volunteers would be very welcome. :)\n. @kbarcza Thanks for sharing!\n. Thanks!\n. The command is\n$ fusermount -u mountpoint\nI will add it to the docs. Thanks for you feedback.\n. I don't think you can use w3m to get a verification code from Google, as it isn't a supported browser (but I will try, as soon as I have some spare time, and I will confirm it to you). But you need authorization only once and then you can transfer authorized tokens to all your servers. If you have a Linux installation with a graphical web browser, you can authorize the application from there, and then copy the directory ~/.gdfuse to the target server. And if you don't have a Linux desktop/laptop/whatever with a GUI, you can copy the URL you get when google-drive-ocamlfuse returns the error \"Error opening URL:\", open it in any web browser, get the verification code, and copying it back to the configuration file (verification_code key). I hope I made myself clear enough.\n. Sorry, I didn't specify that you get the verification code only if you are using the alternative auth method (with your custom client_id and client_secret obtained from Google API console, and only when redirect URL is set to urn:ietf:wg:oauth:2.0:oob).\n. Now the wiki has a page with step by step instructions on how to make the application work on a headless server. Thanks to @jorgebr for writing it.\n. I confirm that it's a bug. But unfortunately just setting the content type to image/jpeg isn't enough. I have to investigate further this issue to understand how to enable file preview.\n. I think I found the problem. I've uploaded a fix in release 0.4.1 (available in my PPA and in OPAM).\n. OK, I'm adding -o mount options. In the next release (0.4.2), you will be able to mount the filesystem on a folder that's not empty, using -o nonempty.\n. I think that is possible, but I don't see the point. What is the advantage of this approach over making symlinks to the subdirectories? In your example, /local/mount/point/ could be a symlink to /gdrive/folder/filter. Remember that this application isn't a sync application, i.e. you don't have a copy of the remote files on your local filesystem.\n. Glad to be of help.\n. That's the way the -debug flag works. When specified, my application pass option -d to libfuse, that implies -f (foreground operation), so the application stays in foreground, until you unmount the filesystem (with fusermount -u mountpoint). If you want, you can have verbose output, with background operation, using -verbose instead of -debug. \n. I will make a couple of tests to find if there is something I can do about that, but the main problem is that when you issue a cp command, the file is copied in small chunks (and that's the way libfuse works). So copy operations aren't atomic, but require a large number of writes. And any write looks up in the cache, to find metadata associated with the file to upload. This could explain the issues you are experiencing (more of a CPU/disk issue than a network issue), but, unfortunately, this problem isn't easy to solve.\n. No, I think I know where the problem is. If I need some other info, I will ask you, thanks.\n. One thing you can try: update to version 0.4.2 (that I have just uploaded) and use option -o big_write. This way you enable writes larger than 4KB and the process should be quicker. If this doesn't solve your problem (at least partially), you should enable verbose logging (or debug mode) and thend send me your ~/.gdfuse/default/gdfuse.log (alessandro dot strada at gmail dot com).\n. Sorry, my fault, it's -o big_writes.\n. Thanks for your feedback.\n. It could be caused by issue #86. If the upload takes more than one hour, your access token expires and Google Drive rejects it. Unfortunately it's a known Google Drive bug, so there is no workaround, but they are working on it (https://code.google.com/p/google-api-python-client/issues/detail?id=231).\n. In version 0.5.21, I've added max_download_speed and max_upload_speed to avoid flooding the connection.\n. Sorry, I don't have Office, so I can't try it directly. Could you please do this test? Save the file on your filesystem, then copy it to the mounted directory. Is it still corrupted?\n. I didn't try it, but it looks like afuse is the tool you are looking for. It is packaged for Ubuntu, and you can configure it running the command:\n$ afuse -o mount_template='google-drive-ocamlfuse %m' -o unmount_template='fusermount -u -z %m' mountpoint\n(replace mountpoint with your actual directory).\nHere you can find further examples.\nPlease, let me know if it works, so I can add this info to the wiki. Thanks\n. I don't know if this solves the issue, but maybe you can use also the %r parameter in the mount template. E.g.:\n$ afuse -o mount_template='google-drive-ocamlfuse -label %r %m' -o unmount_template='fusermount -u -z %m' /my/path\nAnd then access the mounted filesystem in /my/path/default. Maybe, this is the way afuse is using to discriminate local/remote paths.\n. Those directories are created by GNOME (here you can find something about the trash directories), probably as a side effect of automount. But I have no idea how to modify the behavior without affecting other applications.\n. @cubica I don't know if you are still looking for a way to automount the filesystem, But I think I made some progress, and I wrote down a wiki page with instructions on how to proceed. Let me know if this works for you.\n. I'm closing this issue. If something does not work as expected, please reopen it.\n. In release 0.4.5, you can use the desktop format to create a desktop shortcut to the documents. And using the new docs_file_extension option, you can also display file extensions for Google Docs, so that you will be able to edit the docs directly in the web browser, double clicking the files. This is an example config file to obtain that behavior:\nsqlite3_busy_timeout=500\nread_only=false\nconflict_resolution=server\nverification_code=\ndrawing_format=desktop\ndocument_format=desktop\nform_format=desktop\ndocs_file_extension=true\nkeep_duplicates=false\npresentation_format=desktop\ndownload_docs=true\nclient_secret=\nmetadata_cache_time=60\nclient_id=\nspreadsheet_format=desktop\ndebug=false\numask=0o002\nNote that when you change the docs_file_extension from false to true or viceversa, you should clean the cache using the -cc command line option. Otherwise, you will still see the cached files with/without the file extensions.\n. The zip with the xml files is probably a .ods (OpenOffice Spreadsheet). You should see a .desktop file of about 150-160 bytes, and if you print its content with cat, you should see something like this:\n[Desktop Entry]\nType=Link\nName=TestSpreadheet.desktop\nURL=https://docs.google.com/spreadsheet/ccc?key=0[...]c&usp=drivesdk\nCould you please post the content of your config file (after removing the client id/secret if present)?\n. Could you please post the content of your ~/.gdfuse/default/config file?\n. Could you try with this one?\ndocs_file_extension=true\nverification_code=\numask=0o002\ndocument_format=odt\ndebug=false\npresentation_format=pdf\nspreadsheet_format=ods\ndownload_docs=true\nclient_id=\ndrawing_format=png\nconflict_resolution=server\nkeep_duplicates=false\nmetadata_cache_time=60\nclient_secret=\nread_only=false\nform_format=ods\nsqlite3_busy_timeout=500\nThen run\ngoogle-drive-ocamlfuse -cc\nto reset the cache, and then run the usual command to mount you drive.\n. It's probably due to the latency of the remote filesystem.\n. The problem is that the Drive API is returning a 403 HTTP error (Forbidden) for that file, so the application cannot download it. You should try to download and reupload it from the web interface (removing the old copy). Maybe this way, the file should be accessible.\n. Maybe you are going over API quota limits. It should not happen, because the current quota is 100 requests/user/second, and I think it's difficult to reach that limit. But I can run a couple of tests to see if I'm able to reproduce the problem.\n. I did a couple of tests, uploading 1000 small files. On my slow connection (I have an upload rate of about 50KB/s), it took more than 3 hours, but I didn't experience your problem (the forbidden error). Do you have a faster connection? If your connection is very fast, I think you could hit the Drive API quota and get that kind of error. Maybe I can add some code to handle that situation, retrying after a little bit of time.\n. I've found the real problem. It's related to expired download links. Downloading files using an expired link causes a 403 error. I should have fixed this issue in version 0.4.6.\n. I'm closing this issue. If you still have problems, let me know.\n. > I started using your program and it really seems great. I was wondering though whether using rsync is of any \n\nadvantage for this in terms of bandwidth as I have noticed it to be quite slow.\n\nYou will get a little speedup only downloading from the server to your host. Drive API doesn't support partial uploads, so using rsync won't get you quicker uploads.\n\nalso, the mime type of the PDF according to Google is application/octet-stream.\n\nWhich version are you using?\n. The problem is that rsync copies the files using temporary random names (e.g. .filename.pdf.0MN9tN). But I choose the content type from the file extension, and when the file is uploaded, the extension is wrong. I can try to change the content type when the file is renamed, but I don't know if it's enough. I'll let you know if it works.\n. I made a test and it didn't work. It seems a problem on the Drive API side (see http://stackoverflow.com/questions/14629839/unable-to-update-mimetype-using-google-drive-api). The only workaround is to reupload the file, but this way you will upload all files 2 times.\n. Not exactly, there are issues using rsync to upload files, but it works pretty well as a backup tool, to backup the content of your Drive on your host (I'm using it this way).\n. Looking at rsync manual, I found the --inplace option, that avoids creating temporary files. Maybe you can try it to  see if using that option, the file MIME type is set to application/pdf.\n. > imho some suggestions of this issue should be promoted to a wiki page, @astrada I'd gladly help\nSure! Feel free to create a new wiki page. Thanks!\n\nis this considered \"normal\"?\n\nI'm sorry, but I don't have a very stable connection (I'm on LTE that has very variable upload rate) so I can't measure a \"normal\" speed.\n\nare there any fine tunings that can increase upload speed? (besides the --inplace option)\n\nYou can test these options:\n- you can set a very high timeout in metadata_cache_time= so your cache doesn't risk to be invalidated\n- you can try multi-threading with -m (I'm still testing this option so it's not yet super-stable) even if rsync is single-theaded (but you can test parallel)\n- I'm still looking for some way to increase the maximum FUSE block size (that is 128KB)\n\n(slightly OT) is this slowness due to google drive's api in general (meaning: there's no other way to significantly increase transfer speed to/from google drive)?\n\nI'm experiencing very variable transfer rates too, but I'm not sure this is due to Google or to my 4G provider.\n. > wiki page created\nNice, thanks.\n\nrsync: ERROR: cannot stat destination \"/my/ocamlfuse/mount/path\": Transport endpoint is not connected (107)\n\nOK, I will do a couple of tests.\nThanks for your feedback.\n. > But in that particular server I haven't still changed the metadata_cache_time, maybe it's because of that (?)\nNo, I don't think so. It's probably a bug (if you are using multi-threading) or (less likely) you received too many errors server side (each request is retried at most 10 times).\n. You may try to edit the file ~/.opam/system/installed, adding this line:\nocamlnet 3.7.3\n. The problem is the MIME type application/octect-stream. It should be application/pdf. To understand what's going wrong, you should run google-drive-ocamlfuse with -verbose option and upload a pdf file. Then you should send me the log file ~/.gdfuse/default/gdfuse.log to my mail address (alessandro (dot) strada (at) gmail (dot) com). Thanks\n. It looks like xdg-open is configured in an unexpected way. What happens if you run this command?\nxdg-open http://www.google.com\n. As a workaround you can set the BROWSER environment variable to a non-existing command. E.g.:\nBROWSER=firefox google-drive-ocamlfuse\nThis way you should get the exception containing the URL.\n. Ah, yes, you should put your client id/client secret on the command line, otherwise you won't get the verification code.\nBROWSER=firefox google-drive-ocamlfuse -id ######.apps.googleusercontent.com -secret ***********\n. The new package will be available as soon as my pull request OCamlPro/opam-repository#1188 is merged.\n. Unfortunately it's not easy to optimize uploads. One solution could be to add another layer of cache in memory, but it requires a lot of work. So I can't give you a solution anytime soon.\nI'm closing this issue, because it's a duplicate of issue #16.\n. I was experimenting with FUSE mount options and I've added large_read by default. Probably this is causing your issue. But from the docs, it looks like this option is useless. So I'm going to remove it.\n. I've uploaded a package for saucy.\n. Try resetting the cache, with this command:\n$ google-drive-ocamlfuse -cc\nMaybe the problem is that some of the Google Docs were downloaded when docs_file_extension was false, so that xdg-open don't know which program to start.\n. What does it happen if you try to open it directly?\n$ libreoffice Travel-Packing-List.ods\n. OK. Try mounting the filesystem using the -verbose option.\n$ google-drive-ocamlfuse -verbose [mountpoint]\nTry opening the doc, and then send the log files gdfuse.log and curl.log you will find in ~/.gdfuse/default to alessandro(dot)strada(at)gmail(dot)com. I will try to understand what's going on.\n. OK, no problem.\n. Could you please send me the log files? You can obtain them running\n$ google-drive-ocamlfuse -verbose [mountpoint]\nThen try to reproduce that error and send the files gdfuse.log and curl.log (you will find in ~/.gdfuse/default) to alessandro(dot)strada(at)gmail(dot)com\nThis way I can try to understand what's going on. Thanks\n. Yes, I was aware of Grive when I started this project. I wrote my application for a number of reasons:\n- I wanted to test my library gapi-ocaml.\n- I needed a solution to backup my Google Docs (and Grive doesn't download Docs).\n- I wanted to try the \"UNIX\" way: a remote filesystem vs. a sync application. And this is the main difference between my project and Drive.\n. No, at least not yet. But it's on my to-do list.\n. In release 0.5, I've added the configuration option max_cache_size_mb, to specify the maximum size of the cache.\n. Thanks for your feedback. I have not changed that default yet, because I am still checking if it has some unwanted side effects.\n. What extension do you get if you issue an ls command?\n. It seems like ocamldoc is not installed on your machine. Which distribution are you using?\n. Try running opam update before opam install google-drive-ocamlfuse.\n. Run:\n$ google-drive-ocamlfuse -label [label]\nwhere [label] is, for example, the username of the second account. Go through the authorization process for the second account, then run;\n$ google-drive-ocamlfuse -label [label] [mountpoint]\nwhere [mountpoint] is the directory where you want to mount the filesystem.\n. Could you please send me the log files? You can obtain them running this command:\n$ google-drive-ocamlfuse -verbose [mountpoint]\nThen try to reproduce that error and send the file gdfuse.log (you will find in ~/.gdfuse/default) to alessandro(dot)strada(at)gmail(dot)com\nThis way I can try to understand what's going on. Thanks\n. > Is it possible that instead of creating a \"new file\", I get a \"new revision file\" for the original file ?\nI think it's possible, but I have to do some testing to verify if it's feasible. I'll let you know if I manage to implement this feature.\n. Are you using OPAM? Or are you building every package one by one?\nAnyway. It looks like there is no libfuse.a in the library path. In Linux you would find it in /usr/lib/x86_64-linux-gnu/. Unfortunately I don't have a Max OS X box to test the build process.\n. Yes, native linking is the most difficult task (e.g. OCaml build systems don't support pkg_config). I'm just guessing, and I don't know if the problem is the lacking libfuse.a file, or if there is some issue with camlidl (I've checked on my Linux box and the symbols fuse_exited, fuse_process_cmd, and fuse_read_cmd start with a double underscore and not a triple as in your error message). Maybe as a workaround, you could try to run the byte-code version. I've uploaded a modified package here: https://forge.ocamlcore.org/frs/download.php/1358/google-drive-ocamlfuse-0.5.2-byte.tar.gz\nYou can compile it with:\n$ ocaml setup.ml -configure\n$ ocaml setup.ml -build\nAnd if it builds correctly, you can install it with:\n$ ocaml setup.ml -install\nLet me know if it works.\n. To find where the problem is, you could try to:\n1. Download ocamlfuse source from: https://forge.ocamlcore.org/frs/download.php/1074/ocamlfuse-2.7.1-cvs.tar.gz\n2. Compile the library:\n$ CC=gcc CPPFLAGS='-DFUSE_USE_VERSION=26 -I/usr/local/include/osxfuse' PKG_CONFIG_PATH='/usr/local/lib/pkgconfig' INCDIRS=$(opam config var lib)/camlidl/ make -C lib/\n3. Compile the examples:\n$ CC=gcc CPPFLAGS='-DFUSE_USE_VERSION=26 -I/usr/local/include/osxfuse' PKG_CONFIG_PATH='/usr/local/lib/pkgconfig' INCDIRS=$(opam config var lib)/camlidl/ make -C example/\n4. If the examples compile without errors, you can try the hello executable in example subdir to mount a sample fs that contains a single file (hello):\n$ ./hello [mountpoint]\n. What if you try to install the library with make install -C lib/ before compiling the examples?\n. The target directory can be shown by this command:\n$ ocamlfind printconf destdir\nIf it's the opam lib directory, you will not be able to install with make install, unless you remove the opam package with opam remove ocamlfuse.\n. Could you post the content of file lib/Fuse_bindings_stubs.c? So that I can compare it with my version.\n. OK, thanks. It's exactly the same. So the problem is not camlidl. Can you try to:\n1. uninstall with make uninstall -C lib/\n2. edit lib/Makefile, replace\nCLIBS=fuse\nwith\nCLIBS=osxfuse\n3. make lib\n4. make install\n5. make example\n. OK, let's see what symbols are in the library. Could you post me the output of these commands?\n$ nm -D /usr/local/lib/libfuse.dylib | grep fuse_exited\n$ nm -D /usr/local/lib/libfuse.dylib | grep fuse_process_cmd\n$ nm -D /usr/local/lib/libfuse.dylib | grep fuse_read_cmd\n. This is the output on Linux:\n$ nm -D libfuse.so.2.9.2 | grep \"fuse_\\(exited\\|\\(process\\|read\\)_cmd\\)\"\n0000000000009fc0 T __fuse_exited\n0000000000009fc0 T fuse_exited\n0000000000009f90 T __fuse_process_cmd\n0000000000009f90 T fuse_process_cmd\n000000000000a220 T __fuse_read_cmd\n000000000000a220 T fuse_read_cmd\nSo, there is no symbol with the double underscore in osxfuse. If you want, you could try to replace every occurrence of __fuse_exited, __fuse_process_cmd, __fuse_read_cmd (that you can find in Fuse_bindings.idl and Fuse_lib.ml) with the single underscore version. And check if it links (maybe trying to link directly to osxfuse).\n. That error means that there is still an old version of the library somewhere in the library path. Try to uninstall the library, check that no file is left in the /Users/sorin/.homebrew/lib/ocaml/site-packages/ocamlfuse/ directory, then: make clean, make lib, make install, make example.\n. Good news. I think you are right about the underscores. Unfortunately the ocamlfuse package is not actively maintained anymore. But I will make some tests on my machine, to check if I can replace those versions of the functions.\n. Well, thanks for your help. I will test the patched version and if everything is fine, I will publish the new version on OPAM.\n. > Somehow, it still opened Firefox, which is not the default browser\nIt's the fallback if xdg-open does not work.\n\nThen it complained about big_writes. I am not sure if this is an google-drive-ocamlfuse error or an ocamlfuse error. I did not specify -obig_writes on the command line.\n\nIt's set by default (https://github.com/astrada/google-drive-ocamlfuse/blob/master/src/gdfuse.ml#L392), because otherwise (on Linux), files will be written one page (4 KB) at a time, making writes too slow.\n. Try using the -debug command line option. It will make the executable run in foreground and write to log files (~/.gdfuse/default/gdfuse.log and ~/.gdfuse/default/curl.log).\n. Sorry, but I have no idea why the debug flag does not work.\n. Looks like there is some issue with libcurl and https (maybe with openssl or equivalent). But I don't think I can figure it out. Sorry.\n. You can now install my app on MacOS using opam.. It's probably a bug in the way I keep track of cache size. If you enable the -verbose logging, you should be able to see lines like this:\nDownloading resource (id=7)...Updating cache size (36) in db...done\nin the file .gdfuse/default/gdfuse.log. The total cache size is kept in the SQLite db (table metadata) and is updated only when downloading or uploading a resource. If it gets out of sync, it can absolutely cause the issue you are experiencing.\n. Yes, you are right. It works like that, because Google Drive API does not support partial uploads, while FUSE will copy the file chunk by chunk. So if the file is not cached somewhere, the upload would be impossible. In theory, the cached file will be deleted after the upload is completed. If the file remains, there is a bug.\n. Oh, sorry, I didn't understand that you were talking about downloading and not uploading. When you download a resource, the cache is shrinked before the download is performed. That's because, even reading is done chunk by chunk and I think that lantency would be too high, if reading was performed directly from the Drive API. But when I have more time, I will do some testing about direct download without caching.\n. OK, thanks for your feedback. I will try to add a direct download option. I still don't know if it's easy to implement.\n. In version 0.5.10, I've added 2 options to deal with this issue. If you set stream_large_files to true, any file larger than large_file_threshold_mb will be downloaded directly, skipping the cache.\n. That's on purpose. I didn't implemented this feature to prevent data loss caused by undiscovered bugs. You should use the web interface if you really want to permanently delete files from your Drive.\n. Unfortunately, I don't think it's possible to add a progress bar visible in some kind of UI (console or GUI). My application provides a FUSE filesystem, and a filesystem does not provide a callback to report progress to the user interface. Moreover my application is single threaded, because the multi-threading version of the library I'm using is not stable, so there would be no way to notify the calling process in the middle of a download or an upload. Sorry.\n. Thanks for your suggestion. The main problem is that I'm using an OCaml binding of FUSE that's not maintained anymore and I don't know C well enough to patch the library myself.\n. You should try to reproduce the problem, running the program in verbose mode :\n$ google-drive-ocamlfuse -verbose [mountpoint]\nThen you should open the file gdfuse.log (you will find in ~/.gdfuse/default) and look for errors returned by the backend. If there are no errors, it's surely a bug.\n. I'm closing this issue because there was no follow-up.\n. Try using -debug instead of -verbose, this way you will get the curl.log file.\n. The application is trying to retrieve the OAuth2 token. Did you get the verification code yet?\n. OK, probably you have xdg-open configured someway to (maybe) open a text browser (links, w3m, ...). Try running the command this way:\n$ PATH= google-drive-ocamlfuse -label me -id {client id}.apps.googleusercontent.com -secret {client secret} -verbose -debug\nThis way, xdg-open will not be found and you should see the authorization URL to open in a graphical web browser. \n. Thanks for your suggestion. I will add a specific option.\n. You should try to reproduce the problem, running the program in debug mode :\n$ google-drive-ocamlfuse -debug [mountpoint]\nThen you should check the log files gdfuse.log and curl.log (you will find in ~/.gdfuse/default) and look for errors 400/500, returned by Google Drive.\n. @piccaso: thanks for your feedback. There were some issues with the Drive backend on Feb 6. Maybe they are related to your issues.\n\nIs there a better way to recover the mountability in that state?\n\nNo, I've never experienced errors during token refresh. But I never use the -m option, because the FUSE binding I'm using is too unstable with that option turned on.\n\nIf i delete the folder i need to authenticate again, and that cant be automated (right?)\n\nYes, that's the way OAuth works.\n\nWhat would you recomend for copying large amount of data of the drive, are there alternatives to cp which could work better?\n\nNo, I've experienced some of your issues uploading a large amount of files, but downloading usually works with both cp and rsync.\n\ncould you please make a parsable log entry for possibly corupted files.\n\nOK, I'm putting that on my TODO list.\n\nany chance getting more detail out of 'Error: cannot close db'. and of course, any idea how to fix this?\n\nNo, close_db returns a boolean. And a failure, I wasn't able to reopen the DB. So I decided to make the program exit.\n\nmy test system is a pretty old computer and is under full load doing this, maybe this is some kind of race condition... i'll try again without mounting -m\n\nI'm thinking about removing that option, because it usually causes more problems than it solves.\n@brihuega: thanks for your feedback too.\nI will try to improve stability, retrying requests on every error, and try to implement a better error handling mechanism. But it's not easy to do it right.\n. It's \"messsage\" with 3 s's? If it's so, then it's a bug on Google side and my application cannot parse the error.\n. If fusermount -u ... does not work, you should kill the process (with -9 if it is in uninterruptible sleep). This should be enough to force an unmount.\n. The issue occurred to @piccaso was solved fixing issue #80 (client_id and client_secret were not saved in configuration file, so refreshing the token was not possible).\n. You should turn on verbose logging (-verbose option) and then open gdfuse.log and look for lines like these:\n[17.122371] TID=0: flush /filename 0\nGetting metadata from context...valid\nLoading resource /filename (trashed=false) from db...found\nUploading file (cache path=/home/alex/.gdfuse/../cache/..., content type=...)...Refreshing remote resource (id=...,etag=\"...\")...done\ndone\nUpdating resource in db (id=...)...done\nUpdating cache size (...) in db...done\nUpdating context...done\n[21.922442]\nFrom the timestamps, you should be able to measure pure upload speed. That's the speed of the Google endpoint for file upload. If that speed is lower than you expect, there is nothing that can be done (I'm using curl to upload the file, so there is no/little overhead). But if that speed is comparable to 10Mo/s, then you can try to increase metadata_cache_time (to 600 seconds, for example) to avoid checking for server side changes too frequently (you are uploading files, so these changes will be notified, invalidating your cache, and that takes time). You cannot compare my application with Google Drive Client, because it uses a non-public API.\n. Unfortunately Drive API + FUSE is not an ideal combination when you need fast uploads. Maybe you can try this other project: https://github.com/odeke-em/drive (doesn't use FUSE, so maybe has faster uploads).\n. It cannot find the gapi-ocaml dependency. Have you tried installing with OPAM? It should handle dependencies in a nice way.\n. Do you have a 64-bit host?\nIf so, could you look at this thread: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=712126 to check if the bug you are reporting is somewhat related?\n. I've uploaded version 0.5.4 that has an option (curl_debug_off) as a workaround (it worked for some users, but I'm not sure it solves it for everyone). Try to set it to true and check if the problem persists. Otherwise I need a core dump to investigate further.\n. Closing as duplicate of issue #54\n. In theory, any exception is caught and an exit 1 is used to signal the abnormal termination. Could you please post the error message you obtain when it fails to mount?\n. OK, I see. That error is returned by the library I'm using, so it's not easy to fix. As a workaround, could you use the -o nonempty option?\n. If you use the -o nonempty option, FUSE will hide the files already present in that directory, so they won't be available until you unmount your filesystem. Therefore, if those are files used by the system or by some application, the system or that application won't work as expected. This is the risk associated with that option. But your script is just fine as a workaround.\n. Sorry, but this is not a sync app like the Dropbox one. When you unmount your Drive, you cannot access your files anymore.. google-drive-ocamlfuse gives you a view of your Drive. It doesn't download your files locally. If you need to copy your remote files you could use rsync from the mountpoint to a local directory. Or you can switch to a sync app (e.g.: https://github.com/Grive/grive).. Looks like some kind of low level race condition (or deadlock). I don't have explicit synchronization in my code (it should be single threaded). You should try to use the -debug and check the log files in ~/.gdfuse/default/ to see when this error occurs what was the last action performed by the application.\n. I'm closing this issue because there was no follow-up.\n. I tried on a VM running an up to date Ubuntu 12.04 32-bit (kernel 3.8.0-29) and it works as expected. Can you try uninstalling the .deb and compiling it from source with OPAM?\n. Could you post the output of ldd? You should get something like:\n$ ldd google-drive-ocamlfuse\nlinux-vdso.so.1 =>  (0x00007ffff5bfc000)\nlibsqlite3.so.0 => /usr/lib/x86_64-linux-gnu/libsqlite3.so.0 (0x00007faccf9e6000)\nlibz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007faccf7cd000)\n...\n. OK, it looks all right. If you enable debug logging, where does it crash?\n. > [4253:4253:0418/112127:ERROR:nss_util.cc(853)] After loading Root Certs, loaded==false: NSS error code: -8018\nI don't know if it's something related to nss library. Do you have curl installed? What if you try to run the command curl -v https://accounts.google.com/?\n. There must be some kind of conflict. But I have to reproduce it. Maybe if you could show me which packages (related to curl) you have installed:\n$ dpkg --get-selections | grep curl\nI can try to install them to see if I can reproduce your issue.\n. > By the way, if it helps you, and you cannot reproduce it on your system. I could install a virtual machine on which the problem occurs, then give you ssh access to it in order to troubleshoot the code.\nThanks, that would be helpful. I still cannot reproduce the problem on my virtual machine. If you want you can contact me at this email address: alessandro(dot)strada(at)gmail(dot)com\n. I'm sorry but I'm still unable to reproduce the problem.\n. Please verify that you are using the 64-bit version of google-drive-ocamlfuse, if you operating system is 64-bit.\n. Did you compile it using OPAM? Or do you use a prepackaged version?\n. Could you try compiling it using OPAM?\n. If you are on a 64-bit host, you should verify if all the dependencies are 64-bits (FUSE, curl). Otherwise you could try to send me a core dump to my email address (alessandro (dot) strada (at) gmail (dot) com), and I will try to figure out what it's the issue.\n. If you use bash, type this command before running google-drive-ocamlfuse:\n$ ulimit -c unlimited\nFor further information have a look at http://stackoverflow.com/questions/17965/generate-a-core-dump-in-linux\n. I've uploaded version 0.5.4 that has an option (curl_debug_off) to workaround the problem for some users (@lesebas). Try to set it to true and check if the problem persists. Otherwise I need a core dump to investigate further.\n. Try using the -debug command option, and the check the log files in ./gdfuse/default/. If Google Drive's backend returns some error, you should find it in curl.log. \n. It looks like your user does not have permission to access some of the resources. You should check curl.log file and search for 403 errors.\n. Check the config file, it should contain your client_id and client_secret. If not, try adding them.\n. Do you have a lot of Google Docs, Sheets and Slides? Those documents cannot be cached efficiently for a number of reasons. You can try to turn off documents download, editing the config file, setting the download_docs option to false:\ndownload_docs=false\nThis should make things faster.\n. OK. In this case, if you plan to have lots of client side operations, you can try to raise the cache timeout of metadata, e.g.:\nmetadata_cache_time=600\n(or even higher if you usually don't do server side changes). This will avoid excessive polling in that scenario.\n. Is it slow just the first time you open a file or even when you open the same file more than once?\nDid you try putting a big value in metadata_cache_time?\n. Do you have many Google Docs (office-like files)? Those files can't be efficiently cached, so they slow down a lot the resource fetching. If you don't need them on your desktop, you can turn them completely off, putting download_docs=false in the config file. Otherwise, you can turn your docs into links, putting:\ndocument_format=desktop\npresentation_format=desktop\nspreadsheet_format=desktop\ndrawing_format=desktop\nform_format=desktop\nin the config file, and then using -cc command line option to clear the cache.\n. Then I don't think you can gain more speed. The only thing you can do is try to measure the server side latency: you can enable debug output (with -debug) and every operation will be tracked with timestamps. This way you can see if it's slow because of the network.\n. > One question: While links are a good workaround, is it possible to allow fast directory listing\n\n(e.g. ls and find), and only perform the slow document fetch/conversion later when the file is\nopened? Maybe report a 1-byte file (or some other special flag, like a symlink or named pipe)\nuntil the file is actually opened?\n\nUnfortunately it's not easy because it requires opening files with O_DIRECT option, and the FUSE binding I'm using doesn't allow that. I'm keeping this issue open as a reminder.\n. The easiest thing that I can think of is running ls -lR before rsync to warm the cache. I don't know if it's quicker than running just rsync.\n. Well, not really. drive.files.list without filters can be quite slow if you have many files. You can check it enabling lost+found and querying that folder. The changes are already polled the way you are suggesting. Global cache invalidation occurs only if there are more than 50 changes between 2 calls to drive.changes.get.. > ... and a call to drive.changes.get happens every metadata_cache_time seconds right?\nYes, right.\n\nCurrently the first attempt to navigation after mounting is painfully slow but every following navigation attempt is quite fast... but after some time (maybe an hour or so) navigation is slow again.\n\nIt can be slow if you have lots of Google Docs. As they are exported, I've have to download them fully to get their size (otherwise the will be shown as zero-bytes). If you don't care about exporting documents, you could switch formats to desktop, or turning them off setting download_docs=false in the config.. I've uploaded packages for trusty. Let me know if there are any issues.\n. Are you using your custom client_id and client_secret? If it is the case, you should check the config file and look for the keys client_id and client_secret. They should contain your values. If they are empty you will get that kind of error.\n. It's probably a bug. I will look into it.\n. It looks like it's segfaulting when fetching some HTTPS urls. I'm investigating this problem. See issue #54.\n. Closing as duplicate of #106.\n. You should look at the log files in ./gdfuse/default. There are 2 log files: gdfuse.log that traces filesystem activity, and curl.log that traces network activity. The output you get from the -d flag is the low level fuse log. Moreover, I would avoid the -m flag, because in my tests, it didn't work very well.\n. > I see that symbolic links are not supported.\nYes. That's because links are not directly mappable in Google Drive.\n. I'm sorry, but this is not a sync tool. It just lets you mount your Google Drive as a local filesystem, but you have to be online as there is no offline mode. You should try Grive if you want a sync tool for linux.\n. When you have to upload so many files, you should modify the configuration, editing the config file, updating the value metadata_cache_time, putting a very big value (e.g. 36000, that's 10 hours). Otherwise every 60 seconds, the server will be queried to get the modified files and directories, invalidating the cache. I don't know if this is enough to solve your problem, but it surely will help. The mtime is obtained from the Drive API. If you enable debug logging (with -debug flag) and check the file curl.log, you can search for \"modifiedDate\" and you will see what the server is returning. That date is cached in the sqlite3 cache (in ~/.gdrive/default/cache/cache.db). You can see the cached values using a query like this:\nsqlite> select path, modified_date from resource;\nHope this helps.\n. The mtimes will be stable al least until the cache is invalidated, i.e. after metadata_cache_time expires. If you put there a very big number you should not have problems. Then the mtimes will be updated with the values returned by Google Drive. In theory, it should keep your values, but I'm not 100% sure.\n. OK, I see. So Google Drive overrides the modified time value, with the upload time. Therefore the only safe option is to use --size-only.\n. I'm closing the issue because it should be fixed in the next release (0.5.5) along with issue #81 that was causing the main problem (the mtime was incorrectly set to the one of the cached file after download).\n. I'm sorry but I don't think this feature will be easy to implement. I tried to make everything as lazy as possible avoiding to download unnecessary files and metadata. If I implement the offline mode as is, the risk is to be unable to access the majority of files, unless you explicitly download them all. Have you considered using Grive? It's a tool to sync your Drive content to a local directory.\n. The problem is that you don't know in which state a cache entry is. And any entry can be invalidated at any time. But you can do an experiment. Put a very large value in the metadata_cache_time, like 100000000 seconds. This way you are sure that the server will not be polled for changes (for al least 100 days). Then do some kind of rsync to a temp dir (that you can remove later) to trigger the download of all the files. This should simulate an offline cache. Let me know it this works for you.\n. If the changes are client-side changes (and you aren't online) you will get an error. If the changes are server-side, you won't see them. You should consider your offline copy as a read-only mirror of your Drive contents. That's because a FUSE filesystem is a standard filesystem, it does not offer primitives for remote (or distributed) filesystems.\n. There is another project to access Google Drive that should work offline.\n. You should enable debug logging (-debug option) and have a look to the log files in ~/.gdfuse/default (gdfuse.log and curl.log) to see what's going wrong.\n. I'm closing this issue because there was no follow-up.\n. > sh: /lib/cpp: No such file or directory\nIt looks like it cannot find cpp. I don't know why it's searching for it in /lib.\n. I don't know. Maybe there is some kind of problem with OCamlMakefile. Can you try adding a temporary soft-link from /usr/bin/cpp to /lib/cpp?\n. Nope, sorry.\n. It looks like the old names are still cached. Please try to clear the cache, using -cc. It should be enough to regenerate the filenames.\n. I think the problem may be the single threaded nature of the FUSE binding I'm using. Unfortunately the multithreading option (-m) is unstable (usually causes a segmentation fault). Perhaps you can try to use nice to lower the priority of the process (try to use nice with both google-drive-ocamlfuse and rsync).\n. Then, I think you can try to see if the -m option works for you.\n. I don't know. Maybe you can try putting very big numbers in metadata_cache_time and max_cache_size_mb configuration values.\n. Sorry, I never tried anything like that. I don't know what the problem could be.\n. Thanks for your feedback.\n. Ubuntu 14.04 32-bit or 64-bit? Please verify that you are using the correct binary (another user experienced a segmentation fault because he was running a 32-bit executable on a 64-bit host). Then start from scratch removing the .gdfuse directory: \n$ rm -r ~/.gdfuse/default\n. If you are on a 64-bit host, you should verify if all the dependencies are 64-bits (FUSE, curl). Otherwise you could try to send me a core dump to my email address (alessandro (dot) strada (at) gmail (dot) com), and I will try to figure out what it's the issue.\n. If you use bash, type this command before running google-drive-ocamlfuse:\n$ ulimit -c unlimited\nFor further information have a look at http://stackoverflow.com/questions/17965/generate-a-core-dump-in-linux\n. @tonsV2: Yes, please, if it's not a problem, please send the core dump to my personal email. Thanks!\n. In version 0.5.4 I've added an option (curl_debug_off) that can be set to true to workaround the problem causing a segmentation fault on some boxes. If this doesn't solve the issue, let me know and please provide a core dump (or a trace.log generated by gdb).\n. Closing as duplicate of issue #54\n. Thanks for your feedback. I will add a FAQ/Troubleshooting section in the wiki.\n. -o nonempty should work. Do you still get an error adding it?\n. > Hi. I used \"google-drive-ocamlfuse -label myname /home/gdrive -o nonempty\".\n\nBut this command deleting my existing files .\n\nNot really. Your files are temporarily hidden by the mounted file system. When you unmount it, they will be available again.\n\nIs there a way to not delete but sync all together.?\n\nNo, you cannot merge an existing directory (on your local file system) with a FUSE file system. This is actually not a sync application, but it gives you a local view of your remote files on GDrive.. I'm thinking about implementing this feature adding a configuration key as you suggest, and a command line option to be extra-safe.\n. Those errors mean that the application cannot refresh the auth token. Did you use the custom authorization mode with your client_id and client_secret? If so, please check that in the configuration file the keys client_id and client_secret are correctly populated.\n. The problem is that auth tokens expire after 60 minutes. Probably there is a bug that prevents the application from saving your client_id and client_secret in the configuration file. Please check if those values are saved in the file /root/.gdfuse/rpi1_backup/config and add them if the keys client_id and client_secret are empty.\n. Thanks for your feedback\n. Lynx won't work, because Google's OAuth process requires a browser that fully supports Javascript, and currently there is no text-based browser that will allow you to get a valid oauth token. Have you considered the Headless usage option?\n. Try using OPAM (see https://github.com/astrada/google-drive-ocamlfuse/wiki/Installation#installing-with-opam)\n. No, it should not. Did you try again?\n. -debug will activate tracing. You will find 2 log files: curl.log (with network data) and gdfuse.log (with application trace) in ~/.gdfuse/default\n. Please try starting from scratch removing all the files in ~/.gdfuse/default.\n. I don't know. If the error is when you open the authorization page on google, then the problem is not logged (because the error is in the browser opening a page on google server). Would you try the headless mode?\n. I'm closing this issue because there was no follow-up.\n. You can try to increase metadata_cache_time. This way there are less server requests e the cache will not be invalidated too frequently.\n. You can try setting async_upload=true, or you can try the new beta version (0.6.0). that you can install from a new PPA, if you are on Ubuntu.. I think the problem is that FUSE uses blocks of 128KB (at maximum), so you have to wait for a (relatively slow) copy to end before the upload begins. If your use case requires frequent uploads of very big files, you should go with https://github.com/odeke-em/drive. You can then use my app to stream the uploaded files (if they contain videos, for example).. OK, but from the source code you linked:\n\nLimitations\nThis can only write files seqentially, it can only seek when reading.\nRclone mount inherits rclone's directory handling.  In rclone's world\ndirectories don't really exist.  This means that empty directories\nwill have a tendency to disappear once they fall out of the directory\ncache.\n\nI think that rclone has a simple in-memory model, so it's optimized for uploading but has those limitations.. Please, check in you config file to verify that you have your client_id and client_secret specified. They are not saved by default (it's a bug).\n. I set the file's mtime using the modifiedDate field of Drive API. You can see what the API returns, using the -debug option and looking into the log file ~/.gdfuse/default/curl.log. Or you can open (using the sqlite3 client) the cache DB and run this query:\nsqlite> select title, modified_date from resource;\nYou will get a list of all files with their mtime (expressed in seconds from the epoch).\n. I've found the problem. I was incorrectly showing the mtime of the cached file once downloaded. This should be fixed in the next release (0.5.5).\n. In the new web interface of Google Drive, shared files are grouped under the Incoming tab. Did you use the \"Add to My Drive\" button on the files you want to access on your mounted folder?\n. I'm closing this issue because I think I've solved this problem fixing issue #81.\n. Sorry but I cannot reproduce your issue. Could you specify the order of the commands you are running? And rsync options you are using?\n. Sorry, but I wasn't able to reproduce your issue on Ubuntu 14.04 64-bit.\n$ mkdir utf_test\n$ cd utf_test/\n$ echo \"test A\" > \"D\u00f3nde est\u00e1n las llaves.mp3\"\n$ ls -l\ntotal 4\n-rw-rw-r-- 1 alex alex 7 Sep 15 22:37 D\u00f3nde est\u00e1n las llaves.mp3\n$ cd ..\n$ google-drive-ocamlfuse -cc -debug ~/tmp/gdrive\n$ rsync -rlP --delete --size-only --inplace utf_test ~/tmp/gdrive/\nsending incremental file list\nutf_test/D\u00f3nde est\u00e1n las llaves.mp3\n          7 100%    0.00kB/s    0:00:00 (xfr#1, to-chk=0/2)\n$ cd ~/tmp/gdrive\n$ ls -lR utf_test\nutf_test:\ntotal 1\n-rw-rw-r-- 1 alex alex 7 Sep 15 22:37 D\u00f3nde est\u00e1n las llaves.mp3\n$ cd ..\n$ fusermount -u ~/tmp/gdrive\n$ google-drive-ocamlfuse -cc -debug ~/tmp/gdrive\n$ cd gdrive\n$ ls -l utf_test\ntotal 1\n-rw-rw-r-- 1 alex alex 7 Sep 15 22:37 D\u00f3nde est\u00e1n las llaves.mp3\n$ cd /tmp/\n$ fusermount -u ~/tmp/gdrive\n$ google-drive-ocamlfuse -cc -debug ~/tmp/gdrive\n$ rsync -rlP --delete --size-only --inplace utf_test ~/tmp/gdrive/\nsending incremental file list\n$ cd ~/tmp/gdrive\n$ ls -lR utf_test/\nutf_test/:\ntotal 1\n-rw-rw-r-- 1 alex alex 7 Sep 15 22:37 D\u00f3nde est\u00e1n las llaves.mp3\n$ cat utf_test/D\u00f3nde\\ est\u00e1n\\ las\\ llaves.mp3 \ntest A\n. OK, thanks for your feedback.\n. There is an issue with timestamps. What rsync options are you using?\n. This issue is probably the same caused by #81 and #83.\n. Yes, if you are using --size-only the timestamp issue won't affect you.\n\nThere is still heavy upload network activity, so it looks like the file is just cached somewhere and\nkeeps being transferred for hours after rsync says it is done.\n\nYes, it works like that. FUSE writes files in (small) chunks but GDrive API doesn't allow to upload file fragments. The file must be uploaded in a single request. So my app writes the file chunk by chunk in the cache and uploads it when rsync closes it. About the irregular upload rate, I don't know what the problem is: my app just uses libcurl, so it's like uploading the file using the curl command. Maybe you can use -debug and check the real transfer rate, checking timestamps in the log file.\n. > curl.log seems binary and superverbose and it is 1.8GB by now.\nYes, in debug mode it dumps every byte of the request (so it contains a dump of your uploaded file).\nThere are some errors in gdfuse.log:\n\nUploading file (cache path=/home/luisca/.gdfuse/default/cache/0B-dWp7gW1386UHNBOHQwQ2lCRVU, content type=video/x-msvideo)...Exception:Failure(\"Cannot access resource: Refreshing token was not enough\")\nUploading file (cache path=/home/luisca/.gdfuse/default/cache/0B-dWp7gW1386UHNBOHQwQ2lCRVU,\ncontent type=video/x-msvideo)...Exception:Failure(\"Cannot access resource: Refreshing token was not enough\")\n\nProbably there is a bug that prevents token refresh during file upload. I have to see if I can reproduce it.\n. OK, I think I've found the problem and unfortunately it seems to be a Google Drive issue. The problem is that if the upload lasts more than one hour it fails (server side) because the access token is expired. My app will try to re-upload it but it will never succeed. It works only if the file can be uploaded in less than an hour.\n. I've subscribed the various threads on code.google.com reporting the problem. When there is any news, I will update this issue.\n. I just made a test and it looks like it's fixed. A new release is not needed because the bug was on Google side. Please let me know if you are still experiencing problems uploading big files.\n. Does it crash (segmentation fault)?\n. Sure. If you run\n$ google-drive-ocamlfuse -debug\nthen you can check in ~/.gdfuse/default these files: gdfuse.log (application log) and curl.log (gdrive API log).\n. OK, thanks for your feedback.\n. Check if there is something in the directory ~/.gdfuse/default. If so, remove everything and relaunch google-drive-ocamlfuse. Otherwise check the terminal to see if the program returns some kind of error. You can also use -debug to produce log files.\n. I'm closing this issue because there was no follow-up.\n. > Any plans for other cloud storage options (Dropbox, Box, etc.)?\nNo, sorry. It would require a lot of spare time that I don't have.\n. No, there isn't an option like that, because this in not a synchronization tool but a remote filesystem. It downloads files on demand only (on your local filesystem there is only the file cache).\n. > Edit 1: Looks like it might be related to firefox while opeing the authentication page.\nYes. I don't use glib.\nThanks for your feedback\n. Sorry, it's a regression bug of the last version (0.5.8). I will try to fix this as soon as possibile\n. This should be fixed in version 0.5.9. Let me know if this doesn't solve the problem for you. Thanks\n. No, you don't need Drive SDK, because google-drive-ocamlfuse is a stand-alone application. It doesn't integrate with drive UI, and it isn't a Chrome App.\n. OK, thanks\n. I'm sorry, but the FUSE binding I'm using doesn't return an error code in that case, so I'm not able to return a non-zero value.\n. Unfortunately ocamlfuse is no longer maintained. I'm waiting to switch to profuse, i.e. an updated FUSE binding, but it's not yet completed.\n. Please, check if this StackOverflow question is related.\n. I'm closing this issue because there was no follow-up.\n. Check if you have installed fusermount or mount.fuse (fuse package in Ubuntu, fuse-utils package in Debian)\n. Yes, the documentation still points to an old version. To obtain MS Power Point files you can use pptxformat (instead of ppt). Changing the export formats requires clearing the cache (with -cc). I should put it in the docs. If you still experience problems, start the program with -cc -debug and please send me the log files (removing sensitive information) to my personal mail alessandro (dot) strada (at) gmail (dot) com. Thanks\n. > Cannot refresh access token. Quitting.\nThis error should not be related to the Drive API issue. It means that the application cannot refresh the auth token. Are you using the latest version? (There were bugs in previous versions that in some cases caused this issue). What kind of authorization method are you using (proxy or custom client_id/secret)?\n. Check that your configuration file contains your correct client_id/secret\n. Files are downloaded to the cache only if needed. So if you don't copy files under other directories other than \"data\", you will not download them. If you want to be sure that the local cache doesn't exceed a specific quota, you can use the max_cache_size_mb option to limit cache size and if you still want to access bigger files, you can set stream_large_files to true and use large_file_threshold_mb to choose which files you want to cache and which files you want to stream.\n. > Based on @hynese suggestion where can i find the drive.ml on my install?\nIf you are using opam, you can clone the repository locally, edit the src/drive.ml file (https://github.com/astrada/google-drive-ocamlfuse/blob/master/src/drive.ml#L44) at line 44, instead of \"root\" you should enter the folder id of the folder you want to mount. To get the folder id, you can open the Drive web interface, go to the said folder and note the id that is the string that is after the last slash in the URL. Then with opam pin add google-drive-ocamlfuse . -n (from the directory where you cloned the repository) you can select you local version. opam update && opam upgrade should compile and install it. If it doesn't work, opam remove google-drive-ocamlfuse && opam install google-drive-ocamlfuse should do.. If you want you can install opam in a custom directory (without sudo). Check this link and replace /usr/local/bin with a directory you have permission to write to.. > Any plans on adding something like this?\nYes, I just have to find some spare time to add this feature. :-). I just published version 0.6.20 (on the beta channel) where I added a root_folder config option where you can specify a folder id or a remote path to the a custom root folder.. You can insert a folder id or a path. For example root_folder=0B-TenPiSMBplOGg4eXR1c3U0RTg or root_folder=/folder/f1.\n\n. > i assume the regular gdrive api can't reach those?\nYes, right.. I'm using libcurl. From the docs, it looks like the environment variables are supported. If not, I will add the configuration options.\n. OK, thanks for the logs. I think I see where the problem is. Sometimes GDrive API returns an old etag, so subsequent requests fails (that's what is causing the PreconditionFailed exception). To fix this issue I'm going to retry the upload in the exception handler.\nNote that to preserve the right mime type, you should use rsync --inplace otherwise the web interface won't correctly preview uploaded files. You should also modify conflict resolution strategy to conflict_resolution=client. This will solve the issue you got in the second example:\n\nRenaming file (id=0BzN0R8OApi9oNzFiRl9HTWZVRnc) from profile.xml.tmp to profile.xml...Conflict detected: Keeping server changes.\nDeleting resource (id=36) from cache...Resource busy: rename /2014-11-17_21_12_42/profile.xml.tmp\n. encfs reverse mode is only a virtual view on your filesystem (i.e. the FUSE filesystem is not materialized on your disk.) You should then rsync the view to GDrive, e.g.:\n\n$ encfs --reverse ~/tmp/enc/decr ~/tmp/enc2 -o nonempty\n$ rsync -av --inplace ~/tmp/enc2/ ~/gdrive/enc/\n. It is probably a bug in libgcrypt11:\nhttps://bugs.launchpad.net/ubuntu/+source/libgcrypt11/+bug/874307\nhttps://bugs.debian.org/cgi-bin/bugreport.cgi?bug=643336\nIf you can downgrade to a previous version (or upgrade to a newer version), maybe you can solve this isssue (see also http://opensips.org/pipermail/users/2011-October/019422.html).\n. Closing as duplicate of #106.\n. No, unless you select the files you want to see and press \"Add to My Drive\". That is how Google Drive works: you cannot access your incoming files from the API unless they are added to your Drive.\n. Which version of libgcrypt11 do you have?\nThere is an outstanding bug in version 1.5.0 that causes segmentation faults.\n. Try using -o allow_other, e.g.:\n$ google-drive-ocamlfuse -o allow_other /var/www/google-drive\n. PATH=$PATH:$HOME/.opam/system/bin export PATH\nTry expanding $HOME. If the user running /usr/bin/gdfuse is not your user, $HOME will not match with your home and the executable will not be found.\n. I think you should modify /usr/bin/gdfuse this way:\n```\n!/bin/bash\n/home/[... your user name ...]/.opam/system/bin/google-drive-ocamlfuse -lable $1 $*\nexit 0\n```\n(be sure to replace [... your user name ...] with your actual user name).\n. As an alternative, check out this page: https://github.com/astrada/google-drive-ocamlfuse/wiki/Automounting \n. Unfortunately, it looks like OAuth 2.0 for TV and Limited Input Device Applications doesn't support Google Drive: https://developers.google.com/identity/protocols/OAuth2ForDevices#allowedscopes\n. Did you try clearing the cache (with google-drive-ocamlfuse -cc)?\n. I think there is some issues with new spreadsheets. It looks like the same problem is experienced by other users:\nhttp://stackoverflow.com/questions/28028031/google-drive-web-api-does-not-return-open-office-export-link-for-new-spreadsheet\nI will try to see if I can download open office formats document anymore.\n. I've opened issue 3713 on Google APIs bug tracker.\n. I've uploaded version 0.5.13 with a workaround that should re-enable .ods download even for new Sheets.\n. No, sorry. There is no way to access Google+ content using Google Drive API. And even using Google+ API you won't get access to your private content (just to your published public posts, AFAIK).\n. Now it is possible to access and organize photos via Drive. If you add photos to your Drive, you will be able to access them via google-drive-ocamlfuse.\n. If you enable verbose logging (with -debug), you should see lines like this in your gdfuse.log:\nDownloading resource (id=17)...Updating cache size (200) in db...done\nCache is updated downloading resources, writing files, or when a change is detected server side. Note that Google Docs are not cached.\n. > I checked that there is a stream_large_files=true option but I guess it's for download/playback use. \n\nAlready had that option at \"true\" but didn't help my upload problem.\n\nYes. It's just for download.\n\nOr maybe I should just setup a 100G VDI for the cache to use whatever it wants?\n\nUnfortunately, Google Drive API doesn't allow to upload files in chunks. So I have to cache the whole file before uploading it. And there is no workaround. So the only option is to have a virtual disk big enough.\n. Yes, you are right, and I'm using, in fact, resumable uploads. The problem is that you cannot make partial updates to the byte stream. For example, if I modify only few bytes of a file, I have to re-upload the whole file (even in chunks). I cannot upload just the affected chunk.\n. Do you have these lines in ~/.profile?\n```\nOPAM configuration\n. $HOME/.opam/opam-init/init.sh > /dev/null 2> /dev/null || true\n```\nWhat's the content of file ~/.opam/opam-init/variables.sh?\n. Then OPAM will build your executables in /home/Bertie/.opam/4.01.0/bin. Check that, in the folder, you find google-drive-ocamlfuse executable. Then check that your $PATH environment variable contains OPAM bin dir (running echo $PATH).\n. What's the output of command:\n$ which google-drive-ocamlfuse\n. And when you run:\n$ google-drive-ocamlfuse\ngives you \"command not found\"?\n. Sorry, but what do you mean when you say that it's not syncing?\n. Have you tried running the command with -debug command line option, and then check the log files?\n. OK, no problem.\n. > I cannot find the right config file and my cache is growing HUGE overtime... The one in .gdfuse/default/config is set to 512 MB, but my cache was 17 GB after 2 months. Is it somewhere in /etc?\nNo. If you have just one account, .gdfuse/default/config is the only config file used. If you enable logging with -debug you should see messages like\nUpdating cache size (...) in db...\nwhen cache is evicted. Unless you have very big files (multi GB), cache should not grow too much. In that scenario (when you have very big files) the only way to keep a small cache is to activate file streaming in config (e.g.):\nstream_large_files=true\nlarge_file_threshold_mb=16\n. > Is there any drawback in streaming?\nNo, unless these files are modified, because when you modify a file it has to be cached anyway. Streaming is mostly useful for media files.\n. > Do you mean 'unless the files modified -during- the copy process?'\nNo, any time a file is modified (client-side), it has to be downloaded to the cache and then fully uploaded, because Google Drive API does not support partial updates (i.e. even if you modify a single byte of a file, that file must be re-uploaded in its entirety).\n. I think it would work. Googling \"fuse rc.local\" shows that similar scripts are used to mount sshfs at boot, so there should be no problem.\n. -m command line option activates multi-threading. Unfortunately the FUSE library I'm using is not very stable with that option turned on.\n. Pre-release 0.6.0 should address at least part of this issue. If you are on Ubuntu you can add the beta PPA, to check out this version.\n. Closing this issue as the new version is out of beta.. Could you send me your config file to alessandro (dot) strada (at) gmail (dot) com, please? It looks like an error in the config file parser (it's the only place where I use int_of_string).\n. Should be fixed in v0.5.15\n. Sure! Feel free to update the wiki. Thanks!\n. I checked the source code of KeePass 2 and I think that what you have experienced is the default behavior when saving the kdbx file. The DB is saved to a new temporary file, then renamed to match your original filename. There should be an option to turn off this behavior: in Tools -> Options -> Advanced there is a checkbox labeled Use file transactions for writing databases. You should try unchecking this option.\n. Perhaps I'm getting a 400 response because I try to upload a 0-byte file. I will do a couple of tests. Thanks for reporting this issue.\n. > This may be a problem in Google Drive API but wanted to know is there a way to set a create time?\nI don't think it's possibile. I just did a test using Google API Explorer.\nThis is the request, where I try to set the creation date:\n```\nPOST https://www.googleapis.com/drive/v2/files\n{\n \"title\": \"test.txt\",\n \"createdDate\": \"2015-01-01T00:00:00.00Z\"\n}\n```\nAnd this is the response, but the creation date was overwritten with the date of the request:\n```\n200 OK\n{\n \"kind\": \"drive#file\",\n ...\n \"title\": \"test.txt\",\n \"mimeType\": \"text/plain\",\n ...\n \"createdDate\": \"2015-06-07T11:51:56.124Z\",\n \"modifiedDate\": \"2015-06-07T11:51:55.846Z\",\n \"modifiedByMeDate\": \"2015-06-07T11:51:55.846Z\",\n \"lastViewedByMeDate\": \"2015-06-07T11:51:55.846Z\",\n \"markedViewedByMeDate\": \"1970-01-01T00:00:00.000Z\",\n ...\n}\n``\n. No sorry, there is no parameter to set the creation date (https://developers.google.com/drive/v2/reference/files/update).\n. No, you should see all the files every time you runls. Have you tried turning on debug log and then check if~/.gdfuse/default/gdfuse.logor~/.gdfuse/default/curl.logreport any error?\n. I think I will add a configuration option per file-type (e.g.spreadsheet_icon) so that you can specify any icon you want: a system icon or a custom icon (that you can put in~/.local/share/icons/...). Hope that works for you.\n. Should be available in version 0.5.17. Remember to clear the cache after changing icons inconfig` file.\n. I usually upload the .debs as I publish a new version. I think the PPA should already be updated.\n. Don't worry. No need to apologize. :smile: \n. Thanks! I'm glad you find this project useful.\n. > libgcrypt11 version is 1.5.3-2ubuntu4.2\n\ncould be a duplicate of #106\n\nI think so. It should be fixed in libgcrypt11 1.5.4.\n. > Is this a restriction with Google Drive?\nNo, there is no such restriction. I think the problem is that FUSE is not optimized to stream remote contents. Maybe you can play with FUSE options. But unfortunately I don't think there is an easy solution.\n. Well, it's not easy. The file name in the cache directory is the file ID in Google Drive. It is saved in column remote_id of table resource in the SQLite DB.\n. I don't know if this applies to you, but maybe this link is useful: http://permalink.gmane.org/gmane.comp.file-systems.fuse.devel/14350\n. > I checked a few other Fuse packages and don't seem to have the same restriction, is it possible it could be related to OCaml?\nProbably. The FUSE binding I'm using defines FUSE_USE_VERSION 26, so maybe it uses an old version of the API that doesn't support custom sized chunks. I can make a couple of test upgrading FUSE but I don't know if it introduces incompatibilities.\n. Sorry, I didn't have time to test a recompiled FUSE module because I have to recompile the kernel too.\n. I looked into libfuse source code, and it seems that FUSE_USE_VERSION 26 is high enough to use the newest API.\nFrom fuse-2.9.4/include/fuse.h:\n/** @file\n *\n * This file defines the library interface of FUSE\n *\n * IMPORTANT: you should define FUSE_USE_VERSION before including this\n * header.  To use the newest API define it to 26 (recommended for any\n * new application), to use the old API define it to 21 (default) 22\n * or 25, to use the even older 1.X API define it to 11.\n */\nMaybe the problem is MIN_BUFSIZE (see http://sourceforge.net/p/fuse/mailman/message/19952379/). If you have time, maybe you could recompile libfuse with a larger value for MIN_BUFSIZE. Thanks for helping out!\n. > Sorry, I just re-read that last post, and it sounds a little negative! :)\nNo problem. :)\n\nI'm going to tinker with as many things with Fuse as I can, will get back with my findings.\n\nThanks for your time!\n. I think the problem is that FUSE reads at most blocks of 128KB, so probably there is a lot of latency streaming vs downloading to the cache. I have to test if there is some way to further increase the block size.\n. Unfortunately, it seems that getting a larger block size requires a kernel recompilation (see http://comments.gmane.org/gmane.comp.file-systems.fuse.devel/14335). So I don't think it's a viable solution. I think I have to implement an in-memory buffer, but it's not easy because it needs to support multi-threading mode (so concurrent access, out of order reads/writes), and there is the risk of using too much memory, so it needs some policy to free resources.\n. Nice, thanks. It looks like you have to modify at least FUSE_MAX_PAGES_PER_REQ (https://github.com/torvalds/linux/blob/master/fs/fuse/fuse_i.h#L27). In the thread on comp.file-systems.fuse.devel, someone tried to set it to 2560 (10MB). Probably you have also to modify MIN_BUFSIZE (https://github.com/libfuse/libfuse/blob/6adcb719a933a31013c73fda8e0ccb0e13b45e58/lib/fuse_kern_chan.c#L86) in libfuse.\n. And this post suggests to update VM_MAX_READAHEAD too (https://github.com/torvalds/linux/blob/710d60cbf1b312a8075a2158cbfbbd9c66132dcc/include/linux/mm.h#L2053).\n. Yes, buy GDriveF4JS uses an in-memory buffer to optimize streaming performances. I'm not saying that's not feasible, but that building a robust in-memory buffer is not easy.\n. Great! Thanks!\n. > No luck increasing the values, is it possible there's something in ocaml-fuse that can be tweaked? \nI don't think so. Did you also modify MIN_BUFSIZE (https://github.com/libfuse/libfuse/blob/6adcb719a933a31013c73fda8e0ccb0e13b45e58/lib/fuse_kern_chan.c#L86) in libfuse?\n\nIf I use streaming, the cache.db increases, is this caching directory structure and file size etc.?\n\nYes, the sqlite db stores file entries from Drive API.\n\nIf so, what is the TTL of the cache, and/or file/directory listings?\n\nThere is no hard TTL. File entries are deleted when some kind of changes are detected (from the API).\n\nIf I mount read only, and add a file directly to Google Drive, will the new file be picked up instantly? If not, how long will it take for the file to appear?\n\nIt depends on metadata_cache_time in the configuration file. It's expressed in seconds.\n\nIf I use -m, what's the maximum threads I can reach?\n\nThere is no limit (and no pooling). So it depends on the application accessing the filesystem. Usually there are not many concurrent threads.\n. > Also, debug seems to kill it eventually, the curl log was >1GB\nYes. With -debug turned on, it dumps every byte from/to GDrive to curl.log. So if you are streaming video/music, it gets in the way. -verbose should keep gdfuse.log but not dump CURL requests/responses.\n. I created a new PPA with a new version (0.6.0) of my application that should address some of the issues reported. Mainly, it enables multi-threading by default, and add a read-ahead buffer when streaming (stream_large_files=true), using new config options: memory_buffer_size (defaults to 1MB) to specify the minimum size of download buffers, read_ahead_buffers (defaults to 3) to pre-fetch blocks of the same file, and max_memory_cache_size (defaults to 10MB) that specifies the maximum memory occupation of read-ahead buffers (before it starts to deallocate the oldest ones). You should consider this a beta version. I'm not yet publishing it to the main PPA until I've tested it a bit more. If you don't use Ubuntu, let me know, so I can give you instructions on how to install it. Anyway the code is in the new beta branch.\n. With OPAM, you can compile the beta version this way:\nopam pin add google-drive-ocamlfuse https://github.com/astrada/google-drive-ocamlfuse.git#beta\nopam upgrade google-drive-ocamlfuse\n\nThen, to restore the standard version:\nopam pin remove google-drive-ocamlfuse\nopam upgrade google-drive-ocamlfuse\n\n. @zenjabba can you please send gdfuse.log and curl.log to my email (alessandro.strada@gmail.com)?. > After quite a bit of testing, I've had no problems at all. Streaming full Bluyray backups with no hiccups.\nGreat!\n\nWhen you have the time, could you give a bit more detail on how the config options work?\n\nSure. These new options work only if stream_large_files=true. Basically, files to stream are split in blocks of memory_buffer_size bytes (if it's above 0), and read_ahead_buffers specifies how many parallel threads are spun to pre-fetch next blocks. For example, using the default values (with memory_buffer_size = 1MB and read_ahead_buffers = 3), when you access the first bytes of a file, my app starts downloading the first block (of 1MB) and spins 3 new parallel threads to fetch blocks 2, 3, and 4. Then, when you access the second block, my app starts downloading block 5 (blocks 3, and 4 are already downloaded/downloading), and so on. Finally max_memory_cache_size specifies how many blocks can be kept in memory at the same time. So (using the default value of 10MB), in the example, when you access the 8th block, my app starts downloading block 11, and deallocates block 1. This way, I'm optimizing sequential streaming of big files, but making other access patterns (random seeks/reads) slower. Keep in mind that max_memory_cache_size is global and not per file, so if you stream multiple files concurrently, you should probably raise that value. Let me know, if you need further details.. When streaming, each block (of memory_buffer_size bytes) requires an API request, if not cached.\n\nI'm just wondering if the Google API limits could become a problem.\n\nI don't think so. The limit is 1 billion requests/day.. During testing, I never hit that limit. Anyway, if a request returns a 403, it is retried with exponential backoff, so this should not be an issue.. Where does homebrew install libfuse headers (fuse.h)?\nYou should put that dir in CPPFLAGS (overriding the default location) to correctly install ocamlfuse like this:\n$ CPPFLAGS='-DFUSE_USE_VERSION=26 -I/usr/local/include/osxfuse' opam install ocamlfuse\n. It looks like OSX does not have the header file sys/vfs.h (see https://github.com/kartverket/fyba/issues/12). Maybe it can be replaced by sys/mount.h. Unfortunately I don't have an OSX box, so I don't have a way to test possible workarounds.\n. Should be fixed by astrada/ocamlfuse#3.. I added mount option gdfroot to specify that path. You can use it on the command line with -o (e.g. google-drive-ocamlfuse -o gdfroot=/path/to/dir) or in /etc/fstab, e.g.:\ngdfuse#default  /mnt/gdrive     fuse uid=1000,gid=1000,gdfroot=/path/to/dir     0       0\n. From the logs it looks like process was stopped after about 11 seconds. Did you stopped it with Ctrl+C? Or with fusermount? What's the state of your mount point? Try running ls -l /home/user/drive.\n. If you used -debug, you should find all the requests to Drive API in curl.log. If it's getting all the metadata the file should grow quite big. If it's empty there is another problem.\n. OK. I've updated the wiki.\n. I'm sorry but this application doesn't work offline. You should use another tool (e.g. https://github.com/odeke-em/drive) to keep your files even when you are not online.\n. No, sorry. Google Drive API doesn't support uploading a chunk of a file. You have to fully upload it every time.\n. No, FUSE works with file primitives, in this case: write buffer where buffer is 132KB long. The FUSE driver doesn't receive the original file, just some bytes of it, so It has to cache it all before uploading. If GDrive API supported uploading a chunk, I could upload just the buffer and I would not need to cache it.\n. Are you mounting it automatically via /etc/fstab? If so, try adding big_writes to mount options. E.g.:\ngdfuse#default  /mnt/gdrive     fuse    uid=1000,gid=1000,big_writes     0       0\n. No, I just added an option, but it should not affect normal use. I think the problem is that when you upgrade from one version to another, the application clears its cache. Try mounting your drive, then from the command line, run (in the directory where you mounted your drive):\n$ ls -R\nThis command scans recursively all your files (it will take some time), so that the cache can be rebuilt. Maybe this helps.\n. Have you tried setting curl_debug_off to true in config?\n. OK, thanks for your feedback.\n. You should use desktop format for Google Documents you want to access via web. Then you can click on the shortcut from the file explorer to open the browser pointing to your doc.\n. OK, thanks! I'm adding it to the wiki.\n. > Is there a guide?\nNo, sorry.\n\nIs it even possible?\n\nI think so, but I don't think it's easy. You have to compile everything on the NAS because it's based on PPC/ARM. Wouldn't it be easier to install google-drive-ocamlfuse on a standard Linux box and set a cron job to backup your Drive on the NAS? I don't think that mounting Drive directly on the NAS brings any benefit.\n. I published packages for armhf architecture (on Ubuntu) in the PPA. Maybe you can use them on Synology NAS. \n. The official Google Drive client effectively synchronizes files from cloud to pc, while google-drive-ocamlfuse just gives you a view of your cloud filesystem. As there is no physical copy on your pc of your files in the cloud (if you ignore the cache copy that is used just for performance reasons), you wouldn't be able to access \"blacklisted\" files. If you need a sync tool, maybe you can check out drive or grive2.\n. Yes, you are right, but I chose consistency over responsiveness because my main use case is to backup my Drive locally. If you need to access files while offline, maybe you should use a sync tool, e.g. drive or grive2.\n. If you are using opam, you should find binaries under ~/.opam/system/bin or something like that (anyway a subdir of ~/.opam). Maybe you should run eval $(opam config env) to add that directory to the PATH variable.\n. What is the output of opam list?\n. OK, if you installed opam with sudo, you should find binaries here: /root/.opam/4.02.1/bin. But probably you should reinstall them without sudo (with your standard user), so that you will find binaries in ~/.opam/4.02.1/bin, and you will be able to use them without sudo.\n. Have you checked this page on the wiki?\n. If you enable debug logging with -debug, you should see the requests to obtain the token in ~/.gdfuse/default/curl.log. If there is any error, it should be logged there too (or in ~/.gdfuse/default/gdfuse.log).\n. Looks like the application was successfully authorized. What happens if you start the command specifying the mount-point?\n. If you have Google Docs in your root, setting download_docs=false in configuration would probably speed things up a bit.\n. In version 0.6.12 you can turn on XDG Base Directory support with -xdgbd command line option.. The command line option is required only once. The next time you run the application, the configuration file will be loaded from the XDG dir.. With -xdgbd, the old configuration file is not moved, but a new one is created, the old file will remain in the old path (~/.gdfuse/default/config), but it's not needed: you can move it to the new XDG config dir, or delete it, or keep it. The next time you start the application, the XDG config dir will be probed before the old one (somewhat like your pseudo-code), so only the new configuration file will be loaded.. > I don't want to abuse your patience, however now I don't understand the need for the xdg_base_directory option anymore.\nYou are right, it's mostly useless. The only use case where it can be of some use is when you specify a custom config file with -config and you want to use XDG directories for cache and state.\n\nWhat happens if it's changed to false in ~/.config/gdfuse/default/config? Does that mean that then the app for example searches the cache folder in ~/.config/gdfuse/default/cache instead of ~/.cache/gdfuse/? And if it's changed to true (manually, without using the -xdgbd flag) in ~/.gdfuse/default/config? Does it mean that the application uses the XDG paths for everything except for the config file?\n\nYes, it's like that and it's probably confusing. I'm going to remove it in a future release.\n\nWould you accept it if I contributed a \"Configuration path priority\" section in Configuration to clarify all this?\n\nSure! Thanks!\n. OK, thanks. I've updated the docs.\n. Thanks!\n. I will do a couple of tests to check why the error was ignored. Thanks for reporting it.\n. I'm sorry but I was not able to reproduce the issue. When I uploaded a .tar.gz with application/octet-stream as MIME type, the file was uploaded correctly and the type was corrected by Drive to application/x-gzip. For example:\n1) POST (file creation):\n```\n[696.412495] curl: header out: POST /drive/v2/files?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.19) gapi-ocaml/0.2.6/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nContent-Type: application/json\nAuthorization: Bearer \nContent-Length: 120\n[696.412600] curl: data out: {\"mimeType\":\"application/octet-stream\",\"parents\":[{\"id\":\"0B-TenPiSMBplblFKWHQxZGpLaDA\"}],\"title\":\"polymap-0.1-2.tar.gz\"}\n[697.122451] curl: header in: HTTP/1.1 200 OK\n[697.122540] curl: header in: Cache-Control: no-cache, no-store, max-age=0, must-revalidate\n[697.122621] curl: header in: Pragma: no-cache\n[697.122637] curl: header in: Expires: Fri, 01 Jan 1990 00:00:00 GMT\n[697.122650] curl: header in: Date: Mon, 30 Nov 2015 21:37:42 GMT\n[697.122663] curl: header in: Vary: X-Origin\n[697.122677] curl: header in: Content-Type: application/json; charset=UTF-8\n[697.122689] curl: header in: X-Content-Type-Options: nosniff\n[697.122698] curl: header in: X-Frame-Options: SAMEORIGIN\n[697.122709] curl: header in: X-XSS-Protection: 1; mode=block\n[697.122723] curl: info: Server GSE is not blacklisted\n[697.122732] curl: header in: Server: GSE\n[697.122742] curl: header in: Alternate-Protocol: 443:quic,p=1\n[697.122752] curl: header in: Alt-Svc: quic=\":443\"; ma=604800; v=\"30,29,28,27,26,25\"\n[697.122763] curl: header in: Accept-Ranges: none\n[697.122792] curl: header in: Vary: Origin,Accept-Encoding\n[697.122805] curl: header in: Transfer-Encoding: chunked\n[697.122816] curl: header in: \n[697.122829] curl: data in: 4f1\n{\n \"id\": \"0B-TenPiSMBplUTNya2VoVkI5cGc\",\n \"etag\": \"\\\"fbeGFVkCD3tGFpdjp1CYyuwDABw/MTQ0ODkxOTQ2MTYzNA\\\"\",\n \"alternateLink\": \"https://drive.google.com/file/d/0B-TenPiSMBplUTNya2VoVkI5cGc/view?usp=drivesdk\",\n \"title\": \"polymap-0.1-2.tar.gz\",\n \"mimeType\": \"application/octet-stream\",\n \"labels\": {\n  \"starred\": false,\n  \"hidden\": false,\n  \"trashed\": false,\n  \"restricted\": false,\n  \"viewed\": true\n },\n \"createdDate\": \"2015-11-30T21:37:41.634Z\",\n \"modifiedDate\": \"2015-11-30T21:37:41.634Z\",\n \"lastViewedByMeDate\": \"2015-11-30T21:37:41.634Z\",\n \"parents\": [\n  {\n   \"kind\": \"drive#parentReference\",\n   \"id\": \"0B-TenPiSMBplblFKWHQxZGpLaDA\",\n   \"selfLink\": \"https://www.googleapis.com/drive/v2/files/0B-TenPiSMBplUTNya2VoVkI5cGc/parents/0B-TenPiSMBplblFKWHQxZGpLaDA\",\n   \"parentLink\": \"https://www.googleapis.com/drive/v2/files/0\n[697.122902] curl: data in: B-TenPiSMBplblFKWHQxZGpLaDA\",\n   \"isRoot\": false\n  }\n ],\n \"downloadUrl\": \"https://doc-0k-a0-docs.googleusercontent.com/docs/securesc/2jstq75ksd6djnng7gd4lrkuiaklkreg/ha6129fgd7ptruqhv9htgq5jgcfrpv65/1448913600000/01371704680682440553/01371704680682440553/0B-TenPiSMBplUTNya2VoVkI5cGc?e=download&gd=true\",\n \"fileExtension\": \"gz\",\n \"md5Checksum\": \"d41d8cd98f00b204e9800998ecf8427e\",\n \"fileSize\": \"0\",\n \"editable\": true,\n \"explicitlyTrashed\": false\n}\n```\n2) GET:\n```\n[699.032208] curl: header out: GET /drive/v2/files/0B-TenPiSMBplUTNya2VoVkI5cGc?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.19) gapi-ocaml/0.2.6/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nAuthorization: Bearer \n[699.298750] curl: header in: HTTP/1.1 200 OK\n[699.298816] curl: header in: Expires: Mon, 30 Nov 2015 21:37:44 GMT\n[699.298829] curl: header in: Date: Mon, 30 Nov 2015 21:37:44 GMT\n[699.298840] curl: header in: Cache-Control: private, max-age=0, must-revalidate, no-transform\n[699.298850] curl: header in: ETag: \"fbeGFVkCD3tGFpdjp1CYyuwDABw/MTQ0ODkxOTQ2MTYzNA\"\n[699.298860] curl: header in: Vary: Origin\n[699.298871] curl: header in: Vary: X-Origin\n[699.298883] curl: header in: Content-Type: application/json; charset=UTF-8\n[699.298894] curl: header in: X-Content-Type-Options: nosniff\n[699.298905] curl: header in: X-Frame-Options: SAMEORIGIN\n[699.298915] curl: header in: X-XSS-Protection: 1; mode=block\n[699.298927] curl: header in: Content-Length: 1265\n[699.298942] curl: info: Server GSE is not blacklisted\n[699.298953] curl: header in: Server: GSE\n[699.298964] curl: header in: Alternate-Protocol: 443:quic,p=1\n[699.299070] curl: header in: Alt-Svc: quic=\":443\"; ma=604800; v=\"30,29,28,27,26,25\"\n[699.299107] curl: header in: \n[699.299884] curl: data in: {\n \"id\": \"0B-TenPiSMBplUTNya2VoVkI5cGc\",\n \"etag\": \"\\\"fbeGFVkCD3tGFpdjp1CYyuwDABw/MTQ0ODkxOTQ2MTYzNA\\\"\",\n \"alternateLink\": \"https://drive.google.com/file/d/0B-TenPiSMBplUTNya2VoVkI5cGc/view?usp=drivesdk\",\n \"title\": \"polymap-0.1-2.tar.gz\",\n \"mimeType\": \"application/octet-stream\",\n \"labels\": {\n  \"starred\": false,\n  \"hidden\": false,\n  \"trashed\": false,\n  \"restricted\": false,\n  \"viewed\": true\n },\n \"createdDate\": \"2015-11-30T21:37:41.634Z\",\n \"modifiedDate\": \"2015-11-30T21:37:41.634Z\",\n \"lastViewedByMeDate\": \"2015-11-30T21:37:41.634Z\",\n \"parents\": [\n  {\n   \"kind\": \"drive#parentReference\",\n   \"id\": \"0B-TenPiSMBplblFKWHQxZGpLaDA\",\n   \"selfLink\": \"https://www.googleapis.com/drive/v2/files/0B-TenPiSMBplUTNya2VoVkI5cGc/parents/0B-TenPiSMBplblFKWHQxZGpLaDA\",\n   \"parentLink\": \"https://www.googleapis.com/drive/v2/files/0B-TenPiSMBplblFKWHQxZGpLaDA\",\n   \"isRoot\": false\n  }\n ],\n \"downloadUrl\": \"https://doc-0k-a0-docs.googleusercontent.com/docs/securesc/2jstq75ksd6djnng7gd4lrkuiaklkreg/ha6129fgd7ptruqhv9htgq5jgcfrpv65/1448913600000/01371704680682440553/01371704680682440553/0B-TenPiSMBplUTNya2VoVkI5cGc?e=download&gd=true\",\n \"fileExtension\": \"gz\",\n \"md5Checksum\": \"d41d8cd98f00b204e9800998ecf8427e\",\n \"fileSize\": \"0\",\n \"editable\": true,\n \"explicitlyTrashed\": false\n}\n```\n3) PUT:\n```\n[699.300700] curl: header out: PUT /resumable/upload/drive/v2/files/0B-TenPiSMBplUTNya2VoVkI5cGc?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle&updateViewedDate=true HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.19) gapi-ocaml/0.2.6/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nTransfer-Encoding: chunked\nContent-Type: application/json\nAuthorization: Bearer \nIf-Match: \"fbeGFVkCD3tGFpdjp1CYyuwDABw/MTQ0ODkxOTQ2MTYzNA\"\nX-Upload-Content-Type: application/octet-stream\nX-Upload-Content-Length: 1997\nExpect: 100-continue\n[699.367056] curl: header in: HTTP/1.1 100 Continue\n[699.367291] curl: data out: 40e\n{\"alternateLink\":\"https://drive.google.com/file/d/0B-TenPiSMBplUTNya2VoVkI5cGc/view?usp=drivesdk\",\"createdDate\":\"2015-11-30T21:37:41.000Z\",\"downloadUrl\":\"https://doc-0k-a0-docs.googleusercontent.com/docs/securesc/2jstq75ksd6djnng7gd4lrkuiaklkreg/ha6129fgd7ptruqhv9htgq5jgcfrpv65/1448913600000/01371704680682440553/01371704680682440553/0B-TenPiSMBplUTNya2VoVkI5cGc?e=download&gd=true\",\"editable\":true,\"etag\":\"\\\"fbeGFVkCD3tGFpdjp1CYyuwDABw/MTQ0ODkxOTQ2MTYzNA\\\"\",\"fileExtension\":\"gz\",\"id\":\"0B-TenPiSMBplUTNya2VoVkI5cGc\",\"labels\":{\"viewed\":true},\"lastViewedByMeDate\":\"2015-11-30T21:37:41.000Z\",\"md5Checksum\":\"d41d8cd98f00b204e9800998ecf8427e\",\"mimeType\":\"application/octet-stream\",\"modifiedDate\":\"2015-11-30T21:37:41.000Z\",\"parents\":[{\"id\":\"0B-TenPiSMBplblFKWHQxZGpLaDA\",\"kind\":\"drive#parentReference\",\"parentLink\":\"https://www.googleapis.com/drive/v2/files/0B-TenPiSMBplblFKWHQxZGpLaDA\",\"selfLink\":\"https://www.googleapis.com/drive/v2/files/0B-TenPiSMBplUTNya2VoVkI5cGc/parents/0B-TenPiSMBplblFKWHQxZGpLaDA\"}],\"title\":\"polymap-0.1-2.tar.gz\"}\n[699.367376] curl: data out: 0\n[699.745942] curl: header in: HTTP/1.1 200 OK\n[699.746004] curl: header in: X-GUploader-UploadID: AEnB2UpvDEWD6EsNNjY9zFcvZF1WaAm_tTZAVwfMBMoeC0zw8_-D87_Y5-PvIjn26TEk1p6cKXY9zDvKchIPt_MuLJTsHexemw\n[699.746023] curl: header in: Location: https://www.googleapis.com/resumable/upload/drive/v2/files/0B-TenPiSMBplUTNya2VoVkI5cGc?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle&updateViewedDate=true&upload_id=AEnB2UpvDEWD6EsNNjY9zFcvZF1WaAm_tTZAVwfMBMoeC0zw8_-D87_Y5-PvIjn26TEk1p6cKXY9zDvKchIPt_MuLJTsHexemw\n[699.746057] curl: header in: ETag: \"fbeGFVkCD3tGFpdjp1CYyuwDABw/mLNL2XBVN-aH8ZKM7yhxWd_PaPg\"\n[699.746070] curl: header in: Vary: Origin\n[699.746081] curl: header in: Vary: X-Origin\n[699.746092] curl: header in: Cache-Control: no-cache, no-store, max-age=0, must-revalidate\n[699.746104] curl: header in: Pragma: no-cache\n[699.746114] curl: header in: Expires: Fri, 01 Jan 1990 00:00:00 GMT\n[699.746125] curl: header in: Date: Mon, 30 Nov 2015 21:37:44 GMT\n[699.746137] curl: header in: Content-Length: 0\n[699.746154] curl: info: Server UploadServer is not blacklisted\n[699.746176] curl: header in: Server: UploadServer\n[699.746189] curl: header in: Content-Type: text/html; charset=UTF-8\n[699.746201] curl: header in: Alternate-Protocol: 443:quic,p=1\n[699.746212] curl: header in: Alt-Svc: quic=\":443\"; ma=604800; v=\"30,29,28,27,26,25\"\n[699.746224] curl: header in: \n[699.746244] curl: info: Connection #28 to host www.googleapis.com left intact\n[699.747625] curl: info: Found bundle for host www.googleapis.com: 0x16bc690\n[699.747679] curl: info: Re-using existing connection! (#28) with host www.googleapis.com\n[699.747697] curl: info: Connected to www.googleapis.com (216.58.209.106) port 443 (#28)\n[699.747938] curl: header out: PUT /resumable/upload/drive/v2/files/0B-TenPiSMBplUTNya2VoVkI5cGc?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle&updateViewedDate=true&upload_id=AEnB2UpvDEWD6EsNNjY9zFcvZF1WaAm_tTZAVwfMBMoeC0zw8_-D87_Y5-PvIjn26TEk1p6cKXY9zDvKchIPt_MuLJTsHexemw HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.19) gapi-ocaml/0.2.6/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nTransfer-Encoding: chunked\nAuthorization: Bearer \nIf-Match: \"fbeGFVkCD3tGFpdjp1CYyuwDABw/MTQ0ODkxOTQ2MTYzNA\"\nContent-Type: application/octet-stream\nContent-Range: bytes 0-1996/1997\nExpect: 100-continue\n[699.804870] curl: header in: HTTP/1.1 100 Continue\n[699.805190] curl: data out: 7cd\n...\n[700.545312] curl: header in: HTTP/1.1 200 OK\n[700.545380] curl: header in: X-GUploader-UploadID: AEnB2UpvDEWD6EsNNjY9zFcvZF1WaAm_tTZAVwfMBMoeC0zw8_-D87_Y5-PvIjn26TEk1p6cKXY9zDvKchIPt_MuLJTsHexemw\n[700.545394] curl: header in: ETag: \"fbeGFVkCD3tGFpdjp1CYyuwDABw/MTQ0ODkxOTQ2NTA3OA\"\n[700.545405] curl: header in: Vary: Origin\n[700.545414] curl: header in: Vary: X-Origin\n[700.545426] curl: header in: Content-Type: application/json; charset=UTF-8\n[700.545435] curl: header in: Cache-Control: no-cache, no-store, max-age=0, must-revalidate\n[700.545449] curl: header in: Pragma: no-cache\n[700.545539] curl: header in: Expires: Fri, 01 Jan 1990 00:00:00 GMT\n[700.545557] curl: header in: Date: Mon, 30 Nov 2015 21:37:45 GMT\n[700.545568] curl: header in: Content-Length: 1262\n[700.545585] curl: info: Server UploadServer is not blacklisted\n[700.545598] curl: header in: Server: UploadServer\n[700.545608] curl: header in: Alternate-Protocol: 443:quic,p=1\n[700.545619] curl: header in: Alt-Svc: quic=\":443\"; ma=604800; v=\"30,29,28,27,26,25\"\n[700.545643] curl: header in: \n[700.545657] curl: data in: {\n \"id\": \"0B-TenPiSMBplUTNya2VoVkI5cGc\",\n \"etag\": \"\\\"fbeGFVkCD3tGFpdjp1CYyuwDABw/MTQ0ODkxOTQ2NTA3OA\\\"\",\n \"alternateLink\": \"https://drive.google.com/file/d/0B-TenPiSMBplUTNya2VoVkI5cGc/view?usp=drivesdk\",\n \"title\": \"polymap-0.1-2.tar.gz\",\n \"mimeType\": \"application/x-gzip\",\n \"labels\": {\n  \"starred\": false,\n  \"hidden\": false,\n  \"trashed\": false,\n  \"restricted\": false,\n  \"viewed\": true\n },\n \"createdDate\": \"2015-11-30T21:37:41.634Z\",\n \"modifiedDate\": \"2015-11-30T21:37:45.078Z\",\n \"lastViewedByMeDate\": \"2015-11-30T21:37:41.000Z\",\n \"parents\": [\n  {\n   \"kind\": \"drive#parentReference\",\n   \"id\": \"0B-TenPiSMBplblFKWHQxZGpLaDA\",\n   \"selfLink\": \"https://www.googleapis.com/drive/v2/files/0B-TenPiSMBplUTNya2VoVkI5cGc/parents/0B-TenPiSMBplblFKWHQxZGpLaDA\",\n   \"parentLink\": \"https\n[700.545756] curl: data in: ://www.googleapis.com/drive/v2/files/0B-TenPiSMBplblFKWHQxZGpLaDA\",\n   \"isRoot\": false\n  }\n ],\n \"downloadUrl\": \"https://doc-0k-a0-docs.googleusercontent.com/docs/securesc/2jstq75ksd6djnng7gd4lrkuiaklkreg/ha6129fgd7ptruqhv9htgq5jgcfrpv65/1448913600000/01371704680682440553/01371704680682440553/0B-TenPiSMBplUTNya2VoVkI5cGc?e=download&gd=true\",\n \"fileExtension\": \"gz\",\n \"md5Checksum\": \"f4c0dbff2cc518f8123b56ca562cf38a\",\n \"fileSize\": \"1997\",\n \"editable\": true,\n \"explicitlyTrashed\": false\n}\n```\nAs you can see POST sent application/octet-stream (confirmed in the response and in the subsequent GET). But after uploading with PUT, MIME type was returned application/x-gzip.\nCould you send me your logs, where you experience the issue, to my e-mail alessandro(dot)strada(at)gmail(dot)com, please? Feel free to remove any sensitive information. I just need to see the sequence of operations. Thanks!\n. I was thinking: maybe the problem is triggered by the tool used to upload the file. I used cp but maybe you used something else: rsync perhaps?\n. The first SIGSEGV you are reporting is probably caused by -m option. Unfortunately, the FUSE library I'm using doesn't work very well in multi-threading mode. When I tested it, I always experienced segmentation faults. The second SIGSEGV can be caused by issue #54. You can try, as a workaround, to set curl_debug_off=true. Unfortunately, you will lose request/response logging.\n. I see your point. When I have a couple of hours of spare time, I'll make a patch to upload files in separate threads. Thanks for your feedback. \n. In version 0.5.20, the new option async_upload turns on multithreaded uploading. By default this option is turned off, because it still need testing. But, anyway, you can try it.\n. I googled \"curl_mvsnprintf\" and it looks like libcurl may cause multi-threading issues if CURLOPT_NOSIGNAL is set to 0 (that's the default). I will test this option a bit.\n. In version 0.5.21 that uses gapi-ocaml 0.2.8, I've set that option to 1. It looks like this actually solves (at least some) multi-threading issues. Now you can test async_upload and even -m command line option (but note that if you use -m option, async_upload will be turned off because it's useless). I've also raised sqlite3_busy_timeout default value to 5000 because 500 is too low. Please be sure to update your config file too.\n. Pre-release 0.6.0 should address at least part of this issue. If you are on Ubuntu you can add the beta PPA, to check out this version.\n. > When choosing the drive size should I allocate for total amount of data being transferred or the largest file being transferred?\nThe largest file being transferred. But there is a problem: if your cache is nearly full and you upload a big file, it would get copied anyway, and only when the upload is completed, the cache will be freed.\n\nIs there a way to clear the cache during an rsync?\n\nNo. You can try to delete cached files with a script but afterwards, cache size will not be aligned anymore and this could cause other issues.\n. I'm working on a new version (0.6.0) that should support multi-threading. Maybe this will solve your issue.\n. Please try removing the directory .gdfuse (rm -r /home/drj/.gdfuse) and start the authorization procedure again. If the problem persists let me know.\n. OK, I see. Can you please try the alternative authorization method? The problem is that the error you obtain is caused by a page on Google servers that are not under my control, so I don't have logs and it's difficult to understand what went wrong.\n. OK, thanks!\n. Yes. You should set stream_large_files=true so that files larger than large_file_threshold_mb will not be cached.\n. Yes, Google updated the Developers Console UI. I think you should choose \"Other\" as Application type.\n\n. No, sorry. This application just gives you a local view of a remote file-system. It's not a sync app like Dropbox. If you need to have all files on your local hd to access them when offline, you should use another client (for example, this one).\n. Thank you for using my application!\n. >  Is my upload being corrupted somehow?\nI don't know. What if you download them from Google Drive web interface? Are they still not playable? Can you reupload those files?\n. Try using -debug, and then check the log files (gdfuse.log and curl.log) in .gdfuse/default. If you want, you can send them to my email: alessandro (dot) strada (at) gmail (dot) com\n. If you are using OPAM, opam update && opam upgrade should fix this issue. I will upload a new Ubuntu package as soon as I can.\n. Actually it doesn't fix the problem completely. The error is still there if file creation timestamp has a fractional part from 1 to 99 milliseconds. I will push another fix as soon as I can (tonight or tomorrow).\n. Should be fixed with gapi-ocaml 0.2.10\n. Can you please send curl.log to my email (alessandro.strada@gmail.com)? Thanks!\n. OK, thanks for your feedback.\n. Unfortunately, to fix the bug, I needed to use an updated version of a library I'm using (libocamlnet-ocaml), but that version is not available in precise (you need version 3.5.1 or above, but the distribution has only version 3.4.1). So the fixed version won't build on Ubuntu 12.04.\n. If you cannot upgrade your system, your best option is to install the application using OPAM.\n. Right. I've removed obsolete packages from PPA.\n. Please, use -debug and send curl.log and gdfuse.log (in ./gdfuse/default) to my email alessandro.strada@gmail.com. Thanks\n. Yes, Google updated its API so that it caused that error. See issues #159 #160.\n. That's because each time the application receives a request from the kernel (in your case getattr /), it tries to fetch data from the server to get the latest version, and it retries 10 times with exponential backoff (so about 1000 seconds). There is no offline mode. If you want you can try the multi-threading version (with -m) so there is no single thread blocking, but unfortunately the multi-threading mode is not yet stable enough to be production ready.\n. Should be fixed since 0.6.5. See issue #201.. Can you update to the latest version (with OPAM or the Ubuntu package)? This issue was caused by a change in Drive API. See issues #159 #160.\n. Yes, it should be fixed in the last version. See issues #159 and #160.\n. Can you install libncurses-dev (or ncurses-dev)?\n. Thanks! I'm adding the instructions to the wiki.\n. Please update your package to the latest version. It should fix the issue. (See issues #159 and #160.)\n. No problem.\n. Please update to the latest version. It should fix it. (See issues #159 and #160.)\n. No, it does not work like the Dropbox client, so you cannot access your files offline. Please check out another client (e.g. this one).\n. Files are renamed appending a suffix like \"(1)\", \"(2)\", if there is more than one file with the same name in the same folder. As there is no way to keep two files with the same name in the same directory on a UNIX file-system (while Google Drive permits), I have to disambiguate them. If you don't want to have those suffixes, be sure to rename the files so that are no duplicates (in the same folder).\n. No, -cc should do. But you can try removing ~/gdfuse/default/cache with rm -r. If it doesn't work yet, please activate debugging output (with -debug), do an ls -R of your mount point, and send gdfuse.log and curl.log to my email (alessandro.strada@gmail.com). Thanks\n. OK, no problem.\n. Should be fixed in the new 0.6.x series.. To enable logging in ~/.gdfuse/default/gdfuse.log you should use -verbose. Probably there are issues connecting to the Drive API endpoint and there are many retries (and if you enable logging you should see them).\n. If you transferring 300GB, you shouldn't use -debug, because it will dump every transferred byte to curl.log. -d enables FUSE debugging mode (that doesn't log anything to the file-system, so you can turn it on, if you want, but it's not needed).\n. Please update to the latest version. See issues #159 and #160.\n. Which distribution are you using?\n. Are you compiling google-drive-ocamlfuse with OPAM?\n. Can you please post the output of opam list gapi-ocaml?\n. Have you tried reinstalling it?\n$ opam uninstall gapi-ocaml\n$ opam install google-drive-ocamlfuse\n. >  I can't chmod or chown it afterwards.\nThat's because it's not implemented (as there is no easy way to map local users/permissions to Drive users/permissions).\nCan you please send your curl.log (with the error) to my email (alessandro.strada@gmail.com)?\n. > do I need to run as -debug for it to be populated?\nYes, exactly.\n. You should use -oallow_other.\n. Only from the web interface.\n. It should already work as you expect. Have you enabled access to Google Photos via Google Drive (see \"Organize photos & videos using Google Drive\" https://support.google.com/photos/answer/6156103?hl=en). Then you should be able to access to the Google Photo folder.\n. You should use -oallow_other.\n. Try with:\n```\ndrive-google-ocamlfuse -oallow_other /home/azh/GoogleDrive/\n```\n. Sorry, it should be:\n```\ndrive-google-ocamlfuse -o allow_other /home/azh/GoogleDrive/\n```\n. > Does it download the file, raname, then re-upload?\nNo, it shouldn't, unless it is a Google Doc. Let me know if it doesn't work as you expect.\n. In version 0.6.12, I've added cache_directory config option to specify a custom cache path.. Thanks!\n. Can you try mounting with -o direct_io? E.g.:\n$ google-drive-ocamlfuse -o direct_io /mnt/gdrive/\nIt should disable the use of page cache (file content cache) in the kernel for this filesystem.\n. I created a new PPA with a new version (0.6.0) of my application that should support symbolic links. You should consider this a beta version. If you don't use Ubuntu, let me know, so I can give you instructions on how to install it.\n. No, that's the way they are implemented. Those are placeholders (kept in Drive) that are used to store the link name and metadata (the path the link points to). This way, you can clear your cache without losing the symlinks.. > Oh so the actual file the link points to is never uploaded. Got it, thanks!\nNo, if it's out of the mounted directory.\n\nThough that's a bit weird since pretty much everywhere \"symlink support\" usually means it uploads the actual contents\n\nWell, a symbolic link (in a Unix file system) is just a file that stores a path.\n\nso you may want to clarify that in the description\n\nOK, thanks for your feedback. With stream_large_files=true files are stored locally only if you have to upload them. Otherwise, they will not be downloaded in the local cache.\n. Yes, correct. As Google Drive only accepts the entire file during an upload, there is no way to upload single chunks individually. I have to store the whole file in the local cache before being able to start the upload.\n. No, sorry. Uploading requires copying the entire file to the cache, streaming options work only when downloading resources.\n. If max_cache_size_mb is below the file size, the next time you upload/download a file, the bigger file should get evicted from cache (if it doesn't do so, it's a bug).\n. Yes, you should set stream_large_files to true in config. If a file is bigger than large_file_threshold_mb, it will be streamed instead of fully downloaded to the local cache.. After modifying the configuration, you should restart the application.. Yes, correct.. Previously cached files won't be removed, if you want you can remount with -cc to clear the local cache.. Some files will still be downloaded: files smaller than 1MB and Google Docs. Ah, I see. You are uploading files to Google Drive. So, no, I'm sorry, it's not possible to upload files without caching them on disk first.. > If we download to google-drive mount folder, is it actually downloaded to cache folder?\nYes, and when the download is completed (the file is closed by the OS), the file is (starting to be) uploaded to Google Drive servers.\n\nWhen we use that file immediately after download, is it uploaded to Google Drive and downloaded again?\n\nIf you access the file before the upload is completed, yes, the file is probably going to be downloaded again. You can turn on verbose logging with -debug, then check the log files to see what's happening. \n. No, sorry.\n. The default mode is single threaded, so there is no concurrency. If you use -m to activate multi-threading, there is the possibility that the file is downloaded 2 times but this scenario should be quite rare (the 2 processes have to start accessing the file exactly at the same time). To be sure (if you don't mind caching the files), you could use stream_large_files=true, so that you never download big files.\n. > I tried to use stream_large_files=true before but it seems slowdown downloading process.\nSure, but it gets you more consistent results. It's a trade-off.\n. Pre-release 0.6.0 should address at least part of this issue. See this comment for further info.\n. > When you move a file onto the fuse mount, how long does the file sit locally?\nUntil Drive API returns that the file (or one of the folders that contains it) is modified.\n\nIs something like this already happening when you reach the cache size limit (delete the oldest file in cache until under the cache size limit)?\n\nYes, it should work like that.\n. Unfortunately it's not possible, because you need a whole file to begin uploading to Google Drive (via the API). You cannot upload just some bytes of the file, so you have to wait for the file to be completely copied to the local cache. You can check if other tools (e.g. https://github.com/odeke-em/drive), that don't use a FUSE backend, suit better your needs.\n. I think so, if you can install the Go runtime, it should work.\n. From the hamburger menu, you should select API Manager, then Library -> Drive API, then ENABLE. This link has some screenshots: http://www.iperiusbackup.net/en/how-to-enable-google-drive-api-and-get-client-credentials/\n. It looks like you don't have FUSE inside the container. Look at this link: https://openvz.org/FUSE\n. Try with:\n$ ls -l /dev/fuse\nyou should get something like:\ncrw-rw-rw- 1 root root 10, 229 Jan 19 13:53 /dev/fuse\n. OK, you user doesn't have write permissions on the device. Check this link: http://superuser.com/a/800016\n. Probably you have to restart your VE (see https://forum.openvz.org/index.php?t=msg&goto=11793& and http://realtechtalk.com/OpenVZVirtuozzo_Enable_Fuse_in_Container-139-articles).\n. Yes. Modify the configuration file putting read_only=true.\n. AFAIK encfs uses FUSE, so you have a double FUSE layer. This could explain the slowness. Probably you should use another tool to upload your encrypted filesystem to the cloud (e.g. https://github.com/odeke-em/drive).\n. No, because Google Drive doesn't have a UNIX-like permission model. I'm thinking about adding metadata to Drive files, but it's a long term enhancement, so I cannot give you an ETA.\n. I created a new PPA with a new version (0.6.0) of my application that should support permissions. You should consider this a beta version. If you don't use Ubuntu, let me know, so I can give you instructions on how to install it.\n. Looks like you have problems connecting to the authorization endpoint. Are you behind a proxy?\n. The authorization proxy is at: https://gd-ocaml-auth.appspot.com. Check if you can reach it via browser and via curl (from the command line $ curl https://gd-ocaml-auth.appspot.com should return some HTML). Otherwise if it's a problem of net latency, you can try raising the timeout in the config file (connect_timeout_ms).\n. If it happens again, you should raise connect_timeout_ms (for example to 10000).\n. Files are just cached locally (and not stored), and if you don't access them, they won't be downloaded. Cache size is also limited (check the configuration file), although if you upload very big files, that limit can be exceeded. If you need a sync tool, you can check out other projects (e.g.: https://github.com/odeke-em/drive).\n. S\u00ec, \u00e8 possibile. In questa pagina ci sono le istruzioni per procedere. La macchina esterna con il browser serve comunque.\n. Thanks!\n. Actually, you need client id, client secret, and a refresh token (that is stored in ~$HOME/.gdfuse/default/state) to access someone's Drive account. So, knowing only your refresh token is not enough. The other access token that is stored in that file would be enough, but it's short lived (1 hour).\n. Yes, but you could revoke the token here: https://myaccount.google.com/security#connectedapps\n. Precise is not supported anymore because it doesn't have updated enough dependencies, so if you cannot update your distro, you should use OPAM to install this app.\n. No, Google Docs cannot be downloaded in native format (through the API). Your only option is to export them to one of the supported format (https://developers.google.com/drive/v3/web/manage-downloads#downloading_google_documents).\n. I've uploaded packages for yakkety\n. Try setting curl_debug_off to true (see this issue). Otherwise check issue #106.\n. No, I don't think so. It's probably caused by some weird interaction in C libraries. The only workaround is to set the option curl_debug_off set to true in ~/.gdfuse/default/config.\n. > Is the gdrive upload progress related to the copy/paste progress from Nautilus?\nNot really, it's shown while copying the file to upload to the local cache. The actual upload is done when this copy is completed, so it's not tracked by the progress bar.\n. @Moulick yes, correct.. Because FUSE writes are chunked, but Drive API doesn't support uploading a single chunk of a file.. Can you please mount with -debug, reproduce the error, and then send gdfuse.log and curl.log (that are in ~/.gdfuse/default) to my email (alessandro.strada@gmail.com)?\n. Thanks for the log files. I think the issue gets triggered only on 32-bit hosts (that's why I couldn't  reproduce it). The workaround is to set a smaller value for max_upload_chunk_size in ./gdfuse/default/config. I tried to set a default value of 1GB (1073741824), but it's too big for 32-bit architectures. Try setting it to 512MB (536870912), it should solve your problem.\n. Yes, there was a problem with 32-bit architectures. Should be fixed in 0.5.25 (as soon as the package is published to the official repository).\n. It looks like latest version of extlib doesn't compile on 32-bit systems. You can try with opam pin add extlib 1.7.2.. I think you can try installing OCaml (https://github.com/jrcharney/rigel/wiki/OCAML) and then install google-drive-ocamlfuse with OPAM.\n. I didn't know you can enable ARM compiling from PPA settings (http://askubuntu.com/questions/371339/how-to-make-packages-for-arm-in-launchpad/765149#765149). I can enable these targets:\n- ARM ARMv8 (arm64)\n- ARM ARMv7 Soft Float (armel)\n- ARM ARMv7 Hard Float (armhf)\nWhich one do you need?\n. OK, I published arm packages for Ubuntu 16.04. Let me know if you need packages for another version.\n. Well, thanks! So I'm going to select arm64 too.. How do you save the .pptx? That's because I've copied a .pptx to my Google Drive and it was correctly detected as Power Point (mime type application/vnd.openxmlformats-officedocument.presentationml.presentation). If the program you use to save it, uses a temporary filename without the .pptx extesion, you could experience that issue.\n. > What kind of information can I provide you to help getting this resolved?\nAre you using the default configuration? Do you use some kind of non-default command switches (e.g. -m)?\n. Which version are you testing?. You can try the latest beta version (0.6.18) with:\nopam pin add google-drive-ocamlfuse https://github.com/astrada/google-drive-ocamlfuse.git#beta\n\nTo restore the stable version:\nopam pin remove google-drive-ocamlfuse. Opam should report 0.6.17 (because the new version is not in the official repository yet). But if you run `google-drive-ocamlfuse -version` and get 0.6.18, then you got the new beta version.. You should check if your libcurl is compiled with NSS as TLS provider. It can be the same issue reported [here](https://bugzilla.redhat.com/show_bug.cgi?id=1057388), [here](https://serverfault.com/questions/561350/unusually-high-dentry-cache-usage), or [here](https://github.com/curl/curl/issues/1086).. @jheyneman: you should check that your system will use your version of libcurl running\n\npkg-config --variable=libdir libcurl\n\nand checking that it returns the path of your recompiled version. Then you have to recompile ocurl, gapi-ocaml, and google-drive-ocamlfuse\nopam reinstall ocurl gapi-ocaml google-drive-ocamlfuse. With opam you can switch to 0.6.19 with:\n\nopam pin add google-drive-ocamlfuse https://github.com/astrada/google-drive-ocamlfuse.git#beta\n\nTo restore 0.6.17:\nopam pin remove google-drive-ocamlfuse\n\nNote that opam list will still report version 0.6.17, but google-drive-ocamlfuse -version should show 0.6.19. If not, run opam reinstall google-drive-ocamlfuse.. > So is there anything I can try to reduce memory usage by google-drive-ocamlfuse?\nNo, sorry, there are lots of buffers I can't control from the application (mainly FUSE buffers and libcurl buffers). If you need a low memory footprint application you should look for another implementation.. Should be (at least partially) resolved in the new 0.6.x versions.. Yes, sorry. I'm working on a new release (0.6.0) and broke the old one. PR ocaml/opam-repository#7884 (when it's merged) should fix this. In the meantime, as a workaround:\nopam install gapi-ocaml.0.2.13 google-drive-ocamlfuse\n\n. You should be able to access Google Photos directory, if you select \"Create Google Photos folder\" in Drive settings: https://support.google.com/photos/answer/6156103?co=GENIE.Platform%3DDesktop&hl=en&oco=0 (Organize photos & videos using Google Drive). No, sorry, the API doesn't support that feature (http://stackoverflow.com/questions/25772174/api-access-to-different-video-formats-and-resolutions/25815541#25815541).. Nice catch! Thanks!. If you check the log file, you should see if the connection is timing out. If it's so, you can raise connect_timeout_ms in config.. If you are using your own client_id/client_secret you should specify them on the command line, otherwise, my app will try to connect to the standard authorization proxy and will not find any token.. Yes, that's just a warning you can ignore. It should go away with the next update, I'm going to publish in the next days.. Yes, correct.. You should copy ./gdfuse/default/state (and ./gdfuse/default/config if you are using your own client id/secret) on the other machine. Then you should be able to mount your Drive without reauthorizing the app.. Unfortunately, I cannot produce a statically linked executable (as the one provided by rclone) because some of the libraries I'm depending on don't support static linking. For example, libcurl is linked to libkrb5 but it doesn't support static linking anymore (https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=439039). I even tried to recompile libcurl, but then I was blocked by p11-kit (https://lists.freedesktop.org/archives/p11-glue/2011-November/000099.html). Sorry, but I don't think I can build a working static executable.. Yes, it was caused by a breaking change in Drive API v3. It should be fixed in version 0.6.6. This issue is a consequence of upgrading to Drive API v3. The new API changed the exportable formats for Google Forms (the new forms, the old ones are treated like spreadsheets). The only supported format is now .zip, but this format wasn't supported by my application. It should be in version 0.6.7. So after upgrading, you'll have to update the config file changing the format to zip (form_format=zip) and then mount with -cc.. It's by design. That's because you cannot download Google Docs/Sheets in native format through the API, you can only export them to some other format, and layout (for example) is not always preserved. So, to avoid messing with original docs, I decided to make them read-only.. Should be fixed in version 0.6.8. See issue #232.. You should add allow_other to mount options:\ngdfuse#default  /mnt/gdrive fuse uid=1000,gid=1000,allow_other 0 0\n\n. The checklist you used is not updated, you should check out this version: https://github.com/astrada/google-drive-ocamlfuse/wiki/Automounting. I'm not sure what you mean with notification, but I don't think it's a feature supported by FUSE.. At the moment FUSE does not support an inotify interface (check the TODO list). There is a proposal, but it's not yet implemented (requires patching the kernel).. Can you please send the whole curl.log and gdfuse.log to my personal email (alessandro.strada@gmail.com)? Thanks!. Unfortunately, it isn't easy to understand what the problem is. I need to know what happens if you remount the file system without -cc. Is it still broken?. OK, thanks. There is a bug, but I'm not sure it's what you are experiencing. I'll let you know when I publish a fix. (I've removed your previous comments with log files which may contain sensitive info.). I was able to reproduce your issue. It should be fixed in version 0.6.8. Let me know if the fix doesn't work for you.. Yes, you should use -debug instead of -d, because -d is used by libfuse to debug low level syscalls to the file system.. Something like that happens when you are offline (this should explain why it happens sporadically). In theory, when you get online again, you should be able to access your files. If this doesn't happen, you should remount the file system.. Yes, actually mount will show the mountpoint regardless of the connection status, but e.g. df will omit the entry while offline.. Fixed in astrada/gapi-ocaml#13.. If you search into gdfuse.log, don't you find anything starting with CURLE_ (e.g. CURLE_COULDNT_RESOLVE_HOST)?. Anyway version 0.6.8 should fix a similar issue. I don't know if it's the same you are experiencing, but it's worth giving a try.. OK, it's a different bug. Could you please send your gdfuse.log to alessandro.strada@gmail.com? Thanks. Fixed in 0.6.10. @Windywind: can you please rerun with -debug and send gdfuse.log and curl.log (in ~/.gdfuse/default) to alessandro.strada@gmail.com?\n@vlna: can you do the same?\nThanks!. I just published v0.6.11 (to the beta PPA) where you can access your unorganized files setting the new config option lost_and_found to true. You will then see a new directory /lost+found where you should find all your unorganized files. Warning: fetching those files is slow because there is no direct filter to get them. I have to fetch all files and then filter (client side) to get those without parents. I've also added a shared_with_me option to get shared files (that you can see in \"Shared with me\" section of the web interface).. If you remount with -debug, do you get any error in the log files?. See issue #133. Did you updated your kernel without rebooting? If so, a reboot could probably fix it. Otherwise try with depmod -a.. Can you please remount with -debug, run df and then send gdfuse.log and curl.log to my email (alessandro.strada@gmail.com)? Thanks!. It shouldn't be a fuse problem. Have you checked your trash? Is it empty?. Trash is mapped in ./.Trash subdirectory. If you enable delete_forever_in_trash_folder, when you remove files in trash, they will be removed permanently. If you use the -skiptrash switch, the files will not be trashed.. Looks like user vvcares was able to export his folder via FTP (see issue #245). Maybe you could ask him exactly how he did it.. Actually, since version 0.6.7, Apps Scripts should be exported as a json files that contain the full project.. Yes, you are right, there is a bug that prevents apps script downloading.\nIs the mount point non empty because it is still mounted? If so, I advise you to unmount it first (fusermount -u <mountpoint>). Anyway to mount on a non-empty directory you can use -o nonempty. To test the github version, and you are using opam, you can use opam pin like this:\nopam pin add google-drive-ocamlfuse https://github.com/astrada/google-drive-ocamlfuse.git#master\n\n. Should be fixed in 0.6.16, that's now on beta PPA.. No, sorry, it's too complicated. It would require implementing a new virtual file system backed by the json project file. You should probably go for an external editor, e.g.: http://stackoverflow.com/questions/24739294/external-editor-support-for-google-apps-script. Have you checked this wiki page?. Should be fixed in 0.6.15. Sorry, but I don't use Plex, so I don't really have an answer. Personally, I never experienced that error during test.. > Isn't large_read deprecated in newer kernels?\nYes, it should now be a no-op.\n\nAlso you might want to try auto_cache not sure if works with ocaml (@astrada) ?\n\nIt should work, but I never tried.\n\nAnyone tried direct_io mount option?\n\nNot really. I tested it when I was trying to avoid to download Google Docs during readdir (because the file size of Google Docs is not known until I have the exported file) but it gave me problems with libreoffice (it could not determine the correct file size).. > But if I turn off streaming, is the whole file downloaded even though only portion of it is needed or does it download in chunks?\nWith streaming off, it is fully downloaded, even if you need to read just a single byte.. It requires a major refactoring of cache management, so I can't promise anything.. You should install external dependencies before installing this app:\nopam install depext\nopam depext google-drive-ocamlfuse\n\n. OK, I've updated the wiki. Thanks for your feedback.. I'm sorry, but what gdrive folder are you referring to?. > Long story short: is the .shared directory read only?\nYes, correct. It's just a view of your shared files.. No, sorry, but you can always add the shared file to your Drive, so it gets writable (if you have permission). https://support.google.com/drive/answer/2375057?co=GENIE.Platform%3DDesktop&hl=en. Sorry, but if you need to upload big files to Google Drive, there is no way to avoid file caching. So, if you upload files bigger than max_cache_size_mb, the file will copied anyway to the cache before uploading (and it will be removed only when completely uploaded). Otherwise, if your files are already in Drive and you just need to stream them, you can set stream_large_files=true and files bigger than large_file_threshold_mb won't be downloaded to the local cache.. > Then, what if I set max_cache_size_mb to big enough, say 1000GB, and my BitTorrent application is writing into the mount point? What would happen? Is this a solution to avoid caching?\nYes, but you should also set metadata_cache_time to a big number (say 36000 = ten hours) otherwise your torrent will be downloaded/reuploaded every 60 seconds. Moreover, I don't think this is the ideal scenario I had in mind when developing my application. If you only need to upload big files to the cloud, probably you should check other Drive clients (maybe rclone).. > And is there's a way to limit the cache size? If so, then I think that would be a solution, right?\nNot for uploading. The Drive API doesn't allow to upload only a chunk of a file, so I have to store the whole file in the local cache before uploading it. There is no workaround. Maybe other clients use a different approach and have a solution that fits best your requirements.. No, sorry. Since this is not an application that syncs your local filesystem with GDrive, remote files are not downloaded unless you access their content (except for Google Docs). So I don't see any advantage in limiting the file-system view to a subdirectory.. You are right, I've not yet uploaded 0.6.17 to the OPAM repository, but there are no big differences between 0.6.15 and 0.6.17 (just a fix for Google Apps Scripts json files and a useless option removed).. With -label. Check the wiki: https://github.com/astrada/google-drive-ocamlfuse/wiki/Usage. Yes, correct. The local cache stores only some of the remote files to reduce latency.. If you can use -debug to mount your Drive and then send me gdfuse.log and curl.log (you will find in ~/.gdfuse/default/) to alessandro.strada@gmail.com, after you get the error it would help me investigate the issue. Thanks.. OK, thanks for your feedback.. You could use a wrapper shell script (like the one for automounting) and add -debug inside that script.. Upload starts only after calling flush(), release() or fsync() on the specific file.. You can try calling flush (on the file) and os.fsync (on the file descriptor) to verify if upload starts\n(see http://stackoverflow.com/a/7127162/2860133). Or you can try running sync(1) from the shell, this should start uploading all the pending files.. It looks like the same issue reported here.  It also looks like removing package libgnutls-deb0-28 should be enough.. Do you get an error?. It looks like an issue similar to this one. You should check that user dragondon has the same idenitity inside and outside the chroot.. To install, try with opam.. The sqlite error means that your drive is full. With -debug every byte exchanged with Google gets logged. Try with -verbose.. Yes, you can choose a custom folder setting cache_directory= in the configuration file.. Can you please send the log files to alessandro.strada@gmail.com? Thanks!\n. No sorry, this application is not like the DropBox one. The DropBox client syncs your files between a local directory and a remote one. My app gets you a view of your files in the cloud through FUSE that unfortunately hasn't a notification API yet (see #230).. Closing as duplicate of #230.. google-drive-ocamlfuse will get your encrypted files as they are on Drive. So you have to decrypt them locally in another directory, otherwise the decrypted files will be uploaded as well.. You are right. Fixed. Thanks!. You have to unmount and remount.. Sorry, but I don't use that software.. I'm sorry but I don't have a Mac, so I can't reproduce your issue.. Have you tried with -o allow_other?. Can you please mount with -debug and after you get the Input/output error send the log files to my email (alessandro.strada@gmail.com)? Thanks!. @batgau71: Could you remount with -debug and (after you get the Input/output error) send the log files (gdfuse.log and curl.log) to alessandro.strada@gmail.com? Thanks!. @foats: yes, please. Sorry, but I cannot reproduce your issue. Anyway, note that Google Docs, Sheets and Slides are read-only by default because they are exported and Google doesn't give access to their native format. So if you only have those kind of files, you cannot get the write permission whichever umask you choose.. Files are read-only if one of these conditions apply:\n File is a Google Document\n GDrive API returns field canEdit=false\n* stream_large_files=true and large_file_read_only=true. 0.6.18 is not yet released (there are no new features yet, I just pushed some updates for the web site).. Have you installed the depext plugin with opam install depext?. @jrarseneau: I don't think I need curl.log to fix this issue (so, the next time, if you want, you can remount just with -verbose that logs only to gdfuse.log). Can you please send your gdfuse.log to alessandro.strada@gmail.com? Thanks!. Unfortunately this issue is not easy for me to reproduce. It's not an error that gets logged, but it's probably a deadlock that stops the process execution without leaving clear traces. . I'm working on it. I think I know where the problem is, but since I cannot reproduce plex workload, I'm not 100% sure. In the next few days, I'm going to push an experimental patch that needs to be tested. If you tell me how you installed this application (via PPA or OPAM), I will post instructions on how to install this experimental version.. With opam you can test the new beta version with:\nopam pin add google-drive-ocamlfuse https://github.com/astrada/google-drive-ocamlfuse.git#beta\n\nTo restore the stable version:\nopam pin remove google-drive-ocamlfuse\n\nIn the next days, I'm going to publish this beta version to the beta PPA.. I've just published this new version to my beta PPA (https://launchpad.net/~alessandro-strada/+archive/ubuntu/google-drive-ocamlfuse-beta).. Can you please email me your log files? Thanks. @hjone72: Yes -debug dumps every byte exchanged with the server. Using -verbose should be enough for troubleshooting this particular issue.. @hjone72, google-drive-ocamlfuse -version should return 0.6.18 (that's the new beta version).. @buttpain: Which values have you set up in max_memory_cache_size, memory_buffer_size, and read_ahead_buffers?\n@jrarseneau: Are you experiencing abnormal memory usage? Anyone else?. > My understanding was the max_memory_cache_size was an absolute max?\nYes, it should be, but that's about the memory cache. Anyway, did you notice an increase of memory usage between this version and 0.6.17?. @hjone72: gzip should shrink them a bit. Then you could just email them to me (alessandro.strada@gmail.com) or share them to me on Drive or Dropbox, whatever you prefer. Does it work for you? Thanks. @ne0ark: if your mount dropped unexpectedly, there could be unreferenced files in the local cache that will not get cleaned up. Otherwise, if you can reproduce the issue after remounting with -cc -verbose, could you send me your gdfuse.log, so that I can investigate the issue? Thanks.. Not really. Did you experienced issues?. @dany20mh: What kind of drops? Do you get \"Fatal error: out of memory\" too?. @gabo77: Have you tried reducing max_memory_cache (e.g. to 1073741824 = 1GB)? Do you still get OOM errors?. I've prepared a shell script to monitor memory usage of google-drive-ocamlfuse. It logs memory stats (every 5 seconds) to /tmp/gdfuse.mem.log. If you try it, after you get an OOM error, you could send the log file to my email (alessandro.strada@gmail.com), so that I can see how much memory was allocated at the time the program crashed.. @animosity22: it looks like (from feedback I got from different users) a plex scan puts a lot of memory pressure on the app, so it takes a bit of trial/error to find the max memory setting that fits your server memory under that kind of load. (Or, if you know you have to do a full scan, you could reduce temporarily your max_memory_cache_size. The only drawback is that anytime you change that setting you have to unmount/remount your Drive.). @mattbator: OOM errors will not be logged, because the OS kills the process before it can write anything to the log. And the weird state you get is caused by the crash, so that the app doesn't unmount cleanly your directory (you should run fusermount -u to restore the mountpoint state).. @jrarseneau: have you tried setting large_file_read_only=false with version 0.6.18? This way all your files should be modifiable.. I've just uploaded a new beta version (0.6.19) that should fix a problem caused by timeouts during streaming. In version 0.6.18 if you got a connection timeout streaming a block, and the same block was waiting to be read by another thread, you got a deadlock that stopped the process, making the mountpoint unresponsive. . @animosity22: thanks!. @jrarseneau: yes, it is. @raidzx: if you get out of memory errors, you should reduce max_memory_cache_size. If it's another problem, please remount with -verbose and send the log file to my email, thanks! The issue I fixed made the mountpoint unresponsive, it didn't crash the application.. @DTrace001: opam list still shows version 0.6.17, because that is the last version in the official repository, but if you run google-drive-ocamlfuse -version you should see 0.6.19. If not, run opam reinstall google-drive-ocamlfuse.. @halolordkiller3 : opam still shows version 0.6.17, because that is the latest version in the official repository, but, after installing the beta version, if you run google-drive-ocamlfuse -version you should see 0.6.19. If not, run opam reinstall google-drive-ocamlfuse.. From the web interface, there is an \"Add to My Drive\" button (in \"Shared with me\" section, or in the context menu). Otherwise, you can set shared_with_me=true in the config file to have read-only access to all your \"Shared with me\" files under ./.shared.. Dependencies are installed with the deb package. Check that you have enabled Drive API from Google API Console (if you authorized the app with your custom client id/secret).. No, it shouldn't. Could you please remount with -debug and after you get the Input/Output error send gdfuse.log and curl.log to my email (alessandro.strada@gmail.com)? Thanks. Mount-point permission is set when mounting. But be warned that uid/gid are only persisted for backup purposes. They are not enforced, so any user who can access the mount-point can access any file.. @justinglock40: have you checked the wiki page about headless mode?. > So inside the config file there is cache folder section, which if I set it or not, it will only carry a cache.db file that won\u2019t go high that much in size (maybe 15-20 mb), and -cc command will clear that.\n\nAnd then we have max memory cache, which if I set it to 10Gb, it will exactly use 10Gb of my memory and stay there.\n\nThat's if you set stream_large_files=true.\n\nSo I\u2019m not sure how the cache handling here, do we only keep the cache inside the memory and when ever the drive unmount or anything else, the cache is gone, or do we have option to cache the stuff inside a folder (on hard drive) too?\n\nMemory cache is just for streaming. If you set stream_large_files=false, accessed files will be (fully) downloaded to the persistent disk cache. But be warned that if you have very big files (e.g. videos) those will be fully downloaded to the disk cache before you can access the first byte of the file (so it can be very laggy).. No, sorry.. Is it slow even from the shell (using cp for example)?. The problem is that this feature it's only available as part of G Suite, and I don't have a G Suite account.. @CzechJiri: Thanks! That would be great! Ping me at alessandro.strada@gmail.com. > If you integrate the team drive functionality, we would pay you for this feature.\nNo need, thanks. I need only a temporary G Suite account. Please contact me at alessandro.strada@gmail.com. @HelgeS, thank you very much! I'll let you know if I need something to test the implementation.. @HelgeS, I think I have an experimental version that you can test. Let me know if you installed google-drive-ocamlfuse via opam or ppa, so that I can give you instructions on how to install the test version. Thanks!. @d235j I added team_drive_id= in config where you can put the id of the root folder of your team drive. You can get the id from the web interface, clicking on your team drive and copying the id you can find in the url (after https://drive.google.com/drive/folders/). I didn't implement a search by name because it requires admin privileges.. To install the new test version via opam, you can use the following commands:\nopam pin -n add gapi-ocaml https://github.com/astrada/gapi-ocaml#master\nopam pin -n add google-drive-ocamlfuse https://github.com/astrada/google-drive-ocamlfuse#beta\nopam update\nopam upgrade\n\nto restore the stable version:\nopam pin -n remove gapi-ocaml\nopam pin -n remove google-drive-ocamlfuse\nopam update\nopam upgrade\n\n. I've managed to upload updated packages (google-drive-ocamlfuse v0.6.23/gapi-ocaml v0.3.6) for zesty and xenial. Unfortunately I wasn't able to update artful because the OCaml packages I depend on are broken and trusty because the OCaml compiler is too old and doesn't support the last changes I had to make for compatibility with the new OCaml compiler (4.06.0).. Thanks @Tigerhacker. I added a new wiki page with your instructions.. @Stepulin thanks!. No, sorry, there is no offline mode. If you need to keep a local copy of your files, you should install a sync app (e.g. https://github.com/odeke-em/drive).. Feel free to update the docs. Thanks!. > Is there way to check progress of this background upload process, or to disable it so cp command would be done synchronously with the actual upload?\nYes, you can set async_upload=false in the config file.. > So is there any way to check status of this background upload process?\nNo, sorry (FUSE API was not really designed with remote file systems in mind). If you need more control, you should switch to another app (e.g. https://github.com/odeke-em/drive).. How are you building the app? With opam?. Have you tried following these steps?. That's probably caused (as you suspected) by ocurl version mismatch (opam ocurl is newer than arch's one).. > Okay does that mean that there is a bug in the Arch package? Should I report it?\nSorry, but I don't know enough about Arch packaging to answer. Probably that package doesn't work with opam installed (but it's just a guess).. I've uploaded packages for zesty.. Can you please send your gdfuse.log to my email (alessandro.strada@gmail.com)? Thanks. @saitoh183: next time can you send me your core dump (enable it with ulimit -c unlimited)? Thanks!. @saitoh183: ulimit -c unlimited enables processes to produce a core file when they segfaults (see http://stackoverflow.com/questions/17965/how-to-generate-a-core-dump-in-linux-when-a-process-gets-a-segmentation-fault). So if you run that command before google-drive-ocamlfuse segfaults, you will get a core file that you can send me for further analysis.. > Do the 403 rate limit messages appear only from the debug output?\nYes, in the curl.log file. But after a 403 error, you should get an I/O error at the file-system level.. Synchronized means that a file is already been downloaded and the cached copy (on disk) is valid. ToDownload means that the file must be (re)downloaded (either was never been downloaded or the current cached copy is outdated). But with stream_large_files=true, large files are not downloaded to disk, so they are always ToDownload.. Well, but /media/tnvepu36qiohcun8v84ddhsam0/7o55b0s5gogd54idl8c8ape5jd8579gr0oc4tcun9rgicq267b90 is a folder. So it's not a regular file, folders are ToDownload if you never accessed them, and are Synchronized after you access them (with ls, for example).. > You would really need @astrada to comment on how seeking of a file works or if it grabs a full file.\nIt depends: if stream_large_files=true, it downloads only the chunk containing the requested bytes (2 chunks if the requested segment crosses chunk boundary). Otherwise, it will download the full file unless there is a valid copy in the cache.. No, I don't think so. Not yet, at least, until Microsoft implements this feature.. Should be fixed in 0.6.20 (beta channel).. Metadata are cached in a sqlite3 DB, and changes are pulled from the API every metadata_cache_time (in the config file) seconds.. Probably unionfs does some caching on its own (probably in memory).. With that setup, you get 14 blocks of 512MB. When you access the first byte of a file, it starts downloading the first block (of 512MB), then it spawns 5 threads to download the next 2.5GB of the same file. If you access another file, it does the same, starting to download 3GB (in 6 parallel threads). There are still 2 free blocks (you used 2 blocks for streaming and 10 blocks for read-ahead), so if you access a third file, you start downloading 3GB of this new file: you download the first GB, but then you need 4 more blocks. The oldest 4 blocks (sorted by last access) will be reused and overwritten by the content of this new file.. Should be fixed in 0.6.20 (beta channel).. Should be fixed in v0.7.0+.. Probably the upload of the unavailable files is not yet completed. If you want, you can turn async_upload=false, so that the copy/rsync process doesn't end until the file is fully uploaded.. > When running with -debug, it hangs forever here:\nWith -debug the process remains in foreground, so you should open another shell to access your mountpoint. Then, when you get an input/output error, you can check the log files (and in curl.log you should see if Drive returns some kind of auth error).. This link should work. I've updated the wiki. Thanks.. You should turn on verbose logging with -debug. Then you can check curl.log to see what kind of error you are getting from Drive API (if you don't find anything, please send the log files to my personal email: alessandro.strada@gmail.com).. Duplicate of #303.. Have you added my PPA to your repositories?\nsudo add-apt-repository ppa:alessandro-strada/ppa\nsudo apt-get update. If you have another box with a browser, you can authorize the application on that PC, and then copy `~/.gdfuse/default/state` to your headless box.. No, that file would be present if you ran the application on your mac. If you run the application from the headless box, copy the link on your mac and authorize it, it should be enough to wait for the application to fetch the refresh token. If it timeouts, you should retry.. @jsjcjsjc: I'm sorry but jessie is not an Ubuntu distribution. Packages are available for artful, zesty, xenial, and trusty.. If you are using colab (that uses an obsolete Ubuntu version), please check this workaround: https://github.com/astrada/google-drive-ocamlfuse/issues/493#issuecomment-422380636. No sorry, you should patch your kernel and libfuse (see issue #128). Or maybe you can try another client, like rclone that AFAIK caches everything in RAM so it should be faster.. In version 0.7.1, I added a new config option (`write_buffers`). If you turn it to `true`, it caches files in memory before uploading. It should speed up uploads. It's still experimental, so turn it on at your risk. :wink:. It writes to the filesystem only when there are no more free buffers (`max_memory_cache_size` is the total cache size and `memory_buffer_size` is the size of single buffers). Since version 0.7.0, the writes to the sqlite3 db are also cached in memory and flushed every `metadata_memory_cache_saving_interval` seconds. . Looks like you are using an old kernel (see https://askubuntu.com/questions/687369/could-not-open-builtin-file-modules-builtin-bin).. You should remount with `-debug` and then (after running ls in the mountpoint) check `~/.gdfuse/[label]/curl.log` to find what kind of error you are getting from Drive API.. After mounting you have to reproduce the Input/output error, then you should check curl.log.. With `-debug` the process remains in foreground, you should open another shell (or add an `&` to the mount command).. You should enable logging with `-debug`, then check the logs to see if you are getting timeout errors (or cannot connect errors if you are behind a proxy).. OK, thanks for your feedback!. Should be fixed in version 0.6.21 (beta channel).. It's a bug that I'm fixing in the next beta version, but it should be safe to just kill the process after unmounting (it's waiting for a thread that's not running).. Should be fixed in 0.6.20 (beta channel).. You should enable `-verbose` logging, then check what the application is doing during unmount (maybe sometimes your connection goes down?).. Should be fixed in 0.6.20 (beta channel).. Your should restart it with `-debug`, then check gdfuse.log and curl.log (in ~/.gdfuse/default/) to see what kind of error you are getting (it looks like you cannot access the authorization endpoint).. Which version are you running?. After authorizing you should wait for google-drive-ocamlfuse writing \"Access token retrieved correctly.\" on the console. If it fails to retrieve the token in 5 minutes, you should enable logging with `-debug` and then check this log file: `~/.gdfuse/default/curl.log`.. > google-drive-ocamlfuse shows only my Google Drive content.\n\nYou can access shared files that are not in your drive setting shared_with_me=true in the configuration file. They will be available in the special directory .shared.\n\nBonus if anonymous access (mounting shared link to google drive without logging in) will work.\n\nDrive API doesn't allow anonymous access.. That option is already documented in the configuration page of the wiki. The wiki is already referenced by README.\n. Folders/files get renamed if there is more than one file/folder with the same name.. > Is there a way to clear the cache or something ?\nThe -cc command line option clears the cache before mounting.. > Can anyone re-assure these errors from opam wont affect the installation of google-drive-ocamlfuse...\nYes, those errors are caused by unrelated packages.. It looks like opam cannot find a compiler and it tries to install the latest experimental one. Maybe you have to run eval $(opam config env) or you have to re-init opam with opam init --comp 4.04.2.. Please, try installing with OPAM (https://github.com/astrada/google-drive-ocamlfuse/wiki/Installation#installing-with-opam). And be sure to use version 4.04.2 of the compiler.. You should have authorized the application in the web page that should have been opened. If not please retry (if it still doesn't work, please remove the content of your ~/.gdfuse/default directory).. I checked the logs of the oauth endpoint. From about 2 am to 3 am of February 11, there are several exceptions like this one type=InternalTransientError message=Temporary error in fetching URL: https://accounts.google.com/o/oauth2/token. It looks like it was a Google temporary error.. > Is it possible to delete the content of the Trash folder with it?\nYou should set delete_forever_in_trash_folder=true in config.. The local mountpoint is updated only every metadata_cache_time seconds (default is 60 seconds). So it's normal not to see the added document immediately after adding it to you Drive from the web interface.. The update is not automatic, you need to run some command to refresh the mountpoint (e.g., ls).. > Mine sometimes seems to fill up when transferring large files, even though stream_large_files=true\nstream_large_files doesn't work when uploading.\n\nyes, I checked and delete them manually with sudo rm -rf ~/.gdfuse/default/cache/* but my disk still 100%\n\nThey are probably still being used. Unmounting/remounting should free the disk space.. > Will there be an equivalent for uploading in the future? (sorry for the off-topic)\nNo, sorry. The problem is that Drive API only allows uploading whole files.. Because that's how a FUSE driver works. When you copy a file from your local file-system to a FUSE file-system, the operative system issues a sequence of calls: a mknod(2) to create the file, and a series of write(2)s that copies the file by block (of 4KB or 128KB). Eventually flush(2) and close(2) are called to end the copy process and close the target file. So I don't have a single operation that I can stream, but I have a lot of small operations involving file chunks that I have to cache before I can start the uploading process.. You can try modifying this config options: max_memory_cache_size, memory_buffer_size, read_ahead_buffers (see the wiki to know what those parameters are used for).. > is there any other settings?\nNo, sorry.. From the logs, it looks like there is some problem with ocamlnet package in Fedora 26 (netsys library is provided by ocamlnet).\nError: Files /usr/lib64/ocaml/netsys/netsys_oothr_mt.cmxa\n   and /usr/lib64/ocaml/threads/threads.cmxa\n   make inconsistent assumptions over implementation Thread\n\n. Yes, you can specify max_download_speed and max_upload_speed (check the wiki for further info).. That's is caused by limitations in Drive API combined with FUSE. If you need maximum upload/download speed, try other Drive clients: e.g. https://github.com/odeke-em/drive or https://github.com/ncw/rclone.. Can you please send your log files to my email (alessandro.strada@gmail.com), after getting the error? Thanks!. OK, thanks. It looks like an error similar to this one. Maybe you can try to recompile the OCaml compiler with -D_FILE_OFFSET_BITS=64.. Maybe you can try installing google-drive-ocamlfuse with OPAM (see https://blog.danny-willems.be/ocaml-raspberry-pi/, but instead of ocaml 4.03.0, that's quite old, you can install ocaml 4.04.2). See also this link for installing instructions with OPAM.. I see, sorry. When I have some spare time, I will try to reproduce the issue on a virtual machine.. No, sorry. I'm on vacation and I don't have a stable connection. Feel free to try another Drive client (e.g.: https://github.com/odeke-em/drive or https://github.com/ncw/rclone).. You can try with version 4.04.2 of the compiler:\nopam switch 4.04.2\neval `opam config env`\n\nThen you should be able to install ocamlnet.. Can you please remount with -debug and after reproducing the error, send the log files (curl.log and gdfuse.log in ~/.gdfuse/default) to my email (alessandro.strada@gmail.com)? Thanks!. Closing as the problem seems to be in aria2.. The problem is that this feature it's only available as part of G Suite, and I don't have a G Suite account.. Closing as duplicate of #288.. I'm sorry but I cannot reproduce your problem (I don't get album names under Google Photos directory). Can you please remount with -debug and after reproducing the error, send the log files (curl.log and gdfuse.log in ~/.gdfuse/default) to my email (alessandro.strada@gmail.com)? Thanks!. If the target directory isn't empty, you should use this command line option: -o nonempty.. Unfortunately snap has limited support for FUSE (https://docs.snapcraft.io/the-fuse-support-interface/7816): The interface is not automatically connected, there is no support for unprivileged mounts, and mountpoints are limited to some directories.. You don't need to run ocaml setup.ml -configure. That step is only needed if you want to compile the app from source. You just have to run google-drive-ocamlfuse, authorize the access to your Drive and rerun specifying a mountpoint.. On master, I've switched to jbuilder (instead of oasis). So you have to run\njbuilder build @install\n\nto build it. If you have problem with the AUR package I suggest you to install through opam.. You should be able to specify uid and gid when mounting. E.g. -o gid=1000,uid=1000. Should be fixed in v0.7.0+. If you are using a headless host, you should follow these instructions.. No, sorry, this isn't a sync app. You should consider using https://github.com/Grive/grive or https://github.com/odeke-em/drive.. > But, I'd like to know if it's possible to limit the bandwith up/down with google-drive-ocamlfuse.\nYes, there are max_download_speed and max_upload_speed config options.. You should turn on logging (with -debug) and then check the log files to see what kind of errors are you getting.. >  I had some files that were uploading during this that I no longer have easy access to. The .gdfuse/default/cache folder is 32 GB.\nIn the cache folder you can find all the files that were uploading. The name of each cached file is the Google Drive id of the file.\n\nthis recoverable?\n\nNo sorry, it looks like your sqllite db is corrupted (Sqlite3 error: CORRUPT).\n. > If it isn't, can I reset the state of the folder and database without re-setting up the entire folder and settings?\nAfter recovering your files, you can remove everything under .gdfuse/default/cache and you should be able to remount your drive.. When you upload a file, the extension determines its MIME type. Unfortunately that info is not modifiable once the file is created, so if your editor creates a temporary file without extension and then renames it to .txt, your file on Drive will still have application/octet-stream as MIME type.. Those are OPAM errors, but I don't think they are blocking errors.. I think you have a problem with libgmp-dev. Have you installed the external dependencies?\nopam install depext\nopam depext google-drive-ocamlfuse\n\n. Maybe, you can try downgrading zarith to 1.5:\nopam pin add zarith 1.5\n\n. Thanks!. If you need maximum upload speed, consider using https://github.com/odeke-em/drive or https://github.com/ncw/rclone.. Sure it is. But slow uploads are caused by FUSE + Drive API. So if you need fast uploads, feel free to use another client.. > At least as far as fuse is concerned have you filed a bug report? I have no idea what to tell them if I do it.\nNo, it's not a bug. The problem is that FUSE is not designed to serve remote filesystems.. File ID is stored in the sqlite3 DB (in ~/.gdfuse/default/cache/cache.db): column remote_id of table resource. If the file is cached, the corresponding file in ~/.gdfuse/default/cache/ has the file ID as filename.. You can try downgrading zarith to 1.5:\nopam pin add zarith 1.5\n\n. You need cppo. But you should probably try to install using opam.. Downloading/seeding torrents is not really supported because Google Drive API doesn't work well with file chunks, but accepts only whole files.. I'm sorry, but this is not a sync tool. It just lets you mount your Google Drive as a local filesystem, but you have to be online as there is no offline mode. Consider using https://github.com/odeke-em/drive or https://github.com/ncw/rclone.. It looks like you have a problem with certificates on your host. Check this answer on StackOverflow.. No, downloading torrents to the mount-point is not really supported because Google Drive API doesn't work well with file chunks.. Yes, but I would copy the files instead of moving them, to be sure that they are uploaded without errors (then you can remove the old files).. Thanks!. Are the userid/groupid of your user changed after the upgrade?. Have you already tried restarting from scratch? (Removing ~/.gdfuse/default and reauthorizing the app?). If you add them to your Drive, those files will be available as standard files.. > I assume this is because Google insists on wasting their CPU time \"compressing\" stuff and then sending them after that is complete so that the transfer cannot be streamed in a sensible way?\nNo, if you don't enable streaming, files are downloaded to the cache before being accessible. But note that if you enable streaming, large files are made read-only by default.. > Is there a scheduled task to generate md5 sums and it was just taking a really long time?\nNo, md5 checksum is recalculated only before downloading, to check if the cached copy is still synchronized. Maybe the Drive API was returning that those files have been modified (maybe just metadata) and that triggered downloading those files again.. >  I cannot download the files with my own account, unless I share to \"anyone with the link\" and open on incognito tab.\nSo, you can download it via google-drive-ocamlfuse but not via web interface?\nIt that's the case, have you tried reauthorizing the app?. > I don't know if I understand what do you mean by \"download via google-drive-ocamlfuse\".\nIf you mount your Drive, can you access that file (e.g. copy it to another local directory)?. Even after clearing the cache? If so, I would try reauthorizing the app (removing everything under ~/.gdfuse/default and starting over).. If your file is already cached (it's cached before uploading), you will always be able to access it without accessing Drive API, so any error that would be returned by the API won't be triggered without clearing the cache.. No, it should be accessible (I never experienced your issue). If I go to the web interface and check the details of every file uploaded with google-drive-ocamlfuse I get \"Viewers can download\".. Maybe it's an issue of the session in your web browser:\nhttps://productforums.google.com/forum/#!topic/drive/BwruvJYKzWU\nhttps://productforums.google.com/forum/#!topic/drive/yTMj7Icwm3U. You're welcome!. > Since the application unblocks before the upload has finished (or even started), is there any reason why it's blocking for so long?\nThat's because the application needs to copy the file to the local cache before uploading.\n\nIs there any way I can mitigate this (possibly even to 0s since write appears to be async?)\n\nNo, sorry, I don't think so.. You can enable verbose logging (with -verbose), then check ~/.gdfuse/default/gdfuse.log. It reports timestamps for each operation. mknod is called when creating a new file, e.g.:\n[267.602869] TID=293: mknod /mp3/test.mp3 100600\n\nThen you should see a series of write calls:\n[269.549571] TID=296: write /mp3/test.mp3 [131072 bytes] 0 0\n[270.377098] TID=298: write /mp3/test.mp3 [131072 bytes] 131072 0\n[270.848342] TID=300: write /mp3/test.mp3 [131072 bytes] 262144 0\n...\n[275.794260] TID=326: write /mp3/test.mp3 [48605 bytes] 1966080 0\n\nAnd finally a flush:\n[276.051459] TID=327: flush /mp3/test.mp3 0\n\nAnd that starts the upload:\n[276.278254] TID=328: BEGIN: Uploading file (id=76, path=/mp3/test.mp3, cache path=/home/alex/.gdfuse/default/cache/1GNtRaShfc8Pzu7uSkxm85bvNZCPkkiGv, content type=audio/mpeg, content_length=2014685).\n...\n[280.473024] TID=328: END: Uploading file (id=76, path=/mp3/test.mp3, cache path=/home/alex/.gdfuse/default/cache/1GNtRaShfc8Pzu7uSkxm85bvNZCPkkiGv, content type=audio/mpeg).\n\nDuring this test, I copied a 2MB file to my Drive: 9 secs for copying the file to the cache (on a 5400rpm HD), and 4 secs for uploading to Drive (on a 4G connection).\n. Your issue is caused by the fact that FUSE is using the default 4kb block size:\nwrite /testdir/testfile [4096 bytes] 0 0\n\nIn theory big_writes should be turned on by default and you should see 128kb blocks. Are you on an old kernel (https://sourceforge.net/p/fuse/mailman/message/20806709/)?. > I'm not convinced it's a general FUSE limitation\nNo, not really. It's a combination of FUSE+Drive API. But the real issue is why big_writes doesn't work for you.. > How does Google Drive API have anything to do with copying to local cache?\nFUSE writes are block by block, but Drive API doesn't support uploading a single block, you have to upload the whole file. That's why files need to be cached and also why it's slower than sshfuse (because SSH supports chunk uploads).. If you need to max out your upload bandwith, consider using https://github.com/odeke-em/drive (not based on FUSE) or https://github.com/ncw/rclone (that IIRC caches everything in memory).. In version 0.7.1, I added a new config option (write_buffers). If you turn it to true, it caches files in memory before uploading. It should speed up uploads. It's still experimental, so turn it on at your risk. :wink:. I'm working on this issue. The new compiler (4.06.0) uses a special flag that causes that error. If you are in a hurry please switch to 4.05.0.. Should be fixed in version 0.6.22 (with gapi-ocaml 0.3.5).. You should enable debug logging with -debug, then check ~/.gdfuse/default/curl.log where you can find if you are experiencing timeouts or other errors during connection with the auth proxy.. What about gdfuse.log?. If your DNS is slow you can raise connect_timeout_ms in config.. Thanks! I really appreciate this.. You should enable debug logging with -debug, then check gdfuse.log and curl.log in ~/.gdfuse/default and look for errors.. > Should I remount the drive or there's a way to activate the debug without remounting?\nNo, sorry, you have to remount.\n\nAlso, does the -cc clear the logs?\n\nNo, logs are overwritten every time you remount.. > Thread 6730 killed on uncaught exception Sys_error(\"/root/.gdfuse/default/cache/1oZJgCWYaFiS_L_yqQBYko7xxQenjrDql: No such file or directory\")\nIt looks like the cached copy of the file was deleted before the upload was completed.. You should remount (with -cc if it does still not work).. If the process is still running you should kill it. Then you should unmount with fusermount -u mountpoint and then remount.. I think you should be able to set root_folder= to the root folder id you can see in the web interface. Remember to remount with -cc since you changed the root folder, or to use a new label.. It looks like you are fetching packages for Debian Jessie, but the PPA contains packages only for Ubuntu (xenial, zesty, ecc.).. If you want you can enable debug output (with -debug) and check the log files (gdfuse.log and curl.log) in ./gdfuse/googledrive to see what it's doing.. With -debug the process remain in foreground. You should check the content of the log files to see what's going on.. I think the problem is that when you initialized the application with google-drive-ocamlfuse -headled -id... you didn't specify a label (so default label is used), then when you tried to mount the filesystem you specified -label googledrive. You should specify the same label when authorizing and when mounting.. I don't think so (file permissions are not enforced). I think your best option would be to put user files in a staging area then periodically push them with something like drive.. If you enable Google Photos in you Drive, you should see a Google Photo folder in your Drive that you can access with google-drive-ocamlfuse.. I'm sorry but I think that it's not possible to upload photos to Google Photos via the Drive API. You can only upload files.. That error is returned by Google Chrome, and you may ignore it safely. You can reproduce it starting Chrome from bash:\nalex:~$ google-chrome-stable \n[19866:19905:1226/140823.290855:ERROR:browser_gpu_channel_host_factory.cc(107)] Failed to \nlaunch GPU process.\nCreated new window in existing browser session.\n\n. OK, thanks!. Yes, you can edit file state (in ~/.gdfuse/default). There's a key named refresh_token.. A web browser is required by Google for the authentication/authorization, but can use a web browser on another host (see here).. Should be fixed in 0.6.24 (beta channel).. Unfortunately the algorithm is quite complicated. I use the field fullFileExtension (see https://developers.google.com/drive/v3/reference/files), that's basically what Drive API thinks the full extension is. I didn't use fileExtension because this way you don't correctly detect files like .tar.gz, .tar.xz and so on. If you enable logging you should see lines beginning with Filename collision detected: and maybe you get a hint at how the collision is resolved. If you read OCaml, you can find the algo in function disambiguate_filename of file drive.ml.\n\nAnd why is a space added? Space in filenames in Unix/Linux is not nice ;-)\n\nBecause this is what Chrome/Chromium does :wink:.. Sorry, but I cannot reproduce. Have you tried cleaning apt cache?. Should be fixed in 0.6.25 (beta channel).. > the IDEs become very slow. In addition, most of the operations on the files in google drive folder are much slower compared to the official google drive clients of Windows and Mac.\nThat's because Windows and Mac clients are not based on FUSE. You work on your file-system, and those clients keep it in sync with the remote version. If you need similar functionality you should try grive or drive.. No, sorry, I had to remove the async upload option because it increased the probability of losing data.. Because it would be a nightmare to re-synchronize after you get back online. Anyway you can emulate that scenario putting a very high number in metadata_cache_time.. > How do I correctly specify the root_folder?\nIt should be right as you are doing. Alternatively you could specify folder id that you can get from the web interface (it's the id in the url after https://drive.google.com/drive/folders/). Remember remount with -cc after you change the root folder.\n\nHow do I activate reading the config file without going through unmounting and mounting the drive again?\n\nThere is no way. If you need to have more than one root folder, you should use different labels.\n. After changing root_folder you should clear the cache:\ngoogle-drive-ocamlfuse -cc /home/rstudio/'06 Machine Learning'\n\nBut your problem is caused by some error that's occurring during mount. You should be able to get the exception from log files. It's probably due to user permissions of rstudio.. No, sorry. Different APIs.. Sorry, but I'm not able to reproduce it. (I see the subfolders in my Team Drive.). You should unmount your drive (fusermount -u MOUNTPOINT), before uninstalling.. > Thread 511 killed on uncaught exception Failure(\"Error: Invalid request.  There were 17 byte(s) in the request body.  There should have been 62 byte(s) (starting at offset 0 and ending at offset 61) according to the Content-Range header. (HTTP response code: 400)\")\nThat's the problem, I think. If you can reproduce it with debug log on, please send me your log files to my email (alessandro.strada@gmail.com). Thanks!. It's probably a race condition: borg is too fast and modifies a file before it's completely uploaded. You can try to mitigate the issue turning off async uploads, with async_upload=false in config. If it doesn't work, you can try to remount with -s to turn off multi-threading (at risk of making your mounted file-system unresponsive).. Thanks for your feedback!. > In the config there seem to have no such option now but there is an option for cache size, would putting that to 0 turn off the cache?\nNot really. The cache should keep just one file that is the current file being uploaded/downloaded (but I never tested that scenario).. > In addition, the reason why I thought the cache manage more then 1 file is I looked at the code only very briefly but there seem to be a lot more cache code then necessary to manage just 1 file, like a full sqlite db.\nNo, sorry, I wasn't clear: the cache normally keeps more than one file and indeed there is a sqlite db to cache metadata, but if you put 0 in max cache size, it should cache just one file, upload/download it, and then remove it from the cache.\n\nSo should we close this and I can open another issue as a feature request for turning off the cache?\n\nNo, there is no way to turn off the cache completely because otherwise you wouldn't be able to upload files. If you just need a read-only copy with no cache, you should turn on streaming. But keep the issue open, as a reminder to change the default for async_upload.. async_upload was turned off by default and removed in v0.7.0.. Unfortunately it looks like Plex has a pattern of usage that probably causes a deadlock in my application. The problem is that I cannot reproduce it (not having Plex), so I don't know how to possibly fix it.. You can use any browser that can login to your google account and authorize an app, but I don't know any CLI-based browser that can do that.. If you manage to successfully access to your Google Account with elinks, you can just set it as your default browser (https://askubuntu.com/questions/788043/where-to-change-how-xdg-open-opens-urls-sync-with-kde-open), and it will be opened to authorize the application.. If you want you can authorize the application using another device (e.g., a smartphone or a tablet). Check the wiki (https://github.com/astrada/google-drive-ocamlfuse/wiki/Headless-Usage-&-Authorization).. > Also, is it possible to just delete state in ~/.gdfuse/label to refresh the access tokens?\nYes you can do that.. Check the wiki: https://github.com/astrada/google-drive-ocamlfuse/wiki/Automounting. If you can't remove it, it's probably still mounted. So before deleting, you should try to unmount it with fusermount -u MOUNTPOINT. Then, you should check if you still have some process named google-drive-ocamlfuse and kill it.. You don't need to mount before following automount guide (you mount your Drive as the last step of the guide).. Yes, it's a bug that should be fixed in version 0.6.25. It's on the beta ppa.. Yes, unfortunately the artful OCaml packages are broken. You can try to download the bionic .deb and install it manually.. If fusermount is not installed, try to install fuse-utils.\n\nActually, if there is some files in data folder, there will be another error information.\n\"fuse: mountpoint is not empty\"\n\nIf you want to force mount even if your mountpoint is not empty you can use -o nonempty. Although your files in that directory will not be available until you unmount your google drive.. Maybe your issue is similar to this one?. > workaround: don't use Chrome/Chromium ?\nI'm using Chrome without issues. Never tried with Chromium.. Probably your user cannot access fuse drives (maybe it doesn't belong to fuse group).. The max cache size is ignore during uploads (because Drive API doesn't offer a method to upload only a portion of a file). If you are sure you are not modifying those big files locally, you could turn on stream_large_files and set an appropriate large_file_threshold_mb.. @MartinX3 if you don't need to upload big files, you can enable streaming with stream_large_files=true, reducing large_file_threshold_mb if you have lots of small files.. > Which disadvantages will stream_large_files=true gives me?\nIt works for download only. Uploading still requires disk cache.\n\nIs it possible to move the cache into the RAM?\n\nNope, sorry (unless you setup a ramdisk).. No, sorry. Check this comment.. >Maybe work around this with \"multiple accounts\" but have them as the same account but with different roots?\nYes, that's the way to go. Each label/account can have its own independent configuration.\n\nBasically, is there an offline option or function?\n\nNo, there is no offline option (as this is not a sync tool).. > [0.034617] TID=1: Error during request: Code: 6, Description: CURLE_COULDNT_RESOLVE_HOST, ErrorBuffer: Could not resolve host: www.googleapis.com\nThis is the problem. At start time you were probably offline.\n. Thanks for your feedback.. Done. Thanks for your suggestion.. Thanks for your patch.\n\nThe doc folder appears not to be used anymore. The documentation seems to have moved to the Wiki.\n\nI periodically copy the wiki content to the doc folder, but I admit that it usually is not updated. What do you mean with:\n\nWe could move the content of this branch to the doc folder of the main branch (again) to clean up a bit. This would work with GitHub pages.\n\nThe wiki is on a different repo (astrada/google-drive-ocamlfuse.wiki.git) and not on a different branch, did GitHub changed the way you can publish the wiki?. It looks like it's still experimental: http://www.forshee.me/2016/02/22/container-mounts-in-ubuntu-1604.html\nAnyway, if you can run any FUSE implementation in your container, you should be able to run google-drive-ocamlfuse without problems.. Sorry, I don't really know much about containerization technologies. What about https://stackoverflow.com/a/49021109/2860133 ?. You should enable debugging log to see what kind of error are you getting (https://github.com/astrada/google-drive-ocamlfuse#troubleshooting).. Check umask in config, that configuration key determines the default permissions.. I don't think I understood your issue, but I would advise against using a torrent client on a Drive FUSE-mount. That causes lots of unnecessary uploads/downloads. You'd better run rtorrent on a local directory and then upload them copying the files to Drive.. If you don't need to upload big files, but you have big files you want to read (video, music, etc.) you can enable streaming with stream_large_files=true, so you don't have to download them before access.. If the process crashes, you should enable debug logging and check the logs (https://github.com/astrada/google-drive-ocamlfuse#troubleshooting). If there is no errors in the logs, it can be that you are using too much memory (have you enabled streaming? have you increased buffer sizes). If you are using plex (or something like that) there are probably access patterns that lock the process. Unfortunately, I cannot reproduce them since I don't have plex, so I don't know how to fix the issue.. > the fold 'gdrive' is still can't unmount,can't access,can't remove!\nYou should kill the google-drive-ocamlfuse process that's still running, then you can unmount the folder with fusermount and remove it.. It's probably a permission issue (like this one for example).. No, it's not possible. It works only in configuration.. > Is google-drive-ocamlfuse supposed to work that way? I thought it operated more like a mount command?\nThe process remains in foreground if you specify -debug (or -f).. The PPA is for Ubuntu. I don't think you can use it on a Debian.. Sorry, but I didn't try it myself. It was contributed.. No, the 500 error is returned by Google, so it's not an issue of the application. If you want, you can try to logout from all your accounts on the browser that is opening and try again (or clean all the cookies). But there is no way I can fix this.. Maybe there are old files not indexed anymore. If you remount with -cc and the cache keeps growing the same, you can safely add a cron job to purge files in ~/.gdfuse/default/cache and keep only cache.db.. In unix and derivatives, when you delete a file, you don't actually remove it until there are open file descriptors (https://unix.stackexchange.com/questions/237817/delete-file-while-it-is-in-use), so if the upload completes without errors you should have no problems. But it's true that if you delete a file during an upload and the upload fails, it cannot be retried.. If you can reproduce starting with a clean cache (with -cc) and with logs on (-debug), would you please send your log files (gdfuse.log and curl.log) to my email (alessandro.strada@gmail.com)? Thanks!. > Are the threads ok to die like that, or should these errors be caught and handled better?\nNo, the app should retry the request when receiving that error. If you can reproduce while logging with -verbose, would you send me your gdfuse.log (my email is alessandro.strada@gmail.com)? I don't need the whole file. It should be enough to get about 50 lines around the line where you should find the string ServiceError. Thanks!. Have you tried mounting with -o allow_other?. If you want to compile the project, it's probably easier if you install dependencies with opam (https://github.com/astrada/google-drive-ocamlfuse/wiki/Installation#installing-with-opam).. Actually, rclone handles acknowledgeAbuse. I checked their implementation because I don't have such a file (I tried to report some random file but it was not enough).. If your label contains -, you can use quotes, e.g.:\ngoogle-drive-ocamlfuse -label \"sprace-eng\" /home/vfinotti/gdrive-sprace-eng\n\n. Have you tried remounting with \u2018-cc\u2018 too clear the cache?. v0.6.21 doesn't support TeamDrives. Note that Ubuntu 17.10 has reached end of life, so I cannot upload updated packages. Try installing the bionic one if you cannot upgrade your distro.. Can you try with opam (https://github.com/astrada/google-drive-ocamlfuse/wiki/Installation#installing-with-opam)?. It looks like you have problems with the certificate store. What is the output of curl -v https://www.googleapis.com?. Have you tried getting an updated bundle (https://serverfault.com/a/293474)?. You can kill the process anyway using kill -9. The mount point will be left in an error state, so you will have to unmount it with fusermount -u. The problem is that uploading begins when files are closed/released, and in a local file-system, that operation should be very quick. In a previous version the upload process was handled asynchronously but that increased the probability of having data inconsistencies server side.. No, sorry, but it looks like rclone supports them.. I've uploaded packages for cosmic.. I've pushed the debian packaging directory in the debian branch. You can find the man page template here: https://github.com/astrada/google-drive-ocamlfuse/blob/debian/debian/google-drive-ocamlfuse.mkd. If you enable requests logging with -debug in curl.log you should find this a request like the following one that is used to list root folder content:\n[3.588306] curl: header out: GET /drive/v3/files?fields=files%28appProperties%2Ccapabilities%28canEdit%29%2CcreatedTime%2CexplicitlyTrashed%2CfileExtension%2CfullFileExtension%2Cid%2Cmd5Checksum%2CmimeType%2CmodifiedTime%2Cname%2Cparents%2Csize%2Ctrashed%2Cversion%2CviewedByMeTime%2CwebViewLink%29%2CnextPageToken&corpora=teamDrive&includeTeamDriveItems=true&q=%27[TEAM_DRIVE_ID]%27+in+parents+and+trashed+%3D+false&supportsTeamDrives=true&teamDriveId=TEAM_DRIVE_ID HTTP/1.1\n\nCheck that the parameter teamDriveId is correct. The response should look like this (in parents array you should find your Team Drive Id):\njson\n{\n \"files\": [\n  {\n   \"id\": \"FILE_ID\",\n   \"name\": \"test_trash_1.txt\",\n   \"mimeType\": \"text/plain\",\n   \"trashed\": false,\n   \"explicitlyTrashed\": false,\n   \"parents\": [\n    \"TEAM_DRIVE_ID\"\n   ],\n   \"appProperties\": {\n    \"mode\": \"33188\"\n   },\n   \"version\": \"11\",\n   \"webViewLink\": \"https://drive.google.com/file/d/FILE_ID/view?usp=drivesdk\",\n   \"createdTime\": \"2017-12-16T19:35:49.943Z\",\n   \"modifiedTime\": \"2017-12-16T19:47:29.505Z\",\n   \"capabilities\": {\n    \"canEdit\": true\n   },\n   \"fullFileExtension\": \"txt\",\n   \"fileExtension\": \"txt\",\n   \"md5Checksum\": \"d41d8cd98f00b204e9800998ecf8427e\",\n   \"size\": \"0\"\n  },\n  {\n   \"id\": \"FILE_ID\",\n   \"name\": \"TestFolder1\",\n   \"mimeType\": \"application/vnd.google-apps.folder\",\n   \"trashed\": false,\n   \"explicitlyTrashed\": false,\n   \"parents\": [\n    \"TEAM_DRIVE_ID\"\n   ],\n   \"appProperties\": {\n    \"mode\": \"509\"\n   },\n   \"version\": \"7\",\n   \"webViewLink\": \"https://drive.google.com/drive/folders/FILE_ID\",\n   \"viewedByMeTime\": \"2017-12-16T19:46:19.524Z\",\n   \"createdTime\": \"2017-12-16T19:33:16.881Z\",\n   \"modifiedTime\": \"2017-12-16T19:33:16.881Z\",\n   \"capabilities\": {\n    \"canEdit\": true\n   }\n  }\n ]\n}. Have you tried mounting with -o allow_other?. You should remove this file: ~/.gdfuse/default/state, then restart without parameters.. No, it's by design. The .odt is exported from the original Google's internal format. So if you modify it with a tool that isn't the web editor, there is the risk to mess up layout/style/etc. If you want, you can set document_format=desktop (https://github.com/astrada/google-drive-ocamlfuse/wiki/Configuration#document-export-formats) in the configuration, remount with -cc to obtain a desktop file that you can double-click to open the web editor.. I don't find https://www.googleapis.com/auth/accounts.reauth among the accepted scopes: https://developers.google.com/identity/protocols/googlescopes. Maybe that's the one causing the issue.. @JEF1056 @darylfung96: Which version of Ubuntu are you on? (You can get it running: lsb_release -a). I've removed the package for Ubuntu versions that reached end of life (zesty and artful).. @andrewkhimichuk: if you cannot upgrade to bionic, can you try downloading the package (amd64) from bionic and install it with sudo dpkg --install?. Please have a look at https://github.com/astrada/google-drive-ocamlfuse/issues/493#issuecomment-422385109. Thanks to @daniel-leonard-robinson. I just uploaded version 0.7.0. Have you tried running apt-get update again?. Which version of Ubuntu are you on?. To find out which version of Ubuntu you are running, you can use this command: lsb_release -a. Because I removed packages for zesty and artful, since they have reached end of life (https://wiki.ubuntu.com/Releases). If you cannot upgrade to bionic, you should download the package from the ppa (e.g. amd64) and install it with dpkg --install.. Have a look at this script: https://stackoverflow.com/questions/52385655/unable-to-locate-package-google-drive-ocamlfuse-suddenly-stopped-working/52387195#52387195\nBut you can also use the builtin FUSE driver: https://github.com/googlecolab/colabtools/issues/276#issuecomment-422199546\n. I removed packages for zesty and artful, since they have reached end of life (https://wiki.ubuntu.com/Releases). If you cannot upgrade to bionic, you should download the package from the ppa (e.g. amd64) and install it with dpkg --install.. Thanks for your feedback!. See #493.. I think an option is to specify allow_root (and remove allow_other) as mount option (http://manpages.ubuntu.com/manpages/trusty/man8/mount.fuse.8.html), so only the user who mounts (and root) can access the drive.. It looks like the error occurred during unmount. Have you tried remounting?. That depends on how many Google Docs you have shared. Since Google Docs cannot be downloaded lazily, you have to wait until all the docs are cached. If you don't care about Google Docs, you can set download_docs=false in the config, and this should speed things up.. Sorry, I forgot to mention that I removed that option in version 0.7.0 (I've just updated the wiki).. The configuration file is rebuilt every time you remount your drive. That's the strategy I use to migrate configuration when you upgrade the application to new versions (so you get the new options).. > There seems to be a bug because each time the drive is remounted the configuration file is overwritten with the default template.\nThis should not happen. The config file is regenerated from default values only if the file is not present, otherwise previous values should be preserved.. Thanks for your patch. The problem is that the syntax in that file is used to link wiki pages (https://help.github.com/articles/adding-links-to-wikis/#using-a-wiki-markup-syntax-to-make-links).. No, sorry. The reason for this behavior is that this kind of documents cannot be downloaded directly in their native format, but have to be exported in another format (LibreOffice format for example). I've tried to edit this kind of documents and re-import them on Drive, but usually the document layout gets messed, so I've decided to keep these file read-only to avoid unexpected modifications. As a workaround you can configure those files to open directly in the web browser, if you are not on a headless server (https://github.com/astrada/google-drive-ocamlfuse/wiki/Configuration#document-export-formats).. Why don't you try removing opam and install the package in AUR: https://aur.archlinux.org/packages/google-drive-ocamlfuse?. > Are there any updates in the 0.7.0 release that might solve this? I couldn't see any related issues that were addressed.\nYes, I made some fixes that might solve your issue.\n\nLibreOffice will indicate the the file has been modified since it was opened and request confirmation that I still wish to save anyway.\n\nThis shouldn't happen anymore. Let me know if you still experience this behavior.\n. Do you experience the same problem if you upload a new file using Google Drive's web interface?. I don't know. Maybe Mountain Duck creates a 0-length file and then uploads the content?. @canardos can you reproduce your issue mounting with -debug and send gdfuse.log and curl.log to me (alessandro.strada@gmail.com), please?. @dnapier: I'm sorry but it's not the same issue, and I don't see any error in the log files you sent me. If you want you can append an & after your mount command to keep them in background (google-drive-ocamlfuse -debug ~/gdrive &).. Should be fixed by astrada/gapi-ocaml@0dd1d4e released with gapi-ocaml 0.3.8.. Unfortunately Google Drive's API only supports uploading whole files so there are no options you can use to enable streaming for uploads. . It should remove cached files as soon as the upload ends if the total size of cache is above max_cache_size_mb.. If the google-drive-ocaml process hangs, you should kill it with -9, then run fusermount -u [mountpoint].. No, what I mean is that you don't need to reboot, you can have a single cron job that runs a script like this:\nsh\nif [ ! $(find /var/www/web1/media/videos | wc -l) -gt 1 ] || [ ! $(find /var/www/web2/media/videos | wc -l) -gt 1 ]; then\n  killall -9 google-drive-ocamlfuse\n  fusermount -u /var/www/web1/media/videos/\n  fusermount -u /var/www/web2/media/videos/\n  google-drive-ocamlfuse -o allow_other -label storage01 /var/www/web1/media/videos/\n  google-drive-ocamlfuse -o allow_other -label storage02 /var/www/web2/media/videos/\nfi. Hope that works! Sorry but I didn't tested it. Let me know if it breaks.. There is some problem with your certificates/certificate storage. Other than a failing file-system, do you have any proxy/profiler that intercepts SSL connections (something like this)?. If you mount with -debug you will see in curl.log which path/certificate libcurl is going to use. E.g.:\n[0.245063] curl: info:   Trying 172.217.23.106...\n[0.245149] curl: info: TCP_NODELAY set\n[0.278671] curl: info: Connected to www.googleapis.com (172.217.23.106) port 443 (#0)\n[0.297485] curl: info: found 133 certificates in /etc/ssl/certs/ca-certificates.crt\n[0.928481] curl: info: found 402 certificates in /etc/ssl/certs\n[0.928576] curl: info: ALPN, offering h2\n[0.928589] curl: info: ALPN, offering http/1.1\n[1.112760] curl: info: SSL connection using TLS1.2 / ECDHE_ECDSA_CHACHA20_POLY1305\n[1.233939] curl: info:   server certificate verification OK\n[1.233984] curl: info:   server certificate status verification SKIPPED\n[1.234423] curl: info:   common name: *.googleapis.com (matched)\n[1.234440] curl: info:   server certificate expiration date OK\n[1.234449] curl: info:   server certificate activation date OK\n[1.234468] curl: info:   certificate public key: EC/ECDSA\n[1.234478] curl: info:   certificate version: #3\n[1.234523] curl: info:   subject: C=US,ST=California,L=Mountain View,O=Google LLC,CN=*.googleapis.com\n[1.234545] curl: info:   start date: Tue, 30 Oct 2018 13:14:00 GMT\n[1.234557] curl: info:   expire date: Tue, 22 Jan 2019 13:14:00 GMT\n[1.234585] curl: info:   issuer: C=US,O=Google Trust Services,CN=Google Internet Authority G3\n[1.234595] curl: info:   compression: NULL\n\n. Should be fixed in the beta ppa. If you are using opam, you can try the fix before it gets to the main repo, running:\nopam pin -n add gapi-ocaml https://github.com/astrada/gapi-ocaml#master\nopam update\nopam upgrade\n\nTo restore the main repo version:\nopam pin -n remove gapi-ocaml\nopam update\nopam upgrade\n\nThanks for reporting the issue!. > This is a fix or a workarround?\nIt should fix the issue. The opam pin commands are to test the fix before it gets approved and lands in the official opam-repository.. There was a bug (#512) that should be solved in the latest beta version, that caused a similar behavior. File were not closed correctly after upload, so disk space could not be reclaimed until you unmounted your Drive. You can test if the fix solves this issue for you pointing to the beta ppa.. Thanks for your feedback.. You need these external packages: osxfuse, gmp, lzlib, pkg-config, sqlite3. With homebrew, you can use these commands: \nsh\nopam install depext\nopam depext google-drive-ocamlfuse\nHere you can find Travis build logs for macos: https://travis-ci.org/ocaml/opam-repository/jobs/456680431. You have to check if there is a fuse.h in /usr/local/include/osxfuse.. Unfortunately Windows 10 WSL doesn't support FUSE (yet). See https://wpdev.uservoice.com/forums/266908-command-prompt-console-windows-subsystem-for-l/suggestions/13522845-add-fuse-filesystem-in-userspace-support-in-wsl. It looks like you need krb5 (kerberos).. > I have all of the debug info saved in a file. Let me know if you want this information, and how I can provide it to you. (Maybe over email?)\nYes, please, send gdfuse.log to my personal email (you can find it in my profile), so I can check if there are any errors. Consider that I start the uploading on file close. That operation is normally considered instantaneous by the SO so it's probably blocking until close() returns. I think that is the cause of your system being unresponsive. In old versions (before 0.7) the uploading was asynchronous, so you didn't experienced this issue, but there was a risk of data loss because the file could be modified before the upload ended and those changes would be lost.. Have you tried setting the proxy via environment variables (as explained here)?. What if your original issue is a timeout? Have you tried rising the timeout in the config file (connect_timeout_ms)?. Looks like you have a DNS resolution problem. Which version of curl do you have?\ncurl --version\n\nDoes this command work?\nALL_PROXY=socks5h://127.0.0.1:1080 curl --verbose https://accounts.google.com/. I think that Google One is just a new name for Google Drive. I think the current API will work with the new service.. Thanks for your feedback!. Unfortunately hard links are not easy to support, because there are [limitations](https://github.com/libfuse/libfuse/issues/79) in libfuse.. Which version are you using? How do you check if there is a new file? (with a GUI? with the command shell?). > not sure how to check the version\n\ngoogle-drive-ocamlfuse -version\n\n\nThunar\n\nI think this is the problem. Local file-system gets updated only if there is some kind of request (an ls, a stat, etc.). If Thunar doesn't do any request to the file-system, you won't see any update (i.e. uploads from the web interface). I don't know if there is a way to update its view. Otherwise you have to use a cron job or a watch command to periodically ls your drive.. > google-drive-ocamlfuse, version 0.6.23\nOK, that version had the issue you are reporting. If you upgrade to the latest version (0.7.1) you should resolve your problem.. Debian Jessie ships with an old version of the OCaml compiler, so you have to update it. You can do this with:\nopam switch create 4.07.1\neval $(opam env)\n\nAnd then, reinstall google-drive-ocamlfuse:\nopam install google-drive-ocamlfuse\n\n. Unfortunately the opam package included in jessie is old too and you have to update it too. The latest version is 2.0.3. You can upgrade with (see http://opam.ocaml.org/doc/Install.html):\nsh <(curl -sL https://raw.githubusercontent.com/ocaml/opam/master/shell/install.sh)\n\nThen after:\nsudo su - ${USER} -c 'opam init'\n\nYou should add:\nsudo su - ${USER} -c 'opam switch create 4.07.1'\nsudo su - ${USER} -c 'eval $(opam env)'\n\n. google-drive-ocamlfuse -version is returning 0.7.1?. Would you please send me your log files (to my personal email alessandro.strada@gmail.com)?\nMount with -debug -cc, do an ls in your GDrive root, add a file from the web interface, wait for a couple of metadata_cache_time intervals, do another ls, then send me gdfuse.log and curl.log. Thanks!. Duplicate of #523. Should be fixed in 0.7.2 that I just uploaded to my beta ppa.. No, sorry, this isn't a sync application. If you need to access files offline, please check this others apps: https://github.com/odeke-em/drive, https://github.com/ncw/rclone, https://github.com/vitalif/grive2. Nope, sorry.. Thanks!. > Is this file name format associated with this ?\nNo, files are created only under ~/.gdfuse/.. ",
    "tatsujin1": "A bit late to the game... but I'm still having problem with this.\nI can access my own files just fine (cool!), but files shared with me aren't shown...\nI tried running with -debug, but the shared files didn't show up in the log file (a search for \"share\" came up empty).\nShould this work...?\nRunning google-drive-ocamlfuse, version 0.5.2 from the ubuntu repos (13.10) with dependencies from the same, of course.\n. Oh... I see. I didn't know that. That's probably it then... thanks! \n----- Original Message -----\n\nFrom: \"Alessandro Strada\" notifications@github.com\nTo: \"astrada/google-drive-ocamlfuse\"\ngoogle-drive-ocamlfuse@noreply.github.com\nCc: \"tatsujin-github\" github@0x1.se\nSent: Saturday, 28 December, 2013 6:42:03 PM\nSubject: Re: [google-drive-ocamlfuse] Problem with shared documents\n(#1)\nDid you add them to your Drive? (\nhttps://support.google.com/drive/answer/2375057?p=swm_ww&rd=1 )\nIf you don't, you won't be able to see those files in the mounted\ndirectory.\n\u2014\nReply to this email directly or view it on GitHub .\n. \n",
    "giamma": "Thanks for your explanation. Feel free to close this issue document.\n. ",
    "asbruff": "Yes, i solved my problem... Thanks for your help.\nIl giorno 27/mag/2013 23:54, \"Alessandro Strada\" notifications@github.com\nha scritto:\n\nI'm going to close this issue. If you have any problem, please let me know.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/3#issuecomment-18516063\n.\n. with the binary package for Ubuntu 13.04 provided by astrada..\n. \n",
    "ghost": "I am not using Ubuntu, it should work. I get similar error:\nocaml setup.ml -configure\nocamlfind: Package `Fuse' not found\nW: Field 'pkg_fuse' is not set: Command ''/usr/bin/ocamlfind' query -format %d Fuse > '/home/data1/protected/tmp/oasis-019705.txt'' terminated with error code 2\nE: Cannot find findlib package Fuse\nE: Failure(\"1 configuration error\")\neven though I have properly configured and installed ocamlfuse.\n. i don't have the permission to modify the files... how can i fix this ?\n. I wanted to say that I can't edit the synchronized files inside the folder, but I can only read it\n. Ok i got it, however it is always better than the official one for windows ^  ^ \nThanks for the quick answer\n. I don't know how to contact you, so i'm writing here...\nCan i allow the write permission to a single folder ?\n. Hey @astrada , can you tell me is there an exact way to tell, from this gdfuse.log, whether there was or not something corrupted?\nAlso... is it possible to make some kind of error-only or corrupt-only verbosity to that log? Because it's so verbose it consumes megabytes in minutes.\nThank you!. I'm seeing same errors when issuing commands like git init in a directory, or trying to cp/mv a directory into gdrive...\nPS what's the right way to stop/unmount a runing ocamluse mount?\n. thank you! the errors I reported seem to have been solved using the -o big_writes  option.\n. Hi,\nsame problem here running linux Mint 17 32bit.\nInstalled it the \"ppa way\"\n. hi astrada,\ndid you mean me? I'm on a 32bit system as mentioned.\nMind giving me the steps to get a core dump?\n. Debug logs sent. It looks like it's the same problem as issue #159, File creation date cannot be modified.\n. Corrected with gapi-ocaml.0.2.9. \n. This problem still exists on version 0.6.19, with default config (streaming enabled, everything else is unchanged).\n\n```\ncurl -V\ncurl 7.47.0 (x86_64-pc-linux-gnu) libcurl/7.47.0 GnuTLS/3.4.10 zlib/1.2.8 libidn/1.32 librtmp/2.3\nProtocols: dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtmp rtsp smb smbs smtp smtps telnet tftp \nFeatures: AsynchDNS IDN IPv6 Largefile GSS-API Kerberos SPNEGO NTLM NTLM_WB SSL libz TLS-SRP UnixSockets\n```\n. I've  been having a problem, I've been trying to get my pms on a vps, to connect to google drive, but plex wont see any subfolders of the mount, even though i know they are there thanks to ls. I'm sure its a permission problem, but I have been unable to figure out how to get it working.\nIf anyone could help this poor linux noob, I would greatly appreciate it.. @dany20mh that was it thanks :D I feel like an idiot now.. Hello? Anyone?. Wow. ",
    "dappiu": "@asbruff why don't you tell how you solved the problem? I fell in the same identical problem and didn't find a solution yet.\n. I was looking for a solution to the error you reported compiling from\nsource, but thanks anyway and sorry for my misunderstanding :)\n2014-07-24 19:45 GMT+02:00 asbruff notifications@github.com:\n\nwith the binary package for Ubuntu 13.04 provided by astrada..\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/3#issuecomment-50052525\n.\n\n\nDavide Rossi\nGnuPG/PGP Key ID: 4AE92D78\nFINGERPRINT: 33A1 9E37 C91A DDC7 0231 FDE1 AB8D 2D4A 4AE9 2D78\nServer: pgp.mit.edu\n. ",
    "rickhuizinga": "Thanks for your help with this.\nUpon freshly going through the OAuth procedure, I did get a response like you have shown.  After copying the refresh token into the state file, and mounting Google Drive with google-drive-ocamlfuse, I always get a \"Device or resource busy error\" when attempting to get a directory listing of the mount.\n. ",
    "fjaeger": "Maybe we can continue this discussion 'offline' via email? I am reachable via fabian.jaeger@chungwasoft.com\nI would like to use a FUSE-based approach in Kiwingu as having the cloud storage integrated into the general file system of OSX is the design idea of the application. That's why I want to avoid using the Drive API. \nIn Kiwingu I already use other FUSE filesystems like curlftpfs (FTP), sshfs (SFTP) and wdfs (WebDAV) and I would expect it to be relatively easy to integrate google-drive-ocamlfuse in a similar way.\nHow does google-drive-ocamlfuse compare to projects like https://github.com/dsoprea/GDriveFS ?\n. ",
    "narutimateum": "hi.. i cant php upload to folder that i mounted\n. ",
    "Vascom": "I can give you rpm and src.rpm for opam. And you can install google-drive-ocamlfuse via opam.\n. Yes. I have opam spec\nhttps://github.com/RussianFedora/opam/blob/master/opam.spec\nand buils (srpm too) for Fedora http://koji.russianfedora.pro/koji/packageinfo?packageID=48\nFor google-drive-ocamlfuse also need this packages in Fedora:\nocaml gcc gcc-c++ m4 make ocamldoc sqlite-devel libcurl-devel fuse fuse-devel zlib-devel ocaml-camlp4-devel\n. google-drive-ocamlfuse requires many dependencies that not present in Fedora.\n. It copy good only after\ngoogle-drive-ocamlfuse -cc\n. It happens always after copying large number of files. I must do that:\nfusermount -u google.drive\ngoogle-drive-ocamlfuse google.drive\nAnd continue copying.\n. Do it please.\nIt happens after copying about 500-1000 small files (0.1-1 MB).\n. Yes, I have fast connection 50-100 MBit/sec.\n. OK.\n. In Fedora you must do that:\nyum install opam ocaml gcc gcc-c++ m4 make ocamldoc sqlite-devel libcurl-devel fuse-devel zlib-devel ocaml-camlp4-devel\n$ opam init\n$ . /home/USER/.opam/opam-init/init.sh > /dev/null 2> /dev/null || true\n$ opam install google-drive-ocamlfuse\nAfter that you can mount google.drive by google-drive-ocamlfuse command.\n. rm -rf .opam\nAnd run again\nopam init\n. /home/USER/.opam/opam-init/init.sh > /dev/null 2> /dev/null || true\nopam install google-drive-ocamlfuse\n. ",
    "Aikhjarto": "@Vascom you have already build an rpm?\nWould you mind to upload the corresponding spec-file to build.opensuse.org\nIn this way it would be available to a variety of distributions.\n. @Vascom I see you build opam with the spec file. I thought you have build google-drive-ocamlfuse directly als rpm.\n. ",
    "kbarcza": "Hi Mans, If help to your work, a blackPanther OS based RPM and SRC.RPM, the packages are available here:\nftp://ftp.blackpanther.hu/blackPanther/OS/All/Seeker/google-drive-ocamlfuse/ or any mirrors like freepark servers.\nWe will testing the app soon.\n. Your welcome mate :+1: \notherwise does not work for me the last github version because I get \n\"Error: Bad Request\nYour client has issued a malformed or illegal request.\" message from google and \nin the log I saw:\nGetting tokens from GAE proxy...not found, retrying\nGetting tokens from GAE proxy...not found, retrying\nGetting tokens from GAE proxy...not found, retrying\nGetting tokens from GAE proxy...not found, retrying\nGetting tokens from GAE proxy...not found, retrying\nGetting tokens from GAE proxy...not found, retrying\n. ",
    "sergiomb2": "as root:\ndnf copr search opam\ndnf copr enable jonludlam/opam\ndnf install opam\ndnf install ncurses-devel fuse-devel curl-devel sqlite-devel zlib-devel m4\nas user:\nopam init --comp=4.02.1\nopam config env\nopam install google-drive-ocamlfuse\n~/.opam/4.02.1/bin/google-drive-ocamlfuse\nmkdir /home/sergio/gdrive\n~/.opam/4.02.1/bin/google-drive-ocamlfuse /home/sergio/gdrive\nis working on Fedora23\n. I packaged it here: https://copr.fedorainfracloud.org/coprs/sergiomb/google-drive-ocamlfuse/\nwithout opam, just ocaml builds ... \nI save the \"project\" in github https://github.com/sergiomb2/google-drive-ocamlfuse you got the rpm spec files here:\nhttps://github.com/sergiomb2/google-drive-ocamlfuse/blob/master/ocamlfuse/ocamlfuse.spec\nhttps://github.com/sergiomb2/google-drive-ocamlfuse/blob/master/gapi-ocaml/gapi-ocaml.spec\nhttps://github.com/sergiomb2/google-drive-ocamlfuse/blob/master/google-drive-ocamlfuse/google-drive-ocamlfuse.spec\nSRPMS here:\nhttps://copr-be.cloud.fedoraproject.org/results/sergiomb/google-drive-ocamlfuse/fedora-rawhide-x86_64/00166909-ocamlfuse/ocamlfuse-2.7.1-1.cv2.fc25.src.rpm\nhttps://copr-be.cloud.fedoraproject.org/results/sergiomb/google-drive-ocamlfuse/fedora-rawhide-x86_64/00167031-gapi-ocaml/gapi-ocaml-0.2.8-1.fc25.src.rpm\nhttps://copr-be.cloud.fedoraproject.org/results/sergiomb/google-drive-ocamlfuse/fedora-rawhide-x86_64/00167032-google-drive-ocamlfuse/google-drive-ocamlfuse-0.5.22-1.fc25.x86_64.rpm\n. Thanks for the feedback we are fixing it now [1]\n[1]\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1474762. ",
    "andrewferrier": "Thanks for that! The second option definitely sounds workable (I don't have a Linux installation with a graphical env, but I can use my Mac). I pasted in the URL and Google does go through the authorization process, but I don't see a verification code displayed anywhere. Where should I expect it?\n(by the way, once we figure this out, I'd like to volunteer to help clarify the wiki page, if that would help).\n. ",
    "TamasBarta": "It was a really fast response. It works, also I love that you created a PPA. :) I'm really pleased, thanks!\n. ",
    "mklarin": "Tnx for clarify.\nI thought it is sync app.\nRecently come from Windows so I forget about link powers :)\nSometimes there is shared project folder on Drive (images, code, ...) i want to use as folder inside project folder, but with mount + links this is even better then I thought.\nSo this can be used on vm (ex.Koding) without wasting storage space, great!.\n. ",
    "VE7ZBG": "Is there anything I can provide to help?\n. ve7zbg@Aspire-3620 ~ $ google-drive-ocamlfuse -version\ngoogle-drive-ocamlfuse, version 0.4.2\nCopyright (C) 2012-2013 Alessandro Strada\nLicense MIT\nve7zbg@Aspire-3620 ~ $ google-drive-ocamlfuse -o big_write ~/GDrive\nfuse: unknown option `big_write'\n. We have success... sort of.\nFirst, the positives:\n1) Transfer rates went up, a little.  I'm getting about 18 kB/s now.\n2) The copy dialogue gets to update more frequently.  I can even cancel the transfer... with a delay.\n3) My connection seems to be fine.  Although the first test did show flooding, I did not see it on the next attempt.\nAnd now, the negative:\nIt is still a slow transfer and responding to cancel, but I'm willing to agree this is partially due to the old notebook PC I'm using.  It's old and I think the fan has issues, but since my precision screwdriver set is no longer precision, I can't get into this notebook PC to clean it out or whatever is needed.\nI've e-mailed you a tarball containing three files, the LOG file and two ping tests run from my Windows PC during the copy attempts:\n1) gdfuse.log \u2014 As requested, this is the log file from my -o big_writes -verbose attempt to copy the 16MB PDF, which did succeed.\n2) GDrive-Ping_1.txt \u2014 First test copy, which flooded my connection.  You will see 900+ ms ping times to Google.  The PDF was being copied from a thumb drive to Google Drive during this attempt.\n3) GDrive-Ping_2.txt \u2014 Second test copy.  This test was done with the PDF on the local hard drive (on my Desktop) simply because I didn't want a third window in the way.  You will see consistent ~100 ms ping times, which is my normal average.\nUnfortunately, I don't have any -verbose LOG output from the first copy attempt that flooded my connection.  I was only testing the -o big_writes on that attempt.  When that attempt failed, and I cancelled it, I re-mounted using -verbose for the second copy attempt.\n. ",
    "tlex": "A workaround for big files is for me to use dd instead of cp or mv. The results are:\ndd if=bigfile of=/mnt/google/bigfile bs=4MB\n1466+1 records in\n1466+1 records out\n5864865930 bytes (5.9 GB) copied, 1860.34 s, 3.2 MB/s\n. ",
    "ashkot": "We are trying to mount a google drive based on your suggestion on a Ubuntu server and it works initially. We are able to mount the drive. We are then starting file download from an internet URL and putting the files there, the files we work with are huge, each one more than 20GB. \nProblem we are facing is that after a while the drive becomes unresponsive, we cannot unmount it easily. Somehow we have to force unmount it, then remount it. Remounting works but the problem described above repeats.\nAny ideas what might be causing this? Thanks in advance!!\n. ",
    "moz87": "I have the same issue, its very slow to put a file.\nI run the command dd to test the speed and I can't get more than 50kB/s on a VPS\n$ dd if=/dev/zero of=/googleDrive/test bs=1024 count=1k\n1024+0 records in\n1024+0 records out\n1048576 bytes (1,0 MB) cop, 24.2475 s. 43.2kB/s\n. ",
    "pmorch": "Hi,\nWhen I save it to the local directory, I make a note of its MD5 sum. I copy it to the mounted directory, and it keeps the MD5 sum. Both the original local and remote files can open fine.\nIf, instead, I save to the mounted directory, and then copy that file locally, both remote and local files have same MD5 sum and both are corrupt.\nIt seems it is the save process that causes the corruption. I have noticed that word does \"odd\" (but legal) tricks with saving to short-lived/temporary files in the same dir as the file during the save process.\nMicrosoft's Description of how Word creates temporary files e.g. says:\n\nBy saving to a temporary file first and then renaming the file to the proper name, Word ensures the data integrity of your original file against problems (such as a power failure or lost network connections) that may occur while the file is being written.\n. I've also run it like so:\n\ngoogle-drive-ocamlfuse -debug -d <mountpoint> 2>&1 | tee google-drive-ocamlfuse.log\nto give you detailed log on what is going on. Email me privately at peter@morch.com, and I'll send you the log file, and a movie of what is going on in the GUI.\nHowever, now it fails in another way. Instead of Word silently declaring the save a success, and then discovering corruption during open of the saved file, it now discovers the fail during save, asks for a new file name to save under, but still fails again. After that, Word gives up saving.\nFor some reason, after a few tries, I cannot reproduce my first scenario, but only this new one. Either way, though, Word is not able to save to a google-drive-ocamlfuse mountpoint.\n. I can confirm that after commit 9d11a75 (in version 0.4.4), this issue has been resolved for me. Thanks, Alessandro\n. ",
    "cubica": "Thank you very much for the info!\nThe problem with afuse is that it mounts fuse filesystems on-demand based on the subdirectory of mountpoint that is requested.\nFor example, if I launch it like this\n$ afuse -o mount_template='google-drive-ocamlfuse %m' -o unmount_template='fusermount -u -z %m' /my/path\nAnd then I try\n$ ls /my/path/anything\nafuse mounts the google drive in /my/path/anything, where anything can be, well, anything :). So the result is that I get a new mount of the same google drive each time I access a subdirectory of mountpoint, which is not optimal.\nI'm considering other solutions for on-demand mounting like x-systemd.automount in fstab or automount, however any further advice is highly welcome!\n. I've tried this:\n$ ./afuse -o \"mount_template=google-drive-ocamlfuse -label %r %m\" -o \"unmount_template=fusermount -u -z %m\" -o \"populate_root_command=cat /home/cubica/google-drive-labels\" /home/cubica/google-drive\n/home/cubica/google-drive-labels is just a text file with two label names corresponding to my two gdrive accounts, and it's used by afuse's populate_root_command option in order to constrain the mountable directories to that labels (see \"Pre-populating afuse Root\" at https://github.com/pcarrier/afuse).\nThis should work, the only problem is that, when the command is launched and the first label is mounted, the following folders are created inside /home/cubica/google-drive:\n- .Trash\n- .Trash-1000\n- .xdg-volume-info\nSo afuse immediately tries to mount those too, passing their name as a label to google-drive-ocamlfuse, which doesn't recognize such labels so tries to make me authenticate them. This somehow makes gnome hang completely, so I have to switch to a shell with ctrl-alt-f1 and kill both afuse and all the google-drive-ocamlfuse processes in order to revive gnome.\nAny clue about why those dirs appear after the first mount, and how to prevent this misbehavior?\nAs a side note, the dirs are created even without the populate_root_command option, and trying to access them hangs the system as well.\n. Errata corrige: the directories above are created as soon as afuse is launched, with or without the populate_root_command option. I think they are created by GVFS, which is somehow activated by a fuse mount. I'll dig deeper and let you know.\n. ",
    "robinhood007": "NOTE its not an issue, I am just little confused..\nI have added this command: \ngoogle-drive-ocamlfuse \"/home/onelove/loveAtD/Google Drive\"\non my ubuntu startup and it seems to work ok.. now m i going it wrong???\n. First of all nice job man! Now coming to the issue. I am sure you are aware, but making sure its brought to attention. The native google spreadsheets are showing us as zip files with a total of 4 xml files in the archive.\n. No man that is not a link file or  a open document file for sure. have a look in the screen capture.\n\n. here you go mate!\ndocs_file_extension=false\nverification_code=\numask=0o002\ndocument_format=odt\ndebug=false\npresentation_format=pdf\nspreadsheet_format=ods\ndownload_docs=true\nclient_id=\ndrawing_format=png\nconflict_resolution=server\nkeep_duplicates=false\nmetadata_cache_time=60\nclient_secret=\nread_only=false\nform_format=ods\nsqlite3_busy_timeout=500\n. fixed now. \nOn a different issue, have you noticed double click on files/folders doesn't work all the time, though right click open always works.\n. Its fixed in the new version I installed through repo today. So, thanks Alessandro and you may close the issue. I was having problem opening images larger than 1MB in earlier version; however, that problem is resolved in newer version too. \n. ",
    "gaspo": "+1\n. ",
    "JP-Ellis": "Would the partial file transfers of rsync ever cause issues with the Drive API?\nAs for the version, I am using version 0.4.5 from Ubuntu's PPA.\n. Thanks for the quick reply.\nDoes that mean that using rsync (with its default settings) is incompatible with google-drive-ocamlfuse?\n. I don't understand how that would change the exit code (which is the issue at hand).\nIf you are actually referring to the slow down in the parent directory, no I haven't tried, though I don't see why pausing would be more advantageous then simply mounting/dismounting when required.\n. So the error which causes google-drive-ocamlfuse to exit successfully occurs when the folder being mounted to is non empty.  It then displays:\nfuse: mountpoint is not empty\nfuse: if you are sure this is safe, use the 'nonempty' mount option\n. Although I can use this, I did not actually read up under which conditions it is safe to use the -o nonempty parameter and thus have avoided it.  As this is a script run by cron, I do not want it to run a possibly unsafe command in circumstances which the script may not know exactly.\nInstead, I have opted to check it if the mount was successful as follows:\nbash\nif mount -l | grep \"${HOME}/gdrive type fuse.google-drive-ocamlfuse\"&>/dev/null; then\n    echo \"Successful mount\"\nelse\n    echo \"Failed mount\"\nfi\n. ",
    "tobek": "For what it's worth, adding the --inplace option to rsync seems to have fixed this issue for me. I can't be totally certain, because sometimes it worked even before, so maybe I'm just getting lucky.\nFor other Googlers, here's the line in my crontab -e\n0 * * * * date >> ~/drive-rsync-stats.log && rsync -trluhv --delete --stats --inplace /folder/to/upload/* /folder/mounted/with/google-drive-ocamlfuse/ >> ~/drive-rsync-stats.log\nI'm using this so that I can access files from my main computer from all my other devices, so I don't care if this command clobbers stuff on Drive. Working great for me running every hour on my machine.\n@astrada this thing is baller.\n. ",
    "aselvan": "I had the same issue with image files when I used rsync. After adding \"--inplace\" argument, backup to Google Drive via google-drive-ocamlfuse works perfect with rsync. Thanks @astrada\n. Thanks @astrada \nAs you confirmed, Drive API ignores the \"createdDate\". This makes all the files clumped into same date (uploaded date) which makes the timeline meaningless (in photos stream) as well as sorting and grouping of old pictures and movies in Drive!\nI tried using the API drives.files.patch (PATCH verb) and found that it requires a boolean flag \"setModifiedDate=true\" as query parameter to update the modifiedDate field. I wonder if I can pass \"setCreatedDate=true\" on this call to force it to update the createdDate? Unfortunately, I could not try that since API explorer does not allow editing to add that query parameter.\nRequest:\n```\nPATCH https://www.googleapis.com/drive/v2/files/xxxxxxxxxxxxx?fields=originalFilename%2CmodifiedDate%2CcreatedDate&setModifiedDate=true&key={YOUR_API_KEY}\n{\n \"createdDate\": \"2012-12-25T11:51:56.124Z\",\n \"modifiedDate\": \"2012-12-25T11:51:56.124Z\"\n}\n```\nResponse:\n```\n200 OK\n\nShow headers -\n\n{\n \"createdDate\": \"2015-05-25T01:08:42.361Z\",\n \"modifiedDate\": \"2012-12-25T11:51:56.124Z\",\n \"originalFilename\": \"IMG_2195.JPG\"\n}\n```\n. Thank you for your help. \nSince this is not an issue with this driver and the problem is with Google Drive API, I am closing this issue.\n. ",
    "brancomat": "Sorry to comment a closed issue, still this is the best thread I've found on using rsync on google-drive-ocamlfuse (imho some suggestions of this issue should be promoted to a wiki page, @astrada I'd gladly help)\nI generally see everyone experiencing some vaguely defined \"slow\" rating but I can't find something more specific: I'm uploading files with rsync at something less than 1.4 Mbps, I'm wondering:\n- is this considered \"normal\"?\n- are there any fine tunings that can increase upload speed? (besides the --inplace option)\n- (slightly OT) is this slowness due to google drive's api in general (meaning: there's no other way to significantly increase transfer speed to/from google drive)?\nI'm aware that (quoting this interesting thread) there hasn't been a \"clear statement from Google's side about what upload speed is expected\" so I guess it's difficult to talk numbers.\n. Thank you,\nwiki page created\nI'm trying your suggestions, the rsync + parallel experiment went wrong: I was using this approach but after a while I got a lot of:\nrsync error: errors selecting input/output files, dirs (code 3) at main.c(634) [Receiver=3.1.0]\nrsync: ERROR: cannot stat destination \"/my/ocamlfuse/mount/path\": Transport endpoint is not connected (107)\nRight now I've launched manually two separate instances of rsync on different subpath, so far so good but I'm not sure if this is affecting transfer speed\n. It seems that the performance issues (and the \"transport endpoint is not connected\") might be related to the fact that I'm behind a ntlm authenticated proxy (I'm sorry I didn't mention that earlier, my bad). I repeated the test on the same network bypassing the proxy and speed improved greatly. \nAfter around 100GB of data transfer I experienced a:\nDevice or resource busy (16)\nrsync error: error in file IO (code 11) at receiver.c(856) [receiver=3.1.0]\nBut in that particular server I haven't still changed the metadata_cache_time, maybe it's because of that (?)\n. Same thing here since yesterday (with the same effects of #160). curl.log shows a::\n{\n \"error\": {\n  \"errors\": [\n   {\n    \"domain\": \"global\",\n    \"reason\": \"badRequest\",\n    \"message\": \"File creation date cannot be modified.\",\n    \"locationType\": \"other\",\n    \"location\": \"file.created_millis\"\n   }\n  ],\n  \"code\": 400,\n  \"message\": \"File creation date cannot be modified.\"\n }\n}\nfrom www.googleapis.com\n. It works for me! thanks\n. @idvoretskyi, translated recap:\n\"is it possible to use this in a debian server without GUI / X server?\"\n\"yes it is, see Headless Usage & Authentication, but you'll still need a graphical browser in some other machine\"\n. As a side node:\nwith the help of @astrada I recently used memleax to diagnose a suspect memory leak and it turned out to be an issue of libcurl compiled against nss (https://bugzilla.redhat.com/show_bug.cgi?id=1057388 and also https://bugzilla.mozilla.org/show_bug.cgi?id=1202413).\nI had to recompile libcurl against openssl to avoid it.. Thanks, I took the liberty to add the \"read-only\" bit to the configuration page in wiki.. @buttpain my 2 cents:\nit's not always easy to define a broken status: with a large number of files (or a cache that needs some cleaning) some operations could take a large amount of time.\nBack when I had some mountpoint hang issues, my alternatives were either running a command (ls or whatever) in timeout (setting a reasonable amount of time) or something like mount|grep google-drive-ocamlfuse.\nIn both cases you can trap the output and try launching a fuserumount -u $PATH && google-drive-ocamlfuse $PATH if something is wrong (or simply try a -cc to clean the mountpoint cache)\nYMMV, it might not be a bright idea if some other process is trying to write on that mountpoint (you could enter a state in which you unmount the path and for some reason the remount fails and the other process starts writing on the (now very local) mount path filling that partition. (been there, done that). > you can safely add a cron job to purge files in ~/.gdfuse/default/cache and keep only cache.db\nDoes this mean that it's actually safe to remove any file in the cache directory (keeping only cache.db) when the path is mounted? What if there's an ongoing upload? (I suppose it's avoidable by removing only older files, the question is only to understand the scenario). ",
    "kindnation": "Thanks, that worked!\nOn Mon, Oct 7, 2013 at 2:43 PM, Alessandro Strada\nnotifications@github.comwrote:\n\nYou may try to edit the file ~/.opam/system/installed, adding this line:\nocamlnet 3.7.3\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/22#issuecomment-25789938\n.\n. it looks like it opens and responds with:\n\nwww.google.com cookie: PR=ID=2d0e83b4623e3402:FF=0:TM=1381140970:LM=1381\nAllow? (Y/N/Always/neVer)\nOn Mon, Oct 7, 2013 at 5:01 PM, Alessandro Strada\nnotifications@github.comwrote:\n\nIt looks like xdg-open is configured in an unexpected way. What happens\nif you run this command?\nxdg-open http://www.google.com\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/24#issuecomment-25796876\n.\n. Ok, that did put up the exception with the URL.  I opened up a Chrome and\npasted the URL in there and authorized the app, and it responds with:\n\nThe application was successfully granted access. Please wait for the client\nto retrieve the authorization tokens\nHowever, I don't see any additional output for the verification code.\n Where should I be looking to confirm the verification code?\nOn Mon, Oct 7, 2013 at 5:30 PM, Alessandro Strada\nnotifications@github.comwrote:\n\nAs a workaround you can set the BROWSER environment variable to a\nnon-existing command. E.g.:\nBROWSER=firefox google-drive-ocamlfuse\nThis way you should get the exception containing the URL.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/24#issuecomment-25798339\n.\n. awesome, it gave me the verification code.  I edited the config file and\npasted the verification code, and ran google-drive-ocamlfuse -id ######.\napps.googleusercontent.com -secret ***** again, and it outputted\nAccess token retrieved correctly.\n\nYou are absolutely amazing and nothing less than a wizard. THANK YOU!\nOn Mon, Oct 7, 2013 at 7:21 PM, Alessandro Strada\nnotifications@github.comwrote:\n\nAh, yes, you should put your client id/client secret on the command line,\notherwise you won't get the verification code.\nBROWSER=firefox google-drive-ocamlfuse -id ######.apps.googleusercontent.com -secret *****\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/24#issuecomment-25803925\n.\n. reverting back to 0.4.4 has it working again.  I'm on CentOS 6.4 -  2.6.32-042stab078.27 #1 SMP Mon Jul 1 20:48:07 MSK 2013 x86_64 x86_64 x86_64 GNU/Linux\n. \n",
    "avsm": "Now merged; it'll be on the live repo in less than an hour\n. ",
    "krish-nagarajan": "I did that, but file still shows an ODS document as Zip archive data and\nalso xdg-open just hangs trying to open it.\nkrish@xanadu:~$ google-drive-ocamlfuse -cc\nClearing cache...done\nkrish@xanadu:~$ cd googledrive/\nkrish@xanadu:~/googledrive$ ls -l Travel-Packing-List.ods\n-r--r--r-- 1 krish users 7238 Oct 21 15:48 Travel-Packing-List.ods\nkrish@xanadu:~/googledrive$ file Travel-Packing-List.ods\nTravel-Packing-List.ods: Zip archive data, at least v2.0 to extract\nkrish@xanadu:~/googledrive$ xdg-open Travel-Packing-List.ods\nWhat could I be doing wrong??\nOn Mon, Oct 21, 2013 at 3:43 PM, Alessandro Strada <notifications@github.com\n\nwrote:\nTry resetting the cache, with this command:\n$ google-drive-ocamlfuse -cc\nMaybe the problem is that some of the Google Docs were downloaded when\ndocs_file_extension was false, so that xdg-open don't know which program to\nstart.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/30#issuecomment-26697781\n.\n. Nothing, again, it just hangs...\n\nEarlier (before clearing cache) it used to launch a browser (chromium) and\nprompt to save the file (type ZIP I guess).\nNow, no browser window is launched in either case, and it just hangs ???\nMany, many thanks for your assistance. Sorry to bother you...\nOn Mon, Oct 21, 2013 at 3:59 PM, Alessandro Strada <notifications@github.com\n\nwrote:\nWhat does it happen if you try to open it directly?\n$ libreoffice Travel-Packing-List.ods\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/30#issuecomment-26698485\n.\n. Hi,\n\nI looked more closely, and noticed that there was a LibreOffice popup\nwindow prompting that the document was Read Only and I had to click Read\nOnly.\nThen it works FINE...\nMany many thanks indeed. Sorry for the confusion...\nOn Mon, Oct 21, 2013 at 4:05 PM, Krishnan Nagarajan \nkrishnan.nagarajan@gmail.com wrote:\n\nNothing, again, it just hangs...\nEarlier (before clearing cache) it used to launch a browser (chromium) and\nprompt to save the file (type ZIP I guess).\nNow, no browser window is launched in either case, and it just hangs ???\nMany, many thanks for your assistance. Sorry to bother you...\nOn Mon, Oct 21, 2013 at 3:59 PM, Alessandro Strada \nnotifications@github.com wrote:\n\nWhat does it happen if you try to open it directly?\n$ libreoffice Travel-Packing-List.ods\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/30#issuecomment-26698485\n.\n. \n\n",
    "anubeon": "Ooops, I stumbled across the answer in another 'bug' report. I needed to execute with the -cc option to see the 'docs_file_extension=true' configuration change come into effect. \nI would suggest though that this ( i.e. 'docs_file_extension=true') ought to be default behaviour, as one can only imagine that most users using google-drive-ocamlfuse for its Google Documents feature would rather see documents extensions (and thus have them handled as any other document stored locally). Just a thought.\n. ",
    "hdavy2002": "I have the same issue; All my files on the google drive is showing up as zipped. I used a browser to check my drive online.\n. I have the same issue on Ubuntu 12.04. I am using a VPS and after entering the verification code. I get  Segmentation fault.\n. I was trying to use use headless mode. It gave me the link to paste it on a browser. I got the verification code and typed it manually and thats when it popped up.\n. I was wondering how to close this ticket. Anyways, as I said I am a newbie and after some digging into the term Fuse, I realized that it is something built into the kernel. I am using Ramnode and I found and article that I have to open a ticket with them inorder to enable Fuse on my VPS. https://clientarea.ramnode.com/knowledgebase.php?action=displayarticle&id=26\nFood for thought. If the maintainer can add a line for the pre-requisites that Fuse needs to be enabled etc. It will save us newbies some time.  Thanks once again Astrada for this great software.\nCheers\n. ",
    "Danny02": "ah yes, I installed it from my package manager and that part did work, but now a lot of other things throw errors also. I'm running Fedora 17 btw.\n=-=-= Installing ocamlfuse.2.7.1-cvs =-=-=\nBuilding ocamlfuse.2.7.1-cvs:[...]\n[ERROR] The compilation of ocamlfuse.2.7.1-cvs failed.\nRemoving ocamlfuse.2.7.1-cvs.[...]\n=-=-= Installing ocurl.0.6.0 =-=-=\nBuilding ocurl.0.6.0:[...]\n[ERROR] The compilation of ocurl.0.6.0 failed.\nRemoving ocurl.0.6.0.[...]\n=-=-= Installing sqlite3-ocaml.2.0.4 =-=-=\nBuilding sqlite3-ocaml.2.0.4:[...]\n[ERROR] The compilation of sqlite3-ocaml.2.0.4 failed.\nRemoving sqlite3-ocaml.2.0.4.[...]\n=-=-= Installing ocamlnet.3.7.3 =-=-=\ndefault    Downloading https://opam.ocaml.org/archives/ocamlnet.3.7.3+opam.tar.gz[...]\n  ./configure -bindir /home/daniel/.opam/system/bin -disable-pcre -disable-gtk2 -disable-ssl -disable-zip -enable-crypto -with-nethttpd[...]\n[ERROR] The compilation of ocamlnet.3.7.3 failed.\nRemoving ocamlnet.3.7.3. [...]\n[ERROR] Due to some errors while processing { ocamlfuse.2.7.1-cvs, ocamlnet.3.7.3, ocurl.0.6.0, sqlite3-ocaml.2.0.4 }, the following actions will NOT proceed:\n- install google-drive-ocamlfuse.0.4.8\n- install gapi-ocaml.0.2\n===== ERROR while installing ocamlfuse.2.7.1-cvs =====\n opam-version 1.1.0\n os           linux\n command      make -C lib INCDIRS=/home/daniel/.opam/system/lib/camlidl\n path         /home/daniel/.opam/system/build/ocamlfuse.2.7.1-cvs\n compiler     system (3.12.1)\n exit-code    2[...]\n-## stdout ###\n ...[truncated]\n making ._d/Fuse_lib.d from Fuse_lib.ml\n making ._d/Unix_util.d from Unix_util.ml\n making ._d/Result.d from Result.ml\n make[1]: Leaving directory /home/daniel/.opam/system/build/ocamlfuse.2.7.1-cvs/lib'\n make[1]: Entering directory/home/daniel/.opam/system/build/ocamlfuse.2.7.1-cvs/lib'\n ocamlc -c -cc \"cc\" -ccopt \"-fPIC -D_FILE_OFFSET_BITS=64 -I. -pthread \\\n            -DPIC -DNATIVE_CODE   \\\n              -I/home/daniel/.opam/system/lib/camlidl  -o Fuse_bindings_stubs.o \" Fuse_bindings_stubs.c \n make[1]: Leaving directory /home/daniel/.opam/system/build/ocamlfuse.2.7.1-cvs/lib'\n make: Leaving directory/home/daniel/.opam/system/build/ocamlfuse.2.7.1-cvs/lib'\n-## stderr ###\n In file included from Fuse_bindings_stubs.c:17:0:\n Fuse_bindings.h:16:18: schwerwiegender Fehler: fuse.h: Datei oder Verzeichnis nicht gefunden\n Kompilierung beendet.\n make[1]: * [Fuse_bindings_stubs.o] Fehler 2\n make: * [native-code-library] Fehler 2\n===== ERROR while installing ocamlnet.3.7.3 =====\n-# opam-version 1.1.0\n-# os           linux\n-# command      make all\n-# path         /home/daniel/.opam/system/build/ocamlnet.3.7.3\n-# compiler     system (3.12.1)\n-# exit-code    2[...]\n-### stdout ###\n-# ...[truncated]\n-#     -e 's/@PREFERRED_CGI_PKG@//' \\\n-#     -e 's/@REGEXP_PROVIDER@/str/' \\\n-#     -e 's/@COMPAT_PCRE_PROVIDER@//' \\\n-#          -e 's/@ZIP_PROVIDER@//' \\\n-#  META.in >META\n-# make[2]: Leaving directory /home/daniel/.opam/system/build/ocamlnet.3.7.3/src/netsys'\n-# make[1]: Leaving directory/home/daniel/.opam/system/build/ocamlnet.3.7.3/src/netsys'\n-# make[1]: Entering directory /home/daniel/.opam/system/build/ocamlnet.3.7.3/src/netsys'\n-# ocamlfind ocamldep -package \"camlp4.macro\" -syntax camlp4o  *.ml *.mli >depend || { rm -f depend; exit 1; }\n-# make[1]: Leaving directory/home/daniel/.opam/system/build/ocamlnet.3.7.3/src/netsys'\n-### stderr ###\n-# Welcome to Ocamlnet version 3.7.3\n-# ocamlfind: Package `camlp4.macro' not found\n-# make[1]: * [depend] Fehler 1\n-# make: * [all] Fehler 2\n===== ERROR while installing ocurl.0.6.0 =====\n-# opam-version 1.1.0\n-# os           linux\n-# command      ./configure\n-# path         /home/daniel/.opam/system/build/ocurl.0.6.0\n-# compiler     system (3.12.1)\n-# exit-code    1[...]\n-### stdout ###\n-# ...[truncated]\n-# checking for gcc... gcc\n-# checking whether the C compiler works... yes\n-# checking for C compiler default output file name... a.out\n-# checking for suffix of executables... \n-# checking whether we are cross compiling... no\n-# checking for suffix of object files... o\n-# checking whether we are using the GNU C compiler... yes\n-# checking whether gcc accepts -g... yes\n-# checking for gcc option to accept ISO C89... none needed\n-# checking for libcurl cflags... \n-### stderr ###\n-# Package libcurl was not found in the pkg-config search path.\n-# Perhaps you should add the directory containing `libcurl.pc'\n-# to the PKG_CONFIG_PATH environment variable\n-# No package 'libcurl' found\n-# ./configure: line 2586: curl-config: command not found\n-# configure: error: libcurl was not found\n\n===== ERROR while installing sqlite3-ocaml.2.0.4 =====\n-# opam-version 1.1.0\n-# os           linux\n-# command      ocaml setup.ml -build\n-# path         /home/daniel/.opam/system/build/sqlite3-ocaml.2.0.4\n-# compiler     system (3.12.1)\n-# exit-code    1[...]\n-### stdout ###\n-# ...[truncated]\n-# Turn ocaml profile flag on: .......................... false\n-# Compiler support generation of .cmxs.: ............... true\n-# OCamlbuild additional flags: ......................... \n-# Strict compile-time checks: .......................... true\n-# Create documentations: ............................... true\n-# Compile tests executable and library and run them: ... false\n-# ocamldoc: ............................................ /usr/bin/ocamldoc\n-# \n-# /usr/bin/ocamlopt.opt -I /usr/lib64/ocaml/ocamlbuild unix.cmxa /usr/lib64/ocaml/ocamlbuild/ocamlbuildlib.cmxa -myocamlbuild.ml /usr/lib64/ocaml/ocamlbuild/ocamlbuild.cmx -o myocamlbuild\n-# Failure: pkg-config failed for cflags.\n-### stderr ###\n-# Package sqlite3 was not found in the pkg-config search path.\n-# Perhaps you should add the directory containing `sqlite3.pc'\n-# to the PKG_CONFIG_PATH environment variable\n-# No package 'sqlite3' found\n-# E: Failure(\"Command ''/usr/bin/ocamlbuild' lib/libsqlite3_stubs.a lib/dllsqlite3_stubs.so lib/sqlite3.cma lib/sqlite3.cmxa l-ib/sqlite3.a lib/sqlite3.cmxs -tag debug' terminated with error code 2\")\n-The former state can be restored with opam switch import -f \"/home/daniel/.opam/system/backup/state-20130928215953.export\"\n'opam install google-drive-ocamlfuse' failed.\n. ok thx, now a lot more stuff builds but still not everything :(\nin ocamlnet i get ocamlfind: Package `camlp4.macro' not found\nbut i have ocaml-camlp4-devel installed\n. finally thx to you all^^\n. ",
    "JohnathanMarkSmith": "I am still getting this error:\n[ERROR] No package named google-drive-ocamlfuse found\n. ",
    "loicbisiere": "Thank you very much for your excellent job !\n. Thank you ! The new revision mode works very well.\n. ",
    "sorin-ionescu": "Yes, I am using opam.  Like I have said, I have got ocamlfuse to install. It would not install if there was no fuse.\nThe following is how I have got ocamlfuse to install:\nsh\nCC=gcc CPPFLAGS='-DFUSE_USE_VERSION=26 -I/usr/local/include/osxfuse' PKG_CONFIG_PATH='/usr/local/lib/pkgconfig' opam install ocamlfuse\nThe following is how I am trying to get google-drive-ocamlfuse to install:\nsh\nCC=gcc LDFLAGS=-L/usr/local/lib CPPFLAGS='-DFUSE_USE_VERSION=26 -I/usr/local/include/osxfuse' PKG_CONFIG_PATH='/usr/local/lib/pkgconfig' opam install google-drive-ocamlfuse\n. sh\n$ ls /usr/local/lib\nlibfuse.0.dylib        libmacfuse_i64.2.dylib libosxfuse_i32.la\nlibfuse.2.dylib        libmacfuse_i64.dylib   libosxfuse_i64.2.dylib\nlibfuse.dylib          libosxfuse.2.dylib     libosxfuse_i64.dylib\nlibfuse_ino64.2.dylib  libosxfuse.dylib       libosxfuse_i64.la\nlibfuse_ino64.dylib    libosxfuse.la          pkgconfig\nlibmacfuse_i32.2.dylib libosxfuse_i32.2.dylib\nlibmacfuse_i32.dylib   libosxfuse_i32.dylib\n. It won't build. There is definitely something wrong with camlidl, which I don't know what it is since I do not use Ocaml. Arch Linux users have also had issues with camlidl.\nsh\n$ ocaml setup.ml -build\nFinished, 1 target (0 cached) in 00:00:09.\n+ ocamlfind ocamlc -g -linkpkg -package threads -package sqlite3 -package gapi-ocaml -package camlidl -package Fuse -thread src/appDir.cmo src/utils.cmo src/config.cmo src/cache.cmo src/concurrentGlobal.cmo src/keyValueStore.cmo src/state.cmo src/context.cmo src/mime.cmo src/gaeProxy.cmo src/oauth2.cmo src/drive.cmo src/gdfuse.cmo -o src/gdfuse.byte\nFile \"/Users/sorin/.opam/system/lib/Fuse/Fuse.cma(Bigarray)\", line 1:\nWarning 31: files /Users/sorin/.opam/system/lib/Fuse/Fuse.cma(Bigarray) and /Users/sorin/.homebrew/lib/ocaml/bigarray.cma(Bigarray) both define a module named Bigarray\nUndefined symbols for architecture x86_64:\n  \"___fuse_exited\", referenced from:\n      _camlidl_Fuse_bindings___fuse_exited in libFuse_stubs.a(Fuse_bindings_stubs.o)\n     (maybe you meant: _camlidl_Fuse_bindings___fuse_exited)\n  \"___fuse_process_cmd\", referenced from:\n      _camlidl_Fuse_bindings___fuse_process_cmd in libFuse_stubs.a(Fuse_bindings_stubs.o)\n     (maybe you meant: _camlidl_Fuse_bindings___fuse_process_cmd)\n  \"___fuse_read_cmd\", referenced from:\n      _camlidl_Fuse_bindings___fuse_read_cmd in libFuse_stubs.a(Fuse_bindings_stubs.o)\n     (maybe you meant: _camlidl_Fuse_bindings___fuse_read_cmd)\nld: symbol(s) not found for architecture x86_64\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nFile \"_none_\", line 1:\nError: Error while building custom runtime system\nCommand exited with code 2.\nCompilation unsuccessful after building 27 targets (0 cached) in 00:00:13.\nE: Failure(\"Command ''/Users/sorin/.homebrew/bin/ocamlbuild' src/gdfuse.byte -tag debug' terminated with error code 10\")\n. The library compiles. The example won't.\n`` sh\n$ CC=gcc LDFLAGS=\"-L$(brew --prefix)/lib -L/usr/local/lib\" CPPFLAGS=\"-DFUSE_USE_VERSION=26 -I$(brew --prefix)/include -I/usr/local/include/osxfuse\" PKG_CONFIG_PATH='/usr/local/lib/pkgconfig' INCDIRS=$(opam config var lib)/camlidl/ make -C lib/\ncamlidl -header  \\\n                 Fuse_bindings.idl\nmaking ._ncdi/Fuse_bindings.di from Fuse_bindings.mli\nmaking ._d/Fuse_bindings.d from Fuse_bindings.ml\nmaking ._d/Unix_util.d from Unix_util.ml\nmaking ._d/Fuse.d from Fuse.ml\nmaking ._d/Fuse_lib.d from Fuse_lib.ml\nmaking ._d/Result.d from Result.ml\nocamlc -c -cc \"gcc\" -ccopt \"-fPIC -I/Users/sorin/.homebrew/include -F/Users/sorin/.homebrew/Frameworks -D_FILE_OFFSET_BITS=64 -I. -pthread \\\n                -DPIC -DNATIVE_CODE -DFUSE_USE_VERSION=26 -I/Users/sorin/.homebrew/include -I/usr/local/include/osxfuse  \\\n                  -I/Users/sorin/.opam/system/lib/camlidl/  -o Fuse_bindings_stubs.o \" Fuse_bindings_stubs.c\nocamlc -c -cc \"gcc\" -ccopt \"-fPIC -I/Users/sorin/.homebrew/include -F/Users/sorin/.homebrew/Frameworks -D_FILE_OFFSET_BITS=64 -I. -pthread \\\n                -DPIC -DNATIVE_CODE -DFUSE_USE_VERSION=26 -I/Users/sorin/.homebrew/include -I/usr/local/include/osxfuse  \\\n                  -I/Users/sorin/.opam/system/lib/camlidl/  -o Fuse_util.o \" Fuse_util.c\nFuse_util.c: In function \u2018ops_init\u2019:\nFuse_util.c:584: warning: return makes pointer from integer without a cast\nFuse_util.c: In function \u2018set_fuse_operations\u2019:\nFuse_util.c:596: warning: assignment from incompatible pointer type\nFuse_util.c:596: warning: assignment from incompatible pointer type\nocamlc -c -cc \"gcc\" -ccopt \"-fPIC -I/Users/sorin/.homebrew/include -F/Users/sorin/.homebrew/Frameworks -D_FILE_OFFSET_BITS=64 -I. -pthread \\\n                -DPIC -DNATIVE_CODE -DFUSE_USE_VERSION=26 -I/Users/sorin/.homebrew/include -I/usr/local/include/osxfuse  \\\n                  -I/Users/sorin/.opam/system/lib/camlidl/  -o Unix_util_stubs.o \" Unix_util_stubs.c\nar rcs libFuse_stubs.a Fuse_bindings_stubs.o Fuse_util.o Unix_util_stubs.o\nocamlc -c -thread -I /Users/sorin/.opam/system/lib/camlidl/ Fuse_bindings.mli\nocamlopt -c -thread -I /Users/sorin/.opam/system/lib/camlidl/ Fuse_bindings.ml\nocamlopt -c -thread -I /Users/sorin/.opam/system/lib/camlidl/ Result.ml\nocamlopt -c -thread -I /Users/sorin/.opam/system/lib/camlidl/ Fuse_lib.ml\nocamlc -c -thread -I /Users/sorin/.opam/system/lib/camlidl/ Fuse.mli\nocamlopt -c -thread -I /Users/sorin/.opam/system/lib/camlidl/ Fuse.ml\nocamlopt -c -thread -I /Users/sorin/.opam/system/lib/camlidl/ Unix_util.ml\nocamlopt -a -thread     -I /Users/sorin/.opam/system/lib/camlidl/ -linkall  -ccopt -L/Users/sorin/.homebrew/lib -ccopt -L/usr/local/lib      -cclib -lFuse_stubs -cclib -lfuse -cclib -lcamlidl \\\n                 -o Fuse.cmxa Fuse_bindings.cmx Result.cmx Fuse_lib.cmx Fuse.cmx Unix_util.cmx\nmaking ._bcdi/Fuse_bindings.di from Fuse_bindings.mli\nmake[1]:libFuse_stubs.a' is up to date.\nocamlc -c -thread -I /Users/sorin/.opam/system/lib/camlidl/ Fuse_bindings.ml\nocamlc -c -thread -I /Users/sorin/.opam/system/lib/camlidl/ Result.ml\nocamlc -c -thread -I /Users/sorin/.opam/system/lib/camlidl/ Fuse_lib.ml\nocamlc -c -thread -I /Users/sorin/.opam/system/lib/camlidl/ Fuse.ml\nocamlc -c -thread -I /Users/sorin/.opam/system/lib/camlidl/ Unix_util.ml\nocamlmklib    -I /Users/sorin/.opam/system/lib/camlidl/      \\\n                -o Fuse_stubs Fuse_bindings_stubs.o Fuse_util.o Unix_util_stubs.o -lfuse \\\nocamlc -a -thread -dllib dllFuse_stubs.so -custom    -I /Users/sorin/.opam/system/lib/camlidl/ -linkall  -ccopt -L/Users/sorin/.homebrew/lib -ccopt -L/usr/local/lib      bigarray.cma \\\n                -cclib -lFuse_stubs -cclib -lfuse -cclib -lcamlidl -o Fuse.cma  Fuse_bindings.cmo Result.cmo Fuse_lib.cmo Fuse.cmo Unix_util.cmo\n```\nsh\n$ CC=gcc LDFLAGS=\"-L$(brew --prefix)/lib -L/usr/local/lib\" CPPFLAGS=\"-DFUSE_USE_VERSION=26 -I$(brew --prefix)/include -I/usr/local/include/osxfuse\" PKG_CONFIG_PATH='/usr/local/lib/pkgconfig' INCDIRS=$(opam config var lib)/camlidl/ make -C example/\nmake -f Makefile.fusexmp\nmaking ._d/fusexmp.d from fusexmp.ml\nocamlopt -c -thread -I /Users/sorin/.homebrew/lib/ocaml/site-packages/ocamlfuse fusexmp.ml\nFile \"fusexmp.ml\", line 28, characters 0-18:\nError: Unbound module Fuse_bindings\nmake[2]: *** [fusexmp.cmi] Error 2\nmake[1]: *** [native-code] Error 2\nmake: *** [default] Error 2\n. Where will it install? ocamlfuse is already installed via opam.\n. sh\n$ opam info ocamlfuse\n             package: ocamlfuse\n             version: 2.7.1-cvs\n        upstream-url: https://forge.ocamlcore.org/frs/download.php/1074/ocamlfuse-2.7.1-cvs.tar.gz\n       upstream-kind: http\n   upstream-checksum: 3e9e7ee2fd89e033a265840ca5aa4d44\n              author: Vincenzo Ciancia\n             depends: ocamlfind & camlidl\n   installed-version: ocamlfuse.2.7.1-cvs [system]\n         description: OCaml bindings for FUSE (Filesystem in UserSpacE)\n. I got that installed. The same thing occurs.\nsh\n$ LDFLAGS=\"-L$(opam config var lib)/camlidl -L$(brew --prefix)/lib -L/usr/local/lib\" CPPFLAGS=\"-DFUSE_USE_VERSION=26 -I$(brew --prefix)/include -I/usr/local/include/osxfuse\" PKG_CONFIG_PATH='/usr/local/lib/pkgconfig' make -C example\nmake -f Makefile.fusexmp\nmaking ._d/fusexmp.d from fusexmp.ml\nocamlopt -c -thread -I /Users/sorin/.homebrew/lib/ocaml/site-packages/ocamlfuse fusexmp.ml\nocamlopt \\\n                  \\\n                unix.cmxa threads.cmxa -thread     -I /Users/sorin/.homebrew/lib/ocaml/site-packages/ocamlfuse   -ccopt -L/Users/sorin/.opam/system/lib/camlidl -ccopt -L/Users/sorin/.homebrew/lib -ccopt -L/usr/local/lib  -ccopt -L/Users/sorin/.homebrew/lib/ocaml/site-packages/ocamlfuse    bigarray.cmxa Fuse.cmxa -cclib -lfuse -cclib -lcamlidl  -o fusexmp \\\n                fusexmp.cmx\nUndefined symbols for architecture x86_64:\n  \"___fuse_exited\", referenced from:\n      _camlidl_Fuse_bindings___fuse_exited in libFuse_stubs.a(Fuse_bindings_stubs.o)\n     (maybe you meant: _camlidl_Fuse_bindings___fuse_exited)\n  \"___fuse_process_cmd\", referenced from:\n      _camlidl_Fuse_bindings___fuse_process_cmd in libFuse_stubs.a(Fuse_bindings_stubs.o)\n     (maybe you meant: _camlidl_Fuse_bindings___fuse_process_cmd)\n  \"___fuse_read_cmd\", referenced from:\n      _camlidl_Fuse_bindings___fuse_read_cmd in libFuse_stubs.a(Fuse_bindings_stubs.o)\n     (maybe you meant: _camlidl_Fuse_bindings___fuse_read_cmd)\nld: symbol(s) not found for architecture x86_64\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nFile \"caml_startup\", line 1:\nError: Error during linking\nmake[2]: *** [fusexmp] Error 2\nmake[1]: *** [native-code] Error 2\nmake: *** [default] Error 2\nlibfuse is built properly. Many fuse file systems work. I don't understand why it doesn't those symbols and what's up with those underscores.\nsh\n$  file /usr/local/lib/libfuse.dylib\n/usr/local/lib/libfuse.dylib: Mach-O universal binary with 4 architectures\n/usr/local/lib/libfuse.dylib (for architecture ppc7400):    Mach-O dynamically linked shared library ppc\n/usr/local/lib/libfuse.dylib (for architecture ppc64):  Mach-O 64-bit dynamically linked shared library ppc64\n/usr/local/lib/libfuse.dylib (for architecture i386):   Mach-O dynamically linked shared library i386\n/usr/local/lib/libfuse.dylib (for architecture x86_64): Mach-O 64-bit dynamically linked shared library x86_64\n. ``` c\n/ File generated from Fuse_bindings.idl /\ninclude \ninclude \ninclude \ninclude \ninclude \ninclude \ninclude \nifdef Custom_tag\ninclude \ninclude \nendif\ninclude \ninclude \"Fuse_bindings.h\"\nvoid camlidl_ml2c_Fuse_bindings_fuse(value _v1, fuse * _c2, camlidl_ctx _ctx)\n{\n  _c2 = ((fuse *) Bp_val(_v1));\n}\nvalue camlidl_c2ml_Fuse_bindings_fuse(fuse * _c2, camlidl_ctx _ctx)\n{\nvalue _v1;\n  _v1 = camlidl_alloc((sizeof(fuse) + sizeof(value) - 1) / sizeof(value), Abstract_tag);\n  ((fuse ) Bp_val(_v1)) = *_c2;\n  return _v1;\n}\nvoid camlidl_ml2c_Fuse_bindings_str(value _v1, str * _c2, camlidl_ctx _ctx)\n{\n  (*_c2) = camlidl_malloc_string(_v1, _ctx);\n}\nvalue camlidl_c2ml_Fuse_bindings_str(str * _c2, camlidl_ctx _ctx)\n{\nvalue _v1;\n  _v1 = copy_string((*_c2));\n  return _v1;\n}\nvoid camlidl_ml2c_Fuse_bindings_struct_fuse_operation_names(value _v1, struct fuse_operation_names * _c2, camlidl_ctx _ctx)\n{\n  value _v3;\n  value _v4;\n  value _v5;\n  value _v6;\n  value _v7;\n  value _v8;\n  value _v9;\n  value _v10;\n  value _v11;\n  value _v12;\n  value _v13;\n  value _v14;\n  value _v15;\n  value _v16;\n  value _v17;\n  value _v18;\n  value _v19;\n  value _v20;\n  value _v21;\n  value _v22;\n  value _v23;\n  value _v24;\n  value _v25;\n  value _v26;\n  value _v27;\n  value _v28;\n  value _v29;\n  value _v30;\n  value _v31;\n  value _v32;\n  value _v33;\n  value _v34;\n  value _v35;\n  value _v36;\n  value _v37;\n  value _v38;\n  value _v39;\n  value _v40;\n  value _v41;\n  value _v42;\n  value _v43;\n  value _v44;\n  value _v45;\n  value _v46;\n  value _v47;\n  value _v48;\n  value _v49;\n  value _v50;\n  value _v51;\n  value _v52;\n  value _v53;\n  value _v54;\n  value _v55;\n  value _v56;\n  value _v57;\n  value _v58;\n  value _v59;\n  value _v60;\n  _v3 = Field(_v1, 0);\n  if (_v3 == Val_int(0)) {\n    (_c2).init = NULL;\n  } else {\n    _v4 = Field(_v3, 0);\n    (_c2).init = camlidl_malloc_string(_v4, _ctx);\n  }\n  _v5 = Field(_v1, 1);\n  if (_v5 == Val_int(0)) {\n    (_c2).getattr = NULL;\n  } else {\n    _v6 = Field(_v5, 0);\n    (_c2).getattr = camlidl_malloc_string(_v6, _ctx);\n  }\n  _v7 = Field(_v1, 2);\n  if (_v7 == Val_int(0)) {\n    (_c2).readlink = NULL;\n  } else {\n    _v8 = Field(_v7, 0);\n    (_c2).readlink = camlidl_malloc_string(_v8, _ctx);\n  }\n  _v9 = Field(_v1, 3);\n  if (_v9 == Val_int(0)) {\n    (_c2).readdir = NULL;\n  } else {\n    _v10 = Field(_v9, 0);\n    (_c2).readdir = camlidl_malloc_string(_v10, _ctx);\n  }\n  _v11 = Field(_v1, 4);\n  if (_v11 == Val_int(0)) {\n    (_c2).opendir = NULL;\n  } else {\n    _v12 = Field(_v11, 0);\n    (_c2).opendir = camlidl_malloc_string(_v12, _ctx);\n  }\n  _v13 = Field(_v1, 5);\n  if (_v13 == Val_int(0)) {\n    (_c2).releasedir = NULL;\n  } else {\n    _v14 = Field(_v13, 0);\n    (_c2).releasedir = camlidl_malloc_string(_v14, _ctx);\n  }\n  _v15 = Field(_v1, 6);\n  if (_v15 == Val_int(0)) {\n    (_c2).fsyncdir = NULL;\n  } else {\n    _v16 = Field(_v15, 0);\n    (_c2).fsyncdir = camlidl_malloc_string(_v16, _ctx);\n  }\n  _v17 = Field(_v1, 7);\n  if (_v17 == Val_int(0)) {\n    (_c2).mknod = NULL;\n  } else {\n    _v18 = Field(_v17, 0);\n    (_c2).mknod = camlidl_malloc_string(_v18, _ctx);\n  }\n  _v19 = Field(_v1, 8);\n  if (_v19 == Val_int(0)) {\n    (_c2).mkdir = NULL;\n  } else {\n    _v20 = Field(_v19, 0);\n    (_c2).mkdir = camlidl_malloc_string(_v20, _ctx);\n  }\n  _v21 = Field(_v1, 9);\n  if (_v21 == Val_int(0)) {\n    (_c2).unlink = NULL;\n  } else {\n    _v22 = Field(_v21, 0);\n    (_c2).unlink = camlidl_malloc_string(_v22, _ctx);\n  }\n  _v23 = Field(_v1, 10);\n  if (_v23 == Val_int(0)) {\n    (_c2).rmdir = NULL;\n  } else {\n    _v24 = Field(_v23, 0);\n    (_c2).rmdir = camlidl_malloc_string(_v24, _ctx);\n  }\n  _v25 = Field(_v1, 11);\n  if (_v25 == Val_int(0)) {\n    (_c2).symlink = NULL;\n  } else {\n    _v26 = Field(_v25, 0);\n    (_c2).symlink = camlidl_malloc_string(_v26, _ctx);\n  }\n  _v27 = Field(_v1, 12);\n  if (_v27 == Val_int(0)) {\n    (_c2).rename = NULL;\n  } else {\n    _v28 = Field(_v27, 0);\n    (_c2).rename = camlidl_malloc_string(_v28, _ctx);\n  }\n  _v29 = Field(_v1, 13);\n  if (_v29 == Val_int(0)) {\n    (_c2).link = NULL;\n  } else {\n    _v30 = Field(_v29, 0);\n    (_c2).link = camlidl_malloc_string(_v30, _ctx);\n  }\n  _v31 = Field(_v1, 14);\n  if (_v31 == Val_int(0)) {\n    (_c2).chmod = NULL;\n  } else {\n    _v32 = Field(_v31, 0);\n    (_c2).chmod = camlidl_malloc_string(_v32, _ctx);\n  }\n  _v33 = Field(_v1, 15);\n  if (_v33 == Val_int(0)) {\n    (_c2).chown = NULL;\n  } else {\n    _v34 = Field(_v33, 0);\n    (_c2).chown = camlidl_malloc_string(_v34, _ctx);\n  }\n  _v35 = Field(_v1, 16);\n  if (_v35 == Val_int(0)) {\n    (_c2).truncate = NULL;\n  } else {\n    _v36 = Field(_v35, 0);\n    (_c2).truncate = camlidl_malloc_string(_v36, _ctx);\n  }\n  _v37 = Field(_v1, 17);\n  if (_v37 == Val_int(0)) {\n    (_c2).utime = NULL;\n  } else {\n    _v38 = Field(_v37, 0);\n    (_c2).utime = camlidl_malloc_string(_v38, _ctx);\n  }\n  _v39 = Field(_v1, 18);\n  if (_v39 == Val_int(0)) {\n    (_c2).open = NULL;\n  } else {\n    _v40 = Field(_v39, 0);\n    (_c2).open = camlidl_malloc_string(_v40, _ctx);\n  }\n  _v41 = Field(_v1, 19);\n  if (_v41 == Val_int(0)) {\n    (_c2).read = NULL;\n  } else {\n    _v42 = Field(_v41, 0);\n    (_c2).read = camlidl_malloc_string(_v42, _ctx);\n  }\n  _v43 = Field(_v1, 20);\n  if (_v43 == Val_int(0)) {\n    (_c2).write = NULL;\n  } else {\n    _v44 = Field(_v43, 0);\n    (_c2).write = camlidl_malloc_string(_v44, _ctx);\n  }\n  _v45 = Field(_v1, 21);\n  if (_v45 == Val_int(0)) {\n    (_c2).statfs = NULL;\n  } else {\n    _v46 = Field(_v45, 0);\n    (_c2).statfs = camlidl_malloc_string(_v46, _ctx);\n  }\n  _v47 = Field(_v1, 22);\n  if (_v47 == Val_int(0)) {\n    (_c2).flush = NULL;\n  } else {\n    _v48 = Field(_v47, 0);\n    (_c2).flush = camlidl_malloc_string(_v48, _ctx);\n  }\n  _v49 = Field(_v1, 23);\n  if (_v49 == Val_int(0)) {\n    (_c2).release = NULL;\n  } else {\n    _v50 = Field(_v49, 0);\n    (_c2).release = camlidl_malloc_string(_v50, _ctx);\n  }\n  _v51 = Field(_v1, 24);\n  if (_v51 == Val_int(0)) {\n    (_c2).fsync = NULL;\n  } else {\n    _v52 = Field(_v51, 0);\n    (_c2).fsync = camlidl_malloc_string(_v52, _ctx);\n  }\n  _v53 = Field(_v1, 25);\n  if (_v53 == Val_int(0)) {\n    (_c2).setxattr = NULL;\n  } else {\n    _v54 = Field(_v53, 0);\n    (_c2).setxattr = camlidl_malloc_string(_v54, _ctx);\n  }\n  _v55 = Field(_v1, 26);\n  if (_v55 == Val_int(0)) {\n    (_c2).getxattr = NULL;\n  } else {\n    _v56 = Field(_v55, 0);\n    (_c2).getxattr = camlidl_malloc_string(_v56, _ctx);\n  }\n  _v57 = Field(_v1, 27);\n  if (_v57 == Val_int(0)) {\n    (_c2).listxattr = NULL;\n  } else {\n    _v58 = Field(_v57, 0);\n    (_c2).listxattr = camlidl_malloc_string(_v58, _ctx);\n  }\n  _v59 = Field(_v1, 28);\n  if (_v59 == Val_int(0)) {\n    (_c2).removexattr = NULL;\n  } else {\n    _v60 = Field(_v59, 0);\n    (_c2).removexattr = camlidl_malloc_string(_v60, _ctx);\n  }\n}\nvalue camlidl_c2ml_Fuse_bindings_struct_fuse_operation_names(struct fuse_operation_names * _c1, camlidl_ctx _ctx)\n{\n  value _v2;\n  value _v3[29];\n  value _v4;\n  value _v5;\n  value _v6;\n  value _v7;\n  value _v8;\n  value _v9;\n  value _v10;\n  value _v11;\n  value _v12;\n  value _v13;\n  value _v14;\n  value _v15;\n  value _v16;\n  value _v17;\n  value _v18;\n  value _v19;\n  value _v20;\n  value _v21;\n  value _v22;\n  value _v23;\n  value _v24;\n  value _v25;\n  value _v26;\n  value _v27;\n  value _v28;\n  value _v29;\n  value _v30;\n  value _v31;\n  value _v32;\n  memset(_v3, 0, 29 * sizeof(value));\n  Begin_roots_block(_v3, 29)\n    if ((_c1).init == NULL) {\n      _v3[0] = Val_int(0);\n    } else {\n      _v4 = copy_string((_c1).init);\n      Begin_root(_v4)\n        _v3[0] = camlidl_alloc_small(1, 0);\n        Field(_v3[0], 0) = _v4;\n      End_roots();\n    }\n    if ((_c1).getattr == NULL) {\n      _v3[1] = Val_int(0);\n    } else {\n      _v5 = copy_string((_c1).getattr);\n      Begin_root(_v5)\n        _v3[1] = camlidl_alloc_small(1, 0);\n        Field(_v3[1], 0) = _v5;\n      End_roots();\n    }\n    if ((_c1).readlink == NULL) {\n      _v3[2] = Val_int(0);\n    } else {\n      _v6 = copy_string((_c1).readlink);\n      Begin_root(_v6)\n        _v3[2] = camlidl_alloc_small(1, 0);\n        Field(_v3[2], 0) = _v6;\n      End_roots();\n    }\n    if ((_c1).readdir == NULL) {\n      _v3[3] = Val_int(0);\n    } else {\n      _v7 = copy_string((_c1).readdir);\n      Begin_root(_v7)\n        _v3[3] = camlidl_alloc_small(1, 0);\n        Field(_v3[3], 0) = _v7;\n      End_roots();\n    }\n    if ((_c1).opendir == NULL) {\n      _v3[4] = Val_int(0);\n    } else {\n      _v8 = copy_string((_c1).opendir);\n      Begin_root(_v8)\n        _v3[4] = camlidl_alloc_small(1, 0);\n        Field(_v3[4], 0) = _v8;\n      End_roots();\n    }\n    if ((_c1).releasedir == NULL) {\n      _v3[5] = Val_int(0);\n    } else {\n      _v9 = copy_string((_c1).releasedir);\n      Begin_root(_v9)\n        _v3[5] = camlidl_alloc_small(1, 0);\n        Field(_v3[5], 0) = _v9;\n      End_roots();\n    }\n    if ((_c1).fsyncdir == NULL) {\n      _v3[6] = Val_int(0);\n    } else {\n      _v10 = copy_string((_c1).fsyncdir);\n      Begin_root(_v10)\n        _v3[6] = camlidl_alloc_small(1, 0);\n        Field(_v3[6], 0) = _v10;\n      End_roots();\n    }\n    if ((_c1).mknod == NULL) {\n      _v3[7] = Val_int(0);\n    } else {\n      _v11 = copy_string((_c1).mknod);\n      Begin_root(_v11)\n        _v3[7] = camlidl_alloc_small(1, 0);\n        Field(_v3[7], 0) = _v11;\n      End_roots();\n    }\n    if ((_c1).mkdir == NULL) {\n      _v3[8] = Val_int(0);\n    } else {\n      _v12 = copy_string((_c1).mkdir);\n      Begin_root(_v12)\n        _v3[8] = camlidl_alloc_small(1, 0);\n        Field(_v3[8], 0) = _v12;\n      End_roots();\n    }\n    if ((_c1).unlink == NULL) {\n      _v3[9] = Val_int(0);\n    } else {\n      _v13 = copy_string((_c1).unlink);\n      Begin_root(_v13)\n        _v3[9] = camlidl_alloc_small(1, 0);\n        Field(_v3[9], 0) = _v13;\n      End_roots();\n    }\n    if ((_c1).rmdir == NULL) {\n      _v3[10] = Val_int(0);\n    } else {\n      _v14 = copy_string((_c1).rmdir);\n      Begin_root(_v14)\n        _v3[10] = camlidl_alloc_small(1, 0);\n        Field(_v3[10], 0) = _v14;\n      End_roots();\n    }\n    if ((_c1).symlink == NULL) {\n      _v3[11] = Val_int(0);\n    } else {\n      _v15 = copy_string((_c1).symlink);\n      Begin_root(_v15)\n        _v3[11] = camlidl_alloc_small(1, 0);\n        Field(_v3[11], 0) = _v15;\n      End_roots();\n    }\n    if ((_c1).rename == NULL) {\n      _v3[12] = Val_int(0);\n    } else {\n      _v16 = copy_string((_c1).rename);\n      Begin_root(_v16)\n        _v3[12] = camlidl_alloc_small(1, 0);\n        Field(_v3[12], 0) = _v16;\n      End_roots();\n    }\n    if ((_c1).link == NULL) {\n      _v3[13] = Val_int(0);\n    } else {\n      _v17 = copy_string((_c1).link);\n      Begin_root(_v17)\n        _v3[13] = camlidl_alloc_small(1, 0);\n        Field(_v3[13], 0) = _v17;\n      End_roots();\n    }\n    if ((_c1).chmod == NULL) {\n      _v3[14] = Val_int(0);\n    } else {\n      _v18 = copy_string((_c1).chmod);\n      Begin_root(_v18)\n        _v3[14] = camlidl_alloc_small(1, 0);\n        Field(_v3[14], 0) = _v18;\n      End_roots();\n    }\n    if ((_c1).chown == NULL) {\n      _v3[15] = Val_int(0);\n    } else {\n      _v19 = copy_string((_c1).chown);\n      Begin_root(_v19)\n        _v3[15] = camlidl_alloc_small(1, 0);\n        Field(_v3[15], 0) = _v19;\n      End_roots();\n    }\n    if ((_c1).truncate == NULL) {\n      _v3[16] = Val_int(0);\n    } else {\n      _v20 = copy_string((_c1).truncate);\n      Begin_root(_v20)\n        _v3[16] = camlidl_alloc_small(1, 0);\n        Field(_v3[16], 0) = _v20;\n      End_roots();\n    }\n    if ((_c1).utime == NULL) {\n      _v3[17] = Val_int(0);\n    } else {\n      _v21 = copy_string((_c1).utime);\n      Begin_root(_v21)\n        _v3[17] = camlidl_alloc_small(1, 0);\n        Field(_v3[17], 0) = _v21;\n      End_roots();\n    }\n    if ((_c1).open == NULL) {\n      _v3[18] = Val_int(0);\n    } else {\n      _v22 = copy_string((_c1).open);\n      Begin_root(_v22)\n        _v3[18] = camlidl_alloc_small(1, 0);\n        Field(_v3[18], 0) = _v22;\n      End_roots();\n    }\n    if ((_c1).read == NULL) {\n      _v3[19] = Val_int(0);\n    } else {\n      _v23 = copy_string((_c1).read);\n      Begin_root(_v23)\n        _v3[19] = camlidl_alloc_small(1, 0);\n        Field(_v3[19], 0) = _v23;\n      End_roots();\n    }\n    if ((_c1).write == NULL) {\n      _v3[20] = Val_int(0);\n    } else {\n      _v24 = copy_string((_c1).write);\n      Begin_root(_v24)\n        _v3[20] = camlidl_alloc_small(1, 0);\n        Field(_v3[20], 0) = _v24;\n      End_roots();\n    }\n    if ((_c1).statfs == NULL) {\n      _v3[21] = Val_int(0);\n    } else {\n      _v25 = copy_string((_c1).statfs);\n      Begin_root(_v25)\n        _v3[21] = camlidl_alloc_small(1, 0);\n        Field(_v3[21], 0) = _v25;\n      End_roots();\n    }\n    if ((_c1).flush == NULL) {\n      _v3[22] = Val_int(0);\n    } else {\n      _v26 = copy_string((_c1).flush);\n      Begin_root(_v26)\n        _v3[22] = camlidl_alloc_small(1, 0);\n        Field(_v3[22], 0) = _v26;\n      End_roots();\n    }\n    if ((_c1).release == NULL) {\n      _v3[23] = Val_int(0);\n    } else {\n      _v27 = copy_string((_c1).release);\n      Begin_root(_v27)\n        _v3[23] = camlidl_alloc_small(1, 0);\n        Field(_v3[23], 0) = _v27;\n      End_roots();\n    }\n    if ((_c1).fsync == NULL) {\n      _v3[24] = Val_int(0);\n    } else {\n      _v28 = copy_string((_c1).fsync);\n      Begin_root(_v28)\n        _v3[24] = camlidl_alloc_small(1, 0);\n        Field(_v3[24], 0) = _v28;\n      End_roots();\n    }\n    if ((_c1).setxattr == NULL) {\n      _v3[25] = Val_int(0);\n    } else {\n      _v29 = copy_string((_c1).setxattr);\n      Begin_root(_v29)\n        _v3[25] = camlidl_alloc_small(1, 0);\n        Field(_v3[25], 0) = _v29;\n      End_roots();\n    }\n    if ((_c1).getxattr == NULL) {\n      _v3[26] = Val_int(0);\n    } else {\n      _v30 = copy_string((_c1).getxattr);\n      Begin_root(_v30)\n        _v3[26] = camlidl_alloc_small(1, 0);\n        Field(_v3[26], 0) = _v30;\n      End_roots();\n    }\n    if ((_c1).listxattr == NULL) {\n      _v3[27] = Val_int(0);\n    } else {\n      _v31 = copy_string((_c1).listxattr);\n      Begin_root(_v31)\n        _v3[27] = camlidl_alloc_small(1, 0);\n        Field(_v3[27], 0) = _v31;\n      End_roots();\n    }\n    if ((_c1).removexattr == NULL) {\n      _v3[28] = Val_int(0);\n    } else {\n      _v32 = copy_string((_c1).removexattr);\n      Begin_root(_v32)\n        _v3[28] = camlidl_alloc_small(1, 0);\n        Field(_v3[28], 0) = _v32;\n      End_roots();\n    }\n    _v2 = camlidl_alloc_small(29, 0);\n    { mlsize_t _c33;\n      for (_c33 = 0; _c33 < 29; _c33++) Field(_v2, _c33) = _v3[_c33];\n    }\n  End_roots()\n  return _v2;\n}\nextern void camlidl_ml2c_Fuse_bindings_struct_fuse(value, struct fuse , camlidl_ctx _ctx);\nextern value camlidl_c2ml_Fuse_bindings_struct_fuse(struct fuse , camlidl_ctx _ctx);\nvoid camlidl_ml2c_Fuse_bindings_structfusecontext(value _v1, struct fuse_context * _c2, camlidl_ctx _ctx)\n{\n  value _v3;\n  value _v4;\n  value _v5;\n  value _v6;\n  _v3 = Field(_v1, 0);\n  (_c2).fuse = (struct fuse ) Field(_v3, 0);\n  _v4 = Field(_v1, 1);\n  (_c2).uid = Int_val(_v4);\n  _v5 = Field(_v1, 2);\n  (_c2).gid = Int_val(_v5);\n  _v6 = Field(_v1, 3);\n  (*_c2).pid = Int_val(_v6);\n}\nvalue camlidl_c2ml_Fuse_bindings_structfusecontext(struct fuse_context * _c1, camlidl_ctx _ctx)\n{\n  value _v2;\n  value _v3[4];\n  _v3[0] = _v3[1] = _v3[2] = _v3[3] = 0;\n  Begin_roots_block(_v3, 4)\n    _v3[0] = camlidl_alloc_small(1, Abstract_tag);\n    Field(_v3[0], 0) = (value) (_c1).fuse;\n    _v3[1] = Val_int((_c1).uid);\n    _v3[2] = Val_int((_c1).gid);\n    _v3[3] = Val_int((_c1).pid);\n    _v2 = camlidl_alloc_small(4, 0);\n    Field(_v2, 0) = _v3[0];\n    Field(_v2, 1) = _v3[1];\n    Field(_v2, 2) = _v3[2];\n    Field(_v2, 3) = _v3[3];\n  End_roots()\n  return _v2;\n}\nvalue camlidl_Fuse_bindings_fuse_get_context(value _unit)\n{\n  struct __fuse_context *_res;\n  value _vres;\nstruct camlidl_ctx_struct _ctxs = { CAMLIDL_TRANSIENT, NULL };\n  camlidl_ctx _ctx = &_ctxs;\n  _res = fuse_get_context();\n  _vres = camlidl_c2ml_Fuse_bindings_struct___fuse_context(&*_res, _ctx);\n  camlidl_free(_ctx);\n  return _vres;\n}\nvalue camlidl_Fuse_bindings_get_fuse_operations(value _unit)\n{\n  struct fuse_operations *_res;\n  value _vres;\n_res = get_fuse_operations();\n  _vres = camlidl_alloc_small(1, Abstract_tag);\n  Field(_vres, 0) = (value) _res;\n  return _vres;\n}\nvalue camlidl_Fuse_bindings_set_fuse_operations(\n    value _v_op)\n{\n  struct fuse_operation_names const op; /in*/\n  struct fuse_operation_names _c1;\n  struct camlidl_ctx_struct _ctxs = { CAMLIDL_TRANSIENT, NULL };\n  camlidl_ctx _ctx = &_ctxs;\n  op = &_c1;\n  camlidl_ml2c_Fuse_bindings_struct_fuse_operation_names(_v_op, &_c1, _ctx);\n  set_fuse_operations(op);\n  camlidl_free(_ctx);\n  return Val_unit;\n}\nvalue camlidl_Fuse_bindings___fuse_read_cmd(\n    value _v_f)\n{\n  struct fuse f; /in/\n  struct fuse_cmd _res;\n  value _vres;\nf = (struct fuse *) Field(_v_f, 0);\n  enter_blocking_section();\n  _res = __fuse_read_cmd(f);\n  leave_blocking_section();\n  _vres = camlidl_alloc_small(1, Abstract_tag);\n  Field(_vres, 0) = (value) _res;\n  return _vres;\n}\nvalue camlidl_Fuse_bindingsfuseprocess_cmd(\n    value _v_f,\n    value _v_cmd)\n{\n  struct fuse f; /in/\n  struct fuse_cmd cmd; /in/\n  f = (struct fuse ) Field(_v_f, 0);\n  cmd = (struct fuse_cmd ) Field(_v_cmd, 0);\n  enter_blocking_section();\n  fuse_process_cmd(f, cmd);\n  leave_blocking_section();\n  return Val_unit;\n}\nvalue camlidl_Fuse_bindings_ml_fuse_init(value _unit)\n{\n  ml_fuse_init();\n  return Val_unit;\n}\nvalue camlidl_Fuse_bindings_ml_fuse_main(\n    value _v_argv,\n    value _v_op)\n{\n  int argc; /in/\n  str argv; /in/\n  struct fuse_operations const op; /in/\n  mlsize_t _c1;\n  mlsize_t _c2;\n  value _v3;\n  struct camlidl_ctx_struct _ctxs = { CAMLIDL_TRANSIENT, NULL };\n  camlidl_ctx _ctx = &_ctxs;\n  _c1 = Wosize_val(_v_argv);\n  argv = camlidl_malloc(_c1 * sizeof(str ), _ctx);\n  for (_c2 = 0; _c2 < _c1; _c2++) {\n    _v3 = Field(_v_argv, _c2);\n    camlidl_ml2c_Fuse_bindings_str(_v3, &argv[_c2], _ctx);\n  }\n  argc = _c1;\n  op = (struct fuse_operations const *) Field(_v_op, 0);\n  ml_fuse_main(argc, argv, op);\n  camlidl_free(_ctx);\n  return Val_unit;\n}\nvalue camlidl_Fuse_bindings___fuse_exited(\n    value _v_f)\n{\n  struct fuse f; /in*/\n  int _res;\n  value _vres;\nf = (struct fuse *) Field(_v_f, 0);\n  enter_blocking_section();\n  _res = __fuse_exited(f);\n  leave_blocking_section();\n  _vres = Val_int(_res);\n  return _vres;\n}\n```\n. No dice. It's still whining about the same symbols.\n. This is a directory listing of /usr/local/lib.\n/usr/local/lib/libfuse.0.dylib\n/usr/local/lib/libfuse.2.dylib\n/usr/local/lib/libfuse.dylib\n/usr/local/lib/libfuse_ino64.2.dylib\n/usr/local/lib/libfuse_ino64.dylib\n/usr/local/lib/libmacfuse_i32.2.dylib\n/usr/local/lib/libmacfuse_i32.dylib\n/usr/local/lib/libmacfuse_i64.2.dylib\n/usr/local/lib/libmacfuse_i64.dylib\n/usr/local/lib/libosxfuse.2.dylib\n/usr/local/lib/libosxfuse.dylib\n/usr/local/lib/libosxfuse.la\n/usr/local/lib/libosxfuse_i32.2.dylib\n/usr/local/lib/libosxfuse_i32.dylib\n/usr/local/lib/libosxfuse_i32.la\n/usr/local/lib/libosxfuse_i64.2.dylib\n/usr/local/lib/libosxfuse_i64.dylib\n/usr/local/lib/libosxfuse_i64.la\nThe first fuse implementation for Mac OS X was MacFUSE. It has been abandoned. The successor of MacFUSE is OSXFUSE. However, MacFUSE had a flaw, a different API than the Linux version. So, file systems had to be rewritten.  Seeing the API problem ,another project has sprung up, Fuse4X, and it fixed the API. Since then, Fuxe4X has been merged into OSXFUSE. I only have OSXFUSE installed, but that's why there are so many libraries listed.\nThere is no -D switch on Mac OS X. There is also no output from libfuse.dylib. Perhaps it loads libosxfuse.dylib.\nsh\n$ nm -f /usr/local/lib/libfuse.dylib | egrep 'fuse_(exited|(process|read)_cmd)'\n$ nm -f /usr/local/lib/libosxfuse.dylib | egrep 'fuse_(exited|(process|read)_cmd)'\n0000000000004e4d T _fuse_exited\n0000000000004e5b T _fuse_process_cmd\n0000000000004d3a T _fuse_read_cmd\n. This is what's found in libfuse.dylib.\nsh\n$ nm /usr/local/lib/libfuse.dylib\n0000000000000ef4 t __dyld_func_lookup\n0000000000000f04 t _libmacfuse_constructor\n0000000000000efa T _macfuse_version\n                 U _osxfuse_enable_macfuse_mode\n                 U _osxfuse_version\n0000000000000ee0 t dyld_stub_binding_helper\n. I have tried to put -fno-leading-underscore in CPPFLAGS. It didn't work. I have also tried to replace the single underscore version. I have tried to remove one underscore and both underscores with search and replace. Then I get the following.\nsh\nmake -f Makefile.fusexmp\nocamlopt \\\n                  \\\n                unix.cmxa threads.cmxa -thread     -I /Users/sorin/.homebrew/lib/ocaml/site-packages/ocamlfuse   -ccopt -L/Users/sorin/.opam/system/lib/camlidl -ccopt -L/usr/local/lib -ccopt -L/Users/sorin/.homebrew/lib  -ccopt -L/Users/sorin/.homebrew/lib/ocaml/site-packages/ocamlfuse    bigarray.cmxa Fuse.cmxa -cclib -lfuse -cclib -lcamlidl  -o fusexmp \\\n                fusexmp.cmx\nFile \"_none_\", line 1:\nError: Files fusexmp.cmx\n       and /Users/sorin/.homebrew/lib/ocaml/site-packages/ocamlfuse/Fuse.cmxa\n       make inconsistent assumptions over interface Fuse_bindings\nmake[2]: *** [fusexmp] Error 2\nmake[1]: *** [native-code] Error 2\nmake: *** [default] Error 2\n. I have got it to work by removing both underscores and using the following commands. It mounts the Hello World file system.\nsh\nCPPFLAGS=\"-DFUSE_USE_VERSION=26 -I/usr/local/include/osxfuse\" INCDIRS=\"$(opam config var lib)/camlidl make\" -C lib install\nsh\nLDFLAGS=\"-L$(opam config var lib)/camlidl\" make -C example\nI don't think those underscores are correct. I think the compiler should be allowed to add them. By hardcoding those underscores, it causes problems with automatic linking and/or -fno-leading-underscore and -leading-underscore compiler options.\n. I have got google-drive-ocamlfuse to install.\nsh\n$ opam pin ocamlfuse https://github.com/sorin-ionescu/ocamlfuse.git\n$ CPPFLAGS='-DFUSE_USE_VERSION=26 -I/usr/local/include/osxfuse' opam install ocamlfuse\n$ opam install google-drive-ocamlfuse\n. You're welcome.\n. It opened Firefox because it I created a script in $PATH that in turn opens Applications/Firefox.app/Contents/MacOS/firefox; otherwise it would have failed.\nI replaced xdg-open with open and removed -obig_writes. Now it runs without errors.  It mounts. However, the mount point is empty. There are no files listed. \n. I am not sure if it's of any use, but here's the output.\n``` sh\n$ google-drive-ocamlfuse -d -o volname='Google Drive' ~/Desktop/Google\\ Drive\nunique: 0, opcode: INIT (26), nodeid: 0, insize: 56\nINIT: 7.8\nflags=0x00000000\nmax_readahead=0x00100000\n   INIT: 7.8\n   flags=0x00000000\n   max_readahead=0x00100000\n   max_write=0x01000000\n   unique: 0, error: 0 (Undefined error: 0), outsize: 40\nunique: 0, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 0, error: 0 (Undefined error: 0), outsize: 96\nunique: 0, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 0, error: 0 (Undefined error: 0), outsize: 96\nunique: 0, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 0, error: 0 (Undefined error: 0), outsize: 96\nunique: 1, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 1, error: 0 (Undefined error: 0), outsize: 96\nunique: 0, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 0, error: 0 (Undefined error: 0), outsize: 96\nunique: 2, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 2, error: 0 (Undefined error: 0), outsize: 96\nunique: 3, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 3, error: 0 (Undefined error: 0), outsize: 96\nunique: 4, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 4, error: 0 (Undefined error: 0), outsize: 96\nunique: 5, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 5, error: 0 (Undefined error: 0), outsize: 96\nunique: 1, opcode: ACCESS (34), nodeid: 1, insize: 48\nACCESS / 00\n   unique: 1, error: -78 (Function not implemented), outsize: 16\nunique: 6, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 6, error: 0 (Undefined error: 0), outsize: 96\nunique: 0, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 0, error: 0 (Undefined error: 0), outsize: 96\nunique: 2, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 2, error: 0 (Undefined error: 0), outsize: 96\nunique: 3, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 3, error: 0 (Undefined error: 0), outsize: 96\nunique: 4, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 4, error: 0 (Undefined error: 0), outsize: 96\nunique: 5, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 5, error: 0 (Undefined error: 0), outsize: 96\nunique: 1, opcode: GETATTR (3), nodeid: 1, insize: 40\n   unique: 1, error: 0 (Undefined error: 0), outsize: 128\nunique: 6, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 6, error: 0 (Undefined error: 0), outsize: 96\nunique: 0, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 0, error: 0 (Undefined error: 0), outsize: 96\nunique: 2, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 2, error: 0 (Undefined error: 0), outsize: 96\nunique: 3, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 3, error: 0 (Undefined error: 0), outsize: 96\nunique: 4, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 4, error: 0 (Undefined error: 0), outsize: 96\nunique: 5, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 5, error: 0 (Undefined error: 0), outsize: 96\nunique: 1, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 1, error: 0 (Undefined error: 0), outsize: 96\nunique: 6, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 6, error: 0 (Undefined error: 0), outsize: 96\nunique: 0, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 0, error: 0 (Undefined error: 0), outsize: 96\nunique: 2, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 2, error: 0 (Undefined error: 0), outsize: 96\nunique: 3, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 3, error: 0 (Undefined error: 0), outsize: 96\nunique: 4, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 4, error: 0 (Undefined error: 0), outsize: 96\nunique: 5, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 5, error: 0 (Undefined error: 0), outsize: 96\nunique: 1, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 1, error: 0 (Undefined error: 0), outsize: 96\nunique: 6, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 6, error: 0 (Undefined error: 0), outsize: 96\nunique: 0, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 0, error: 0 (Undefined error: 0), outsize: 96\nunique: 2, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 2, error: 0 (Undefined error: 0), outsize: 96\nunique: 3, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 3, error: 0 (Undefined error: 0), outsize: 96\nunique: 4, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 4, error: 0 (Undefined error: 0), outsize: 96\nunique: 5, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 5, error: 0 (Undefined error: 0), outsize: 96\nunique: 1, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 1, error: 0 (Undefined error: 0), outsize: 96\nunique: 6, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 6, error: 0 (Undefined error: 0), outsize: 96\nunique: 0, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 0, error: 0 (Undefined error: 0), outsize: 96\nunique: 2, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 2, error: 0 (Undefined error: 0), outsize: 96\nunique: 3, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 3, error: 0 (Undefined error: 0), outsize: 96\nunique: 4, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 4, error: 0 (Undefined error: 0), outsize: 96\nunique: 5, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 5, error: 0 (Undefined error: 0), outsize: 96\nunique: 1, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 1, error: 0 (Undefined error: 0), outsize: 96\nunique: 6, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 6, error: 0 (Undefined error: 0), outsize: 96\nunique: 0, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 0, error: 0 (Undefined error: 0), outsize: 96\nunique: 2, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 2, error: 0 (Undefined error: 0), outsize: 96\nunique: 3, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 3, error: 0 (Undefined error: 0), outsize: 96\nunique: 4, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 4, error: 0 (Undefined error: 0), outsize: 96\nunique: 5, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 5, error: 0 (Undefined error: 0), outsize: 96\nunique: 1, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 1, error: 0 (Undefined error: 0), outsize: 96\nunique: 6, opcode: STATFS (17), nodeid: 1, insize: 40\n   unique: 6, error: 0 (Undefined error: 0), outsize: 96\nunique: 0, opcode: DESTROY (38), nodeid: 1, insize: 40\n   unique: 0, error: 0 (Undefined error: 0), outsize: 16\n``\n.-debug` doesn't work.\nsh\n$ google-drive-ocamlfuse -debug -o volname='Google Drive' ~/Desktop/Google\\ Drive\nStarting application setup (label=default).\nOpening log file: /Users/sorin/.gdfuse/default/gdfuse.log\n[1]    29273 illegal hardware instruction  google-drive-ocamlfuse -debug -o volname='Google Drive'\n. Before it crashes, this is ~/.gdfuse/default/gdfuse.log.\n[0.000459] TID=0: Setting up default filesystem...\n[0.000543] TID=0: Loading configuration from /Users/sorin/.gdfuse/default/config...done\nSaving configuration in /Users/sorin/.gdfuse/default/config...done\n[0.001641] TID=0: Loading application state from /Users/sorin/.gdfuse/default/state...done\nCurrent version: 0.5.2\nSetting up cache db...done\nSetting up CURL...done\nRefresh token already present.\n[0.005730] TID=0: Starting filesystem /Users/sorin/Desktop/Google Drive\n[0.061853] TID=0: init_filesystem\n[0.065457] TID=0: statfs /\nLoading metadata from db...not valid\nRefreshing metadata...\nBefore it crashes, this is ~/.gdfuse/default/curl.log.\n[0.090360] curl: info: About to connect() to www.googleapis.com port 443 (#0)\n[0.090508] curl: info:   Trying 74.125.196.95...\n[0.102681] curl: info: connected\n[0.102831] curl: info: Connected to www.googleapis.com (74.125.196.95) port 443 (#0)\n[0.104359] curl: info: SSLv3, TLS handshake, Client hello (1):\n. ",
    "volopasse": "I appreciate the issue was closed long time ago, but I'm facing a similar problem when trying to install. I'm running OSX 10.10. Install fails with a very similar pattern - looks like OSXFUSE is not being linked up correctly. Both FUSE and opam were installed via Homebrew if that is relevant\n```\n=-=- Processing actions -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  \ud83d\udc2b \n[ERROR] The compilation of ocamlfuse failed at \"make -C lib\n        INCDIRS=/Users/Denis/.opam/system/lib/camlidl\".\nProcessing  1/2: [ocamlfuse: ocamlfind remove]\n=== ERROR while installing ocamlfuse.2.7.1-cvs2 ==============================\nopam-version 1.2.1\nos           darwin\ncommand      make -C lib INCDIRS=/Users/Denis/.opam/system/lib/camlidl\npath         /Users/Denis/.opam/system/build/ocamlfuse.2.7.1-cvs2\ncompiler     system (4.02.1)\nexit-code    2\nenv-file     /Users/Denis/.opam/system/build/ocamlfuse.2.7.1-cvs2/ocamlfuse-38445-e7966f.env\nstdout-file  /Users/Denis/.opam/system/build/ocamlfuse.2.7.1-cvs2/ocamlfuse-38445-e7966f.out\nstderr-file  /Users/Denis/.opam/system/build/ocamlfuse.2.7.1-cvs2/ocamlfuse-38445-e7966f.err\nstdout\ncamlidl -header  \\\n[...]\nmaking ._ncdi/Fuse_bindings.di from Fuse_bindings.mli\nmaking ._d/Fuse_bindings.d from Fuse_bindings.ml\nmaking ._d/Fuse.d from Fuse.ml\nmaking ._d/Fuse_lib.d from Fuse_lib.ml\nmaking ._d/Unix_util.d from Unix_util.ml\nmaking ._d/Result.d from Result.ml\nocamlc -c -cc \"cc\" -ccopt \"-fPIC -D_FILE_OFFSET_BITS=64 -I. -pthread \\\n-DPIC -DNATIVE_CODE   \\\n-I/Users/Denis/.opam/system/lib/camlidl  -o Fuse_bindings_stubs.o \" Fuse_bindings_stubs.c\nstderr\nIn file included from Fuse_bindings_stubs.c:17:\n./Fuse_bindings.h:16:10: fatal error: 'fuse.h' file not found\n#include \n^\n1 error generated.\nmake[1]: *** [Fuse_bindings_stubs.o] Error 2\nmake: *** [native-code-library] Error 2\n=-=- Error report -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  \ud83d\udc2b \nThe following actions were aborted\n  \u2217  install google-drive-ocamlfuse 0.5.17\nThe following actions failed\n  \u2217  install ocamlfuse 2.7.1-cvs2\nNo changes have been performed\n```\n. ",
    "scriby": "It looks like the act of simply copying a large file out of the google drive onto the local disk causes it to get placed in cache (even if the file is larger than the specified cache size)\n. Yes, all I am doing is\n1. Copy 1 GB+ file from Google Drive to linux file system\n2. Observe that even after the transfer completes, the .cache folder is > 1 GB in size (it was a few K before)\nThere did not appear to be any errors or issues during the copy.\n. ",
    "glubox": "Hi, I would also like to see direct download option. I'm using several google drives to store large number of multimedia files which are only read by the system on which the drive is mounted, everything works fine, but sometimes when my app want to access a file on the drive there is little delay and this causes my app to stall forever. What i observed is that when my app hits a file on the drive it is downloaded in the cache folder, may be if the download does not happen very fast i get the problem. If the download could begin in chunks may be my app will read the file seamlessly. \n. ",
    "PlexMediaFan": "AS - Thanks for all your hard work in making our google drives available to Linux No-GUI installs!\nI have the same need as stated in this thread -\nI have a VPS running Ubuntu 13.10 CLI only; running as a plex media server.  However, the local space on the server is very limited, so I am using google-drive-ocamlfuse version 0.5.3-0ubuntu1 to access the google drive with my media on it (and it works great).  The problem occurs when I access my media on my google drive - it immediately downloads the whole movie (5-6GB) from the google drive and stores it locally in VPS's cache before it plays or really does anything.  Can you maybe add an option to NOT download and cache the entire movies/media that are accessed on google drive?  I don't have the local HD space on my VPS to support caching files that size and really just want the ability to read my movie files on GD just like a regular hard drive and then Plex Media Server will send to me for viewing.  \nThanks Again\n. ",
    "iplor": "Thanks for the very useful program.\nI wondered if you have made any progress with this request? As with the users above, I'm dealing with large media files that I wish to access directly from the mounted drive, without caching locally first. The local caching causes long delays and timeouts in my scenario.\nAt the moment I can only achieve this using the NetDrive commercial app on Windoze. This is essentially quite similar to your project, but it only needs to cache for uploads.\n. I've just tested 0.5.10 from the ubuntu repo, and I can confirm that the new options are present and working. Good work. Thank you.\n. I look forward to hearing the results of the testing. The very low streaming speed is killing my use-case, too.\nIt seems to me that Google drive is at its slowest when dealing many small requests. For those of us trying to stream large media files, I wonder if increasing the blocksize up into the megabyte range might be more efficient?\n. This might be of interest. I found another project called HTTPFS, which allows you to mount a single file over HTTP, using fuse.\nI get transfer speeds of around 15 Mbits/s when accessing the mounted file.  This is still well below the full speed I get from a straight web download, but it is around 3 times faster than what I'm getting with google-drive-ocamlfuse. Since both are using fuse, and the speeds differ significantly, I thought it might suggest some clues.\n. My suspicion is that the main problem is not Fuse itself. I also used another project in the past, GDriveF4JS, and that used to have no problems streaming full HD. It was always a bit unstable, though, and now broken and abandoned. But I can't see how these other Fuse systems can stream efficiently if the problem is fundamental to the default kernel Fuse parameters. \nPerhaps they are using a partial caching method, rather than relying on caching entire files?\n. I certainly appreciate the hard work you've already put it to this. If you have time to implement such a solution in the future, that would be terrific.\nHave you discovered if tweaking the Fuse parameters in the kernel also solves the problem? It's obviously not an ideal solution, but it'd be fine for those of us that don't mind tinkering with things.\n. When you say it's performing very nicely with the -m switch, what sort of speeds are you getting?\n. Do you think the idea of an in-memory buffer to get efficient streaming  is something that will be implemented? I tried playing around with compiling a tweaked kernel, but I still wasn't getting good performance.\n. That's great - thanks very much. I'll give it a try and see if it works better than my own attempts.\nI'm still curious to hear if the idea of a memory buffer, as mentioned by Astrada,  is on the roadmap, or if that's something that's not being considered at the moment.\n. I installed those packages, and booted with the custom kernel. The difference is clearly noticeable, but for me the transfer speed now jumps wildly from 0 to full speed, and everywhere in between. The average rate is still too low, and the erratic speed means that streaming is still not working well.\n. Officially 75 mbits, in practice more like 60. When downloading files using the google client or google-drive-ocamlfuse in non-streaming mode, I get almost full speed, at 5 - 6 megabytes per second. \n. Thanks so much for doing this. So far, it's looking really encouraging. I'm seeing smooth playback of large videos, and the available bandwidth being well-utilised. I'm still messing around with the config options, and I'll report back after using it for a while.. After quite a bit of testing, I've had no problems at all. Streaming full Bluyray backups with no hiccups. Brilliant.\nWhen you have the time, could you give a bit more detail on how the config options work?. Thank you, that's clear.\nOne more question - does this method generate a lot of API requests, like a separate request for each small block that is cached? I'm just wondering if the Google API limits could become a problem.. I'm sure you know more about it than I. \nI've just seen the issue come up in a couple of other projects that use the Drive API (I think rclone was one of them). I believe the issue was around the rate limit, rather than the absolute limit. I've had no trouble so far, so hopefully it's a non-issue here.. ",
    "jay4fusion": "I think just a simple option to the command line utility to print this info would be very useful\n. I see,\nthanks for letting me know!\nThank you for the great plugin!\nJay\nOn Thu, Jan 9, 2014 at 3:07 PM, Alessandro Strada\nnotifications@github.comwrote:\n\nUnfortunately, I don't think it's possible to add a progress bar visible\nin some kind of UI (console or GUI). My application provides a FUSE\nfilesystem, and a filesystem does not provide a callback to report progress\nto the user interface. Moreover my application is single threaded, because\nthe multi-threading version of the library I'm using is not stable, so\nthere would be no way to notify the calling process in the middle of a\ndownload or an upload. Sorry.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/44#issuecomment-31976305\n.\n. \n",
    "bericp1": "I'll just drop a +1 here.\n. ",
    "fastcat": "A suggestion: provide an API for a separate UI process to query sync status.  A couple ways you could do this:\n1. Create a unix domain socket in the root of the FUSE mount which the FUSE daemon itself is listening to.  Not sure if this introduces any circular dependencies unmounting, though.\n2. Create a unix domain socket in a well known location (/tmp) that can be used to query status.  Creating things with predictable names in /tmp has security problems, though.\n3. Newer versions of FUSE support handling custom ioctls in the fs, you could provide some custom ioctls issued to a magic file or the root mount directory to query the status.  Supporting ioctls to directories requires even newer versions than ioctls to files, but either should work on any current linux distro.\n4. For maximum compatibility with things other than Linux (e.g. MacFUSE), provide a magic file in the root of the mounted FS that, when read, provides status.\nNote that, for all the above options that add files to the FS, it is OK in fuse to have a filename that does not show in directory listings but is accessible when opened by name.  Or you could have the status socket/file visible, if that's not a problem.\n. Thanks, got the new version and confirmed the format change & cache clear works :). ",
    "quattrocup": "Thanks.  I do get the curl.log file now but it is empty.  \nwhat is the step after \"Setting up CURL...done\" supposed to be?  maybe that will give me a clue as to what is failing.\n. No verification code.  I created the OAuth2 id on google and run this command:\ngoogle-drive-ocamlfuse -label me -id {client id}.apps.googleusercontent.com -secret {client secret} -verbose -debug\nit just hangs after \"Setting up CURL...done\"\nit does not appear to be connecting out to anything.\ni followed this guide to set up authorization:\nhttps://github.com/astrada/google-drive-ocamlfuse/wiki/Headless-Usage-%26-Authorization\n. yup, that did it!  after it failed to find xdg-open, it continued with the setup and i was able to get the verification URL.  \nthanks!\njust a suggestion for future release, you should provide an option or switch to run the initialization on a headless server such that it will not try and open a browser automatically and instead spit out the verification URL.  This is what Grive and InSync do.  However Grive does not sync Google Docs like google-drive-ocamlfuse does and InSync charges $15.\n. ",
    "brihuega": "I didn't reproduce the problem exactly in the debug session. Copying a big\namount of files it threw a couple of \"resource busy\" errors, and then\nstopped the process with the message:\nError: cannot close sqlite db.\nPlease restart the program\nAt the same time, the \"cp\" command issues a lot of messages (one for every\nfile pending to copy):\nCannot 'stat': el otro extremo de la conexi\u00f3n no esta conectado.\nThe logs show \"500 Internal Server\" errors on the \"resource busy\" errors:\n----gdfuse.log---\nLoading resource /Mis im\u00c3\u00a1genes/100504 (plantar un arbol - caleta)\n(trashed=false) from db...found\nGetting resource 100504 020.jpg (in folder 0BwglN0eAuhZ_THdFelo2OGNGTVE)\nfrom server...ServiceError\nException:Failure(\"{\\\"error\\\":{\\\"errors\\\":[{\\\"domain\\\":\\\"global\\\",\\\"reason\\\":\\\"internalError\\\",\\\"message\\\":\\\"Internal\nError\\\"}],\\\"code\\\"\n:500,\\\"message\\\":\\\"Internal Error\\\"}}\")\nBacktrace:\n[4039.294670] TID=0: getattr /Mis im\u00c3\u00a1genes\n----curl.log-----\n[4033.976285] curl: info: About to connect() to www.googleapis.com port 443\n(#0)\n[4033.976315] curl: info:   Trying 173.194.78.95...\n[4034.012024] curl: info: connected\n[4034.062611] curl: info: found 153 certificates in\n/etc/ssl/certs/ca-certificates.crt\n[4034.228901] curl: info:        server certificate verification OK\n[4034.229061] curl: info:        common name: .googleapis.com (matched)\n[4034.229073] curl: info:        server certificate expiration date OK\n[4034.229082] curl: info:        server certificate activation date OK\n[4034.229097] curl: info:        certificate public key: RSA\n[4034.229107] curl: info:        certificate version: #3\n[4034.229179] curl: info:        subject: C=US,ST=California,L=Mountain\nView,O=Google Inc,CN=.googleapis.com\n[4034.229194] curl: info:        start date: Wed, 15 Jan 2014 14:37:40 GMT\n[4034.229205] curl: info:        expire date: Thu, 15 May 2014 00:00:00 GMT\n[4034.229243] curl: info:        issuer: C=US,O=Google Inc,CN=Google\nInternet Authority G2\n[4034.229261] curl: info:        compression: NULL\n[4034.229271] curl: info:        cipher: ARCFOUR-128\n[4034.229279] curl: info:        MAC: SHA1\n[4034.229368] curl: header out: GET\n/drive/v2/files?fields=items%28alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2Cexplici\ntlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents\n%2Ctitle%29%2CnextPageToken&maxResults=1&q=title+%3D+%27100504+020.jpg%27+and+%270BwglN0eAuhZ_THdFelo2OGNGTVE%27+in+parents+and+trashed+\n%3D+false HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.2) gapi-ocaml/0.2.1/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nAuthorization: Bearer\nya29.1.AADtN_Wz-w4X8Acsn7ASfhRAdxZtK0a6qtj1RKZCYyZlfAUwMezZbyYS9NnanA\n[4039.270979] curl: header in: HTTP/1.1 500 Internal Server Error\n[4039.271031] curl: header in: Content-Type: application/json; charset=UTF-8\n[4039.271049] curl: header in: Date: Fri, 31 Jan 2014 10:25:32 GMT\n[4039.271071] curl: header in: Expires: Fri, 31 Jan 2014 10:25:32 GMT\n[4039.271080] curl: header in: Cache-Control: private, max-age=0\n[4039.271089] curl: header in: X-Content-Type-Options: nosniff\n[4039.271098] curl: header in: X-Frame-Options: SAMEORIGIN\n[4039.271106] curl: header in: X-XSS-Protection: 1; mode=block\n[4039.271114] curl: header in: Server: GSE\n[4039.271123] curl: header in: Alternate-Protocol: 443:quic\n[4039.271132] curl: header in: Transfer-Encoding: chunked\n[4039.271141] curl: header in:\n[4039.271150] curl: data in: b4\n{\n \"error\": {\n  \"errors\": [\n   {\n    \"domain\": \"global\",\n    \"reason\": \"internalError\",\n    \"message\": \"Internal Error\"\n   }\n  ],\n  \"code\": 500,\n  \"message\": \"Internal Error\"\n }\n}\n[4039.271170] curl: info: Connection #0 to host www.googleapis.com left\nintact\n[4039.271234] curl: info: Closing connection #0\n[\nOn the final lines of gdfuse.log:\nThread id=0: Error: cannot close db\n[10660.184127] TID=0: Exiting.\nCURL cleanup...done\nClearing context...done\n\nSaludos de jose.brihuega@gmail.com\nC\u00e1diz - Espa\u00f1a\n2014-01-31 Alessandro Strada notifications@github.com:\n\nYou should try to reproduce the problem, running the program in debug mode\n:\n$ google-drive-ocamlfuse -debug [mountpoint]\nThen you should check the log files gdfuse.log and curl.log (you will find\nin ~/.gdfuse/default) and look for errors 400/500, returned by Google Drive.\n\nReply to this email directly or view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/48#issuecomment-33771188\n.\n. Thanks for your efforts making this great software. I will give you more\nfeedback if I find it.\n\n\nSaludos de jose.brihuega@gmail.com\nC\u00e1diz - Espa\u00f1a\n2014-02-08 18:27 GMT+01:00 piccaso notifications@github.com:\n\nThanks for your response @astrada https://github.com/astrada!\nI don't think its related to the Feb 6 issue, i still have the same\nproblems (even without -m)\nHere is my workaround for now:\nLarge timeouts and cache - just clear cache on demand.\nIn my case no problem, because the content does not change atm.\nmetadata_cache_time=6000\nsqlite3_busy_timeout=50000\nmax_cache_size_mb=5120\nusing rsync with -W (no delta operations - copy Whole file) speeds it up\nand handles corrupted files better than cp\nWhen the connection fails i just umount and delete the state file - so the\ncache is not lost - and redo the autentication part, mount and than restart\nrsync.\nTo bad i cant wrap my head around ocaml, but it looks like i can only code\nin languages that have curly braces :/\n\nReply to this email directly or view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/48#issuecomment-34549818\n.\n. I did another test with similar results, but I got another message repeated several times:\n\nWarning: Unexpected leaf: name=messsage data_type=Scalar in GapiService.RequestError.parse\n. No, I copied it by hand and mispelled it.\n. ",
    "piccaso": "I guess this is related...\nI wanted to make a backup of my files stored in gdrive - so its the opposite direction.\nFirst i tried tar but tar complaint about files changed while beeing read.\nSo i tried rsync but it did not copy a single file in 5 minutes.\nThen i used plain old cp, which worked in the beginning.\nLater fuse complaint about a disconnected endpoint and did not stop after remounting.\nSo i turned on the log -verbose and saw some authentication errors\nafter that i deleted the $HOME/.gdfuse/ folder and started started from scratch.\n(did not keep the log file, but i will get back to this point again later)\ni mounted the folder with:\ngoogle-drive-ocamlfuse -m -cc -verbose -label  \nand started cp with:\ncp -avr  \nafter 584 files where copied i got 'Transport endpoint is not connected', here is a tail from the log:\nThis is probably related so i'll post it here...\n[1420.815159] TID=9735: read <somefilename> buf 655360 0\n[Getting metadata from context...valid\nLoading resource <somefilename> (trashed=false) from db...found\n1420.815371] TID=9736: read <somefilename> buf 786432 0\nGetting metadata from context...valid\nLoading resource <somefilename> (trashed=false) from db...found\nThread id=9734: Error: cannot close db\n[1425.810249] TID=9734: Exiting.\nCURL cleanup...done\nClearing context...done\nremounted with:\ngoogle-drive-ocamlfuse -verbose -label  \nand started copy again:\ncp -avru  \na copuple of minutes later 'endpoint is not connected' again and log tail looks like this:\nRemoving file (/root/.gdfuse/<label>/cache/0B9ulBR3Y_MjrUTd2Zs0hqXA2RVE: resource 584) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/0B9ulBR3Y_MjrM2xLadjRG012VkU: resource 585) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/0B9ulBR3Y_MjrRHhodVdffUtMQ1U: resource 586) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/0B0O_abtEZsTVVnhOWXRjYsFFSGs: resource 587) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/0B0O_abtEZsTVSE4wWsstS0prWHc: resource 588) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/0B0O_abtEZsTVM3Nwbk5xxWlQc28: resource 589) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/0B0O_abtEZsTVa3A3aUZLRaryYWs: resource 590) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/0B2weBPlqIflIVUxYMzlKRVgasdE: resource 630) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/0AtulBR3Y_MjrdEo3zZVQTM1OGadxUmVNQ2w1RFFwREE: resource 633) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/0AsEO6-ojjER0dFhP0cHBqSa1aahESUdXQTluX3VOeEE: resource 634) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/0AsEO6-ojjER0dDFQvdkVCNaaajhCRjBNNEljdm1CdFE: resource 635) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/0AtulBR3Y_MjrdHNwW1remhKabzVlTmhUOXZlbEc0RFE: resource 636) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/0AsEO6-ojjER0dE0jVsV1pXbXaka3azJLc1A5SFhwdlE: resource 637) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/1Xgp-iLqZX4Lg4dd8sx_xNI1uo2pBEUSMGSehq7AClDc: resource 638) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/1FPjpObOz6Z31-tAkdVmkfS6EaC5xrdpJh8pJ8vBoT7o: resource 640) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/1fLhnb1gIRGkLFWXfydEgMl9DDk3v4OZ1km86NEbflNo: resource 641) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/0B9ulBR3Y_MjrRUd1QldEY2dFMDA: resource 644) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/0ArlfH8xmiYlddHFWSXZdbFpOV1dVUkVOYnBlVnNvNnc: resource 645) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/0ArlfH8xmiYlddFpicGNndVVPRjlOcXZfVzVQUUFkR3c: resource 646) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/1hwKFodpHIDntUWKCd3kcqdCV5uRninsegyIStHSdv6o: resource 649) from cache...done\nRemoving file (/root/.gdfuse/<label>/cache/0B9ulBR3Y_MjrLWVIUWgyNnFqNlk: resource 672) from cache...done\nRefreshing access token...fail (error_code=Exception)\nError refreshing access token (try=0):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\nfail (error_code=Exception)\nError refreshing access token (try=1):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\nfail (error_code=Exception)\nError refreshing access token (try=2):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\nfail (error_code=Exception)\nError refreshing access token (try=3):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\nfail (error_code=Exception)\nError refreshing access token (try=4):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\nfail (error_code=Exception)\nError refreshing access token (try=5):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\n[373.701048] TID=0: Exiting.\nCURL cleanup...done\nClearing context...done\nafter remounting i get a similar output\neven with -cc\ni also noticed some files where corupted every time i found this in the logs:\nread <some-filename>not valid\ni'm using google-drive-ocamlfuse version 0.5.3 on Ubuntu 13.10 'headless'\nso... \n- Is there a better way to recover the mountability in that state?\n- If i delete the  folder i need to authenticate again, and that cant be automated (right?)\n- What would you recomend for copying large amount of data of the drive, are there alternatives to cp which could work better?\n- could you please make a parsable log entry for possibly corupted files.\n- any chance getting more detail out of 'Error: cannot close db'.\n- and of course, any idea how to fix this?\nThanks!\nFlo\n. setting 'sqlite3_busy_timeout=5000' helped, but when it was time to refresh the access token things went wrong again.\nRemoving file (/root/.gdfuse/laserbox/cache/0B9ulBR3Y_MjrY085N3dqc29aMGs: resource 1752) from cache...done\nRefreshing access token...fail (error_code=Exception)\nError refreshing access token (try=0):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\nfail (error_code=Exception)\nError refreshing access token (try=1):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\nfail (error_code=Exception)\nError refreshing access token (try=2):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\nfail (error_code=Exception)\nError refreshing access token (try=3):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\nfail (error_code=Exception)\nError refreshing access token (try=4):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\nfail (error_code=Exception)\nError refreshing access token (try=5):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\n[3120.002309] TID=22027: Exiting.\nCURL cleanup...done\nClearing context...done\nmy test system is a pretty old computer and is under full load doing this, maybe this is some kind of race condition... i'll try again without mounting -m\nCheers\nFlo\n. Thanks for your response @astrada!\nI don't think its related to the Feb 6 issue, i still have the same problems (even without -m)\nHere is my workaround for now:\nLarge timeouts and cache - just clear cache on demand.\nIn my case no problem, because the content does not change atm.\nmetadata_cache_time=6000\nsqlite3_busy_timeout=50000\nmax_cache_size_mb=5120\nusing rsync with -W (no delta operations - copy Whole file) speeds it up and handles corrupted files better than cp\nWhen the connection fails i just umount and delete the state file - so the cache is not lost - and redo the autentication part, mount and than restart rsync.\nTo bad i cant wrap my head around ocaml, but it looks like i can only code in languages that have curly braces :/\n. ",
    "miriampl": "Hi I have same problem with upload\nNetwork speed 100/100 MB\nUpload a iso file 400MB in speed of 134 kB/s\nI have changed metadata_cache_time - this has no effect.\ndon't know what the... by such speed it's not usable. In upload by browser i is 100MB for that folder with the same file\n. ",
    "bean5": "I haven't tried google-drive-ocamlfuse, but I follow the project. Have you considered pausing the thread's process itself instead of turning it on and off (at least when you are opening the folder to peruse your documents)?\n. ",
    "arifmahmudrana": "@astrada but what I always want to mount it on start up like Dropbox linux client, it also will show the error I guess.\nI have a google account that I mounted & that is not empty after syncing now I added google-drive-ocamlfuse ~/gdrive in my start up application but it doesn't mount & when running google-drive-ocamlfuse ~/gdrive it prints this error.\nNow if I run command as you mentioned google-drive-ocamlfuse -o ~/gdrive it will hide the files already synced & again download the files I guess.. @astrada so do I have to again delete the folder then again using access token again sync all the file\nAlso can you explain the workflow or how it works. ",
    "apxen": "I am running google-drive-ocamlfuse version 0.5.3\n. ",
    "geoffwhittington": "yes definitely. Let me see what I can do\nEdit: I have had trouble compiling from source due to time and some ignorance. Thanks for jumping in @vixxtor \n. ",
    "vixxtor": "I am getting exactly the same error (even with the latest version from source).\nIf you want, we can work to find the cause - I can help you with any logs you might need.\nRunning Ubuntu 13.04.\n. This also crashes on Ubuntu 13.10.\n. victor@victor-E6520:~$ ldd /home/victor/.opam/system/bin/google-drive-ocamlfuse\n    linux-gate.so.1 =>  (0xb7714000)\n    libsqlite3.so.0 => /usr/lib/i386-linux-gnu/libsqlite3.so.0 (0xb7643000)\n    libz.so.1 => /lib/i386-linux-gnu/libz.so.1 (0xb762a000)\n    libcurl-gnutls.so.4 => /usr/lib/i386-linux-gnu/libcurl-gnutls.so.4 (0xb75ce000)\n    librt.so.1 => /lib/i386-linux-gnu/librt.so.1 (0xb75c5000)\n    libpthread.so.0 => /lib/i386-linux-gnu/libpthread.so.0 (0xb75aa000)\n    libfuse.so.2 => /lib/i386-linux-gnu/libfuse.so.2 (0xb7576000)\n    libm.so.6 => /lib/i386-linux-gnu/libm.so.6 (0xb7533000)\n    libdl.so.2 => /lib/i386-linux-gnu/libdl.so.2 (0xb752d000)\n    libc.so.6 => /lib/i386-linux-gnu/libc.so.6 (0xb7379000)\n    libidn.so.11 => /usr/lib/i386-linux-gnu/libidn.so.11 (0xb7346000)\n    librtmp.so.0 => /usr/lib/i386-linux-gnu/librtmp.so.0 (0xb732b000)\n    libgcrypt.so.11 => /lib/i386-linux-gnu/libgcrypt.so.11 (0xb72a7000)\n    libgnutls.so.26 => /usr/lib/i386-linux-gnu/libgnutls.so.26 (0xb71e1000)\n    libgssapi_krb5.so.2 => /usr/lib/i386-linux-gnu/libgssapi_krb5.so.2 (0xb71a4000)\n    liblber-2.4.so.2 => /usr/lib/i386-linux-gnu/liblber-2.4.so.2 (0xb7195000)\n    libldap_r-2.4.so.2 => /usr/lib/i386-linux-gnu/libldap_r-2.4.so.2 (0xb7144000)\n    /lib/ld-linux.so.2 (0xb7715000)\n    libgpg-error.so.0 => /lib/i386-linux-gnu/libgpg-error.so.0 (0xb713f000)\n    libtasn1.so.3 => /usr/lib/i386-linux-gnu/libtasn1.so.3 (0xb712c000)\n    libp11-kit.so.0 => /usr/lib/i386-linux-gnu/libp11-kit.so.0 (0xb710d000)\n    libkrb5.so.3 => /usr/lib/i386-linux-gnu/libkrb5.so.3 (0xb703f000)\n    libk5crypto.so.3 => /usr/lib/i386-linux-gnu/libk5crypto.so.3 (0xb7017000)\n    libcom_err.so.2 => /lib/i386-linux-gnu/libcom_err.so.2 (0xb7012000)\n    libkrb5support.so.0 => /usr/lib/i386-linux-gnu/libkrb5support.so.0 (0xb7008000)\n    libresolv.so.2 => /lib/i386-linux-gnu/libresolv.so.2 (0xb6ff1000)\n    libsasl2.so.2 => /usr/lib/i386-linux-gnu/libsasl2.so.2 (0xb6fd6000)\n    libgssapi.so.3 => /usr/lib/i386-linux-gnu/libgssapi.so.3 (0xb6f9a000)\n    libkeyutils.so.1 => /lib/i386-linux-gnu/libkeyutils.so.1 (0xb6f96000)\n    libheimntlm.so.0 => /usr/lib/i386-linux-gnu/libheimntlm.so.0 (0xb6f8d000)\n    libkrb5.so.26 => /usr/lib/i386-linux-gnu/libkrb5.so.26 (0xb6f0c000)\n    libasn1.so.8 => /usr/lib/i386-linux-gnu/libasn1.so.8 (0xb6e6c000)\n    libhcrypto.so.4 => /usr/lib/i386-linux-gnu/libhcrypto.so.4 (0xb6e38000)\n    libroken.so.18 => /usr/lib/i386-linux-gnu/libroken.so.18 (0xb6e23000)\n    libwind.so.0 => /usr/lib/i386-linux-gnu/libwind.so.0 (0xb6df9000)\n    libheimbase.so.1 => /usr/lib/i386-linux-gnu/libheimbase.so.1 (0xb6dea000)\n    libhx509.so.5 => /usr/lib/i386-linux-gnu/libhx509.so.5 (0xb6da5000)\n    libcrypt.so.1 => /lib/i386-linux-gnu/libcrypt.so.1 (0xb6d73000)\n. victor@victor-E6520:~$ google-drive-ocamlfuse -debug -d \nStarting application setup (label=default).\nOpening log file: /home/victor/.gdfuse/default/gdfuse.log\n[4253:4253:0418/112127:ERROR:nss_util.cc(853)] After loading Root Certs, loaded==false: NSS error code: -8018\nSegmentation fault\nvictor@victor-E6520:~$ cat .gdfuse/default/gdfuse.log \n[0.000247] TID=0: Setting up default filesystem...\n[0.000283] TID=0: Loading configuration from /home/victor/.gdfuse/default/config...not found.\nSaving configuration in /home/victor/.gdfuse/default/config...done\nSaving configuration in /home/victor/.gdfuse/default/config...done\n[0.000427] TID=0: Loading application state from /home/victor/.gdfuse/default/state...not found.\nSaving application state in /home/victor/.gdfuse/default/state...done\nCurrent version: 0.5.3\nSetting up cache db...done\nSetting up CURL...done\nSaving application state in /home/victor/.gdfuse/default/state...done\nStarting web browser with command: xdg-open \"https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=Mh1coybPOTovZEtqK-u%2F68InB1LDi5HaAGBmyMcJ0iQ\"...done\nvictor@victor-E6520:~$ \n. victor@victor-E6520:~$ curl -v https://accounts.google.com/\n- About to connect() to accounts.google.com port 443 (#0)\n-   Trying 173.194.70.84...\n- Connected to accounts.google.com (173.194.70.84) port 443 (#0)\n- successfully set certificate verify locations:\n-   CAfile: none\n  CApath: /etc/ssl/certs\n- SSLv3, TLS handshake, Client hello (1):\n- SSLv3, TLS handshake, Server hello (2):\n- SSLv3, TLS handshake, CERT (11):\n- SSLv3, TLS handshake, Server key exchange (12):\n- SSLv3, TLS handshake, Server finished (14):\n- SSLv3, TLS handshake, Client key exchange (16):\n- SSLv3, TLS change cipher, Client hello (1):\n- SSLv3, TLS handshake, Finished (20):\n- SSLv3, TLS change cipher, Client hello (1):\n- SSLv3, TLS handshake, Finished (20):\n- SSL connection using ECDHE-RSA-AES128-SHA\n- Server certificate:\n-    subject: C=US; ST=California; L=Mountain View; O=Google Inc; CN=accounts.google.com\n-    start date: 2014-04-09 12:02:27 GMT\n-    expire date: 2014-07-08 00:00:00 GMT\n-    subjectAltName: accounts.google.com matched\n-    issuer: C=US; O=Google Inc; CN=Google Internet Authority G2\n-    SSL certificate verify ok.\n\nGET / HTTP/1.1\nUser-Agent: curl/7.29.0\nHost: accounts.google.com\nAccept: /\n< HTTP/1.1 302 Moved Temporarily\n  < Content-Type: text/html; charset=UTF-8\n  < Strict-Transport-Security: max-age=10893354; includeSubDomains\n  < X-Frame-Options: DENY\n  < Location: https://accounts.google.com/ManageAccount\n  < Content-Length: 223\n  < Date: Sat, 19 Apr 2014 20:22:06 GMT\n  < Expires: Sat, 19 Apr 2014 20:22:06 GMT\n  < Cache-Control: private, max-age=0\n  < X-Content-Type-Options: nosniff\n  < X-XSS-Protection: 1; mode=block\n  < Server: GSE\n  < Alternate-Protocol: 443:quic\n  < \n  \n\nMoved Temporarily\n\n\nMoved Temporarily\n  The document has moved here.\n  \n\n- Connection #0 to host accounts.google.com left intact\n. victor@victor-E6520:~$ dpkg --get-selections | grep curl\ncurl                        install\nlibcurl3:i386                   install\nlibcurl3-gnutls:i386                install\nlibcurl4-gnutls-dev             install\npython-pycurl                   install\n. By the way, if it helps you, and you cannot reproduce it on your system. I could install a virtual machine on which the problem occurs, then give you ssh access to it in order to troubleshoot the code.\n. I have a virtual machine reproducing the issue. I can give you access to it for testing. I wrote you a personal e-mail in order to establish the troubleshooting session.\n. \n",
    "lesebas": "Hello,\nI'm on a 64bits system with Archlinux and I encounter exactly the same problem. I've tried to clear the cache, clearind all directory ~/.gdfuse and reinstall the packqge but I've always the same error :+1: \n![code][sebastien@Lupus ~]$ google-drive-ocamlfuse -debug /home/sebastien/Documents/Google\\ Drive/\nStarting application setup (label=default).\nOpening log file: /home/sebastien/.gdfuse/default/gdfuse.log\nErreur de segmentation (core dumped)\n[/code]\n. Hello,\nAny news about the problem... I'm still stuck in \"segmentation fault\" on my both machine.\nHere are the installed curl package :\n[sebastien@Lupus ~]$ pacman -Qs curl\nlocal/curl 7.36.0-1\n    An URL retrieval utility and library\nlocal/ocaml-curl 0.5.3-3\n    OCaml bindings to libcurl networking library\nlocal/python2-pycurl 7.19.3.1-2\n    A Python 2.x interface to libcurl\n[sebastien@Lupus ~]$ \n. Hello,\nI still have the segmentation fault issue with the following command :\ngoogle-drive-ocamlfuse -debug /home/sebastien/google_drive/\nor even\ngoogle-drive-ocamlfuse  /home/sebastien/google_drive/\nBy the way if I use the -d option it works : \ngoogle-drive-ocamlfuse -d /home/sebastien/google_drive/\nAny idea?\nPS : I have the problem on my desktop and my laptop both on archlinux 64 bits\n. I'm on archlinux, so the package is compiled with the PKGBUILD on AUR.\n. Does it work if you use the option -d ?\n. I've try to install it with opam... but it's really recommended with archlinux as you bypass pacman... so I forgave it.\n. How can I get the core dump ?\n. I did not not manage to get the core dump with systemd (I will try later). By the way, I manage to reproduce the bug into gdb (after compiling the package with !strip and debug option) : \n(gdb) run -debug /home/sebastien/Documents/Google\\ Drive/\nStarting program: /usr/bin/google-drive-ocamlfuse -debug /home/sebastien/Documents/Google\\ Drive/\nStarting application setup (label=default).\nOpening log file: /home/sebastien/.gdfuse/default/gdfuse.log\n[New Thread 0x7ffff4d49700 (LWP 4361)]\n[Thread 0x7ffff4d49700 (LWP 4361) exited]\nProgram received signal SIGSEGV, Segmentation fault.\n0x00000000005b39f1 in caml_absf_mask ()\n(gbd)\nBy the way if I launch the deamon in gdb without -debug option : no problem!\nI will send you the trace.log by e-mail.\n. Ok it works fine with the version 0.5.4 and the option curl_debug_off set to true in ~/.gdfuse/default/config.\nMany thanks for your support!\n. ",
    "elucarelli": "I complied it using OPAM but same issue. I'm on Ubuntu 12.04 32 bit\n. HI have same issue on Ubuntu 12.04 LTS 32bit.\nI installed it using opam.\n. ",
    "root101jp": "I have exactly same issue of vixxtor on Ubuntu12.04(32bit) and have same debug error message.\nDo you have any update about this issue ?\n. ",
    "matofesi": "Ok. I've checked curl.log - there are no errors there.\nAnd then I uploaded some 700 MB of data without single error... I'll watch it some more and try to catch it again ;)\n. ",
    "paha77": "You were right.  Drive API was not turned ON.  Thank you for your great\nsupport.\nOn Fri, Mar 21, 2014 at 10:18 AM, Alessandro Strada \nnotifications@github.com wrote:\n\nIt looks like your user does not have permission to access some of the\nresources. You should check curl.log file and search for 403 errors.\n\nReply to this email directly or view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/56#issuecomment-38259317\n.\n. Thanks again.\n. \n",
    "Mensen": "google-drive-ocamlfuse, version 0.5.3\nCopyright (C) 2012-2013 Alessandro Strada\nLicense MIT\n. Thanks for the quick reply... I don't use google docs though.\nThe files are just 'regular' random files, usually of very small size in the order of just a few kb... however the most recent copying of one whole directory of 1.8 Mb was timed at a fairly consistent 1.1 kb/s\n. ",
    "Vladx71": "Hi,\nThanks for your work, because at this moment this is the only way to access google drive from Linux (above the crappy web interface). However it is very very slow... not just the folder browsing. but opening any file is extremely slow. Currently I'm using the following options to mount my drives\n/root/.opam/system/bin/google-drive-ocamlfuse -m -o big_writes -label $1 $*\nis there any further fine-tune option to speed-up access to files stored on gdrive or this is cannot be faster due to google/technology?\nthank you\nL:\n. it seems it is very very slow when I open directory first, when I open\ndirectory second time it is fast, so it is looks like related to caching.\nI've increased metadata_cache_time to 600 but does not help a lot. for\nexample opening a directory with a ~30 files takes several minutes.\nL:\nOn 2 July 2014 19:25, Alessandro Strada notifications@github.com wrote:\n\nIs it slow just the first time you open a file or even when you open the\nsame file more than once?\nDid you try putting a big value in metadata_cache_time?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/58#issuecomment-47808318\n.\n. Not at all. I have just a few documents like this and navigation is very\nslow in directories where I have no google docs at all\n\nL:\nOn 3 July 2014 08:48, Alessandro Strada notifications@github.com wrote:\n\nDo you have many Google Docs (office-like files)? Those files can't be\nefficiently cached, so they slow down a lot the resource fetching. If you\ndon't need them on your desktop, you can turn them completely off, putting\ndownload_docs=false in the config file. Otherwise, you can turn your docs\ninto links, putting:\ndocument_format=desktop\npresentation_format=desktop\nspreadsheet_format=desktop\ndrawing_format=desktop\nform_format=desktop\nin the config file, and then using -cc command line option to clear the\ncache.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/58#issuecomment-47872443\n.\n. \n",
    "jcward": "I have the same problem with very slow access, from ls to nautilus, because I do have a lot of Google Docs in my drive, and this did work for me:\n\nyou can turn your docs into links...\n\ndocument_format=desktop\npresentation_format=desktop\nspreadsheet_format=desktop\ndrawing_format=desktop\nform_format=desktop\n\nThen google-drive-ocamlfuse -cc to clear cache.\n\nThanks, @astrada!\nOne question: While links are a good workaround, is it possible to allow fast directory listing (e.g. ls and find), and only perform the slow document fetch/conversion later when the file is opened? Maybe report a 1-byte file (or some other special flag, like a symlink or named pipe) until the file is actually opened?\n. Ok, thanks! The links feature is nice and will work for now.\n. ",
    "aplund": "I'm not sure if this is the right place for this comment, but it seems that doing lots of metadata requests (e.g. lstat()) is also extremely slow if the metadata cache expiry has occurred, say when you start up at the beginning of the day.  From my connection, one lstat() request takes anywhere from 500ms-1000ms. If you are getting the sizes of 1000 files, there's 10-20minutes in just checking metadata.\nThis is particularly a problem when doing an rsync update.  Is there any way to get some kind of bulk metadata update out of the google drive api, or perhaps a more efficient way of asking rsync to check file sizes?\n. ",
    "Torsten85": "@astrada couldn't you fetch the whole google drive (drive.files.list should do this without many requests and also delivers all the stats needed) and instead of flushing the cache after metadata_cache_time simply poll drive.changes.get and flush only the parts of the cache that really have changes? This would dramatically speed navigation up. . @astrada ... and a call to drive.changes.get happens every metadata_cache_time seconds right? \nCurrently the first attempt to navigation after mounting is painfully slow but every following navigation attempt is quite fast... but after some time (maybe an hour or so) navigation is slow again.. @scoopydude2002 Did you do some special settings except \"allow_other\" and \"stream large files\"? What is your transfer speed? When I try to copy one single file from the mount to the local hard drive I only get 3-5 MB/s which is too low for high bitrate movies.. thanks @scoopydude2002, that helps a lot. I'll try your config tonight.\nI see your using unionfs for local storage. do you have some scripts (rclone?) in place to automatically upload your local folder to google drive? \nsorry if this gets a little bit offtopic. . @scoopydude2002 thats a great info. May I ask how big your media library is?. @scoopydude2002 Thanks. One question remains concerning your upload script: Why do you copy and move? Shouldn't just moving be enough? . @scoopydude2002 ah ok. Thanks for your support! My setup is now pretty much like yours and its working great most of the time. But apporx. once a day, google-drive-ocamlfuse suddenly stops. The logs (curl.log & gdfuse.log) are both empty... did you experience the same? . @scoopydude2002 you could simply adjust your remount script to do something like this bevor remounting:\nbash\ncat curl.log >> curl.combined.log\ncat gdfuse.log >> gdfuse.combined.log\nthat would keep all logs concatenated in one file... or am I missing something here?. @scoopydude2002 would you share your cronjob script that checks if the mount still exists and remounts it if necessary? Would help a lot.. ",
    "OndraZizka": "Where is the config file? --> ~/.gdfuse/default/config\n. Hi, here's my take on this: to my mountGdrive script, I added tree /gdrive/ & which caches all the files in the background within around 5 minutes. I've increased the cache size to 2048 MB (it has to be a geeky number indeed) and the timeout to 3600. That's no big deal, I hav a plenty of space and the caching seems to work fine; besides, I only work with non-doc files over fuse and the docs don't really change often.. ",
    "thmxv": "Just to summarize my experience. We use a TeamDrive to collaborate on an 'medium sized' asset collection (at the moment 150.000 files totaling 100GB, but destined to grow a lot). We have some custom tools that needs to go over the whole asset repository and get a listing of every directory (using a in house optimized version of python's os.walk() function).\n- Getting he first listing is painfully slow, even if not having any Google docs\n- Using ls -lR or tree to build a warm cache helps a lot\n- Even with a warm cache, listing the whole drive is still 10x slower than on windows running the same python code on a  FileStream mount.. ",
    "FabioPedretti": "\nDo you have many Google Docs (office-like files)? Those files can't be efficiently cached, so they slow down a lot the resource fetching. If you don't need them on your desktop, you can turn them completely off, putting download_docs=false in the config file. Otherwise, you can turn your docs into links, putting:\ndocument_format=desktop\npresentation_format=desktop\nspreadsheet_format=desktop\ndrawing_format=desktop\nform_format=desktop\nin the config file, and then using -cc command line option to clear the cache.\n\nCould you think about making this the default? Or at least have a command line option to force this for all formats?\nThanks.. ",
    "hotice": "Thank you! Everything seems to be working.\n. Symlinks to either files or folders end up as 0 byte binaries in my Google Drive. Am I supposed to configure something regarding this?. Oh so the actual file the link points to is never uploaded. Got it, thanks! Though that's a bit weird since pretty much everywhere \"symlink support\" usually means it uploads the actual contents, so you may want to clarify that in the description so you don't get useless comments/bug reports like mine :). ",
    "bkCDL": "PPA seems to be hanging today. When I enter the command: \"sudo add-apt-repository ppa:alessandro-strada/ppa\", my console just sits there. I think it's been over 1/2 hour now. \n. ",
    "ovel2clock": "@astrada That was it.  Thanks for the help!\nBTW, I did the headless install and it didn't update client_id and client_secret within the config file.  Not sure if it's intended to be like that or if it's a bug.  \nRegardless, awesome utility!\n. ",
    "markginter31": "Ubuntu 14.04 32bit (using repository) - in Nautilus I get the Transport endpoint not connected.  I have all info filled in in the config file.  Had to do a headless install b/c it kept segfaulting.  Finished setup via headless - but now it segfaults when it gets to the refreshing metadata point.\n[0.000311] TID=0: Setting up iu12gdrive filesystem...\n[0.000357] TID=0: Loading configuration from /home/mark/.gdfuse/iu12gdrive/config...done\nSaving configuration in /home/mark/.gdfuse/iu12gdrive/config...done\n[0.000668] TID=0: Loading application state from /home/mark/.gdfuse/iu12gdrive/state...done\nCurrent version: 0.5.3\nSetting up cache db...done\nSetting up CURL...done\nRefresh token already present.\n[0.003806] TID=0: Starting filesystem iu12gdrive\n[0.014587] TID=0: init_filesystem\n[0.014733] TID=0: getattr /.Trash\nLoading metadata from db...not found\nRefreshing metadata...\nand then segfault in the terminal -- shows it's connected in nautilus but gives transport error when I click on the folder.\n. ",
    "JoseFMP": "I do not see why it's duplicated of #106.\nIn any case, it's still happening. Ubuntu 18.04, google-drive-ocamlfuse, version 0.7.0\nAny workarounds? It's a pity as this was the only decent solution to sync with google drive from ubuntu.. ",
    "philcolbourn": "I got past this error on file 760 by\ntouch ~/gdrive/autorun.inf\nThis resulted in this log:\nunique: 760, opcode: LOOKUP (1), nodeid: 1, insize: 52, pid: 30300\nLOOKUP /autorun.inf\ngetattr /autorun.inf\n   unique: 760, error: -2 (No such file or directory), outsize: 16\nunique: 761, opcode: LOOKUP (1), nodeid: 1, insize: 52, pid: 30303\nLOOKUP /autorun.inf\ngetattr /autorun.inf\n   unique: 761, error: -2 (No such file or directory), outsize: 16\nunique: 762, opcode: CREATE (35), nodeid: 1, insize: 68, pid: 30303\n   unique: 762, error: -38 (Function not implemented), outsize: 16\nunique: 763, opcode: MKNOD (8), nodeid: 1, insize: 68, pid: 30303\nmknod /autorun.inf 0100664 0x0 umask=0002\ngetattr /autorun.inf\n   NODEID: 525\n   unique: 763, success, outsize: 144\nunique: 764, opcode: OPEN (14), nodeid: 525, insize: 48, pid: 30303\nopen flags: 0x8801 /autorun.inf\n   open[0] flags: 0x8801 /autorun.inf\n   unique: 764, success, outsize: 32\nunique: 765, opcode: FLUSH (25), nodeid: 525, insize: 64, pid: 30303\nflush[0]\n   unique: 765, success, outsize: 16\nunique: 766, opcode: SETATTR (4), nodeid: 525, insize: 128, pid: 30303\nutime /autorun.inf 1397131769 1397131769\ngetattr /autorun.inf\n   unique: 766, success, outsize: 120\nunique: 767, opcode: FLUSH (25), nodeid: 525, insize: 64, pid: 30303\nflush[0]\n   unique: 767, success, outsize: 16\n. Thanks!\nI stopped using -m.\nI had issues with 2 files: autorun.inf and .xdg-volume-info. I had to run\ngoogle-drive-ocamlfuse (GDC), then touch these files in my local gdrive. I\ntried all sorts of things like deleting them so I'm not sure what actually\nworked. But GDC does not complain anymore.\nMy file manager (nautilus?) seems to be much less responsive now.\nWhat should happen when I drop my network connection?\nThanks again for your reply.\nNice work,\nPhil\nOn Fri, Apr 11, 2014 at 12:16 AM, Alessandro Strada \nnotifications@github.com wrote:\n\nYou should look at the log files in ./gdfuse/default. There are 2 log\nfiles: gdfuse.log that traces filesystem activity, and curl.log that\ntraces network activity. The output you get from the -d flag is the low\nlevel fuse log. Moreover, I would avoid the -m flag, because in my tests,\nit didn't work very well.\n\nReply to this email directly or view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/61#issuecomment-40086541\n.\n. I see that symbolic links are not supported.\n\nPhil\nOn Sat, Apr 12, 2014 at 5:34 PM, phil colbourn philcolbourn@gmail.comwrote:\n\nThanks!\nI stopped using -m.\nI had issues with 2 files: autorun.inf and .xdg-volume-info. I had to run\ngoogle-drive-ocamlfuse (GDC), then touch these files in my local gdrive. I\ntried all sorts of things like deleting them so I'm not sure what actually\nworked. But GDC does not complain anymore.\nMy file manager (nautilus?) seems to be much less responsive now.\nWhat should happen when I drop my network connection?\nThanks again for your reply.\nNice work,\nPhil\nOn Fri, Apr 11, 2014 at 12:16 AM, Alessandro Strada \nnotifications@github.com wrote:\n\nYou should look at the log files in ./gdfuse/default. There are 2 log\nfiles: gdfuse.log that traces filesystem activity, and curl.log that\ntraces network activity. The output you get from the -d flag is the low\nlevel fuse log. Moreover, I would avoid the -m flag, because in my\ntests, it didn't work very well.\n\nReply to this email directly or view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/61#issuecomment-40086541\n.\n. \n\n",
    "yjqz1986": "Friend I need your help!! When I installed google-drive-ocamlfuse, I saw the following error: su: google-drive-ocamlfuse: no se encontro la orden. I have installed de ocamlfuse\n. ",
    "luisca": "Alessandro, looks like that was not the source of the problem. I changed metadata_cache_time and rerun rsync without the '-c', and it started re-uploading already uploaded files. With -c it correctly skips the files that were already downloaded.\nI saw that curl.log includes the md5 of the files, but I am assuming that using -c in rsync actually has to download the whole file anyway.\nUsing sqlite (the dir here is .gdfuse, not .gdrive) the modified time looks fine, and the file size matches the file size on local disk. Any idea on how to debug further?\n. yjqz1986, that has nothing to do with this issue, and it is not even a bug of this program, so please look for help where appropriate. You don't need to run the program with \"su\", and that is just a problem with your PATH configuration, you can probably get help in a generic Unix/Linux help forum.\n. Alessandro, it looks like my problems were actually self-inflicted.\nI just realized the Unix mtimes are kept locally in the cache. After rsync failed a few times, I started always mounting the Google drive with the -cc option, which was clearing the cache, causing a timestamp mismatch and therefore rsync to re-upload everything after each re-mount.\nCan I expect the cache-derived mtimes to be stable, or will they at some point get resetted to whatever Google says they are? Another option for me is to use the rsync option --size-only, which ignores the timestamps and seems to work well, at the cost that if a file is modified but keeps the same size, it will not be updated.\n. So it is not necessary to refresh the cache at all? If I clear the cache,\nit does not keep the mtimes I set, but the actual time when I uploaded the\nfile.\nOn Thu, Apr 24, 2014 at 12:35 AM, Alessandro Strada \nnotifications@github.com wrote:\n\nThe mtimes will be stable al least until the cache is invalidated, i.e.\nafter metadata_cache_time expires. If you put there a very big number you\nshould not have problems. Then the mtimes will be updated with the values\nreturned by Google Drive. In theory, it should keep your values, but I'm\nnot 100% sure.\n\nReply to this email directly or view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/63#issuecomment-41251582\n.\n\n\nLuis Carlos Cobo Rus       GnuPG ID: 44019B60\n. No luck, same behavior with nice -n 19 on both commands.\nOn Sat, May 24, 2014 at 1:30 AM, Alessandro Strada <notifications@github.com\n\nwrote:\nI think the problem may be the single threaded nature of the FUSE binding\nI'm using. Unfortunately the multithreaded option (-m) is unstable\n(usually causes a segmentation fault). Perhaps you can try to use nice to\nlower the priority of the process (try to use nice with both\ngoogle-drive-ocamlfuse and rsync).\n\nReply to this email directly or view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/69#issuecomment-44081314\n.\n\n\nLuis Carlos Cobo Rus       GnuPG ID: 44019B60\n. No luck, same problems.\nOn Sun, May 25, 2014 at 11:49 PM, Alessandro Strada \nnotifications@github.com wrote:\n\nThen, I think you can try to see if the -m option works for you.\n\nReply to this email directly or view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/69#issuecomment-44161715\n.\n\n\nLuis Carlos Cobo Rus       GnuPG ID: 44019B60\n. Having the same issues (crashing the system forcing the root filesystem going to read-only mode) with 400MB files.\n. No, I have plenty of free space. However apparently there is some bug on\nthe SSD firmware of my computer (Lenovo X230) and I wonder if it may be\nrelated. Looks like a firmware update is on the way, so will update if that\nwas it.\nOn Fri, Jun 27, 2014 at 10:27 PM, SirSid notifications@github.com wrote:\n\nI've noticed my root partition gets dangerously close to filling up when I\ncopy large files. I assume the file lives in the cache (which is in my\nroot) until the upload is complete. Could your root partition getting full\ncause this problem?\n\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/69#issuecomment-47419029\n.\n\n\nLuis Carlos Cobo Rus       GnuPG ID: 44019B60\n. After upgrading to trusty and upgrading HD firmware (there was some issue with it under heavy load), this particular problem is gone, so closing the issue. I still have trouble with large files, but will set a separate issue.\n. google-drive-ocamlfuse -cc $MOUNT_DIRECTORY\nrsync -rlP --delete --exclude $EXCLUDED_DIR  --size-only --inplace /media/luisca/ElaineBackup/luisca $MOUNT_DIRECTORY\n. Thanks for taking a look at this!! I will try your command sequence and my\nuse case again and let you know if the problem is gone or may be something\nelse. Thanks!\nOn Mon, Sep 15, 2014 at 1:51 PM, Alessandro Strada <notifications@github.com\n\nwrote:\nSorry, but I wasn't able to reproduce your issue on Ubuntu 14.04 64-bit.\n$ mkdir utf_test\n$ cd utf_test/\n$ echo \"test A\" > \"D\u00f3nde est\u00e1n las llaves.mp3\"\n$ ls -l\ntotal 4\n-rw-rw-r-- 1 alex alex 7 Sep 15 22:37 D\u00f3nde est\u00e1n las llaves.mp3\n$ cd ..\n$ google-drive-ocamlfuse -cc -debug ~/tmp/gdrive\n$ rsync -rlP --delete --size-only --inplace utf_test ~/tmp/gdrive/\nsending incremental file list\nutf_test/D\u00f3nde est\u00e1n las llaves.mp3\n          7 100%    0.00kB/s    0:00:00 (xfr#1, to-chk=0/2)\n$ cd ~/tmp/gdrive\n$ ls -lR utf_test\nutf_test:\ntotal 1\n-rw-rw-r-- 1 alex alex 7 Sep 15 22:37 D\u00f3nde est\u00e1n las llaves.mp3\n$ cd ..\n$ fusermount -u ~/tmp/gdrive\n$ google-drive-ocamlfuse -cc -debug ~/tmp/gdrive\n$ cd gdrive\n$ ls -l utf_test\ntotal 1\n-rw-rw-r-- 1 alex alex 7 Sep 15 22:37 D\u00f3nde est\u00e1n las llaves.mp3\n$ cd /tmp/\n$ fusermount -u ~/tmp/gdrive\n$ google-drive-ocamlfuse -cc -debug ~/tmp/gdrive\n$ rsync -rlP --delete --size-only --inplace utf_test ~/tmp/gdrive/\nsending incremental file list\n$ cd ~/tmp/gdrive\n$ ls -lR utf_test/\nutf_test/:\ntotal 1\n-rw-rw-r-- 1 alex alex 7 Sep 15 22:37 D\u00f3nde est\u00e1n las llaves.mp3\n$ cat utf_test/D\u00f3nde\\ est\u00e1n\\ las\\ llaves.mp3\ntest A\n\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/85#issuecomment-55656712\n.\n\n\nLuis Carlos Cobo Rus       GnuPG ID: 44019B60\n. Your script works well for me too, actually I'm seeing the files must have\nsome old filename encoding, the example I gave looks like D?nde est?n las\nllaves.mp3 in \"ls\" and D$'\\363'nde\\ est$'\\341'n\\ las\\ llaves.mp3 if I try\nfilename completion. I could synchronize those files to other Linux\nmachines fine, but I guess Google Drive doesn't support that encoding. In\nany case, I'm happy considering this resolved. Thanks!\nOn Mon, Sep 15, 2014 at 2:24 PM, Luis Carlos Cobo Rus luiscarlos@gmail.com\nwrote:\n\nThanks for taking a look at this!! I will try your command sequence and my\nuse case again and let you know if the problem is gone or may be something\nelse. Thanks!\nOn Mon, Sep 15, 2014 at 1:51 PM, Alessandro Strada \nnotifications@github.com wrote:\n\nSorry, but I wasn't able to reproduce your issue on Ubuntu 14.04 64-bit.\n$ mkdir utf_test\n$ cd utf_test/\n$ echo \"test A\" > \"D\u00f3nde est\u00e1n las llaves.mp3\"\n$ ls -l\ntotal 4\n-rw-rw-r-- 1 alex alex 7 Sep 15 22:37 D\u00f3nde est\u00e1n las llaves.mp3\n$ cd ..\n$ google-drive-ocamlfuse -cc -debug ~/tmp/gdrive\n$ rsync -rlP --delete --size-only --inplace utf_test ~/tmp/gdrive/\nsending incremental file list\nutf_test/D\u00f3nde est\u00e1n las llaves.mp3\n          7 100%    0.00kB/s    0:00:00 (xfr#1, to-chk=0/2)\n$ cd ~/tmp/gdrive\n$ ls -lR utf_test\nutf_test:\ntotal 1\n-rw-rw-r-- 1 alex alex 7 Sep 15 22:37 D\u00f3nde est\u00e1n las llaves.mp3\n$ cd ..\n$ fusermount -u ~/tmp/gdrive\n$ google-drive-ocamlfuse -cc -debug ~/tmp/gdrive\n$ cd gdrive\n$ ls -l utf_test\ntotal 1\n-rw-rw-r-- 1 alex alex 7 Sep 15 22:37 D\u00f3nde est\u00e1n las llaves.mp3\n$ cd /tmp/\n$ fusermount -u ~/tmp/gdrive\n$ google-drive-ocamlfuse -cc -debug ~/tmp/gdrive\n$ rsync -rlP --delete --size-only --inplace utf_test ~/tmp/gdrive/\nsending incremental file list\n$ cd ~/tmp/gdrive\n$ ls -lR utf_test/\nutf_test/:\ntotal 1\n-rw-rw-r-- 1 alex alex 7 Sep 15 22:37 D\u00f3nde est\u00e1n las llaves.mp3\n$ cat utf_test/D\u00f3nde\\ est\u00e1n\\ las\\ llaves.mp3\ntest A\n\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/85#issuecomment-55656712\n.\n\n\nLuis Carlos Cobo Rus       GnuPG ID: 44019B60\n\n\nLuis Carlos Cobo Rus       GnuPG ID: 44019B60\n. I don't believe this is the same issue as #81 and #83, it's something particular to large files, it gets noticeably stuck. Maybe it uploads a 1GB file in a reasonable amount of time and then it takes forever to upload a 1MB file (hours), often just crashing after a while.\nI use:\nrsync -rlP --delete --size-only --inplace\n. Also, I just reproduced this issue of rsync hanging right after finishing a large file on 0.5.6. There is still heavy upload network activity, so it looks like the file is just cached somewhere and keeps being transferred for hours after rsync says it is done.\n. So is it ok to put a large file (e.g. >1GB) on that cache? Usually the\nproblem is rsync 'thinks' it uploaded the file but stalls immediately\nafterwards, I assume while the actual upload happens. The problem is that\nit takes for ever and there must be some problem or timeout because\neventually rsync crashes. Rerunning rsync will not try to re-upload the\nfile but the file is not really uploaded, so after remounting it will try\nto upload again.\nOn Wed, Sep 3, 2014 at 10:52 AM, Alessandro Strada <notifications@github.com\n\nwrote:\nYes, if you are using --size-only the timestamp issue won't affect you.\nThere is still heavy upload network activity, so it looks like the file is\njust cached somewhere and\nkeeps being transferred for hours after rsync says it is done.\nYes, it works like that. FUSE writes files in (small) chunks but GDrive\nAPI doesn't allow to upload file fragments. The file must be uploaded in a\nsingle request. So my app writes the file chunk by chunk in the cache and\nuploads it when rsync closes it. About the irregular upload rate, I don't\nknow what the problem is: my app just uses libcurl, so it's like uploading\nthe file using the curl command. Maybe you can use -debug and check the\nreal transfer rate checking timestamps in the log file.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/86#issuecomment-54337532\n.\n\n\nLuis Carlos Cobo Rus       GnuPG ID: 44019B60\n. Trying again, after the file is 'done' in rsync, it stays uploading at\naround 450KiB. gdfuse.log doesn't show much info (last msg was Uploading\nfile....) long ago. curl.log seems binary and superverbose and it is 1.8GB\nby now.\nOn Mon, Sep 8, 2014 at 6:21 PM, Luis Carlos Cobo Rus luiscarlos@gmail.com\nwrote:\n\nSo is it ok to put a large file (e.g. >1GB) on that cache? Usually the\nproblem is rsync 'thinks' it uploaded the file but stalls immediately\nafterwards, I assume while the actual upload happens. The problem is that\nit takes for ever and there must be some problem or timeout because\neventually rsync crashes. Rerunning rsync will not try to re-upload the\nfile but the file is not really uploaded, so after remounting it will try\nto upload again.\nOn Wed, Sep 3, 2014 at 10:52 AM, Alessandro Strada \nnotifications@github.com wrote:\n\nYes, if you are using --size-only the timestamp issue won't affect you.\nThere is still heavy upload network activity, so it looks like the file\nis just cached somewhere and\nkeeps being transferred for hours after rsync says it is done.\nYes, it works like that. FUSE writes files in (small) chunks but GDrive\nAPI doesn't allow to upload file fragments. The file must be uploaded in a\nsingle request. So my app writes the file chunk by chunk in the cache and\nuploads it when rsync closes it. About the irregular upload rate, I don't\nknow what the problem is: my app just uses libcurl, so it's like uploading\nthe file using the curl command. Maybe you can use -debug and check the\nreal transfer rate checking timestamps in the log file.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/86#issuecomment-54337532\n.\n\n\nLuis Carlos Cobo Rus       GnuPG ID: 44019B60\n\n\nLuis Carlos Cobo Rus       GnuPG ID: 44019B60\n. From gdfuse.log:\n[1454.642829] TID=0: getattr /luisca/Fotos/Campamento/Campamento\n2013/Videos/corto_2011final.avi\nGetting metadata from context...valid\nLoading resource /luisca/Fotos/Campamento/Campamento\n2013/Videos/corto_2011final.avi (trashed=false) from db...found\n[1454.643472] TID=0: flush /luisca/Fotos/Campamento/Campamento\n2013/Videos/corto_2011final.avi 0\nGetting metadata from context...valid\nLoading resource /luisca/Fotos/Campamento/Campamento\n2013/Videos/corto_2011final.avi (trashed=false) from db...found\nUploading file (cache\npath=/home/luisca/.gdfuse/default/cache/0B-dWp7gW1386UHNBOHQwQ2lCRVU,\ncontent type=video/x-msvideo)...Refreshing access token...ok\nSaving application state in /home/luisca/.gdfuse/default/state...done\nUploading file (cache\npath=/home/luisca/.gdfuse/default/cache/0B-dWp7gW1386UHNBOHQwQ2lCRVU,\ncontent type=video/x-msvideo)...Exception:Failure(\"Cannot access resource:\nRefreshing token was not enough\")\nBacktrace:\n[7268.868247] TID=0: release /luisca/Fotos/Campamento/Campamento\n2013/Videos/corto_2011final.avi O_RDONLY\nGetting metadata from context...valid\nLoading resource /luisca/Fotos/Campamento/Campamento\n2013/Videos/corto_2011final.avi (trashed=false) from db...found\nUploading file (cache\npath=/home/luisca/.gdfuse/default/cache/0B-dWp7gW1386UHNBOHQwQ2lCRVU,\ncontent type=video/x-msvideo)...Refreshing access token...ok\nSaving application state in /home/luisca/.gdfuse/default/state...done\nUploading file (cache\npath=/home/luisca/.gdfuse/default/cache/0B-dWp7gW1386UHNBOHQwQ2lCRVU,\ncontent type=video/x-msvideo)...Exception:Failure(\"Cannot access resource:\nRefreshing token was not enough\")\nBacktrace:\n[10906.457303] TID=0: flush /luisca/Fotos/Campamento/Campamento\n2013/Videos/corto_2011final.avi 0\nGetting metadata from context...valid\nLoading resource /luisca/Fotos/Campamento/Campamento\n2013/Videos/corto_2011final.avi (trashed=false) from db...found\nUploading file (cache\npath=/home/luisca/.gdfuse/default/cache/0B-dWp7gW1386UHNBOHQwQ2lCRVU,\ncontent type=video/x-msvideo)...Refreshing access token...ok\nSaving application state in /home/luisca/.gdfuse/default/state...done\nUploading file (cache\npath=/home/luisca/.gdfuse/default/cache/0B-dWp7gW1386UHNBOHQwQ2lCRVU,\ncontent type=video/x-msvideo)...Exception:Failure(\"Cannot access resource:\nRefreshing token was not enough\")\nBacktrace:\n[14536.238445] TID=0: release /luisca/Fotos/Campamento/Campamento\n2013/Videos/corto_2011final.avi O_WRONLY\nGetting metadata from context...valid\nLoading resource /luisca/Fotos/Campamento/Campamento\n2013/Videos/corto_2011final.avi (trashed=false) from db...found\nUploading file (cache\npath=/home/luisca/.gdfuse/default/cache/0B-dWp7gW1386UHNBOHQwQ2lCRVU,\ncontent type=video/x-msvideo)...Refreshing access token...ok\nSaving application state in /home/luisca/.gdfuse/default/state...done\nUploading file (cache\npath=/home/luisca/.gdfuse/default/cache/0B-dWp7gW1386UHNBOHQwQ2lCRVU,\ncontent type=video/x-msvideo)...\nOn Mon, Sep 8, 2014 at 9:07 PM, Luis Carlos Cobo Rus luiscarlos@gmail.com\nwrote:\n\nTrying again, after the file is 'done' in rsync, it stays uploading at\naround 450KiB. gdfuse.log doesn't show much info (last msg was Uploading\nfile....) long ago. curl.log seems binary and superverbose and it is 1.8GB\nby now.\nOn Mon, Sep 8, 2014 at 6:21 PM, Luis Carlos Cobo Rus <luiscarlos@gmail.com\n\nwrote:\nSo is it ok to put a large file (e.g. >1GB) on that cache? Usually the\nproblem is rsync 'thinks' it uploaded the file but stalls immediately\nafterwards, I assume while the actual upload happens. The problem is that\nit takes for ever and there must be some problem or timeout because\neventually rsync crashes. Rerunning rsync will not try to re-upload the\nfile but the file is not really uploaded, so after remounting it will try\nto upload again.\nOn Wed, Sep 3, 2014 at 10:52 AM, Alessandro Strada \nnotifications@github.com wrote:\n\nYes, if you are using --size-only the timestamp issue won't affect you.\nThere is still heavy upload network activity, so it looks like the file\nis just cached somewhere and\nkeeps being transferred for hours after rsync says it is done.\nYes, it works like that. FUSE writes files in (small) chunks but GDrive\nAPI doesn't allow to upload file fragments. The file must be uploaded in a\nsingle request. So my app writes the file chunk by chunk in the cache and\nuploads it when rsync closes it. About the irregular upload rate, I don't\nknow what the problem is: my app just uses libcurl, so it's like uploading\nthe file using the curl command. Maybe you can use -debug and check the\nreal transfer rate checking timestamps in the log file.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/86#issuecomment-54337532\n.\n\n\nLuis Carlos Cobo Rus       GnuPG ID: 44019B60\n\n\nLuis Carlos Cobo Rus       GnuPG ID: 44019B60\n\n\nLuis Carlos Cobo Rus       GnuPG ID: 44019B60\n. Thanks, let me know if you need me to do anything to help!\nOn Mon, Sep 15, 2014 at 3:15 AM, Alessandro Strada <notifications@github.com\n\nwrote:\ncurl.log seems binary and superverbose and it is 1.8GB by now.\nYes, in debug mode it dumps every byte of the request (so it contains a\ndump of your uploaded file).\nThere are some errors in gdfuse.log:\nUploading file (cache\npath=/home/luisca/.gdfuse/default/cache/0B-dWp7gW1386UHNBOHQwQ2lCRVU,\ncontent type=video/x-msvideo)...Exception:Failure(\"Cannot access resource:\nRefreshing token was not enough\")\nUploading file (cache\npath=/home/luisca/.gdfuse/default/cache/0B-dWp7gW1386UHNBOHQwQ2lCRVU,\ncontent type=video/x-msvideo)...Exception:Failure(\"Cannot access resource:\nRefreshing token was not enough\")\nProbably there is a bug that prevents token refresh during file upload. I\nhave to see if I can reproduce it.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/86#issuecomment-55572577\n.\n\n\nLuis Carlos Cobo Rus       GnuPG ID: 44019B60\n. Thanks, appreciate it.\nOn Tue, Sep 16, 2014 at 3:45 PM, Alessandro Strada <notifications@github.com\n\nwrote:\nI've subscribed the various threads on code.google.com reporting the\nproblem. When there is any news, I will update this issue.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/86#issuecomment-55825390\n.\n\n\nLuis Carlos Cobo Rus       GnuPG ID: 44019B60\n. Looks like it actually has now been fixed. Should it just work now or we will need a new release?\n. Seems to be working, thanks!\nOn Thu, Dec 4, 2014 at 2:39 PM, Alessandro Strada notifications@github.com\nwrote:\n\nI just made a test and it looks like it's fixed. A new release is not\nneeded because the bug was on Google side. Please let me know if you are\nstill experiencing problems uploading big files.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/86#issuecomment-65716850\n.\n\n\nLuis Carlos Cobo Rus       GnuPG ID: 44019B60\n. ",
    "agoode": "For your mtimes to be preserved, you need to set modifiedDate to the file mtime in the Files resource when you insert. And potentially set the setModifiedDate parameter in the update method.\n. ",
    "fommil": "Grive is unusably slow and unmaintained.\nWhat would happen if the user were to do something that loaded all the files in Drive... would all the data then not be available in the cache? And then why not allow this to work when there is no internet connection... e.g. if the connection is flakey.\n. @astrada thanks, what if the files are changed in this time? (assume there are no conflicts)\n. ",
    "muranyia": "FYI Grive stopped working a few days ago.\n. ",
    "sanmai-NL": "Thanks for the analysis. Do you have suggestions as to how I can control where cpp is sought?\n```\n$ command -v cpp\n/usr/bin/cpp\n```\n```\n$ cpp -v\nUsing built-in specs.\nCOLLECT_GCC=cpp\nTarget: x86_64-unknown-linux-gnu\nConfigured with: /build/gcc/src/gcc-4.9-20140507/configure --prefix=/usr --libdir=/usr/lib --libexecdir=/usr/lib --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=https://bugs.archlinux.org/ --enable-languages=c,c++,ada,fortran,go,lto,objc,obj-c++ --enable-shared --enable-threads=posix --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-clocale=gnu --disable-libstdcxx-pch --disable-libssp --enable-gnu-unique-object --enable-linker-build-id --enable-cloog-backend=isl --disable-cloog-version-check --enable-lto --enable-plugin --enable-install-libiberty --with-linker-hash-style=gnu --disable-multilib --disable-werror --enable-checking=release\nThread model: posix\ngcc version 4.9.0 20140507 (prerelease) (GCC) \nCOLLECT_GCC_OPTIONS='-E' '-v' '-mtune=generic' '-march=x86-64'\n /usr/lib/gcc/x86_64-unknown-linux-gnu/4.9.0/cc1 -E -quiet -v - -mtune=generic -march=x86-64\nignoring nonexistent directory \"/usr/lib/gcc/x86_64-unknown-linux-gnu/4.9.0/../../../../x86_64-unknown-linux-gnu/include\"\ninclude \"...\" search starts here:\ninclude <...> search starts here:\n/usr/lib/gcc/x86_64-unknown-linux-gnu/4.9.0/include\n /usr/local/include\n /usr/lib/gcc/x86_64-unknown-linux-gnu/4.9.0/include-fixed\n /usr/include\nEnd of search list.\n``\n. Switching from OCaml 4.00.1 to 4.01.0 withopam switch` fixed the problem. Any idea why the OCamlMakefile fails with OCaml 4.00.1?\n. ",
    "spider623": "i installed it 2 days ago on Linux 990FXLinux 3.14.3-1-MANJARO #1 SMP PREEMPT Tue May 6 21:46:41 UTC 2014 x86_64 GNU/Linux\nand installed just fine without errors\n. ",
    "madnessmike": "Yep, it works\nThanks a lot! :D\n. ",
    "SirSid": "I've noticed my root partition gets dangerously close to filling up when I copy large files. I assume the file lives in the cache (which is in my root) until the upload is complete. Could your drive be getting full when you move large files? The symptoms that you mention remind me of my laptop when I accidently filled its main partition \n. Looks like they started working on a fix for it again: https://code.google.com/p/google-api-python-client/issues/detail?id=231#c37 \n. ",
    "vikjon0": "Hello, anyway we could look into this?\nIf I unmount gdrive the share works, mount it again and it does not work.\nWhat I have tried is to mount it as root instead with the assumption that root is running samba and the mount does only seem to work for the user who mounted it.\nIs this correct? It seems like if I mount it as me, root cannot access it.\nI am not that good but a wild guess is that when I have mounted it as root and try to access it through samba as my self it does not work.\nAnyway to mount it for general access to all system users?\nI guess I could try enable root and access samba as it.\n. I have now tested. The issue is indeed permission. The drive need to be available for the user running samba and the user accessing the share simultaneous. This can be accomplish by accessing the share with root but that is not ideal.\nI will look a new issue\nEDIT: I realize that it if it works to mount on boot as root this means it is available to the user. This does not seem to work for me. I will try again.\n. Ok, solved.\nIt is has to be mounted with the -o allow_other as per\nhttp://blog.woralelandia.com/2012/07/16/fuse-mount-options/\nIn the mount on boot instructions this parameter is set in fstab.\nSo to recap, For samba to work root needs to have access to the mounted drive as well as the client side user.\nNow it works fine.\n. ",
    "simonjamain": "Hi, bumped on the same problem, can you @vikjon0 or @astrada tell me exactly what command do you run with -o allow_other as per ? I tried to put this in /etc/fuse.conf but did not seemed to work even after reboot. Thanks and sorry for asking on this 2 year old thread.\n. ",
    "yamaura": "add just a line user_allow_other to /etc/fuse.conf\nand add -o allow_other option to google-drive-ocamlfuse command. ",
    "tonsV2": "I have the same issue. I simply installed the package from the repository.\nUbuntu 14.04 64-bit here.\n. Segmentation fault.\nsnot@laptop:~$ cat /home/snot/.gdfuse/default/gdfuse.log\n[0.000148] TID=0: Setting up default filesystem...\n[0.000179] TID=0: Loading configuration from\n/home/snot/.gdfuse/default/config...done\nSaving configuration in /home/snot/.gdfuse/default/config...done\n[0.000348] TID=0: Loading application state from\n/home/snot/.gdfuse/default/state...done\nCurrent version: 0.5.3\nSetting up cache db...done\nSetting up CURL...done\nSaving application state in /home/snot/.gdfuse/default/state...done\nStarting web browser with command: xdg-open \"\nhttps://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=qGhTbiL7PNV1W5eHxCj7XMCIt5TWsvIs2SWSSxdQDHE\n\"...done\nDo you want the core dump?\nOn Tue, Jul 22, 2014 at 3:25 PM, Alessandro Strada <notifications@github.com\n\nwrote:\nIf you use bash, type this command before running google-drive-ocamlfuse:\n$ ulimit -c unlimited\nFor further information have a look at\nhttp://stackoverflow.com/questions/17965/generate-a-core-dump-in-linux\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/71#issuecomment-49737877\n.\n. \n",
    "vvcares": "Hi. I used \"google-drive-ocamlfuse -label myname /home/gdrive -o nonempty\".\nBut this command deleting my existing files .\nIs there a way to not delete but sync all together.?. Hi Astrada. Thank you for your guidance & YES, successfully done.\n(Sorry - if my question may out of your topic)\nI can see my gdrive folder inside my home/public_html folder. But is this folder can make use of as public access web folder.?\nI tried to change the folder permissions to web user name, but seems failed. Via my normal FTP user access, i cant see my folder there. . Seems i got it.\nI have to runt the ocamlfuse command as a 'user account'. So now i can use my drive as my hosting space... ",
    "mesmariusz": "Content of my gdfuse.log\n[0.017575] TID=0: Setting up rpi1_backup filesystem...\n[0.018262] TID=0: Loading configuration from /root/.gdfuse/rpi1_backup/config...done\nSaving configuration in /root/.gdfuse/rpi1_backup/config...done\n[0.282718] TID=0: Loading application state from /root/.gdfuse/rpi1_backup/state...done\nCurrent version: 0.5.3\nSetting up cache db...done\nSetting up CURL...done\nRefresh token already present.\n[0.335672] TID=0: Starting filesystem /media/backup\n[0.364782] TID=0: init_filesystem\n[294.241680] TID=0: getattr /\nLoading metadata from db...not valid\nRefreshing metadata...Refreshing access token...fail (error_code=Exception)\nError refreshing access token (try=0):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\nfail (error_code=Exception)\nError refreshing access token (try=1):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\nfail (error_code=Exception)\nError refreshing access token (try=2):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\nfail (error_code=Exception)\nError refreshing access token (try=3):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\nfail (error_code=Exception)\nError refreshing access token (try=4):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\nfail (error_code=Exception)\nError refreshing access token (try=5):\nException:GaeProxy.ServerError(\"error_code Exception\")\nBacktrace:\n[335.814038] TID=0: Exiting.\nCURL cleanup...done\nClearing context...done\nAs I wrote before. On the begining it is working. After some minutes... Errors...\n. Because I have no GUI on my Ubuntu Server I did it this way (even several times):\n$ fusermount -u /media/backup\nthen cleaning /root/.gdfuse\nthen\n$ google-drive-ocamlfuse -headless -label rpi1_backup -id ###my_google_id##.apps.googleusercontent.com -secret ###my_secret###\n$ google-drive-ocamlfuse -label rpi1_backup /media/backup/\nFinal result is: \"Access token retrieved correctly\".\nOf course id and secret are filled by my real one.\nThen it is working ok for some time. After several minutes It crashes and acces for mounted drive is not possible.\nAny idea?\n. I can confirm client_secret and client_id is empty inside my\n/root/.gdfuse/rpi1_backup/config\n. Then I inserted them manually into /root/.gdfuse/rpi1_backup/config\nThen:\n$ fusermount -u /media/backup\n$ google-drive-ocamlfuse -label net_backup /media/backup\nAnd it works correctly now. I hope this time it will work all the time (not a temporary sollution).\nThank you very much for your help.\nBest Regards\n. This was a sollution:\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/56\nCan be closed now.\n. ",
    "frank-dspeed": "thx for your answer plz can you supply me with infos how to build it on a normal ubuntu 14.10 machine then i can add the option without a extern browser via node.js and webkit webkit is able to get that oauth token headless :D you can google a bit about it when you supply me the infos i will contribute back to that repo with a fully headless automated version \ni simply need build instructions for the dependencys\nOCaml >= 3.12.0\nFindlib >= 1.2.7\nocamlfuse >= 2.7.1\ngapi-ocaml >= 0.2.1\nsqlite3-ocaml >= 1.6.1\n. ",
    "adojaan": "I tried multiple times, restarted browser,etc. Any command line flags for additional debugging?\n. From gdfuse log\n...\nSaving application state in /home/kristjan/.gdfuse/default/state...done\nStarting web browser with command: xdg-open \"https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleus\nercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%\n2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=Rwk1ABXF0OXbTNx8Tu7Nw1kwxeyQM8xMgNKITKWCo\n5w\"...done\nGetting tokens from GAE proxy...not found, retrying\nGetting tokens from GAE proxy...not found, retrying\nGetting tokens from GAE proxy...not found, retrying ... and so on\nFrom curl log\n[0.043789] curl: info: Hostname was NOT found in DNS cache\n[0.056149] curl: info:   Trying 173.194.70.141...\n[0.092577] curl: info: Connected to gd-ocaml-auth.appspot.com (173.194.70.141) port 443 (#0)\n[0.141081] curl: info: found 166 certificates in /etc/ssl/certs/ca-certificates.crt\n[0.304820] curl: info:   server certificate verification OK\n[0.305020] curl: info:   common name: .appspot.com (matched)\n[0.305047] curl: info:   server certificate expiration date OK\n[0.305060] curl: info:   server certificate activation date OK\n[0.305079] curl: info:   certificate public key: RSA\n[0.305092] curl: info:   certificate version: #3\n[0.305154] curl: info:   subject: C=US,ST=California,L=Mountain View,O=Google Inc,CN=.appspot.com\n[0.305192] curl: info:   start date: Wed, 04 Jun 2014 09:13:52 GMT\n[0.305205] curl: info:   expire date: Tue, 02 Sep 2014 00:00:00 GMT\n[0.305241] curl: info:   issuer: C=US,O=Google Inc,CN=Google Internet Authority G2\n[0.305265] curl: info:   compression: NULL\n[0.305279] curl: info:   cipher: AES-128-CBC\n[0.305293] curl: info:   MAC: SHA256\n[0.305407] curl: header out: GET /gettokens?requestid=Rwk1ABXF0OXbTNx8Tu7Nw1kwxeyQM8xMgNKITKWCo5w HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.3) gapi-ocaml/0.2.1/Unix\nHost: gd-ocaml-auth.appspot.com\nAccept: /\nAccept-Encoding: identity\n[0.484495] curl: header in: HTTP/1.1 200 OK\n[0.484610] curl: header in: Cache-Control: no-cache\n[0.484651] curl: header in: Content-Type: text/plain\n[0.484678] curl: header in: Vary: Accept-Encoding\n[0.484707] curl: header in: Date: Mon, 07 Jul 2014 07:10:57 GMT\n[0.484745] curl: info: Server Google Frontend is not blacklisted\n[0.484768] curl: header in: Server: Google Frontend\n[0.484777] curl: header in: Alternate-Protocol: 443:quic\n[0.484789] curl: header in: Transfer-Encoding: chunked\n[0.484796] curl: header in: \n[0.484805] curl: data in: 9\nNot_found\n0\n[0.484821] curl: info: Connection #0 to host gd-ocaml-auth.appspot.com left intact\n[5.491376] curl: info: Hostname was NOT found in DNS cache\n[5.503667] curl: info:   Trying 173.194.70.141...\n[5.539438] curl: info: Connected to gd-ocaml-auth.appspot.com (173.194.70.141) port 443 (#1)\n[5.589677] curl: info: found 166 certificates in /etc/ssl/certs/ca-certificates.crt\n[5.768683] curl: info:   server certificate verification OK\n[5.768856] curl: info:   common name: .appspot.com (matched)\n[5.768878] curl: info:   server certificate expiration date OK\n[5.768890] curl: info:   server certificate activation date OK\n[5.768905] curl: info:   certificate public key: RSA\n[5.768916] curl: info:   certificate version: #3\n[5.768978] curl: info:   subject: C=US,ST=California,L=Mountain View,O=Google Inc,CN=.appspot.com\n[5.768993] curl: info:   start date: Wed, 04 Jun 2014 09:13:52 GMT\n[5.769006] curl: info:   expire date: Tue, 02 Sep 2014 00:00:00 GMT\n[5.769040] curl: info:   issuer: C=US,O=Google Inc,CN=Google Internet Authority G2\n[5.769063] curl: info:   compression: NULL\n[5.769073] curl: info:   cipher: AES-128-CBC\n[5.769080] curl: info:   MAC: SHA256\n[5.769187] curl: header out: GET /gettokens?requestid=Rwk1ABXF0OXbTNx8Tu7Nw1kwxeyQM8xMgNKITKWCo5w HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.3) gapi-ocaml/0.2.1/Unix\nHost: gd-ocaml-auth.appspot.com\nAccept: /\nAccept-Encoding: identity\n[5.949609] curl: header in: HTTP/1.1 200 OK\n[5.949726] curl: header in: Cache-Control: no-cache\n[5.949788] curl: header in: Content-Type: text/plain\n[5.949874] curl: header in: Vary: Accept-Encoding\n[5.949904] curl: header in: Date: Mon, 07 Jul 2014 07:11:03 GMT\n[5.949942] curl: info: Server Google Frontend is not blacklisted\n[5.949971] curl: header in: Server: Google Frontend\n[5.949994] curl: header in: Alternate-Protocol: 443:quic\n[5.950027] curl: header in: Transfer-Encoding: chunked\n[5.950069] curl: header in: \n[5.950132] curl: data in: 9\nNot_found\n0\n[5.950212] curl: info: Connection #1 to host gd-ocaml-auth.appspot.com left intact\n[10.954572] curl: info: Hostname was NOT found in DNS cache\n[10.966803] curl: info:   Trying 173.194.70.141...\n[11.002531] curl: info: Connected to gd-ocaml-auth.appspot.com (173.194.70.141) port 443 (#2)\n[11.055154] curl: info: found 166 certificates in /etc/ssl/certs/ca-certificates.crt\n[11.247243] curl: info:      server certificate verification OK\n[11.247474] curl: info:      common name: .appspot.com (matched)\n[11.247501] curl: info:      server certificate expiration date OK\n[11.247515] curl: info:      server certificate activation date OK\n[11.247534] curl: info:      certificate public key: RSA\n[11.247548] curl: info:      certificate version: #3\n[11.247648] curl: info:      subject: C=US,ST=California,L=Mountain View,O=Google Inc,CN=.appspot.com\n[11.247674] curl: info:      start date: Wed, 04 Jun 2014 09:13:52 GMT\n[11.247689] curl: info:      expire date: Tue, 02 Sep 2014 00:00:00 GMT\n[11.247745] curl: info:      issuer: C=US,O=Google Inc,CN=Google Internet Authority G2\n[11.247779] curl: info:      compression: NULL\n[11.247795] curl: info:      cipher: AES-128-CBC\n[11.247808] curl: info:      MAC: SHA256\n[11.247917] curl: header out: GET /gettokens?requestid=Rwk1ABXF0OXbTNx8Tu7Nw1kwxeyQM8xMgNKITKWCo5w HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.3) gapi-ocaml/0.2.1/Unix\nHost: gd-ocaml-auth.appspot.com\nAccept: /\nAccept-Encoding: identity\n[11.427210] curl: header in: HTTP/1.1 200 OK\n[11.427322] curl: header in: Cache-Control: no-cache\n[11.427357] curl: header in: Content-Type: text/plain\n[11.427391] curl: header in: Vary: Accept-Encoding\n[11.427423] curl: header in: Date: Mon, 07 Jul 2014 07:11:08 GMT\n[11.427455] curl: info: Server Google Frontend is not blacklisted\n[11.427501] curl: header in: Server: Google Frontend\n[11.427538] curl: header in: Alternate-Protocol: 443:quic\n[11.427590] curl: header in: Transfer-Encoding: chunked\n[11.427657] curl: header in: \n[11.427696] curl: data in: 9\nNot_found\n0\n[11.427781] curl: info: Connection #2 to host gd-ocaml-auth.appspot.com left intact\n. Hi,\nI removed the contents of default folder and tried with\ngoogle-drive-ocamlfuse -debug\nfollowing output was displayed on console until I cancelled after message\n(Error: Bad Request Your client has issued a malformed or illegal request.)\nin the browser\nStarting application setup (label=default).\nOpening log file: /home/kristjan/.gdfuse/default/gdfuse.log\nATTENTION: default value of option force_s3tc_enable overridden by\nenvironment.\n^C\nI targzipped the default folder with all-new content, please see if you\nfind something there. (If these files contain any sensitive information,\nplease delete from github, it is my real google account). By the way - I\ntried with another account, too, but the result was the same (attached, too)\nThank you very much!\nBest regards, Kristjan Adojaan\nKristjan Adojaan, M.Sc.\n5D Vision O\u00dc juhataja\ntelefon 503 9306, 747 5542\nskype adojaan\nkristjan@5dvision.ee\n5D Vision - enam kui tosin edukat aastat 5D maailmas!\n5D Vision | +372 747 5542 | Riia 181C III korrus, Tartu 51014 |\nwww.5dvision.ee\nOn Tue, Jul 8, 2014 at 11:23 PM, Alessandro Strada <notifications@github.com\n\nwrote:\nPlease try starting from scratch removing all the files in\n~/.gdfuse/default.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/78#issuecomment-48395063\n.\n. \n",
    "anderskvist": "I had the same error, but fixed it with a clean browser with no cached accounts...\n. ",
    "idevin": "Same thing\n. ",
    "wallnas": "I have modify metadata_cache_time = 180  and 360 : There is very slow.\n. ",
    "lightsurge": "I'm also experiencing this slow-down even with a high metadata_cache_time. In my case I gzip a large amount of data into the google drive folder... so possibly because it gradually writes the compressed file into the google drive folder this is causing excessive syncing. I'm going to try moving pre-prepared files rather than performing heavy operations on files inside the sync folder.\n. ",
    "ardhipoetra": "Second this.\nI know this is old issue, I got only 100-150 kB/s with FUSE. In the other hand, I can upload 2-3 MB/s with browser. Tried changing the metadata_cache_time produces the same result. \nMy concern (for now) only for uploading though.. Still, the speed very slow. Now it has around 400kB/s. Moreover, I uploaded from SSD, so I don't think the bottleneck is from the disk.\nAs for comparison, I can get good performance with https://github.com/odeke-em/drive. rclone also implements FUSE to mount cloud storage, one of them is google drive. I tried that before and mostly can get higher speed although it still in experimental stage. I will look into the details later.. ",
    "hbakhtiyor": "i'm experiencing the same issue with speed of uploading. Hi @RobertusIT, how did you resolve the issue?. ",
    "vi": "Maybe the slowness is due to not using the database properly: https://github.com/astrada/google-drive-ocamlfuse/issues/308#issue-230220653. It fully syncs the database state to disk every 128 kilobytes.\nUsing libeatmydata hack forces unsafe, unsynced database mode and makes uploads fast.. eatmydata google-drive-ocamlfuse makes upload fast.. Does it stop doing fsync for each uploaded file (or part of a large file)?. This should be mentioned in README or some documentation linked from README.\nHow do input ID from sharable link to the google-drive-ocamlfuse without going though browser? Just cd into .shared/<ID> doesn't work.\nIdea: supporting entering URI-named directories, like cd 'https://drive.google.com/open?id=ABCDE0987...21'.. Added a bullet point for this in README in #322.. ",
    "wifiuk": "i also get crippling slow folder loading and file transfers. ",
    "d0wnblog": "Work's now but getting only 50-80 KB/s @ upload... :/\n. ",
    "hardkorova": "Yes, right. \n. ",
    "paprika27": "how do i find out? it gets stuck without anything other than I posted. Is there a log?\n. gthanks! At first, i was getting nothing as the token was taken or somesuch. I then deleted the folder default, reran the command with debug option. here's my output:\n$ google-drive-ocamlfuse -debug\nStarting application setup (label=default).\nOpening log file: /home/p/.gdfuse/default/gdfuse.log\n[12139:12139:0827/182740:ERROR:nss_util.cc(853)] After loading Root Certs, loaded==false: NSS error code: -8018\nATTENTION: default value of option force_s3tc_enable overridden by environment.\n[WARNING:flash/platform/pepper/pep_module.cpp(63)] SANDBOXED\n[12139:12169:0827/182756:ERROR:raw_channel_posix.cc(139)] recvmsg: Connection reset by peer\n[12139:12169:0827/182756:ERROR:channel.cc(297)] RawChannel fatal error (type 1)\n[12139:12169:0827/182756:ERROR:raw_channel_posix.cc(139)] recvmsg: Connection reset by peer\n[12139:12169:0827/182756:ERROR:channel.cc(297)] RawChannel fatal error (type 1)\n[12139:12169:0827/182756:ERROR:raw_channel_posix.cc(139)] recvmsg: Connection reset by peer\n[12139:12169:0827/182756:ERROR:channel.cc(297)] RawChannel fatal error (type 1)\n[12139:12169:0827/182756:ERROR:raw_channel_posix.cc(139)] recvmsg: Connection reset by peer\n[12139:12169:0827/182756:ERROR:channel.cc(297)] RawChannel fatal error (type 1)\n[12139:12169:0827/182757:ERROR:raw_channel_posix.cc(139)] recvmsg: Connection reset by peer\n[12139:12169:0827/182757:ERROR:channel.cc(297)] RawChannel fatal error (type 1)\n[12139:12169:0827/182757:ERROR:raw_channel_posix.cc(139)] recvmsg: Connection reset by peer\n[12139:12169:0827/182757:ERROR:channel.cc(297)] RawChannel fatal error (type 1)\nwhile the browser says\nThe application was successfully granted access. Please wait for the client to retrieve the authorization tokens\nwith the curl.log empty and gdfuse.log reading thus:\n[0.018772] TID=0: Setting up default filesystem...\n[0.018906] TID=0: Loading configuration from /home/p/.gdfuse/default/config...not found.\nSaving configuration in /home/p/.gdfuse/default/config...done\nSaving configuration in /home/p/.gdfuse/default/config...done\n[0.019803] TID=0: Loading application state from /home/p/.gdfuse/default/state...not found.\nSaving application state in /home/p/.gdfuse/default/state...done\nCurrent version: 0.5.3\nSetting up cache db...done\nSetting up CURL...done\nthanks for your help so far!\nEDIT: upon closing the browser, the terminal informed me that it received the token correctly?! now let me see if it's not working already all of a sudden.\nEDIT2: WHAT? It just works now. No idea what changed. Anyway, SOLVED!\n. ",
    "nebulousdog": "Thanks for clarifying! \n. ",
    "DeV1L": "Update to 0.5.9. fixed this problem. Thanks Thanks a lot!\n. Thanks, it's helped!\n. ",
    "scottcarol": "Thanks, updated the wiki.\nOn Tue, Sep 30, 2014 at 10:50 AM, Alessandro Strada \nnotifications@github.com wrote:\n\nNo, you don't need Drive SDK, because google-drive-ocamlfuse is a\nstand-alone application. It doesn't integrate with drive UI, and it isn't a\nChrome App.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/93#issuecomment-57353756\n.\n. \n",
    "Dmole": "Could you file a bug upstream?\nYou could in the mean time monitor stderr for that message and return non-zero based on that... which is what I do in a wrapper script... if you wanted.\n. ",
    "MadeofTin": "Found the headless mode, but it is giving me issues.\nGives me a 401 error. Maybe it is time to update the guide. I think it would also be nice to add a link to headless authorisation in the main authorise page so it is easier to find for people like me.\nThis is the error I get\n```\n401. That\u2019s an error.\nError: invalid_client\nno application name\nRequest Details\nresponse_type=code\n scope=https://www.googleapis.com/ auth/drive\naccess_type=offline\nredirect_uri=urn:ietf:wg:oauth:2.0:oob\napproval_prompt=force\nclient_id=**\nThat\u2019s all we know.\n```\n. Hmmm.... haven't had time to do a full run through again. But I think this\nmight fix it. I'll let you know and then maybe we(or I) can update the\ndocumentation to cover this case.\nCheers,\nJames\nOn Thu, Oct 9, 2014 at 3:13 AM, Alessandro Strada notifications@github.com\nwrote:\n\nPlease, check if this StackOverflow question\nhttp://stackoverflow.com/questions/18677244/error-invalid-client-no-application-name\nis related.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/95#issuecomment-58411093\n.\n. \n",
    "stephanhughson": "TinmanPiano, in case it's useful, here are my notes for authenticating on a headless server. The instructions below work for me each time, so hopefully they will work for you too.\nVisit https://console.developers.google.com/ and sign in as a Google Administrator.\nGo to Projects - Create Project. Project name Offsite-Storage-1\nDisable the default APIs under APIs & Auth. (BigQuery API, Google Cloud SQL, Google Cloud Storage, Google Cloud Storage JSON API).\nEnable the Drive API in the same area (you don't need the drive SDK).\nGo to APIs & Auth - Credentials - Create new Client ID.\nAPIs & Auth - Credentials - Create new Client ID - Configure Consent Screen - enter the project name again, then choose \"installed application\" and \"other\", then \"Create Client ID\".\nexport https_proxy=http://serverproxy.XXXXXXXXX.whatever:3128 (you're about to download an access token). - if you want to use a proxy... you need Internet access anyway.\nmkdir /mnt/Offsite-Storage-1; google-drive-ocamlfuse -headless -id ClientIDfromWebpage -secret SecretFromWebpage -label Offsite-Storage-1 /mnt/Offsite-Storage-1/\nFollow the link it gives you, then paste the access code into the server.\nType mount and df -h , you should see the new storage mounted.\nI hope that works for you.\n. I've tested it out some more (there was room for confusion earlier as my own user's settings were picked up instead of the user I was mounting as).\nUsing export works.\nUsing .curlrc didn't work in my testing.\nThanks again :+1: \n. ",
    "antoniocoratelli": "setting \"presentation_format=pdf\" solves the problem for presentation, but it downloads the file in ppt format (wtf?). the same setting does not work for module and sheet.\n-r--r--r-- 1 pi pi 4029 ott 31 12:22 test-doc.odt\n-r--r--r-- 1 pi pi  486 ott 31 12:23 test-draw.svg\n?????????? ? ?  ?     ?            ? test-module.ods\n-r--r--r-- 1 pi pi 1284 ott 31 12:22 test-presentation.ppt\n?????????? ? ?  ?     ?            ? test-sheet.ods\n. ",
    "ivan": "The workaround for being able to start it up again is fusermount -u MOUNTPOINT\n. I was running the latest (as of yesterday) google-drive-ocamlfuse from https://launchpad.net/~alessandro-strada/+archive/ubuntu/ppa\nI used headless authorization with my own custom client_id/secret.\n. Yep, it did, and when I started google-drive-ocamlfuse up again, it worked fine.  I haven't seen that particular failure again.\n. ",
    "kilabyte": "Any plans to specify the mount point subdirectory? I really don't want my whole drive mounted on the server (security). Based on @hynese suggestion where can i find the drive.ml on my install?. hmmm thats the kicker is i don't have root/sudo access to add opam. ",
    "rourke": "I really don't want to install opam and compile it to be able to mount a subdirectory. Seems like a big hassle over having an option like root_directory as an argument. Any plans on adding something like this?. ",
    "bayoumedic": "i am needing this too. ",
    "jmiller0": "Has anyone gotten this to work, what is the syntax?. ",
    "felixbrucker": "This does not seem to work with folder ids from backuped computers like the following:\n\ni assume the regular gdrive api can't reach those?. ",
    "sinjar666": "Thanks for the reply\n. ",
    "kaoz3000": "Wow, thank you for your fast response!\nI had 1.5.0 now I downgraded to 1.4.5 and it works perfectly!\nhave a nice new 2015!\nFelix\n. Hi astrada,\nthank you again!\nNow it works!\n. ",
    "jimmyye": "Ubuntu 14.04 downgrade to 1.4.4 works. In case somebody looks for it:\nhttp://archive.ubuntu.com/ubuntu/pool/main/libg/libgcrypt11/\n. ",
    "PeterDaveHello": "Try to use type google-drive-ocamlfuse to get the full path of it, and use it to replace google-drive-ocamlfuse in line 2.\n. There is not a problem of this project, but a path issue of your shell script, you should prepare the search path for it, or you should give the command a full path.\n. I think there should not be a space after assign symbol.\n. ",
    "JaysonWonder": "thanks this works!\n. I added the path to my shell. Do you think there was an error with this?\nThis is what I did:\nPATH= $PATH:$HOME/.opam/system/bin export PATH\nIs it wrong?\nOn Jan 8, 2015 11:31 PM, \"Peter Dave Hello\" notifications@github.com\nwrote:\n\nThere is not a problem of this project, but a path issue of your shell\nscript, you should prepare the search path for it, or you should give the\ncommand a full path.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/108#issuecomment-69288758\n.\n. No. That was just a typo on my post. I went and verified that it was\ncorrect. I have also confirmed that the path is being set by my .bashrc\ncorrectly so it is another problem.\n\nStill works with the full path to ocamlfuse specified. No big issue just\nwondered if this was a bug or user problem. Can't seem to find anything\nelse wrong.\nAlso do I need to still have a googledrive directory in my home? Since one\nis created now in /mnt?\nThanks\nOn Jan 8, 2015 11:46 PM, \"Jayson Wonder\" jaysonwonder@gmail.com wrote:\n\nI added the path to my shell. Do you think there was an error with this?\nThis is what I did:\nPATH= $PATH:$HOME/.opam/system/bin export PATH\nIs it wrong?\nOn Jan 8, 2015 11:31 PM, \"Peter Dave Hello\" notifications@github.com\nwrote:\n\nThere is not a problem of this project, but a path issue of your shell\nscript, you should prepare the search path for it, or you should give the\ncommand a full path.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/108#issuecomment-69288758\n.\n. Well in actual fact I am running everything as root. Unless I specify  the\nfull path it won't work. Any other ideas?\nOn Jan 11, 2015 5:40 AM, \"Alessandro Strada\" notifications@github.com\nwrote:\n\nPATH=$PATH:$HOME/.opam/system/bin export PATH\nTry expanding $HOME. If the user running /usr/bin/gdfuse is not your\nuser, $HOME will not match with your home and the executable will not be\nfound.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/108#issuecomment-69488653\n.\n. \n",
    "janxkoci": "Interestingly if I just run the following line from terminal:\ngoogle-drive-ocamlfuse ~/Gdrive/\nit works ok, but if I put it into autostart apps (with Ubuntu GUI), it won't start. I'll try the full path and report here if it worked after reboot later today (I'm at work).. This would be great - elementaryOS disabled PPAs (and I'm still not sure I want to re-enable them) so I want to try other ways to get my packages, mainly with snap and flatpak.\nI could try to learn how to make snap package out of existing repository, when I have some free time. I'm not a dev though, so I don't know how that will go :smile: From the docs it doesn't seem hard, I'll see.. ",
    "jagrutiboda": "/bin/bash: google-drive-ocamlfuse: command not found error occur so please solve this error. ",
    "w-flo": "Dirty workaround that worked for me:\n- Backup /usr/bin/xdg-open\n- Create new /usr/bin/xdg-open file, and make sure to mark it executable. Contents:\n```\n!/bin/sh\necho $1 1>&2\nexit 0\n```\n(Not sure if exit 0 is required, I guess not. Just make sure not to echo anything to STDOUT, that apparently makes google-drive-ocamlfuse think that something failed and exit instead of polling for access grant.)\n- Run google-drive-ocamlfuse, which should print the URL and you can use your favourite web access method to go to that URL and grant access, then follow the usual docs. Don't forget to put the original xdg-open script back in place.\n. ",
    "blueyed": "Too bad.\nThanks for your support and this tool nevertheless!\n. ",
    "ddurdle": "\"Unfortunately, Google Drive API doesn't allow to upload files in chunks. So I have to cache the whole file before uploading it. And there is no workaround. So the only option is to have a virtual disk big enough.\"\nNews to me.  Google Drive supported chunk transfers (as long as chunks are in multiples of 512 bytes) since Google Docs API.\nhttps://developers.google.com/drive/web/manage-uploads\n. ",
    "Bertieio": "On ~/.profile im getting file not found \nand the contents of ~/.opam/opam-init/variables.sh is:\nCAML_LD_LIBRARY_PATH=/home/Bertie/.opam/4.01.0/lib/stublibs; export CAML_LD_LIBRARY_PATH;\nMANPATH=$MANPATH:/home/Bertie/.opam/4.01.0/man; export MANPATH;\nMAKELEVEL=; export MAKELEVEL;\nMAKEFLAGS=; export MAKEFLAGS;\nPERL5LIB=/home/Bertie/.opam/4.01.0/lib/perl5:$PERL5LIB; export PERL5LIB;\nOCAML_TOPLEVEL_PATH=/home/Bertie/.opam/4.01.0/lib/toplevel; export OCAML_TOPLEVEL_PATH;\nPATH=/home/Bertie/.opam/4.01.0/bin:$PATH; export PATH;\n. All of them are in the right places and the $PATH files are correct\n. ~/.opam/4.01.0/bin/google-drive-ocamlfuse \n. No thats fixed but now its not syncing. Sorry I edited my first post to say a reboot fixed it. \n. I run google-drive-ocamlfuse and nothing happens, It worked originally after a restart but I had to restart again and I stopped being able to access gdrive \n. ",
    "Rick7C2": "I'm dumb lol.\nI had .gdfuse/default/config set at 512MB But I mounted with -label I had the wrong config file edited.\n. I take it you are trying to stream the transcoded version from google drive web? If so it take a bit for google drive to process newly uploaded videos. If it is only new upoads giving you this issue then give it some time.\n. Thanks for confirming @astrada I just tested a small batch of files and it is indeed renaming them serverside. About to edit my script and batch rename all of the files.\n. Not sure if there is a way to configure google-drive-ocamlfuse but you could symlink the directory.\nln -s /DirectoryOnYour2TBPartituon/cache /home/seymour1/.gdfuse/default/cache\nMake sure to stop google-drive-ocamlfuse first.\nMove the original cache directly where ever you want it on your 2TB partition. Make sure the original cache dir is gone before running the above command. If not the symlink will not work.\nThis will alow the actual data to be stored on the partition you want while still being found in the other. Basically it's a shortcut.\n. ",
    "woftor": "I cannot find the right config file and my cache is growing HUGE overtime... The one in .gdfuse/default/config is set to 512 MB, but my cache was 17 GB after 2 months. Is it somewhere in /etc?\nAs a workaround I have a cronjob clear the cache weekly with\ngoogle-drive-ocamlfuse -cc\n. I will try that, because I have indeed large files (3GB+).\nIs there any drawback in streaming?\nThank you for the response!\n. Do you mean 'unless the files modified -during- the copy process?' Trying to fully onderstand what is happening. Again thanks, really elaborating.\n. ",
    "m94mni": "Thanks! Somewhat hidden option, but perhaps for good reason...\nTried it together with rcoup/rsync_parallel.sh and got a flood of errors and then transport disconnected. \nLots of rsync lines like\nBilder/2008-08-12--2008-12-06/IMG_3007.JPG\nrsync: read errors mapping \"/home/mikaeln/gdrive/Bilder/2008-08-12--2008-12-06/IMG_2995.JPG\": Device or resource busy (16)\nIt would be a great option to have stable! \n. ",
    "pbx": "Thanks for the quick reply. Here's my config: http://dpaste.com/3B4BDSD\n. ",
    "fluffman86": "Thanks for the help! Changing that option worked. I never changed it when switching from grive to ocamlfuse. Weird, but glad it's working now. \n. ",
    "lickdragon": "[14.905518] TID=0: fopen /poplar/data1/CCA_run_creation/Quist2012/.hg/undo.bookmarks O_RDONLY\nGetting metadata from context...valid\nLoading resource /poplar/data1/CCA_run_creation/Quist2012/.hg/undo.bookmarks (trashed=false) from db...found\n[14.905937] TID=0: fopen /poplar/data1/CCA_run_creation/Quist2012/.hg/undo.bookmarks O_WRONLY\nGetting metadata from context...valid\nLoading resource /poplar/data1/CCA_run_creation/Quist2012/.hg/undo.bookmarks (trashed=false) from db...found\n[14.906300] TID=0: truncate /poplar/data1/CCA_run_creation/Quist2012/.hg/undo.bookmarks 0\nGetting metadata from context...valid\nLoading resource /poplar/data1/CCA_run_creation/Quist2012/.hg/undo.bookmarks (trashed=false) from db...found\nTruncating file (id=0BzBx9fimjv-7ZUFqUk4teEEzTFU)...Updating resource in db (id=35)...done\ndone\n[15.002096] TID=0: getattr /poplar/data1/CCA_run_creation/Quist2012/.hg/undo.bookmarks\nGetting metadata from context...valid\nLoading resource /poplar/data1/CCA_run_creation/Quist2012/.hg/undo.bookmarks (trashed=false) from db...found\n[15.002581] TID=0: flush /poplar/data1/CCA_run_creation/Quist2012/.hg/undo.bookmarks 0\nGetting metadata from context...valid\nLoading resource /poplar/data1/CCA_run_creation/Quist2012/.hg/undo.bookmarks (trashed=false) from db...found\nUploading file (cache path=/root/.gdfuse/default/cache/0BzBx9fimjv-7ZUFqUk4teEEzTFU, content type=application/octet-stream)...Error during request: Error:  (HTTP response code: 400)\nGiving up\nException:Failure(\"Error:  (HTTP response code: 400)\")\nBacktrace:\n[15.558681] TID=0: release /poplar/data1/CCA_run_creation/Quist2012/.hg/undo.bookmarks O_RDONLY\nGetting metadata from context...valid\nLoading resource /poplar/data1/CCA_run_creation/Quist2012/.hg/undo.bookmarks (trashed=false) from db...found\nUploading file (cache path=/root/.gdfuse/default/cache/0BzBx9fimjv-7ZUFqUk4teEEzTFU, content type=application/octet-stream)...Error during request: Error:  (HTTP response code: 400)\nGiving up\nException:Failure(\"Error:  (HTTP response code: 400)\")\nBacktrace:\n[16.016069] TID=0: flush /poplar/data1/CCA_run_creation/Quist2012/.hg/undo.bookmarks 0\nGetting metadata from context...valid\nLoading resource /poplar/data1/CCA_run_creation/Quist2012/.hg/undo.bookmarks (trashed=false) from db...found\nUploading file (cache path=/root/.gdfuse/default/cache/0BzBx9fimjv-7ZUFqUk4teEEzTFU, content type=application/octet-stream)...Error during request: Error:  (HTTP response code: 400)\nGiving up\nException:Failure(\"Error:  (HTTP response code: 400)\")\nBacktrace:\n[16.546175] TID=0: release /poplar/data1/CCA_run_creation/Quist2012/.hg/undo.bookmarks O_WRONLY\nGetting metadata from context...valid\nLoading resource /poplar/data1/CCA_run_creation/Quist2012/.hg/undo.bookmarks (trashed=false) from db...found\nUploading file (cache path=/root/.gdfuse/default/cache/0BzBx9fimjv-7ZUFqUk4teEEzTFU, content type=application/octet-stream)...Error during request: Error:  (HTTP response code: 400)\nGiving up\nException:Failure(\"Error:  (HTTP response code: 400)\")\nBacktrace:\n[30.277040] TID=0: getattr /\nGetting metadata from context...not valid\nRefreshing metadata...done\nUpdating metadata in db...done\nUpdating context...done\nGetting changes from server...done\nUpdating resource cache...done\nUpdating trashed resources...done\nRemoving deleted resources...Updating cache size (0) in db...done\nUpdating context...done\ndone\nInvalidating trash bin resource...done\nLoading resource / (trashed=false) from db...found\n[31.597481] TID=0: statfs /\nGetting metadata from context...valid\n[31.617937] TID=0: Exiting.\nCURL cleanup...done\nClearing context...done\n. [15.313989] curl: header in: HTTP/1.1 200 OK\n[15.314008] curl: header in: Expires: Thu, 21 May 2015 20:29:11 GMT\n[15.314021] curl: header in: Date: Thu, 21 May 2015 20:29:11 GMT\n[15.314034] curl: header in: Cache-Control: private, max-age=0, must-revalidate, no-transform\n[15.314046] curl: header in: ETag: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\"\n[15.314058] curl: header in: Vary: Origin\n[15.314091] curl: header in: Vary: X-Origin\n[15.314101] curl: header in: Content-Type: application/json; charset=UTF-8\n[15.314109] curl: header in: X-Content-Type-Options: nosniff\n[15.314117] curl: header in: X-Frame-Options: SAMEORIGIN\n[15.314125] curl: header in: X-XSS-Protection: 1; mode=block\n[15.314134] curl: header in: Content-Length: 1270\n[15.314143] curl: info: Server GSE is not blacklisted\n[15.314151] curl: header in: Server: GSE\n[15.314159] curl: header in: Alternate-Protocol: 443:quic,p=1\n[15.314167] curl: header in: \n[15.314177] curl: data in: {\n \"id\": \"0BzBx9fimjv-7ZUFqUk4teEEzTFU\",\n \"etag\": \"\\\"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\\\"\",\n \"alternateLink\": \"https://docs.google.com/a/wisc.edu/file/d/0BzBx9fimjv-7ZUFqUk4teEEzTFU/edit?usp=drivesdk\",\n \"title\": \"undo.bookmarks\",\n \"mimeType\": \"application/octet-stream\",\n \"labels\": {\n  \"starred\": false,\n  \"hidden\": false,\n  \"trashed\": false,\n  \"restricted\": false,\n  \"viewed\": true\n },\n \"createdDate\": \"2015-05-21T20:16:34.507Z\",\n \"modifiedDate\": \"2015-05-21T20:16:34.421Z\",\n \"lastViewedByMeDate\": \"2015-05-21T20:16:34.421Z\",\n \"parents\": [\n  {\n   \"kind\": \"drive#parentReference\",\n   \"id\": \"0BzBx9fimjv-7RU56QWVtaXZZSHM\",\n   \"selfLink\": \"https://www.googleapis.com/drive/v2/files/0BzBx9fimjv-\n[15.314205] curl: data in: 7ZUFqUk4teEEzTFU/parents/0BzBx9fimjv-7RU56QWVtaXZZSHM\",\n   \"parentLink\": \"https://www.googleapis.com/drive/v2/files/0BzBx9fimjv-7RU56QWVtaXZZSHM\",\n   \"isRoot\": false\n  }\n ],\n \"downloadUrl\": \"https://doc-0k-90-docs.googleusercontent.com/docs/securesc/83ugdofc3ce5878s88akla24pqgrt7sb/jp6fdu9b1s6nlu9162h3ptokh5g4mt60/1432238400000/10252121649383171870/10252121649383171870/0BzBx9fimjv-7ZUFqUk4teEEzTFU?h=15376047860968316007&e=download&gd=true\",\n \"fileExtension\": \"bookmarks\",\n \"md5Checksum\": \"d41d8cd98f00b204e9800998ecf8427e\",\n \"fileSize\": \"0\",\n \"editable\": true\n}\n[15.314223] curl: info: Connection #0 to host www.googleapis.com left intact\n[15.314586] curl: info: Found bundle for host www.googleapis.com: 0x1f01180\n[15.314602] curl: info: Re-using existing connection! (#0) with host www.googleapis.com\n[15.314614] curl: info: Connected to www.googleapis.com (2607:f8b0:4009:80b::200a) port 443 (#0)\n[15.314654] curl: header out: PUT /resumable/upload/drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle&updateViewedDate=true HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.15) gapi-ocaml/0.2.6/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nTransfer-Encoding: chunked\nContent-Type: application/json\nAuthorization: Bearer ya29.egGhHXTnpC75tOlmlt6GFy46jjcQ1I0AKlXzIVpL-OLBD4bSRbtBnTM41z9EDweNPP_mIzj3DBtJyg\nIf-Match: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\"\nX-Upload-Content-Type: application/octet-stream\nX-Upload-Content-Length: 0\nExpect: 100-continue\n[15.320795] curl: header in: HTTP/1.1 100 Continue\n[15.320858] curl: data out: 430\n{\"alternateLink\":\"https://docs.google.com/a/wisc.edu/file/d/0BzBx9fimjv-7ZUFqUk4teEEzTFU/edit?usp=drivesdk\",\"createdDate\":\"2015-05-21T20:16:34.000Z\",\"downloadUrl\":\"https://doc-0k-90-docs.googleusercontent.com/docs/securesc/83ugdofc3ce5878s88akla24pqgrt7sb/jp6fdu9b1s6nlu9162h3ptokh5g4mt60/1432238400000/10252121649383171870/10252121649383171870/0BzBx9fimjv-7ZUFqUk4teEEzTFU?h=15376047860968316007&e=download&gd=true\",\"editable\":true,\"etag\":\"\\\"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\\\"\",\"fileExtension\":\"bookmarks\",\"id\":\"0BzBx9fimjv-7ZUFqUk4teEEzTFU\",\"labels\":{\"viewed\":true},\"lastViewedByMeDate\":\"2015-05-21T20:16:34.000Z\",\"md5Checksum\":\"d41d8cd98f00b204e9800998ecf8427e\",\"mimeType\":\"application/octet-stream\",\"modifiedDate\":\"2015-05-21T20:16:34.000Z\",\"parents\":[{\"id\":\"0BzBx9fimjv-7RU56QWVtaXZZSHM\",\"kind\":\"drive#parentReference\",\"parentLink\":\"https://www.googleapis.com/drive/v2/files/0BzBx9fimjv-7RU56QWVtaXZZSHM\",\"selfLink\":\"https://www.googleapis.com/drive/v2/files/0BzBx9fimjv-7ZUFqUk4\n teEEzTFU/parents/0BzBx9fimjv-7RU56QWVtaXZZSHM\"}],\"title\":\"undo.bookmarks\"}\n[15.320894] curl: data out: 0\n[15.501338] curl: header in: HTTP/1.1 200 OK\n[15.501360] curl: header in: Location: https://www.googleapis.com/resumable/upload/drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle&updateViewedDate=true&upload_id=AEnB2UofaGgwU2Z1mC-7Z9QcOD4-itnJisrAyIgMD90Cb02R_jq2cPGVM8_lcJ8NYJJqNJph_wl9BFWq78RzV2fgp8XxwzEcvg\n[15.501376] curl: header in: ETag: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/mLNL2XBVN-aH8ZKM7yhxWd_PaPg\"\n[15.501388] curl: header in: Vary: Origin\n[15.501400] curl: header in: Vary: X-Origin\n[15.501412] curl: header in: Cache-Control: no-cache, no-store, max-age=0, must-revalidate\n[15.501424] curl: header in: Pragma: no-cache\n[15.501439] curl: header in: Expires: Fri, 01 Jan 1990 00:00:00 GMT\n[15.501447] curl: header in: Date: Thu, 21 May 2015 20:29:11 GMT\n[15.501456] curl: header in: Content-Length: 0\n[15.501465] curl: info: Server UploadServer (\"Built on May 8 2015 11:09:05 (1431108545)\") is not blacklisted\n[15.501473] curl: header in: Server: UploadServer (\"Built on May 8 2015 11:09:05 (1431108545)\")\n[15.501482] curl: header in: Content-Type: text/html; charset=UTF-8\n[15.501490] curl: header in: Alternate-Protocol: 443:quic,p=1\n[15.501498] curl: header in: \n[15.501508] curl: info: Connection #0 to host www.googleapis.com left intact\n[15.502113] curl: info: Found bundle for host www.googleapis.com: 0x1f01180\n[15.502127] curl: info: Re-using existing connection! (#0) with host www.googleapis.com\n[15.502138] curl: info: Connected to www.googleapis.com (2607:f8b0:4009:80b::200a) port 443 (#0)\n[15.502178] curl: header out: PUT /resumable/upload/drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle&updateViewedDate=true&upload_id=AEnB2UofaGgwU2Z1mC-7Z9QcOD4-itnJisrAyIgMD90Cb02R_jq2cPGVM8_lcJ8NYJJqNJph_wl9BFWq78RzV2fgp8XxwzEcvg HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.15) gapi-ocaml/0.2.6/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nTransfer-Encoding: chunked\nAuthorization: Bearer ya29.egGhHXTnpC75tOlmlt6GFy46jjcQ1I0AKlXzIVpL-OLBD4bSRbtBnTM41z9EDweNPP_mIzj3DBtJyg\nIf-Match: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\"\nContent-Type: application/octet-stream\nContent-Range: bytes 0--1/0\nExpect: 100-continue\n[15.508684] curl: header in: HTTP/1.1 100 Continue\n[15.508727] curl: data out: 0\n[15.554004] curl: header in: HTTP/1.1 400 Bad Request\n[15.554022] curl: header in: Content-Length: 37\n[15.554034] curl: header in: Date: Thu, 21 May 2015 20:29:11 GMT\n[15.554049] curl: info: Server UploadServer (\"Built on May 8 2015 11:09:05 (1431108545)\") is not blacklisted\n[15.554061] curl: header in: Server: UploadServer (\"Built on May 8 2015 11:09:05 (1431108545)\")\n[15.554073] curl: header in: Content-Type: text/html; charset=UTF-8\n[15.554085] curl: header in: Alternate-Protocol: 443:quic,p=1\n[15.554097] curl: header in: \n[15.554112] curl: data in: Failed to parse Content-Range header.\n[15.554122] curl: info: Connection #0 to host www.googleapis.com left intact\n[15.558446] curl: info: Hostname was NOT found in DNS cache\n[15.562566] curl: info:   Trying 2607:f8b0:4009:80b::200a...\n[15.568650] curl: info: Connected to www.googleapis.com (2607:f8b0:4009:80b::200a) port 443 (#0)\n[15.580159] curl: info: found 173 certificates in /etc/ssl/certs/ca-certificates.crt\n[15.643683] curl: info:      server certificate verification OK\n[15.643871] curl: info:      common name: .storage.googleapis.com (matched)\n[15.643883] curl: info:      server certificate expiration date OK\n[15.643893] curl: info:      server certificate activation date OK\n[15.643910] curl: info:      certificate public key: RSA\n[15.643934] curl: info:      certificate version: #3\n[15.643979] curl: info:      subject: C=US,ST=California,L=Mountain View,O=Google Inc,CN=.storage.googleapis.com\n[15.643994] curl: info:      start date: Wed, 06 May 2015 09:59:24 GMT\n[15.644006] curl: info:      expire date: Tue, 04 Aug 2015 00:00:00 GMT\n[15.644034] curl: info:      issuer: C=US,O=Google Inc,CN=Google Internet Authority G2\n[15.644057] curl: info:      compression: NULL\n[15.644065] curl: info:      cipher: AES-128-GCM\n[15.644073] curl: info:      MAC: AEAD\n[15.644119] curl: header out: GET /drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.15) gapi-ocaml/0.2.6/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nAuthorization: Bearer ya29.egGhHXTnpC75tOlmlt6GFy46jjcQ1I0AKlXzIVpL-OLBD4bSRbtBnTM41z9EDweNPP_mIzj3DBtJyg\n[15.747022] curl: header in: HTTP/1.1 200 OK\n[15.747045] curl: header in: Expires: Thu, 21 May 2015 20:29:11 GMT\n[15.747057] curl: header in: Date: Thu, 21 May 2015 20:29:11 GMT\n[15.747070] curl: header in: Cache-Control: private, max-age=0, must-revalidate, no-transform\n[15.747082] curl: header in: ETag: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\"\n[15.747094] curl: header in: Vary: Origin\n[15.747105] curl: header in: Vary: X-Origin\n[15.747121] curl: header in: Content-Type: application/json; charset=UTF-8\n[15.747129] curl: header in: X-Content-Type-Options: nosniff\n[15.747137] curl: header in: X-Frame-Options: SAMEORIGIN\n[15.747145] curl: header in: X-XSS-Protection: 1; mode=block\n[15.747153] curl: header in: Content-Length: 1270\n[15.747163] curl: info: Server GSE is not blacklisted\n[15.747170] curl: header in: Server: GSE\n[15.747178] curl: header in: Alternate-Protocol: 443:quic,p=1\n[15.747186] curl: header in: \n[15.747216] curl: data in: {\n \"id\": \"0BzBx9fimjv-7ZUFqUk4teEEzTFU\",\n \"etag\": \"\\\"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\\\"\",\n \"alternateLink\": \"https://docs.google.com/a/wisc.edu/file/d/0BzBx9fimjv-7ZUFqUk4teEEzTFU/edit?usp=drivesdk\",\n \"title\": \"undo.bookmarks\",\n \"mimeType\": \"application/octet-stream\",\n \"labels\": {\n  \"starred\": false,\n  \"hidden\": false,\n  \"trashed\": false,\n  \"restricted\": false,\n  \"viewed\": true\n },\n \"createdDate\": \"2015-05-21T20:16:34.507Z\",\n \"modifiedDate\": \"2015-05-21T20:16:34.421Z\",\n \"lastViewedByMeDate\": \"2015-05-21T20:16:34.421Z\",\n \"parents\": [\n  {\n   \"kind\": \"drive#parentReference\",\n   \"id\": \"0BzBx9fimjv-7RU56QWVtaXZZSHM\",\n   \"selfLink\": \"https://www.googleapis.com/drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU/parents/0BzBx9fimjv-7RU56QWVtaXZZSHM\",\n   \"parentLink\": \"https://www.googleapis.com/drive/v2/files/0BzBx9fimjv-7RU56QWVtaXZZSHM\",\n   \"isRoot\": false\n  }\n ],\n \"downloadUrl\": \"https://doc-0k-90-docs.googleusercontent.com/docs/securesc/83ugdofc3ce5878s88akla24pqgrt7sb/jp6fdu9b1s6nlu9162h3ptokh5g4mt60/1432238400000/10252121649383171870/10252121649383171870/0BzBx9fimjv-7ZUFqUk4teEEzTFU?h=15376047860968316007&e=download&gd=true\",\n \"fileExtensio\n[15.747248] curl: data in: n\": \"bookmarks\",\n \"md5Checksum\": \"d41d8cd98f00b204e9800998ecf8427e\",\n \"fileSize\": \"0\",\n \"editable\": true\n}\n[15.747262] curl: info: Connection #0 to host www.googleapis.com left intact\n[15.747621] curl: info: Found bundle for host www.googleapis.com: 0x1f04450\n[15.747637] curl: info: Re-using existing connection! (#0) with host www.googleapis.com\n[15.747649] curl: info: Connected to www.googleapis.com (2607:f8b0:4009:80b::200a) port 443 (#0)\n[15.747690] curl: header out: PUT /resumable/upload/drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle&updateViewedDate=true HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.15) gapi-ocaml/0.2.6/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nTransfer-Encoding: chunked\nContent-Type: application/json\nAuthorization: Bearer ya29.egGhHXTnpC75tOlmlt6GFy46jjcQ1I0AKlXzIVpL-OLBD4bSRbtBnTM41z9EDweNPP_mIzj3DBtJyg\nIf-Match: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\"\nX-Upload-Content-Type: application/octet-stream\nX-Upload-Content-Length: 0\nExpect: 100-continue\n[15.753857] curl: header in: HTTP/1.1 100 Continue\n[15.753924] curl: data out: 430\n{\"alternateLink\":\"https://docs.google.com/a/wisc.edu/file/d/0BzBx9fimjv-7ZUFqUk4teEEzTFU/edit?usp=drivesdk\",\"createdDate\":\"2015-05-21T20:16:34.000Z\",\"downloadUrl\":\"https://doc-0k-90-docs.googleusercontent.com/docs/securesc/83ugdofc3ce5878s88akla24pqgrt7sb/jp6fdu9b1s6nlu9162h3ptokh5g4mt60/1432238400000/10252121649383171870/10252121649383171870/0BzBx9fimjv-7ZUFqUk4teEEzTFU?h=15376047860968316007&e=download&gd=true\",\"editable\":true,\"etag\":\"\\\"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\\\"\",\"fileExtension\":\"bookmarks\",\"id\":\"0BzBx9fimjv-7ZUFqUk4teEEzTFU\",\"labels\":{\"viewed\":true},\"lastViewedByMeDate\":\"2015-05-21T20:16:34.000Z\",\"md5Checksum\":\"d41d8cd98f00b204e9800998ecf8427e\",\"mimeType\":\"application/octet-stream\",\"modifiedDate\":\"2015-05-21T20:16:34.000Z\",\"parents\":[{\"id\":\"0BzBx9fimjv-7RU56QWVtaXZZSHM\",\"kind\":\"drive#parentReference\",\"parentLink\":\"https://www.googleapis.com/drive/v2/files/0BzBx9fimjv-7RU56QWVtaXZZSHM\",\"selfLink\":\"https://www.googleapis.com/drive/v2/files/0BzBx9fimjv-7ZUFqUk4\n teEEzTFU/parents/0BzBx9fimjv-7RU56QWVtaXZZSHM\"}],\"title\":\"undo.bookmarks\"}\n[15.753953] curl: data out: 0\n[15.949815] curl: header in: HTTP/1.1 200 OK\n[15.949837] curl: header in: Location: https://www.googleapis.com/resumable/upload/drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle&updateViewedDate=true&upload_id=AEnB2UpndX6Sz6OElwp6LiqPHfxXBILja_K0-EPfHE5aLNPa05sh5EB2kaBBFPtVsLplh-rUTwCbzhX2UQz1CtC9NdrU_DTrkA\n[15.949853] curl: header in: ETag: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/mLNL2XBVN-aH8ZKM7yhxWd_PaPg\"\n[15.949865] curl: header in: Vary: Origin\n[15.949877] curl: header in: Vary: X-Origin\n[15.949890] curl: header in: Cache-Control: no-cache, no-store, max-age=0, must-revalidate\n[15.949901] curl: header in: Pragma: no-cache\n[15.949917] curl: header in: Expires: Fri, 01 Jan 1990 00:00:00 GMT\n[15.949924] curl: header in: Date: Thu, 21 May 2015 20:29:11 GMT\n[15.949933] curl: header in: Content-Length: 0\n[15.949943] curl: info: Server UploadServer (\"Built on May 8 2015 11:09:05 (1431108545)\") is not blacklisted\n[15.949951] curl: header in: Server: UploadServer (\"Built on May 8 2015 11:09:05 (1431108545)\")\n[15.949959] curl: header in: Content-Type: text/html; charset=UTF-8\n[15.949968] curl: header in: Alternate-Protocol: 443:quic,p=1\n[15.949976] curl: header in: \n[15.949986] curl: info: Connection #0 to host www.googleapis.com left intact\n[15.959422] curl: info: Found bundle for host www.googleapis.com: 0x1f04450\n[15.959460] curl: info: Re-using existing connection! (#0) with host www.googleapis.com\n[15.959473] curl: info: Connected to www.googleapis.com (2607:f8b0:4009:80b::200a) port 443 (#0)\n[15.959556] curl: header out: PUT /resumable/upload/drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle&updateViewedDate=true&upload_id=AEnB2UpndX6Sz6OElwp6LiqPHfxXBILja_K0-EPfHE5aLNPa05sh5EB2kaBBFPtVsLplh-rUTwCbzhX2UQz1CtC9NdrU_DTrkA HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.15) gapi-ocaml/0.2.6/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nTransfer-Encoding: chunked\nAuthorization: Bearer ya29.egGhHXTnpC75tOlmlt6GFy46jjcQ1I0AKlXzIVpL-OLBD4bSRbtBnTM41z9EDweNPP_mIzj3DBtJyg\nIf-Match: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\"\nContent-Type: application/octet-stream\nContent-Range: bytes 0--1/0\nExpect: 100-continue\n[15.966031] curl: header in: HTTP/1.1 100 Continue\n[15.966074] curl: data out: 0\n[16.011234] curl: header in: HTTP/1.1 400 Bad Request\n[16.011251] curl: header in: Content-Length: 37\n[16.011264] curl: header in: Date: Thu, 21 May 2015 20:29:12 GMT\n[16.011278] curl: info: Server UploadServer (\"Built on May 8 2015 11:09:05 (1431108545)\") is not blacklisted\n[16.011290] curl: header in: Server: UploadServer (\"Built on May 8 2015 11:09:05 (1431108545)\")\n[16.011302] curl: header in: Content-Type: text/html; charset=UTF-8\n[16.011314] curl: header in: Alternate-Protocol: 443:quic,p=1\n[16.011326] curl: header in: \n[16.011340] curl: data in: Failed to parse Content-Range header.\n[16.011352] curl: info: Connection #0 to host www.googleapis.com left intact\n[16.015855] curl: info: Hostname was NOT found in DNS cache\n[16.019975] curl: info:   Trying 2607:f8b0:4009:80b::200a...\n[16.026035] curl: info: Connected to www.googleapis.com (2607:f8b0:4009:80b::200a) port 443 (#0)\n[16.037671] curl: info: found 173 certificates in /etc/ssl/certs/ca-certificates.crt\n[16.101336] curl: info:      server certificate verification OK\n[16.101525] curl: info:      common name: .storage.googleapis.com (matched)\n[16.101537] curl: info:      server certificate expiration date OK\n[16.101546] curl: info:      server certificate activation date OK\n[16.101563] curl: info:      certificate public key: RSA\n[16.101573] curl: info:      certificate version: #3\n[16.101618] curl: info:      subject: C=US,ST=California,L=Mountain View,O=Google Inc,CN=.storage.googleapis.com\n[16.101632] curl: info:      start date: Wed, 06 May 2015 09:59:24 GMT\n[16.101644] curl: info:      expire date: Tue, 04 Aug 2015 00:00:00 GMT\n[16.101673] curl: info:      issuer: C=US,O=Google Inc,CN=Google Internet Authority G2\n[16.101697] curl: info:      compression: NULL\n[16.101705] curl: info:      cipher: AES-128-GCM\n[16.101713] curl: info:      MAC: AEAD\n[16.101759] curl: header out: GET /drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.15) gapi-ocaml/0.2.6/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nAuthorization: Bearer ya29.egGhHXTnpC75tOlmlt6GFy46jjcQ1I0AKlXzIVpL-OLBD4bSRbtBnTM41z9EDweNPP_mIzj3DBtJyg\n[16.189891] curl: header in: HTTP/1.1 200 OK\n[16.189912] curl: header in: Expires: Thu, 21 May 2015 20:29:12 GMT\n[16.189924] curl: header in: Date: Thu, 21 May 2015 20:29:12 GMT\n[16.189937] curl: header in: Cache-Control: private, max-age=0, must-revalidate, no-transform\n[16.189949] curl: header in: ETag: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\"\n[16.189961] curl: header in: Vary: Origin\n[16.189972] curl: header in: Vary: X-Origin\n[16.189990] curl: header in: Content-Type: application/json; charset=UTF-8\n[16.189998] curl: header in: X-Content-Type-Options: nosniff\n[16.190006] curl: header in: X-Frame-Options: SAMEORIGIN\n[16.190014] curl: header in: X-XSS-Protection: 1; mode=block\n[16.190022] curl: header in: Content-Length: 1270\n[16.190032] curl: info: Server GSE is not blacklisted\n[16.190040] curl: header in: Server: GSE\n[16.190047] curl: header in: Alternate-Protocol: 443:quic,p=1\n[16.190056] curl: header in: \n[16.190244] curl: data in: {\n \"id\": \"0BzBx9fimjv-7ZUFqUk4teEEzTFU\",\n \"etag\": \"\\\"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\\\"\",\n \"alternateLink\": \"https://docs.google.com/a/wisc.edu/file/d/0BzBx9fimjv-7ZUFqUk4teEEzTFU/edit?usp=drivesdk\",\n \"title\": \"undo.bookmarks\",\n \"mimeType\": \"application/octet-stream\",\n \"labels\": {\n  \"starred\": false,\n  \"hidden\": false,\n  \"trashed\": false,\n  \"restricted\": false,\n  \"viewed\": true\n },\n \"createdDate\": \"2015-05-21T20:16:34.507Z\",\n \"modifiedDate\": \"2015-05-21T20:16:34.421Z\",\n \"lastViewedByMeDate\": \"2015-05-21T20:16:34.421Z\",\n \"parents\": [\n  {\n   \"kind\": \"drive#parentReference\",\n   \"id\": \"0BzBx9fimjv-7RU56QWVtaXZZSHM\",\n   \"selfLink\": \"https://www.googleapis.com/drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU/parents/0BzBx9fimjv-7RU56QWVtaXZZSHM\",\n   \"parentLink\": \"https://www.googleapis.com/drive/v2/files/0BzBx9fimjv-7RU56QWVtaXZZSHM\",\n   \"isRoot\": false\n  }\n ],\n \"downloadUrl\": \"https://doc-0k-90-docs.googleusercontent.com/docs/securesc/83ugdofc3ce5878s88akla24pqgrt7sb/jp6fdu9b1s6nlu9162h3ptokh5g4mt60/1432238400000/10252121649383171870/10252121649383171870/0BzBx9fimjv-7ZUFqUk4teEEzTFU?h=15376047860968316007&e=download&gd=true\",\n \"fileExtensio\n[16.190301] curl: data in: n\": \"bookmarks\",\n \"md5Checksum\": \"d41d8cd98f00b204e9800998ecf8427e\",\n \"fileSize\": \"0\",\n \"editable\": true\n}\n[16.190321] curl: info: Connection #0 to host www.googleapis.com left intact\n[16.190694] curl: info: Found bundle for host www.googleapis.com: 0x1f887b0\n[16.190709] curl: info: Re-using existing connection! (#0) with host www.googleapis.com\n[16.190721] curl: info: Connected to www.googleapis.com (2607:f8b0:4009:80b::200a) port 443 (#0)\n[16.190762] curl: header out: PUT /resumable/upload/drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle&updateViewedDate=true HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.15) gapi-ocaml/0.2.6/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nTransfer-Encoding: chunked\nContent-Type: application/json\nAuthorization: Bearer ya29.egGhHXTnpC75tOlmlt6GFy46jjcQ1I0AKlXzIVpL-OLBD4bSRbtBnTM41z9EDweNPP_mIzj3DBtJyg\nIf-Match: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\"\nX-Upload-Content-Type: application/octet-stream\nX-Upload-Content-Length: 0\nExpect: 100-continue\n[16.196972] curl: header in: HTTP/1.1 100 Continue\n[16.197036] curl: data out: 430\n{\"alternateLink\":\"https://docs.google.com/a/wisc.edu/file/d/0BzBx9fimjv-7ZUFqUk4teEEzTFU/edit?usp=drivesdk\",\"createdDate\":\"2015-05-21T20:16:34.000Z\",\"downloadUrl\":\"https://doc-0k-90-docs.googleusercontent.com/docs/securesc/83ugdofc3ce5878s88akla24pqgrt7sb/jp6fdu9b1s6nlu9162h3ptokh5g4mt60/1432238400000/10252121649383171870/10252121649383171870/0BzBx9fimjv-7ZUFqUk4teEEzTFU?h=15376047860968316007&e=download&gd=true\",\"editable\":true,\"etag\":\"\\\"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\\\"\",\"fileExtension\":\"bookmarks\",\"id\":\"0BzBx9fimjv-7ZUFqUk4teEEzTFU\",\"labels\":{\"viewed\":true},\"lastViewedByMeDate\":\"2015-05-21T20:16:34.000Z\",\"md5Checksum\":\"d41d8cd98f00b204e9800998ecf8427e\",\"mimeType\":\"application/octet-stream\",\"modifiedDate\":\"2015-05-21T20:16:34.000Z\",\"parents\":[{\"id\":\"0BzBx9fimjv-7RU56QWVtaXZZSHM\",\"kind\":\"drive#parentReference\",\"parentLink\":\"https://www.googleapis.com/drive/v2/files/0BzBx9fimjv-7RU56QWVtaXZZSHM\",\"selfLink\":\"https://www.googleapis.com/drive/v2/files/0BzBx9fimjv-7ZUFqUk4\n teEEzTFU/parents/0BzBx9fimjv-7RU56QWVtaXZZSHM\"}],\"title\":\"undo.bookmarks\"}\n[16.197066] curl: data out: 0\n[16.471183] curl: header in: HTTP/1.1 200 OK\n[16.471205] curl: header in: Location: https://www.googleapis.com/resumable/upload/drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle&updateViewedDate=true&upload_id=AEnB2UpLbmvYSLG-HiTWqwMis0xNWMUjvOxxXdiXcOJsMlcAdk5qww_UOxhDdN6ISyG8Og1jf-2AlE-Jd8QvStEbUYeYPJcGMw\n[16.471220] curl: header in: ETag: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/mLNL2XBVN-aH8ZKM7yhxWd_PaPg\"\n[16.471232] curl: header in: Vary: Origin\n[16.471244] curl: header in: Vary: X-Origin\n[16.471256] curl: header in: Cache-Control: no-cache, no-store, max-age=0, must-revalidate\n[16.471268] curl: header in: Pragma: no-cache\n[16.471283] curl: header in: Expires: Fri, 01 Jan 1990 00:00:00 GMT\n[16.471292] curl: header in: Date: Thu, 21 May 2015 20:29:12 GMT\n[16.471300] curl: header in: Content-Length: 0\n[16.471316] curl: info: Server UploadServer (\"Built on May 8 2015 11:09:05 (1431108545)\") is not blacklisted\n[16.471324] curl: header in: Server: UploadServer (\"Built on May 8 2015 11:09:05 (1431108545)\")\n[16.471333] curl: header in: Content-Type: text/html; charset=UTF-8\n[16.471342] curl: header in: Alternate-Protocol: 443:quic,p=1\n[16.471350] curl: header in: \n[16.471360] curl: info: Connection #0 to host www.googleapis.com left intact\n[16.471793] curl: info: Found bundle for host www.googleapis.com: 0x1f887b0\n[16.471806] curl: info: Re-using existing connection! (#0) with host www.googleapis.com\n[16.471818] curl: info: Connected to www.googleapis.com (2607:f8b0:4009:80b::200a) port 443 (#0)\n[16.471856] curl: header out: PUT /resumable/upload/drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle&updateViewedDate=true&upload_id=AEnB2UpLbmvYSLG-HiTWqwMis0xNWMUjvOxxXdiXcOJsMlcAdk5qww_UOxhDdN6ISyG8Og1jf-2AlE-Jd8QvStEbUYeYPJcGMw HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.15) gapi-ocaml/0.2.6/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nTransfer-Encoding: chunked\nAuthorization: Bearer ya29.egGhHXTnpC75tOlmlt6GFy46jjcQ1I0AKlXzIVpL-OLBD4bSRbtBnTM41z9EDweNPP_mIzj3DBtJyg\nIf-Match: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\"\nContent-Type: application/octet-stream\nContent-Range: bytes 0--1/0\nExpect: 100-continue\n[16.478010] curl: header in: HTTP/1.1 100 Continue\n[16.478047] curl: data out: 0\n[16.541513] curl: header in: HTTP/1.1 400 Bad Request\n[16.541532] curl: header in: Content-Length: 37\n[16.541544] curl: header in: Date: Thu, 21 May 2015 20:29:12 GMT\n[16.541559] curl: info: Server UploadServer (\"Built on May 8 2015 11:09:05 (1431108545)\") is not blacklisted\n[16.541570] curl: header in: Server: UploadServer (\"Built on May 8 2015 11:09:05 (1431108545)\")\n[16.541583] curl: header in: Content-Type: text/html; charset=UTF-8\n[16.541594] curl: header in: Alternate-Protocol: 443:quic,p=1\n[16.541606] curl: header in: \n[16.541617] curl: data in: Failed to parse Content-Range header.\n[16.541632] curl: info: Connection #0 to host www.googleapis.com left intact\n[16.546040] curl: info: Hostname was NOT found in DNS cache\n[16.550197] curl: info:   Trying 2607:f8b0:4009:80b::200a...\n[16.556356] curl: info: Connected to www.googleapis.com (2607:f8b0:4009:80b::200a) port 443 (#0)\n[16.573002] curl: info: found 173 certificates in /etc/ssl/certs/ca-certificates.crt\n[16.637052] curl: info:      server certificate verification OK\n[16.637271] curl: info:      common name: .storage.googleapis.com (matched)\n[16.637290] curl: info:      server certificate expiration date OK\n[16.637303] curl: info:      server certificate activation date OK\n[16.637321] curl: info:      certificate public key: RSA\n[16.637334] curl: info:      certificate version: #3\n[16.637379] curl: info:      subject: C=US,ST=California,L=Mountain View,O=Google Inc,CN=.storage.googleapis.com\n[16.637398] curl: info:      start date: Wed, 06 May 2015 09:59:24 GMT\n[16.637410] curl: info:      expire date: Tue, 04 Aug 2015 00:00:00 GMT\n[16.637439] curl: info:      issuer: C=US,O=Google Inc,CN=Google Internet Authority G2\n[16.637462] curl: info:      compression: NULL\n[16.637470] curl: info:      cipher: AES-128-GCM\n[16.637478] curl: info:      MAC: AEAD\n[16.637546] curl: header out: GET /drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.15) gapi-ocaml/0.2.6/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nAuthorization: Bearer ya29.egGhHXTnpC75tOlmlt6GFy46jjcQ1I0AKlXzIVpL-OLBD4bSRbtBnTM41z9EDweNPP_mIzj3DBtJyg\n[16.745945] curl: header in: HTTP/1.1 200 OK\n[16.746012] curl: header in: Expires: Thu, 21 May 2015 20:29:12 GMT\n[16.746022] curl: header in: Date: Thu, 21 May 2015 20:29:12 GMT\n[16.746031] curl: header in: Cache-Control: private, max-age=0, must-revalidate, no-transform\n[16.746039] curl: header in: ETag: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\"\n[16.746048] curl: header in: Vary: Origin\n[16.746056] curl: header in: Vary: X-Origin\n[16.746066] curl: header in: Content-Type: application/json; charset=UTF-8\n[16.746074] curl: header in: X-Content-Type-Options: nosniff\n[16.746082] curl: header in: X-Frame-Options: SAMEORIGIN\n[16.746090] curl: header in: X-XSS-Protection: 1; mode=block\n[16.746099] curl: header in: Content-Length: 1270\n[16.746110] curl: info: Server GSE is not blacklisted\n[16.746118] curl: header in: Server: GSE\n[16.746126] curl: header in: Alternate-Protocol: 443:quic,p=1\n[16.746134] curl: header in: \n[16.746167] curl: data in: {\n \"id\": \"0BzBx9fimjv-7ZUFqUk4teEEzTFU\",\n \"etag\": \"\\\"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\\\"\",\n \"alternateLink\": \"https://docs.google.com/a/wisc.edu/file/d/0BzBx9fimjv-7ZUFqUk4teEEzTFU/edit?usp=drivesdk\",\n \"title\": \"undo.bookmarks\",\n \"mimeType\": \"application/octet-stream\",\n \"labels\": {\n  \"starred\": false,\n  \"hidden\": false,\n  \"trashed\": false,\n  \"restricted\": false,\n  \"viewed\": true\n },\n \"createdDate\": \"2015-05-21T20:16:34.507Z\",\n \"modifiedDate\": \"2015-05-21T20:16:34.421Z\",\n \"lastViewedByMeDate\": \"2015-05-21T20:16:34.421Z\",\n \"parents\": [\n  {\n   \"kind\": \"drive#parentReference\",\n   \"id\": \"0BzBx9fimjv-7RU56QWVtaXZZSHM\",\n   \"selfLink\": \"https://www.googleapis.com/drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU/parents/0BzBx9fimjv-7RU56QWVtaXZZSHM\",\n   \"parentLink\": \"https://www.googleapis.com/drive/v2/files/0BzBx9fimjv-7RU56QWVtaXZZSHM\",\n   \"isRoot\": false\n  }\n ],\n \"downloadUrl\": \"https://doc-0k-90-docs.googleusercontent.com/docs/securesc/83ugdofc3ce5878s88akla24pqgrt7sb/jp6fdu9b1s6nlu9162h3ptokh5g4mt60/1432238400000/10252121649383171870/10252121649383171870/0BzBx9fimjv-7ZUFqUk4teEEzTFU?h=15376047860968316007&e=download&gd=true\",\n \"fileExtensio\n[16.746193] curl: data in: n\": \"bookmarks\",\n \"md5Checksum\": \"d41d8cd98f00b204e9800998ecf8427e\",\n \"fileSize\": \"0\",\n \"editable\": true\n}\n[16.746209] curl: info: Connection #0 to host www.googleapis.com left intact\n[16.746555] curl: info: Found bundle for host www.googleapis.com: 0x1f01b00\n[16.746570] curl: info: Re-using existing connection! (#0) with host www.googleapis.com\n[16.746583] curl: info: Connected to www.googleapis.com (2607:f8b0:4009:80b::200a) port 443 (#0)\n[16.746635] curl: header out: PUT /resumable/upload/drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle&updateViewedDate=true HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.15) gapi-ocaml/0.2.6/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nTransfer-Encoding: chunked\nContent-Type: application/json\nAuthorization: Bearer ya29.egGhHXTnpC75tOlmlt6GFy46jjcQ1I0AKlXzIVpL-OLBD4bSRbtBnTM41z9EDweNPP_mIzj3DBtJyg\nIf-Match: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\"\nX-Upload-Content-Type: application/octet-stream\nX-Upload-Content-Length: 0\nExpect: 100-continue\n[16.752768] curl: header in: HTTP/1.1 100 Continue\n[16.752821] curl: data out: 430\n{\"alternateLink\":\"https://docs.google.com/a/wisc.edu/file/d/0BzBx9fimjv-7ZUFqUk4teEEzTFU/edit?usp=drivesdk\",\"createdDate\":\"2015-05-21T20:16:34.000Z\",\"downloadUrl\":\"https://doc-0k-90-docs.googleusercontent.com/docs/securesc/83ugdofc3ce5878s88akla24pqgrt7sb/jp6fdu9b1s6nlu9162h3ptokh5g4mt60/1432238400000/10252121649383171870/10252121649383171870/0BzBx9fimjv-7ZUFqUk4teEEzTFU?h=15376047860968316007&e=download&gd=true\",\"editable\":true,\"etag\":\"\\\"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\\\"\",\"fileExtension\":\"bookmarks\",\"id\":\"0BzBx9fimjv-7ZUFqUk4teEEzTFU\",\"labels\":{\"viewed\":true},\"lastViewedByMeDate\":\"2015-05-21T20:16:34.000Z\",\"md5Checksum\":\"d41d8cd98f00b204e9800998ecf8427e\",\"mimeType\":\"application/octet-stream\",\"modifiedDate\":\"2015-05-21T20:16:34.000Z\",\"parents\":[{\"id\":\"0BzBx9fimjv-7RU56QWVtaXZZSHM\",\"kind\":\"drive#parentReference\",\"parentLink\":\"https://www.googleapis.com/drive/v2/files/0BzBx9fimjv-7RU56QWVtaXZZSHM\",\"selfLink\":\"https://www.googleapis.com/drive/v2/files/0BzBx9fimjv-7ZUFqUk4\n teEEzTFU/parents/0BzBx9fimjv-7RU56QWVtaXZZSHM\"}],\"title\":\"undo.bookmarks\"}\n[16.752857] curl: data out: 0\n[16.970172] curl: header in: HTTP/1.1 200 OK\n[16.970233] curl: header in: Location: https://www.googleapis.com/resumable/upload/drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle&updateViewedDate=true&upload_id=AEnB2UqUOxta-wJExxLdMWp7aBW_O-WVwUvqd9fsm1NZzXrbRkECsghpMchGaNANJTQN58TrmjIJSxdQErmH6ttobD_WCAKUiA\n[16.970245] curl: header in: ETag: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/mLNL2XBVN-aH8ZKM7yhxWd_PaPg\"\n[16.970254] curl: header in: Vary: Origin\n[16.970262] curl: header in: Vary: X-Origin\n[16.970271] curl: header in: Cache-Control: no-cache, no-store, max-age=0, must-revalidate\n[16.970279] curl: header in: Pragma: no-cache\n[16.970288] curl: header in: Expires: Fri, 01 Jan 1990 00:00:00 GMT\n[16.970296] curl: header in: Date: Thu, 21 May 2015 20:29:12 GMT\n[16.970305] curl: header in: Content-Length: 0\n[16.970317] curl: info: Server UploadServer (\"Built on May 8 2015 11:09:05 (1431108545)\") is not blacklisted\n[16.970325] curl: header in: Server: UploadServer (\"Built on May 8 2015 11:09:05 (1431108545)\")\n[16.970334] curl: header in: Content-Type: text/html; charset=UTF-8\n[16.970342] curl: header in: Alternate-Protocol: 443:quic,p=1\n[16.970350] curl: header in: \n[16.970364] curl: info: Connection #0 to host www.googleapis.com left intact\n[16.980425] curl: info: Found bundle for host www.googleapis.com: 0x1f01b00\n[16.980468] curl: info: Re-using existing connection! (#0) with host www.googleapis.com\n[16.980481] curl: info: Connected to www.googleapis.com (2607:f8b0:4009:80b::200a) port 443 (#0)\n[16.980567] curl: header out: PUT /resumable/upload/drive/v2/files/0BzBx9fimjv-7ZUFqUk4teEEzTFU?fields=alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle&updateViewedDate=true&upload_id=AEnB2UqUOxta-wJExxLdMWp7aBW_O-WVwUvqd9fsm1NZzXrbRkECsghpMchGaNANJTQN58TrmjIJSxdQErmH6ttobD_WCAKUiA HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.15) gapi-ocaml/0.2.6/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nTransfer-Encoding: chunked\nAuthorization: Bearer ya29.egGhHXTnpC75tOlmlt6GFy46jjcQ1I0AKlXzIVpL-OLBD4bSRbtBnTM41z9EDweNPP_mIzj3DBtJyg\nIf-Match: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/MTQzMjIzOTM5NDQyMQ\"\nContent-Type: application/octet-stream\nContent-Range: bytes 0--1/0\nExpect: 100-continue\n[16.988254] curl: header in: HTTP/1.1 100 Continue\n[16.988300] curl: data out: 0\n[17.081944] curl: header in: HTTP/1.1 400 Bad Request\n[17.081971] curl: header in: Content-Length: 37\n[17.081984] curl: header in: Date: Thu, 21 May 2015 20:29:13 GMT\n[17.081999] curl: info: Server UploadServer (\"Built on May 8 2015 11:09:05 (1431108545)\") is not blacklisted\n[17.082011] curl: header in: Server: UploadServer (\"Built on May 8 2015 11:09:05 (1431108545)\")\n[17.082023] curl: header in: Content-Type: text/html; charset=UTF-8\n[17.082038] curl: header in: Alternate-Protocol: 443:quic,p=1\n[17.082046] curl: header in: \n[17.082054] curl: data in: Failed to parse Content-Range header.\n[17.082068] curl: info: Connection #0 to host www.googleapis.com left intact\n[30.276427] curl: info: Hostname was NOT found in DNS cache\n[30.280540] curl: info:   Trying 2607:f8b0:4009:80b::200a...\n[30.286664] curl: info: Connected to www.googleapis.com (2607:f8b0:4009:80b::200a) port 443 (#0)\n[30.298830] curl: info: found 173 certificates in /etc/ssl/certs/ca-certificates.crt\n[30.360784] curl: info:      server certificate verification OK\n[30.360973] curl: info:      common name: .storage.googleapis.com (matched)\n[30.360986] curl: info:      server certificate expiration date OK\n[30.360996] curl: info:      server certificate activation date OK\n[30.361012] curl: info:      certificate public key: RSA\n[30.361021] curl: info:      certificate version: #3\n[30.361066] curl: info:      subject: C=US,ST=California,L=Mountain View,O=Google Inc,CN=.storage.googleapis.com\n[30.361080] curl: info:      start date: Wed, 06 May 2015 09:59:24 GMT\n[30.361092] curl: info:      expire date: Tue, 04 Aug 2015 00:00:00 GMT\n[30.361121] curl: info:      issuer: C=US,O=Google Inc,CN=Google Internet Authority G2\n[30.361143] curl: info:      compression: NULL\n[30.361151] curl: info:      cipher: AES-128-GCM\n[30.361159] curl: info:      MAC: AEAD\n[30.361206] curl: header out: GET /drive/v2/about?fields=etag%2ClargestChangeId%2Cname%2CpermissionId%2CquotaBytesTotal%2CquotaBytesUsed%2CremainingChangeIds%2CrootFolderId&maxChangeIdCount=500&startChangeId=548654 HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.15) gapi-ocaml/0.2.6/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nAuthorization: Bearer ya29.egGhHXTnpC75tOlmlt6GFy46jjcQ1I0AKlXzIVpL-OLBD4bSRbtBnTM41z9EDweNPP_mIzj3DBtJyg\nIf-None-Match: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/uYExjLLZXNyrHOenUJcN1OYeUkI\"\n[31.019425] curl: header in: HTTP/1.1 200 OK\n[31.019448] curl: header in: Expires: Thu, 21 May 2015 20:29:27 GMT\n[31.019460] curl: header in: Date: Thu, 21 May 2015 20:29:27 GMT\n[31.019473] curl: header in: Cache-Control: private, max-age=0, must-revalidate, no-transform\n[31.019485] curl: header in: ETag: \"ULTBkNKmycdCqkp_rFmHqVG1eVs/6N1dTs-UpiecqsBgYz0JNoazdgU\"\n[31.019497] curl: header in: Vary: Origin\n[31.019513] curl: header in: Vary: X-Origin\n[31.019523] curl: header in: Content-Type: application/json; charset=UTF-8\n[31.019531] curl: header in: X-Content-Type-Options: nosniff\n[31.019540] curl: header in: X-Frame-Options: SAMEORIGIN\n[31.019547] curl: header in: X-XSS-Protection: 1; mode=block\n[31.019556] curl: header in: Content-Length: 309\n[31.019565] curl: info: Server GSE is not blacklisted\n[31.019573] curl: header in: Server: GSE\n[31.019581] curl: header in: Alternate-Protocol: 443:quic,p=1\n[31.019589] curl: header in: \n[31.019597] curl: data in: {\n \"etag\": \"\\\"ULTBkNKmycdCqkp_rFmHqVG1eVs/6N1dTs-UpiecqsBgYz0JNoazdgU\\\"\",\n \"name\": \"CHAD SEYS\",\n \"quotaBytesTotal\": \"11122790475615\",\n \"quotaBytesUsed\": \"127674197855\",\n \"largestChangeId\": \"548655\",\n \"remainingChangeIds\": \"1\",\n \"rootFolderId\": \"0ADBx9fimjv-7Uk9PVA\",\n \"permissionId\": \"10252121649383171870\"\n}\n[31.019613] curl: info: Connection #0 to host www.googleapis.com left intact\n[31.119227] curl: info: Hostname was NOT found in DNS cache\n[31.123355] curl: info:   Trying 2607:f8b0:4009:80b::200a...\n[31.129460] curl: info: Connected to www.googleapis.com (2607:f8b0:4009:80b::200a) port 443 (#0)\n[31.141167] curl: info: found 173 certificates in /etc/ssl/certs/ca-certificates.crt\n[31.203409] curl: info:      server certificate verification OK\n[31.203597] curl: info:      common name: .storage.googleapis.com (matched)\n[31.203609] curl: info:      server certificate expiration date OK\n[31.203619] curl: info:      server certificate activation date OK\n[31.203635] curl: info:      certificate public key: RSA\n[31.203645] curl: info:      certificate version: #3\n[31.203688] curl: info:      subject: C=US,ST=California,L=Mountain View,O=Google Inc,CN=.storage.googleapis.com\n[31.203702] curl: info:      start date: Wed, 06 May 2015 09:59:24 GMT\n[31.203713] curl: info:      expire date: Tue, 04 Aug 2015 00:00:00 GMT\n[31.203741] curl: info:      issuer: C=US,O=Google Inc,CN=Google Internet Authority G2\n[31.203764] curl: info:      compression: NULL\n[31.203772] curl: info:      cipher: AES-128-GCM\n[31.203780] curl: info:      MAC: AEAD\n[31.203827] curl: header out: GET /drive/v2/changes?fields=items%28deleted%2Cfile%28alternateLink%2CcreatedDate%2CdownloadUrl%2Ceditable%2Cetag%2CexplicitlyTrashed%2CexportLinks%2CfileExtension%2CfileSize%2Cid%2Clabels%2ClastViewedByMeDate%2Cmd5Checksum%2CmimeType%2CmodifiedDate%2Cparents%2Ctitle%29%2CfileId%29%2CnextPageToken&startChangeId=548654 HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.15) gapi-ocaml/0.2.6/Unix\nHost: www.googleapis.com\nAccept: /\nAccept-Encoding: identity\nAuthorization: Bearer ya29.egGhHXTnpC75tOlmlt6GFy46jjcQ1I0AKlXzIVpL-OLBD4bSRbtBnTM41z9EDweNPP_mIzj3DBtJyg\n. On Thursday, May 21, 2015 23:46:07 Alessandro Strada wrote:\n\nPerhaps I'm getting a 400 response because I try to upload a 0-byte file. I\nwill do a couple of tests. Thanks for reporting this issue.\n\nHi Alessandro,\n    My experience was that files greater than 0 bytes also had this problem.\nThe problem seems very weird.  a) The reason why rsync wants to update the file \nis b/c of the time (sizes are the same). b) rsync only fails if it is \nencountering the file as part of a recursion.  If file specified explicitly then \nthe rsync succeeds and does not try to transfer the file again.\n    If I have time today I'll try to reproduce and 'stat' the file before and \nafter rsync succeeds.\nThanks for checking,\nChad.\n. ",
    "gogeccc": "The same happens when you upload a file using Google Drive web service.\nI guess that's the reason why they have 'New' instead of 'Upload' button :-D\n. ",
    "jjordanthesailor": "Sounds great!\nOn Tue, Jun 16, 2015, 12:15 PM Alessandro Strada notifications@github.com\nwrote:\n\nI think I will add a configuration option per file-type (e.g.\nspreadsheet_icon) so that you can specify any icon you want: a system\nicon or a custom icon (that you can put in ~/.local/share/icons/...).\nHope that works for you.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/126#issuecomment-112533324\n.\n. Wow, Thanks for the quick response.  I'm really lazy and don't have a\nsystem handy with a build environment set up, how often do you usually\nupdate the PPA?\n\nThanks again,\n-j\nOn Tue, Jun 16, 2015 at 1:22 PM Alessandro Strada notifications@github.com\nwrote:\n\nShould be available in version 0.5.17. Remember to clear the cache after\nchanging icons in config file.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/126#issuecomment-112557511\n.\n. Yes, it is updated, I should have taken a look before I asked, sorry.\n\nOn Tue, Jun 16, 2015 at 1:38 PM Alessandro Strada notifications@github.com\nwrote:\n\nI usually upload the .debs as I publish a new version. I think the PPA\nshould already be updated.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/126#issuecomment-112562051\n.\n. 5.17 Love it, beautiful, fantastic!  Got to spread the word on this awesome\nsoftware sent a tip to OMG Chrome to do an article on it.  Don't know if\nthey will.  I have Peppermint 5 on a couple of systems and just started\nplaying with Chromixium both of those projects should include\ngoogle-drive-ocamlfuse in their installations.   This was really all that\nwas missing to allow a Linux installation to do everything a ChromeBook can\nand still have the power of Linux when you want or need it.\n\nGrazie mille. Andare al piano di sotto per abbracciare il mio 1992 Alfa\nRomeo Spider Veloce in tuo onore.  (Google translate, my 1 semester of\nItalian is pretty much gone.)\n-j\nOn Tue, Jun 16, 2015 at 1:40 PM Alessandro Strada notifications@github.com\nwrote:\n\nDon't worry. No need to apologize. [image: :smile:]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/126#issuecomment-112563106\n.\n. \n",
    "aranc": "libgcrypt11 version is 1.5.3-2ubuntu4.2\ncould be a duplicate of https://github.com/astrada/google-drive-ocamlfuse/issues/106\n. ",
    "stickenhoffen": "I tested with the ( -m ) option, and the download speed maxes out at 400KiB/s.\n. Thanks for getting back, I tried pretty much all combinations of options.\nSince the files download very quickly when stream is switched off, is there anyway to use the partial file from the cache while it is still downloading?\n. Thanks. I tried recompiling the Fuse kernel module and setting FUSE_MAX_PAGES_PER_REQ to higher than 32, but I still get the same chunk size on download:\n..\nStream resource (id=147,offset=34701312,finish=34832383,length=131072)...done\nStream resource (id=147,offset=34832384,finish=34963455,length=131072)...done\n...\nStumped for now.\nCheers.\n. Thanks, yes I already saw that, very helpful.\nI checked a few other Fuse packages and don't seem to have the same restriction, is it possible it could be related to OCaml?\nAlso, if I use the direct_io fuse parameter, it drops even further to 16384 sized chunks.\n. I can help test here if you like, let me know what you need.\n. Hi, I built a new kernel with:\n#define FUSE_MAX_PAGES_PER_REQ 256\nAnd a libfuse2 with:\n#define MIN_BUFSIZE 0x101000\nSame results. Not sure else where to look.\nEdit: Also, fuse is built as a module; I read that was necessary somewhere else.\n. Sorry, I just re-read that last post, and it sounds a little negative! :)\nI'm going to tinker with as many things with Fuse as I can, will get back with my findings.\nThanks.\n. I finally got back to this, and built a new kernel, increasing FUSE_MAX_PAGES_PER_REQ to 2560 and VM_MAX_READAHEAD 10240, and reached streaming speeds of around 2MiB/s, so we're on the right track. I will increase again and rebuild, and let you know how it goes.\nEdit: Bytes not bits.\n. No luck increasing the values, is it possible there's something in ocaml-fuse that can be tweaked? Something seems to be capping the upper limit at 2MB chunks.\nIt's not all bad news though, with the -m switch, it's performing very nicely for me! :)\nI have a couple of questions.\nIf I use streaming, the cache.db increases, is this caching directory structure and file size etc.?\nIf so, what is the TTL of the cache, and/or file/directory listings?\nIf I mount read only, and add a file directly to Google Drive, will the new file be picked up instantly? If not, how long will it take for the file to appear?\nIf I use -m, what's the maximum threads I can reach?\nThanks so much.\n. > I don't think so. Did you also modify MIN_BUFSIZE (https://github.com/libfuse/libfuse/blob/6adcb719a933a31013c73fda8e0ccb0e13b45e58/lib/fuse_kern_chan.c#L86) in libfuse?\nYes, I increased substantially but it didn't seem to make any difference at all.\n. By the way, I was hesitant to use the -m switch, but it is really quite stable for me. Also, debug seems to kill it eventually, the curl log was >1GB so I assume it's my crappy dev box, but I plan to test on a system with more memory and SSD in the next couple of days, so can pass you logs.\n. @iplor It's hard to say with any accuracy, I would say I can reach around 50-60mbit/s. I also tried a quick test with seven simultaneous video streams, and it seems to cope nicely. I'll try a few more tests over the weekend.\n. Hi, I've been travelling so haven't had much time to look at this. Increasing the numbers didn't seem to increase performance that much for me after a point, but I'm in Singapore, perhaps I'm just hitting the limits with Google.\nPerhaps if I build a new stock Ubuntu kernel with the changes, someone can install and help test?\n. Sorry for the delay, been so busy with work. Packages for Ubuntu 16.04 x86_64 are here:\nhttps://gofile.io/?id=J6Zb6h\nI've also found that using the following command line parameters appears to work well in multithreaded mode:\ngoogle-drive-ocamlfuse -verbose -m -o allow_other -o sync_read drive\n. Curious, how fast is your Internet there?\n. This is awesome, thank you so much!. I'm pretty sure the issue with mounts dropping is the multithreadness.\nWith a single thread it's smooth sailing, but of course doesn't perform the same.. I think I just invented a new word.. ",
    "dellipse": "I would also like to see an improvement in streaming large video files.  What kind of performance tuning options would you suggest?  I am aware of many others looking for this as well, so if we are able to come up with a working solution, I would be happy to create a Wiki page with the recommended options.\n. ",
    "zenjabba": "I'm finding this problem. When I use the cache, I can get 1GB/s transfers, but when I tell it to stream, I get no more than 400Kib/s\nAny thoughts?\n. I have not been able to increase the block size but I know I can get >1gb/s from Google Drive when I copy files down from it to this machine, so it should in theory be wickedly fast.\n\nOn 13 Mar 2016, at 5:04 PM, iplor notifications@github.com wrote:\nI look forward to hearing the results of the testing. The very low streaming speed is killing my use-case, too.\nIt seems to me that Google drive is at its slowest when dealing many small requests. For those of us trying to stream large media files, I wonder if increasing the blocksize up into the megabyte range might be more efficient?\n\u2014\nReply to this email directly or view it on GitHub https://github.com/astrada/google-drive-ocamlfuse/issues/128#issuecomment-196054372.\n. I\u2019m happy to recompile the kernel\u2026 I\u2019ll setup a test machine where we can setup this if it helps.\nOn 15 Mar 2016, at 5:43 PM, Alessandro Strada notifications@github.com wrote:\nUnfortunately, it seems that getting a larger block size requires a kernel recompilation (see http://comments.gmane.org/gmane.comp.file-systems.fuse.devel/14335 http://comments.gmane.org/gmane.comp.file-systems.fuse.devel/14335). So I don't think it's a viable solution. I think I have to implement an in-memory buffer, but it's not easy because it needs to support multi-threading mode (so concurrent access, out of order reads/writes), and there is the risk of using too much memory, so it needs some policy to free resources.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub https://github.com/astrada/google-drive-ocamlfuse/issues/128#issuecomment-197036431\n. email me zenjabba@gmail.com for access to test system\n. I\u2019m planning on testing it today, I hope I see such amazing throughput :)\nOn 25 Nov 2016, at 2:22 PM, iplor notifications@github.com wrote:\nThanks so much for doing this. So far, it's looking really encouraging. I'm seeing smooth playback of large videos, and the available bandwidth being well-utilised. I'm still messing around with the config options, and I'll report back after using it for a while.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub https://github.com/astrada/google-drive-ocamlfuse/issues/128#issuecomment-263013474, or mute the thread https://github.com/notifications/unsubscribe-auth/AApfuPY6uxC3dcreOxh4ZANZ2SFHfnftks5rBzVpgaJpZM4FIUPN.\n\n\n. Worked great under Ubuntu, now trying to compile for Centos and having many problems getting the beta edition to be found.\n\nOn 25 Nov 2016, at 2:22 PM, Stephen Thompson zenjabba@gmail.com wrote:\nI\u2019m planning on testing it today, I hope I see such amazing throughput :)\n\nOn 25 Nov 2016, at 2:22 PM, iplor notifications@github.com> wrote:\nThanks so much for doing this. So far, it's looking really encouraging. I'm seeing smooth playback of large videos, and the available bandwidth being well-utilised. I'm still messing around with the config options, and I'll report back after using it for a while.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub https://github.com/astrada/google-drive-ocamlfuse/issues/128#issuecomment-263013474, or mute the thread https://github.com/notifications/unsubscribe-auth/AApfuPY6uxC3dcreOxh4ZANZ2SFHfnftks5rBzVpgaJpZM4FIUPN.\n\n\n. I\u2019m getting this error with the beta version\n[43.522596] TID=202: END: Checking MD5 checksum (path=/Backups/plexpy.log, cache path=/root/.gdfuse/default/cache/0B61rZZCN5Xw1VXNWaEUtYW9qNzA, hash=e7904832b3b44acdcd713ef287fc4d9f): Computed MD5 checksum: 02c6e6c047a2f9a4bd68efb3ac637ca6\n[43.522610] TID=202: BEGIN: Downloading resource (id=86) to /root/.gdfuse/default/cache/0B61rZZCN5Xw1VXNWaEUtYW9qNzA\n[43.522617] TID=202: BEGIN: Updating cache size (delta=18981) in db\n[43.526199] TID=202: END: Updating cache size (new size=78467) in db\n[43.526217] TID=202: BEGIN: Updating resource state in db (id=86, state=Downloading)\n[43.529946] TID=202: END: Updating resource state in db (id=86)\n[44.204101] TID=202: BEGIN: Updating resource state in db (id=86, state=ToDownload)\n[44.208368] TID=202: END: Updating resource state in db (id=86)\n[44.208659] TID=202: Error during request: hd\n[44.208681] TID=202: Giving up\n[44.208694] TID=202: Exception:Failure(\"hd\u201d)\n\nOn 29 Nov 2016, at 2:59 AM, Alessandro Strada notifications@github.com wrote:\nWith OPAM, you can compile the beta version like this:\nopam pin add google-drive-ocamlfuse https://github.com/astrada/google-drive-ocamlfuse.git#beta\nopam upgrade google-drive-ocamlfuse\nThen, to restore the standard version:\nopam pin remove google-drive-ocamlfuse\nopam upgrade google-drive-ocamlfuse\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub https://github.com/astrada/google-drive-ocamlfuse/issues/128#issuecomment-263501559, or mute the thread https://github.com/notifications/unsubscribe-auth/AApfuHxSEOBmjn51zzK4LeJ7H8bCcg28ks5rC9tVgaJpZM4FIUPN.\n\n\n. ",
    "jant90": "Very interesting thread, so how is the solution working out? And will it make it into the main branch of google-drive-ocamlfuse?\nAlso, not sure if it helps as I'm no developer, but acd_cli can stream files at high speed (~30MB/s), so that's already about 100 times faster. Not sure if that's a FUSE speedlimit or if the connection is simply saturated but maybe it's worth looking into their implementation/solution?\n. I'm running Manjaro (Arch based distro) and curl and libcurl are up to date and using OpenSSL and I installed google-drive-ocamlfuse 0.6.17 through the AUR and I too see a memory leak.\nRight after mounting memory usage of google-drive-ocamlfuse is usually only around 15MB. However when I start streaming a large file memory usage increases to around 130-180MB, and even though that's more than the limit I've configured that is perfectly acceptable for me if that memory would free up as soon as the file has been read (large file stream ends).\nSo I tried reading 4 files of 1GB from the mountpoint simultaneously and I saw memory usage increasing to over 500MB. And when the files were fully read memory usage stayed at that 500MB. The only way to get memory back down was to unmount and mount again. While still mounted (with memory usage at around 500MB I read 4 different large files simultaneously again, this time the increase in memory wasn't that much, it increased to around 600MB. And in fact I even noticed a few small drops in memory while the files were read (e.g. 550MB, 540MB, 560MB) indicating that the internal memory cleanup is doing something I guess.\nAfter a while (I usually check my system a day later or something) I often find google-drive-ocamlfuse's memory usage at 1100-2100MB.\nSo is there anything I can try to reduce memory usage by google-drive-ocamlfuse?\nI use these custom settings for google-drive-ocamlfuse:\nmax_memory_cache_size=78643200 # 75MB\nmemory_buffer_size=7864320 # 7.5MB\nread_ahead_buffers=1\nstream_large_files=true\nSome information about my system:\n$ curl -V\ncurl 7.54.0 (x86_64-pc-linux-gnu) libcurl/7.54.0 OpenSSL/1.1.0f zlib/1.2.11 libpsl/0.17.0 (+libicu/59.1) libssh2/1.8.0 nghttp2/1.22.0\n$ ldd /usr/bin/google-drive-ocamlfuse\n        [...]\n        libcurl.so.4 => /usr/lib/libcurl.so.4 (0x00007fe9c15ec000)\n        [...]\n        libssl.so.1.1 => /usr/lib/libssl.so.1.1 (0x00007fe9c001d000)\n$ ldd /usr/lib/libcurl.so.4\n        [...]\n        libssl.so.1.1 => /usr/lib/libssl.so.1.1 (0x00007f0abfdc6000). A few days ago I updated to 0.6.20 beta (from 0.6.17 stable) and while it may be too early to tell I do believe I see huge improvements in memory usage, awesome! No more than 250MB RAM usage so far. Lets see if it can get through the week without a mount drop or error of some sort, I have a good feeling that it will with all the fixes done in the beta's. Thank you @astrada!. ",
    "seiferma": "Thanks for sharing your experience with us @stickenhoffen. Did you get any new results or even improvements?\n. I would be happy to support you in testing your approach. You can share the compiled kernel with us or provide some short instructions on how to change and compile the kernel.\n. ",
    "joemarshalljm": "I'm new to all this, can someone explain what I need to do with the Packages for Ubuntu in order to see the increased speed for video?\n. I worked it out - need to mount from the Plex username. Rookie mistake!\n. ",
    "tchey290": "@iplor Can you share you config/setup that is letting you stream full bluray backups without any issues? I can't seem to stream even smaller video files.. @animosity22 how do you have yours configured that it works flawlessly? So far I have had nothing but issues trying to use this with plex.. @animosity22  Ok, and what is your mounting command look like?. @animosity22  Thanks for those settings! I'm not sure which ones did it, but now I am actually able to stream plex! Thank you!. ",
    "AndreTheHunter": "I still get an error even when I set the CPPFLAGS (notice the -DFUSE_USE_VERSION=26 -I/usr/local/include/osxfuse).\n```\nThe following actions will be performed:\n  \u2217  install ocamlfuse 2.7.1-cvs2\n=-=- Gathering sources =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  \ud83d\udc2b \n[ocamlfuse] Archive in cache\n=-=- Processing actions -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  \ud83d\udc2b \n[ERROR] The compilation of ocamlfuse failed at \"make -C lib INCDIRS=/Users/andre.dejager/.opam/system/lib/camlidl\".\nProcessing  1/1: [ocamlfuse: ocamlfind remove]\n=== ERROR while installing ocamlfuse.2.7.1-cvs2 ==============================\nopam-version 1.2.2\nos           darwin\ncommand      make -C lib INCDIRS=/Users/andre.dejager/.opam/system/lib/camlidl\npath         /Users/andre.dejager/.opam/system/build/ocamlfuse.2.7.1-cvs2\ncompiler     system (4.02.3)\nexit-code    2\nenv-file     /Users/andre.dejager/.opam/system/build/ocamlfuse.2.7.1-cvs2/ocamlfuse-24062-6d07ae.env\nstdout-file  /Users/andre.dejager/.opam/system/build/ocamlfuse.2.7.1-cvs2/ocamlfuse-24062-6d07ae.out\nstderr-file  /Users/andre.dejager/.opam/system/build/ocamlfuse.2.7.1-cvs2/ocamlfuse-24062-6d07ae.err\nstdout\n[...]\nmaking ._d/Fuse.d from Fuse.ml\nmaking ._d/Fuse_lib.d from Fuse_lib.ml\nmaking ._d/Unix_util.d from Unix_util.ml\nmaking ._d/Result.d from Result.ml\nocamlc -c -cc \"cc\" -ccopt \"-fPIC -D_FILE_OFFSET_BITS=64 -I. -pthread \\\n-DPIC -DNATIVE_CODE -DFUSE_USE_VERSION=26 -I/usr/local/include/osxfuse  \\\n-I/Users/andre.dejager/.opam/system/lib/camlidl  -o Fuse_bindings_stubs.o \" Fuse_bindings_stubs.c\nocamlc -c -cc \"cc\" -ccopt \"-fPIC -D_FILE_OFFSET_BITS=64 -I. -pthread \\\n-DPIC -DNATIVE_CODE -DFUSE_USE_VERSION=26 -I/usr/local/include/osxfuse  \\\n-I/Users/andre.dejager/.opam/system/lib/camlidl  -o Unix_util_stubs.o \" Unix_util_stubs.c\nstderr\nUnix_util_stubs.c:30:10: fatal error: 'sys/vfs.h' file not found\n#include \n^\n1 error generated.\nmake[1]: *** [Unix_util_stubs.o] Error 2\nmake: *** [native-code-library] Error 2\n=-=- Error report -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  \ud83d\udc2b \nThe following actions failed\n  \u2217  install ocamlfuse 2.7.1-cvs2\nNo changes have been performed\n=-=- ocamlfuse.2.7.1-cvs2 troobleshooting -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  \ud83d\udc2b \n=> This package relies on external (system) dependencies that may be missing. `opam depext ocamlfuse.2.7.1-cvs2' may help you find the correct installation for your system.\n```\n. ",
    "m3i58": "Thank you for your answer. Now I let it run for over 120 minutes, but it still is refreshing database.\n\n0.000175] TID=0: Setting up default filesystem...\n[0.000208] TID=0: Loading configuration from /home/user/.gdfuse/default/config...done\nSaving configuration in /home/user/.gdfuse/default/config...done\n[0.000452] TID=0: Loading application state from /home/user/.gdfuse/default/state...done\nCurrent version: 0.5.17\nSetting up cache db...done\nSetting up CURL...done\nRefresh token already present.\n[0.002733] TID=0: Starting filesystem /home/user/drive\n[0.006386] TID=0: init_filesystem\n[1117.753339] TID=0: getattr /\nLoading metadata from db...not found\nRefreshing metadata...\n\nGranted the Google Drive content is big (the content is less then 2TB). Still it takes a lot of time...\n. Ahhh, Drive API was off :)\nMaybe it woud be a good idea to put it into the guide for less experienced users like myself?\n. Thank you for your reply.\nBut can't it be uploaded directly from the source file, without caching it?\n. I very much appreciate your reply. Hopefully Google API will progress.\n. ",
    "Styxe": "I will try it, thank you for the reply and the software anyway.\nGood luck for the future\n. ",
    "neilcsmith-net": "No, I'm not.  Is that something that changed behaviour?  Can that be set in config?\n. Well, thought I'd try that, but sceptical that was the cause because I do clear the cache regularly (different problem with a file not being updated).  Anyway, even before trying that, it's now working fine. :confused: I assume that the update may have been a coincidence and this was actually some issue with the network connection with Drive.  If so, slightly concerning that it rendered the system unusable, even requiring REISUB at times.\nHowever, as it's currently working I'll close this and keep an eye on it.\nMany thanks for the great software and help!\n. ",
    "oleid": "I'm not at the same machine at the moment,  but I installed google-drive-ocamlfuse there, too. This time, it's a Gentoo based system. There, I have the same problem, but the program dies with SIGILL instead SIGSEGV.\nNevertheless, on this machine, setting curl_debug_off to true helps! I will check at home, too. What is the problem with  curl debugging? \n. Confirmed, curl_debug_off = true helps on ArchLinux, too.\n. ",
    "sebast331": "That works half way. Under Gnome, it would require the file to have a .desktop extension to work properly. Or is it working for everyone else but me?\nEDIT: I had to also set the setting docs_file_extension=true\n. ",
    "kraken17": "It does not seem to have compiled into that directory.\n. [WARNING] Running as root is not recommended\nInstalled packages for 4.02.1:\nbase-bigarray                 base  Bigarray library distributed with the OCaml\nbase-threads                  base  Threads library distributed with the OCaml c\nbase-unix                     base  Unix library distributed with the OCaml comp\nbiniou                       1.0.9  Binary data format designed for speed, safet\ncamlidl                       1.05  Stub code generator for OCaml\ncamlp4                      4.02+6  Camlp4 is a system for writing extensible pa\ncppo                         1.3.1  Equivalent of the C preprocessor for OCaml p\ncryptokit                     1.10  Cryptographic primitives library.\neasy-format                  1.0.2  High-level and functional interface to the F\nextlib                       1.6.1  A complete yet small extension for OCaml sta\ngapi-ocaml                   0.2.6  A simple OCaml client for Google Services.\ngoogle-drive-ocamlfuse      0.5.18  A FUSE filesystem over Google Drive\nocamlfind                    1.5.5  A library manager for OCaml\nocamlfuse               2.7.1-cvs2  OCaml bindings for FUSE (Filesystem in UserS\nocamlnet                     4.0.4  Internet protocols (http, cgi, email etc.) a\nocurl                        0.7.5  Bindings to libcurl\nsqlite3                      4.0.0  sqlite3-ocaml - SQLite3 bindings\nxmlm                         1.2.0  Streaming XML codec for OCaml\nyojson                       1.2.3  Yojson is an optimized parsing and printing\n. Good man - it was in 4.02.1 - in root - as this is a production server (web server) I cannot run it as anything other than root - can i?\n. ok - another problem - do you want me to open another issue. Can you please publish concise information on how to use the headless system. I cannot get it to authenticate using google apis system from the instructions i have found on the internet.\n. yes - way out of date - in the end had to make a fedora gui vmbox and get the code authenticated that way. We need to find a way to do this cos apparently it only lasts 30 days now.\n. ",
    "godofgrunts": "Okay I'll try that when I get home.\nOn Wed, Nov 4, 2015 at 2:01 PM, Alessandro Strada notifications@github.com\nwrote:\n\nIf you enable debug logging with -debug, you should see the requests to\nobtain the token in ~/.gdfuse/default/curl.log. If there is any error, it\nshould be logged there too (or in ~/.gdfuse/default/gdfuse.log).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/142#issuecomment-153831308\n.\n\n\nThank you,\n             Ryan Whited\n. ",
    "abrambailey": "Got it working with the mount-point. But very slow. What I can do to in my config to speed things up?\n. Thanks, that definitely helped. Great work on this project :)\n. ",
    "farvardin": "hello,\nI had the same problem. It seems I installed and authorised google-drive-ocamlfuse last year, but I didn't remember it :)\nAnyway, mounting the google drive didn't work correctly, maybe because it was a previous version or the tokens were too old. \nI removed ~/.gdfuse/default/state and it authorised again, and it worked fine.\n. ",
    "FVFaleiro": "The @farvardin solution works for me too in Ubuntu 16.04. :)\n. ",
    "josuearisty": "Hi\nIm using Ubuntu 17 and browser is not opening, I tried on a vps too and the same result, no browser.\nI tried every tip I see here and nothing happens.\nIs this tool working right now?\nThanks. Hi, where can I find that alternative way? It never open the browser.\nSo I have never used this app before.\nEdit: finally worked with Headless environment.. ",
    "kynikos": "Hi, first of all thank you for this very nice piece of software!\nI know I should have noticed this issue before you patched it (by coincidence I'm only a few days late), however I think a more correct implementation of the XDG standard that also keeps backwards compatibility shouldn't require command-line options, something like (pseudocode):\n```\ndefine xdgcfgdir, xdgdatadir, xdgcachedir, homedir\nif $HOME is defined\n    xdgcfgdir = $HOME/.config/gdfuse\n    xdgdatadir = $HOME/.local/share/gdfuse\n    xdgcachedir = $HOME/.cache/gdfuse\n    homedir = $HOME/.gdfuse\nif $XDG_CONFIG_HOME is defined\n    xdgcfgdir = $XDG_CONFIG_HOME/gdfuse\nif $XDG_DATA_HOME is defined\n    xdgdatadir = $XDG_DATA_HOME/gdfuse\nif $XDG_CACHE_HOME is defined\n    xdgcachedir = $XDG_CACHE_HOME/gdfuse\nif xdgcfgdir, xdgdatadir, xdgcachedir are assigned and xdgcfgdir already exists in the file system\n    if xdgdatadir does not exist in the file system\n        create and initialize xdgdatadir\n    if xdgcachedir does not exist in the file system\n        create xdgcachedir\n    read the configuration from xdgcfgdir and use xdgcachedir, xdgdatadir\nelse if homedir is assigned and already exists in the file system\n    read the configuration from homedir and use it for data and cache\nelse if xdgcfgdir, xdgdatadir, xdgcachedir are assigned\n    create and initialize xdgcfgdir\n    # The code below is the same as in the first 'if' block, perhaps reuse it\n    if xdgdatadir does not exist in the file system\n        create and initialize xdgdatadir\n    if xdgcachedir does not exist in the file system\n        create xdgcachedir\n    read the configuration from xdgcfgdir and use xdgcachedir, xdgdatadir\nelse if homedir is assigned\n    create and intialize homedir\nelse\n    return an error (or create .gdfuse in the working directory?)\n. I see, thanks, I should have read the updated wiki pages before. However I'm not very good at reading OCaml, and there's one thing that I don't understand from the docs, or maybe I misunderstood: you wrote that `-xdgbd` is needed only once, and then the value is stored in an `xdg_base_directory` option in the configuration file; is this configuration file staying in `~/.gdfuse/default/config` regardless of the `xdg_base_directory` value , or how does the application understand where to find the file before being able to read the `xdg_base_directory` option inside of it?. A-ha, so basically now the logic to read the configuration is:\nif ~/.config/gdfuse already exists\n    use it\nelse if ~/.gdfuse already exists\n    use it\nelse\n    create the config in ~/.gdfuse\n``\nI don't want to abuse your patience, however now I don't understand the need for thexdg_base_directoryoption anymore. What happens if it's changed tofalsein~/.config/gdfuse/default/config? Does that mean that then the app for example searches the cache folder in~/.config/gdfuse/default/cacheinstead of~/.cache/gdfuse/? And if it's changed totrue(manually, without using the-xdgbdflag) in~/.gdfuse/default/config`? Does it mean that the application uses the XDG paths for everything except for the config file?\nWould you accept it if I contributed a \"Configuration path priority\" section in Configuration to clarify all this?. Awesome, so this is my commit on the wiki, I hope it will make it clearer also for others :). ",
    "rainabba": "I'm presently trying to build on/for Debian/Jessie (for Docker) using this guide and hitting the following error. Can anyone explain what's going on or how to resolve this error (or tell me a version I should be using)?\n```\n=== ERROR while installing ocamlfuse.2.7.1-cvs5 ==============================#\nopam-version 1.2.0\nos           linux\ncommand      ocaml setup.ml -build\npath         /root/.opam/system/build/ocamlfuse.2.7.1-cvs5\ncompiler     system (4.01.0)\nexit-code    1\nenv-file     /root/.opam/system/build/ocamlfuse.2.7.1-cvs5/ocamlfuse-5472-a4abd5.env\nstdout-file  /root/.opam/system/build/ocamlfuse.2.7.1-cvs5/ocamlfuse-5472-a4abd5.out\nstderr-file  /root/.opam/system/build/ocamlfuse.2.7.1-cvs5/ocamlfuse-5472-a4abd5.err\nstdout ###\n...[truncated]\nmaking ._d/Unix_util.d from Unix_util.ml\nmaking ._d/Thread_pool.d from Thread_pool.ml\nmaking ._d/Fuse_result.d from Fuse_result.ml\nocamlc -c -cc \"cc\" -ccopt \"-fPIC -D_FILE_OFFSET_BITS=64 -I. -pthread \\\n-DPIC -DNATIVE_CODE   \\\n-I/root/.opam/system/lib/camlidl  -o Fuse_bindings_stubs.o \" Fuse_bindings_stubs.c\n../OCamlMakefile:1013: recipe for target 'Fuse_bindings_stubs.o' failed\nmake[1]: Leaving directory '/root/.opam/system/build/ocamlfuse.2.7.1-cvs5/lib'\n../OCamlMakefile:725: recipe for target 'native-code-library' failed\nmake: Leaving directory '/root/.opam/system/build/ocamlfuse.2.7.1-cvs5/lib'\nstderr ###\nIn file included from Fuse_bindings_stubs.c:17:0:\nFuse_bindings.h:17:18: fatal error: fuse.h: No such file or directory\n#include \n^\ncompilation terminated.\nmake[1]: *** [Fuse_bindings_stubs.o] Error 2\nmake: *** [native-code-library] Error 2\nE: Failure(\"Command 'make -C lib INCDIRS=/root/.opam/system/lib/camlidl' terminated with error code 2\")\n=-=- Error report -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\nThese actions have been completed successfully\n - install   extlib-compat.1.7.0\n - install   gapi-ocaml.0.3.4\n - install   base-num.base\n - install   conf-libcurl.1\n - install   conf-which.1\n - install   cppo.1.5.0\n - install   xmlm.1.2.0\n - install   camlidl.1.05\n - install   cryptokit.1.10\n - install   conf-pkg-config.1.0\n - install   base-bytes.legacy\n - install   yojson.1.3.3\n - install   ocamlbuild.0\n - install   ocamlfind.1.7.3\n - install   ocurl.0.7.9\n - install   sqlite3.4.1.3\n - install   easy-format.1.2.0\n - install   ocamlnet.4.1.2\n - install   base-ocamlbuild.base\n - install   conf-zlib.1\n - install   biniou.1.0.13\n - install   num.0\n - install   conf-m4.1\nThe following failed\n - install   ocamlfuse.2.7.1-cvs5\nDue to the errors, the following have been cancelled\n - install   google-drive-ocamlfuse.0.6.17\n```. Haha. That quickly I changed my search and landed at: https://github.com/astrada/google-drive-ocamlfuse/wiki/How-to-install-from-source-on-Debian-Jessie\nShall I do a PR to make that more discoverable?. Going to close this since it's not blocking for me anymore. ",
    "zardii": "Being in no way whatsoever affiliated with the previous developer (other than working in the same company) I have to say that the bug was very painful for us indeed ;]\n. ",
    "dos1": "Generally the files were being copied by our Java application, but when we noticed the issue we started debugging it manually with cp.\nI figured out that while images and randomly generated data worked well, any tar.gz file was failing.\nAs I said, it just stopped working at Nov 20 - we haven't changed anything; files copied earlier were there intact, while everything after that was empty.\nGiven that it just suddenly changed, it's most likely something on Google Drive side and I guess it could be everything - from gradually deployed change on their side, to some strange kind of rate limiting on our account (we're copying quite a lot of tar.gz files there). As adding MIME type worked, I think the real issue is \"what google-drive-ocamlfuse does when it gets server error\" instead of \"what caused the server error\", that's why I filled this ticket.\nAnyway, sent the log to you, maybe it'll be useful :)\n. ",
    "CorvusCorax": "I tried running it without the -m option. However by the looks of it, this really means google-drive-ocamlfuse will run as a single thread in total and as soon as it starts any request to google via curl - be it upload, download or just metadata check, it seems to block all accesses to the filesystem in the meantime, even to data that is already cached and could be served in milliseconds.\nThat the first access to a block file is slow due to the need to check its existence on gdrive and download it (together with the low performance of gdrive itself) is unavoidable. Like any cache-miss.\nBut if each and every upload of data to gdrive - which should by all means happen asynchronously - blocks the filesystem in the meantime, the overall performance - especially for my usecase becomes terribly abysmal.\nI again tried the mkfs.ext3 command on a simulated block device with one file per block. I had to abort it after 1 hour without even seeing the 2nd output line of mkfs:\n\nCreating filesystem with ... blocks and ... inodes\n\nIts bad enough that the FUSE library would only allow one access per time, however for my usecase that's acceptable as only a single thread would access the block device at any time. (mkfs is not multi threaded either ;) )\nBut the cache writeback by all means should happen in background, or having a write cache itself becomes quite useless.\nIs there a way to make google-drive-ocamlfuse spawn a separate thread for uploading changed files in background at least? (as that should be detached from fuse operation)\n. btw setting curl_debug_off=true does indeed get rid of the segfault with -debug enabled.\n. awesome, thanks, that sounds like it would be quite a useful improvement as far as performance is concerned\n. Awesome, I'll give it a try once I'm home tonight :)\n. Took me a bit longer, sorry.\nI get segfaults when using async_upload - similar to the ones happening when I use \"-m\". \n'''\n/home/raven/.opam/system/bin/google-drive-ocamlfuse -o allow_root -skiptrash /mnt/googledrive/ -cc -f\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib64/libthread_db.so.1\".\nClearing cache...done\n[New Thread 0x7ffff1c5b700 (LWP 5419)]\n[New Thread 0x7ffff245c700 (LWP 5370)]\nProgram received signal SIGSEGV, Segmentation fault.\n0x00007ffff744f919 in ?? () from /usr/lib64/libcurl.so.4\n(gdb) bt\n0  0x00007ffff744f919 in ?? () from /usr/lib64/libcurl.so.4\n1  0x00007ffff74506a7 in curl_mvsnprintf () from /usr/lib64/libcurl.so.4\n2  0x00007ffff743b5e0 in ?? () from /usr/lib64/libcurl.so.4\n3  0x00007ffff7430a67 in ?? () from /usr/lib64/libcurl.so.4\n4  0x00007ffff7ae99b5 in ?? () from /usr/lib64/libsqlite3.so.0\n5  0x0000000100000000 in ?? ()\n6  0x00007ffff7bb9150 in ?? () from /usr/lib64/libsqlite3.so.0\n7  0x00007ffff7ae36ee in sqlite3_free () from /usr/lib64/libsqlite3.so.0\n8  0x00007ffff7b0014c in ?? () from /usr/lib64/libsqlite3.so.0\n9  0x0000000000000000 in ?? ()\n'''\nThat backtrace looks weird - It wouldn't call curl from within libsqlite, would it? Has the stack gotten corrupted?\n. ",
    "abarrac1": "I did a search on previous issues and it looks like I have the same issue as #113.  When choosing the drive size should I allocate for total amount of data being transferred or the largest file being transferred?  Is there a way to clear the cache during an rsync?\n. ",
    "goodevilgenius": "running with -debug simply outputs \nStarting application setup (label=default, base_dir=/home/user/.gdfuse).   \nOpening log file: /home/user/.gdfuse/default/gdfuse.log\n/home/user/.gdfuse/default/gdfuse.log says:\nSaving application state in /home/drj/.gdfuse/default/state...done\nCurrent version: 0.5.21\nSetting up cache db...done\nSetting up CURL...done\nSaving application state in /home/drj/.gdfuse/default/state...done\nStarting web browser with command: xdg-open \"https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=uzG4BoGpQxNjRXcmujO0cO%2FfVqnkmnUSclz5PqzILa8\"...done\nGetting tokens from GAE proxy...not found, retrying\nGetting tokens from GAE proxy...not found, retrying\nGetting tokens from GAE proxy...not found, retrying\n. I've done that several times, with no change.\nI've got version 0.5.21-0ubuntu1~ubuntu15.04.1.\n. ",
    "whywaita": "Thank you!\n. ",
    "Fuco1": "Awesome, apparently I was hit by a temporary blindness :)\nThanks for quick reply!\n. ",
    "tlaitinen": "Great, now it works, Thanks!\n. ",
    "cal2195": "use the -oallow_other argument when starting google-drive-ocamlfuse! :)\n. ",
    "kevinjpickard": "Thank you! Add a space in between the -o and the allow_other and it works beautifully! :) Thanks again, I've been banging my head against the wall for weeks with this one.\n. ",
    "AloiSama": "Sorry for reopening this, but does this still work? I keep reading that PLEX will cause google to ban you for 24 hrs.. ",
    "ReneHollander": "Thanks for your answer!\n. ",
    "philipp-roesch": "Fixed with gapi-ocaml.0.2.9.\n. For me too. Thanks!\n. ",
    "officeutils": "A lot of thanks for your software.\nI got same error. Waiting for updates too.\n. ",
    "kaoecoito": "I installed today by opam! It was installed version of google-drive-ocamlfuse 0.5.22 with 0.2.10 gapi-ocaml and still encounter the same error.\nThe log return\n[253.890437] TID=0: BEGIN: Updating resource state in db (id=3137, state=Uploading)\n[253.940902] TID=0: END: Updating resource state in db (id=3137)\n[253.941137] TID=0: BEGIN: Uploading file (id=3137, path=/teste.txt, cache path=/root/.gdfuse/default/cache/0B-3JMDulqlGOeC1hUVlYcXIyVEU, content type=text/plain).\n[255.409387] TID=0: ServiceError\n[255.410040] TID=0: Exception:Failure(\"{\\\"error\\\":{\\\"errors\\\":[{\\\"domain\\\":\\\"global\\\",\\\"reason\\\":\\\"badRequest\\\",\\\"message\\\":\\\"File creation date cannot be modified.\\\",\\\"locationType\\\":\\\"other\\\",\\\"location\\\":\\\"file.created_millis\\\"}],\\\"code\\\":400,\\\"message\\\":\\\"File creation date cannot be modified.\\\"}}\")\n. I sent the log for you but I realized that although the Occam inform you that I already have the newest version of gapi-ocaml in the log it states that the used version is 0.2.8. Then I will delete everything that is installed the libs and try to install again to see if it with the correct version.\nThank you\n. It works now. Thanks\n. After March 16, I started having this same problem. I can no longer create any file in the folder mounted on google drive.\n[46.966276] TID=0: BEGIN: Uploading file (id=3053, path=/.teste.txt.swp, cache path=/root/.gdfuse/default/cache/0B-3JMDulqlGOSTZRNkM2RmZncEU, content type=application/octet-stream).\n[48.248217] TID=0: ServiceError\n[48.248432] TID=0: Exception:Failure(\"{\\\"error\\\":{\\\"errors\\\":[{\\\"domain\\\":\\\"global\\\",\\\"reason\\\":\\\"badRequest\\\",\\\"message\\\":\\\"File creation date cannot be modified.\\\",\\\"locationType\\\":\\\"other\\\",\\\"location\\\":\\\"file.created_millis\\\"}],\\\"code\\\":400,\\\"message\\\":\\\"File creation date cannot be modified.\\\"}}\")\n. ",
    "foxy82": "How can I upgrade gapi-ocaml on Ubuntu? Is it a case of compiling it myself?\n. Never mind - remoinv the PPA and readding it seemed to make it realise I didn't have the latest version.\n. ",
    "adamnovak": "It looks like the Ubuntu package for 12.04 in the PPA was never updated to fix this issue. The precise view of the PPA still has the 2.8 version of gapi-ocaml and not the 2.10 version.\nI'm definitely having this problem, and I just installed the PPA today.\n. It definitely works via OPAM.\nIf 12.04 can't be fixed, it might make sense to take down the PPA for it,\nso people don't think it's going to work.\nOn Fri, Sep 30, 2016 at 5:28 AM, Alessandro Strada <notifications@github.com\n\nwrote:\nIf you cannot upgrade your system, your best option is to install the\napplication using OPAM.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/159#issuecomment-250732136,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AE0_X1UN1q3iTw0psiCXHr62i41x0zA3ks5qvQB7gaJpZM4Hw-lb\n.\n. \n",
    "sandler31": "Hello @astrada, I've sent you my logs too, I have the same problem.\n. ",
    "jetteim": "Just confirm I'\u043c\u0443 got the same issue a week ago\n. ",
    "noesisconsultancy": "Seems like Older Version has issue, after upgrading to latest one, issue resolved\n. ",
    "carlosgs83": "That's right! Thanks!\n. ",
    "ngr": "Unfortunately I can't build the latest version on AWS Linux.\nocamlfind 1.6.2 now requires conf-ncurses and the build of this packet fails.\nI use the following CentOS repository on AWS Linux:\nhttp://download.opensuse.org/repositories/home:ocaml/CentOS_7/home:ocaml.repo\nThe stdrerr and stdout files are empty. Any idea how to fix this?\n```\n[root@ip-10-0-0-120 ~]# opam install conf-ncurses\n[WARNING] Running as root is not recommended\nThe following actions will be performed:\n  \u2217  install conf-ncurses 1\n=-=- Gathering sources =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n=-=- Processing actions -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n[ERROR] The compilation of conf-ncurses failed at \"pkg-config ncurses\".\n=== ERROR while installing conf-ncurses.1 ====================================\nopam-version 1.2.2\nos           linux\ncommand      pkg-config ncurses\npath         /root/.opam/system/build/conf-ncurses.1\ncompiler     system (4.02.3)\nexit-code    1\nenv-file     /root/.opam/system/build/conf-ncurses.1/conf-ncurses-3348-dd682e.env\nstdout-file  /root/.opam/system/build/conf-ncurses.1/conf-ncurses-3348-dd682e.out\nstderr-file  /root/.opam/system/build/conf-ncurses.1/conf-ncurses-3348-dd682e.err\n```\n. Yep! It is called ncurses-devel for CentOS.\nThis helped.\nHere is a short manual of how to install google-drive-ocamlfuse on Amazon Linix:\n```\nConnect CentOS repository. It will work for Amazon-Linux.\nsudo wget http://download.opensuse.org/repositories/home:ocaml/CentOS_7/home:ocaml.repo -P /etc/yum.repos.d/\nInstall Ocaml and required dependencies.\nsudo yum install opam ocaml gcc gcc-c++ m4 make ocamldoc sqlite-devel libcurl-devel fuse-devel zlib-devel ocaml-camlp4-devel ncurses-devel\nopam init\nopam install google-drive-ocamlfuse\n```\nThank you!\n. ",
    "garion-hall": "Thank you very much and sorry for the duplicate\n. ",
    "Heland": "Hi,\nYou are right!\nAfter removing the rpm, and fixing the cryptokit 1.9 failure\nI updated it to the latest version with opam\nopam update && opam upgrade. \nNow I have again full accessworking.\nThanks for your quick support\nBR\nSt\u00e9phane\n. ",
    "berni421": "Much appreciated. However,  in these cases there are no duplicate versions showing on the google drive listing at https://drive.google.com. e.g. I will struggle to fix by renaming because they are not actually ambiguous. :-)\n. Just to add there may well have duplicates at some stage in the past. However, not now.  Is there some kind of extra meta data update needed? -cc does not fix.\n. Doh.  Yes -cc fixed. Sorry, I had omitted to use the -label option to point to the correct cache.\n. ",
    "bAndie91": "Ok, it is done by fuse:\ngoogle-drive-ocamlfuse -o fsname=gdrive:mygooglename mountpoint\n. ",
    "deajan": "Should I leave -debug and -d too or only -verbose ?\n. Thanks, I'll report back when I have something strange in the logs.\n. It seems that with -headless flag, I don't get 100% cpu usage.\n. ",
    "roodyster": "Hi, I am having the same issue.\n. ",
    "rccipriani": "Same issue, running latest version. I can create a file (touch test), but if I try to write to the file with vi I see \" E667: Fsync failed\".   A \"cp\" gives me: \"cp: failed to close \u2018/mnt/google/test\u2019: Device or resource busy\"\nIn the debug log:\n[162.264861] TID=0: Exception:Failure(\"{\\\"error\\\":{\\\"errors\\\":[{\\\"domain\\\":\\\"global\\\",\\\"reason\\\":\\\"badRequest\\\",\\\"message\\\":\\\"File creation date cannot be modified.\\\",\\\"locationType\\\":\\\"other\\\",\\\"location\\\":\\\"file.created_millis\\\"}],\\\"code\\\":400,\\\"message\\\":\\\"File creation date cannot be modified.\\\"}}\")\nBacktrace:\n.\n. I'm running CentOS 7.2.1511, kernel is  3.10.0-327.13.1.el7.x86_64 \n. Yes, using OPAM. It was working at one point, but unfortunately I can't say what changed that might have broken it.  Perhaps Google changed something? \n. gapi-ocaml  0.2.10  A simple OCaml client for Google Services.\n. Same results. :( \n. Noticed that, while I can \"touch\" an empty file, I can't chmod or chown it afterwards. \n. Gotcha. Google is weird. I appreciate your efforts :).\nI don't have anything in my curl.log, do I need to run as -debug for it to be populated?\n. ",
    "Spencerh20": "Worked.\n. ",
    "Windywind": "Same problem, -o allow_other worked, but files created by other users are shown as root's files, why this?. NO luck, still the same.\nI mounted as root, accessed asroot, why should I use \"allow_other\"?. I have docx, png, jpg, 7z and some subfolders, nothing else in my Drive.. @astrada OK, I will do it now. caused by disabled Google project, problem solved.. I didn't even know this (maybe because I cannot see the 'trash' from the mount point. Problem solved, but can you add some features to do with the trash? I removed a file, that means I don' need it any more, not \"in trash\"...Or maybe just make \"rm\" from a mounted google drive means \"permanent delete\"? . OK, thanks very much. Well, actually, I kind of solved the problem.\nAll you need to notice is, remember to mount with the fuse option \"-o allow_other\".\nVery kind of astrada~ :D\nThanks again.. Then, what if I set max_cache_size_mb to big enough, say 1000GB, and my BitTorrent application is writing into the mount point? What would happen? Is this a solution to avoid caching?\nP.S. In the original post, by \"downloading\", I mean downloading from the Internet to my Linux machine. (And with google-drive mounted, the application is writing into the mount point, so, that would be an upload to the google-drive). As I said, I don' have a large storage on my server. \nWhat I want is, the BT client be able to write directly into the mount point (my google drive). It's not as simple as uploading a big file, but real-time writing, so I don' think Rclone would do the job.\nAnd is there's a way to limit the cache size? If so, then I think that would be a solution, right?. ",
    "paede81": "Perfect thanks! That was the point! It's working now\n. ",
    "ZhukovGreen": "Hello Allesandro,\nmay you please provide a little bit more instructions?\nI was trying /root@AZH:~# /home/azh/GoogleDrive/~ -o allow_other but it gives me\nNo such file or directory\n. Yes, I tried this, but the result is\ngoogle-drive-ocamlfuse: unknown option '-oallow_other'\n. Yes, now it works. thank you\n. ",
    "seymour1": "Ah, you know, that's not a bad idea! I'll keep that in mind for the future. It's probably overkill to add directly to google-drive-ocamlfuse.\nThe upload I was trying repeatedly failed for this reason, but I didn't see any output stating why (the why being that the cache was out of space to write). Maybe the documentation or logging could be better in that respect? Otherwise, this issue can be closed.\n. ",
    "ysharoiko": "Made an experiment.\nsaved flie test-2.csv with system calls f.save(fname) from python. And check if it os.path.exists(fname)\nThen I removed it from google interface.\nLocal system was still showing it as an existsing.\nI repeated saving. It showed me no errors and os.path showed true.\nAnd in google interface there were no file.\nThan local file desappered - guess cache refreshed.\nI did repeat my saving test with adding new files test-4.csv and test-5.csv\nAnd these new files created exactly, but the file test-2.csv still did not created. But os.path on it showed true!!! \nHowever this file is not shown locally neither in google interface.\n. If I create files from linux console and delete it from console - everything is well. Very badly. I thought to interfere with users from linux console.\n. ",
    "ittus": "I think you can set stream_large_files=true and large_file_threshold_mb=1. So it will stream all the files which are larger than 1MB. \n. That means if I use the file immediately before it's fully uploaded to Google Drive, there is a chance that I will only access a part (not whole) of that file. Because this file is downloaded again, and normally download speed is faster than upload speed. \nIs there any way to know when file is completely uploaded to Google Drive?\n. Thank @astrada. \nAre there any way to know/test it's working in multithread mode properly? I tried to use stream_large_files=true before but it seems slowdown downloading process.\n. 1 solution is add mount command to crontab configuration. \n\n@reboot google-drive-ocamlfuse -label default /path/to/mount\n. \n",
    "mateussaggin": "Thank you Ittus for your quick response, I will try this right now! I'll return to post the result.\n. Thank you so much Ittus I did everything work properly. I have only one question. Doing that I will have my files stored on my server or just on google? \nI have a very limited space locally. Currently I'm using something around 500TB in the cloud. The files that are already on Gdrive will be synchronized locally?\nThank you so much. If anyone needs help with this, for sure, I will be available to help, just contact me.\n. Hi astrada thank for your quick response. Understand that, thanks. But will the files be temporarily allocated in my local disk when uploading files?\n. Thank you again astrada. All my question where answered. I will make this work. If anyone needs help, please contact me.\n. ",
    "JohNan": "Hi. I have a similar case as @mateussaggin and I am wondering if there's a local copy stored if I move from a local folder /tmp/file1.bin to the mounted folder? The reason I want to use it is to free space locally by uploading to Drive. \nDo I need to set stream_large_files=true for this to work as I want?\nDo I need to remount if I set stream_large_files=true for it to take affect?\nWhat happens if I set stream_large_files=true with the already moved files?\nthanks!\n. Thanks for your reply. \nBut will the cache be emptied once the file is uploaded (it's fine for me to temporarily keep the file) \n. Thanks for the quick response. \nNo, I'm not using any special switches. Simply the command that I wrote above. It is a headless server though, so the setup is the only non-default configuration that has been used.\n. ",
    "jiangjiangflies": "@astrada \n\nNo, sorry. Uploading requires copying the entire file to the cache, streaming options work only when downloading resources.\n\nI'm downloading files to mounted google drive. Is it possible not to save files to local cache?. @astrada \nI set stream_large_files to true and large_file_threshold_mb to 1. Then the size of the downloaded file in google drive is the same as the local cache.  Why?. Sorry again. How should I restart the application? should I mount it again?. Again. The cache file size (0B5F-rbY1D1XCWU5VcEZsWklFcXc) is the same as my downloaded file in google drive. Did I do something wrong?. I cleaned the cache, and remount again but  local cache is still generated when I download files.. I download YouTube videos with youtube-dl to mounted google drive. The cache file size is the same as the downloaded video. Maybe you can have a try.. What a pity.... Thanks, anyway. If so, this application is almost the same as gdrive.. ",
    "MarioPL98": "Thanks. Will it work on debian 8? \n. ",
    "madorian": "I figured it out;) Thanks. \n. This is the reply I have gotten from my Bytesized provider: \n\"I have confirmed that the kernel module is loaded and i\u00b4ve made sure that your container now has the neccesary permisions. Could you try again now and let us know.\"\nIs there any way I can test fuse in openvz to let them understand that they've set it up wrong?\n. Ok, I get the exact same with sshfs:\n`\nb0ef@famadorian:~$ sudo apt-get install sshfs\n[sudo] password for b0ef: \nReading package lists... Done\nBuilding dependency tree\nReading state information... Done\nThe following NEW packages will be installed:\n  sshfs\n0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\nNeed to get 41.7 kB of archives.\nAfter this operation, 138 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu xenial/universe amd64 sshfs amd64 2.5-1ubuntu1 [41.7 kB]\nFetched 41.7 kB in 0s (748 kB/s) \nperl: warning: Setting locale failed.\nperl: warning: Please check that your locale settings:\n        LANGUAGE = (unset),\n        LC_ALL = (unset),\n        LANG = \"nb_NO.UTF-8\"\n    are supported and installed on your system.\nperl: warning: Falling back to the standard locale (\"C\").\nlocale: Cannot set LC_CTYPE to default locale: No such file or directory\nlocale: Cannot set LC_MESSAGES to default locale: No such file or directory\nlocale: Cannot set LC_ALL to default locale: No such file or directory\nSelecting previously unselected package sshfs.\n(Reading database ... 26837 files and directories currently installed.)\nPreparing to unpack .../sshfs_2.5-1ubuntu1_amd64.deb ...\nUnpacking sshfs (2.5-1ubuntu1) ...\nProcessing triggers for man-db (2.7.5-1) ...\nSetting up sshfs (2.5-1ubuntu1) ...\nb0ef@famadorian:~$ ls\ngdrive  wtf.log\nb0ef@famadorian:~$ # sshfs root@foo.org:/root /mnt/foo.org.root\nb0ef@famadorian:~$ mkdir sshfs\nb0ef@famadorian:~$ sshfs root@foo.org:/root sshfs/\nfuse: failed to open /dev/fuse: Permission denied\nb0ef@famadorian:~$ sudo sshfs root@foo.org:/root sshfs/\nfuse: mount failed: No such device\nb0ef@famadorian:~$ \n`\n. b0ef@famadorian:~$ ls -lh /dev/fuse \ncrw-r--r-- 1 root root 10, 229 Jul 25 06:12 /dev/fuse\n. Hmm, I did: \nchmod a+rw /dev/fuse\nNow I have: \nb0ef@famadorian:~$ ls -lh /dev/fuse \ncrw-rw-rw- 1 root root 10, 229 Jul 25 06:12 /dev/fuse\nb0ef@famadorian:~$ sudo google-drive-ocamlfuse -label me -debug gdrive/\nStarting application setup (label=me, base_dir=/home/b0ef/.gdfuse).\nOpening log file: /home/b0ef/.gdfuse/me/gdfuse.log\nfuse: mount failed: No such device\nb0ef@famadorian:~$ \n, so still the same\n. Right, I'll let them know: \ncat /proc/filesystems \nnodev   cgroup\nnodev   devpts\nnodev   mqueue\n        ext3\n        ext4\nnodev   nfs\nnodev   nfs4\nnodev   delayfs\nnodev   devtmpfs\nnodev   sysfs\nnodev   proc\nnodev   tmpfs\nnodev   binfmt_misc\n. Excellent;) Working now, thanks a lot.\n. Ok, found the read_only=true flag. Sorry for the noise. I looked in man.\n. It is mounted read only, but it still displays: \ngoogle-drive-ocamlfuse on /home/b0ef/gdrive type fuse.google-drive-ocamlfuse (rw,nosuid,nodev,relatime,user_id=1000,group_id=1000)\nI can't write to it, so everything is great; just mentioning;)\n. And immediately I found allow_other;) Damn, let me try this;)\n. Hmm, is this supposed to work. Seems someone else tried that: \nhttp://serverfault.com/questions/544788/fuse-mounted-google-drive-via-google-drive-ocamlfuse-for-wordpress-and-permiss\n. Well, after trying it, it does work;) Damn, I feel so stupid today;), hehe\n. ",
    "Orbixx": "It would be helpful if the application gave the option of requesting a read-only token from Google. A malicious attacker that compromises the credentials could destroy an entire Google Drive account and even a Google Photos account too if configured to show on GDrive.\n. To anybody who wants similar behaviour and don't mind compiling, edit line containing: \"let scope = [GapiDriveV2Service.Scope.drive]\" in oauth2.ml to: \"let scope = [GapiDriveV2Service.Scope.drive_readonly]\"\n. ",
    "Thinkscape": "Experiencing the same problem. It doesn't occur with other fuse mounts (i.e. gdfs). Raw write speed is 10x-50x faster than via encfs.. ",
    "blitz313": "I had the same problem back in June as well - I was in the process of switch from acd_cli to Google Drive equivalent.  I originally was attempting to make my implementation work with google-drive-ocamlfuse, however I had to switch over to gdfs for this same reason.  The ENCFS volume never mounts.  I'd like to switch over to google-drive-ocaml-fuse b/c gdfs hasn't had a commit in over a year - and honestly I have issues with Plex playback.  So, any help with resolving the ENCFS loading would be greatly appreciated.. ",
    "akwala": "Had the same issue with another PPA, so this is likely not a problem with google-drive-ocamlfuse PPA. Hence, I'm closing it.\n. In case anyone else runs into this problem...\nIf add-apt-repository fails to add keys try adding them using apt-key. The following worked for me in this case:\nsudo apt-key adv --keyserver pool.sks-keyservers.net --recv-keys 0xAD5F235DF639B041\n. ",
    "tomgoldsmith": "Ahh I see yes that makes sense. Totally understand about the ETA.\nJust for info my use case here is that I want to store a set of scripts that I can use on multiple machines without having to store them in a git repository and have to pull them when changes are made and so I can quickly reinstate them on a clean build. \nVery much appreciate the work so far! Thank you!\n. ",
    "buffyg": "No proxy, have no problems doing the authorization from the browser, it's only the CLI utilities that have issues. Tried refreshing all ocaml components.\n. I've run into the same problem, web page says, \"Cannot get authorization tokens, please try again.\" rm -rf ~/.gdfuse/default is accordingly joyless.. ",
    "davidneudorfer": "Same issue here:\n```\ntime curl -I https://gd-ocaml-auth.appspot.com\nHTTP/1.1 200 OK\nDate: Fri, 21 Oct 2016 04:43:02 GMT\nExpires: Fri, 21 Oct 2016 04:53:02 GMT\nCache-Control: public, max-age=600\nETag: \"6EjDMA\"\nX-Cloud-Trace-Context: 7a80774c8626fc81e415015b1c579df1\nContent-Type: text/html\nServer: Google Frontend\nContent-Length: 2455\nAlt-Svc: quic=\":443\"; ma=2592000; v=\"36,35,34,33,32\"\nreal    0m5.885s\nuser    0m0.060s\nsys     0m0.010s\n```\n```\ncat /root/.gdfuse/me/gdfuse.log\n[0.000287] TID=0: Setting up me filesystem...\n[0.000314] TID=0: Loading configuration from /root/.gdfuse/me/config...done\n[0.000406] TID=0: BEGIN: Saving configuration in /root/.gdfuse/me/config\n[0.000527] TID=0: END: Saving configuration in /root/.gdfuse/me/config\n[0.000570] TID=0: Loading application state from /root/.gdfuse/me/state...done\nCurrent version: 0.5.24\nSetting up cache db...done\nSetting up CURL...done\n[23.222566] TID=0: Error during request: Code: 28, Description: CURLE_OPERATION_TIMEOUTED, ErrorBuffer: Resolving timed out after 5534 milliseconds\n[23.222589] TID=0: Retrying (1/10)\n[30.067578] TID=0: Error during request: Code: 28, Description: CURLE_OPERATION_TIMEOUTED, ErrorBuffer: Resolving timed out after 5544 milliseconds\n[30.067601] TID=0: Retrying (2/10)\n[38.590850] TID=0: Error during request: Code: 28, Description: CURLE_OPERATION_TIMEOUTED, ErrorBuffer: Resolving timed out after 5542 milliseconds\n[38.590862] TID=0: Retrying (3/10)\n```\nEDIT: this eventually worked\n. ",
    "NdS-Research-Facilities": "fixed it. ",
    "idvoretskyi": "@filippo2658 @astrada as an end-user of the utility, I'd like to see the GitHub conversations in English.\n. ",
    "filippo2658": "Grazie mille!!!!\n----Messaggio originale----\nDa: \"Alessandro Strada\" notifications@github.com\nData: 23/08/2016 12.21\nA: \"astrada/google-drive-ocamlfuse\"google-drive-ocamlfuse@noreply.github.com\nCc: \"filippo2658\"filippodaddi@libero.it, \"Author\"author@noreply.github.com\nOgg: Re: [astrada/google-drive-ocamlfuse] INSTALLAZIONE E AUTENTICAZIONE SERVER SENZA GUI (#199)\nS\u00ec, \u00e8 possibile. In questa pagina ci sono le istruzioni per procedere. La macchina esterna con il browser serve comunque.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. ",
    "matiasw": "Google-drive-ocamlfuse should be made to fail gracefully when offline. There's more info about this at http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=846367 , but basically the mount point is \"busy\", and makes my file manager, your desktop environment hang.. ",
    "Schisco94": "I'm having a similar issue. I'm using the new Mint 18 on KDE. Every time I disconnect from the Internet, I can navigate in dolphin no problem, except when I try to view /home/%User%/. My entire home folder hangs, and I can't access any documents. I tried to do an ls, and came up with the same result. As long as I have an Internet connection, I can access my home folder, but I'm running a lab that forces me to disconnect from the Internet to do properly.. ",
    "mcrucifix": "mmm.... someone who would steal the computer and access the physical drive would be able to keep getting refreshing tokens, isnt'it ? . If course this won't happen if the gdfuse config dir is on an encrypted section. \n. I agree that this is a reasonable solution. \nMichel. \n. ",
    "R-Chandra": "You could always use something like ecryptfs to encrypt the contents of $HOME/.gdfuse if you wanted to secure it better.  That would require, at minimum, inserting the proper crypto keys into the kernel in order to read the data (which is done by the utilities in the ecryptfs-utils Ubuntu package, such as ecryptfs-mount-private).  I haven't delved deeply into that, but by default it encrypts into $HOME/.Private and mounted on $HOME/Private.  I have done the setup and then edited $HOME/.ecryptfs/Private.mnt to mount it on $HOME/encry, so it could equally do .gdfuse.  Presumably you could also set up more than one of these, just .Private and Private being the defaults.. I just fetched google-drive-ocamlfuse_0.6.4-0ubuntu1~ubuntu14.04.1_amd64.deb from the PPA and installed it (with gdebi-gtk; complained a newer version was available, but did it anyway), and it works OK, lists the \"root\" just fine.. Hey, thanks, dude.  I'll have to give that new version a try.. ",
    "natoriousbigg": "I only use whole disk encryption on all my Linux servers now. Easy to set up and performance is better. . I've been testing GDO for about 24 hours now with Plex, did a few library scans, and haven't gotten banned yet. Had make some changes to the config file and the mount settings to make Plex stream stable.. @Torsten85 I'm getting about 10MB/s transfer (with 2 video streams running in the background) from the mount with these settings. Sometimes it's a lot higher due to memory caching. I don't really know exactly which settings helped, but this is what's working for me right now. I'm about to play a 80mbps video without buffer.\nConfig file. Customized settings are marked with ##:\napps_script_format=json\napps_script_icon=\nasync_upload=true\ncache_directory=/tmp/gdrive-cache\nclient_id=\nclient_secret=\nconnect_timeout_ms=5000\ncurl_debug_off=false\ndata_directory=\ndelete_forever_in_trash_folder=false\ndocs_file_extension=true\ndocument_format=odt\ndocument_icon=\ndownload_docs=false\ndrawing_format=png\ndrawing_icon=\nform_format=zip\nform_icon=\nfusion_table_format=desktop\nfusion_table_icon=\nkeep_duplicates=false\nlarge_file_read_only=true ##\nlarge_file_threshold_mb=16\nlog_directory=\nlost_and_found=false\nlow_speed_limit=0\nlow_speed_time=0\nmap_format=desktop\nmap_icon=\nmax_cache_size_mb=512000 ## 500GB\nmax_download_speed=0\nmax_memory_cache_size=10737418240 ## 10GB\nmax_retries=8\nmax_upload_chunk_size=1099511627776\nmax_upload_speed=0\nmemory_buffer_size=8388608 ## 8MB\nmetadata_cache_time=60\npresentation_format=pdf\npresentation_icon=\nread_ahead_buffers=5 ##\nread_only=true\nshared_with_me=false\nspreadsheet_format=ods\nspreadsheet_icon=\nsqlite3_busy_timeout=5000\nstream_large_files=true ##\numask=0o002\nverification_code=\nxdg_base_directory=false\n\ngoogle-drive-ocamlfuse mount settings:\ngoogle-drive-ocamlfuse -o allow_other,ro,atomic_o_trunc,large_read,default_permissions /mnt/gdrive\n\nunionfs mount:\nunionfs-fuse -o cow,allow_other,direct_io,atomic_o_trunc,large_read,default_permissions /mnt/local=rw:/mnt/gdrive=ro /mnt/union. I do use rclone to upload, can post in a bit. I did a full library scan with Plex and got 42,000 API hits (mostly drive.files.get) without getting banned so far:\n\n\n. My library is currently only 20TB right now. Any here's my upload script. Make sure you configure it for your setup:\n#!/bin/bash\n\ntoday=`date '+%m-%d-%Y'`;\n\nRCLONE_BIN=\"rclone\"\n\nLOCALDIR=\"/mnt/local\"\nLOCALDIR_TV=\"tv\"\nLOCALDIR_MOVIES=\"movies\"\n\nGDRIVE_RCLONE_REMOTE=\"gdrive\"\n\nTV_UPLOAD_OPTIONS=\"--no-traverse --acd-upload-wait-per-gb 90s --bwlimit 20M --delete-after --verbose\"\nTV_MOVE_MIN_AGE=\"4w\"\nTV_COPY_MIN_AGE=\"1w\"\nMOVIES_UPLOAD_OPTIONS=\"--no-traverse --acd-upload-wait-per-gb 90s --bwlimit 20M --delete-after --verbose\"\nMOVIES_MOVE_MIN_AGE=\"52w\"\nMOVIES_COPY_MIN_AGE=\"1w\"\n\necho \"Starting TV move.\"\n${RCLONE_BIN} move $TV_UPLOAD_OPTIONS --min-age $TV_MOVE_MIN_AGE --log-file=\"$(dirname $0)/${today}-tv_move.log\" \"${LOCALDIR}/$LOCALDIR_TV\" \"${GDRIVE_RCLONE_REMOTE}:$LOCALDIR_TV\"\n\necho \"Starting TV copy.\"\n${RCLONE_BIN} copy $TV_UPLOAD_OPTIONS --min-age $TV_COPY_MIN_AGE --log-file=\"$(dirname $0)/${today}-tv_copy.log\" \"${LOCALDIR}/$LOCALDIR_TV\" \"${GDRIVE_RCLONE_REMOTE}:$LOCALDIR_TV\"\n\necho \"Starting Movies move.\"\n${RCLONE_BIN} move $MOVIES_UPLOAD_OPTIONS --min-age $MOVIES_MOVE_MIN_AGE --log-file=\"$(dirname $0)/${today}-movies_move.log\" \"${LOCALDIR}/$LOCALDIR_MOVIES\" \"${GDRIVE_RCLONE_REMOTE}:$LOCALDIR_MOVIES\"\n\necho \"Starting Movies copy.\"\n${RCLONE_BIN} copy $MOVIES_UPLOAD_OPTIONS --min-age $MOVIES_COPY_MIN_AGE --log-file=\"$(dirname $0)/${today}-movies_copy.log\" \"${LOCALDIR}/$LOCALDIR_MOVIES\" \"${GDRIVE_RCLONE_REMOTE}:$LOCALDIR_MOVIES\"\n\necho \"Upload Complete.\"\n\nThe \"--acd-upload-wait-per-gb\" option is only needed if you're uploading to ACD. If you're using GDrive it'll just be ignored.. @Torsten85 You notice the dates, I would copy files older than 1 weeks then and then move files that are over than 2 weeks. I want to keep files longer locally as a cache but don't want to risk losing them if the HD crashes. . @Torsten85 I'm getting frequent mount dropping as well. You'll have to turn on debug mode \"-debug\" to enable logging. I have it on but I have a script that checks the mount and remount is. But whenever this happens the log gets overwritten. I'll wonder if it's related issue #236 and the memory cache setting that we're using. I'll post it as a new issue if I can catch it in the log.. @Torsten85 I'm not sure how your set up is but I actually use unionfs to stack my local drive on top of gdrive and then on top of ACD (unionfs-fuse /mnt/local=rw:/mnt/gdrive=ro:/mnt/acd=ro). With the above unionfs settings, whenever the google-drive-ocamlfuse mount drops the ACD mount takes over as failover mechanism. I didn't realize this was possible just something I noticed and found it useful.. @Torsten85 I can post in a bit but it's a complicated set of scripts. It'll only work if you have the same set up: local drive mounted on gdrive mount on acd . You can use acdcli or rclone to mount acd. But I use rclone crypt and not encfs.\n@jmoriau The mount does drop about once a day.. Would you consider adding this as a feature in the future (caching of files in chunks and only download portions that are needed)?. Awesome. Hope this feature can be added in the near future. . ",
    "bs27975": "Thanks. OPAM install seems to have worked just fine.\n. Should the config defaults not be (include):\ndocument_format=gdoc\ndrawing_format=gdraw\nform_format=gform\nmap_format=gmap\npresentation_format=gslides\nspreadsheet_format=gsheet\n. Thanks. I was all set to close this git issue, then - except, it does seem pointful to update the wiki article to make explicit that export == translate - and a Google limitation beyond anyone's control. (Which may be self-evident to those with such experience, but to those new to things, such as the likely readers of that wiki page, not so much.) This git issue could then be closed.\n. ",
    "christiaanderidder": "The syntax that is currently suggested in the wiki also seems to cause issues with snapd.\nThe usage of a prefix causes the following error: https://bugs.launchpad.net/snappy/+bug/1760841\nChanging fstab from gdfuse#default /home/$USERNAME/gdrive fuse uid=1000,gid=1000,user 0 0 to default /home/$USERNAME/gdrive fuse.gdfuse uid=1000,gid=1000,user 0 0 seems to fix the issue.\n. ",
    "ninlilizi": "This is also completely broken on Debian 9.\nFollowing the wiki results in:\nmount: wrong fs type, bad option, bad superblock on gdfuse#default,\n       missing codepage or helper program, or other error\n   In some cases useful info is found in syslog - try\n   dmesg | tail or so.\n\n. ",
    "shlomitsadok": "Seems to be working. Thanks!\n. ",
    "d235j": "Yep looks like it's #54. Anything I can do to help resolve this problem?\n. @HelgeS you need a newer version of gapi-ocaml.\n@astrada how does one use the new team drive functionality? I'd like to test it and got everything built.. ",
    "Moulick": "So that means If I copy a 1GB file from my home dir to the drive, it'll first cache it ?. And why is this caching necessary? When copying from my local drive to\nGoogle drive, the file is already in a sense cached?\nOn Jan 20, 2018 9:39 PM, \"Alessandro Strada\" notifications@github.com\nwrote:\n\n@Moulick https://github.com/moulick yes, correct.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/210#issuecomment-359182170,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/APDMJ80YCJCSGAeTOvqV1kvfToMcy73Pks5tMg-9gaJpZM4Kae-n\n.\n. \n",
    "vlna": "It works with line changed to max_upload_chunk_size=536870912\nOriginal value was max_upload_chunk_size=-1073741824\nSolved. Thx.\n. Similar issue here, I had to move out Form (Google document type) from Drive root to subfolder to make root available. The subfolder with the one file is not accessible now.. ",
    "sionie": "0.5.25 works! thanks :)\n. ",
    "tulasinandan": "Hi I'm having the same problem installing on Debian 9 - 32 bit:\n```\nMakefile:50: recipe for target 'IO.cmo' failed\nmake[1]: Leaving directory '/home/tulasi/.opam/system/build/extlib.1.7.4/src'\nMakefile:14: recipe for target 'build' failed\nstderr\nfatal: Not a git repository: '.git'\nFile \"IO.ml\", line 577, characters 10-21:\nError: Integer literal exceeds the range of representable integers of type int\nmake[1]: *** [IO.cmo] Error 2\nmake: *** [build] Error 2\n=-=- Error report -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\nThe following actions were aborted\n  \u2217  install gapi-ocaml             0.3.6 \n  \u2217  install google-drive-ocamlfuse 0.6.23\nThe following actions failed\n  \u2217  install extlib 1.7.4\nNo changes have been performed\n```\nAny suggestions on fixing it? Thanks!``. That worked. Thanks!. ",
    "XJDHDR": "I have one question though: How much effort would be required to add ARM binaries to your PPA repository? The reason I ask is because I would much rather use Ubuntu's built-in package manager to manage my software rather than installing another software manager just to install yours.\nOtherwise, I could certainly try it. I do however notice that the installation instructions for OCAML has the words \"TODO: REDO THE INSTALLATION INSTRUCTIONS!\" at the top so I'm already worried. Furthermore, they state that building OPAM and OCAML will take a very long time (hours, judging by the suggestions provided for what to do in the meantime). \n. That's good to hear. Anyway, the Samsung Exynos 5 SOC used by the Odroid XU3 uses the armhf architecture.\n. I can't believe it's been a month already. \nI just managed to install your program on my Odroid and it seems to be working very well. Thank you! I am having one small issue but I'll create a seperate ticket for that.\nAs for the other two ARM architectures:\nI'm not aware of any computer that requires armel and, from what I've read, armel is slower and older than armhf. I don't think you need to worry about this unless someone requests it.\nARMv7 is still the most popular architecture for ARM computers though as time goes by, ARMv8 is only going to become more popular. In this list, I count 7 computers that can use arm64 binaries:\nhttps://en.wikipedia.org/wiki/Comparison_of_single-board_computers#CPU.2C_GPU.2C_memory. ",
    "babam86": "Can you to provide a static binary for ARMv7, I will use it on Android.\nThanks.. Okay, thanks.. ",
    "rootministrator": "I think I have also bumped to a memory leak issue.\nI have google-drive-ocamlfuse installed on a RPI3 (running osmc, which is basicly Debian / Raspbian).\nI would like to use the service to stream large video files (often >10GB) from my google drive on the PI with Kodi.\nI currently use these modified settings:\nmax_memory_cache_size=50485760\nmemory_buffer_size=15485760\nread_ahead_buffers=1 \nI've been playing around with the  numbers above for a while. With default config it was not good at all, however these limits it seems to be OK for most files. At least for a couple of minutes... Then, it is a complete disaster, sometimes even the player crashes.\nI started monitoring memory usage, and I noticed that free memory goes from ~440 MB to below 30 MB (in approx 5 minutes, depending on the bitrate) \nAs for my understanding, \"max_memory_cache_size=50485760\" should set the maximum allocated cache memory to ~50MB, not a single byte more, and that is definetly what happens. \nCan you please look up to this, @astrada? Can you suggest some workarounds I can try before a proper fix of the issue?\nIf you need any further details to investigate this issuse, feel free to ask.\nThx in advance!\n. I have installed it with OPAM about 3 weeks ago. \nAccording to the 'state' next to the configuration it is: saved_version=0.6.17\nI had to manually install a couple of dependencies within opam, because depext was not familiar with my distribution. \n(I'm adding this only because it might have something to do with the issuse, however I think every package installation went well, and also, GDO works fine except this leak issue)\n. Do I need a special switch for opam to upgrade to beta releases? \nI did opam update, and upgrade after pinning, but I got back 0.6.17.\nWhen I do an opam update now, I get:\n=-=- Synchronizing development packages -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n[google-drive-ocamlfuse] https://github.com/astrada/google-drive-ocamlfuse.git#beta already up-to-date\n. Thanks!\nI'm on 0.6.18 now, and the issue is still persistent. I've started to play with kodi's own cache settings (even fully disabling it), but  makes no difference.\nI tested sharing movies on my local network, kodi plays fine, with >400 MB of free memory.\nHowever, if I start streaming from google drive, free memory 'vanishes' in 5 minutes. I ran top command, to follow memory usage, the interesting thing is not the GDO process itself takes the memory over (it uses about ~10-20%, depending on cache settings, which is fine, kodi takes also about ~20%, and the rest is just gone, I mean it is not listed by top.  \nThat's why I suspect it is a memory leak, but I am up to suggestions, what to test.\n. ",
    "jheyneman": "How does one replace libcurl? I compiled curl from source with ./configure --with-ssl and installed it. I'm still getting a steady increase in RAM usage when using Plex until the system runs out of memory or I get the \"Transport endpoint is not connected\" message if I try to access the mount.\nIt seems I installed it correctly...\n$ curl -V\ncurl 7.54.0 (x86_64-pc-linux-gnu) libcurl/7.54.0 OpenSSL/1.0.2k zlib/1.2.8. @astrada I should have mentioned that I installed from your PPA, and I'm on Ubuntu 16.04. The command you suggested returned: \n$ pkg-config --variable=libdir libcurl\n/usr/local/lib\nDefinitely pointing to the correct one, shows mod date of yesterday. \nedit: I uninstalled and reinstalled with opam. I'll see how that goes for a while.. ",
    "JustinGrote": "Seeing similar issue on Ubuntu 16.04LTS. Here's the relevant section from my config (streaming is enabled):\nkeep_duplicates=false\nlarge_file_read_only=true\nlarge_file_threshold_mb=16\nlog_directory=/var/log/google-drive-ocamlfuse\nlost_and_found=false\nlow_speed_limit=0\nlow_speed_time=0\nmap_format=desktop\nmap_icon=\nmax_cache_size_mb=512\nmax_download_speed=0\nmax_memory_cache_size=10485760\nmax_retries=8\nmax_upload_chunk_size=10995116\nmax_upload_speed=0\nmemory_buffer_size=1048576\nmetadata_cache_time=60\npresentation_format=pdf\npresentation_icon=\nread_ahead_buffers=3\nPer above I would expect max cache memory to be 10 meg right (plus some overhead I would expect for the code so ~15-20 meg). Right now it's using ~300 meg\n\nNot sure if my libcurl was complied with TLS as mentioned above, just using the defaults. Here's my curl-v output\n```\ncurl -V\ncurl 7.47.0 (x86_64-pc-linux-gnu) libcurl/7.47.0 GnuTLS/3.4.10 zlib/1.2.8 libidn/1.32 librtmp/2.3\nProtocols: dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtmp rtsp smb smbs smtp smtps telnet tftp\nFeatures: AsynchDNS IDN IPv6 Largefile GSS-API Kerberos SPNEGO NTLM NTLM_WB SSL libz TLS-SRP UnixSockets\n```\n. ",
    "VarmingDK": "I have the same problem - memory consumption just keeps growing on Ubuntu 16.04\ncurl 7.47.0 (x86_64-pc-linux-gnu) libcurl/7.47.0 GnuTLS/3.4.10 zlib/1.2.8 libidn/1.32 librtmp/2.3\ngoogle-drive-ocamlfuse, version 0.6.19. I've followed the step by step guide above and at step 4 I get this:\ncurl 7.54.0 (x86_64-pc-linux-gnu) libcurl/7.54.0 OpenSSL/1.0.2g zlib/1.2.8\nWhich is what we want. I then do the step 5 and fixed the compilation errors but when I ldd I get this:\nlibcurl-gnutls.so.4 => /usr/lib/x86_64-linux-gnu/libcurl-gnutls.so.4 (0x00007fc54f760000)\nAny ideas to what could have gone wrong?. I just tried building OpenSSL as well but I still in step 5 see libcurl-gnutls  :(. I see this:\n/usr/bin/google-drive-ocamlfuse\nand with ldd I see this:\n$ ldd /usr/bin/google-drive-ocamlfuse\n    libcurl-gnutls.so.4 => /usr/lib/x86_64-linux-gnu/libcurl-gnutls.so.4 (0x00007f89fd4df000)\n    libgnutls.so.30 => /usr/lib/x86_64-linux-gnu/libgnutls.so.30 (0x00007f89fbbf0000)\nAlso a lot more lines but those must be the relevant ones.. It is the PPA - the guide didn't say otherwise (and I'm new to building). How do I install using opam?\nEDIT: Found it in the Wiki. Installed by opam instead but now I get nothing when doing the \nwhich google-drive-ocamlfuse\nSo I don't know with which directory to try the ldd command..?. Great - now I got this on ldd:\n$ ldd ~/.opam/system/bin/google-drive-ocamlfuse\n    linux-vdso.so.1 =>  (0x00007ffc6c9fc000)\n    libsqlite3.so.0 => /usr/lib/x86_64-linux-gnu/libsqlite3.so.0 (0x00007fcf70d8d000)\n    libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fcf70b70000)\n    libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007fcf70955000)\n    libgmp.so.10 => /usr/lib/x86_64-linux-gnu/libgmp.so.10 (0x00007fcf706d5000)\n    libcurl.so.4 => /usr/local/lib/libcurl.so.4 (0x00007fcf7046b000)\n    librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fcf70262000)\n    libfuse.so.2 => /lib/x86_64-linux-gnu/libfuse.so.2 (0x00007fcf70026000)\n    libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fcf6fd1d000)\n    libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fcf6fb18000)\n    libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fcf6f74f000)\n    /lib64/ld-linux-x86-64.so.2 (0x00005585caf39000)\n    libssl.so.1.0.0 => /lib/x86_64-linux-gnu/libssl.so.1.0.0 (0x00007fcf6f4e6000)\n    libcrypto.so.1.0.0 => /lib/x86_64-linux-gnu/libcrypto.so.1.0.0 (0x00007fcf6f0a1000)\nBut I still get google-drive-ocamlfuse not found even after adding to ~/.profile. Should I do something after adding the OPAM configuration (like restarting something or )?. I got it - had to logout of the SSH and login again  :)  Thanks A LOT for helping! Now I just gotta test.\n. ",
    "Experian": "I had the same problem with curl using GnuTLS as its TLS backend on Ubuntu 16.04. I switched to OpenSSL and it seems to be fine now. Here's how I did it;\n1- Remove the installed version of curl using;\nsudo apt-get purge curl\n2- Download the latest version of curl tarball from here;\nhttp://curl.haxx.se/download.html\n3- Extract, make and install\nYou need to make sure there is an OpenSSL dev lib available on your system or SSL will not be enabled. You can either build OpenSSL from source or install libssl-dev with sudo apt-get install libssl-dev.\ntar -xvzf curl-*.tar.gz\ncd curl-*\n./configure --libdir=/usr/lib/x86_64-linux-gnu\nmake\nsudo make install\nOptional: If you'd like to specify your compiled version of OpenSSL you can use ./configure --with-ssl=/usr/local/ssl. You can obtain the latest tarballs from https://github.com/openssl/openssl/releases\n4- Verify that curl is using OpenSSL\ncurl -V\noutput: curl 7.54.0 (x86_64-pc-linux-gnu) libcurl/7.54.0 OpenSSL/1.0.2g zlib/1.2.8\nNote: Make sure that curl and libcurl show the same version number i.e 7.54.0 in the above output. If that is not the case, you might need to replace the old libcurl version with the new one, otherwise curl won't work. \nsee here: https://juniway.blogspot.de/2015/12/curl-48-unknown-option-was-passed-in-to.html\n5- Recompile google-drive-ocamlfuse\nopam reinstall ocurl gapi-ocaml google-drive-ocamlfuse\nYou can verify google-drive-ocamlfuse is using the right libcurl using ldd\nldd /path/to/google-drive-ocamlfuse\nThe output should look something like this.\nlibcurl.so.4 => /usr/local/lib/libcurl.so.4 (0x00007f01e72e1000)\n. What do you see if you try?\nwhich google-drive-ocamlfuse. I think that is the PPA version. You need to uninstall that and install using opam.\nTo uninstall try apt remove google-drive-ocamlfuse.\nIf you have already installed using opam,  google-drive-ocamlfuse might not be in your system path. To fix that you can try adding this to ~/.profile.\n```\nOPAM configuration\n. $HOME/.opam/opam-init/init.sh > /dev/null 2> /dev/null || true\n```\nsee https://github.com/astrada/google-drive-ocamlfuse/issues/114. To install using opam, try the following;\nsudo apt-get install opam ocaml make fuse camlp4-extra build-essential pkg-config\nopam init\nopam install depext\nopam depext google-drive-ocamlfuse\nopam install ocurl gapi-ocaml google-drive-ocamlfuse. Did you try adding the following to ~/.profile? \nIt should fix google-drive-ocamlfuse not found.\n```\nOPAM configuration\n. $HOME/.opam/opam-init/init.sh > /dev/null 2> /dev/null || true\n``\ngoogle-drive-ocamlfuse is probably in~/.opam/system/bin/`\n. Glad to hear that :). ",
    "SteelyNinja": "Ignore now solved.\nFor info if you update curl to version 7.54 as per guide above, you can then not install via OPAM it fails with  \"curl: option --compressed: the installed libcurl version doesn't support this\" error\nRolled back to old curl - installed then upgraded and works ok.\n. Quick update after trying this for a couple of days.\nThe memory leak appears to be gone which is really good but, this process produces a fixed version based on 0.6.17\nI was previously running the beta 0.6.19 which did seem to fix some of the dropping issues that are now back!\nSeems I can have the odd drop, or less drops and a memory leak at the moment. \nis there any way to fix this memory leak in the 0.6.19 somehow?. Perfect - now trialing 0.6.19 with memory leak fix.. Hi, just found this thread and I am having the same issues with drops on scans. (running 0.6.19) now.\nI will capture a verbose log and see what it says....\nThe mount is fine during normal playing, but on scan there seems t be a lot more activity and the mount drops.\n\nEdit - logs sent. \n",
    "Anonymousey": "I'm still getting memory leaks with the latest version. ",
    "Symbiot78": "I tried adding -headless.. will check tomorrow..\nthe plex scan is still EXTREMELY slow.. but haven't tried playing anything yet..\n. ",
    "PeteLawler": "Woohoo! Thanks mate, no apologies needed. That new release looks rather sweet. Take care :). See #221, it was an easy fix far as I can tell.. ",
    "marlarius": "Thanks. It works!. ",
    "karbowiak": "Oh damn, and here i was hoping it would be :(\nAh well, i'll close this up again then :)\nKeep up the good work! :). ",
    "acs-ferreira": "gdfuse.log\n[2.525512] TID=0: BEGIN: Getting tokens from GAE proxy\n[2.954983] TID=0: END: gettokens not found, retrying\n[2.956799] TID=0: Retrying (1/10) get_tokens after exception: Not_found\n[6.034922] TID=0: END: gettokens not found, retrying\n[6.036159] TID=0: Retrying (2/10) get_tokens after exception: Not_found\n[8.779751] TID=0: END: gettokens not found, retrying\n[8.781396] TID=0: Retrying (3/10) get_tokens after exception: Not_found\n[15.293283] TID=0: END: gettokens not found, retrying\n[15.294374] TID=0: Retrying (4/10) get_tokens after exception: Not_found\n[23.723090] TID=0: END: gettokens not found, retrying\n[23.724388] TID=0: Retrying (5/10) get_tokens after exception: Not_found\n[42.361741] TID=0: END: gettokens not found, retrying\n[42.363074] TID=0: Retrying (6/10) get_tokens after exception: Not_found\n[75.629484] TID=0: END: gettokens not found, retrying\n[75.630969] TID=0: Retrying (7/10) get_tokens after exception: Not_found\n[141.235163] TID=0: END: gettokens not found, retrying\n[141.236632] TID=0: Retrying (8/10) get_tokens after exception: Not_found\n[270.322169] TID=0: END: gettokens not found, retrying\n[270.323568] TID=0: Retrying (9/10) get_tokens after exception: Not_found\n[528.547025] TID=0: END: gettokens not found, retrying\n[528.548310] TID=0: Retrying (10/10) get_tokens after exception: Not_found\n[1043.486967] TID=0: END: gettokens not found, retrying\n[1043.488102] TID=0: Error during get_tokens after 10 attempts: Not_found\ncurl.log\n```\n[1042.837111] curl: header out: GET /gettokens?requestid=MC3JdiiUrUr0m82EKdJM8HMNz2915qbmbuy-xUuaG2I HTTP/1.1\nUser-Agent: google-drive-ocamlfuse (0.5.25) gapi-ocaml/0.2.13/Unix\nHost: gd-ocaml-auth.appspot.com\nAccept: /\nAccept-Encoding: identity\n[1043.475475] curl: header in: HTTP/1.1 200 OK\n[1043.475515] curl: header in: Cache-Control: no-cache\n[1043.475525] curl: header in: Content-Type: text/plain\n[1043.475534] curl: header in: X-Cloud-Trace-Context: c94fa718797f90f647197d87db4dc6a8;o=1\n[1043.475541] curl: header in: Date: Tue, 06 Dec 2016 17:50:05 GMT\n[1043.475561] curl: info: Server Google Frontend is not blacklisted\n[1043.475568] curl: header in: Server: Google Frontend\n[1043.475576] curl: header in: Content-Length: 9\n[1043.475582] curl: header in: Alt-Svc: quic=\":443\"; ma=2592000; v=\"35,34\"\n[1043.475588] curl: header in:\n[1043.475595] curl: data in: Not_found\n[1043.475608] curl: info: Connection #10 to host gd-ocaml-auth.appspot.com left intact\n``. Ok thanks @astrada, i've followed the Headless instructions, write required values onconfig` file, but since 10mn it only shows\nWarning: Unexpected leaf: name=modified data_type=Scalar in GapiDriveV2Model.Labels.parse \nrepeatedly.... Ok, PPA version is 0.5.25, will it be update to 0.6.x?. ",
    "andreas-lenze": "I had the same issue. I opened the \"gdfuse.log\" file, copied the long \"https:/accounts.google.com/...\" URL into Chrome, and was asked by Google to accept a connection to \"gdfuse\". Things worked from there!. ",
    "apesofhove": "Hi, many thanks for the fantastically quick response. I was almost going to report that it had not worked after installing from the beta repository but then realised I hadn't remounted :-)\nMUCH quicker than the 0.6.4 version with cp/rm script that I had been using (3x + faster)\nMany thanks again.. ",
    "serhatcevikel": "Thanks. By the way, that's a great application.. ",
    "StefanoGITA": "But this is a risk:\nhttp://unix.stackexchange.com/questions/205383/mounting-google-drive-with-google-drive-ocamlfuse\nother ideas?\nStefano G.\n2016-12-19 10:48 GMT+01:00 Alessandro Strada notifications@github.com:\n\nYou should add allow_other to mount options:\ngdfuse#default    /mnt/gdrive fuse uid=1000,gid=1000,allow_other 0 0\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/229#issuecomment-267923006,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AXgeHwSExMdS3A3QCxE-inZjAc8hFYMeks5rJlL6gaJpZM4LQidQ\n.\n. \n",
    "imcatta": "You can create a script that uses inotify. Look at this python package https://pypi.python.org/pypi/inotify. ",
    "supercalhotas": "Closed, some weird OPAM setting, pinned the installation to 0.5.22, and output 0.6.7 to opam show google-drive-ocamlfuse.\nAfter fix it, did a few tests, working like charm with 0.6.7 now :)\nThanks!. ",
    "DerUntote": "i had to reboot my server. Now i will save g-d-ocamlfuse outout to file\ngoogle-drive-ocamlfuse -f -m -d  -cc -label me -id 123123-123123.apps.googleusercontent.com -secret  12312asd3123 ~/data/remote/gdrive  | tee -a \"/home/slave/logs/ocamlfuse.log\nanother issue i found is that no gdfuse.log file is written. it will be created, but has no entries.\n. its not possible to list directories anymore, but it is possible to direct access a file. I know where a file is, access thsi file directly over the full path then i can access it. But it is not possible to list the subdirectory the file is in.\nMy Logfile is 1,7GB but with a tail with this exception\n```\n[111396.803637] TID=1051883: BEGIN: Loading resource /.enc.data (trashed=false) from db\n[111396.803925] TID=1051883: END: Loading resource /.enc.data (trashed=false) from db: Found (id=6, state=ToDownload)\n[111397.172562] TID=1051883: END: Getting folder content (path=/.enc.data, trashed=false)\n[111397.186126] TID=1051883: Exception:Option.No_value\nBacktrace:\n[111397.186390] TID=1051884: releasedir /.enc.data O_RDONLY\n```\nAny idea whats wrong?\n. Hey,\ni just want to provide as much info as possible to fix this annoying issue that sometimes i cant ls/list g-d-ocamlfuse mount directories/subdir. But it is possible to direct access files. \nu delive a great peace of software :) thanks, have a nice year.. ",
    "fhteagle": "I am closing this for now as it is \"working\" at the moment. When I originally posted the bug, \"mount\" did not show the g-d-o mountpoint as mounted. Checked again just because, and sure enough g-d-o mountpoint was on the list. I umounted, rm'd the g-d-o config dir, and went through the authorization steps again and it is working for now. Will update if I have something more solid later.\n. So the g-d-o fuse entry will not show in the output of the mount command when I am offline? . ",
    "fenixjr": "Did a search on an old saved log i had from a crash and didn't see anything like that. it failed on me overnight, but i wasn't running debug that time. I've got it running debug again... see if/when it crashes.. I was on a fresh install anyways, already using 0.6.8. \nsame issue again after a random amount of time. Not \"CURLE\" etc. in the gdfuse.log\n[35071.478455] TID=53491: Allocating memory buffer (remote id=0B48HHR3jhVibcFhvOFBJM2ZOS1E, index=-1773459471708, size=1048576)\n[35071.478483] TID=53491: BEGIN: Stream resource (id=5309, offset=-1859607039005687808, finish=-1859607039004639233, length=1048576)\nThat's the final lines in gdfuse.log. I can double check an old log i have saved, but thats the first time i noticed an insanely large negative number referring to, unless i'm mistaken, the section of the files it's streaming. Everything above it looks significantly more sane.\n. I double checked an old gdfuse.log and didn't see the negative references, so not sure if this is something coming from 0.6.8, but i sent the new log your way.. [415.183022] TID=67355: Error during request: Code: 28, Description: CURLE_OPERATION_TIMEOUTED, ErrorBuffer: Resolving timed out after 5518 milliseconds\nOn the 0.6.9 pulled from beta PPA. This was with 10x default memory cache and buffer settings though, gonna put that back to default as i noticed i was hitting memory limits i think. I just knew that caused issues a lot in the past and i wanted to see if that changed.\nAdditional unrelated observation(and a million variables, so no way to tell,) overall speeds seem to have lowered. I saw speeds upwards of 150MB/s during some initial testing the past couple weeks. now i've seen it top out at like 55MB/s. Again, just an observation. that's still crazy fast compared to what i originally expected, and what is needed, just thought i'd share.. Okay even on default settings, still lost the mount. BUT i saw that it was in fact still a memory error, nothing showed in the log but i saw it in the terminal before the process was killed. Before i start the mount i have about 1.5GB of RAM free. As the program is accessing files it seems to pretty steadily guzzle up memory, and then balances at about 100MB of RAM free for a while, until(i assume) it finally tries to use too much then fails and quits.\nMaybe a leak somewhere?. looks like what happens when i've got permission issues with fuse mounts. try mounting with -o allow_other and see if you can view files then.. ",
    "atomicthumbs": "this seems to time out (maybe?) with no results when there's a large number of files. . ",
    "sbouhnik": "Hi,\nI have ran depmod -a and I got the following error:\nroot@mp3:~# depmod -a\ndepmod: WARNING: could not open /lib/modules/2.6.32-042stab127.2/modules.order: No such file or directory\ndepmod: WARNING: could not open /lib/modules/2.6.32-042stab127.2/modules.builtin: No such file or directory\nCan you help ?\nRegards,\nShmuel. ",
    "orf53975": "+1\n. ",
    "jstraw4663": "Sent. ",
    "peter-kehl": "Alessandro,\nUnfortunately, version 0.6.15 gets an Apps Script as an empty file.  (Potentially related to the fact that Google API shows them with size 0, as they don't count towards user's quota.)\nOutput from google-drive-ocamlfuse -debug <mountpoint> seems to give normal.\nhttps://github.com/astrada/google-drive-ocamlfuse/wiki/Troubleshooting > clearing the cache seems impossible:\n```\ngoogle-drive-ocamlfuse -cc \nClearing cache...done\nfuse: mountpoint is not empty\nfuse: if you are sure this is safe, use the 'nonempty' mount option\ngoogle-drive-ocamlfuse -cc -nonempty \ngoogle-drive-ocamlfuse: unknown option '-nonempty'.\n```\nHow can users install from GitHub source (to experiment)?\nPlease, advise on the above: clearing the cache and installing from GitHub.. Thank you for the fix and explanation.\nThat matches the JSON representation from Google Drive. However, Google Apps Script (https://script.google.com) shows those parts of JSON as separate files.\nIdeally, google-drive-ocamlfuse would split one .json file from G. Drive into separate .js (or .gs) local files (and then join them when pushing back to G. Drive). How feasible would it be?. ",
    "jmoriau": "@scoopydude2002 I'm experiencing the same thing, random mount drops \"Transport endpoint is not connected\".. @scoopydude2002 yeah same, I added a cronjob to unmount/remount every morning.. ",
    "Tangoes": "I recently installed on my headless server, and with fuse, I get 30 Mbits/s speeds... very bad for plex!. ",
    "dany20mh": "@Torsten85 @scoopydude2002 @jmoriau \nI have problem with the mount drops too, it do it around 1-2 days, I still don't know what cause of the problem.\nI appreciate if you guys share how you handle the drop of mount and remounting.\nBecause mine when it drop it's not there but the system say the drive is still mounted until I unmount it.\nI use this code, but it's not working properly when it drops.\n```\nLOCALMOUNTPOINT=\"/Users/danial/mtng\"\nif mount | grep \"on $LOCALMOUNTPOINT\" > /dev/null; then\n    echo \"mounted\"\nelse\n    echo \"not mounted\"\nfi\n```\nSo I appreciate it if you guys help me too.. @SuperGaco\nMake sure you are using allow_other . @SuperGaco \nIt happens to all of us, glad I could help.. @ne0ark \nI never used any of these options, what are those exactly ?\n. Thanks :) . @hjone72 What\u2019s your script for running the mount back again if it drop?. For me, I had more drop with the new beta version and as soon as I revert back to the released version, so far it\u2019s smooth and I haven\u2019t notice any drop yet.. Make sure you use the allow_other in your mount command and that let others to use the mount point.. So you can use it like this :\ngoogle-drive-ocamlfuse -o allow_other ./mountpoint\nAs far as uploading, I don\u2019t do any upload with any of them so you have to try and see which one is work better for you, but make sure don\u2019t use rclone mount, because than cause ban with google.. Thanks for the answer.\nFor the second part, if we disable the stream, can the file download in the chunk instead of whole file and it save in chunk by chunk ?. ",
    "ne0ark": "Isn't large_read deprecated in newer kernels? Also you might want to try auto_cache not sure if works with ocaml (@astrada) ? Anyone tried  direct_io mount option?\n   auto_cache\n          This option enables automatic flushing of the data cache on open(2). The cache will only be flushed if the modification time or the size of the file has changed.\n\n. You really don't need large_read with 16.04. You can use auto_cache.. @jrarseneau mout.fuse uses libfuse if you do man fuse you will see large_read is deprecated. Also you might want to check google api console to see that you are not hitting 100 sec per user limit which is set to 1000.. @astrada you can replicated it if you install https://github.com/Storj/storjshare-daemon and point it to mounted drive.. @astrada PPA :). Seems like something funky is going on:\n140G    /root/.gdfuse/storj/cache\nMy config:\nmax_cache_size_mb=20480\n~/.gdfuse/storj/cache# ls -lath\ntotal 140G\ndrwx------ 3 root root 4.0K Apr 11 14:05 ..\ndrwx------ 2 root root 264K Apr 11 14:05 .\n-rw-r--r-- 1 root root 2.8M Apr 11 14:05 cache.db\n-rw-r--r-- 1 root root 3.5M Apr 11 13:17 0B1Kk_ENv1qSGb3RYaTFnVGlTcDA\n-rw-r--r-- 1 root root 1.0T Apr 11 09:12 0B1Kk_ENv1qSGNDRsRUk4SHYteGM\n-rw-r--r-- 1 root root 1.0T Apr 11 00:26 0B1Kk_ENv1qSGaHRxMnh0UzhyajQ\n-rw-r--r-- 1 root root 1.0T Apr 11 00:21 0B1Kk_ENv1qSGZk5ack1fSnZVZms\n-rw-r--r-- 1 root root 1.0T Apr 10 23:28 0B1Kk_ENv1qSGVFh2ZFdmQWVGcFk\n-rw-r--r-- 1 root root 1.0T Apr 10 23:28 0B1Kk_ENv1qSGQkFqc0dfdndXWHM\nfuse    uid=0,gid=0,auto_cache,default_permissions,allow_other,atomic_o_trunc\n@astrada any clue why its using more then allowed?\n. @astrada I also send you a log with IO error. Would you be able to check it? It seems like cache is still skipping some parts.\nThanks. ",
    "ml3000": "Why do you mount as read only?. I am getting the out of memory error constantly.. I am using Ubuntu 16.04, Within VMware Workstation. The host has 12GB of\nram. The VM is now assigned 4GB. When watching the System Monitor the ram\nusage starts to creep up but then fails before long.\nThe Ubuntu machine by itself usually runs at around 1.5GB. I upped the RAM\nin the machine to 6GB which did not fix the issue. The ram utilization that\ntime didn't even get to 50%\nIt was failing about 20 minutes into any video but I found out if I do a\nlibrary scan I can get it to fail 100% of the time consistently.\nOn Mon, Apr 24, 2017 at 6:41 PM, gts24 notifications@github.com wrote:\n\n@ml3000 https://github.com/ml3000 What version of software, what os,\nwhat hardware?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/281#issuecomment-296867634,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/Aagkf2QFPTgQtZfBjezz5fvF70YrBxBrks5rzU9kgaJpZM4MpSjm\n.\n\n\n-- \n~ Matt Lindsay\n   208.820.1236\n. Yes I do. I changed it to 1GB and still get the error.\nOn Apr 25, 2017 7:24 AM, \"Alessandro Strada\" notifications@github.com\nwrote:\n\n@gabo77 https://github.com/gabo77: Have you tried reducing\nmax_memory_cache (e.g. to 1073741824 = 1GB)? Do you still get OOM errors?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/281#issuecomment-297046691,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/Aagkfy2FdtltUouOR9SeHkqucI5POqW-ks5rzgIYgaJpZM4MpSjm\n.\n. I went ahead and send you the logs. Also, currently I am running 0.6.17, I\ndid try 0.6.18 with the same results.\n\nOn Tue, Apr 25, 2017 at 8:16 AM, 3psus notifications@github.com wrote:\n\nBefore posting, I reduced max_memory_cache_size to 5GB. I've also set an\nalarm to be notified if the server runs OOM. Your script is now running, I\nwill keep you updated.\nI'm running version 0.6.17\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/281#issuecomment-297063755,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/Aagkf3qLxS1Lj9ARP7z33spn_WFPxXmsks5rzg5OgaJpZM4MpSjm\n.\n\n\n-- \n~ Matt Lindsay\n   208.820.1236\n. ",
    "animosity22": "So, are you getting the problem while using a unionfs type mount in addition?\nI've been running solid for a number of days with plex scans, playing, analyzing and it's been working like a charm.\nLast night, I was being bold and was going to add my mount using a unionfs to make this a bit smoother and less hands on. As soon as I added that, I got multiple where my mount got that error and I had to restart the mount. I was tired so I didn't end up saving the logs, but once removed, I went to bed and everything scanned superb.\nI had ~90k API calls over the last 5 hours and no ban that I can tell with all working well.\nI'm basically doing this first:\n```\nMount GD Fuse\n/home/felix/.opam/system/bin/google-drive-ocamlfuse /GD -o allow_other\nMount the 3 Directories via rclone for the encrypt\n/usr/bin/rclone mount \\\n--allow-other \\\n--read-only \\\n--default-permissions \\\n--uid 1000 \\\n--gid 1000 \\\n--umask 002 \\\n--acd-templink-threshold 0 \\\n--buffer-size 100M \\\n--timeout 5s \\\n--contimeout 5s \\\n--syslog \\\n--stats 1m \\\n-v \\\nmedia: /media &\n```\nas my data is encrypted.\nand I was trying to do something like this:\nunionfs -o allow_other /movies=RW:/media/Movies=RO /Movies/\n/movies is my local RW and the RO is my GD mounted decrypted rclone mount.\nI'm super hesitant to touch anything as we watched a number of movies, tv shows over the last few days with superb streaming, no buffering and things just working flawless without the unionfs tossed in the mix.. Are you seeing any issues with Plex? I've been testing over the last few days as my GD mounted and then an rclone mount since I have it encrypted.\nSeems to be streaming/working so much better than ACD. I did a few library scans and it seems flawless. Almost seems to be good to be true.. @tchey290 - I dunno. I seem to be a lucky one as I'm Ubuntu 16.04, gig Fios and my config pretty straight forward:\n``\nmax_cache_size_mb=100000\nmax_memory_cache_size=8000000000\nmax_upload_chunk_size=1099511627776\nmemory_buffer_size=52428800\nmetadata_cache_time=600\nread_ahead_buffers=5\nread_only=false\nstream_large_files=true\n```. My mount is pretty vanilla as the buffering happens in ocamlfuse:\n/usr/bin/rclone mount \\\n--allow-other \\\n--default-permissions \\\n--uid 1000 \\\n--gid 1000 \\\n--umask 002 \\\n--acd-templink-threshold 0 \\\n--syslog \\\n--stats 1m \\\n-v \\\nmedia: /media &. Hmm. I'm getting the same thing once every day or so. I'll setup debug for the next time as it does work superb minus the odd crash.\n[26258.646605] google-drive-oc[10266]: segfault at 7f1171ff9000 ip 00000000005eea70 sp 00007f114dffa5c0 error 4 in google-drive-ocamlfuse[400000+286000]\nI changed my mount to add a debug.. I gzipped up my logs as I got it to happen again. Quite a bit log as almost 40GB uncompressed. I'll email them over.. Hmm. I hit an Out of Memory last night. My server has 32GB of memory and I have a ~16GB max memory set. First time I saw this item pop up and I had plenty of memory left on the system:\nI use netdata/grafana and capture all those items already:\nhttp://imgur.com/u01rztT - App Memory\nhttp://imgur.com/rz7BhLZ - System Memory from the same time\nI was rescanning my TV section last night after fixing a unionfuse mount when the issue occurred. Definitely a lot of access and some 403s, but not too bad. I was running maybe 1-2% rate limit errors on ~20k API calls.. I'll try to reduce it down a little and see if that helps out. I wasn't seeing any 'out of memory' issue from a server though with the bigger numbers. I went for the if I have the memory, use it for this, which maybe was too much?\nI've seen no issues during normal scans or playback as that works like a champ.\n@jrarseneau - are you doing a \"refresh\" for your libraries when you see issues? My normal \"Scan\" libraries run insanely fast once my refresh is done. I do a few things to make sure media is analyzed and run some stats so you can see that info. I use ocamlfuse ->rclone encrypted mount -> unionfus (local storage RW/ rclone mount RO).\n. @mattbator  - if you check dmesg, do you see a crash message for it? . @jrarseneau  - In reading through the mount options, what does:\natomic_o_trunc - what does this do?\nlarge_read seems to be not there and no reason to use:\nlarge_read\n              Issue  large  read  requests.   This  can improve performance for some filesystems, but can also degrade performance. This\n              option is only useful on 2.4.X kernels, as on 2.6 kernels requests size is automatically determined  for  optimum  perfor\u2010\n              mance.\nI use auto_cache on mine instead.. @astrada - I've only had like 1 drop in the last week or so , but I'll give the beta a try and see how that works.. @raidzx - what's your library mainly made of the size? I ran through ~13k TV shows and ~1.5k Movies without an issue. I also have Verizon Fios Gigabit so I see ~400-500Mbs when I did my full scans so it definitely pulled down some data. . I made the change on the on the page as I hit the same issue.. It seems to only go to stderr as I had to grab it in a log and I didn't see those 403s anywhere else. I had to put on debug for them to show up in the mount:\n/usr/bin/google-drive-ocamlfuse /GD -o allow_other,auto_cache -headless -debug >> /home/felix/.gdfuse/default/mount.log 2>&1 &\nI can see the 403s in my \"mount.log\" that I made by getting stderr in there.\nBy I/O error, you mean one file just returns an error? I'm able to still browse / use the file system without need to remount it.. My other item was trying to figure out what's the difference between being synchronized and toDownload?\nI'm thinking Plex is misbehaving and trying to rescan a file instead of just checking the timestamp on on it or I'm not having that file properly in my cache.db\n```\n[162.794636] TID=70522: BEGIN: Getting metadata from context\n[162.794652] TID=70522: END: Getting metadata: Valid\n[163.794494] TID=70523: statfs /\n[106.479018] TID=70435: END: Loading resource /media/smu5ej34ujbdoip1cm3mlk92q4/0erksbdifc83apo7c01v2jqajrhle9i9aljsi603u1jn2lftqdbg (trashed=false) from db: Found (id=195, state=Synchronized)\n[106.479063] TID=70436: getattr /media/smu5ej34ujbdoip1cm3mlk92q4/0erksbdifc83apo7c01v2jqajrhle9i9aljsi603u1jn2lftqdbg/um4qcri48kr7rpa7odk8aamnof1du48m82r6504vpahohr1dvub69n5do6qgv5vbbuqklfcqlq\nhvk\n[106.479071] TID=70436: BEGIN: Getting metadata from context\n[106.479073] TID=70436: END: Getting metadata: Valid\n[106.479075] TID=70436: BEGIN: Loading resource /media/smu5ej34ujbdoip1cm3mlk92q4/0erksbdifc83apo7c01v2jqajrhle9i9aljsi603u1jn2lftqdbg/um4qcri48kr7rpa7odk8aamnof1du48m82r6504vpahohr1dvub69n5do6\nqgv5vbbuqklfcqlqhvk (trashed=false) from db\n[106.479235] TID=70436: END: Loading resource /media/smu5ej34ujbdoip1cm3mlk92q4/0erksbdifc83apo7c01v2jqajrhle9i9aljsi603u1jn2lftqdbg/um4qcri48kr7rpa7odk8aamnof1du48m82r6504vpahohr1dvub69n5do6qg\nv5vbbuqklfcqlqhvk (trashed=false) from db: Found (id=2970, state=ToDownload)\n[106.479273] TID=70437: fopen /media/smu5ej34ujbdoip1cm3mlk92q4/0erksbdifc83apo7c01v2jqajrhle9i9aljsi603u1jn2lftqdbg/um4qcri48kr7rpa7odk8aamnof1du48m82r6504vpahohr1dvub69n5do6qgv5vbbuqklfcqlqhv\nk O_RDONLY\n[106.479280] TID=70437: BEGIN: Getting metadata from context\n[106.479282] TID=70437: END: Getting metadata: Valid\n[106.479285] TID=70437: BEGIN: Loading resource /media/smu5ej34ujbdoip1cm3mlk92q4/0erksbdifc83apo7c01v2jqajrhle9i9aljsi603u1jn2lftqdbg/um4qcri48kr7rpa7odk8aamnof1du48m82r6504vpahohr1dvub69n5do6\nqgv5vbbuqklfcqlqhvk (trashed=false) from db\n[106.479447] TID=70437: END: Loading resource /media/smu5ej34ujbdoip1cm3mlk92q4/0erksbdifc83apo7c01v2jqajrhle9i9aljsi603u1jn2lftqdbg/um4qcri48kr7rpa7odk8aamnof1du48m82r6504vpahohr1dvub69n5do6qg\nv5vbbuqklfcqlqhvk (trashed=false) from db: Found (id=2970, state=ToDownload)\n[106.479617] TID=70438: read /media/smu5ej34ujbdoip1cm3mlk92q4/0erksbdifc83apo7c01v2jqajrhle9i9aljsi603u1jn2lftqdbg/um4qcri48kr7rpa7odk8aamnof1du48m82r6504vpahohr1dvub69n5do6qgv5vbbuqklfcqlqhvk\n [16384 bytes] 0 0\n[106.479640] TID=70432: [106.479648] TID=70438: BEGIN: Getting metadata from context\nEND: Loading resource /media/smu5ej34ujbdoip1cm3mlk92q4/0erksbdifc83apo7c01v2jqajrhle9i9aljsi603u1jn2lftqdbg/7nbk65d8p56121rjooon132narni2h4nrciv16dpbrsmo522oj3ie55qldvd1ma9lp145rvjfhsbc (trash\ned=false) from db: Found (id=2971, state=ToDownload)\n```\nThose are 2 files with one being just 'checked' and not downloaded and the later seems to be \"toDownload\".. I have the following in my config:\nlarge_file_threshold_mb=16\nstream_large_files=true\nSo in theory, any of my stuff should be 'large_files' in my media area.\n[35.269672] TID=44016: END: Loading resource /media/tnvepu36qiohcun8v84ddhsam0/7o55b0s5gogd54idl8c8ape5jd8579gr0oc4tcun9rgicq267b90 (trashed=false) from db: Found (id=1057, state=Synchronized)\nHere is the file size:\nfelix@plex:~$ ls -al /GD/media/tnvepu36qiohcun8v84ddhsam0/7o55b0s5gogd54idl8c8ape5jd8579gr0oc4tcun9rgicq267b90\ntotal 3432745\ndrwxrwxr-x 2 felix felix       4096 Apr 21 21:11 .\ndrwxrwxr-x 2 felix felix       4096 Apr 19 12:51 ..\n-r--r--r-- 1 felix felix 3515121844 Mar 21 06:34 999cogl215i9mr9fn64fjoclmc58klaafom7ulsc73l07a2ifavg\nfelix@plex:~$ du -ms /GD/media/tnvepu36qiohcun8v84ddhsam0/7o55b0s5gogd54idl8c8ape5jd8579gr0oc4tcun9rgicq267b90\n3353    /GD/media/tnvepu36qiohcun8v84ddhsam0/7o55b0s5gogd54idl8c8ape5jd8579gr0oc4tcun9rgicq267b90\nfelix@plex:~$\nHere is my cache:\nfelix@plex:~/.gdfuse/default/cache$ du -ms\n209 .\n. I'm not sure based on that how my file is downloaded to my cache.. I meant to post the log earlier as well.\ngdfuse.log.zip\n. Ah, thanks as I missed that was a directory as the encrypted names are annoying at times. \nSo it's most definitely plex being annoying than as it's supposed to just check if it's the same file and not open it.\nAppreciate the help as I'll dig into the plex logs a bit more.. So I played around a bit and found something that I settled on. I figured I'd share my settings/mount stuff in case someone wants to try to use the the same thing.\n```\nMount GD Fuse\n/usr/bin/google-drive-ocamlfuse /GD -o allow_other -debug &\n/usr/bin/google-drive-ocamlfuse /GD -o allow_other,auto_cache -headless -debug >> /home/felix/.gdfuse/default/mount.log 2>&1 &\nsleep 2\nMount the 3 Directories via rclone for the encrypt\n/usr/bin/rclone mount \\\n--allow-other \\\n--read-only \\\n--default-permissions \\\n--uid 1000 \\\n--gid 1000 \\\n--umask 002 \\\n--acd-templink-threshold 0 \\\n--buffer-size 100M \\\n--timeout 5s \\\n--contimeout 5s \\\n--syslog \\\n--stats 1m \\\n-v \\\nmedia: /media &\nWait a sec\nsleep 2\n/usr/bin/unionfs-fuse -o cow,allow_other,direct_io,auto_cache,sync_read /movies=RW:/media/Movies=RO /Movies\n/usr/bin/unionfs-fuse -o cow,allow_other,direct_io,auto_cache,sync_read /tv=RW:/media/TV=RO /TV\n```\n/GD is my GDrive\n/media is an encrypted rclone mount.\nI have a local /tv and /movies directories.\nI use /TV and /Movies for my fuse mounts that combines everything.\nI'll close this item out as everything seems to be working as intended from my perspective. Appreciate the clarifications and the help!. Can you see what's accessing those files? The product really doesn't \"do\" anything unless you have something accessing the data that is mounted.. @shadowsbane0 - turn on -verbose on your mounts options and you can see what files are being accessed. You can also run \"lsof \"</MOUNTPOINT\" as root to see what files are currently being used. You have some processing that's doing something causing the outbound data as it's not the mount directly.. @mikeferr - Emby does a 'ffprobe' of each file on first scan and it depends on how the mount handles opening up the file. You can run rclone with -vv to see the difference or run this with debug or verbose. It only does that probe one time unless the mod time on the file changes. You would really need @astrada to comment on how seeking of a file works or if it grabs a full file.\nIf you ever lose your mount and rescan, you have to re-ffprobe the file again as it doesn't retain the info. Plex handles that via 'do not delete on scan' or something along those lines. \nTo me, that's really understanding how the tool works and using it for the proper use-case. Emby with ffprobe doesn't play nice with this, but rclone does. Rclone doesn't play nice with Plex, but this really does.. That seems to be ok when I did some testing with ffprobe as it works as I expected.\n@mikeferr  - did you have sream_large_files=true setup? I ran a ffprobe with that and it seems to work properly:\n```\nfelix@plex:/media/TV/NCIS$ time ffprobe NCIS.S13E19.HDTV-1080p.mkv\nffprobe version 2.8.11-0ubuntu0.16.04.1 Copyright (c) 2007-2017 the FFmpeg developers\n  built with gcc 5.4.0 (Ubuntu 5.4.0-6ubuntu1~16.04.4) 20160609\n  configuration: --prefix=/usr --extra-version=0ubuntu0.16.04.1 --build-suffix=-ffmpeg --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --cc=cc --cxx=g++ --enable-gpl --enable-shared --disable-stripping --disable-decoder=libopenjpeg --disable-decoder=libschroedinger --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmodplug --enable-libmp3lame --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-librtmp --enable-libschroedinger --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxvid --enable-libzvbi --enable-openal --enable-opengl --enable-x11grab --enable-libdc1394 --enable-libiec61883 --enable-libzmq --enable-frei0r --enable-libx264 --enable-libopencv\n  libavutil      54. 31.100 / 54. 31.100\n  libavcodec     56. 60.100 / 56. 60.100\n  libavformat    56. 40.101 / 56. 40.101\n  libavdevice    56.  4.100 / 56.  4.100\n  libavfilter     5. 40.101 /  5. 40.101\n  libavresample   2.  1.  0 /  2.  1.  0\n  libswscale      3.  1.101 /  3.  1.101\n  libswresample   1.  2.101 /  1.  2.101\n  libpostproc    53.  3.100 / 53.  3.100\nInput #0, matroska,webm, from 'NCIS.S13E19.HDTV-1080p.mkv':\n  Metadata:\n    encoder         : libebml v1.3.1 + libmatroska v1.4.2\n    creation_time   : 2016-03-22 20:45:47\n  Duration: 00:43:27.98, start: 0.000000, bitrate: 12898 kb/s\n    Stream #0:0(eng): Video: h264 (High), yuv420p(tv, bt709/unknown/unknown), 1920x1080, SAR 1:1 DAR 16:9, 23.98 fps, 23.98 tbr, 1k tbn, 47.95 tbc (default)\n    Metadata:\n      BPS             : 11386481\n      BPS-eng         : 11386481\n      DURATION        : 00:43:27.939000000\n      DURATION-eng    : 00:43:27.939000000\n      NUMBER_OF_FRAMES: 62528\n      NUMBER_OF_FRAMES-eng: 62528\n      NUMBER_OF_BYTES : 3711906229\n      NUMBER_OF_BYTES-eng: 3711906229\n      _STATISTICS_WRITING_APP: mkvmerge v8.4.0 ('A better way to fly') 64bit\n      _STATISTICS_WRITING_APP-eng: mkvmerge v8.4.0 ('A better way to fly') 64bit\n      _STATISTICS_WRITING_DATE_UTC: 2016-03-22 20:45:47\n      _STATISTICS_WRITING_DATE_UTC-eng: 2016-03-22 20:45:47\n      _STATISTICS_TAGS: BPS DURATION NUMBER_OF_FRAMES NUMBER_OF_BYTES\n      _STATISTICS_TAGS-eng: BPS DURATION NUMBER_OF_FRAMES NUMBER_OF_BYTES\n    Stream #0:1: Audio: dts (DTS), 48000 Hz, 5.1(side), fltp, 1536 kb/s (default)\n    Metadata:\n      BPS             : 1509749\n      BPS-eng         : 1509749\n      DURATION        : 00:43:27.979000000\n      DURATION-eng    : 00:43:27.979000000\n      NUMBER_OF_FRAMES: 244498\n      NUMBER_OF_FRAMES-eng: 244498\n      NUMBER_OF_BYTES : 492174474\n      NUMBER_OF_BYTES-eng: 492174474\n      _STATISTICS_WRITING_APP: mkvmerge v8.4.0 ('A better way to fly') 64bit\n      _STATISTICS_WRITING_APP-eng: mkvmerge v8.4.0 ('A better way to fly') 64bit\n      _STATISTICS_WRITING_DATE_UTC: 2016-03-22 20:45:47\n      _STATISTICS_WRITING_DATE_UTC-eng: 2016-03-22 20:45:47\n      _STATISTICS_TAGS: BPS DURATION NUMBER_OF_FRAMES NUMBER_OF_BYTES\n      _STATISTICS_TAGS-eng: BPS DURATION NUMBER_OF_FRAMES NUMBER_OF_BYTES\nreal    0m2.031s\nuser    0m0.020s\nsys 0m0.004s\n```\nRoughly 2 seconds for a 4G file is pretty fast.. @mikeferr - that would download every single file on the plex scan. Also, ACD doesn't support mod times and gogle does. So, you may have to re-analyze if you change from ACD to GD in Plex, which is time consuming and annoying.. A torrent use case is not really a good one for cloud storage since it has to read entire files and such. I'd recommend using local storage for that over a cloud one as the IOPs needed for torrenting would be painful.. Ok. I'm just trying to figure this out:\n/GD -my ocamlfuse mount\n/TV - my ocamlfuse and local storage unionfs-fuse mount \nThe /GD always takes 10 seconds to list out which doesn't seem bad.\nMy /TV mount takes 30 - 60 seconds the first time, but is instant after that.\n```\nfelix@plex:/GD/media/smu5ej34ujbdoip1cm3mlk92q4$ time ls -alR | wc -l\n15317\nreal    0m11.174s\nuser    0m0.264s\nsys 0m0.564s\nfelix@plex:/GD/media/smu5ej34ujbdoip1cm3mlk92q4$\nfelix@plex:/GD/media/smu5ej34ujbdoip1cm3mlk92q4$\nfelix@plex:/GD/media/smu5ej34ujbdoip1cm3mlk92q4$ time ls -alR | wc -l\n15317\nreal    0m10.796s\nuser    0m0.156s\nsys 0m0.520s\nfelix@plex:/GD/media/smu5ej34ujbdoip1cm3mlk92q4$\nfelix@plex:/GD/media/smu5ej34ujbdoip1cm3mlk92q4$\nfelix@plex:/GD/media/smu5ej34ujbdoip1cm3mlk92q4$ time ls -alR | wc -l\n15317\nreal    0m10.756s\nuser    0m0.224s\nsys 0m0.488s\nfelix@plex:/GD/media/smu5ej34ujbdoip1cm3mlk92q4$ time ls -alR /TV | wc -l\n14936\nreal    0m1.551s\nuser    0m0.040s\nsys 0m0.060s\nfelix@plex:/GD/media/smu5ej34ujbdoip1cm3mlk92q4$ time ls -alR /TV | wc -l\n14936\nreal    0m0.498s\nuser    0m0.048s\nsys 0m0.044s\nfelix@plex:/GD/media/smu5ej34ujbdoip1cm3mlk92q4$ time ls -alR | wc -l\n15317\nreal    0m10.860s\nuser    0m0.212s\n```\nI'm not sure what is causing my unionfs mount to respond so much faster than the other mount.\n/usr/bin/unionfs-fuse -o cow,allow_other,direct_io,auto_cache,sync_read /local/tv=RW:/media/TV=RO /TV\nMy GD mount:\n/usr/bin/google-drive-ocamlfuse /GD -o allow_other,auto_cache -verbose >> /home/felix/.gdfuse/default/mount.log 2>&1 &\nAny thoughts?. Seems like when I do a ls -alR on the mount, it does check every file form the db:\n[1484.495524] TID=210828: BEGIN: Loading resource /media/smu5ej34ujbdoip1cm3mlk92q4/vvph2hckkfr210fkn0ev5m6lp9lfvj7m49v3flr7v144h523h9c0/1ads5a714a6e696rq278kvdpuht9rrvri6775uhe39totv930h34c0sfmh9f015ftvbljgvho68b0 (trashed=false) from db\n[1484.495683] TID=210828: END: Loading resource /media/smu5ej34ujbdoip1cm3mlk92q4/vvph2hckkfr210fkn0ev5m6lp9lfvj7m49v3flr7v144h523h9c0/1ads5a714a6e696rq278kvdpuht9rrvri6775uhe39totv930h34c0sfmh9f015ftvbljgvho68b0 (trashed=false) from db: Found (id=14857, state=ToDownload)\nbut on my fuse mount, it seems to never go back to /GD until a ~5 minutes or so go by and it rechecks.. Cool. Thanks for the info.. That helps a bit. So I'm configured bigger than I wanted and I can tweak my config to do what I want. Thanks for the detailed reply.\nI'm going to drop down to 50M so I get around 250M with the 5 threads in my buffer. That should be plenty.. ",
    "tjustice86": "@scoopydude2002 Is there a link on a how-to to setup my VPS like you have yours? I'm running into issues and I'm not too linux-savy.. Once I have the provider enable fuse, will I be able to mount the drive?. ",
    "GottZ": "@natoriousbigg how do you get around sudden unmounts of ocaml-fuse? i experienced it to be quite unstable in the long run. does not even try to reconnect in such a state but lags the whole system on each file system access..\ntbh right now i'm even creating my own google drive fuse wrapper to get around that but i'd like to avoid spending too much time on that.\nfun fact: i even have plex cloud.. its horrible. don't use it.. ",
    "mjr0483": "What is 500GB in reference to here? I only have 128GB SSD and this value seems way out?\nmax_cache_size_mb=512000 ## 500GB. ",
    "fylla12": "I think I have figured it out. For  a debian test system I tried to install\nit from the source code.\nI used the instruction found at\ngithub.com/astrada/google-drive-ocamlfuse/wiki/\nHow-to-install-from-source-on-Debian-Jessie\nThe steps\n\"\n                                  opam install depext\n                                  opam depext google-drive-ocamlfuse\n\"\nwas not mentioned there.  I included those steps and the following did work.\nopam init\nopam update\nopam install depext\nopam depext google-drive-ocamlfuse\nopam install google-drive-ocamlfuse\n. /home/user/.opam/opam-init/init.sh > /dev/null 2> /dev/null || true\nBut the where binary ended up was not clear because\n\"~/.opam//bin/google-drive-ocamlfuse\" hinted in the instruction did not\nexist.\nAt my system it ended up as\n/home/XXXX/.opam/system/bin/google-drive-ocamlfuse\nI could mount my google drive.\nSome of the hang issues I had experienced seems to be gone and not to be\nmissed.\nOn 1 February 2017 at 18:57, Alessandro Strada notifications@github.com\nwrote:\n\nYou should install external dependencies before installing this app:\nopam install depext\nopam depext google-drive-ocamlfuse\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/249#issuecomment-276747050,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AYTuRfnQqKMKsi2CZrlGOtj7v4SAn07Zks5rYNWLgaJpZM4L0OmW\n.\n. \n",
    "narayana1043": "Sorry for the confusion.\nThis is the folder.\n/home/$USERNAME/gdrive\nInitially I couldn't get rid of this folder, However I managed to login as root immediately after booting and deleted it from there.\nThe problem I noticed was that the folder permissions are\ndr-x------\n. ",
    "jmage619": "Hi, sorry for commenting on an old thread, but are there any plans to allow the shared_with_me files to be writeable if you were granted write access?. ",
    "Stayingfalse": "Enabling Stream Large Files in config has eradicated this issue though with it disabled it seemed to repeat consistently. I assume it was some form of memory/cache issue as some of the files were 5GB+ in size. ",
    "YipYup": "@astrada Is it possible to pass the -debug parameter when mounting in /etc/fstab ?\nHere is what I currently have in my fstab.\ngdfuse#UserLabelHere      /mnt/gdrive  fuse    rw,allow_other  0       0. One more example.\nThread 680595 killed on uncaught exception Failure(\"Error: Invalid request.  There were 820510720 byte(s) in the request body.  There should have been 1415072573 byte(s) (starting at offset 0 and ending at offset 1415072572) according to the Content-Range header. (HTTP response code: 400)\"). Thread 1024388 killed on uncaught exception Failure(\"Error: Invalid request.  There were 996212736 byte(s) in the request body.  There should have been 1491472403 byte(s) (starting at offset 0 and ending at offset 1491472402) according to the Content-Range header. (HTTP response code: 400)\"). From what I can tell, each time this happens, the file transfer from cache to Google Drive does not finish properly, and I end up with a 0-byte file on my Google Drive, to say that whatever file I am moving or copying, ends up bad on Google Drive. I suppose this is acceptable if you're doing a cp instead of a mv, but It has resulted in losing my data when using mv.. Thread 1856853 killed on uncaught exception Failure(\"Error: Invalid request.  There were 630784000 byte(s) in the request body.  There should have been 991414107 byte(s) (starting at offset 0 and ending at offset 991414106) according to the Content-Range header. (HTTP response code: 400)\")\nSegmentation fault. Have you tried clearing the cache, or checking your caching settings?. Can you confirm your mount is still active? I do something like:\ndf -h. @guestname You would still have potentially \"missing\" files that weren't cached. Depending on programs you use to access your mount, this could cause undesired behavior. Imagine a database engine which is accessing a single database spanned over multiple flat files. If a few flat files were missing (due to being offline), and only some parts of it were cached, your program could behave very unexpectedly. It's almost safer to present all or nothing, as it is ultimately up to individual applications for how they would handle the scenario of a missing file. Since there is no guaranteed commonality, the best universal option for @astrada is to offer it as Online, or Offline and unavailable -- the same behavior a typical volume mount would follow. Then the application itself accessing the volume can decide how to handle the volume state.. ",
    "WihanB": "What would you recommend I do? Call flush from within my python script (how would this affect performance?) or is there a way to periodically flush? maybe I could write a function that does that in an external background script?\n\nOn 27 Feb 2017, at 8:21 PM, Alessandro Strada notifications@github.com wrote:\nUpload starts only after calling flush(), release() or fsync() on the specific file.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/astrada/google-drive-ocamlfuse/issues/261#issuecomment-282804539, or mute the thread https://github.com/notifications/unsubscribe-auth/ASl1q4N129m-zaw-wZp-frztSy8hrrN9ks5rgxQ9gaJpZM4MNFav.\n\n\n-- \nThis message and attachments are subject to a disclaimer. Please refer to \nhttp://www.it.up.ac.za/documentation/governance/disclaimer/ for full \ndetails.. Thanks so much,\nIt works now, calling os.fsync() after each write command did the trick\n. ",
    "drkitty28": "I also have a problem with authorization.  The google-drive-ocamlfuse command does not open a web page.. I just checked the xdg-open command and on its own, it does open the provided url. So, it must be how it's called from google-drive-ocamlfuse?   I'm using Ubuntu 16.04 and google-drive-ocamlfuse 0.6.17.. ",
    "mrvonwyl": "I found a solution now: for me the problem was, that my apt repos wre somehow misconfigured. Removing the mentioned package led to more problems, because some dependencies from different repos and debian versions were blocking each other.\nLong storie short, I removed libgnutls-deb0-28, fixed my repos installed everything that was needed, and then it worked.\nThanks for the tip!. ",
    "themaddoctor": "Something similar for me:\ngoogle-drive-ocamlfuse mounts/GoogleDrive -id XXXX -secret XXXX\nPlease enter the verification code: XXXX\nCannot retrieve auth tokens.\nFailure(\"Code: 60, Description: CURLE_SSL_CACERT, ErrorBuffer: SSL certificate problem: unable to get local issuer certificate\\n\")\n. same error\n. ocaml-sqlite requires ocaml-base, which does not compile.\nbase-0.11.0 # jbuilder build\n      ocamlc src/.base.objs/base__Formatter.{cmi,cmti} (exit 2)\n(cd _build/default && /usr/bin/ocamlc.opt -w -40 -safe-string -g -bin-annot -I src/.base.objs -I /usr/lib64/ocaml/site-lib/sexplib0 -I compiler-stdlib/src/.caml.objs -I shadow-stdlib/src/.shadow_stdlib.objs -no-alias-deps -open Base__ -o src/.base.objs/base__Formatter.cmi -c -intf src/formatter.mli)\nFile \"src/formatter.mli\", line 9, characters 9-30:\nError: Unbound module Caml.Format\n      ocamlc src/.base.objs/base__Import0.{cmi,cmo,cmt} (exit 2)\n(cd _build/default && /usr/bin/ocamlc.opt -w -40 -safe-string -g -bin-annot -I src/.base.objs -I /usr/lib64/ocaml/site-lib/sexplib0 -I compiler-stdlib/src/.caml.objs -I shadow-stdlib/src/.shadow_stdlib.objs -no-alias-deps -open Base__ -o src/.base.objs/base__Import0.cmo -c -impl src/import0.ml)\nFile \"src/import0.ml\", line 7, characters 6-394:\nError: The signature constrained by with' has no component named Pervasives\nbase-0.11.0 # dune build\n      ocamlc src/.base.objs/base__Import0.{cmi,cmo,cmt} (exit 2)\n(cd _build/default && /usr/bin/ocamlc.opt -w @a-4-29-40-41-42-44-45-48-58-59-60-40 -strict-sequence -strict-formats -short-paths -keep-locs -safe-string -g -bin-annot -I src/.base.objs -I /usr/lib64/ocaml/site-lib/sexplib0 -I compiler-stdlib/src/.caml.objs -I shadow-stdlib/src/.shadow_stdlib.objs -no-alias-deps -open Base__ -o src/.base.objs/base__Import0.cmo -c -impl src/import0.ml)\nFile \"src/import0.ml\", line 7, characters 6-394:\nError: The signature constrained bywith' has no component named Pervasives\n      ocamlc src/.base.objs/base__Formatter.{cmi,cmti} (exit 2)\n(cd _build/default && /usr/bin/ocamlc.opt -w @a-4-29-40-41-42-44-45-48-58-59-60-40 -strict-sequence -strict-formats -short-paths -keep-locs -safe-string -g -bin-annot -I src/.base.objs -I /usr/lib64/ocaml/site-lib/sexplib0 -I compiler-stdlib/src/.caml.objs -I shadow-stdlib/src/.shadow_stdlib.objs -no-alias-deps -open Base__ -o src/.base.objs/base__Formatter.cmi -c -intf src/formatter.mli)\nFile \"src/formatter.mli\", line 9, characters 9-30:\nError: Unbound module Caml.Format\n. No, not really. Building opam requires some things that require opam. Anyway, i got the damn thing built, but get this error:\n```\n401. That\u2019s an error.\nError: invalid_client\nThe OAuth client was not found.\nRequest Details\nclient_id=thomas.a.XXXXXXX\nredirect_uri=urn:ietf:wg:oauth:2.0:oob\nscope=https://www.googleapis.com/auth/drive\nresponse_type=code\naccess_type=offline\napproval_prompt=force\n\nThat\u2019s all we know.\n```\n. Failure(\"Code: 60, Description: CURLE_SSL_CACERT, ErrorBuffer: SSL certificate problem: unable to get local issuer certificate\\n\")\n. This is what happens. I do have a bunch of certificates in /etc/ssl/certs (but they are possible old).\ncurl -v https://www.googleapis.com\n\nRebuilt URL to: https://www.googleapis.com/\nHostname was NOT found in DNS cache\nTrying 64.233.185.95...\nConnected to www.googleapis.com (64.233.185.95) port 443 (#0)\nSSLv3, TLS handshake, Client hello (1):\nSSLv3, TLS handshake, Server hello (2):\nSSLv3, TLS handshake, CERT (11):\nSSLv3, TLS alert, Server hello (2):\nSSL certificate problem: unable to get local issuer certificate\nClosing connection 0\ncurl: (60) SSL certificate problem: unable to get local issuer certificate\nMore details here: http://curl.haxx.se/docs/sslcerts.html\n\ncurl performs SSL certificate verification by default, using a \"bundle\"\n of Certificate Authority (CA) public keys (CA certs). If the default\n bundle file isn't adequate, you can specify an alternate file\n using the --cacert option.\nIf this HTTPS server uses a certificate signed by a CA represented in\n the bundle, the certificate verification probably failed due to a\n problem with the certificate (it might be expired, or the name might\n not match the domain name in the URL).\nIf you'd like to turn off curl's verification of the certificate, use\n the -k (or --insecure) option.\n. Updating libcurl fixed it, even without a new cert bundle. Thanks for your time.\nI can access stuff on the GD from the command line, but something about it is causing Konqueror (my file manager) to freeze up. Should I open an issue for that?\n. ",
    "DragonDon": "Seems that it was a permissions thing as \"See #624 for a fix: first sudo enter-chroot, then sudo chown -R 1000:1000 \"$HOME\".\" solved the issue.\nThanks!. ",
    "lzhqweq1": "Thank you for you help\ntdrobiei@mail.ccsf.edu\nFrom: Alessandro Strada\nDate: 2017-03-05 00:40\nTo: astrada/google-drive-ocamlfuse\nCC: lzhqweq1; Manual\nSubject: Re: [astrada/google-drive-ocamlfuse] How to use in Centos? (#264)\nTry with opam.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. ",
    "jaredneedell": "I should note if I clear the cache using -cc, I can get the folder to remount. Is it possible to force ocaml to use a different folder for the cache? I have a mount point that has 500GB of free space which is more forgiving than the 10GB it has right now working out of the home folder.. Thanks @astrada I'll give it a try\n@pisto \nI have that setting set but doesn't seem to do anything.\nstream_large_files=true\n. @astrada I changed the cache dir, it never got over 1mb and I was syncing my music folder to Plex when I got the error endpoint not connected. Took about 20 minutes.. ",
    "pisto": "@jaredneedell you should probably look into force streaming of large files, that's what I do.. ",
    "memnos": "OK, tnx.\nGot it.\n. ",
    "KenSharp": "Works fine for me with Xenial. Even popped up recently to tell me that my Drive was almost full so it appears to be working correctly.. Have you tried running the Linux binary (with the required libraries installed, probably from the ports)?. You can download from Google Photos, but uploading does not work. At least not the way intended.. I guess you're not interested.. What does this have to do with the project?. ",
    "mtrambaioli": "please ignore wrong post. ",
    "drgroot": "It's not really an issue, seeing as it works completely fine in the shell (if I am to use the cp comand)\nI think it's because of the way Finder treats the mounted file system. Just something to be aware of, maybe mention it in the wiki or something?\nI haven't tried indexing it with spotlight (I'd rather not due to the large number of possible API requests), but I plan to make a small test folder and see what happens when spotlight indexes it. . ",
    "trulow": "I am also getting this issue, I wanted to try this out since it's been reported to work with Plex and not hit Googles API limit. Unfortunately all my files are showing up as Zero KB.. ",
    "cddoma": "I was able to get Finder working properly by adding either osxfuse mount options 'noapplexattr' or 'auto_xattr'.\ngoogle-drive-ocamlfuse -o auto_xattr mountpoint. ",
    "timothyclemansinsea": "Nvm hadn't enabled gdrive. ",
    "qazero": "Yes, no luck. . There seems to be a bug because each time the drive is remounted the configuration file is overwritten with the default template. I understand that the configuration file is rebuilt every time but should't the tweaks to the original template be migrated to the rebuilt config each time the drive is remounted? If not, then this is a huge logic issue.\nFor example, in the original template, the \"stream_large_files\" setting is set to false but in my tweaked config I have this set to true as well as many other tweaks to other settings as well. I have a script which I run on cron to check if google-drive-ocamlfuse is still mounted and if it isn't then I remount it. The problem arises when the drive is remounted because the modified config I am using gets overwritten. Therefore, once the drive is remounted the performance is horrible making it unable for my needs. But when I am using my tweaked modified config file the performance is excellent.\nThat being said, what would you recommend to fix this? Am I doing something wrong? Settings in the config file shouldn't need to be re-entered each time the drive is remounted, they should be persistent. Please advise, thanks.. Why would the original config get overwritten? I can assure you the file is/was present but every so often I find the values get overwritten. It doesn't happen every time the drive is re-mounted but it does happen every so often for some reason.... ",
    "batgau71": "Hello ! \nAny news for the question of qazero ? I'm also not able to chmod any file or directory ..\nThanks a lot. ",
    "foats": "@astrada: Do I need to do the same to get an answer?  I'm trying to chmod 755 my files and I get \"chmod: changing permissions of '' : Value too large for defined data type.  I've tried several different mounting options with no luck. ",
    "bkayes": "I just ran into this same issue, and using -o allow_other fixed it for me.. Take a look at #277 . ",
    "EnorMOZ": "Updated to 0.6.18 and the same issue occurs as above. Am I doing something wrong ? Is there a way to have it produce the actual umask necessary or is this a bug ?. These are mostly video files. Are you unable to reproduce all of the issues or just the umask part ? We could do a screen share if that would help. . Thanks for the info will double check the configs when I can to see if that last condition are both true. . stream_large_files=true and large_file_read_only=true were both set so I guess that is why the -r--r--r--  I need stream_large_files=true since it would download the entire file vs chunks when that is false. And I need to be able to replace the existing large file so once I changed large_file_read_only=false it set the expected umask of -rw-rw-r--. . I am not seeing 0.6.19 via opam.\n$ /usr/local/bin/google-drive-ocamlfuse -version\ngoogle-drive-ocamlfuse, version 0.6.18\nCopyright (C) 2012-2017 Alessandro Strada\nLicense MIT\n```\n$ opam pin add google-drive-ocamlfuse https://github.com/astrada/google-drive-ocamlfuse.git#beta\n[NOTE] Package google-drive-ocamlfuse is already git-pinned to https://github.com/astrada/google-drive-ocamlfuse.git#beta.\n       This will erase any previous custom definition.\nProceed ? [Y/n] Y\n[google-drive-ocamlfuse] https://github.com/astrada/google-drive-ocamlfuse.git#beta updated\n[google-drive-ocamlfuse] Installing new package description from https://github.com/astrada/google-drive-ocamlfuse.git#beta\n\n$ opam list google-drive-ocamlfuse\nAvailable packages for system:\ngoogle-drive-ocamlfuse  0.6.17 (pinned)  A FUSE filesystem over Google Drive\n\n$ opam upgrade\nEverything as up-to-date as possible (run with --verbose to show unavailable upgrades).\n```. I see the issue now. It installed it under a different path. Running 0.6.19 now. ",
    "DocML": "Yes, of course I installed opam install depext. But it did not installed all dependencies for Debian Jessie.\nI found a description that said to run: sudo apt-get install m4 libcurl4-gnutls-dev libfuse-dev libsqlite3-dev\nThis installed the missing packages and and I could follow the rest of the instructions.\nThank you!. ",
    "jrarseneau": "I should add that my primary use is for Plex, so I'm streaming multi-gigabyte files regularly. Plex also has an intensive media scanner that index/analyzes the files quite often (all that to say that there is a lot of activity).\n@ne0ark I've used large_read with other FUSE systems (primarily node-gdrive-fuse) and never had drops.\n. @ne0ark I'm only peaking about 3req/s so no where near the limits.\nMy error ratio is 0.18% though, which seems a bit high.. Logs sent!\nThanks!. @astrada \nAny updates to this? Have you had a chance to review the logs? I have had the mount drop a few times this week since sending you the logs last weekend.\nThanks!. This past weekend had about 8-10 drops. What was odd is these drops were both clustered in 2 instances where it dropped about 4 times in 5 minutes.\n. Same. PPA\nAnd thanks for this!. Testing with opam, was quite easy to switch and I wanted to provide feedback as quick as possible.\nJust mounted it now, so will post back when/if it drops. . @buttpain \nThis may be a separate issue? If as you state above, your \"media becomes unavailable\", and then you indicate you had a 24 hour ban, this would explain why your media is unavailable. \nNow this may be related to something google-drive-ocamlfuse is doing, but I'm not entirely convinced it's the same issue as this thread?\nThis thread is about the google-drive-ocamlfuse process dying unexpectedly. The process is killed, the mount is dropped. It is not about files becoming unavailable or bans.\n. Ah ok.. I'm still running on the initial mount from a few days ago, hasn't dropped yet. But I have gone a week before without a drop.\n. @astrada My memory usage is fine (~1-1.5GB, but that's based on my config), here are the settings I'm using for the above configs.\nNo crashes yet, however I have had to restart my server due to a glitch (not related) so my counter is restarted since yesterday the 9th.\nmax_memory_cache_size=1073741824\nmemory_buffer_size=8388608\nread_ahead_buffers=5\n. Hi\nAny movement on this @astrada ?\n. Yes, but not drops (yet), but I've rebuilt my server due to a hard drive failure.\nI'm sending you the logs of an issue I'm having that's crashing some applications that try to perform actions on a mounted folder (did not happen in 0.6.17).\n. @animosity22 \nI don't do manual Plex refreshes. I have the setting to scan only on library changes and it only scans the folder affected. \nI also no longer use large_read. \nI have not gotten a drop in over a week with 0.6.17, but after a bit, I do get this weird issue where the files on google drive can't be read properly as an ls -l provides:\n?????????? ? ?        ?           ?            ? Redacted - S03E03 - Redacted.mkv\nI am not seeing OOM errors here, but I am now running the script. I cannot use 0.6.18 as it wrecks havoc on my system with Plex, Sonarr and Radarr throwing errors about not being able to modify files on the UnionFS mount (even though the mount hasn't changed from 0.6.17 to .18) so not sure what's going on, but hope that 0.6.18 doesn't go live with that change as I'll have to stick to .17\n. Is the beta on the PPA?. ",
    "buttpain": "I've got exactly the same issue and i'm using it for plex. It also seems to delete the folder which it was mounted to, until i reboot and the folder comes back. A shame really considering this is by far the best performing solution for mounting google drive for streaming on plex. But for me it seems to be far more frequent, about every 5-10 minutes. Yeah it's ppa for me to, thanks for the possible fix I'm looking forward to it :). Does anybody know of a temporary solution? like for detecting when it breaks you are able to unmount the drive and quickly remount. Tried using inotify-tools but it isn't being set off when the drive breaks.. Thanks for the response brancomat, actually got a reaaallllyyy ghetto solution working just a couple hours ago.  \nls -l /var/lib/plexmediaserver/media/plex/log.txt\nif [ -rw-r--r-- 1 root root 20904 Apr  4 17:34 /var/lib/plexmediaserver/media/plex/log.txt ]; then\n    echo OK\nelse\n    fusermount -z -u /var/lib/plexmediaserver/media/ & google-drive-ocamlfuse -label me /var/lib/plexmediaserver/media/ -o allow_other,auto_cache\nfi\nThat script checks the properties of a log.txt file created on my google drive, and if it gets backs the properties of the log file then nothing will happen. But if it doesn't get back the properties then the drive gets unmounted and then immediately remounted. Now to have it be running constantly I had this command in the /etc/rc.local file\nsudo -H -u root screen -d -m watch -n1 sh /etc/init.d/boot.sh\nThis means that every second my script that I made will be run, checking if the log file on the mounted drive is still accessible, and if not unmount, remount.. Would really like to test it out, but i can't seem to get opam to work on my vps.. Unfortunately this doesn't fix the issue, I analyse my library in plex and before it can finish all my movies become unavailable.. will do when my 24 hour api ban ends. But from what I remember from when I checked before there was nothing in the log which seemed to indicate that anything had gone wrong which seemed very odd.. Oh no, the 24 hour ban happened because I tried too many time to update my plex library, after I tested google-drive-ocamlfuse and started using rclone mount instead. . Yeah when I'm just streaming content off of plex it's totally fine. But issues arise when ever I attempt to analyse, or update the library. I forgot to post this a couple days ago, but the latest beta uses an insane amount of RAM. Filling up all 8GB of the RAM i have on my VPS and causing it to shutdown by streaming one file.. ",
    "lcasey001": "Haven't had the same frequency as the others above using OPAM, not sure if that makes a difference or not but willing to test the OPAM version here.. ",
    "haljordan2814": "In for PPA version. thanks for looking into this. @ml3000 What version of software, what os, what hardware?. https://www.linode.com/docs/applications/cloud-storage/access-google-drive-linode  this should assist. (this may also assist with the install process as well http://www.webupd8.org/2013/09/mount-google-drive-in-linux-with-google.html ) . ",
    "hjone72": "I found that having -debug on caused my curl.log to get so large that ocaml crashed.\nThread 20284 killed on uncaught exception Sys_error(\"No space left on device\")\nThread 20286 killed on uncaught exception Sys_error(\"No space left on device\")\nThread 20287 killed on uncaught exception Sys_error(\"No space left on device\")\nThread 20283 killed on uncaught exception Sys_error(\"No space left on device\")\nThread 20285 killed on uncaught exception Sys_error(\"No space left on device\"). @astrada, thank you. I've added your beta repository and run sudo apt-get upgrade is there any way to ensure that I am running your latest version? I am still facing an unmount issue.. @astrada Definitely running the beta. While the mount in foreground mode -f it randomly unmounts displaying a message Fatal error: out of memory.. I've got a script running that remounts it so I've lost the logs this time, but next time it drops i'll post them.. @dany20mh, its a modification of this one. https://gist.github.com/jrarseneau/e634ca4b71c3281b6f57779c7957ba34\n@astrada, Mine is using 1.6GiB and my config is set to the following. \nmax_memory_cache_size=504857600\nmemory_buffer_size=8388608\nread_ahead_buffers=5\nMy understanding was the max_memory_cache_size was an absolute max?\nI haven't had a crash in nearly 12 hours now, but as soon as I do I'll grab the logs.. @astrada, It is hard to say as I wasn't running version 0.6.17 for very long. sorry.. @astrada, Mount was stable almost all day. Then it crashed three times in two hours. I've got the three log files but they are quite large (1GB, 207MB, 66MB). How would you like me to share them?. @astrada, I've emailed you a link to my logs. :)\nThanks!. @shadowsbane0 do you have streaming enabled?\nSee here: https://github.com/astrada/google-drive-ocamlfuse/issues/248. ",
    "gabo77": "We're also getting the OOM error, about 2 times/day. Like others in this thread, this mount is used for Plex.\nOperating System: CentOS Linux 7 (Core)\nKernel: Linux 3.10.0-514.10.2.el7.x86_64\n40GB RAM VPS\nmax_memory_cache_size=10737418240\nmemory_buffer_size=1048576 (but actually changed that one without saving old value before posting)\nread_ahead_buffers=5\n. Before posting, I reduced max_memory_cache_size to 5GB. I've also set an alarm to be notified if the server runs OOM. Your script is now running, I will keep you updated. \nI'm running version 0.6.17. ",
    "mattbator": "I'm seeing the same issue as @buttpain - occasionally the mount will disappear and leaves the directory it was mounted to in a strange place (see attached image). Not seeing anything in the gdfuse.log that would indicated it crashed - it ends with a BEGIN: Loading resource message, nothing regarding running out of memory, space aliens or heartburn.\nI'm on 0.6.17. Anecdotally I'll say the mount is most stable when I'm just streaming media via Plex, I believe all of the issues I've seen have been when new media is being uploaded to Google Drive (via rclone move) - though I suspect it's more the scanning after a file has been uploaded rather than the uploading itself. I have all scheduled Plex scans disabled and when I add new media I have Plex Media Scanner only scan the directory containing the new file, so at most it would only be touching 24 files on a scan.\n\n. I had already rebooted, will try to capture any relevant entries from dmesg the next time it happens.. ",
    "raidzx": "I am running into this same issue. Seems to happen after scans in plex, not sure what the error is when it crashes but I never went over 50% system ram usage.. Unfortunately 0.6.19 doesn't seemed to have fixed the problem. Plex scans did seem to last a bit longer this time but it still crashed after a 30 min scan.. @astrada I have e-mailed you my log with -verbose. Please let me know if there is anything else I can do to help you track this down.\nThanks!. ",
    "halolordkiller3": "@astrada so for some reason I still don't see 6.19. Here is what I have done.\n`opam pin add google-drive-ocamlfuse https://github.com/astrada/google-drive-ocamlfuse.git#beta\n[NOTE] Package google-drive-ocamlfuse is already git-pinned to https://github.com/astrada/google-drive-ocamlfuse.git#beta.\n       This will erase any previous custom definition.\nProceed ? [Y/n] y\n[google-drive-ocamlfuse] https://github.com/astrada/google-drive-ocamlfuse.git#beta updated\n[google-drive-ocamlfuse] Installing new package description from https://github.com/astrada/google-drive-ocamlfuse.git#beta\ngoogle-drive-ocamlfuse needs to be installed.\nThe following actions will be performed:\n  \u2217  install google-drive-ocamlfuse 0.6.17*\nDo you want to continue ? [Y/n] n\n`\n. This is all I see in the log\nSetting up cache db...done\nSetting up CURL...done\nRefresh token already present.\n[0.001580] TID=0: Starting filesystem google/\n[0.007847] TID=2: init_filesystem\n[263.796374] TID=0: Exiting.\nWaiting for pending upload threads (0)...done\nStopping buffer eviction thread (TID=1)...done\nCURL cleanup...done\nClearing context...done\n. is there a way to do this headless? it seems it just wants to get stuck at opening log file. I'm an idiot k this is what I'm seeing:\n[219.871050] TID=3: getattr /\n[219.871268] TID=3: BEGIN: Loading metadata from db\n[219.872325] TID=3: END: Getting metadata: Not found\n[219.872350] TID=3: BEGIN: Refreshing metadata\n[220.232577] TID=3: Service error: {\"error\":{\"errors\":[{\"domain\":\"usageLimits\",\"reason\":\"accessNotConfigured\",\"message\":\"Project 794316966374 is not found and cannot be used for API calls. If it is recently created, enable Drive API by visiting https://console.developers.google.com/apis/api/drive.googleapis.com/overview?project=794316966374 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.\",\"extendedHelp\":\"https://console.developers.google.com/apis/api/drive.googleapis.com/overview?project=794316966374\"}],\"code\":403,\"message\":\"Project 794316966374 is not found and cannot be used for API calls. If it is recently created, enable Drive API by visiting https://console.developers.google.com/apis/api/drive.googleapis.com/overview?project=794316966374 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.\"}}.\n[220.248279] TID=3: Input/output error: stat /\n. I'm an idiot forgot to enable gdrive api lol you can close this. ",
    "JAC2703": "Hi,\nI'm having the same issue and it's almost certainly down to high memory use. It seems that memory use shoots up to about 86% (of 3GB) within moments of initiating a scan. \nI'm running version .19 from the beta PPA repo. \nConfig as follows:\nconnect_timeout_ms=5000\ncurl_debug_off=false\ndata_directory=\ndelete_forever_in_trash_folder=false\ndocs_file_extension=true\ndocument_format=odt\ndocument_icon=\ndownload_docs=true\ndrawing_format=png\ndrawing_icon=\nform_format=zip\nform_icon=\nfusion_table_format=desktop\nfusion_table_icon=\nkeep_duplicates=false\nlarge_file_read_only=false\nlarge_file_threshold_mb=32\nlog_directory=\nlost_and_found=false\nlow_speed_limit=0\nlow_speed_time=0\nmap_format=desktop\nmap_icon=\nmax_cache_size_mb=30240\nmax_download_speed=0\nmax_memory_cache_size=419430400\nmax_retries=80\nmax_upload_chunk_size=1099511627776\nmax_upload_speed=0\nmemory_buffer_size=10485760\nmetadata_cache_time=360\npresentation_format=pdf\npresentation_icon=\nread_ahead_buffers=12\nread_only=false\nshared_with_me=false\nspreadsheet_format=ods\nspreadsheet_icon=\nsqlite3_busy_timeout=5000\nstream_large_files=true\numask=0o022\nverification_code=. ",
    "Soulplayer": "I have the exact same problem but I can't seem to update to the beta... . ",
    "asdub": "Anyone figure this out yet? \nHaving the exact same issue. Version 6.21. ",
    "percula": "I authorized using the standard GAE way, so that isn't the issue. Could it be that it takes awhile to cache files? It's been about an hour now since I installed and I still get that same error.. Ok, those logs really helped. There was one particular file that was causing the error, a \"Wildflower Identification.odt\" google doc. The file was rather large and was causing this error in the logs:\n[255.101347] TID=583: BEGIN: Updating resource state in db (id=224, state=ToDownload)\n[255.121909] TID=583: END: Updating resource state in db (id=224)\n[255.121952] TID=583: Service error: {\"error\":{\"errors\":[{\"domain\":\"global\",\"reason\":\"exportSizeLimitExceeded\",\"message\":\"This file is too large to be     exported.\"}],\"code\":403,\"message\":\"This file is too large to be exported.\"}}.\n[255.131402] TID=583: Input/output error: stat /Wildflower Identification.odt\n[255.131623] TID=598: releasedir / O_RDONLY\n[263.742384] TID=599: statfs /\n[263.742468] TID=599: BEGIN: Getting metadata from context\n[263.742484] TID=599: END: Getting metadata: Not valid\n\nI sent the file to trash, then emptied the trash and the problem was resolved! Perhaps there is a try-catch statement you could write in google-drive-ocamlfuse to skip files like this.. There was a bit more in the logs that may help:\n[249.390831] TID=583: BEGIN: Loading resource /Wildflower Identification.odt (trashed=false) from db\n[249.391065] TID=583: END: Loading resource /Wildflower Identification.odt (trashed=false) from db: Found (id=224, state=ToDownload)\n[249.391343] TID=583: BEGIN: Downloading resource (id=224) to /home/peter/.gdfuse/default/cache/1gf8fNeUcox2RKY_VNMfoDzJGinosjhVZsT-N694mSfY\n[249.391357] TID=583: BEGIN: Updating cache size (delta=0) in db\n[249.391363] TID=583: END: No need to update cache size\n[249.391367] TID=583: BEGIN: Updating resource state in db (id=224, state=Downloading)\n[249.412071] TID=583: END: Updating resource state in db (id=224)\n[249.890444] TID=584: statfs /\n[249.890521] TID=584: BEGIN: Getting metadata from context\n[249.890545] TID=584: END: Getting metadata: Valid\n[250.111134] TID=585: statfs /\n[250.111206] TID=585: BEGIN: Getting metadata from context\n[250.111221] TID=585: END: Getting metadata: Valid\n[250.118979] TID=586: statfs /\n[250.119055] TID=586: BEGIN: Getting metadata from context\n[250.119071] TID=586: END: Getting metadata: Valid\n[250.126990] TID=587: statfs /\n[250.127061] TID=587: BEGIN: Getting metadata from context\n[250.127084] TID=587: END: Getting metadata: Valid\n[250.134811] TID=588: statfs /\n[250.134871] TID=588: BEGIN: Getting metadata from context\n[250.134884] TID=588: END: Getting metadata: Valid\n[250.143190] TID=589: statfs /\n[250.143270] TID=589: BEGIN: Getting metadata from context\n[250.143288] TID=589: END: Getting metadata: Valid\n[250.150800] TID=590: statfs /\n[250.150880] TID=590: BEGIN: Getting metadata from context\n[250.150908] TID=590: END: Getting metadata: Valid\n[250.158639] TID=591: statfs /\n[250.158692] TID=591: BEGIN: Getting metadata from context\n[250.158705] TID=591: END: Getting metadata: Valid\n[250.166819] TID=592: statfs /\n[250.166893] TID=592: BEGIN: Getting metadata from context\n[250.166934] TID=592: END: Getting metadata: Valid\n[250.173436] TID=593: statfs /\n[250.173591] TID=593: BEGIN: Getting metadata from context\n[250.173621] TID=593: END: Getting metadata: Valid\n[250.180563] TID=594: statfs /\n[250.180606] TID=594: BEGIN: Getting metadata from context\n[250.180620] TID=594: END: Getting metadata: Valid\n[250.188755] TID=595: statfs /\n[250.188800] TID=595: BEGIN: Getting metadata from context\n[250.188812] TID=595: END: Getting metadata: Valid\n[250.196587] TID=596: statfs /\n[250.196636] TID=596: BEGIN: Getting metadata from context\n[250.196651] TID=596: END: Getting metadata: Valid\n[250.204640] TID=597: statfs /\n[250.204684] TID=597: BEGIN: Getting metadata from context\n[250.204699] TID=597: END: Getting metadata: Valid\n[255.101347] TID=583: BEGIN: Updating resource state in db (id=224, state=ToDownload)\n[255.121909] TID=583: END: Updating resource state in db (id=224)\n[255.121952] TID=583: Service error: {\"error\":{\"errors\":[{\"domain\":\"global\",\"reason\":\"exportSizeLimitExceeded\",\"message\":\"This file is too large to be exported.\"}],\"code\":403,\"message\":\"This file is too large to be exported.\"}}.\n[255.131402] TID=583: Input/output error: stat /Wildflower Identification.odt\n[255.131623] TID=598: releasedir / O_RDONLY\n[263.742384] TID=599: statfs /\n[263.742468] TID=599: BEGIN: Getting metadata from context\n[263.742484] TID=599: END: Getting metadata: Not valid. ",
    "simonsmh": "permission error?\nMaybe try to run it as www-data, or run nginx as root.. ",
    "lwl12": "Seems there don't support chmod / chown ? . thanks. Please close this :-)\n2017\u5e744\u670811\u65e5\u661f\u671f\u4e8c\uff0cAlessandro Strada notifications@github.com \u5199\u9053\uff1a\n\nMount-point permission is set when mounting. But be warned that uid/gid\nare only persisted for backup purposes. They are not enforced, so any user\nwho can access the mount-point can access any file.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/284#issuecomment-293050976,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AJzab5Z-Jbb0ObD45Ew9h9s5Sd-TKT60ks5ruoB7gaJpZM4MzoMd\n.\n. \n",
    "justinglock40": "Yea I got through all that stuff but I don't see the files\nOn Apr 6, 2017, 15:11 -0500, Alessandro Strada notifications@github.com, wrote:\n\n@justinglock40 (https://github.com/justinglock40): have you checked the wiki page (https://github.com/astrada/google-drive-ocamlfuse/wiki/Headless-Usage-%26-Authorization) about headless mode?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub (https://github.com/astrada/google-drive-ocamlfuse/issues/285#issuecomment-292301777), or mute the thread (https://github.com/notifications/unsubscribe-auth/AWD90UDkyHZZwMUF0XXTLps3u8V25UzXks5rtUcEgaJpZM4M11BI).\n\n. So I got the drive mounted however now my applications like Sonarr, Radarr, SABnzd. can't see the mountpoint I create, but in terminal I can see the mount and the files inside. Any idea how I can fix this?. What's the allow other command? if this works by showing as a drive. Will I need rclone for anything or will this take care of everything?\n. As far as uploading is concerned?\n. ",
    "RobertusIT": "Sei anche tu italiano ma scrivo in inglese per far comprendere a tutti ;)\nI'm trying now to use: cp -R\nOn my Rpi3, i hope to solve with it, because i have a lot of pics and videos, and i want to synch in my google photos without lost my time at pc.\nI will inform you about result ;)\nThanks a lot for the support and for this amazing job.. With:\ncp -R * /home/pi/GDRIVE/Google\\ Foto/2016 and i trying to upload video .mp4.\nbut the problem is that my quota is out now, and i set up in my phone and via web, high quality ( so all videos up 1080p, can be resize by google and space doesn't be touched )\nbut all upload with gdrive, use quota i guess.\nCan you help me to fix it? \nCan i set this somewhere?\nedit: i see in google photo help, and says that all files uploaded from drive, can't be stored with free quota, otherwise if you use google photo app, from phone or desktop, can be stored with free quota.\nSo there isn't a way to upload from line command in google photo, with free quota, and via web if you select a lot of files, google chrome crash, also in a strong system.\nThe only  way is from phone i guess.. Upload my photos with this method isn't good because use quota, and you\ncan't do nothing for that. Is Google that do it when you upload in this\nway. But if you upload from web or phone,  no problem.\nIl 11 mag 2017 18:13, \"Abd ar-Rahman Hamidi\" notifications@github.com ha\nscritto:\n\nHi @RobertusIT https://github.com/robertusit, how did you resolve the\nissue?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/287#issuecomment-300839554,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ALD_nq2jDDsIc55-V1EJgx3oMXW1OMIoks5r4zOagaJpZM4M4Hfk\n.\n. \n",
    "robindirksen1": "When will this become available?. ",
    "CzechJiri": "@astrada I can give you temporary access to our G Suite account if this helps\n. ",
    "Kosudo": "Perhaps this may help (2nd November):\n\"You\u2019ve told us that you want more tools to see and manage all of the Team Drives in your domain in one location. Today, we\u2019re making that easier by providing new methods in the Google Drive API that enable developers to build tools for Team Drive membership management, cybersecurity solutions, and more.\"\nView and modify all of your Team Drives using the Google Drive API https://gsuiteupdates.googleblog.com/2017/11/view-and-modify-all-of-your-team-drives.html. ",
    "revoluzzer": "If you integrate the team drive functionality, we would pay you for this feature. It's a very important point for my company.\nThank you very much.\nRegards \nVolker Holthaus. If you have a ppa version, i could test the team drive functionality.\nRegards\nVolker. ",
    "dbosso": "Team drives can be shared with non g-suite accounts, perhaps you just need a drive shared? I can do this if it will expedite the feature.. ",
    "HelgeS": "@astrada I have invited you as a member to a team drive at my organization.. I'm on arch linux, but it seems the install scripts there are stuck on 0.6.21 and still require a setup.ml, which no longer exists.\nBut using the release sources for version 0.6.23, I get a compile error:\n$ cd google-drive-ocamlfuse-0.6.23\n$ ocaml --version\nThe OCaml toplevel, version 4.05.0                                                                                       \n$ jbuilder build @install\n    ocamldep bin/gdfuse.depends.ocamldep-output\n    ocamldep src/google_drive_ocamlfuse.dependsi.ocamldep-output\n    ocamldep src/google_drive_ocamlfuse.depends.ocamldep-output\n      ocamlc src/bufferPool.{cmi,cmti}\n      ocamlc src/threadPool.{cmi,cmti}\n      ocamlc src/keyValueStore.{cmi,cmo,cmt}\n      ocamlc src/buffering.{cmi,cmti}\n    ocamlopt src/bufferPool.{cmx,o}\n      ocamlc src/utils.{cmi,cmo,cmt}\n    ocamlopt src/keyValueStore.{cmx,o}\n      ocamlc src/concurrentGlobal.{cmi,cmo,cmt}\n      ocamlc src/state.{cmi,cmo,cmt}\n    ocamlopt src/utils.{cmx,o}\n      ocamlc src/config.{cmi,cmo,cmt}\n    ocamlopt src/concurrentGlobal.{cmx,o}\n    ocamlopt src/state.{cmx,o}\n    ocamlopt src/threadPool.{cmx,o}\n      ocamlc src/appDir.{cmi,cmo,cmt}\n      ocamlc src/mime.{cmi,cmo,cmt}\nFile \"src/mime.ml\", line 6, characters 50-66:\nWarning 3: deprecated: String.lowercase\nUse String.lowercase_ascii instead.\n    ocamlopt src/buffering.{cmx,o}\n      ocamlc src/cache.{cmi,cmo,cmt}\n      ocamlc src/context.{cmi,cmo,cmt}\n      ocamlc src/gaeProxy.{cmi,cmo,cmt}\n      ocamlc src/oauth2.{cmi,cmo,cmt}\n    ocamlopt src/config.{cmx,o}\n    ocamlopt src/appDir.{cmx,o}\n      ocamlc src/drive.{cmi,cmo,cmt} (exit 2)\n(cd _build/default && /usr/bin/ocamlc.opt -w -40 -g -bin-annot -I /usr/lib/ocaml -I /usr/lib/ocaml/Fuse -I /usr/lib/ocaml/biniou -I /usr/lib/ocaml/bytes -I /usr/lib/ocaml/cryptokit -I /usr/lib/ocaml/curl -I /usr/lib/ocaml/easy-format -I /usr/lib/ocaml/extlib -I /usr/lib/ocaml/gapi-ocaml -I /usr/lib/ocaml/netstring -I /usr/lib/ocaml/netsys -I /usr/lib/ocaml/sqlite3 -I /usr/lib/ocaml/threads -I /usr/lib/ocaml/yojson -I /usr/lib/ocaml/zarith -no-alias-deps -I src -o src/drive.cmo -c -impl src/drive.ml)\nFile \"src/drive.ml\", line 593, characters 24-28:\nError: The function applied to this argument has type\n         ?base_url:string ->\n         ?corpus:GapiDriveV3Service.FilesResource.Corpus.t ->\n         ?spaces:string ->\n         ?orderBy:string ->\n         ?pageToken:string ->\n         GapiConversation.Session.t ->\n         GapiDriveV3Model.FileList.t * GapiConversation.Session.t\nThis argument cannot be applied with label ~supportsTeamDrives. I managed to get it to work on my machine, too.\nGreat job, thank you so much!. ",
    "Tigerhacker": "@astrada just trying to test this out, installed via the beta ppa repo and have put into the config file\n~/.gdfuse/\\<myLabelName>/config\nteam_drive_id=0AF3...\nfor my team drive however I am not seeing a new folder appear. Where should the team drive appear, would it just appear as another folder under the mount point? Or am I missing some steps?\nThanks.. Ah, never mind, figured it out, for anyone else wondering, it appears you need to\n\ncreate a new label, i.e. google-drive-ocamlfuse -headless -label myTeamDrive ... and authorise\nadd the team_drive_id config before the first time you mount\nonce mounted with google-drive-ocamlfuse -label myTeamDrive /mountPoint, the team drive is mounted there (instead of your usual 'My Drive')\n\nSo if you want to mount both your drive and a team drive, you will need to create separate labels for each. ",
    "Stepulin": "Hi,\nI would like to confirm that with version 0.6.23 adding team drive works.\nWith version 0.6.21 the line team_drive_id= was always deleted after remounting the drive.\nNicely done!\nPS: I took the liberty and put steps for Debian Stretch that worked for me.. ",
    "h-evers": "Confirm what @Stepulin said for 0.6.21 removing the \"team_drive_id\" at startup. Sadly building 0.6.23/24 on Ubuntu 17.10 (Artful) via \"opam\" is not possible due to \"libselinux-dev\" dependency error. Any idea?. ",
    "rbroderi": "I was able to manually download 0.6.24 package from https://launchpad.net/~alessandro-strada/+archive/ubuntu/ppa/+packages and install it on artful. ",
    "tolgahantunc": "Thank you for answer and suggestion. . ",
    "gzohop": "Don't know how I missed that options in the config file :)\nThanks :+1: . OK, I've Checked that option but it's no good.\nCopy progress works as before and during that time file gets copied to cache I presume. After that actual upload starts and copy operation \"hungs\" on 100% until upload is finished.\nOverall speed is lower because during copy to cache there is no actual upload.\nIt's faster to use async mode because then after copy to cache upload starts and another file gets copied to cache.\nSo is there any way to check status of this background upload process?\n. Ok, thanks. ",
    "neon64": "I just ran exactly what it described in the README:\nocaml setup.ml -configure\nocaml setup.ml -build. Okay I really have no idea what I'm doing with OCaml, but I made sure the environment variables for opam were set correctly and I ran those two commands again.. This time it is src/utils.cmi and /usr/lib/ocaml/gapi-ocaml/gapiCurl.cmi that conflict. Okay I apologise - I never saw those instruction initially. They worked like a charm.. Now I suppose the question is, why do the ocaml setup.ml commands not work? I'm guessing its because some of the dependencies are missing. That they also means that the ArchLinux PKGBUILDs are missing something, because they didn't build for me either.. Okay does that mean that there is a bug in the Arch package? Should I report it?. Thank you very much for your help by the way. ",
    "nov1n": "Hey cheers, thanks for your swift actions!. ",
    "saitoh183": "@astrada \nim having a similar issue after over a month of stability.\nUsing ubuntu 16.10 server and I was force to remove large_read or else I get fuse: invalid argument but I worked the first time I ran it.\nhere is the message i found in the dmesg:\n[106639.227651] google-drive-oc[20671]: segfault at 6597bb933000 ip 00000bb73589ea96 sp 00006597a8b842c0 error 4 in google-drive-ocamlfuse[bb73559a000+3a1000]\n[106639.227668] grsec: From 24.202.151.170: Segmentation fault occurred at 00006597bb933000 in /usr/bin/google-drive-ocamlfuse[google-drive-oc:20671] uid/euid:0/0 gid/egid:0/0, parent \n/lib/systemd/systemd[systemd:1] uid/euid:0/0 gid/egid:0/0\nThere was nothing in the gdfuse.log file\nMy config:\n```\napps_script_format=json\napps_script_icon=\nasync_upload=true\ncache_directory=/plexdata/gdrive_cache_ocaml\nclient_id=\nclient_secret=\nconnect_timeout_ms=5000\ncurl_debug_off=false\ndata_directory=\ndelete_forever_in_trash_folder=false\ndocs_file_extension=true\ndocument_format=odt\ndocument_icon=\ndownload_docs=true\ndrawing_format=png\ndrawing_icon=\nform_format=zip\nform_icon=\nfusion_table_format=desktop\nfusion_table_icon=\nkeep_duplicates=false\nlarge_file_read_only=false\nlarge_file_threshold_mb=350\nlog_directory=/logs/gfuse.log\nlost_and_found=false\nlow_speed_limit=0\nlow_speed_time=0\nmap_format=desktop\nmap_icon=\nmax_cache_size_mb=512\nmax_download_speed=0\nmax_memory_cache_size=15000000000\nmax_retries=8\nmax_upload_chunk_size=1099511627776\nmax_upload_speed=0\nmemory_buffer_size=1048576\nmetadata_cache_time=60\npresentation_format=pdf\npresentation_icon=\nread_ahead_buffers=3\nread_only=false\nshared_with_me=false\nspreadsheet_format=ods\nspreadsheet_icon=\nsqlite3_busy_timeout=5000\nstream_large_files=true\numask=0o022\nverification_code=\n```. @astrada what is the exact command you want me to run.. Sorry I'm not very versed in Linux debugging. Someone suggested that I run dmesg and post the output here. . Ok, I will. is it space intensive because when i added verbose or debug to ocamlfuse, the file grew so fast that i had to stop it before it eat up all the disk space. Also where is the dump file located?. @astrada sorry for not replying sooner but the mount hasn't dropped since my last post. So when it does, I will. ",
    "shadowsbane0": "@animosity22 - Folder or file access from Google Drive wouldn't be an issue because it would be considered inbound data which Amazon doesn't charge for.\n@hjone72 - Negative on the streaming. \nAs I mentioned I have a Google Apps account with about 15TB of data. On a given day I add a few GB to that total through the mount point. I should only see the amount of usage for the data that I put to the various folders within the mount point. \nIn the past three days my server blew through 10TB of outbound data. I can assure you I did not transfer that amount of data to my Google Drive folders. In that time span I only had one large data transfer to my Google Drive of 54GB of which I killed it at 52 percent once I was alerted to the insane outbound data rate. I added 10 files of less than 2GB each and about 20 files of less than 300mb each. None of that adds up to 10TB of data. This server is solely used to put data to my Google Drive account. \nI've installed NTOP on my server in hopes of getting a clear picture of where so much data is going. Unfortunately, so far it only shows me that data is outbound to the internal interface on the AWS server. I cannot see the NAT'ing that is going on or the external interface to monitor how much and where that traffic is going. NTOP is a new app to me so I may have to do some tweaking of it to get the results I want. I went with the AWS Lightsail because it was supposed to save me money - that didn't really work out.. @animosity22 - BTW I do run Plex but I run Plex Cloud and it connects directly to my Google Drive through it's own API outside of my Amazon AWS instance.. @animosity22 - I'll do that. I've also got a junk laptop that I'm going to setup to mirror my AWS server. I can use it on my main ISP without any extra cost to see if I can replicate and nail down the issue. I can't do it on my AWS server right now because ANYTHING I do on it right now is costing me an extra $100 a day until the billing cycle rolls over. I currently have it stopped so the bleeding of my cash flow will end.. So, I finally finish rebuilding a local test server...wow! I installed EtherApe on it and I see the issue and it's a big one. Currently, there are seven different connection to Google Servers each consuming around 35-50 Mbps because I'm using qBittorrent directly to my folders. Average file size is around 850 bytes but the amount of data being transferred is incredibly more than what I'm putting into the folders. I've only added 1GB of data to my Google Drive folders but I've transferred 18.39GB on one connection 16.06 on another, and on it goes for each connection. All told in only a three hour period I've hit the Google servers for 62GB of data out for the 1GB of data in actual file space.\n\nFrom: Animosity022 notifications@github.com\nSent: Monday, May 1, 2017 9:59 AM\nTo: astrada/google-drive-ocamlfuse\nCc: shadowsbane0; Mention\nSubject: Re: [astrada/google-drive-ocamlfuse] Huge Data Usage (#296)\nThat seems to be ok when I did some testing with ffprobe as it works as I expected.\n@mikeferrhttps://github.com/mikeferr - did you have sream_large_files=true setup? I ran a ffprobe with that and it seems to work properly:\nfelix@plex:/media/TV/NCIS$ time ffprobe NCIS.S13E19.HDTV-1080p.mkv\nffprobe version 2.8.11-0ubuntu0.16.04.1 Copyright (c) 2007-2017 the FFmpeg developers\n  built with gcc 5.4.0 (Ubuntu 5.4.0-6ubuntu1~16.04.4) 20160609\n  configuration: --prefix=/usr --extra-version=0ubuntu0.16.04.1 --build-suffix=-ffmpeg --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --cc=cc --cxx=g++ --enable-gpl --enable-shared --disable-stripping --disable-decoder=libopenjpeg --disable-decoder=libschroedinger --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmodplug --enable-libmp3lame --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-librtmp --enable-libschroedinger --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxvid --enable-libzvbi --enable-openal --enable-opengl --enable-x11grab --enable-libdc1394 --enable-libiec61883 --enable-libzmq --enable-frei0r --enable-libx264 --enable-libopencv\n  libavutil      54. 31.100 / 54. 31.100\n  libavcodec     56. 60.100 / 56. 60.100\n  libavformat    56. 40.101 / 56. 40.101\n  libavdevice    56.  4.100 / 56.  4.100\n  libavfilter     5. 40.101 /  5. 40.101\n  libavresample   2.  1.  0 /  2.  1.  0\n  libswscale      3.  1.101 /  3.  1.101\n  libswresample   1.  2.101 /  1.  2.101\n  libpostproc    53.  3.100 / 53.  3.100\nInput #0, matroska,webm, from 'NCIS.S13E19.HDTV-1080p.mkv':\n  Metadata:\n    encoder         : libebml v1.3.1 + libmatroska v1.4.2\n    creation_time   : 2016-03-22 20:45:47\n  Duration: 00:43:27.98, start: 0.000000, bitrate: 12898 kb/s\n    Stream #0:0(eng): Video: h264 (High), yuv420p(tv, bt709/unknown/unknown), 1920x1080, SAR 1:1 DAR 16:9, 23.98 fps, 23.98 tbr, 1k tbn, 47.95 tbc (default)\n    Metadata:\n      BPS             : 11386481\n      BPS-eng         : 11386481\n      DURATION        : 00:43:27.939000000\n      DURATION-eng    : 00:43:27.939000000\n      NUMBER_OF_FRAMES: 62528\n      NUMBER_OF_FRAMES-eng: 62528\n      NUMBER_OF_BYTES : 3711906229\n      NUMBER_OF_BYTES-eng: 3711906229\n      _STATISTICS_WRITING_APP: mkvmerge v8.4.0 ('A better way to fly') 64bit\n      _STATISTICS_WRITING_APP-eng: mkvmerge v8.4.0 ('A better way to fly') 64bit\n      _STATISTICS_WRITING_DATE_UTC: 2016-03-22 20:45:47\n      _STATISTICS_WRITING_DATE_UTC-eng: 2016-03-22 20:45:47\n      _STATISTICS_TAGS: BPS DURATION NUMBER_OF_FRAMES NUMBER_OF_BYTES\n      _STATISTICS_TAGS-eng: BPS DURATION NUMBER_OF_FRAMES NUMBER_OF_BYTES\n    Stream #0:1: Audio: dts (DTS), 48000 Hz, 5.1(side), fltp, 1536 kb/s (default)\n    Metadata:\n      BPS             : 1509749\n      BPS-eng         : 1509749\n      DURATION        : 00:43:27.979000000\n      DURATION-eng    : 00:43:27.979000000\n      NUMBER_OF_FRAMES: 244498\n      NUMBER_OF_FRAMES-eng: 244498\n      NUMBER_OF_BYTES : 492174474\n      NUMBER_OF_BYTES-eng: 492174474\n      _STATISTICS_WRITING_APP: mkvmerge v8.4.0 ('A better way to fly') 64bit\n      _STATISTICS_WRITING_APP-eng: mkvmerge v8.4.0 ('A better way to fly') 64bit\n      _STATISTICS_WRITING_DATE_UTC: 2016-03-22 20:45:47\n      _STATISTICS_WRITING_DATE_UTC-eng: 2016-03-22 20:45:47\n      _STATISTICS_TAGS: BPS DURATION NUMBER_OF_FRAMES NUMBER_OF_BYTES\n      _STATISTICS_TAGS-eng: BPS DURATION NUMBER_OF_FRAMES NUMBER_OF_BYTES\nreal    0m2.031s\nuser    0m0.020s\nsys     0m0.004s\nRoughly 2 seconds for a 4G file is pretty fast.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/296#issuecomment-298337815, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AaxnGMiIKtGw5VrueP3bbpJ1qF5jtOGqks5r1eUqgaJpZM4NJm06.\n. ",
    "mikeferr": "This happens to me to. Followed @animosity22 post on rclone forum https://forum.rclone.org/t/403-forbidden-with-google-drive-impossible-to-use-rclone-mount/1955/4?. As soon as I initiated a library scan in a fresh Emby install. It starts downloading gigabytes and gigabytes of data. So I went back to scanning using a straight up rclone mount and that works fine.. @animosity22 nope, I didn't have stream_large_files=true, I might try setting it up again this weekend. Got a \"download quota reached\" on google drive when I scanned my library though the rclone mount.. ",
    "Dark-Witcher": "Thank you for the response. . ",
    "sbcrumb": "Yes just did a cache clear and my cache settings are the default of 60. Still is not updating.\nI have the same mount point mounted with rclone and that is updating just fine.. Yes it is still active. I see some files still just not all. Also just rebooted and remounted it..\n. ",
    "enz1ey": "The directories still show as inaccessible the next day after uploading. As I said, it stays that way until remounting with the -cc option. It doesn't happen all the time, either, just about half the time new media is uploaded. . This is the same behavior as my issue, #303. Problem occurs randomly with newly uploaded files and persists until remounting with -cc option. . ",
    "kdizzay": "I want to +1 this, exact issue and symptoms. In my case when this happens, again the files are fully uploaded, only happens to a set of newer files. The workaround is to clear the cache and rescan the affected folders and everything is good. Keeping an eye on cache, it's not that the cache is full either, hardly 10-20% full so that's not the root cause. . +1, let me turn on debugs and see if I can reproduce with new media. It's on and off so can't seem to reliably reproduce the issue but it happens maybe 6/10 times.. ",
    "XanderStrike": "Ah thank you, didn't realize I needed to access the drive to get debug information. I was getting the drive disabled error, which is bizarre because it was definitely enabled (as gdrive works).\nDisabling it and re-enabling it appears to have worked. Thanks!. ",
    "davidcmag": "Yes.  I finally got it to load, however, because it's a headless ubuntu\nserver, I don't know how to finish the setup.  there is no browser on the\nheadless server.\nCan you possibly help?\nThanks\nOn Sun, May 21, 2017 at 3:33 PM, Alessandro Strada <notifications@github.com\n\nwrote:\nHave you added my PPA to your repositories?\nsudo add-apt-repository ppa:alessandro-strada/ppa\nsudo apt-get update\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/astrada/google-drive-ocamlfuse/issues/307#issuecomment-302957787,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AFhx8yJ6yVorQUpP2PKPJgj-6X8EbS_fks5r8JGhgaJpZM4NhlSL\n.\n. I don't have admin access to my google drive account, so I can't create a new project to get the id and the secret for the command line.   Any help would be appreciated.  thanks. ok.  I've authorized the app from a browser on my mac, and successfully got the \"The application was successfully granted access. Please wait for the client to retrieve the authorization tokens.\" message.  However, where do I find ~/.gdfuse/default/stat to copy and paste?\n\nthanks for your help!!\n. ",
    "fjen": "It's not necessary that the application id and secret come from the same google account as the gdrive. Just use another one.. The question is whether the database updates are really necessary. I think they contribute in a significant way to this slow behaviour.\nRclone unfortunately doesn't support other fuse filesystems (enfs, gocryptfs) above it.. It's not only an old kernel but also an openvz container. Fuse be enabled on the provider side.. ",
    "jsjcjsjc": "I have the sam problem, it looks there is problem on your PPA?\n```\nW: Failed to fetch http://ppa.launchpad.net/alessandro-strada/google-drive-ocamlfuse-beta/ubuntu/dists/jessie/main/binary-amd64/Packages  404  Not Found\nW: Failed to fetch http://ppa.launchpad.net/alessandro-strada/ppa/ubuntu/dists/jessie/main/binary-amd64/Packages  404  Not Found\nE: Some index files failed to download. They have been ignored, or old ones used instead.\n```\n. ",
    "sneha-gathani": "I am facing this problem.\nAccording to your answer, i added\n\"sudo add-apt-repository ppa:alessandro-strada/ppa\"\n\"sudo apt-get update\"\nbut i still face the same problem -\nsudo apt-get install google-drive-ocamlfuse\nReading package lists... Done\nBuilding dependency tree\nReading state information... Done\nE: Unable to locate package google-drive-ocamlfuse. ",
    "Wigberg": "ok, it seems my libcurl3-amd64 and libcurl3-gnutls is to new for this, after downgrade to an older libcurl*  i got the tokens, after upgrade it i dont get tokens just a timeout and the message \n[23.143984] TID=0: Retrying (1/8)\n[24.948649] TID=0: Error during request: Code: 35, Description: CURLE_SSL_CONNECT_ERROR, ErrorBuffer: gnutls_handshake() failed: Public key signature verification has failed.\n. ",
    "Nodens-": "Confirmed. Behavior on 0.6.21 looks normal now.. Indeed this sounds like an implementation problem. Writing to cache is irrelevant to the drive api and should be as fast as any other fuse fs when writing on local storage. The only condition where a delay is logical is when cache is full at which point it would block until upload finished (assuming slower upstream than disk i/o).\nAny processing related to the drive api should happen after the transfer to cache has finished. Is this not the case right now? If it is, then there must be a bug somewhere on cache handling... ",
    "quamis": "This is still happening in \n$ google-drive-ocamlfuse -version\ngoogle-drive-ocamlfuse, version 0.7.1\nCopyright (C) 2012-2018 Alessandro Strada\nLicense MIT\n\nIf it helps, it's an Ubuntu Server.\nThis is a problem for me, because I'm running a sort of backup process daily, for multiple accounts, and this will tend to leave lots of processes behind.\nAn news about the bug?\n. I'm using google-drive-ocamlfuse to backup & download my photos from gdrive to my local computer. Recently I've started using Google One. The backup process I've setup with google-drive-ocamlfuse works without any changes, so I'd say that it uses the same API.\nThis issue should be closed. ",
    "tcf909": "I ended up not following the EXACT sequence in the wiki. After I double checked my sequencing all was working.\nAlso of note, I needed to make sure my OPAMROOM environmental variable was set outside of ~ since I'm using docker containers and my users home gets mounted to a local volume.. ",
    "skywayskase": "Never mind, I should have searched more before posting.... ",
    "joma74": "Similar has happened to me on Ubuntu 16.04. Never called a webpage to authorize access to my Google account. But did not quit either.\nWhat worked is to set Firefox as the default browser via Ubuntu Details/Default Applications. On restarting google-drive-ocamlfuse the authorization worked again seamless. Note that Chromium has been set as the default browser before.. ",
    "PSOHart": "That's what I thought and I double check but there is no duplicate on the Drive.\nIs there a way to clear the cache or something ?\nMaybe clean and reinstall ?. Perfect\nUnmount, clear the cache, remount, the folders and files have the correct names.\nThank you. ",
    "kurdtpage": "I have the exact same error on Debian Jessie (fresh install). Running opam update (or any other opam command) after this gives:\nPlease run 'opam init' first to initialize the state of OPAM.. I am experiencing the same issue, using version 0.6.21 on Ubuntu 17.10 (artful) with 5GB RAM. The issue is worse when using a program that accesses multiple files at the same time, like an IDE. Eventually the RAM gets filled up and the system halts, regardless of CPU usage.. ",
    "snoopy0815": "To make it more concrete :\nI get an error message telling me that I have not enough rights to delete files or folders inside the trash folder. \nEverything else works like a charm... \n. Hello astrada,\nThank you very much for your quick help!!\nAfter changing this setting to true - and un-mounting / re-mounting the drive it works perfectly fine :)\nTHANK YOU :)\nCheers.. additional: I've digged a little deeper in Zarith 1.6 where an update of caml_z_arm.S took place a month ago.\nthe following lines of code seemed to be the problem.\nbash\n/* makes the stack non-executable. */\n        .section .note.GNU-stack,\"\",@progbits\nif I comment-out the .section   at least I'm able to install Zarith on its own following the installation guideline on https://github.com/ocaml/Zarith\n(with the original files from version 1.6 I get exactly the same error from above...)\nbut how can I do this in the context of google-drive-ocamlfuse - cause by following the installation guide there, the file caml_z_arm.S gets overwritten again - including the unwanted additional line of code...\nDoes anybody have any ideas?  - or maybe I'm in a totally wrong direction?\ncheers.... Hi astrada,\nperfect hint - I was not aware of this command.\nIt worked like a charm for me and google-drive-ocamlfuse is up and running since a few minutes again :)\nI've added a pull request @ Zarith - https://github.com/ocaml/Zarith/issues/14\n  based on a hint by bschommer how the real issue can be fixed.\nThank you so much for your help!\ncheers.. ",
    "krusic22": "Try using full path at /gdrive.\nI personally use a cronjob @reboot. I don't want to open another issue just for one question so I'm just going to as it here.\nWhy is the upload so slow. I have a server with a 200Mbps/200Mbps connection. When you try to download a file it waits for 5 seconds then slowly starts ramping up the download speed till it hits somewhere around 10Mbps. But uploading... it never goes above 1Mbps. When I try to upload via the normal drive interface (Inside Chrome)  it downloads and uploads instantly. . ",
    "xxx31fr": "Follow the tuto for automount and change gdfuse with sed command \nI william sens youthe full command to pass when i'm at home. found the solution. ",
    "jrpelegrina": "First of all thanks for the quick response ...\nThe problem is that after the metadata_cache_time the document is not displayed. I launched the debug log and do not see the cache update after that time ..... OK\nOn another computer works perfectly may be due (as I read in other issues) to the amount of google docs that has the google-drive account, which causes it not to cache correctly\nThank you. ",
    "jody-frankowski": "Have you checked ~/.gdfuse/default/cache? Mine sometimes seems to fill up when transferring large files, even though stream_large_files=true.. @astrada I see. Will there be an equivalent for uploading in the future? (sorry for the off-topic). @astrada I can understand that, but why would it be necessary for the file to be copied in the cache?. ",
    "imannms": "yes, I checked and delete them manually with sudo rm -rf ~/.gdfuse/default/cache/*\nbut my disk still 100%. Solved.\nCache is stored in /root/[mountpoint], not /home/user/[mountpoint].\n(Before I only checked /home/user/.)\nAnd after delete them manually, now my disk is free.\nYou can close this.\nThanks for your response.. ",
    "xxtensazenxx": "After messing with settings I (kind of ) got it. But the downloads always start off really slow and crawl up to my max download speed eventually, making it take long for streams, due to the download speed not starting at a good speed. is there any other settings? \nThanks again for your help . ",
    "pedrom71": "I sent you the email. The generated file setup.data after command ocaml setup.ml -configure contains this lines:\narchitecture=\"arm\"\nccomp_type=\"cc\"\nocaml_version=\"4.01.0\"\nstandard_library_default=\"/usr/lib/ocaml\"\nstandard_library=\"/usr/lib/ocaml\"\nstandard_runtime=\"/usr/bin/ocamlrun\"\nbytecomp_c_compiler=\"gcc -fno-defer-pop -Wall -D_FILE_OFFSET_BITS=64 -D_REENTRANT -fPIC\"\nnative_c_compiler=\"gcc -Wall -D_FILE_OFFSET_BITS=64 -D_REENTRANT\"\nIn bytecomp compiler and native compiler already contains the flag -D_FILE_OFFSET_BITS=64. So is already compiled with that flag, I guess. I don't know any other way to compile it with ocaml.. The error is still ocurring. I installed OPAM and ocaml 4.04.2, and then installed google-drive-ocamlfuse with opam. The error is the same, when I copy a 1Gb file, throws the same error.\nI also tried installing google-drive-ocamlfuse from source with last version of ocaml (4.05.0), but ocurs the same error.. Have you found a solution for that problem?. ",
    "coderforlife": "Shortly after submitting this bug I renamed those folder and since I have tried quite a few things to reproduce this myself, but I have failed to reproduce it. If I ever come across it again I will send more information.. ",
    "lenberman": "Thanks and sorry for the bother.  I thought I RTFM, but I guess I missed it.   Thanks for the project and your time.\n. ",
    "mediapinta": "Hi, I have a similar problem and I solved it removing the content of ~/.gdfuse/default directory (as pointed here by astrada).\nI hope it helps you.\nGreetings,\nifrit. Ok. I fixed it. I just removed the content of ~/.gdfuse/default directory (as pointed here by astrada)\nThank you for this great tool!\nGreetings,\nifrit. ",
    "terencode": "@astrada The same error appear on Arch when building using the AUR git package.. Ok, I'll tell the maintener about it.. ",
    "thebitbrine": "@xxx31fr Please share it with us.. -Always Read The Documentation Before Asking Questions-\n. ",
    "nrajasekharan": "Get it to work by changing the mount command in Startup Applications to the following:\ngoogle-drive-ocamlfuse \"/home/thebitbrine/gdrive\". ",
    "Coalijes": "Great,\nI followed the instructions and it worked perfectly\nThanks a lot and keep up the good work!. ",
    "Mettcr3w": "Hi!\nI ran into the same issue. The google drive mount disconnects randomly after some activity and I guess it has something to do with API limits, but I cannot prove this.\nDo you have any idea how to fix it?. Hello @astrada !\nI already did and at the time of occurence there are no abnormalities.\nI'm using the mount point with my Plex Media Server to stream my videos and share it with my family/friends. In exceptional cases it can happen, that there are watching 4 or more people simultaneously.\nI grepped for \"error\" in the debug log and somewhere in the middle something like this appears:\n[4363.465699] TID=144622: Retrying (2/8) final_step after exception: Failure(\"Sqlite3 error: BUSY\")\n[4363.497676] TID=144621: Retrying (2/8) final_step after exception: Failure(\"Sqlite3 error: BUSY\")\n[4363.752216] TID=144620: Retrying (2/8) final_step after exception: Failure(\"Sqlite3 error: BUSY\")\nBut I guess it has nothing to do with my problem and it is just a temporary error, because the mount point is crashing much later (with no clear reason).. ",
    "Crenor": "I am getting disconnects - and/or it just stops working. Not hitting the API limit, I just have to do a fuse disconnect and then re-connect and it works again.\nMy issue is it looks to be working, I try to get a file, it downloads some, then just stops.\nI have a script to check to re-connect if it is disconnected, but I don't have anything for stops working, but looks ok...\nThis is so far only happening when I do a large scan of a folder with +1000 items.. ",
    "tonebender": "I might add -- saving the txt file locally and then copying it to Drive (with cp in linux terminal) will make it show up as txt (not binary) in Drive in browser.. Ah, that makes sense! I guess I could fix it by saving everything to a temporary spool directory or so, which I then copy to gdrive.\nThanks for the quick reply!. ",
    "tknr": "thank for reply.\ni checked dependency,\n```\nopam install depext\n$ opam depext google-drive-ocamlfuse \nDetecting depexts using flags: x86_64 linux centos\nThe following system packages are needed:\n- fuse-devel\n- gmp\n- gmp-devel\n- libcurl-devel\n- m4\n- openssl-devel\n- pkgconfig\n- sqlite-devel\n- which\n- zlib-devel\nName        : fuse-devel                   Relocations: (not relocatable)\nVersion     : 2.8.3                             Vendor: CentOS\nRelease     : 5.el6                         Build Date: 2016\u5e7405\u670811\u65e5 16\u664238\u520649\u79d2\nInstall Date: 2016\u5e7405\u670827\u65e5 01\u664222\u520641\u79d2      Build Host: worker1.bsys.centos.org\nGroup       : Development/Libraries         Source RPM: fuse-2.8.3-5.el6.src.rpm\nSize        : 108219                           License: LGPLv2+\nSignature   : RSA/SHA1, 2016\u5e7405\u670812\u65e5 19\u664250\u520613\u79d2, Key ID 0946fca2c105b9de\nPackager    : CentOS BuildSystem http://bugs.centos.org\nURL         : http://fuse.sf.net\nSummary     : File System in Userspace (FUSE) devel files\nDescription :\nWith FUSE it is possible to implement a fully functional filesystem in a\nuserspace program. This package contains development files (headers,\npgk-config) to develop FUSE based applications/filesystems.\nName        : gmp                          Relocations: (not relocatable)\nVersion     : 4.3.1                             Vendor: CentOS\nRelease     : 12.el6                        Build Date: 2017\u5e7403\u670822\u65e5 12\u664247\u520619\u79d2\nInstall Date: 2017\u5e7404\u670806\u65e5 14\u664250\u520628\u79d2      Build Host: c1bm.rdu2.centos.org\nGroup       : System Environment/Libraries   Source RPM: gmp-4.3.1-12.el6.src.rpm\nSize        : 657883                           License: LGPLv2+ and  GPLv3+ and LGPLv3+\nSignature   : RSA/SHA1, 2017\u5e7403\u670824\u65e5 00\u664201\u520645\u79d2, Key ID 0946fca2c105b9de\nPackager    : CentOS BuildSystem http://bugs.centos.org\nURL         : http://gmplib.org/\nSummary     : A GNU arbitrary precision library\nDescription :\nThe gmp package contains GNU MP, a library for arbitrary precision\narithmetic, signed integers operations, rational numbers and floating\npoint numbers. GNU MP is designed for speed, for both small and very\nlarge operands. GNU MP is fast because it uses fullwords as the basic\narithmetic type, it uses fast algorithms, it carefully optimizes\nassembly code for many CPUs' most common inner loops, and it generally\nemphasizes speed over simplicity/elegance in its operations.\nInstall the gmp package if you need a fast arbitrary precision\nlibrary.\nName        : gmp-devel                    Relocations: (not relocatable)\nVersion     : 4.3.1                             Vendor: CentOS\nRelease     : 12.el6                        Build Date: 2017\u5e7403\u670822\u65e5 12\u664247\u520619\u79d2\nInstall Date: 2017\u5e7404\u670806\u65e5 15\u664200\u520624\u79d2      Build Host: c1bm.rdu2.centos.org\nGroup       : Development/Libraries         Source RPM: gmp-4.3.1-12.el6.src.rpm\nSize        : 344621                           License: LGPLv2+ and  GPLv3+ and LGPLv3+\nSignature   : RSA/SHA1, 2017\u5e7403\u670824\u65e5 00\u664204\u520614\u79d2, Key ID 0946fca2c105b9de\nPackager    : CentOS BuildSystem http://bugs.centos.org\nURL         : http://gmplib.org/\nSummary     : Development tools for the GNU MP arbitrary precision library\nDescription :\nThe libraries, header files and documentation for using the GNU MP\narbitrary precision library in applications.\nIf you want to develop applications which will use the GNU MP library,\nyou'll need to install the gmp-devel package.  You'll also need to\ninstall the gmp package.\nName        : libcurl-devel                Relocations: (not relocatable)\nVersion     : 7.19.7                            Vendor: CentOS\nRelease     : 53.el6_9                      Build Date: 2017\u5e7404\u670803\u65e5 23\u664214\u520607\u79d2\nInstall Date: 2017\u5e7404\u670806\u65e5 14\u664259\u520639\u79d2      Build Host: c1bm.rdu2.centos.org\nGroup       : Development/Libraries         Source RPM: curl-7.19.7-53.el6_9.src.rpm\nSize        : 497647                           License: MIT\nSignature   : RSA/SHA1, 2017\u5e7404\u670805\u65e5 01\u664234\u520615\u79d2, Key ID 0946fca2c105b9de\nPackager    : CentOS BuildSystem http://bugs.centos.org\nURL         : http://curl.haxx.se/\nSummary     : Files needed for building applications with libcurl\nDescription :\ncURL is a tool for getting files from FTP, HTTP, Gopher, Telnet, and\nDict servers, using any of the supported protocols. The libcurl-devel\npackage includes files needed for developing applications which can\nuse cURL's capabilities internally.\nName        : m4                           Relocations: (not relocatable)\nVersion     : 1.4.13                            Vendor: CentOS\nRelease     : 5.el6                         Build Date: 2010\u5e7411\u670812\u65e5 02\u664217\u520634\u79d2\nInstall Date: 2011\u5e7407\u670814\u65e5 18\u664246\u520644\u79d2      Build Host: c6b3.bsys.dev.centos.org\nGroup       : Applications/Text             Source RPM: m4-1.4.13-5.el6.src.rpm\nSize        : 560949                           License: GPLv3+\nSignature   : RSA/8, 2011\u5e7407\u670803\u65e5 13\u664244\u520633\u79d2, Key ID 0946fca2c105b9de\nPackager    : CentOS BuildSystem http://bugs.centos.org\nURL         : http://www.gnu.org/software/m4/\nSummary     : The GNU macro processor\nDescription :\nA GNU implementation of the traditional UNIX macro processor.  M4 is\nuseful for writing text files which can be logically parsed, and is used\nby many programs as part of their build process.  M4 has built-in\nfunctions for including files, running shell commands, doing arithmetic,\netc.  The autoconf program needs m4 for generating configure scripts, but\nnot for running configure scripts.\nInstall m4 if you need a macro processor.\nName        : openssl-devel                Relocations: (not relocatable)\nVersion     : 1.0.1e                            Vendor: CentOS\nRelease     : 57.el6                        Build Date: 2017\u5e7403\u670823\u65e5 06\u664247\u520609\u79d2\nInstall Date: 2017\u5e7404\u670806\u65e5 14\u664259\u520631\u79d2      Build Host: c1bm.rdu2.centos.org\nGroup       : Development/Libraries         Source RPM: openssl-1.0.1e-57.el6.src.rpm\nSize        : 2288823                          License: OpenSSL\nSignature   : RSA/SHA1, 2017\u5e7403\u670824\u65e5 00\u664200\u520642\u79d2, Key ID 0946fca2c105b9de\nPackager    : CentOS BuildSystem http://bugs.centos.org\nURL         : http://www.openssl.org/\nSummary     : Files for development of applications which will use OpenSSL\nDescription :\nOpenSSL is a toolkit for supporting cryptography. The openssl-devel\npackage contains include files needed to develop applications which\nsupport various cryptographic algorithms and protocols.\nName        : pkgconfig                    Relocations: (not relocatable)\nVersion     : 0.23                              Vendor: CentOS\nRelease     : 9.1.el6                       Build Date: 2010\u5e7408\u670819\u65e5 12\u664230\u520632\u79d2\nInstall Date: 2011\u5e7407\u670814\u65e5 18\u664245\u520643\u79d2      Build Host: c6b1.bsys.dev.centos.org\nGroup       : Development/Tools             Source RPM: pkgconfig-0.23-9.1.el6.src.rpm\nSize        : 140091                           License: GPLv2+\nSignature   : RSA/8, 2011\u5e7407\u670803\u65e5 13\u664257\u520605\u79d2, Key ID 0946fca2c105b9de\nPackager    : CentOS BuildSystem http://bugs.centos.org\nURL         : http://pkgconfig.freedesktop.org\nSummary     : A tool for determining compilation options\nDescription :\nThe pkgconfig tool determines compilation options. For each required\nlibrary, it reads the configuration file and outputs the necessary\ncompiler and linker flags.\nName        : sqlite-devel                 Relocations: (not relocatable)\nVersion     : 3.6.20                            Vendor: CentOS\nRelease     : 1.el6_7.2                     Build Date: 2015\u5e7408\u670817\u65e5 19\u664225\u520628\u79d2\nInstall Date: 2015\u5e7408\u670820\u65e5 15\u664257\u520637\u79d2      Build Host: c6b8.bsys.dev.centos.org\nGroup       : Development/Libraries         Source RPM: sqlite-3.6.20-1.el6_7.2.src.rpm\nSize        : 289974                           License: Public Domain\nSignature   : RSA/SHA1, 2015\u5e7408\u670818\u65e5 00\u664229\u520610\u79d2, Key ID 0946fca2c105b9de\nPackager    : CentOS BuildSystem http://bugs.centos.org\nURL         : http://www.sqlite.org/\nSummary     : Development tools for the sqlite3 embeddable SQL database engine\nDescription :\nThis package contains the header files and development documentation\nfor sqlite. If you like to develop programs using sqlite, you will need\nto install sqlite-devel.\nName        : which                        Relocations: (not relocatable)\nVersion     : 2.19                              Vendor: CentOS\nRelease     : 6.el6                         Build Date: 2011\u5e7409\u670823\u65e5 19\u664247\u520656\u79d2\nInstall Date: 2012\u5e7401\u670807\u65e5 04\u664231\u520657\u79d2      Build Host: c6b18n1.dev.centos.org\nGroup       : Applications/System           Source RPM: which-2.19-6.el6.src.rpm\nSize        : 73004                            License: GPLv3\nSignature   : RSA/SHA1, 2011\u5e7409\u670826\u65e5 13\u664226\u520613\u79d2, Key ID 0946fca2c105b9de\nPackager    : CentOS BuildSystem http://bugs.centos.org\nURL         : http://www.xs4all.nl/~carlo17/which/\nSummary     : Displays where a particular program in your path is located\nDescription :\nThe which command shows the full pathname of a specified program, if\nthe specified program is in your PATH.\nName        : zlib-devel                   Relocations: (not relocatable)\nVersion     : 1.2.3                             Vendor: CentOS\nRelease     : 29.el6                        Build Date: 2013\u5e7402\u670822\u65e5 08\u664202\u520617\u79d2\nInstall Date: 2013\u5e7403\u670814\u65e5 21\u664214\u520632\u79d2      Build Host: c6b9.bsys.dev.centos.org\nGroup       : Development/Libraries         Source RPM: zlib-1.2.3-29.el6.src.rpm\nSize        : 117496                           License: zlib and Boost\nSignature   : RSA/SHA1, 2013\u5e7402\u670824\u65e5 02\u664240\u520628\u79d2, Key ID 0946fca2c105b9de\nPackager    : CentOS BuildSystem http://bugs.centos.org\nURL         : http://www.gzip.org/zlib/\nSummary     : Header files and libraries for Zlib development\nDescription :\nThe zlib-devel package contains the header files and libraries needed\nto develop programs that use the zlib compression and decompression\nlibrary.\nAll required OS packages found.\nthe required packages seems installed...hmm.... thank,i tried\n[tknr@server ~]$ opam pin add zarith 1.5\n[NOTE] Package zarith is already version-pinned to 1.5.\n       This will erase any previous custom definition.\nProceed ? [Y/n] Y\n[zarith] https://github.com/ocaml/Zarith/archive/release-1.5.tar.gz downloaded\n[tknr@server ~]$ opam install google-drive-ocamlfuse\nThe following actions will be performed:\n  \u2217  install google-drive-ocamlfuse 0.6.21\n=-=- Gathering sources =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n[google-drive-ocamlfuse] Archive in cache\n=-=- Processing actions -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n[ERROR] The compilation of google-drive-ocamlfuse failed at \"ocaml setup.ml -build\".\nProcessing  1/1: [google-drive-ocamlfuse: ocaml]\n=== ERROR while installing google-drive-ocamlfuse.0.6.21 =====================\nopam-version 1.2.2\nos           linux\ncommand      ocaml setup.ml -build\npath         /home/tknr/.opam/4.05.0/build/google-drive-ocamlfuse.0.6.21\ncompiler     4.05.0\nexit-code    1\nenv-file     /home/tknr/.opam/4.05.0/build/google-drive-ocamlfuse.0.6.21/google-drive-ocamlfuse-29810-58c514.env\nstdout-file  /home/tknr/.opam/4.05.0/build/google-drive-ocamlfuse.0.6.21/google-drive-ocamlfuse-29810-58c514.out\nstderr-file  /home/tknr/.opam/4.05.0/build/google-drive-ocamlfuse.0.6.21/google-drive-ocamlfuse-29810-58c514.err\nstdout\n[...]\n/home/tknr/.opam/4.05.0/bin/ocamlfind ocamlopt -g -linkpkg -package Fuse -package camlidl -package gapi-ocaml -package sqlite3 -package threads -thread -I src src/utils.cmx src/config.cmx src/appDir.cmx src/bufferPool.cmx src/buffering.cmx src/cache.cmx src/concurrentGlobal.cmx src/keyValueStore.cmx src/state.cmx src/threadPool.cmx src/context.cmx src/gaeProxy.cmx src/mime.cmx src/oauth2.cmx src/drive.cmx src/gdfuse.cmx -o src/gdfuse.native\n+ /home/tknr/.opam/4.05.0/bin/ocamlfind ocamlopt -g -linkpkg -package Fuse -package camlidl -package gapi-ocaml -package sqlite3 -package threads -thread -I src src/utils.cmx src/config.cmx src/appDir.cmx src/bufferPool.cmx src/buffering.cmx src/cache.cmx src/concurrentGlobal.cmx src/keyValueStore.cmx src/state.cmx src/threadPool.cmx src/context.cmx src/gaeProxy.cmx src/mime.cmx src/oauth2.cmx src/drive.cmx src/gdfuse.cmx -o src/gdfuse.native\n/home/tknr/.opam/4.05.0/lib/zarith/libzarith.a(caml_z.o): \u95a2\u6570 `ml_z_mul' \u5185:\ncaml_z.c:(.text+0x515f): `__gmpn_sqr' \u306b\u5bfe\u3059\u308b\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u306a\u3044\u53c2\u7167\u3067\u3059\n/home/tknr/.opam/4.05.0/lib/zarith/libzarith.a(caml_z.o): \u95a2\u6570 `ml_z_powm_sec' \u5185:\ncaml_z.c:(.text+0xbdd8): `__gmpz_powm_sec' \u306b\u5bfe\u3059\u308b\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u306a\u3044\u53c2\u7167\u3067\u3059\ncollect2: error: ld returned 1 exit status\nFile \"caml_startup\", line 1:\nError: Error during linking\nCommand exited with code 2.\nstderr\nE: Failure(\"Command ''/home/tknr/.opam/4.05.0/bin/ocamlbuild' src/gdfuse.native -tag debug' terminated with error code 10\")\n=-=- Error report -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\nThe following actions failed\n  \u2217  install google-drive-ocamlfuse 0.6.21\nNo changes have been performed\n```\nbut failed...\n[tknr@server ~]$ gcc -v\nUsing built-in specs.\nCOLLECT_GCC=gcc\nCOLLECT_LTO_WRAPPER=/home/opt/rh/devtoolset-3/root/usr/bin/../libexec/gcc/x86_64-redhat-linux/4.9.2/lto-wrapper\nTarget: x86_64-redhat-linux\nConfigured with: ../configure --prefix=/opt/rh/devtoolset-3/root/usr --mandir=/opt/rh/devtoolset-3/root/usr/share/man --infodir=/opt/rh/devtoolset-3/root/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --enable-languages=c,c++,fortran,lto --enable-plugin --with-linker-hash-style=gnu --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.9.2-20150212/obj-x86_64-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.9.2-20150212/obj-x86_64-redhat-linux/cloog-install --with-mpc=/builddir/build/BUILD/gcc-4.9.2-20150212/obj-x86_64-redhat-linux/mpc-install --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux\nThread model: posix\ngcc version 4.9.2 20150212 (Red Hat 4.9.2-6) (GCC)\nsome gcc or any version is wrong?. ",
    "Abrahamh08": "It seems to unmount at random times on os x.. ",
    "alexsharaz": "Latest and greatest version on macOS works for a while then a \"df\" gives you the mounted  drive with . size of \"0\" used/available. unmount/remount and everything fine for a while ... ~1 day then happens again.\nUsing it with Google drive in read only mode. Running Plex accessing movies on the drive. ",
    "kellnerp": "I was aware of those. But is this program not supported anymore? . I was aware of those. But is this program not supported anymore? . Thanks. What I needed to know.. Thanks. What I needed to know.\nAt least as far as fuse is concerned have you filed a bug report? I have no idea what to tell them if I do it. . ",
    "Hypeouseaus": "````\n                                                                                                         [#######################################################################################] 100%\n:: Running post-transaction hooks...\n(1/1) Arming ConditionNeedsUpdate...\n==> Making package: ocaml-extlib 1.7.2-1 (2017. 10. 15. (\uc77c) 01:39:02 KST)\n==> Checking runtime dependencies...\n==> Checking buildtime dependencies...\n==> Retrieving sources...\n  -> Downloading ocaml-extlib-1.7.2.tar.gz...\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   125    0   125    0     0    125      0 --:--:--  0:00:01 --:--:--    72\n100 85934    0 85934    0     0  17186      0 --:--:--  0:00:05 --:--:-- 28644\n==> Validating source files with sha256sums...\n    ocaml-extlib-1.7.2.tar.gz ... Passed\n==> Extracting sources...\n  -> Extracting ocaml-extlib-1.7.2.tar.gz with bsdtar\n==> Starting build()...\nfatal: Not a git repository: '.git'\nmake -C src build\nmake[1]: Entering directory '/tmp/yaourt-tmp-userkim/aur-ocaml-extlib/src/ocaml-extlib-1.7.2/src'\nocamlfind ocamlc -pp \"cppo -D OCAML4  -D OCAML4_02  -D OCAML4_03  -D OCAML4_04  -D OCAML4_05  -D WITH_BYTES\" -g -package bytes -i extBytes.ml > extBytes.mli\nsh: cppo: command not found\nFile \"extBytes.ml\", line 1:\nError: Error while running external preprocessor\nCommand line: cppo -D OCAML4  -D OCAML4_02  -D OCAML4_03  -D OCAML4_04  -D OCAML4_05  -D WITH_BYTES 'extBytes.ml' > /tmp/ocamlpp9f31b3\nmake[1]:  [Makefile:53: extBytes.mli] Error 2\nmake[1]: Leaving directory '/tmp/yaourt-tmp-userkim/aur-ocaml-extlib/src/ocaml-extlib-1.7.2/src'\nmake:  [Makefile:14: build] Error 2\n==> ERROR: A failure occurred in build().\n    Aborting...\n==> ERROR: Makepkg was unable to build .\n==> Restart building ocaml-extlib ? [y/N]\n==> -------------------------------------\n==> ==> Restart building gapi-ocaml ? [y/N]\n==> -----------------------------------\n==> ==> Restart building google-drive-ocamlfuse ? [y/N]\n==> -----------------------------------------------\n==> \n````\nI don't know what's the problem but currently:\n````\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     OS: Manjaro 17.0.5 Gellivara\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     Kernel: x86_64 Linux 4.9.53-1-MANJARO\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     Uptime: 26m\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588            \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     Packages: 1158\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     Shell: bash 4.4.12\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     Resolution: 1920x1080\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     DE: KDE 5.38.0 / Plasma 5.10.5\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     WM: KWin\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     WM Theme: Breath\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     GTK Theme: Breath [GTK2/3]\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     Icon Theme: maia\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     Font: Noto Sans Regular\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     CPU: Intel Core i3-4150 @ 4x 3.5GHz [27.8\u00b0C]\n                                  GPU: GeForce GT 720\n                                  RAM: 2316MiB / 7932MiB\n````\nedited typo. ",
    "Jimster121": "Thank you for your fast response! I would like to add that it actually syncs the changes I made to a file, I just had to open the Google Drive folder. . ",
    "Munchotaur": "Thanks for the link. I reinstalled the certs etc as suggested in the linked answer. Didn't help at all, unfortunately. \nWhen I run with -debug, this block of certificate-related errors keeps occurring:\nBacktrace:\n[2.013989] TID=17: opendir / O_RDONLY,O_NONBLOCK\n[2.014106] TID=17: BEGIN: Loading metadata from db\n[2.015069] TID=17: END: Getting metadata: Not found\n[2.015155] TID=17: BEGIN: Refreshing metadata\n[2.147190] TID=17: Error during request: Code: 60, Description: CURLE_SSL_CACERT, ErrorBuffer: server certificate verification failed. CAfile: /etc/ssl/certs\n/ca-certificates.crt CRLfile: none\n[2.147329] TID=17: Giving up\n[2.147424] TID=17: Exception:Failure(\"Code: 60, Description: CURLE_SSL_CACERT, ErrorBuffer: server certificate verification failed. CAfile: /etc/ssl/certs/ca\n-certificates.crt CRLfile: none\\n\")\n. ",
    "zerfeus": "Does it work well if you move full heavy files (already downloaded)?. ",
    "Nick-Elliott-UK": "No my uid and gid are still 1000. Yes. I created a new profile.\nOK you will be happy to hear the problem is definitely to do with permissions on my system and not google-drive-ocamlfuse. I tried to do the same operations on some files in another directory on Google Drive and there was no error.\nI've just done a detailed check on all the files and directories. For some reason all the files in just one directory are assigned to \"nogroup\". This is why I'm getting the error. \nNow I need to work out why or how this happened in the first place.\nThank you for your help, the userid/groupid comment was very useful. Sorry for wasting your time!\nNick. ",
    "cfra": "Okay, that solves the issue for me, thank you. Just out of curiosity, I am trying to download huge uncompressable files and right now the cp is \"hanging\" after having opened the first file of the output, but having written 0 bytes.\nI assume this is because Google insists on wasting their CPU time \"compressing\" stuff and then sending them after that is complete so that the transfer cannot be streamed in a sensible way?. Ah, I found it, and you replied even faster than that. Just for reference if anybody should stumble across this being in the same situation as me:\nI have put the following into .gdfuse/default/config\nlarge_file_read_only=true\nlarge_file_threshold_mb=16\nstream_large_files=true\nNow it seems to work reasonably well. \n@astrada thank you for writing this excellent piece of software. I was beginning to fear I would never get those 200GB of video footage scattered out of Google Drive without being driven insane. Your software actually made it usable. If we should ever meet in a bar, drinks are definitely on me.. ",
    "gunslingerfry": "Ok. So there's still problems. My curl log is 11G overnight gdfuse.log is 124M overnight. I'm going to turn off the curl logging. It also allocated 1.1GB of virt memory and filled my swap. I'm going to restart this and start logging the gdfuse.log growth to see if I can identify if it correlates with any access I'm doing. \nSorry for the running commentary. I'll try and catch it while it's being naughty and post some of the logs.. I solved this by using the setting the root folder as shown in the wiki: https://github.com/astrada/google-drive-ocamlfuse/wiki/Configuration. ",
    "lcssanches": "@astrada thanks for your reply. This is an amazing project!\nI don't know if I understand what do you mean by \"download via google-drive-ocamlfuse\". But if I upload a file through my desktop, the file just appears on my server few seconds after that.\nI'm using Ubuntu Server, so I had to do the headlesss authorization.\nI think it's worth to mention that only download is been forbidden, if I open the Google Drive page and select a file to preview, it's shown perfectly.. Yes, I can.. Yes.\nI will try to reauthorizing the app, but what's the relation between the cache and download from Google Drive web page?. Got the same error after reauthorize.\nI don't know if this make sense, but maybe the file created by the Google Drive API is only accessible by the API? Though I can download through the link share.. OK, I found something interisting here.\nI uploaded a file via browser and I cannot download either. I fave created a GoogleDoc file and was just fine.\n\nThis was uploaded by google-drive-ocamlfuse\n\nI will try to figure out what's going on.. From the second link you get\n\n\"This usually happens if you have multiple signed in accounts in your browser.\nAccount 1 is attempting to download the file\nAccount 2 ends up doing the request to download the file\nsince account 2 doesn't have access to the said file, you will get the forbidden message\nclearing the cache/cookies can fix this but you can also try downloading the files while in incognito mode to ensure that you only have one account signed in.\"\n\nI have 4 active accounts on my browser.\nWas that.\nThanks for your help!\n. Should I remount the drive or there's a way to activate the debug without remounting?\nAlso, does the -cc clear the logs?. Here is the output. I got one exception and one file was not uploaded correctly.\n```\nroot@backup:/# google-drive-ocamlfuse -debug /google-drive\nStarting application setup (label=default, base_dir=).\n[0.000154] TID=0: Loading configuration from /root/.gdfuse/default/config...done\nOpening log file: /root/.gdfuse/default/gdfuse.log\nThread 6730 killed on uncaught exception Sys_error(\"/root/.gdfuse/default/cache/1oZJgCWYaFiS_L_yqQBYko7xxQenjrDql: No such file or directory\")\n```\nI have a script to backup all my containers, one of those containers is named \"backup\", who has the google drive. This container is also backuped. That's the reason I have the -cc, if not it will become bigger and bigger.\nTheses lines may interest you.\nlxc exec backup -- google-drive-ocamlfuse -cc\nlxc exec backup -- mkdir -p /google-drive/servers/$CONTAINER_NAME\n[...]\nlxc file push $IMAGE_NAME.tar.gz backup/tmp/\nlxc exec backup -- dd if=/tmp/$IMAGE_NAME.tar.gz of=/google-drive/servers/$CONTAINER_NAME/$IMAGE_NAME.tar.gz bs=4MB\nThe failed file was the second one, with 565MB. The first one has 235MB.\n. Hi astrada. I changed my script, to avoid always cleaning cache .\nLet's see if it work!\nThank you!. I'm closing this because after the changes on my script, based on your reply, the error does not occurs anymore.\nThanks!. ",
    "anthonybilinski": "The file is only around 2 MB and I'm running off an SSD with low load, surely copying to local cache should take under a second, instead of the 45s I'm seeing? I can't imagine the time I'm seeing is due to I/O limitations.. Thanks. I ran it with -verbose and got the following data:\nThe testfile is 7.5MB.\n[92.872092] TID=625: mknod /testdir/testfile 100644\n...\n[133.088217] TID=8248: flush /testdir/testfile 0\nso the saving application is blocked for 40s waiting for the local cache copy.\nFor comparison, copying the same file to /tmp (storage-backed, not tmpfs):\n```\n$ time cp /media/gdrive/testdir/testfile /tmp/\nreal    0m0.078s\nuser    0m0.000s\nsys     0m0.009s\n```\nSo the local cache copy used by google-drive-ocamlfuse is taking over 500x longer than a raw cp. I get that there's some metadata overhead needed to track the file in the cache.. but this seems like way too much.\nHere's the log from mknod until flush.\ntest-save.log\nI think the grand majority of the time isn't spent on any hardware limitation, but on the huge number of db accesses to copy the file. For example, the file is loaded from db 7624 times in this small save:\n```\n$ grep \"BEGIN: Loading resource.*from db\" test-save.log | wc -l\n7624\n````. I'm not convinced it's a general FUSE limitation, because using sshfs (also FUSE) on loopback, which should have much higher overhead with ssh protocol, encryption, compression, network stack etc, sending /dev/urandom which is incompressible, I still get 103 MB/s write speed and 147 MB/s read speed compared to the 0.1875 MB/s with google-drive-ocamlfuse from my 7.5 MB over 40s which is just copying to local disk, without all of the added ssh overhead.\nMy kernel is 4.13.0-16-generic.. How does Google Drive API have anything to do with copying to local cache? Isn't this purely the google-drive-ocamlfuse implementation, which should be tailored for FUSE since that's what it's built for?\nAs for why big_writes isn't working, I tried remounting with google-drive-ocamlfuse -o big_writes -verbose and I see writes in the log all alternating between 3584 bytes and 512 bytes. \ncat /sys/kernel/mm/transparent_hugepage/enabled gives\nalways [madvise] never\nwhich should enable when requested, apparently.\nI then did a test with a 1GB file instead of the 7MB one I was using, and saw writes of 131072 bytes, must be using the big_blocks. This increases my write time to 6MB/s, from the 0.1875 MB/s I was seeing before. Definitely an improvement, but still extremely slow for local access.\nI can't see a way to force this use of bigger blocks on small files, maybe Linux is making the decision because of the small file size, I'm not sure.\nI still don't see any reason why copying to cache should be slower than any other FUSE, regardless of block size. Increasing block size may mitigate the slowness somewhat by decreasing the number of transactions, but the root cause must be the slow handling of each block.. I'm not saying that you shouldn't copy to local cache and block upload instead - which may be impossible due to Google Drive API. I'm saying that copying to local disk cache should be able to max disk throughput, i.e. 300+MB/s on an SSD, instead of the 0.2MB/s I'm getting. This is before any changes have been uploaded. The fact that the whole file has to be uploaded to Google Drive doesn't affect the local copy speed, only the amount that has to be locally copied.. This isn't about upload bandwidth, it's about local cache. google-drive-ocamlfuse maxes my upload bandwidth no problem once the file is copied into cache.\nThe problem is that copying into cache is the more critical part, since it blocks applications, and is extremely slow with no reason, since it should only be limited by disk write speed and cache implementation.. ",
    "BungeeCloud": "Hey, I can confirm here the problem on nvme:\nhttps://cdn.bungeecloud.org/i/151133158200141.jpg\nand still on it. Testing it here with Ubuntu 17:10. Hi,\nI found a part of the solution. It is working fine under a ram disk till that is full.\n@anthonybilinski\nmount -t tmpfs none /root/.gdfuse/default/cache\nhttps://cdn.bungeecloud.org/i/151229449800117.jpg\n(Here in a upload from a other server. Screen on the target server with the mounted FS as temp FS!)\nBut that is not  a so great solution as I do not have 100+GB ram for moving files ;)\nI have a nvme in raid 1. So why is that with the normal file system so extream slow!\nRegards,\nPaul\n. ",
    "leighsjo": "Thanks, astrada.  It solves the problem. . ",
    "thardbarger": "Same problem here.  Curl.log is blank.. Am running Raspbian Jessie.  I set up google-drive-ocamlfuse three months ago with no problems.  Just moved from London to Washington DC and encountered error when booting for first time after I arrived.  Have tried changing locale options.\n[0.001417] TID=0: Setting up default filesystem...\n[0.001535] TID=0: BEGIN: Saving configuration in /home/pi/.gdfuse/default/config\n[0.002245] TID=0: END: Saving configuration in /home/pi/.gdfuse/default/config\n[0.002358] TID=0: Loading application state from /home/pi/.gdfuse/default/state...done\nCurrent version: 0.6.17\nSetting up cache db...done\nSetting up CURL...done. Left it running over night and found \u201chostname not found in DNS cache\u201d in curl log.  Did not realise log is written over when restarting the process so letting it run again to get the full copy.  Thanks. I found that I need to have a web browser (have tried Chromium and Midori) open before the program will proceed.  I get the following repeatedly in the curl log.  Clearing browser cache makes no difference.\n[60.911664] curl: info: Hostname was NOT found in DNS cache\n[66.426945] curl: info: Resolving timed out after 5515 milliseconds\n[66.427220] curl: info: Closing connection 0\n. Had an incorrect dns server in dhcpcd.conf.  Changed and all working fine now.  Many thanks! - Tom . ",
    "da0ist": "jamesward@X250:~$ ls -l /media\nls: cannot access '/media/gdrive': Transport endpoint is not connected\ntotal 0\nd????????? ? ? ? ?            ? gdrive\njamesward@X250:~$ google-drive-ocamlfuse -cc /media/gdrive\nError: Mountpoint /media/gdrive should be an existing directory.\n. Rebooting seems to have fixed it eventually. I found Ubuntu 17.10 too unstable to do my work, so I've installed Debian this morning. Thanks for your help!. ",
    "aapthorp": "Had the same issue today. Added ,_netdev after 'user' to my fstab entry for gdrive. e.g. user,_netdev This seemed to fix it.\n. ",
    "HuLian21": "\n\nI don't know why, but it's looks locked. Than I try chmod a+rw command, but it's looks not very useful.\nI used OPENVZ/Ubuntu16.04/Linux 2.6.32-042stab123.9\n. I was tryed your command, and it still can't work. \ngoogle-drive-ocamlfuse -label googledrive ~/googledrive -o nonempty -debug\nStarting application setup (label=googledrive, base_dir=).\n[0.001636] TID=0: Loading configuration from /root/.gdfuse/googledrive/config...done\nOpening log file: /root/.gdfuse/googledrive/gdfuse.log\nI used chmod a+wr for curl.log and gdfuse.log\n. \nThank you for your patience, I tried, there is no write in the curl.log file, but gdfuse.log has\n```\n[0.005311] TID=0: Setting up googledrive filesystem...\n[0.005341] TID=0: BEGIN: Saving configuration in /root/.gdfuse/googledrive/config\n[0.005516] TID=0: END: Saving configuration in /root/.gdfuse/googledrive/config\n[0.006408] TID=0: Loading application state from /root/.gdfuse/googledrive/state...done\nCurrent version: 0.6.21\nSetting up cache db...done\nSetting up CURL...done\n```\nNothing else in there.. ",
    "pmetzger": "Weird. Even though you fixed it, the README.md doesn't have it. Did you commit it to the wrong branch?. Yah, you committed it on the gh-pages branch.. ",
    "yebrahim": "Thanks @astrada, will give that a try.. ",
    "ArminXG": "Works now, thank you!. ",
    "guestname": "This happens with me as well, and I'm using the default configuration.. What about turning the files read-only once you go offline?. ",
    "Queequeg92": "@cpassos Dude. I recommend pcloud to you. It's fabulous. You can refer to my link to register. https://www.pcloud.com/welcome-to-pcloud/?discountcode=9DoCgsm7cRysu1zLHryDqzZV. @astrada Thank you for your awesome work:gift::gift::gift:. But I often edit my code in google drive. I really need a faster sync tool. I find pcloud, which has official clients for win, mac and linux.. ",
    "matjazmav": "Yes it's too slow to be used on daily bases. I'm using Ubuntu 18.04. And when saving plain text files it take like up to 10 seconds for file to saved and program to be responsive. I'm editing in vim. \nCan I do anything? Like async save? I'm on version 0.7.0. ",
    "averyfreeman": "It just needed to sync all the files first.  I am closing the issue. . ",
    "agilebean": "thank you very much, alessandro!\nwith the folder id, it worked right away!\nstrangely, the folder name doesn't work. . Thanks a lot! \nClearing the cache indeed solved the issue, now I get the permissions set.\nThe syncing process also works correctly in ubuntu file system, but in Rstudio Server, the directory and files disappear.\nThat must be some other reason unrelated to ocamlfuse maybe...\n. ",
    "Disctanger": "@xinity \nIf you have a lots of files in your subfolder, then it will take some time to load/show folders. Sometimes waiting for some time and reloading is the solution of this problem. In my case I have two team drives:\nTEAM DRIVE 1 =======|1 folder =======|30 folders =======|etc.\nTEAM DRIVE 2 =======|10 folders =======|etc.\nI don' know why TEAM DRIVE 1 takes long time to load when I come to directory with \"30 folders\", but eventually I will be able to see folders if I wait for some time. But TEAM DRIVE 2 does not take long time to show up when I come to \"10 folders\" directory. Moreover, TEAM DRIVE 1 is quite huge comparatively to DRIVE 2. I don't know how ocaml-fuse works but waiting was my solution. \nMaybe @astrada does not have big (or huge) team drive, and there fore he was not able to reproduce this scenario.\nI hope this info will help you and please let us know if you find a solution.. ",
    "xinity": "Thanks a lot guys!\nI'll try again this morning and let you know how it goes after waiting a bit :blush:. tested it again today, still no team drive directories and a very very slow sync (behind a 100mbits fiber though)\n. ",
    "user414": "Thanks for your reply, there was a lot of logs and not sure what is sensitive information and what is not so investigating a little more I took out the part from the log that seem to correspond. Please note that borg backup doesn't work for me at all, so potentially on linux it should be easy to reproduce. I have not tried with a simple backup of a few files see if it would work, this backup was about 11GB and took some hours to at the end be full of empty files.  Let me know if there is still sensitive info in there that I should remove. Thanks.\n```\n[320.766586] curl: info: Connection #0 to host www.googleapis.com left intact\n[320.863874] curl: info:   Trying 172.217.0.106...\n[320.912125] curl: info: Connected to www.googleapis.com (172.217.0.106) port 443 (#0)\n[320.917418] curl: info: found 148 certificates in /etc/ssl/certs/ca-certificates.crt\n[320.943168] curl: info: found 604 certificates in /etc/ssl/certs\n[320.943204] curl: info: ALPN, offering http/1.1\n[320.968473] curl: info:   Trying 172.217.1.10...\n[320.974027] curl: info:   Trying 172.217.0.234...\n[321.010980] curl: info: Connected to www.googleapis.com (172.217.1.10) port 443 (#0)\n[321.015770] curl: info: Connected to www.googleapis.com (172.217.0.234) port 443 (#0)\n[321.016224] curl: info: found 148 certificates in /etc/ssl/certs/ca-certificates.crt\n[321.022032] curl: info: found 148 certificates in /etc/ssl/certs/ca-certificates.crt\n[321.042201] curl: info: found 604 certificates in /etc/ssl/certs\n[321.042236] curl: info: ALPN, offering http/1.1\n[321.051325] curl: info: found 604 certificates in /etc/ssl/certs\n[321.051369] curl: info: ALPN, offering http/1.1\n[321.220534] curl: info: SSL connection using TLS1.2 / ECDHE_ECDSA_AES_128_GCM_SHA256\n[321.220960] curl: info:     server certificate verification OK\n[321.220968] curl: info:     server certificate status verification SKIPPED\n[321.221050] curl: info:     common name: .googleapis.com (matched)\n[321.221055] curl: info:     server certificate expiration date OK\n[321.221059] curl: info:     server certificate activation date OK\n[321.221066] curl: info:     certificate public key: EC\n[321.221069] curl: info:     certificate version: #3\n[321.221091] curl: info:     subject: C=US,ST=California,L=Mountain View,O=Google Inc,CN=.googleapis.com\n[321.221120] curl: info:     start date: Tue, 30 Jan 2018 08:56:10 GMT\n[321.221125] curl: info:     expire date: Tue, 24 Apr 2018 08:30:00 GMT\n[321.221139] curl: info:     issuer: C=US,O=Google Inc,CN=Google Internet Authority G2\n[321.221149] curl: info:     compression: NULL\n[321.221152] curl: info: ALPN, server accepted to use http/1.1\n[321.221186] curl: header out: PATCH /resumable/upload/drive/v3/files/1j5DHESUXTwnr7zTauYuOp5oCvCzgToNn?fields=appProperties%2Ccapabilities%28canEdit%29%2CcreatedTime%2CexplicitlyTrashed%2CfileExtension%2CfullFileExtension%2Cid%2Cmd5Checksum%2CmimeType%2CmodifiedTime%2Cname%2Cparents%2Csize%2Ctrashed%2Cversion%2CviewedByMeTime%2CwebViewLink&supportsTeamDrives=true HTTP/1.1\nHost: www.googleapis.com\nUser-Agent: google-drive-ocamlfuse (0.6.24) gapi-ocaml/0.3.6/Unix\nAccept: /\nAccept-Encoding: identity\nTransfer-Encoding: chunked\nContent-Type: application/json\nX-Upload-Content-Type: application/octet-stream\nX-Upload-Content-Length: 62\nExpect: 100-continue\n[321.294173] curl: header in: HTTP/1.1 100 Continue\n[321.294203] curl: data out: 2b\n{\"modifiedTime\":\"2018-02-16T15:25:06.000Z\"}\n[321.294212] curl: data out: 0\n[321.312833] curl: info: SSL connection using TLS1.2 / ECDHE_ECDSA_AES_128_GCM_SHA256\n[321.313240] curl: info:     server certificate verification OK\n[321.313247] curl: info:     server certificate status verification SKIPPED\n[321.313324] curl: info:     common name: .googleapis.com (matched)\n[321.313329] curl: info:     server certificate expiration date OK\n[321.313333] curl: info:     server certificate activation date OK\n[321.313340] curl: info:     certificate public key: EC\n[321.313343] curl: info:     certificate version: #3\n[321.313364] curl: info:     subject: C=US,ST=California,L=Mountain View,O=Google Inc,CN=.googleapis.com\n[321.313370] curl: info:     start date: Tue, 30 Jan 2018 08:56:10 GMT\n[321.313375] curl: info:     expire date: Tue, 24 Apr 2018 08:30:00 GMT\n[321.313387] curl: info:     issuer: C=US,O=Google Inc,CN=Google Internet Authority G2\n[321.313396] curl: info:     compression: NULL\n[321.313399] curl: info: ALPN, server accepted to use http/1.1\n[321.313428] curl: header out: PATCH /resumable/upload/drive/v3/files/1j5DHESUXTwnr7zTauYuOp5oCvCzgToNn?fields=appProperties%2Ccapabilities%28canEdit%29%2CcreatedTime%2CexplicitlyTrashed%2CfileExtension%2CfullFileExtension%2Cid%2Cmd5Checksum%2CmimeType%2CmodifiedTime%2Cname%2Cparents%2Csize%2Ctrashed%2Cversion%2CviewedByMeTime%2CwebViewLink&supportsTeamDrives=true HTTP/1.1\nHost: www.googleapis.com\nUser-Agent: google-drive-ocamlfuse (0.6.24) gapi-ocaml/0.3.6/Unix\nAccept: /\nAccept-Encoding: identity\nTransfer-Encoding: chunked\nContent-Type: application/json\nX-Upload-Content-Type: application/octet-stream\nX-Upload-Content-Length: 17\nExpect: 100-continue\n[321.325304] curl: info: SSL connection using TLS1.2 / ECDHE_ECDSA_AES_128_GCM_SHA256\n[321.325716] curl: info:     server certificate verification OK\n[321.325724] curl: info:     server certificate status verification SKIPPED\n[321.325804] curl: info:     common name: .googleapis.com (matched)\n[321.325809] curl: info:     server certificate expiration date OK\n[321.325822] curl: info:     server certificate activation date OK\n[321.325830] curl: info:     certificate public key: EC\n[321.325833] curl: info:     certificate version: #3\n[321.325855] curl: info:     subject: C=US,ST=California,L=Mountain View,O=Google Inc,CN=.googleapis.com\n[321.325860] curl: info:     start date: Tue, 30 Jan 2018 08:56:10 GMT\n[321.325865] curl: info:     expire date: Tue, 24 Apr 2018 08:30:00 GMT\n[321.325877] curl: info:     issuer: C=US,O=Google Inc,CN=Google Internet Authority G2\n[321.325887] curl: info:     compression: NULL\n[321.325890] curl: info: ALPN, server accepted to use http/1.1\n[321.325916] curl: header out: PATCH /drive/v3/files/1j5DHESUXTwnr7zTauYuOp5oCvCzgToNn?fields=appProperties%2Ccapabilities%28canEdit%29%2CcreatedTime%2CexplicitlyTrashed%2CfileExtension%2CfullFileExtension%2Cid%2Cmd5Checksum%2CmimeType%2CmodifiedTime%2Cname%2Cparents%2Csize%2Ctrashed%2Cversion%2CviewedByMeTime%2CwebViewLink&supportsTeamDrives=true HTTP/1.1\nHost: www.googleapis.com\nUser-Agent: google-drive-ocamlfuse (0.6.24) gapi-ocaml/0.3.6/Unix\nAccept: /\nAccept-Encoding: identity\nTransfer-Encoding: chunked\nContent-Type: application/json\nExpect: 100-continue\n[321.377958] curl: header in: HTTP/1.1 100 Continue\n[321.377993] curl: data out: 2b\n{\"modifiedTime\":\"2018-02-16T15:25:06.000Z\"}\n[321.378001] curl: data out: 0\n[321.710237] curl: header in: HTTP/1.1 200 OK\n[321.710255] curl: header in: X-GUploader-UploadID: AEnB2Uox5gGnanHqEtsnpEKNJSXh7lFv7rR5bxwxuCYpCJHI4YgPTj4sbQsxBaeOliOLkHbDwOzHT_jU6-w1AKZJLdRcCDDCTQ\n[321.710260] curl: header in: Location: https://www.googleapis.com/resumable/upload/drive/v3/files/1j5DHESUXTwnr7zTauYuOp5oCvCzgToNn?fields=appProperties%2Ccapabilities%28canEdit%29%2CcreatedTime%2CexplicitlyTrashed%2CfileExtension%2CfullFileExtension%2Cid%2Cmd5Checksum%2CmimeType%2CmodifiedTime%2Cname%2Cparents%2Csize%2Ctrashed%2Cversion%2CviewedByMeTime%2CwebViewLink&supportsTeamDrives=true&upload_id=AEnB2Uox5gGnanHqEtsnpEKNJSXh7lFv7rR5bxwxuCYpCJHI4YgPTj4sbQsxBaeOliOLkHbDwOzHT_jU6-w1AKZJLdRcCDDCTQ\n[321.710263] curl: header in: Vary: Origin\n[321.710266] curl: header in: Vary: X-Origin\n[321.710268] curl: header in: Cache-Control: no-cache, no-store, max-age=0, must-revalidate\n[321.710271] curl: header in: Pragma: no-cache\n[321.710273] curl: header in: Expires: Mon, 01 Jan 1990 00:00:00 GMT\n[321.710276] curl: header in: Date: Fri, 16 Feb 2018 15:25:06 GMT\n[321.710278] curl: header in: Content-Length: 0\n[321.710282] curl: header in: Server: UploadServer\n[321.710284] curl: header in: Content-Type: text/html; charset=UTF-8\n[321.710287] curl: header in: Alt-Svc: hq=\":443\"; ma=2592000; quic=51303431; quic=51303339; quic=51303338; quic=51303337; quic=51303335,quic=\":443\"; ma=2592000; v=\"41,39,38,37,35\"\n[321.710290] curl: header in: \n[321.710296] curl: info: Connection #0 to host www.googleapis.com left intact\n[321.710381] curl: info: Found bundle for host www.googleapis.com: 0x7fa4cd4a0550 [can pipeline]\n[321.710390] curl: info: Re-using existing connection! (#0) with host www.googleapis.com\n[321.710395] curl: info: Connected to www.googleapis.com (172.217.0.106) port 443 (#0)\n[321.710413] curl: header out: PUT /resumable/upload/drive/v3/files/1j5DHESUXTwnr7zTauYuOp5oCvCzgToNn?fields=appProperties%2Ccapabilities%28canEdit%29%2CcreatedTime%2CexplicitlyTrashed%2CfileExtension%2CfullFileExtension%2Cid%2Cmd5Checksum%2CmimeType%2CmodifiedTime%2Cname%2Cparents%2Csize%2Ctrashed%2Cversion%2CviewedByMeTime%2CwebViewLink&supportsTeamDrives=true&upload_id=AEnB2Uox5gGnanHqEtsnpEKNJSXh7lFv7rR5bxwxuCYpCJHI4YgPTj4sbQsxBaeOliOLkHbDwOzHT_jU6-w1AKZJLdRcCDDCTQ HTTP/1.1\nHost: www.googleapis.com\nUser-Agent: google-drive-ocamlfuse (0.6.24) gapi-ocaml/0.3.6/Unix\nAccept: /\nAccept-Encoding: identity\nTransfer-Encoding: chunked\nContent-Type: application/octet-stream\nContent-Range: bytes 0-61/62\nExpect: 100-continue\n[321.781673] curl: header in: HTTP/1.1 100 Continue\n[321.781705] curl: data out: 11\n{\"exclusive\": []}\n[321.781718] curl: data out: 0\n[321.814716] curl: header in: HTTP/1.1 200 OK\n[321.814735] curl: header in: X-GUploader-UploadID: AEnB2Ur257mZ5H2eHfI6ek7k9-8AKoIb9BVIlU9s_MJpyil58Wy1sOr2nf4V3zbl8gnw-XN3beEu-QwB81WGQaaSTLLfSrXJ6Q\n[321.814740] curl: header in: Location: https://www.googleapis.com/resumable/upload/drive/v3/files/1j5DHESUXTwnr7zTauYuOp5oCvCzgToNn?fields=appProperties%2Ccapabilities%28canEdit%29%2CcreatedTime%2CexplicitlyTrashed%2CfileExtension%2CfullFileExtension%2Cid%2Cmd5Checksum%2CmimeType%2CmodifiedTime%2Cname%2Cparents%2Csize%2Ctrashed%2Cversion%2CviewedByMeTime%2CwebViewLink&supportsTeamDrives=true&upload_id=AEnB2Ur257mZ5H2eHfI6ek7k9-8AKoIb9BVIlU9s_MJpyil58Wy1sOr2nf4V3zbl8gnw-XN3beEu-QwB81WGQaaSTLLfSrXJ6Q\n[321.814743] curl: header in: Vary: Origin\n[321.814746] curl: header in: Vary: X-Origin\n[321.814749] curl: header in: Cache-Control: no-cache, no-store, max-age=0, must-revalidate\n[321.814751] curl: header in: Pragma: no-cache\n[321.814754] curl: header in: Expires: Mon, 01 Jan 1990 00:00:00 GMT\n[321.814757] curl: header in: Date: Fri, 16 Feb 2018 15:25:06 GMT\n[321.814759] curl: header in: Content-Length: 0\n[321.814763] curl: header in: Server: UploadServer\n[321.814766] curl: header in: Content-Type: text/html; charset=UTF-8\n[321.814768] curl: header in: Alt-Svc: hq=\":443\"; ma=2592000; quic=51303431; quic=51303339; quic=51303338; quic=51303337; quic=51303335,quic=\":443\"; ma=2592000; v=\"41,39,38,37,35\"\n[321.814771] curl: header in: \n[321.814778] curl: info: Connection #0 to host www.googleapis.com left intact\n[321.814851] curl: info: Found bundle for host www.googleapis.com: 0x7fa4c82b4320 [can pipeline]\n[321.814860] curl: info: Re-using existing connection! (#0) with host www.googleapis.com\n[321.814865] curl: info: Connected to www.googleapis.com (172.217.1.10) port 443 (#0)\n[321.814885] curl: header out: PUT /resumable/upload/drive/v3/files/1j5DHESUXTwnr7zTauYuOp5oCvCzgToNn?fields=appProperties%2Ccapabilities%28canEdit%29%2CcreatedTime%2CexplicitlyTrashed%2CfileExtension%2CfullFileExtension%2Cid%2Cmd5Checksum%2CmimeType%2CmodifiedTime%2Cname%2Cparents%2Csize%2Ctrashed%2Cversion%2CviewedByMeTime%2CwebViewLink&supportsTeamDrives=true&upload_id=AEnB2Ur257mZ5H2eHfI6ek7k9-8AKoIb9BVIlU9s_MJpyil58Wy1sOr2nf4V3zbl8gnw-XN3beEu-QwB81WGQaaSTLLfSrXJ6Q HTTP/1.1\nHost: www.googleapis.com\nUser-Agent: google-drive-ocamlfuse (0.6.24) gapi-ocaml/0.3.6/Unix\nAccept: /\nAccept-Encoding: identity\nTransfer-Encoding: chunked\nContent-Type: application/octet-stream\nContent-Range: bytes 0-16/17\nExpect: 100-continue\n[321.880804] curl: header in: HTTP/1.1 100 Continue\n[321.880855] curl: data out: 11\n{\"exclusive\": []}\n[321.880872] curl: data out: 0\n[322.012565] curl: header in: HTTP/1.1 400 Bad Request\n[322.012589] curl: header in: X-GUploader-UploadID: AEnB2Uox5gGnanHqEtsnpEKNJSXh7lFv7rR5bxwxuCYpCJHI4YgPTj4sbQsxBaeOliOLkHbDwOzHT_jU6-w1AKZJLdRcCDDCTQ\n[322.012592] curl: header in: Content-Length: 181\n[322.012595] curl: header in: Date: Fri, 16 Feb 2018 15:25:07 GMT\n[322.012598] curl: header in: Server: UploadServer\n[322.012602] curl: header in: Content-Type: text/html; charset=UTF-8\n[322.012604] curl: header in: Alt-Svc: hq=\":443\"; ma=2592000; quic=51303431; quic=51303339; quic=51303338; quic=51303337; quic=51303335,quic=\":443\"; ma=2592000; v=\"41,39,38,37,35\"\n[322.012607] curl: header in: \n[322.012610] curl: data in: Invalid request.  There were 17 byte(s) in the request body.  There should have been 62 byte(s) (starting at offset 0 and ending at offset 61) according to the Content-Range header.\n[322.012617] curl: info: Connection #0 to host www.googleapis.com left intact\n[322.326966] curl: info: Done waiting for 100-continue\n[322.327009] curl: data out: 10\n{\"trashed\":true}\n[322.327018] curl: data out: 0\n. @astrada Thanks for the suggestions, will do more test and try this and post back with the results. Will also try and put my observations and suggestions from all this in a few concise points.. After some more testing the suggestion of usingasync_upload=false``` in the config worked for us without turning off the multi-threading. We will continue using it like that and see. Here is some suggestions and observations after our testing\n\nSince it seems that asynchronous upload is not as reliable as synchronous upload and the penalty seemed small to us perhaps the default should be changed to synchronous upload. \nWe ran into issues when using asynchronous upload and having errors where the cache would become corrupted. This was quite a serious issue since it seems like our backup worked when in reality there's actually no file in your google drive. Since for a file system I prefer reliability to performance I was thinking perhaps either tracking down those issues so that the cache is always valid even if there is some file upload error, or for now adding an option to turn off the cache. In the config there seem to have no such option now but there is an option for cache size, would putting that to 0 turn off the cache?\n\nThe rest worked well. While it's unfortunate that google cloud storage API and google drive API are not the same it might be less work to piggyback on the GCS fuse project and add a few google drive specific overwrite to benefit from all the other work they have done, or it might be more of a mess :-) In any case thanks for the help and the code.   \n. @astrada Should we close this? Just pasting one question from the previous comment I think you may have missed it.\n\"In the config there seem to have no such option now but there is an option for cache size, would putting that to 0 turn off the cache?\"\nThanks\n. > Not really. The cache should keep just one file that is the current file being uploaded/downloaded (but I never tested that scenario).\nThanks for the info. For reference, either we were really lucky because when we tested our backup we picked a random file to open in all the 11 GB of file and it worked. Either we were very lucky and somehow out of thousands of files picked the last one which was uploaded and it gave us the impression that the backup had worked or the cache keep more then 1 file. In addition, the reason why I thought the cache manage more then 1 file is I looked at the code only very briefly but there seem to be a lot more cache code then necessary to manage just 1 file, like a full sqlite db. It thought it was heavy to just manage 1 file and just assume with all that code it was a full cache functionality. However, perhaps I got confused and all that code is part of a standard cache component but somewhere it limits the number of entries in the cache to 1.. So should we close this and I can open another issue as a feature request for turning off the cache? And perhaps one to change the default if you agree?. ",
    "dragonator4": "I think I was totally off track when I opened the bug report. The issue was not kernel 4.13, but the fact that one of my Google accounts is managed by my university, and has a policy that external apps will be granted access only for 30 days. This is something that is new, probably, because I had been using this program for well over a year without it this hanging business happening before (hence also my confusion about the kernel update, compounded by the fact that quite a few programs were glitchy after the update).\nSo, I think it is safe to close this bug report. I would like to open a new one, with a feature request that there be something to \"refresh\" the access tokens. Thanks!. ",
    "ChrisColotti": "Also just caught this after a reboot\nFeb 21 07:14:56 plex systemd[1]: mnt-gdrive.mount: Mounting timed out. Stopping.\nFeb 21 07:14:56 plex systemd[1]: mnt-gdrive.mount: Unit entered failed state.. Buggered.  I found another tool PLexDrive that seems to work, but would be nice if someone using this with plex can comment.. ",
    "themurph2k": "I believe elinks can be compiled with Javascript support. Thanks much, I will give it a try! I was hoping to avoid installing a desktop environment completely, though :(. ",
    "chaimleib": "I tried this out, but I ran into trouble.\n$ google-drive-ocamlfuse -headless -id `jq .installed.client_id ~/gdrive-ocamlfuse-secrets.json` -secret `jq .installed.client_secret ~/gdrive-ocamlfuse-secrets.json`                                                                                                                                      \nPlease, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=%22xxxxxxxxxxxxxx.apps.googleusercontent.com%22&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\nPlease enter the verification code:\nWhen I opened the link from the CLI in Chrome, I got a 401:\n\n\nThat\u2019s an error.\n\nError: invalid_client\nThe OAuth client was not found.\nRequest Details\nclient_id=\"xxxxxxxxxxxxxx.apps.googleusercontent.com\"\nredirect_uri=urn:ietf:wg:oauth:2.0:oob\nscope=https://www.googleapis.com/auth/drive\nresponse_type=code\naccess_type=offline\napproval_prompt=force\n\nI have the Google Drive API added to my project's library. Not sure what is wrong.\nEDIT: PEBKAC. I forgot to use jq -r for raw output without quotes.. ",
    "JacobEFO": "Helle Astrada\nThank you very much for your link, I am sorry I did not find it myself. However I tried doing the script thingie, and it did not automount on start-up, and now I am unfortunately stuck with a folder named \"Google_Drive\" which I cannot mount to or delete.\nEven if i try \"sudo rm -rf Google_Drive\" it says \"rm: cannot remove 'Google_Drive': is a directory\". I have no idea, what to do with it. \nDo you have any suggestions? \nBest regards Jacob. Hello Astrada\nThat worked splendidly.\nNow in order to use the automount function, should I use ocamlfuse (the first guide) and set the mount point before I follow the automount guide?\nBest regards Jacob. ",
    "usawa": "beta ppa doesn't include a build for Ubuntu 17.10 artful.... I manually donwloaded and installed the 0.6.25 from bionic to artful and it's ok. Thanks. To close.. ",
    "Cieper": "On Debian, an apt-get install fuse fixed this issue for me.. ",
    "JonasUliana": "worked well with fuse dependency (in Debian), maybe this can be considered a closed issue?  . ",
    "mozai": "I tried the workaround in that bug report:\n\nsudo chmod 755 /root /root/.config\nmkdir /root/.config/chromium\nsudo chown -R $USER:$USER /root/.config/chromium\n\nStill the same problem.  Watched it with strace -f, saw execve(\"/usr/bin/chromium-browser\", [\"/usr/bin/chromium-browser\", \"https://accounts.google.com/o/oa\"...] but I believe I'm missing the syscalls inside chromium.\nGoing to try changing my default browser to 'firefox' next... and that worked, it opened a new tab and google prompts me for authenticating with gdfuse: \"gdfuse wants to view and manage the files in your Google Drive.\"\nworkaround: don't use Chrome/Chromium ?. ",
    "hyperpress": "Well, it's a permissions issue on the folder. So, I'd check ownership/perms on the folder and also verify that your fstab user uid/gid is correct.. ",
    "ryanmjacobs": "Okay, I saw this line in the config file: max_cache_size_mb=512. But maybe it isn't being considered?\n\n. ",
    "rkkoszewski": "Hi, how is it looking g with this? I have a 2gb partition where I'm running this and it keeps running out of memory due to this cache.\nIs there any way to disable it yet?\nI'm using:\nmax_cache_size_mb=0\nstream_large_files=true\nlarge_file_threshold_mb=0\nI tried also with large_file_threshold_mb=1 and different combinations. The cache is still using gigabytes of space till the whole partition is full and the whole system freezes. Any way to disable completely cache? I'm using rsync to sync my files to the Google drive.. Are you mounting in an lxc container?. I did manage to make to make google-drive-ocamlfuse work in an LXC container. But you need to configure AppArmor and OpenVZ to allow access to /dev/fuse. It's not an google-drive-ocamlfuse issue though, but you just don't have proper permission to /dev/fuse in your containers.. ",
    "MartinX3": "I want to download 8GB on my external HDD.\nI made the mount point on my HDD and copied the files via file explorer on the same partition on my external hdd.\nBut it want to cache every downloaded file first on my too small home partition.\nHanging my system, consuming 100% on one cpu thread and freezing.. Thank you very much for your fast response!\nAt the moment, I don't need to upload files, only download up to 2.2 GB large files.\nBut I don't know if I want to upload files in the future with your tool.\nWhich disadvantages will stream_large_files=true gives me?\nIs it possible to move the cache into the RAM?. I will try it, thank you. :). ",
    "olsonpm": "Okay - I'll keep the small sleep time then.  I have a feeling digging into why the wireless signal all of a sudden takes longer to initiate is something I'd rather avoid.\nThanks for getting back to me.. ",
    "Theumis": "To update my issue: I still get the error \"Transport endpoint is not connected\" randomly but I can't find the CURLE_OPERATION_TIMEOUTED in the gdfuse.log anymore.\nBut what could be the problem, that google-drive-ocamlfuse is disconnecting nearly every day?. ",
    "KharmaScribbles": "I have been using GDrive daily for the past month or more, manually mounting it with the same command after each boot. Made an alias, everything worked fine, I have had no problems saving to it locally, remotely, other programs reading or writing to it worked great etc.  All of a sudden today, April 25, I am getting the same error as above about \"Transport endpoint not connected\" when I run my mount command.  When I run the command terminal appears to complete successfully, as terminal just prompts me with a new line as usual.  \nHowever, starting today, after attempting to mount GDfuse my entire Thunar file manager freezes up and eventually produces an error that it can't open my home directory because of the unconnected endpoint related to my GDfuse mountpoint. When the error in Thunar displays, I can see my mountpoint for GDfuse created and appears mounted, but if I click on the Unmount button I get an error that it can't unmount, because of a missing end point. I can not browse to it either.\nMy audio programs like Cmus and Mocp also can not read my home dir, they freeze and never complete so I have to kill their processes.\nNumerous reboots.\nNothing at all in the log.\nMy cache file in .gdfuse was insanely huge, but even after deleting the contents and rebooting I still receive the error, so probably not related.\nUbuntu 16.04\n. ",
    "emmmmmmp": "I got the same problem after downloading a large file(4.4GB). \nUbuntu 16.04. ",
    "ebeeman": "I attempted to figure out how opam learns about dependencies and this allowed me to solve this issue.\nI found additional depext troubleshooting steps from \"Debian install issue #280\" and tried to apply the findings.\nRan this command to find out if the dependencies were able to be listed:\n      $ opam list --rec --required-by google-drive-ocamlfuse\n\nResulted in:\nAvailable packages recursively required by google-drive-ocamlfuse.0.6.23 for system:\n<...........truncated as too many to list.........>\nSince that worked OK, I figured I would try the app name version found via above \"google-drive-ocamlfuse.0.6.23\" directly into the depext command that was causing the error:\n        $ opam depext google-drive-ocamlfuse.0.6.23\n\nResulted in:\nDetecting depexts using flags: x86_64 osx homebrew\nThe following system packages are needed:\n - Caskroom/cask/osxfuse\n - gmp\n - lzlib\n - pkg-config\n - sqlite3\nUpdating Homebrew...\nThat allowed the software to install and I was able to complete instructions.  Not sure why the difference in results between using google-drive-ocamlfuse.0.6.23 vs google-drive-ocamlfuse but thought you should be aware of this.\n. ",
    "bittner": "\nWhat do you mean with:\n\nI thought the doc folder could/should host the GitHub pages (instead of the gh-pages branch). GitHub pages can be configured that way. - I didn't intend the Wiki. Sorry for the confusion.\n\nI periodically copy the wiki content to the doc folder\n\nWhy do you feel this is necessary?. ",
    "Guilty-King": "@astrada thank you for the helpful link. As the post is from 2016 lxc is now replaced by docker ? \nI just started using docker and don't know anything about linux containers / lxc\nHave you tried the method in above post?. The stackoverflow answer doesn't help as it needs to run the container in privileged mode and attach fuse device from host and so the container is dependent on host and porting the container to another host won't work. But thank you for your answer I will do some more research. ",
    "antisocial89": "So for whatever reason, switching from Ubuntu 16.04 to CentOS 7 resolved my issue.\nI went from 3KB/sec to 105MB/sec, don't know. Oh well.. ",
    "blaggacao": "431 My damn router!. My damn router!.",
    "ripoyet": "Thank you very much. Other thing that I wanted to know. I've changed the permissions and now I have all of them, but if want to enter inside with my rtorrent client, the folder doesn't exist. Thanks in advance.. ",
    "link0087": "Hi, \ni have the same problem too, the real problem is that it is not easy to detect when it crash. there is a way to mount all on debug mode without left the shell opened? beacuase is somenthing that happen every day but randomly so i cant left the shell opened on debug mode.\nhere is my configuration:\napps_script_format=json\napps_script_icon=\nasync_upload=true\ncache_directory=\nclient_id=MY-CLIENT-ID\nclient_secret=MY-CLIENT-SECRET\nconnect_timeout_ms=5000\ncurl_debug_off=false\ndata_directory=\ndelete_forever_in_trash_folder=false\ndocs_file_extension=true\ndocument_format=odt\ndocument_icon=\ndownload_docs=true\ndrawing_format=png\ndrawing_icon=\nform_format=zip\nform_icon=\nfusion_table_format=desktop\nfusion_table_icon=\nkeep_duplicates=false\nlarge_file_read_only=false\nlarge_file_threshold_mb=16\nlog_directory=\nlost_and_found=false\nlow_speed_limit=0\nlow_speed_time=0\nmap_format=desktop\nmap_icon=\nmax_cache_size_mb=512\nmax_download_speed=0\nmax_memory_cache_size=10485760\nmax_retries=8\nmax_upload_chunk_size=1099511627776\nmax_upload_speed=0\nmemory_buffer_size=1048576\nmetadata_cache_time=60\npresentation_format=pdf\npresentation_icon=\nread_ahead_buffers=3\nread_only=false\nroot_folder=\nshared_with_me=false\nspreadsheet_format=ods\nspreadsheet_icon=\nsqlite3_busy_timeout=20\nstream_large_files=true\nteam_drive_id=\numask=0o1411\nverification_code=\nMy server is ubuntu 16.04\nThank you\nF.\n. up. ok i found what happens, but without your debug i donno why.\non the syslog i found\nOut of memory: Kill process 749 (google-drive-oc) score 96 or sacrifice child\nJul  4 10:13:11  kernel: [] Killed process 749 (google-drive-oc) total-vm:671628kB, anon-rss:377648kB, file-rss:740kB\nSo i think that there is maybe some loop on the coode in some case? \nDo you think that we can have the -debug option on background mode?\nThank you so much\n. ",
    "inventor96": "@link0087 I've created this cronjob to enable debug logging in the background:\n@reboot sleep 10; google-drive-ocamlfuse -debug -config /opt/gdfuse/default/config /opt/gdrive > /opt/gdfuse/gdfuse.cron.log 2>&1\nAnd I have this setting in my config file: log_directory=/opt/gdfuse/default\nBecause it's running as a cron job and not a user, I've explicitly specified the other directory values, too.\nWith all this set up, I get the debug logging in /opt/gdfuse/default/gdfuse.log, and any output from the command itself in /opt/gdfuse/gdfuse.cron.log. ",
    "selay01": "after exec \"!google-drive-ocamlfuse gdrive\",i loss the contrl of gdrive,can't unmount,can't access,can't remove!\ncould google-drive-ocamlfuse run on google's colaboratory?if not,how could i remove the fold 'gdrive'?. i successfully reinstalled the google-drive-ocamlfuse as follow,\nthe fold 'gdrive' is still can't unmount,can't access,can't remove!\n1. Install a Drive FUSE wrapper google-drive-ocamlfuse.\n!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n!apt-get update -qq 2>&1 > /dev/null\n!apt-get -y install -qq google-drive-ocamlfuse fuse\n2. Generate auth tokens for Colab\nfrom google.colab import auth\nauth.authenticate_user()\n3. Generate creds for the Drive FUSE library.\nfrom oauth2client.client import GoogleCredentials\ncreds = GoogleCredentials.get_application_default()\nimport getpass\n!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\nvcode = getpass.getpass()\n!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n4. Create a directory and mount Google Drive using that directory.\n!mkdir -p godrive\n!google-drive-ocamlfuse godrive\nprint('Files in Drive:')\n!ls godrive/\n5. Create a file in Drive.\n!echo \"This newly created file will appear in your Drive file list.\" > godrive/created.txt. @astrada  I think you are right.\nThe virtual machine of google's colab is destroyed automaticly after a long time, so the file  'gdrive' is not a problem any more.\nI will close this issue.. ",
    "Themex": "Thx for the answer!\nI've tried another method. Apache2 uses www-data user, so I've found out it's gid and uid in terminal. \nid www-data\nI used them as args on mounting. \ngoogle-drive-ocamlfuse /var/www/html/user-media -f -o uid=33, gid=33, allow_other\nSo that I can access files on gdrive from apache2. I've got one more little question\nIs it possible to apply root_folder on mount?\nFor instance,\ngoogle-drive-ocamlfuse /var/www/html/user-media -f -o uid=33, gid=33, allow_other, root_folder=my_folder_id_or_path_whatever. ",
    "GiGaFotress": "it looks like my googleaccount has been banned accuse this bug. closed. ",
    "cowboycodur": "https://github.com/astrada/google-drive-ocamlfuse/wiki/Headless-Usage-&-Authorization. Make sure this folder already exists/media/external/cloud/google-drive/\nAlso since you have had a failed attempt, it may just hang everytime now. That happened to me. Delete your .gdfuse directory and try again.. ",
    "OneCDOnly": "Thanks @cowboycodur, that's what I needed. :)\nSo, followed the instructions (with a bit of variation as Google have changed parts of the API site), and the authentication  appears to have worked correctly: Access token retrieved correctly, but I'm not 100% sure. \n``\n$ google-drive-ocamlfuse -label me /media/external/cloud/google-drive/\nfuse: failed to exec fusermount: No such file or directory\n````\nSo,sudo apt-get install fuseto getfusermount`.\nWhen I tried mounting my Google Drive, the process appears to hang:\n$ google-drive-ocamlfuse -label me -debug /media/external/cloud/google-drive/\nStarting application setup (label=me, base_dir=).\n[0.000417] TID=0: Loading configuration from /home/me/.gdfuse/me/config...done\nOpening log file: /home/me/.gdfuse/me/gdfuse.log\nSo, I checked the [~/.gdfuse/me/gdfuse.log] file:\n$ cat gdfuse.log \n[0.001271] TID=0: Setting up me filesystem...\n[0.001360] TID=0: BEGIN: Saving configuration in /home/me/.gdfuse/me/config\n[0.041641] TID=0: END: Saving configuration in /home/me/.gdfuse/me/config\n[0.041929] TID=0: Loading application state from /home/me/.gdfuse/me/state...done\nCurrent version: 0.6.24\nSetting up cache db...done\nSetting up CURL...done\nRefresh token already present.\n[0.044800] TID=0: Starting filesystem /media/external/cloud/google-drive/\n[0.052132] TID=1: init_filesystem\n[0.052266] TID=1: BEGIN: Getting root folder id (team drive id=, root folder=) from server\n[0.052443] TID=1: END: Getting root folder id (id=root) from server\nAnd nothing else happens. I also tried with sudo, but got the same thing - process appears to hang. \nAny ideas? Thank you.. Yes, that directory exists. \nI removed .gdfuse then went through the authorisation process again, and am back to the same issue - attempting to mount just stalls.. OK, I've done some more checking.\nI logged in though a second terminal and discovered that even though the process appears to hang, my Google Drive is being mounted. But if I CTRL+C the original google-drive-ocamlfuse in my first terminal, the mount drops immediately. \nIs google-drive-ocamlfuse supposed to work that way? I thought it operated more like a mount command?. Oh damn, didn't think of that! \nYes, works fine now that I've removed -debug.\nThanks for your help guys. :)\n. ",
    "remram44": "This is not how mounting works. But you should be able to move those files out of the way before mounting, then move them back.\nEither way, mounting over your whole home seems like a bad idea. Why not mount specific folders, such as Documents/Pictures/Music, and keep your home local?\nNote that google-drive-ocamlfuse itself has its configuration in a dot-folder in your home. How will it work if that file now lives on Google Drive?. ",
    "jpescola": "I'm just following the installation manual: \nhttps://github.com/astrada/google-drive-ocamlfuse/wiki/Installation. ",
    "marinoborges": "I installed successfully through these steps:\nroot@gdrivefuse:~# apt-get update\nroot@gdrivefuse:~# apt-get upgrade #version debian 9.4\nroot@gdrivefuse:~# apt install opam software-properties-common dirmngr libcurl4-gnutls-dev libfuse-dev libgmp-dev libsqlite3-dev m4 pkg-config zlib1g-dev fuse\nroot@gdrivefuse:~# useradd -m -s \"/bin/bash\" gdrive-backup\nroot@gdrivefuse:~# sudo gdrive-backup\ngdrive-backup@gdrivefuse:~$ opam init\ngdrive-backup@gdrivefuse:~$ eval `opam config env`\ngdrive-backup@gdrivefuse:~$ . /home/gdrive-backup/.opam/opam-init/init.sh > /dev/null 2> /dev/null || true\ngdrive-backup@gdrivefuse:~$ opam update\ngdrive-backup@gdrivefuse:~$ opam install depext\ngdrive-backup@gdrivefuse:~$ opam depext google-drive-ocamlfuse\ngdrive-backup@gdrivefuse:~$ opam install google-drive-ocamlfuse. ",
    "thwaller": "I have authenticated using my own OAuth2 client ID, using the method above is still not working with same results.. This was closed as it was fixed? There is no comment as to what has happened.. ",
    "eightwt": "When I try to authenticate using:\n$ google-drive-ocamlfuse\nI get nothing. The browser does not launch, however the .gfuse directory is created.. ",
    "grasmanek94": "same here..\nroot@SERVER:/home/insync# google-drive-ocamlfuse -version\ngoogle-drive-ocamlfuse, version 0.6.24\nCopyright (C) 2012-2018 Alessandro Strada\nLicense MIT\nconfig:\napps_script_format=json\napps_script_icon=\nasync_upload=true\ncache_directory=\nclient_id=****************************************************************************************\nclient_secret=*******************************\nconnect_timeout_ms=5000\ncurl_debug_off=false\ndata_directory=\ndelete_forever_in_trash_folder=false\ndocs_file_extension=true\ndocument_format=odt\ndocument_icon=\ndownload_docs=true\ndrawing_format=png\ndrawing_icon=\nform_format=zip\nform_icon=\nfusion_table_format=desktop\nfusion_table_icon=\nkeep_duplicates=false\nlarge_file_read_only=false\nlarge_file_threshold_mb=128\nlog_directory=\nlost_and_found=false\nlow_speed_limit=0\nlow_speed_time=0\nmap_format=desktop\nmap_icon=\nmax_cache_size_mb=1638\nmax_download_speed=0\nmax_memory_cache_size=1073741824\nmax_retries=8\nmax_upload_chunk_size=10995116277\nmax_upload_speed=0\nmemory_buffer_size=107374182\nmetadata_cache_time=7200\npresentation_format=pdf\npresentation_icon=\nread_ahead_buffers=3\nread_only=false\nroot_folder=\nshared_with_me=false\nspreadsheet_format=ods\nspreadsheet_icon=\nsqlite3_busy_timeout=30000\nstream_large_files=true\nteam_drive_id=\numask=0o022\nverification_code=\nthe interesting parts:\nlarge_file_threshold_mb=128\nmax_cache_size_mb=1638\nmax_memory_cache_size=1073741824\nmax_upload_chunk_size=10995116277\nmemory_buffer_size=107374182\nread_ahead_buffers=3\nwhich would mean.. 128 MiB, 1638 MiB, 1074 MiB, 11GiB, 107 MiB.. and if we take the biggest and multiply that by 3 we get 33 GB.. yet:\nroot@SERVER:/home/sync/.gdfuse/default# du cache -h\n241G    cache\nEven if I add together all cache sizes it's just 14GB, 17 times less than the current cache.\nMaybe cache max size =\nmax_retries*read_ahead_buffers*(large_file_threshold_mb+max_cache_size_mb+max_memory_cache_size+max_upload_chunk_size+memory_buffer_size)\n?\nHow can I prevent this from happening?. ",
    "pahome": "It's ok when I mount by sudo:\n~$ sudo google-drive-ocamlfuse googledrivercloud/. ",
    "Nate872711": "try logging into the user account with \"su - username\" after you login to the ssh root. try \"fusermount -zu /locationofmount\"\n. ",
    "taozitaozi": "\nAre you mounting in an lxc container?\n\nOpenVZ  Server.Debian GNU/Linux 7.9 (wheezy).\nCan't mounting\uff1f\nThanks. > I did manage to make to make google-drive-ocamlfuse work in an LXC container. But you need to configure AppArmor and OpenVZ to allow access to /dev/fuse. It's not an google-drive-ocamlfuse issue though, but you just don't have proper permission to /dev/fuse in your containers.\nThanks,I'll try it.. ",
    "Z3TA": "ok, I got some progress using opam, but still stuck on this:\n```\n$ jbuilder build @install\nError: External library \"cryptokit\" not found.\n-> required by \"src/jbuild (context default)\"\n$ opam install cryptokit\n[NOTE] Package cryptokit is already installed (current version is 1.12).\n$ jbuilder external-lib-deps --missing @install\nError: The following libraries are missing in the default context:\n- Fuse\n- camlidl\n- cryptokit\n- extlib\n- gapi-ocaml\n- sqlite3\n```\nI have however installed all of them except Fuse (how do I install Fuse?) \n. ",
    "Masterxilo": "Yes man astrada, fantastic job. I am pretty sure this makes this mounter the first to implement that, thanks so much.\nOne could of course encrypt the files but I like them indexing the stuff for me. . ",
    "kopax": "Any update on this?\n. ",
    "coretemp": "I was aware that https://github.com/astrada/google-drive-ocamlfuse/issues/58 existed, but it is 4 years old. \nMy opinion of this tool currently is that it has negative value. I.e., if it hadn't existed, I wouldn't have wasted time to set it up. \nMy best result was to do a partial directory listing with ls that took ten minutes with an I/O error on top. This includes following all the relevant suggestions I have seen.  \nThis seems to match the experience of others. The S3 fuse package doesn't have these terrible slow speeds. Sure, the backends are different, but both use FUSE. So, unless you are saying it's Google, it just seems your application is the cause.\nI think you should actually warn people with a big disclaimer that for team drives it essentially doesn't work. \nPerhaps it's just me, but I don't see the point of publishing software under your name when it is this bad.  . ",
    "jellisii": "I'm having a similar problem here:  After I alter the config to add team_drive_id=[MYID] and mount, the config entry for team_drive_id gets removed from the config file and I only get my personal drive, even when using the -cc option.  \nVersion info:\ntext\njellisii@mybox:~$ google-drive-ocamlfuse -version\ngoogle-drive-ocamlfuse, version 0.6.21\nCopyright (C) 2012-2017 Alessandro Strada\nLicense MIT\nUsing PPA:\n```text\njellisii@mybox:~$ cat /etc/apt/sources.list.d/alessandro-strada-ubuntu-google-drive-ocamlfuse-beta-artful.list\ndeb http://ppa.launchpad.net/alessandro-strada/google-drive-ocamlfuse-beta/ubuntu artful main\ndeb-src http://ppa.launchpad.net/alessandro-strada/google-drive-ocamlfuse-beta/ubuntu artful main\nOS infotext\njellisii@mybox:~$ lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 17.10\nRelease:    17.10\nCodename:   artful\n```\n. ",
    "kirbyfan64": "If sudo pkill -KILL google-drive-ocamlfuse isn't working, you have problems in your system elsewhere. You could also try fusermount -u directory-you-mounted.. ",
    "rayhogenson": "I believe I am experiencing the same issue. I have google-drive-ocamlfuse installed from the beta PPA.\nThe issue can be reliably reproduced by running\ntruncate -s 50G ~/drive/test, which I would expect to take a while, it's uploading 50 gigabytes after all, but what I don't expect is I can't kill the truncate process, I can't kill google-drive-ocamlfuse while it's hung. Of course, I can't unmount either.\nAnd note the problem doesn't just occur when I truncate files, random processes get hung as OP describes, but truncate is the most reliable way I've found to demonstrate the issue. Let me know if you want logs.. ",
    "Kabouik": "Note that I am having trouble getting Airsonic to see my files in the mount point, but I am not sure whether it is 100% related to the above issue. The issue I have with Airsonic is some files that do show up in terminal are not scanned in the Airsonic library. Those that do not show up in terminal, of course, are not scanned either.\nNow could it be that files that don't show up in terminal and files that do but are not seen by Airsonic are missed for the same reason? No idea, but I created an issue on both Github projects so we can see how interrelated they are.. Two hours later, the folder Public broadcasting service was still not listed in the mount point. I did the following again:\ntouch /home/airsonic/googledrive/gdrivemus1/toto.txt\nls -la /home/airsonic/googledrive/gdrivemus1/\nrm /home/airsonic/googledrive/gdrivemus1/toto.txt\nls -la /home/airsonic/googledrive/gdrivemus1/\nAnd the folder suddenly appeared. It did so only after the rm. It is important to remember that the same manipulation two hours ago had no effect though, despite the fact that the folder was successfully added to Google Drive 30 minutes earlier.\nAirsonic still can't see it.. ",
    "lfxc": "was this ever resolved?  I create files on google drve using the web interface, and they don't show up on the server via the mount point\n. not sure how to check the version, but I just installed 2 days ago on debian 9 using these instructions:  https://github.com/astrada/google-drive-ocamlfuse/wiki/How-to-install-from-source-on-Debian-Jessie\nlet me know how to check the version and I will do so.\nIn terms of checking for the file I had Thunar open - I'm using xfce4, and I also checked through a shell thinking it may have been Thunar, but it didn't show up through a shell either.\nShould be very easy to recreate:\n1) Mount google drive in Linux desktop\n2) open file manager (Thunar in my case), and go to folder where you will upload file to\n3) upload file through the google drive browser interface using another machine.\nIn my case though, I used the same account for the google drive mount as well as for the upload from another machine, if that makes any difference.\nthanks\n. Version:\ngoogle-drive-ocamlfuse, version 0.6.23\nCopyright (C) 2012-2017 Alessandro Strada\nLicense MIT\nThat is the problem.  I do an \"ls\" from a terminal window and the file doesn't show up.  I just uploaded another test file and I'm still not seeing it after doing \"ls\" commands from a terminal for over 12 minutes.\nOne interesting thing I just noticed which may provide a clue is that the GoogleDrive mount was performed by a regular user but when I log in as root and try to \"cd\" to that mount point I get \"permission denied\"\n. that's good news.  given that I use the installation instructions here:\nhttps://github.com/astrada/google-drive-ocamlfuse/wiki/How-to-install-from-source-on-Debian-Jessie\nHow can I update to 7.1?  Not sure how to do that\nthanks\n. hmm. not working.  \nI run\n     opam siwtch create 4.07.1\nand get:\n     opam: Invalid switch subcommand \"create\"\n     Usage: opam switch [OPTION]... [COMMAND] [ARG]...\n     Try opam switch --help' oropam --help' for more information.\n. may be easier to just do the install using the latest version.  Here are the commands I use to install.  WHat would need to be changed to use the latest version?  thanks\n    apt-get -y install m4 libcurl4-gnutls-dev libfuse-dev libsqlite3-dev opam ocaml make fuse camlp4-extra build-essential pkg-config zlib1g zlib1g-dev\n\n   #!/bin/bash\n    USER=SomePerson\n    groupadd fuse\n    adduser ${USER} fuse\n    chown root:fuse /dev/fuse\n    chmod 660 /dev/fuse\n    #as user\n    sudo su - ${USER} -c 'opam init'\n    sudo su - ${USER} -c 'opam update'\n    sudo su - ${USER} -c 'opam install google-drive-ocamlfuse'\n\n. I'm getting the errors at the end of this message running \"google-drive-ocamlfuse ~/GoogleDrive\", but it did connect to GoogleDrive.\nI'm still not seeing updates on the desktop for files uploaded through the web.  In fact it's been about 4 hours now and nothing has updated, running ls commands or otherwise.\nErrors running \"google-drive-ocamlfuse ~/GoogleDrive\":\n[7721:7721:0206/142339.331153:ERROR:gl_surface_glx.cc(424)] glxQueryVersion failed\n[7721:7721:0206/142339.335369:ERROR:gl_initializer_x11.cc(147)] GLSurfaceGLX::InitializeOneOff failed.\n[7721:7721:0206/142339.391668:ERROR:viz_main_impl.cc(184)] Exiting GPU process due to errors during initialization\nGkr-Message: secret service operation failed: Failed to execute program org.freedesktop.secrets: No such file or directory\n[7:14:0206/142340.365806:ERROR:command_buffer_proxy_impl.cc(105)] ContextResult::kTransientFailure: Shared memory region is not valid\nGkr-Message: secret service operation failed: Failed to execute program org.freedesktop.secrets: No such file or directory\nAccess token retrieved correctly.\n. yes.   its' funny because if I delete something from the web interface, that change shows on the mount point.  But if I add something, then nothing shows up on the mount point.\nthe errors in previous post are actually related to Google Chrome.  It doesn't seem to have an effect on your app. sent email. ",
    "karahan": "Is it possible to utilize server accounts? @stevenmccord @astrada . ",
    "strugee": "I should also note that I have IPv6 disabled on my system.. Apparently my ISP's DNS resolvers are crap. I switched the box over to 1.1.1.1 and everything works perfectly now.\nSorry for the noise.. ",
    "m-p-3": "Not sure what happened but I tried it again this morning without changing anything on my system and now it mapped my Team Drive.. still a mystery :/. ",
    "chiayewken": "After !kill -9 -1 to reset the notebook, issue no longer presents itself.. ",
    "hotarunso31": "You can select \u201ccommon view properties for all folders\u201d in the settings. This make dolphin lost the ability of per-directory configuration but no more .directory file creation/removal. ",
    "gph82": "Any other solution along the lines of what @battaglia01 proposes, like \"Is there some way to leave these files away from ocamlfuse? Or to make them local, cache-only or something?\". ",
    "JoelParke": "I too have this issue, but not this Colab which I am not using.  I am doing a normal OAuth with passport.  What is even stranger is that when I connect to my development machine at 'localhost' all works as expected.  Yet when I attempt to connect on my server at xxxx.com, I get 401.  Even stranger, by github, gitlab, and facebook connections through passport, all work properly.  Only google has an issue.  It almost feels like one of my package.json changes is the problem... \nWe will see.... But I hate this sort of regression.  When I figure it out, I will report back.\nStill working this issue.  What it even stranger is that this works fine on my development machine using localhost, but fails now on my production server at https://ddd.sss.com.  Go figure.   Note that this had been working for ~4 years.. ",
    "bwbasheer": "No, but I will do that.  That rings a bell from earlier reading.  Need to take better lab notes.  Thanks much.\nFrom: Alessandro Strada notifications@github.com\nSent: Tuesday, September 4, 2018 3:40 PM\nTo: astrada/google-drive-ocamlfuse google-drive-ocamlfuse@noreply.github.com\nCc: B.W. Basheer bwbasheer@retrievalsystems.com; Author author@noreply.github.com\nSubject: Re: [astrada/google-drive-ocamlfuse] Mount once; use by many (#485)\nHave you tried mounti with -o allow_other?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/485#issuecomment-418492389, or mute the threadhttps://github.com/notifications/unsubscribe-auth/ABe26HsH3EVbqA-bDrBHUOGxy1LubF3Fks5uXtbygaJpZM4WR545.\nDisclaimer\nThe information contained in this communication from the sender is confidential. It is intended solely for use by the recipient and others authorized to receive it. If you are not the recipient, you are hereby notified that any disclosure, copying, distribution or taking action in relation of the contents of this information is strictly prohibited and may be unlawful.\nThis email has been scanned for viruses and malware, and may have been automatically archived by Mimecast Ltd, an innovator in Software as a Service (SaaS) for business. Providing a safer and more useful place for your human generated data. Specializing in; Security, archiving and compliance. To find out more visit the Mimecast website.\n. Me too.  Thanks much.\nFrom: Gerald Leung notifications@github.com\nSent: Tuesday, September 11, 2018 3:40 PM\nTo: astrada/google-drive-ocamlfuse google-drive-ocamlfuse@noreply.github.com\nCc: B.W. Basheer bwbasheer@retrievalsystems.com; Author author@noreply.github.com\nSubject: Re: [astrada/google-drive-ocamlfuse] Mount once; use by many (#485)\nHave you tried mounting with -o allow_other?\nWorks for me\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/astrada/google-drive-ocamlfuse/issues/485#issuecomment-420398049, or mute the threadhttps://github.com/notifications/unsubscribe-auth/ABe26LBVzKBTSyQPPp090_lvTjUQLZbcks5uaBGCgaJpZM4WR545.\nDisclaimer\nThe information contained in this communication from the sender is confidential. It is intended solely for use by the recipient and others authorized to receive it. If you are not the recipient, you are hereby notified that any disclosure, copying, distribution or taking action in relation of the contents of this information is strictly prohibited and may be unlawful.\nThis email has been scanned for viruses and malware, and may have been automatically archived by Mimecast Ltd, an innovator in Software as a Service (SaaS) for business. Providing a safer and more useful place for your human generated data. Specializing in; Security, archiving and compliance. To find out more visit the Mimecast website.\n. ",
    "nonfatalexec": "\nHave you tried mounting with -o allow_other?\n\nWorks for me. ",
    "mariansam": "Good to know.. but, it's not that necessary :smile: . ",
    "theFuzzyWebDev": "Although I thought I did aready, the Drive API wasn't enabled. Looks to be working now!. ",
    "benguyer": "I think it would be helpful to add this information to the Wiki.. ",
    "cramjaco": "I see. Thanks.. ",
    "JEF1056": "I also discovered that the last line,\napt-get -y install -qq google-drive-ocamlfuse fuse\nresults in\nE: Unable to locate package google-drive-ocamlfuse\neven though the repository has been added. Maybe this could be the cause of the issue?\n\nany idea when this issue can be fixed? My entire workflow is depending on it ^-^. @DonPex Changing my code to this:\n```\nInstall a Drive FUSE wrapper.\nhttps://github.com/astrada/google-drive-ocamlfuse\n!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n!apt-get update -qq 2>&1 > /dev/null\n!wget https://launchpad.net/~alessandro-strada/+archive/ubuntu/ppa/+files/google-drive-ocamlfuse_0.7.0-0ubuntu1~ubuntu18.04.1_amd64.deb\n!apt-get install libfuse2\n!dpkg --install google-drive-ocamlfuse_0.7.0-0ubuntu1~ubuntu18.04.1_amd64.deb\n!apt-get -y install -qq google-drive-ocamlfuse fuse\n```\nbased off astrata's answer seemed to fix the issue.. ",
    "darylfung96": "I am also having this issues today. I am getting the same issue today. ",
    "andrewkhimichuk": "Distributor ID: Ubuntu\nDescription:    Ubuntu 17.10\nRelease:    17.10\nCodename:   artful. ",
    "DonPex": "@astrada: I tried to install the package with sudo dpkg --install but it says that another package is needed:\nPreparing to unpack google-drive-ocamlfuse_0.7.0-0ubuntu1_ubuntu18.04.1_amd64.deb ...\nUnpacking google-drive-ocamlfuse (0.7.0-0ubuntu1 ~ ubuntu18.04.1) over (0.7.0-0ubuntu1 ~ ubuntu18.04.1) ...\ndpkg: dependency problems prevent configuration of google-drive-ocamlfuse:\n google-drive-ocamlfuse depends on libfuse2 (>= 2.8); however:\n  Package libfuse2 is not installed.\ndpkg: error processing package google-drive-ocamlfuse (--install):\n dependency problems - leaving unconfigured\nErrors were encountered while processing:\n google-drive-ocamlfuse. ",
    "Tvde1": "Same for me today. With code that has been running for a month every day.. I have a python notebook that runs this every day:\n```\nInstall a Drive FUSE wrapper.\nhttps://github.com/astrada/google-drive-ocamlfuse\n!apt-get install -y -qq software-properties-common python-software-properties module-init-tools > /dev/null\n!add-apt-repository -y ppa:alessandro-strada/ppa > /dev/null\n!apt-get update -qq > /dev/null\n!apt-get -y install -qq google-drive-ocamlfuse fuse > /dev/null\n```\nAnd it strarted failing today. Do I need to check now?. I'm using colab notebooks so I don't know. How can I check?. That is not a solution to the problem. ",
    "pjsudharshan": "I am facing the same issue! What's happening???? I am trying to install this from yesterday. Why is it failing since yesterday which kept working for months???. ",
    "KKeishiro": "I've got the same issue on Google colab notebook.. ",
    "yisonglee": "Warning: apt-key output should not be parsed (stdout is not a terminal)\nE: Unable to locate package google-drive-ocamlfuse\nI just encountered the same problem when i run keras in colaboratory.. ",
    "mahmoudSalim": "The solution is this using colab library\nThanks to Craig Citro\nhttps://github.com/googlecolab/colabtools/issues/276\nSolution : \nfrom google.colab import drive\ndrive.mount('/content/drive'). ",
    "jayjtao": "@astrada  need help - steps/details to do this on colab?. ",
    "linxid": "Solution:\nfrom google.colab import drive\ndrive.mount('/content/drive')\nFor example,If you want to find your file:\nimport os\nos.chdir(\"drive/My Drive/Colab/AIC\"). > > Solution:\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\nFor example,If you want to find your file:\nimport os\nos.chdir(\"drive/My Drive/Colab/AIC\")\n\nThis worked for 2 days for me ,now that is also not working :|\n\nThis still worked for me,I am using Colab training NN model.. ",
    "rushic24": "\nSolution:\nfrom google.colab import drive\ndrive.mount('/content/drive')\nFor example,If you want to find your file:\nimport os\nos.chdir(\"drive/My Drive/Colab/AIC\")\n\nThis worked for 2 days for me ,now that is also not working :|. ",
    "RecepCil": "\nThe solution is this using colab library\nThanks to Craig Citro\ngooglecolab/colabtools#276\nSolution :\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nI've got the same issue. I am new to Google Colab so this is a bit confusing for me. I tried your solution but it doesn't work for this problem. Have I done something wrong?\nfrom google.colab import drive\ndrive.mount('/content/drive')\n!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n!apt-get update -qq 2>&1 > /dev/null\n!apt-get -y install -qq google-drive-ocamlfuse fuse\nfrom google.colab import auth\nauth.authenticate_user()\nfrom oauth2client.client import GoogleCredentials\ncreds = GoogleCredentials.get_application_default()\nimport getpass\n!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\nvcode = getpass.getpass()\n!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n. ",
    "SoleSensei": "You should choose just one mounting method.\n1. Using build-in Fuse\npy\n!echo 'Mounting...'\nfrom google.colab import drive\ndrive.mount('/content/drive/')\n!ls '/content/drive/My Drive'\n!echo 'Mounted'\n2. Using google-drive-ocamlfuse Fuse Driver\npy\n!echo 'Installing required software'\n!apt-get install -y -qq software-properties-common module-init-tools 2>&1 > /dev/null\n!echo 'Add apt-repository with Google.Drive Fuse'\n!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n!echo 'Updating packages...'\n!apt-get update -y -qq\n!echo 'Installing google-drive-ocamlfuse fuse...'\n!apt-get install -y -qq google-drive-ocamlfuse fuse\n!echo 'Authenticate Fuse in Google.Drive...'\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\nimport getpass\nauth.authenticate_user()\ncreds = GoogleCredentials.get_application_default()\n!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\nvcode = getpass.getpass('Enter auth code here: ')\n!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n!echo 'Authenticated!'\n!echo 'Creating mount directory'\n!mkdir ./drive2\n!echo 'Mounting...'\n!google-drive-ocamlfuse ./drive2\n!ls ./drive2\n!echo 'Mounted!'. ",
    "sagar-m": "Hi, thank you for your note. This is a bit confusing, since I am new to it. Should I be making a change to the below code?\n!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n!apt-get update -qq 2>&1 > /dev/null\n!apt-get -y install -qq google-drive-ocamlfuse fuse\nfrom google.colab import auth\nauth.authenticate_user()\nfrom oauth2client.client import GoogleCredentials\ncreds = GoogleCredentials.get_application_default()\nimport getpass\n!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\nvcode = getpass.getpass()\n!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\nAs per this link, this is what we are supposed to do for running or Importing .py Files with Google Colab:\nRun these codes first in order to install the necessary libraries and perform authorization.\n!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n!apt-get update -qq 2>&1 > /dev/null\n!apt-get -y install -qq google-drive-ocamlfuse fuse\nfrom google.colab import auth\nauth.authenticate_user()\nfrom oauth2client.client import GoogleCredentials\ncreds = GoogleCredentials.get_application_default()\nimport getpass\n!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\nvcode = getpass.getpass()\n!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\nThank you very much.. @daniel-leonard-robinson yes you are right, they are identical. I was merely making the point, that I am following a tutorial, and this is what the step was. Please note the line above the duplicate code, it is \"Run these codes first in order to install the necessary libraries and perform authorization.\" . Please can @daniel-leonard-robinson  or @astrada share the complete steps, how I can continue in Google Colab, as before, without breaking. Thank you very much.. @daniel-leonard-robinson that worked beautifully. Relieved! I noticed that you commented out three lines and added three lines. Do you mind, explaining what these new lines are doing? On the whole, I don't know what these codes are doing. Thank you very much. Appreciate it.. @daniel-leonard-robinson thanks for the explanation of your updated lines. And what are the rest of the lines doing? Thank you again. I mean what is the purpose of these lines of code. . ",
    "daniel-leonard-robinson": "@sagar-m those two code snippets are identical. Anyway, @astrada, would you mind being more specific about the dpkg --install method?. Ah, one can download the package from https://launchpad.net/~alessandro-strada/+archive/ubuntu/google-drive-ocamlfuse-beta/+build/15331130 and install it from there. Hope this works for you, too. Otherwise just change your machine type (amd, arm, i386 etc..)\n```bash\n!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n!apt-get update -qq 2>&1 > /dev/null\n!apt-get -y install -qq google-drive-ocamlfuse fuse\n!wget https://launchpad.net/~alessandro-strada/+archive/ubuntu/google-drive-ocamlfuse-beta/+build/15331130/+files/google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n!dpkg -i google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n!apt-get install -f\n!apt-get -y install -qq fuse\nfrom google.colab import auth\nauth.authenticate_user()\nfrom oauth2client.client import GoogleCredentials\ncreds = GoogleCredentials.get_application_default()\nimport getpass\n!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\nvcode = getpass.getpass()\n!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n```. I made an answer here too: https://stackoverflow.com/a/52387195/3477932\nwget will download the .deb file, dpkg -i will attempt to install it, and apt-get install -f will re-attempt to install it but fix the dependencies. Lastly, fuse still needs to be installed.. ",
    "kabili207": "After a bit of use, I realized this was really only happening in KeePassXC. Digging around the source code, it's explicitly deleting and re-creating the file. I haven't been able to recreate my initial test with mv, so I feel safe saying this is an issue with KeePassXC.. ",
    "Tschebbischeff": "I'm afraid I have to get back to this. While it is true, that I am also just happened to have this problem on KeePassXC so far, I have done tests with manually moving files.\nThe file will also be sent to the trash bin, when using mv from terminal to move files on the same filesystem itself (mounted drive to another folder on the same mounted drive) and from the host filesystem to the mounted drive.\nSo I am quite convinced this might not be an issue with only KeePassXC.\nThe correct behavior should be to \"update\" the file like it does when copying the file, which will result in a new \"version\" of the file.\nSo I would propose the same @kabili207 did before. To add an option that overwrites the existing file when doing an mv without trashing it from google drives perspective.\nEdit: While this won't probably fix the problem with KeePassXC, since it recreates the file explicitly this is still somewhat odd behavior. When I move files by drag and drop in Nautilus the \"recreation\" also happens.. ",
    "klemy": "Thanks for responding me.\nI have removed the allow_others option from mntoptions allow in /etc/security/pam_mount.conf.xml and added the allow_root in ~/.pam_mount.conf.xml.\nI now have this error :\ngdm-session-worker[22792]: (mount.c:72): umount messages:\ngdm-session-worker[22792]: (mount.c:76): fusermount: failed to unmount /home/[USER]/Cloud/Google/[ABC]: Invalid argument\ngdm-session-worker[22792]: (mount.c:892): unmount of gdfuse#[ABC] failed. ",
    "LoYungSum": "Also, I don't know why I cannot config the automount correctly.\n/usr/bin/gdfuse\n`#!/bin/bash\nsu user -l -c \"google-drive-ocamlfuse /home/user/GoogleDrive -label user\"\nexit 0`\n/etc/fstab\ngdfuse. ",
    "m-rufflesmcghie": "Are \"root\" & \"user\" in the samba group ?? I'm working on auto mount myself at the moment 1st attempt failed :-(. ",
    "maravento": "@LoYungSum Read this: Share Google Drive with Samba (using google-drive-ocamlfuse)\nThanks to this project. ",
    "avatar-lavventura": "I am only sharing a folder or a zipped file and I don't care about any Google Docs file. \nI have experienced 3-5 minutes delay for all the shared folder or zipped file even I don't share any Google Docs file, is it normal?\nAs I understand no matter what, Google Docs always re-cached whenever a shared operation is done, which leads this few minutes delay.\nI have set download_docs=false in the config. I will let you if I still observe 3-5 minutes delay.. ",
    "sagaciouskjb": "Okay, thanks for speedy reply!. ",
    "kirk86": "@astrada thanks for the prompt response, appreciate the info.\nClosing, to keep things clear.. ",
    "didipluto": "I try installing compiler :\nopam switch install ocaml-base-compiler\nThe sqlite3 can be installed but it gets the error in :\nError: /usr/lib/ocaml/sqlite3/sqlite3.cmi\nis not a compiled interface for this version of OCaml.\nIt seems to be for a newer version of OCaml. . ",
    "cysiekw": "Same here, distro arch error when build sqlite3-ocaml-4.4.0 if i do opam switch error in sqlite3.cmi . Never mind after few hours of fighting i have installed by opam on the go!  All the errors i have was based on version of ocaml-base-compiler i tried  4.04.2, 4.05.0 and 4.06.1 problematic packages ocaml-cryptokit, ocaml-zarith, ocaml-sqlite3 and yes i was using AUR ! \n. ",
    "os369510": "I was same problem here and I upgraded the ocaml-sqlite3 to version 4.4.1-1.\nIt works for me.\nReference:\nhttps://aur.archlinux.org/packages/ocaml-sqlite3/. ",
    "canardos": "That's great to hear. I'll update to 0.7.0 and report back after I've had some time to test.. Just to update, I've been using 0.7.0 for a few days now, and while I haven't yet encountered the corruption issue, the \"This document has been externally modified. Do you want to continue saving?\" notification is still an issue.\nI haven't seen it in LibreOffice yet, but am seeing it in text files using Mousepad 0.4.1 (XFCE 4.12). Will continue testing.. I think you can close this issue. I've been using 0.7.0 for a couple of weeks on a number of machines and haven't encountered the corruption issue.\nThe \"This document has been externally modified...\" message I noted in my previous comment was only seen once, so I'm thinking it's possible the file truly wasn't closed properly on another machine.\nI've seen some file cache/syncing issues with the new version, but I'll open a new issue if that persists.. I am not sure if it is the same issue, but am experiencing the following 0-length issue, resulting in data loss:\n\nMount drive\nCreate a new file in LibreOffice Calc, add content, save to mounted location.\nAt this point, the local file system shows the file as having positive size, but Google Drive (via web/phone interface) shows file present, but with 0-size.\nClose file, exit LibreOffice\nUnmount drive\nThe file on Google Drive is still 0-length\n\n~~If the drive is remounted immediately, then the local file system will show the positive size file (presumably writes are not being flushed and the local cache is still valid?). But if I wait a day, then after remounting I see the 0-length file only (local cache expired?) and the original data is lost.~~ Turns out that was just the file manager metadata cache\nThis is 0.7.0 on 4.17.19-1-MANJARO using LibreOffice 6.0.7.3.0+.. ",
    "BowDownGitHub": "After removing the 2 configurations, and reauthorizing them from scratch with google API it works again. Weird! . ",
    "rqi14": "\nDo you experience the same problem if you upload a new file using Google Drive's web interface?\n\nHi. I think they are the same. I uploaded using Mountain Duck and I can see it via both Mountain Duck and Expandrive but not with google-drive-ocamlfuse. \nI cannot test anymore because it seems deleting the default folder and authorizing again solved the problem. Thanks anyway.. > Do you experience the same problem if you upload a new file using Google Drive's web interface?\nHi. It appeared again so I did more testing. It turned out that this happens only when I upload files using Mountain Duck but it is normal when I upload with web interface or expandrive. However, it is weird that the file size appears to be correct in both Expandrive and google drive web interface but not in google-drive-ocamlfuse. Do you have any idea why?. ",
    "dnapier": "I'm experiencing the same as canardos.  \nI'm not sure if this is related, but I use 3 gdrives, 1 default and 2 using the -label flag.  Of the two using the label flag, the third is a team drive.  I use a script to run through all 3 after network connection is established.  The two drives using the -label flag mount, but do not show as drives on the desktop nor in the 'Places' panel menu.\nMy mount script is this:\ngoogle-drive-ocamlfuse ~/gdrive\ngoogle-drive-ocamlfuse -label work_drive ~/gdrive_work\ngoogle-drive-ocamlfuse -label work_team_drive ~/gdrive_team_work\nInteresting note: when I run my unmount script that consists of: \nfusermount -u ~/gdrive\nfusermount -u ~/gdrive_work\nfusermount -u ~/gdrive_team_work\nthe first drive icon disappears, and then the gdrive_work icon shows up.\nI'll do the same as requested of canardos, and email you the -debug/gdfuse.log/curl.log.\nUpdate: When using the debug flag, the 2nd/3rd drives do not mount due to the first command not releasing the thread, so I am unable to retrieve the logs for those drives.. ",
    "petrohiv": "I see. Thanks a lot for your answer! But can you reveal caching strategy then? Does ocamlfuse remove from cache storage any files that are not in downloading or uploading at the moment?. ",
    "ilovetosharing": "wow thank you. i will use it. thank you very much.\n. ",
    "mschaaf": "I don't use any proxy/profiler that intercepts SSL connections. I also don't see how the linked post applies here. Please clarify what I should look for exactly.\nIf there is an issue with my certificate storage why does it work on a fresh start of google-drive-ocamlfuse?\nHow can I debug the difference in the certificate handling between a fresh start of google-drive-ocamlfuse and the handling on the metadata refresh?\nWho is throwing the error \"CURLE_SSL_CACERT_BADFILE\" google-drive-ocamlfuse or a 3rd party library?. So I started with debug and get the same message like you, but when it fails the message is:\n[8282.566132] curl: info: Connection #0 to host www.googleapis.com left intact\n[8282.619868] curl: info:   Trying 216.58.207.42...\n[8282.619967] curl: info: TCP_NODELAY set\n[8282.639570] curl: info: Connected to www.googleapis.com (216.58.207.42) port 443 (#0)\n[8282.639797] curl: info: error reading ca cert file /etc/ssl/certs/ca-certificates.crt (Error while reading file.)\n[8282.639911] curl: info: Closing connection 0\n[9124.139679] curl: info: Could not resolve host: www.googleapis.com\n[9124.139759] curl: info: Closing connection 0\n[9124.144580] curl: info: Could not resolve host: www.googleapis.com\n[9124.144637] curl: info: Closing connection 0\nThe file /etc/ssl/certs/ca-certificates.crt is readable as the user that starts the service.\nThe gdfuse.log has the following in it at the \ntime of failure\n```\n[8282.950376] TID=93597: Error during request: Code: 77, Description: CURLE_SSL_CACERT_BADFILE, ErrorBuffer: 0~T\u00e9<\u007f\n[8282.950447] TID=93597: Giving up\n[8282.950508] TID=93597: Exception:Failure(\"Code: 77, Description: CURLE_SSL_CACERT_BADFILE, ErrorBuffer: 0~T\\233<\\127\\n\")\nBacktrace:\n[8284.269589] TID=1: Flushing DB...[8284.270796] TID=1: Retrying (1/8) final_step after exception: Failure(\"Sqlite3 error: CANTOPEN\")\n[8285.839713] TID=1: Retrying (2/8) final_step after exception: Failure(\"Sqlite3 error: CANTOPEN\")\n[8287.890251] TID=1: Retrying (3/8) final_step after exception: Failure(\"Sqlite3 error: CANTOPEN\")\n[8292.202643] TID=1: Retrying (4/8) final_step after exception: Failure(\"Sqlite3 error: CANTOPEN\")\n[8300.990211] TID=1: Retrying (5/8) final_step after exception: Failure(\"Sqlite3 error: CANTOPEN\")\n[8317.010240] TID=1: Retrying (6/8) final_step after exception: Failure(\"Sqlite3 error: CANTOPEN\")\n[8349.342196] TID=1: Retrying (7/8) final_step after exception: Failure(\"Sqlite3 error: CANTOPEN\")\n[8413.530260] TID=1: Retrying (8/8) final_step after exception: Failure(\"Sqlite3 error: CANTOPEN\")\n[8541.955867] TID=1: Error during final_step after 8 attempts: Failure(\"Sqlite3 error: CANTOPEN\")\n[8541.956023] TID=1: Retrying (1/8) close_db after exception: Failure(\"close_db\")\n[8543.785186] TID=1: Retrying (2/8) close_db after exception: Failure(\"close_db\")\n[8546.348769] TID=1: Retrying (3/8) close_db after exception: Failure(\"close_db\")\n[8550.598195] TID=1: Retrying (4/8) close_db after exception: Failure(\"close_db\")\n[8559.466345] TID=1: Retrying (5/8) close_db after exception: Failure(\"close_db\")\n[8576.028543] TID=1: Retrying (6/8) close_db after exception: Failure(\"close_db\")\n[8608.366172] TID=1: Retrying (7/8) close_db after exception: Failure(\"close_db\")\n[8673.238186] TID=1: Retrying (8/8) close_db after exception: Failure(\"close_db\")\n[8801.758164] TID=1: Error during close_db after 8 attempts: Failure(\"close_db\")\n[8801.758282] TID=1: Error in finally block:\nException:Failure(\"close_db\")\nBacktrace:\n[9124.445248] TID=93598: getattr /\n[9124.445341] TID=93598: BEGIN: Getting metadata from context\n[9124.445378] TID=93598: END: Getting metadata: Not valid\n[9124.445410] TID=93598: BEGIN: Refreshing metadata\n[9124.446324] TID=93599: getattr /\n[9124.450095] TID=93598: Error during request: Code: 6, Description: CURLE_COULDNT_RESOLVE_HOST, ErrorBuffer: Could not resolve host: www.googleapis.com\n[9124.450140] TID=93598: Offline\n[9124.450185] TID=93598: Exception:Failure(\"Code: 6, Description: CURLE_COULDNT_RESOLVE_HOST, ErrorBuffer: Could not resolve host: www.googleapis.com\\n\")\nBacktrace:\n[9124.450213] TID=93599: BEGIN: Getting metadata from context\n[9124.450373] TID=93599: END: Getting metadata: Not valid\n[9124.450420] TID=93599: BEGIN: Refreshing metadata\n[9124.454934] TID=93599: Error during request: Code: 6, Description: CURLE_COULDNT_RESOLVE_HOST, ErrorBuffer: Could not resolve host: www.googleapis.com\n[9124.454970] TID=93599: Offline\n[9124.455008] TID=93599: Exception:Failure(\"Code: 6, Description: CURLE_COULDNT_RESOLVE_HOST, ErrorBuffer: Could not resolve host: www.googleapis.com\\n\")\nBacktrace:\n[9421.582030] TID=93600: getattr /\n[9421.582092] TID=93601: getattr /\n[9421.582151] TID=93600: BEGIN: Getting metadata from context\n[9421.582250] TID=93600: END: Getting metadata: Not valid\n[9421.582305] TID=93600: BEGIN: Refreshing metadata\n[9421.586815] TID=93600: Error during request: Code: 6, Description: CURLE_COULDNT_RESOLVE_HOST, ErrorBuffer: Could not resolve host: www.googleapis.com\n[9421.586840] TID=93600: Offline\n[9421.586865] TID=93600: Exception:Failure(\"Code: 6, Description: CURLE_COULDNT_RESOLVE_HOST, ErrorBuffer: Could not resolve host: www.googleapis.com\\n\")\nBacktrace:\n[9421.586874] TID=93601: BEGIN: Getting metadata from context\n[9421.586913] TID=93601: END: Getting metadata: Not valid\n[9421.586931] TID=93601: BEGIN: Refreshing metadata\n[9421.591343] TID=93601: Error during request: Code: 6, Description: CURLE_COULDNT_RESOLVE_HOST, ErrorBuffer: Could not resolve host: www.googleapis.com\n[9421.591364] TID=93601: Offline\n[9421.591387] TID=93601: Exception:Failure(\"Code: 6, Description: CURLE_COULDNT_RESOLVE_HOST, ErrorBuffer: Could not resolve host: www.googleapis.com\\n\")\nBacktrace:\n[9424.144247] TID=93602: getattr /\n[9424.144413] TID=93602: BEGIN: Getting metadata from context\n[9424.144474] TID=93602: END: Getting metadata: Not valid\n[9424.144521] TID=93602: BEGIN: Refreshing metadata\n[9424.145537] TID=93603: getattr /\n[9424.149517] TID=93602: Error during request: Code: 6, Description: CURLE_COULDNT_RESOLVE_HOST, ErrorBuffer: Could not resolve host: www.googleapis.com\n[9424.149608] TID=93602: Offline\n[9424.149700] TID=93602: Exception:Failure(\"Code: 6, Description: CURLE_COULDNT_RESOLVE_HOST, ErrorBuffer: Could not resolve host: www.googleapis.com\\n\")\nBacktrace:\n[9424.151165] TID=93603: BEGIN: Getting metadata from context\n[9424.151314] TID=93603: END: Getting metadata: Not valid\n[9424.151380] TID=93603: BEGIN: Refreshing metadata\n[9424.156317] TID=93603: Error during request: Code: 6, Description: CURLE_COULDNT_RESOLVE_HOST, ErrorBuffer: Could not resolve host: www.googleapis.com\n[9424.156393] TID=93603: Offline\n[9424.156478] TID=93603: Exception:Failure(\"Code: 6, Description: CURLE_COULDNT_RESOLVE_HOST, ErrorBuffer: Could not resolve host: www.googleapis.com\\n\")\nBacktrace:\n```\nIt looks to me like google-drive-ocamlfuse is not able to recover from a connection lose.. ",
    "meszape": "Thanks for the fix!. I can confirm, that this is fixed!\nThx. ",
    "Calendal": "\nShould be fixed in the beta ppa. If you are using opam, you can try the fix before it gets to the main repo, running:\nopam pin -n add gapi-ocaml https://github.com/astrada/gapi-ocaml#master\nopam update\nopam upgrade\nTo restore the main repo version:\nopam pin -n remove gapi-ocaml\nopam update\nopam upgrade\nThanks for reporting the issue!\n\nThis is a fix or a workarround?. > > This is a fix or a workarround?\n\nIt should fix the issue. The opam pin commands are to test the fix before it gets approved and lands in the official opam-repository.\n\nOK! Thanks for the effort.. ",
    "Medow": "I can confirm that using this beta is solving the issue.\nThanks.. ",
    "sunandosamaddar": "Hi @astrada, hearty new year wishes! Here's my observation. I did port install osxfuse gmp pkgconfig sqlite3 and followed steps in INSTALL after unpacking lzlib-1.10.tar.gz (from-homepage). Found that opam install google-drive-ocamlfuse did not pick up the default sqlite3 system installation so I used port install. However, now there's still a build failure just for ocamlfuse 2.7.1-cvs5 with the same error shown in above snippet. I matched dependencies from the build logs but no change.\n```\nThe following actions will be performed:\n  \u2217 install ocamlfuse              2.7.1-cvs5 [required by google-drive-ocamlfuse]\n  \u2217 install google-drive-ocamlfuse 0.7.1\n===== \u2217 2 =====\nDo you want to continue? [Y/n] y\n<><> Gathering sources ><><><><><><><><><><><><><><><><><><><><><><><><><><>  \ud83d\udc2b \n[google-drive-ocamlfuse.0.7.1] found in cache\n[ocamlfuse.2.7.1-cvs5] found in cache\n<><> Processing actions <><><><><><><><><><><><><><><><><><><><><><><><><><>  \ud83d\udc2b \n[ERROR] The compilation of ocamlfuse failed at \"/Users/sunandosamaddar/.opam/opam-init/hooks/sandbox.sh build ocaml setup.ml -build\".\n=== ERROR while compiling ocamlfuse.2.7.1-cvs5 ===============================\ncontext     2.0.2 | macos/x86_64 | ocaml-base-compiler.4.07.1 | https://opam.ocaml.org#50da06f4\npath        ~/.opam/SWITCH/.opam-switch/build/ocamlfuse.2.7.1-cvs5\ncommand     ~/.opam/opam-init/hooks/sandbox.sh build ocaml setup.ml -build\nexit-code   1\nenv-file    ~/.opam/log/ocamlfuse-13415-d2d111.env\noutput-file ~/.opam/log/ocamlfuse-13415-d2d111.out\noutput\n[...]\nmaking ._d/Fuse_result.d from Fuse_result.ml\nocamlc -c -cc \"cc\" -ccopt \"-fPIC -D_FILE_OFFSET_BITS=64 -I. -pthread \\\n-DPIC -DNATIVE_CODE -I/usr/local/include/osxfuse  \\\n-I/Users/sunandosamaddar/.opam/SWITCH/lib/camlidl  -o Fuse_bindings_stubs.o \" Fuse_bindings_stubs.c\nIn file included from Fuse_bindings_stubs.c:17:\n./Fuse_bindings.h:17:10: fatal error: 'fuse.h' file not found\n#include \n^~~~~~~~\n1 error generated.\nmake[1]: *** [Fuse_bindings_stubs.o] Error 2\nmake: *** [native-code-library] Error 2\nE: Failure(\"Command 'make -C lib INCDIRS=/Users/sunandosamaddar/.opam/SWITCH/lib/camlidl' terminated with error code 2\")\n<><> Error report <><><><><><><><><><><><><><><><><><><><><><><><><><><><><>  \ud83d\udc2b \n\u250c\u2500 The following actions failed\n\u2502 \u03bb build ocamlfuse 2.7.1-cvs5\n\u2514\u2500 \n\u2576\u2500 No changes have been performed\n``. You're a genius! I can now see-version`:-D.. ",
    "kwcooper": "I had a similar problem on Ubuntu 18.04. Copying large files to the mounted drive folder causes the filesystem to freeze, typically towards the last couple mb of the copy (which is slightly frustrating). \nThat said, this is a wonderful piece of software (Thank you!) . ",
    "gabefair": "I'm noticing the same issue too. Were you able to figure it out?. Congratulations on getting it working.\nWhen you are ready, don't forget to also close your ticket.. ",
    "jellytea": "Can I set it up with \"Settings\" of Ubuntu18.04?. Oh, no!\n[0.016309] TID=0: Setting up default filesystem...\n[0.016425] TID=0: BEGIN: Saving configuration in .gdfuse/default/config\n[0.019553] TID=0: END: Saving configuration in .gdfuse/default/config\n[0.060069] TID=0: Loading application state from /.gdfuse/default/state...done\nCurrent version: 0.7.1\nSetting up cache db...done\nSetting up CURL...done\n[0.063518] TID=0: Starting flush DB thread (TID=1, interval=30s)\n[0.063647] TID=0: BEGIN: Saving application state in .gdfuse/default/state\n[0.063972] TID=0: END: Saving application state in .gdfuse/default/state\n[0.064020] TID=0: BEGIN: Starting web browser with command: xdg-open \"https://accounts.google.com/o/oauth2/auth?client_id=xxx\"\n[0.275609] TID=0: END: Starting web browser with command: xdg-open \"https://accounts.google.com/o/oauth2/auth?xxx\"\n[0.275660] TID=0: BEGIN: Getting tokens from GAE proxy\n[0.278111] TID=0: Retrying (1/8) get_tokens after exception: Failure(\"Code: 7, Description: CURLE_COULDNT_CONNECT, ErrorBuffer: Failed to receive SOCKS4 connect request ack.\\n\")\n[1.523871] TID=0: Retrying (2/8) get_tokens after exception: Failure(\"Code: 7, Description: CURLE_COULDNT_CONNECT, ErrorBuffer: Failed to receive SOCKS4 connect request ack.\\n\")\n[3.679546] TID=0: Retrying (3/8) get_tokens after exception: Failure(\"Code: 7, Description: CURLE_COULDNT_CONNECT, ErrorBuffer: Failed to receive SOCKS4 connect request ack.\\n\")\n[8.392285] TID=0: Retrying (4/8) get_tokens after exception: Failure(\"Code: 7, Description: CURLE_COULDNT_CONNECT, ErrorBuffer: Failed to receive SOCKS4 connect request ack.\\n\")\n[16.659821] TID=0: Retrying (5/8) get_tokens after exception: Failure(\"Code: 7, Description: CURLE_COULDNT_CONNECT, ErrorBuffer: Failed to receive SOCKS4 connect request ack.\\n\")\n[33.193978] TID=0: Retrying (6/8) get_tokens after exception: Failure(\"Code: 7, Description: CURLE_COULDNT_CONNECT, ErrorBuffer: Failed to receive SOCKS4 connect request ack.\\n\")\n[65.378930] TID=0: Retrying (7/8) get_tokens after exception: Failure(\"Code: 7, Description: CURLE_COULDNT_CONNECT, ErrorBuffer: Failed to receive SOCKS4 connect request ack.\\n\"). No timeout, still not accessible. My proxy is socks5 instead of socks4. Do I need to change it in curl?. I must use Shadowsocks to access Google, I am using it.. proxychains env ALL_PROXY=socks5h://127.0.0.1:1080 google-drive-ocamlfuse -headless -label googledrive -id xxx -secret xxx\nProxyChains-3.1 (http://proxychains.sf.net)\nPlease, open the following URL in a web browser: xxx\nPlease enter the verification code: xxx\n|S-chain|-<>-127.0.0.1:1080-<><>-127.0.0.1:1080-<><>-OK\n|DNS-request| accounts.google.com \n|S-chain|-<>-127.0.0.1:1080-<><>-4.2.2.2:53-<><>-OK\n|S-chain|-<>-127.0.0.1:1080-<><>-4.2.2.2:53-<><>-OK\n|DNS-response|: accounts.google.com does not exist\nCannot retrieve auth tokens.\nFailure(\"Code: 6, Description: CURLE_COULDNT_RESOLVE_HOST, ErrorBuffer: Could not resolve host: accounts.google.com\\n\")\nThe command to remove the \"proxychains\" does not work.\nDeleting the \"env\" command does not work, too.. ",
    "fgilbert68": "Thanks for your reply, let's cross fingers.. ",
    "AndreRisnes": "That was fast! 0.7.3 seems to work fine on my machine now, thank you.. ",
    "ithinkmatt": "I just changed the script to mount to /home/m/google-drive (instead of /google-drive-mount). Same issue the directory just disappears. But, the directory reappears after boot (with the start up script disabled).\nThanks,\nMatt. I figured the issue out. For some reason, only on my laptop, I had to add the mount to FSTab. Then, set .sh to run at startup. Now * google-drive-ocamlfuse /home/m/google-drive-mount successfully mounts every time the machine boots.\nI still have no idea why the process is different on each machine, but happy to have the issue resolved.. ",
    "Hydramus": "Thanks for the choices, would you have a personal recommendation out of the three? . "
}
{
    "rauchg": "I deem this overkill now. I might revisit my decision\n. It's not published yet as we're gonna make some live tests first with tons of load. But this is pretty similar to the end result. It will be published in parallel with the new socket.io\n. Good find\n. Thanks! Sorry for the oversight. I thought I had tests for pings.\n. This is the second ping-related bug, we need better tests for ping/pong.\n. Yeah the existing transport right now was copied over from socket.io. Needs to be refactored.\nAlso, since it's not engine.io's business per se to handle policies, I moved it to the server attachment phase:\nhttps://github.com/LearnBoost/engine.io/blob/master/lib/engine.io.js#L147\nhttps://github.com/LearnBoost/engine.io/blob/master/lib/server.js#L246\n. Fixed in client\n. The reason I didn't want to go with a policy server is that you would probably run it standalone, outside of engine.io, right?\n. @3rd-Eden We have a policy file in lib/transports/\n. We're not doing this. If someone needs a policy server they should set up their own standalone, where they could also handle non-engine related Flash security policies.\nWe also added a policyPort option to engine.io-client to facilitate this.\n. Ensured with https://github.com/LearnBoost/engine.io/commit/79d76b6aa2466faccee487da42c0d43b136f054a\n. By abstract I mean that accessing a client by session id would imply not accessing a object that's tied to a transport, but accessing to a \"communication layer\" with the client.\n. We're going to make it optional instead.\n. I don't understand how this can be happening.\nIn prepare we always set req.query to something:\nhttps://github.com/LearnBoost/engine.io/blob/master/lib/server.js#L107\nAnd all requests that engine.io handles are prepare'd:\nhttps://github.com/LearnBoost/engine.io/blob/master/lib/server.js#L148\nhttps://github.com/LearnBoost/engine.io/blob/master/lib/server.js#L200\nIn addition, we even have tests for invalid transport names (including native object stuff like constructor haha)\nhttps://github.com/LearnBoost/engine.io/blob/master/test/server.js#L16\n. I'm gonna create a label spooky for mysterious stuff like this :D\n. @Gottox any update on this ?\n. Thanks @Gottox \n. Very interesting. I wonder if trying the upgrade to a different host (eg: upgrade.yourdomain.com) would help avoid this situation\n. Can't reproduce\n. That looks like the old engine client\n. :)\n. Can you point to where this happens?\n. Like\n. @pesho sticky load balancing, for example with Amazon ELB\n. @pesho you are absolutely right. Creating a ticket\n. Done\n. @kapouer good find. It seems to me that some sockets are throwing instead of emitting error events, therefore never getting properly shut down.\nWhat do you think about this assessment @einaros ?\n. Sure but it's ws:\nat WebSocket.send (/xxx/node_modules/engine.io/node_modules/websocket.io/node_modules/ws/lib/WebSocket.js:175:16)\n. /me kills him\nOn 0.1.2 we upgraded the ws transport. @einaros do you have any guesses what could be causing this?\n. @kapouer do the sockets stay open in this case too?\n. Just restart engine.io every 10 connections (reference) (just kidding)\n. Thanks. Closing this for the time being.\n. Would you mind adding a test to prevent a regression?\nAlso, let's add a // short comment of why we're adding the check for posterity\n. Thanks!\n. Do you know why this only happens on 0.8?\n. Also, is this happening on a test suite of yours?\n. An engine.io specific test would be sweeeeet.\n. Fixed with a different approach\n. Thanks a lot for the test\n. Actually we should turn this into a cryptographically stronger id generator, as in socket.io. I'll leave this open as a reminder.\n. I mean the odds are so low that you almost want it to crash just for the awe.\n. I'll add this when I refactor the id generation method.\n. We need to append a timestamp as well. I think we're only doing it for IE? In any case this ticket belongs in engine.io-client\n. @plievone forceBust is being renamed :P\n. Browsers shouldn't cache those GET responses though. If IE and Android are doing that, it's a bug on their part.\n. Please let me know if you still see this behavior with 0.1.1\n. Sweet!\n. LGTM\n. If you really need to retain the original reference don' t use the convenience methods.\n. The main reason is consistency: there can be engine instances that don't have a httpServer property, and that's not a good thing.\n. I'm working on integrating the two, but engine.io needs a few more features to be 100% ready (marked right now with a \"Socket.IO 1.0\" label on the engine.io / engine.io-client repositories.\n. Yes\n. That's actually the right response, and this is a bug. Thanks for reporting.\n. Great, thanks a lot @guille\n. I want this\n. @EugenDueck I wonder if all that we should do is add a callback to send. In the case of websocket, we fire when the ws transport fires the callback. In the case of polling, when the incoming GET poll request consumes the message.\nThen it's up to a higher-level impl to decide on the conflation mechanics.\n. Agreed completely. In fact, in engine.io Socket hols an array of packet objects, and only the Transport deals with encoding logic.\n. Days\n. Yep. In fact, that's why I want the reliable messaging layer directly in socket.io. It'd be really cool if processes could die, come and go, yet messages stay in let's say redis.\n. Per discussion, we're moving it into a different module.\n. We ideally would provide the right interfaces to make this pluggable. I like this though, thanks for bringing it up @EugenDueck \n. This would be cool to have a as a default with a configurable timeout. For you still want disconnections to be fired and buffers be discarded (or given to the developer?) past a certain threshold. \nFor example, if a reconnection is established within 15 seconds, to the user/developer we're like \"a disconnection never happened\".\nAlthough this might be a better fit at the socket.io level, since it would bloat engine.io considerably. \n. @EugenDueck I want something like this for socket 1.0/1.0+. Engine.IO will remain WS-like as far as the message reception guarantees.\n. @EugenDueck per your other message, basically by controlling what gets written at all times with the callback, instead of relying on engine.io's internal buffer\n. It's the jsonp stuff. Whitelist it\n. But this makes me think of an improvement. We should lazily define the global (aka: first time transport gets used)\n. Thanks @Contra \n. https://github.com/LearnBoost/engine.io-client/issues/40\n. LGTM\n. socket closing detection was taken care of already by request close event.\n. LGTM\n. Looks\nGood\nTo\nMe\n. Indeed it needs to be created\n. Getting rid of bench\n. What do you think about passing more context to the conflater in addition to the messages?\n. Also, I'm thinking we could completely break out the conflater logic by emitting an event prior to the flushing taking place, that could potentially just alter the buffer.\njs\nengine.on('flush', function(buffer){\n  // do something to the buffer\n});\n. js\nengine.on('flush', require('engine-conflate')(function(x){ return x; }));\n. That's what the engine-conflate plugin would do though, so that ultimately you get the interface that just deals with the messages.\n. So, essentially we just need to add that line of code that triggers the event on this.server\n. Also it would be great if you could put that module together, include the tests, and I'll reference it from the README\n. Guillermo's hitchhiking guide to engine.io plugin galaxy:\n1. Create a new repo (I suggest engine.io-conflation)\n2. Include engine.io, mocha and should/expect.js as the devDependencies in package.json, along with version, name, etc. Don't include engine.io as dependencies so that it's clear it's version-agnostic. See the package.json in this repo for reference.\n3. Put the tests you created into its own suite under test/\n4. Include a Makefile with a test task.\n5. Make a single file engine.io-conflation.js and reference it from package.json's main. Alternative you could call it index.js but I'm not a fan.\n6. Make sure your module exports a function that returns the function that separates messages from non-messages. That function that's returned should take the function that you were passing as opts.conflation in your example as its main parameter.\n7. Make sure node_modules is in .gitignore\n8. Use git release 0.0.1 followed by npm publish once it's ready. git release is part of @visionmedia's git-extras. To easily maintain a changelog like in engine.io, execute git changelog prior to each git release.\nThen, in the README, instruct the user on how to load it to their engine.io server:\njs\nvar conflation = require('engine.io-conflation')\nengine.on('flush', conflation(function(x){ return x; });\nYou can also include an overview of the goals of the library, and other usage examples.\nYou might also want to mention that the flush event is only available in 0.2.1+.\n. @kapouer it's not really. The module itself does not depend on the existence of the flush event. The flush event is what makes the module convenient to use. You could, for instance, override the whole flush method in engine.io <= 0.2.0 and execute the conflation logic there manually.\n. Yeah I guess you could even do an abstraction like engine.io-conflation > conflation, but it seems overkill\n. Excellent\nOn Sun, Aug 12, 2012 at 1:38 AM, EugenDueck notifications@github.comwrote:\n\n@guille https://github.com/guille The first version is here:\nhttps://github.com/EugenDueck/engine.io-conflation But it needs version\n0.2.0 of engine.io to run, or to run the tests.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/LearnBoost/engine.io/pull/55#issuecomment-7673784.\n\n\nGuillermo Rauch\nLearnBoost CTO\nhttp://devthought.com\n. Done\n. @eugendueck what do you think?\n. Writing an explanation here since Afshin asked of what we need to do to make this happen.\nBasically when a Socket is holding a buffer, we want to tell the transport to write that buffer. Meanwhile, while the transport is writing it (for example, polling might have to wait for an incoming poll GET request), this buffer could grow at the socket level. \nThe socket will fire this callback when the drain event is executed by the transport, which means the message was sent successfully. The extra complication is that while we're waiting on this event two other events can be emitted by the transport that should clean up this event: error or close. For example, the client might not re-open a GET request, triggering a timeout and a close. Or the write could fail and an error is emitted followed by a close (see websocket write callback).\nThe other extra complication is that we need to keep track of callbacks even when the transport is not writable yet (when things just get buffered at the socket).\nFInally, it'd be cool to emit an event when the socket is closed and things didn't get flushed out (basically when a writeBuffer has items).\n. Also important is to consider \nhttps://github.com/LearnBoost/engine.io/issues/59\nBasically instead of always assuming that things are going to be handled well by the transport and resetting the writeBuffer, we should keep track of the length of the writeBuffer in the closure that's being passed down to the transport, then slice the writeBuffer it it succeeds (because it could keep growing after we commit that buffer to the transport)\n. LGTM, needs tests\n. ok adding\n. done\n. You can override at your own peril by simply bypassing attach. You'll notice that the logic of attach is really basic, and mostly just a convenience method. The handleUpgrade/handleSocket/handleRequest methods are straightforward and well documented.\n. Also, notice that I'm referencing exports.Server, which means you can simply do:\njs\nrequire('engine.io').Server = YourServer;\n. Done. See https://github.com/LearnBoost/engine.io/pull/64\n. Needs tests. Remember to test for callbacks in buffered messages.\n. This doesn't consider transport upgrades. The packet could end up being drained by a different transport.\n. Also, if transport drains more than once the callback will fire more than once.\n. We actually don't need \"one\" test for this, it has to be tested very thoroughly. \n. This is looking pretty sweet!\n. Tests still have bad indentation. Also, the new engine.io style is multiple var instead of var + commas. Thanks!\n. @eugendueck i called you eugen duck again, sorry.\n. LGTM\n. Oh so I was spot on! :P\n. hot\n. Yep, fully agree with both statements. \n. Hot, thanks\n. Also, please add yourself to package.json contributors, thanks for all your insight and patches\n. I think a custom check function would be better suited, that gets the request object just like the stock one.\n. Excellent question\u2026 I'm down with that plan.\n. Well that should trigger a cross-domain poll. Maybe it's cross-domain that's failing?\n. Wow, feature! jk Not sure what explains this, but I might have run into this during development haha\n. @carlos8f can you still reproduce this?\n. Sweet thanks carlos\n. @mmastrac I see what you mean now. error must be firing twice on the transport right?\n. Yep because we detach the error event.\n. So this is an easy fix \n. Please test with master\n. My last wish would be a test that calls .send with a callback WHILE polling is happening (so that we can test what happens when a packet goes in to the socket writeBuffer having a callback)\nBasically we would need to listen on the client poll event, then call .send with a callback again. \n. As part of that test we could also make sure that writeBuffer is being populated to avoid false positives.\n. @afshinm basically we need a test that sends a message with a callback which gets passed to the transport immediately, then we want to send a message with a callback that ends up in the writeBuffer until the transport drains, consumes it and fires the callback.\n. I think you might have covered this in your tests but I would like to see if that we effectively check within the test that the message is in the writeBuffer.\n. This looks outstanding. \n. Looks REAL good to me\n. Yes\n. Rebase please\n. It looks good, but attach needs a refactor methinks.\nAlso, why setMaxListeners(1000) ?\n. How many instances do you have?\n. What's the benefit over having one instance with a regexp?\n. Yeah, keep in mind per SPEC engine.io also gives flexibility with the query string. You can pass arbitrary data related to the connection there too.\n. In a way I would say the \"path\" is a special engine.io thing, almost not user-land. The main reason for this is that otherwise we're almost introducing a pseudo/limited router in an attach function. It's just not right\n. I'm also curious as to why retaining /engine.io and passing variable identifying information as part of the query string is not a viable option. At the end of the day, the only reason we have the idea of \"dynamic path names\" in the web today is for user-friendliness, which obviously is not a valid use case for us. Most variable parameters are perfectly fine if set as part of the query string.\n. Also in a way you break the \"contract\" with other software that could be looking at your requests to determine what kind they are. Since we can't set a header \"X-Requested-With: Engine\" or \"X-Engine-IO\", the only way external components could know engine is in place is by looking at the path. \nFor example, I believe http://nodetime.com/ looks for \"/socket.io\" to give you socket.io analytics.\n. @cadorn I think what we should give the flexibility of regular expressions / functions / etc is the resource, not the path. I think I'm going to make the path immutable, in fact.\n. Let's do a smaller pull request first that just adds regular expression / function support for resource, with the appropriate documentation.\nThen we'll address the approach to multiple attachments to the same server. This pull request does too much and it's hard to review.\n. @cadorn the test \"server close should trigger on client if server does not meet ping timeout\" fails a lot, from your other pull request \n. DEBUG=* make test\nLet's document this in the README.\n. Please only include the commits related to this fix.\n. Uhm, how come no tests changes / new tests ?\n. @cadorn I think it's specially important for this one because we already have ping timeout tests, so obviously they're wrong and incomplete =]\n. @cadorn I'm pretty sure that prior to this commit all ping-related tests were passing. Since we changed the ping logic it also seems to me that we should be able to produce a test that fails in the absence of this logic change?\n. There's a ticket with this discussion. 10 second ping timeout is way too extreme.\n. @dvv suggested the current ping defaults.\n. Btw, do we have a test for the client closing on the absence of a server ping within (interval + timeout) ?\n. Also, since you're now sending the pingInterval in the handshake we need to update SPEC in the engine.io-client repo and bump up the revision to 2, since clients will be rendered incompatible.\n. Sounds good, since we're changing pings we might as well switch to that. Good point @cadorn \n. @cadorn just merged this, looking good. Make sure to submit a PR for SPEC.\n. Thanks a lot!\n. @cadorn if we make it configurable we lose the ability to identify engine.io requests beyond the scope of this project. Like I explained in the other ticket, an external piece of software/hardware, like a load balancer, should be able to analyze an incoming request and say \"this is an engine.io request\". Keep in mind we can't add specific identifying HTTP headers such as X-Requested-With to websocket or jsonp requests.\nI'd like to understand why the path being configurable is a desirable thing in your setup. Alternatively we can also discuss maybe introducing a fixed query string EIO=1 or something along those lines.\n. What I want is the ability to identify the protocol/service by just looking at the traffic. This is what HTTP headers are for. It's a very simple intent. Unfortunately, we can't add custom headers in 2 out of our 3 transports.\nOptions left that I can think of: \n1. url fragment\n2. query string\nSince the SPEC is already in place for (1), I wanted to hear arguments against (1) / for (2), or an idea for (3) that I'm missing. I don't get where interfaces.json comes into play.\n. Actually I'm sold on (2): it gives us the chance to also include the protocol revision number as part of requests.\n__eio={protocol revision number}\n. I'll tweak the SPEC. I'm going to state that:\n1. the unequivocal way of identifying engine.io requests is by looking for the query string __eio key.\n2. the path fragment under which the engine.io request is sent is configurable, but the default for server implementations should be /engine.io.\nThis effectively merges path and resource into one thing: path, which is documented as the \"mounting point\" of a engine.io server instance. We drop the concept of default in the URL.\nOverall, we're adding extra information to the request (protocol revision), we're dropping server complexity (one less option to maintain, test and document), and default conventional URLs will look shorter.\n. Yeah. You could also articulate it as resource get renamed to path and defauls to /engine.io and path goes away, considering we still want your patches for regular expression / string / fn support.\n. mount would be better I think, for consistency with the terminology used by connect/express for this similar concept.\n. Replacing with /mount ticket\n. Yeah, we need to figure out the way of making it not rely on time but events.\n. By adding this chance we inevitably have to introduce authentication semantics, such as specific authentication error events. I don't think this logic belongs in engine.io\n. Eventually verification needs to be async for the purpose of optionally storing everything in redis, not for custom user authentication logic.\n. Well because if you need authentication, you need a protocol on top of engine.io => socket.io. We need to focus all our efforts in making the authentication API in socket.io the best that we possibly can =]\n. Well socket.io can always close it.\n. In a way I want engine.io to be \"true\" TCP for the web. Even origin stuff doesn't really matter to me, that belongs in a higher-level abstraction.\n. Make sure that this is the case.\n. Please rebase against master\n. All origins should be allowed\u2026 what exactly is Firefox saying?\n. Wow my bad. Thanks for catching this @joewalnes \n. engine.io 0.3.3 and engine.io-client 0.3.3 out.\n. Shouldn't the client reconnect because his sid is going to be denied if servers changed?\n. https://github.com/LearnBoost/engine.io/blob/master/lib/server.js#L82\n. If user handshakes and connects to server A, only server A will know that the given session id exists there. If user attempts to poll against server B, the session id will be rejected (as per verify above).\n. Unless I'm not understanding what you mean by \"server changed\". I'm guessing you refer to the request being load balanced to a new server different from the handshaking one.\n. You will know because all requests pass through verify, and the request will return a 500, which will trigger a transport error on the client.\nAre you saying that you need to distinctively tell when this happened? If so, we should maybe switch from 500 to something else, then handle it appropriately on the transport, and then fire close with a different error code (like unknown session id).\n. The only way to get a payload is with a request, since the client polls.\n. It seems to me that an event that emits with the packet object reference so that you can modify it at will would be better?\n. Why would we use an option when we have an emitter in place? I'm against the particular implementation. Why wouldn't we also allow the alteration of any packet, instead of just pong packets?\n. Nice find @cadorn. Can you remove the console.logs ?\n. I'm gonna investigate.\n. pingInterval: 10, pingTimeout: 5 is gonna be a problem for this test\n. The solution will be https://github.com/LearnBoost/engine.io/issues/97 for now.\n. (which is being worked on atm)\n. ?\n. Thanks man\n. socket.request should still be there?\n. That looks good @ruxkor !\n. Yeah if you can help me test out #95 that'd be sick.\n. We should also check out what ServerResponse.write is doing with the buffer before it makes it to the net.Stream.\n. Actually we should look at what's making ws error out instead.\n. @mjgil no, he's using an experimental stream branch he was working on. You should instead .send directly instead of using .pipe\n. @3rd-Eden do you think that pull is a good fix ^ ?\n. boss_mode=on\n. It's working great for me. Can you show me your code?\n. Oh the client in the latency example is hard coded to a broken version I\nthink\nOn Tue, Oct 30, 2012 at 7:55 PM, Alexey Kupershtokh \nnotifications@github.com wrote:\n\nwicked@wnote:~/Alawar$ npm install engine.io\n...\nwicked@wnote:~/Alawar$ cd node_modules/engine.io/examples/latency/\nwicked@wnote:~/Alawar/node_modules/engine.io/examples/latency$ node index.js\nlistening on localhost:3000\nThen I opened http://localhost:3000/ and http://127.0.0.1:3000/ and it\ndidn't work in both cases.\nThe problem also remains if I clone the git repo.\nThe node is 0.8.14 from Chris Lee deb-repository.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/LearnBoost/engine.io/issues/108#issuecomment-9931125.\n\n\nGuillermo Rauch\nLearnBoost CTO\nhttp://devthought.com\n. The 4 in 400 already indicates error, so I don't see the benefit of JSON with error: true and reason over just sending the reason as text/plain.\n. 400 >= res.statusCode ?\n. If we were doing something like \n{ code: 1, message: 'Error message' }\nit would be more justified, since we're adding more information. But error: true is redundant, and the message can just be the body.\n. I actually think we should go with codes, and document them in the spec.\nUNKNOWN_TRANSPORT\nUNKNOWN_SID\nBAD_HANDSHAKE_METHOD\n. And let's get rid of error: true\n. The spec is in the engine.io-client repo btw (tree/master/SPEC.md)\n. Beautiful, we need to update SPEC now\n. I wanted to do this a while ago, but I think Einar said it was a good idea to keep engine.io pings. I don't remember why, however.\nI'm interested in revisiting this.\n. Perfect, thanks @Jxck \n. https://github.com/LearnBoost/engine.io/blob/master/lib/socket.js#L275\npacketCreate would be emitted with the object passed to writeBuffer there.\nEssentially:\njs\nvar packet = { type: type, data: data };\nthis.emit('packetCreate', packet);\nThis would allow 3rd parties to modify / examine outgoing packets\n. wow.\n. We want to do this for path.\n. I want to unify path and resource into a single thing.\n. Some of the tests in your pull request were adding support for regexp/function for path (something I definitely want) but they're not obsoleting resource. For more information go here: https://github.com/LearnBoost/engine.io/issues/116\n. Oh sorry\n. @kazan1000 checking packet roundtrip success is exactly what we do. We expect a ping/pong roundtrip with the custom text data probe \n. Would you mind running a packet analyzer and showing me the network traffic?\n. Can you give me an example of the message payload that was being eaten? Was it utf8?\n. So I'm just trying to understand what exactly about the encoding in base64 is helping mitigate the issue?\n. @kazan1000 please re-open if you have more details\n. I see. What's happening here is that you're calling close on the same stack as when we're opening, therefore the upgrade probes fire anyways. I'll add an extra check\n. Fixed in 0.3.10\n. @Contra it defaults to true because that's the behavior in Node when you don't attach any listener to the upgrade event, as to avoid leaking open sockets taking in data and eventually leaving your server out of memory.\n. Thanks Ven.\n. What's the issue? Please use the mailing list for support.\n. Documentation doesn't get any more thorough than this:\nhttps://github.com/LearnBoost/engine.io#api \n. https://github.com/LearnBoost/engine.io/tree/master/examples/latency\nexamples is the first link I see under \"engine.io\" in the project page.\n. @roamm this makes a test fail:\n1) server send callback should execute when message sent (polling):\n     Error: expected 0 to equal 1\n      at Assertion.assert (/Users/guillermorauch/Projects/engine.io/node_modules/expect.js/expect.js:99:13)\n      at Assertion.be.Assertion.equal (/Users/guillermorauch/Projects/engine.io/node_modules/expect.js/expect.js:200:10)\n      at Assertion.(anonymous function) [as be] (/Users/guillermorauch/Projects/engine.io/node_modules/expect.js/expect.js:73:24)\n      at Object.listen.allowUpgrades [as _onTimeout] (/Users/guillermorauch/Projects/engine.io/test/server.js:795:26)\n      at Timer.list.ontimeout (timers.js:101:19)\n. It's relevant to the client as well, that's why it's sent in the handshake.\n. This is excellent @roamm Thanks a lot.\n. @sweetieSong please rebase this.\n. I'm down for creating \"engine.io-gzip\" to keep the bloat out if possible.\n. Can you explain exactly what issues you've encountered that these fixes address? It's very hard for me to merge this without more explanation and tests.\n. Let's turn these changes into smaller pull requests with a better explanation / tests (if possible) for each. Closing for now.\n. Published\n. > data is raw utf-8 string, not a buffer, because socket.io doesn't support binary.\nThis is fine for the moment.\n\n_ I think we should only emit \"end\" on server close, transport close\n\nIs this consistent with what Node does?\n\nshould we support setEncoding method?\n\nI think maybe not for the moment.\n\nsocket.io 'close' clashes with node api\n\nHow so ?\n. I think we need to address that. We always need to emit error if there was an error, followed by close. For the purposes of compatibility with the Node API, we can emit end upon examination of the close reason.\nThis new behavior also merits new tests separate from the stream tests as well just to retain our sanity.\n. We definitely need to prioritize this.\n. @roamm can we make more focused pull requests? Like one for idempotent close, that could be a PR by itself. Will make reviewing easier\n. Thanks!\n. See #148\n. We're now using the one with a callback.\n. @plievone merged\n. Done\n. +1\n. +1 @3rd-Eden \n. @mjgil looks perfect. Let's add a test.\n. (to prevent regressions in the future)\n. <3\n. You have to configure nginx to proxy websocket.\n. Query string?\n. Oh sorry I misunderstood. Why not a normal packet?\n. @3rd-Eden I understand. This sort of thing belongs more on the socket.io side though. I want to keep engine.io  minimal like WS if possible.\n. I'd just like to see how we can implement this without compromising the API too greatly. For example, ideally we can do it by listening on a packet event an altering the data? (in which case we can probably already do it!)\n. @c4milo are you coming to jsconf.co?\n. Great\n. Fixed by @sweetieSong\n. Why not use the query string of the URL? Each new engine.io server means in the best case scenario one extra dedicated socket and in the worst 2 sockets (POST/GET requests for XHR polling)\n. yep, the query string gets carried over with each transport.\n. +1\n. Yes\n. If what's blocked is the \"length\" of the polling, engine.io should still work \n. master or latest tag?\n. @plievone after sending one packet we should clear the timer. GOOOD catch\n. Good point lets patch it  \n\nGuillermo Rauch\nSent on my phone\nOn Monday, June 3, 2013 at 3:21 AM, plievone wrote:\n\nHi @guille (https://github.com/guille), thanks for this, but unfortunately now the client can miss the noop packet (it comes via polling transport, whereas probe pong comes via websocket, the ordering can change?) and stall in upgrade? Perhaps best for now would be to have a longer, 500 ms interval just in case, and trigger it also on the leading edge separately.\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/LearnBoost/engine.io/issues/174#issuecomment-18832638).\n. Open for discussion on how to improve\n. Fixed on 0.6.2\n. @alaa-eddine did you find out anything else? Are you starting hte client and server in the same process or different processes? Does that change anything?\n. Also @isaacs any insight into how 0.10 could have broken this?\n. I don't think it's our job to be caching it. It'd make more sense to maintain node's behavior\n. I think we need more comments in the code to explain the rationale. We also need to document the new parameter in verify\n. This absolutely needs tests as well to avoid regressions\n. Thanks a lot @kapouer you the man\n. Very much so. Upgrading breaks polling on Squid and others.\n. Because the websocket request gets passed as a regular request which then makes the socket error. It should be ignored instead. The carnegie guys are working on this one.\n. This is not a good idea since the socket could be re-used by the browser for other requests.\n. CPUs won't actually be spinning, it'll just wait on I/O.\n\nIt's the responsibility of the client to sever the connection if it wants\nto do so. We might want to add this behavior to engine.io-client when we\nknow we have access to the underlying socket (ie it's running from node).\nOn Wednesday, July 3, 2013, Arnout Kazemier wrote:\n\n@guille https://github.com/guille leaving the server spinning because\nit has unanswered requests isn't good either.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/LearnBoost/engine.io/issues/178#issuecomment-20417210\n.\n\n\nGuillermo Rauch\nCloudup CTO\nhttp://devthought.com\n. @c4milo no because node should clean it up. \nIf you look at the \"Hello World\" of node:\n\nyou never actually close the socket yourself, and the server is keep-alive compliant out of the box.\nEngine.IO shouldn't \"take over\" the socket necessarily. We could make it an option, for example if you plan on having a dedicated domain for engine.io\n. @mokesmokes keep in mind the engine.io socket does not equal TCP socket. The browser could continue to use the TCP socket for other requests that are NOT captured by engine.io. The close packet is clear in the intent of closing the engine.io socket, which is why we fire close immediately, not when the underlying TCP socket closes.\nLike I said, the improvement we could make here is to enforce the TCP close if the user desires that. But that behavior must be opt-in.\n. close(true) maybe ? (famous last words)\n. Sure but closing the socket would incur in an entire roundtrip to set it up again if the browser needs to extend the pool\n. I don't want to have a util file. Let's just inline the merge.\n. @dainis we should fail gracefully with an error event, as if there was a parser error\n. @lemonzi +1\n. @3rd-Eden I actually think binary support is lower-level than strings :)\n. Definitely but I see that more as the \"fallback\" just like engine already\nhas polling for websocket. Its ugly but supporting binary is extremely\nimportant\nOn Sep 30, 2013 11:07 AM, \"Arnout Kazemier\" notifications@github.com\nwrote:\n\n@guille https://github.com/guille Not when you start adding another\nlayer of base64 decoder for it\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/LearnBoost/engine.io/issues/182#issuecomment-25387597\n.\n. This is one of the features that I'm prioritizing this month. It's probably gonna look like this\n\njs\nvar socket = new engine.Socket('ws://localhost', { binary: true });\nif you want to disable base64 fallback and have it emit an error instead if the browser doesn't support binary:\njs\nvar socket = new engine.Socket('ws://localhost', { binary: true, base64: false });\nthoughts?\n. One of the things I'm unsure about is whether we should emit error or throw right away.\n. @lemonzi that's what we're going to be doing in socket.io :). You'll be able to emit streams, or json objects, or a hybrid.\nEngine.IO itself needs to maintain the same functionality as websocket. We'll be supporting binaryType, and have a plaintext and a binary protocol depending on what the user specifies.\n. Sure. My understanding was that you wanted per-frame. After discussing it on #socket.io the other day we decided to do binaryType like WS as opposed to the constructor. Forgot to update here.\nThe other thing we discussed was implementing responseType for XHR as the fallback\n. @lemonzi what i'm also considering is that the client advertises their capabilities in the handshake in terms of binary support and then the server responds appropriately (encoding in base64 when needed). We would only keep the base64 flag to force \"no base64 fallback\".\n. Thanks for staying on top of important issues @defunctzombie \n. x and ? are antipatterns. We should always pin.\n. Thanks @jbowes \n. Is this BC with Node 0.8/0.9? \n. Thanks btw @kkoopa @mahnunchik \n. Thx @mahnunchik \n. Done\n. Good stuff. The build fails for some reason ?\n\n. Seems like it's node 0.10 only\n. We use engine.io over SSL with no issues.\n. Simply attach engine.io to a https.Server, or put a HTTP engine.io in front of Nginx doing the SSL for example (that's what I prefer)\n. I agree with @mokesmokes \n. Yup for that you would use a message bus. We definitely don't want to solve that in engine.io directly. \n. @3rd-Eden what sort of cleanup does ws need? Also, I think cleanup should happen automatically one we attach to a web server.\n. @TooTallNate I thought you'd fixed this with the global component?\n. I believe we have a test for this?\n. https://github.com/LearnBoost/engine.io/blob/master/test/engine.io.js#L61\n. I would say the test is correct because of a potentially sub-optimal default in engine.io client.\nAt the moment, we're looking for a policy server for XMLSocket over port 843:\nhttps://github.com/LearnBoost/engine.io-client/blob/master/lib/socket.js#L85\nhttps://github.com/LearnBoost/engine.io-client/blob/master/lib/transports/flashsocket.js#L147\nThat would mean that you need to set up a separate TCP server and attach the policy server listener to it.\nThis is bad for a number of reasons:\n- Setting up a server with flashsocket support is kinda hard to do. You need a separate server over port 843, which is privileged. Flash's behavior when port 843 is taken is to attempt to load the policy from the port you're trying to connect to, but we don't want 843 to time out.\n- Port 843 will be blocked by most corporate proxies, resulting in a timeout and ultimately accessing the same engine.io port.\nUltimately, since the web-socket-js project (the Flash bridge we use) has an API call to access loadPolicyFile we could and should tell the bridge to request directly to the port engine.io is serving.\nThis will make flashsocket easier to set up and work better for proxies when you're just serving engine.io on 80 or 443. But obviously we're going to need to figure out a way of accessing the data packets in a way where the node HTTP server \"ignores\" the obvious HTTP errors (since the policy request is XML). \nHopefully this is doable without great compromises.\n. @mokesmokes good find \u2013 would indeed be nice to get a test\n. We need a testcase for this.\n. Basically the scenario you described that this change fixes should be reproduced in a test.\n. I was just curious if this test is only passes on the presence of another bug: that calling .close() immediately is not firing the close event on the next tick.\n. In other words, if I call conn.close(), why would close need to fire with ping timeout if I just forced a close?\n. @mokesmokes cool \u2013 let's revisit the close reason / semantics after this.\n. But was the WS upgrade confirmed?\n. And the polling requests stop?\n. Awesome\n. How do we differentiate a close from network issues from any other close on the websocket client?\n. The problem that I see with this is that we can't trust that by reopening we haven't missed packets. If an error occurs we need to inform the user that, and have him reopen the socket.\n. Awful decision on our side. We shouldn't touch req in this obtrusive way.\n. (it was my decision btw :P)\n. @timoxley ideally we have a helper that just returns the query object from the request, and we don't touch it\n. What version of the client are you using?\n. Seems like we broke the example. Checking it now\n. Fixed with https://github.com/LearnBoost/engine.io/pull/209\n. Thanks!\n. @mmastrac would be cool to have that in a wiki article (\"Setting up engine.io with HAProxy\")\n. Right, the goal was to move it to userland.\n. hashtag lame\n. Docs plz\n. Why is it not recommended?\nOn Mon, Feb 3, 2014 at 2:05 PM, Roman Shtylman notifications@github.comwrote:\n\nAdded some more documentation about this to the readme as well as which\nmethods I felt are [Recommended] vs which are not.\n\nReply to this email directly or view it on GitHubhttps://github.com/LearnBoost/engine.io/pull/213#issuecomment-34005978\n.\n\n\nGuillermo Rauch\nCloudup CTO\nhttp://devthought.com\n. That's a really good point. In this case I think there's something to be said about the convenience of being able to require('engine.io')(server) and be done with things tho.\n. Like you don't need a reference to the module itself, you just want the io isntance with the attached http server.\n. We need to update the README to use the shorter syntax whenever possible.\n. Yes. We gotta remove the \"recommendations\" from the README.\n. Huge simplification. Thanks @defunctzombie \n. @defunctzombie does this LGTY?\n. The upgrade event is not necessarily off\u2026 the semantics could be \"we're upgrading to this transport\" event or \"we upgraded to this transport\" event.\n. Also, we're changing the semantics of what request is for. The one that originated the socket is useful because websocket has limited headers (like no user-agent).\n. @mokesmokes if we document the upgrade as being before the transport is swapped, the developer doing a nextTick in that event handler would accomplish what you need.\n. @mokesmokes are we tagging it onto the transport object? Isn't it socket.request?\n. Let's do separate pull requests to discuss the upgrade event and request property.\n. Then again all of our other non-writable properties are not marked as such.\n. So let's just do a simple prop\n. Do we have a test for this event? It'd be cool to have a test with the new semantics. As in, test that socket.transport.name changes after upgrade fires.\n. Great stuff as usual @mokesmokes \n. I think it's definitely better suited as a global ignore.\n. Amazing\n. This broke 0.8 - reverting to process.nextTick\n. There's a lot of people on 0.8 still\n. You mean so that the clients side socket assumes the handshake is gonna work?\n. Interesting\u2026 it's very tempting to add this since it can result in a pretty sizable performance improvement. On the other hand it's scary to add this much complexity, and we could try to do it on userland.\n. No, he definitely wants to reduce latency. If I'm following right this is akin to the \u00f8mq \"faster than tcp\" approach based on buffering http://zeromq.org/area:faq#toc6\n. It also reminds me of the QUIC roundtrip reduction approach\n\nRound-trip times, roughly defined by the speed of light are essentially fixed; the way to decrease connection latency is to make fewer round-trips. Much of the work on QUIC is concentrated on reducing the round trips required when establishing a new connection, including the handshake step, encryption setup, and initial data requests. QUIC clients would, for example, include the session negotiation information in the initial packet\n\nhttp://en.wikipedia.org/wiki/QUIC\n. Query strings have limits in size and also are commonly logged. Having messages in the query string would break the principle of least astonishment. If you start looking at the traffic and you see your first message in a query string you'll be like:\n\n. @binlain please join us in ##socket.io freenode when you have a chance\n. Also just noticed the PR title should be Fewer instead of Less\n. Kidding (mostly)\n. What do you mean by a custom handshake function?\n. Sounds complex. Why not let the user do that upon the open event ?\n. @nullbox @rase- this is definitely a blocker for socket.io 1.0 release\n. I'm reverting that commit\n. @rase- we must be missing some crucial tests if this didn't come up as a failure\n. Actually @Swaagie the changes mostly seem to be limited to the tests, and the other 2 seem harmless. Are you sure this commit breaks it?\n. @binlain you'll have to have a mechanism for users to get new code if you want zero downtime. What I usually do is send a socket.io event reload for extreme cases like that, that introduces a random reload in the next few seconds.\nYou can't expect your server to always be backwards compatible, and people could have their tabs open for days at a time.\n. Can you elaborate on what forceJSONP: true fixes?\n. https://github.com/LearnBoost/engine.io-client/issues/160#issuecomment-37507655\n. Thanks @j-salazar \n. Thanks for the test case @lpinca \u2013 will take a look now.\n. @lpinca I can't reproduce. Have tried many different things. Any specific OS version? I tried XP\n\n. Were you trying this over localhost? I wonder if this is a race condition that's being \"fixed\" by my slow tunnel.\n. Gonna give it a shot. Thanks. My thinking is that my cross domain situation\nprobably used JSONP and it ended up working better that way.\nOn Sun, Mar 16, 2014 at 7:37 AM, Luigi Pinca notifications@github.comwrote:\n\nHere it is https://dl.dropboxusercontent.com/u/58444696/engine.io.JPG\nserver and client are on two different VM hosted on the same machine.\nI had that result on third page refresh.\n\nReply to this email directly or view it on GitHubhttps://github.com/LearnBoost/engine.io/issues/233#issuecomment-37753815\n.\n\n\nGuillermo Rauch (@rauchg https://twitter.com/rauchg)\n. The requests that engine.io captures shouldn't have been touched by restify, right? So why would there be a collision ?\n. I don't mind having req._query.\n. Can you post the two stack traces for it? Would be really helpful to fix it.\n. I think we should probably have something like this that's async instead.\n. I do too\n. Socket.IO 1.0 (1.0.0-pre2 on NPM and master on git) already implement engine.io.\nYou should use engine.io standalone only if you want basic web socket (no json, custom events, reconnection, rooms, namespaces)\n\u2014\nSent from Mailbox\nOn Fri, May 16, 2014 at 9:56 PM, Erick Arroyo notifications@github.com\nwrote:\n\nHi everyone maybe this is not the place to ask this, but i just found this package called engine.io and its describes it self as :\nEngine.IO is the implementation of transport-based cross-browser/cross-device bi-directional communication layer for Socket.IO.\nAs we now if type on line command npm install socket,io it will install the 0.9.16 version and in the master repository there is an 1.x in development, so my question is: Should we stop using socket.io and use engine.io? Should we use both? When should we use one or another,\nPD: I'm kinda new but i want to have this clear, so thanks for advance for ur answers.\nReply to this email directly or view it on GitHub:\nhttps://github.com/LearnBoost/engine.io/issues/251\n. Please refer to Github. Both are actively maintained. We tagged the prerelease 1.0 version a couple weeks ago in preparation for the final release :)\u2014\nSent from Mailbox\n\nOn Fri, May 16, 2014 at 10:15 PM, Erick Arroyo notifications@github.com\nwrote:\n\nOh thanks, So its socket.io an active project? cuz been a while that the version 1.0 is there but not released to NPM, i mean if u go to https://www.npmjs.org/package/socket.io u will see the last version is 0.9.16, when its gonna be the 1.0 version released is there any ETA? im asking cuz if im starting a new project should we use 0.9.16? or 1.0 still in beta, what do u recommend me?\nReply to this email directly or view it on GitHub:\nhttps://github.com/LearnBoost/engine.io/issues/251#issuecomment-43398045\n. A PR for this would be wonderful @pawelatomic.\nThanks for reporting\n. Socket#id\nhttps://github.com/LearnBoost/engine.io#properties-2\n. Good catch, thanks!\n. You're the best @mokesmokes \n. Thanks so much @ifraixedes \n. SSL is a good thing, but there can still be server middleware that breaks the connection once it's decrypted.\n\nIt's also not a reasonable expectation for our users.\nAgreed on sticky session being a good thing, but we need to figure out a solution for our heroku users.\n\u2014\nSent from Mailbox\nOn Thu, Jun 12, 2014 at 7:09 AM, mokesmokes notifications@github.com\nwrote:\n\nSeveral points:\n1. If you use SSL you will find that websockets pass on almost all networks (I have never seen them fail), including mobile. Thus I would strongly recommend to always use SSL with socket.io and engine.io\n2. If you insist on using Heroku, then you can use an external load balancer (e.g. haproxy) to effectively create sticky sessions to your Heroku servers.\n3. Frankly, using Redis for pub/sub on all messages in order to scale never made sense to me. As you scale your app, your back-end messages will increase exponentially (since you have more servers sending messages to more servers.... ouch......). So while this may seem a more \"fair\" way to scale, all you're doing is introducing more (unnecessary) load to your cluster. You will eventually reach a point where your entire cluster chokes on backend messages instead of serving clients.\n4. To add to the previous point - without sticky sessions you will need to load per-client session data on numerous servers - yet another ouch.\nBottom line: sticky sessions are a good thing. Strongly suggest you use them, not work against them.\nReply to this email directly or view it on GitHub:\nhttps://github.com/Automattic/engine.io/issues/261#issuecomment-45895722\n. For LBs that support TCP mode that should be fine, but the ones that don't understand the request will simply break it (Squid being a fairly common occurrence)\u2014\nSent from Mailbox\n\nOn Thu, Jun 12, 2014 at 7:20 AM, mokesmokes notifications@github.com\nwrote:\n\n@guille external LB with stickiness based on client IP, for example, should do the trick.\nReply to this email directly or view it on GitHub:\nhttps://github.com/Automattic/engine.io/issues/261#issuecomment-45897201\n. I don't intend to support it within the core codebase for sure\u2014\nSent from Mailbox\n\nOn Thu, Jun 12, 2014 at 7:31 AM, Arnout Kazemier notifications@github.com\nwrote:\n\n-1 from me as well, this should be solved in higher level abstractions such as https://github.com/topcloud/socketcluster\nReply to this email directly or view it on GitHub:\nhttps://github.com/Automattic/engine.io/issues/261#issuecomment-45898774\n. We're looking into adding hooks to make it possible to retrieve and alter state outside-of-process. We'll need to see what sort of complexity it adds to the codebase.\n. Yeah, I think the discussion is wrongfully getting diverted to UX manifestations that are not linked to the low level networking implications of sticky load balancing. For example, if a connection is very quickly re-established, you most likely don't want to bother the user by telling them the app is reconnecting. In the same vein, you don't want to block input by the user anyways, and you most likely want to render their interactions optimistically. I blogged about this here.\n\nUX aside, we do need a solution for our Heroku users, and we do want to maintain a simple codebase. We're looking into trying to achieve both.\n. Great fix, great tests\u2026\n. Yep we're already doing this. Next release will have it.\n. Thanks @binlain as ush\n. Release is coming up. Fixing some issues with tests.\n. Can't merge @fjakobs \n. Thanks as usual!\n. Pushed a fix. Thanks @fjakobs for the original patch, and @adamrothman for the reminder.\n. I think the Socket constructor could do this itself, since it receives the req parameter.\n. I'm down. But we need:\n- An option to turn it off (gzip: false)\n- Tests\n. Really appreciate it\n. Might also want to apply it for the jsonp transport.\n. Speaking of which, we might want to consider using this to turn off/on WebSocket DEFLATE frames\n. Too slow, better  for non-realtime compression, as it's basically brute force gzip.\nWe do use zopfli on the socket.io CDN\n\u2014\nSent from Mailbox\nOn Wed, Jul 30, 2014 at 1:38 AM, Arnout Kazemier notifications@github.com\nwrote:\n\nyou might also want to explore zopfli for compression.\nReply to this email directly or view it on GitHub:\nhttps://github.com/Automattic/engine.io/pull/271#issuecomment-50588360\n. Superb! Thanks for the update\u2014\nSent from Mailbox\n\nOn Wed, Jul 30, 2014 at 1:48 AM, Ben notifications@github.com wrote:\n\nIt slipped my mind this morning that I'm leaving on a short 5 day vacation today, so i'll finish up this PR when I get back if someone else doesn't beat me to it.\nI measured the impact of this change on our prod server this morning, it's a 3x speed improvement for responses in the ~500kb range. Absolutely terrific, shaved 2 seconds off our app bootstrapping process.\nReply to this email directly or view it on GitHub:\nhttps://github.com/Automattic/engine.io/pull/271#issuecomment-50589333\n. @ben-mg this is better suited for server-side (Node.JS) tests, where you should be able to test it\n. \n. This is one of the best issue descriptions I've read in a while. Thanks for taking the time.\n\nCan you send me the complete headers of the request that yields OPTIONS? Ideally we wouldn't need the pre-flight. I'm suspecting you're sending binary data which is resulting in a Content-Type switch?\nRequests that do not need pre-flight according to MDN are:\n```\nA simple cross-site request is one that:\nOnly uses GET, HEAD or POST. If POST is used to send data to the server, the Content-Type of the data sent to the server with the HTTP POST request is one of application/x-www-form-urlencoded, multipart/form-data, or text/plain.\nDoes not set custom headers with the HTTP Request (such as X-Modified, etc.)\n```\nBefore proceeding with the OPTIONS fix, I want to make sure it's happening for a good reason.\n. +1 :datfeel:\n. Agreed with @mokesmokes\n\u2014\nGuillermo Rauch \u2013 @rauchg https://twitter.com/rauchg\nOn Tue, Sep 2, 2014 at 9:14 PM, mokesmokes notifications@github.com wrote:\n\nI think the default behavior should stay as today (break) since in 90% of\ncases it's unintended behavior and we don't want this happening silently.\nWe can add a flag that if explicitly set the server sends OK on every\nOPTIONS request.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/Automattic/engine.io/issues/279#issuecomment-54250220\n.\n. +1. Big time priority.\n\nOn Mon Dec 01 2014 at 3:17:05 AM Mark Mokryn notifications@github.com\nwrote:\n\nSo basically what I think we need is:\n1) OPTIONS turned off by default on engine.io server side\n2) By default forceBase64 for XHR if client detects CORS scenario (WS is\nof course fine)\n3) Add a forceBinaryXHRCors flag (default=false) which will disable (2)\nabove, thus the browser will emit OPTIONS requests, and the developer will\nneed to turn on the OPTIONS flag server-side.\nSo basically, by default all works smoothly and no OPTIONS as most people\nwould probably prefer. If people insist on using octet-stream then they\nshould manually configure this, but it should be supported. I really don't\nthink we should just send binary data blindly in CORS scenarios.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/Automattic/engine.io/issues/279#issuecomment-65024839\n.\n. Yes. Bandwidth is hardly ever the bottleneck, latency is.\n. In this case, an additional OPTIONS request guarantees an extra roundtrip for us. The extra bandwidth cost for frames would hardly ever result in extra roundtrips.\n. We actually do want to send 400 because it's a engine protocol error, not a CORS protocol error.\n. Good find. Ideally we should find a way of expressing this as a persistently-failing test as opposed to a race condition, so that we can introduce a regression test.\n. Thanks a lot for the fix @lpinca, @3rd-Eden. I apologize for the delay in merging.\n. I think it's better to handle this through an event emitted at the transport layer, so you could add the headers programmatically by receiving the request reference.\n. Rebase\n. Awesome PR is awesome\n. Shouldn't we call .abort on the XHR too, as opposed to getting rid of the event cleanup?\n. If we're getting a large incoming POST, we need to shut it down. Stop buffering data, hence the .abort.\n. An example would be the server closing the socket through socket.close(), and the client trying to send a lot of data.\n. Much better. Our abort was lame.\n. Yes, next immediate one\n. When we call sendPacket manually, should the compress flag be undefined so that it obtains it from the default provided by the configuration in the constructor? \n\nOtherwise, it seems that if we disable compression globally, it'll still force to true when we send non-user packets (like noop)\n. Before, it seems that it was always defaulting to compress: true if it was undefined?\n. @nkzawa please rebase so I can merge.\n. Awesome! Thank you :)\n. Yep, I'll get this published asap\n. Merged\n. Great patch. Thanks!\n. In this case you're sending the headers over the socket. What you want to do is introspect the request of the socket and check its headers.\n. You have to rebase this @adrai \u2013 Thanks for your contributions & follow-ups \ud83d\udc4a\n. This is great!\n. @nolanlawson have definitely stumbled upon this myself and it's puzzling, but I don't think there's much we can do\u2026\n. yep thanks @nolanlawson \n. Cool!\n. Thanks everyone\n. Thank you shinnn :)\n. \ud83d\ude0d\n. Thanks @nkzawa @3rd-Eden \ud83c\udf89\n. This PR has been merged and included in a release already. What version are you running?\n. @rikukissa so is this specifically related to SSL handling?\n. @rikukissa thanks a lot!!\n. Huge fix @nkzawa :)\n. Totally understood. We'll get a release out ASAP.\n. \"of tests\" you mean? We have really solid coverage for the behaviors changed here.\n. Release coming asap!\nOn Mon, Nov 9, 2015 at 12:10 PM Yan Qing notifications@github.com wrote:\n\nLook for @rauchg https://github.com/rauchg\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/socketio/engine.io/issues/362#issuecomment-155030448.\n. @titoBouzout \ud83d\ude29 we did mention it in the changelog, but we could have been more explicit about the specific datastructures involved. Apologies.\n. Thanks @nkzawa and @darrachequesne \n. Done. Thank you\n. @3rd-Eden I'm aware (I received the security disclosure on ms). I still think it's a good idea to have debug up to date.\n. @3rd-Eden agreed. In fact, it's debatable that ms itself should handle the limit, and not whatever is handling user input.\n. Actually, while it's faster, I just realized it's not cryptographically sound. Math.random can't be relied upon, so this would mean very trivial session hijacking.\n. (it is great for use on the client to ID JSONP requests however)\n. Also, just realized it's not even random, just linear, so it's not a good fit for this.\n. Done in -client too. Thanks\n. Good find @lpinca. Instead of mutating that property directly, can you call a method discard()?\n. (probably in the Transport prototype, and it's ok if it just sets discarded = true)\n. Awesome @lpinca \n. Awesome stuff as always :)\n. This sounds amazing!\n. I'll merge if you send a PR.\n. > Node.js is broken on Windows and needs to be fixed for us to support the platform\n\nActually I can't :'(\n. I just saw the PR to be able to opt-into it. Reviewing now.\n. @alexhultman no sarcasm, I didn't realize #390 was a PR (thought it was an issue)\n. @alexhultman @kapouer I'm open to also adding a specific test that involves using uws as a dep, so that we know this option works for a realistic scenario.\n. @alecbenson thanks for the heads-up, upgrading\n. d9dda2b..298cb6f\n. id && this.clients[id] would be preferred here\n. UTF-8 is needed because sometimes socket.io is embedded in websites not served with utf8, so it defaults to the wrong thing\n. I was coincidentally thinking about doing this yesterday. Good move\n. Bad indentation \n. We use 2 space indents\n. indent\n. single quotes\n. Is it necessary that the polling transport reports what sequence id it's flushed? If you think about it, the Socket should know this, as the polling buffer won't keep growing, the writeBuffer at the socket level will.\n. Sure but the transports don't really need to know about sequence ids.\n. As far as Socket goes, it doesn't matter if it's polling or websocket, it just needs to wait on the drain event, for which it should know what the sequence id is.\n. I think it's redundant to have it both in onClose and onError\n. style problems\n. you can simply check that callback exists.\n. Unneeded whitespace\n. Unneeded \\n's\n. Why the changes to .gitignore?\n. I don't think we want to show this so prominently. It's kindof an advanced-ish feature\n. If this fires 4 times and the callbacks don't fire you get a false positive here.\n. Bad indent\n. Missing space\n. This applies to a few other tests too. We can't always rule 100% of false positives but keeping separate counters should help.\n. double \\n\n. Thanks, I was looking at this yesterday and was thinking of simplifying \n. Maybe this whole block would be nicer as a switch ? Or maybe a helper function outside of .attach? .attach is looking unwieldy \n. It's really hard for me to follow the logic here :\\ Variable names are too long as well.\n. double indent\n. Why did you undo this test? \n. This change was needed when we upgraded node-xmlhttprequest to 1.5.0\n. Why would you want that event in user land?\n. If anything, it'd be more useful to have on packet, then examine packet type.\n. The bigger concern is that all the ping related pull requests have broken tests in different ways.\n. Yep, packet is better for that case.\n. Let's cache this outside where you defined isIOS directly, no point in checking for whether it's undefined.\n. Let's link to the gist that describes this technique in the comment to make it briefer.\n. No lib/index plz\n. No lib/index\n. No lib/index\n. No lib/index\n. Remove\n. Remove\n. You can use a simple {} and then call defineGetter on them.\n. Same for other elements below\n. @albertyfwu do you think we should have a test for this situation as well?\n. Not engine.io related\n. Move to support/tests/\n. Move to support/tests/\n. Move to support/tests/\n. Move to support/tests/\n. It would be great to reuse the engine.io-client that's under node_modules so that the functional tests and the cloud tests use the same codebase.\n. Move to support/tests/\n. All the globals that are not used in the tests directly belong in the test runner (support)\n. Let's use ENV vars instead:\n- SAUCE_USERNAME\n- SAUCE_USERKEY\n. Not all dependencies belong in devDependencies. The ones that are for regular usage belong here, the ones that are for tests and development belong in devDependencies.\n. It would be great if the build files end up in support as well so test/ remains uncluttered for the developer.\n. (Y)\n. What's the purpose of errors.txt?\n. Do we need the globals here vs support?\n. Oh awesome, can we remove .txt? Let's call it errors.out\n. +1\n. whitespace?\n. Did you add the whitespace or remove it?\n. Thanks, github is not showing me the dark character.\n. cc @xixixao \n. wouldn't this .close call generate a close event right away?\n. I think this should be in its own dedicated example as to avoid confusion with the typical usecase\n. How about Blob? \n. and BlobBuilder\n. Missing \\n\n. Unneeded parens\n. Why do we handle undefined ?\n. Unneeded parens\n. Unneeded parens\n. Extra =\n. Extra =\nAlso we usually do 'string' == var for readability (reverse order)\n. For one liners we still use braces for readability.\n. nice\n. Should the parser API figure out types instead, and we maintain the same signatures and public API as before?\n. Extra \\n\n. This would be optional. We need to document the various ways this object can be instantiated.\n. I mean for constructing.\n. We don't really use defineProperty much, why not this.request = req ?\n. Right I thought I was commenting on the ctor docs\n. Why do we have to initialize them as null ?\n. This is not always the case, as in ActiveXObject. Why is this always true ?\n. unneeded parens\n. Missing \\n before\n. \\n after { and before } plz\n. syntax\n. typo\n. typo (paramters)\n. I wonder if we should split the packet buffer instead of ignoring?\n. Do we want to default this to true? \n. Considering our most common case (JSON packets), it might be a good idea.\n. Yeah, compression at the expense of latency is no good.\n. I think here it would definitely be faster to do:\njs\nreturn options && options.compress;\nAnd also maybe ensure we pass compress: true? Having defaults in a low-level API like this is kinda weird.\n. In other words, if we want a default for the public-facing API .send(), we set compress: true there.\n. +1\n. so much cleaner :) thank you\n. is this a bug? why would ws mutate parameters?\n. ",
    "kapouer": "It works only with a70dcaa92, bbc988822 is not enough as Ruxkor points out...\n. First really working setup :)\n. Damn i hallucinated badly.\n. Since engine.io 0.1.2 it seems the errors i'm seeing have changed :\n\n\nError: reserved fields must be empty\nat WebSocket.onError (/home/test/node_modules/engine.io/lib/transport.js:77:13)\nat WebSocket.emit (events.js:70:17)\nat Receiver.onerror (/home/test/node_modules/engine.io/node_modules/ws/lib/WebSocket.js:542:10)\nat Receiver.error (/home/test/node_modules/engine.io/node_modules/ws/lib/Receiver.js:301:8)\nat Receiver.processPacket (/home/test/node_modules/engine.io/node_modules/ws/lib/Receiver.js:187:10)\nat Receiver.add (/home/test/node_modules/engine.io/node_modules/ws/lib/Receiver.js:93:24)\nat Socket.firstHandler (/home/test/node_modules/engine.io/node_modules/ws/lib/WebSocket.js:500:22)\nat Socket.emit (events.js:67:17)\nat TCP.onread (net.js:367:14)\n\nNote that the client is still 0.1.0 so kill me if that's the reason.\n. it's hard to mesure so i'm not sure, but i'm almost sure there are much less sockets left open. The ratio i mesure has been staying low for days.\n.  Don't include engine.io as dependencies so that it's clear it's version-agnostic\nYou might also want to mention that the flush event is only available in 0.2.1+.\nis somewhat contradictory.\n. i just realized it's related to #95\n. Please comment this test: https://gist.github.com/kapouer/7634923\n. @guille \nit's only a concern if the engine.io server is attached to http.Server, no ?\nWhy not opt-in when engine.listen is called, and opt-out when engine.attach is called ?\n. Isn't this related to https://github.com/LearnBoost/engine.io/pull/177 ?\n. you don't have to create a new PR if you want to overwritethe commits\nYou can clone the repository locally, checkout your patch branch, commit --amend or reset HEAD^ then when you are done, commit and push -f. The PR will be updated.\n. Heroku can seppuku ;)\n. Section 3 of JSON RFC4627 answers your question - sock.js effectively encodes strings in UTF-8 as well if it encodes in JSON. Does it not ?\n. This might be the answer: Encode strings with utf8.js to support multibyte strings ?\n. May i suggest using a custom function(req) instead ?\n. If is sufficient to test that memory is released when you call global.gc() (using --expose_gc flag).\nIf you don't want memory to grow too much, see https://github.com/nodejs/node/wiki/Frequently-Asked-Questions\n. max-old-spaces-size will trigger gc more often, but not necessarily just after a disconnection.\nTo know that you must call gc() explicitely after a disconnection. Then you can profile if memory is released (or not).\n. Update: the project is moving quite fast, and pass almost all engine.io tests, see https://github.com/alexhultman/uWebSockets/issues/65.\nThe port to uws looks trivial.\n. @lpinca that's not a huge blocker, since it's probably not that hard to implement those flags again.\nI believe engine.io could accept an option to require('thatwsclient'). Let's PR that proposal :)\n. See #390 \n. It's socket.io-client/node_modules/engine.io-client/node_modules/ws\nor socket.io/node_modules/engine.io/node_modules/ws so the wsEngine option is cute and explicit.\n. Are they ? Damn\n. Not dead. Crystalized :)\n. Test is coming\n. @rauchg i went a little further than a specific test. Also i removed the ./node_modules/.bin/ part in npm script, since npm already looks there.\n. Amen to that, i just committed in wrong order.\n. that's precisely what https://github.com/socketio/engine.io/pull/390/commits/53b6ae515d299bf17f2cd43831cf26b7147be9ba does\n. or to be more precise, there's one test for the wsEngine option alone, and all tests are run twice.\n. Provided you run them using npm test\n. As an engine.io user, (used to the xxx, xxx-client convention) i'd expect env vars to be\nEIO_WS_ENGINE\nEIO_CLIENT_WS_ENGINE\n. Let me fix order of commits...\n. Last thing to \"fix\", up to you @rauchg : https://travis-ci.org/socketio/engine.io/jobs/128421493#L136 requires https://docs.travis-ci.com/user/languages/javascript-with-nodejs#Node.js-v4-(or-io.js-v3)-compiler-requirements\n. The tests will fail on nodejs < 4 - is it a problem ?\n. Or uws could fallback to ws for those versions ?\n. I can add that modification to the PR, indeed\n. I don't think static linking was needed ? The problem here is more about travis config - i'm pretty sure the config i added in the .travis.yml file has not been taken into account (at least not the addons part).\nNow we just need @rauchg to review and accept the pull request.\n. I'd rather put just version 4 or 5 and rely on https://docs.travis-ci.com/user/languages/javascript-with-nodejs#Provided-Node.js-Versions\n. And put a warning when testing ssl using uws and a version < 4.4.3 (is this the required minimum for ssl to work ?)\n. Apparently other disabled tests are not writing a warning, so i just disabled when node < 4.4.3 and module is uws and test is using https.\n. Node.js caches require calls, so it's perfectly fine to require the same module over and over.\n. While engine.io is for the server part and engine.io-client for the client part, i agree with your remark about the naming. Better be explicit than vague. I updated #390.\n. It's not uncommon that upstream authors forget to git push && git push --tags.\nFor the problem you're seeing: is it that you have socket.io 1.6.8 on server, and socket.io-client 1.6.9 ?\n. Are you using certificates ?\nBecause https://github.com/socketio/engine.io-client/commit/2c55b278a491bf45313ecc0825cf800e2f7ff5c1 has changed default behavior.\n. That's the only change between 1.6.8 and 1.6.9. The server version bump is a dull. The client changes only that behavior. It should have been a minor version bump at least...\nAnyway, just set opts.rejectUnauthorized = false; and you're good to go !\n. Usually you can pass options, please see that with sails.io.js.\n. You can actually start using uws integration right now, please read the UWS readme about socket.io.\nNo reason to rush things ! i would find it a thousand times better if engine.io had an optional dependency on uws by default and kept a dependency on ws, and tried to load the uws module first and fallback to ws.\nI think it's a little bit too soon to do that, though. The experience might be disappointing to the conservative user, but seeing how @alexhultman maintains uWebSocket, there is no doubt that in a few uws releases, engine.io should try using uws first in a default install.\nAlso i find it a little hypocrit to \"leave the door open to other ws implementations\" - there is little chance we see a lot of well-maintained websocket implementations... yes i mean that my PR #390 wasn't really a good idea.\n. The problem doesn't come from the package.json file itself: a missing license there has no legal implication. However, the published base64id npm package doesn't contain its LICENSE file - this is indeed an issue.\n. That would be awesome, and it should have no impact on current user base.\n. Maybe now that uws@0.12.0 is so nice and stable, it should be listed as an optional dependency and be picked by engine.io, falling back to ws when uws fails to load ?. Well the message in the gulpfile is wrong now, let me fix it.\nOr not. It's all right. Somehow i'm confused about that.\nOh now i understand why - travis installs node 0.10 for testing with node 6 !\nhttps://travis-ci.org/socketio/engine.io/jobs/182596511. Forwarded test failure to https://github.com/uWebSockets/uWebSockets/issues/365.. @alexhultman thank you for your clear answer.\n@darrachequesne the problem happened to be, IMO, in engine.io, so i tried to fixed it properly in https://github.com/socketio/engine.io/pull/459/commits/04bde4cba8a7bd5dbfdcdea1f084b52d197cd8bc and rebased against latest master.. The failure\nserver close should trigger on both ends upon ping timeout:\n     Uncaught Error: expected 'transport error' to equal 'ping timeout'\nhappens on master as well, not related to this PR.. Seems to pass, but let me remove this commit and make another PR just for that.. Yep :). Hi,\nthis quote from npm's documentation explains it all: \"The difference (for optionalDependencies) is that build failures do not cause installation to fail.\".\nSo in any case npm install engine.io will try to install uws. Should it fail for any reason, it would fallback to the slow, horrible, scary native ws engine.. Pushing the reasoning further i would say uws could actually provide ws as a fallback in case it fails to install :). but you can consider this a new year's joke.. That's fine for me, as long as it works :). I'm currently using version 0.13.0 in production - it can replace version 0.12.0 as is.. Best thing to do, considering the situation is to use \"uws\": \"latest\" (edit: use latest tag). A mention is a good idea, but i'd word that differently like\n\"An alternative c++ addon is also available by installing uwsmodule\"\n. Well, since it's in the devDependencies, and all tests pass, you won't have to PR that much. Only when some other update really breaks a test.\n. For the record (i replied somewhere else already):\nEIO_WS_ENGINE for engine.io\nEIO_CLIENT_WS_ENGINE for engine.io-client\n. That one i suppose: https://github.com/alexhultman/uWebSockets/issues/92.\n. It also means \"probably super faster and using much less memory\" to a lot of people. What would be the point to use another websocket implementation otherwise ?\n. Indeed !. ",
    "nicokaiser": "Yeah, you're right...\n. See my thoughts in #23 \u2013\u00a0an instance would have to\n- either allow \"remote clients\" (accept connections with an unknown session id and ask the Messenger (message queue) if there are buffered packets\n- or distribute all known clients to all instances (which would add much overhead for synchronizing)\nHowever the problem (also described in #23) would be: if we implement this consequently, the following scenario could happen:\n- Client connects to ws1.example.com, gets session id, ws1 gets the \"connection\" event and handles the socket object\n- Client uses ws2 for polling\n- So the \"Messenger\" component must send every message for this Client through the external message queue, which is quite inefficient...\n. I'll add the thoughts we discussed about \u00f8mq to this issue:\n\nIdea:\nSessions could be sticked to processes (e.g. the process that gets the initial handshake from the client is responsible for the session and handles it \u2013 \"only\" client communication is done by pubsub. With 1. however this could lead to very inefficient distribution (many clients are connected to processes that don't actually handle their requests but only forward them to a different one).\n- instead of Socket.writeBuffer, write message to queue (\u00f8mq or RabbitMQ)\n- transport is closed (timeout, close request) => delete queue\n- transport becomes writable (or another process gets a poll for an 'unknown' socket id) => queue is flushed to this client (no such queue => session id is invalid)\nAre \"persistent channels\" possible in \u00f8mq? I.e. channels you can write to with the subscribers eventually appering later? Is this even possible without a message broker like RabbitMQ?\n. @einaros Currently it looks quite good, no crashes and only moderate growth in memory consumption (apart from Node not releasing unused memory). But, I set custom \"error\" and \"timeout\" handlers to the socket before handing them to ws.handleUpgrade. The wsio.Socket does the same, so there should be no difference.\n. I'm also getting these errors with plain ws, but I thought this was because of broken clients or proxies.\nIn my case, the first WebSocket packet they sent was the header again, yet I was not able to reproduce this...\nAm 04.08.2012 um 17:25 schrieb Guillermo Rauch reply@reply.github.com:\n\n/me kills him\nOn 0.1.2 we upgraded the ws transport. @einaros do you have any guesses what could be causing this?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/LearnBoost/engine.io/issues/33#issuecomment-7502434\n. The \"reserved fields must be empty\" message is emitted by ws when a client sends invalid WebSocket data. We experience this to happen quite often, don't know if the reverse proxy or broken clients (or both) are the culprit.\n\nSo this error is very common in real life environments (and thus not an error rather a \"invalid client request\") so it should not kill the whole server.\nDo you have an 'error' handler for your socket objects? Like this:\nserver.on('connection', function (socket) {\n  socket.on('message', function () { ... });\n  socket.on('close', function () { ... });\n  socket.on('error', function (err) {\n    debug('socket error: ', err); // or just do nothing\n  });\n});\n. Ok, sounds good. And remove \"polifyfile\" from the dependencies?\n. ",
    "3rd-Eden": "@guille then you shouldn't support the websocket-js library either as it depends on having a policy file server\n. @owenb I worked on this in my stream branch: https://github.com/3rd-Eden/engine.io/tree/stream. The only that my small test reached 100% cpu and started leaking memory.. So I'm still debugging it, but it's coming. I hope that i can finish it before realtime conf starts ;p\n. @juliangruber it's already working, just need to find time to add tests to it.. See https://github.com/3rd-Eden/engine.io/tree/stream\n. @juliangruber That is correct, because it's a writeable stream\n. @juliangruber would work i guess\n. @guille but as I see it now, there is NO way in engine.io to add authentication, either as third party or as build-in. I agree that authentication might be out of scope for this project but there should be hooks where third party could add it in.\n. Sure but so far as I can see, there is also no way that socket.io could implement auth on top of engine.io because there is no way to decline a connection. It's only possible when a connection has already been made.\n. Socket.request is what you are looking for.\nspcket.request.connection.address()\nSent from my iPhone\nOn Jul 1, 2013, at 4:16 AM, larbear notifications@github.com wrote:\n\nI couldn't get anything in this issue to work. I wound up monkeypatching server.js, adding:\nsocket.remoteAddress = req.connection.remoteAddress;\nin the handshake() function, right after:\nvar socket = new Socket(id, this, transport);\nThere is some indication from other issues that this would only be needed in Node 0.10.x.\n\u2014\nReply to this email directly or view it on GitHub.\n. It seems to disappear randomly, so it's advised to just capture it early. I'm guessing that Node might make those values undefined after the initial request is closed. Or something odd like that as engine.io doesn't touch it. It just stores it.\n\nMy work around was to capture the ip address other data before i'm passing the request in to Engine.IO:\nhttps://github.com/3rd-Eden/primus/blob/master/transformers/engine.io/server.js#L46-L53\n. @guille it looks legit, but haven't tested it.\n. But the body of the request doesn't need to be JSON, it can just be plain text.\nOn Wednesday 31 October 2012 at 16:43, Fedor Indutny wrote:\n\nhttps://github.com/LearnBoost/engine.io/pull/109/files#L1R53 <- see, I'm checking what error has happened\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/LearnBoost/engine.io/pull/109#issuecomment-9949608).\n. I think that we should seize every oppertunity to optimize heartbeats as we send them so frequently \n\nOn Nov 1, 2012, at 5:30 PM, Guillermo Rauch notifications@github.com wrote:\n\nI wanted to do this a while ago, but I think Einar said it was a good idea to keep engine.io pings. I don't remember why, however.\nI'm interested in revisiting this.\n\u2014\nReply to this email directly or view it on GitHub.\n. Also: http://blog.nodejitsu.com/getting-refunds-on-open-source-projects read that\n. Gzipping data is good idea but it should indeed only be done for large responses. The hard part here is to find out how much time it would take to gzip a response and sending the response vs just sending the response. We don't really care about bandwidth but the response times here.\n. WebSockets do have compression, its implemented as sub protocols.\n\nSent from my iPhone\nOn Feb 2, 2013, at 12:51 PM, plievone notifications@github.com wrote:\n\nNginx default gzip_min_length is 20 bytes, which is clearly too low. But considering the response times, fitting responses to single MTU (1400 bytes, for example, or minimally around 500 bytes on the internet routes), can make a difference for message latency if packet retransmissions are probable for flaky connections, so it might make sense to gzip even quite small messages if they can be compressed from 4k->1k. But I presume memory usage will be more of a concern as the gzip module might create quite large buffers and in that case only really large messages should be compressed. (And as websockets don't have native compression yet, this is for long polling only...)\n\u2014\nReply to this email directly or view it on GitHub.\n. It quite hard to review these fixes if there aren't any tests written against them\n. Shouldn't this be solved before the HTTP requests constructs a websocket transport? \n. @guille Please do tell how you send query strings from the server to the client ;)\n. See https://github.com/3rd-Eden/engine.io/compare/open-data for an implementation\n. @guille A normal packet adds extra unneeded overhead and that's something you want to avoid in heavy real-time applications. If you need this custom data for every request (for example an auth token) you would have to implement a lot of custom logic for your own custom open event as you have to wait for your package to be received.\n. Socket.IO will not allow custom data handshake data either, so that doesn't solve anything. I guess i'll have to fork off then.\n. @visionmedia it's not just protocol framing overhead, it's also the overhead of the HTTP request headers as it doesn't start out with a HTTP request and the increase of pointless latency before you can start sending data because you have to wait for the message to be received by the client. All these issues and overhead is resolved by just allowing custom data to be send with the initial response. \n. Had the same issues. I got the feeling that the remoteAddress is removed after the original connection is closed. Which kinda makes sense, as the initial request is made by a polling request and closed before a connection event is triggered.\n\nI got around this by caching the remoteAddress & port on a different property on the request object; \nhttps://github.com/3rd-Eden/primus/blob/master/transformers/engine.io/server.js#L46-L53\n. Not sure if this also happening for other transports like WebSockets etc.\n. @guille leaving the server spinning because it has unanswered requests isn't good either.\n. @lemonzi @guille Implementing that kinda defeats the purpose of saying that engine.io is a low level library when you're going to implement high level abstractions like this directly in to engine.io.\nYou can just add your own library on top of engine.io to take care of the binary encoding/decoding. This is exactly what i've been doing with primus. It supports different message encoders which include EJSON, the JSON format used by meteor to transform arraybuffers, and BinaryPack for doing msgpack like encoding on binary.\n. @guille Not when you start adding another layer of base64 decoder for it\n. Why not just 0.4.x it, that solves a lot of annoyance for future updates. If you want to have a dependency lockdown, you should just npm-shrinkwrap your application IMHO.\n. Could be a race condition? Have you tried restarting the build to see if it's resolved?\n. @nodejitsu, modulus and other PAAS providers do provide sticky load balancing. But keep in mind that doesn't fix message broadcasting or sending a message to a socket that is connected on a different server. You would still need to create your own logic for this but it depends on your application's needs.\n. @AndreasMadsen You know what even funnier, you're still missing a call to server.ws.close() to properly clean up the WebSocket server ;)\n. Would be nice to see a test case against this.\n. @crzidea that's how it was designed to work, it will use polling until has a confirmation that it's actually working over websockets.\n. maybe all custom properties should be namespaced under req.engineio \n. This could probably use a test.\n. highfive\n. So basically you want to save bandwidth at the cost of latency. Creating a \"near\" realtime experience instead of a realtime experience.\n\nOn Feb 12, 2014, at 20:55, binlain notifications@github.com wrote:\nYes. And the server shouldn't flush after every message and instead wait a little for more messages to write the reply to the handshake along with more messages in one request.\n\u2014\nReply to this email directly or view it on GitHub.\n. In your test I see you're only using Engine.IO instead of Primus. I assume that primus yields the same results?\n. Could you check current master if the issue has been resolved?\n. Can you post the actual chars that caused this?\nOn Apr 9, 2014, at 23:29, James Campos notifications@github.com wrote:\nin this demo, i send 2 utf8 strings. only one is received, and is garbled. i am using engine.io 1.0.5\napp.js:\nvar engine = require('engine.io');\nvar server = engine.listen(1337);\nserver.on('connection', function(sock) {\n  sock.send('utf8 \u2014 string');\n  sock.send('utf8 \u2014 string');\n});\nindex.html:\n\n\n  var socket = eio('ws://localhost:1337');\n  socket.on('open', function () {\n    socket.on('message', function (data) {\n      console.log('data:', data);\n    });\n    socket.on('close', function () {\n      console.log('close');\n    });\n  });\n\n\u2014\nReply to this email directly or view it on GitHub.\n. Shouldn't this be reported in the Socket.IO repository instead of Engine.IO as this is a problem with the integration of Engine.IO in to Socket.IO.\n. The problem here is that you refresh while your still on a polling transport which uses timeouts to disconnect. Once this timeout expires the socket will cleaned up. So if you connect again you have 2 sockets until the timeout expires.\n\nAnd as you can see from your log and timestamps in your log you're refreshing so damn fast which cant even be considered normal behaviour.\n. -1 from me as well, this should be solved in higher level abstractions such as https://github.com/topcloud/socketcluster\n. Why doesn't Heroku update their docs to point out that real-time applications will suffer?\n. you might also want to explore zopfli for compression.\n. Just to get this straight, you would rather have a increased file size for the data that you transfer (for every single message) instead of a one time OPTIONS request? The upgrading of transports is already happening through probing so it shouldn't delay the current connection and I highly doubt that a single extra request to your sever is so heavy that it would affect performance or dramatically increases server load.\nThe suggestions solutions only add more odd edge cases on the client instead of a simple fix on the server. This doesn't make a lot of sense to me.\n. @guille But latency isn't an issue here as you are already connected using jsonp\n. Of course the pull request above doesn't help as it's just returning early now and ignoring the data that was supposed to be send. Also, it's a server side issue not a client side issue.\n. @defunctzombie you can still have your proxy handle this by turning compression off in engine.io right?\n. @neemah Just get off Heroku and use hosting provider that actually supports real-time applications (and has a load balancer that uses sticky sessions). \n. @defunctzombie sticky-session is seriously flawed as it does the sticky load balancing based on the incoming IP address. So when you run this behind another load balancer all ip's will be the same as the loadbalancers IP, causing all connections to go to one single node process. \n. HAProxy, nginx, http-proxy(node) and many others.\n\nOn Feb 13, 2015, at 11:54, Slava Tsyrulnik notifications@github.com wrote:\n@3rd-Eden i'd be very pleased if you suggest one that will handle sticky-session.\n\u2014\nReply to this email directly or view it on GitHub.\n. @STRML if Primus doesn't pick up those connection terminations it's a bug in Primus so please report that in https://github.com/primus/primus as for the disabling of ping/pong's. I would love to see that as well. \n. Actually, you don't need this context there as you already have a self variable that points to this. You can just reference that instead of slowing down the forEach loop because your required context setting.\n. Move the self.transport.close(); under the onClose so no test regression is introduced, it should stop the tests from failing and will make people press the merge button. \n. The lack of these for such critical bugs is disturbing.\n. Yes, \"tests\". Well this fixes a critical bug, yet there are no tests added to ensure that this never happens before. While the coverage for behaviors might be great, coverage for these kind of bugs is not\n. The debug package is not vulnerable for the security issues related to the ms package. The said security issue is only triggered when receiving a large (1000+ chars) incorrect input string that needs to be parsed in to a numbers. As debug uses the numbers to human readable string feature of ms it is not vulnerable. \n. @rauchg I know, I just wanted to prevent the FUD that is spreading around it.\n. There is no vulnerability in ws 1.1.0 - That advisory is poorly worded.  Engine.IO already fixed their vulnerablity here: https://github.com/socketio/engine.io/commit/ead9dfe206f60e1a2c60042d340601af9e2b7a1f & https://github.com/socketio/engine.io/commit/17ec2150c00e3e0e68fa6a490f940c2f4c8f11bc\n. Looks to me that you're fixing symptoms here instead of the actual bug, you probably want to prevent undefined's from ending up in that array in the first place.. i would lazy require this when the options.policyPort has been set.\n. that would be possible, if you wrap the require in a try/catch statement and tell users that they need to import the policy file or they are going to spam us with bug reports :p\n\nOn Friday, April 6, 2012 at 11:40 AM, Nico Kaiser wrote:\n\n\n@@ -4,6 +4,7 @@\n*/\nvar http = require('http')\n- , policyfile = require('policyfile')\n\nOk, sounds good. And remove \"polifyfile\" from the dependencies?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/LearnBoost/engine.io/pull/19/files#r654435\n. shouldn't it also emit a heart beat then?\n. Me neither, because we do emit emit heart beats and people might expect that the event is called like every 20/25 seconds or so and if you have a lot of message traffic, you would not receive any of those events. cc/ @guille \n. Well there have been reports from people who are using socket.io to know when the heartbeat is received so there are probably some usecases for it.\n. shouldn't these new dependencies be devDependencies instead of regular dependencies ?\n. This should probably be a devDependency\n. \n",
    "Gottox": "I almost forgot about this issue. :D\nI'm currently trying to reproduce this with node.js. Expect something at the weekend :)\n. Ok, seems like I've got something:\nPatch to engine.io-client which reproduces the error:\n```\ndiff --git a/lib/transports/websocket.js b/lib/transports/websocket.js\nindex f7f0788..112382c 100644\n--- a/lib/transports/websocket.js\n+++ b/lib/transports/websocket.js\n@@ -21,6 +21,7 @@ module.exports = WS;\n  */\nfunction WS (opts) {\n+  opts.query.transport = 'polling';\n   Transport.call(this, opts);\n };\n```\nServer/Client code:\n```\nvar engine = require('./engine.io')\n, server = engine.listen(3000)\n, eio = require('./engine.io-client')\n, socket = new eio.Socket({host: 'localhost', port: 3000});\nserver.on('connection', function (socket) {\n    socket.send('string');\n});\n```\n. ",
    "martintajur": "Allright, I checked my own code more throughly once more and discovered I had the handleUpgrade method bound to httpServer twice which was wrong and caused the upgrade not to actually happen. It's highly likely that this also caused Squid to go nuts.\nI fixed my code and I will go and visit that Squid-enabled network soon to see if it works now.\nHowever, I also discovered that when I was actually able to go through the upgrade (using Chrome 18), it produces a 10+ seconds lag each time right after the upgrade before the requests actually start moving. Almost like they get queued somewhere internally and then all of a sudden they start to move, or almost like it takes 10+ seconds for any websocket traffic to actually start flowing... Is this known behaviour?\n. > Allright, I checked my own code more throughly once more and discovered I had the handleUpgrade method bound to httpServer twice which was wrong and caused the upgrade not to actually happen.\nBtw, the reason I had this bound twice was that I didn't pay enough attention to the fact that when attaching Engine.IO to httpServer, it creates the handleUpgrade binding automatically. Perhaps it would make sense to build a simple check to avoid having double bindings?\n. ",
    "drauschenbach": "+1. Here's what I get in my browser:\nClient: http://myapp.cfapps.io/engine.io/default/?uid=08466758765289687168635428&transport=polling&sid=beRS_wcgNaQOvzETAAAb\nServer: 400 (Bad Request)\n. Hi Guillermo - it's been ages since Portland. And yeah, I'm still using SocketStream.\n. It turns out CloudFoundry is supporting web sockets, but on port 4443.\nHow do you configure engine.io to use a specific port for just web sockets (without messing up xhr)?\nss.ws.transport.use 'engineio', {client:{host:'localhost', port:4443}}\n. ",
    "pesho": "Guillermo, can you briefly explain why this cookie is necessary/desirable?\n. Can you please make it optional?\nI, for example, achieve stickiness by hashing the source IP. In such case the cookie is just useless traffic.\n. @guille you rock, thanks :)\n. Resolved by #390, closing. Thanks @kapouer @alexhultman @rauchg!\n. Moreover, it should specify an exact version. Not a version range.\n. ",
    "cprcrack": "Is cases like sticky load balancing or other factors outside the control of Socket.IO the only reason for the \"io\" cookie to be used? I mean, if the Socket.IO library itself does NOT use the cookie, shouldn't it be disabled by default? I just asked the question at StackOverflow but someone downvoted me, am I missing something? https://stackoverflow.com/questions/52386218/in-socket-io-what-is-the-io-cookie-used-for. ",
    "einaros": "In the last version of ws, much has been done to take care of this. I suspect that the websocket handlers in websocket.io, which are not part of the parsers imported from ws, could need similar measures.\nIdeally, as much as possible of what websocket.io is doing should be left to ws, so that fixes can land in one place rather than several :)\nOn 8. juli 2012, at 23:07, Guillermo Rauchreply@reply.github.com wrote:\n\n@kapouer good find. It seems to me that some sockets are throwing instead of emitting error events, therefore never getting properly shut down.\nWhat do you think about this assessment @einaros ?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/LearnBoost/engine.io/issues/33#issuecomment-6834802\n. I'm not entirely up to speed on how websocket.io's hybi.js uses ws. Which version of ws is running here? As I said, much has been done in the last release. If an earlier one is in use, then I am not surprised. If it is the latest version, I will have to look into how the library is being used, and see how exactly this can occur.\n\n@nicokaiser, have you seen any improvements with regards to lingering sockets using barebones ws lately?\n. Interfering proxies or client issues may cause that. I'm pretty confident a\nws client setver combo without intermediates wouldn't cause that.\nOn Aug 4, 2012 5:53 PM, \"Nico Kaiser\" \nreply@reply.github.com\nwrote:\n\nI'm also getting these errors with plain ws, but I thought this was\nbecause of broken clients or proxies.\nIn my case, the first WebSocket packet they sent was the header again, yet\nI was not able to reproduce this...\nAm 04.08.2012 um 17:25 schrieb Guillermo Rauch reply@reply.github.com:\n\n/me kills him\nOn 0.1.2 we upgraded the ws transport. @einaros do you have any guesses\nwhat could be causing this?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/LearnBoost/engine.io/issues/33#issuecomment-7502434\n\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/LearnBoost/engine.io/issues/33#issuecomment-7502629\n. \n",
    "contra": "I forgot to mention this problem only occurs on 0.8 - is there a better way to fix this?\n. Ran more tests and it seems to be an issue on 0.6, 0.7, and 0.8 - 0.9 seems to be fine though. Mocha won't run on 0.4 so I haven't checked that yet.\nFailing tests (mocha): https://secure.travis-ci.org/#!/wearefractal/protosock/jobs/1999498\nCode: http://github.com/wearefractal/protosock\nCode isn't very organized right now. I'll try to write an engine.io specific test later today.\n. Any chance of getting a new npm version for this?\n. Seems I'm an idiot - destroyUpgrade: false on attach fixed the issue. Is there any reason this defaults to true?\n. \nFocus your time on improving the way you understand vs. harassing people to make you understand.\n. Related: #203 \n. @guille \nrestify overrides the http IncomingMessage prototype and tacks on a bunch of stuff\nSpecifically this https://github.com/mcavage/node-restify/blob/master/lib/request.js#L176 was causing the breakage.\nUnrelated but this was also an issue: https://github.com/mcavage/node-restify/blob/master/lib/server.js#L250\nIt seems like they manually whitelisted socket.io to ignore from their stuff. Polling requests for engine.io were still ending up in their routing system where it was adding headers and generally being shitty. I was able to get around this by binding directly to the internal http server within restify. Open issue is at https://github.com/mcavage/node-restify/issues/575\n. I see this https://github.com/socketio/engine.io/issues/166 but it doesn't apply since there is no upgrade being attempted, the client is set to transports: ['polling']\n. Nope, js client (engine.io-client)\n. I opened this issue last year so I don't remember the details - if it had been responded to I would have been able to provide more info. My solution was to quit using engine.io FWIW\n. ",
    "EugenDueck": "Agree, but even with a stronger id generator, the check would still be necessary to stay safe.\n. I couldn't find the equivalent of a ;-) in your message - are you really THAT careless or was the ;-) implied in the \"awe\" or do I misunderstand the risk?\nDoesn't an id clash screw up an existing session and despite \"cryptographically strong id generation\" risk sending information - potentially authorization protected - to the wrong client?\nP.S. Btw I don't care whether I get the credits for this proposed incredibally awesome and sophisticated code change by being pulled in, I just think this check should be implemented one way or another.\n. Gotcha. Thanks.\n. Is this to make it harder for clients to deduce existing session ids (from a session id they got via handshake and knowledge about the generation algorithm) and take them over?\n. Somehow I must have forgotten to paste in the footnote that I refer to in the text. So I'll try to reproduce it:\n[1] This could be implemented by either one of following or some combination, potentially configurable:\n- Make the buffer bounded, and when it gets full, kick client\n- Use a ring buffer, where older entries get overwritten\n- Call buffer = conflate(buffer) when buffer is full - but this depends on conflate(buffer) returning less elements than it receives, but that could be checked, and a fallback to kick client could be implemented\n. Quoting my last statement:\n\n... depends on conflate(buffer) returning less elements than it receives\n\nThis reminds me that calling this feature conflation would probably be too narrow. If I'll come up with a better name, I'll post it here, but conceptually, this is a general feature that lets application code manipulate (conflate, augment, modify) the buffer, just before being actually sent to a client. Somewhat long for a name...\n. > I wonder if all that we should do is add a callback to send. \nThat's basically what I'm doing, except that - in my socket.io implementation that I just tested yesterday and it seems fine - I wrap the packets in a new Packet class, which holds and caches\n- the data variant of the packet (i.e. the JS object)\n- the on-the-wire variant of the packet\nIt offers methods getAsData and getAsOnTheWireOrSomething (i forgot the exact name I used). Those are the ones that are used by socket.io, and those methods will create the representation if it does not exist and cache it for reuse.\nOtherwise there'd be a lot of waste in back- and forth-encoding, especially with multicast (rooms) and broadcast.\nWill try to prepare a engine.io variant of it as soon as time permits. Question: Do you agree that the callback (provided by the app that is using socket.io/engine.io) should not see the on-the-wire representation but the JS object?\n. Alright. Will hopefully get a pull-request done later today.\nQuestion about the rough timeline. When talking engine.io 1.0, are you talking days, or weeks, or months?\n. Gotcha. One more question about the big picture: As I don't see them in engine.io: stores will stay in socket.io?\n. Indeed.\nI'm sorry this is getting even more off-topic, but in terms of the big picture, which it is good to always have when making non-trivial changes: Does (the current and/or the future) socket.io's RedisStore allow to non-sticky sessioned load balancing? (Just haven't had the time to dig into it yet)\n. Fyi, regarding my comment about \"MBWS doesn't support volatile packages\" - MBWS's solution to that problem is MBLWS, the lightweight variant of the protocol without the sequence number and resync feature. According to Mark Hapner:\n\nIf you have a mix of messages, some requiring once-and-only-once delivery QoS (MBWS) and some requiring best-effort QoS (MBLWS) then you would use a separate socket for each.\nThe HyBi working group is currently in the process of extending WS to support multiplexing of logical WS sockets over a TCP socket. Once this is available, you can have a single socket that carries both.\n. Agree, the buffers should be prevented from growing indefinitely.\n\nA timeout and/or one of the solutions proposed here would do: https://github.com/LearnBoost/engine.io/issues/45#issuecomment-7506036\n. This is not a high priority feature for me right now, will try to figure out in the future how/if this could be done in the socket.io layer.\n. Hm, I guess I messed things up by creating a new issue with this pull request, rather than attaching it to (#45) - which I have to figure out how that'd work.\n. > What do you think about passing more context to the conflater in addition to the messages?\nGood idea. Like socket.this as another param?\nAlso, for reasons of efficiency, we could consider passing an array of the wrapped messages {type: 'message', data: 'foo'}, rather than unwrapping them and passing an array of 'foo' only to have to repackage them later - in case of messages that weren't touched by the conflater at least.\n. > , that could potentially just alter the buffer.\nSounds good. Also addresses my efficiency concern in the last comment. It means, however, that the conflater sees non-message types, and has to take care of not messing those up. Fine with me, though.\n. Makes sense. Will, again,  try to do later today.\n. If you could point me to some example of such a module where I could look for inspiration (and some copy-and-paste), that'd be great.\n. Wow. Thanks! Will check it out later.\n. @guille\nLooking at the proposed interface:\nvar conflation = require('engine.io-conflation')\nengine.on('flush', conflation(function(x){ return x; });\nit could be implemented completely independent of the engine.io code base. Although it will have to know what type of objects engine.io holds in its writeBuffer, so in that sense, it is tied to the internals of engine.io and version dependent, in case engine.io decides to make a breaking change related to the writeBuffer.\n. @guille Just set up all the boilerplate and started hacking.\nMy conflation-function - stripped of all the stuff necessary to give only the high-level messages to the client we discussed (#6 of your hitchhiker's guide) - will look something like this. conflater is the fn the user of engine.io-conflation passes in.\nconflation = function (conflater) {\n  return function conflateOnFlush(socket, writeBuffer) {\n    socket.writeBuffer = conflater(writeBuffer);\n  };\n}\nIn other words, as this is an event and the result will go nowhere, rather than returning the conflated writeBuffer - which is the purely functional style I could do in the pull-request that is the base of this issue - I'm changing the writeBuffer field on the passed in socket object directly.\nI'd rather do this the functional way (which isolates me a bit from internal changes in engine.io), but I don't see how. I guess events are a one-way street.\n. The other option would be to modify the writeBuffer in-place, which isn't too clean either, but in this case, I think I actually prefer it over directly assigning to socket.writeBuffer.\n. @guille The first version is here: https://github.com/EugenDueck/engine.io-conflation But it needs version 0.2.0 of engine.io to run.\n. Can you push 0.2.0to npmjs.org? Then engine.io-conflation will be make testable etc. without manual intervention.\n. Sounds like a plan. It's nice and clean.\n. > UTF-8 is needed because sometimes socket.io is embedded in websites not served with utf8, so it defaults to the wrong thing\nI see. Well in that case I guess just copy-and-paste the headers.Connection change, rather than pulling the whole thing...\n. > @EugenDueck i called you eugen duck again, sorry.\nWell, the correct spelling would actually be D\u00fcck, but who cares... :)\n. Awesome, thanks you pulled it in despite so little comment from my side. Two comments I wanted to add:\n- I removed/replaced UTF-8 because that's not how they are implemented in JavaScript (it's either UCS2 or UTF-16, according to the spec) and you were probably thinking about how it's sent over the wire (JSON / UTF-8). For a user of engine.io it should matter primarily that any JS string will work. The on-the-wire representation is more of an implementation detail.\n- Apart from the caching aspect, this toString() guarantee is what makes my whole engine.io-conflation worthwhile on a higher level: The user can use whatever objects they actually work on, and get the exact instances back in the conflater, no need to decode JSON again into something that already existed in proper object form before. toString() will obviously cache the json (or whatever) representation to make the aforementioned broadcast / multicast scenario efficient.\nWill have to see how that'll tie in with the new socket.io code base, wrt Stores that have to do serialization, like RedisStore. Obviously, in that case the object will have to be decoded from a string, but we'll see when we get there.\n. FYI: I added a more realistic conflation example that makes use of the toString() guarantee to https://github.com/EugenDueck/engine.io-conflation (search for realistic example on that page).\nIt showcases how easy it is to use from an engine.io user's perspective, not having to deal with (de)serialization directly.\n. With pleasure: https://github.com/LearnBoost/engine.io/pull/66\n. ",
    "donpark": "+1 Given how little effort it takes to check, I have to agree with @EugenDueck.\n. ",
    "plievone": "Thanks! Yes, adding a timestamp on the url is more reliable, but if the server does not announce cache headers, in practice many will/should config timestampRequests every time, as we have no way to assume if some browser will cache GETs or not... Thanks for you quick response, will try out.\n. Works on android nicely. I'll experiment with some other phones too.\n. Works now, thanks!\n. It's available in socket.request.connection.remoteAddress?\n. Hi, socket.request works (as I use socket.request.query etc quite much to keep the browser tabs identified) but as it is a getter, the socket.request will become undefined after websocket upgrade when original transport request is cleaned up after some time. So you just have to save the socket.request or needed params from socket.request.connection etc already on connection.\n. Nginx default gzip_min_length is 20 bytes, which is clearly too low. But considering the response times, fitting responses to single MTU (1400 bytes, for example, or minimally around 500 bytes on the internet routes), can make a difference for message latency if packet retransmissions are probable for flaky connections, so it might make sense to gzip even quite small messages if they can be compressed from 4k->1k. But I presume memory usage will be more of a concern as the gzip module might create quite large buffers and in that case only really large messages should be compressed. (And as websockets don't have native compression yet, this is for long polling only...)\n. @guille, are there important failing test cases here? https://github.com/roamm/engine.io/commit/aa0209f03238b0422b73d51c7f59b4f7490edce9\n. I might have had the current master branch of the client in use. Reverting the client to 0.4.3 fixed the problem. \n. Hi, _callbacks in an object containing callbacks arrays (note the underscore), so its ok, but perhaps there is a condition in either the emitter implementation (https://github.com/component/emitter/blob/master/index.js) or emitter user (https://github.com/LearnBoost/engine.io-client/) that could allow the callbacks to have some truthy but non array value. You could move this issue to that client repo issue tracker.\n. Silly me, of course Array.prototype.indexOf is not available on IE8, it should be shimmed. See https://github.com/component/emitter/pull/21\n. I guess you closed this by accident? :)\nNow, perhaps sending multiple noop packets is intentional -- in my brief testing now the client failed to upgrade with only one packet, as it came before polling cycle started pausing and then the situation stalled until eventually ping timeout was emitted and the socket was closed. If it really is so, there might be a problem with the process here... Hopefully it is my setup (open socket, client side close it in 300 ms, then open it again in 1000 ms, then see what happens).\n. Hi @guille, thanks for this, but unfortunately now the client can miss the noop packet (it comes via polling transport, whereas probe pong comes via websocket, the ordering can change?) and stall in upgrade? Perhaps best for now would be to have a longer, 500 ms interval just in case, and trigger it also on the leading edge separately.\n. I remember client side strophe.js deferring all sends with setTimeout(fun, 0) (or some configurable amount) and also providing socket.flush() to be able to send immediately if needed. Often this saves a roundtrip when multiple rpc calls would be triggered by some user event or for example when initializing a modularized app:\nCurrently:\njs\nsocket.send(); // Would trigger a roundtrip\n/* Somewhere later in the same execution flow */\nsocket.send(); // Would be queued until response\n/* Somewhere later in the same execution flow */\nsocket.send(); // Would be queued until response\nWith deferring:\njs\nsocket.send(); // Would be queued until next tick\n/* Somewhere later in the same execution flow */\nsocket.send(); // Would be queued until next tick\n// Optionally socket.flush(); would try to flush right away\n/* Somewhere later in the same execution flow */\nsocket.send(); // Would be queued until next tick\n// Next tick all would be flushed in batch, saving roundtrip\n. ",
    "TooTallNate": "good\n. You should update it to SVG while you're at it ;)\n. Because \"I need you more tonight!\" \u2669\n. ",
    "matthewmueller": "Awesome, thanks!\n. oops, wrong lib. this is about client-side. Here's a link to it:\nhttps://github.com/LearnBoost/engine.io-client/issues/146\n. ",
    "afshinm": "Thanks @guille, I'll start implementing this one right now.\n. Tests added.\n. Thanks. Indentation problems also fixed.\n. Tests code styling also fixed with new engine.io rules. Lib files also reviewed again.\n. Unnecessary functionality removed.\n. Tests added. Hope I understood what you mean.\n. I think this new tests can cover everything we want about this feature. Please review them.\n. Thanks, Code styling problem in server.js also fixed (double \\n)\n. Rebased.\n. Wrong bug. It's a problem in my packages version. \n. Correct. I think we should rewrite this test again.\nActually I rely on timeout(s) because I didn't find correct events to write test better.\n. I used SourceTree for rebasing with master but Github Mac client for commit. So in this pull request I see last commit changes also, I think this is related to using different clients for committing and rebasing.\n. Done.\n. Ok. Please check the code again. \nVariable names changed to a simpler one and also some comments added in order to increase code readability.\n. Ops, Excuse me for that. Fixed.\n. New commits added for issue #89.\n. Ok, conflicts resolved. Please check them again.\n. Yes, Actually any transport has two type of drain  now, with sequence id and without sequence id.  When the drain emits, the listener get one parameter (sequence id) in callback and then we going to search that is there any callback function for this sequence id or not, if yes, we execute it.\n. You mean always emit an empty drain event on polling transports? (Please clarify all transports or just polling) \n. I think this is the problem of my GitHub client on Mac. I can't rollback this change, however nothing happened actually in this file and nothing changed.\n. Oh! Ok, So please let me to change the variable names. \n. Sorry. I think I have some conflicts in my github repository, let me check it again.\n. In order to keep packets length\n. Remove received packets from writeBuffer\n. If there's a new packet that we didn't send it to client. This condition occurs when transport is lock and writeBuffer is growing in memory.\n. ",
    "carlos8f": "I felt passing in custom classes was cleaner, especially fixing Server.handshake() which in engine core hardcodes the Socket class. But I ended up implementing your solution and overriding Server.handshake() since you declined the pull request. Anyway, thanks @guille !\n. So far, Safari doesn't seem to be affected.\n. I found a strange solution: the bug only happens when the client's host option is different than I am connecting to in the browser. I.e.,\njavascript\nvar socket = new eio.Socket({host: '10.0.0.115', port: this.port, transports: ['polling'], upgrade: false});\nand going to localhost:3000/ works, but going to 10.0.0.115:3000/ fails!\nedit:  xhr-polling only WORKS when the client's host option is different than I am connecting to in the browser.\n. Sorry, I meant that cross-domain is working and same-domain is actually broken.\n. I can readily reproduce this with the app below (engine.io 0.3.5)\nhttps://github.com/carlos8f/node-chat-yardstick/tree/34bc69a4560f856f1f6a20c77af20be451772580\nWhen you turn on upgrades it works, and when you specify a different host than you're connecting to, it works. But polling + same domain disconnects after 5 seconds with \"transport error\".\n. ",
    "creationix": "Just wondering, but why is a string path acceptable sugar without resorting to a full check function, but a regexp not ok?  What if we support both regexp and full check function?\n. ",
    "mmastrac": "Unfortunately that doesn't fix things - the error is an unhandled event emitted from the transport. I could try adding an error handler to the transport to patch around this, but I think there is a bug underlying this.\n. @guille Correct - and it throws an unhandled exception the second time.\n. Great- if you have a suggested patch, I can test it out with our load test. Thanks!\n. I've been using the following with haproxy and engine.io 0.3.9 (an older version, but has this changed?) with great success:\nstick-table type string len 32 size 400K expire 10m\nstick on url_param(uid)\n. Definitely...  the code is still running today and was benchmarked against 12k+ real-life connections on a major content company's site. \nThe only thing is that automated UID generation on connections seems to have been removed in this commit, which was essential to making it work properly:\nhttps://github.com/LearnBoost/engine.io-client/commit/04b3da094428374d33f4522394c8c3a0ee3cd9b6\nI haven't tested recently, but setting opts.query.uid = (something) might be a good replacement.\n. @peteruithoven -- I mentioned this in the comment above, but automatic UID generation was removed from socket.io core. You'll have to set opts.query.uid to something before your connection is made which will restore the functionality.\n. ",
    "cadorn": "See: https://github.com/LearnBoost/engine.io/pull/76\n. @guille \nHmm, this may need a bit more work. In our use-case we register many instances of engine.io at different paths/routes on the same server and are getting this warning:\n(node) warning: possible EventEmitter memory leak detected. 11 listeners added. Use emitter.setMaxListeners() to increase limit.\nTrace: \nat Socket.<anonymous> (events.js:139:15)\nat Server.handleSocket (/home/ubuntu/c9/node_modules/cloud9/node_modules/smith.io/node_modules/engine.io/lib/server.js:280:10)\nat HTTPServer.<anonymous> (/home/ubuntu/c9/node_modules/cloud9/node_modules/smith.io/node_modules/engine.io/lib/engine.io.js:160:14)\nat HTTPServer.emit (events.js:88:20)\nat TCP.onconnection (net.js:876:8)\nThe problem is here: https://github.com/LearnBoost/engine.io/blob/fefb643c4a6ff77d38571e2bc5832cd0a471f840/lib/engine.io.js#L147\nWhere many listeners are attached to the same server.\nLooks like we need one listener per server and then check routes before delegating to engine.io instance. Any suggestions on how that should best be done?\nI would be happy to take a stab at it.\n. Alright, all fixed. Let me know what needs to change for this to land.\n. > It looks good, but attach needs a refactor methinks.\nIt does not look pretty but kind of all belongs into attach. Not sure how to refactor.\n\nAlso, why setMaxListeners(1000) ?\n\nWe attach many engine.io instances at different paths to same server. If no setMaxListeners we get (node) warning: possible EventEmitter memory leak detected. 11 listeners added.. Using large number as it should not throw warning even if many instances.\n. Not 100% sure but >1k possibly as high as 5k.\nWe could make it a config option?\n. I suppose its a limitation of our wrapper.\nWe would need the path along with the socket for engine.on(\"connection\") events: https://github.com/c9/smith.io/blob/master/server-plugin/plugin.js#L29\nCan that be done? I have not looked at the engine.io code too closely.\nWith that change we could lower setMaxListeners to maybe 100 so that multiple engine.io instances per server are still supported in case you want 12+ independent namespaces.\n. Ok, I'll fix our wrapper and set the limit to 100.\n. We currently only load balance based on the path. I suppose that can be updated to look at the query string as well.\n. I'll fix our wrapper to create only one engine.io instance and use the resource instead of path to register a dynamic route. We can then land this and you can make the path immutable.\n. Feels much cleaner. Made the path already immutable as it was easier.\n. I can do this but it will take some time.\n. Hmm. Seems to be fine for me. Any way I can reproduce your environment? What is the failure message?\n. Created separate issues.\n. Want me to do that?\n. Oops.\n. Fixed.\n. There is still something up. Looking at it further but looks like it is in our abstraction.\n. Ok, all good now (pushed fix to client).\n. I did not have time, will do next week.\n. One test was failing which is fixed now and the fix makes sense to me.\nI am not sure what other tests we need. They seem to cover everything. The primary change I made concerned the way the logic of the interval & timer was tied. The fact that the existing tests still pass seem to be a good indicator that this seems to be an improvement as the code makes sense now when following the flow of the connection lifecycle.\n. The test I had to fix did not set the pingInterval which is set too high by default to illicit the behaviour the test was looking for. I think the pingInterval should have been set all along. The test passes with the modification before applying this pull request. I think we are good here.\nLooking into adding a test to cover this pull request.\n. Added three tests that pass now while failing before.\nAlso, I think we should update the defaults to:\nthis.pingTimeout = opts.pingTimeout || 10000;    # was 60000\nthis.pingInterval = opts.pingInterval || 25000;\nWhat is the reasoning behind the previous defaults?\n. The pingTimeout now refers to the time it can take for the client to receive a pong. If a request does not go through within 10 seconds it is likely it will not come through.\n. > do we have a test for the client closing on the absence of a server ping within (interval + timeout) ?\nAdded.\n. For continuation see: #84\n. > Also, since you're now sending the pingInterval in the handshake we need to update SPEC in the engine.io-client repo and bump up the revision to 2, since clients will be rendered incompatible.\nThinking about this reminded me of an observation I made when writing this patch.\nWhy are pings initiated by the server?\nThis patch provides the following:\nserver on `pingInterval` send `ping` and expect `pong` within `pingTimeout` or emit `ping timeout`\nclient on `ping` send `pong` and expect ack `pong` within `pingTimeout` or emit `ping timeout`\nclient expect `ping` within every `pingInterval + pingTimeout` or emit `ping timeout`\nBy necessity to accomplish pingInterval + pingTimeout timeout on client, server must send an ack pong packet (this is new). Instead of the ack pong packet we cloud look for clean transport request close after sending pong packet on client but that does not ensure server is getting our responses.\nIf pings were initiated by client we could get rid of the ack pong packet (good especially for polling) and still timeout within pingInterval + pingTimeout on client:\nclient on `pingInterval` send `ping` and expect `pong` within `pingTimeout` or emit `ping timeout`\nserver on `ping` send `pong` and expect next `ping` within `pingInterval + pingTimeout`\nThis would mean more changes to the SPEC and client implementations but is a cleaner and lighter lifecycle. I am leaning towards this approach as I typically always initiate pings on clients.\n. Done here & LearnBoost/engine.io-client#51\nAll tests still pass! (other than two minor adjustments)\nNow only need to update SPEC.\n. See #115 for spec.\n. > an external piece of software/hardware, like a load balancer, should be able to analyze an incoming request and say \"this is an engine.io request\"\nThis is nice but I wonder how practical it is in practice. I don't think anybody is going to hardcode a load-balancer route with /engine.io and distribute it without making it configurable. Load balancer routes are typically under the control of the organization running the system and thus configurable. Any third party service will most likely add a configurable route (especially in the case where an alternative engine.io SPEC implementation is used and mounted to /alternative.io).\nFor discoverability I would much rather prefer something like:\n/interfaces.json ~ {\n  \"/alternative.io\": {\n    \"spec\": \"https://github.com/LearnBoost/engine.io-client/blob/0.2.2/SPEC.md\"\n  }\n}\nThis elevates discoverability to a common place (to be standardized) and allows system authors to structure their URI namespace as they wish.\nEverything can default to /engine.io and you will get the results you are looking for by default. If a system author chooses to change the prefix they understand that they may no longer be compatible with certain configurations & services (which as I mentioned will likely offer config options anyway). Telling this system author that he cannot use engine.io unless he prefixes with /engine.io will likely get a I'll monkey-patch or use X library instead response.\n\nI'd like to understand why the path being configurable is a desirable thing in your setup.\n\nWe mount everything to /<user>/<project>/<service>. engine.io would be the only exception to this and while possible introduces extra code and URL routing in our setup which is not desirable.\nPlaying the devil's advocate: Who are you to tell me how to configure my URI namespace? I know what I am doing, am fully aware of the consequences and don't give a crap about anyone else!\nFrom experience, trying to push standardization from the POV of one project never works. In this case I see the only out an approach equivalent to /interfaces.json that is supported by a larger group of implementers.\n. > 1 url fragment\nWho are you to force me to use a specific URI scheme to mount my services to?\n\n2 query string\n\nNo objections. Anything after <path>/<resource>/ is domain of engine.io.\n\ninterfaces.json\n\nYou would request /interfaces.json and look for your spec https://github.com/LearnBoost/engine.io-client to determine the route to watch.\n. So resource goes and only path is left? So PR #76 needs to work on the path again?\n. Let's call the option resource then. I find that path confuses people as they think of a filesystem path.\n. Alright, mount it is.\n. @guille FYI, I am away from the office next week.\n. Hmm. We don't check the sid on the server.\n. I need to know on the client 100% of the time when the server instance has changed so I can re-init the UI.\nHow does the client currently know that the server instance has changed?\n. > the request being load balanced to a new server different from the handshaking one\nRight. I need to know this fact on the client 100% of the time.\n. > Are you saying that you need to distinctively tell when this happened? If so, we should maybe switch from 500 to something else, then handle it appropriately on the transport, and then fire close with a different error code (like unknown session id).\nThat should work.\nWhat is your objection to a pingPayload? It could also be used to send regular status updates to client without incurring extra request.\n. It does it now on the ping response. So I should rather say pongPayload. We could make the pongPayload option an optional function so you can send different data for each ping.\n. > It seems to me that an event that emits with the packet object reference so that you can modify it at will would be better?\nThat is how the pongPacket works now. I can modify that object and it gets sent with the next heartbeat. This is a bit of a side-effect but is surely simpler than a function.\nSo you closed this PR. Does that mean you are against this pongPacket functionality?\n. Ah, ok, now I get it. Yes, that would solve the issue.\nI'll re-work this PR.\n. Fails with ws@0.4.20 and ws@0.4.22.\n. Any idea what is going on? Should I add the other permutations of this test?\nWhat happens now?\n. They can be increased as long as the test continues to fail.\n. LearnBoost/engine.io#97 is the solution I thought of as well but was not sure if that was in fact the issue.\nThe only problem with it is that if anything is relying on a regular heartbeat event it may not trigger for a while if the server continues to stream data. Then again, the heartbeat is there to see if connection is still live and if we are receiving messages it is so in theory there should be no problem delaying it. I am ok with that.\n. This is done after LearnBoost/engine.io-client#61 lands.\n. Oops.\n. I think so. I'll have to take a look.\n. Is this the sec now? https://github.com/LearnBoost/engine.io-protocol\nWas there not another doc? The change we made back then was to the ping/pong logic to improve stability and reduce requests.\n. Feel free to refactor as long as you don't break the tests ;)\n. ",
    "owenb": "Hey Arnout\nI recall you tweeting about this last week. Has the code made it into master yet, or were you proposing an API?\nIt would be great to have this.\n. Great news :) I will hopefully be able to try out your branch before NodeDublin. Thanks!\n. Thanks!\n. @plievone The API has changed in the last few months and this no longer works.\nAny idea where this info is buried? I'm guessing somewhere within socket.transport but it's not leaping out...\n. Nope, fraid not. This used to work:\njs\nio.on('connection', function(socket) {\n  socket.on('message', function(msg){\n    console.log(socket.request) // = undefined\n  }\n}\n. Thanks @plievone Got this working now.\n. ",
    "juliangruber": "@3rd-Eden maybe have a look at https://github.com/substack/shoe for a working implementation based on sockjs which should be easy to adapt\n. https://github.com/dominictarr/stream-spec\n. @3rd-Eden I see it still emits 'message' instead of 'data'. So you won't be able to pipe!\n. In your code the stream is readable...and why not make it emit message and data?\n. Hm I don't really get why you would implement only half of the stream interface, what is the win?\n. ",
    "Raynos": "\n100% Node.JS core style\n\nYou can't claim to use node.js core style if it's not a stream. \n. \nhttps://github.com/Raynos/engine.io-stream\nFeel free to port or inline that into engine.io. Or tell people that want a stream to just use that.\n. Trailing whitespace, yes. Text editor >_>;\n. Remove it of course.\n. ",
    "impronunciable": "Does this make sense? https://github.com/danzajdband/engine.io/blob/master/lib/server.js#L83\n. ",
    "gkostov": "I also think that verification must be asynchronous for few reasons:\n- it allows to be used as the plugin point for connection brokerage (through Redis, IPC, etc.) to make clustering of servers a simpler process so we'll have true scaling ability (setting up a sticky LB is not always possible),\n- there will be no significant performance penalty compared to the synchronous call,\n- it will not cause issues with the execution flow (there are many async calls already involved),\n- it is simple to do.\n. ",
    "JosephJNK": "This was my bad.  We were using Sauce Labs for cross-browser testing and it seems to be a problem on their end-- local installations of Firefox are working fine.\n. ",
    "larbear": "I couldn't get anything in this issue to work, trying everything I could think of in the open and message/data event handlers. I wound up monkeypatching server.js, adding:\nsocket.remoteAddress = req.connection.remoteAddress;\nin the handshake() function, right after:\nvar socket = new Socket(id, this, transport);\nThere is some indication from other issues that this would only be needed in Node 0.10.x.\n. ",
    "aops": "Unfortunately address() returns the address of the server. However, I did find socket.request.connection._peername does contain the remote IP address in the open event (sometimes).\n. ",
    "lukeburns": "@larbear, grab the address right after opening each socket:\nengine.on('connection', function(socket) {\n  socket.ip = socket.request.connection.remoteAddress;\n  ...\n}\nNot sure why request information disappears with upgrading transports, but I believe that preserves the address after upgrade.\n. ",
    "ruxkor": "I just implemented a naive approach for this, but am unsure if this is the right way to do it (hence no pull request).\n. could you explain why you removed this statement?\n. you're right, this line is indeed obsolete in engine.io; I mistook this diff for the client when I wrote my comment, sorry about that.\n. ",
    "mjgil": "Hey guys, I just ran this with my own txt file and was wondering if this was the error to be fixed.\nstream.js:66\n    dest.end();\n         ^\nTypeError: Object # has no method 'end'\n    at onend (stream.js:66:10)\n    at EventEmitter.emit (events.js:126:20)\n    at afterRead (fs.js:1330:12)\n    at Object.wrapper as oncomplete\n. ",
    "defunctzombie": "Was this fixed? Has the issue gone away?\n. Based on some recent commits I think this is resolved and should be closed. If there are issues discovered they should be new issues with failing cases.\n. still relevant?\n. Yes, that is the spec now. Closing this since any new issue can be opened in the appropriate place.\n. @plievone Is this still an open issue?\n. Is this PR still relevant?\n. The referenced PR was merged if that means anything towards closing this issue.\n. I think this is no longer relevant because of how we build the dist file or how browserify packages.\n. Can be closed as far as I can tell.\n. Could you explain this PR and why it can't be done any other way except with support in engine.io ?\n. Have you tried DNS round robin or simply having separate DNS names? a.example.tld, b.example.tld ? Load balancing is not a one size fits all solution as there are many ways to attack this problem. It seems that many of the PRs and issues raised on this repo about load balancing have to do with using a single proxy server for the connection that then proxies to various workers on the backend. There are many ways to even have this particular setup (IP sharding, headers, cookies) and they depend on your needs and possibly imposed requirements. It may be useful just to create a wiki page with various examples on how to load balance under different circumstances. I am not confident a library patch is going to solve this.\n. @crzidea depends on your setup :) It will always depend on your setup. If you want to hop on #socket.io and ping me about it, I am happy to brainstorm ideas.\n. @mokesmokes can you provide a patch?\n. Where do the docs live?\n. Added some more documentation about this to the readme as well as which methods I felt are [Recommended] vs which are not.\n. Because it was only put in to be backwards compatible with current usage. I\nthink being clear about what is happening is better than terseness in this\ncase. We could remove those recommendations since they are opinions anyway.\nTechnically if something isn't recommended we shouldn't even have api for\nit.\nOn Feb 3, 2014 5:13 PM, \"Guillermo Rauch\" notifications@github.com wrote:\n\nWhy is it not recommended?\nOn Mon, Feb 3, 2014 at 2:05 PM, Roman Shtylman <notifications@github.com\n\nwrote:\nAdded some more documentation about this to the readme as well as which\nmethods I felt are [Recommended] vs which are not.\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/LearnBoost/engine.io/pull/213#issuecomment-34005978>\n.\n\n\nGuillermo Rauch\nCloudup CTO\nhttp://devthought.com\n\nReply to this email directly or view it on GitHubhttps://github.com/LearnBoost/engine.io/pull/213#issuecomment-34006810\n.\n. Yes, except for the case I actually opened the issue for which was to\ncreate a server with no attachment so you can attach later.\nOn Feb 3, 2014 7:18 PM, \"Guillermo Rauch\" notifications@github.com wrote:\nThat's a really good point. In this case I think there's something to be\nsaid about the convenience of being able to require('engine.io')(server)and be done with things tho.\n\nReply to this email directly or view it on GitHubhttps://github.com/LearnBoost/engine.io/pull/213#issuecomment-34017371\n.\n. @guille which syntax? `require('engine.io')(http_server) ?\n\nI like the var eio = require('engine.io')(); eio.attach(http_server) approach for the flexibility it provides in having a server object before attaching.\nLet me know what format is desired and I will update the readme.\n. Is there still something to do on this PR?\n. Updated the PR. Removed the Recommended words.\n. Added my comments to the PR. Need to also make sure that all tests are passing. Not sure why we don't get tests running on PR's now but we might want to look into that.\n. Now it LGTM. Just squash commits :)\n@guille I think we should have a \"how to contribute\" section that talks about squashing commits in PRs. I will often see PRs get commits piled on to fix the comments which can turn a simple 1 commit PR into many commits and makes the history noisy. The intermediate commits are not useful in the end.\n. :thumbsup: \n. -1 Just put it in your global git ignore settings\n. Not everyone uses osx :) Every git project doesn't need a contribute guide about this. Learning tools is always going to be part of the process with using tools.\n. This is not the best place for such questions. The real way to find out is to try it and do some benchmarks for your particular needs.\n. Closing for inactivity in responding to followup.\n. #213 will be a better way to do this.\n. I favor PR #213 over this approach :)\n. Support for this will not be added back in via redis (this adds more complexity to the app to maintain for those that do not have this problem).\nOne way to possibly solve this nicely is to provide a hook into the pre-handled but parsed request that allows the developer to bypass engine.io handling the message and maybe pass it off to some other engine instance. I am not familiar enough with the internals to know off the top of my head if such a hook is possible without a major re-organization.\nYes, deploying on heroku is popular but their lack of sticky support is also a choice. There are many tradeoffs in what platform you select and what you are able to accomplish with that platform. It is unreasonable to expect that a project support any arbitrary technology decision of another company (but certainly popular technologies do get more votes and eyeballs for support). I think the best course of action for you @tjsail33 is to create a PR (or several) that shows some possible ways your problem can be fixed. The reason you are not getting much love in this thread is because none of the maintainers or top contributors have this problem personally and thus will not likely set out to fix it (the code for it was removed because it caused other problems and it was decided that maintaining it was not worth it in the core).\n. I am tired of the useless back and forth. We know this issue is contended by both sides and if a solution is presented we will evaluate it. We do not need to talk about heroku any more without seeing some actual code.\n. @rase- issue still exists? /cc @rauchg you recently ran into something ipv6 related?\n. I believe #299 supersedes this PR. Let me know if that is not the case. \n. This makes the second point text start on the same line as the heading. Should be changed to keep current formatting and just fix the numbering.\n. fixed manually\n. These questions are better asked on stackoverflow and not this issue/bug tracker.\n. No issues using latest version that I was able to find. If you still see an issue, please explain how to reproduce.\n. Issue and discussion belongs in engine.io-protocol where we could create and maintain generic server tests. I am happy to review PRs for this.\nUntil then, a good test is to spin up an engine.io server and have your implementation talk to it. I know that is not ideal as it makes the implementation the \"protocol\" but that is what we have for now.\n. There are two issues here. We should be supporting OPTIONS unconditionally so that even if the browser makes a pre-flight it doesn't break the existing session stickiness. And we should also see if we can make engine.io not do the pre-flight request at all.\n. Why don't we want this happening silently? I would think we want this to just work without messing about with settings.\n. We should fix this. OPTIONS preflights can happen when doing CORS stuff and this behavior is really annoying to an end user to deal with. It basically renders this module unusable behind the amazon ELB even when sticky sessions are on. I do not think this has anything to do with careless coding and is happening under the simplest uses of this module.\n. For anyone that runs into this before we fix it. You can work around it by using the manual request handling (on request from the http server) and responding to OPTIONS yourself.\n. Mildly related: https://github.com/Automattic/engine.io/issues/300\n. I am closing this as it seems no agreement is reached without potentially breaking something. Unless something is specifically broken here that we need to fix, I don't see an issue.\n. Closing for lack of activity from OP.\n. Closing since some advice was provided.\n. Please see the engine.io-protocol repo for documentation\n. @nkzawa Why wouldn't we just let the proxy handle this?\n. Can confirm this issue. What is even stranger is that if you use a cluster size of 2 there is no problem but with 3 ore more the problem starts to happen.\n. Interesting find, but the reason this is happening is the same reason we require sticky sessions on load balancers when running engine.io servers.\nThe reason for the xhr poll error is because the different poll requests are being sent to different cluster backends. Each cluster backend is a separate nodejs process and does not share memory with the other process. What happens is that the session is established with the first request (and session id assigned) but future requests get routed to a different process which does not know about the session id.\nFurther, the actual error from the response is being masked by the 'xhr poll error' hardcoded string. Upon inspecting the responseText, the following message is shown:\n{\"code\":1,\"message\":\"Session ID unknown\"}\nThis is an amusing way to expose the fact that we require sticky sessions so that requests can be routed to the correct backend that is aware of active session ids.\nIf you want to use cluster, you will need an adapter on top of engine.io server that will share session ids and session data between servers or avoid using cluster and instead run multiple separate processes behind a load balancer which supports sticky sessions.\nI think we should update our README/docs/guide to mention that cluster should be avoided due to this limitation. We should also pass along the response text error so that debugging this is easier in the future.\n. Additional references: https://github.com/indutny/sticky-session\n(tho it may not work 100%, but a good starting point for an engine.io-cluster-support module)\n. @3rd-Eden yep, that is why we don't recommend it outright\n. @3rd-Eden there are problems with using amazon as well since their ELB doesn't support HTTP 1.1 so you have to pick between having websockets (tcp load balance) or polling (http with sticky).\n. This issue does not have enough information to help us debug. It is possible your server has a memory issue or some other problem. Without a failing client/server we can run and load balance or a much more detailed reason for the failure we cannot debug this.\n. How is this optional for the attach method? If you don't provide something to attach to... then attach can't do anything.\n. Yea, But that would be documented under the Server constructor docs right? For this attach call it is required. At least that is how the code currently is.\n. good catch, fixed and removed the handleSocket docs as well since that no longer exists\n. ",
    "AlexeyKupershtokh": "FF 16.0.2 on the same machine:\n[05:51:27.849] NS_ERROR_MALFORMED_URI: The URI is malformed @ http://localhost:3000/engine.io-dev.js:1778\n. wicked@wnote:~/Alawar$ npm install engine.io\n...\nwicked@wnote:~/Alawar$ cd node_modules/engine.io/examples/latency/\nwicked@wnote:~/Alawar/node_modules/engine.io/examples/latency$ node index.js\nlistening on localhost:3000\nThen I opened http://localhost:3000/ and http://127.0.0.1:3000/ and it didn't work in both cases.\nThe problem also remains if I clone the git repo.\nThe node is 0.8.14 from Chris Lee deb-repository.\n. Also if you're going to update it, consider to fix this code as well:\njavascript\nserver.listen(process.env.PORT || 3000, function(){\n  console.log('\\033[96mlistening on localhost:3000 \\033[39m');\n});\nIt always shows the 3000 even if I start it with different port.\n. ",
    "indutny": "yeah, agreed. it's just hard to check it in tests...\n. https://github.com/LearnBoost/engine.io/pull/109/files#L1R53 <- see, I'm checking what error has happened\n. @guille force pushed, will open PR for engine.io-client soon\n. Any update?\n. Nvm, wrong pull request target.\n. huh?\n. Man, that's really a github UI bug... It wasn't intended to go here :)\n. May be, I dunno...\n. ",
    "Jxck": "ah, I see.\n. ",
    "ghost": "Unfortunately i could not install packet analyser on my workstation.\nStrange behavior of engine.io transport \"probe\" protocol with antivirus. \nIncoming \"pong\" packets was delivered to client correctly, but \"message\" packets was not. \nThey are was eaten by antivirus buffer completely.\nBut some testing and patching was solved my problem.\n1. Changed \"probe\" protocol protects from false connection upgrading in case of tested kind of buffering.\n2. Base64 encoding of packets allows to use websockets with web shields of tested antivirus (Kaspersky).\n   Without this encoding no false upgrading happens, websocket transport correctly goes down and polling works.\nChanged files was sent for your consideration to rauchg@gmail.com \nNice day.\n. No, it was pure 7bit ascii .\nAll of \"message\" packets was eaten (e.g. in my modified latency example without base64 encoding of packets query (HELO) message was delivered to server but the answer (EHLO) message was unable to reach client). \nIn original example it was on \"upgrade\" phase, so client was paused \"polling\" transport and tried to switch to broken \"websocket\" .\nFor people who are interested in the details, options of Kaspersky antivirus that broke websocket transport was:\n\"anti-fishing\" and \"anti-banner\" in web-antivirus preferences.\n. It seems that this is not an error in my code as I've check it in another project.\n. Hey,\nThanks for that it works :)\n. ",
    "thalesfsp": "Guille... i tryied #irc and stackoverflow and noone answer... can u please help?\n. @visionmedia Lol, about this problem #1430.... \n\"We called an Apple expert and he confirmed that Safari on the iPad is very... annoying. Apple added a lot of security features to the Safari on the iPad (as I experienced before) and there are no workarounds. We either have to make an app or use another device since it works flawless on desktops and even on my Android phone. I've also found Opera for iPad, though I'm not sure if that helps. Thank you SO much for all your help. Instead of hugging I will just give the bounty to you! Well deserved! \u2013 Tim S. Jun 7 at 7:25\"\nI'm troll? auehauehauhauehauehae! U are stupid. Read more.\nAbout Engine.IO. ISSUES:\nIncomplete and not very well organized Doc.\nLack of examples that explore a set of different scenarios.\nBut i think that u don't have it in mind ;)\n. Well man, i read and before came here i try IRC and other ways. How to end this stupid topic:\nPossible answers to my question:\n- \"It's not possible\" - 5 seconds to write\n- \"Its possible and after i will try to put an example\" - because the reported scenario is very common (express 3 and E.IO)\n- \"Yes, because EIO is in an early stage we still don't have a good reference doc, but still watch us\" - I will love EIO\n- \"I maintain 39075304895734805 projects\" - Sorry, but that's your problem. I learn that i become responsible (levy, receive suggestions and criticism, adviced) for the things I produce. #nocoments...\nThis issue is crazy of my mind?\nNo. Where u talk about ssl? May my browser CRTL + F don't find in your page\nWhats mr. Google say about:\nhttps://www.google.com.br/search?client=safari&rls=en&q=engine.io+guide&ie=UTF-8&oe=UTF-8&redir_esc=&ei=R7jAUPugNovo8gS9lYHwDw\nhttps://www.google.com.br/search?client=safari&rls=en&q=engine.io+tutorial&ie=UTF-8&oe=UTF-8&redir_esc=&ei=WrjAUMj0NoaQ9gStjIG4Aw\nhttps://www.google.com.br/search?client=safari&rls=en&q=engine.io+example&ie=UTF-8&oe=UTF-8&redir_esc=&ei=bLjAUJegCIr29gTMuIHwDQ\nWhy u don't fix/correct the problem?\nIf i know how to, it's obviously that i NEVER came here to stress me like this.\nThanks\n. One example and I'm still don't find SSL and the rest of i described :) Guille, thanks man, but this will get us nowhere. After tons of search (unnecessary waste of time) I finally find help in the Forrst community, whew!\n. #nocoments\n. ",
    "tj": "troll alert https://github.com/visionmedia/express/issues/1430\n. framing overhead sounds pretty negligible to me, not sure that really warrants invading a higher level envelope just to shave a few bytes\n. fair enough!\n. ",
    "roamm": "@guille I fixed this and added 2 more test cases, in a new pull request.\n. Actually I don't use engine.io in production but I do use socket.io. And I plan to use engine.io for next version with our homemade communication mechanism between nodes. I started reading the code a couple of weeks ago and to fix potential bugs (not necessary true) according to my understanding. I can explain my thoughts here.\n. BTW, the self.flush() change in socket.js might not be correct depending on the original purpose.\nI'll comment in line.\n. Comments added.\n. I broke this into smaller pieces and added 7 test cases.\n. @guille Actually this has nothing to do with #148 , it's about #132 .\n. If we don't specify websocket, it defaults to use polling, which is already covered in the following test case.\n. Because I don't see it's used or referenced.\n. This to make it safe to call close() as many times as you wish. Right now it might be guaranteed in Socket, I think it's better to check here to avoid subtle problem in future.\n. Polling transport doesn't have an 'open' readyStatus and it's indeed referenced in maybeUpgrade() in socket.js.\nWebSocket has not 'open' readyStatus either and it should have although it's never referenced.\n. This is needed because polling connection may break due to network issues. We need to clean it up.\n. Just a sanity check\n. It's better to clean up the polling connection. And actually it's better to end the connection actively in case of a malfunctioning client that doesn't close the polling connection after sending a close packet.\n. Need to trigger close event for a forced close.\n. It's set false in req.cleanup.\n. send() will handle this and trigger close event\n. I did a full search and found no 'client error'\n. So is 'server close'. And add a missing reason 'forced close'\n. @guille We may still want to flush in the upgraded connection, right? If so, I'll revert this part.\n. send() or flush() doesn't necessarily really flush writeBuffer to the underlying transport, so data may get lost.\n. For websocket, close() immediately invokes the close method of the underlying socket. And at the time, this.writable might be false, causing the send() or flush() buffers the data only in this.writeBuffer, not into the underlying socket.\n. ",
    "aeosynth": "ok, but pingIntervalTimer still looks useless\n. @guille ping? i've been using this in production, and have stopped receiving complaints about connection errors\n. Go for it.\nOn Jul 8, 2013 7:48 PM, \"Conrad Pankoff\" notifications@github.com wrote:\n\nAny action on this? I've just found that the same thing happens on a\nparticular wireless operator in Australia, owing to their use of a\ntransparent proxy implemented with squid. If there are no current plans to\nwrite tests and get this merged, I'd be willing to wade around in whatever\nthe testing bits are here and figure it out, but it would probably take me\na lot longer than it'd take someone who's already familiar with engine.iointernals.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/LearnBoost/engine.io/pull/177#issuecomment-20650313\n.\n. i did - the utf8 long dash (unicode #8212)\n. http://stackoverflow.com/questions/6965107/converting-between-strings-and-arraybuffers\nhttp://updates.html5rocks.com/2012/06/How-to-convert-ArrayBuffer-to-and-from-String\n\nusing Uint16Array instead of Uint8Array would help, but would still not work for 3 and 4 byte characters\nperhaps a dedicated utf8 encoder / decoder should be used, such as utf8.js\n. using the same demo, and engine.io 1.2.1, i get a new error in chrome: Uncaught RangeError: Invalid string length engine.io.js:2773\n. ",
    "sweetieSong": "Think I'm rebased.\n. What's the expected behavior of connection after sending the payload?\n. The errors.out and non-server from common.js are now extracted.\n. errors.txt is the realtime output of the cloud test\n. This is old code that wasn't rebased properly\n. We had decided earlier that the errors produced at realtime would be directed to errors.txt instead of displaying on the terminal. This was due to the way that the terminal was always drawn so that the errors would be overwritten.\n. We followed the conventions for common.js when it had global for all other functions declared there.\n. done\n. ",
    "xixixao": "\n_ I think we should only emit \"end\" on server close, transport close\n\nNode Stream emits either error or end, but right now in Socket most errors go through onClose without onError (that's only one type of error) and therefore they won't be reported to Stream API as errors, only as ends.\nI would have to do a \"switch\" depending on the reason string or rather rewrite the errors to go through a separate handler.\nQuestion is how important is to report these as errors (ping timeout, client error, parse error).\nAlso, as I noted, emitting error before close breaks some of the error tests:  \n1) server close should trigger if a poll request is ongoing and the underlying\n   socket closes, as in a browser tab close:\n  Error: poll connection closed prematurely\n  at XHR.Transport.onError (EngineIO\\lib\\transport.js:78:15)\n\n\nsocket.io 'close' clashes with node api\n\nnvm\n. Ok, let me add it to this request.\n. @mikermcneil I haven't found the time to update this to the new node stream API. If anyone does find the time, this is one of the things that are missing for socket.io 1.0 release.\n. Close does not close the immediately, so no data will be lost.\n. ",
    "ilsken": "+1, this would be really useful especially if you want to do things like have a dnode client/server over engine.io\nIf anyone is looking for this functionality now it looks like this package provides a shim for it \n. ",
    "mikermcneil": "Sexy- let me know if I can help.\n. @alecbenson @rauchg Thanks!  I'll keep an eye out for the publish for engine.io and socket.io (it's pinned).\nBut in the mean time, for the record: this vulnerability doesn't actually affect Socket.io or Engine.io.  That's because Engine.io (and therefore Socket.io) is not using accepts (and therefore negotiator) to check for accepted languages.  In other words, this is the same story as with the compression dependency in Sails and Connect (which @dougwilson verified this morning.)\n. ",
    "chirag04": "try setting 'sync disconnect on unload' to true when connecting via socket.io client.\nSomething like this:\nvar options = {\n  'sync disconnect on unload':false\n};\nsocket = io.connect(url, options);\nDoc here: https://github.com/LearnBoost/socket.io/wiki/Configuring-Socket.IO\nLet me know if this works for you.\n. ",
    "alexvorobyov": "I'm experiencing the same issue. The problem is that when request have no Connection: \"upgrade\" header the upgrade is not happening. So server's handleRequest method is trying to pass this request with transport=websocket in query string to existing polling transport and it fails to handle it.\n. ",
    "c4milo": "I'm for this improvement as well, it definitely will reduce latency in high load scenarios. For example, TCP Fast Open is a similar technique in that it aims to reduce roundtrips by sending data along with the first SYN packet. My 2 cents.\n. @guille o.O I'm organizing it as well! \n. Looking forward to see you there @guille!\n. @guille this PR seems legit to me. Wouldn't leaving the socket open cause more harm than good? \n. Nodejs cleans it up after 2 minutes which is the timeout by default for idle sockets. If you don't close the socket and your server has too many concurrent connections, you might run out of file descriptors too soon, even if your clients are closing the connections upon disconnections.  Another scenario, from of the top of my head, that might not work reliably is when you want to support presence, you might see clients connected when in reality they are not.\n. close(true) looks good to me. But IMHO, true should be the default value. The reason being is that once you wrap your network stack with engine.io, the more intuitive behavior is to use it to manage your connection lifecycle as well.  If a browser really wants to re-use existing connections it would do it despite your actions with the actual user facing API. Chrome, for example, has an internal pool of sockets and it does re-use them despite of what you do with the API**. Additionally, remember that not all the clients for an engine.io service are browsers. \n**You can test it by yourself using chrome://net-internals/#sockets\n. sure, I just don't think that default behavior is intuitive and legit for non xhr-polling cases. If anything, it will be seen as an issue instead.\n. Anyway, I'm fine with having it as an opt-in :) Thank you @guille \n. ",
    "dominictarr": "Oh, thanks, that should work - so engine.io will perserve the querystring across all fallbacks?\nIt's not the problem of adding the servers, but telling the server the set of ids it should care about - there could be an arbitrary amount - and I just want to proxy it on with consistent hashing, (for example)\n. cool, thanks!\n. why not use the real Stream#pipe function from node? - a lot of development work went into getting it right.\n. ",
    "nullbox": "I wrote a small failing test case for this\nhttps://github.com/nullbox/engine.io/compare/master...issue-171\n. It seems like I can't reproduce the rising clientsCount with this test https://github.com/nullbox/engine.io/compare/master...stress-test\nBut it sure eats all the memory\n. ",
    "davidhcummings": "Does this issue belong in engine.io-client?\n. @brishin yeah, looks like 5 instances each of .on() and .once(). There's also an instance in lib/util.js and many more in the various lib/transports\n. This PR contains the fixes to issue 168 proposed in pull 177. Also includes a test for the fix that will fail on unfixed Engine.IO code.\n. As stated in the Goals, most web proxies do not support websockets. The expected behavior for Engine.IO should therefore be to failover to polling (or rather, never upgrade to websockets) if a proxy is present, even when the browser specifically requests to upgrade.\n. Confirmed to work with Squid as well!\n\n. ",
    "brishin": "Follow up: it seems like very little code depends on events internally in the client code: https://github.com/LearnBoost/engine.io-client/blob/master/lib/socket.js\n. @davidhcummings Right, but there are only 3(?) instances of using the Emitter on the Socket object.\nI don't think that the lib/transports are important because users shouldn't be messing with the transport objects anyway.\n. ",
    "konstantinzolotarev": "Faced with same issue. \nI understand that it's not a real issue but mistake in code that based on engine.io. \nBut if I need to remove all close listeners from socket using socket.removeAllListeners('close') it will cause a memory leak into application. Because server.clients would be never cleaned and connection amount wouldn't reduce after some socket closes connection.\nBecause of this: https://github.com/socketio/engine.io/blob/master/lib/server.js#L334\nThis block would never be reached. Might be it makes sense to split internal events and events that developers could rely on ? \nFor example for internal use could be created internal-close event but for usage normal close event would be fired ?. And another similar issue:\n```\n/var/app/current/node_modules/engine.io/lib/transport.js:88\n    var err = new Error(msg);\n              ^\nRangeError: Maximum call stack size exceeded\n    at new Error (native)\n    at WebSocket.Transport.onError (/var/app/current/node_modules/engine.io/lib/transport.js:88:15)\n    at emitOne (events.js:96:13)\n    at WebSocket.emit (events.js:188:7)\n    at WebSocket.close (/var/app/current/node_modules/ws/lib/WebSocket.js:134:10)\n    at WebSocket.doClose (/var/app/current/node_modules/engine.io/lib/transports/websocket.js:132:15)\n    at WebSocket.Transport.close (/var/app/current/node_modules/engine.io/lib/transport.js:75:8)\n    at Socket.clearTransport (/var/app/current/node_modules/engine.io/lib/socket.js:284:18)\n    at Socket.onClose (/var/app/current/node_modules/engine.io/lib/socket.js:310:10)\n    at Socket.onError (/var/app/current/node_modules/engine.io/lib/socket.js:120:8)\n``. @lpinca I think this issue could be closed. \nBecause all future conversation and problems should be into websockets/ws#758. Yep will do tomorrow. @darrachequesne I'll check version thank you.\nYep I know that I'm able to enable debug mode. but it gives totally useless messages. \nAnother issue with it that when I'm enabling debug messages socket.io starts to print all messages it receive/send. It would be nice if there would be several log levels. \nExample: \n -level one- received/sent messages\n -level two- transport close details (something more thantransport closed`)\nSome more examples: \nhttps://github.com/socketio/engine.io/blob/master/lib/socket.js#L102 - here if socket receive pack with error - it might contain some useful information in packet. But message will be parse error. And developers will need to handle packet event, and recheck  type == 'error' in handler. \nhttps://github.com/socketio/engine.io/blob/master/lib/socket.js#L190\nhttps://github.com/socketio/engine.io/blob/master/lib/socket.js#L217 - this 2 lines wouldn't provide any information about why transport closed. it means developer have to clone repo and patch it.\nSame with forced close message. It sends from several different places. And you have to guess where system closed connection.\n. @darrachequesne I'll try. but not sure about available free time )\n. @darrachequesne I created an sample PR with idea. https://github.com/socketio/engine.io/pull/496\nPls take a look if you are ok with this approach I'll finish with other parts of the system. Related to #488 . @darrachequesne Yep make sense. Will continue then. This PR Could be closed.. I was not sure but in https://github.com/socketio/engine.io/blob/master/lib/server.js#L299 \nSame statement wrapped in try..catch so I did same.. ",
    "darrachequesne": "Closed due to inactivity, please reopen if needed.. Closing due to inactivity, please reopen if needed.\n. Has this been fixed yet?\n\n\nOPTIONS turned off by default on engine.io server side\nBy default forceBase64 for XHR if client detects CORS scenario (WS is of course fine)\nAdd a forceBinaryXHRCors flag (default=false) which will disable (2) above, thus the browser will emit OPTIONS requests, and the developer will need to turn on the OPTIONS flag server-side.\n\n\n. > What I did in #484, would allow the handshake to work, but the session would still not be right.\n@MiLk could you please explain why that wouldn't work? Is there something we can fix?\n. @MiLk maybe a Access-Control-Allow-Methods: GET, POST is missing then?. What I meant is adding Access-Control-Allow-Methods: GET, POST would remove the OPTIONS request in 3/, right?\n\nOPTIONS /socket.io/?EIO=3&transport=polling&t=... (allow GET and POST, not only GET)\nGET /socket.io/?EIO=3&transport=polling&t=...\nGET /socket.io/?EIO=3&transport=polling&t=...&sid=...\nPOST /socket.io/?EIO=3&transport=polling&t=...&sid=.... @julianlam I didn't see any PR open for that matter, did you find a solution?\n. Closed as duplicate of https://github.com/socketio/engine.io/issues/279. Closed due to inactivity. Please reopen if needed.\n. Has this been fixed yet?. Closed due to inactivity, please reopen if needed.\n. PR is welcome!. @lpinca You're right, pings are sent by the client. But a timer is set on both the client and the server (allowing to detect any unavailability from the other side), isn't that the case with primus?\n. PR is welcome!\n. Closing due to inactivity, please reopen if needed.\n. @miguelgrinberg @calzoneman what do you suggest to fix that issue? should we remove utf8.encode/decode calls in engine.io-parser?. What's weird is that, in engine.io-parser, both encodePacket and decodePacket methods (which are used by the polling transport) provide an utf8(en|de)code option, but encodePayload and  decodePayload methods always set it to true:\n\n```js\nexports.encodePacket = function (packet, supportsBinary, utf8encode, callback) {\n  ...\n  if (undefined !== packet.data) {\n    encoded += utf8encode ? utf8.encode(String(packet.data)) : String(packet.data);\n  }\n  ...\n}\nexports.encodePayload = function (packets, supportsBinary, callback) {\n  ...\n    exports.encodePacket(packet, supportsBinary, true, function(message) {\n      doneCallback(null, setLengthHeader(message));\n    });\n  ...\n};\n```\nutf8encode was added in this commit: https://github.com/socketio/engine.io-parser/commit/95840ca6b6d393d9f9140b105405809094af643b\n. I think the difference between both transports is that polling uses parser.(en|de)codePayload whereas websocket uses parser.(en|de)codePacket:\njs\nWebSocket.prototype.send = function (packets) {\n  packets.forEach(function (packet) {\n    parser.encodePacket(packet, self.supportsBinary, function (data) {\n  ... // utf8encode = undefined\n};\n// when\nPolling.prototype.send = function (packets) {\n  ...\n  parser.encodePayload(packets, this.supportsBinary, function (data) {\n  ... // encodePayload then calls encodePacket with utf8encode = true\n};\nAnd here:\njs\nexports.encodePacket = function (packet, supportsBinary, utf8encode, callback) {\n  ...\n  // data fragment is optional\n  if (undefined !== packet.data) {\n    encoded += utf8encode ? utf8.encode(String(packet.data)) : String(packet.data);\n  }\n  ..\n};\nThe question being, is utf8encode flag even needed?. @buunguyen sorry for the delay. Please feel free to open a PR for that feature!\n. Closed by https://github.com/socketio/engine.io/pull/305\n. Closed due to inactivity, please reopen if needed.. Hi! That's what I'd found too, you committed faster than me :smile: \n:+1: \n. Please re-open if needed. Thanks!\n. Could you please provide a sample test case for this? Shouldn't the error be caught in the try...catch?\n. Closing due to inactivity, please reopen if needed.\n. @digawp how about using something like https://github.com/Flet/semistandard? Won't it be easier to maintain?\n. Thanks a lot for the hard work, btw!\n. Merged as https://github.com/socketio/engine.io/commit/7cbdd5e5d9aedc2aec25663ba3cf1b7a78a494e5.\nThanks a lot!\n. Shouldn't this.ws.close() be surrounded  with try .. catch?\n. @kapouer thanks!\n. Closing, as this is now fixed in sails.io.js.\n. Hi! I could reproduce the clientsCount rising with only one client, yet the pingTimeout gets triggered after 60s and then the clientsCount decreases:\nclientsCount++ > 1\nclientsCount-- > 0\nclientsCount++ > 1 // \"slow\" reload\nclientsCount-- > 0\nclientsCount++ > 1\nclientsCount-- > 0\nclientsCount++ > 1\nclientsCount++ > 2\nclientsCount++ > 3\nclientsCount++ > 4\nclientsCount++ > 5\n// ~ 60s (only one browser opened)\nclientsCount-- > 4\nclientsCount-- > 3\nclientsCount-- > 2\nclientsCount-- > 1\n// (closing browser)\nclientsCount-- > 0\nDo you confirm that behaviour?\n. RangeError: Maximum call stack size exceeded can happen when the array this.packetsFn is too big (too many arguments):\njs\n[].push.apply([], new Array(200000));\n// throws 'RangeError: Maximum call stack size exceeded'\nIf it is actually the reason, it can be fixed with a for loop:\njs\nthis.sentCallbackFn.push.apply(this.sentCallbackFn, this.packetsFn);\n// becomes\nfor (var i = 0; i < this.packetsFn.length; i++) {\n  this.sentCallbackFn.push(this.packetsFn[i]);\n};\nRelated: https://github.com/socketio/socket.io/issues/2589\n. Closed by 298cb6fd6f1e6ab63c2a3fb5f5d762643c138135.\nThanks!\n. Closed by https://github.com/socketio/engine.io/commit/330526a491b8fee58e7223fe874fcaa8cac1409c\n. Closed by https://github.com/socketio/engine.io/pull/451.. Closed by 5301695c93529ea0509b1bb09e6fa4ed80a9bc9b.\nThanks!\n. Closed by 5301695c93529ea0509b1bb09e6fa4ed80a9bc9b.\nThanks!\n. Closed by 959c8be0cc0aa6e1be4d5021f1d83b56a157943f\n. Thanks!\n. Closed by https://github.com/socketio/engine.io/pull/461.. Hi! Sorry for the delay. Couldn't you use socket.io-client? (reconnection attempts are managed here). Closed due to inactivity.. Please open an issue here: https://github.com/socketio/socket.io-client-java\n. @MoLow thanks for the huge work! That is indeed a very interesting feature.\nFirst, if we want to merge this, I think we should make it work a bit like socket.io-adapter and socket.io-redis, with a default in-memory storage as noted by @zbigg. That'll need a bit of testing too :smile: \nNote: I'm still trying the assess impact on socket.io above\n. @MoLow I think what @zbigg suggested is that the socket's writeBuffer, storing the packets waiting to be sent to the client, should also be persisted in Redis.\n. @MoLow @zbigg I wonder whether this is even possible. I mean, handshake and buffer could indeed be shared between servers, but what about messages sent from the client, which can eventually reach any server? In that case, where should the on('<event>') be triggered?\n. Closed due to inactivity, please reopen if needed.. Closed by 5301695c93529ea0509b1bb09e6fa4ed80a9bc9b.\nThanks!\n. Doesn't that issue rather belong to engine.io-client-java?\nBesides, the maven dependency seem to be up-to-date now:\njava\n    <dependency>\n      <groupId>com.squareup.okhttp3</groupId>\n      <artifactId>okhttp-ws</artifactId>\n      <version>3.4.1</version>\n    </dependency>\n. @alex3d hi! Could you provide the said POC please? Besides, any idea how to fix?\n. Thanks, I could reproduce with the POC you provided (which is basically just a client sending big buffers very quickly).\nSomehow, couldn't that issue be \"easily\" mitigated with both the maxPayload option (which is the maximum allowed message size in bytes, ref) and some rate-limiter which will limit the number of bytes per second a given socket can send (and close it)?. @Dynalon that's an interesting idea indeed! Would you have time to send a pull request?. If I understand correctly, you want to limit the total number of bytes in the writeBuffer array, right? (here). Closed by https://github.com/socketio/engine.io/commit/330526a491b8fee58e7223fe874fcaa8cac1409c\n. You're right, the default behaviour is to generate a new id on reconnection. You can provide your own generateId method though:\njs\nvar engine = require('engine.io');\nengine.generateId = <your custom function>\nPlease see the sample test case.. @dlimkin could you please fix the conflicts and document the cookieHttpOnly option please?\n. Merged as 60780e2fb7c7ce8e6a990f65fe55e4334daec615 and https://github.com/socketio/engine.io/pull/441. Thanks a lot!\n. @hairyhenderson Hi, thanks for the PR! I think the dependencies are pinned to avoid API breakage.\n. @hairyhenderson yep, you're right, something like greenkeeper would certainly be useful here.\nClosing this for now!\n. I don't see any particular reason. PR is welcome!\n. Closed due to inactivity, please reopen if needed.. Hi! The internals of socket.io has indeed changed between 1.3.7 and 1.4.5, the sockets object is now an hash instead of an array.\nYou should be able to go through the said hash to retrieve your user_id:\njs\nfor (var socket in io.sockets.sockets) {\n  if (socket.user_id == \"myid\") return socket;\n}\nOr, if your user_ids are unique, you could have another hash with user_id as key and socket.id as value:\njs\nvar socketId = userHash[\"myid\"];\nvar socket = io.sockets.sockets[socketId];\n. @leonard-sxy the option wsEngine should enable you to use the uws engine, doesn't it? Please note that you can also set the env variable EIO_WS_ENGINE.\n. Please reopen if needed.\n. @kapouer let's do that! could you please open a PR?. Thanks!\n. @QingYong where did you get the packet from? It seems that issue belongs to engine.io-java, doesn't it?\n. Closed due to inactivity, please reopen if needed.. It does indeed look like a bug (the change itself: https://github.com/socketio/engine.io/pull/393)\nI guess the line delete this.ws; should be removed, since this.ws can be reused later.\n. @kaansoral PR is welcome!\n. Closed due to inactivity, please reopen if needed.. @perrin4869 thanks! Can you indeed add the test you're suggesting please?\n. @perrin4869 thanks!\n. @3rd-Eden my reasoning is that when using socket.send('a'), callback arg is undefined, right?\njs\nSocket.prototype.send =\nSocket.prototype.write = function (data, options, callback) {\n  this.sendPacket('message', data, options, callback);\n  return this;\n};. @3rd-Eden or have I misunderstood your comment?. Steps to reproduce:\njs\nvar opts = { allowUpgrades: false, transports: ['websocket'] };\nvar engine = listen(opts, function (port) {\n  var socket = new eioc.Socket('ws://localhost:%d'.s(port), { transports: ['websocket'] });\n  engine.on('connection', function (conn) {\n    for (var i = 0; i < 200000; i++) {\n      conn.send('a');\n    }\n  });\n});\n. http.ServerRequest indeed refers to Node.js v0.6/v0.8.\nCould you also update the jsdoc here https://github.com/socketio/engine.io/blob/master/lib/server.js please?. @DominikPalo friendly ping. Merged as https://github.com/socketio/engine.io/commit/8450d03f0659ef0e3d9eaecf89563d963be6b06a. Thanks!. How about adding a new error code, named REQUEST_DENIED (or something like that), which will be used when allowRequest fails and in that case the CORS headers would not be added? Would that make any sense?. You mean, something like the following?\njs\nServer.prototype.verify = function (req, upgrade, fn) {\n(...)\n  } else {\n  // handshake is GET only\n  if ('GET' !== req.method) return fn(Server.errors.BAD_HANDSHAKE_METHOD, false);\n  if (!this.allowRequest) return fn(null, true);\n  return this.allowRequest(req, function (err, success) {\n    fn(err ? Server.errors.REQUEST_DENIED : null, success);\n  });\n);\n. Actually, it seems the error code is used in the response (https://github.com/socketio/engine.io/issues/259)\nI'm wondering what the best way to handle that case is. Should every error message triggered by allowRequest be without CORS headers? Should we add yet another flag for that?\nRelated: https://github.com/socketio/engine.io/issues/211 & https://github.com/socketio/engine.io/issues/281. cc @akamensky @jdolega. Hi, thanks for that PR! I made some minor comments, could you please check them?\nDid you see any other area of improvement?. Thanks!. cc @julianlam @beatjoerg @vostrik\n. Also, the test for uws seems to fail:\n\n. @alexhultman I guess it was implemented like node.js native net.Server, where you can close the server and then listen again. What do you think?\n. @kapouer @alexhultman thanks!. Hi @truongsinh, sorry for the delay! That indeed looks promising, I'll be happy to merge your pull request for that feature.\nI'm afraid you'll have to dig a bit into engine.io internals though :muscle: . @hearsh I'm afraid no work has been done on our side yet, I don't know if @truongsinh has had some time to dedicate to it.. Hi! That should be fixed in 1.8.2. Related: https://github.com/socketio/socket.io/issues/2779. @Matt90o thanks!. I manually restarted the test, oddly it seems to pass now.\n. Closing, as it seems the issue was related to npm.. Good catch! Would you mind opening a PR for that?. Closed by https://github.com/socketio/engine.io/pull/469.. Thanks!. > https://github.com/socketio/engine.io/tree/1.8.2/lib/transports#L118\nThe line does not seem valid, could you please check?. Do you know where that TypeError: Cannot read property 'displayName' of undefined is thrown?\nThe self.onError('write error', err.stack); is used here:\njs\nTransport.prototype.onError = function (msg, desc) {\n  if (this.listeners('error').length) {\n    var err = new Error(msg);\n    err.type = 'TransportError';\n    err.description = desc;\n    this.emit('error', err);\n  } else {\n    debug('ignored transport error %s (%s)', msg, desc);\n  }\n};\n. Closed due to inactivity, please reopen if needed.. Question: Should it rather be an async callback? I was thinking to use it that way in Socket.IO:\ninitialMessage: '0' ('0' being the encoded form of the CONNECT packet, for the default namespace /). Closed by https://github.com/socketio/engine.io/pull/501.. @lpinca thanks a lot!. Closed by https://github.com/socketio/engine.io/pull/480, thanks for the report.. @paulrobello I'll be happy to merge your pull request.. Released as 1.8.3 and 2.0.2.\n@lpinca thanks a lot!. Done, thanks! (again). server.close does close all clients, but it does not close the underlying http server. To properly stop it, you should use engine.httpServer.close().. Closed due to inactivity, please reopen if needed.. @arden could you please open a pull request for that? I didn't dig into the release log, any incompatible change?. Closed by https://github.com/socketio/engine.io/pull/489.\n@kapouer thanks for the details :+1: . Thanks for the pull request!\nWhat do you think of a handlePreflightRequest option, which could be either a boolean (what you're currently suggesting) or a function allowing the user to properly handle those OPTIONS requests?\njs\nvar io = require('socket.io')(server, {\n  handlePreflightRequest: false,\n  // or\n  handlePreflightRequest: (res, req) => {\n    // set the proper headers\n  } \n});\n. @MiLk thanks a lot!. How about:\njs\nthis.generateId(req, function (id) {\n  // ...\n});. Which version of engine.io are you using? Could this be related to https://github.com/socketio/engine.io/issues/283 (fixed in 1.8.2 by https://github.com/socketio/engine.io/pull/458)?\nRegarding the debug message, you should be able to see the engine.io logs with DEBUG=engine node index.js, does it help? Else, do you know what we could improve here?. @konstantinzolotarev could you please open a pull request with the change you're suggesting?. Any idea why the build would fail?. @eordano thanks!. Is the debugError actually needed in lib\\server.js? I think it might make sense in lib\\socket.js, in order to split received/sent messages (how about naming it trace()?) from transport details. What do you think?. @kievechua thanks!. @kapouer what's the point of versioning then?. The first argument of allowRequest is an IncomingMessage, so you should be able to get the IP address of the request with:\njs\nfunction allowRequest(req, next) {\n  let ip = req.headers['x-forwarded-for'] || req.connection.remoteAddress;\n}\n . See https://github.com/socketio/engine.io/pull/506.. May I ask what is your use case?. Couldn't you add the info to the req object, and then access it later in the connect handler? (the request property of the socket, reference). Or am I missing something?. @pensierinmusica let's keep it the way it currently is then, if you don't mind. Thanks for the pull request!. Good question! It seems that was decided a long time ago (https://github.com/socketio/engine.io/commit/44fd1b97a1ff6abe619f0cd88be03406ede4daf3 and https://github.com/socketio/engine.io/commit/587cb14972258434bf559715c49a4fad5031096f) but I've found no reason to choose it over uuid (I wasn't on the project back then).\n. Please open a pull request over there: https://github.com/DefinitelyTyped/DefinitelyTyped. @flenter may I ask what use case you have in mind (which would not be covered by the allowRequest option)?. Merged! Sorry for the delay... Hi! I think the pull request contains unrelated changes, could you edit it please?\nAlso, do you have a case where those undefined properties are actually accessed?. Good catch, thanks! https://github.com/socketio/engine.io/pull/531 should fix the issue.. @olegmdev yes, the failure seems unrelated to the change. Could you add a test for that behavior please?. Closed by https://github.com/socketio/engine.io/pull/535.. @SijieDingEsna May I ask what your use-case is? Couldn't you simply specify { transports: ['websocket'] } on the client?. Closed due to inactivity, please reopen if needed.. @dennisat you should be able to send a Buffer without any configuration:\n```js\nvar engine = require('.');\nvar server = engine.listen(3000);\nserver.on('connection', function(socket){\n  socket.send('utf 8 string');\n  socket.send(new Buffer([0, 1, 2, 3, 4, 5])); // binary data\n});\nvar socket = require('engine.io-client')('ws://localhost:3000');\nsocket.on('message', function(data){\n  console.log(data);\n  // prints:\n  // utf 8 string\n  // \n});\n``. Closed by https://github.com/socketio/engine.io/pull/545. The failure was not related touws.. @st0ck53y thanks!. @nightwing nice, thanks for the clear explanation!. Closed by #529 . Hi! Couldn't we remove that line instead?req.connection.destroy();should trigger anonClose` event, right? I mean:\n```\n  function cleanup () {\n-   chunks = isBinary ? new Buffer(0) : '';\n    req.removeListener('data', onData);\n    req.removeListener('end', onEnd);\n    req.removeListener('close', onClose);\n-   self.dataReq = self.dataRes = null;\n+   self.dataReq = self.dataRes = chunks = null;\n  }\nfunction onClose () {\n    cleanup();\n    self.onError('data request connection closed prematurely');\n  }\nfunction onData (data) {\n    var contentLength;\n    if (typeof data === 'string') {\n      chunks += data;\n      contentLength = Buffer.byteLength(chunks);\n    } else {\n      chunks = Buffer.concat([chunks, data]);\n      contentLength = chunks.length;\n    }\nif (contentLength > self.maxHttpBufferSize) {\n\n\nchunks = isBinary ? new Buffer(0) : '';\n      req.connection.destroy();\n    }\n  }\n``\n. Well that makes sense, thanks!. @efkan merged, thanks!. @efkan I'm currently working on releasing a new version of socket.io, but yes both of your propositions (asyncgenerateIdand https://github.com/socketio/engine.io/issues/538) should be included in versionengine.io@4.0.0`, sorry for the delay.\n\n. @efkan I'll cherry-pick it.. @Donutttt @mtrabelsi uws is not the default websocket engine anymore (since 3.2.0)\nSo by default, require('socket.io')(80); will now use ws.\nThe performance of the ws package can be improved with two native libs (which are not included by default):\nnpm i --save utf-8-validate bufferutil\nTo use uws, you have to install it and select it as wsEngine:\nnpm i --save uws\n// and then\nvar io = require('socket.io')(80, { wsEngine: 'uws' });\nHope that clears things up!. Thanks! Sorry for the delay.. Closed by https://github.com/socketio/engine.io/pull/543. Thanks for the heads-up!. @lpinca thanks!. Closed by 77ca65273fa4da67e804e36a41d3593e9d6e7a87.. Good catch, thanks!. Thanks!. You're welcome!. Thanks!. @shapel thanks!. Thanks for the pull request, could you please review my comment? I'll try to fix the issue with json-polling.. Hmm... it seems you are looking for socket.handshake.address (ref) :\njs\nio.on('connection', (socket) => {\n  let ip = socket.handshake.address;\n  // ...\n});\nAs usual, if you find ways to improve the documentation, please do open a PR! :+1: . socket.handshake.address should actually be equal to the remoteAddress attribute:\n\nhttps://github.com/socketio/engine.io/blob/3.3.2/lib/socket.js#L37\nhttps://github.com/socketio/socket.io/blob/2.2.0/lib/socket.js#L124. Hi! I haven't had the time to check the cws module yet, but that looks promising. Are you able to connect with the client?. Well that's a good question! The code was last updated here: https://github.com/socketio/engine.io/commit/a70dcaa9210a8a7b314ceca755f7547297e2478b\n\nIf I'm not mistaken, no origin means the request was not sent by a browser. We could maybe replace it with:\njs\nif (req.method === 'OPTIONS' && req.headers.origin) { \n  headers['Access-Control-Allow-Credentials'] = 'true'; \n  headers['Access-Control-Allow-Origin'] = req.headers.origin; \n}\nWhat is your take on this?. From that commit https://github.com/socketio/engine.io/commit/d74f93e4f927b29f2948116b2d218518dc08feea, I'd say it should default to 1000.\n. I understand the purpose here, but won't the comment seem a bit weird since the line delete this.ws; will be deleted?\n. How about renaming that method send, to better reflect its purpose?. Same h\u00e8re, how about something like onEnd?. Shouldn't it be !== here?. According to the comment above // keep require('ws') as separate expression for packers (browserify, etc), shouldn't it rather be wsModule = require('ws'); here?\nRelated: https://github.com/socketio/engine.io/pull/418. Shouldn't the \"uws\" in dev dependencies be removed?. Is this try..catch needed here? Is there a particular issue?. How about callback(null, base64id.generateId());, so that errors can also be handled (see https://github.com/socketio/engine.io/pull/518)?. @ChALkeR \n```\n\neslint lib/ test/ *.js\n\n~/engine.io/lib/transports/polling.js\n  165:27  error  'new Buffer()' was deprecated since v6. Use 'Buffer.alloc()' or 'Buffer.from()' (use 'https://www.npmjs.com/package/safe-buffer' for '<4.5.0') instead  node/no-deprecated-api\n``. Next question then :smile: : do you suggest adding yet another dependency (safe-buffer), or losing support for Node.js<4.5.0?. Couldn't it be replaced byBuffer.from(upgradeHead)` directly?. ",
    "alaa-eddine": "@guille client and server are on different processes, I'll try tomorrow with a single process and post back the result here.\n. sorry for the delay. \ntested with server and client on the same process, and I have the same behaviour !\nwhre in your code do you set socket.request.connection.remoteAddress ?\n. @3rd-Eden this is what I was looking for ! I'll give it a try thank's :)\nbtw, why you dont submit a pull request to @guille  ?\n. ",
    "jigneshnavsoft": "i am not getting socket.request.connection.remoteAddress\n. ",
    "prawnsalad": "Any update on this? I'm having the issue here, crippling some users of the Kiwi IRC nodejs project.\nUsers report it happening when behind a reverse proxy, but I can't see any common factor between the users that the issue occurs with. request.remoteAddress is undefined.\nIs there any other way for us to get the clients IP?\n. Now this issue is closed, what is the recommended way to get the remote clients address? Just closing it doesn't help anybody with the issue.\nDo we have to manually manage remoteAddress ourselves as @3rd-Eden has gone about it? If so, it seems a very round about way just to get a clients address for a web service.\n. ",
    "rase-": "Based on the conversation, I think this could be closed.\n. engine.io-protocol pull request: https://github.com/LearnBoost/engine.io-protocol/pull/15\nengine-client pull request: LearnBoost/engine.io-client#244\n. Thanks for the great comments. I'll make the convention changes. Also I completely agree about doing the detection for encoding binary data in the parser. There is no Blob or BlobBuilder in Node is there? It should, however, be possible to send those from a browser client (which in the current implementation is not possible).\n. Thanks for the great comments again. I'll make the style changes. Hmm yes, that value put to the prototype is just a default value, which is set to false in handshaking if needed. I should change that to not be set in the prototype at all.\n. We can close this, since #227 is merged, I think.\n. :+1: \n. It is the right way around. Imo you should not connect clients and servers of different versions.\n. In version 1.0 binary xhr2 binary support is used, unless the client informs in the handshake that it's only capably of b64 encoded xhr.\n. I don't really follow the suggestion you gave. 1.0 has a lot of changes, and if you don't run version 1.0 both on the server and client, there's trouble ahead.\n. The best thing to do is to keep the client and the server of the same version so that you don't get unexpected behavious just because of that.\n. Oh I get it, you're talking about updating everything without any downtime?\n. Yeah I get it. I just misinterpreted what you were saying before. \n. Maybe @guille has some good advice for how to deal with the issue.\n. Hmm weird. I seem to be unable to reproduce this when using IE 8 and a modern.ie VM (XP).\n. Moving this to socket.io\n. Cache busting should be in use: https://github.com/LearnBoost/engine.io-client/blob/master/lib/transports/polling.js#L222\n. I think it's better this way than directly adding this support to engine.io (like in my last PR), since engine.io is supposed to be a low-level library. What do you think @guille?\nIf this is how you would like to have it, I can add documentation for it.\n. https://github.com/LearnBoost/engine.io-parser/pull/19 should've taken care of this\n. Not sure, but might be problematic: https://github.com/LearnBoost/engine.io/pull/241\n. Thanks for the feedback @lorxu. I'll close in favor of #213.\n. Fix merged in #254\n. Thanks a ton @pawelatomic!\n. @aeosynth thanks for reporting. https://github.com/Automattic/engine.io-parser/issues/24 should take care of this issue. :) https://cloudup.com/cdKWuhwskKy\n. Thanks @ismarslomic for the observation! @verrier, could you see if this is the case for you too? That the hanging sockets would still close after a while? Or do they definitely hang forever?\n. Great. Thanks @verrier, @3rd-Eden and @ismarslomic!\n. @ismarslomic yes, due to what @3rd-Eden perfectly described above.\n. Seen any lately @laino?\n. :+1:\n. Fix merged for parse in https://github.com/Automattic/engine.io-client/issues/329.\n. @defunctzombie I think we still need to update the way we handle the output from parseuri in engine.io-client\n. True. Will move it.\n. Moved.\n. What's your situation with this @ben-ng?\n. @ben-ng I can look into getting some tests in if you like.\n. Thanks for reporting! Can you share anything more about your setup? Like the socket.io version you're using, and possibly a code sample with which we could try to reproduce the behavior?\n. The encoding doesn't happen on WebSockets. The reason we do so when polling, is that we interpret the response as binary, making it possible to receive binary or a string at any given time. We need to encode multibyte characters so that they can still be decoded back to their original value on the receiving end. \n. Thanks @reem!\n. Should be addressed by https://github.com/Automattic/engine.io-client/pull/324\n. Adding some more tests after initial review.\n. policyFile isn't here anymore, is it?\n. Perfect!\n. Oops. Too much WordPress.\n. Fixed.\n. ",
    "kaansoral": "I've created this feature request: #443\nReally wish, after 3+ years, there would be an easy solution\n. I added this console.log(this.socket._socket.remoteAddress); to engine.io/transports/websocket.js\nAnd sadly the remoteAddress doesn't change there, the value is the initial value, so I can't think of a solution, without diving much deeper into the workings of the \"ws\"/websocket\n. I would say it's needed, but don't know about the doability, I've tried achieving it for a long duration but seemed impossible at high level (diving into the ws library, maybe they didn't deem it important to keep track of the IP)\nMy solution was to have a separate HTTP periodic routine and an individual rotating key that each client needs to take from the socket, and periodically deliver via HTTP, where the IP is accessible\nOtherwise, a player with a dynamic IP, could connect to the server 100's of times, and the server would think there are 100's of unique clients. ",
    "deoxxa": "Any action on this? I've just found that the same thing happens on a particular wireless operator in Australia, owing to their use of a transparent proxy implemented with squid. If there are no current plans to write tests and get this merged, I'd be willing to wade around in whatever the testing bits are here and figure it out, but it would probably take me a lot longer than it'd take someone who's already familiar with engine.io internals.\n. ",
    "mokesmokes": "@guille - perhaps I'm missing something but in this case the client sent a 'close' packet - thus the client's intent is pretty much clear - no? It just seems like the benefit of keeping the socket open versus closing it is rather small - i.e. the real-time issue pointed out by @c4milo, memory consumption, etc. Thoughts?\n. @guille - here's a scenario: in a mobile app (both Android & iOS) - the app frequently voluntarily disconnects. For example - any time the home button is clicked, screen locked, etc. This is a requirement since any I/O in this state may crash the app. We then reconnect when the app is re-focused. So potentially there will be loads of zombie sockets hanging around. Perhaps the solution is to have the standard 'close' as you have today, but also add an additional 'hardClose' client command - which can be used when required? (e.g. the scenario described above)\n. I hope we have a deal ;-)\n. @kapouer - it should be left up to the developer. You're right that if the code is engine.listen then the default should be true, but also in cases where it's attached to the http server it may be true (application dependent) - e.g. my scenario above (which is what I'm doing BTW).\n. Closed by https://github.com/LearnBoost/engine.io/pull/217\n. Why should scalability be inherently built into engine.io? Scale architecture is best done if specific to the application and infrastructure. For example - most Node.js PAAS providers today provide session affinity, including websockets, so all your connections are maintained versus a specific server. Some providers (e.g. Modulus.io) also give you the capability to target specific servers for your requests. So all you have to do is at initial client connection do a DB lookup as to which server your client should target from now on, and then presto - all traffic goes to that specific server, no need for Redis traffic on every message. Load balance the initial request yourself! This will have much higher performance than any stock solution. See https://modulus.io/codex/projects/scaling , for example.\n. @s0s0s0 - bottom line is you go with a PAAS provider who enables this, or you roll your own solution with node-http-proxy or bouncy, etc.\n. @3rd-Eden - sticky load balancing is not enough. The common example of users in chat rooms, ideally you want to keep each room entirely on a server. So sticky balancing is required, but you also need server targeting (at least for the initial request - after that it is \"sticky\"). This solves many scenarios - if you can partition clients to servers, and then perhaps \"move\" them later - e.g. they change chat rooms (send a new \"server target\" request which again becomes sticky). For more complex scenarios you may need some message bus, but in many (perhaps most?) cases not. But you do need the two features I mentioned, at a minimum, in the load balancer. \n. Fix suggestion: Not clearing the timers in Socket.prototype.close should do it. The timers will be cleaned up later if the close succeeds, or expire, as in the scenario above.\n. PR works, tested websockets and polling sockets abruptly disconnecting.\n. Test added to  PR. Fails with timeout on 0.7.10, passes with fix.\n. What would you like to see in the test case? I counted eight server-side connection close() tests - those should continue to pass with this fix.\n. The test will timeout without the fix - try it. With the fix, after close() is called, and a polling request is unavailable and never arrives on this socket, the close event will eventually be fired due to a timer expiring.\n. The way the close cycle works in the code, today, is that the event is fired only when the close is sent to to client - not before. So all I did was keep the timers in place. If there is no \"send\" scenario (i.e. no polling request available)- the close will occur due to the timers expiring.\nFrankly - I'm not a fan of this approach. But if this is the architecture, then you can't clear the timers when calling close(), and the reason for the close event will be the eventual timeout - but this is a purely semantic difference. The important thing is that a close event is fired, that clientsCount is updated, etc\n. And if there is a polling request available, or if one arrives prior to the timeout - you will get the forced close reason.\n. BTW - client side there are even bigger issues closing..... The socket immediately transitions to the closed state even though it's trying to fire a close packet.... I've seen plenty of frozen sockets due to this, that cannot be reopened.\n. socket.request seems to be defined for polling and for direct WS, but I'm getting undefined for a socket that was upgraded to WS.\n. Personally I think this PR is only going half way: if the connection can be made sticky in this manner then the PR should go all the way and support targeting also on the initial request. Then the app developer can have a choice between this targeting scheme and the header based scheme, which must be supported since several Node vendors already implemented header-based LB. As a general note, I'm not sure if a URL param LB scheme is practical across all HTTP methods, so while this may work for engine.io it may not be compatible with the rest of the app - so is this for an edge case? \nAnd why is IP address stickiness not reliable? And can you also look at the source port to LB on?\n. I am now actually convinced that query string is the way to go, for the client to hint to the LB what to target. However, the only thing that needs to be done is to be able to set a new query string on open(), and not just in the constructor. Everything else should be done outside of engine.io. So it should be a 2-3 line PR in the client, nothing server side. \n. @crzidea very simple - I'm already implementing it in my app:\n1) client issues GET myapp.com/getServerID?p1=qaz&p2=wsx etc. Server replies with {\"serverId\" : \"someTagYourLoadBalancerKnows\"}. Note this request is load balanced randomly to one of my servers which does a lookup to get the serverID - I don't care where it goes. I care about my engine.io connection.\n2) engine.io open request with query string \"serverId=someTagYourLoadBalancerKnows\"\n. @crzidea is the engine.io open request really your first app request? If not, stick this lookup in one of the earlier GET requests and that's it. You can even stick the serverId in your rendered template, etc.\n. Well, in this case you may have no choice but do the extra initial request. But look at the bright side: you have a real opportunity to make an informed decision which server to target - your overall performance and scalability may actually be higher.\n. So please accept this one :)\nhttps://github.com/LearnBoost/engine.io-client/pull/221\n. Closing per this: https://github.com/LearnBoost/engine.io-client/pull/216\n. What happens if you timestamp the request to bust caching? Cached responses/headers will often screw up cross origin requests. If for some reason your browser cached a same-origin request to the same resource - this is the error you will see when you try to get the resource cross origin.\n. What does the Chrome network panel show you? Is the request actually going out to the server? Look at the request and response headers, etc.\n. Yup, I see it. sendErrorMessage() should be setting CORS headers if required. It's a bug.\n. But the bug occurs even without preflighted requests. I think the fix should be as I wrote in the my previous comment.\n. Yup, will do.\n. Note that commit message re GH-207 refers to engine.io-client of course......\n. @defunctzombie  Changed the throw calls - thanks for the comments. Other issues tracked your comments. The tests all pass assuming https://github.com/LearnBoost/engine.io-client/pull/239 merged, of course.\n. OK, this and https://github.com/LearnBoost/engine.io-client/pull/239 should be GTG unless any further comments.\n. Man I hate playing with git like that.... but done ;-)\n. I believe this is almost closed by #243. The only thing really left to do is for allowRequest to reject the request with a custom message, and not one of the predefined codes, so the client can get an app-specific rejection.\n. Closing, allowRequest does the job.\n. Just tested it in my app :) It crashed before the fix, valid data after it (was trying to access request.query)\n. @3rd-Eden you wanted a test..... lol, the test also found another bug - 'upgrade' emit timing was off.... fixed. In any case, this test will fail with 0.8.2 on socket.request being undefined after upgrade, after fix all passes clean.\n. @guille - the upgrade event timing is mostly for testability reasons. If you want to check that request is valid after the upgrade, it should signal that the upgrade is done (i.e. after the new transport was set). The current timing (in the middle of the upgrade process) just makes no sense.\n. And regarding the semantics of request..... up to you: let's either set it to the original, or to the upgrade request. But currently it's undefined after the upgrade, so please let me know. Plus, it's frankly silly to tag it onto the transport object if after the upgrade the request has nothing to do with the transport.... so please decide on how you want it and I'll fix it over the weekend.\n. It's currently a getter for transport.request.\nWanna change it to be a plain socket property?\n. Re the upgrade timing - but the question is why? Seems simplest to me that it's at the end. Don't see the logic of the current placement (middle of the process). It's not at the start, as currently coded\n. Done. Socket.request set as a non-writable property inside Socket constructor. Upgrade timing will be in a separate PR.\n. Commits squashed, test included, should be good to go.\n. Just to make it non-writable. We could skip it of course, not a big deal either way. Take your pick.\n. ok, done\n. fix https://github.com/LearnBoost/engine.io/issues/181\n. Test added.\n. It relates to this as well: https://github.com/LearnBoost/engine.io/issues/215\nSo here's an idea:\n1. Connection request contains the first client message in the query string. This part of the query string of course will not be repeated in later XHRs during this connection. So in the API we can add an initialMessage string to the connection options.\n2. Custom handshake response can return server data in the connection ack.\n3. When message is received in connection ack the message event can be fired in the next tick after open event\n. Yup, could change the connection request to POST and have the initial message in the body. Then all you need is an optional handshake interceptor on the server side (similar to socket.io 0.x) and it's done. Am I right?\n. Btw - using request body in initial POST request will only work for xhr. Websocket connection must be opened with GET according to the spec, so for that we can only use query string or not support it at all for that transport.\n. So any thoughts about proceeding on this by changing xhr open to POST and an optional initialMessage in the request body, and adding this message to the query string for websockets? This coupled with a custom handshake function server-side.\n. I mean that the user can supply a function that will accept or reject the connection. Of course, this function can also act on the initial message, if supplied.\n. Actually you're right. If the client can send an initial message in the connection request a custom handshake is of less importance, and the client will wait for a server response rather than immediately sending another message packet following the open. So can we agree on the above? XHR open is a POST and for websockets initial message in the query string?\n. Websocket cannot be opened with POST, the spec requires GET, no option besides query string unless we want to do crazy stuff with cookies which is probably a worse idea. So this idea is really to target Android versions prior to Kit Kat: I don't really see the importance of this feature for non-mobile, and iPhone and Kit Kat can use websockets with the rememberUpgrade feature which will almost always work (I'm assuming most apps will run engine.io over SSL, so websockets actually work reliably).\n. @nullbox perhaps all the memory is being eaten since you're not dereferencing the items in the clients array upon client close?\n. +1\n. Closing - the custom message will be in the code property of the response. A bit confusing naming, but it works. \n. Several points:\n1. If you use SSL you will find that websockets pass on almost all networks (I have never seen them fail), including mobile. Thus I would strongly recommend to always use SSL with socket.io and engine.io\n2. If you insist on using Heroku, then you can use an external load balancer (e.g. haproxy) to effectively create sticky sessions to your Heroku servers.\n3. Frankly, using Redis for pub/sub on all messages in order to scale never made sense to me. As you scale your app, your back-end messages will increase exponentially (since you have more servers sending messages to more servers.... ouch......). So while this may seem a more \"fair\" way to scale, all you're doing is introducing more (unnecessary) load to your cluster. You will eventually reach a point where your entire cluster chokes on backend messages instead of serving clients.\n4. To add to the previous point - without sticky sessions you will need to load per-client session data on numerous servers - yet another ouch.\nBottom line: sticky sessions are a good thing. Strongly suggest you use them, not work against them. \n. @guille external LB with stickiness based on client IP, for example, should do the trick. My provider (Modulus) does sticky sessions based on query param, for example (if I want to override the client IP stickiness).\n. I think supporting Redis flooding in engine.io is way too complex. It's not just the messages - but also heartbeats, etc - and all that complexity just to support a scaling architecture that is problematic to start with..... -1 from my point of view.\n. It seems that Heroku is starting to support websockets: https://devcenter.heroku.com/articles/heroku-labs-websockets, however it's unclear to me if there will be sticky sessions for polling requests.\nRegarding using a LB: if you don't have a URL per server, you can try using a different app name per server.\nOr of course use a PAAS provider which supports sticky sessions and websockets - there's no lack of these.\n. I still think you Heroku fans are trying to fit a square peg in a round hole. Socket.io, engine.io et. al exist in order to support long-lived, bi-directional communication. Heroku does not seem to want to support long-lived client-server connections (besides websockets), unlike many other vendors that do. Use the vendor that answers your overall needs best.\n. @tjsail33 +1 on what @defunctzombie said: as a contributor to this project, and not just a user, I can rightfully feel concerned about what appears to me to be a major complication to the code base, if I believe there are better alternatives.\nAs to the technical points you raise, those can be solved via other means, without shoehorning the project into stateless connections:\n1. On a scale down, your clients should just see a close event (which can always happen for example due to a transport error), and they should perform a standard reconnect. I assume that this basic functionality should be done without a new login being forced on the user. Reconnection and client/server logic is outside the scope of engine.io.\n2. During the scale-up, you're (wrongfully) assuming that connections will be randomly spread following the scale-up event. This is not necessarily the case. For example - some vendors allow you to specifically name the server you wish to target with new requests. Some will take server load (# connections) into account. Or you can construct such a policy by yourself using a LB. Or you can gradually close connections on the loaded server, which will cause a reconnect that may be spread to other servers, etc. \nIn general - it's certainly possible for disconnections and reconnections to be 100% transparent to your users, without modifying engine.io. This is what most of us are doing.\n. There are frameworks for pub/sub (such as Faye), which may be more suitable for a Heroku-like architecture. Engine.io shouldn't be all things to all people on all possible platforms, IMHO.\n. There is no single \"best pattern\" for LB, each app with its own requirements and design decisions. \nWhat you seem to want is a pub/sub framework, while engine.io is really a websocket-like framework, and is about point-to-point bi-directional communication.\nBTW - websockets implicitly require sticky sessions (i.e. all communications in a websocket connection will always use the same server), and Heroku has taken the strange (in my eyes) approach of supporting websockets, but not sticky sessions for standard HTTP requests.\nSo all the arguments about how awesome it is to scale with no sticky sessions are thrown out the window once your clients use websockets. \n. Perhaps I was unclear: generally, I assume you would want to enable websockets in engine.io. Assuming most of your connections come from modern browsers, and certainly if most users use modern browsers and you use SSL connections - then the vast majority of connections in engine.io will be websockets. But you also write that sticky sessions are not good for load balancing. But websockets are sticky by definition. So how will you load balance them?\n. @mr-mig instead of shoe-horning engine.io into a pub/sub model which would be required for Heroku, why not just use one of the available pub/sub frameworks?\n. Plus the performance hit and additional server load.... do what you can to avoid them as part of your connection.\n. I think the default behavior should stay as today (break) since in 90% of cases it's unintended behavior and we don't want this happening silently. We can add a flag that if explicitly set the server sends OK on every OPTIONS request. \n. @defunctzombie because in many cases preflighted requests occur due to careless coding, and can be avoided. I think if they occur the dev should be fully aware that they are happening, not make it a silent default decision. Just my $0.02 :)\n. Yup, it's not the user's fault, it's engine.io: https://github.com/Automattic/engine.io-client/blob/master/lib/transports/polling-xhr.js#L166\nBinary transfers are the culprit. In this case wouldn't it just be better to encode/parse the text in engine.io rather than doing xhr.setRequestHeader('Content-type', 'application/octet-stream');? We really should avoid OPTIONS at all costs, it's a total performance killer and resource hog on the server.\n. So basically what I think we need is:\n1) OPTIONS turned off by default on engine.io server side\n2) By default forceBase64 for XHR if client detects CORS scenario (WS is of course fine)\n3) Add a forceBinaryXHRCors flag (default=false) which will disable (2) above, thus the browser will emit OPTIONS requests, and the developer will need to turn on the OPTIONS flag server-side.\nSo basically, by default all works smoothly and no OPTIONS as most people would probably prefer. If people insist on using octet-stream then they should manually configure this, but it should be supported. I really don't think we should just send binary data blindly in CORS scenarios.\n. @3rd-Eden it's only a one-time request if Access-Control-Max-Age is honored by the browser, and I'm not sure (bears checking) if we satisfy the requirements. Even then, browser performance is inconsistent, see http://stackoverflow.com/questions/23543719/cors-access-control-max-age-is-ignored\nEDIT: I think we will see OPTIONS on every request due to our cache busting :-/\nSo I think in this case we agree that in CORS we much prefer base64 binary transfers.\n. Need this for the fix: https://github.com/Automattic/engine.io-parser/pull/36 , so we can encode binary data when sending in a CORS situation\n. @akamensky your PR https://github.com/Automattic/engine.io/pull/282 will cause this issue to reopen: https://github.com/Automattic/engine.io/issues/211\nIf we don't set CORS headers correctly (i.e. for an invalid request from a.local when a.local is indeed allowed) then we will get a CORS error when in fact the origin was valid. The way to handle this should be different in my opinion: perhaps origins should be set on engine.io and not socket.io (or passed from socket.io to engine.io), and in the engine.io verify function check the headers, act on CORS first and only if that passes do the rest of the checks.\nEDIT: I'm convinced that origins should be an engine.io property. This is basic network configuration and not high level sugar.\n. @akamensky no, the purpose is not necessarily just for debugging. If a client makes a request which is valid from an origin standpoint, but invalid from an engine.io protocol standpoint (could happen in production due to many reasons), he should get a 4XX error and not get a response with screwed up CORS headers which will not enable the client app to understand the true nature of the error (which is what #211 complained about). If the CORS headers are wrong the engine.io error is not even seen by the client app (blocked by the browser).\nA full and correct solution should be to move the origin handling from socket.io to engine.io where it belongs, and the engine.io verify function should verify the entire request - from a CORS perspective (first)and as well as engine.io protocol perspective.\n. @akamensky whether to check CORS and protocol in one function or two is a matter of style. But the essence is the same: the request must be verified on both issues. Note it's not a \"processing\" issue but a protocol issue - e.g. verify checks that the request has a valid ID, etc. So yes, it needs to happen in engine.io and of course you also need to modify socket.io to account for this change.\n. The best way to deal with it is to first figure out why you're generating OPTIONS requests. Do your best to eliminate them.\n. You normally want to make sure you're generating only \"simple requests\". Preflighted requests are slower for the client and increase the load on the server. Avoid them if you can... I'm also running in a CORS scenario and making sure to generate only simple requests.\n. ELB supports cookie based sticky sessions: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/US_StickySessions.html\nso you can limit your traffic to polling and it will work.\nThe other option is to use another provider which supports websockets, sticky sessions, etc or configure your own load balancer (haproxy, nginx, node-http-proxy, bouncy, etc).\nI'm using Modulus.io who not only support sticky sessions and websockets, but their LB also allows you to target a specific instance by setting an instance ID in the query string, which for me is really useful.\n. ",
    "yields": "done :)\n. ",
    "dainis": "wouldn't it be just better to toString that buffer in websocket's onData handler?\n. so policy server has to attached as TCP server and not as HTTP server? at least judging from tests\nso built in policy server is pretty much useless\n. ok in docs it says that handleSocket will be called with request from http server, so test implementation is incorrect or docs are incorrect?\n. ",
    "lemonzi": "It would be really nice to transparently wrap Buffers into Base64-encoded strings, set a flag and decode them back to an ArrayBuffer (or to a normal array if ArrayBuffers are not supported) at the client-side. It would be a good workaround for supporting binary data.\nI guess there's already been a lot of discussion on why not support native binary, but this would make the API ready for it, and hopefully somebody will implement it some day (especially now, where as far as I can see the only overhead added to the messages is a number, easy to insert into a binary array).\n. @3rd-Eden That's true if binary messages always rely on textual encoding, but my point was that implementing this functionality in the core would allow the use of native binary transports where available (AFAIK in websockets, but very likely in more). \nDoing it in base64 adds a significant overhead (about 30% if I recall well), and it's costly to decode, especially on the client side where resources are limited, so doing it natively would be a win.\nThe higher level would be then implementing things like float arrays on top of the raw byte stream (which is not trivial, one reason being that endianness is up to the browser in typed arrays).\n. So, it would be:\n- If using websockets (or xhr with custom encoding?), use native binary.\n- If not, use base64 encoding.\n  - If ArrayBuffer is available, use it. If not, use Array.\n  - If atob and btoa are available, use them. If not, use a manual implementation.\nA preliminary test: http://jsperf.com/base64-to-uint8array\n. It would be more flexible to have hybrid sockets, accepting both text and\nbinary messages, as happens with websockets. Probably trickier to\nimplement, though, because any javascript object being sent would have to\nbe traversed first to check whether it contains binary data or not. But it\nwould be awesome to send objects with mixed text and binary data to the\nclient with a single call.\nTo avoid breaking things, the binary flag could activate the hybrid mode.\nAnd another flag, defaulting to hybrid, could restrict it to only accept\nbinary messages. The base64 flag would work the same way.\nIs it possible to do so with transports such as Flash?\n. But websockets is hybrid, isn't it? The binaryness can be specified for\nevery frame, at least with the WS library and the native client api (\nhttps://github.com/einaros/ws#sending-binary-data)\n. binaryType is fine, but as far as I understood it only defines which format to use after decoding binary data, not whether to work with binary or text. Any frame can be sent as text or binary by setting an option flag in node WS, and via type checking in the client API. I might have got it wrong though, the docs are a bit confusing...\n. Well, I assumed something like this. What's the altermative? The client or server directly using a certain protocol based on whether text data is being sent over websockets or not?\n. I had been out for a while. Good to know it's already working! So yes, I guess the issue can be closed now...\n. ",
    "laino": "Sooooooo. How's it looking? I still have to use JSON instead of msgpack. I would really appreciate some work on this, with kisses and all.\n. Yes, it subsequently uses the websocket.\n. Yes, there's not more polling after that. It just keeps using the websocket connection to transfer data.\n. This should actually be on engine.io-client's issues\n. There is probably be an error event if the websocket doesn't get closed properly.\n. Edit: Don't merge this yet. I'll create a new pull request.\n. Ah right. I'll do that then. Thanks.\n. Now I'm done.\n. Ah. I didn't know you're trying to be backwards compatible that far.\n. Yes. And the server shouldn't flush after every message and instead wait a little for more messages to write the reply to the handshake along with more messages in one request.\n. In the end it will save bandwith AND reduce latency.\nBecause this is how it conventionally worked:\n1) Send handshake\n2) Wait for reply to handshake\n3) Send message\n4) Wait for some response to that message\nAlmost all use cases of engine.io will work like that.\nNew model:\n1) Send handshake and first message\n2) Wait for reply to the handshake and response to the message\nTo the user everything will look the same, except that engine.io will intelligently combine the messages and make everything magically faster.\nThe delay to wait before sending out the messages for good could be user configurable, where a delay of zero or less would mean to flush it immediately. From what I've seen engine.io uses already a write queue, and this could be done in less than a few lines of code (I have a 'working' prototype on a branch of my engine.io fork, but it's not ready)\n. And in my prototype I only use setImmediate(this.flush), which will give the user just enough time to write something after engine.io informed him of the new client. This doesn't introduce any extra latency (well, maybe a few nanoseconds) but saves at least one round-trip\n. I finished the prototype. One test case had to be modified very slightly by increasing a timeout. All tests are passing. I'll have to play around with it a little. Currently it used setImmediate, which would have to be replaced by setTimeout(f,0). But first I'll do some testing in some real life applications.\n. Yep. POST is probably a better idea.\n. Playing around with it, I don't think I understand engine.io well enough yet. Might take me some time.\n. I wouldn't put it in the query string, just use POST if possible and if it isn't supported... well then we'll have to live with more round-trips. And websockets do have lower latency anyways, because we're skipping the TCP handshake for each message (unless we have keep-alive)\n. Indeed, after changing them to\njs\nif (req.query && req.query.b64) {\n   transport.supportsBinary = true;\n} else {\n   transport.supportsBinary = false;\n}\nmy old clients can connect. Patch in 2 minutes.\n. Great, now this seems to break the new client running with the new server because the 1.0 client will not actually use the b64 parameter.\n. Wait... to you guys a client should NOT support b64 if the URL contains ?b64=true, cause that is what the code does.\n. I'm kinda stuck at how I am supposed to get my clients to the new version. If I just replace the server software all clients will stop working and the page can't refresh automatically. Right now I'm thinking of updating the client once and changing the code so it appends b64=true, then updating the server, then updating the client once again, removing b64=true and updating engine.io on the client. \n. Well. You have to somehow tell the client to refresh, which you can't without a working engine.io. I run volafile.io, some people keep it open 24/7 and expect it to refresh automatically for every update. If I fuck up an update the clients will stop working, all users will have to manually refresh the page (most will need some time to notice anything is wrong at all), volafile will look empty for at least a day, the upload queues of everyone will get stuck and I will get angry tweets. \nSo I have to try upgrading without any downtime.\n. Well. I just tried it the way I described. It didn't work.\nBut at least it was not a major fuckup and resulted in only 2 min downtime while I frantically edited js files via VIM over ssh. Upload queues were lost, but nothing of value.\n. Except some people now get \"allocation size overflow\". Fuck. I hate updating stuff. Always end up messing with stuff I shouldn't.\n. Aaaaaaand forceJSONP: true fixes it\n. The problem is likely a large amount of messages/data in the response. It can get up to 100kb in the first response (Still retrieved via AJAX/etc, websocket only kicks in after that). \n. Either it's some weird timing issue or I had a second console.log lingering about somewhere. I can't reproduce it anymore.\n. My suggestion is to wrap it in a try-catch in a isolated function (for performance)\n. No. They're gone.\n. Because that will a) make it clear to the programmer what kinds of attributes this class will have and b) improve performance because the VM will not have to change the internal class at runtime.\n. ",
    "mahnunchik": "+1\n. Maybe it would be a good idea to add support nodejs version 0.10 to .travis.yml ?\nhttps://github.com/LearnBoost/engine.io/pull/187\n. It is very strange because i did not change anything in library, I only change example.\nAnd tests were passed.\nBut now they are filed: https://travis-ci.org/mahnunchik/engine.io\nUPD after restarting tests they have been passed https://travis-ci.org/mahnunchik/engine.io\n. May be it related to https://github.com/LearnBoost/engine.io-client/pull/181\n. ",
    "kkoopa": "Have not tested properly on that old versions, but I do not see why it would not be compatible.\n. ",
    "jondubois": "Does engine.io support the WebSocket protocol over SSL/TLS (wss://)? If so, then how does engine.io get the certificate and private key to encrypt/decrypt websocket frames?\nEDIT Nevermind, this is probably an issue with my proxy.\n. This is an engine.io-client issue. I wrote the fix here: https://github.com/LearnBoost/engine.io-client/pull/205\n. ",
    "AndreasMadsen": "@3rd-Eden Haha, I would suggest that it is added to the server.close method, since the .ws property isn't even documented.\nBut @guille appears to be right, the only thing ws.close really dose it to terminate the clients and since that is done in https://github.com/LearnBoost/engine.io/blob/master/lib/server.js#L143 anyway there aren't much point to it. Also because clientTracking is set to false the .clients array in the ws module don't get pushed to, so ws.close will not event terminate the clients :)\nBut I would still prefer if it got added to server.close for future security.\n. ",
    "paulbjensen": "Thank you for spotting this, I've just run into this issue in a VERY PAINFUL way. I will be upgrading engine.io in chassis.io very quickly.\nThank you again. \n. ",
    "crzidea": "Have it fixed? I found it as well. :)\n. I have tried many ways of LB engine.io Socket. But they are not regular supported:\n1. cookie or sticky session can't be supported by IE using cross domain JSONP.\n2. IP address not support proxy well and not reliable enough if too many clients using same IP address.\nFinnally I tried query string based load balancing. It can be config very easily and regular supported.\nAnd it should be supported in engine.io because all of these steps should be done right after a socket handshake, otherwise client can't let load balacers know which worker (or node instance) should serve this socket. I can't do that only if overide handshake method.\n. @mokesmokes Could you please explain how to make client know which worker should connect on the first time handshaking? And could this way actually solve your problem?\n@defunctzombie In general, engine.io-client must GET 3 times to make a stable websocket connection. So how to make sure all these 3 requests be handled by the same worker while use DNS round robin?\n. That is a way actually many apps are going. But this just add another request to our servers which should avoid.\nMaybe I should close this PR if no one want this feature with engine.io.\n. Yes, the open request really is my first request. I even have no HTML stuff to do in my case. It sounds strange, but it really does.\n. @mmastrac That is actually what I want. Thank you so much!\n. ",
    "timoxley": "FYI I've opened a reciprocal issue on restify: https://github.com/mcavage/node-restify/issues/486\n. @guille what's the alternative here? engine.io seems to extensively use the req as a namespace for sharing data, you'd need to move to more 'pure' functions or perhaps utilising some kind of wrapper that prevents modification of the original req, e.g. wrapping it in Object.create(req)\n. @3rd-Eden yep, that's probably the simplest solution. Is there another way though? Although it's the norm, treating req as a dumping ground is a probably a pretty crappy practice in the first place.\n. @guille ok, req.query is used in multiple locations, but I guess it's probably not so much of a performance hit to just calculate it on the fly.\nAlso, something similar should probably be done with these other properties attached to the req:\n- req.websocket\n- req.res\n. bump\n. ",
    "andybons": "env:\nMozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36\nI just did a fresh pull and followed the instructions at https://github.com/LearnBoost/engine.io/tree/master/examples/latency\nI get the following error when trying to load localhost:3000\nUncaught Error: Failed to require \"engine.io\" from \"eio-latency/index.js\"  build.js:21\n. ",
    "peteruithoven": "@mmastrac, could you clarify if your solution works (using uid in HAProxy)?\nLooking at the query string in a browser I don't see this uid anymore, but I do see sid (socket id), is this something we can use in HAProxy? This sid is received from the server on the first request and is then always added to requests. \nIf this sid isn't usefull, would a user specific url parameter be useful? This is something we use in an API for authentication, it would be great if we can reuse this for sticky sessions. \n. @mmastrac, is this something the client has to do? Because that's hard to explain to API users. \nSince Socket.io depends on sticky sessions it would be great if they could implement this in their socket.io-client. But then again, I understand their reasoning in wanting to define the socket id on the server (to prevent conflicts). \nI'll try setting up sticky sessions using the existing sid or our own user specific key (which we also use for authentication) and report back. \n. We are now using our user authentication url parameter (key) for sticky sessions in HAProxy, seems to work great! \nRelevant part of the backend config: \n```\nUse url_param for load balancing (same key goes to same server if avail)\nSee http://cbonte.github.io/haproxy-dconv/configuration-1.5.html#4.2-balance%20url_param\nbalance             url_param key\nserver nodeapp1 127.0.0.1:5000 check\nserver nodeapp2 127.0.0.1:5001 check\nserver nodeapp3 127.0.0.1:5002 check\nserver nodeapp4 127.0.0.1:5003 check\n```\n. ",
    "nherment": "@kapouer I don't think it is related unless I miss something.\n@mokesmokes I tried and it does not help\n. It is a typical CORS error. I can see the request headers not the response. The server is answering the request with a HTTP 400 code but the browser is blocking the response. I assume it is because the response headers are missing the CORS headers. \nI think I understand the problem pretty well. I'd be happy to chat about it over skype or hangout.\n. Thanks !\nCc @vanchi-zendesk @samshull\n. ",
    "trabus": "I have been experiencing this as well, but have been able to isolate the cause for our situation, and it is scale related. \nWe are using a hardware load balancer in front of several servers, which was using cookies for stickiness. However, OPTIONS preflights don't usually contain cookies, and so they were getting routed to the wrong server frequently. Because the request was sent to the wrong server, it sees the request coming to a non-existent socket and responds with a 400/Bad request. The response code is contained in the transport, so any misdirected requests will see these errors.\nTo temporarily fix this, we are intercepting all OPTIONS requests and responding from the load balancer. I would like to propose the idea of responding to all preflight requests from the server level, to mitigate this issue.\nI have successfully done this locally already by moving the OPTIONS response code to server.js, but wanted to get thoughts before submitting a pull request.\n. ",
    "herkyl": "+1\n. ",
    "PatrickJS": "Not everyone knows about global git and it would be so simple to add this. Unless you write a contribute guide explaining that there is no reason not to accept this at the current state of the repo\n. From what I'm hearing my bot submitted a pull request that does solve a small edge case and your solution was \n\nWhile it's true that not everyone uses osx I still believe the repo should have the concern of rejecting files before submission. If you think otherwise then I would suggest that the original creators of the repo should consider a contribution guide to bring more transparency.\n. ",
    "samaanghani": "Problem results from Nginx switching connection header to 'close' because it doesn't support web sockets. Results in 'error triggered by discarded transport'. We replicated the test using a bad request that we made to match the one produced by Nginx. Not tested on explicitly tested on Squid but should work for both. Working on that now.\n. ",
    "JulianSlzr": "Was looking at this earlier; problem is that the 'connection' event is emitted before the close handler is set up in server.js.\nAlso, @nullbox the heap use goes up because after doing \"new eioc.Socket\", clients[i] is not the only reference to that socket, so overwriting clients[i] won't trigger auto garbage collection. \n. @guille bump\n. ",
    "Swaagie": "The symptoms of this issue seem very similar to https://github.com/LearnBoost/engine.io/issues/228 \n. Hmm yeah you got a point there, I'l double check. The behavior is prevalent in the webops interface of nodejitsu. Webops uses the flatiron framework, so it might be a regression between a small layer @3rd-Eden wrote on top for proper integration. I'll give it some additional attention. At this point in time my only evidence is that the commit before that one is working properly.\n. The same failure is prevalent between the tagged releases 0.9 vs 1.0 btw, haven't been able to pinpoint it further, still investigating though\n. @guille managed to pinpoint it to a cause, the transport.supportsBinary is set to true, if forced to false it will stop loop handshaking. I'm not familiar enough with engine.io code to know where this loop handshake is enforced, could it be an interaction between the client and server code? \n. To be exact the offending line is https://github.com/LearnBoost/engine.io/blob/master/lib/server.js#L216, the continuous handshaking seems to be a symptom of looping protocol update attempts\n. Well I was right in my assumptions, it was indeed an interaction between the server/client, however that was completely my failure, forgot to update the client script :/ sorry for the trouble! Keep up the good work\n. ",
    "lpinca": "@3rd-Eden yes, as reported here https://github.com/primus/primus/issues/173#issuecomment-37723135\n. @guille same here (XP), with a VM taken from http://modern.ie/en-us/virtualization-tools#downloads.\nI tested it only in the console. Gonna post a screenshot.\n. Here it is https://dl.dropboxusercontent.com/u/58444696/engine.io.JPG\nserver and client are on two different VM hosted on the same machine.\nI had that result on third page refresh.\n. @guille i can confirm that my failing test is using xhr-polling.\nIt seems that i can't reproduce it if i use version 0.7.9 of both engine.io and engine.io-client\nVersion 0.7.10 is affected.\nhttps://github.com/LearnBoost/engine.io/compare/0.7.9...0.7.10\nhttps://github.com/LearnBoost/engine.io-client/compare/0.7.9...0.7.10\nBut i can't see anything wrong there.\n. Little update:\nin version 0.7.9 i can't reproduce it because this issue https://github.com/LearnBoost/engine.io-client/blob/380faf769dc07119827f4ca5dc8bac85dacf8849/lib/transports/index.js#L43-L45 that has been fixed later, forces it to use XDomainRequest.\n. @rase- unfortunately it does not work. I merged that pull request before reporting the issue and it still happens.\n. @rauchg @rase- @nkzawa ping.\nAny chance for this to get merged any time soon?\n. @rauchg do you mean req.abort()?\nThe events cleanup is not removed https://github.com/Automattic/engine.io/blob/1e36bfee8296f12165bd62eb38d9cbc99b46944a/lib/transports/polling.js#L182\n. I understand, will update it in few minutes.\n. @rauchg yes that makes sense. I've just updated the patch.\n. @nkzawa should I close this? Looks like #299 does the same thing but with better control over the single messages.\n. It will be updated when #299 will be merged, the same is for the client.\n. This is no longer needed, see 600ed08.\n. @STRML good job!\n. Maybe it makes sense to reopen as this was a feature request which I also support.. @darrachequesne thanks.\nIf I'm not wrong Engine.IO sends ping messages from client to server. We are currently doing the same in Primus but just FYI we are considering reversing the direction of pings and make the server send them to the client as browsers started to throttle timers. See https://github.com/primus/primus/issues/348 if you are interested.. Yes, but the client timer that sends the ping is delayed by the browser so the server incorrectly closes the connection.\nImagine that you used setTimeout(sendPing, 20000); on the client. The sendPing callback is not actually called after 20 sec but after 40 sec.\nHope it makes sense :) . This happens because the g++ version is too old.\nYou have to install a newer version of g++ (>= 4.8) or wait for the Ubuntu 14.04 support which is planned.\nAnyway those errors have no impact. Both utf-8-validate and bufferutil are optional dependencies of ws.\n. @STRML it should be here.\n. We are disabling binary support in primus so binary data should not come in.\n. @STRML there is an open pr exactly for what you are describing (#305) but it's odd.\n. @necccc this has been fixed in https://github.com/socketio/engine.io/pull/305#issuecomment-157184323. We have to wait for the next version of Engine.IO.\n. @noamshemesh that's weird, do you have a way to reproduce it?\n. @noamshemesh please also open a new issue on Primus or continue here: https://github.com/primus/primus/issues/196.\n. Ok, let's revert this.\n. @nkzawa What would be in your opinion the best way to check if doClose has been called as a result of a transport upgrade?\nI was thinking about adding an argument to clearTransport like fromUpgrade and pass it all along till doClose:\n- https://github.com/socketio/engine.io/blob/a31551e141d3ea227e3f85b93285007ca16e4de0/lib/socket.js#L276\n- https://github.com/socketio/engine.io/blob/a31551e141d3ea227e3f85b93285007ca16e4de0/lib/transport.js#L59\n- https://github.com/socketio/engine.io/blob/a31551e141d3ea227e3f85b93285007ca16e4de0/lib/transports/polling.js#L356\nbut there must be a cleaner way.\n. @rauchg PTAL.\n. oh nvm I misread, I guess you want a method, will update in a bit.\n. Done.\n. I don't think it is possible to switch to uws yet, one blocker for example is the lack of permessage-deflate support.\n. Take a look at the websocket transport.\n. @kapouer \n\n@darrachequesne the problem happened to be, IMO, in engine.io, so i tried to fixed it properly in 04bde4c and rebased against latest master.\n\nI disagree, when you use\njs\nconst wss = new WebSocket.Server({ noServer: true });\nyou want an external server to be used and, in fact, ws works merely as an 'upgrade' handler when noServer is true. Since there is no server, wss.close() only closes the open sockets, but clientTracking is false so wss.close() is practically a noop and this was probably the reason why wss.close() wasn't used in the first place.\nOn the other hand uws always uses its internal C++ server and if I'm not wrong when you call wss.close() you also close this C++ server even if noServer is true.\nNow 2.0.0 is out and I think we should at least fix #473. Something like this should work\n```diff\ndiff --git a/lib/server.js b/lib/server.js\nindex 0da4c23..99dfb41 100644\n--- a/lib/server.js\n+++ b/lib/server.js\n@@ -61,6 +61,8 @@ function Server (opts) {\n       compression.threshold = 1024;\n     }\n   });\n+\n+  this.init();\n }\n/*\n@@ -104,23 +106,24 @@ Server.prototype.clients;\n  /\nServer.prototype.init = function () {\n-  if (~this.transports.indexOf('websocket')) {\n-    var wsModule;\n-    try {\n-      wsModule = require(this.wsEngine);\n-    } catch (ex) {\n-      this.wsEngine = 'ws';\n-      // keep require('ws') as separate expression for packers (browserify, etc)\n-      wsModule = require('ws');\n-    }\n-    var WebSocketServer = wsModule.Server;\n-    this.ws = new WebSocketServer({\n-      noServer: true,\n-      clientTracking: false,\n-      perMessageDeflate: this.perMessageDeflate,\n-      maxPayload: this.maxHttpBufferSize\n-    });\n+  if (!~this.transports.indexOf('websocket')) return;\n+\n+  if (this.ws) this.ws.close();\n+\n+  var wsModule;\n+  try {\n+    wsModule = require(this.wsEngine);\n+  } catch (ex) {\n+    this.wsEngine = 'ws';\n+    // keep require('ws') as separate expression for packers (browserify, etc)\n+    wsModule = require('ws');\n   }\n+  this.ws = new wsModule.Server({\n+    noServer: true,\n+    clientTracking: false,\n+    perMessageDeflate: this.perMessageDeflate,\n+    maxPayload: this.maxHttpBufferSize\n+  });\n };\n/**\n``. Another option would be to leave everything as is and specify thatServer.prototype.init()` should be called when passing requests.\n```js\nvar engine = require('engine.io');\nvar server = new engine.Server();\nserver.init();\nserver.on('connection', function(socket){\n  socket.send('hi');\n});\n// \u2026\nhttpServer.on('upgrade', function(req, socket, head){\n  server.handleUpgrade(req, socket, head);\n});\nhttpServer.on('request', function(req, res){\n  server.handleRequest(req, res);\n});\n``. @konstantinzolotarev see https://github.com/websockets/ws/issues/758.. @konstantinzolotarev agreed.. This prevents us from upgrading toengine.io@2.0.0` in Primus.\ncc: @kapouer . The same issue also happens when using an already listening server:\n```js\n'use strict';\nconst eioc = require('engine.io-client');\nconst eio = require('engine.io');\nconst http = require('http');\nconst server = http.createServer();\nserver.listen(3000, () => {\n  const engine = eio.attach(server);\nengine.on('connection', (socket) => {\n    socket.on('message', (data) => socket.send(data));\n    socket.on('close', () => console.log('close'));\n  });\nconst socket = eioc('http://localhost:3000');\nsocket.on('open', () => socket.send('foo'));\n  socket.on('message', (data) => console.log(data));\n  socket.on('close', () => console.log('close'));\n});\n```. @exsilium https://github.com/socketio/engine.io/pull/459#issuecomment-274808715 or apply this patch.. On the server this is not a issue as server to client frames are not masked, so it can be ignored here. On the client (engine.io-client) it may be an issue.\nWe have already backported the fix to the v1.x branch and we plan to release 1.1.2 soon.\nI have no internet connection atm as my ISP decided to cancel my contract. I'm using my cell phone now with only few MB of traffic left.. FYI we also released ws@1.1.2 so you can use that if updating to version 2 is not possible due to the breaking changed.. @darrachequesne you might want to tag version 2.0.2 as latest :)\nnpm dist-tag add engine.io@2.0.2 latest\ncurrently the \"latest\" version is 1.8.3.. @darrachequesne it looks like failures happen when using the uws engine (gulp test; EIO_WS_ENGINE=uws gulp test;).\nI guess something has not been backported to the 1.8.x branch.. Also 0.10 and 0.12 results seem to be false positives.. It is a standard method as http.Server is an EventEmitter.. ",
    "utan": "+1\n. ",
    "nhitchins": "Also experienced this issue with node 10.10. upgrading to 10.11+ seems to have resolved it for us.\n. ",
    "eddify": "We are seeing this occur on safari as well\n. My current workaround is to add something dynamic to the server URL like a unix time, this has been working well for me.  Still, this should be fixed at library level, perhaps just appending a dynamic query parameter on the 1st polling request.\n<script src=\"/path/to/engine.io.js\"></script>\n<script>\n  var socket = new eio.Socket('ws://localhost/?cache_bust=' + (new Date().getTime()));\n  socket.on('open', function () {\n    socket.on('message', function (data) { });\n    socket.on('close', function () { });\n  });\n</script>\n. ",
    "kevin-roark": "Tests were failing locally for me but they were also failing for the upstream repo with no changes soooo i don't know whats happening\n. Yeah you're right didn't realize this had just been changed. Either I can change the name or we can deal with users having a few extra params in _query. What do you guys think?\n. I'm gonna go ahead and close this. User can access query by parsing socket.request.url\n. ",
    "CoericK": "Oh thanks, So its socket.io an active project? cuz been a while that the version 1.0 is there but not released to NPM, i mean if u go to https://www.npmjs.org/package/socket.io u will see the last version is 0.9.16, when its gonna be the 1.0 version released is there any ETA? im asking cuz if im starting a new project should we use 0.9.16? or 1.0 still in beta, what do u recommend me?\n. Oh thanks Guille, Keep doing a great job.\nI will be waiting for some news.\n. ",
    "shinnn": "\nupdate it to SVG\n\nhttps://github.com/Automattic/engine.io/pull/327\n. :+1: \n. Related official blog post: The Travis CI Blog: Faster Builds with Container-Based Infrastructure and Docker\n. Friendly ping :)\n. +1\n. @chapgaga https://github.com/Automattic/engine.io/blob/860af4dc0b47e55a623e7e56ce097ba4b610264b/package.json#L17\n. @Nibbler999 is right.\n. +1\n. :+1: \n. ",
    "ismarslomic": "+1 \nI can confirm the same behavior in my tests. Im using socket.io 1.0.4 and socket.io-client 1.0.4. My test has been performed within one single Chrome tab where I were hitting refresh button many times after each other. The good thing is that after a while (seconds or something), I see the disconnect events for remaining sockets by looking at the server  log.\n. I have repeated my test once again, and It looks like @3rd-Eden is right. I can still see disconnect events in my server log for those remaining sockets which timeout has expired.\n. No problem! So, this is expected behavior right?\n. ",
    "verrier": "@rase-  I don't believe I'm seeing the disconnects events at all.. not even several seconds later. I'll have to make a simpler test app to really be sure... but I'm not seeing anything regarding disconnects in engine.io or socket.io\n@3rd-Eden This issue was originally in the socket.io repo (linked in the original issue for reference) but it was suggested that it be moved here especially since it doesn't seem like engine.io is correctly picking up the disconnect event.\n. 3rd-Eden: I agree, this isn't normal behavior on a good active connection... I originally found this issue when I had a javascript error when handling a response sent immediately over socket.io which caused the connection to never upgrade to websockets (see the conversation on https://github.com/Automattic/socket.io/issues/1580)\nI don't believe that engine.io is correctly propagating the disconnect events when the polling transport expires... I'll need to dive into it further here to be certain, especially in light of what @ismarslomic said. At the very least, socket.io never seems to get disconnect events from engine.io when the polling timeout expires which causes sockets to stay lingering when they've been long gone.\nI'll try to make a simpler test case to see if it can be reproduced here soon\n. I got it guys. Here is a little quick test app that makes this easy to reproduce (using 1.0.4 server and 1.0.4 client)\nClient side code:\n```\n\n\nvar socket = io('http://10.1.4.57/', { 'secure': false, 'reconnection delay': 1000 });</p>\n<p>socket.on('connect', function(){\n    a.b; //Syntax error, wont upgrade to websockets\n})\n\n```\nServer side code:\n``` javascript\nvar server = require(\"http\").createServer().listen(80, '10.1.4.57');\nio = require('socket.io')(server);\nio.on('connection', function(socket){\n    console.log(new Date() + \"Socket: \" + socket.id + \" - Connect. TotalSockets=\" + io.sockets.sockets.length);\n    socket.on('disconnect', function(){\n        console.log(new Date() + \"Socket: \" + socket.id + \" - Disconnect. TotalSockets=\" + io.sockets.sockets.length);\n    });\n});\n```\nThis results in every time you load the page, an additional socket being created... eventually, the sockets will time out after a longish period of time (I just wasn't waiting long enough in my tests)\nServer log:\n```\nTue Jun 03 2014 17:01:19 GMT-0400 (EDT)Socket: YErEUyWtdxvGnxnrAAAA - Connect. TotalSockets=1\nTue Jun 03 2014 17:01:21 GMT-0400 (EDT)Socket: xLtzJujyiT12uC1uAAAB - Connect. TotalSockets=2\nTue Jun 03 2014 17:01:21 GMT-0400 (EDT)Socket: idkqSSpnbKKOiz7YAAAC - Connect. TotalSockets=3\nTue Jun 03 2014 17:01:22 GMT-0400 (EDT)Socket: 6tNVi7rkjupGkEBpAAAD - Connect. TotalSockets=4\nTue Jun 03 2014 17:01:23 GMT-0400 (EDT)Socket: zzm2yWvuvVDrD1rBAAAE - Connect. TotalSockets=5\nTue Jun 03 2014 17:01:23 GMT-0400 (EDT)Socket: xHeGtS8jrAqyPUDFAAAF - Connect. TotalSockets=6\nTue Jun 03 2014 17:01:24 GMT-0400 (EDT)Socket: NOz6kOodR5SmDDzkAAAG - Connect. TotalSockets=7\nTue Jun 03 2014 17:02:44 GMT-0400 (EDT)Socket: YErEUyWtdxvGnxnrAAAA - Disconnect. TotalSockets=6\nTue Jun 03 2014 17:02:46 GMT-0400 (EDT)Socket: xLtzJujyiT12uC1uAAAB - Disconnect. TotalSockets=5\nTue Jun 03 2014 17:02:46 GMT-0400 (EDT)Socket: idkqSSpnbKKOiz7YAAAC - Disconnect. TotalSockets=4\nTue Jun 03 2014 17:02:47 GMT-0400 (EDT)Socket: 6tNVi7rkjupGkEBpAAAD - Disconnect. TotalSockets=3\nTue Jun 03 2014 17:02:48 GMT-0400 (EDT)Socket: zzm2yWvuvVDrD1rBAAAE - Disconnect. TotalSockets=2\nTue Jun 03 2014 17:02:48 GMT-0400 (EDT)Socket: xHeGtS8jrAqyPUDFAAAF - Disconnect. TotalSockets=1\nTue Jun 03 2014 17:02:49 GMT-0400 (EDT)Socket: NOz6kOodR5SmDDzkAAAG - Disconnect. TotalSockets=0\n```\n. ",
    "timcosta": "@mokesmokes your points about flooding Redis are good points which i didn't address in my original post, but is why I expected the non-sticky sessions to not be supported by default in the core codebase. \nRegarding your point about the external LB, that will not work with Heroku as far as I know. We are given a url (https://appname.herokuapp.com for example) to point to, and Heroku has a LB behind that that we cannot modify or bypass, so I think that breaks the HAproxy suggestion.\nWhat I'm getting out of this conversation is that I essentially need to run socketcluster (or something like it) as a separate application on a service like Amazon EB that does support sticky sessions. Is that what the general consensus is?\nI have a open ticket with Heroku support and am communicating with one of their engineers about this as well, and I hope between the Heroku team and the community here we can figure something out that will work for all Heroku users and can potentially be added to the readme or upgrade instructions.\n. Their websokects lab works perfectly, and we have been using it in conjunction with socket.io 0.9.17 for some time now. It's just the newly required stickiness that causes the problems.\nAs of right now, Heroku is not planning to add stickiness to their LB, as it has not really been an issue until now. \nIf you have a suggestion for another PAAS provider that bundles everything up nicely like Heroku does, I would welcome the suggestion, but no one else I have found integrates with 3rd party add ons and controls the app deployment process so well.\n. @guille any update on this? Sorry to be such a pain, but socket.io/engine.io is a critical part of my application and being able to upgrade to 1.0.x would be great.\n. @mokesmokes, all due respect, but this is a real issue and if you have nothing meaningful to add to this conversation other than telling us to use other hosting providers or rewrite part of our servers, please stay out of it. This support used to exist, and all we are asking is that support for one of the leading Node.js hosting platforms be added back in. \n\nOn Jun 21, 2014, at 6:31 PM, mokesmokes notifications@github.com wrote:\nI still think you Heroku fans are trying to fit a square peg in a round hole. Socket.io, engine.io et. al exist in order to support long-lived, bi-directional communication. Heroku does not seem to want to support long-lived client-server connections (besides websockets), unlike many other vendors that do. Use the vendor that answers your overall needs best.\n\u2014\nReply to this email directly or view it on GitHub.\n. I agree that lack of sticky support is a choice, but for some applications it makes sense. My application has about 9 peak hours during the day where there are frequent scaling operations, both up and down. During the scaling events during those 9 hours of peak usage, using sticky sessions has two main effects:\n1. During a scale down, any users logged in to the server being removed are suddenly cut off and forced to log back in to another server. Obviously this is not good UX, and needs to be avoided. \n2. During a scale up, load is not moved away from the servers that are experiencing high load. Scaling events are triggered when server response times increase above an \"acceptable\" value, and adding another server does absolutely nothing to relieve stress when sticky sessions are enabled, except for any new sessions may be created.\n\nMy application uses socket.io/engine.io for all of the real time functionality in it, which is every single part of the page. It would be nice to be able to continue supporting functional scaling that decreases load on existing servers, and be able to scale down without breaking off the sessions of some users. I understand that this is an implementation choice of the creator and collaborators if socket.io and engine.io, but I simply think that there should be some option, be it an adaptor, plugin, whatever you want to call it, that supports using Redis or something else as the back end because using sticky sessions simply isnt ideal for every single app. It doesn't even really have to do with hosting provider. It has to do with what configuration leads to the ideal responsiveness of the app being written.\nI will look into a PR or adaptor or whatever, but personally I have very little experience dealing with this type of system and I do not know how \"ideal\" the code I could come up with would be, and I really think that some sort of native support would be great. \nAnd yes, I understand that it is not all about my use case and my application. But I am not the only person who has a real time app behind a log in mechanism who would like to be able to scale servers freely without causing a poor experience for the end user.\n. ",
    "lcalvy": "Hi,\nMe too it's a blocking issue, I talk with Heroku support that tell me engine.io will never support sticky less loadbalancer.\nIf engine.io is not compatible with heroku and not compatible with out of the box nodejs cluster mode it's a breaking bad news for nodejs community. \nI think it's a major missing feature for the first worldwide socket framework!\nWhat is the amount a work about support this feature? Can we help?\n\nLe 21 juin 2014 \u00e0 23:42, TIm Costa notifications@github.com a \u00e9crit :\n@guille any update on this? Sorry to be such a pain, but socket.io/engine.io is a critical part of my application and being able to upgrade to 1.0.x would be great.\n\u2014\nReply to this email directly or view it on GitHub.\n. In my opinion it's not about to be the framework for all things to all people on all plateform. It's just about don't sticky engine.io with network architecture choice. Non sticky session is the best pattern for load balancing, this issue is also present when you use nodejs cluster mode.\nI'm really afraid to see a lot of developers  don't even realize that it would not work at all in this conditions. \n\nNo other realtime communication framework deal with this real life constraint. If engine.io can support this kind of architecture, it's a major benefit that guaranty compatibility with high scalability platforms.\nIt's the goal of a framework to simplify complexity.\n\nLe 22 juin 2014 \u00e0 11:38, mokesmokes notifications@github.com a \u00e9crit :\nThere are frameworks for pub/sub (such as Faye), which may be more suitable for a Heroku-like architecture. Engine.io shouldn't be all things to all people on all possible platforms, IMHO.\n\u2014\nReply to this email directly or view it on GitHub.\n. But engine.io use protocol upgrade and have the awesome feature to handle old browser with long polling and other fallbacks. \n\nWhy use engine.io if it's mandatory to only use websocket technology?\n\nLe 22 juin 2014 \u00e0 12:26, mokesmokes notifications@github.com a \u00e9crit :\nThere is no single \"best pattern\" for LB, each app with its own requirements and design decisions. \nWhat you seem to want is a pub/sub framework, while engine.io is really a websocket-like framework, and is about point-to-point bi-directional communication.\nBTW - websockets implicitly require sticky sessions (i.e. all communications in a websocket connection will always hit the same server), and Heroku has taken the strange (in my eyes) approach of supporting websockets, but not sticky sessions for standard HTTP requests.\nSo all the arguments about how awesome it is to scale with no sticky sessions are thrown out the window once your clients use websockets.\n\u2014\nReply to this email directly or view it on GitHub.\n. Thank you for this input and don't worry, no drama here. Just trying to deal with the big mistake : using heroku and socket.io together.\n\nHave a good day and be sure I appreciate the amazing work engine.io team has done.\n\nLe 22 juin 2014 \u00e0 13:10, John Radicans notifications@github.com a \u00e9crit :\nI've been trying to prevent my self from commenting but I just had to post this because I can't deal with this pointless drama in my inbox anymore. I love Engine.IO but I feel that you guys are totally lost here..\nWhy do you expect the framework to change because you've made poor architecture mistakes while designing your application. You guys need to realize that building a real-time component in your infrastructure is not the same as hosting a damn blog. Just because non-stickly support works great for a blog it doesn't mean you can apply this dated pattern to anything else you are planning to build.\nEmbracing the solutions that are already here, which are well tested and proven in production environment. If you want to scale, the last thing you want to do is introduce more complexity to your stack. And that is something you are all suggesting, add more complexity to the project just because you guys decided to go with heroku and have no clue how to host a real-time application (if you had a clue you wouldn't have gone with heroku in the first place).\nPolling fallbacks require times to ensure that the connection is not seen as closed when the poll requests end. Time is state and this state should only belong to one process that handles it. You don't want to add this state to a remote process which you need to lookup every single time you receive a connection. This is a real-time server you're using, adding more remote processes and lookups will only introduce more latency which is something you've been trying to prevent in the first place. The only \"solution\" to this would be syncing state or \"shared memory\" but there are obvious drawbacks to that approach because you're using Node. Which is that there is a memory limit of a single node process which is limited to about 1-2gb syncing state would mean that you purposely prevent your cluster to scale to a certain level because your cluster is so big that theres only room for state syncs and no more room for memory for an incoming connection.\nAnyways, that's my 2 cents.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "radicans": "I've been trying to prevent my self from commenting but I just had to post this because I can't deal with this pointless drama in my inbox anymore. I love Engine.IO but I feel that you guys are totally lost here..\nWhy do you expect the framework to change because you've made poor architecture mistakes while designing your application. You guys need to realize that building a real-time component in your infrastructure is not the same as hosting a damn blog. Just because non-stickly support works great for a blog it doesn't mean you can apply this dated pattern to anything else you are planning to build.\nEmbracing the solutions that are already here, which are well tested and proven in production environment. If you want to scale, the last thing you want to do is introduce more complexity to your stack. And that is something you are all suggesting, add more complexity to the project just because you guys decided to go with heroku and have no clue how to host a real-time application (if you had a clue you wouldn't have gone with heroku in the first place).\nPolling fallbacks require times to ensure that the connection is not seen as closed when the poll requests end. Time is state and this state should only belong to one process that handles it. You don't want to add this state to a remote process which you need to lookup every single time you receive a connection. This is a real-time server you're using, adding more remote processes and lookups will only introduce more latency which is something you've been trying to prevent in the first place. The only \"solution\" to this would be syncing state or \"shared memory\" but there are obvious drawbacks to that approach because you're using Node. Which is that there is a memory limit of a single node process which is limited to about 1-2gb syncing state would mean that you purposely prevent your cluster to scale to a certain level because your cluster is so big that theres only room for state syncs and no more room for memory for an incoming connection. \nAnyways, that's my 2 cents. \n. ",
    "mr-mig": "Why don't you guys add the line to the docs pointing out that Heroku users will suffer?\n. ",
    "JTallis": "From my understanding, it starts off with xhr-polling, into regular polling then websockets. What happens after that is that it appears that the websocket transport gets restarted which could be what causes the error but I cannot be sure. Instead, so far what I've done is that I've set it to only use polling and the issue in question no longer occurs. The websocket transport should provide more verbose errors instead of a json error when it in fact isn't originally a json error. This makes it hard to debug when you have no error!\nI'm glad I sorted that out. I might do a debug mode which uses only polling, just so I get those nice errors. It's actually an engine.io-client issue in a way but it didn't appear like that first.\nI apologise for being in the wrong place. It was an issue that needed further looking into as I went along.\n. Ah nice! I've got the latest engine.io-client available from npm so looking forward to seeing this soon. I'm not sure how an undefined variable error would cause such a thing but if it at least tells me useful errors then all is great. The websocket transport works better at detecting a client disconnection so would be nice to have.\n. Realised it was else where that caused the random space. I should be able to work with this better now.\n. ",
    "nkzawa": "I think this issue is already fixed on the master branch.\nhttps://github.com/Automattic/engine.io-client/commit/90717904019fae9a87186821cc86dc2186b877bf\n. Should be fixed in https://github.com/socketio/engine.io-client/pull/444\n. It looks like this issue was fixed by https://github.com/Automattic/engine.io/pull/285,  https://github.com/Automattic/engine.io-client/pull/351 and https://github.com/Automattic/engine.io-client/pull/352.\n. Hi. I tried to reproduce the problem, but couldn't (I just sent the file from server to client).\nA sample code would be helpful.\nAlso could you check if this is the same issue with https://github.com/Automattic/engine.io-client/issues/326 ?\n. :+1: \n. It seems we should have a minimum size threshold for http compression, since compressing small data makes them larger.\n. @defunctzombie mainly because we want to turn the compression on/off per message(payload) to emulate the permessage-deflate extension of WebSocket. I think we can't support this with proxy.\nAnd also we can't use Content-Type header to decide whether to compress data or not like a http server.\n. @lpinca updated, thx!\n. @leeus I'm working to reflect code review. Maybe this will be merged after that if there is no problem anymore.\n. @rauchg fixed and rebased. could you check again?\n. compress: true for sendPacket(send) is just one of the conditions, so it's ignored when compression is disabled globally and when client doesn't support compression.\n. @rauchg rebased\n. Thanks and I'm sorry for the delay.\n. Hmm, it looks this PR doesn't change the behavior to me. Anyway, I'd like to reproduce the problem. How can I do that?\n. @STRML :+1: \n. This happens when you try to send a message when underlying websocket is closing.\n. Thanks for your report! WS is bumped to 0.7.1 on master.\nhttps://github.com/Automattic/engine.io/commit/600ed08b69cb4e06084564a7db74b4a20faaf6da\n. Engine.io is just a transport layer for socket.io, and doesn't have such feature. You can use socket.io for that as you do already.\n. I noticed that this behavior would be intended to wait client timeout so that the closing reason becomes ping timeout on both side. But I heard this behavior causes a problem as follows.\n\nit mainly causes an issue on mobile safari (tested on v 7.2).\ngiven two tabs both running a websocket connection, when you open one of them, the other tab that is in the background will lose its connection server-side because it cannot receive a pong from the client. when you return the background tab in the foreground, the client waits for another ping interval before it closes.\nsimply put, the timeout time is doubled in a sense.\nsince the websocket is still open when the server times out, client-side is still trying to get a reaction from the server.\n. This looks a bug of the doc since attach() doesn't create a new instance.\n\nhttps://github.com/Automattic/engine.io#methods\n. Umm, actually the docs seems ok. Server#attach doesn't return a server instance, but engine#attach returns it. \n. It seems your are using old version of engine.io-client. If so, please update it.\n. New versions have been released, thanks!\n. thx :smile: \n. :+1:  sorry for the delay but can you fix conflicts?\n. fixed in https://github.com/socketio/engine.io/pull/370\n. :+1: \n. Shipped in v1.6.x \n. :+1: \n. thanks, changed to use self instead of this.\n. :+1: \nThe delay after the disconnect event should be the close timeout of websocket.\n. @whifflevcq thank you for your feedback. So it's still not perfect, but anyway I believe closing all tcp sockets on client disconnection would solve the issue.\n. I looked through the source code and found a few suspicious points.\nTransports are not explicitly closed on error. Usually, transports will close themselves when an error occurs, but it's not always true.\nhttps://github.com/socketio/engine.io/blob/master/lib/socket.js#L95\nhttps://github.com/socketio/engine.io/blob/master/lib/socket.js#L117\nWebSocket connections are not closed when upgrade timeout since transport.readyState will never change to open (which should be a bug).\nhttps://github.com/socketio/engine.io/blob/master/lib/socket.js#L169\nRegarding polling transports, http server automatically closes sockets when they time out, so maybe we don't need to worry them.\nhttps://nodejs.org/api/http.html#http_server_settimeout_msecs_callback\n. @nicofff I wonder why the PR doesn't solve your case. Do you have any particular settings for http server like timeout and keepAlive?\n. thanks!\n. Fixed https://github.com/socketio/engine.io/commit/1a115d349310f418eabbe8cfc99702cdeeecdfa7\n. I will see If I can add tests.\n. Thanks for your report! I will check the problem.\n. The problem should be fixed now.\n. @nicofff I just tested with socket.io but it was ok. How can I reproduce?\n. Ah, true! thanks!\n. I guess you use a non-js client? Let us know the details.\n. @contra how can we reproduce? Does this still occur on the latest version?\n. So sorry for the delay. Anyway, closed for now. It could be reopened when we got reports from other people.\n. This wouldn't happen in the latest version since native addons were removed from ws.\n. Fixed in 1.6.x\n. There is no official iOS clienr for engine.io. But socket.io client for iOS exists https://github.com/socketio/socket.io-client-swift .\n. I think there is no reasonable reason here, guess it's just for making path check easy.\n. Yes, only one poll request per session is allowed.\nI believe that no response is sent when data requests overlap is a bug, but will be fixed with the PR https://github.com/socketio/engine.io/pull/338.\n. Fixed in 1.6.x\n. It might be better to have threshold setting which data with a size below that won't get compressed. \n. :+1: skipping shouldClose and closeTimeoutTimer sounds good to me.\n. @lpinca That sounds good to me, I have no other good idea.\nAt first, I thought we might be able to do handshake when pausing transport, so that polling transport on server would get a pausing request and could be cleanly closed on both side. But this would cause to change the protocol and affect a lot.\n. Thanks! I like the simple solution. @rauchg can you check for sure?\n. Please describe the details. What versions of socket.io and node?\n. Thanks. It seems this is an error of ngrok module. Maybe the module doesn't work on Windows? I will report it to the ngrok author.\nAnyway, I think you would be able to install socket.io by npm, since ngrok is a dev dependency.\n. it's likely the issue of Flask-SocketIO. Please report it to their issue tracker. Feel feel to reopen. \n. Umm, I'm not sure which is better, but thought ignoring the option would be faster since we need a poll request per packet in the worst case if we don't do so.\n. The reason of socket closing becomes \"transport close\" if we call here.\nThis method will be called on clearTransport() anyway.\n. onClose would never be called if we didn't manually call it. \n. for the case that client didn't send a poll request anymore.\n. res._header doesn't look public field. Maybe we should use getHeader() instead?\n. We might be able to find a better default, but it's the same with http compression for now.\n. It's not a bug of ws. Obviously, it's not good, but ws just works like that. \nhttps://github.com/nkzawa/ws/blob/master/lib/WebSocket.js#L230\n. Or we probably should call this a bug.\n. We'd need to fix check function, since it wrongly matches a path having a same prefix.\nconsole.log(path); // '\"/foo\"\nconsole.log(req.url); // \"/foobar\"\ncheck(req); // true\n. please don't use ^, otherwise LGTM\n. ",
    "saibotsivad": "That is to say, the npm version has \"engine.io-parser\": \"1.0.6\" while it should be \"engine.io-parser\": \"1.0.7\" and (IIRC) you have to bump the package.json file version number to get npm to accept the change.\n. ",
    "adamrothman": "+1 this is causing lotsa trouble\n. ",
    "fjakobs": "awesome. thanks\n. ",
    "ben-ng": "Cool! Will add those after work.\n. It slipped my mind this morning that I'm leaving on a short 5 day vacation today, so i'll finish up this PR when I get back if someone else doesn't beat me to it.\nI measured the impact of this change on our prod server this morning, it's a 3x speed improvement for responses in the ~500kb range. Absolutely terrific, shaved 2 seconds off our app bootstrapping process.\n. Thank you for the comments @defunctzombie, I've implemented them.\nWorking on tests next.\n. Any suggestions on how to test this? I tried setting Accept-Encoding in engine.io-client's polling transport, but it won't let me because it's an unsafe header.\nEven if that worked, I'd need to expose the Content-Encoding of the message somehow in order to assert on it, and that seems like Bad Karma.\n. I couldn't figure out how to write the tests, and don't have the time to do it right now :/ (job, full-time student, etc..)\n. @rase- that would really help me get started! I do not understand how the test suite works.\n. ",
    "salzhrani": "Any updates ?\n. ",
    "merty": "Oh, didn't notice that. Fixed!\n. ",
    "jhribal": "thanks, but in that case why engine.io uses that utf8encode/utf8decode custom functions and not using just json.encode?\n. I think you can have multibyte strings even without encoding to utf8.\nJavascript strings are encoded in ucs2 and you can have those surrogate pairs to represent\nchars outside of BMP. \nAnd i was thinking about that json encode should encode to utf-8 as you referenced that ietf.\nIf it is true, than when i call JSON.stringify, it should return utf-8 string and because javascript is using ucs2, than for instance console.log(JSON.stringify(obj)) should't work (but it works and outputs normaly those characters as expected). \n. that means JSON.stringify returns ucs2 string and not utf-8\n. ",
    "dmkc": "@guille Thanks for looking into this so quickly! The Content-Type for the requests is indeed application/octet-stream. According to MDN that makes it a preflighted request. Hrm. Here are the complete headers:\nAccept:*/*\nAccept-Encoding:gzip,deflate\nAccept-Language:en-US,en;q=0.8\nConnection:keep-alive\nContent-Length:65\nContent-type:application/octet-stream\nCookie:io=Cu4ixu0L9h01xctVABFa; AWSELB=D92B29591C6D9DF48A890DD33BA2DB332633D5E9B0B9494FF3E6BBC16DECE6A0CB3F6B058AEAE907E103688331ECDF2318C3C96EE3035ABD6F0AAE7F5EE38DAA554DC79045\nHost:subdomainof.awesome.internal.site.com\nOrigin:http://awesome.internal.site.com\nReferer:http://awesome.internal.site.com\nUser-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\n. @mokesmokes Here the preflight requests are occurring because of the request's content-type. Is choosing this content-type a decision made by socket.io? I am not configuring it anywhere explicitly. If that's the case then the default treatment of the OPTIONS req makes sense.\n. ",
    "robertjustjones": "+1\n. ",
    "ashaffer": "+1 for this issue\n. For anyone else with this issue, the following code will fix it:\n``` javascript\nmodule.exports = function(srv) {\n  var listeners = srv.listeners('request').slice(0);\n  srv.removeAllListeners('request');\n  srv.on('request', function(req, res) {\n    if(req.method === 'OPTIONS' && req.url.indexOf('/socket.io') === 0) {\n      var headers = {};\n      if (req.headers.origin) {\n        headers['Access-Control-Allow-Credentials'] = 'true';\n        headers['Access-Control-Allow-Origin'] = req.headers.origin;\n      } else {\n        headers['Access-Control-Allow-Origin'] = '*';\n      }\n  headers['Access-Control-Allow-Headers'] = 'origin, content-type, accept';\n  res.writeHead(200, headers);\n  res.end();\n} else {\n  listeners.forEach(function(fn) {\n    fn.call(srv, req, res);\n  });\n}\n\n});\n};\n```\nApplied after socket.io:\ne.g:\njavascript\nio.listen(server);\nhandleOptions(server);\n. ",
    "sacheendra": "There is a small addition to the above code that is needed to fix it. \nThere needs to be a access-control-allow-headers header\n``` javascript\nmodule.exports = function(srv) {\n  var listeners = srv.listeners('request').slice(0);\n  srv.removeAllListeners('request');\n  srv.on('request', function(req, res) {\n    if(req.method === 'OPTIONS' && req.url.indexOf('/socket.io') === 0) {\n      var headers = {};\n      if (req.headers.origin) {\n        headers['Access-Control-Allow-Credentials'] = 'true';\n        headers['Access-Control-Allow-Origin'] = req.headers.origin;\n      } else {\n        headers['Access-Control-Allow-Origin'] = '*';\n      }\n  headers['Access-Control-Allow-Methods'] = 'GET,HEAD,PUT,PATCH,POST,DELETE';\n  headers['Access-Control-Allow-Headers'] = 'origin, content-type, accept';\n  res.writeHead(200, headers);\n  res.end();\n} else {\n  listeners.forEach(function(fn) {\n    fn.call(srv, req, res);\n  });\n}\n\n});\n};\n```\nThis solved the problem for me.\n. ",
    "MiLk": "Hello,\nI'm trying to set up socket.io servers behind AWS ALB.\nThe stickiness is using cookies as stated in http://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions\n\nApplication Load Balancers support load balancer-generated cookies only. The name of the cookie is AWSALB. The contents of these cookies are encrypted using a rotating key. You cannot decrypt or modify load balancer-generated cookies.\n\nBecause the web application and the ALB are not on the same domain, OPTIONS requests are sent during the handshake phase for long-polling.\nHowever the OPTIONS requests don't have cookies attached to them, and are sent following the ALB distribution method instead of being sticky.\nThe OPTIONS requests being handled late inside the polling-xhr transport, there is a verification step being done early which fails because the session is not valid on all the servers.\nAll of that leads to the handshake failure.\nWhile I agree that the aforementioned points should be addressed to avoid doing OPTIONS requests, OPTIONS requests should not trigger all the described logic.\nThe rfc2616 says:\n\nThe OPTIONS method represents a request for information about the communication options available on the request/response chain identified by the Request-URI. This method allows the client to determine the options and/or requirements associated with a resource, or the capabilities of a server, without implying a resource action or initiating a resource retrieval.\nFrom my understanding, OPTIONS should only specify the headers for CORS allowing subsequent requests to be sent.\nIt SHOULD NOT:\n Verify that the session is valid (https://github.com/socketio/engine.io/blob/bd1e81ec3d11f18c5fc09c55745d5ff66d037cc1/lib/server.js#L225)\n Initiate the handshake (https://github.com/socketio/engine.io/blob/bd1e81ec3d11f18c5fc09c55745d5ff66d037cc1/lib/server.js#L235)\n* Try to initiate a connection (https://github.com/socketio/engine.io/blob/bd1e81ec3d11f18c5fc09c55745d5ff66d037cc1/lib/server.js#L316-L339)\n\nI've come up with a way to by-pass the verification steps and delegate earlier the processing of the OPTIONS requests to the transport with https://github.com/MiLk/engine.io/commit/c8012eb24b20dda7b9a679cde8629361cadf2f12\nI'm not convinced that's the best way to handle it.\nMaybe the handling the OPTIONS requests directly in the listener is a better idea as proposed by other people in this thread.\nI'm willing to spend more time on this issue.\nWhat are your views on that?\nI'll explore the other solution which is trying to disable CORS by changing the content-type and see where I go with that.\nEdit:\nI've added an option to let the application handle OPTIONS requests instead of engine.io.\nhttps://github.com/socketio/engine.io/pull/484. Hi @JohnCoding94,\nWe ended-up disabling the polling transport and forcing all the clients to use websockets, because the OPTIONS requests are not sending the cookies, and ALB is using rotating keys for their cookies.\nWhat I did in #484, would allow the handshake to work, but the session would still not be right.\nHowever, it might work if you are not using CORS, because the only issue I can think of is the OPTIONS request done during the handshake. https://github.com/socketio/socket.io-client/blob/master/docs/API.md#with-websocket-transport-only\nWe have now a few thousands WS connections established staying stable over 24h periods. We force connections to be closed in the middle of the night).. The flow I currently see is:\n1. GET /socket.io/?EIO=3&transport=polling&t=...\n2. GET /socket.io/?EIO=3&transport=polling&t=...&sid=...\n3. OPTIONS /socket.io/?EIO=3&transport=polling&t=...&sid=...\n4. POST /socket.io/?EIO=3&transport=polling&t=...&sid=...\nIn 1., the session is created. socket.io assigns a sid and ALB create a new cookie.\nIn 2., we send a new request with the sid provided by socket.io and the cookie sent by ALB.\nIn 3., we send an OPTIONS request before doing the POST request used in the ping process.\nBut it seems that OPTIONS requests are not sending the cookies.\n(https://fetch.spec.whatwg.org/#cors-protocol-and-credentials Note that even so, a CORS-preflight request never includes credentials.).\nBecause of that, ALB send a new cookie, which might associate the client to the same server or a new server.\nIn 4., if the cookie links the user to a new server, the ping request will arrive to a server without the session, and we have to re-establish the connection from 1.\nAll of that is happening because:\n ALB is using rotating keys (http://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions The contents of these cookies are encrypted using a rotating key., meaning a different cookie is sent with every request).\n OPTIONS request are assigning a new cookie for sticky sessions.\nThe only way to make the polling transport in the current state of ALB would be to avoir the preflight requests.\nHowever, it should work fine with any loadbalancer using a consistent hashing algorithm, or without setting a new cookie on preflight requests.\nBasically, the issue didn't change much since the original post. #484 is just making it fail at the POST request instead of the OPTIONS request.. No, it's because the OPTIONS request get a new ALB session cookie, which makes the POST request to go to a server where the socket.io session doesn't exist.\nI would need to setup something to test again, but IIRC the error came from https://github.com/socketio/engine.io/blob/master/lib/server.js#L161. I'm trying to reproduce the issue, but so far the connection seems actually stable.\n\n\nI'm using https://github.com/socketio/engine.io/tree/a63c7b787c54b3a47da7f355826bf2770139c62b.\n```js\nvar app = require('express')();\nvar cors = require('cors');\napp.options('*', cors({\n  origin: true,\n  methods: 'POST',\n  allowedHeaders: ['Content-Type'],\n  credentials: true,\n}));\nvar server = require('http').Server(app);\nvar io = require('socket.io')(server, {\n  handlePreflightRequest: false\n});\n...\n```\n. The issue is still happening on Safari\n\n. Chrome and FF seem to be both OK. Only Safari 10.1.1 has an issue.\nBut the behaviour is definitely different from what I observed a few months ago.\nHowever, my socket.io client and server libraries are not up-to-date (1.1.0 and 1.8 respectively).\nSince we forced our clients to use websockets, it's no longer an issue for us, but we will definitively update everything within a few months, I will be able to give a new feedback then.\n. Using 2.0.3 on the server and the following on the client, we successfully serve WebSockets and long polling with a HA setup in production behind ALB (about 2k conn / server).\n\"socket.io\": {\n      \"version\": \"1.6.0\",\n      \"resolved\": \"https://registry.npmjs.org/socket.io/-/socket.io-1.6.0.tgz\",\n      \"integrity\": \"sha1-PkDZMmN+a9kjmBslyvfFPoO24uE=\",\n      \"requires\": {\n        \"debug\": \"2.3.3\",\n        \"engine.io\": \"1.8.0\",\n        \"has-binary\": \"0.1.7\",\n        \"object-assign\": \"4.1.0\",\n        \"socket.io-adapter\": \"0.5.0\",\n        \"socket.io-client\": \"1.6.0\",\n        \"socket.io-parser\": \"2.3.1\"\n      }\nWe didn't do any change on the server since last August (except Node version upgrade).\nThe setup is what I described earlier in https://github.com/socketio/engine.io/issues/279#issuecomment-309203357. That would work too. I will implement it tomorrow.. You can now do:\njs\nvar app = require('express')();\nvar server = require('http').Server(app);\nvar io = require('socket.io')(server, {\n  handlePreflightRequest: (req, res) => {\n    let headers = {};\n    if (req.headers.origin) {\n      headers['Access-Control-Allow-Credentials'] = 'true';\n      headers['Access-Control-Allow-Origin'] = req.headers.origin;\n     } else {\n       headers['Access-Control-Allow-Origin'] = '*';\n     }\n     headers['Access-Control-Allow-Methods'] = 'GET,HEAD,PUT,PATCH,POST,DELETE';\n     headers['Access-Control-Allow-Headers'] = 'origin, content-type, accept';\n     res.writeHead(200, headers);\n     res.end();\n  }\n});\nio.on('connection', function(){ /* \u2026 */ });\nserver.listen(3000);\nOr:\njs\nvar app = require('express')();\nvar cors = require('cors');\napp.options('*', cors());\nvar server = require('http').Server(app);\nvar io = require('socket.io')(server, {\n  enableOptions: false\n});\nio.on('connection', function(){ /* \u2026 */ });\nserver.listen(3000);. The last error on travis doesn't seem to be related to that PR.\nhttps://travis-ci.org/socketio/engine.io/jobs/206487174. ",
    "JohnCoding94": "Hi @MiLk,\nI am also trying to use socket.io behind an AWS ALB.\nCould you please share a sample of code enabling us to make the connection on the server side?\nI see a lot of peoples having the same issue on stackoverflow, it would be really helpful!\nThanks!. @MiLk You may already be aware of that but just for information, i ended up setting a connection from a subdomain to my racine domain using SSL, and any OPTIONS request is made, i didn't set any specific options on my socket.io server.\nSo it performs the classic long pooling handshake, then switch to websocket protocol, and everything seems to work fine.. ",
    "scream314": "What solved the same issue for me:\nI noticed I had an old version of socket.io dropped in my code and it was used instead of an npm package or whatever. After switching to an up-t-date version of socket.io-client the issue disappeared, there is no OPTIONS request sent funking up the AWSALB cookie.\nAfterwards I tried to reproduce the issue in Chrome, FF and Safari, too, with no success.\n@MiLk How do you receive a 400 for the GETs above? Is this happening only in Safari? Could you check what does Firefox's Console say? (So far it was the best one I used, at least when it comes to meaningful error messages.). ",
    "franza": "Any updates on this issue? Is it really enough to use handlePreflightRequest: false and handle OPTIONS requests as @MiLk showed?. ",
    "akamensky": "As per comments in https://github.com/Automattic/socket.io/issues/1778 I will provide PR based on 403 response code and not setting CORS headers at all\n. @mokesmokes From #211 it looks like this piece of code was made for the purpose of only easier debugging, while by introducing this code it in fact broke CORS specifications. IMO debugging of any application should not be done using CORS. It can be done from same domain.\nThe sole purpose of Access-Control-Allow-Origin is to show which domain/domains are allowed to access this resource. Therefore just writing error code and still sending this header is wrong as a developer who does not know about this trick in engine.io will think something is wrong with his application.\nIf it is really necessary to give ability to put this header even for not allowed domain - this can be done as optional debugging mode and leave standard (production) mode to follow specifications.\n. @mokesmokes Thanks for clarification. I agree that in case request coming from actually allowed origin must not get that response. For such request headers should be set correctly. However I believe that in such situations function of processing error and setting CORS headers should not be combined in one (which as it seem is the case here). CORS headers is a one things and processing error is completely different. I guess most correct approach is to move CORS headers handling into engine.io completely (as you proposed) and same time split headers setting and error processing into two separate functions.\nI can look into that, however this will also take time as two packages need to be changed together to fix this thing.\n. ",
    "cheshun": "Is this problem solved?\n. ",
    "ookke": "I'm also seeing similar intermittent 502's with the same nginx log message with engine.io 1.5.1 so I guess this issue is still valid.\n. ",
    "beatjoerg": "Yes, we still see randomly 502, even with the patch above applied. \nI will look into this issue more deeply as soon as I can allocate a bit of time.\n. ",
    "andreassemborg": "I have the same problem.\n. ",
    "ryoqun": "Maybe we're affected this issue too.\n. @zcstarr Hmm, that's interesting info. Thanks for the tip!\n. ",
    "zcstarr": "Actually , I thought I had this issue earlier, then I put some debuggers in the Server code \n/socket.io/node_modules/engine.io/lib/server.js  specifically @ the server upgrade event handling, I realized that nginx was trying to upgrade an already upgraded web socket connection. There's no handler for that, so the solution in the Server.js code is to eventually time out and drop the socket connection , which triggered everyone's favorite 502, upstream prematurely closed connection while reading response header from upstream. Hope this helps !\n. ",
    "componentDidMount": "i have that problem aswell, 1 machine using nginx to proxy 10 different node processes, now i want every node process to host a little socket.io server but i can only have 1 running at the same time.\nall the others return 502 Bad Gateway \n. ",
    "julianlam": ":+1: still happens from what we can tell\n. @zcstarr If this is indeed the cause, then the underlying issue is that the client is trying to upgrade the connection twice, though I can't fathom a reason as to why that would happen.\nIf the connection is already upgraded, then the repeated call should also be answered with the expected HTTP/1.1 101 Switching Protocols, right?\n. @rauchg Can we get this addressed please? The errors propagated up to nginx cause it to consider the app \"down\", when it isn't.\n. :+1:. Excellent work @darrachequesne \ud83d\ude04 \nPing @barisusakli. ",
    "mQckingbird": "Any updates on this?. ",
    "awdrius": "If I use socket.io client for the website, isn't OPTIONS a mandatory request that browsers issue for the cross domain preflights?\nE.g. https://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS#Preflighted_requests\n. Unfortunately it is not possible as the API is accessed though multiple request channels and is exposed as a separate subdomain. \nI think that a setting to allow OPTIONS preflight request to be handled by engine.io through socket.io (or other) would make sense. It looks like there a plenty of devs out there who are burned by this after upgrading their libraries. A note in documentation with potential workaround(s), explanation, etc. would help. There's nothing worse than trying to make something work and have no idea what's wrong (-.\n[Edit] Explanation of \"simple request\" for those who might use it as a fix: https://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS\n. ",
    "dirkmc": "Hi mokesmokes, thanks for getting back to me so quickly.\nI would definitely prefer to use websockets as the transport rather than polling.\nThe nice thing about ELB is that it's elastic, meaning it can expand to higher capacity on demand. If instead I create my own LB using one of the options that you mentioned like nginx, I will need to route all traffic through one machine, which means there's a single point of failure. Additionally there is an upper limit to the number of sockets one machine can handle, and the site could potentially have millions of connections.\nI think probably the solution I'm going to go with is a mixture of these two approaches. I'll have ELB with TCP/SSL listeners, then behind that I'll have several instances, each with nginx and my websocket service on it. The nginx instances will take care of sticky load balancing by ip address using the ip_hash directive.\nIn order to make this work I'll need\n- To build nginx with realip so that it converts the x-forwarded-for header sent by ELB into what looks like an IP address to nginx\n- A monitoring script on each instance that\n  - polls ELB for new instances being added\n  - polls the other instances to make sure they're alive\n- If there's a change (a new instance comes up or one goes down) the monitoring script will overwrite the nginx config and signal nginx to reread its config so it can adjust load balancing to include new instances or exclude instances that are down. As far as I know nginx doesn't break existing connections when rereading config so it should not affect established connections.\n. I just wanted to give you guys an update for posterity.\nAfter taking a look at nginx, I found that most of the features I wanted to use require purchasing the commercial version, and the documentation was a little hard to follow. So instead I implemented a pure node js solution. This makes deployment much simpler, and also means I can couple the sticky load balancing with my websocket service quite tightly, so they both go down and come up together.\nI used node-http-proxy from nodejitsu to do the proxying, and proxywrap to take care of the PROXY protocol that Amazon's ELB uses to indicate the client's original IP address.\nThen I just needed to write code to ping the websocket service instances periodically and reroute traffic if one goes down. In the end it was only about 100 lines of code, and now the web site for Dasher is running on socket.io 1.x\nThanks again for the help, and the great library!\nDirk\n. Hey Pete,\nI have several service instances behind ELB, and each one has node-http-proxy/proxywrap on it.\nHere's a gist with the code:\nhttps://gist.github.com/dirkmc/967e827779ca7ab8c55f\nYou need to require() this file before setting up your http service in your main app.js file. You also need to define a route at /status.json that returns 200 so that the services can monitor each other to see when one goes down.\n. That's an interesting solution, although it feels a little strange to me to have ELB handle http traffic but not websocket traffic. The nice thing about ELB is that it will scale up elastically, but of course you need to have capacity on the back end to support however much traffic it passes through. The other nice thing is that ELB will terminate SSL for you, so you don't have to worry about having certificates deployed to all your instances.\nMy original plan was to have the proxy query ELB using the AWS API to find out about configuration changes (eg adding a new service box) but then I decided that I'm not going to change it very often so I would just hard code it in the config for simplicity. This means that if you want to add or remove an instance you need to change the config and deploy to all machines.\nSo far everything seems to be working pretty well.\nCheers,\nDirk\n. ",
    "petrogad": "@dirkmc Just curious-\nSo behind your ELB do you have 1 server that is your \"proxy\" server pushing out to each of your other EC2 instances? Or does each server behind the elb have node-http-proxy / proxywrap on it? \nI'm struggling with getting this setup on my instance and attempting to understand the server architecture.  \nThank you\n-Pete\n. Outstanding! Thank you so much for the help, I truly appreciate it.  Have you had any issues with this approach? Thus far I've been exposing the IP to the client as described here:\nhttp://coding-ceo.ghost.io/how-to-run-socket-io-behind-elb-on-aws/\nHave you been able to automate the scaling up and down by modifying the proxy configs on the fly?\nAgain thank you for the help, greatly appreciated :)\n-Pete\n. ",
    "jgrund": "Will this fix be pushed to a release soon?\n. ",
    "leearmstrong": "When will this make it into the live SocketIO code? Anyway to use it now?\n. Any ETA when this will be merged. This could be huge!!\n. ",
    "neemah": "sticky-session does not help if project runs on Heroku with several dynos. And this became show-stopper for horizontal scaling in our app :(\nWhat happens is:\n1. client connects to website (let's say backend no. 1 handles this connection). engine.io saves socket id in local variable.\n2. during protocol upgrade session client reconnects to website (backend no. 2 handles this connection). Engine.io checks socket id of second request in local client hash and fails (server.js: Server.verify()) with UNKNOWN_SID error.\nThis is so far the cause of the problem. Any suggestions how to handle this will be very helpful.\nThanks in advance.\n. @3rd-Eden i'd be very pleased if you suggest one that will handle sticky-session.\n. ",
    "defjamuk": "Is there any reason we need sticky sessions? It's an anti pattern. I would like to store the session information in a distributed database such as cassandra. If someone could point me in the right direction I would be willing to develop a module to do this with a cassandra data store. It would help our application horizontally scale on AWS.\n. ",
    "wzrdtales": "@3rd-Eden That is why I build https://github.com/wzrdtales/socket-io-sticky-session to support hashing informations from layer 4 instead. But I also would prefer to be able to use something else than sticky sessions, with layer 4 information it is now also possible to balance in a bit more controlled behavior, but the best thing would be to be able to just balance clients without caring to much about the handshake.\nThus the best option would be if engine.io would finally support a handshake that works across servers. For example in combination with a storage in between like redis.\n. ",
    "flashmandv": "Here is all the possible debug info: https://github.com/Automattic/socket.io/issues/1925\n. ",
    "johanneswuerbach": "Ah, I missed the ws bump there.\n. Rebased and passing now :-)\n. Yeah, looks like it was bumped now.\n. Strange looks like https://github.com/socketio/engine.io/compare/1.5.3...master it was only bumped in a tag\n. ",
    "STRML": "We are seeing this in #356 and https://github.com/primus/primus/issues/196#issuecomment-146569882. It is very hard to reproduce and I have not successfully been able to do so, but I am seeing it hit our production servers about once or twice a week. Last time I saw it, I was running iojs v2.5.0.\nI am, however, able to reproduce the issue using the following script:\n``` javascript\nvar stream = require('stream');\nvar a = new stream.PassThrough();\na.write('foobar');\nconsole.log('before handler');\na.on('data', function(chunk) {\n  console.log('data handler', chunk);\n});\nconsole.log('after handler');\na.setEncoding('utf8');\n```\nThe correct output is:\nbefore handler\nafter handler\ndata handler foobar\nEvery Node version > 0.10 has the correct behavior that I tested, but 0.10.40 does not:\nbefore handler\ndata handler <Buffer 66 6f 6f 62 61 72>\nafter handler\nSince engine.io still supports 0.10 it makes sense to merge this. I am not sure why I was seeing it in other versions - I will continue to investigate - but this is still a solid fix and needed for 0.10 users.\n. For future reference, this appears to be the commit that fixed the bug, which landed in v0.11.5. It's also known as streams3. It makes the commit comment \"There is no API change beyond this added flexibility\" somewhat inaccurate: https://github.com/nodejs/node/commit/0f8de5e1f96a07fa6de837378d29ac5f2719ec60#diff-ba6a0df0f5212f5cba5ca5179e209a17R709\n. @nkzawa Could you please review?\n. Thanks @rauchg, had this biting us at random for a long time now.\n. This is something we'll have to be prepared for in every browser - Safari\nhas been doing this for years, Chrome is next and I'm sure FF and IE are\nnot far behind. We simply can't rely on client timers being anywhere near\nreliable. I've seen 130+ (!) delays in Chrome 56. Word from them is that\nthey will set a max, but the writing is on the wall.\nOn Jan 22, 2017 3:28 PM, \"Luigi Pinca\" notifications@github.com wrote:\n\nYes, but the client timer that sends the ping is delayed by the browser so\nthe server incorrectly closes the connection.\nImagine that you used setTimeout(sendPing, 20000); on the client. The\nsendPing callback is not actually called after 20 sec but after 40 sec.\nHope it makes sense :)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/socketio/engine.io/issues/312#issuecomment-274361089,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABJFP9A_SydFX0hUCYKgTSZY1IZojB5Uks5rU8npgaJpZM4Dp2fJ\n.\n. Ah I think I know how this happens:\n\nThe 'data' listener must be called with null at some point while not in Binary mode.\nBecause the param data is not a string, this calls Buffer.concat(['', null]) which throws the error.\nA fix for this should simply be a if (!data) return at the beginning of the 'data' listener, because even in Binary mode this would cause a problem (Buffer.concat([new Buffer(0), null]) throws cannot read property 'length' of null).\nEdit: Actually this is not quite right, it will throw the length error before throwing the no method 'copy' error. So whatever data is getting through must not be a String, must have a length property, but is not a Buffer. OR - a Buffer is coming through even though isBinary is false.\n. So if it can't be a null being passed (because it's the wrong error), and HTTP requests are always supposed to call 'data' with Strings when req.setEncoding() is called, the only thing I can think of is that maybe the 'data' event is firing immediately after setting up the event listener (is that possible?) and before we call setEncoding, causing a Buffer to come down with the event.\n. Just saw this today on an instance running v4.1.2:\nTypeError: buf.copy is not a function\n    at Function.Buffer.concat (buffer.js:238:9)\n    at IncomingMessage.onData (/srv/api/node_modules/engine.io/lib/transports/polling.js:147:23)\n    at emitOne (events.js:77:13)\n    at IncomingMessage.emit (events.js:169:7)\n    at IncomingMessage.Readable.read (_stream_readable.js:360:10)\n    at flow (_stream_readable.js:733:26)\n    at resume_ (_stream_readable.js:713:3)\n    ...\n. ",
    "lyn0032": "just try..catch it. ",
    "kovbal": "When do you plan to release a new version?\n. ",
    "rmg": "Probably not necessary now that the converged node-v4.0.0 has been released, marking the end of the iojs release line.\n. Looks like it won't be as easy as just updating the version in package.json :-(\nTo save the digging, the failure on Travis is on this line:\nhttps://github.com/socketio/engine.io/blob/a4dcf1958b53ce15b74b783730e557a9e131eafa/test/server.js#L2204\n. Since node-v4.0.0 was just released, this is now even more useful to land/release.\n. PR #341 fixes this (and predates this issue!)\n. Duplicate of #341.\n. Duplicate of #341.\n. ",
    "yosuke-furukawa": "yes. I will close this.\n. ",
    "mathiasbynens": "Some confusing use of incorrect terminology here. I\u2019ll try to clear that up:\n\nSo the problem to me seems to be that the parser is expecting a raw string, but since the string is coming from node's HTTP server, it has already been decoded ~~to UTF8~~ from UTF-8 to Unicode, and ~~UTF8 decoding is not idempotent~~ this should only happen once.\n. \n",
    "calzoneman": "Thanks for clarifying that.\n\nOn Mar 16, 2015, at 08:39, Mathias Bynens notifications@github.com wrote:\nSome confusing use of terminology here. I\u2019ll try to clear that up:\nSo the problem to me seems to be that the parser is expecting a raw string, but since the string is coming from node's HTTP server, it has already been decoded to UTF8 from UTF-8 to Unicode, and UTF8 decoding is not idempotent this should only happen once.\n\u2014\nReply to this email directly or view it on GitHub.\n. It's been a while since I dug into this issue, but I'll try to take some time this week to look at it again.  I agree with Miguel; I think the problem is likely in the polling transport since the websocket transport does not appear to have this issue.. Bump.\n. Any news on this?\n. \n",
    "miguelgrinberg": "I stumbled upon this issue in the reverse direction on my Python engine.io server (https://github.com/miguelgrinberg/Flask-SocketIO/issues/246). The official JS engine.io client also sends strings with a double utf-8 encode to the server.\nMy solution is to check if a double utf-8 decode can be applied. If that succeeds, then I assume the packet must have been double encoded. If the second conversion fails due to invalid chars, then I assume the packet must have been single encoded.\n. @darrachequesne it's been a while since I looked at this. I don't remember the full details, but keep in mind that the websocket transport does not have this problem, I think the best place to address this is in the long-polling code, not on the parser.. ",
    "amiuhle": "Oops, just noticed this is a duplicate of  #306.\nBasically the same, but I updated the README, too.\n. ",
    "adrai": "uups sorry.... this was too fast... (I copied an older test that I made in engine.io-client)\n. ok?\n. done\n- And thanks to you ;-)\n. ",
    "nolanlawson": "OK, I can see that this is happening because there are 8 open pending GETs at the path:\n/engine.io/?EIO=3&transport=polling&t=1429386433980-9&sid=<sid>\nWhat's weird is that I'm calling close() on all my clients, so shouldn't that cancel these requests? Is there some way to manually cancel them? Or do I need to just make sure I only have one socket per browser or something, instead of constantly closing and re-opening sockets?\n. Never mind, I figured it out. My problem was I had too many open sockets on the client side that I wasn't close()ing.\nEvery open socket represents an open XHR GET longpoll request when you're in longpoll mode, and if you have >8 you're out of luck because the browser will stop sending any more requests. My bad.\n. Yup, hopefully the next poor soul who runs into this problem will google it and find this issue. :smiley:.\n. ",
    "Karnith": "bump... except for engine.io, all of socket.io is failing compile on win 8.1. (engine.io-client, socket.io-client, socket.io)\n. ",
    "martinoppitz": "Yes. That is the problem. Ws 0.7.1 must be released in engine io....  Wait wait wait\n. ",
    "rajatksud": "BUMP .... On npmjs engine io is referring to previous version of ws 0.4.31 due to which it fails to compile. The version on git is correct.\n. ",
    "barryloper": "The NPM version 1.5.1 of engine.io-client requires ws@0.4.31, while engine.io requires 0.5.0. The older version of ws is failing to compile on my Ubuntu 15.04 x64 system. Any reason the client requires a different version? Can you just update the req. to at least 0.5.0?\n. ",
    "tian0cai": "I also encountered the same problem.\nYEOMAN's almost never be used.\n. ",
    "yujiosaka": "@nkzawa  thanks!\n. ",
    "zensh": "@kapouer Server.prototype.generateId(req)?\nI have update the PR\n. @rauchg Could you merge this PR?\n. @rauchg Could you do me a favor?\n. Look for @rauchg\n. ",
    "chapgaga": "@johanneswuerbach what ws means?\n. @shinnn is engine.io websocket implementation for socket.io?\n. ",
    "stevedw": "Ah - I see from the protocol page that the library is expecting a particular protocol sequence and is not  a raw transport mechanism.\n. ",
    "whifflevcq": "Thanks @apeace and @nkzawa . You used transport polling or websocket?\nIn client, I use socket-client-java. When client send handshake socket, 3 TCP connection established. 3th connection will create socket connection. The remaining connections will be killed after 120s(Idle timeout). My problem is that the connection was not killed. I think transport.close() just solve 3th connection as I said above. But I'll test. Tomorrow will be about 1000 clients, hope everything will be fine.\n. I also recently tested over 1000 client user socket-client-java 1h ago. Normal would be about 6k TCP EST connection. And now ~ 1200 TCP EST connection. Seem the issue was solved. I will continue to monitor and feedback!\n. After about 30h. With ~700 client, have ~4000 TCP EST connection.Seem like to still memory leak. However this is a good sign. Because ago with about 1000 clients, after 12 hours, have ~ 30.000 TCP connection.\n. @c8439 In socket.io/node_modules/engine.io/lib/socket.js. Fix setPingTimeOut\nSocket.prototype.setPingTimeout = function () {\n  var self = this;\n  clearTimeout(self.pingTimeoutTimer);\n  self.pingTimeoutTimer = setTimeout(function () {\n    self.onClose('ping timeout');\n    self.transport.close(); // Fix this...\n  }, self.server.pingInterval + self.server.pingTimeout);\n};\n. @c8439 you should tag this issue in netty-socketio and thanks to mrniko help\n. ",
    "apeace": "After about 20 hours in production, I'm glad to report that this patch seems to be working. We're still hovering around ~200 open sockets for ~200 clients. Typically by this point we'd have > 1000 sockets for ~200 clients.\n. I can confirm what @whifflevcq said. There do still seem to be some sockets left open.\nAfter almost four days of running with this change, I'm seeing ~280 sockets open, with ~200 clients. After restarting the server (all sockets closed, then clients reconnect), I'm back to ~200 sockets open.\nSo this fix is a drastic improvement, but there does still seem to be another leak of socket file descriptors.\n. @3rd-Eden moving the line like you suggested didn't fix the failing test case for me. The same test case is failing on the master branch, it seems unrelated:\n1) server extraHeaders should arrive from client to server via WebSockets:\n     Uncaught Error: expected undefined to equal 'my-secret-access-token'\nThough maybe if we fix that one, we can get this PR merged :P\nThanks to everyone for your help! Glad this problem is fixed in production for all of you :)\n. ",
    "c8439": "@apeace Transport.close () how to configure, the effect is good\n. @whifflevcq  Hello, you that effect is good, what is modified\n. @whifflevcq  My server is this, https://github.com/mrniko/netty-socketio;\n. ",
    "nicofff": "After almost a day of usage this PR is looking good.\nI am seeing a slight difference between open sockets and connected clients, but it's remaining constant so far. I'm thinking it might have to do with counting sockets that are closing as active. \nJust updated my log collection script to account for this.\n. We ran this fix with the transports set to [\"websockets\"] for a few days, and then switched back the the default [\"polling\",\"websockets\"].\nIt does appear that this PR does help a lot with the issue when using websockets, but there is still another issue related to the polling transport.\n\n. Still seeing problems if the polling transport is enabled (either alone or in conjunction with websockets), even after applying this PR and @nkzawa's. \n. We're using https, no options other than cert and key. \n. Just tested this patch. It's causing reconnects every 30s\n. ",
    "char-lieee": "I had the same issue over last couple of weeks. My app is handling 2.5k users on 3.5k connected sockets online. I have up to 25k sessions per day.\nI've noticed that during the night when number of users drops to 100, number of opened connections on the server doesn't decrease, stays the same. \nOf course it was a big issue for my app - I had to increase ulimit -n on the server and restart my node.js app every couple of days. \nThis fix solves the problem. After implementing opened connections are killed when sockets disconnect. \nCheck out the graph. I started counting opened connection in the middle of the week, later on I implemented the fix, restarted server, there's weekend and after weekend you can see significant improvement. \n\nThanks a lot for this fix!\nBTW: My app is using polling as a default transport method. My clients do not use websockets.\n. I forgot to mention that fix was applied to application running on polling method. I don't use websockets at all, so in my case it works for polling. \n. ",
    "danpe": ":+1: \n. ",
    "rikukissa": "Any updates on this one? We're still experiencing a lot of connections not being closed when using polling transport with websocket transport. We're also using HTTPS.\n. The most recent one (1.6.4). Everything seems to be working fine now after we stopped using socket.io's https config property and started proxying all connections through nginx.\n. It seems so, but I really can't say for sure. Lets consider this as solved and I'll get back to you if we figure out what caused this in a first place. \n. ",
    "posita": ":+1: @rauchg, it may not come up very often, but when it does, it can be a real pain (see #363). :wink:\n. Okay, I did some more experimenting. I have been unable to produce this with a rudimentary JavaScript client. :confused:\nI tried inserting socat as a \"sniffer\" for each separate POST connection by doing something like this:\nsh\nfor (( i=0 ; i < 5 ; ++i)) ; do\n    socat 2>|\"socat-${i}.txt\" \\\n        -t10 -v -x \\\n        \"UNIX-LISTEN:integrations/node/http-snoop-${i}.sock,fork,mode=750,reuseaddr,unlink-early\" \\\n        UNIX-CONNECT:integrations/node/http.sock &\ndone\nThen I had the POSTers each connect to a different respective socket monitored by socat.1\nI still got a deadlock with my Twisted python version (note index 2):\n``` sh\n...\n================================================================\n\n\n\nFAIL: at least one of the requests is hung after 100 seconds! <<<\npost_deferreds[0]: >\npost_deferreds[1]: >\npost_deferreds[2]: \npost_deferreds[3]: >\npost_deferreds[4]: >\n================================================================\n...\n```\n\n\n\nServer logs from Twisted python clients:\nsh\n...\n[13:00:56.068] [LOG]     engine intercepting request for path \"/engine.io/\" +2m\n[13:00:56.068] [LOG]     engine handling \"GET\" http request \"/engine.io/?transport=polling\" +1ms\n[13:00:56.068] [LOG]     engine handshaking client \"i5IEMr13XT65gRsIAAAF\" +0ms\n[13:00:56.069] [LOG]     engine:socket sending packet \"open\" ({\"sid\":\"i5IEMr13XT65gRsIAAAF\",\"upgrades\":[\"websocket\"],\"pingInterval\":25000,\"pingTimeout\":60000}) +1ms\n[13:00:56.069] [LOG]     engine:polling setting request +0ms\n[13:00:56.069] [LOG]     engine:socket flushing buffer to transport +0ms\n[13:00:56.070] [LOG]     engine:polling writing \"       \ufffd0{\"sid\":\"i5IEMr13XT65gRsIAAAF\",\"upgrades\":[\"websocket\"],\"pingInterval\":25000,\"pingTimeout\":60000}\" +1ms\n[13:00:56.070] [LOG]     engine:socket executing batch send callback +0ms\n[13:00:56.130] [LOG]     engine intercepting request for path \"/engine.io/\" +60ms\n[13:00:56.130] [LOG]     engine handling \"POST\" http request \"/engine.io/?transport=polling&sid=i5IEMr13XT65gRsIAAAF\" +0ms\n[13:00:56.132] [LOG]     engine setting new request for existing client +1ms\n[13:00:56.135] [LOG]     engine intercepting request for path \"/engine.io/\" +3ms\n[13:00:56.138] [LOG]     engine handling \"POST\" http request \"/engine.io/?transport=polling&sid=i5IEMr13XT65gRsIAAAF\" +4ms\n[13:00:56.139] [LOG]     engine setting new request for existing client +1ms\n[13:00:56.140] [LOG]     engine:socket transport error +1ms\n[13:00:56.140] [LOG]     engine:polling received \"\ufffd2probe\" +0ms\n[13:00:56.140] [LOG]     engine:socket packet received with closed socket +0ms\n[13:00:56.141] [LOG]     engine intercepting request for path \"/engine.io/\" +1ms\n[13:00:56.143] [LOG]     engine handling \"POST\" http request \"/engine.io/?transport=polling&sid=i5IEMr13XT65gRsIAAAF\" +2ms\n[13:00:56.144] [LOG]     engine intercepting request for path \"/engine.io/\" +1ms\n[13:00:56.144] [LOG]     engine handling \"POST\" http request \"/engine.io/?transport=polling&sid=i5IEMr13XT65gRsIAAAF\" +0ms\n[13:00:56.145] [LOG]     engine intercepting request for path \"/engine.io/\" +1ms\n[13:00:56.147] [LOG]     engine handling \"POST\" http request \"/engine.io/?transport=polling&sid=i5IEMr13XT65gRsIAAAF\" +2ms\n...\nsocat sniffer output from Twisted python clients (note that nothing comes back over the wire from Engine.IO for index 2):\n``` sh\n% # ================================================================\n% cat socat-0.txt\n\n2015/11/13 13:00:56.126765  length=287 from=0 to=286\n 50 4f 53 54 20 2f 65 6e 67 69 6e 65 2e 69 6f 2f  POST /engine.io/\n 3f 74 72 61 6e 73 70 6f 72 74 3d 70 6f 6c 6c 69  ?transport=polli\n 6e 67 26 73 69 64 3d 69 35 49 45 4d 72 31 33 58  ng&sid=i5IEMr13X\n 54 36 35 67 52 73 49 41 41 41 46 20 48 54 54 50  T65gRsIAAAF HTTP\n 2f 31 2e 31 0d 0a                                /1.1..\n 43 6f 6e 6e 65 63 74 69 6f 6e 3a 20 63 6c 6f 73  Connection: clos\n 65 0d 0a                                         e..\n 43 6f 6e 74 65 6e 74 2d 4c 65 6e 67 74 68 3a 20  Content-Length:\n 39 0d 0a                                         9..\n 41 63 63 65 70 74 3a 20 61 70 70 6c 69 63 61 74  Accept: applicat\n 69 6f 6e 2f 6f 63 74 65 74 2d 73 74 72 65 61 6d  ion/octet-stream\n 0d 0a                                            ..\n 41 63 63 65 70 74 3a 20 61 70 70 6c 69 63 61 74  Accept: applicat\n 69 6f 6e 2f 6a 73 6f 6e 0d 0a                    ion/json..\n 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 20 61 70  Content-Type: ap\n 70 6c 69 63 61 74 69 6f 6e 2f 6f 63 74 65 74 2d  plication/octet-\n 73 74 72 65 61 6d 0d 0a                          stream..\n 48 6f 73 74 3a 20 2e 25 32 46 69 6e 74 65 67 72  Host: .%2Fintegr\n 61 74 69 6f 6e 73 25 32 46 6e 6f 64 65 25 32 46  ations%2Fnode%2F\n 68 74 74 70 2d 73 6e 6f 6f 70 2d 30 2e 73 6f 63  http-snoop-0.soc\n 6b 3a 38 30 0d 0a                                k:80..\n 41 63 63 65 70 74 2d 43 68 61 72 73 65 74 3a 20  Accept-Charset:\n 55 54 46 2d 38 0d 0a                             UTF-8..\n 0d 0a                                            ..\n--\n2015/11/13 13:00:56.139455  length=9 from=287 to=295\n 00 06 ff 32 70 72 6f 62 65                       ...2probe\n--\n< 2015/11/13 13:00:56.141060  length=190 from=0 to=189\n 48 54 54 50 2f 31 2e 31 20 32 30 30 20 4f 4b 0d  HTTP/1.1 200 OK.\n 0a                                               .\n 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 20 74 65  Content-Type: te\n 78 74 2f 68 74 6d 6c 0d 0a                       xt/html..\n 43 6f 6e 74 65 6e 74 2d 4c 65 6e 67 74 68 3a 20  Content-Length:\n 32 0d 0a                                         2..\n 41 63 63 65 73 73 2d 43 6f 6e 74 72 6f 6c 2d 41  Access-Control-A\n 6c 6c 6f 77 2d 4f 72 69 67 69 6e 3a 20 2a 0d 0a  llow-Origin: ..\n 53 65 74 2d 43 6f 6f 6b 69 65 3a 20 69 6f 3d 69  Set-Cookie: io=i\n 35 49 45 4d 72 31 33 58 54 36 35 67 52 73 49 41  5IEMr13XT65gRsIA\n 41 41 46 0d 0a                                   AAF..\n 44 61 74 65 3a 20 46 72 69 2c 20 31 33 20 4e 6f  Date: Fri, 13 No\n 76 20 32 30 31 35 20 32 31 3a 30 30 3a 35 36 20  v 2015 21:00:56\n 47 4d 54 0d 0a                                   GMT..\n 43 6f 6e 6e 65 63 74 69 6f 6e 3a 20 63 6c 6f 73  Connection: clos\n 65 0d 0a                                         e..\n 0d 0a                                            ..\n 6f 6b                                            ok\n--\n% # ================================================================\n% cat socat-1.txt\n2015/11/13 13:00:56.129741  length=287 from=0 to=286\n 50 4f 53 54 20 2f 65 6e 67 69 6e 65 2e 69 6f 2f  POST /engine.io/\n 3f 74 72 61 6e 73 70 6f 72 74 3d 70 6f 6c 6c 69  ?transport=polli\n 6e 67 26 73 69 64 3d 69 35 49 45 4d 72 31 33 58  ng&sid=i5IEMr13X\n 54 36 35 67 52 73 49 41 41 41 46 20 48 54 54 50  T65gRsIAAAF HTTP\n 2f 31 2e 31 0d 0a                                /1.1..\n 43 6f 6e 6e 65 63 74 69 6f 6e 3a 20 63 6c 6f 73  Connection: clos\n 65 0d 0a                                         e..\n 43 6f 6e 74 65 6e 74 2d 4c 65 6e 67 74 68 3a 20  Content-Length:\n 39 0d 0a                                         9..\n 41 63 63 65 70 74 3a 20 61 70 70 6c 69 63 61 74  Accept: applicat\n 69 6f 6e 2f 6f 63 74 65 74 2d 73 74 72 65 61 6d  ion/octet-stream\n 0d 0a                                            ..\n 41 63 63 65 70 74 3a 20 61 70 70 6c 69 63 61 74  Accept: applicat\n 69 6f 6e 2f 6a 73 6f 6e 0d 0a                    ion/json..\n 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 20 61 70  Content-Type: ap\n 70 6c 69 63 61 74 69 6f 6e 2f 6f 63 74 65 74 2d  plication/octet-\n 73 74 72 65 61 6d 0d 0a                          stream..\n 48 6f 73 74 3a 20 2e 25 32 46 69 6e 74 65 67 72  Host: .%2Fintegr\n 61 74 69 6f 6e 73 25 32 46 6e 6f 64 65 25 32 46  ations%2Fnode%2F\n 68 74 74 70 2d 73 6e 6f 6f 70 2d 31 2e 73 6f 63  http-snoop-1.soc\n 6b 3a 38 30 0d 0a                                k:80..\n 41 63 63 65 70 74 2d 43 68 61 72 73 65 74 3a 20  Accept-Charset:\n 55 54 46 2d 38 0d 0a                             UTF-8..\n 0d 0a                                            ..\n--\n2015/11/13 13:00:56.139738  length=9 from=287 to=295\n 00 06 ff 32 70 72 6f 62 65                       ...2probe\n--\n< 2015/11/13 13:00:56.146527  length=228 from=0 to=227\n 48 54 54 50 2f 31 2e 31 20 34 30 30 20 42 61 64  HTTP/1.1 400 Bad\n 20 52 65 71 75 65 73 74 0d 0a                     Request..\n 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 20 61 70  Content-Type: ap\n 70 6c 69 63 61 74 69 6f 6e 2f 6a 73 6f 6e 0d 0a  plication/json..\n 41 63 63 65 73 73 2d 43 6f 6e 74 72 6f 6c 2d 41  Access-Control-A\n 6c 6c 6f 77 2d 4f 72 69 67 69 6e 3a 20 2a 0d 0a  llow-Origin: ..\n 44 61 74 65 3a 20 46 72 69 2c 20 31 33 20 4e 6f  Date: Fri, 13 No\n 76 20 32 30 31 35 20 32 31 3a 30 30 3a 35 36 20  v 2015 21:00:56\n 47 4d 54 0d 0a                                   GMT..\n 43 6f 6e 6e 65 63 74 69 6f 6e 3a 20 63 6c 6f 73  Connection: clos\n 65 0d 0a                                         e..\n 54 72 61 6e 73 66 65 72 2d 45 6e 63 6f 64 69 6e  Transfer-Encodin\n 67 3a 20 63 68 75 6e 6b 65 64 0d 0a              g: chunked..\n 0d 0a                                            ..\n 32 39 0d 0a                                      29..\n 7b 22 63 6f 64 65 22 3a 31 2c 22 6d 65 73 73 61  {\"code\":1,\"messa\n 67 65 22 3a 22 53 65 73 73 69 6f 6e 20 49 44 20  ge\":\"Session ID\n 75 6e 6b 6e 6f 77 6e 22 7d 0d 0a                 unknown\"}..\n 30 0d 0a                                         0..\n 0d 0a                                            ..\n--\n% # ================================================================\n% cat socat-2.txt # <-- this is the deadlocked one\n2015/11/13 13:00:56.126726  length=287 from=0 to=286\n 50 4f 53 54 20 2f 65 6e 67 69 6e 65 2e 69 6f 2f  POST /engine.io/\n 3f 74 72 61 6e 73 70 6f 72 74 3d 70 6f 6c 6c 69  ?transport=polli\n 6e 67 26 73 69 64 3d 69 35 49 45 4d 72 31 33 58  ng&sid=i5IEMr13X\n 54 36 35 67 52 73 49 41 41 41 46 20 48 54 54 50  T65gRsIAAAF HTTP\n 2f 31 2e 31 0d 0a                                /1.1..\n 43 6f 6e 6e 65 63 74 69 6f 6e 3a 20 63 6c 6f 73  Connection: clos\n 65 0d 0a                                         e..\n 43 6f 6e 74 65 6e 74 2d 4c 65 6e 67 74 68 3a 20  Content-Length:\n 39 0d 0a                                         9..\n 41 63 63 65 70 74 3a 20 61 70 70 6c 69 63 61 74  Accept: applicat\n 69 6f 6e 2f 6f 63 74 65 74 2d 73 74 72 65 61 6d  ion/octet-stream\n 0d 0a                                            ..\n 41 63 63 65 70 74 3a 20 61 70 70 6c 69 63 61 74  Accept: applicat\n 69 6f 6e 2f 6a 73 6f 6e 0d 0a                    ion/json..\n 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 20 61 70  Content-Type: ap\n 70 6c 69 63 61 74 69 6f 6e 2f 6f 63 74 65 74 2d  plication/octet-\n 73 74 72 65 61 6d 0d 0a                          stream..\n 48 6f 73 74 3a 20 2e 25 32 46 69 6e 74 65 67 72  Host: .%2Fintegr\n 61 74 69 6f 6e 73 25 32 46 6e 6f 64 65 25 32 46  ations%2Fnode%2F\n 68 74 74 70 2d 73 6e 6f 6f 70 2d 32 2e 73 6f 63  http-snoop-2.soc\n 6b 3a 38 30 0d 0a                                k:80..\n 41 63 63 65 70 74 2d 43 68 61 72 73 65 74 3a 20  Accept-Charset:\n 55 54 46 2d 38 0d 0a                             UTF-8..\n 0d 0a                                            ..\n--\n2015/11/13 13:00:56.139332  length=9 from=287 to=295\n 00 06 ff 32 70 72 6f 62 65                       ...2probe\n--\n% # ================================================================\n% cat socat-3.txt\n2015/11/13 13:00:56.133296  length=287 from=0 to=286\n 50 4f 53 54 20 2f 65 6e 67 69 6e 65 2e 69 6f 2f  POST /engine.io/\n 3f 74 72 61 6e 73 70 6f 72 74 3d 70 6f 6c 6c 69  ?transport=polli\n 6e 67 26 73 69 64 3d 69 35 49 45 4d 72 31 33 58  ng&sid=i5IEMr13X\n 54 36 35 67 52 73 49 41 41 41 46 20 48 54 54 50  T65gRsIAAAF HTTP\n 2f 31 2e 31 0d 0a                                /1.1..\n 43 6f 6e 6e 65 63 74 69 6f 6e 3a 20 63 6c 6f 73  Connection: clos\n 65 0d 0a                                         e..\n 43 6f 6e 74 65 6e 74 2d 4c 65 6e 67 74 68 3a 20  Content-Length:\n 39 0d 0a                                         9..\n 41 63 63 65 70 74 3a 20 61 70 70 6c 69 63 61 74  Accept: applicat\n 69 6f 6e 2f 6f 63 74 65 74 2d 73 74 72 65 61 6d  ion/octet-stream\n 0d 0a                                            ..\n 41 63 63 65 70 74 3a 20 61 70 70 6c 69 63 61 74  Accept: applicat\n 69 6f 6e 2f 6a 73 6f 6e 0d 0a                    ion/json..\n 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 20 61 70  Content-Type: ap\n 70 6c 69 63 61 74 69 6f 6e 2f 6f 63 74 65 74 2d  plication/octet-\n 73 74 72 65 61 6d 0d 0a                          stream..\n 48 6f 73 74 3a 20 2e 25 32 46 69 6e 74 65 67 72  Host: .%2Fintegr\n 61 74 69 6f 6e 73 25 32 46 6e 6f 64 65 25 32 46  ations%2Fnode%2F\n 68 74 74 70 2d 73 6e 6f 6f 70 2d 33 2e 73 6f 63  http-snoop-3.soc\n 6b 3a 38 30 0d 0a                                k:80..\n 41 63 63 65 70 74 2d 43 68 61 72 73 65 74 3a 20  Accept-Charset:\n 55 54 46 2d 38 0d 0a                             UTF-8..\n 0d 0a                                            ..\n--\n2015/11/13 13:00:56.139181  length=9 from=287 to=295\n 00 06 ff 32 70 72 6f 62 65                       ...2probe\n--\n< 2015/11/13 13:00:56.148496  length=228 from=0 to=227\n 48 54 54 50 2f 31 2e 31 20 34 30 30 20 42 61 64  HTTP/1.1 400 Bad\n 20 52 65 71 75 65 73 74 0d 0a                     Request..\n 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 20 61 70  Content-Type: ap\n 70 6c 69 63 61 74 69 6f 6e 2f 6a 73 6f 6e 0d 0a  plication/json..\n 41 63 63 65 73 73 2d 43 6f 6e 74 72 6f 6c 2d 41  Access-Control-A\n 6c 6c 6f 77 2d 4f 72 69 67 69 6e 3a 20 2a 0d 0a  llow-Origin: ..\n 44 61 74 65 3a 20 46 72 69 2c 20 31 33 20 4e 6f  Date: Fri, 13 No\n 76 20 32 30 31 35 20 32 31 3a 30 30 3a 35 36 20  v 2015 21:00:56\n 47 4d 54 0d 0a                                   GMT..\n 43 6f 6e 6e 65 63 74 69 6f 6e 3a 20 63 6c 6f 73  Connection: clos\n 65 0d 0a                                         e..\n 54 72 61 6e 73 66 65 72 2d 45 6e 63 6f 64 69 6e  Transfer-Encodin\n 67 3a 20 63 68 75 6e 6b 65 64 0d 0a              g: chunked..\n 0d 0a                                            ..\n 32 39 0d 0a                                      29..\n 7b 22 63 6f 64 65 22 3a 31 2c 22 6d 65 73 73 61  {\"code\":1,\"messa\n 67 65 22 3a 22 53 65 73 73 69 6f 6e 20 49 44 20  ge\":\"Session ID\n 75 6e 6b 6e 6f 77 6e 22 7d 0d 0a                 unknown\"}..\n 30 0d 0a                                         0..\n 0d 0a                                            ..\n--\n% # ================================================================\n% cat socat-4.txt\n2015/11/13 13:00:56.127379  length=287 from=0 to=286\n 50 4f 53 54 20 2f 65 6e 67 69 6e 65 2e 69 6f 2f  POST /engine.io/\n 3f 74 72 61 6e 73 70 6f 72 74 3d 70 6f 6c 6c 69  ?transport=polli\n 6e 67 26 73 69 64 3d 69 35 49 45 4d 72 31 33 58  ng&sid=i5IEMr13X\n 54 36 35 67 52 73 49 41 41 41 46 20 48 54 54 50  T65gRsIAAAF HTTP\n 2f 31 2e 31 0d 0a                                /1.1..\n 43 6f 6e 6e 65 63 74 69 6f 6e 3a 20 63 6c 6f 73  Connection: clos\n 65 0d 0a                                         e..\n 43 6f 6e 74 65 6e 74 2d 4c 65 6e 67 74 68 3a 20  Content-Length:\n 39 0d 0a                                         9..\n 41 63 63 65 70 74 3a 20 61 70 70 6c 69 63 61 74  Accept: applicat\n 69 6f 6e 2f 6f 63 74 65 74 2d 73 74 72 65 61 6d  ion/octet-stream\n 0d 0a                                            ..\n 41 63 63 65 70 74 3a 20 61 70 70 6c 69 63 61 74  Accept: applicat\n 69 6f 6e 2f 6a 73 6f 6e 0d 0a                    ion/json..\n 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 20 61 70  Content-Type: ap\n 70 6c 69 63 61 74 69 6f 6e 2f 6f 63 74 65 74 2d  plication/octet-\n 73 74 72 65 61 6d 0d 0a                          stream..\n 48 6f 73 74 3a 20 2e 25 32 46 69 6e 74 65 67 72  Host: .%2Fintegr\n 61 74 69 6f 6e 73 25 32 46 6e 6f 64 65 25 32 46  ations%2Fnode%2F\n 68 74 74 70 2d 73 6e 6f 6f 70 2d 34 2e 73 6f 63  http-snoop-4.soc\n 6b 3a 38 30 0d 0a                                k:80..\n 41 63 63 65 70 74 2d 43 68 61 72 73 65 74 3a 20  Accept-Charset:\n 55 54 46 2d 38 0d 0a                             UTF-8..\n 0d 0a                                            ..\n--\n2015/11/13 13:00:56.139469  length=9 from=287 to=295\n 00 06 ff 32 70 72 6f 62 65                       ...2probe\n--\n< 2015/11/13 13:00:56.144137  length=228 from=0 to=227\n 48 54 54 50 2f 31 2e 31 20 34 30 30 20 42 61 64  HTTP/1.1 400 Bad\n 20 52 65 71 75 65 73 74 0d 0a                     Request..\n 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 20 61 70  Content-Type: ap\n 70 6c 69 63 61 74 69 6f 6e 2f 6a 73 6f 6e 0d 0a  plication/json..\n 41 63 63 65 73 73 2d 43 6f 6e 74 72 6f 6c 2d 41  Access-Control-A\n 6c 6c 6f 77 2d 4f 72 69 67 69 6e 3a 20 2a 0d 0a  llow-Origin: ..\n 44 61 74 65 3a 20 46 72 69 2c 20 31 33 20 4e 6f  Date: Fri, 13 No\n 76 20 32 30 31 35 20 32 31 3a 30 30 3a 35 36 20  v 2015 21:00:56\n 47 4d 54 0d 0a                                   GMT..\n 43 6f 6e 6e 65 63 74 69 6f 6e 3a 20 63 6c 6f 73  Connection: clos\n 65 0d 0a                                         e..\n 54 72 61 6e 73 66 65 72 2d 45 6e 63 6f 64 69 6e  Transfer-Encodin\n 67 3a 20 63 68 75 6e 6b 65 64 0d 0a              g: chunked..\n 0d 0a                                            ..\n 32 39 0d 0a                                      29..\n 7b 22 63 6f 64 65 22 3a 31 2c 22 6d 65 73 73 61  {\"code\":1,\"messa\n 67 65 22 3a 22 53 65 73 73 69 6f 6e 20 49 44 20  ge\":\"Session ID\n 75 6e 6b 6e 6f 77 6e 22 7d 0d 0a                 unknown\"}..\n 30 0d 0a                                         0..\n 0d 0a                                            ..\n--\n```\n\nCompare this to when I run the JavaScript client version (ran with ( cd .../txsocketio/deadlock && npm install && npm start )).\nServer logs from JavaScript clients:\nsh\n...\n[12:58:39.255] [LOG]     engine intercepting request for path \"/engine.io/\" +7m\n[12:58:39.255] [LOG]     engine handling \"GET\" http request \"/engine.io/?transport=polling\" +0ms\n[12:58:39.257] [LOG]     engine handshaking client \"Qr_EvRVelef7YMRDAAAE\" +2ms\n[12:58:39.257] [LOG]     engine:socket sending packet \"open\" ({\"sid\":\"Qr_EvRVelef7YMRDAAAE\",\"upgrades\":[\"websocket\"],\"pingInterval\":25000,\"pingTimeout\":60000}) +0ms\n[12:58:39.257] [LOG]     engine:polling setting request +0ms\n[12:58:39.258] [LOG]     engine:socket flushing buffer to transport +1ms\n[12:58:39.258] [LOG]     engine:polling writing \"       \ufffd0{\"sid\":\"Qr_EvRVelef7YMRDAAAE\",\"upgrades\":[\"websocket\"],\"pingInterval\":25000,\"pingTimeout\":60000}\" +0ms\n[12:58:39.258] [LOG]     engine:socket executing batch send callback +0ms\n[12:58:39.328] [LOG]     engine intercepting request for path \"/engine.io/\" +70ms\n[12:58:39.329] [LOG]     engine handling \"POST\" http request \"/engine.io/?transport=polling&sid=Qr_EvRVelef7YMRDAAAE\" +1ms\n[12:58:39.329] [LOG]     engine setting new request for existing client +0ms\n[12:58:39.330] [LOG]     engine:polling received \"\ufffd2probe\" +1ms\n[12:58:39.330] [LOG]     engine:socket packet +0ms\n[12:58:39.330] [LOG]     engine:socket got ping +0ms\n[12:58:39.331] [LOG]     engine:socket sending packet \"pong\" (undefined) +1ms\n[12:58:39.332] [LOG]     engine intercepting request for path \"/engine.io/\" +1ms\n[12:58:39.332] [LOG]     engine handling \"POST\" http request \"/engine.io/?transport=polling&sid=Qr_EvRVelef7YMRDAAAE\" +0ms\n[12:58:39.332] [LOG]     engine setting new request for existing client +0ms\n[12:58:39.333] [LOG]     engine:polling received \"\ufffd2probe\" +1ms\n[12:58:39.333] [LOG]     engine:socket packet +0ms\n[12:58:39.334] [LOG]     engine:socket got ping +1ms\n[12:58:39.334] [LOG]     engine:socket sending packet \"pong\" (undefined) +0ms\n[12:58:39.335] [LOG]     engine intercepting request for path \"/engine.io/\" +1ms\n[12:58:39.335] [LOG]     engine handling \"POST\" http request \"/engine.io/?transport=polling&sid=Qr_EvRVelef7YMRDAAAE\" +0ms\n[12:58:39.335] [LOG]     engine setting new request for existing client +0ms\n[12:58:39.336] [LOG]     engine:polling received \"\ufffd2probe\" +1ms\n[12:58:39.336] [LOG]     engine:socket packet +0ms\n[12:58:39.336] [LOG]     engine:socket got ping +0ms\n[12:58:39.337] [LOG]     engine:socket sending packet \"pong\" (undefined) +1ms\n[12:58:39.342] [LOG]     engine intercepting request for path \"/engine.io/\" +5ms\n[12:58:39.343] [LOG]     engine handling \"POST\" http request \"/engine.io/?transport=polling&sid=Qr_EvRVelef7YMRDAAAE\" +0ms\n[12:58:39.343] [LOG]     engine setting new request for existing client +1ms\n[12:58:39.343] [LOG]     engine:polling received \"\ufffd2probe\" +0ms\n[12:58:39.343] [LOG]     engine:socket packet +0ms\n[12:58:39.344] [LOG]     engine:socket got ping +1ms\n[12:58:39.344] [LOG]     engine:socket sending packet \"pong\" (undefined) +0ms\n[12:58:39.345] [LOG]     engine intercepting request for path \"/engine.io/\" +1ms\n[12:58:39.345] [LOG]     engine handling \"POST\" http request \"/engine.io/?transport=polling&sid=Qr_EvRVelef7YMRDAAAE\" +0ms\n[12:58:39.346] [LOG]     engine setting new request for existing client +1ms\n[12:58:39.346] [LOG]     engine:polling received \"\ufffd2probe\" +0ms\n[12:58:39.346] [LOG]     engine:socket packet +0ms\n[12:58:39.347] [LOG]     engine:socket got ping +1ms\n[12:58:39.353] [LOG]     engine:socket sending packet \"pong\" (undefined) +6ms\n...\nsocat sniffer output from JavaScript clients (truncated, they're pretty much all like this):\n``` sh\n...\n% # ================================================================\n% cat socat-1.txt\n\n2015/11/13 12:58:39.325950  length=304 from=0 to=303\n 50 4f 53 54 20 2f 65 6e 67 69 6e 65 2e 69 6f 2f  POST /engine.io/\n 3f 74 72 61 6e 73 70 6f 72 74 3d 70 6f 6c 6c 69  ?transport=polli\n 6e 67 26 73 69 64 3d 51 72 5f 45 76 52 56 65 6c  ng&sid=Qr_EvRVel\n 65 66 37 59 4d 52 44 41 41 41 45 20 48 54 54 50  ef7YMRDAAAE HTTP\n 2f 31 2e 31 0d 0a                                /1.1..\n 41 63 63 65 70 74 3a 20 61 70 70 6c 69 63 61 74  Accept: applicat\n 69 6f 6e 2f 6f 63 74 65 74 2d 73 74 72 65 61 6d  ion/octet-stream\n 2c 61 70 70 6c 69 63 61 74 69 6f 6e 2f 6a 73 6f  ,application/jso\n 6e 0d 0a                                         n..\n 41 63 63 65 70 74 2d 43 68 61 72 73 65 74 3a 20  Accept-Charset:\n 55 54 46 2d 38 0d 0a                             UTF-8..\n 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 20 61 70  Content-Type: ap\n 70 6c 69 63 61 74 69 6f 6e 2f 6f 63 74 65 74 2d  plication/octet-\n 73 74 72 65 61 6d 0d 0a                          stream..\n 68 6f 73 74 3a 20 2f 55 73 65 72 73 2f 6d 61 74  host: /Users/mat\n 74 2f 44 6f 63 75 6d 65 6e 74 73 2f 64 65 76 2f  t/Documents/dev/\n 74 78 73 6f 63 6b 65 74 69 6f 2f 69 6e 74 65 67  txsocketio/integ\n 72 61 74 69 6f 6e 73 2f 6e 6f 64 65 2f 68 74 74  rations/node/htt\n 70 2d 73 6e 6f 6f 70 2d 31 2e 73 6f 63 6b 0d 0a  p-snoop-1.sock..\n 63 6f 6e 74 65 6e 74 2d 6c 65 6e 67 74 68 3a 20  content-length:\n 39 0d 0a                                         9..\n 43 6f 6e 6e 65 63 74 69 6f 6e 3a 20 63 6c 6f 73  Connection: clos\n 65 0d 0a                                         e..\n 0d 0a                                            ..\n--\n2015/11/13 12:58:39.330303  length=9 from=304 to=312\n 00 06 ff 32 70 72 6f 62 65                       ...2probe\n--\n< 2015/11/13 12:58:39.344884  length=190 from=0 to=189\n 48 54 54 50 2f 31 2e 31 20 32 30 30 20 4f 4b 0d  HTTP/1.1 200 OK.\n 0a                                               .\n 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 20 74 65  Content-Type: te\n 78 74 2f 68 74 6d 6c 0d 0a                       xt/html..\n 43 6f 6e 74 65 6e 74 2d 4c 65 6e 67 74 68 3a 20  Content-Length:\n 32 0d 0a                                         2..\n 41 63 63 65 73 73 2d 43 6f 6e 74 72 6f 6c 2d 41  Access-Control-A\n 6c 6c 6f 77 2d 4f 72 69 67 69 6e 3a 20 2a 0d 0a  llow-Origin: ..\n 53 65 74 2d 43 6f 6f 6b 69 65 3a 20 69 6f 3d 51  Set-Cookie: io=Q\n 72 5f 45 76 52 56 65 6c 65 66 37 59 4d 52 44 41  r_EvRVelef7YMRDA\n 41 41 45 0d 0a                                   AAE..\n 44 61 74 65 3a 20 46 72 69 2c 20 31 33 20 4e 6f  Date: Fri, 13 No\n 76 20 32 30 31 35 20 32 30 3a 35 38 3a 33 39 20  v 2015 20:58:39\n 47 4d 54 0d 0a                                   GMT..\n 43 6f 6e 6e 65 63 74 69 6f 6e 3a 20 63 6c 6f 73  Connection: clos\n 65 0d 0a                                         e..\n 0d 0a                                            ..\n 6f 6b                                            ok\n--\n% # ================================================================\n% cat socat-2.txt\n2015/11/13 12:58:39.326715  length=313 from=0 to=312\n 50 4f 53 54 20 2f 65 6e 67 69 6e 65 2e 69 6f 2f  POST /engine.io/\n 3f 74 72 61 6e 73 70 6f 72 74 3d 70 6f 6c 6c 69  ?transport=polli\n 6e 67 26 73 69 64 3d 51 72 5f 45 76 52 56 65 6c  ng&sid=Qr_EvRVel\n 65 66 37 59 4d 52 44 41 41 41 45 20 48 54 54 50  ef7YMRDAAAE HTTP\n 2f 31 2e 31 0d 0a                                /1.1..\n 41 63 63 65 70 74 3a 20 61 70 70 6c 69 63 61 74  Accept: applicat\n 69 6f 6e 2f 6f 63 74 65 74 2d 73 74 72 65 61 6d  ion/octet-stream\n 2c 61 70 70 6c 69 63 61 74 69 6f 6e 2f 6a 73 6f  ,application/jso\n 6e 0d 0a                                         n..\n 41 63 63 65 70 74 2d 43 68 61 72 73 65 74 3a 20  Accept-Charset:\n 55 54 46 2d 38 0d 0a                             UTF-8..\n 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 20 61 70  Content-Type: ap\n 70 6c 69 63 61 74 69 6f 6e 2f 6f 63 74 65 74 2d  plication/octet-\n 73 74 72 65 61 6d 0d 0a                          stream..\n 68 6f 73 74 3a 20 2f 55 73 65 72 73 2f 6d 61 74  host: /Users/mat\n 74 2f 44 6f 63 75 6d 65 6e 74 73 2f 64 65 76 2f  t/Documents/dev/\n 74 78 73 6f 63 6b 65 74 69 6f 2f 69 6e 74 65 67  txsocketio/integ\n 72 61 74 69 6f 6e 73 2f 6e 6f 64 65 2f 68 74 74  rations/node/htt\n 70 2d 73 6e 6f 6f 70 2d 32 2e 73 6f 63 6b 0d 0a  p-snoop-2.sock..\n 63 6f 6e 74 65 6e 74 2d 6c 65 6e 67 74 68 3a 20  content-length:\n 39 0d 0a                                         9..\n 43 6f 6e 6e 65 63 74 69 6f 6e 3a 20 63 6c 6f 73  Connection: clos\n 65 0d 0a                                         e..\n 0d 0a                                            ..\n 00 06 ff 32 70 72 6f 62 65                       ...2probe\n--\n< 2015/11/13 12:58:39.336700  length=190 from=0 to=189\n 48 54 54 50 2f 31 2e 31 20 32 30 30 20 4f 4b 0d  HTTP/1.1 200 OK.\n 0a                                               .\n 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 20 74 65  Content-Type: te\n 78 74 2f 68 74 6d 6c 0d 0a                       xt/html..\n 43 6f 6e 74 65 6e 74 2d 4c 65 6e 67 74 68 3a 20  Content-Length:\n 32 0d 0a                                         2..\n 41 63 63 65 73 73 2d 43 6f 6e 74 72 6f 6c 2d 41  Access-Control-A\n 6c 6c 6f 77 2d 4f 72 69 67 69 6e 3a 20 2a 0d 0a  llow-Origin: ..\n 53 65 74 2d 43 6f 6f 6b 69 65 3a 20 69 6f 3d 51  Set-Cookie: io=Q\n 72 5f 45 76 52 56 65 6c 65 66 37 59 4d 52 44 41  r_EvRVelef7YMRDA\n 41 41 45 0d 0a                                   AAE..\n 44 61 74 65 3a 20 46 72 69 2c 20 31 33 20 4e 6f  Date: Fri, 13 No\n 76 20 32 30 31 35 20 32 30 3a 35 38 3a 33 39 20  v 2015 20:58:39\n 47 4d 54 0d 0a                                   GMT..\n 43 6f 6e 6e 65 63 74 69 6f 6e 3a 20 63 6c 6f 73  Connection: clos\n 65 0d 0a                                         e..\n 0d 0a                                            ..\n 6f 6b                                            ok\n--\n...\n```\n\nThe curl version mirrors the JavaScript version.\nsocat sniffer output from curl clients (truncated, they're pretty much all like this):\n``` sh\n...\n% # ================================================================\n% cat socat-2.txt\n\n2015/11/13 13:22:50.870300  length=190 from=0 to=189\n 50 4f 53 54 20 2f 65 6e 67 69 6e 65 2e 69 6f 2f  POST /engine.io/\n 3f 74 72 61 6e 73 70 6f 72 74 3d 70 6f 6c 6c 69  ?transport=polli\n 6e 67 26 73 69 64 3d 75 6d 67 56 67 59 54 68 78  ng&sid=umgVgYThx\n 42 78 46 4e 74 42 4e 41 41 41 53 20 48 54 54 50  BxFNtBNAAAS HTTP\n 2f 31 2e 31 0d 0a                                /1.1..\n 48 6f 73 74 3a 20 68 74 74 70 0d 0a              Host: http..\n 55 73 65 72 2d 41 67 65 6e 74 3a 20 63 75 72 6c  User-Agent: curl\n 2f 37 2e 34 35 2e 30 0d 0a                       /7.45.0..\n 41 63 63 65 70 74 3a 20 2a 2f 2a 0d 0a           Accept: /..\n 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 20 61 70  Content-Type: ap\n 70 6c 69 63 61 74 69 6f 6e 2f 6f 63 74 65 74 2d  plication/octet-\n 73 74 72 65 61 6d 0d 0a                          stream..\n 43 6f 6e 74 65 6e 74 2d 4c 65 6e 67 74 68 3a 20  Content-Length:\n 39 0d 0a                                         9..\n 0d 0a                                            ..\n 00 06 ff 32 70 72 6f 62 65                       ...2probe\n--\n< 2015/11/13 13:22:50.875725  length=195 from=0 to=194\n 48 54 54 50 2f 31 2e 31 20 32 30 30 20 4f 4b 0d  HTTP/1.1 200 OK.\n 0a                                               .\n 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 20 74 65  Content-Type: te\n 78 74 2f 68 74 6d 6c 0d 0a                       xt/html..\n 43 6f 6e 74 65 6e 74 2d 4c 65 6e 67 74 68 3a 20  Content-Length:\n 32 0d 0a                                         2..\n 41 63 63 65 73 73 2d 43 6f 6e 74 72 6f 6c 2d 41  Access-Control-A\n 6c 6c 6f 77 2d 4f 72 69 67 69 6e 3a 20 2a 0d 0a  llow-Origin: *..\n 53 65 74 2d 43 6f 6f 6b 69 65 3a 20 69 6f 3d 75  Set-Cookie: io=u\n 6d 67 56 67 59 54 68 78 42 78 46 4e 74 42 4e 41  mgVgYThxBxFNtBNA\n 41 41 53 0d 0a                                   AAS..\n 44 61 74 65 3a 20 46 72 69 2c 20 31 33 20 4e 6f  Date: Fri, 13 No\n 76 20 32 30 31 35 20 32 31 3a 32 32 3a 35 30 20  v 2015 21:22:50\n 47 4d 54 0d 0a                                   GMT..\n 43 6f 6e 6e 65 63 74 69 6f 6e 3a 20 6b 65 65 70  Connection: keep\n 2d 61 6c 69 76 65 0d 0a                          -alive..\n 0d 0a                                            ..\n 6f 6b                                            ok\n--\n...\n```\n\n\n1 These are the local modifications I made to each test client to enable this:\n``` diff\ndiff --git a/deadlock/server.js b/deadlock/server.js\nindex c0d79f4..1822e03 100644\n--- a/deadlock/server.js\n+++ b/deadlock/server.js\n@@ -41,13 +41,11 @@ request(req_options, function (error, response, body) {\n     console.log('sid: ' + sid);\n     req_options.url += '&sid=' + sid;\n     req_options.headers['Content-Type'] = 'application/octet-stream';\n-    req_options.method = 'POST';\n-    req_options.body = packet_array;\n     console.log('start');\n for (var i = 0; i < num_packets; ++i) {\n     (function (i) {\n\n\nrequest(req_options, function (error, response, body) {\n\nrequest({ url: req_options.url.replace('http.sock', 'http-snoop-' + i + '.sock'), headers: req_options.headers, method: 'POST', body: packet_array }, function (error, response, body) {\n                 console.log('end ' + i + ': ' + body);\n             });\n         })(i);\ndiff --git a/tests/integration_deadlock.py b/tests/integration_deadlock.py\nindex 9f6ac83..58b7d64 100644\n--- a/tests/integration_deadlock.py\n+++ b/tests/integration_deadlock.py\n@@ -91,8 +91,8 @@ class TestDeadlock(t_unittest.TestCase):\n             b'Content-Type':   [ b'application/octet-stream' ],\n         })\n\n\ndef _sendpacket(_session, _payload):\n\npost_url_bytes = url_bytes + b'&' + parse.urlencode({ b'sid': _session })\ndef _sendpacket(_i, _session, _payload):\npost_url_bytes = url_bytes.replace(b'http.sock', b'http-snoop-{}.sock'.format(i)) + b'&' + parse.urlencode({ b'sid': session })\n             body_producer = t_client.FileBodyProducer(io.BytesIO(_payload))\n             d = agent.request(b'POST', post_url_bytes, post_headers, body_producer)\n             d.addErrback(lambda : [ _ ] if isinstance(, t_failure.Failure) else )\n@@ -105,11 +105,11 @@ class TestDeadlock(t_unittest.TestCase): # In my testing, this needs to loop at least 3 times to reliably\n # trigger the deadlock\n\n\nfor _ in range(5):\nfor i in range(5):\n         # One will not likely observe the deadlock after uncommenting\n         # the following (which will wait 10+ ms between POST attempts)\n         # yield t_task.deferLater(reactor, 10 / 1000, lambda: None)\npost_deferreds.append(_sendpacket(session, packet_raw))\n\npost_deferreds.append(_sendpacket(i, session, packet_raw))\ndef _printpostdeferreds(_summary):\n     print('================================================================', file=sys.stderr)\n```\n\n\n\n\nMy modified curl command was as follows:\nsh\n(\n    echo -n $'\\x00\\x06\\xff2probe' >|data.bin\n    for (( i=0; i < 1; ++i )) ; do\n        (\n            sid=\"$( python -c 'import sys, urllib ; print(urllib.urlencode({ \"sid\": sys.argv[1].strip() }))' \"$( curl --max-time 1 --verbose --unix-socket ./integrations/node/http.sock 'http:/engine.io/?transport=polling' 2>&1 | grep '^< Set-Cookie: io=' | sed '-es%^.*: io=%%' )\" )\"\n            f() { curl --data-binary \\@data.bin --header 'Content-Type: application/octet-stream' --max-time 10 --request POST --unix-socket \"./integrations/node/http-snoop-${1}.sock\" \"http:/engine.io/?transport=polling&${sid}\" || echo '>>>>>>>> TIMED OUT! <<<<<<<<' }\n            f 0 & f 1 & f 2 & f 3 & f 4 &\n        ) &\n        sleep 1\n    done\n    sleep 10\n    rm -f data.bin\n)\nThese are primarily notes to myself, since I haven't checked in these changes.\n. One thing to note is that with the Twisted version, the POST data from each connection comes in practically right on top of each other, irrespective of when the connection was made. I haven't observed this with the JavaScript or curl versions.\n. It looks like the hung connection is resulting in Socket.onError being called, but the default logs don't seem to indicate why. I printed out err and got this (apparently from here):\n[14:46:10.801] [LOG]     engine:socket Error: data request overlap from client\n    at XHR.Transport.onError (/.../txsocketio/integrations/node/node_modules/engine.io/lib/transport.js:74:15)\n    at XHR.Polling.onDataRequest (/.../txsocketio/integrations/node/node_modules/engine.io/lib/transports/polling.js:115:10)\n    at XHR.Polling.onRequest (/.../txsocketio/integrations/node/node_modules/engine.io/lib/transports/polling.js:55:10)\n    at XHR.onRequest (/.../txsocketio/integrations/node/node_modules/engine.io/lib/transports/polling-xhr.js:47:33)\n    at /.../txsocketio/integrations/node/node_modules/engine.io/lib/server.js:182:46\n    at Server.verify (/.../txsocketio/integrations/node/node_modules/engine.io/lib/server.js:130:3)\n    at Server.handleRequest (/.../txsocketio/integrations/node/node_modules/engine.io/lib/server.js:174:8)\n    at Server.<anonymous> (/.../txsocketio/integrations/node/node_modules/engine.io/lib/server.js:366:12)\n    at Server.emit (events.js:110:17)\n    at HTTPParser.parserOnIncoming [as onIncoming] (_http_server.js:491:12) +0ms\nWhat's bizarre about this is that either Node or Engine.IO is holding the connection open after this point, but no response is being sent back to the client, causing it to hang. I think that's the bug:\n1. The connection remains open; and/or\n2. No response is being sent to the client.\nSo possibly the reason I was unable to reproduce this using JavaScript or curl was because neither was fast enough to overlap requests, which is one of Twisted's strong suits.\nAs an aside, does this mean that Engine.IO's server-side polling transport can only handle one POST per session at a time?\n. >  I believe that no response is sent when data requests overlap is a bug, but will be fixed with the PR #338.\nAwesome! Thanks! :+1:\n. ",
    "nuclearace": ":+1: \n. :+1:  @rauchg \n. ",
    "Nibbler999": "The test failure is unrelated.\n. You are probably using the wrong version of utf-8-validate. ws 1.x uses utf-8-validate 1.2.x. https://github.com/websockets/ws/blob/v1.x/package.json#L37\nIf you install utf-8-validate 1.2.x in the ws node_modules directory you should be fine.. ",
    "CxRes": "@rauchg: Can you please fix this asap. It is holding a lot of us back!!!!!! Also please bump the version number.\n. ",
    "kriswill": "+1  This is blocking the world for a lot of people.\n. ",
    "Cap32": "+1 This is blocking the world for a lot of people.\n. :+1: \n. ",
    "lygstate": "That's a lot of annoy. ws are failed too much.\n. ",
    "iamstarkov": "So what about bringing socketio to the nodejs@4.0?\n. ",
    "SpainTrain": "Restating the obvious: Until this lands, engine.io and its dependents will be uninstallable under node v4.0.0 (latest stable node).\n@rauchg @nkzawa can you please merge and release this update?  That would be so  :heart: \nFWIW, in addition to breaking dependent packages, this is causing administrative overhead for the maintainers of your dependencies due to users filing false positive issues (e.g. https://github.com/websockets/bufferutil/issues/15, https://github.com/websockets/utf-8-validate/issues/13)\n. Don't see it on master - https://github.com/socketio/engine.io/blob/master/package.json#L29\n. \n. The package in the npm registry has the correct package.json, so npm i works.\nThanks @rauchg!\n. > @SpainTrain Which version are you used?\n```\n$ npm cache clean\n$ npm info engine.io version\n1.5.3\n$ npm info engine.io dependencies\n{ base64id: '0.1.0',\n  debug: '1.0.3',\n  'engine.io-parser': '1.2.1',\n  ws: '0.8.0' }\n```\n. ",
    "marcelaraujo": "https://github.com/socketio/engine.io/blob/1.5.3-patch/package.json\n???\n. https://github.com/socketio/engine.io-client/issues/420\n:(\n. @SpainTrain Which version are you used?\n. Tks sir!\n. ",
    "hinaloe": "+1\n. ",
    "liscovich": "+1\n. ",
    "Zwerge": "+1\n. ",
    "RamIdeas": ":+1:\n. ",
    "gsklee": "@nuclearace Could you please take a look at this issue? It's blocking me from installing either BrowserSync or Webpack Dev Server on my Mac. I'm using io.js 3.3.0.\nv1.2.1:\n```\n\nbufferutil@1.2.1 install /Users/kaylee/Projects/GitHub/gsklee/libstack-frontend/node_modules/bufferutil\nnode-gyp rebuild\n\nCXX(target) Release/obj.target/bufferutil/src/bufferutil.o\n  SOLINK_MODULE(target) Release/bufferutil.node\nbufferutil@1.2.1 node_modules/bufferutil\n\u251c\u2500\u2500 bindings@1.2.1\n\u2514\u2500\u2500 nan@2.0.8\n```\nv1.1.0:\n```\n\nbufferutil@1.1.0 install /Users/kaylee/Projects/GitHub/gsklee/libstack-frontend/node_modules/bufferutil\nnode-gyp rebuild\n\nCXX(target) Release/obj.target/bufferutil/src/bufferutil.o\nIn file included from ../src/bufferutil.cc:16:\n../node_modules/nan/nan.h:261:25: error: redefinition of '_NanEnsureLocal'\nNAN_INLINE v8::Local _NanEnsureLocal(v8::Local val) {\n                        ^\n../node_modules/nan/nan.h:256:25: note: previous definition is here\nNAN_INLINE v8::Local _NanEnsureLocal(v8::Handle val) {\n                        ^\n../node_modules/nan/nan.h:661:13: error: no member named 'smalloc' in namespace\n      'node'\n    , node::smalloc::FreeCallback callback\n      ~~~~~~^\n../node_modules/nan/nan.h:672:12: error: no matching function for call to 'New'\n    return node::Buffer::New(v8::Isolate::GetCurrent(), data, size);\n           ^~~~~~~~~~~~~~~~~\n/Users/kaylee/.node-gyp/3.3.0/include/node/node_buffer.h:31:40: note: candidate\n      function not viable: no known conversion from 'uint32_t'\n      (aka 'unsigned int') to 'enum encoding' for 3rd argument\nNODE_EXTERN v8::MaybeLocal New(v8::Isolate isolate,\n                                       ^\n/Users/kaylee/.node-gyp/3.3.0/include/node/node_buffer.h:43:40: note: candidate\n      function not viable: 2nd argument ('const char ') would lose const\n      qualifier\nNODE_EXTERN v8::MaybeLocal New(v8::Isolate isolate,\n                                       ^\n/Users/kaylee/.node-gyp/3.3.0/include/node/node_buffer.h:28:40: note: candidate\n      function not viable: requires 2 arguments, but 3 were provided\nNODE_EXTERN v8::MaybeLocal New(v8::Isolate isolate, size_t length);\n                                       ^\n/Users/kaylee/.node-gyp/3.3.0/include/node/node_buffer.h:36:40: note: candidate\n      function not viable: requires 5 arguments, but 3 were provided\nNODE_EXTERN v8::MaybeLocal New(v8::Isolate isolate,\n                                       ^\nIn file included from ../src/bufferutil.cc:16:\n../node_modules/nan/nan.h:676:12: error: no viable conversion from\n      'v8::MaybeLocal' to 'v8::Local'\n    return node::Buffer::New(v8::Isolate::GetCurrent(), size);\n           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n/Users/kaylee/.node-gyp/3.3.0/include/node/v8.h:210:7: note: candidate\n      constructor (the implicit copy constructor) not viable: no known\n      conversion from 'v8::MaybeLocal' to 'const\n      v8::Local &' for 1st argument\nclass Local {\n      ^\n/Users/kaylee/.node-gyp/3.3.0/include/node/v8.h:210:7: note: candidate\n      constructor (the implicit move constructor) not viable: no known\n      conversion from 'v8::MaybeLocal' to 'v8::Local &&'\n      for 1st argument\nclass Local {\n      ^\n/Users/kaylee/.node-gyp/3.3.0/include/node/v8.h:214:13: note: candidate template\n      ignored: could not match 'Local' against 'MaybeLocal'\n  V8_INLINE Local(Local that)\n            ^\n/Users/kaylee/.node-gyp/3.3.0/include/node/v8.h:327:13: note: candidate template\n      ignored: could not match 'S ' against 'v8::MaybeLocal'\n  V8_INLINE Local(S that)\n            ^\nIn file included from ../src/bufferutil.cc:16:\n../node_modules/nan/nan.h:683:26: error: no member named 'Use' in namespace\n      'node::Buffer'\n    return node::Buffer::Use(v8::Isolate::GetCurrent(), data, size);\n           ~~~~~~~~~~~~~~^\nIn file included from ../src/bufferutil.cc:7:\n/Users/kaylee/.node-gyp/3.3.0/include/node/v8.h:221:5: error: assigning to\n      'v8::Primitive volatile' from incompatible type 'v8::Value '\n    TYPE_CHECK(T, S);\n    ^~~~~~~~~~~~~~~~\n/Users/kaylee/.node-gyp/3.3.0/include/node/v8.h:180:37: note: expanded from\n      macro 'TYPE_CHECK'\n    (static_cast(0)) = static_cast(0);      \\\n                                    ^ ~~~~~~~~~~~~~~~~~~\n../node_modules/nan/nan.h:414:12: note: in instantiation of function template\n      specialization 'v8::Local::Local' requested here\n    return NanEscapeScope(NanNew(v8::Undefined(v8::Isolate::GetCurrent())));\n           ^\n../node_modules/nan/nan.h:398:30: note: expanded from macro 'NanEscapeScope'\ndefine NanEscapeScope(val) scope.Escape(_NanEnsureLocal(val))\n                         ^\n\nIn file included from ../src/bufferutil.cc:7:\n/Users/kaylee/.node-gyp/3.3.0/include/node/v8.h:221:5: error: assigning to\n      'v8::Boolean volatile' from incompatible type 'v8::Value '\n    TYPE_CHECK(T, S);\n    ^~~~~~~~~~~~~~~~\n/Users/kaylee/.node-gyp/3.3.0/include/node/v8.h:180:37: note: expanded from\n      macro 'TYPE_CHECK'\n    *(static_cast(0)) = static_cast(0);      \\\n                                    ^ ~~~~~~~~~~~~~~~~~~\n../node_modules/nan/nan.h:424:12: note: in instantiation of function template\n      specialization 'v8::Local::Local' requested here\n    return NanEscapeScope(NanNew(v8::True(v8::Isolate::GetCurrent())));\n           ^\n../node_modules/nan/nan.h:398:30: note: expanded from macro 'NanEscapeScope'\ndefine NanEscapeScope(val) scope.Escape(_NanEnsureLocal(val))\n                         ^\n\nIn file included from ../src/bufferutil.cc:7:\n/Users/kaylee/.node-gyp/3.3.0/include/node/v8.h:221:5: error: assigning to\n      'v8::Function volatile' from incompatible type 'v8::Value '\n    TYPE_CHECK(T, S);\n    ^~~~~~~~~~~~~~~~\n/Users/kaylee/.node-gyp/3.3.0/include/node/v8.h:180:37: note: expanded from\n      macro 'TYPE_CHECK'\n    *(static_cast(0)) = static_cast(0);      \\\n                                    ^ ~~~~~~~~~~~~~~~~~~\n../node_modules/nan/nan.h:1514:12: note: in instantiation of function template\n      specialization 'v8::Local::Local' requested here\n    return NanEscapeScope(NanNew(handle)->Get(kCallbackIndex)\n           ^\n../node_modules/nan/nan.h:398:30: note: expanded from macro 'NanEscapeScope'\ndefine NanEscapeScope(val) scope.Escape(_NanEnsureLocal(val))\n                         ^\n\nIn file included from ../src/bufferutil.cc:7:\n/Users/kaylee/.node-gyp/3.3.0/include/node/v8.h:221:5: error: assigning to\n      'v8::Object volatile' from incompatible type 'v8::Value '\n    TYPE_CHECK(T, S);\n    ^~~~~~~~~~~~~~~~\n/Users/kaylee/.node-gyp/3.3.0/include/node/v8.h:180:37: note: expanded from\n      macro 'TYPE_CHECK'\n    *(static_cast(0)) = static_cast(0);      \\\n                                    ^ ~~~~~~~~~~~~~~~~~~\n../node_modules/nan/nan.h:1632:12: note: in instantiation of function template\n      specialization 'v8::Local::Local' requested here\n    return NanEscapeScope(handle->Get(NanNew(key)).As());\n           ^\n../node_modules/nan/nan.h:398:30: note: expanded from macro 'NanEscapeScope'\ndefine NanEscapeScope(val) scope.Escape(_NanEnsureLocal(val))\n                         ^\n\n9 errors generated.\nmake: *** [Release/obj.target/bufferutil/src/bufferutil.o] Error 1\ngyp ERR! build error\ngyp ERR! stack Error: make failed with exit code: 2\ngyp ERR! stack     at ChildProcess.onExit (/Users/kaylee/.nvm/versions/io.js/v3.3.0/lib/node_modules/npm/node_modules/node-gyp/lib/build.js:269:23)\ngyp ERR! stack     at emitTwo (events.js:87:13)\ngyp ERR! stack     at ChildProcess.emit (events.js:172:7)\ngyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:200:12)\ngyp ERR! System Darwin 14.5.0\ngyp ERR! command \"/Users/kaylee/.nvm/versions/io.js/v3.3.0/bin/iojs\" \"/Users/kaylee/.nvm/versions/io.js/v3.3.0/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js\" \"rebuild\"\ngyp ERR! cwd /Users/kaylee/Projects/GitHub/gsklee/libstack-frontend/node_modules/bufferutil\ngyp ERR! node -v v3.3.0\ngyp ERR! node-gyp -v v2.0.2\ngyp ERR! not ok\nnpm ERR! Darwin 14.5.0\nnpm ERR! argv \"/Users/kaylee/.nvm/versions/io.js/v3.3.0/bin/iojs\" \"/Users/kaylee/.nvm/versions/io.js/v3.3.0/bin/npm\" \"install\" \"bufferutil@1.1.0\"\nnpm ERR! node v3.3.0\nnpm ERR! npm  v2.13.3\nnpm ERR! code ELIFECYCLE\nnpm ERR! bufferutil@1.1.0 install: node-gyp rebuild\nnpm ERR! Exit status 1\nnpm ERR!\nnpm ERR! Failed at the bufferutil@1.1.0 install script 'node-gyp rebuild'.\nnpm ERR! This is most likely a problem with the bufferutil package,\nnpm ERR! not with npm itself.\nnpm ERR! Tell the author that this fails on your system:\nnpm ERR!     node-gyp rebuild\nnpm ERR! You can get their info via:\nnpm ERR!     npm owner ls bufferutil\nnpm ERR! There is likely additional logging output above.\nnpm ERR! Please include the following file with any support request:\nnpm ERR!     /Users/kaylee/Projects/GitHub/gsklee/libstack-frontend/npm-debug.log\n```\ncc @shakyShane @sokra \n. ",
    "ide": "cc @rauchg -- this effectively adds Node 4.x support :D\n. we good now https://github.com/socketio/engine.io/commit/1a115d349310f418eabbe8cfc99702cdeeecdfa7\nthanks @rauchg \n. ",
    "masakij": "+1\n. ",
    "cismous": "+1\n. ",
    "austinpray": "\n. ",
    "Mumakil": "We tried running this, but ended up in a situation where the websocket disconnects 30s after an upgrade from xhr-polling. My initial take would be that the event listeners from the old transport are not cleared when the client upgrades and the closeTimeout fires and emits a close event.\n. ",
    "cjsaylor": ":+1: \n. ",
    "julianduque": "already landed\n. ",
    "sanemat": "@rauchg @nkzawa \n. ",
    "aaaristo": "We are experiencing this as well and honestly i don't think this is a good solution:\nhttps://github.com/socketio/engine.io/blob/1ecf02e32f05cf118f5506aaf704bb1059560836/lib/transports/polling.js#L76\nWhat about mobile devices with any kind of retry mechanism embedded for any http request?\nOr retries from a reverse proxy like AWS CloudFront: http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/RequestAndResponseBehaviorCustomOrigin.html#request-custom-request-timeout\nCouldn't we replace the current request with the last one? Thoughts?\n. ",
    "episodeyang": ":+1: ! \nwas just about to open one. I'm running into this problem as well.\n. ",
    "necccc": "Saw this with 4.2.2 too, when used with primus\nbuf.copy is not a function\n/buffer.js:238 \u2192 Function.Buffer.concat\n/server/node_modules/engine.io/lib/transports/polling.js:147 \u2192 IncomingMessage.onData\n/events.js:77 \u2192 emitOne\n/events.js:169 \u2192 IncomingMessage.emit\n/_stream_readable.js:360 \u2192 IncomingMessage.Readable.read\n/_stream_readable.js:743 \u2192 flow\n/_stream_readable.js:723 \u2192 resume_\n/node.js:441 \u2192 doNTCallback2\n/node.js:355 \u2192 process._tickCallback\n. ",
    "noamshemesh": "still happens, even after upgrading to primus 4.0.4 and engine.io 1.6.4. node version 4.2.4\nany ideas?\n. we stream data from the db and send it with primus function spark.write\nit works well for almost all the times but when there is a lot of data to stream it's crashing with this error:\nTypeError: buf.copy is not a function\n    at Function.Buffer.concat (buffer.js:238:9)\n    at IncomingMessage.onData (/opt/bigpanda/web-api/node_modules/engine.io/lib/transports/polling.js:160:23)\n    at emitOne (events.js:77:13)\n    at IncomingMessage.emit (events.js:169:7)\n...(wrappers)...\n    at IncomingMessage.Readable.read (_stream_readable.js:360:10)\n    at flow (_stream_readable.js:743:26)\n    at resume_ (_stream_readable.js:723:3)\nThanks! in the meanwhile I will try to reproduce it separately\n. I enabled DEBUG=engine:polling logs and just before it crashed the debug lines are:\n```\nengine:polling setting request +25s\nengine:polling writing \"97:0{\"sid\":\"\"\n,\"upgrades\":[\"websocket\"],\"pingInterval\":25000,\"pingTimeout\n\":60000}\" +0ms\nengine:polling setting request +5s \nengine:polling received \"21:4{\"type\":\"subscribe\"}\" +183ms\nengine:polling writing \"1:6\" +167ms\n```\ndoes it help?\n. ok after some more debugging it seems like it crashes when the server receives a big buffer from the client. not sure why the buffer doesn't have the copy function\n. ",
    "Xaekai": "Yeah, this really damaged our users experience for an event we only do once a year. :disappointed: \n. ",
    "Rajan": "@zhangxiaolian1991 Were you able to use socket.io swift client to connect with engine.io?\n. ",
    "ilearnio": "My server code looks like that:\n``` javascript\nimport koa from 'koa';\nimport http from 'http';\nimport Primus from 'primus';\nimport render from 'koa-ejs';\nimport serve from 'koa-static';\nimport Router from 'koa-router';\nlet app = koa();\nrender(app, {\n  root: 'public/client/apps',\n  layout: '',\n  viewExt: '',\n  cache: false,\n  debug: true\n});\n// public routes\nvar publicRouter = new Router();\npublicRouter.get('/', function*(next) {\n  yield this.render('/index.ejs', {\n    title: 'Hello!',\n    head: '',\n    body_content: '',\n    inline_scripts: ''\n  });\n});\napp.use(serve('public', {}));\napp.use(publicRouter.middleware());\nlet server = http.createServer(app.callback());\nlet primus = new Primus(server, {\n  transformer: 'engine.io',\n  parser: 'JSON'\n});\nprimus.on('connection', function(spark) {\n  if (spark) {\n    spark.send('news', { hello: 'world' });\n    spark.on('my other event', function (data) {\n      console.log(data);\n    });\n  }\n});\n// start server\nserver.listen.call(server, process.env.PORT);\n```\n. Sorry, my mistake :/ Closing\n. ",
    "Anson2048": "@kapouer Thank you for your guidance. I set the --max_old_space_size=128. but dosn\u2018t work. I run command is coffee --nodejs  --max_old_space_size=128. \n. ",
    "FoghostCn": "I am waiting too,custom socket id is useful for me,but I found it isn't in the latest version\n. ",
    "titoBouzout": "we just forgot to tell people room datatype changed :P \"minor\" change btw :P \n. ",
    "tripu": "I agree that this vulnerability isn't too ugly.\nI also think that if the cost of updating debug is nil and there are no good reasons not to update, it's best to do it. If nothing else, to keep validators quiet and happy.\n. ",
    "egoroof": ":+1: \n. ",
    "diwu1989": "really need a new release, thanks\n. ",
    "ef4": "Make that 1.0.1, there's a critical vulnerability in prior versions: https://nodesecurity.io/advisories/67\n. :clap: thanks\n. ",
    "addaleax": "Maybe update this to 1.0.1 as per #375?\n. @darrachequesne I think the question is, why add the eslint-disable-line instead of going with the advice in the deprecation message and using Buffer.alloc(0)?. ",
    "apollocatlin": "+1 for 1.0.1\n. +1\n. ",
    "dignifiedquire": "Done\n. ",
    "vicneanschi": "I tried master branch of socket.io.\nVersions (from log above)\n22953 error node v4.2.5\n22954 error npm v2.14.12\n. Ok. Thanks.\nI wanted to run tests - so I need dev dependencies.\n. ",
    "hifall": "I have seen this too, while using Socket.IO. Any update on this?\n. ",
    "jcgertig": "Im also getting this issue im not sure I can give an example. Im even able to see a emit trigger client side but then the result never comes back. ",
    "zhiyiwenbin": "It should be a BUG caused by some versions of browsers.\nI guess this exception would be caught by these browsers run-time.\nThe browser where I got this error log from, is Google Chrome 68.0.3440.106 64-bit.\nShall we check the state of WebSocket instance before using send()?\n. ",
    "sqren": "This sounds like a reasonable change :+1: \n. ",
    "2color": "Sorry this has already been merged\n. ",
    "quicklyfrozen": "Oops, should probably be against in engine.io-client \n. ",
    "alexhultman": "@rauchg How would this happen in practice? I'm not very far from passing all tests and the only \"porting\" work required is to change require('ws').Server to require('uws').Server. Opt-in option for a test period?\n. All Engine.IO tests now pass with \u00b5WS as the ws implementation.\n. Sarcasm?\n. @rauchg Well in that case I must say I'm sorry for the harsh words. I'm not known for having the best patience. The option name is a little undecided IMO - it all depends on if the option is guaranteed to only apply to the server or not.\nSocketCluster is going to name its option wsEngine I think.\n. The goal in this would be to allow users of Socket.IO (and other projects) to easily swap to uws (or any other ws-implementation) without changing any of their code. Something like this:\njavascript\nvar io = require('socket.io')(80, { wsEngine: 'uws' }); // wsModule, wsEngine, whatever cute name\nThis option should only apply to the server (\u00b5WS is only implementing the Server part of ws).\n. Okay then, it seems these options are Server options and that would of course explain it. Then I just wonder about eio_ws_module -> shouldn't that one be in caps? Other environment variables in the sources are in caps.\n. It seems they are only Server options. They have different options for Socket which would be the client if I understand things.\n. But you should still keep the name of EIO_SERVER_WS_MODULE since that one is global to the process. But wsModule can stay.\n. This project seems dead, they aren't accepting any PRs for months.\n. Well it's dead to me if they are just going to ignore PR's and not respond to any questions. They can call themselves \"fastest\" (note: superlative) just like everyone is doing these days even though this is very far from the truth. I don't care anymore.\n. If you expect the test to succeed we need to depend on uws so that it downloads and installs it.\n. Is it possible to run the entire test two times, one with ws and one with uws?\n. I will need to push a new version to pass the last 3 tests, will do shortly.\n. In Primus we added a check to the test so that Node < 4 was skipped in tests. You could check to see if EIO_WS_ENGINE is set and skip tests for specified Node.js versions.\n. @kapouer Should I link statically this release or should we fix the travis g++? You should be able to modify the .travis.yml yourself?\n. Okay I published v0.3.4 that passes every test. I did link statically in this build still.\n. It seems you are not downloading the latest version? It's failing where it shouldn't fail if you have the latest ver.\n. You need 4.4.3 or 4.4.4 Node.js - it was fixed just recently.\n. @rauchg I think we are pretty happy with this. Sorry for the extra checks but those are needed because of bugs in Node.js prior to 4.4.3 and we cannot (yet) specify Node.js 4.4.3 in Travis. Thoughts?\n. @rauchg @kapouer Thanks for the collaboration :)\n. Engine.io 2.x has uws by default, you could look that up yourself. Nobody can know if it will work or not, you have to test. JavaScript technically has support for enforced encapsulation but no library is really making use of it, instead most libraries \"mark variables as private\" by putting an underscore infront but since this is not enforced you still can write code that depends on this \"private\" variable and thus breaks when you swap to uws as it has far different internals. If people would just learn to properly encapsulate their code it would be a different story.. The most important part is wsServerModule. If you call it wsModule it implies changing the client implementation as well. If we then think long term and person X comes along with an optimized client implementation they should be able to specify wsClientModule. Or lets say person Y comes with a complete implementation with both server and client support and they specify wsModule then it doesn't use the client implementation. If Engine.IO then changes wsModule to mean both, it breaks for those who specified a module with only server support.\n. @kapouer Maybe I'm wrong, it seems these are just Server options?\n. @rauchg Wtf man, why hasn't this been merged? These versions are extremely out of date. If we are going to do this you will need to merge simple version bumps in time. Otherwise there is no idea in doing this at all.\n. I've run tests in parallel in a loop and triggered the same ping time out error on both uws 0.4.0, uws 0.7.7 and ws. So it happens with both uws and ws.\nAs you can see here, the first test round (that is the ws test round) fails:\n```\n\nengine.io@1.6.11 test /home/alexhultman/engine.io\ngulp test; EIO_WS_ENGINE=uws gulp test;\n\n[05:59:24] Using gulpfile ~/engine.io/gulpfile.js\n[05:59:24] Starting 'test'...\n\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\u2024\n46 passing (2s)\n  1 failing\n1) server close should trigger on both ends upon ping timeout:\n     Uncaught Error: expected 'transport error' to equal 'ping timeout'\n      at Assertion.assert (node_modules/expect.js/expect.js:99:13)\n      at Assertion.be.Assertion.equal (node_modules/expect.js/expect.js:200:10)\n      at Assertion.(anonymous function) [as be] (node_modules/expect.js/expect.js:73:24)\n      at Socket.onClose (test/server.js:424:29)\n      at Socket.Emitter.emit (node_modules/component-emitter/index.js:134:20)\n      at Socket.onClose (node_modules/engine.io-client/lib/socket.js:705:10)\n      at Socket.onError (node_modules/engine.io-client/lib/socket.js:671:8)\n      at XHR. (node_modules/engine.io-client/lib/socket.js:261:10)\n      at XHR.Emitter.emit (node_modules/component-emitter/index.js:134:20)\n      at XHR.Transport.onError (node_modules/engine.io-client/lib/transport.js:65:8)\n.javascript\nvar eioc = require('engine.io-client');\nvar listen = require('./common').listen;\nvar expect = require('expect.js');\ndescribe('server', function () {\n  it('should allow client reconnect after restarting (ws)', function (done) {\n    var opts = { transports: ['websocket'] };\n    var engine = listen(opts, function (port) {\n      // these two lines make the difference between pass or fail\n      engine.httpServer.close();\n      engine.httpServer.listen(port);\n  var socket = new eioc.Socket('ws://localhost:%d'.s(port), { transports: ['websocket'] });\n\n  engine.once('connection', function (conn) {\n    setTimeout(function () {\n      conn.close();\n    }, 10);\n  });\n\n  socket.once('close', function (reason) {\n    expect(reason).to.be('transport close');\n    done();\n  });\n});\n\n});\n});\n```\nThis is the only failing test and it passes if you comment out the .close and .listen. This test is very strange and the httpServer being closed and listened on is not part of uws but engine.io itself. I cannot see why it would fail.. I've found the issue. Engine.io calls close on the websocket server whenever anything calls close on the httpServer. Engine.io then expects to reuse this closed server, which is not the case in uws. You need to fix this in Engine.io - you are not allowed to close a server and use it later. I would just remove those two lines since they make no sense to have in the first place.. Engineio controls the webspcket module with handleUpgrade and close. The websocket module doesnt care aboit http listen/close in this case - it only cares about the fact that you call websocket module close followed by handleUpgrade. You cannot use a websocket module after closing it. You have to remove or replace this failing test because it fails due to invalid usage of uws.. The ping failure is an old issue not related to uws. It happens with both ws and uws at random times due to too small timeout. Increase the timer for that case.. #461 . Why is it an optional dependency? I heard this means it won't get downloaded by default and thus not really be used in most cases. Wouldn't it be better to make it a real dependency? Installation of uws cannot fail, it always succeeds silently. If someones server suddenly would blow up they could report an issue and use wsEngine: ws until it gets fixed.. Oh, my bad then. Even better, I could make it default to ws :+1:. Sadly it still randomly fails. Seems to have failed for ws now recently. uws is not deployed to or in any way supposed to work with Yarn. uws supports Node.js and NPM, nothing else. If you still insist on using it in environments where it is not intended to work then that's your issue.. Any idea why it would fail? Seems both ws ans uws fails. Rerun tests?. 1. uws is not an Electron module. It is a Node.js module. This is clearly stated in the README and FAQ. You're far from the first one to report such a \"bug\".\n\n\nuws is not a PlayStation 4 game. It is also not an app for Windows Phone. Neither is it a swimming pool. It is a Node.js module.\n\n\nuws (for Node.js) has precompiled binaries and will gracefully fall back to these if compilation fails. Also, there is no need whatsoever to tinker with headers like @vincentbriglia - that's not even the proper way! Nobody tells you to mix and match like that! \"npm install uws\" is official, nothing else!\n\n\nElectron is not even a server project!. Wow, that's a lot of downvotes. Yeah, I guess it all falls on me to somehow fix up completely separate environments that I didn't even target.\n\n\n\n../src/Networking.h:7:10: fatal error: 'openssl/opensslv.h' file not found\n\ninclude \nElectron is not providing OpenSSL headers, go shame Electron for this - I'm no magician and I cannot just make third-parties obey by Node.js rules. They are not even the same platform!\nThis would be the same as require('fs') failing due to missing the fs module in Node.js - it makes no sense from a Node.js module's perspective.\nConsider this thought experiment:\nWhat if I took, say, Express.js and shoved that JavaScript code into a third-party JavaScript environment that lacked 90% of all prorgamming interfaces that are in Node.js. Would you blame Express.js for this? Because it obviously wouldn't work.\nWhat if I took Socket.IO and tried to run it in a Python environment? Ruby environment?\nYou starting to get my point? I can't fix things I have no control over and I never said it would work in Electron, they are not the same platform just because a portion of some JavaScript happens to work in both.\nC++ is not scripting, you can't just take a Box and shove it through a Round hole and expect it to just magically work.\n\nIn any case, just use ws instead because with Socket.IO you don't see any difference since the bottleneck is not in ws nor uws, it is in Socket.IO itself.. Isn't this the same problem again? The option itself can be 'wsEngine' but the (global to the process) should probably be named something implying it's only a server option, right? EIO_SERVER_WS_ENGINE but option wsEngine?\n. This needs to be at least ^0.3.4\n. I also think this needs to update to the latest 4.x to pass (nodejs bug)\n. I guess fix version is okay but in that case you will get many PRs from me, wanting to up the version when I fix things.\n. Maybe we could write \"Possible values include 'ws' and 'uws', while the default is 'ws'.\" now that we added specific testing for uws in particular?\n. https://github.com/nodejs/node/issues/6338\n. I think \"C++\" sounds bad to most Node.js developers though, it's \"non-pure\" even if it's 20x faster.\n. ",
    "yinlubin1989": "\u5389\u5bb3\u4e86 \u6211\u7684\u54e5 \u5389\u5bb3\u6211\u7684 uws. ",
    "jkarneges": "Apparently it even used to work this way:\nhttp://www.devthought.com/2012/07/07/the-realtime-engine/\nWould a patch to support the WebSocket interface be accepted?\n. We went ahead and made a wrapper to emulate the WebSocket class. Here it is if anyone's interested: https://github.com/fanout/engine.io-as-websocket\n. ",
    "palavrov": "@rauchg, @kapouer Guys, this PR breaks the ability to pack socket.io / engine.io with browserify because it is not able to detect the module name anymore. I.e. every time require with variable breaks it. I'm going to fix it in #397 to avoid creating a new PR.\n. @john0312, this PR fixes another regression with 6d92897 \n. Fixed!\nIt was added by npm.\n. ",
    "lakamsani": "Hi guys, what is the status on this and is there a user level doc? I have an app in production that is working well from a functional standpoint with socket.io(1.7.3) , socket.io-redis (4.0.0), ioredis(3.0.0-0) and node.js 7.7.1. I 'd love to get better performance form uws but need to make sure my app doesn't break and can't afford to rip out socket.io  at this point. I have the same (unanswered) question as 'sgrytoyr' on this thread https://news.ycombinator.com/item?id=12905939 @alexhultman related to sending message received on server A to socket on server B . If I just set the env EIO_WS_ENGINE=uws will it work? I don't see uws when I do a find under my app's node_modules. Is there a specific socket.io node.js server version where it has the uws dependency? I 'd happily be a beta test user for this if it helps. . OK. I messaged @goldfire on his tweet today about successful move of his app to uws (https://twitter.com/GoldFireStudios/status/846340097180098561). He said he did a custom implementation of messaging between servers but suggested one option could be: https://github.com/mmalecki/primus-redis-rooms. Also socket.io 1.7.3 uses engine.io 1.8.3.  What version of socket.io will use engine.io 2.x?. ",
    "LordMajestros": "What happens if you don't add the origin option at all?\n. ",
    "koszny": "if origins's empty all cors requests are allowed,\nit's to be delivered together with \nhttps://github.com/socketio/socket.io/pull/2556\n. ",
    "ksylvan": "Yes I am using certificates.\nI'm using sails.js on the server and mocha with socket.io-client for the unit tests. \n. Thank you for your response! I'm using the sails.io.js wrapper to connect to socket.io-client and engine.io. I don't see what the easy way is to set that option.\nhttps://github.com/balderdashy/sails.io.js#for-nodejs\n. https://github.com/balderdashy/sails.io.js/pull/103 fixes the issue for me, along with setting the connection option before sails initiates the socket connection.\n. ",
    "medemi68": "Hopefully this is accepted soon.\n. ",
    "john0312": "This seemed to be fixed by by commit 298cb6fd6f1e6ab63c2a3fb5f5d762643c138135. However, that commit is not included in v1.6.11, but is instead present in the current master branch, because 1.6.10-pre is merged. Could @rauchg release another version base on the current master branch (966344219d3c20439a75ee0323c86a148922088c) so that jaredallard/nexe#219 can be fixed? Thank you.\n. ",
    "Reeley": "Yes i do! Thanks for the information :)\n. ",
    "JoneXie1986": "me too\nsocket.io 1.4.6\nRangeError: Maximum call stack size exceeded\n    at Socket.flush (/data/system/ws-cluster/node_modules/socket.io/node_modules/engine.io/lib/socket.js:413:32)\n    at Socket.sendPacket (/data/system/ws-cluster/node_modules/socket.io/node_modules/engine.io/lib/socket.js:392:10)\n    at Socket.send.Socket.write (/data/system/ws-cluster/node_modules/socket.io/node_modules/engine.io/lib/socket.js:353:8)\n    at writeToEngine (/data/system/ws-cluster/node_modules/socket.io/lib/client.js:148:17)\n    at /data/system/ws-cluster/node_modules/socket.io/lib/client.js:156:9\n    at Encoder.encode (/data/system/ws-cluster/node_modules/socket.io/node_modules/socket.io-parser/index.js:135:5)\n    at Client.packet (/data/system/ws-cluster/node_modules/socket.io/lib/client.js:155:20)\n    at Socket.packet (/data/system/ws-cluster/node_modules/socket.io/lib/socket.js:210:15)\n    at Socket.emit (/data/system/ws-cluster/node_modules/socket.io/lib/socket.js:155:12)\n. ",
    "gzskyking": "me too\nsocket.io 1.4.8\n/node_modules/engine.io/lib/socket.js:338\n      this.sentCallbackFn.push.apply(this.sentCallbackFn, this.packetsFn);\n                               ^\nRangeError: Maximum call stack size exceeded\n    at Socket.flush (/home/gzliuhaiping/workspace/stf/stf-web/openstf/node_modules/engine.io/lib/socket.js:338:32)\n    at emitNone (events.js:72:20)\n    at WebSocket.emit (events.js:166:7)\n    at /home/gzliuhaiping/workspace/stf/stf-web/openstf/node_modules/engine.io/lib/transports/websocket.js:94:14\n    at afterWrite (_stream_writable.js:346:3)\n    at nextTickCallbackWithManyArgs (node.js:477:18)\n    at process._tickCallback (node.js:375:17)\n. ",
    "furiousOyster": "It seems that if you do not allow the main thread to take a break and process the queued up IO, then you will receive this error as the queues will overflow.  If you break out of your current op, by calling setTimeout() or some other means to yield execution momentarily, the buffers will be cleared.\nThis seems to be self limiting, in that if allowing the IO to flush correctly, there is no amount of load that you can create which will cause stack size exceptions like these.\n. ",
    "tmpvar": "\ud83d\udc4d  old versions of negotiator (specified by accepts@1.1.4) are vulnerable - https://nodesecurity.io/advisories/106\n. This can be replaced by a PR that bumps base64id to 1.0.0 (https://github.com/faeldt/base64id/issues/6). ",
    "timaschew": "298cb6fd6f1e6ab63c2a3fb5f5d762643c138135\n. ",
    "ChisholmKyle": "Why is accepts@1.1.4 still a dependency in engine.io npm registry tarball https://registry.npmjs.org/engine.io/-/engine.io-1.6.11.tgz? Or am I confused about npm dependency handling? See also https://github.com/socketio/socket.io/issues/2591\n. ",
    "alokrajiv": "Snyk.io is also catching the issue and is passing the warning to many libraries that depend on engine.io:\nhttps://snyk.io/test/npm/engine.io/1.6.11\n. ",
    "hairyhenderson": "This is really weird... the engine.io@1.6.11 that's installed from npm right now has accepts@1.1.4 as a dependency:\nconsole\n$ grep accepts package.json \n    \"accepts\": \"1.1.4\",\nLooks like 298cb6f never made it into the 1.6.11 tag:\nhttps://github.com/socketio/engine.io/compare/1.6.11...master\n. @rauchg - would it be possible to get a 1.6.12 released with the 298cb6f fix in it?\n. /cc @darrachequesne @rauchg - any possibility of getting this resolved finally?\n. FYI, this fixes #410 \n. @3rd-Eden - The \"vulnerability\" is caused by the fact that 1.1.0 had no default maxPayload. It's a much safer posture to just set a sane default (which you did in 1.1.1).\nIMO it's still important (and low-risk!) to update the dependency. socket.io/engine.io/etc are now considered as \"insecure\" by NSP, which, right or wrong, will cause a lot of nervousness among NSP users.\n. @rauchg @darrachequesne - can one of you release a 1.6.12 from the master branch?\n. So, that travis failure is kind of a good indication that this is working :wink:\n[18:47:37] Using gulpfile ~/build/socketio/engine.io/gulpfile.js\n[18:47:37] Starting 'nsp'...\n[18:47:37] 'nsp' errored after 225 ms\n[18:47:37] Error in plugin 'gulp-nsp'\nMessage:\n    (+) 1 vulnerabilities found\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               \u2502 DoS due to excessively large websocket message        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Name          \u2502 ws                                                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Installed     \u2502 1.1.0                                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Vulnerable    \u2502 <=1.1.0                                               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Patched       \u2502 >=1.1.1                                               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Path          \u2502 engine.io@1.6.11 > ws@1.1.0                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 More Info     \u2502 https://nodesecurity.io/advisories/120                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nGuess I'll update the ws dep here too so this passes...\n. @darrachequesne ok, that's fair, although if you're worried about API breakage, I'd recommend using a tool like Greenkeeper to help make sure dependencies are kept up-to-date and tests are still passing.\nIMO, if the tests pass, the new dependency version should be fine. If it's not, that means there's missing tests \ud83d\ude09 \n. ",
    "mavrick": "Any chance we can get this fixed?\n. Can we get this patched please!\n. ",
    "YasharF": "I believe the engine.io npm package needs a minor version bump to resolve the issue in the issue after the dependency upgrade.  The minor version bump would also resolve the issue with https://github.com/socketio/engine.io/issues/410\n. The dependency chain that I am running into per Snyk.io is:\nsocket.io@1.4.8 \u203a socket.io-client@1.4.8 \u203a engine.io-client@1.6.11 \u203a ws@1.0.1\nIf I gathered it correctly, here are a bunch of version bumps that need to happen to close this issue:\n1. https://github.com/socketio/engine.io-client/blob/master/package.json , engine.io-client has the new version of ws, but needs a minor version bump for the npm package to be updated (aka to 1.6.12)\n2. Once the above change (1) is done, https://github.com/socketio/socket.io-client/blob/master/package.json will need to pick up 1.6.12 of engine.io-client, and also get at least a minor version bump (aka to 1.4.9)\n3. Once the above change (2) is done, https://github.com/socketio/socket.io/blob/master/package.json needs to pick up the new version of socket.io-client (aka 1.4.9) and have a minor version bump (aka to 1.4.9)\n. Yes! https://github.com/socketio/engine.io/commit/330526a491b8fee58e7223fe874fcaa8cac1409c   : engine.io version bump commit to 1.7.0\n. ",
    "yasserf": "Thanks for the quick reply! I could see that I could override websocketServer with the uwS one. The only thing that's slightly annoying is that it would create a ws sever first and then I'll override it, but I'll certainly be able to work around that.\nI think having an optional dependency makes alot more sense, for those where node-gyp isn't installed having uws not optional would result in a headache for some window users and node-gyp.\n. ",
    "edi9999": "Can you please publish a new version to npm once this is merged ?\n. Here it is : https://github.com/socketio/engine.io/pull/451. ",
    "steveharrison": "Apologies, this was an issue with our internal registry.\n. ",
    "tjni": "What I think happened here is the following:\n1. The code in base64-arraybuffer, since its first commit, uses Uint8Array, so its encode and decode methods did not support IE <= 9. Commit niklasvh/base64-arraybuffer@1616895 lifted the usage of Uint8Array to when the module is required instead of when its methods are called.\n2. socketio/socket.io-client@e97f904 bumped the version of base64-arraybuffer to 0.1.5.\n3. Version 1.4.8 of socket.io-client regenerated the socket.io.js file that is used in the browser to include the breaking change.\nAs an immediate fix, I suggest pinning the version of base64-arraybuffer to < 0.1.5 and building a new socket.io.js file. Long term, perhaps base64-arraybuffer can be updated to use an array (or a polyfill) when typed arrays are not available.\n. Actually, it doesn't make sense to me for base64-arraybuffer to account for browsers that do not support ArrayBuffer. Including it in socket.io.js should be conditional.\n. ",
    "boneskull": "This has broken our IE8 tests.\n. ",
    "connor4312": "Preferably ws would be locked to a major version ^1.1.1 so that consumers can readily pull in updates for high-severity issues like this without waiting on 3+ hour latency for someone from socket.io to merge it. The maintainer of ws has been good, both in ws and other modules he maintains, in following semver, so there should not be issues with a major version constraint.\n. ",
    "cmrigney": "@3rd-Eden I see.  Unfortunately, NSP security check will still fail since Engine.IO is using that version of the package.  I'll just add an exception for now.\n. ",
    "oriweingart": "+1\n. ",
    "sgress454": "Looks like this was patched on the master branch, and the package version was bumped to 1.6.11, but the patch didn't actually make it into the published version of 1.6.11 on NPM for some reason.  When the current code is published on NPM, this issue can be closed.\n. Hi @paulrobello, please see my comment about the related PR in engine.io-client (asking that the version be changed to 1.1.2 to allow continued support for Node < 4).. ",
    "clarkio": "For some reason when this package gets installed as a child dependency it pulls in version 1.6.11, but still shows ws as 1.1.0 in the package.json. My app has socket.io version 1.4.8 as its direct dependency here.\nI'm using Node.js v4.4.7 and npm 2.15.8. Still digging into why this is happening, but hoping maybe someone else has come across this as well and found a fix.\nBelow is a snap of what was pulled in, yet 1.1.1 is found here: https://github.com/socketio/engine.io/blob/master/package.json#L32\n\n. Looking at the release commit it seems ws wasn't actually updated to 1.1.1 which is why it's still failing in NSP: https://github.com/socketio/engine.io/commit/10c4360bde25271279d5b1250327a7db68a1532d#diff-b9cfc7f2cdf78a7f4b91a753d10865a2R30\n. ",
    "jhenriquez": "It seems it still hasn't made it into npm.\n\nMaybe the question is silly but is there any way I could force ws to be upgraded?\n. ",
    "petermikitsh": "Maybe a 1.6.12 should be released that includes the fix to NSP 120.\n. We should also clarify that there is real value added in getting this fix available to package users. As mentioned in NSP-120, bad actors will be able to repeatedly crash your node process through continually sending large requests. Being able to rather easily threaten the availability of an application is a serious matter, and it's one that shouldn't be taken lightly. It has been some time since the initial report, but a patch is better late than never.\n. ",
    "jkomyno": "+1\n. ",
    "andresmatasuarez": "+1\n. ",
    "rsp": "@rauchg @darrachequesne Since this issue has already been addressed several months ago, is there any chance that someone will run: npm version patch && npm publish any time soon to close this issue? Or maybe someone has published a fork on npm that we can use in the meantime?\nThe ws version in dependencies was changed to 1.1.1 by @hairyhenderson in commit 959c8be0 from July 13 and it's fine on master:\nhttps://github.com/socketio/engine.io/blob/master/package.json#L32\nBut the lastest version of engine.io on npm and the latest tag on GitHub is 1.6.11 from June 24 which still has ws 1.1.0 in package.json:\nhttps://github.com/socketio/engine.io/blob/1.6.11/package.json#L30\nThis causes every module that uses engine.io to be marked with warnings about high severity vulnerabilities by services like Snyk: https://snyk.io/test/npm/engine.io\nSince it can be fixed (and has been fixed) with a one-byte change, any ETA on publishing that change?\n. ",
    "MoLow": "Well, I think maybe instead of providing as an option the host and the port of redis,\na function which is a getter/setter/deleter for existing sessions (which I implemented as Server.prototype.client) could be a better solution.\nwill that be better?\n. Wow, thanks for the feedback. \nso @zbigg you are basically saying it is impossible to manage multi-process/thread websocekts in nodejs, without putting the processes behind some kind of proxy?\n\nAgree in general, this should be pluggable mechanism which defaults to in-mem storage\n\ndose this any benefit besides enabling stickey sessions? as @zbigg any other solution than in-memory session store could potentially mess up buffer ordering, or cause missing a message\n. ",
    "zbigg": "Hi,\nThanks for this PR, i also needed cross-process socket-sharing in socket.io/engine.io like this, but assumed it's too bit big task.\nThis PR does one thing - session sharing in redis - cool. But IMHO, it's not enough.\nI this this PR is missing one important feature - buffer sharing, without it it's just half of proper \"multi-process support\".\nWhat i mean, is that socket.send may buffer packets. And this buffer is in one particular process that at this time decided to send something. In reality, next XHR-poll request (or WS reconnect) may arrive at other process and thus message is missed.\nIn wost weird case, this packet may be drained (delivered to client) later out-of-order because client happened to connect again to process with stale buffer.\nNevertheless, i think that this patch may work for particular scenarios where messages may be missed (and reordered) like in simple signaling that triggers client to update its resources (this may be also my case!).\nIf that patch would be followed up to custom session/buffer store socket.io/engine.io may work behind ELB/HAProxy without sticky sessions which would be a huge win (AFAIR, no WS library supports it out-of-box without hacks).\n\nfunction which is a getter/setter/deleter for existing sessions\n\nAgree in general, this should be pluggable mechanism which defaults to in-mem storage.\n. ",
    "oliverhausler": "@darrachequesne Yes, you are right. But if the maven dependency is up-to-date now, then this is obsolete I assume. So I'm closing it.\n. ",
    "Dynalon": "I want to re-open this as I think this is still not fixed. I was just examining the engineio code, and it seems writeBuffer in socket.js is just an unbound array serving as a queue, holding an arbitrary amount of packets without any limit.\nPackets are added to the writeBuffer via the sendPacket function, not checking for a maximum amount of packages already in queue. The writeBuffer is flushed explicity, or after the drain event happens in a transport (which ultimately is an event triggered by the kernel that will tell you that the tcp write buffer has become available for writing again).\nThe scenario to produce a DoS attack should be very easy to setup:\n\nSetup a client (receiver) and throttle the websocket tcp connections to very low amount (just enough so the tcp connection wont breakdown, i.e. 1kb/sec) using os-level tools for network throttling (on linux i.e. using iptables in conjunction with tc). Due to the nature of TCP flow control, the engineio server will not send more data to the client as it can receive which is ~1kb/sec.\nSetup a malicous client (sender) that sends data to this throttled client via an engineio server as intermediary, sending as much data as possible.\n\nThe result should be that the writeBuffer is written at roughly the same speed as the sender network speed (depends on enabled packet compression). In ordinary setups with V8 memory limit around 1.8GB per process, you should be able to fill up all the server process memory in less than a minute using a 1 Gbit/s link, and only a few minutes using a 100 Mbit/s link.\nTo fix this, a maximum packet limit has to be set for writeBuffer and the API should be able to emit errors when the limit is reached (in which case the .send() has to error on the API consumer side).\n. @darrachequesne @yang I did some thinking and comparison and have come to the conclusion that the only way to be safe is to buffer a limited amount of packets, and then drop the client/receiver (=disconnect the socket) if he can not keep up receiving. This is how redis pubsub is implemented. \nIf one would throttle the sending client, a single receiver can throttle broadcast frequency for potentially thousand of other users. If we drop (=disconnect) the sending client, the same would hold true. A receiving client can always be throttled by using TCP throttling on the transport layer, and it can always be slower than what ever throttled rate the sender uses.\nThe way it is implemented right now not only introduces a DoS scenario, it might be the reason for some bad socket.io performance benchmarks. If the engineio does buffer packages forever with no limit, memory will start to fill up as soon as a single client cannot keep up with send frequency. Even if 1000 clients receive a broadcast packet in a timely manner, the single client who cannot keep up will eventually take all the memory of the node process (which IIRC is limited at 1.8GB in V8 by default).\nA suggestion to fix this: Instead of an unbound buffer, there must be a more clever buffer queue that has a (configurable limit), or even better a soft- and a hard limit. Clients might be able to go over the soft limit (say 16 Megabytes) for a fixed amount of time to handle bursts, but once the hard limit is reached (i.e. 32 Megabytes) or the soft limit is overused for a fixed amount of time, the client must eventually be disconnected and the queue be trashed.. I dug a little into the code and wrote a middleware that can at least limit the number of packages per socket in a given timeframe. I'm still trying to figure out a way to read the total byte size written to a socket, which seems not to be available in the socket.connection object that the middleware can access. I'll dig deeper, maybe this requires patching socketio/uws to make the byte counts available (they are available on node's socket object for sure, but I am not sure how uws exposes this). If you know a solution on the top of your head this would help me greatly.\nMain issue is, without the byte sizes there is not way to properly handle a maximum buffer size. Implementing the rate limit as a middleware whould have the advantage that this could live out of tree, and different middlewares for different queueing strategies could be used.. Well it does not matter where to put the limit, but somewhere down the pipeline there has to be a limiting buffer which - when full - will result the client to be disconnected. Right now I couldn't find any kind of limit neither in engineio or uws c++ code (though I just skimmed the sources), so data to a client seems to be buffered indefinetely if the client cannot keep up receiving (think TCP congestion).\nIn non-async world and i.e. good-old C, the send() command would block if the kernel TCP buffers of a socket are full, and since the process gets blocked no receiving data is read from the the sender's socket; this automatically prevents memory exhaustion. This is of course not an option in 1:n messaging systems such as socketio, where a single packet (higher order socketio packet, not tcp) is saved in memory, and then relayed to multiple receiving clients. If a single client cannot keep up (=receives at a slower rate than the sender sends data) this will ultimately fill up all the memory, unless there is some kind of buffer limit which drops the slow client when reached (i.e. like redis which drops clients after a buffer limit of 32MB).\nI still need more time to investigate to find out how and where data is buffered using the different transports in engineio/socketio. I could manage to get the number of bytes written/received by a socket in the middleware using the packetCreate and packet events (emitted in engines Socket class) looking at the data.length property. This at leasts helps to set ratelimits if a client is too slow or a sender sends to aggresively.. ",
    "yang": "@darrachequesne Limiting message size and rate only delay the issue\u2014a client who isn't reading packets can still cause your server to consume memory unbounded.  Also, cutting these down isn't applicable to all applications.. ",
    "westy92": "+1 for the cookieHttpOnly option.\n. ",
    "dlimkin": "Done!\n- cookiePath option default set to '/';\n- cookieHttpOnly option default set to true;\n- add documentation for the cookieHttpOnly option;\n- added compatibility with node v7;\n. ",
    "nkramaric": "I am also having this issue. Even if the request is coming from the same origin the header should be set. Any reason for explicitly setting it to '*'?\n. ",
    "chewax": "Thanks @darrachequesne.\nI tried the former in all the possible shapes you could imagine but could not retrieve the user_id. \nNevertheless, I coded the latter as a workaround in the meantime, and kind of liked it better.\nThen i just need use\njavascript\nio.to(userHash[\"myId\"]).emit()\nI think it might be faster too so i am sticking to it.\nThanks for the answer!\n. As far as I am concerned its resolved, so im closing it.\n. ",
    "perrin4869": "+1 Got the same problem in my tests where I check for socket reconnection\n. I could try to add the test, but I am not very well acquainted with engine.io, I'm more familiar with socket.io, and the test suggested used socket.io.\n. Managed to add a test to prevent regression :)\n. Maybe a similar test should be added to socket.io\n. Yeah it's there in order for people not to add it again in the future, and give the reasoning why.\nI tend to add this kind of comments since sometimes I forget the reason I removed a certain line and saves me the time in trying it again, but of course it can be removed. It'll be a bit redundant if we manage to test this\n. At the end of the day, I think that, since deleting this.ws is such a tempting thing to do, the comment might help future contributors understand why we can't do it, and will save them time it would take them to figure it out on their own.\n. ",
    "FarzanRNobakht": "I had used engine.io version 2 and issue still exists\nbut down graded to engine.io and it was OK!. ",
    "DominikPalo": "Thanks, done. ",
    "jdolega": "I believe that would require to expose error codes to the outside i.e. the allowRequest function is actually injected via the opts, and hence the the actual error code is set on the callback function, in the caller code (https://github.com/socketio/socket.io/blob/master/lib/index.js#L62).\nOther option could be to add separate argument to the callback function \nServer.prototype.handleRequest = function (req, res) {\n(...)  \n  var self = this;\n  this.verify(req, false, function (err, success) {\n    if (!success) {\n      sendErrorMessage(req, res, err);\n      return;\n    }\nor maybe (imho better) add separate callback function for handling 'allowance' to the verifymethod, which than can be passed to the external allowRequest handler (instead today's generic callback).. Yeap. That would do the trick.. ",
    "Kenzku": "Hej, how I can turn the CORS off to prevent things like this:\n```\nPOST /socket.io/?EIO=3&transport=polling&t=1481690658797-5&sid=Dp18NNt_bWPkB4rGAAAP HTTP/1.1\nOrigin: https://evilhost.net\nHTTP/1.1 400 Bad Request\nContent-Type: application/json\nAccess-Control-Allow-Credentials: true\nAccess-Control-Allow-Origin: https://evilhost.net\nDate: Fri, 03 Feb 2017 12:52:00 GMT\nConnection: keep-alive\nTransfer-Encoding: chunked\n28\n{\"code\":0,\"message\":\"Transport unknown\"}\n0\n```. ",
    "billouboq": "Will check tonight and change what you suggested. We can't bump ws right now without doing a major release because ws 2.0.0 dropped older node support (0.10 and 0.12). ",
    "vostrik": "Thank you!\nPlease, update npm package:\nhttps://www.npmjs.com/package/socket.io. ",
    "vpxavier": "hello, is this added to socket.io 1.7.3?\nThanks a lot. ",
    "truongsinh": "Related to https://github.com/socketio/engine.io/issues/366. ",
    "hearsh": "hey any update on this?. ",
    "dongmaster": "How to reproduce this bug:\nThings that are assumed: You know how to use a terminal.\n1. Install Elixir\n2. We're gonna create a new Elixir project now. Go to the directory where you keep projects and such.\n3. Execute the following commands in terminal:\n$ mix new hello_world\n$ cd hello_world\nOpen the mix.exs file in the hello_world directory and replace its contents with the following:\n```elixir\ndefmodule HelloWorld.Mixfile do\n  use Mix.Project\ndef project do\n    [app: :hello_world,\n     version: \"0.1.0\",\n     elixir: \"~> 1.3\",\n     build_embedded: Mix.env == :prod,\n     start_permanent: Mix.env == :prod,\n     deps: deps()]\n  end\n# Configuration for the OTP application\n  #\n  # Type \"mix help compile.app\" for more information\n  def application do\n    [\n      applications: [:logger, :crypto, :ssl]\n    ]\n  end\n# Dependencies can be Hex packages:\n  #\n  #   {:mydep, \"~> 0.3.0\"}\n  #\n  # Or git/path repositories:\n  #\n  #   {:mydep, git: \"https://github.com/elixir-lang/mydep.git\", tag: \"0.1.0\"}\n  #\n  # Type \"mix help deps\" for more examples and options\n  defp deps do\n    [\n      {:websocket_client, git: \"https://github.com/jeremyong/websocket_client\"}\n    ]\n  end\nend\n```\nExecute the following commands in a terminal:\n$ mix deps.get\nReplace the contents of hello_world/lib/hello_world.ex with the following:\n```elixir\ndefmodule HelloWorld do\n  @behaviour :websocket_client_handler\ndef start_link do\n    :websocket_client.start_link(\"wss://volafile.io/api/\", MODULE, [])\n  end\ndef init([], _conn_state) do\n    :websocket_client.cast(self, {:text, \"message 1\"})\n    {:ok, 2}\n  end\ndef websocket_handle({:pong, _}, _conn_state, state) do\n    {:ok, state}\n  end\ndef websocket_handle({:text, msg}, _conn_state, 5) do\n    IO.puts \"Received message: #{msg}\"\n    {:close, \"\", \"done\"}\n  end\ndef websocket_handle({:text, msg}, _conn_state, state) do\n    IO.puts \"Received message: #{msg}\"\n    Process.sleep(1000)\n    {:reply, {:text, \"hello, this is message #{state}\"}, state + 1}\n  end\ndef websocket_info(:start, _conn_state, state) do\n    {:reply, {:text, \"erlang message received\"}, state}\n  end\ndef websocket_terminate(reason, _conn_state, state) do\n    IO.puts \"Websocket closed because of \\\"#{reason}\\\" at the state \\\"#{state}\\\"\"\n    :ok\n  end\nend\n```\nExecute the following command in the hello_world directory:\n$ iex -S mix\nYou will now be dropped into the Elixir REPL. Execute the following command in the REPL:\nHelloWorld.start_link\nA stacktrace should show in the REPL.\nDo note that the URL is in the start_link function of the hello_world.ex file.. ",
    "ISKU": "good!. ",
    "hansenyang": "https://github.com/socketio/engine.io/blob/1.8.2/lib/transports/websocket.js#L118\nshould be here, thank you\nif there are data in buffer, they should be sent to client, I think. Not sure where it throws, but at line https://github.com/socketio/engine.io/blob/1.8.2/lib/transports/websocket.js#L118\nif update it with\nself.onError('write error', err);\nthe crash is fixed. ",
    "exsilium": "Hit this myself, any suggestions for a fix?. @lpinca Appreciated! :bowtie: . I see... @lpinca , As i see #476 being open... I gather it will be eventually be pulled in for 2.0.1? Cheers!. Fixes #542 . Can we have this pulled and released, please?. ",
    "ekryski": "I saw this as well. See here for more details. https://snyk.io/vuln/npm:ws:20160920\n@darrachequesne @rauchg not sure what the implications are but it would probably be good to get a fix in soon. Let me know if/how I can help \ud83d\ude04 . ",
    "NaLLiFFuNT": "I've faced this problem too.. ",
    "paulrobello": "@darrachequesne Pull requests created for both server and client. ",
    "legikaloz": "@darrachequesne you suggested to use engine.httpServer.close() but I can't find this method nor in the ioServer nor in the io variables. Have I missed something?\nWhat I'm doing now:\nvar server = https.createServer(httpsOptions,app);\nvar ioServer = require('engine.io');\nvar io =ioServer.attach(server);\nAttempt to close the connections:\nserver.close(()=>{\n        console.log('Server closed');\n        setTimeout(()=>{\n            console.log('Process exit now!');\n            process.exit(1)\n        },10000)\n    });\nWith the call of server.close() the server stops accept new connections, but existing connections  doesn't get closed when they're trying to reconnect. This is the reason I proposed to check the server.listening property before intercept an incoming request. Without this extra check the server never gets closed, hence the callback never will be called.\n. ",
    "efkan": "@darrachequesne , it didn't come to mind!\nOn the other hand I would have to wrap the rest of the code into a callback function.\nI guess async-await provides more readability. But actually it doesn't matter for me.\nThank you!\n. I couldn't pass the tests on Travis.\nSo, I'm closing the PR.\n. @darrachequesne , I've tried to commit my PR. Unfortunately with a callback it throws different error. They may be reletad with this keyword. Meybe not.\nAs a result, I gave up for now.. Hello @darrachequesne ,\nI've seen that this PR had to be reverted.\n[revert] Make generateId method async\nThat is a breaking change, which mandates a major bump.\nI could not find any issue about it. \nI was wondering that what is the breaking change and is there anything I can do to help on this?. @darrachequesne , Do you want me to apply my code again on a new PR?. Note: there is another breaking change problem with generateId method. \nhttps://github.com/socketio/engine.io/pull/535#issuecomment-371093251\nIf the problem cannot be solved this issue must be closed.. You're right! I'll update the code.. ",
    "djoq": "not sure, it's a deep tree of dependenciess. I am forced to build a version 1.8.3 locally or it's utf and validation error mayhem, this is only way I can build both mac and Linux. Where are those versions specified?. It is fine, just engine.io is stale and ws is at 2.2. ",
    "misterigl": "any reason socketio is not merging this?. ",
    "barryvdh": "As the old versions from uws are actually removed, should a new version of engine.io be tagged, and socket.io be updated to use the new version + tagged? Otherwise uws won't be installed at all for current releases, right? (Although it is still optional, it would give warnings)\nEdit: Oh, socketio isn't using uws by default yet, so that's not actually a problem.. Yeah that would break with breaking changes., bad idea. . ",
    "pensierinmusica": "Oops, dind't see it at first, awesome thx!!  :). Mmm.. I think I've figured out what the issue is, and created a pr for it  :). Sure, in the verify method we check several user info and in case all checks are fine, would like to create a session with some of this user info, linking it to the socket id.\nIf we have access to the socket id inside the verify callback we can do it. Otherwise we need to wait for the connect event and run another query on the db to get exactly the same information we already had available.. Yes it's possible, and it's what I ended up doing in the meantime. I just think that it'd be more convenient to have the candidate id directly in the verify callback though. But you know this project better, so your call :). Sure, no worries. Btw @darrachequesne quick question, I've noticed that to generate the socket id server.js is currently using base64id. Is there a particular reason to prefer that over uuid (v4)?\nCheers!. ",
    "albertogasparin": "I also had issues with uws under Ubuntu 14.04.1 (4.2.0-27-generic x86_64). \nSomehow it was breaking requests to other resources (static files like js/css/images) closing the http connection unexpectedly and making the browser failing with ERR_EMPTY_RESPONSE.\nInitially I thought that the problem was the middleware handling the static resources, but it turned out the connection was closed before the middleware even started streaming the resource to the client. I am instantiating socket.io as suggested:\n```js\nimport http from 'http';\nimport socketIO from 'socket.io';\nexport default function(app) {\n  app.server = http.createServer(app.callback());\n  // override app.listen to start socket server too\n  app.listen = (...args) => {\n    app.server.listen.call(app.server, ...args);\n    return app.server;\n  };\n  app.io = socketIO(app.server, { serveClient: false });\n}\n```\nChanging wsEngine to ws magically fixes the issue, making requests no longer randomly failing \ud83e\udd14\n. ",
    "tonysepia": "+1. ",
    "flenter": "I think it helps if I explain how I came to this pr: \nA vulnerability scanner found this problem it send a get request to /socket.io and got a 400 repsonse with CORS headers set to '*'. After some digging I found that this is default behaviour in several places in the code.\nWhat allowRequest didn't cover\nIf you specify origins on socket.io a allowRequest function will be passed to engine.io that checks if a request is allowed based on the origins setting. The error response however ignores this and can respond with Access-Control-Allow-Origin: '*' if the origins header was not set (and otherwise returns the requests origin header value).\nThe new origins option is used for two things, the first one is to return the correct header for errors handled by the sendErrorMessage function. The second use is explained below.\nThe PR however contains another change and that came up after I noticed that the long-polling transport behaves similarly to the error handling function regarding CORS header values. This is why the opts object is added to transports constructor. It allows the origins value to be passed to the (xhr-polling) transport and the responses can contain the correct CORS header values.. Is there anything I can do to help progress this pr?. I've rebased the pr so there are no more merge conflicts. ",
    "uncinimichel": "+1\n. ",
    "pratheekhegde": "When will this PR get merged? Is there any blocker?. ",
    "mooflu": "As an alternative, would it be easier to provide official api access to the headers before they are written. We ran into the same issue as @flenter where a penetration testing suite flagged socket.io as having arbitrary origins trusted. Our current approach is to register for the socket connection's transport 'headers' event as well as intercept the response's writeHead for the handshake in allowRequest (no access to socket transport at that stage) and modify the headers. Needless to say, that's icky. In our case we don't even need CORS at all. . ",
    "holm": "Would be really good to have this issue sorted out one way or the other.. ",
    "ntazelaar": "Hi,\nYes, there are some unrelated changes because there were some conflicts on the lines that I edited. That's why I had to merge some changes from your fork to my fork.\nThere are no conflicts now so this should not be an issue. Most of the changes in my fork are already in the main repository.\nThe reason I had to make these changes is because we were experiencing crashes when we were testing our Node.js application with the OWASP Zapp tool. This is a tool for testing the security in applications. These crashes don't happen anymore after the changes I made.\nKind regards,\nNick Tazelaar\n\nFrom: Damien Arrachequesne notifications@github.com\nSent: Saturday, October 7, 2017 1:56:06 PM\nTo: socketio/engine.io\nCc: Nick Tazelaar; State change\nSubject: Re: [socketio/engine.io] Added some checks to prevent usage of undefined properties (#515)\nHi! I think the pull request contains unrelated changes, could you edit it please?\nAlso, do you have a case where those undefined properties are actually accessed?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHubhttps://github.com/socketio/engine.io/pull/515#issuecomment-334929971, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AQpQ5KhHWy-bZE7JyA7-iYhM0Bap5Cfoks5sp2bVgaJpZM4N6Acy.\n. ",
    "jharris4": "This seems to be specific to the use of optionalDependencies section in package.json. I've filed a new issue in the yarn repro. https://github.com/yarnpkg/yarn/issues/3728\nThis issue can probably be closed, since it seems to be a bug in yarn. I only filed it here in case anyone else stumbles onto the same issue. :-). It's actually the uws package dependency that's causing the issue. I believe it may be because the package.json is missing node-gyp as a dependency. Created an issue/PR to fix it:\nhttps://github.com/uNetworking/bindings/issues/31 . If/when that gets merged, I can confirm that it's fixed, and then it'd be great to update the uws version in engine.io's dependencies.... ",
    "olegmdev": "Tests passed on my end successfully. Travis is showing some weird errors for me right now. Looks like it depends on luck to be built successfully :). ",
    "dennisat": "For client is described how apply the binary mode here: https://github.com/socketio/engine.io-client#sending-and-receiving-binary\nBut for server where should we apply this?. Thanks for the clarification!. Sorry, nothing is wrong with this setup.\nThere was an issue with the deploy process and the container.. ",
    "KevinW1998": "I've had the same issue. It seems that it is a issue with uws. If it fails to use uws it will fallback to ws (expection is thrown and catched). If this happens a second time the fallback fails (exception is not thrown) but the wsModule is set to undefined. Therefor the error.\nA temporary workaround is to use the ws module.. ",
    "leader22": "I've had the same issue too.\nAnd my case, it was because make command does not installed and failed to build uws.\nMy workaround is to modify code like\njs\nwsServer1 = engineIo(httpServer1, { wsEngine: 'ws' });\nor wsEngine can be set by process.env.EIO_WS_ENGINE.\nhttps://github.com/socketio/engine.io/blob/master/lib/server.js#L39. ",
    "raulmt": "Hi @darrachequesne . At first I thought \"no, because if onClose is called and sets chunks to null, then the next call to onData if any will fail with the same problem (chunks not being a Buffer). But as I described in #528 the only way I can think of this could happen is if onData callback is already in the callback queue before consequences related to destroying the socket even happen. So if that's the case, the onClose callback because of destroying the socket should be warranted to happen after that eventual onData next call, so chunks would still be a Buffer\u2026\nI tested what you proposed and it seems to work too, both in cases where max buffer is reached and when it's not. I updated this PR with your suggestion since it's simpler. Thanks :D. @darrachequesne actually I had to add that line setting an empty buffer when self.maxHttpBufferSize is reached back\u2026 onEnd is still called, and before onClose, so self.onData is called, and that makes a couple of tests fail because we shouldn't emit data if the max http buffer size was reached. I kept the change inside cleanup though\u2026. ",
    "errorx666": "Can we make this support Promises (async functions) as well as callbacks?. ",
    "vincentbriglia": "I'm getting the same in stdout, but the latter error is a question of setting the right location for your openssl headers. \nSomething like LDFLAGS=-L/usr/local/opt/openssl/lib CPPFLAGS=-I/usr/local/opt/openssl/include HOME=~/.electron-gyp node-gyp rebuild --target=1.7.9 --arch=x64 --dist-url=https://atom.io/download/electron in your uws folder might do the trick. \nhowever, for me that doesn't remove the first error and it's doing my head in. ",
    "d4tocchini": "this was a nightmare of a bug.  my solution:\neliminate socketio. \ni was looking for an excuse to use webrtc as the interwindow ipc mechanism in electron anyway.... ",
    "airtoxin": "Set environment variable export EIO_WS_ENGINE=ws to use ws instead of uws.. ",
    "deniskrop": "What's the best way to resolve this?. ",
    "OliverRadini": "In case anyone stumbles across this and is confused about how to switch to ws from uws, I found that you can just change this on the options, for instance:\njs\nvar io = require('socket.io')(80, { wsEngine: 'ws' });\nIf there's a better way, please correct me.. ",
    "mtrabelsi": "if you use socket io in node install this as well : \nnpm i --save uws\nnpm i --save utf-8-validate\nnpm i --save bufferutil\n. ",
    "ProductOfAmerica": "I solved this in WebStorm by entering yarn add socket.io into the console manually. WebStorm seemed to think the package was installed, and yarn wouldn't do anything when I yarn install'd.. ",
    "vkartaviy": "@darrachequesne Could you please make a release with updated uws dependency to npm? \nThanks!. ",
    "endbay": "Is there any reason why this was set to 5secs by default? With this new default settings, i received ping timeouts on the client WebSocket is already in CLOSING or CLOSED state.. If i set it back to 60secs on backend, the error is gone.. ",
    "kevinhodges": "Seeing similar symptoms, be good to get a resolution!\n  . ",
    "Maledong": "In my test, it seems ONLY 10.148.0 (uws) works on me without any other configs, leave it here for others if they need help \nBTW\uff1aWill this be a hole in the future?!. Thanks @kkomelin . Thanks @slartibardfast . ",
    "kkomelin": "Here is more info about what happened with uws https://www.reddit.com/r/node/comments/91kgte/uws_has_been_deprecated/. ",
    "slartibardfast": "hi, a fork / continuation is available, it even has a typescript based js component\nhttps://www.npmjs.com/package/@clusterws/uws\nhttps://github.com/ClusterWS/uWS\nmight be worth considering a switch?. ",
    "Kamil93": "Has someone tested it with engine.io? https://www.npmjs.com/package/@clusterws/cws. I will, thanks for reply. The big problems are posts around the web about \"this is a problem\" and provided solutions that sometimes work, in some version of eio/sio work etc. so together with hole in documentation it makes me to post and issue here.\nI've already found by consolelogging socket object that socket itself has remoteAddress property that contains correct (in my situation) IP, is it safe way? Or I should use socket.handshake.address?. Unfortunatelly not, I've discovered that there are some incompatibilites with original ws module, like handleUpgrade has different arguments, 2 instead of 4, the main important one was callback param, I've done some changes to this method and it works but then another problem occured so I gave up for a while.\nWonder if it's even possible to support it in engine.io without big changes. @darrachequesne \nSo far so good, I've upgraded \"handleUpgrade\" method to be compatible with ws module and also changed default EventEmitter like here: https://github.com/ClusterWS/cWS#replace-eventemitter\nto classic one from 'events' module and it works! Gonna test it more. @yambakshi, it is because you need to install uws first from npm (watch out for versions, read about uws author and why he had deleted it from npm). ",
    "StefansArya": "Maybe uws was deprecated because uWebSockets.js was released... > No..\nOk, I will change my comment..\nMaybe we can use the official version of uWebSockets.js instead of the deprecated uws library... Maybe you're right..\nI haven't explore it much, so I will accept your suggestion.. ",
    "DarkGhostHunter": "\nMaybe uws was deprecated because uWebSockets.js was released..\n\nNo.. . We should use cWS instead of uWS. Who knows if the author gets another\ntantrum and deletes the project itself.\nEl lun., 11 de feb. de 2019 22:19, StefansArya notifications@github.com\nescribi\u00f3:\n\nNo.\nhttps://www.reddit.com/r/node/comments/91kgte/uws_has_been_deprecated/.\nOk, I will change my comment..\nMaybe we can use the official version of uWebSockets.js\nhttps://github.com/uNetworking/uWebSockets.js instead of the deprecated\nuws library..\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/socketio/engine.io/issues/560#issuecomment-462570788,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AE51lwYoiFZEHBhGu9jSdRBUElW_bYctks5vMhaFgaJpZM4WgpsF\n.\n. \n",
    "shapel": "562 . #561 .",
    "oliversalzburg": "Seems to me like the test failures were introduced by engine.io-client@3.3.0.. I believe https://github.com/socketio/engine.io-client/commit/99bcc622cb2cda123a2653c7d031e232ee6789e9 introduced these failures, because window is not available. I left a comment on the relevant line. @darrachequesne Should be good now :). Didn't properly read what the code was doing. Adjusted.. ",
    "yambakshi": "Whats the recommended uws workaround? Will it be removed from the wsEngine switch in server.js? (line 107)\nIt still throws an exception\nModule not found: Error: Can't resolve 'uws'. ",
    "klimashkin": "\nread about uws author and why he had deleted it from npm\n\n@Kamil93 What do you mean? Could you elaborate more on where to read?. ",
    "jordonias": "While polling the client sends a POST request on emit. I believe I would expect the Access-Control-Allow-Origin from the response.\nhttps://github.com/socketio/engine.io-client/blob/f62fca4b052b54992d3b1e0c476c62ffd82f216a/lib/transports/polling-xhr.js#L107\nIs there any reason to not just do the following?\njs\nif (req.headers.origin) { \n  headers['Access-Control-Allow-Credentials'] = 'true'; \n  headers['Access-Control-Allow-Origin'] = req.headers.origin; \n}\nAnd I believe modern browsers will also not set the origin header on same-origin requests when method is GET or OPTIONS.. @darrachequesne any input on this?. ",
    "headlessme": "Actually, looks like this line is why you have to implement handlePreflightRequest:\nhttps://github.com/socketio/engine.io/blob/cb0ac6fddcad12c454651bf0e1a312a154e228a4/lib/server.js#L463\nIf you don't implement it then OPTIONS requests get pushed straight into the handleRequest flow for middleware without a fallback option.\n. ",
    "digawp": "ESLint requires us to make use of err when handling the error callback. Should I make an exception to the rule here, or is the way I handle it currently is fine? It passes the test now.\n. Refactor assignment in a while-loop to a for-loop.\n. new eioc.Socket(...) changed into eioc(...) (if the socket created is not used afterwards)\nThere are multiple occurrence of this in test/server.js as well\n. This is the diff comment that requires attention\nThis variable destroyUpgradeTimeout is not actually used, but it seems as if it should be used, looking at line 450.\n. This line here. If it were to use the destroyUpgradeTimeout in line 411, it should default to 1000ms. But because it is using the destroyUpgradeTimeout from options, if it is undefined, it will default to 0ms.\nSo my question is, which is the intended behavior, if options.destroyUpgradeTimeout is undefined? Defaults to 1000 or 0 ms?\n. ",
    "ChALkeR": "Why?. @darrachequesne Any of those would be fine. Is there a reason for keeping support to Node.js versions <4.5.0? Those are obsolete and are not getting security updates. Even 4.x branch would be out of security support in a few months.\nAlso, you can use Buffer.concat([]) for this specific line (to construct an empty Buffer) to keep support for old Node.js versions and avoid hitting deprecated API. But I would recommend just to drop support for everything older than 4.5.0.. "
}
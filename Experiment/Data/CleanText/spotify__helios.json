{
    "danielnorberg": "Looks like this code was initially written with the intent of having the start point be randomized.\n. Yeah I think we might want to randomize port allocation to avoid e.g. a port previously used by a container for service foo being reused by a container for foo service bar while clients are still attempting to send traffic it. I haven't given it a whole lot of thought tho.\n. @matslina Turns out that the search for a free port currently continues from where the last allocation finished. So it's not randomized but it won't immediately reuse ports.\n. Closing due to #79.\n. I'd like Helios to prove that it can properly pull images.\n. I think I might still want to have the registration domain and name be part of the ServiceRegistration object passed to ServiceRegistrar.register()\nI.e., have the address and domain passed to ServiceRegistrarFactory only be used for resolving the registry to talk to, and not used in the actual service registration.\nThen we can leave the ServiceRegistrarFactory interface as is.\n. Having pondered this a bit more, here's what I want to do:\n- Leave the ServiceRegistrarFactory interface as is.\n  - Both create and createForDomain specify that address and domain are implementation specific and they are as such sufficient even for e.g. a skydns plugin. \n  - create should create a registrar that talks to the registry that address specifies. E.g., tcp://foo.bar:4711 may mean talk to foo.bar:4711 while srv://prod.foo.net may result in the resolution of SRV _skydns._http.prod.foo.net. This is per-plugin implementation specific. Helios must not attempt to parse or create an address string.\n  - createForDomain should make use of a resolution mechanism to resolve the registry. E.g., SRV _skydns._http.${domain}. \n    - Helios \"understands\" what a domain is and can provide/create one, although the plugin might do with it as it wishes. \n    - A less DNS-centric service registration plugin may make use of a different mechanism to resolve the registry for domain. \n  - A plugin may accept additional configuration through environment variables, or from a file on disk, or a combination.\n- Add fields to ServiceRegistration as needed.\n  - The classes in com.spotify.helios.serviceregistration are provided by Helios at runtime. We can therefore add fields as necessary without breaking existing plugins. Any fields we add to enable use of e.g. skydns should therefore not require changes to our existing plugins. http://docs.oracle.com/javase/specs/jls/se7/html/jls-13.html\n. I do not agree that domain is a nameless specific hack. Most environments will have domains of some kind (that may coincide with DNS domain or not) and a way of discovering service registration endpoints for these domains. Therefore I think that having a first-class domain concept in Helios is useful.\nAlthough the possibility of having multiple plugins and per-job service registration endpoint selection could be useful, I don't think that we need this right now. Let's iterate in smaller steps. If we need separate CI hosts for now, that's fine.\nWe don't need to json-serialize the ServiceRegistration object.\n. What jvm and version were you using? Was that in a container limited to 250mb ram?\n. Ok, when using the default profile surefire will fork out and run as many tests in parallel as you have cpu (ht) cores in your environment. Try adding -Ddebug to avoid forking. I.e., mvn clean test -Ddebug.\n. Yeah, we're having surefire run tests in subprocesses, and the subprocess is unable to allocate enough ram to run.\nYou might be able to build helios itself by doing mvn clean package -DskipTests, but you're probably not going to be able to run the tests on a 250mb ram vps.\n. Are you running mvn clean package -DskipTests in the project root? Running maven in a submodule folder won't work, because maven.\n. It builds fine for me if I do a mvn clean package -DskipTests in the root.\nhttps://gist.github.com/danielnorberg/2e6a5de86ef98c8f203d\n. If you still want to build helios yourself, try telling maven to use less ram: \nMAVEN_OPTS=\"-Xmx128m\" mvn clean package -DskipTests\n. Is the master running on that host?\n. This now works internally.\n. :beers: \n. :poop: \n. just realized that I hadn't thought about -q -f, hold off on merging a bit\n. There, now it handles hosts -q -f as well.\n. Add tests for expires field to JobTest. \nYou might also want to rebase on master to get less spurious build failures.\n. :+1: \n. Did you push the changes?\n. Seems there's a few checkstyle warnings.\n. Sorry for all the nits.\n. :+1: \n. :+1: \n. Looks good apart from the whitespace trimming issue.\n. :+1: \n. Looks good overall. There's a few nits and a simple race condition around the file creation that would be great to have fixed.\n. :+1: good stuff!\n. :+1: \n. :+1: \n. :+1: \n. It shouldn't be necessary to look up ports from docker.\n. See #66\n. :+1: \n. - This is a breaking change for current helios client users\n- A lot of the benefit could be achieved by just having better javadocs\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :poop: \n. :+1: \n. @drewcsillag Everything except TemporaryJobsTest.testDeploymentFailure passes. Could you help me take a look at what's going on?\n. Huh, seems like tests passed on travis. Maybe that test just spuriously fails on my laptop. \n. Now that the agent is polling, it takes a few seconds for it to find out that a container exited. So TemporaryJobsTest.testDeploymentFailure fails because it needs TemporaryJobs to fail if the container exits quickly. We need to resolve this race somehow before merging.\nMaybe we can change TemporaryJobs to continuously monitor the containers in the background and fail the test if they exit.\n. nasty\n:+1: :poop: \n. :+1: :poop: \n. :+1: \n. :+1: \n. Could you try helios status --json instead?\n. :+1: \n. :+1: \n. + :100: :whale: \n. :+1: \n. This is to allow use with e.g. :poop: cucumber-java :poop: .\n. :+1: \n. Maybe we should have some test covering this.\n. Good catch!\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. Crazy hack. Long term maybe we want to either deprecate the multi domain support or have the concrete command implementations return an Object that can then be put into a {\"$DOMAIN\": $VALUE} dict before json serializing and returning it.\nI.e., pull the stdout/stderr output code up into ControlCommand.java or something.\n. :+1: \n. Also drop the OS column.\n. Both. I'm trying to save screen space and I think that the current output is probably confusing/not very useful.\n. Is this PR something we want?\n. :+1: \n. Consider using curator namespaces for this.\nhttp://curator.apache.org/curator-framework/index.html\n. Not sure if this is what's happening here but I've seen those exceptions in the past as a result of a zookeeper client that's currently in use being closed during shutdown. So the \"Client is not started\" message might be misleading.\nI wouldn't mind taking a look at the namespacing branch if you want to push it.\n. looking\n. Yeah, like @drewcsillag says, we explicitly do not deregister services when the agent shuts down. Overall a design goal for Helios is to allow for upgrading/restarting platform components with minimal production service impact.\n. Cool! =)\n. We really should. \n@drewcsillag What do you say?\n. This might be a red herring. Might simply be an incompatibility/regression when using docker 1.3.\n. This does indeed seem to be a docker 1.3.x regression.\nhttps://gist.github.com/danielnorberg/5c79fba8aed5e87ea47e\n- Docker 1.0.0\n``` json\n        \"Cmd\": [\n            \"go-wrapper\",\n            \"run\"\n        ],\n    \"Entrypoint\": [\n        \"/helios/syslog-redirector\",\n        \"-h\",\n        \"10.0.3.1:6514\",\n        \"-n\",\n        \"job_test_14b6385a:v1299bdff:b72aded0929d001a44189064cc156d1d629b0638\",\n        \"--\"\n    ],\n\n```\n- Docker 1.3.1\n``` json\n        \"Cmd\": [],\n    \"Entrypoint\": [\n        \"/helios/syslog-redirector\",\n        \"-h\",\n        \"10.0.3.1:6514\",\n        \"-n\",\n        \"job_test_f0c43994:v73b4a43f:b853a59c4ee94982f78c799723daa41e1f5386ad\",\n        \"--\"\n    ],\n\n```\nIn docker 1.3.x, if Entrypoint is set on the container config when the container is created, then the Cmd from the image config doesn't survive.\nI.e. Helios tries to decorate the Entrypoint to inject the syslog-redirector and then dockerd wipes out the Cmd.\nNot sure if this is new intended docker 1.3.x behavior.\nWe ought to be able to work around this by going through all base layers and to reconstruct the Cmd, but it would seem to require at least one request per base layer to dockerd to fetch image info. \nWe should poke the docker guys to find out if this is a regression that is known and/or can be fixed upstream.\n. Unpleasant side-effect of using the buggy apache http client. Would be nice if this could've been encapsulated inside the DockerClient, but this is definitely a feasible workaround.\n:+1: \n. It's being used as part of the normal system tests and I've manually verified as well.\n. So here's my understanding of what's going wrong.\n1. The HeliosClient POST /jobs a Job with a full id but no creatingUser set.\n2. The HeliosMaster POST /jobs handler sets the creatingUser field and rebuilds the job, causing a new hash to be generated.\n3. Lose. The new hash does not match the hash that was sent by the client.\nThere's three ways forward that I can think of right now.\n1. Make the CLI and HeliosClient users set the creatingUser field before posting. This is a breaking change.\n2. Exclude the creatingUser field from the hash generation. In practice this is what the CLI does today, so even though it's a breaking change it would only impact users of HeliosClient that are currently explicitly setting creatingUser on the Job.\n3. Remove hash equality validation from JobValidator and leave it up to users to enforce hash equality if they want. My impression is that users today do not really leverage the hash to enforce Job consistency between different Helios clusters, so this might be a minimally impacting change.\n. :+1: :bear: :elephant: \n. s/:bear:/:beer:\n. Java and Unix sockets are not the best of friends, although I believe the docker client that helios-testing uses does support unix sockets.\nIs it possible to simply configure docker beta for mac to listen on a tcp port?\n. Also, do you have any docker beta invites to hand out? ;)\n. there were warnings for this?\n. there were warnings for this?\n. we should change the cli to not use getDeployedHosts instead\n. we should change the cli to not use getDeployedHosts instead\n. So we should annotate it with @Override then I guess\n. Can we add an @Override instead?\n. Actually, seems we've open sourced our logging lib. How about ripping out our duplicate and using that instead?\nhttps://github.com/spotify/logging-java\nhttp://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22com.spotify%22%20AND%20a%3A%22logging%22\n. I'm explicitly removing the spotify specific SRV format.\n_%s._http.%s is the generic form (for helios, given it uses http).\nFor our internal use, we're going to have to specialize our environments/packages to provide the _spotify-%s._http.services.%s format via the HELIOS_SRV_FORMAT env var.\n. ```\nThe format of the SRV RR\nHere is the format of the SRV RR, whose DNS type code is 33:\n    _Service._Proto.Name TTL Class SRV Priority Weight Port Target\n\n```\nhttp://www.ietf.org/rfc/rfc2782.txt\nIf someone wants a non-standard SRV format, then can supply it using the HELIOS_SRV_FORMAT env var.\nE.g., HELIOS_SRV_FORMAT=\"%s.%s\" for your example.\n. checkNotNull(p.registrationDomain, \"registrationDomain\")\n. null by default instead of empty string?\n. s/advertisedHost/host in order to match the naming of other fields.\n. isEmptyOrNull\n. checkNotNull(domain, \"domain\");\n. shouldn't we use null instead to mean \"not set\" or \"absent\"?\nHeh, which would mean removing the null check instead =)\n. Should the default be the /etc/resolv.conf domain config?\n. isNullOrEmpty\n. add registrationDomain field tests to verifyBuilder as well\n. Have a NO_DOMAIN constant instead of using a literal empty string?\n. is this supposed to be isNullOrEmpty(registrationDomain) ?\nIt should probably be the first check if registrationDomain might be null, as the first if statement will NPE in that case.\n. Consider using Name.fromString(domain, origin)?\nhttp://www.xbill.org/dnsjava/doc/org/xbill/DNS/Name.html#fromString(java.lang.String,%20org.xbill.DNS.Name)\n. Put this utility method elsewhere and make it public instead of using @VisibleForTesting\nConsider relying on dnsjava parsing instead.\n. agreed\n. do not use star imports\n. or(EMPTY_REGISTRATION_DOMAIN)\n. checkNotNull(p.registrationDomain, \"registrationDomain\");\n. restore this empty line\n. s/advertisedHost/host\n. Can we have this method use job.getRegistrationDomain() and defaultRegistrationDomain directly instead of passing them in?\nAnd to fit in with the naming of the methods in the rest of the class, can we drop the get prefix? Or seeing as the purpose of the method is to return a fully qualified registration domain, maybe call it registrationDomain() or fullyQualifiedRegistrationDomain()?\nAlso, javadocs would be nice.\n. Use try-with-resources.\n. EMPTY_REGISTRATION_DOMAIN\n. This should be END.gsub(/^ {4}/, ''). See #57.\n. this is unexpected and might signal programming error, log warning.\n. log warning.\n. private?\n. Remove get prefix. Generally, is is an anti-pattern to prefix java methods with get simply because they return something, unless it's a JavaBean.\n. package private?\n. There is a race condition here where another JobPrefixFile might delete this file after we create it but before we lock it.\nCreate the file with a suffix, lock it, and then rename it to its final name:\njava\nfinal Path dir = Paths.get(PREFIX_FILE_DIRECTORY);\nfinal Path file = dir.resolve(prefix);\nfinal Path tmp = dir.resolve(prefix + \".tmp\");\ntry {\n  this.channel = FileChannel.open(tmp, CREATE_NEW, WRITE);\n  this.lock = channel.lock();\n  Files.move(tmp, file);\n} catch (IOException e) {\n  throw new RuntimeException(\"Failed to create lock file: \" + file, e);\n}\nThen have the cleanup code exclude files ending with .tmp.\n. The responsibility of this class seems to be to deal with job (un)deploys. Can we call it Jobs instead of generic Utils?\n. private\n. private\n. Might delete() ever be called twice? Then we might want to synchronize it and assign null to fileLock. Otherwise we'll get exceptions when we try to double-release.\n. Might still be wise to log at debug level.\n. create files in a temp dir that gets removed in @After?\n. Yeah, having typed value objects instead of Strings might've been good. Unfortunate that java doesn't allow e.g. golang style type Foo String\n. at least package private then?\n. I think I've had exceptions thrown at me for calling release twice. Maybe that was when calling release after closing the file. Which we should also do.\n. Anything that does not form part of the public api and is not expected to be used by users should be private. For inspiration on why, see Effective Java item 13.\n. I usually inline checkNotNull into the assignment. This makes is easier to see that the null check hasn't been omitted for some assignment. Also provide variable names to checkNotNull.\njava\nthis.file = checkNotNull(file, \"file\");\n...\n. Did you intend to add javadoc here?\n. That's done in after() by executor.shutdownNow()\n. Derp. Please ignore previous comment. I'll fix.\n. Fixed. Good catch!\n. catch on the same line as }\n. Would it be worth using JobId.toShortString() ?\n. SGTM.\n. Creating stack traces for all threads might be expensive. Should be possible to get the stack trace for the current thread directly: Thread.currentThread().getStackTrace()\n. Actually, turns out that you might want to do new Throwable().getStackTrace() instead. It's supposedly more than 10x faster than Thread.getStackTrace() and also includes the current method stack frame.\nhttp://bugs.java.com/bugdatabase/view_bug.do?bug_id=6375302\n. runner.stopAsync().awaitTerminated(); stopped and joined the runner thread, so we probably want to keep that to avoid race conditions. Without joining the runner thread here, the containerNotRunning() check below might pass before the runner thread has managed to start the container.\n. The reason that TaskRunner has a thread was to allow for a synchronous and blocking programming style when interacting with the docker daemon while ensuring that manipulation of different containers did not interfere. I now think that was both overly defensive and optimistic wrt to the concurrency capabilities of Docker.\nI want to get rid of both the Supervisor and TaskRunner threads and have the single Agent reactor thread do all docker daemon interaction synchronously. If a slow pull happens to block starting another container, so be it. For purposes of automatically restarting containers when they crash we should probably just configure the docker daemon to do that now that it has grown that capability.\n. Then restarting the agent would stop the container, and we probably don't want that.\n. @drewcsillag Not sure about the rename. containerConfig is already used by the parameter with the same name, which actually holds the config of the container we're about to start. imageConfig is the config of the image used by the container.\n@davidxia dockerd seems to provide the inherited/effective CMD in imageInfo.config. API docs are not very informative on this point. Might be worth investigating a bit more.\n. ",
    "matslina": "Would you rather have it randomized?\n. I'm wary of introducing new states unless we absolutely have to. It's messy enough as it is. Letting it health check forever seems fine to me. Job definition can always be extended with an optional restartIfHealthCheckFailsForMoreThanThisManySeconds or something, should we ever need that behaviour.\n. :+1: \n. And commenting here would of course have been more appropriate...\n. ",
    "rohansingh": "Reasoning?\n. This is something we've considered. There are some things to think about around availability when both the agent and master are running in containers. For example, what supervises the Helios agent container and keeps it running? Maybe Upstart?\nIt's definitely a possibility though, especially for development.\n. With helios-solo, we now have Helios-in-Docker for development purposes. helios-solo brings up a simple one-agent cluster in a Docker container.\n@aidanhs, with helios-solo, upgrades are done by running helios-use latest. However, this is a destructive operation that wipes your whole helios-solo instance. Persisting the state directory is definitely a possibility, probably would be a pretty small change to the docker run command in helios-up.\nIn addition to helios-solo, we actually also build a helios-master and helios-agent Docker image with the build-images Maven profile:\nmvn -P build-images package\nThese are more flexible and could, in theory, be configured to run in a cluster configuration. We use them for integration testing, but I don't see us recommending them as the blessed way to install Helios and run a cluster.\n. Closing this issue since we no longer use or recommend Vagrant.\n. +:100: \n. :whale:\n. This also fixes #46.\n. :+1: \n. I like it.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. At some point we might want to add something in before_deploy to block deployments of branches other than master (or we might not). But no need to do that now.\n:+1: \n. :+1: \nAlso would be good to somehow internally document the secure vars, so that if we ever switch build systems, we can migrate things.\n. :+1: \n. :+1: \n. :+1: plz merge kthxbye\n. :+1: \nBuild passed at https://circleci.com/gh/spotify/helios/136\n. What does docker info return after you vagrant up?\n. Hmm, that's very strange that docker info seems to work but that the tests fail when trying to reach Docker on the same endpoint. I'm not sure how to troubleshoot further, maybe someone else has other ideas.\nIf you're willing to wait a bit, one thing we're working on right now is ditching the whole vagrant up approach and running Helios in Docker. Hopefully this should make the networking issues a lot more transparent and things should work out of the box with less effort.\n. Looks like the DOCKER_HOST is being reset to /var/run/docker.sock somehow?\nDo you have anything that might be doing that, maybe in your .mavenrc or\nmvn script or elsewhere?\nAlso, does running it all on one line have any effect:\n$ DOCKER_HOST=tcp://192.168.33.10:2375 mvn clean test\nOn Wed, Aug 13, 2014 at 4:38 PM, Bismoy Murasing notifications@github.com\nwrote:\n\nwell I tried that and I don't know why I'm getting Permission denied !!\n2014/08/14 02:05:58 Post http:///var/run/docker.sock/images/create?fromImage=busybox&tag=:\ndial unix /var/run/docker.sock: permission denied\nMay be there is something missing in my system ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/issues/124#issuecomment-52107204.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n\nTruly one of the most exciting diffs I have ever seen ;-)\n. :+1: \n. :+1: \n. This has a huge performance overhead, so not merging to master for now.\n. :+1: \n. Are these figures from CircleCI or your own machine?\n. This should be further mitigated now that #279 is merged.\n. I think this breaks two things, in contrast to how SystemTestBase does cleanup for testTag-tagged tests now:\n1. If we run the Helios tests on a CI cluster that's also running TemporaryJobs tests, those temp jobs tests will be incorrectly killed.\n2. If we run multiple or parallelized instances of the Helios tests on a single host, they will clobber each other.\nI think a better approach would be to modify TemporaryJobs to allow specifying a custom prefix or suffix, and then use testTag for that prefix/suffix in the tests.\n. Fixed by #260.\n. Looks great to me. @carlanton Would you mind adding an end-to-end test in helios-system-tests that ensures that the resource settings specified for a job are actually applied to the Docker container? If you're not sure how to approach this, let me know and I can take a crack at it.\n@drewcsillag @danielnorberg Want to review this and make sure the design makes sense?\n. Some day we will have a workable build system...\n. Yeah, CircleCI requires LXC. Let me take a look at this and see what I can figure out.\nSorry for the trouble, @carlanton.\n. Alright, looks like the problem is that CircleCI doesn't support memory/swap limits. I think the best way to deal with this is actually to use the output from docker info, which reports whether the Docker instance supports those limits.\nThis requires a modification to docker-client, which I'm now working on.\n. :+1: \n. :+1:\n. :+1:\n. :+1: \n. I'm confused. Is this a duplicate of #271?\n. :+1: \n. :+1: \n. If you rebase on latest master, it'll be much more likely to pass tests.\n. :+1:\n. :+1: \n. :+1: \n. I've tested this and it fails without #299. It passes once #299 is applied. Meaning that it's an accurate test of the issue reported in #293.\n. :+1: \n. @danielnorberg @drewcsillag While I'm glad it works, any ideas on how this test successfully deploys and runs 40 concurrent jobs while the docker-client connection pool size is only 20? It checks for job state of RUNNING too, so it's not just fire-and-forget.\n. Ah, never mind. The tests passed because waitContainer is overridden in PollingDockerClient to use inspectContainer instead of actually keeping a container open. So Helios isn't blocked by the client connection pool size. Woot woot.\n@danielnorberg Do you think the PollingDockerClient.waitContainer implementation should actually be migrated to docker-client itself?\n. @drewcsillag Fine, fine ;-)\n. v2.7.5 doesn't actually have any new features.\n. Apologies for the late response. Helios does indeed support SSL connections to Docker since some versions ago, though unfortunately it looks like I failed to document this. Either use the --docker-cert-path flag for the Helios agent, or set the DOCKER_CERT_PATH environment variable.\nI'll leave this issue open until we have this documented correctly.\n. :+1: \n. Oops. Nice catch.\n. :+1: \n. I think there's still some instances with incorrect casing. Maybe try /[a-z]auto/?\n. Just to summarize our in-person discussion for the record: yeah, it's \"alpha\", but all known issues are pretty well-documented in the ZooKeeper JIRA \u2014 and it will probably be several releases before we get through alpha, beta, RC, etc. Here's the proposed schedule:\nhttp://markmail.org/message/ymxliy2rrwjc2pmo\nNone of the 3.5.0 issues affect our usage of it as a client library.\n. Yup. The ensurePath is namespaced, and getState is namespace agnostic.\n. What's the deal with this? Should it still be an open pull request?\n. We actually talked about this a while ago at Spotify. If I recall, we punted on it just because it wasn't high-priority and we found other ways to do what we wanted to use tags for.\nSeems like a pretty sane feature to have though.\n. :+1: \n. I'm going to try to review this today. Just a heads-up, here's 3 important things that I'd like to ensure:\n1. Changing the tags changes the job hash.\n2. Old jobs that have no tags still deserialize properly.\n3. The job hash is unchanged for old jobs that have no tags.\n. Apologies, just been swamped here by day-to-day support and stuff. Going to try to get this merged by the end of the week. You mind if I rebase it?\n. Looks good to me and seems to pass all checks.\n. @carlanton Thanks for the commit and apologies for the very long lead time. Hope it doesn't discourage you from contributing in the future, will try to be more responsive.\n. @carlanton We're evaluating a bunch of cloud platforms. What I can tell you is that we've run Helios open source on both Google's cloud and on Amazon, and I've also tested Azure. It was pretty easy to get up and running on all of the above and we don't have any plans to abandon anything. *\n* My personal view, I obviously don't speak for all of Spotify. For all I know tomorrow we will decide this whole Internet thing was a fad and will start shipping audio cassettes to all of our customers.\n. :+1: \n. :+1: \n. Hmm, that didn't go so well.\n. Continued in #678.\n. :-1: in favor of #384 \n. I like the approach. Makes sense.\n. I'm actually a bit skeptical about raising this for this reason. While HeliosClient and TemporaryJobs might be thread-safe (are they?), it would still be best for each thread to have its own TemporaryJobs and HeliosClient.\nSo in this case, I'd recommend using <parallel>classes</parallel> instead of all.\n. Oh yeah. Debian packages. All the control files in src/deb need to declare a Java 8 dependency.\n. Not that I'm aware of. Maybe just an oversight.\n:+1: \n. Can we close this since @carlanton has added the tests to #370?\n. :+1: if you squash your commits.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. Abandoning in favor of #397.\n. I'd like to close this in favor of #401 if that's OK?\n. Personally feel that IllegalArgumentException is a ridiculous exception to throw when DNS resolution fails. Grr, ZooKeeper.\n:+1: \n. The major change in this over #398 is that each type of health check is in its own class.\nNote that I also renamed the docker exec-based healthcheck to ExecHealthCheck and {\"type\": \"exec\"}. I think this name better reflects what the check does.\n. Closed in favor of #401.\n. Included in #401.\n. Excellent idea. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. 137 is usually an out-of-memory error condition. Perhaps your boot2docker VM is just starved?\n. mvn install just installs JAR's to your local repo, it doesn't install Helios anywhere.\nIf you wish to run Helios, here are some instructions for the \"manual\" approach:\nhttps://github.com/spotify/helios#manual-approach\nYou can then run helios-agent or helios-master however you'd like, maybe as part of some init system (I'm not familiar with CentOS). Alternatively, you could take a look at the Debian packages and try to port them somehow (./src/deb).\nThe final possibility is to use alien to convert our release deb's to rpm's, but that's probably not a good idea.\nHopefully this answers your question.\n. :+1: \n. Maybe could be an extension to helios watch, to stop watching when all statuses are RUNNING.\n. I wonder if this is necessary anymore with deployment groups. Effectively  helios rolling-update does the same thing.\n. Check out the latest update. I realized that getEnvironment() doesn't actually return what I think it does. It returns the --env flags that are set when starting the Helios agent, not the environment variables for the Helios agent itself.\nSo to get the Docker host and cert path from the agent, I'm now having it explicitly report those.\n. So much yak shaving.\n. This type of thing should be possible once we complete #502.\n. Yeah, we pull images from the public Docker hub during the tests and sometimes this times out. Retrying the build now.\n. @drewcsillag So that's why we needed your patch. Oops.\n. @drewcsillag So actually here, we are reading back records that start with underscore from SkyDNS as part of an integration test of helios-solo:\nhttps://github.com/spotify/helios/blob/master/helios-integration-tests/src/test/java/com/spotify/helios/HeliosSoloIT.java#L167\nIs that what you would expect to not work? As far as I can tell the hidden-key feature still hasn't been removed from etcd, but this test somehow passes.\n. Great, thanks for the explanation. Found some docs on this page that corroborate that:\nhttps://coreos.com/docs/distributed-configuration/etcd-api/\nBasically underscore nodes don't show up in directory lists. I think this is fine for our uses as long as a hostname doesn't start with underscore, which would be illegal anyway. Thanks Drew!\n. :+1: \n. Actually we shouldn't need this. See #452 for discussion.\n. Not in helios-solo. The registrar for helios-solo should always just register stuff in the local domain.\nOn our machines, looking up Spotify services from helios-solo jobs would still work since we have a search domain of \"spotify.net\". But the helios-solo jobs themselves shouldn't be registered in \"spotify.net\".\n. Thanks. I noticed this in our logs too but hadn't had time to dig deeper. I'll try to look into it now.\n. Looks like the tests are failing due to DockerTimeoutException. We see this happen sometimes, I think it has to do with performance issues with either Docker daemon or the Docker Hub when we try to pull public images as part of integration tests.\nI'll try to retrigger again in a little bit.\n. :+1: from me. @davidxia?\n. You'll want to bump the version in solo/base/version.txt.\n. :+1: \nAfter you merge it:\n1. make push from the solo/base directory.\n2. Bump the version in the FROM in solo/docker.\n. Yes.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. @gimaker Yeah, I think it's the timing that's a problem. On cloudy CI boxes especially, we can fail to get into a flapping state pretty much ever.\nIf mutable public static field is so bad, maybe we should just make this and related properties configurable for real via agent arguments.\n. :+1: \n. Damn. Any thoughts from anyone on whether we fix this by just making the current behavior documented and canonical, or by breaking the old behavior but making it match up with Docker and the docs?\n. :+1: \n. :+1: but guessing this will have to be rebased after #487 is merged.\n. Yes, I think it should fail.\n. What exception are you seeing? If nginx is handling SSL termination then the Helios master shouldn't know or care.\n. Looks like this needs rebasing.\n. :+1: \n. @gimaker @rculbertson \n. @rculbertson @gimaker \n. Yes, it does.\n. @gimaker\n. We should definitely have helios status --rollout <something> so you can monitor a rollout, and possible helios rollout --abort as well.\n. Fixed by #521.\n. :+1: \n. :+1: \n. Is this PR deprecated by #473?\n. :+1: \n. Has some test failures that look like they might be related to name things?\n. :+1: \n. :+1: (assuming test failure is spurious)\n. :+1: \n. Maybe also log stuff? I would also be interested to know how or when this happens. Bad data shouldn't just get written.\n. :+1: \n. :+1: \n. Looks like the build failure is partially due to the issue addressed in #522.\n. :+1: \n. @davidxia @gimaker @rculbertson \n. @gimaker \n. @davidxia \n. @rculbertson @gimaker @davidxia\n. @davidxia @gimaker \nThis should fix all those damn build errors due to DeploymentTest.\n. @davidxia This should fix at least one of the failures we see in Jenkins.\n. Rebase on master to get the test fix from #532.\n. OK, I just rebased. Had to force push though.\n. @davidxia \n. :+1:\n. @gimaker \n. :+1: \n. :cat: sorting cat sez plz implement natural :1234: ordering kthxbye :trollface: \n. :+1: \n. :+1: \n. :+1: \n. @davidxia \n. +:100: \n. @davidxia @gimaker This is the first step to being able to record rolling update events. Still need to implement a RollingUpdateHistoryWriter that extends QueuingHistoryWriter.\n. :+1: \n. :+1: \n. Looks like at least the \"very broken in ZK\" error message has been replaced with something more useful.\n. @gtonic We actually need all those nodes to run the tests. If you only run with 4 nodes, then one of the integration tests suites doesn't run.\nWe do build PR's from forks though. For example: https://circleci.com/gh/spotify/helios/1701\nI think in this case you've setup CircleCI explicitly for your fork, so it's using your account with the free plan. If you didn't have that and instead just open a PR, then the build will run with the Spotify plan that supports the parallelism that we need.\n. :+1: \n. :+1: \n. :shipit: \n. @davidxia \n. This is sweet. Could we rip out the other Kafka stuff and use this for sending task history as well?\n. Never mind, just saw #567 :)\n:+1: \n. :+1: \n:shipit: \n. :+1: \n. :+1: \n. :+1: \n. I think they were commented out because they were broken.\n. :shipit: \n. Merging since I need this for some further cleanup.\n. @davidxia \n. :+1: \n. @davidxia \n. @davidxia @gimaker \n. @gimaker @davidxia \n. I think even with this, the deployment group will still end up failed if you stop a job after it's been deployed but before the AWAIT_RUNNING step for the host succeeds, right?\nI'm starting to like more and more the idea of just explicitly stopping the deployment group if you stop or undeploy any of its jobs.\n. Confused. Is this PR supposed to be open or is it obsoleted?\n. :+1: \n. @davidxia \n. @igorbernstein\n. :+1: \n. :+1: \n. @gimaker Want to take a look as well? I think you know Java stuff better than either of us.\n. @davidxia @gimaker \n. Basically grabbed the old implementation out of the log, renamed it, and ripped the Kafka out of it again.\n. :+1: \n. helios-services/src/main/java/com/spotify/helios/master/ZooKeeperMasterModel.java:812:12: WhitespaceAround: 'catch' is not followed by whitespace.\n. :+1: \n. :+1: \n. Looks great.\n:+1: \n. :+1: \n:fire_engine: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \nLooks good.\n. I think the assumption was that when zkRegistrar.startAsync().awaitRunning() returns, registration should be complete.\n. :+1: \n. :+1: \n. :+1: \n. :+1: other than inline comments\n. :+1: \n. @davidxia @gimaker \n. Milliseconds since epoch is also what we use for the timestamp in task status events. :+1: for consistency.\n. :+1: \n. :+1: \n. Does this replace #686 then?\n. @davidxia We were already paying for it and not using it.\n. @davidxia \n. This was a red herring. The real issue is addressed by #703.\n. So this means retries won't round-robin to different hosts? I think that's the main thing that makes the retry useful (if a host is down, try another).\nI think the fact that -z doesn't work is kind of expected. It's the same way that curl https://my-actual-machine.something.cloud.spotify.net will have an SSL error as opposed to curl https://my-nice-cname.spotify.net.\nIf we want to keep -z working, I'd prefer keeping the round-robin retry and having a flag to disable hostname verification altogether (similar to curl -k).\n. @davidxia @gimaker @mattnworb \n. Yeah, it's a mess. Was taking a look at Jersey source though and it looks like they do something very similar to patch around this limitation in HttpsURLConnection, so that makes me feel better.\n. :+1: \n. @davidxia @mattnworb @gimaker \n. Ah yup, looks like you had it right. The configuration actually changed at some point, block.on.buffer.full is what the new config is supposed to be like.\n. Fixed to use new config format. Imma merge it if there's no objections.\n. :+1: \n. @mattnworb Good point. I'll try to refactor so that if there's an issue with instantiating the AgentProxy, we still continue the request, just without client-side certs.\n. With the latest version, if there's any error with the AgentProxy or fetching identities, we just fallback to a normal HTTPS connection with no client certificate. Same with if a user is not passed to the DefaultRequestDispatcher constructor.\n. Definitely needs more tests. Working on that.\n. So this builds and it works, and there is some documentation now. Could use more tests though.\nBut also I'd like to merge it before it gets too far off master again, and I'm out of office until mid-November. Thoughts on merging?\n. > Do you think you could also add an example of the nginx config to have it pass the certificate to an arbitrary endpoint (or point to the nginx doc on that topic)?\nThe doc has a link to the nginx SSL module documentation that shows the option to turn on client certificate support. I'll add an example config for validating the cert.\n\nFor using existing x509 certificates, could the client just check for a file in a well-known location before it contacts the ssh-agent?\n\nYup, that's exactly what I was thinking. Figured we can implement that later though.\n. @mattnworb It should be fairly simple. I actually hacked it for testing at some point. The actual work is only a few lines. Probably what'll be more of a pain is all the plumbing for configuring the cert location, reading & parsing it from disk, etc.\n. I'll go ahead and merge this for now, and start work on an end-to-end integration test and certificate file support separately.\n. To be honest I am a little bit scared of shading due to experiences with it in docker-client. Kind of wonder if it's better to just force people to upgrade.\n. :+1: \n. :+1: \n. :+1: \n. Parts or all of these commits should be cherry-picked back in:\n- ece828e251262b534efb7e54a9b87272276af5d2\n- a6dcc0f46c69a5ee3e392018bca118cf775a8e41\n- 75d4107e416cbeca67805d6278198c76c4d4d9d2\n- 1c0010de7bd444224b382aaede8e86badbc2b76b\n. Opened #725 on that branch. It builds. I vote we close this and merge that.\n. :+1: \n. :+1: \n. @negz \n. Yeah, it would be sweet if the whole thing became part of the CLI so that the shell scripts can be killed. But personally I'm also OK with still having the shell scripts. Up to you.\nIf it looks like replacing all the scripts entirely is going to be a lot of work, it would be nice to have an initial version with limited functionality so that people can benefit from it sooner in their builds.\n. Fixed by #769.\n. @davidxia \n. @gimaker @davidxia \n. So what's the plan with this? Doing the cert file stuff off master is going to make it a giant pain to rebase on top of this if we're planning on merging this.\n. Ah never mind, just saw #780.\n. @staffan @mattnworb @davidxia \n. @gimaker @davidxia @mattnworb \n. @davidxia \n. It didn't actually make things faster.\n. :+1: \n. Merge conflict. womp womp.\n. @mattnworb @gimaker @davidxia \n. I'm not sure how I managed to increase code coverage with this, but I'll take it.\n. @mattnworb \n. :+1: \n. :+1: \n. @mattnworb @gimaker @davidxia @negz \n. Guess I'm just not stylish enough.\n. @mattnworb Nope, it doesn't load a keystore from disk. That would be the Java(tm) way to do it, but nobody keeps their certs and keys in Java keystores so it would be a pain in the ass.\nInstead, we create a keystore in memory and put the certificate and key from disk into that keystore. The keystore needs a password when we put the key into it, even though it's only going to exist for the lifetime of the process. So we just have some random password that we use. Most of this code is lifted from spotify/docker-client.\n. etcd would be nice, but to be honest, I just don't see how this would be possible. Unfortunately a lot of ZooKeeper concepts and primitives are woven throughout the agent and master code.\n. Yeah this makes a lot more sense.\n:+1: \n. :+1: \n. @davidxia @mattnworb \n. Awesome.\n:+1: \n. @mattnworb @gimaker @davidxia \n. This is looking good to me at this point.\n@davidxia @mattnworb Could you please review?\n@gimaker Was there anything more you wanted to add or do?\n. @gimaker I think for now let's stick with the permissions we have. If giving agents more permissions becomes necessary in the future, we can do it then. Folks will have to rerun helios-initialize-acl, but that shouldn't be too big of a deal.\n. :+1: \n. @mattnworb @davidxia \n. @gimaker @davidxia @mattnworb \n. @gimaker \n. @gimaker \n. @spotify/helios-team \n. @gimaker Nah, check out https://github.com/spotify/helios/pull/847/files#diff-65069841b102a37ed7d79b23a0b97e02R190. If the cached certificate is expired, the cache will not be used and instead it will fallback to normal mode.\nBut yeah I'll add tests.\n. Please re-review, I've taken a completely different and (I think) cleaner approach where the caching is handled entirely inside X509CertificateFactory.\n. @mattnworb Thanks, addressed your comments \u2014 added & fixed logging and the tests.\n@spotify/helios-team Going to merge this now (once the tests pass) unless anyone has any objections.\n. @dxia You mean for CertificateAndPrivateKey.from? Because that's the only logic in there.\n. For the record, not writing a test for CertificateAndPrivateKey.from since the logic of reading PEM files from disk is covered by X509CertificateFactoryTest.\n. @spotify/helios-team \n. Test failure seems unrelated. I'm merging.\n. @spotify/helios-team \n. Great idea. :+1: \n. @spotify/helios-team \n. Looks like the key blob it outputs now is actually incorrect. Looking into it.\n. @gimaker \nPreviously:\n$ helios -d ash masters -v\n16:56:08.198 INFO  X509CertificateFactory: generating an X509 certificate with key ID 80:1A:1C:76:84:65:E8:70:25:7F:A2:3E:56:C7:83:A2:32:3D:77:49\nash2-heliosmaster-a5.ash2\nash2-heliosmaster-a1.ash2\nash2-heliosmaster-a4.ash2\nash2-heliosmaster-a3.ash2\nash2-heliosmaster-a2.ash2\nNow:\n$ helios -d ash masters -v\n16:54:53.738 INFO  X509CertificateFactory: generating an X509 certificate for rohan from identity=/Users/rohan/.ssh/id_rsa\n16:54:54.474 INFO  X509CertificateFactory: generating an X509 certificate with key ID 8A:31:B5:CE:09:89:96:C2:83:AF:99:82:E7:86:78:53:D1:D3:B8:E4\nash2-heliosmaster-a5.ash2\nash2-heliosmaster-a1.ash2\nash2-heliosmaster-a4.ash2\nash2-heliosmaster-a3.ash2\nash2-heliosmaster-a2.ash2\n. My mistake, didn't include @rculbertson's commit that had tests. Included now.\n. Going to leave the builder pattern in for now as we discussed.\n. Yeah, this is valid. See https://google-styleguide.googlecode.com/svn/trunk/javaguide.html#s4.5.2-line-wrapping-indent, and also 4.6.3.\nReindentend when adding setExpires.\n. No, will revert.\n. 1. Let's move the after_deploy section to just after the before_deploy section above.\n2. Let's moe the actual steps into .travis.sh, like we have for all the other lifecycle scripts.\n. Needs to be rebased (LONG_WAIT_MINUTES is now LONG_WAIT_SECONDS).\n. MINUTES should be SECONDS.\n. Can be promoted to a private final member.\n. Can be promoted to a private final member.\n. Not necessary, the container should be started when startContainer returns. Otherwise something went wrong. We use this same pattern in other places.\n. Unfortunately only the latest tag is available. However, I'll switch to using uggedal/alpine-3.0, which should be safer than using the edge version.\n. Fixed.\n. Yes, since this class implements ContainerDecorator but doesn't need to do anything here.\n. disableAutoRegistration (capitalization)\n. Maybe use isNullOrEmpty here? Best practice is to treat null/empty strings the same.\n. Would be good to understand why?\n. Maybe move this log line back into the try.\n. final\n. Mark params final too.\n. :+1: :smiley: \n. Same reason you have the log.debug for sending an event to Kafka in that block. Just to put the log statement next to where that is actually happening, right?\n. Really nitpicking here but I don't think this import should be in the same stanza as the static imports.\n. I wonder, do we want to explicitly define path, port, and cmd as first-class members of HealthCheck? I'd lean toward having an args dictionary and have each type of health check be responsible for parsing its own args.\nThat way we keep from exploding into a giant list of possible fields over time, most of which don't apply to most health checks.\n. Nope. The ExecHealthCheck takes a cmd to exec. I guess maybe it could be command?\n. Haha. Yup. Sorry. Good catch.\n. Added of methods for this and the other health check types.\n. Yeah, I think these all need to be nullable since they could be specified as null in JSON.\n. Actually it's more like this:\n1. The container starts listening with nc on the poke port, but not the health port.\n2. We check that it hasn't been registered. It shouldn't be registered since it's not listening on the health port and should fail the health check.\nThen: \n1. We poke the container on the poke port. This is done in a polling loop since we want to keep trying to poke until the poke succeeds. It might fail if nc is really slow to start for some reason, so we try until it succeeds.\n2. The poke causes the first nc to exit, and the second nc to start listening on the health port.\nWe then check to see that the service has been registered now, since it's listening on the health port and should pass the health check.\n. Note that by the time we're running the health check, we should be pretty confident things aren't null since the JobValidator should have caught any null path or port. However, when the object is first constructed, the values could indeed be null.\n. That's interesting. I guess I'm down to merge this, but academically it would be nice to figure out why it's different.\n. Oops.\n. Done.\n. Don't even want to log this?\n. final List<String> command\n. Great idea.\n. One thing i just realized, I believe we already have the Docker version in the agent state somewhere. That's what we get from helios hosts. So we should use that instead of making a new Docker request.\n. What's wrong with isNullOrEmpty?\n. Using \"i.e.\" and \"e.g.\" in the same line. +:100: English points.\n. Right, I'm an idiot. Never mind.\n. It's more because we need the host status to filter, and right now we don't get the host status if all you ask for is JSON output.\n. Done.\n. Sweet test, dawg.\n. I think this is why the tests are failing. Doesn't the true make it so the master never registers by default?\nPart of the master registration is creating the appropriate root nodes in ZooKeeper. Not creating those nodes would explain why the SoloIT fails.\n. If the /config or /config/jobs paths haven't been created for some reason then the create ops in the transaction above will throw.\n. Broken link.\n. If we change it what will happen to existing stuff? We just clear out all the deployment groups we've created in testing?\n. Pseudo-code:\nfor (final X event : opsEvents.getEvents()) {\n  kafkaSender.send(KafkaRecord.of(event.key, event.timestamp, event.toBytes()));\n}\n. I would s/5/KAFKA_SEND_TIMEOUT/ or something similar, so it's not just a magic number in the middle of the class.\n. Fixed in #568.\n. Maybe just uber-paranoia but it becomes NPE-proof if you do statusFilter.equals( ... ) instead.\n. Instead of having this conditional, how about something like:\njava\nhosts = (!isNullOrEmpty(host)) ? singletonList(host) : client.getChildren(...);\nThat is to say, there's no need to do the ZK operation if a host is provided. We can just trust that it's an actual host for the job. If it isn't, then it will get filtered out later when we loop through the hosts list.\n. This can become final again if you decide to use the approach in the next comment.\n. Any reason to not just defer this until we have the previous states and construct the error message all in one go?\n. Can this be final since its assignment is the last statement in the try below? Not sure. Java... how does it work?\n. Maybe parameterize the function with final int maxStates or something like that and use that here instead of just hardcoding 10.\n. Eh? Should this be here? Also why is it @javax.annotation.Nullable instead of just @Nullable?\n. Maybe move the failed target count to be a param, so that if somebody calls isOverFailureThreshold in the future they aren't confused why it always counts at least one host as failed.\n. Have numTargets be an Integer and verify that it's not null.\n. Change semantics so that 0 and 100 are allowed values, with 0 meaning any failures cause the rolling update to fail.\n. If you want, you could just do:\nfinal String token = firstNonNull(deploymentGroup.getRolloutOptions.getToken(), Job.EMPTY_TOKEN);\n. Same as above, firstNonNull should work here and it's a bit easier to read.\n. Should be private (maybe)?\n. Yeah, it is really tied to the implementation right now. If anything changes, either the verification or the mock will break. Perhaps just verifying that something happened is OK.\nAnother approach would be to use an org.apache.curator.test.TestingServer to actually bring up an in-memory ZK, write some nodes into it, and then ensure the nodes are gone after running deregisterHost. Though not sure that this is the right place to do that kind of test (maybe?).\n. Also add something to clear this out in withoutMetaParameters below. That keeps it from affecting the hash.\n. Talked about this on Friday, but it doesn't make sense for this to be here. It's tied too closely with crtauth. It should go into the helios-crtauth module. Either that or we should just make crtauth the only supported authentication mechanism and dump the whole modularity.\n. Probably should add:\nSystem.setProperty(\"sun.net.http.allowRestrictedHeaders\", \"true\");\nconnection.setRequestProperty(\"Host\", hostname);\nFor the case where Helios is behind nginx or some other proxy that serves multiple hostnames on the same IP.\nAnother way to approach this rather than creating an IP URI, etc., is to just set the request proxy to the IP:\nProxy proxy = new Proxy(Proxy.Type.HTTP, new InetSocketAddress(\"10.0.0.1\", 8080));\nconn = new URL(urlString).openConnection(proxy);\nThat might actually simplify the code quite a bit, though I'm not sure if it would work with HTTPS.\n. Proxy.Type.HTTP includes both HTTP and HTTPS proxies. The master endpoint will work fine as a \"proxy\" for HTTP, but not sure if it will forward HTTPS correctly. The nice thing about doing it this way would be that you could probably dump the hostname verification stuff.\nBut it might also not work at all. Up to you if you want to investigate or just punt on it.\nI think long-term the best solution is to use the Apache HTTP client that lets you override DNS resolution and have  explicitly resolve to the single IP that we've selected.\n. I kind of feel that this should actually go in HeliosClient. I mean, there's nothing really CLI-specific about it, is there? That way existing HeliosClient usages would continue to work without having to worry about doing setAuthProviderFactory. Thoughts?\n. I wonder, should we make it so the client just gives up and throws an exception if it's asked to authenticate over a plain HTTP connection?\n. Meh. We could just be like, all auth params must be env vars.\n. Good point.\n. Yeah, I guess not. That's just the default behavior of ssh-agent behavior, all requests are SHA1-hashed and signed. Maybe we could change the method name though. @davidxia \n. Good point.\n. Touch\u00e9.\n. Deques are the shit.\n. Yeah that's on purpose, to see if the override on line 85 is successful.\n. The members are static so that they can be accessed from with JobWithConfigTestImpl. The setup is not @BeforeClass because if it is, the mocks aren't ready at that point. In practice though the setup only runs once since there is only one test() method that runs all the tests in the inner class.\n. Should be final.\n. Unfortunately no since the creation of the DigestCalculator can throw an exception, and that doesn't play nice with static constructors.\n. I stuck to these since it's how DOCKER_CERT_PATH works.\n. Ha. Damn IntelliJ smart refactoring.\n. Nope, just a run-away IntelliJ refactor.\n. isNullOrEmpty ?\n. isNullOrEmpty here and for masterDigest as well.\n. I'd replace this with org.apache.zookeeper.server.auth.DigestAuthenticationProvider.generateDigest().\n. isNullOrEmpty\n. Same here on using DigestAuthenticationProvider.generateDigest instead of doing it ourselves.\n. I went with Initializer because then the bin script gets them name helios-initialize-acl. If we go with migrate, then the script ends up being helios-migrate-acl or helios-acl-migrate or something which I think is less clear.\nBut, I'm also kind of indifferent. What do you think?\n. Why? We'd still have to check if the optional is null and then check if it contains a value. Right now we just check if the object is null.\n. You can't split on chars, only on regular expressions.\n. It really isn't. I just always do split(quote(\"string\")) out of an abundance of caution so that nobody comes back and tries to edit the string without realizing that it's a regex.\n. Very clever.\n. Heh. I actually didn't know about that syntax.\n. Right, it should be a fixed delay, not a fixed rate. Though also I'm now realizing that this will retry attaching a bunch if a given container exits.\n. This is Spotify-internal, wouldn't put this in the OSS docs. (GitHub account instructions)\n. Use backticks () instead of double-quotes (\") for commands, filenames, etc., to get the proper Markdown formatting.\n. Probably. Let's discuss on an internal channel.\n. Why is this needed? If you want to store a type of time unit with a value,Durationis probably a better choice.\n. Should probably be!HostStatus.Status.Down.equals(...). It just replicates what was in syslog-redirector which was already open-sourced with this hard-coded.\n. Everywhere in Helios uses the Guava class. I think because we started at Java 7?\n. I'd rather not use that, it has stuff to deal withSNAPSHOTand similar. It's very definitely for POM versions.\n. So then we have a mix of both optional types throughout the code? Don't think that's a good idea. If we want to switch tojava.util.Optionaleverywhere, that should be separate.\n. Are you sure this logic wasn't correct before? Wouldn't this now throw an NPE ifsource` is null?. ",
    "mrfuxi": "If Vagrant is your go to tool in this case, what about having Docker as provisioner ;)\n. ",
    "aidanhs": "Docker has restart policies (now), which would keep it running.\nMy question would be how you'd manage upgrades of helios (as entertaining as it'd be, I'm not sure it makes sense to have the helios agent managing the helios agent - you probably don't want to ever be able to undeploy, for example!). \nIn theory it should be as simple as persisting the agent state directory and running an ssh command against a node to take it down and back up. I'll have to try it and see.\n. Quite interested in a UI.\n@davidxia is yours still available somewhere? I'm a fan of react myself but listing images from a registry sounds pretty good for what I'm thinking of.\n. ",
    "drewcsillag": "\nFirst, I think that I hate the  create/createForDomain division.  Domain is a nameless-specific hack, and there's no way to generalize it in a way that will work cleanly for the non-Spotify case.  There should be one create() method, with a connect string that the plugin can use to decide what to do.  In the nameless case the connect strings could be domain://foo.com or tcp://blahboaloueo:234 or even srv://rnoneth.spotify.com.  In either case, I agree, that the format of the string should be opaque to helios -- unless we go the json route as I mentioned in the email I sent you (before I saw this).\nplugin config by env vars --> +1\n\nFor people that aren't @danielnorberg (email content follows):\nI was thinking, if we want to generify things, lets go all the way on this:\n- make it possible to have multiple plugins registered (the first in the list is the default?).  This way, we won't necessarily need specific agents for our CI use case.\n- like you argued, pass all relevant registration bits through the json-serialized ServiceRegistration object through the Registrar interface.  I think your last point thoroughly obviates this idea, but included since it was in the original email -- the json encoding part anyway\n- make registrar configuration either by json string on commandline, or possibly by json/yaml config file.  If we go with the command-line option, I'm thinking that you'd specify the --service-registrar-plugin multiple times and it's --service-registrar-config options multiple times as well.  Or specify all the registrars and their config by a single yaml/json config file.  Added:--> Or make it opaque to helios altogether.\n- This would allow us to even have the same plugin registered twice if we want/need to with totally different configs, if the config allows us to specifically assign names to each instance of the plugin -- by default, each plugin type would have a default name.\nHere's a question, instead of specifying the same plugin twice (if the use case is reasonable), have the plugin name cause all the relevant config to be included in the ServiceRegistration?  That feels like it opens things up to errors more than just having the plugin itself do the name->config args bit.\nedit: formatting\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. On Mon 30 Jun 2014 11:07:52 AM EDT, Daniel Norberg wrote:\n\nDid you push the changes?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/49#issuecomment-47543175.\n\nNot yet.\n. Changes pushed.\n. re: nits.  No worries.  Good catches all around.\n. +1\n. Whitespace trimming fixed.  I had thought (prior to #57) that 6 might be wrong.\n. +1\n. +1\n. If you could leave off the change to JobValidatorTest (I've got the fix in my branch already here, so I'd prefer to not have to deal with the conflict, that'd be great.\n. +1\n. :+1: \n. :+1: \n. :+1: \n. @rculbertson @philipcristiano Tests added.  As far as CLI parsing, it happens after this anyhow, so this can't override the relevant CLI optinos, but I did test that a) the basic bits work, and that if you have a config file, it overrides as appropriate.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. Make the one aforementioned change and squash the commits first, but otherwise\n:+1:\ni.e. feel free to merge afterwards.\n. Contrary to what it says above, build 460 did in fact succeed.\nhttps://travis-ci.org/spotify/helios/builds/30844461\n. :+1: \n. :+1: \n. :+1: \n. Yay! got to be PR100! W00t!\n. :+1: assuming travis is content with it :)\n. I was explicit about the dest tag just because I've been bitten on pushing branches without an explicit destination branch (e.g. if the upstream of the branch is set to origin/master, rather than the desired branch), and didn't want the same to be possible with tags -- it may not be since tags don't have an upstream, but...  paranoia.\n. :+1: \n. Fix push release to use master for the tag push rather than release\n. :+1: \n. :+1: \n. Also, did you check that if no --dns arg is specified that container's dns is just whatever is on the agent machine?  That is, not specifying --dns maintains current behavior.\n. :+1: \n. :+1: \n. In #195, you'll now get the message\nJob hash was not specified in job id [busybox:null].\nJob version was not specified in job id [busybox:null].\nWhich hopefully will be clearer about what whent wrong.\n. @op Do you have a full stack trace by any chance, or is that all you got back?\n. Also, if you happen to have a repro script, that'd be super-handy.\nThanks.\n. PR #140 should fix when submitted (shortly).\n. :+1: \n. I'm wondering if the initial pulling of the busybox image is taking too long (or failing altogether).  You might try doing a docker pull busybox and rerun the test as a way to validate this.\n. Ok, that one test failed indicates that things are basically, working.  The FlappingTest, in particular has been flaky for a while, so since that's the only one that failed, I'd say things are actually working ok.  If you want, you can try rerunning the tests, or just rerunning FlappingTest by running:\nmvn test -Dtest=FlappingTest -DfailIfNoTests=false\nBut again, since that's the only test that failed, it sounds like things are working.\n. You still need the DOCKER_HOST=tcp://192.168.33.10:2375  prefix bit in front of the maven command.  If you put \nexport DOCKER_HOST=tcp://192.168.33.10:2375\ninto your ~/.bashrc, it will always be set, so you don't have to remember.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. Yes, please add a JobCreateCommandTest.  If you can add an explicit test for the file-with-arguments case, as well as various forms of bogus create commands, that would be good.  If you want to, perhaps even adding tests for the job create REST API to make sure that we catch things there too.  I think the REST API makes an explicit exception to not have to supply the hash (as a client shouldn't be expected to be able to produce it), which should be maintained.\n. Fix the bit in the test re: the agent, and then :+1: \n. The reaping failure definitely looks to be a docker connectivity issue -- my guess is that if you deployed a job, it would go nowhere.  Unstated in the current docs -- or at least I can't find it (which I'll fix today) is that the DOCKER_HOST environment variable must be set to the docker host/port -- or it seems like there ought to be a command line switch as to make this discoverable (something else to look into doing).  So you might want to start docker like this:\ndocker -d -H 127.0.0.1:2375\nThen start your agent like so:\nexport DOCKER_HOST=tcp://127.0.0.1:2375\nbin/helios-agent --zk x.x.x.x:2181\nThen start your agent as before.\nThanks for reporting this!\n. Ahhh, I didn't see it in our own docs (a line break issue as it turns out).  So you can do\nbin/helios-agent --zk x.x.x.x:2181 --docker tcp://127.0.0.1:2375\n. And one last brain fart:  --docker defaults to tcp://localhost:2375 anyway, so if you start docker as above, the agent should \"just work\" (TM)\n. Updated the docs in #145 and #142 to hopefully make this less confusing.\n. I would probably do this as a system test instead of a unit test.  Look under helios-system-tests for some examples.\n. :+1: \n. :+1: \n. Helios currently doesn't support linked containers.  We've thought of some way as to how this would work, but it's not likely to be soon.\n. Since the docker client is a mock, you should be able to make it do whatever you want.  I agree with your assessment: SupervisorTest does seem to be the proper place to put it, and verifySSASDC seems to be a good base to start from.\n. Fix the item mentioned above, and it's :+1: \n. +:100: !!!\n. :+1:  I think multidomains are super helpful too.\n. :+1: \n. :+1: \n. :+1: \n. TaskStatuses include deployments from whatever machine the task happens to live on.  Should we be filtering out deployments that aren't on the specified hosts?\nI have my branch drewc/json-helios-status here #194 which doesn't do this, but can be made to.\nEdit: added pr#\n. :+1: \n. Done.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \nAlso, try to find a way to at least test this locally before merging :)\n. Not thrilled with the TargetAndClient class name if someone has any better suggestions.\n. :+1: \n. That sounds like a very reasonable request.  Let me talk with the team to get it hashed out.\n. @davidxia Acutally #181, but yes.. it is :)  Closing.\n@mudasirmirza, there's a new switch for the master and agent called --zk-namespace which essentially specifies the path prefix (no leading or trailing slash).\nThough I just noticed, I didn't update the docs to include this.  Fix coming shortly.\n. Per request from Marc Bruggman\n. This is to address the case of issue #174\n. @danielnorberg \nGave namespaces a swing, and spend a few hours trying to make it work (can push the branch if you want to look to see if I'm doing something obviously stupid), and all I can see is that namespaces must be at least partially broken.  Some things wind up in /, some things wind up under the namespaced thing, and I get a few hangs, etc.  It's like the MasterZooKeeperRegistrar (from what it looks like) somehow ignores the namespace.  I also get a long startup pause and then this:\njava.lang.IllegalStateException: Client is not started\n    at com.google.common.base.Preconditions.checkState(Preconditions.java:176) ~[helios-services-0.8.10-SNAPSHOT-shaded.jar:0.8.10-SNAPSHOT]\n    at org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:113) ~[helios-services-0.8.10-SNAPSHOT-shaded.jar:0.8.10-SNAPSHOT]\n    at org.apache.curator.utils.EnsurePath$InitialHelper$1.call(EnsurePath.java:148) ~[helios-services-0.8.10-SNAPSHOT-shaded.jar:0.8.10-SNAPSHOT]\n    at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[helios-services-0.8.10-SNAPSHOT-shaded.jar:0.8.10-SNAPSHOT]\n    at org.apache.curator.utils.EnsurePath$InitialHelper.ensure(EnsurePath.java:140) ~[helios-services-0.8.10-SNAPSHOT-shaded.jar:0.8.10-SNAPSHOT]\n    at org.apache.curator.utils.EnsurePath.ensure(EnsurePath.java:99) ~[helios-services-0.8.10-SNAPSHOT-shaded.jar:0.8.10-SNAPSHOT]\n    at org.apache.curator.framework.imps.NamespaceImpl.fixForNamespace(NamespaceImpl.java:74) [helios-services-0.8.10-SNAPSHOT-shaded.jar:0.8.10-SNAPSHOT]\n    at org.apache.curator.framework.imps.NamespaceImpl.newNamespaceAwareEnsurePath(NamespaceImpl.java:87) [helios-services-0.8.10-SNAPSHOT-shaded.jar:0.8.10-SNAPSHOT]\n    at org.apache.curator.framework.imps.CuratorFrameworkImpl.newNamespaceAwareEnsurePath(CuratorFrameworkImpl.java:468) [helios-services-0.8.10-SNAPSHOT-shaded.jar:0.8.10-SNAPSHOT]\n    at org.apache.curator.framework.recipes.cache.PathChildrenCache.<init>(PathChildrenCache.java:223) [helios-services-0.8.10-SNAPSHOT-shaded.jar:0.8.10-SNAPSHOT]\n    at org.apache.curator.framework.recipes.cache.PathChildrenCache.<init>(PathChildrenCache.java:206) [helios-services-0.8.10-SNAPSHOT-shaded.jar:0.8.10-SNAPSHOT]\n    at com.spotify.helios.servicescommon.coordination.ZooKeeperHealthChecker.<init>(ZooKeeperHealthChecker.java:55) [helios-services-0.8.10-SNAPSHOT-shaded.jar:0.8.10-SNAPSHOT]\n    at com.spotify.helios.master.MasterService.<init>(MasterService.java:130) [helios-services-0.8.10-SNAPSHOT-shaded.jar:0.8.10-SNAPSHOT]\n    at com.spotify.helios.master.MasterMain.startUp(MasterMain.java:75) [helios-services-0.8.10-SNAPSHOT-shaded.jar:0.8.10-SNAPSHOT]\n    at com.google.common.util.concurrent.AbstractIdleService$2$1.run(AbstractIdleService.java:54) [helios-services-0.8.10-SNAPSHOT-shaded.jar:0.8.10-SNAPSHOT]\n    at com.google.common.util.concurrent.Callables$3.run(Callables.java:95) [helios-services-0.8.10-SNAPSHOT-shaded.jar:0.8.10-SNAPSHOT]\n    at java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]\nGiven that this branch is only explicitly doing what namespacing should have, it shouldn't be anything subtle on our end going on w/r/t the underlying zookeeper paths.\n. Branch is drewc/zkpp.  The interesting bit about the client not started thing is that it's very deterministic (it happens when doing namespacing every time and doesn't when you don't).  Also, when checking zk with zkCli after starting the master, it looks like the MasterZooKeeperRegistrar is creating the paths in the root path instead of the namespaced path.  But status info is being written into the namespaced area.  Also, the fact that a method named curator.newNamespaceAwareEnsurePath exists gives me reason to be wary that the namespacing might not be fully baked though....\n. @danielnorberg ping re: namespacing\n. abandoning in favor of #181 \n. :+1: \n. This is the telling line:\n01:49:13.337 helios[25032]: ERROR [Reactor(zk-client-async-init)] AgentZooKeeperRegistrar: Another agent already registered as 'buildbot.mysite.com.br' \nI'm assuming the stack-trace is from the agent log.  It looks like one of a few things has happened:\n- you tried to run two agents simultaneously\n- you ran an agent, wiped it's state directory, and subsequently started the agent without deregistering it (use the helios deregister command to fix that)\n- something I've not thought of.\nIf it's the first or second cases, usually what will fix it is to stop the agent, run helios -z tcp://localhost:5801 deregister buildbot.mysite.com.br and restart the agent.  If the agent still dies with the same exception, or the deregistration fails (we have an open bug about a particular case you might run into), then you can try the next way.  Stop the master, agent and zookeeper.  Wipe the zookeeper data directory (assuming you're not sharing it with another app) which usually lives in /var/lib/zookeeper.  Then restart zookeeper, the master and the agent (in that order).  You'll have to recreate your job, but after that, it should work.\n. I can :+1: part :)\n. Not obvious, and perhaps we need to do more than put this in the docs (which I think it is, but I'm not sure)...  Before the command line for the container put --.  This tells the helios executable to stop looking for command line switches.  Actually, this applies to most programs, not just helios :)\nSo your command would be:\nhelios create JOB_ID IMAGE_ID -- CMD1 -vv\nedit: I accidentally a -vv\n. :+1: \n. :+1: \n. Agreed.  Just because I'm curious, I assume that the newly deployed job will fail repeatedly until the one in grace goes away?\nMaybe we need to rethink some of the way this works a bit.  Like as we have an expiration time on the job, an undeploy on a grace-period job would set an undeploy expiration time for when it should be undeployed or something.  This way, if you undeploy it again, we can decide that it's a forcible undeploy, and it should maintain proper job state and such....  Just thinking aloud.\n. It looks like we're not really reading from stdin properly, fix in progress.\n. Fixed by #198\n. Cool to hear about the plugin!  Let us know when you think it's ready and we can link to it (assuming it will also be open-source).\nYou are correct: the shutDown() method of the AgentService is just never called.  In AgentMain, we just call awaitTerminated and System.exit(). There doesn't appear to be anything that would cause shutDown() to be called.  This is a bug.\nThat all said, I would caution you against deregistering services in shutDown() -- or at least that doing so may be a bad idea.  The reason being that if you want to upgrade the agent, or if the agent has a bug and restarts, the services running underneath them will normally still be running happily.  While, depending on the ttl you set in consul (I assume it has such a thing) they will eventually be expired out anyway, consider if you really want a reduction in service availability if the agent is bounced -- e.g. if you're upgrading a bunch of machines, you'd have to be careful to stagger it so that you don't accidentally reduce service availibility to zero, even though the system as a whole is actually capable of responding to requests.\n. :+1: \n. Definitely plan to in the next week or two.  I only recently discovered that we hadn't. I thought we already had. \n. Open sourced at https://github.com/spotify/syslog-redirector\n. Should fix #199 \n. Still reevaluating this change, but fortunately according to here, signal handling is supported in openjdk7, so there's at least one opensource implementation of signal handling in the same sun.misc package.\n. :+1: \n. Is this supposed to be reviewed separate from #208 as #208 is a superset of this?\n. :+1:  Please squash the commits and I will merge.\n. :+1: \n. Nice catch!\n. Oops!  Thanks for picking up on this!\n:+1: will merge once CI says all is good.\n. :+1: I'll push the fix to my previous comment.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. Given the number of problems we've had in the past when setting hostnames in containers, and the miniscule, if any, value in setting it, I'm in favor of just not setting it at all anymore.\n. Previous to the most recent case, containers would fail when calling gethostname inside the container (which they reasonably expect to never fail).  Most recently, I think it's docker 1.3 that doesn't like hostnames this long.\n. :+1: \n. :+1: \n. :+1: \n. This should be fixed (and experimentally seems to hold for the downed machines we had) as of 0.8.18.  Removal of the undeploy tombstones fixed this.\n. To clarify, if you deregister a host, I believe it should cause the jobs (if the agent were still running) to be undeployed.  Either way, the reactions of a deregistered are kinda moot anyway -- assuming it doesn't deploy more containers or random containers.\n. :+1: \n. I'm happy with the design.  Please add appropriate bits to JobTest -- it's admittedly an annoying test, but it's picked up a number of subtle bugs in the past.\n. Looks good!  Just need to rebase onto master and squash your commits (not necessarily in that order :), and I think it should be good to go.\n. :+1: \n. rebased.\n. :+1: \n. @rohansingh Similar, but different.  This one is job status, the other is host status.\n. Take a look at the contents of /etc/init.d/helios-agent and master.  They should look like shell scripts and similar to other files in /etc/init.d.  The Exec format error message indicates that something may have gotten corrupted somewhere.\n. :+1: \n. on host2 you should have a /etc/default/helios-agent something like this:\n```\nENABLED=true\nHELIOS_AGENT_OPTS=\"--syslog --zk host1:2181 \\\n    --docker=127.0.0.1:2375 \\\n    --state-dir /var/lib/helios-agent \\\n    --http=http://127.0.0.1:5803\"\n```\nFor host 1, you should only include host2's address to the --zk argument if zookeeper is running there.  On host 2 you should only need docker, helios-services, helios-agent.  You might want the helios-tools installed there too though.  You can install zookeeper on host2, but you don't need to.\nAlso, you don't want to have spaces around the = signs.\n. Ok, it definitely appears that your first host is connecting just fine.  Can you post the logs of the second host in a gist somewhere so I can see what, if anything, it is complaining about?\n. Will be in the next helios release.\n. :+1: \n. :+1: \n. If your container already logs to syslog, you could configure it to do this.  As far as Helios itself, while we don't provide the ability to do this currently, the SyslogRedirectingContainerDecorator could probably configured in such a way to make syslog redirector (possibly with additional configuration) do this -- the app name definitely could be done this way with no changes to syslog-redirector, at least.  Tag (I assume you mean LOCAL_0, etc) would require changes to syslog-redirector.\nFWIW, currently, with syslog-redirector, it uses the job name as APP_NAME.\n. Also, btw there's the --syslog-redirect-to which if you have syslog-redirector installed, will activate the aforementioned SyslogRedirectingContainerDecorator for all containers a given agent starts.\n. :+1: \n. :+1: \n. Also, I tried a few different values of revision, to no avail.\n. This last bit seems pretty odd:\nFailed to read artifact descriptor for com.spotify:helios-client:jar:0.8.0-SNAPSHOT: Could not transfer artifact com.spotify:helios-parent:pom:0.8.${revision} from/to clojars.org (http://clojars.org/repo): Illegal character in path at index 55: http://clojars.org/repo/com/spotify/helios-parent/0.8.${revision}/helios-parent-0.8.${revision}.pom -> [Help 1]\nWhy is it trying to do anything with 0.8.0-SNAPSHOT?\n. Flushed my ~/.m2/repository directory.  I must have had some crud in there during the window where this was actually busted.\n. :+1: \n. Why not just do a dns lookup on the agent machine names return by helios hosts?  This implies that they are registering with their host names (as opposed to something else).\n. Hold off.  Got a few weird exceptions on subsequent test runs.  Looks like for things that don't return\nvalues, we may be able to shutdown the client before the request finished.  Cool.\n. :+1: \n. :+1: \n. Nice work!\n. :+1: \n. Modulo functional vs. non-functional bits (java 8 with lambdas may change the value equation), :+1: \n. :+1: \n. :+1: \n. Consider keeping the old, and emitting a warning about it's deprecation.  We can then set a date at which we'll get rid of it.\n. :+1: \n. Cool stuff! :D\nSent from my iPhone\n\nOn Feb 17, 2015, at 5:14 PM, Andrey Sibiryov notifications@github.com wrote:\n@rculbertson @rohansingh @davidxia @danielnorberg\n\u2014\nReply to this email directly or view it on GitHub.\n. A little late to the party, but does newer mainline etcd still treat entries that start with underscore as hidden?\n. What I recall is that if you looked up the thing with the underscore, if the discovery record was written as a subtree item of the underscore-prefixed entry, you wouldn't see it, but if it was written directly at the underscore name, it was ok.  I don't recall/know what solo is doing when it puts in it's records into etcd.  Hopefully this makes sense to someone not in my head.\n. Love when we can delete code!\n. Yup. There's a getPrefixPattern in a parent class or interface.\n. Yup. There's a getPrefixPattern in a parent class or interface, and the version here doesn't override.\n. Nope.  Since it's a private method within SyslogAppender.\n. done\n. done\n. Shouldn't this be: spotify-%s._http.services.%s ?\n. Also you might want to extract the string as it's own constant, like SPOTIFY_SRV_FORMAT since probably no one else would understand why this was the default.  Arguably, there should be no default, or it should be something fairly generic, like %s.%s\n. I've seen at other places some schemes where protocol is not part of the srv name, and there are no 's at all, so the srv name would be something like service.datacenter.prod.foo.com, i.e. the SRV_FORMAT would be just %s.%s\n. This looks similar to what you did in PR43.  Overlapping PRs? \n. @danielnorberg I was unaware (and I'm guessing other Co.'s are too) of the RFC.  Objection withdrawn.\n. rename to shortenHostName\n. Actually, in light of the below comment, changed to Optional.fromNullable(registrationDomain).or(\"\");\n. Done\n. Done\n. Done\n. Done\n. done\n. Really not a fan of using nulls when String already has a good value meaning empty.\n. If we insist that registrationDomain has a non-null value, no need for fancy isEmptyOrNull.\n. done\n. done\n. since registrationDomain is ensured to be not-null .equals is appropriate -- but isNullOrEmpty(\"\") here was clearly wrong (refactoring frijoles)\n. Can't create Names with empty strings.\n. Done\nI need to figure out how to make eclipse not do that.... \n. first part done.\n\nWould love to use dnsjava to do the parsing, alas it doesn't expose it in a usable way.\n. You should update JobTest for the new field.\n. final?\n. consider an import of Map.Entry to allow you to elide 'Map.'\n. these should be indented another 2 spaces.  Here and below.\n. indent another 2 spaces.\n. ditto, 2 spaces, here and below\n. consider creating an EMPTY_EXPIRY constant.\n. +2 spaces\n. ditto +2\n. or consider putting this on one line (or only split across 2 if it won't fit on 1).\n. can this just be exit (that is, without the 1 as suffix)?\n. If you make the EMPTY_EXPIRES, you could use it here.\n. Indent here is off\n. consider just having two constructors in lieu of the builder.\n. did you mean to reindent this?\n. ditto reindentation\n. EMPTY_EXPIRES?\n. done\n. done\n. done\n. done\n. I forgot about twr.  Cleaned this up nicely.  Thanks!\n. done\n. On Thu 03 Jul 2014 11:17:25 AM EDT, Andrei Sfat wrote:\n\nIn\nhelios-services/src/test/java/com/spotify/helios/servicescommon/ServiceParserTest.java:\n\n@@ -0,0 +1,29 @@\n+package com.spotify.helios.servicescommon;\n+\n+import com.google.common.base.Charsets;\n+\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.nio.ByteBuffer;\n+\n+import static org.junit.Assert.*;\n\nyo, @drewcsillag https://github.com/drewcsillag, check this out:\nhttp://blog.xebia.com/wp-content/uploads/2008/10/organize_static_imports.gif\nSeems to be as same as in IntelliJ (as I use IntelliJ and hate\nwildcards). You need to specify a silly huge number to avoid getting\nyour imports wrapped in wildcards. For instance, I specify a number\nlike 2000 for each type (non-static and static).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/49/files#r14519173.\n\nThat wasn't the problem (mine was set to 99).  It's that at some point, \nI evidently turned off organizing imports on save.\n. Not for this PR, but when doing my attempt, I kept having to trace what each of the Strings was.  That is: Map<String, String> doesn't tell me what I need to know. Going forward, we shouldn't use Strings for things like this.  But then again, maybe I've been reading too much on Haskell...\nedit: less than and greater than don't work correctly in comments\n. Here and below, since there are no bodies, you can just use {} instead of {\\n\\n}\n. Either uncomment the line, or delete it.  Also feels like too many blank lines in this method.\n. statically import fail\n. final.  Here and assignments below.\n. static?\n. inline into line 138 (where it's .thenReturn(czkClient))?  Or at least move this line to the line above that line.\n. final\n. final\n. I know this isn't your fault, but since you're here, please make this final.\n. final\n. Consider just making a mock for the ZKTM instead of an inner class.\n. statically import assertTrue\n. final ( to match the other params)\n. Should return Builder like the other set* methods.\n. should the future be cancelled if verifyJobsHealthy throws?\n. So as to make the logic a bit clearer to read, just return errors after this, as there is nothing after the else.\nAlso, you may want to take a look at: https://drew.thecsillags.com/refactoring-flow-control-and-conditionals/ for general approaches to making flow control stuff like this a bit easier to visually parse.\n. consider extracting a validateJobName method for this.\n. consider extracting a validateJobVersion method\n. consider extracting a validateJobHash method\n. Done.\n. You could serialize it via Json.asString possibly?  That means the key is a json string, which is a bit weird, but it's then at least machine readable....  \nAnother idea: the structure could be something like this: \n{ \"shared.cloud\": \n  {\n   \"endpoints\": [ list of endpoints ]\n   \"srv\": \"helios\"\n   ...other details?...\n   \"jobs\" : { \n       \"ad-event-tracker:0.0.8-SNAPSHOT-637574b:91b946c81bbb4aeafe7f243e39b6d63e4110d124\":{\n         \"command\":[\n            \"server\",\n            \"ad-event-tracker.yaml\"\n         ],\n         \"env\":{\n            ...\n         }\n    }\n}\n. Can we just call getClass(), so when this gets invariably copied later, that it will do the right thing?  Here and the others also.\n. Sorry if I'm being picky, but you can just call getClass() (without 'this.')\n. Since unregister() already does the isPresent() check, I'd just unconditionally call it here.\n. I think you can probably change the return type to List<? extends Target>\n. Also, there's stuff in puppet to set the HELIOS_MASTER env var to site:// blah on the machines in the cluster itself, so after this goes in, do the appropriate puppet change to change the env var to domain:// blah.\n. Hmmm.  You sure you want UNKNOWN here?  IIRC, UNKNOWN is what should be there if the job has been deployed but the agent has yet to act on it.  Though it may be that by the time verifyHealthy is called that if the agent hasn't acted, it's probably time to call things off.\n. Hmmm.  IDK...  I'm leaning towards keeping application.conf, because if the user is using typesafe, loading config from a different place might muck things up... \n. Yes.  Added another sentence to clarify.  This sentence mostly was about the profile, but you raise a good poing anyway.\n. very much so :)\n. yup... blasted leftover debug code :)\n. Good points, both.  Fixed.\n. Yes.  Basically java insists that if you call this(whatever) in a constructor, that it's the first thing in the method.  Thus, you can't call some other (static) method to compute anything and assign it to a variable.  So you have to build it all up functionally.  That's what I was griping about.  I'll make the comment clearer.\n. Thanks, I always forget that exists\n. correct! fixed\n. It's a leftover from the refactoring the HostDisplayer out.  It now doesn't need to be plumbed through other stuff to get to displayTask().\n. final\n. An evident lack of observation skills :)\n. Indentation here is 4, should be 2.\n. Extracting this to a method that looks like what follows makes the code a bit cleaner and clarifies the intent.\n``` java\nprivate List getChildren() {\n  final Stat childrenStat = new Stat();\n  while (true) {\n    final List possibleChildren = curator\n        .getChildren()\n        .storingStatIn(childrenStat)\n        .usingWatcher(childrenWatcher)\n        .forPath(path);\nif (clusterId == null) {\n  return possibleChildren;\n}\n\n// check that cluster node is right\ntry {\n  curator.inTransaction()\n      .check().forPath(String.format(\"/config/id/%s\", clusterId)).and()\n      .check().withVersion(childrenStat.getVersion()).forPath(path).and()\n      .commit();\n} catch (KeeperException.BadVersionException e) {\n  // Jobs have somehow changed while we were creating the transaction, retry.\n\n  // is some form of sleep or backoff appropriate here?\n  continue;\n}\nreturn possibleChildren;\n\n}\n}\n```\nI'm guessing that @Kobolog wrote this.  Presuming so, you might want to read this on ways to make the flow control bits a bit clearer.  This way you can get rid of state flags (here children==null) and reduce some redundant code.\n. Sorry, my bad.  I made a few leaps here and used rationale that I totally didn't explain at all.  Let me fix that.  Apologies for the wall o' text.\n- I suggested the implementation as a separate method since the code went from one line to about twenty in a function that was only about twenty to start with.  So now a function named sync() is now spending about half it's code loading rather than doing what it's name implies.\n  - This way, in the cases where the reader doesn't care about how the children are obtained and only cares about the syncing operation itself can now ignore how children are gotten. \n  -  It documents intent better.\n  - Contrary to just about every other language I know, function calls in java are cheap, so runtime overhead is negligible, and in some cases the JVM can even make methods extracted this way faster than the inline case.  FWIW, in Java, it's a common enough operation that IDE's have hotkeys to do method extraction, as well as \"jump to function\" in the case they do care to know.\n  - Decomposition into methods allows you to minimize what the reader needs to keep in their heads.  They can understand an algorithm in it's parts.  Specifically, it makes explicit to the reader the effective lifetimes of variables.  It can expose more clearly the structure of conditional logic with respect to the parts.  It can also assist the compiler in doing dataflow and escape analysis, since dependencies between functions are made explicit.  In this case, if they only care about how we deal with the handling of the id node, they don't have to learn about how syncing works.\n  - It can aid in testing as you can now test these smaller functions independently.\n- Extraction as a method allows us to conveniently bail out of the loop via return, making the condition of the while altogether unnecessary.  This allows for a simpler top-down reading of the code, since the reader doesn't have to keep the loop condition in their head -- especially since looping a second time is the exceptional case, rather than the norm.  Nor do you have to set up the condition to be true on the first go around, and subsequently false to bail out.\n- Use of final: unlike the properties of const-correctness in C/C++, in Java, we only have final which guarantees that the variable won't be reassigned after initialization.  Generally, if it's fairly trivial to be able to make variables final, it's preferable for readability -- this way, when you see something that isn't final, it's a strong signal to the reader that they need to pay closer attention to how it's used.  It also can make the compiler/jvm able to perform some optimizations.\nAs for code bloat, even though I spaced things out a bit more (including breaking of lines in the assignment of possibleChildren), unless I miscounted, it's a line or two shorter; depending on how you count.  The suggested code also has only three assigments and one conditional, yours has five and two respectively.\nAs for \"Function is a minimal unit of reusable code\": In monolithic functions that do more than one \"thing\", the lifetime of the variables can be muddled; for example, is a variable defined at the top of the function only used near the top, or is it used throughout the whole thing?  You have to read the whole function to know.  If there's significant amounts of conditional logic with long then/else bodies, readability suffers quite a bit too, especially when you see a series of close braces -- which block did these close?  How far up do I need to scroll to find out?  Oh crap, what line was I on again?...  When functions like this are decomposed, most of these problems vanish.  Decomposed functions tend to evolve nicely over time too, whereby the monolithic functions tend to get more and more complex over time.\nIn short: The easiest code to read is the code you, or future-you, do not have to read.\nI think the book Effective Java 2nd Edition has something to say on this kind of thing, but I may be wrong.  Either way, EJ2E is definitely worth a read just anyway.\nLastly: in the code I suggested, I goofed.  I should have, instead of writing String.format(\"/config/id/%s\", clusterId), it should instead have written Paths.configId(clusterId).\n. consider putting this in the finally block, before unregister()\n. As TaskRunner implements AbstractExecutionThreadService, this probably should be renamed to shutDown() to override (since stopAsync().awaitTerminated() used to be called on it)?  Unless it's extension of InterruptingExecutionThreadService can be gotten rid of -- I don't recall why it extends it in the first place.  @danielnorberg, do you remember? Also, the return value of stop() is never used by any callers.\n. Tthanks for this comment!\n. ok, given what @danielnorberg said (assuming I understood), keep the stopAsync awaitTerminated bit here.  See below for the rest.\n. rename this to shutDown it will be called under the covers by stopAsync.awaitTerminated \n. Oh, good point.  Never mind then.\n. Generally agreed, but I think in this case it appears to be more of a problem of conflation of two things that happened to happen at the same time.  Anyway, as long as the runner gets it's stopAsync method called (however that happens) reliably at some point so we don't leak threads, I'm happy.  Maybe TaskRunner should call stopAsync at the end of stop()?\n. final\n. Done.\n. Oh duh.  Fixed.\n. You'll need to have the standard copyright header here.\n. final.  Here and for subparser, command, jobs, JobId jobId in the loop and status inside the loop\n. final or inline into the call to assertEquals\n. final or inline into the call to assertEquals\n. final, also jobIds String line in the loop header.\n. final or inline below\n. while you're there, suggest rename of imageConfig to containerConfig\n. Want to Polling.await here?\n. comment looks wrong?\n. suggest pinning a specific image tag, so we don't get bit like we did with busybox?\n. Consider extracting the log reader into a separate function.\n. File doesn't end with newline\n. add image tag?\n. As much as I like the idea of transform, etc. these kinds of things, in practice are considerably more verbose than the equivalent for loop (and with it, you could probably skip the try block too).\n. ditto here wrt functional-vs-loop.  Compare:\nint matchingContainerCount = 0;\nfor (final Container c : dockerClient.listContainers) {\n  for (final String name : c.names) {\n    if (name.contains(jobName)) {\n    matchingContainerCount ++;\n    }\n  }\n}\n. ",
    "ramonck": "Java version \"1.7.0_55\"\nOpenJDK Runtime Environment (IcedTea 2.4.7) (7u55-2.4.7-1ubuntu1)\nOpenJDK 64-Bit Server VM (build 24.51-b03, mixed mode)\nYes I'm trying from a VPS with that limitation of resource.\n. ok, so I did mvn clean, then the mvn clean test -Ddebug, here's what I got.\n\nT E S T S\nOpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000f5a00000, 21757952, 0) failed; error='Cannot allocate memory' (errno=12)\n\nThere is insufficient memory for the Java Runtime Environment to continue.\nNative memory allocation (malloc) failed to allocate 21757952 bytes for committing reserved memory.\nAn error report file with more information is saved as:\n/root/helios/helios-services/hs_err_pid16406.log\nResults :\nTests run: 0, Failures: 0, Errors: 0, Skipped: 0\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO]\n[INFO] Helios Parent ..................................... SUCCESS [6.702s]\n[INFO] Helios Client ..................................... SUCCESS [12.728s]\n[INFO] Helios Service Registration ....................... SUCCESS [0.856s]\n[INFO] Helios Tools ...................................... SUCCESS [3.325s]\n[INFO] Helios Testing Common Library ..................... SUCCESS [2.503s]\n[INFO] Helios Services ................................... FAILURE [15.993s]\n[INFO] Helios System Tests ............................... SKIPPED\n[INFO] Helios Testing Library ............................ SKIPPED\n[INFO] Helios Integration Tests .......................... SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 42.998s\n[INFO] Finished at: Wed Jun 25 21:02:07 UTC 2014\n[INFO] Final Memory: 32M/82M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.16:test (default-test) on project helios-services: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:2.16:test failed: The forked VM terminated without saying properly goodbye. VM crash or System.exit called ?\n[ERROR] Command was/bin/sh -c cd /root/helios/helios-services && /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java -jar /root/helios/helios-services/target/surefire/surefirebooter6172600170219112804.jar /root/helios/helios-services/target/surefire/surefire3506246647327018276tmp /root/helios/helios-services/target/surefire/surefire_14749132935136608764tmp\n[ERROR] -> [Help 1]\n[ERROR]\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException\n[ERROR]\n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn  -rf :helios-services\n. Daniel,\nFirst thanks for the help and effort, that did create the target for the main folder \"helios\", but the helios-tools folder has no target built yet :(, and that's my main focus in order to run the helios in bin folder.\nThe output when I ran mvn clean package -DskipTests inside helios-tools folder:\n[INFO] Scanning for projects...\n[INFO]\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Helios Tools 0.0.29-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[WARNING] The POM for com.spotify:helios-client:jar:0.0.29-SNAPSHOT is missing, no dependency information available\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 2.665s\n[INFO] Finished at: Thu Jun 26 00:02:35 UTC 2014\n[INFO] Final Memory: 4M/12M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal on project helios-tools: Could not resolve dependencies for project com.spotify:helios-tools:jar:0.0.29-SNAPSHOT: Failure to find com.spotify:helios-client:jar:0.0.29-SNAPSHOT in http://clojars.org/repo was cached in the local repository, resolution will not be reattempted until the update interval of clojars.org has elapsed or updates are forced -> [Help 1]\n[ERROR]\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException\nBest Regards,\n. Yes I did it in root first, it ran, but didn't generate the helios-tools target like the with \"Tests\" did\n. The mvn clean test starts a target folder in helios-tools but since it doesn't complete it doesn't create the objects I need for the bin -> helios requiments\n. I get a killed in the middle of the way.\nhttps://gist.github.com/ramonck/e73eb5c67c25faa8e9a7#file-helios_mvn-clean-package-dskiptests\n. I tried the less memory and still the the \"killed\" in Maven. Went the binary path, no problem, installed. Now I have this: \nhelios create foo:v1 rohan/memcached-tiny -z URL\nCreating job: {\"command\":[],\"env\":{},\"id\":\"foo:v1:2a89d5a87851d68678aabdad38d31faa296f5bf1\",\"image\":\"rohan/memcached-tiny\",\"ports\":{},\"registration\":{},\"volumes\":{}}\njava.net.URISyntaxException: Relative path in absolute URI: http://null:-1URL/jobs/?user=root\n\nTest 2 putted the URL within http://URL/ got the following:\ncom.spotify.helios.common.HeliosException: bad reply: Response{method='POST', uri=http://helios/jobs/?user=root, status=400, payload=\n\nNo Application Configured\n\n      body { background-color: #fff; margin: 0px; padding-top: 280px; text-align: center; }\n      h1   { font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 24px; font-weight: normal; }\n      h2   { font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; color: #999; font-weight: normal; }\n    \n\n\n\nNo Application Configured\nThis domain is not associated with an application.\n\n\n\n. I got a HeliosClient failed to connect, I'll try to open up the firewall rule to accept TCP 5801 then will see.\nI think it's ok, doesn't have to be default to localhost:5801 thinking about a multi-server environment which is what I think is obvious is just fine. Will open up the rules and will get back to you guys if it went ok.\nThank you very much for the support, I'm sure that this thread will help others.\n. I Opened the port and nothing still, how do I check ig Helios Client is running?\noot@as1:~# helios create foo:v1 rohan/memcached-tiny -z http://localhost:5801\nCreating job: {\"command\":[],\"env\":{},\"id\":\"foo:v1:2a89d5a87851d68678aabdad38d31faa296f5bf1\",\"image\":\"rohan/memcached-tiny\",\"ports\":{},\"registration\":{},\"volumes\":{}}\n19:18:22.016 WARN  HeliosClient: Failed to connect, retrying in 5 seconds.\n19:18:27.221 WARN  HeliosClient: Failed to connect, retrying in 5 seconds.\n^Croot@as1:~# \n. Daniel, \nI was not running helios-master, so when I did..\nI did a helios-master and did it from that VPS and now from another instance just to double check and there is this error when I run it.\njava.net.ConnectException: Connection refused\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.7.0_5                                                                                        5]\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739                                                                                        ) ~[na:1.7.0_55]\n        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocket                                                                                        NIO.java:350) ~[helios-services-0.0.30-shaded.jar:0.0.30]\n        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068)                                                                                         ~[helios-services-0.0.30-shaded.jar:0.0.30]\n23:23:11.401 helios[10582]: ERROR [pool-5-thread-1] ConnectionState: Connection                                                                                         timed out for connection string (localhost:2181) and timeout (15000) / elapsed (                                                                                        15936)\norg.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionL                                                                                        oss\n        at org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java                                                                                        :198) [helios-services-0.0.30-shaded.jar:0.0.30]\n        at org.apache.curator.ConnectionState.getZooKeeper(ConnectionState.java:                                                                                        88) [helios-services-0.0.30-shaded.jar:0.0.30]\n        at org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeep                                                                                        erClient.java:113) [helios-services-0.0.30-shaded.jar:0.0.30]\n        at org.apache.curator.utils.EnsurePath$InitialHelper$1.call(EnsurePath.j                                                                                        ava:150) [helios-services-0.0.30-shaded.jar:0.0.30]\n        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) [helio                                                                                        s-services-0.0.30-shaded.jar:0.0.30]\n        at org.apache.curator.utils.EnsurePath$InitialHelper.ensure(EnsurePath.j                                                                                        ava:142) [helios-services-0.0.30-shaded.jar:0.0.30]\n        at org.apache.curator.utils.EnsurePath.ensure(EnsurePath.java:101) [heli                                                                                        os-services-0.0.30-shaded.jar:0.0.30]\n        at org.apache.curator.framework.recipes.cache.PathChildrenCache.refresh(                                                                                        PathChildrenCache.java:469) [helios-services-0.0.30-shaded.jar:0.0.30]\n        at org.apache.curator.framework.recipes.cache.RefreshOperation.invoke(Re                                                                                        freshOperation.java:35) [helios-services-0.0.30-shaded.jar:0.0.30]\n        at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(P                                                                                        athChildrenCache.java:755) [helios-services-0.0.30-shaded.jar:0.0.30]\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:47                                                                                        1) [na:1.7.0_55]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [na:1.7.0_55                                                                                        ]\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:47                                                                                        1) [na:1.7.0_55]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [na:1.7.0_55                                                                                        ]\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:47                                                                                        1) [na:1.7.0_55]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [na:1.7.0_55                                                                                        ]\n        at java.util.concurrent.Sc^CheduledThreadPoolExecutor$ScheduledFutureTas                                                                                        k.access$201(ScheduledThreadPoolExecutor.java:178) [na:1.7.0_55]\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.                                                                                        run(ScheduledThreadPoolExecutor.java:292) [na:1.7.0_55]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.                                                                                        java:1145) [na:1.7.0_55]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor                                                                                        .java:615) [na:1.7.0_55]\n        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_55]\n23:23:11.406 helios[10582]: INFO  [Thread-2] Server: Graceful shutdown SocketCon                                                                                        nector@0.0.0.0:5802\n23:23:11.411 helios[10582]: INFO  [Thread-2] Server: Graceful shutdown Instrumen                                                                                        tedBlockingChannelConnector@0.0.0.0:5801\n23:23:11.415 helios[10582]: INFO  [Thread-2] Server: Graceful shutdown o.e.j.s.S                                                                                        ervletContextHandler{/,null}\n23:23:11.415 helios[10582]: INFO  [Thread-2] Server: Graceful shutdown o.e.j.s.S                                                                                        ervletContextHandler{/,jar:file:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/ext/pu                                                                                        lse-java.jar!/}\n23:23:12.030 helios[10582]: INFO  [MasterService STARTING-SendThread(localhost:2                                                                                        181)] ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181.                                                                                         Will not attempt to authenticate using SASL (unknown error)\n23:23:12.030 helios[10582]: WARN  [MasterService STARTING-SendThread(localhost:2                                                                                        181)] ClientCnxn: Session 0x0 for server null, unexpected error, closing socket                                                                                         connection and attempting reconnect\njava.net.ConnectException: Connection refused\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.7.0_5                                                                                        5]\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739                                                                                        ) ~[na:1.7.0_55]\n        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocket                                                                                        NIO.java:350) ~[helios-services-0.0.30-shaded.jar:0.0.30]\n        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068)                                                                                         ~[helios-services-0.0.30-shaded.jar:0.0.30]\n23:23:13.131 helios[10582]: INFO  [MasterService STARTING-SendThread(localhost:2                                                                                        181)] ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181.                                                                                         Will not attempt to authenticate using SASL (unknown error)\n23:23:13.132 helios[10582]: WARN  [MasterService STARTING-SendThread(localhost:2                                                                                        181)] ClientCnxn: Session 0x0 for server null, unexpected error, closing socket                                                                                         connection and attempting reconnect\njava.net.ConnectException: Connection refused\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.7.0_5                                                                                        5]\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739                                                                                        ) ~[na:1.7.0_55]\n        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocket                                                                                        NIO.java:350) ~[helios-services-0.0.30-shaded.jar:0.0.30]\n        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068)                                                                                         ~[helios-services-0.0.30-shaded.jar:0.0.30]\n23:23:13.466 helios[10582]: INFO  [Thread-2] ContextHandler: stopped o.e.j.s.Ser                                                                                        vletContextHandler{/,jar:file:/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/ext/puls                                                                                        e-java.jar!/}\n23:23:13.467 helios[10582]: INFO  [Thread-2] ContextHandler: stopped o.e.j.s.Ser                                                                                        vletContextHandler{/,null}\n23:23:13.467 helios[10582]: ERROR [pool-5-thread-1] PathChildrenCache:\njava.lang.InterruptedException: null\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared                                                                                        Nanos(AbstractQueuedSynchronizer.java:1038) ~[na:1.7.0_55]\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireShare                                                                                        dNanos(AbstractQueuedSynchronizer.java:1326) ~[na:1.7.0_55]\n        at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:282) ~[                                                                                        na:1.7.0_55]\n        at org.apache.curator.CuratorZookeeperClient.internalBlockUntilConnected                                                                                        OrTimedOut(CuratorZookeeperClient.java:322) ~[helios-services-0.0.30-shaded.jar:                                                                                        0.0.30]\n        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:105) ~[heli                                                                                        os-services-0.0.30-shaded.jar:0.0.30]\n        at org.apache.curator.utils.EnsurePath$InitialHelper.ensure(EnsurePath.j                                                                                        ava:142) ~[helios-services-0.0.30-shaded.jar:0.0.30]\n        at org.apache.curator.utils.EnsurePath.ensure(EnsurePath.java:101) ~[hel                                                                                        ios-services-0.0.30-shaded.jar:0.0.30]\n        at org.apache.curator.framework.recipes.cache.PathChildrenCache.refresh(                                                                                        PathChildrenCache.java:469) ~[helios-services-0.0.30-shaded.jar:0.0.30]\n        at org.apache.curator.framework.recipes.cache.RefreshOperation.invoke(Re                                                                                        freshOperation.java:35) ~[helios-services-0.0.30-shaded.jar:0.0.30]\n        at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(P                                                                                        athChildrenCache.java:755) ~[helios-services-0.0.30-shaded.jar:0.0.30]\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:47                                                                                        1) [na:1.7.0_55]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [na:1.7.0_55                                                                                        ]\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:47                                                                                        1) [na:1.7.0_55]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [na:1.7.0_55                                                                                        ]\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:47                                                                                        1) [na:1.7.0_55]\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [na:1.7.0_55                                                                                        ]\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.                                                                                        access$201(ScheduledThreadPoolExecutor.java:178) [na:1.7.0_55]\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.                                                                                        run(ScheduledThreadPoolExecutor.java:292) [na:1.7.0_55]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.                                                                                        java:1145) [na:1.7.0_55]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor                                                                                        .java:615) [na:1.7.0_55]\n        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_55]\n. I got it working finally, the problem was the zooKeeper. I followed these instructions and was running fine afterwords the helios-master.\nhttp://askubuntu.com/questions/314210/how-do-i-find-out-the-version-of-zookeeper-i-am-running\nThanks for the help guys.\n. Based on what I learned here I contributed making a setup script for ubuntu servers.\nhttps://github.com/ramonck/Helios-Setup/\nBest Regards,\n. ",
    "philipcristiano": "@ramonck I've uploaded our debs and jars to the 0.0.30 release https://github.com/spotify/helios/releases/tag/0.0.30 so you can install them directly. We have started documenting the deployment as well at https://github.com/spotify/helios/wiki/How-to-deploy-Helios \n. @ramonck The URL needs to be replaced with the URL of your master process. If you are running it from the same node the master is on it should be -z http://localhost:5801. Probably makes sense that we set that as the default value. \n. It seems like it is having trouble connecting to zookeeper. Have you set it up and passed in the string with the --zk option? \n. (as far as I can tell) this makes sense. It seems though that this is something that should have tests, it wouldn't be difficult to change the semantics of CLI parsing but has a high user impact when changed. \n. :+1: \n. :+1: \n. :+1: \n. @danielnorberg less :poop:-y now?\n. The RFC seems like a sane default. This will certainly cause us pain on the next release. We might want to hold off on merging this unless someone else complains until we have a own our packaging to change the defaults.\n. ",
    "rculbertson": "According to sonatype doc, description and url elements are needed as well.\n. :+1: \n. :+1: \n. Looks like this is no longer an issue. I just brought up a new vagrant image and it came up ok.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. LGTM, but probably someone more familiar with Vagrantfiles should also take a look.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. I added a test that makes sure a nameserver is set even when --dns is not set.\n. :+1: \n. :+1: \n. LGTM\n. LGTM\n. Agreed. To catch this issue, we'll need a way replicate the scenario where the agent can be reached by IP but not hostname. \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. Awesome! :+1: \n. :+1: \n. :+1: \n. :+1: \n. We discussed at standup. People liked that the column heading JOBS makes it explicit these values refer to jobs and not hosts. But there was concern the meaning of the \"0/1\" syntax might not be obvious.\nBefore discussing further, we thought we'd just ask what prompted this PR. Is the current implementation confusing? Trying to save screen space?\n. LGTM aside from the minor comment I left.\n. @davidxia It did make me question my sanity for a while :)\n. @davidxia You'd need to use generic wildcards, which may turn out to be a little ugly. This might be why Target was a class instead of an interface. Take a look at the comments here: https://github.com/spotify/helios/pull/163#discussion-diff-17121779\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: (although a little bit of this code is mine, so maybe someone else should review as well)\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. It's too bad we can't do this without using sun.misc packages, since it's oracle specific and not officially supported. But I guess we could always switch to calling halt if things don't shut down after some timeout.\nLGTM.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. Tested on OSX.\n:+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. Oh opps..didn't realize you had updated the PR, so I just did the same thing and force pushed.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: other than the minor thing about unused imports and fields\n. This fixes #287\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: although might want to hold off on merging until 2.7.4 is available in maven central\n. Good point about using EMPTY_TOKEN instead of null. Changed that in my last checkin. \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. I think it would be reasonable to show what version of docker each build was tested against. Showing a range of versions might be tough unless we start testing against multiple versions.\n. :+1: \n. I tested yesterday and it added maybe 1.5 minutes? We could build the containers as part of a pre-integration test phase. That way they are only built if you're about to run tests which require them. If it would be useful to be able to create the containers as part of the package phase, we could create a profile which does that.\n. I added a profile for building the containers.\n    mvn -P build-containers package\nWe can also make it part of the pre-integration test phase once we have some test to run.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. LGTM. Should be able to merge once we're upgraded to Java 8 everywhere.\n. :)\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. Yea, it's not helpful. It generates a list of IP address to connect to, but because it couldn't resolve any of them, the list is empty. At that point it should throw a helpful exception, but instead passes that to some class to make the connections. That class can't do anything with an empty list, so throws the IllegalArgumentException. Oh well.\n. You had pointed out those networking exceptions in the log before the agent became unresponsive, so I tried to recreate those conditions. I tried bringing down zookeeper while the agent was connected, and then tried having the agent connect to a zookeeper host which doesn't exist to get. \n. :+1: \n. @dxia Good points. I addressed all of them in the second commit. Let me know what you think.\n. :+1: \n. :+1: \n. :+1: \n. This happens when the health check is null, and a NPE gets thrown. PR #421 will fix this.\n. :+1: \n. :+1: \n. :+1:  Very cool\n. :+1: \n. @rohansingh \n. Ah, ok. Bumped it to 0.3.\n. :+1: \n. There is a deregister command. Just pass it the name of the agent you want to deregister. Run helios deregister --help for more details.\n. :+1: \n. :+1: \n. Turns out the bad data was being written to disk by an agent, and it would put it back into ZK every time the agent restarted (or reconnected to ZK for some other reason). Still seems like a good PR though...better if 'helios hosts' doesn't break for an entire site because of one agent.\n. :+1: \n. Agreed tests would be good for this. This isn't easy right now because the class is structured in a way that makes it difficult to inject unexpected data. It would be good to refactor this so more methods are made public, and data is easily injected, so we can test at a more granular level.\n. :+1: \n. @gimaker @davidxia \n. Added a test.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. +1 to @davidxia's comments, otherwise :+1: \n. No timeout on the awaitTerminated method?\n. Do you need to call statusUpdater.update() after this line?\n. Need to call statusUpdater.update() after calling setGoal?\n. Good point. Just looked at the Javadoc for release, and it says calling it twice has no effect, so we should be good.\n. Done.\n. It's public because TemporaryJob uses it as well.\n. It's public because TemporaryJob uses it as well.\n. Done\n. Done.\n. Good catch. Done.\n. Done.\n. This is called from TemporaryJobs to determine if a JobPrefixFile is already locked, so it must be public. This is a little awkward though, so I'll submit another commit which cleans this up.\n. Done.\n. Made some changes so this method is no longer needed.\n. Done.\n. We could. Nothing special about the get methods compared to other methods in the class. If we want to hide things, then might as well make everything package private.\n. Ah, my first time using checkNotNull, should have read the javadoc.\n. Oops. Will add now.\n. I think just git push ${TAGREF} would work too, but guess it doesn't matter either way.\nLGTM \n. FIxed.\n. Might be worth mentioning that if you're on OS X using boot2docker, everything just works, and these steps are only when installing docker manually.\n. I think you can use ${project.basedir} so you can leave out the ../../ \n. I think it would be better to return an instance of ExplicitTarget instead of Target. It's more flexible because callers can then access methods specific to ExplicitTarget without having to cast. You'd still be able to assign to Target references, so would lose any functionality.\n. Since equals is defined, it's best practice to also define hashCode. IntelliJ can generate both for you.\n. Like the ExplicitTarget.from(...), probably better to have this return List<SrvTarget>. It's a little odd to have SrvTarget.from(...) return something other than SrvTarget. Callers who need a List of Target will need to do something like List<? extends Target> list = SrvTarget.from(...)\n. This should implement hashCode as well\n. I'd say it's customary to leave equals out of the interface. Just like hashCode and toString, these methods can be called on any object, so no need to put them in the interface. Others might disagree.\n. Looking through the code some more, perhaps this is why Target was a class instead of an interface, so callers wouldn't have to deal with generic wildcards. If dealing with wildcards feels too ugly, maybe it's worth keeping Target as an abstract class with factory methods?\n. Even with the shortened hash you end up with a pretty long job name:\ntmp_e480c699_registry_80_spotify_wiggum_0.0.1-SNAPSHOT-e62e3c6.DIRTY:6715f4a6:0f462d0\nSo as it's being used now, we would end up with this line in the logs:\nDeploying registry:80/spotify/wiggum:0.0.1-SNAPSHOT-e62e3c6.DIRTY (Job tmp_e480c699_registry_80_spotify_wiggum_0.0.1-SNAPSHOT-e62e3c6.DIRTY:6715f4a6:0f462d0) to standalone.local.\nKnowing the job name can be useful, so we log it when we create the job at debug:\nCreating job tmp_e480c699_registry_80_spotify_wiggum_0.0.1-SNAPSHOT-e62e3c6.DIRTY:6715f4a6:0f462d054fed3908cf96c6980ec74940ade4c382\nMaybe we change that to use the shortened hash, and log it at info?\n. Pushed commit where so we now log \"Creating job\" at info level. The logs now look like this:\n16:22:21.491 [helios-test-runner-0] INFO  c.s.helios.testing.TemporaryJob - Creating job tmp_b961854c_registry_80_spotify_wiggum_0.0.1-SNAPSHOT-9363049.DIRTY:7a6f83e3:a02582b\n16:22:21.586 [helios-test-runner-0] INFO  c.s.helios.testing.TemporaryJob - Deploying registry:80/spotify/wiggum:0.0.1-SNAPSHOT-9363049.DIRTY (Job a02582b) to standalone.local.\n16:22:22.145 [helios-test-runner-0] INFO  c.s.helios.testing.TemporaryJob - Job state: PULLING_IMAGE\n16:22:22.657 [helios-test-runner-0] INFO  c.s.helios.testing.TemporaryJob - Job state: PULLING_IMAGE\n16:22:23.174 [helios-test-runner-0] INFO  c.s.helios.testing.TemporaryJob - Job state: RUNNING\n16:22:23.682 [helios-test-runner-0] INFO  c.s.helios.testing.TemporaryJob - Probing: wiggum @ 192.168.59.103:20005\n16:22:24.186 [helios-test-runner-0] INFO  c.s.helios.testing.TemporaryJob - Probing: wiggum @ 192.168.59.103:20005\n16:22:24.189 [helios-test-runner-0] INFO  c.s.helios.testing.TemporaryJob - Up: wiggum @ 192.168.59.103:20005\n16:22:26.289 [main] INFO  com.spotify.helios.testing.Jobs - Undeploying registry:80/spotify/wiggum:0.0.1-SNAPSHOT-9363049.DIRTY (Job a02582b) from standalone.local.\n. Good point. We saw a case where TempJobs tried couldn't deploy because the agent was down. I thought this would have been the UNKNOWN case, but that was incorrect. In that case the new code will now log \"Task status not available on host\". I'll remove the UNKNOWN case until we see an actual issue with it.\n. Should .uesrendpoints() be .endpoints()?\n. Doesn't have to be part of this PR, but was wondering if a name other than application.conf would be more clear. Maybe tempjobs.conf?\n. This means if I set additional properties on the builder object, it will override the values in the properties file? (Not saying that's good or bad, just making sure I understand the sentence correctly)\n. Ah, so the name application.conf is specific to typesafe? I thought we had just chosen that on our own.\n. should this be removed?\n. Minor nitpick, but I think it would be a little more efficient to add all the variables from the file first, and then just overwrite them with values from the CLI. It would be a few less lookups in the hashmap. Not a big deal, so I'll still +1 if you don't feel like changing.\n. Does this comment still apply? If so, what needs to be called first?\n. Looks like full is not used\n. To be consistent with the logging, maybe this should use getDescription(Job job)? \ne.g. format(\"Failed to undeploy job %s - %s\", getJobDescription(job), errors.get(0))\n. any reason for two ifs instead of this?\nif (server != null && !server.isStopping())\n. Don't think it matters much in this context, but using AtomicBoolean.compareAndSet would remove a small race condition.\n. What would you think about making this a debug message? That would make it match this log message from earlier in the function, which can also be caused by a file getting deleted by a test running in parallel.\nlog.debug(\"Unable to create JobPrefixFile for {}\", file.getPath());\n. Do we want to include deployUser in the toString() method?\n. I think this could be replaced with just this?\nfinal String dockerCertPath = getenv(\"DOCKER_CERT_PATH\");\n. unused import\n. unused import\n. There are a few unused imports and fields now that this method is gone. Github won't let me comment on those lines, but here are the imports/fields:\nimport com.google.common.base.Strings;\nimport com.google.common.collect.ImmutableMap;\nimport org.apache.commons.lang.text.StrSubstitutor;\nprivate final String jobDeployedMessageFormat;\nprivate static final Logger log = LoggerFactory.getLogger(TemporaryJobBuilder.class);\n. Good point. Just added start and stop to the list. You don't need to specify the token for inspect though.\n. Could we instead change the @see tag to this? Formatting looks a little funny, but it passes the javadoc check.\n* @see <a href=\n* \"https://github.com/spotify/helios/blob/master/docs/testing_framework.md#configuration-by-file\"\n* >Helios Testing Framework - Configuration By File</a>\n. Yea, awaitHasContainerId was a copy and paste of awaitUp, but it didn't need to be. It was awkward having two methods, because you could apply a timeout to both, so you could wait twice as long as the timeout value. \n. Good catch. I moved the imports around manually, and forgot to clean them up. Will push another commit.\n. Another option would be to make each HealthCheck it's own object. This has a few benefits.\n1. More type safe than a map of Objects, which can be messy when getting values out of the map because you need to cast everything. Also not sure if a map of Objects will cause issues when deserializing.\n2. Users can write very concise code when creating healthchecks in TempJobs, e.g. HeathCheck.newHttpCheck(\"/healthcheck\", 8080);\n3. More efficient wrt serialization than a single object, because you will serialize only the fields needed for that health check type.\n. Should this be exec?\n. Sorry, I meant the argument to super(). Should it be EXEC instead of TCP?\n. Several of our descriptors have of factory methods like this. Might be nice to add of to these object as well since it's a little more concise than invoking the builder directly.\n. Unused import\n. Any reason to make these fields Nullable? Looks like the builder ensures that they are not null\n. Does this need to be nullable?\n. Does this need to be nullable?\n. I think Nullable is used to indicate that a null value is valid and expected, which I don't think it is in this case? There are other parts of the code which assume the getters will return a non-null value. Right now those generate findbugs warnings since they are Nullable. So I think we'd either need to handle nulls in those cases, or make this not nullable.\n. unused import\n. unused import\n. Should the first %n be there? Looks like the other items start printing the value on the same line?\n. If we health check forever, something at a higher level (i.e. helios-helper) will need to decide when to give up, and what action to take (probably undeploy). I think that's ok, and pretty straight forward to reason about.\nBut I think things get trickier if we put that logic in the agent.\n- If health checks fail, should we let the container keep running? If we do, and it eventually becomes healthy, should we detect that and mark it as RUNNING? \n- Should we stop the container? If so, should helios restart it as if it exited with an error code?\n- Should we reuse FAILED (which currently means your container exited with an error code) or introduce HEALTH_CHECK_FAILED?\n- If we health check forever, callers can decide when to give up. If the agent gives up should we make the timeout configurable?\nI'm inclined to let it health check forever, and revisit if it's proves insufficient, but maybe I'm overthinking this?\n@rohansingh @matslina?\n. I'm running 1.5.0. executionDriver is native-0.2 for me too.\n. It looks like they changed this intentionally.\nhttps://github.com/docker/libcontainer/pull/328\nAnd according to this, returning 137 is the correct thing to do.\nhttp://tldp.org/LDP/abs/html/exitcodes.html\nI'll change the commit message to explain this.\n. Any reason to require 7 as well?\n. Yea logging it is a good idea. Just pushed that.\n. Good point. Changed the message.\n. @staffan do you remember how we got this message to occur earlier? Not sure how we would end up here.\n. Ah right. If you undeploy and the updater kicks in before the agent removed the status. Will updated the message.\n. ",
    "adzynia": "This might help. Thank you @danielnorberg \n. ",
    "davidxia": "I haven't heard complaints about this. Here's the substring search code for reference https://github.com/spotify/helios/blob/master/helios-services/src/main/java/com/spotify/helios/master/resources/JobsResource.java#L104\njava\nif (entry.getKey().toString().contains(q)) {\n. Should I add a test for JobCreateCommandTest.run()?\n. @drewcsillag Am I on the right track? There are so many things to mock.\n. @rohansingh \n. @drewcsillag I'm wondering where to write a test for this. The two relevant test classes are TaskRunnerTest and SupervisorTest. TaskRunnerTest only has a test that throws an exception on run(), and I'm not sure how to write a test that makes docker.inspectImage(image); not throw.\nIt seems like a test in SupervisorTest modeled on verifySupervisorStartsAndStopsDockerContainer() would be good?\n. Great to know there'a plugin to do this.\n. Right now using multiple domains with the --json switch will output valid JSON within each domain's output, but the entire output isn't valid JSON because it's interrupted by headers like\n```\nhelios ([http://ash2-heliosmaster-a3.ash2.spotify.net.:5801, http://ash2-heliosmaster-a2.ash2.spotify.net.:5801, http://ash2-heliosmaster-a1.ash2.spotify.net.:5801])\n\n```\nhelios create --json -d ash,lon -f helios_job_config.json ad-event-tracker:FOOBAR registry:80/spotify/ad-event-tracker:0.0.9-SNAPSHOT-d5f127b server ad-event-tracker.yaml\nwill output\n```\nhelios ([http://ash2-heliosmaster-a3.ash2.spotify.net.:5801, http://ash2-heliosmaster-a2.ash2.spotify.net.:5801, http://ash2-heliosmaster-a1.ash2.spotify.net.:5801])\n\n\"ad-event-tracker:0.0.8-2015c18.ad-event-tracker-0.0.8:46d656ede83c081128b8701213b9e4910f617024\"\nhelios ([http://lon3-heliosmaster-a1.lon3.spotify.net.:5801, http://lon3-heliosmaster-a3.lon3.spotify.net.:5801, http://lon3-heliosmaster-a2.lon3.spotify.net.:5801])\n\"ad-event-tracker:0.0.8-2015c18.ad-event-tracker-0.0.8:46d656ede83c081128b8701213b9e4910f617024\"\n```\nCan we make the output not have the headers and instead output valid JSON with something like this?\n{\n   \"ash (srv: helios)\":{\n      \"errors\":[ ],\n      \"id\":\"ad-event-tracker:FOOBAR:3ada44a457f3fa2e6ca9041a20043948122ad244\",\n      \"status\":\"OK\"\n   },\n   \"lon (srv: helios)\":{\n      \"errors\":[ ],\n      \"id\":\"ad-event-tracker:FOOBAR:3ada44a457f3fa2e6ca9041a20043948122ad244\",\n      \"status\":\"OK\"\n   }\n}\nThis should with other subcommands, but I haven't checked all of them. helios jobs --json -d shared.cloud,ash ad-event-tracker will output\n```\n{\n   \"shared.cloud (srv: helios)\":{\n      \"ad-event-tracker:0.0.7:11cc0bbef6b42bc5539a370e0aa22f2799cebf69\":{\n         \"command\":[\n            \"server\",\n            \"ad-event-tracker.yaml\"\n         ],\n         \"env\":{\n     }\n  },\n  \"ad-event-tracker:0.0.8-SNAPSHOT-5150577:5bb99de55f9fc0a1add1db0000d95ff4bae598cc\":{\n     \"command\":[\n        \"server\",\n        \"ad-event-tracker.yaml\"\n     ],\n     \"env\":{\n\n     }\n  },\n  \"ad-event-tracker:0.0.8-SNAPSHOT-637574b:91b946c81bbb4aeafe7f243e39b6d63e4110d124\":{\n     \"command\":[\n        \"server\",\n        \"ad-event-tracker.yaml\"\n     ],\n     \"env\":{\n\n     }\n\n},\n   \"ash (srv: helios)\":{\n      \"ad-event-tracker:0.0.7:5eb27f43f8716ae831027a1ac4a39a390bae05b0\":{\n         \"command\":[\n            \"server\",\n            \"ad-event-tracker-production.yaml\"\n         ],\n         \"env\":{\n     }\n  }\n\n}\n}\n```\n. This PR depends on https://github.com/spotify/helios/pull/163/\n. @danielnorberg Multi domains are so convenient. I'd like to see them stay and do the second option you suggested above.\nIt's also be nice to to have a system test that creates more than one master in different domains to test this. But it'll be a lot of changes. More than Id like to make to unblock myself on other stuff at this point.\n. fixed by #194 \n. I agree the original output is confusing because it might imply that X hosts (with that name?) are running and Y hosts are deployed instead of X jobs running on that host and Y jobs deployed on that host.\nTo prevent confusion of what JOBS 0/1 means, we can just add some explanation to the output of helios hosts -h.\n. I don't have a super strong opinion. But I think this is an improvement And am leaning towards merging.\n. @rculbertson nice work! Hopefully this wasn't too hard to figure out...\n. :+1: \n. @danielnorberg \n. @rculbertson yea I reverted the interface part. I just made ExplicitTarget have just one URI instead of a list of URIs.\n. @Kobolog FYI so you can add yourself too.\n. :+1:  aww too late!\n. :+1: \n. Is this fixed by https://github.com/spotify/helios/pull/176?\n. Looks good to me. Just a minor typo in user_manual.md \"there will be an environmen variable\"\n. @drewcsillag Nice thanks.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. @rculbertson \n. fixed by https://github.com/spotify/helios/commit/3179e2dfce334cf56b335553da067478f0542c66\n. :+1: \n. closing because this isn't a PR\n. see https://github.com/spotify/helios/issues/237\n. @tedyoung Yea, I'm not sure why this happens. All I know is that with commit https://github.com/spotify/helios/commit/8462a411b4374fe980cd834328f056c8ac1b1b75, the container starts successfully on 1.3 and has the logs below.\nWithout https://github.com/spotify/helios/commit/8462a411b4374fe980cd834328f056c8ac1b1b75, the container can't start and shows the logs further below.\nIf I modify this line to just have a repeated 17 times, it still works. If I add just one for a to have 18 chars, it fails. I have no idea :(\nDon't set hostname\n```\nawsuse1-heliosagent-a1% tail -f /spotify/log/helios/info.log | grep ContainerConfig\n2014-10-27T13:04:56.692+00:00 awsuse1-heliosagent-a1.blixt.cloud.spotify.net helios[12244]: Creating container with ContainerConfig: ContainerConfig{hostname=null, domainname=awsuse1-heliosagent-a1.blixt.cloud.spotify.net, user=null, memory=null, memorySwap=null, cpuShares=null, cpuset=null, attachStdin=null, attachStdout=null, attachStderr=null, portSpecs=null, exposedPorts=[4229/tcp, 8080/tcp, 8081/tcp], tty=null, openStdin=null, stdinOnce=null, env=[SPOTIFY_ROLE=heliosagent, SPOTIFY_POD=awsuse1, HELIOS_PORT_http-admin=awsuse1-heliosagent-a1.blixt.cloud.spotify.net:8061, JVM_ARGS=-Ddw.lasertag.endpoint='tcp://apollo-a1.shared.cloud.spotify.net:5700', SPOTIFY_SITE=awsuse, SPOTIFY_SYSLOG_PORT=514, SPOTIFY_FFWD_HOST=10.99.0.1, HELIOS_PORT_http=awsuse1-heliosagent-a1.blixt.cloud.spotify.net:8060, SPOTIFY_FFWD_STATSD_UDP_PORT=8125, SPOTIFY_LOCAL_ADDRESS=10.99.0.1, HELIOS_PORT_hermes=awsuse1-heliosagent-a1.blixt.cloud.spotify.net:5700, SPOTIFY_SYSLOG_HOST=10.99.0.1, SPOTIFY_DOMAIN=blixt.cloud.spotify.net, SPOTIFY_FFWD_RIEMANN_TCP_PORT=5555], cmd=[server, ad-server-proxy-stage.yaml], image=registry:80/spotify/ad-server-proxy:0.0.33-SNAPSHOT-1b03062, volumes=[/helios], workingDir=null, entrypoint=[/helios/syslog-redirector, -h, 10.99.0.1:514, -n, ad-server-proxy:0.0.33-SNAPSHOT-1b03062:598f0f6548db2679528e7ecd6e91ab7354aaa472, --, /bin/bash, -c, exec java $JVM_DEFAULT_ARGS $JVM_ARGS -jar /ad-server-proxy-0.0.33-SNAPSHOT.jar \"$@\", bash], networkDisabled=null, onBuild=null}\n2014-10-27T13:04:57.055+00:00 awsuse1-heliosagent-a1.blixt.cloud.spotify.net helios[12244]: created container: TaskConfig{job=Job{id=ad-server-proxy:0.0.33-SNAPSHOT-1b03062:598f0f6548db2679528e7ecd6e91ab7354aaa472, image=registry:80/spotify/ad-server-proxy:0.0.33-SNAPSHOT-1b03062, command=[server, ad-server-proxy-stage.yaml], env={JVM_ARGS=-Ddw.lasertag.endpoint='tcp://apollo-a1.shared.cloud.spotify.net:5700'}, ports={hermes=PortMapping{internalPort=4229, externalPort=5700, protocol=tcp}, http=PortMapping{internalPort=8080, externalPort=8060, protocol=tcp}, http-admin=PortMapping{internalPort=8081, externalPort=8061, protocol=tcp}}, registration={asp/hm=ServicePorts{ports={hermes={}}}, asp/http=ServicePorts{ports={http={}}}}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=null}, host=awsuse1-heliosagent-a1.blixt.cloud.spotify.net, ports={hermes=5700, http=8060, http-admin=8061}, envVars={SPOTIFY_FFWD_HOST=10.99.0.1, SPOTIFY_FFWD_STATSD_UDP_PORT=8125, SPOTIFY_LOCAL_ADDRESS=10.99.0.1, SPOTIFY_ROLE=heliosagent, SPOTIFY_POD=awsuse1, SPOTIFY_SITE=awsuse, SPOTIFY_SYSLOG_HOST=10.99.0.1, SPOTIFY_DOMAIN=blixt.cloud.spotify.net, SPOTIFY_FFWD_RIEMANN_TCP_PORT=5555, SPOTIFY_SYSLOG_PORT=514}, containerDecorator=com.spotify.helios.agent.SyslogRedirectingContainerDecorator@557d5003, defaultRegistrationDomain=shared.cloud.spotify.net}: ContainerCreation{id=b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7, warnings=null}, ContainerConfig{hostname=null, domainname=awsuse1-heliosagent-a1.blixt.cloud.spotify.net, user=null, memory=null, memorySwap=null, cpuShares=null, cpuset=null, attachStdin=null, attachStdout=null, attachStderr=null, portSpecs=null, exposedPorts=[4229/tcp, 8080/tcp, 8081/tcp], tty=null, openStdin=null, stdinOnce=null, env=[SPOTIFY_ROLE=heliosagent, SPOTIFY_POD=awsuse1, HELIOS_PORT_http-admin=awsuse1-heliosagent-a1.blixt.cloud.spotify.net:8061, JVM_ARGS=-Ddw.lasertag.endpoint='tcp://apollo-a1.shared.cloud.spotify.net:5700', SPOTIFY_SITE=awsuse, SPOTIFY_SYSLOG_PORT=514, SPOTIFY_FFWD_HOST=10.99.0.1, HELIOS_PORT_http=awsuse1-heliosagent-a1.blixt.cloud.spotify.net:8060, SPOTIFY_FFWD_STATSD_UDP_PORT=8125, SPOTIFY_LOCAL_ADDRESS=10.99.0.1, HELIOS_PORT_hermes=awsuse1-heliosagent-a1.blixt.cloud.spotify.net:5700, SPOTIFY_SYSLOG_HOST=10.99.0.1, SPOTIFY_DOMAIN=blixt.cloud.spotify.net, SPOTIFY_FFWD_RIEMANN_TCP_PORT=5555], cmd=[server, ad-server-proxy-stage.yaml], image=registry:80/spotify/ad-server-proxy:0.0.33-SNAPSHOT-1b03062, volumes=[/helios], workingDir=null, entrypoint=[/helios/syslog-redirector, -h, 10.99.0.1:514, -n, ad-server-proxy:0.0.33-SNAPSHOT-1b03062:598f0f6548db2679528e7ecd6e91ab7354aaa472, --, /bin/bash, -c, exec java $JVM_DEFAULT_ARGS $JVM_ARGS -jar /ad-server-proxy-0.0.33-SNAPSHOT.jar \"$@\", bash], networkDisabled=null, onBuild=null}\n```\n```\nawsuse1-heliosagent-a1% sudo tail -f /var/log/upstart/docker.log\n[debug] server.go:1181 Calling GET /images/{name:.}/json\n[info] GET /v1.12/images/registry:80/spotify/ad-server-proxy:0.0.33-SNAPSHOT-1b03062/json\n[4be0d334] +job image_inspect(registry:80/spotify/ad-server-proxy:0.0.33-SNAPSHOT-1b03062)\n[4be0d334] -job image_inspect(registry:80/spotify/ad-server-proxy:0.0.33-SNAPSHOT-1b03062) = OK (0)\n[debug] server.go:1181 Calling POST /containers/create\n[info] POST /v1.12/containers/create?name=helios-FF00707F1FCD98638AA820469D18577A9706A0AE-ad-server-proxy_0_0_33-SNAPSHOT-1b03062_598f0f6_25b46505\n[4be0d334] +job create(helios-FF00707F1FCD98638AA820469D18577A9706A0AE-ad-server-proxy_0_0_33-SNAPSHOT-1b03062_598f0f6_25b46505)\n[4be0d334] +job log(create, b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7, registry:80/spotify/ad-server-proxy:0.0.33-SNAPSHOT-1b03062)\n[4be0d334] -job log(create, b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7, registry:80/spotify/ad-server-proxy:0.0.33-SNAPSHOT-1b03062) = OK (0)\n[4be0d334] -job create(helios-FF00707F1FCD98638AA820469D18577A9706A0AE-ad-server-proxy_0_0_33-SNAPSHOT-1b03062_598f0f6_25b46505) = OK (0)\n[debug] server.go:1181 Calling POST /containers/{name:.}/start\n[info] POST /v1.12/containers/b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7/start\n[4be0d334] +job start(b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7)\n[4be0d334] +job allocate_interface(b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7)\n[4be0d334] -job allocate_interface(b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7) = OK (0)\n[4be0d334] +job allocate_port(b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7)\n[debug] /sbin/iptables, [--wait -t nat -A DOCKER -p tcp -d 0/0 --dport 5700 ! -i spotify0 -j DNAT --to-destination 10.99.0.144:4229]\n[debug] /sbin/iptables, [--wait -I FORWARD ! -i spotify0 -o spotify0 -p tcp -d 10.99.0.144 --dport 4229 -j ACCEPT]\n[4be0d334] -job allocate_port(b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7) = OK (0)\n[4be0d334] +job allocate_port(b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7)\n[debug] /sbin/iptables, [--wait -t nat -A DOCKER -p tcp -d 0/0 --dport 8060 ! -i spotify0 -j DNAT --to-destination 10.99.0.144:8080]\n[debug] /sbin/iptables, [--wait -I FORWARD ! -i spotify0 -o spotify0 -p tcp -d 10.99.0.144 --dport 8080 -j ACCEPT]\n[4be0d334] -job allocate_port(b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7) = OK (0)\n[4be0d334] +job allocate_port(b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7)\n[debug] /sbin/iptables, [--wait -t nat -A DOCKER -p tcp -d 0/0 --dport 8061 ! -i spotify0 -j DNAT --to-destination 10.99.0.144:8081]\n[debug] /sbin/iptables, [--wait -I FORWARD ! -i spotify0 -o spotify0 -p tcp -d 10.99.0.144 --dport 8081 -j ACCEPT]\n[4be0d334] -job allocate_port(b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7) = OK (0)\n[4be0d334] +job log(start, b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7, registry:80/spotify/ad-server-proxy:0.0.33-SNAPSHOT-1b03062)\n[4be0d334] -job log(start, b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7, registry:80/spotify/ad-server-proxy:0.0.33-SNAPSHOT-1b03062) = OK (0)\n[4be0d334] -job start(b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7) = OK (0)\n[debug] server.go:1181 Calling GET /containers/{name:.}/json\n[info] GET /v1.12/containers/b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7/json\n[4be0d334] +job container_inspect(b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7)\n[4be0d334] -job container_inspect(b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7) = OK (0)\n[debug] server.go:1181 Calling GET /containers/{name:.}/json\n[info] GET /v1.12/containers/b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7/json\n[4be0d334] +job container_inspect(b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7)\n[4be0d334] -job container_inspect(b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7) = OK (0)\n[debug] server.go:1181 Calling GET /containers/{name:.*}/json\n[info] GET /v1.12/containers/b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7/json\n[4be0d334] +job container_inspect(b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7)\n[4be0d334] -job container_inspect(b86abf89473e65f7fced7a2f5817182db737b86a4a112e35cd1944959e3572f7) = OK (0)\n[debug] server.go:1181 Calling GET /version\n[info] GET /v1.12/version\n```\nSet hostname\n```\nawsuse1-heliosagent-a1% tail -f /spotify/log/helios/info.log | grep ContainerConfig\n2014-10-27T13:10:47.660+00:00 awsuse1-heliosagent-a1.blixt.cloud.spotify.net helios[13112]: Creating container with ContainerConfig: ContainerConfig{hostname=ad_server_proxy_SNAPSHOT_1b03062, domainname=awsuse1-heliosagent-a1.blixt.cloud.spotify.net, user=null, memory=null, memorySwap=null, cpuShares=null, cpuset=null, attachStdin=null, attachStdout=null, attachStderr=null, portSpecs=null, exposedPorts=[4229/tcp, 8080/tcp, 8081/tcp], tty=null, openStdin=null, stdinOnce=null, env=[SPOTIFY_ROLE=heliosagent, SPOTIFY_POD=awsuse1, HELIOS_PORT_http-admin=awsuse1-heliosagent-a1.blixt.cloud.spotify.net:8061, JVM_ARGS=-Ddw.lasertag.endpoint='tcp://apollo-a1.shared.cloud.spotify.net:5700', SPOTIFY_SITE=awsuse, SPOTIFY_SYSLOG_PORT=514, SPOTIFY_FFWD_HOST=10.99.0.1, HELIOS_PORT_http=awsuse1-heliosagent-a1.blixt.cloud.spotify.net:8060, SPOTIFY_FFWD_STATSD_UDP_PORT=8125, SPOTIFY_LOCAL_ADDRESS=10.99.0.1, HELIOS_PORT_hermes=awsuse1-heliosagent-a1.blixt.cloud.spotify.net:5700, SPOTIFY_SYSLOG_HOST=10.99.0.1, SPOTIFY_DOMAIN=blixt.cloud.spotify.net, SPOTIFY_FFWD_RIEMANN_TCP_PORT=5555], cmd=[server, ad-server-proxy-stage.yaml], image=registry:80/spotify/ad-server-proxy:0.0.33-SNAPSHOT-1b03062, volumes=[/helios], workingDir=null, entrypoint=[/helios/syslog-redirector, -h, 10.99.0.1:514, -n, ad-server-proxy:0.0.33-SNAPSHOT-1b03062:598f0f6548db2679528e7ecd6e91ab7354aaa472, --, /bin/bash, -c, exec java $JVM_DEFAULT_ARGS $JVM_ARGS -jar /ad-server-proxy-0.0.33-SNAPSHOT.jar \"$@\", bash], networkDisabled=null, onBuild=null}\n```\n```\nawsuse1-heliosagent-a1% sudo tail -f /var/log/upstart/docker.log\n[debug] server.go:1181 Calling POST /containers/create\n[info] POST /v1.12/containers/create?name=helios-FF00707F1FCD98638AA820469D18577A9706A0AE-ad-server-proxy_0_0_33-SNAPSHOT-1b03062_598f0f6_bb1bcb6a\n[4be0d334] +job create(helios-FF00707F1FCD98638AA820469D18577A9706A0AE-ad-server-proxy_0_0_33-SNAPSHOT-1b03062_598f0f6_bb1bcb6a)\n[4be0d334] +job log(create, 43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190, registry:80/spotify/ad-server-proxy:0.0.33-SNAPSHOT-1b03062)\n[4be0d334] -job log(create, 43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190, registry:80/spotify/ad-server-proxy:0.0.33-SNAPSHOT-1b03062) = OK (0)\n[4be0d334] -job create(helios-FF00707F1FCD98638AA820469D18577A9706A0AE-ad-server-proxy_0_0_33-SNAPSHOT-1b03062_598f0f6_bb1bcb6a) = OK (0)\n[debug] server.go:1181 Calling POST /containers/{name:.}/start\n[info] POST /v1.12/containers/43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190/start\n[4be0d334] +job start(43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190)\n[4be0d334] +job allocate_interface(43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190)\n[4be0d334] -job allocate_interface(43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190) = OK (0)\n[4be0d334] +job allocate_port(43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190)\n[debug] /sbin/iptables, [--wait -t nat -A DOCKER -p tcp -d 0/0 --dport 5700 ! -i spotify0 -j DNAT --to-destination 10.99.0.145:4229]\n[debug] /sbin/iptables, [--wait -I FORWARD ! -i spotify0 -o spotify0 -p tcp -d 10.99.0.145 --dport 4229 -j ACCEPT]\n[4be0d334] -job allocate_port(43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190) = OK (0)\n[4be0d334] +job allocate_port(43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190)\n[debug] /sbin/iptables, [--wait -t nat -A DOCKER -p tcp -d 0/0 --dport 8060 ! -i spotify0 -j DNAT --to-destination 10.99.0.145:8080]\n[debug] /sbin/iptables, [--wait -I FORWARD ! -i spotify0 -o spotify0 -p tcp -d 10.99.0.145 --dport 8080 -j ACCEPT]\n[4be0d334] -job allocate_port(43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190) = OK (0)\n[4be0d334] +job allocate_port(43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190)\n[debug] /sbin/iptables, [--wait -t nat -A DOCKER -p tcp -d 0/0 --dport 8061 ! -i spotify0 -j DNAT --to-destination 10.99.0.145:8081]\n[debug] /sbin/iptables, [--wait -I FORWARD ! -i spotify0 -o spotify0 -p tcp -d 10.99.0.145 --dport 8081 -j ACCEPT]\n[4be0d334] -job allocate_port(43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190) = OK (0)\n[4be0d334] +job log(start, 43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190, registry:80/spotify/ad-server-proxy:0.0.33-SNAPSHOT-1b03062)\n[4be0d334] -job log(start, 43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190, registry:80/spotify/ad-server-proxy:0.0.33-SNAPSHOT-1b03062) = OK (0)\n[4be0d334] +job release_interface(43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190)\n[debug] /sbin/iptables, [--wait -t nat -D DOCKER -p tcp -d 0/0 --dport 5700 ! -i spotify0 -j DNAT --to-destination 10.99.0.145:4229]\n[debug] /sbin/iptables, [--wait -D FORWARD ! -i spotify0 -o spotify0 -p tcp -d 10.99.0.145 --dport 4229 -j ACCEPT]\n[debug] /sbin/iptables, [--wait -t nat -D DOCKER -p tcp -d 0/0 --dport 8060 ! -i spotify0 -j DNAT --to-destination 10.99.0.145:8080]\n[debug] /sbin/iptables, [--wait -D FORWARD ! -i spotify0 -o spotify0 -p tcp -d 10.99.0.145 --dport 8080 -j ACCEPT]\n[debug] /sbin/iptables, [--wait -t nat -D DOCKER -p tcp -d 0/0 --dport 8061 ! -i spotify0 -j DNAT --to-destination 10.99.0.145:8081]\n[debug] /sbin/iptables, [--wait -D FORWARD ! -i spotify0 -o spotify0 -p tcp -d 10.99.0.145 --dport 8081 -j ACCEPT]\n[4be0d334] -job release_interface(43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190) = OK (0)\n[4be0d334] +job release_interface(43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190)\n[info] Unable to unmap port 0.0.0.0:5700: port is not mapped\n[info] Unable to unmap port 0.0.0.0:8060: port is not mapped\n[info] Unable to unmap port 0.0.0.0:8061: port is not mapped\n[4be0d334] -job release_interface(43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190) = OK (0)\n[4be0d334] +job log(die, 43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190, registry:80/spotify/ad-server-proxy:0.0.33-SNAPSHOT-1b03062)\n[4be0d334] -job log(die, 43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190, registry:80/spotify/ad-server-proxy:0.0.33-SNAPSHOT-1b03062) = OK (0)\nCannot start container 43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190: sethostname invalid argument\n[4be0d334] -job start(43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190) = ERR (1)\n[error] server.go:1207 Handler for POST /containers/{name:.}/start returned error: Cannot start container 43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190: sethostname invalid argument\n[error] server.go:110 HTTP Error: statusCode=500 Cannot start container 43c7ef8f04cfece64c01bd7d9eaf17133842e8187080a27f97bc0024aaef2190: sethostname invalid argument\n```\n. @tedyoung Interesting. I'll have to look into this more when I get some time to find out where we're adding more chars.\n. @tedyoung @rohansingh \nContinuing this thread in this issue here: https://github.com/spotify/helios/issues/237\n. @rculbertson \n. @tedyoung\nSo I took another look at this. The log lines I pasted in https://github.com/spotify/helios/pull/235 with Creating container with ContainerConfig are created by this line in DefaultDockerClient. \nImmediately after, all the client does is serialize config to JSON and make a POST request to the Docker daemon (code here).\nWe are using Docker API v 1.12. This is the container creation endpoint docs.\nSo I think the final hostname we send to Docker daemon is the string we used to set here. There shouldn't be any other manipulation of this hostname string, or at least I can't see it being changed anywhere.\n@rohansingh Want to sanity check this to make sure I'm not missing anything?\n. The mystery is solved!\nTLDR: Docker 1.3.x, unlike Docker 1.0.0 concatenates Hostname and Domainname and passes that to sethostname which fails. Their error message isn't helpful. Fixed by https://github.com/spotify/helios/pull/295\nWe can reproduce this problem just by mimicking Helios' requests to the Docker daemon via curl.\nSuccessful container\nCreate a container. Notice the Hostname is a and the Domainname has 62 characters.\n```\n$ curl  -H \"Content-Type: application/json\" -X POST \"http://127.0.0.1:2375/v1.12/containers/create?name=helios-EF43F202F31D673F2ECD576ABCC09B146FC61E43-dxia-test_2_7ebd524_55ed8fd1\" -d'{ \"Hostname\":\"a\", \"Domainname\": \"foohostnameaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \"User\":null, \"Memory\":null, \"MemorySwap\":null, \"CpuShares\":null, \"Cpuset\":null, \"AttachStdin\":null, \"AttachStdout\":null, \"AttachStderr\":null, \"PortSpecs\":null, \"Tty\":null, \"OpenStdin\":null, \"StdinOnce\":null, \"Env\":null, \"Cmd\":[ \"date\" ], \"Image\":\"registry:80/spotify/wiggum:0.0.1-SNAPSHOT-e5a02c2\", \"Volumes\":{ \"/tmp\": {} }, \"WorkingDir\":null, \"NetworkDisabled\": null, \"ExposedPorts\":{} }'\n{\"Id\":\"fcbb64b2374953a3c3bb67497361cba90678ebdc174a8b8b67e40480eeca1205\",\"Warnings\":null}\n```\nStart the container.\n```\n Hostname was NOT found in DNS cache\n   Trying 127.0.0.1...\n* Connected to 127.0.0.1 (127.0.0.1) port 2375 (#0)\n\nPOST /v1.12/containers/659d800ce09d8b47c3aabac5fec1d8cb9fd11fa7555cfb4dbbb45bb7d4bd381b/start HTTP/1.1\nUser-Agent: curl/7.35.0\nHost: 127.0.0.1:2375\nAccept: /\nContent-Type: application/json\nContent-Length: 67\n\nupload completely sent off: 67 out of 67 bytes\n< HTTP/1.1 204 No Content\n< Date: Thu, 13 Nov 2014 20:27:59 GMT\n<\nConnection #0 to host 127.0.0.1 left intact\n```\n\n\ndocker ps shows it running.\n$ docker ps\nCONTAINER ID        IMAGE                                                             COMMAND                CREATED             STATUS              PORTS                                            NAMES\n659d800ce09d        registry:80/spotify/wiggum:0.0.1-SNAPSHOT-e5a02c2                 \"\\\"/bin/bash -c 'exe   38 seconds ago      Up 3 seconds                                                         helios-EF43F202F31D673F2ECD576ABCC09B146FC61E43-dxia-test_2_7ebd524_55ed8fd1\nInspect the container's HostnamePath and the contents of that file.\n$ docker inspect 659d800ce09d | grep HostnamePath\n    \"HostnamePath\": \"/var/lib/docker/containers/659d800ce09d8b47c3aabac5fec1d8cb9fd11fa7555cfb4dbbb45bb7d4bd381b/hostname\",\n$ sudo cat /var/lib/docker/containers/659d800ce09d8b47c3aabac5fec1d8cb9fd11fa7555cfb4dbbb45bb7d4bd381b/hostname\na.foohostnameaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\na.foohostnameaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa has 64 characters.\nUnsuccessful container\nNow set Domainname to a string with 63 characters.\n```\n$ curl  -H \"Content-Type: application/json\" -X POST \"http://127.0.0.1:2375/v1.12/containers/create?name=helios-EF43F202F31D673F2ECD576ABCC09B146FC61E43-dxia-test_3_7ebd524_55ed8fd1\" -d'{ \"Hostname\":\"a\", \"Domainname\": \"foohostnameaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0\", \"User\":null, \"Memory\":null, \"MemorySwap\":null, \"CpuShares\":null, \"Cpuset\":null, \"AttachStdin\":null, \"AttachStdout\":null, \"AttachStderr\":null, \"PortSpecs\":null, \"Tty\":null, \"OpenStdin\":null, \"StdinOnce\":null, \"Env\":null, \"Cmd\":[ \"date\" ], \"Image\":\"registry:80/spotify/wiggum:0.0.1-SNAPSHOT-e5a02c2\", \"Volumes\":{ \"/tmp\": {} }, \"WorkingDir\":null, \"NetworkDisabled\": null, \"ExposedPorts\":{} }'\n{\"Id\":\"0ed9aea7a43ef8a8deb6c6081860f6f801da07056fc3778c449f02ee022aa496\",\"Warnings\":null}\n```\nStart the container.\n```\n$ url  -v -H \"Content-Type: application/json\" -X POST \"http://127.0.0.1:2375/v1.12/containers/0ed9aea7a43ef8a8deb6c6081860f6f801da07056fc3778c449f02ee022aa496/start\" -d '{\"Binds\":[\"/usr/lib/helios:/helios:ro\"],\"PortBindings\":{},\"Dns\":[]}'\n Hostname was NOT found in DNS cache\n   Trying 127.0.0.1...\n* Connected to 127.0.0.1 (127.0.0.1) port 2375 (#0)\n\nPOST /v1.12/containers/0ed9aea7a43ef8a8deb6c6081860f6f801da07056fc3778c449f02ee022aa496/start HTTP/1.1\nUser-Agent: curl/7.35.0\nHost: 127.0.0.1:2375\nAccept: /\nContent-Type: application/json\nContent-Length: 67\n\nupload completely sent off: 67 out of 67 bytes\n< HTTP/1.1 500 Internal Server Error\n< Content-Type: text/plain; charset=utf-8\n< Date: Thu, 13 Nov 2014 20:30:39 GMT\n< Content-Length: 118\n<\nCannot start container 0ed9aea7a43ef8a8deb6c6081860f6f801da07056fc3778c449f02ee022aa496: sethostname invalid argument\nConnection #0 to host 127.0.0.1 left intact\n```\n\n\nInspect the still-born container's HostnamePath and the contents of that file.\n```\n$ docker ps -a\nCONTAINER ID        IMAGE                                                             COMMAND                CREATED             STATUS                           PORTS                                            NAMES\n0ed9aea7a43e        registry:80/spotify/wiggum:0.0.1-SNAPSHOT-e5a02c2                 \"\\\"/bin/bash -c 'exe   47 seconds ago                                                                                        helios-EF43F202F31D673F2ECD576ABCC09B146FC61E43-dxia-test_3_7ebd524_55ed8fd1\n$ docker inspect 0ed9aea7a43e | grep HostnamePath\n    \"HostnamePath\": \"/var/lib/docker/containers/0ed9aea7a43ef8a8deb6c6081860f6f801da07056fc3778c449f02ee022aa496/hostname\",\n$ sudo cat /var/lib/docker/containers/6af89fa5439f8ec1bacbbe950a282e1840c20b54dd4d552d6d9fe345083cd890/hostname\na.foohostnameaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0\n```\na.foohostnameaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0 has 65 characters.\nMoral of the story\nSo Docker 1.3.x, unlike Docker 1.0.0 concatenates Hostname and Domainname and passes that to sethostname which fails. The error message isn't helpful, and why does Docker concatenate hostname and domainname in 1.3.x?\nFixed by https://github.com/spotify/helios/pull/295\ncc @tedyoung\nThanks to @rculbertson @drewcsillag who found and fixed it!\n. I tried to find the implementation of the system call sethostname on our machines to try to find out what was going on. But no luck. Anybody have any clue what happened?\n. @sfussenegger @carlanton Thanks for reporting this. We're messing up the upload somehow. I'll look into this.\n. @sfussenegger @carlanton I was curling them with -d instead of --data-binary. The latest stable works: https://github.com/spotify/helios/releases/tag/0.8.42\n. Hm, seems like I can just separate stdout and stderr and all is good.\n. So --force in this particular command means something more like \"yes.\" It skips the interactive prompt asking the user to confirm deregistration.\nSo if there really were a \"force\", the question seems to be what should we do with jobs? Leave them as is or undeploy them? I lean towards leaving them as is and letting user figure it out, but I don't know the implications for zookeeper and master's records on agents.\n. @rohansingh Just checking this is ok to merge.\n. @rculbertson We can do -RN for @drewcsillag's release note generator. In that case, typo in buildler :)\n. :+1: \n. @rculbertson I added the RN to the commit message and amended.\n. LGTM despite tests failing.\n. @rculbertson \n. :+1: \n. cc @rohansingh @rculbertson @Kobolog @drewcsillag \n. @rohansingh These are from jenkins-helios.spotify.net. I can collect more data from other successful builds.\nI've noticed our successful builds are now at least 20min :(\n. ## build # | link to test durations\n61 http://pastebin.com/9VAi3FJ1\n62 http://pastebin.com/x7mrzh8n\n63 http://pastebin.com/LVj2yv37\n64 http://pastebin.com/GZebqUVB\n65 http://pastebin.com/fVtuqQ0a\n. ameliorated by https://github.com/spotify/helios/pull/269\n. After merging https://github.com/spotify/helios/pull/269:\n280.01 sec - in com.spotify.helios.system.ZooKeeperHeliosFailoverTest\n157.488 sec - in com.spotify.helios.system.JobExpirationTest\n150.945 sec - in com.spotify.helios.servicescommon.coordination.ZooKeeperUpdatingPersistentDirectoryTest\n141.922 sec - in com.spotify.helios.system.TerminationTest\n138.37 sec - in com.spotify.helios.system.ClusterDeploymentTest\n137.942 sec - in com.spotify.helios.system.DeploymentTest\n135.741 sec - in com.spotify.helios.system.FlappingTest\n132.292 sec - in com.spotify.helios.system.JobHistoryTest\n131.148 sec - in com.spotify.helios.testing.TemporaryJobsTest\n85.076 sec - in com.spotify.helios.system.ZooKeeperRestoreTest\n66.797 sec - in com.spotify.helios.system.AgentZooKeeperDownTolerationTest\n64.752 sec - in com.spotify.helios.servicescommon.coordination.PersistentPathChildrenCacheTest\n41.16 sec - in com.spotify.helios.system.ReapingTest\n38.36 sec - in com.spotify.helios.system.MultiplePortJobTest\n35.872 sec - in com.spotify.helios.system.DnsServerTest\n32.239 sec - in com.spotify.helios.system.AgentRestartTest\n31.324 sec - in com.spotify.helios.system.CliJobCreationTest\n28.923 sec - in com.spotify.helios.system.QueryFailureTest\n28.499 sec - in com.spotify.helios.system.ContainerHostNameTest\n28.43 sec - in com.spotify.helios.system.VersionResponseFilterTest\n27.256 sec - in com.spotify.helios.system.ZooKeeperClusterIdTest\n26.42 sec - in com.spotify.helios.system.CliDeploymentTest\n23.53 sec - in com.spotify.helios.system.VolumeTest\n22.26 sec - in com.spotify.helios.agent.QueueingHistoryWriterTest\n21.345 sec - in com.spotify.helios.system.UndeployFilteringTest\n19.273 sec - in com.spotify.helios.system.VersionCommandTest\n18.984 sec - in com.spotify.helios.system.EnvironmentVariableTest\n18.81 sec - in com.spotify.helios.testing.ProberTest\n16.186 sec - in com.spotify.helios.system.DeregisterTest\n14.753 sec - in com.spotify.helios.system.ConfigFileJobCreationTest\n14.057 sec - in com.spotify.helios.system.JobWatchTest\n13.612 sec - in com.spotify.helios.system.NamespaceTest\n13.158 sec - in com.spotify.helios.system.JobServiceRegistrationTest\n12.788 sec - in com.spotify.helios.system.JobWatchExactTest\n12.032 sec - in com.spotify.helios.system.PredefinedPortImageDeploymentTest\n11.129 sec - in com.spotify.helios.testing.ConfigTest\n11.088 sec - in com.spotify.helios.system.ImageMissingTest\n10.747 sec - in com.spotify.helios.system.JobListTest\n10.606 sec - in com.spotify.helios.ZooKeeperMasterModelIntegrationTest\n9.458 sec - in com.spotify.helios.system.PortCollisionJobTest\n9.262 sec - in com.spotify.helios.system.MasterServiceRegistrationTest\n9.256 sec - in com.spotify.helios.system.ZooKeeperNamespacingTest\n9.213 sec - in com.spotify.helios.system.AgentReportingTest\n9.019 sec - in com.spotify.helios.system.UndeployRaceTest\n8.748 sec - in com.spotify.helios.system.IdMismatchJobCreateTest\n8.336 sec - in com.spotify.helios.system.CliMasterListTest\n8.116 sec - in com.spotify.helios.servicescommon.coordination.ZooKeeperHealthCheckerTest\n6.135 sec - in com.spotify.helios.system.MasterRespondsWithNoZKTest\n4.769 sec - in com.spotify.helios.system.AgentStateDirConflictTest\n2.977 sec - in com.spotify.helios.agent.SupervisorTest\n2.626 sec - in com.spotify.helios.agent.GracePeriodTest\n1.884 sec - in com.spotify.helios.agent.AgentTest\n1.719 sec - in com.spotify.helios.servicescommon.SentryTest\n1.496 sec - in com.spotify.helios.agent.TaskMonitorTest\n1.351 sec - in com.spotify.helios.master.ExpiredJobReaperTest\n1.34 sec - in com.spotify.helios.agent.TaskRunnerTest\n1.236 sec - in com.spotify.helios.system.MasterResolutionFailureMessageTest\n1.235 sec - in com.spotify.helios.servicescommon.ZooKeeperRegistrarTest\n1.205 sec - in com.spotify.helios.agent.DockerHealthCheckerTest\n0.975 sec - in com.spotify.helios.testing.DefaultDeployerTest\n0.861 sec - in com.spotify.helios.common.descriptors.JobTest\n0.828 sec - in com.spotify.helios.common.descriptors.TaskStatusTest\n0.805 sec - in com.spotify.helios.servicescommon.ResolverConfReaderTest\n0.785 sec - in com.spotify.helios.testing.TempJobsProfileOverrideTest\n0.758 sec - in com.spotify.helios.servicescommon.RiemannHeartBeatTest\n0.754 sec - in com.spotify.helios.common.JobValidatorTest\n0.707 sec - in com.spotify.helios.agent.MonitoredDockerClientTest\n0.647 sec - in com.spotify.helios.cli.CliParserTest\n0.591 sec - in com.spotify.helios.common.JsonTest\n0.588 sec - in com.spotify.helios.cli.CliConfigTest\n0.546 sec - in com.spotify.helios.common.descriptors.JobIdTest\n0.417 sec - in com.spotify.helios.agent.FlapControllerTest\n0.266 sec - in com.spotify.helios.agent.PortAllocatorTest\n0.219 sec - in com.spotify.helios.servicescommon.NoOpRiemannClientTest\n0.212 sec - in com.spotify.helios.common.context.ContextTest\n0.201 sec - in com.spotify.helios.common.PomVersionTest\n0.189 sec - in com.spotify.helios.cli.OutputTest\n0.174 sec - in com.spotify.helios.cli.command.HostResolverTest\n0.164 sec - in com.spotify.helios.testing.HostPickingStrategiesTest\n0.104 sec - in com.spotify.helios.common.VersionCompatibilityTest\n0.06 sec - in com.spotify.helios.system.SyslogRedirectionTest\n0.057 sec - in com.spotify.helios.system.ZooKeeperCuratorFailoverTest\n. Fixed by https://github.com/spotify/helios/pull/260\n. When I run tests without this commit, I get these leftover containers:\nCONTAINER ID        IMAGE                       COMMAND                CREATED             STATUS              PORTS                     NAMES\n24d48a7c1b79        busybox:buildroot-2014.02   \"sh -c 'while :; do    6 minutes ago       Up 6 minutes                                  helios-F96CA98C2B50CC2201E45FF860829EE48CF036CB-tmp-20141102-76a141b8_busybox_47738452_9e4cf1f_fdcebb8e\n0b84e9dc23ad        busybox:buildroot-2014.02   \"sh -c 'while :; do    6 minutes ago       Up 6 minutes                                  helios-F96CA98C2B50CC2201E45FF860829EE48CF036CB-tmp-20141102-76a141b8_busybox_394f15d6_d8b2580_8b784c00\n48da914b37ec        busybox:buildroot-2014.02   \"sh -c 'while :; do    7 minutes ago       Up 7 minutes                                  helios-086ED8D6845D924E0B0BAF5966F4D523A13DD689-tmp-20141102-2225f6f1_busybox_cc3cff4e_d15b72c_36056bd6\n77e9597caf69        busybox:buildroot-2014.02   \"nc -p 4711 -lle cat   7 minutes ago       Up 7 minutes        0.0.0.0:30733->4711/tcp   helios-086ED8D6845D924E0B0BAF5966F4D523A13DD689-tmp-20141102-2225f6f1_busybox_35af9aad_d869dcd_5fdb276e\nf958e0a422cb        busybox:buildroot-2014.02   \"sh -c 'while :; do    13 minutes ago      Up 13 minutes       0.0.0.0:24851->4712/tcp   helios-D8BE6B7F5A9631A9858D0C6E60492C7AEC4279E6-tmp-20141102-db5e8e3d_busybox_c20a792b_f560693_dd5e4190\ndbe795e82787        busybox:buildroot-2014.02   \"sh -c 'while :; do    13 minutes ago      Up 13 minutes       0.0.0.0:24850->4711/tcp   helios-D8BE6B7F5A9631A9858D0C6E60492C7AEC4279E6-tmp-20141102-db5e8e3d_busybox_505fcbdf_bf7e59a_f65662fb\n. @carlanton Helios system tests are quite intensive. So CircleCI's build agents often spuriously fail. I've triggered them to see if they'll pass since the error log looks like a spurious failure I've seen before.\n. @carlanton yay external contributors \\o/\n. @carlanton Yea, I feel your pain with CircleCI. I'm running your branch with mvn clean test on my mac with boot2docker now.\n. Tests passed for me.\n. \\o/\n. @rculbertson @rohansingh \nall seems confusing. tools or cli might be better\n. @rohansingh found another one\n. :+1: \n. @rohansingh \n. duplicate of https://github.com/spotify/helios/pull/269\n. :+1: \n. @alejandrojnm Thanks for reaching out. This is a perfect place to ask.\nThis is definitely something we at Spotify are interested in making. We've only just started brainstorming about what a helios web UI would look like. Some things we envision end users wanting are\n- ability to inspect jobs and hosts from multiple domains\n- ability to see which jobs are deployed to which hosts\n- a history of a job's deployments and history of a host's deployments\n- ease in creating jobs and deploying them\nOthers on my team can add anything I've missed here.\nI've begun a personal hack project here https://github.com/davidxia/helios-web-ui. I decided to go with dropwizard, angular.js, and bootstrap. I wanted the web UI to be able to read not only from our Helios masters, but our internal docker registry (v0.8.1) so that we can let users see docker images.\nDropwizard is the backend which agreggates data from multiple helios domains and in some cases is needed to set CORS headers that the docker registry doesn't set, angular provides the front-end functionality, and bootstrap makes it look nice and responsive right off the bat.\nLet us know what you think. It's hardcoded right now to talk to our internal data centers which are behind VPN. There's probably some work involved in setting up a dummy helios backend to supply fake data though.\n. That looks pretty neat. Is it just a mockup so far or do you have code that\nI can run locally?\nOn Thu, Nov 13, 2014 at 10:28 AM, alejandrojnm notifications@github.com\nwrote:\n\nHello @davidxia https://github.com/davidxia I have been working on the\nGUI and this is the idea so far, I hope I your opinion on this, I would\nhave to remove or put something [image: Helios Manager]\nhttps://camo.githubusercontent.com/a46cc003bd0959c1047189cb845fd7d32adf4643/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f4232564c715244494541417253475a2e6a70673a6c61726765\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/issues/283#issuecomment-62906808.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. @alejandrojnm sorry i haven't gotten back to you in such a long time. Things have been busy. I can check this out soon.\n. It doesn't. It just needs to be able to talk to the Helios master http api\nOn Wednesday, October 14, 2015, Konrad notifications@github.com wrote:\n\nAwesome. Does this UI need to run on the same instance as the Helios\nmaster?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/issues/283#issuecomment-148255222.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. :+1: \n. You want the helios API to return helios agents' IP addresses?\n. @alejandrojnm Do you still have questions about this or can I close this issue?\n. my system tests hang when doing mvn clean test.\nIt's some test that runs after ZooKeeperRestoreTest\ndocker ps shows\nCONTAINER ID        IMAGE                       COMMAND                CREATED             STATUS              PORTS               NAMES\n5d5928f5d389        busybox:buildroot-2014.02   \"sh -c 'trap 'exit 0   46 minutes ago      Up 43 minutes                           helios-5725F0C9C85D3C1A49E1CDB8633C13D50D281813-job_test_d7c01236_v224f8d73_afaf255_41b194f2\nce7fcd3da891        busybox:buildroot-2014.02   \"sh -c 'trap 'exit 0   46 minutes ago      Up 43 minutes                           helios-967624526CFB7B796DB78A8FF51090824CD0B2D9-job_test_690a3309_v100621bb_b5adef8_28aba098\nNothing is showing up in docker log\n. @drewcsillag works now. Took me about 17 min\n. @rculbertson \n. Fixes https://github.com/spotify/helios/issues/293\n. @rohansingh \n. :+1: \n. :+1: \n. @rohansingh \n. A minor breaking change. Let me know if this is worth it.\n. Now we keep the old and emit warning about deprecation.\n. @vanomashey I see you're using helios 0.8.108. Does this still happen with the latest release 0.8.125?\n. @vanomashey Is this still an issue?\n. Good. I'll close this then. Let us know if you have more questions.\n. @rculbertson @rohansingh \n. @jypma I see that our docker-client can set the X-Registry-Auth header on docker pull and push.\nI don't think helios calls authConfig() which sets the authConfig attribute in PollingDockerClient that extends DefaultDockerClient.\nWe are not using X-Registry-Auth. I'm wondering how this would work. We'd probably want the Helios agent to be able to pull from different registries. So we'd want to set different auth headers depending on where the docker-client is pulling from, ie depending on the docker image ID.\nThis is where I'm not sure how it'd work. Based on the current design of Helios code, one place to put the auth creds is in the Helios job. When the Helio sagent deploys or starts a job, it reads in the auth creds and passes that to docker-client which can then pull the image from the private registry.\nI'm not sure it's ok to have auth creds in plaintext in a Helios job though...\n. Yea I realized this too. Are you simply using docker without Helios and\nwith a private registry requiring auth?. @jypma totally agree with your points on option 3 above. Sorry, I've been punting on this for a while because you're the first to ask for this. @Kobolog What do you think?\n. @Kobolog @rculbertson I stubbed out a helios-secret-provider module that lets you plugin secret providers. It's the same pattern as helios-service-registration if we want to take that approach. https://github.com/spotify/helios/tree/dxia/secret-provider\n. @Kobolog @jypma I just added per pull auth headers to docker-client https://github.com/spotify/docker-client/pull/118\n. Hi @rhudson. Helios has the latest docker-client which supports HTTPS connections to Docker, but I'm not sure if/when we'll have Helios use this.\nPerhaps @rohansingh has a better idea.\n. Looks good to me, but I haven't tested this.\n. :+1: \n. :+1: \n. @gtonic Thank you for the heads up. We didn't mean to release this version on github but just internally. Just use 0.8.129 for now, and we'll fix our end.\n. :+1: \n. :+1: \n. :+1: \n. @perfa Thanks for this. Were you also running helios version 0.8.130 on some.fqdn.net? Do you have logs from that host or any idea of what the response was that caused this? Wondering how I can repro this.\n. @rculbertson \n. @rculbertson \n. :+1: \n. :+1: \n. Can we close since this is merged or are there other things to do?\n. @rculbertson \n. Yea I agree.\nOn Sunday, January 25, 2015, Ryan Culbertson notifications@github.com\nwrote:\n\nI think it would be reasonable to show what version of docker each build\nwas tested against. Showing a range of versions might be tough unless we\nstart testing against multiple versions.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/issues/342#issuecomment-71381376.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. :+1: \n. @noaresare Thanks for this!\n. :+1: \n. :+1: \n. :+1: Does this add a little more time to our builds? Took 11:23min for me to mvn clean package\n. yea that sounds like a good idea\n. @rculbertson \n. :+1: \n. @rculbertson \n. :+1: \n. @philipvonbargen I fixed the typos\n. TODO add registration state like task states\n. @philipvonbargen closing in favor of https://github.com/spotify/helios/pull/398\n. Wow good find. I took a look at the at your fix and it seems sensible. I didn't look at zookeeper mailing lists and change log though. I'll defer to you on that. It is a bit scary it's an alpha though...\n. In that case :+1: \n. @Kobolog not sure.\nWe should prohibit replacement of a registered agent that's UP and require explicit deregistration first. Only allow replacement of registered agents that are DOWN.\n. @rasmusbergpalm My tests in this PR https://github.com/spotify/helios/issues/382 above keep failing. I haven't had time to fix it though.\n. I noticed 2 other places in DefaultZooKeeperClient where there's client.getZookeeperClient: getState() and ensurePath(). I'm not that familiar with namespaces, but I guess those 2 places are taken care of already?\n. cool, in that case :+1: \n. :+1: \n. Oh interesting. I see here:\n\nThe tags is a list of opaque value to Consul, but can be used to distinguish between \"master\" or \"slave\" nodes, different versions, or any other service level labels \n\nDo you know how many people are using helios-consul?\n. @carlanton really cool to see you guys using helios!\n. Maybe add some more tests like these https://github.com/spotify/helios/pull/386?\n@rohansingh I didn't know how to test for bullets 2 and 3 here though.\n. :+1: \n. :+1: \n. I'm wondering how it would it break users if we went with choice 1?\n. :+1: \n. @rculbertson We'll also need to update circle.yml to use jdk 1.8 https://github.com/spotify/helios/pull/384/files#diff-29944324a3cbf9f4bd0162dfe3975d88R18. See https://github.com/spotify/helios/pull/384\n. :+1: \n. TODO write tests\n@rculbertson What do you think of this approach?\n. Users like this guy! @dflemstr\n. The tests fail for some reason...\nOn Tue, Oct 6, 2015 at 9:59 AM, Matt Brown notifications@github.com wrote:\n\n@davidxia https://github.com/davidxia still interested in doing this?\nLet me know if you want any help in resolving issues.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/384#issuecomment-145864459.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. awesome the tests pass now. Would be great to see this merged once the big auth work is done\n. @mattnworb @gimaker Is there anything left to do code-wise here? Would be great to merge this.\n. @mattnworb Sounds great. Sorry, I rebased on master and forgot about they stray commit. I'll remove that now.\n. Any reason we didn't run these 2 tests before?\n. :+1: \n. Wow, nice catch. How did you debug this? bc I couldn't figure this out.\n. :+1: This is great. Some suggestions here https://github.com/spotify/helios/pull/402. Perhaps someone else should review as well. A lot of code in here.\n. @evgeny-goldin FYI, we took a different approach.\n. @rohansingh some more nitpicky edits for you :)\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. Not sure why CircleCI failed. Rerunning. LGTM though.\n. :+1: \n. :+1: \n. ```\nhelios inspect -d DOMAIN helioscanary-service:0.0.1-SNAPSHOT-27b096a:342e49e\nId: helioscanary-service:0.0.1-SNAPSHOT-27b096a:342e49e17188abc5e96cce5febc1849362f0a819\nImage: helioscanary-service:0.0.1-SNAPSHOT-27b096a\nCommand: []\nEnv:\nHealth check: type: http, port: http, path: /healthcheck\nGrace period (seconds): null\nPorts: hm=5700/tcp\n       http=8080/tcp\nReg: helioscanary/hm=hm\nToken:\nVolumes: /etc/spotify:/etc/spotify\n         /spotify:/spotify\nhelios --version\nSpotify Helios CLI 0.8.213\n```\nMaybe there's an incompatiblity with that job?\n. :+1:\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. @rohansingh \n. :+1: \n. @rasmusbergpalm Thanks, great to see it's useful for you. Waiting for all instances of a job to be RUNNING seems like a common use case regardless of health checks. I'm not sure how it should be implemented right now, but we'll keep this in mind.\n. Yea I like this idea.\nOn Thu, Mar 12, 2015 at 1:26 PM, Rohan Singh notifications@github.com\nwrote:\n\nMaybe could be an extension to helios watch, to stop watching when all\nstatuses are RUNNING.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/issues/437#issuecomment-78537435.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. LGTM. I'm also checking i can mvn package locally\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. we don't need spotify domains?\n. oh i see. it's for registration :+1: \n. :+1: \n. good point, we'll put this in our TODO\nOn Mon, Mar 23, 2015 at 11:31 AM, Evgeny Goldin notifications@github.com\nwrote:\n\nThe error was caused by broken JSON at \"~/.helios/config\" of the \"root\"\nuser. What could be done, though, is a more explicit error of what went\nwrong and a path to the JSON file that couldn't be read.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/issues/457#issuecomment-85050829.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. @jypma Closing as this issue is fixed by https://github.com/spotify/helios/pull/461.\n. @carlanton I've retriggered CircleCI (I think you should be able to as well). We'll see if it passes this time.\n. :+1: seems more likely this will happen the more jobs there are.\n. @nexus49 Our docker-client supports private registries (see docs here), but helios isn't setup to use that functionality yet.\nWe haven't built this yet bc we haven't needed it and we're quite busy with other things at the moment. But we're open to a PR. The place to change is here. dockerClient needs to be instantiated with the private registry's auth credentials.\n. @andredasilvapinto Thanks for your feedback. I feel like the docs are already clear about this since it states \"Ensure you have installed Docker and have configured it correctly so that commands like docker info work.\" If docker isn't running, docker info gives you \"2015/04/01 15:55:16 Cannot connect to the Docker daemon. Is 'docker -d' running on this host?\"\nMaybe we just add emphasis to this sentence in case it's easy to miss?\n. Would docker ps instead of docker info be a better test command?\n. @rculbertson \n. :+1: \n. @rohansingh @rculbertson @gimaker What do you think?\n. compare with https://github.com/spotify/helios/pull/472\n. @rohansingh Should we just remove the name method? https://github.com/spotify/helios/pull/511\nI also ran into an NPE when trying to repro: https://github.com/spotify/helios/pull/510\n. @mbruggmann Hey, I'll be able to look at this soon. Been pretty busy lately.\n. @rohansingh I think @mbruggmann is on vacation. I made some minor edits here https://github.com/mbruggmann/helios/pull/1/files. Not sure when he's back.\n. @rohansingh I squashed into 1 commit here https://github.com/spotify/helios/pull/482\n. superseded by https://github.com/spotify/helios/pull/482\n. :+1:\n. @rohansingh \n. :+1: \n. @rohansingh FlappingTest still fails a lot on CircleCI with TimeoutException. I'll open another PR and try changing FlapController instead.\n. and..another circleci failure. See https://github.com/spotify/helios/pull/483\n. closed by https://github.com/spotify/helios/pull/484\n. @rculbertson @gimaker \n. This is a slightly ugly fix for this test that always times out. It times out because the container doesn't exit 10 times in 200 seconds. Instead of increasing the timeout which will increase our test duration, I just changed the field to be public and non-final and alter it before the test starts.\nWe could have a set method for that field in the FlapController, but then we'd need to figure out a way to pass some value through a dozen layers of factories and builders and other crap.\n. Longer timeout here https://github.com/spotify/helios/pull/484\n. closed by https://github.com/spotify/helios/pull/484\n. @rohansingh \n. Nice catch @mattnworb. I agree we should fix docs for now and not break anything.\n. @rohansingh \n. @rohansingh @gimaker \n. yea\n. Yea I'm OK with failing too.\n. :+1:\n. @damoon You're right. Thanks for the catch. I started stubbing this out here https://github.com/spotify/helios/pull/596\n. @rohansingh since you commented on the issue :)\n. @gimaker If you're talking about the extra stuff I added here https://github.com/spotify/helios/pull/494/files#diff-f2d9b1d73d2606703030267fe9e52573R89, I wanted to test that a valid config file was parsed correctly.\n. @gimaker \n. @gimaker \n. @gimaker \n. @rohansingh\n. :+1:\n. :+1: \n. @rculbertson @rohansingh @gimaker \n. @rohansingh I guess so? Closing then\n. Yea the HealthCheckTest failed. I'm assuming that's unrelated.\n. @rohansingh \n. @gimaker \n. @rculbertson \n. @rohansingh We log.debug here https://github.com/spotify/helios/pull/516/files#diff-8d500c531ffb1f7836e9d5b60585a002R850. Should we increase the log level?\nBad data shouldn't just be written, but bad Jobs are getting into zookeeper even with the job validation on server side somehow.\n. @rculbertson \n. @gilles\n. @rohansingh Also here? https://github.com/spotify/helios/blob/master/helios-system-tests/src/main/java/com/spotify/helios/system/PredefinedPortImageDeploymentTest.java#L54\notherwise :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: After a quick read through. Maybe use more immutable lists? :)\n. :+1:\n. @rohansingh @gimaker Ignore all the unknowns?\n. @rohansingh @gimaker \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. @gimaker @rohansingh @rculbertson \n. PR for this https://github.com/spotify/helios/pull/650, but I closed it as it because no one wanted it besides me and it might confuse users.\n. @gimaker @rohansingh \n. no strong preferences here. If we're still not super committed to these command names, we can warn early adopters that these names could change for now. :+1: \n. :+1: \n. @gtonic Thanks for this. We used to set hostnames but removed that when upgrading to docker 1.3. The reason was that somewhere our code was generating a long hash and concatenating that with the hostname the user specified. In many cases, this concatenated string exceeded the hostname character limit of the operating system and the container wouldn't start.\nWe also didn't need to set the hostname. And so we removed it.\nSee https://github.com/spotify/helios/commit/f8885cdf9feae4c732a1e1f8da017d8ca7486c38 and https://github.com/spotify/helios/pull/295/files. We could try doing this again, but we'd need to check the length.\n. @gtonic Ah sorry I misunderstood. Should've actually looked at your code. I think giving the user the ability to set the hostname is reasonable. Thoughts, @rohansingh?\nI left some comments inline.\n. @gtonic Thanks for this. I had a few more comments. I also don't know why circleCI is using your configuration instead of ours which can do parallelism 5.\n. @gtonic Here are some system tests to add that actually start a container and check the hostname is set from within the container with hostname -f.\n```\ncommit 4514bc81e8024991408f3db222aaf3911c8904d2\nAuthor: David Xia david@davidxia.com\nDate:   Thu Aug 6 09:32:30 2015 -0400\nAdd system tests for setting container hostname\n\ndiff --git a/helios-system-tests/src/main/java/com/spotify/helios/system/ContainerHostNameTest.java b/helios-system-tests/src/main/java/com/spotify/helios/system/ContainerHostNameTest.java\nnew file mode 100644\nindex 0000000..cd70d30\n--- /dev/null\n+++ b/helios-system-tests/src/main/java/com/spotify/helios/system/ContainerHostNameTest.java\n@@ -0,0 +1,77 @@\n+/\n+ * Copyright (c) 2014 Spotify AB.\n+ \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ /\n+\n+package com.spotify.helios.system;\n+\n+import com.spotify.docker.client.DockerClient;\n+import com.spotify.docker.client.LogStream;\n+import com.spotify.helios.common.descriptors.Job;\n+import com.spotify.helios.common.descriptors.JobId;\n+import com.spotify.helios.common.descriptors.TaskStatus;\n+\n+import org.junit.Test;\n+\n+import java.util.List;\n+\n+import static com.spotify.docker.client.DockerClient.LogsParameter.STDERR;\n+import static com.spotify.docker.client.DockerClient.LogsParameter.STDOUT;\n+import static com.spotify.helios.common.descriptors.HostStatus.Status.UP;\n+import static com.spotify.helios.common.descriptors.TaskStatus.State.EXITED;\n+import static java.util.Arrays.asList;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.containsString;\n+\n+public class ContainerHostNameTest extends SystemTestBase {\n+\n+  @Test\n+  public void testValidHostname() throws Exception {\n+    startDefaultMaster();\n+    startDefaultAgent(testHost());\n+    awaitHostStatus(testHost(), UP, LONG_WAIT_SECONDS, SECONDS);\n+\n+    try (final DockerClient dockerClient = getNewDockerClient()) {\n+\n+      final List command = asList(\"hostname\", \"-f\");\n+\n+      // Create job\n+      final JobId jobId = createJob(Job.newBuilder()\n+                                .setName(testJobName)\n+                                .setVersion(testJobVersion)\n+                                .setImage(BUSYBOX)\n+                                .setHostname(testHost())\n+                                .setCommand(command)\n+                                .build());\n+\n+      // deploy\n+      deployJob(jobId, testHost());\n+\n+      final TaskStatus taskStatus = awaitTaskState(jobId, testHost(), EXITED);\n+\n+      final String log;\n+      try (final LogStream logs = dockerClient.logs(taskStatus.getContainerId(), STDOUT, STDERR)) {\n+        log = logs.readFully();\n+      }\n+\n+      assertThat(log, containsString(testHost()));\n+    }\n+  }\n+}\ndiff --git a/helios-system-tests/src/main/java/com/spotify/helios/system/JobCreateTest.java b/helios-system-tests/src/main/java/com/spotify/helios/system/JobCreateTest.java\nnew file mode 100644\nindex 0000000..6c2e929\n--- /dev/null\n+++ b/helios-system-tests/src/main/java/com/spotify/helios/system/JobCreateTest.java\n@@ -0,0 +1,51 @@\n+/\n+ * Copyright (c) 2014 Spotify AB.\n+ \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ /\n+\n+package com.spotify.helios.system;\n+\n+import com.spotify.helios.common.descriptors.Job;\n+\n+import org.hamcrest.Matchers;\n+import org.junit.Test;\n+\n+import static com.spotify.helios.common.descriptors.HostStatus.Status.UP;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+import static org.junit.Assert.assertThat;\n+\n+public class JobCreateTest extends SystemTestBase {\n+\n+  @Test\n+  public void testInvalidHostname() throws Exception {\n+    startDefaultMaster();\n+    startDefaultAgent(testHost());\n+    awaitHostStatus(testHost(), UP, LONG_WAIT_SECONDS, SECONDS);\n+\n+    // Create job\n+    final String output = createJobRawOutput(Job.newBuilder()\n+                                                 .setName(testJobName)\n+                                                 .setVersion(testJobVersion)\n+                                                 .setImage(BUSYBOX)\n+                                                 .setHostname(\"$%^&\")\n+                                                 .build());\n+    assertThat(output, Matchers.containsString(\"Invalid hostname \"));\n+\n+  }\n+}\ndiff --git a/helios-system-tests/src/main/java/com/spotify/helios/system/SystemTestBase.java b/helios-system-tests/src/main/java/com/spotify/helios/system/SystemTestBase.java\nindex db35024..7b392c0 100644\n--- a/helios-system-tests/src/main/java/com/spotify/helios/system/SystemTestBase.java\n+++ b/helios-system-tests/src/main/java/com/spotify/helios/system/SystemTestBase.java\n@@ -738,6 +738,13 @@ public abstract class SystemTestBase {\n   }\nprotected JobId createJob(final Job job) throws Exception {\n+    final String createOutput = createJobRawOutput(job);\n+    final String jobId = WHITESPACE.trimFrom(createOutput);\n+\n+    return JobId.fromString(jobId);\n+  }\n+\n+  protected String createJobRawOutput(final Job job) throws Exception {\n     final String name = job.getId().getName();\n     checkArgument(name.contains(testTag), \"Job name must contain testTag to enable cleanup\");\n@@ -746,10 +753,7 @@ public abstract class SystemTestBase {\n     Files.write(serializedConfig, configFile, Charsets.UTF_8);\n final List<String> args = ImmutableList.of(\"-q\", \"-f\", configFile.getAbsolutePath());\n\n\nfinal String createOutput = cli(\"create\", args);\n\nfinal String jobId = WHITESPACE.trimFrom(createOutput);\n\nreturn JobId.fromString(jobId);\nreturn cli(\"create\", args);\n   }\n\nprotected void deployJob(final JobId jobId, final String host)\ndiff --git a/helios-tools/src/main/java/com/spotify/helios/cli/command/JobCreateCommand.java b/helios-tools/src/main/java/com/spotify/helios/cli/command/JobCreateCommand.java\nindex 4a0e0e6..135f1e8 100644\n--- a/helios-tools/src/main/java/com/spotify/helios/cli/command/JobCreateCommand.java\n+++ b/helios-tools/src/main/java/com/spotify/helios/cli/command/JobCreateCommand.java\n@@ -21,41 +21,11 @@\npackage com.spotify.helios.cli.command;\n-import static com.google.common.base.Charsets.UTF_8;\n-import static com.google.common.base.Optional.fromNullable;\n-import static com.google.common.base.Strings.isNullOrEmpty;\n-import static com.spotify.helios.common.descriptors.PortMapping.TCP;\n-import static com.spotify.helios.common.descriptors.ServiceEndpoint.HTTP;\n-import static java.util.Arrays.asList;\n-import static java.util.regex.Pattern.compile;\n-import static net.sourceforge.argparse4j.impl.Arguments.append;\n-import static net.sourceforge.argparse4j.impl.Arguments.fileType;\n-import static net.sourceforge.argparse4j.impl.Arguments.storeTrue;\n-\n-import java.io.BufferedReader;\n-import java.io.File;\n-import java.io.IOException;\n-import java.io.PrintStream;\n-import java.nio.file.Files;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Collection;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.concurrent.ExecutionException;\n-import java.util.regex.Matcher;\n-import java.util.regex.Pattern;\n-\n-import net.sourceforge.argparse4j.inf.Argument;\n-import net.sourceforge.argparse4j.inf.Namespace;\n-import net.sourceforge.argparse4j.inf.Subparser;\n-\n-import org.joda.time.DateTime;\n-\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n+\n import com.spotify.helios.client.HeliosClient;\n import com.spotify.helios.common.JobValidator;\n import com.spotify.helios.common.Json;\n@@ -69,6 +39,37 @@ import com.spotify.helios.common.descriptors.ServicePorts;\n import com.spotify.helios.common.descriptors.TcpHealthCheck;\n import com.spotify.helios.common.protocol.CreateJobResponse;\n+import net.sourceforge.argparse4j.inf.Argument;\n+import net.sourceforge.argparse4j.inf.Namespace;\n+import net.sourceforge.argparse4j.inf.Subparser;\n+\n+import org.joda.time.DateTime;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.PrintStream;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Charsets.UTF_8;\n+import static com.google.common.base.Optional.fromNullable;\n+import static com.google.common.base.Strings.isNullOrEmpty;\n+import static com.spotify.helios.common.descriptors.PortMapping.TCP;\n+import static com.spotify.helios.common.descriptors.ServiceEndpoint.HTTP;\n+import static java.util.Arrays.asList;\n+import static java.util.regex.Pattern.compile;\n+import static net.sourceforge.argparse4j.impl.Arguments.append;\n+import static net.sourceforge.argparse4j.impl.Arguments.fileType;\n+import static net.sourceforge.argparse4j.impl.Arguments.storeTrue;\n+\n public class JobCreateCommand extends ControlCommand {\nprivate static final JobValidator JOB_VALIDATOR = new JobValidator();\n@@ -300,7 +301,10 @@ public class JobCreateCommand extends ControlCommand {\n       builder.setImage(imageIdentifier);\n     }\n\nbuilder.setHostname(options.getString(hostnameArg.getDest()));\nfinal String hostname = options.getString(hostnameArg.getDest());\nif (!isNullOrEmpty(hostname)) {\nbuilder.setHostname(hostname);\n\n}\nfinal List command = options.getList(argsArg.getDest());\n if (command != null && !command.isEmpty()) {\n```\n. @gtonic If you could also squash everything down into 1 commit that would be great.\n. @gtonic I can take care of the squash and rebase then in another PR. Thanks again.\n. superseded by https://github.com/spotify/helios/pull/612\n. :+1: Yea spending some time to create tests for these edge cases will be good to do in the future.\n. @rohansingh Let me know which edits help in making it more readable.\n. @gimaker \n. :+1: \n. :+1: \n. :+1: \n. @rohansingh \n. @rohansingh @gimaker @rculbertson \n. @rohansingh @gimaker \n\n\nI was thinking about how to use composition instead of inheritance here. We could have TaskHistoryWriter have QueuingHistoryWriter as an attribute. QueuingHistoryWriter wouldn't be abstract, and we'd remove the generic TEvent and just have it write arrays of bytes to ZK (as @rohansingh suggested today).\nThe abstract method getKey() would instead be implemented to return a checksum of the byte array. But what about getTimestamp() which TaskHistoryWriter implemented in a way to return the timestamp attribute of the TaskStatusEvent? If QueuingHistoryWriter naively wrote byte[] to ZK, it wouldn't be able to know the timestamp or ZK path.\nWe could have a QueuingHistoryEvent interface with getTimestamp() and getZkEventsPath() and have QueuingHistoryWriter.add() take that type as an argument, but I'm not sure what the advantage of that is over the current design.\n. :+1: \n. @gimaker Should these tests be uncommented?\n. @gimaker @rohansingh @rculbertson \n. @rohansingh \n. @rculbertson \n. :+1: \n. :+1: \n. @rohansingh @gimaker @rculbertson \n. :+1: \n. replaced by https://github.com/spotify/helios/pull/583\n. It's open. Yea I changed the status a bunch.\n. testing this out\n. @rculbertson @rohansingh \n. @rohansingh @danielnorberg \n. @rculbertson \n. @gimaker \n. @gimaker \n. Hm now I'm thinking this whole filtering thing should be done server-side.\n. Yea I think server-side will have better performance and easier to maintain.\nOn Tue, Jul 28, 2015 at 12:53 PM, Staffan Gim\u00e5ker notifications@github.com\nwrote:\n\n[image: :+1:]\nNo strong opinions on server/client side filtering for this case. That\nsaid, I'm tempted to move the label filtering to the server-side so that it\nworks when --json is specified (or, alternatively, just do the label\nfiltering on the client-side as well).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/587#issuecomment-125677685.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. @gimaker I moved the logic to the server.\n. @rohansingh @gimaker There are probably a few more cases where we can optimize the cli commands and move logic into the server side.\n. @gimaker How does it look now?\n. :+1: \n. @rohansingh \n. Yea, it's a lot more info. We wanted the error message from DeploymentGroupStatus and were lazy...\n. Thanks. Let's merge for now and change later as needed.\n. @rohansingh yay, finally fixed the tests. See comments inline.\n. @rohansingh tests passed for me locally\n. :+1: \n. @rohansingh @gimaker Any reason we weren't doing this before?\n. Yea, I figured if the host didn't exist as a result of a race, the rolling update would just time out waiting for the job to reach running. Why did we do it in getUndeployOperations()?\n. @rohan?\nOn Wed, Jul 29, 2015 at 1:23 PM, Staffan Gim\u00e5ker notifications@github.com\nwrote:\n\nI have no clue to be honest.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/594#issuecomment-126025826.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. @rohan Haha, sorry. @rohansingh ^\n. This is useful in debugging our users' failed tempjobs. I often want to see where their client is connecting. This won't print to console when you run helios ....\n. @gimaker ^\n. @rohansingh @gimaker \n. :+1: \n. This is a slightly related question. Should helios status --job jobID show hosts that are DOWN?\nJOB ID                                HOST                                       GOAL     STATE            CONTAINER ID    PORTS\nhelioscanary:0.8.0-74b6afb:3d335f4    host1              START    RUNNING          b4d570e         hm=20000:20000 http=20001:20001\nhelios hosts host1 will show it as down.\nHOST                             STATUS        DEPLOYED    RUNNING    CPUS    MEM     LOAD AVG    MEM USAGE    OS                         HELIOS     DOCKER          LABELS\nhost1    Down 1 day    1           1          1       3 gb    0.96        0.87         Linux 3.13.0-34-generic    0.8.447    1.6.0 (1.18)    helioscanary=true, role=heliosagent\n. fixed by #630 and #624 \n. @rohansingh \n. WIP\n. Ah this is for myself. It's work in progress.\nOn Friday, July 31, 2015, Staffan Gim\u00e5ker notifications@github.com wrote:\n\na more descriptive commit message would make this easier to review. e.g.\nwhat logic is moved from the CLI to the server?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/601#issuecomment-126724524.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. What do you guys think of this PR? See my commit msg:\n``\nThis commit moves the logic for listing helios masters from the CLI to\nthe server (see the much shorterMasterListCommand.java` class).\nJSON is emitted by calling a different HTTP endpoint and just having the\nclient print out the string it receives in the HTTP reply.\nBenefits are a simpler client, less chance of client-server\nincompatibilities, and potentially better performance due to masters'\nproximity to the ZooKeeper cluster.\nIf we like this idea, we can do this for the other CLI commands.\n```\n. @gimaker @rculbertson \n. http://clearthehaze.com/2014/09/dropwizard-ssl/\n. This can be a rollout option. The user would need to ensure the job doesn't\nuse static ports.\nOn Monday, August 3, 2015, Mo alserr notifications@github.com wrote:\n\nLooks really good with the (labels and deployment group) update although\nwill be much better if re-order the rollout tasks more to something like\nthis\n\"rolloutTasks\": [\n{\n\"action\": \"DEPLOY_NEW_JOB\",\n\"target\": \"helios-agent\"\n},\n{\n\"action\": \"AWAIT_RUNNING\",\n\"target\": \"helios-agent\"\n},\n{\n\"action\": \"UNDEPLOY_OLD_JOBS\",\n\"target\": \"helios-agent\"\n}\n]\nWhich can easily apply the Zero down time concept , and not effect the\nprevious job in case of timeout ,\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/issues/606.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. closing as #607 has been merged\n. :+1: \n. :+1: \n. Replaced by https://github.com/spotify/helios/pull/618\n. @cldmnky Glad to hear you found a workaround. FYI clients can now create jobs without specifying a hash. See https://github.com/spotify/helios/pull/678\n. @gtonic Sorry for the delayed response. This is an interesting idea.\nI'm wondering if there are any downsides to resolving all env vars in the commands.\n. @gtonic Feel free to take a quick look to check I didn't miss anything after rebasing.\n@rohansingh @gimaker Anyone want to review? I'm pretty confident in the change.\n. inspired by @gimaker here https://github.com/spotify/helios/pull/587#discussion_r35781309\n. @rohansingh @gimaker Can it be as simple as this?\n. assuming tests pass, :+1: \n. Hi @udomsak,\nGlad to see you using Helios. Currently you can only deploy one jobID (job\nname + version + hash) on each Helios agent, ie you can't have the same\njobID running on one agent. At Spotify, we use Puppet to install and run\nthe Helios agent and our Helios users create 20 agents that each run the\njob.\nHow you scale up containers depends on what you're running inside. If you know you need a minimum of n python processes, for example, it'd probably make more sense to have all n processes in one container instead of creating n containers.\nWe don't have a public Helios IRC channel or Slack as we're a small team\nand probably wouldn't be able to respond to questions. But you are\ndefinitely welcome to start one on freenode or Slack.\nOn Mon, Aug 10, 2015 at 6:47 AM, udomsak notifications@github.com wrote:\n\nI have question about Helios scale/up down container cluster. What\nappropriate way to scale up / down cluster container how do you this job ?\n- By jobs ( 1 container per job ) i see each job create 1 container\n  per time. It's correct or not ?\n- By configuration mangement tool like Puppet, Chef\n- Do you have community of Helios by IRC or Slack / something like\n  that. ?\nSo if 1 job is container thus if i want to scale up 20container then i\nneed 20jobs right ? This kind of config i plan to use chef interface with\nHelios client.\nShould i do this way. ? I very glad you are build this powerfull tool and\nglue with CI tools. +1:\nThank you,\nme\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/issues/616.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. @udomsak Each helios agent can register the services it's running into a service discovery tool of your choosing. It's done via a plugin. The current available plugins are helios-skydns and helios-consul and you can also make your own :)\n. @udomsak Sorry helios doesn't support these settings yet. Helios uses docker-client which does support setting these (see here), but you'd need to add code to TaskConfig, SupervisorFactory, and probably AgentService.\nI don't actually know what happens to containers that hit the resource limits.\n. @udomsak Thanks for letting us know. Key has been updated. Please apt-key import again. If that doesn't work, try apt-key del and then import again.\n. @rculbertson \n. @staffan\n. Ah I see. Good catch.\n. :+1: \n. Should we log in places like this? https://github.com/spotify/helios/blob/master/helios-services/src/main/java/com/spotify/helios/master/ZooKeeperMasterModel.java#L1664\n. :+1: \n. @gimaker \n. :+1: \n. @gimaker \n. @gimaker \n. @staffan @rohansingh \n. Still need to update RollingUpdateOpFactoryTest\n. Added new RollingUpdateOpFactoryTest here https://github.com/spotify/helios/pull/631/files#diff-850af0ee5778ee512a514c4dcbd33b7cR209\n. @rohansingh Updated according to your comments. Also changed the failure threshold to be a float instead of int.\n. @gimaker Added testTaskErrorTransitionToFailed() and testTaskErrorTransitionToDone() to RollingUpdateOpFactoryTest.\n. I tweaked isOverFailureThreshold() so that a rollout fails when 1 out of 2 hosts fails with a 50% failure threshold. Before you had to set a 51% threshold.\n. Replaced by https://github.com/spotify/helios/pull/650\n. @gimaker @rohansingh \n. :+1: \n. @rikhal\n. @gimaker  @rohansingh \n. :+1: Test?\n. @staffan @rohansingh \n. So I believe you can call start() in the middle of a rolling-update. It would be when the user runs rolling-update twice, the second time before the first has finished.\nnextTask() and error() will probably throw exceptions if called before start(), but the ZooKeeperMasterModel that was calling RollingUpdateOpFactory needs to know this now anyways.\nIt's probably not the best way, but I think it's better than now for these reasons:\n1. Fixes a bug\n2. Consolidates more ZK ops and event data into one place where it's less likely we'll forget to pair ops and events\n3. Increases test coverage of the business logic around starting a rolling-update\n. @rohansingh \n. @rohansingh \n. @rohansingh @gimaker \n. There were some spurious test failures. But it seems OK now.\n. @goldfishy Do you have data in your ZK cluster that was created from a previous version of helios? Sometimes old data is serialized in a way that newer versions of helios can't parse. Try helios remove-deployment-group?\n. @goldfishy Have you seen this error again? If not, could I close this?\n. @rohansingh @gimaker \n. @rohansingh \n. We seem to follow this pattern elsewhere.\n. See https://github.com/spotify/helios/blob/master/helios-client/src/main/java/com/spotify/helios/common/descriptors/Job.java#L201\n. @rohansingh @gimaker \n. Even though helioscanary is the only one that needs this right now, I feel that's an important enough piece of infra that's so blocked by deployment group failures that it makes this feature valuable.\n. Thanks\n. @rohansingh @gimaker \n. @rohansingh \n. Starting a helios host whose clock is ahead and has the same hostname as an already registered agent, will have the same effect as a user deregistering a perfectly OK host. I'm not sure we should worry about this case.\n. @mattnworb @gimaker Updated with another commit.\n. @rohansingh Do you know why we start LabelReporter and EnvironmentVariableReporter before knowing if the agent registered in ZK successfully? https://github.com/spotify/helios/pull/654/files#diff-144948f2deffedddb6b1bda1077673a9R351\n. @rohansingh Do you think the CountDownLatch is a good approach?\n. @gimaker @rohansingh @mattnworb \n. @mattnworb Yea I was wondering about the best default.\n1. default to https fallback to http\n2. default to http and user opts into https\n3. default to https and user opts out of it\n   ...\n. @mattnworb Yea, we can just leave it up to the operators of helios masters to decide. If they only want clients to talk https, they'd make masters only listen on localhost. If they only want http, they just don't setup SRV records.\n. @mattnworb Thanks, tests pass now.\n. @mattnworb Good points. I added comments.\n. Thanks, I'll wait for @gimaker and/or @rohansingh to take a look too.\n. @rohansingh @gimaker \n. Seems like many users still use --sites. I'm going to wait to merge this until we get prbot to find and replace most of helios -s: https://ghe.spotify.net/search?utf8=%E2%9C%93&q=%22helios+-s%22&type=Code&ref=searchresults.\n. And on closer look those repos except two are all magneto. I've opened PRs for those two. I'm going to merge.\n. Depends on https://github.com/spotify/docker-client/pull/240\n. We decided to use TLS client-side certs: https://github.com/spotify/helios/pull/717 which makes this proposal obsolete.\n. @gimaker @rohansingh \n. @danielnorberg \n. @gimaker @rohansingh \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. Thanks, @drorweiss. I see you're the CEO of codota. :) I'm wondering if there are Helios users that want this.\n. Thanks, we'll try to check it out.\n. I'm going to close for now. We can revisit this if there are users who want the tool.\n. ugly but I guess it gets the job done?\n. I considered using Joda time and milliseconds. I used Date to be consistent with the expires field in Job class. DateTime seems more human readable than milliseconds. But if we do that itd be nice to also change expires which might be tricky to be backwards compatible. \nI'm also fine with keeping it as Date. Thoughts?\n. I made created optional and to be set by the caller. Otherwise the tests become a pain to fix.\n. @mattnworb Good points, I'll do milliseconds and have the server set it. But we'll need to ignore date when generating the job hash right?\n. @mattnworb yea, it might make testing easier in the future. See latest commit\n. @mattnworb @gimaker \n. :+1: \n. helios inspect with data from shared.cloud works. helios create with old clients works.\n. :100: \n. @gimaker Do you mean static variables like ENDPOINT1 and SITE1?\n. @mattnworb I fixed the DeregisterTests. Try rebasing on master.\n. @rohansingh @gimaker @mattnworb \n. Do we need to? The client can't tell from the token if it's expired since\nthe whole thing is hashed. The client can just always try the token if it\nexists and redo the handshake to get a new one if expired?\nOn Tue, Oct 13, 2015 at 4:46 PM, Matt Brown notifications@github.com\nwrote:\n\nWe'll also need a place to store expiration times for access tokens, or\n(if crtauth provides that by baking it into the token) then a method for\nchecking if an arbitrary token (as present in the client request) is still\nvalid.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/686#issuecomment-147847747.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. @mattnworb Oh, gotcha. I think crtauth-java does that for us. https://github.com/spotify/crtauth-java/blob/0a7ca12fe1e052047b40aab6487fc9e12f4952f1/src/main/java/com/spotify/crtauth/CrtAuthServer.java#L275\n. @mattnworb I just included the helios-crtauth module.\n. :+1: \n. :+1: \n. :+1: \n. @mattnworb I'm loving the tests. Nice job.\n. How much more is the extra container?\n. :+1: \n. @rohansingh @mattnworb @gimaker \nThe HostnameVerifier logic breaks helios -z when used with TLS. Resolving hostnames and retrying unique IPs would've been nice to have in the case of one hostname with multiple A records, but the added complexity and the regression don't seem worth the trouble.\n. @rohansingh @mattnworb @gimaker \n. @rohansingh @mattnworb @gimaker\n. :+1 for reclaiming our code from apache\n. :+1: I wonder why it only broke on staffan's computer.\n. @mattnworb @rohansingh @gimaker \n. We need to edit https://github.com/spotify/helios/blob/master/helios-services/src/main/java/com/spotify/helios/master/MasterService.java#L263 to allow unauthed access to certain endpoints like /_auth.\n. @mattnworb Yea you're right. I didn't quite see it last time. thanks\n. Looks great. :+1: \n. Is showing the username something we'd rather not leak when we move to authorization?\n. @mattnworb Yea I think you're right then.\n. LGTM\n. @rohansingh Let me know if another kind of exception other than Runtime from AgentProxy.newInstance() is more suitable.\n. I'm OK with merging. Awesome work all\nOn Thursday, October 29, 2015, Matt Brown notifications@github.com wrote:\n\nYup, that's exactly what I was thinking. Figured we can implement that\nlater though.\nRight, I was asking more to see if it was a simple one or three line thing\nthat could be added now or if it needs to be plugged into a lot more places\nwith the bouncycastle stuff.\nAssuming the answer is the latter, merging sounds good to me though.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/717#issuecomment-152265067.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. Do we want to switch to Apache HTTP client from HttpUrlConnection at any point? DefaultRequestDispatcher seems pretty complex still. I'm wondering if it's worth it to clean it up.\n. :+1: \n. if you rebase off master, tests should pass\n. replaced by https://github.com/spotify/helios/pull/726\n. @dflemstr I don't know much about systemd, but perhaps you could also update the docs? BTW, circleCI is failing ATM.\n. @dflemstr if you rebase off master, tests should pass\n. @mattnworb @rohansingh @gimaker \n. :+1: \n. +1\nOn Thursday, October 29, 2015, Matt Brown notifications@github.com wrote:\n\n+1\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/725#issuecomment-152167517.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. @mattnworb No the two are unrelated. I havent looked into why tests fail on this one yet.\n. @rohansingh \n. Replaced by https://github.com/spotify/helios/pull/749\n. @gimaker Let me know what you think of this approach.\n. TODO: move more endpoint validation and selection logic out of DefaultRequestDispatcher.\n. @rohansingh @mattnworb\n. @gimaker @rohansingh @mattnworb \n. Half the time we use toStringHelper(getClass()) and the other half we use toStringHelper(this). I just picked one.\n. @piccobit Thanks for the bug report. We're all using 14.04 over here so I haven't gotten a chance to repro it. Your log output is about the helios job reaper failing (the reaper kills stray docker containers that the agent has no record of starting), which seems unrelated to the actual job failing.\nIs this still a problem and if so do you have more logs from the agent and/or docker daemon?\n. This is a wild guess, but perhaps this recently merged PR for docker-client\nwould help?\nhttps://github.com/spotify/docker-client/pull/222/files#diff-a479355e73762bc07b466a285d105088R166\nOn Thu, Nov 5, 2015 at 4:51 AM, Hans-Dieter Stich notifications@github.com\nwrote:\n\nHi David,\nHere is a fresh log from tomorrow morning (sorry for the long log, but I'm\nnot allowed to attach the log):\nListening for transport dt_socket at address: 5006\n10:19:16.128 helios[14151]: DEBUG [AgentMain STARTING] logging: Logging Provider: org.jboss.logging.Slf4jLoggerProvider\n10:19:16.131 helios[14151]: INFO  [AgentMain STARTING] Version: HV000001: Hibernate Validator 5.1.1.Final\n10:19:16.136 helios[14151]: DEBUG [AgentMain STARTING] DefaultTraversableResolver: Cannot find javax.persistence.Persistence on classpath. Assuming non JPA 2 environment. All properties will per default be traversable.\n10:19:16.151 helios[14151]: DEBUG [AgentMain STARTING] ValidationXmlParser: Trying to load META-INF/validation.xml for XML based Validator configuration.\n10:19:16.152 helios[14151]: DEBUG [AgentMain STARTING] ValidationXmlParser: No META-INF/validation.xml found. Using annotation based configuration only.\n10:19:16.376 helios[14151]: DEBUG [AgentMain STARTING] log: Logging to Logger[org.eclipse.jetty.util.log] via org.eclipse.jetty.util.log.Slf4jLog\n10:19:16.382 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: i.d.j.MutableServletContextHandler@3b60b48a{/,null,null} added {org.eclipse.jetty.servlet.ServletHandler@49ecee7a,AUTO}\n10:19:16.382 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: i.d.j.MutableServletContextHandler@71bc895{/,null,null} added {org.eclipse.jetty.servlet.ServletHandler@310db1d8,AUTO}\n10:19:16.402 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@310db1d8 added {tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true,AUTO}\n10:19:16.403 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@310db1d8 added {[/tasks/]=>tasks,POJO}\n10:19:16.405 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: filterNameMap={}\n10:19:16.405 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: pathFilters=null\n10:19:16.405 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletFilterMap=null\n10:19:16.406 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletPathMap={/tasks/=tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true}\n10:19:16.406 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletNameMap={tasks=tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true}\n10:19:16.436 helios[14151]: INFO  [AgentMain STARTING] AgentService: Starting metrics\n10:19:16.485 helios[14151]: INFO  [AgentMain STARTING] CuratorFrameworkImpl: Starting\n10:19:16.487 helios[14151]: DEBUG [AgentMain STARTING] CuratorZookeeperClient: Starting\n10:19:16.487 helios[14151]: DEBUG [AgentMain STARTING] ConnectionState: Starting\n10:19:16.487 helios[14151]: DEBUG [AgentMain STARTING] ConnectionState: reset\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:zookeeper.version=3.5.0-alpha-1615249, built on 08/01/2014 22:13 GMT\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:host.name=helios-agent-host.example.com\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:java.version=1.7.0_85\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:java.vendor=Oracle Corporation\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-7-openjdk-amd64/jre\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:java.class.path=/usr/share/helios/lib/services/helios-services-0.8.562-shaded.jar\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:java.io.tmpdir=/tmp\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:java.compiler=\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:os.name=Linux\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:os.arch=amd64\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:os.version=3.13.0-66-generic\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:user.name=helios\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:user.home=/var/lib/helios-agent\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:user.dir=/var/lib/helios-agent\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:os.memory.free=195MB\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:os.memory.max=3555MB\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:os.memory.total=240MB\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Initiating client connection, connectString=helios-master-host.example.com:2181 sessionTimeout=60000 watcher=org.apache.curator.ConnectionState@3932b27d\n10:19:16.499 helios[14151]: DEBUG [AgentMain STARTING] ClientCnxn: zookeeper.disableAutoWatchReset is false\n10:19:16.512 helios[14151]: INFO  [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Opening socket connection to server helios-master-host.example.com/10.222.132.82:2181. Will not attempt to authenticate using SASL (unknown error)\n10:19:16.517 helios[14151]: INFO  [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Socket connection established to helios-master-host.example.com/10.222.132.82:2181, initiating session\n10:19:16.519 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Session establishment request sent on helios-master-host.example.com/10.222.132.82:2181\n10:19:16.667 helios[14151]: DEBUG [AgentMain STARTING] ServiceRegistrars: No service registrar plugin configured\n10:19:16.668 helios[14151]: INFO  [AgentMain STARTING] ServiceRegistrars: No address nor domain configured, not creating service registrar.\n10:19:16.692 helios[14151]: INFO  [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Session establishment complete on server helios-master-host.example.com/10.222.132.82:2181, sessionid = 0x150af72f074002a, negotiated timeout = 40000\n10:19:16.698 helios[14151]: INFO  [AgentMain STARTING-EventThread] ConnectionStateManager: State change: CONNECTED\n10:19:16.698 helios[14151]: DEBUG [Curator-ConnectionStateManager-0] PersistentPathChildrenCache: connection state change: CONNECTED\n10:19:17.001 helios[14151]: INFO  [AgentMain STARTING] ServerFactory: Starting helios-agent\n10:19:17.005 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {dw{STOPPED,8<=0<=1024,i=0,q=0},AUTO}\n10:19:17.006 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {com.spotify.helios.servicescommon.ManagedStatsdReporter@4b8a34d2,AUTO}\n10:19:17.006 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {com.spotify.helios.servicescommon.RiemannSupport@5e91edbc,AUTO}\n10:19:17.006 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {com.spotify.helios.agent.DockerHealthChecker@3e8f0225,AUTO}\n10:19:17.006 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {com.spotify.helios.servicescommon.RiemannHeartBeat@1ed5eb39,AUTO}\n10:19:17.007 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {com.spotify.helios.servicescommon.coordination.ZooKeeperHealthChecker@406a400c,AUTO}\n10:19:17.007 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {AgentService [NEW],AUTO}\n10:19:17.008 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {org.eclipse.jetty.server.handler.ErrorHandler@60c25cf7,AUTO}\n10:19:17.009 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@49ecee7a added {io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc,AUTO}\n10:19:17.010 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@49ecee7a added {[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc,POJO}\n10:19:17.011 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: filterNameMap={io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc=io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc}\n10:19:17.011 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: pathFilters=[[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc]\n10:19:17.011 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletFilterMap={}\n10:19:17.011 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletPathMap=null\n10:19:17.011 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletNameMap={}\n10:19:17.012 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@49ecee7a added {io.dropwizard.servlets.ThreadNameFilter-7d4b57ae,AUTO}\n10:19:17.012 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@49ecee7a added {[/]/[]==1=>io.dropwizard.servlets.ThreadNameFilter-7d4b57ae,POJO}\n10:19:17.012 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: filterNameMap={io.dropwizard.servlets.ThreadNameFilter-7d4b57ae=io.dropwizard.servlets.ThreadNameFilter-7d4b57ae, io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc=io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc}\n10:19:17.012 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: pathFilters=[[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc, [/]/[]==1=>io.dropwizard.servlets.ThreadNameFilter-7d4b57ae]\n10:19:17.012 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletFilterMap={}\n10:19:17.012 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletPathMap=null\n10:19:17.012 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletNameMap={}\n10:19:17.017 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@49ecee7a added {io.dropwizard.jetty.BiDiGzipFilter-857e3fa,AUTO}\n10:19:17.017 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@49ecee7a added {[/]/[]==31=>io.dropwizard.jetty.BiDiGzipFilter-857e3fa,POJO}\n10:19:17.017 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: filterNameMap={io.dropwizard.jetty.BiDiGzipFilter-857e3fa=io.dropwizard.jetty.BiDiGzipFilter-857e3fa, io.dropwizard.servlets.ThreadNameFilter-7d4b57ae=io.dropwizard.servlets.ThreadNameFilter-7d4b57ae, io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc=io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc}\n10:19:17.017 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: pathFilters=[[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc, [/]/[]==1=>io.dropwizard.servlets.ThreadNameFilter-7d4b57ae, [/]/[]==31=>io.dropwizard.jetty.BiDiGzipFilter-857e3fa]\n10:19:17.017 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletFilterMap={}\n10:19:17.017 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletPathMap=null\n10:19:17.017 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletNameMap={}\n10:19:17.022 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@49ecee7a added {com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f@84b6e75==com.sun.jersey.spi.container.servlet.ServletContainer,1,true,AUTO}\n10:19:17.022 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@49ecee7a added {[/]=>com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f,POJO}\n10:19:17.022 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: filterNameMap={io.dropwizard.jetty.BiDiGzipFilter-857e3fa=io.dropwizard.jetty.BiDiGzipFilter-857e3fa, io.dropwizard.servlets.ThreadNameFilter-7d4b57ae=io.dropwizard.servlets.ThreadNameFilter-7d4b57ae, io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc=io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc}\n10:19:17.022 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: pathFilters=[[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc, [/]/[]==1=>io.dropwizard.servlets.ThreadNameFilter-7d4b57ae, [/]/[]==31=>io.dropwizard.jetty.BiDiGzipFilter-857e3fa]\n10:19:17.022 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletFilterMap={}\n10:19:17.022 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletPathMap={/=com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f@84b6e75==com.sun.jersey.spi.container.servlet.ServletContainer,1,true}\n10:19:17.022 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletNameMap={com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f=com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f@84b6e75==com.sun.jersey.spi.container.servlet.ServletContainer,1,true}\n10:19:17.023 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: com.codahale.metrics.jetty9.InstrumentedHandler@33dba6d1 added {i.d.j.MutableServletContextHandler@3b60b48a{/,null,null},AUTO}\n10:19:17.025 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@310db1d8 added {com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true,AUTO}\n10:19:17.025 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@310db1d8 added {[/]=>com.codahale.metrics.servlets.AdminServlet-3c873f94,POJO}\n10:19:17.025 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: filterNameMap={}\n10:19:17.025 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: pathFilters=null\n10:19:17.025 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletFilterMap=null\n10:19:17.025 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletPathMap={/tasks/=tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true, /=com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true}\n10:19:17.026 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletNameMap={com.codahale.metrics.servlets.AdminServlet-3c873f94=com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true, tasks=tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true}\n10:19:17.026 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@310db1d8 added {io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97,AUTO}\n10:19:17.026 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@310db1d8 added {[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97,POJO}\n10:19:17.026 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: filterNameMap={io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97=io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97}\n10:19:17.026 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: pathFilters=[[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97]\n10:19:17.026 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletFilterMap={}\n10:19:17.026 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletPathMap={/tasks/=tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true, /=com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true}\n10:19:17.026 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletNameMap={com.codahale.metrics.servlets.AdminServlet-3c873f94=com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true, tasks=tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true}\n10:19:17.034 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: HttpConnectionFactory@665dd0e2{HTTP/1.1} added {HttpConfiguration@193cd9ef{32768,8192/8192,https://:0,[ForwardedRequestCustomizer@6a07e6da]},POJO}\n10:19:17.036 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: com.codahale.metrics.jetty9.InstrumentedConnectionFactory@4f22f4e6 added {HttpConnectionFactory@665dd0e2{HTTP/1.1},AUTO}\n10:19:17.038 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@1f7d19f2{null}{0.0.0.0:0} added {org.eclipse.jetty.server.Server@c2028db,UNMANAGED}\n10:19:17.038 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@1f7d19f2{null}{0.0.0.0:0} added {dw{STOPPED,8<=0<=1024,i=0,q=0},AUTO}\n10:19:17.039 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@1f7d19f2{null}{0.0.0.0:0} added {org.eclipse.jetty.util.thread.ScheduledExecutorScheduler@31741ab7,AUTO}\n10:19:17.039 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@1f7d19f2{null}{0.0.0.0:0} added {org.eclipse.jetty.io.ArrayByteBufferPool@4e326f04,POJO}\n10:19:17.039 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@1f7d19f2{null}{0.0.0.0:0} added {com.codahale.metrics.jetty9.InstrumentedConnectionFactory@4f22f4e6,AUTO}\n10:19:17.040 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@1f7d19f2{HTTP/1.1}{0.0.0.0:0} added {org.eclipse.jetty.server.ServerConnector$ServerConnectorManager@7ebdcdf7,MANAGED}\n10:19:17.041 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {dw-admin{STOPPED,1<=0<=64,i=0,q=0},AUTO}\n10:19:17.041 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: HttpConnectionFactory@7e7eb4f6{HTTP/1.1} added {HttpConfiguration@309ac35b{32768,8192/8192,https://:0,[ForwardedRequestCustomizer@8fa44b]},POJO}\n10:19:17.041 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: com.codahale.metrics.jetty9.InstrumentedConnectionFactory@19378d8d added {HttpConnectionFactory@7e7eb4f6{HTTP/1.1},AUTO}\n10:19:17.042 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@e2e30ea{null}{0.0.0.0:0} added {org.eclipse.jetty.server.Server@c2028db,UNMANAGED}\n10:19:17.042 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@e2e30ea{null}{0.0.0.0:0} added {dw-admin{STOPPED,1<=0<=64,i=0,q=0},AUTO}\n10:19:17.042 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@e2e30ea{null}{0.0.0.0:0} added {org.eclipse.jetty.util.thread.ScheduledExecutorScheduler@566b3836,AUTO}\n10:19:17.042 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@e2e30ea{null}{0.0.0.0:0} added {org.eclipse.jetty.io.ArrayByteBufferPool@7a8d59eb,POJO}\n10:19:17.043 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@e2e30ea{null}{0.0.0.0:0} added {com.codahale.metrics.jetty9.InstrumentedConnectionFactory@19378d8d,AUTO}\n10:19:17.043 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@e2e30ea{HTTP/1.1}{0.0.0.0:0} added {org.eclipse.jetty.server.ServerConnector$ServerConnectorManager@560a9228,MANAGED}\n10:19:17.043 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {application@1f7d19f2{HTTP/1.1}{0.0.0.0:5803},AUTO}\n10:19:17.043 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {admin@e2e30ea{HTTP/1.1}{0.0.0.0:5804},AUTO}\n10:19:17.044 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: io.dropwizard.jetty.RoutingHandler@b7d2830 added {com.codahale.metrics.jetty9.InstrumentedHandler@33dba6d1,AUTO}\n10:19:17.044 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: io.dropwizard.jetty.RoutingHandler@b7d2830 added {i.d.j.MutableServletContextHandler@71bc895{/,null,null},AUTO}\n10:19:17.049 helios[14151]: DEBUG [AgentMain STARTING] AbstractLifeCycle: starting io.dropwizard.jetty.Slf4jRequestLog@69b08471\n10:19:17.050 helios[14151]: DEBUG [AgentMain STARTING] AbstractLifeCycle: STARTED io.dropwizard.jetty.Slf4jRequestLog@69b08471\n10:19:17.050 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.handler.RequestLogHandler@4656b3b1 added {io.dropwizard.jetty.Slf4jRequestLog@69b08471,UNMANAGED}\n10:19:17.050 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {io.dropwizard.jetty.Slf4jRequestLog@69b08471,MANAGED}\n10:19:17.051 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.handler.RequestLogHandler@4656b3b1 added {io.dropwizard.jetty.RoutingHandler@b7d2830,AUTO}\n10:19:17.052 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.handler.StatisticsHandler@154162c3 added {org.eclipse.jetty.server.handler.RequestLogHandler@4656b3b1,AUTO}\n10:19:17.052 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {org.eclipse.jetty.server.handler.StatisticsHandler@154162c3,AUTO}\n10:19:17.056 helios[14151]: INFO  [AgentService STARTING] AgentService:\n            .  . _ \n /   |   \\   _ |  | |**| _**  _   /  _  \\     _   /  |\n/    ~    /  |  | |  |/  _ \\/  /  /  /\\  \\  / _/ _ \\ /    \\   \\\n\\    Y    /\\  /|  ||  (  <> )** \\  /    |    \\/ /_/  >  /|   |  \\  |\n |  /  _*  >_/**|_**/  > _|_  /_  /   >|  /|\n       \\/       \\/                   \\/          \\//_____/      \\/     \\/\n10:19:17.057 helios[14151]: DEBUG [PersistentPathChildrenCache STARTING] PersistentPathChildrenCache: starting cache\n10:19:17.058 helios[14151]: DEBUG [Reactor(zk-ppcc:/config/hosts/helios-agent-host/jobs)] PersistentPathChildrenCache: updating: /config/hosts/helios-agent-host/jobs\n10:19:17.058 helios[14151]: DEBUG [Reactor(zk-ppcc:/config/hosts/helios-agent-host/jobs)] PersistentPathChildrenCache: syncing: /config/hosts/helios-agent-host/jobs\n10:19:17.065 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 1,3  replyHeader:: 1,400144,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974}\n10:19:17.066 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 2,3  replyHeader:: 2,400144,0  request:: '/config/hosts/helios-agent-host/id,F  response:: s{24,24,1446052557953,1446052557953,0,0,0,0,40,0,24}\n10:19:17.067 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 3,12  replyHeader:: 3,400144,0  request:: '/config/hosts/helios-agent-host/jobs,T  response:: v{},s{21,21,1446052557932,1446052557932,0,32,0,0,0,0,358972}\n10:19:17.072 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 4,4  replyHeader:: 4,400144,0  request:: '/config/hosts/helios-agent-host/id,F  response:: #46354130343641383244353633363436344239323732304444373945384439463330333341353941,s{24,24,1446052557953,1446052557953,0,0,0,0,40,0,24}\n10:19:17.073 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 5,12  replyHeader:: 5,400144,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: v{},s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974}\n10:19:17.073 helios[14151]: INFO  [Reactor(zk-client-async-init)] AgentZooKeeperRegistrar: Matching agent id node already present, not registering agent F5A046A82D5636464B92720DD79E8D9F3033A59A: helios-agent-host\n10:19:17.073 helios[14151]: DEBUG [Reactor(zk-client-async-init)] AgentZooKeeperRegistrar: Creating up node: /status/hosts/helios-agent-host/up\n10:19:17.074 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.server.Server@c2028db\n10:19:17.074 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting dw{STOPPED,8<=0<=1024,i=0,q=0}\n10:19:17.076 helios[14151]: DEBUG [Reactor(zk-ppcc:/config/hosts/helios-agent-host/jobs)] PersistentPathChildrenCache: children: []\n10:19:17.087 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED dw{STARTED,8<=8<=1024,i=8,q=0}\n10:19:17.090 helios[14151]: DEBUG [AgentService STARTING] ContainerLifeCycle: application@1f7d19f2{HTTP/1.1}{0.0.0.0:5803} added {sun.nio.ch.ServerSocketChannelImpl[/0.0.0.0:5803],POJO}\n10:19:17.090 helios[14151]: INFO  [AgentService STARTING] SetUIDListener: Opened application@1f7d19f2{HTTP/1.1}{0.0.0.0:5803}\n10:19:17.090 helios[14151]: DEBUG [AgentService STARTING] ContainerLifeCycle: admin@e2e30ea{HTTP/1.1}{0.0.0.0:5804} added {sun.nio.ch.ServerSocketChannelImpl[/0.0.0.0:5804],POJO}\n10:19:17.091 helios[14151]: INFO  [AgentService STARTING] SetUIDListener: Opened admin@e2e30ea{HTTP/1.1}{0.0.0.0:5804}\n10:19:17.091 helios[14151]: DEBUG [AgentService STARTING] LifecycleEnvironment: managed objects = [com.spotify.helios.servicescommon.ManagedStatsdReporter@4b8a34d2, com.spotify.helios.servicescommon.RiemannSupport@5e91edbc, com.spotify.helios.agent.DockerHealthChecker@3e8f0225, com.spotify.helios.servicescommon.RiemannHeartBeat@1ed5eb39, com.spotify.helios.servicescommon.coordination.ZooKeeperHealthChecker@406a400c, AgentService [STARTING], AgentService [STARTING]]\n10:19:17.092 helios[14151]: INFO  [Reactor(zk-client-async-init)] AgentZooKeeperRegistrar: ZooKeeper registration complete\n10:19:17.096 helios[14151]: INFO  [AgentService STARTING] Server: jetty-9.0.z-SNAPSHOT\n10:19:17.106 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting org.eclipse.jetty.server.Server@c2028db\n10:19:17.106 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.spotify.helios.servicescommon.ManagedStatsdReporter@4b8a34d2\n10:19:17.106 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.spotify.helios.servicescommon.ManagedStatsdReporter@4b8a34d2\n10:19:17.106 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.spotify.helios.servicescommon.RiemannSupport@5e91edbc\n10:19:17.106 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.spotify.helios.servicescommon.RiemannSupport@5e91edbc\n10:19:17.106 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.spotify.helios.agent.DockerHealthChecker@3e8f0225\n10:19:17.106 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.spotify.helios.agent.DockerHealthChecker@3e8f0225\n10:19:17.106 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.spotify.helios.servicescommon.RiemannHeartBeat@1ed5eb39\n10:19:17.107 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.spotify.helios.servicescommon.RiemannHeartBeat@1ed5eb39\n10:19:17.107 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.spotify.helios.servicescommon.coordination.ZooKeeperHealthChecker@406a400c\n10:19:17.108 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.spotify.helios.servicescommon.coordination.ZooKeeperHealthChecker@406a400c\n10:19:17.108 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting AgentService [STARTING]\n10:19:17.108 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED AgentService [STARTING]\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.server.handler.ErrorHandler@60c25cf7\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting org.eclipse.jetty.server.handler.ErrorHandler@60c25cf7\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.ErrorHandler@60c25cf7\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting dw-admin{STOPPED,1<=0<=64,i=0,q=0}\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED dw-admin{STARTED,1<=1<=64,i=0,q=0}\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.server.handler.StatisticsHandler@154162c3\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting org.eclipse.jetty.server.handler.StatisticsHandler@154162c3\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.server.handler.RequestLogHandler@4656b3b1\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting org.eclipse.jetty.server.handler.RequestLogHandler@4656b3b1\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting io.dropwizard.jetty.RoutingHandler@b7d2830\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting io.dropwizard.jetty.RoutingHandler@b7d2830\n10:19:17.110 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.codahale.metrics.jetty9.InstrumentedHandler@33dba6d1\n10:19:17.110 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting com.codahale.metrics.jetty9.InstrumentedHandler@33dba6d1\n10:19:17.110 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting i.d.j.MutableServletContextHandler@3b60b48a{/,null,null}\n10:19:17.115 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting i.d.j.MutableServletContextHandler@3b60b48a{/,null,STARTING}\n10:19:17.116 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@49ecee7a\n10:19:17.116 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: filterNameMap={io.dropwizard.jetty.BiDiGzipFilter-857e3fa=io.dropwizard.jetty.BiDiGzipFilter-857e3fa, io.dropwizard.servlets.ThreadNameFilter-7d4b57ae=io.dropwizard.servlets.ThreadNameFilter-7d4b57ae, io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc=io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc}\n10:19:17.116 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: pathFilters=[[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc, [/]/[]==1=>io.dropwizard.servlets.ThreadNameFilter-7d4b57ae, [/]/[]==31=>io.dropwizard.jetty.BiDiGzipFilter-857e3fa]\n10:19:17.116 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: servletFilterMap={}\n10:19:17.116 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: servletPathMap={/=com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f@84b6e75==com.sun.jersey.spi.container.servlet.ServletContainer,1,true}\n10:19:17.116 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: servletNameMap={com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f=com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f@84b6e75==com.sun.jersey.spi.container.servlet.ServletContainer,1,true}\n10:19:17.117 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@49ecee7a\n10:19:17.117 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@49ecee7a\n10:19:17.117 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc\n10:19:17.117 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc\n10:19:17.118 helios[14151]: DEBUG [AgentService STARTING] FilterHolder: Filter.init io.dropwizard.jersey.filter.AllowedMethodsFilter@c0c4929\n10:19:17.118 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting io.dropwizard.servlets.ThreadNameFilter-7d4b57ae\n10:19:17.118 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED io.dropwizard.servlets.ThreadNameFilter-7d4b57ae\n10:19:17.118 helios[14151]: DEBUG [AgentService STARTING] FilterHolder: Filter.init io.dropwizard.servlets.ThreadNameFilter@43a2b7e8\n10:19:17.118 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting io.dropwizard.jetty.BiDiGzipFilter-857e3fa\n10:19:17.118 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED io.dropwizard.jetty.BiDiGzipFilter-857e3fa\n10:19:17.118 helios[14151]: DEBUG [AgentService STARTING] FilterHolder: Filter.init io.dropwizard.jetty.BiDiGzipFilter@25fa74b6\n10:19:17.120 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f@84b6e75==com.sun.jersey.spi.container.servlet.ServletContainer,1,true\n10:19:17.120 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f@84b6e75==com.sun.jersey.spi.container.servlet.ServletContainer,1,true\n10:19:17.120 helios[14151]: DEBUG [AgentService STARTING] ServletHolder: Filter.init com.sun.jersey.spi.container.servlet.ServletContainer@49162edf\n10:19:17.130 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:/status/hosts/helios-agent-host/up serverPath:/status/hosts/helios-agent-host/up finished:false header:: 6,1  replyHeader:: 6,400145,0  request:: '/status/hosts/helios-agent-host/up,,v{s{31,s{'world,'anyone}}},1  response:: '/status/hosts/helios-agent-host/up\n10:19:17.131 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 7,3  replyHeader:: 7,400145,0  request:: '/status/hosts/helios-agent-host,F  response:: s{18,18,1446052557907,1446052557907,0,46,0,0,0,6,400145}\n10:19:17.132 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 8,3  replyHeader:: 8,400145,0  request:: '/status/hosts/helios-agent-host,F  response:: s{18,18,1446052557907,1446052557907,0,46,0,0,0,6,400145}\n10:19:17.133 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 9,3  replyHeader:: 9,400145,0  request:: '/status/hosts/helios-agent-host,F  response:: s{18,18,1446052557907,1446052557907,0,46,0,0,0,6,400145}\n10:19:17.134 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 10,3  replyHeader:: 10,400145,0  request:: '/status,F  response:: s{4,4,1446052557726,1446052557726,0,3,0,0,0,3,15}\n10:19:17.135 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 11,3  replyHeader:: 11,400145,0  request:: '/status/hosts/helios-agent-host/environment,F  response:: s{297744,384673,1446107828884,1446559355698,17,0,0,0,2,0,297744}\n10:19:17.138 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:/status/hosts/helios-agent-host/up serverPath:/status/hosts/helios-agent-host/up finished:false header:: 12,3  replyHeader:: 12,400145,0  request:: '/status/hosts/helios-agent-host/up,T  response:: s{400145,400145,1446715157091,1446715157091,0,0,0,94768500370047018,0,0,400145}\n10:19:17.140 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 13,3  replyHeader:: 13,400145,0  request:: '/status/hosts/helios-agent-host/labels,F  response:: s{297745,384672,1446107828889,1446559355695,17,0,0,0,2,0,297745}\n10:19:17.141 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 14,3  replyHeader:: 14,400145,0  request:: '/status/hosts/helios-agent-host/agentinfo,F  response:: s{29,399922,1446052609183,1446711858715,5554,0,0,0,608,0,29}\n10:19:17.142 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 15,3  replyHeader:: 15,400145,0  request:: '/status/hosts,F  response:: s{11,11,1446052557787,1446052557787,0,19,0,0,0,3,351126}\n10:19:17.154 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 16,5  replyHeader:: 16,400146,0  request:: '/status/hosts/helios-agent-host/environment,#7b7d,-1  response:: s{297744,400146,1446107828884,1446715157138,18,0,0,0,2,0,297744}\n10:19:17.163 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 17,5  replyHeader:: 17,400147,0  request:: '/status/hosts/helios-agent-host/labels,#7b7d,-1  response:: s{297745,400147,1446107828889,1446715157140,18,0,0,0,2,0,297745}\n10:19:17.165 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 18,5  replyHeader:: 18,400148,0  request:: '/status/hosts/helios-agent-host/agentinfo,#7b22696e707574417267756d656e7473223a5b222d44636f6d2e73756e2e6d616e6167656d656e742e6a6d7872656d6f74652e706f72743d39323033222c222d44636f6d2e73756e2e6d616e6167656d656e742e6a6d7872656d6f74652e726d692e706f72743d39323033222c222d44636f6d2e73756e2e6d616e6167656d656e742e6a6d7872656d6f74652e73736c3d66616c7365222c222d44636f6d2e73756e2e6d616e6167656d656e742e6a6d7872656d6f74652e61757468656e7469636174653d66616c7365222c222d446a6176612e6e65742e70726566657249507634537461636b3d74727565222c222d6167656e746c69623a6a6477703d7472616e73706f72743d64745f736f636b65742c7365727665723d792c73757370656e643d6e2c616464726573733d35303036225d2c226e616d65223a2231343135314075756434706477752e637730312e636f6e746977616e\n 2e636f6d222c22737065634e61\n6d65223a224a617661205669727475616c204d616368696e652053706563696669636174696f6e222c227370656356656e646f72223a224f7261636c6520436f72706f726174696f6e222c227370656356657273696f6e223a22312e37222c22737461727454696d65223a313434363731353135353636302c22757074696d65223a313430362c2276657273696f6e223a22302e382e353632222c22766d4e616d65223a224f70656e4a444b2036342d4269742053657276657220564d222c22766d56656e646f72223a224f7261636c6520436f72706f726174696f6e222c22766d56657273696f6e223a2232342e38352d623033227d,-1  response:: s{29,400148,1446052609183,1446715157141,5555,0,0,0,604,0,29}\n10:19:17.166 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:/status/hosts serverPath:/status/hosts finished:false header:: 19,12  replyHeader:: 19,400148,0  request:: '/status/hosts,T  response:: v{'xxxx018,'helios-agent-host,'helios-master-host},s{11,11,1446052557787,1446052557787,0,19,0,0,0,3,351126}\n10:19:17.169 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:/status/hosts/xxxx018 serverPath:/status/hosts/xxxx018 finished:false header:: 20,4  replyHeader:: 20,400148,0  request:: '/status/hosts/xxxx018,T  response:: ,s{350448,350448,1446128476820,1446128476820,0,26,0,0,0,6,386007}\n10:19:17.170 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:/status/hosts/helios-agent-host serverPath:/status/hosts/helios-agent-host finished:false header:: 21,4  replyHeader:: 21,400148,0  request:: '/status/hosts/helios-agent-host,T  response:: ,s{18,18,1446052557907,1446052557907,0,46,0,0,0,6,400145}\n10:19:17.171 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:/status/hosts/helios-master-host serverPath:/status/hosts/helios-master-host finished:false header:: 22,4  replyHeader:: 22,400148,0  request:: '/status/hosts/helios-master-host,T  response:: ,s{301105,301105,1446109821539,1446109821539,0,8,0,0,0,6,386023}\n10:19:17.198 helios[14151]: INFO  [AgentService STARTING] WebApplicationImpl: Initiating Jersey application, version 'Jersey: 1.18.1 02/19/2014 03:28 AM'\n10:19:17.236 helios[14151]: DEBUG [AgentService STARTING] DropwizardResourceConfig: resources = [com.spotify.helios.agent.AgentModelTaskResource, com.spotify.helios.agent.AgentModelTaskStatusResource]\n10:19:17.236 helios[14151]: DEBUG [AgentService STARTING] DropwizardResourceConfig: providers = [io.dropwizard.jersey.caching.CacheControlledResourceMethodDispatchAdapter, io.dropwizard.jersey.guava.OptionalResourceMethodDispatchAdapter, io.dropwizard.jersey.guava.OptionalQueryParamInjectableProvider, io.dropwizard.jersey.validation.ConstraintViolationExceptionMapper, io.dropwizard.jersey.jackson.JsonProcessingExceptionMapper, com.codahale.metrics.jersey.InstrumentedResourceMethodDispatchAdapter, io.dropwizard.errors.EarlyEofExceptionMapper]\n10:19:17.256 helios[14151]: INFO  [AgentService STARTING] DropwizardResourceConfig: The following paths were found for the configured resources:\nGET     /helios/tasks (com.spotify.helios.agent.AgentModelTaskResource)\nGET     /helios/taskstatus (com.spotify.helios.agent.AgentModelTaskStatusResource)\n10:19:17.440 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAddCookies: CookieSpec selected: default\n10:19:17.440 helios[14151]: DEBUG [jersey-client-async-executor-0] RequestAddCookies: CookieSpec selected: default\n10:19:17.445 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAuthCache: Auth cache not set in the context\n10:19:17.445 helios[14151]: DEBUG [jersey-client-async-executor-0] RequestAuthCache: Auth cache not set in the context\n10:19:17.446 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection request: [route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:19:17.446 helios[14151]: DEBUG [jersey-client-async-executor-0] PoolingHttpClientConnectionManager: Connection request: [route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:19:17.454 helios[14151]: DEBUG [jersey-client-async-executor-0] PoolingHttpClientConnectionManager: Connection leased: [id: 1][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 2 of 100; total allocated: 2 of 100]\n10:19:17.454 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection leased: [id: 0][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 2 of 100; total allocated: 2 of 100]\n10:19:17.455 helios[14151]: DEBUG [jersey-client-async-executor-1] MainClientExec: Opening connection {}->unix://localhost:80\n10:19:17.455 helios[14151]: DEBUG [jersey-client-async-executor-0] MainClientExec: Opening connection {}->unix://localhost:80\n10:19:17.543 helios[14151]: INFO  [AgentService STARTING] ContextHandler: Started i.d.j.MutableServletContextHandler@3b60b48a{/,null,AVAILABLE}\n10:19:17.543 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED i.d.j.MutableServletContextHandler@3b60b48a{/,null,AVAILABLE}\n10:19:17.544 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.codahale.metrics.jetty9.InstrumentedHandler@33dba6d1\n10:19:17.544 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting i.d.j.MutableServletContextHandler@71bc895{/,null,null}\n10:19:17.544 helios[14151]: INFO  [AgentService STARTING] AdminEnvironment: tasks =\nPOST    /tasks/gc (io.dropwizard.servlets.tasks.GarbageCollectionTask)\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] AdminEnvironment: health checks = [deadlocks, docker, zookeeper]\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting i.d.j.MutableServletContextHandler@71bc895{/,null,STARTING}\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@310db1d8\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: filterNameMap={io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97=io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97}\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: pathFilters=[[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97]\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: servletFilterMap={}\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: servletPathMap={/tasks/=tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true, /*=com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true}\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: servletNameMap={com.codahale.metrics.servlets.AdminServlet-3c873f94=com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true, tasks=tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true}\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@310db1d8\n10:19:17.546 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@310db1d8\n10:19:17.546 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true\n10:19:17.546 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true\n10:19:17.546 helios[14151]: DEBUG [AgentService STARTING] ServletHolder: Filter.init io.dropwizard.servlets.tasks.TaskServlet@63c29805\n10:19:17.546 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true\n10:19:17.546 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true\n10:19:17.546 helios[14151]: DEBUG [AgentService STARTING] ServletHolder: Filter.init com.codahale.metrics.servlets.AdminServlet@7c8aa36d\n10:19:17.549 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97\n10:19:17.549 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97\n10:19:17.549 helios[14151]: DEBUG [AgentService STARTING] FilterHolder: Filter.init io.dropwizard.jersey.filter.AllowedMethodsFilter@31b54801\n10:19:17.550 helios[14151]: INFO  [AgentService STARTING] ContextHandler: Started i.d.j.MutableServletContextHandler@71bc895{/,null,AVAILABLE}\n10:19:17.550 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED i.d.j.MutableServletContextHandler@71bc895{/,null,AVAILABLE}\n10:19:17.550 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED io.dropwizard.jetty.RoutingHandler@b7d2830\n10:19:17.550 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.RequestLogHandler@4656b3b1\n10:19:17.550 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.StatisticsHandler@154162c3\n10:19:17.550 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting application@1f7d19f2{HTTP/1.1}{0.0.0.0:5803}\n10:19:17.550 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.util.thread.ScheduledExecutorScheduler@31741ab7\n10:19:17.550 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.util.thread.ScheduledExecutorScheduler@31741ab7\n10:19:17.550 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.codahale.metrics.jetty9.InstrumentedConnectionFactory@4f22f4e6\n10:19:17.551 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting HttpConnectionFactory@665dd0e2{HTTP/1.1}\n10:19:17.551 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED HttpConnectionFactory@665dd0e2{HTTP/1.1}\n10:19:17.551 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.codahale.metrics.jetty9.InstrumentedConnectionFactory@4f22f4e6\n10:19:17.551 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.server.ServerConnector$ServerConnectorManager@7ebdcdf7\n10:19:17.551 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@1d6807d4 keys=-1 selected=-1\n10:19:17.551 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@1d6807d4 keys=0 selected=0\n10:19:17.552 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=8<=1024,i=8,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@1d6807d4 keys=0 selected=0\n10:19:17.552 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@5b456a1f keys=-1 selected=-1\n10:19:17.552 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@5b456a1f keys=0 selected=0\n10:19:17.552 helios[14151]: DEBUG [dw-46-selector-0] SelectorManager: Starting Thread[dw-46-selector-0,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@1d6807d4 keys=0 selected=0\n10:19:17.552 helios[14151]: DEBUG [dw-46-selector-0] SelectorManager: Selector loop waiting on select\n10:19:17.552 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=8<=1024,i=7,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@5b456a1f keys=0 selected=0\n10:19:17.553 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@5f4f60b0 keys=-1 selected=-1\n10:19:17.553 helios[14151]: DEBUG [dw-47-selector-1] SelectorManager: Starting Thread[dw-47-selector-1,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@5b456a1f keys=0 selected=0\n10:19:17.553 helios[14151]: DEBUG [dw-47-selector-1] SelectorManager: Selector loop waiting on select\n10:19:17.553 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@5f4f60b0 keys=0 selected=0\n10:19:17.553 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=8<=1024,i=6,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@5f4f60b0 keys=0 selected=0\n10:19:17.553 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@2c5506e9 keys=-1 selected=-1\n10:19:17.553 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@2c5506e9 keys=0 selected=0\n10:19:17.553 helios[14151]: DEBUG [dw-49-selector-2] SelectorManager: Starting Thread[dw-49-selector-2,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@5f4f60b0 keys=0 selected=0\n10:19:17.553 helios[14151]: DEBUG [dw-49-selector-2] SelectorManager: Selector loop waiting on select\n10:19:17.553 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=8<=1024,i=5,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@2c5506e9 keys=0 selected=0\n10:19:17.554 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@2c35fbb6 keys=-1 selected=-1\n10:19:17.554 helios[14151]: DEBUG [dw-51-selector-3] SelectorManager: Starting Thread[dw-51-selector-3,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@2c5506e9 keys=0 selected=0\n10:19:17.554 helios[14151]: DEBUG [dw-51-selector-3] SelectorManager: Selector loop waiting on select\n10:19:17.554 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@2c35fbb6 keys=0 selected=0\n10:19:17.554 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=8<=1024,i=4,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@2c35fbb6 keys=0 selected=0\n10:19:17.554 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@2a65ea1a keys=-1 selected=-1\n10:19:17.554 helios[14151]: DEBUG [dw-52-selector-4] SelectorManager: Starting Thread[dw-52-selector-4,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@2c35fbb6 keys=0 selected=0\n10:19:17.554 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@2a65ea1a keys=0 selected=0\n10:19:17.554 helios[14151]: DEBUG [dw-52-selector-4] SelectorManager: Selector loop waiting on select\n10:19:17.555 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=8<=1024,i=3,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@2a65ea1a keys=0 selected=0\n10:19:17.555 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@4cdf1446 keys=-1 selected=-1\n10:19:17.555 helios[14151]: DEBUG [dw-50-selector-5] SelectorManager: Starting Thread[dw-50-selector-5,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@2a65ea1a keys=0 selected=0\n10:19:17.555 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@4cdf1446 keys=0 selected=0\n10:19:17.555 helios[14151]: DEBUG [dw-50-selector-5] SelectorManager: Selector loop waiting on select\n10:19:17.555 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=8<=1024,i=2,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@4cdf1446 keys=0 selected=0\n10:19:17.555 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@2d64880e keys=-1 selected=-1\n10:19:17.555 helios[14151]: DEBUG [dw-53-selector-6] SelectorManager: Starting Thread[dw-53-selector-6,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@4cdf1446 keys=0 selected=0\n10:19:17.555 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@2d64880e keys=0 selected=0\n10:19:17.555 helios[14151]: DEBUG [dw-53-selector-6] SelectorManager: Selector loop waiting on select\n10:19:17.556 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=8<=1024,i=1,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@2d64880e keys=0 selected=0\n10:19:17.556 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.server.ServerConnector$ServerConnectorManager@7ebdcdf7\n10:19:17.556 helios[14151]: DEBUG [dw-54-selector-7] SelectorManager: Starting Thread[dw-54-selector-7,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@2d64880e keys=0 selected=0\n10:19:17.556 helios[14151]: DEBUG [dw-54-selector-7] SelectorManager: Selector loop waiting on select\n10:19:17.556 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=9<=1024,i=0,q=0} dispatched org.eclipse.jetty.server.AbstractConnector$Acceptor@183ee521\n10:19:17.556 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=9<=1024,i=1,q=0} dispatched org.eclipse.jetty.server.AbstractConnector$Acceptor@7282ee7a\n10:19:17.557 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=10<=1024,i=1,q=0} dispatched org.eclipse.jetty.server.AbstractConnector$Acceptor@7d4cf957\n10:19:17.557 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=11<=1024,i=1,q=1} dispatched org.eclipse.jetty.server.AbstractConnector$Acceptor@84a83ca\n10:19:17.557 helios[14151]: INFO  [AgentService STARTING] ServerConnector: Started application@1f7d19f2{HTTP/1.1}{0.0.0.0:5803}\n10:19:17.557 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED application@1f7d19f2{HTTP/1.1}{0.0.0.0:5803}\n10:19:17.557 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting admin@e2e30ea{HTTP/1.1}{0.0.0.0:5804}\n10:19:17.557 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.util.thread.ScheduledExecutorScheduler@566b3836\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.util.thread.ScheduledExecutorScheduler@566b3836\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.codahale.metrics.jetty9.InstrumentedConnectionFactory@19378d8d\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting HttpConnectionFactory@7e7eb4f6{HTTP/1.1}\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED HttpConnectionFactory@7e7eb4f6{HTTP/1.1}\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.codahale.metrics.jetty9.InstrumentedConnectionFactory@19378d8d\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.server.ServerConnector$ServerConnectorManager@560a9228\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@7ecaf643 keys=-1 selected=-1\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@7ecaf643 keys=0 selected=0\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=1<=64,i=1,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@7ecaf643 keys=0 selected=0\n10:19:17.559 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@565dd9a1 keys=-1 selected=-1\n10:19:17.559 helios[14151]: DEBUG [dw-admin-61-selector-0] SelectorManager: Starting Thread[dw-admin-61-selector-0,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@7ecaf643 keys=0 selected=0\n10:19:17.559 helios[14151]: DEBUG [dw-admin-61-selector-0] SelectorManager: Selector loop waiting on select\n10:19:17.559 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@565dd9a1 keys=0 selected=0\n10:19:17.559 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=2<=64,i=1,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@565dd9a1 keys=0 selected=0\n10:19:17.559 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@781c1417 keys=-1 selected=-1\n10:19:17.559 helios[14151]: DEBUG [dw-admin-73-selector-1] SelectorManager: Starting Thread[dw-admin-73-selector-1,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@565dd9a1 keys=0 selected=0\n10:19:17.559 helios[14151]: DEBUG [dw-admin-73-selector-1] SelectorManager: Selector loop waiting on select\n10:19:17.559 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@781c1417 keys=0 selected=0\n10:19:17.560 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=3<=64,i=1,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@781c1417 keys=0 selected=0\n10:19:17.560 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@5eef3d7b keys=-1 selected=-1\n10:19:17.560 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@5eef3d7b keys=0 selected=0\n10:19:17.560 helios[14151]: DEBUG [dw-admin-74-selector-2] SelectorManager: Starting Thread[dw-admin-74-selector-2,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@781c1417 keys=0 selected=0\n10:19:17.560 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=4<=64,i=1,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@5eef3d7b keys=0 selected=0\n10:19:17.560 helios[14151]: DEBUG [dw-admin-74-selector-2] SelectorManager: Selector loop waiting on select\n10:19:17.560 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@5d8f1dcf keys=-1 selected=-1\n10:19:17.560 helios[14151]: DEBUG [dw-admin-75-selector-3] SelectorManager: Starting Thread[dw-admin-75-selector-3,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@5eef3d7b keys=0 selected=0\n10:19:17.560 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@5d8f1dcf keys=0 selected=0\n10:19:17.560 helios[14151]: DEBUG [dw-admin-75-selector-3] SelectorManager: Selector loop waiting on select\n10:19:17.561 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=5<=64,i=1,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@5d8f1dcf keys=0 selected=0\n10:19:17.561 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@59687025 keys=-1 selected=-1\n10:19:17.561 helios[14151]: DEBUG [dw-admin-76-selector-4] SelectorManager: Starting Thread[dw-admin-76-selector-4,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@5d8f1dcf keys=0 selected=0\n10:19:17.561 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@59687025 keys=0 selected=0\n10:19:17.561 helios[14151]: DEBUG [dw-admin-76-selector-4] SelectorManager: Selector loop waiting on select\n10:19:17.561 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=6<=64,i=1,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@59687025 keys=0 selected=0\n10:19:17.561 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@988c127 keys=-1 \n. I've made a PR here https://github.com/spotify/helios/pull/734\n. @gimaker @mattnworb \n. @mattnworb @gimaker \n. This was motivated by me trying to upgrade the docker-client dep in helios-services (PR here). That caused maven-enforcer to complain about a ton of upper-bound requirements. Fixing them was tedious as the dependency versions were duplicated everywhere.\n. needs to be rebased and have its dep versions tweaked after #733 is merged\n. :+1: \n. @negz \n. @negz \n. @mattnworb @negz @rohansingh \n. I couldn't find a better way that worked. This roughly doubles the size of the helios-tools jar. But I haven't measured the difference in speed which is more important here.\n. :+1: \n. Since we did this for docker-client, let's do it here too.\n. Having the plugin in pluginManagement didn't execute checkstyle.\n. @negz \n. @mattnworb \n. Any objections on merging this?\n. :+1:\n\nOn Thursday, November 12, 2015, Nic Cope notifications@github.com wrote:\n\n@mattnworb https://github.com/mattnworb Habit from ALF PRs - it's short\nfor 'peanut gallery'. ;)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/746#issuecomment-156294089.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. :+1: \n. ha marginally?\n. :+1: \n. @rohansingh @mattnworb \n. Example of a failure https://jenkins-helios2.spotify.net/job/helios-build/com.spotify$helios-system-tests/604/testReport/junit/com.spotify.helios.system/UndeployRaceTest/test/\n. LGTM but I don't have debian so I'll trust this works\n. @mattnworb \n. @rohansingh @mattnworb \n. ah good point\n. How about if we output it once so that users can be warned? I've wanted to refactor the Dispatcher to only fetch agent identities once.\n. Replaced with https://github.com/spotify/helios/pull/757\n. For now. @rohansingh @mattnworb @gimaker \n. Yea, I haven't had the chance to add a test yet. Good idea about passing in\nthe list of identities. But it seems like if you don't create a new\ninstance of an AgentProxy on every connection, the TLS handshake is messed\nup somehow. I wasn't able to drill down into why though...\nOn Fri, Nov 20, 2015 at 1:09 PM, Matt Brown notifications@github.com\nwrote:\n\n[image: :+1:] to the change itself.\nI think this class would be a lot more testable though (it has no test\ntoday) if instead you passed either the List or AgentProxy\ninstance in the constructor, so that the class that is making http requests\ndidn't also have to be concerned with how to get the ssh-agent identities\nor how to get an AgentProxy etc.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/757#issuecomment-158478315.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. I'm having trouble thinking of a sane way to make DefaultRequestDispatcher more testable. How could I mock out the URL connection part?...\n. I got this error when I re-used AgentProxy instances across connections.\nOn Fri, Nov 20, 2015 at 4:47 PM, Matt Brown notifications@github.com\nwrote:\n\nFWIW I broke something along the way with these changes, so it still needs\na ton of work\n$> bin/helios -vvvv -d shared.cloud.spotify.net masters\nrunning in helios project, using /Users/mattbrown/code/helios/helios-tools/target/helios-tools-0.8.0-SNAPSHOT-shaded.jar\n16:44:56.393 DEBUG AgentProxies$DefaultAgentProxy: connected to [family=PF_UNIX path=/private/tmp/com.apple.launchd.Aifmik2Y6u/Listeners]\n16:44:56.424 DEBUG AgentOutput: Sent SSH2_AGENTC_REQUEST_IDENTITIES message to ssh-agent.\n16:44:56.424 DEBUG AgentInput: Received SSH2_AGENT_IDENTITIES_ANSWER message from ssh-agent.\n16:44:56.838 DEBUG AuthenticatingHttpConnector: connecting to https://172.16.11.109:443/masters/?user=mattbrown\n16:44:56.860 TRACE DefaultHttpConnector: req: GET https://172.16.11.109:443/masters/?user=mattbrown 1 Helios-Version=[0.8.0-SNAPSHOT] 0 \"\"\n16:44:57.262 DEBUG RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\njava.lang.RuntimeException: java.lang.UnsupportedOperationException\n    at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1454) ~[na:1.8.0_40]\n    at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1440) ~[na:1.8.0_40]\n    at sun.net.www.protocol.http.HttpURLConnection.getHeaderField(HttpURLConnection.java:2978) ~[na:1.8.0_40]\n    at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:489) ~[na:1.8.0_40]\n    at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338) ~[na:1.8.0_40]\n    at com.spotify.helios.client.AuthenticatingHttpConnector.connect(AuthenticatingHttpConnector.java:112) ~[helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at com.spotify.helios.client.DefaultRequestDispatcher$1.call(DefaultRequestDispatcher.java:72) ~[helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at com.spotify.helios.client.DefaultRequestDispatcher$1.call(DefaultRequestDispatcher.java:68) ~[helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_40]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_40]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_40]\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_40]\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [na:1.8.0_40]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_40]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_40]\n    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_40]\nCaused by: java.lang.UnsupportedOperationException: null\n    at com.spotify.helios.client.tls.SshAgentSSLSocketFactory.createSocket(SshAgentSSLSocketFactory.java:82) ~[helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:453) ~[na:1.8.0_40]\n    at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_40]\n    at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1512) ~[na:1.8.0_40]\n    at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1440) ~[na:1.8.0_40]\n    at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) ~[na:1.8.0_40]\n    ... 12 common frames omitted\nLooking at\nsun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:453)\nit calls a different overload of SshAgentSSLSocketFactory.createSocket(..)\nbecause it caught an IOException from the original overload of\ncreateSocket (something with bouncycastle and the error message java.io.IOException:\nInternal TLS error, this could be an attack)\nbut perhaps that helps make my case that this could all use better testing [image:\n:smirk:]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/759#issuecomment-158533882.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. Fixed by #801 \n. :+1: \n. @mattnworb \n. @mattnworb \n. One place to log would be here https://github.com/spotify/helios/blob/master/helios-services/src/main/java/com/spotify/helios/master/ZooKeeperMasterModel.java#L345, but I don't know if this kind of denormalized data happens enough to have any logging add more signal than noise.\nI'd like to add a test, but it seems hard to simulate cases like this in ZooKeeper. Let me know if you have any good ideas.\n. @mattnworb I'm not sure. I don't think this can result from a race between a create and a remove DG command because the create command creates /config/deployment-group/<name>, /status/deployment-group/<name>, and /status/deployment-group/hosts/<name> in a transaction. And the remove DG command wouldn't have tried to delete the second two nodes if the first wasn't there. And if the first node was there, that means the other two are as well since they're created in a single transaction.\nMaybe a master was restarted during the removal of the radio-spaces DG?\n. Aye that works\nOn Wednesday, November 25, 2015, Staffan Gim\u00e5ker notifications@github.com\nwrote:\n\nSeems like the config/status/hosts should always exist (provided that the\nDG does), right? Checking that status & hosts exists seems superfluous.\nSeems like we should be able to put deletes for all 3 paths in a\ntransaction.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/765#issuecomment-159673974.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. :+1: If it passes on our jenki\n. :100: other than some misspelled words :)\n. looks pretty cool :+1: \n. :+1: \n. I still have some more work left to do for this. But I'll have it in good shape soon \n. Ha, this rebase will be painful...\n. nice\nOn Wed, Dec 9, 2015 at 10:50 AM, Matt Brown notifications@github.com\nwrote:\n\nClosed #781 https://github.com/spotify/helios/pull/781.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/781#event-487072367.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. :+1: @rohansingh ha codecov is probably counting the coverage in a way that doesn't take into account comments?\n. @mattnworb If it's the PublicKey is RSAPublicKey maybe you can use something like this? https://github.com/spotify/crtauth-java/blob/master/src/main/java/com/spotify/crtauth/Fingerprint.java#L60\n. I think this will be a large amount of work and given the direction of the project, we probably won't be able to do implement this ourselves. Any objections if I close this issue?\n. @rohansingh \n. :+1: \n. Codecov doesn't update when you force push?\n. :+1: \n. @mattnworb It splits the x509 cert into 64 character lines.\n. :+1: \n. \n:+1: \n. Seems pretty useful to me :+1: \n. This helps with #762. Hostnames like foo/bar show up in the JSON as foo%2Fbar but at least the command doesn't have a strange error message. Perhaps we should replace printf() with print() in other places where there are no args just to be safe?\n. I've converted more unecessary printf to print\n. The url encoding happens in HeliosClient\n. @mattnworb \n. womp womp czechstyle\n. We can also remove from SystemTestBase masterAdminEndpoint(). startJob(), deregisterHost(), and readLogFully() are also unused. \n. :+1: \n. I'm just getting familiar with @negz' code for helios solo. I did one of his TODOs just to see how it'd turn out. I don't feel strongly about this PR and would feel fine leaving this alone.\n. Cool, if there are no objections, I'll merge in a bit.\n. @negz @mattnworb \n. Need to merge https://github.com/spotify/docker-client/pull/321 and release docker-client first.\n. @rohansingh \n. :+1: \n. @mattnworb \n. We ended up using null here https://github.com/spotify/helios/pull/810/files#diff-bfa745ad3a4df20a664d2f81b84b9d35L625.\n. :+1:\n. @rohansingh @mattnworb \n. Good idea. I think the solo container should be a unique name per test run, otherwise other projects' ITs running on the same host will end up using the same solo container.\n. :+1: \n. @mattnworb Should the docs say to be aware of HELIOS_HOST_FILTER? https://github.com/spotify/helios/blob/master/helios-testing/src/test/java/com/spotify/helios/testing/HeliosSoloDeploymentTest.java#L66\n. The latter makes more sense.\n. yea I'll rebase\n. @gimaker @mattnworb @rohansingh \n. Ah yea I haven't gotten to that part yet.\n. Ideally, the watchdog would start 1 helios-solo at the beginning, we'd tell all the test classes which helios-solo container name to use, and then the watchdog would tear down helios-solo at the end. But I can't figure out have all the test classes share 1 helios-solo container namespace.\nWe can hold off on this and pick it up if necessary after using helios-solo in the wild a bit.\n. closing for now. We can reopen later if we need it\n. This would be nice, but there are some details to hash out.\nJobRemoveCommand inherits from WildcardJobCommand which searches all jobs for a string containing the job arg passed to helios remove <job arg>. WildcardJobCommand then errors out if there's zero or more than one matching job.\nWhat should happen if I run helios remove <job1> <job2> <job3> and job1 matches one job exactly, job2 matches none, and job3 matches more than one?\n. :+1:\n. @rohansingh @mattnworb \n. :+1: \n. It's a bit hard to say without more details (eg helios and docker version, error logs, etc), but in a nutshell the helios agent talks to the local docker daemon. Docker handles all the image pulling itself. In recent docker versions, docker will try to pull from the v2 registry and then fall back to v1.\nSo it doesn't make sense to say \"helios is using the registry v1\" as helios doesn't know or care. Does this make sense?\n. @mattnworb @rohansingh @gimaker\n. @alanw\n. Would a command line arg with a default be better?\n. @rohansingh @gimaker @mattnworb \n. @spdage\n. :+1: \n. :+1: \n. Is this the right way for this use-case? https://ghe.spotify.net/dflemstr/barge/pull/6\n. @mattnworb @gimaker \n. @gimaker @mattnworb @rohansingh \n. yup :(\n. I can pass in an implementation of the Clock interface like we do elsewhere.\n. @mattnworb Sorry, I should've wrote Sleeper instead of Clock. My second commit will mock the Sleeper and test that the .sleep() is called with correct args.\n. :100: \n. :+1: \n. Nice catch. :+1: But we were seeing new agents failing to deregister old agents even when the status node was most likely there right?\n. :+1: \n. @spotify/helios-team Finally got the tests to pass.\n. Thanks. Yea I forgot about those.\n. @negz @gimaker \n. weird, could've sworn I logged into circleCI's heios-solo container after tests failed and ran java -version only to see 1.7. /shrugs\n. @mattnworb I've added a unit test in the last commit. I left the system tests since they test the master and agent are actually registering the gauges and emitting those metrics. Let me know if these tests are not worth it though.\n. Good points. I'll remove the system test. It added too much code for my\ntaste anyways.\nOn Fri, Feb 5, 2016 at 10:49 AM, Matt Brown notifications@github.com\nwrote:\n\nIf you think the system test adds value I won't quibble with adding it;\nalthough I don't think I would have chosen to test that piece of logic\nmyself.\nBut:\n- adding Gauge and Metrics as Descriptor subclasses feels weird to me,\n  since they really aren't\n- Instead of making them Descriptors to gain access to easy JSON to\n  POJO serialization/deserialization, I personally find it easier to just use\n  ObjectMapper and it's tree API (JsonNode, ObjectNode) in tests\n- I think these classes, and AgentArgsBuilder, should be in\n  src/test/java if they are only for testing purposes and never to be used in\n  the actual main source code\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/840#issuecomment-180411079.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. finally tests passed\n. @mattnworb @negz @gimaker \n. whoops\n. Ah, github is just being dumb. The diff should show a revert of all commits here https://github.com/spotify/helios/pull/384\n. This is just to see if switching to java8 made tests break or be flakier on circleCI\n. The git commit hash changed for that latest commit. But for some reason github doesn't realize I removed 4 commits from master. I'll try this again.\n. Assuming it all works :+1: \n. The last commit to fix the race makes sense.\n. Yea it'd be nice to have tests for this :)\n. I haven't gotten a chance to read the whole thing yet, but do we want another test for CertificateAndPrivateKey?\n. @pehrs Thanks for the PR. Just curious what's your use case?\n. @mattnworb @pehrs I added a test for UDP probing.\n. @mattnworb @pehrs I added a test for this. Let me know if it looks OK.\n. Yup. By specifying .port(\"default\", 4711, \"udp\") here https://github.com/spotify/helios/pull/848/files#diff-6da031aab2d8f23a98de0f1b4163735cR62, the prober will send a UDP packet that will be ignored instead of a port not reachable.\nA temp job will block on the deploy() until the probing for all specified ports is successful and will throw an error if any port fails to be probed.\nAs for the indirection, I wanted to use certain variables defined in TemporaryJobsTestBase here https://github.com/spotify/helios/blob/master/helios-testing/src/test/java/com/spotify/helios/testing/TemporaryJobsTestBase.java#L35, and I think I have to nest the test in order to use them. Let me know if I'm wrong or if there's a better way.\n. Nice catch :+1: \n. Interesting that we have such a specific test...\n. @spotify/helios-team \n. Let me know if tests like this are worth having\njava\n@Test\n  public void testEnsurePath() throws Exception {\n    final ReportingZooKeeperClient reportingClient =\n        new ReportingZooKeeperClient(client, reporter, TAG);\n    reportingClient.ensurePath(\"path\");\n    verify(reporter).time(eq(TAG), eq(\"ensurePath\"), Matchers.any(ZooKeeperCallable.class));\n  }\n. :+1: \n. Yea not sure.\n. @spotify/helios-team \n. awaitHostRegistered() checks that the hostname is returned from ZooKeeperMasterModel.listHost(). listHost() returns the znode children of /config/hosts. Since AgentZooKeeperRegistrar.tryToRegister() happens first and writes /config/hosts/<hostname>, this should guarantee that when awaitHostRegistered() returns, /config/hosts/<hostname> will exist.\n. @spotify/helios-team \n. @spotify/helios-team \n. ah you're right. \n. @spotify/helios-team \n. Just a general question: doesn't docker pull first compare the sha256 hashes of the local and remote layers an image is made of and only pulls layer content from a remote registry if any of the hashes are different?\n```\ndocker pull registry.spotify.net/spotify/trusty-python34:0.22\n0.22: Pulling from spotify/trusty-python34\n65eee62c6cfe: Already exists\nc6b72cd673e6: Already exists\nd0966855f86e: Already exists\n...\nDigest: sha256:683b39ab2a34076b99804ef0dcce79b9ae3781059c62f1b8eee765eaa684ff8f\nStatus: Image is up to date for registry.spotify.net/spotify/trusty-python34:0.22\ndocker daemon log\ntime=\"2016-02-19T18:43:03.395871333Z\" level=debug msg=\"Calling POST /images/create\"\ntime=\"2016-02-19T18:43:03.395917966Z\" level=info msg=\"POST /v1.20/images/create?fromImage=registry.spotify.net%2Fspotify%2Ftrusty-python34%3A0.22\"\ntime=\"2016-02-19T18:43:03.395934209Z\" level=debug msg=\"Warning: client and server don't have the same version (client: 1.8.1, server: 1.8.3)\"\ntime=\"2016-02-19T18:43:03.430087944Z\" level=debug msg=\"hostDir: /etc/docker/certs.d/registry.spotify.net\"\ntime=\"2016-02-19T18:43:03.448096580Z\" level=debug msg=\"hostDir: /etc/docker/certs.d/registry.spotify.net\"\ntime=\"2016-02-19T18:43:03.448173171Z\" level=debug msg=\"Trying to pull registry.spotify.net/spotify/trusty-python34 from https://registry.spotify.net v2\"\ntime=\"2016-02-19T18:43:04.434619215Z\" level=debug msg=\"Pulling tag from V2 registry: \\\"0.22\\\"\"\ntime=\"2016-02-19T18:43:05.428247437Z\" level=debug msg=\"Using fallback hash for sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\"\ntime=\"2016-02-19T18:43:05.428401918Z\" level=debug msg=\"H({\\\"architecture\\\":\\\"amd64\\\",\\\"author\\\":\\\"Rohan Singh \\\\\"rohan@spotify.com\\\\\"\\\",\\\"config\\\":{\\\"Hostname\\\":\\\"b18b64a5fb68\\\",\\\"Domainname\\\":\\\"\\\",\\\"User\\\":\\\"\\\",\\\"AttachStdin\\\":false,\\\"AttachStdout\\\":false,\\\"AttachStderr\\\":false,\\\"Tty\\\":false,\\\"OpenStdin\\\":false,\\\"StdinOnce\\\":false,\\\"Env\\\":null,\\\"Cmd\\\":null,\\\"Image\\\":\\\"\\\",\\\"Volumes\\\":null,\\\"WorkingDir\\\":\\\"\\\",\\\"Entrypoint\\\":null,\\\"OnBuild\\\":null,\\\"Labels\\\":null},\\\"container\\\":\\\"b18b64a5fb6864bcd00141410f21f7ccc83d6a73c1cd0c043d6977cb9fb53e48\\\",\\\"container_config\\\":{\\\"Hostname\\\":\\\"b18b64a5fb68\\\",\\\"Domainname\\\":\\\"\\\",\\\"User\\\":\\\"\\\",\\\"AttachStdin\\\":false,\\\"AttachStdout\\\":false,\\\"AttachStderr\\\":false,\\\"Tty\\\":false,\\\"OpenStdin\\\":false,\\\"StdinOnce\\\":false,\\\"Env\\\":null,\\\"Cmd\\\":[\\\"/bin/sh\\\",\\\"-c\\\",\\\"#(nop) MAINTAINER Rohan Singh \\\\\"rohan@spotify.com\\\\\"\\\"],\\\"Image\\\":\\\"\\\",\\\"Volumes\\\":null,\\\"WorkingDir\\\":\\\"\\\",\\\"Entrypoint\\\":null,\\\"OnBuild\\\":null,\\\"Labels\\\":null},\\\"created\\\":\\\"2015-03-24T19:49:23.748828324Z\\\",\\\"docker_version\\\":\\\"1.5.0\\\",\\\"layer_id\\\":\\\"sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\\\",\\\"os\\\":\\\"linux\\\"}) = sha256:65eee62c6cfe8f1083007fb1b3b073c7f291d8221eb80a4e9d23b1b5d0397cd4\"\n```\n. :+1: \n. @rohansingh \ne.g. libc6 vuln\n. @spotify/helios-team \n. @mattnworb Thanks for the suggestions.\n. @spotify/helios-team \n. Reap not deployed jobs with no history if their creation date is null or too long ago.\nRefactor a bit for readibility.\n. nice tests\n. :+1: \n. checkstyle womp womp\n. :+1: \n. LGTM\n. LGTM as well, but I'm not too familiar with this part of the code. @rohansingh Is the best to ask.\n. :+1: \n. :+1: \n. :+1: \n. TODO: add test\n. @gimaker @mattnworb @negz \n. @mattnworb I added a couple of unit tests. I left the original test. Let me know if I should delete that one and whether to add more unit tests for the various conditionals in undeployLeftoverJobs().\n. @negz @mattnworb \n. If HSD could keep track of the docker containers it deployed that would be a more direct way. Right now it only has indirect references to docker containers via helios jobs. To make this more transparent and less work for users, TempJobs would then somehow need to know it's using HSD and tell HSD about what it's deployed.\nI think for the time being, this PR will still help. I'm not sure I want to alter TempJobs that much right now...\n. When you say \"trying to do cleanup on the java side, in HeliosSoloDeployment.close(), is bound to always miss stuff,\" do you mean have logic in start.sh instead?\n. @gimaker \n. @mattnworb I explored how to optionally set HSD in TemporaryJobs so that when a job is deployed, TemporaryJobs sends a reference of the docker container ID to HSD. But there are many more levels of indirection than I first thought: TemporaryJobs -> TemporaryJobsBuilder -> Deployer -> DefaultDeployer -> TemporaryJob.\nI'm leaning towards just merging this. Let me know what you think.\n. Merging for now. I'm open to better solutions as we think of them.\n. @rohansingh @gimaker \n. My second commit also fixes it for tests.\n. @gimaker \n. This PR points out an existing issue, but, yes, we should minimize user effort.\n. @jwiklund\n. @gimaker \n. It would've been nice to just output the docker log <container ID> command directly, but TemporaryJob doesn't know about HSD.\n. sure I'll merge.\n. 1 and 2. good ideas\n3. capAdds was nice and terse compared to addedCapabilities, but I'll go with a bit longer but clearer.\n. yay, build is green\n. :+1: \n. @mattnworb \n. @gimaker \n. @mattnworb \n. I forgot to support for CLI flags. The previous commit supported reading from a job file.\n. This should be good to go now.\n. :+1: \n. @gimaker @rohansingh @mattnworb @negz \n. We should somehow pin our skydns build artifact. Right now skydns is rebuilt from master here https://github.com/skynetservices/skydns whenever someone manually builds helios solo base image. We have no way of knowing what kind of skydns we've built later.\n. This doesn't work on my mac now. See #900 for an alternative approach\n. @negz @mattnworb @rohansingh @gimaker \n. @gimaker If it looks good to you, I'll merge and try to get a release out.\n. We end up needing commits like https://github.com/spotify/helios/pull/900/commits/1c8b1241ff0d99aa529c63a942e0221ce4f47c73\n. cc @gilles \n. \ud83d\udc4d \n. A migration of ZK data will be tricky. Will older helios CLIs be able to handle the new types of data? We've had issues with data/schema changes causing old CLIs to break.\n. I saw the tests failed because com.spotify.helios.system.DeploymentGroupTest hung for more than 10 minutes. I retried and the same thing happened. Is there something that makes that test hang?\n. For the new classes, it's helpful to have javadocs briefly explaining how they're used by other classes. A great example is the one you did for DeploymentGroupResponse.\n. I looked at about half of it, I'll go through the other half soon.\n. It doesn't, but even if data and code in the masters are compatible, that\ndata could make old clients blow up right? An example is how the\nHeliosClient.deploymentGroup() now returns a DeploymentGroupResponse\ninstead of a DeploymentGroup. I think old clients will choke because they\nwon't know how to deserialize this. Let me know if I'm missing something.\n. Ah, that's right.\nOn Mon, Apr 25, 2016 at 2:22 PM, Matt Brown notifications@github.com\nwrote:\n\nas long as the JSON returned by the endpoint in the new version matches\nthe JSON that was returned by the old version (i.e. what an old CLI client\nexpects), then it should be fine.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/903#issuecomment-214469734\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. Would be nice to have docs :)\n. I had the same question. Everything seems to still work if I change Object operand to String operand and BiPredicate<String, Object> predicate to BiPredicate<String, String> predicate.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. LGTM\n. @mattnworb @negz @gimaker \nDon't know what's up with codecov.\n. Thanks to @negz for pointing out that we have code in HeliosSoloDeployment that sets env vars for the solo startup. I think it's better to put this logic there as we're doing something similar for --name.\nI think name is used for agent registration/hostname and the id is used to make sure the same hostname won't take over unless it also has the same id. This id is also used for the namespace in Reaper.\n. @spkrka Thanks for your PR. I just released 0.8.780 which has this patch https://github.com/spotify/helios/pull/910. Do you mind if we close this? We can always reopen and adopt if we find we need it later.\n. @mattnworb @spkrka @gimaker @negz How about this? https://github.com/spotify/helios/pull/914\n. @gimaker @mattnworb @negz \n. @gimaker Good points. See 2nd commit.\n. @gimaker @negz @mattnworb \n. @negz @gimaker @mattnworb \n. @mattnworb Regarding https://github.com/spotify/helios/pull/917#issuecomment-219407338, yes. I was planning on this TempJob change only supporting helios-solo.\n. Replaced by #922 as neither staffan nor I could figure out how to make tests pass. #922 has smaller, easier to understand commits that all pass tests.\n. CircleCI now takes twice as long to build :) I suspect many of the tests no longer make sense, eg HeliosIT or the tests that inherit TemporaryJobsTestBase.\n. Tests became flaky starting with at least commit e014893. Ignoring the mind-blown inception of HeliosIT deploying a helios master, ZK, and agent as separate containers using helios-solo, how is the agent job already deployed? https://circleci.com/gh/spotify/helios/4547. I don't see any other instances where we deploy the agent as a job.\n. @gimaker Would you say this one is ready for review?\n. @mattnworb This is ready for review. I'm trying to keep each commit green and reviewable by itself when taken in order.\n. Yes, we can.\nOn Thu, May 26, 2016 at 9:35 AM, Matt Brown notifications@github.com\nwrote:\n\n@davidxia https://github.com/davidxia great, will take a look today.\nCan we merge and release #927 https://github.com/spotify/helios/pull/927\nfirst though, so that users can upgrade to it and not have to deal with any\nincompatibilities before having to deal with the incompatibilities in this\nchange?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/922#issuecomment-221872106\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. @mattnworb regarding https://github.com/spotify/helios/pull/922#issuecomment-221967892. I'm pretty sure users can still specify REGISTRAR_HOST_FORMAT with a helios.conf file in their resources directory.\nI had to change the SRV record in HeliosSoloIT because HeliosSoloDeployment is created automatically now by TemporaryJobs and I didn't know how to programmatically alter the helios.conf file in the resources file. So I'll remove the TODO. But let me know if there's a better way to test this or if I'm mistaken about the preservation of the functionality.\n. I'll hold off on updating the docs for now. Things are probably going to change. I left it as a TODO in the PR description above.\n. Yea, I was thinking the same.\nOn Tuesday, May 31, 2016, Matt Brown notifications@github.com wrote:\n\nI like the idea of including a documentation update in the PR that makes\nthe change itself, so that it is much harder to forget to update the docs.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/spotify/helios/pull/922#issuecomment-222753523, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/AAdVbQqszX9AEdm7Rd-C4F5XKvMejSr1ks5qHGotgaJpZM4IiRpz\n.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. Mats said we don't need these metrics anymore. And the code for it wasn't\ngreat to start with I guess.\nOn Friday, May 20, 2016, Matt Brown notifications@github.com wrote:\n\nI love diff stats like +53 \u2212489 but what was the reason for this change?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/923#issuecomment-220570584\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. As of 6f39ad7, not specifying a HeliosDeployment for TemporaryJobs will start helios-solo:\n``` java\npublic class FooTest {\n@Rule\n  public TemporaryJobs temporaryJobs = TemporaryJobs.create();\n@Test\n  public void test() throws Exception {\n    final TemporaryJob memcacheServerJob = temporaryJobs.job()\n        .image(\"spotify/lasertag-memcached:latest\")\n        .port(\"memcached\", 11211)\n        .deploy();\n    System.out.println(\"hi\");\n  }\n@Test\n  public void test2() throws Exception {\n    final TemporaryJob memcacheServerJob = temporaryJobs.job()\n        .image(\"spotify/lasertag-memcached:latest\")\n        .port(\"memcached\", 11211)\n        .deploy();\n    System.out.println(\"there\");\n  }\n}\n```\nSpecifying one will use that pre-existing one:\n``` java\npublic class FooTest2 {\n@Rule\n  public TemporaryJobs temporaryJobs = TemporaryJobs.builder()\n      .heliosDeployment(ExistingHeliosDeployment.newBuilder()\n                            .domain(\"example.com\")\n                            .build()\n      ).build();\n@Test\n  public void test3() throws Exception {\n    final TemporaryJob memcached = temporaryJobs.job()\n        .image(\"spotify/lasertag-memcached:latest\")\n        .port(\"memcached\", 11211)\n        .deploy(\"heliosagent1.example.com\");\n    memcached.hosts();\nSystem.out.println(\"HOO\");\n\nfinal Undeployer undeployer = temporaryJobs.undeployer();\nfinal List<AssertionError> errors = undeployer.undeploy(memcached.job(), memcached.hosts());\nSystem.out.println(errors);\n\n}\n}\n``\n. What class should do the undeploying of aTemporaryJob? It's theTemporaryJobitself right now in master, but we said this class was doing too much. In my example above, there's anUndeployeryou get fromTemporaryJobs, but that seems ugly as well.\n. @gimaker Want to take a stab at splitting upTemporaryJobsintoRuleandCloseable` or do we still need more refactoring?\n. Look at all those beautiful green checkmarks with verified badges!\n. @gimaker Your changes LGTM. Shall we merge?\n. All commits have been cherry-picked to https://github.com/spotify/helios/pull/922\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @mattnworb \n. \ud83d\udc4d  Were we just afraid of cruft buildup originally?\n. :+1:\n. It seems worth it to me for the reasons Matt pointed out. Any reduction in\nduplicate logic if possible would be good though.\nOn Monday, June 6, 2016, Nic Cope notifications@github.com wrote:\n\nI won't argue that they don't add value - my main concern is around the\ncognitive and (potential) support burden of maintaining two similar yet\ndifferent implementations of the same concept. I wonder whether the value\nthey add is worth that.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/spotify/helios/pull/934#issuecomment-223960123, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/AAdVbWzM8xOQVJzUNLne3ovM6Ql_6fGCks5qJCHygaJpZM4IqspS\n.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. \ud83d\udc4d \n\ud83d\udcaf on refactoring the tests\n. Good catch. Lgtm\n. good idea \ud83d\udc4d \n. \ud83d\udc4d \n. cc @spotify/helios-team \n. @negz Would like to get this into the next helios deployment.\n. @drewcsillag Cool, thanks. I'll do this.\n. Is there a better method than this? toString() will return helios (srv: ash).\n. Yea I would've liked that, but if you use --master <endpoint>, there won't be a domain. Just a single endpoint. \n. :+1: \n. Sure thing.\n. I had to remove this because failed helios create --json commands won't return a jobId since no job was created. Let me know if this will be a problem to remove this.\n. I replaced this test class with helios-system-tests/src/main/java/com/spotify/helios/system/CliJobCreationTest.java:128.\n. I replaced this test class with helios-system-tests/src/main/java/com/spotify/helios/system/CliJobCreationTest.java:128.\n. Ah silly me. I missed that\n. @rculbertson How can I make the returned SrvTarget object play nice with the return type of this function?\n. Changes to this file will break users who rely on site:// in their .helios/config file. But I doubt that many people are using this feature. We can check both and merge them together like we're doing for the CLI args. Do this for a period of time. Then switch off the deprecated site://.\n. Support the old site:// for a little while here.\n. @drewcsillag Wow thanks. Wouldn't have known this.\n. Oh duh, yea.\n. Doesn't really matter, but perhaps hostPattern.isEmpty()?\n. ts seems unused?\n. I'm guessing the full variable is supposed to indicate whether to return the full hostname? Not being used yet though.\n. Would it be taskStatuses.put(formatHostname(full, host), null); on line 170?\n. @rculbertson Should we check job != null before this?\n. I noticed that dockerd provides redundant info in both containerConfig and config. Is one preferred over the other?\n. @dano yea I saw that the docker API returns the same data in \"config\" as\n\"container_config\" so I was curious why the redundancy.\nOn Monday, November 17, 2014, Daniel Norberg notifications@github.com\nwrote:\n\nIn\nhelios-services/src/main/java/com/spotify/helios/agent/SyslogRedirectingContainerDecorator.java:\n\n@@ -57,8 +57,9 @@ public void decorateHostConfig(HostConfig.Builder hostConfig) {\n   @Override\n   public void decorateContainerConfig(Job job, ImageInfo imageInfo,\n                                       ContainerConfig.Builder containerConfig) {\n-    ContainerConfig imageConfig = imageInfo.containerConfig();\n-    ContainerConfig imageConfig = imageInfo.config();\n\n@drewcsillag https://github.com/drewcsillag Not sure about the rename.\ncontainerConfig is already used by the parameter with the same name,\nwhich actually holds the config of the container we're about to start.\nimageConfig is the config of the image used by the container.\n@davidxia https://github.com/davidxia dockerd seems to provide the\ninherited/effective CMD in imageInfo.config. API docs are not very\ninformative on this point. Might be worth investigating a bit more.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/299/files#r20475308.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. and on job inspect, start, and stop?\n. User can configure it to use either Unix sockets or HTTP. It defaults to Unix.\n. Do we actually specifically build for Ubuntu 14.04 or is it just a generic deb package? Is it built from jdeb plugin specified in pom.xml? I'm not too familiar with this part.\n. sounds good to me\n. good point\n. Basically, I moved registerHost() and deregisterHost() and any helper methods to a new class ZooKeeperRegistrarUtil since I needed to reuse that logic in AgentZooKeeperRegistrar.\n. At first it looked like ZooKeeperRegistrar was a good class to put this, but I think that class does something very different and is mostly used as an instance. registerHost() and deregisterHost(), on the other hand, work nicely as static methods.\n. Added an extra method here for touching files.\n. Ah so this logic was moved to https://github.com/spotify/helios/pull/376/files#diff-d935303eb7db0e249425458704c02455R246?\n. Should tags be a List rather than a Set as the description here says?\n. Perhaps update this comment to be like this since it's no longer a placeholder?\n. @drewcsillag Hm, what did you mean by this comment?\n. I'm just trying to understand this test. So we poll the \"poke\" port until we can make a TCP connection to it. Then we check that registrar.register() has been called within LONG_WAIT_SECONDS because that means the health check was executed and succeeded?\n. capitalize \"http\" and remove the \",\" since it's not a compound sentence?\n\"will make\" -> \"makes\" to get rid of one word? In this case, also \"consider\" -> \"considers\"\n. \"if it's\". nitpickiness: This health check is successful -> This health check succeeds (more active voice with one less word).\n. \"Register the service in service discovery\" Won't always be nameless, and this is public so external people won't know about nameless :)\n. replace nameless with service discovery or service registrar. \"before registering your service with whatever service registration plugin you provided.\"\n. For some reason, I found \"using exponential backoff from 1 second to 30 seconds\" confusing. Maybe \"using exponential backoff that ranges randomly from 1 to 30 seconds.\"?\n. Yea it can all be on the same line.\n. Should we just keep health checking forever? The state will stay as HEALTHCHECKING. Should it be marked as FAILED?\n. Yea we can leave it for now and let helios-helper handle poll for RUNNING. If that doesn't happen within X time, we undeploy it and fail the build. We can add that logic here https://ghe.spotify.net/helios/helios-helper/blob/master/helios_helper/helios_deploy_job.py#L155\n. @rculbertson I get -1 on my boot2docker version: v1.3.2, Git commit: e41a9ae. executionDriver() returns \"native-0.2\"\nWhat version of boot2docker are you using?\n. I just upgraded from 1.3.2 to 1.5.0 and got 137.\n. I guess not? I forgot to remove that\nOn Friday, March 6, 2015, Ryan Culbertson notifications@github.com wrote:\n\nIn src/deb/helios-agent/control\nhttps://github.com/spotify/helios/pull/384#discussion_r25987314:\n\n@@ -3,7 +3,7 @@ Version: [[version]]\n Section: non-free/net\n Priority: optional\n Architecture: all\n-Depends: java7-runtime, helios-services (= [[version]])\n+Depends: java7-runtime, java8-runtime, helios-services (= [[version]])\n\nAny reason to require 7 as well?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/384/files#r25987314.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. Why this change? Will the hardcoded version be a problem?\n. Maybe update the JSON in the javadoc above?\n. That's only for strings right? Now this var is a List\n. Haha, you mean Latin points. We should add NB, quid pro quo, etc...\n. Can we also update the javadoc above at https://github.com/spotify/helios/pull/504/files?diff=split#diff-7501c3813397bd34d3f91f9c755fcda6R56?\n. I guess we don't want to do this because then the JSON output will be slow?\n. Perhaps it's better to just have users remember to configure it to output to target or add to .gitignore?\n. doh, thanks\n. @rohansingh It's the client.transaction() that throws the NoNodeException right? How can this happen with ensurePath()?\n. Should we add an ensurePath() for those paths as well? Does this happen when the master is slow to startup?\n. It will work after we merge :). This is just internal variable naming. The json key will still be \"job\" like in Deployment.java\n. Maybe a bit more info about how this might've happened like the message here https://github.com/spotify/helios/pull/556/files#r35044609?\n. @rohansingh I just saw in the master logs that e.getPath() sometimes throws NPE because e.path is null. What should we do in that case?\n. Removed JobPortAllocationConflictException since we don't check for it. Should we?\n. We aren't checking if a host exists here. We check in getUndeployOperations(). Should we check again here or wait for rollingUpdateAwaitRunning() to time out?\nThe chances of an agent going away between undeploy and await running are slim. But if it does happen, the error message in the deployment group status will be clearer than just timed out while retrieving job status.\n. Can we change this back? See https://github.com/spotify/helios/pull/554\n. Can you also add a testInvalidHostnames() and give hostnames whose length exceeds 63 chars or contain invalid chars?\nYou can add JobValidator.validateJobHostname() and call it here https://github.com/spotify/helios/blob/master/helios-client/src/main/java/com/spotify/helios/common/JobValidator.java#L72. Should we have this method accept only [a-z0-9-] where the first char has to be alphanumeric according to RFC 1123? No upper case letters or let those through?\n. Can you also update this part of the docs? https://github.com/spotify/helios/blob/master/docs/user_manual.md#using-a-helios-job-config-file\n. ah good point\n. Yea, quite gross\n. good point\n. Yea I was wondering if the client should be smart like that. You are too nice to the user. I can change it.\n. For some reason, the memory limits aren't being respected on docker.createContainer() here.\n. Ah yes. Because of the argparse Choices, if the user uses lowercase, an error will occur before we even get here.\n. It should work. Here's a test: https://github.com/spotify/helios/pull/587/files#diff-dc34e9cadd23a8d110d25e12c41041e8R81\n. Just tested both manually too just to make sure. status is just set to null.\n. Doesn't work on circleci.\n. NB\n. For some reason, cmd() can return null now.\n. I made it a map in case we want to add more query parameters in the future. And uri() takes a map anyways.\n. Hostnames are case insensitive. Perhaps remove the uppercase letters here? @rohansingh why do we have capital letters in the DOMAIN_PATTERN below?\n. Nitpick: use Strings.isNullOrEmpty() as we do that throughout the rest of the code.\n. The checkstyle plugin doesn't check test code, but we'd like to keep these lines here to a maximum of 100 chars.\n. Can you also add \"hostname: The hostname to pass to the container\" to the explanation below starting at line 174? Make sure to put it in the same order as the JSON here.\n. Should be\njava\n    final String hostname = options.getString(hostnameArg.getDest());\n    if (!isNullOrEmpty(hostname)) {\n      builder.setHostname(hostname);\n    }\nso we don't override hostnames set in the job config file.\n. Use a more specific variable name?\n. client is unused\n. I added the @DefaultValue(\"\") since it's more explicit.\n. I added the @DefaultValue(\"\") since it's more explicit.\n. Unused vars\n. This seems ugly. And which class should such a function live (this class is already too big).\n. I'll limit to 10. See getPreviousJobStates()\n. Ah thanks. I think our pattern is to use @Nullable in these cases with this.failureThreshold = Optional.fromNullable(failureThreshold).or(0); because @DefaultValue() only takes String.\n. Can we make the method names and eventType the same?\n. Oh nvm/\n. Is it worth it to create classes for these objects?\n. Yea a separate commit sounds good. Something like the below?\nDeploymentGroupEvent class:\n- eventType\n- timestamp\n- deploymentGroup\n- success\n- reason\n- failedTask\nDeploymentGroupTaskEvent class inherits from DeploymentGroupTask and also includes:\n- action\n- target\n- error\n- errorCode\n- metadata\n. Good idea.\n. IntelliJ added it for me. Idk why. I'll remove.\n. Seems reasonable, but IntelliJ complains that previousStates = emptyList(); in the catch might have already been assigned to.\n. Good point.\n. @DefaultValue() only takes String and seems like it's used for a different purpose.\n* Defines the default value of request metadata that is bound using one of the \n * following annotations:\n * {@link javax.ws.rs.PathParam}, \n * {@link javax.ws.rs.QueryParam}, \n * {@link javax.ws.rs.MatrixParam},\n * {@link javax.ws.rs.CookieParam},\n * {@link javax.ws.rs.FormParam},\n * or {@link javax.ws.rs.HeaderParam}.\n * The default value is used if the corresponding metadata is not present in the\n * request.\n. I thought it made it easier to read. But I can revert.\n. yes yes\n. Done.\n. For existing deployment group data in ZK, I think we can do:\n@JsonProperty(\"numTargets\") @Nullable final Integer numTargets,\nand\nthis.numTargets = Optional.fromNullable(numTargets).or(0);\nYea, any host failing will cause the DG to fail: https://github.com/spotify/helios/pull/631/files#diff-850af0ee5778ee512a514c4dcbd33b7cR396\n. Here's the good stuff.\n. Couldn't find an easy way to set the ttl to a positive number but not slow down the test by passing in a fake Clock object via CLI args here. See https://github.com/spotify/helios/pull/644/files#diff-dce8417dd28c57a529199badc738ddf3R80\n. Thanks\n. Making a new env var to not break users, if any, that were using HELIOS_SRV_FORMAT.\n. @rohansingh Should we check that transaction() was called with these particular ops or just verify that the call was made? Test is brittle because if implementation changes, it will fail.\n. I'm going to go with the first approach. We already have ITs for deregistering in DeregisterTest.\n. Couldn't find an easy way to set the ttl to a positive number but not slow down the test by passing in a fake Clock object via CLI args here. See https://github.com/spotify/helios/pull/654/files#diff-dce8417dd28c57a529199badc738ddf3R71\n. Adding this breaks the tests. @gimaker @rohansingh @mattnworb maybe you could take a second look with me when you guys get a chance.\n. Yea, but I have to figure out a way to pass a fake clock via the CLI args. That isn't straight forward :(\n. Consider using finals here. We seem to like that.\n. s/=/delimiter/\n. Yea, let me try that out.\n. Actually I already have an end-to-end test for this in DeregisterTest.testRegistrationResolution()\n. Isn't this a getter?\n. Thanks. I was wondering whether it belonged there.\n. yes with --user <username\n. private SSH key\n. How about starting the master with --auth and if no --auth-plugin <path/to/jar> is provided, it uses crtauth?\n. Another switch here when starting the master to make auth required for client versions greater than X? Defaults to requiring auth regardless of version if --auth is specified.\n. I had docs in this PR: https://github.com/spotify/helios/pull/655/files#diff-ca6bc2e6c7f2fbcadd6202b5707a25adR108. TLDR: use a web server to handle SSL/TLS\n. Did the first. Maybe your second point would work? But I see that Proxy.Type only has HTTP, SOCKS, and DIRECT.\n. We can lift this method and the one from helios-service-registration, as well as the two method above this, into a new subproject (eg helios-plugins) and have these two depend on that one in their poms.\nIt's nice to not have to include any helios projects in the service provider poms though. If we didn't have to create, release, and depend on another helios subproject, this would be good to do, but it feels a bit not worth it to me?\n. yea good idea\n. yup\n. I'll do this for the other Nops\n. thanks for the reminder\n. Maybe just call this file \"Security\" since we can talk about encrypted comms, authentication, and later authorization together here? We can append a link like this to README.md:\n* **[Security](security.md)**\n  How to secure Helios\n. I don't think this variable name is long enough :)\n. Should we pick either all or yes?\n. I have no preferences. One switch seems easier for user but slightly more complex code. Two switches might be simpler code.\n. You're right. basic auth, oauth also require credentials to be sent in the client's Authorization request header. We can remove this method.\n. Must've left this by accident.\n. s/it's/its/\n. s/apply to a certain/apply to certain/\n. s/version/versions/. We also don't need the comma\n. Do we need to catch this? I can't see what in the try would throw this.\n. Ah ok\n. nit: don't need this comma here?\n. nit: don't need this comma here either (not an independent clause).\n. s/Helio's/Helios'/\n. Perhaps we should document this with a TODO or solicitation for contributions of different ways to look up passwords?\n. Ah, I see the javadoc above. Maybe just mention that this implementation isn't very ready in the docs but that PRs are welcome?\n. Do we need this line?\n. nitpick: newline here and in other files?\n. Should we also mention http-basic here or is the implementation too \"basic\" to be of any real use?\n. Would requiring config file(s) be better than all these env vars? We could look in /etc/default/helios-master (which is already being written to and read from by the debian package) and then fallback to a json file in the current working directory.\n. If we go with using config files (see my comment https://github.com/spotify/helios/pull/698/files#r42532056), maybe users can specify the type of KeyProvider in the config file?\nShould we allow combining multiple types of KeyProviders? Users might want to have LDAP for humans but flat files for CI agents because their IT department doesn't like adding non-human LDAP credentials. I'm not sure if this would make the implementation less secure though, eg timing attacks.\n. :+1: \n. I need the crt auth server here somehow.\n. Ah, If we're writing tokens to disk at a fixed path, should we make sure the writing to file part is thread-safe?\n. yea thanks for noticing this\n. I'm pretty much only using AtomicReference so I can later modify it inside the anonymous Function class here https://github.com/spotify/helios/pull/712/files#diff-04d67aa64d1fea0945d7a366258ab0ebR119. Is there a nicer way?\n. > Is there a way to tell if a token is expired on the client-side?\nWith crtauth no :( We can use any available cached tokens and if that gets\n401, we redo the handshake to get a new one and cache that?\nOn Fri, Oct 23, 2015 at 10:17 AM, Staffan Gim\u00e5ker notifications@github.com\nwrote:\n\nIn\nhelios-client/src/main/java/com/spotify/helios/auth/CrtAuthProvider.java\nhttps://github.com/spotify/helios/pull/712#discussion_r42869304:\n\n\nfinal String username,\nfinal Signer signer) {\nthis.dispatcher = dispatcher;\nthis.username = username;\nthis.crtClient = new CrtAuthClient(signer, authServer);\n}\n  +\n@Override\npublic String currentAuthorizationHeader() {\n// TODO (dxia) Should we use a lock here? It doesn't really matter if the token isn't\n// thread-safe. We should get valid tokens each time. The lock just prevents\n// another thread from doing extra work to get an extra token when it could just wait.\nif (token == null) {\nsynchronized (lock) {\ntry {\ntoken = renewAuthorizationHeader(null).get();\n\n\nWe probably need to tweak the AuthProvider interface to work well with\nthe stored token, or rethink something else.\nCurrently, with AuthProviderSelector, the first request is always sent\nwithout authorization. Upon getting a 401 back it will select an\nAuthProvider and proceed to call renewAuthorizationHeader(). Now, we\ncould tweak that to only call renewAuthorizationHeader() if the\ncurrentAuthorizationHeader() returns null on the selected provider - BUT,\nif we do that and the current token is expired the request will failed. Is\nthere a way to tell if a token is expired on the client-side?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/712/files#r42869304.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. Is the semicolon here https://github.com/spotify/helios/pull/713/files#diff-7f9fb5083fc38f934dca424ad96be781R111 required?\n. :+1: \n. the problem is if it's not final, I can't reference it inside the anonymous Function class. And if it is final, I can't change it. So I need a reference to a String instead of a String itself.\n. I could create an inner class but then I still need to pass the String by reference or somehow modify the enclosing instance of the inner class.\n. Yup can do.\n. 0.1.2 is released on maven central now\n. I changed this in 0.1.2 to be final AgentProxy agentProxy = AgentProxies.newInstance(), ie I hid the impl class in a static factory.\n. Same here final AgentProxy agentProxy = AgentProxies.newInstance()\n. I updated java docs for AgentProxy.sign() https://github.com/spotify/ssh-agent-proxy/commit/db2f61cc6526460aa8cf0f2cc63afa8f225bd505\n. oh yea duh\n. Any reason we don't get a list of identities once and store it outside the for loop?\n. I could've used guava's Iterators.cycle() which will create an Iterator that cycles. But this doesn't let me randomize the starting point. I can declare endpointIterator as an EndpointIterator or make a CyclingIterator interface that extends Iterator. I noticed guava doesn't do this though. Their Iterators.cycle() just returns Iterator<T> and they note the cyclic nature in the javadoc.\nNo strong preferences here. Let me know what you think.\n. Yea I didn't change it at first to keep the code changes minimal. But I can do that.\n. @rohansingh wrote this part and yea I was wondering why he used Deque. I'm going to leave this up to him though as it's not related to this PR.\n. oops\n. thanks I always forget to make defensive copies.\n. It is. I lifted this code from github.com/spotify/futures-extra. I'll remove the predicate.\n. Do we not need this anymore?\n. Ah ok\n. I couldn't figure out how to modify checkstyle.xml to allow this.\n. @rohansingh We need some way to differentiate between app-level and TLS-level unauthed responses. Would a header set by nginx work?\n. This is probably a matter of personal preference, but guava has a isNullOrEmpty() that this code base seems to like a lot.\n. It seems like the updateState() method is a convenience method so the caller doesn't need to remember to call StatusUpdater.update() and catch an InterruptedException. Maybe instead of\nupdateState(FAILED);\n    updateContainerError(containerError);\nyou could do\nstatusUpdater.setState(FAILED);\n    statusUpdater.setContainerError(containerError);\n    statusUpdater.update();\n. It should never return null. I guess isNullOrEmpty() looks nice and protects against NPEs. Otherwise, you could just use isEmpty().\n. :( checkstyle fail. See circleci link\n. Yes. I should be clearer about why using HTTPS, which is good in and of itself for encryption, causes the CLI to try to get your ssh-agent identities, which is needed for authentication which is hopefully done over an encrypted channel.\n. @gimaker @mattnworb This should now happen only once. Open to suggestions on how to improve this message.\n. ah yea, I fixed this in a subsequent commit here https://github.com/spotify/helios/pull/757/commits, but thanks for noticing this.\n. Yea, that was my first attempt, but that broke ZooKeeperMasterModelIntegrationTest. testRemoveNonExistingDeploymentGroup() which expected DeploymentGroupDoesNotExistException: https://circleci.com/gh/spotify/helios/3297\n. The problem mbetter had was that /config/deployment-group/<name> didn't exist but /status/deployment-group/<name> and /status/deployment-group/hosts/<name> did. He couldn't create or remove his DG.\nThis change will remove all relevant DG ZK nodes even if the /config/deployment-group/<name> path doesn't exist.\n. s/Gropup/Group/\n. s/Alredy/Already/\n. are you missing a word in the comment here? s/will usually the/will usually use the/\n. Maybe \"initiated rolling-update\"?\n. @mattnworb The other place is here https://github.com/spotify/helios/blob/master/helios-services/src/main/java/com/spotify/helios/master/ZooKeeperMasterModel.java#L475\n. Is there any benefit to using a try catch here and letting the ClientCertificatePath constructor handle the logic?\n. className is now not needed. We can also delete integrationMode, testHost, masterName.\n. Either change soloResource to solo or the solo on line 23 to soloResource to be consistent.\n. I'm trying to figure out how to create a test suite rule.\n. It needs to be commented out for now because it'll pull down the spotify/helios-solo:latest image from docker hub which doesn't have the watchdog. That's also the reason I had to modify circle.sh to build and tag a spotify/helios-solo:latest image. We can put these back later I think.\n. I've seen this in a few other test classes as well. Just for my own knowledge, What's this supposed to do and why don't we need it?\n. We can add:\nOnly the Helios client supports HTTPS. The master doesn't. One needs a web server like nginx sitting in front to handle the TLS handshake and termination.\nThis web server can validate the client cert itself or by asking another service that checks a keystore like ldap. We could even open-source the x509-verifier and include sample nginx configs.\n. Oh I see you have that below.\n. typo in explicitily.\n. But it doesn't mention that the master doesn't support HTTPS, so maybe we can leave something about that here?\n. openssl req -new -newkey rsa:2048 -days 3650 -nodes -x509 -keyout key.pem \\\n     -out unsigned_cert.pem -subj \"/CN=jenkins\" -sha256\n. make this package-private?\n. It compiles for me. The only thing that directly calls this constructor is the test. MasterService and AgentService just use the public static ctor above.\n. Can't we just use ordinary indexing here, ie %s\\n%s\\n%s? I actually didn't know about explicit indexing syntax, but it seems ordinary indexing is more readable.\n. s/cacheCertrificatePath/cacheCertificatePath/\n. Make this and END CERT public static final Strings in X509CertificateFactory?\n. 0.4 has java8 installed. 0.3 doesn't\n. Should I add some invariants here like checkNotNull(dockerHost)? And if so which ones?\n. These ifs seem ugly. I'm wondering if there's a prettier way.\n. I think this will fix test failures like DeregisterTest.testJobsArePreservedWhenReregistering() here https://circleci.com/gh/spotify/helios/3795.\n. Ah thanks. For context, I changed it to an Integer because every other gauge is numeric, and I didn't know how to make jackson parse something that could be more than one type.\n. Sure thing. I might abandon this as it seems to break a bunch of tests...\n. You're right. IDE didn't catch these.\n. It'd be nice to actually have a no op, but the nice thing about the current interface is that a Timer takes care of timing the duration. It's a bit more unwieldy if the caller has to handle timing a Callable when the Timer class already does this. If there's not a strong objection, I might just leave this here.\n. good point thanks\n. Hm, it might not be too bad. Let me try\n. It's a bit verbose. I'll revert this change and just run it on a personal branch.\n. Nice, I like it.\n. Do you mean predicate.negate()?\n. Any idea about this?\n. Would we ever allow this to be absent? If not, perhaps just checkNotNull() in the constructor and remove the Optional?\n. Why only checkNotNull() in some of the classes that extend SignalAwaitingService?\n. Good point. I've created a new PR using the thread-safe jodatime here https://github.com/spotify/helios/pull/874\n. Whoa, I never knew this was in here.\n. Yea, that's too long. I'll make it configurable.\n. yup\n. Maybe @rohansingh didn't care much about test sources? I might as well fix these too?\n. Thanks. I haven't read much of the Guava docs. But are they saying firstNonNull() is better than or()? It reads like both a fine. firstNonNull() is more concise though.\n\nWhenever you want a null value to be replaced with some default value instead, use Objects.firstNonNull(T, T). As the method name suggests, if both of the inputs are null, it fails fast with a NullPointerException. If you are using an Optional, there are better alternatives -- e.g. first.or(second).\n. sure :)\n\nOn Wed, Mar 30, 2016 at 4:30 PM, Matt Brown notifications@github.com\nwrote:\n\nIn docs/user_manual.md\nhttps://github.com/spotify/helios/pull/892#discussion_r57958285:\n\n@@ -133,6 +133,8 @@ example that uses all the available configuration keys with an explanation of ea\njson\n {\n+ \"addedCapabilities\" : [ \"IPC_LOCK\", \"SYSLOG\" ],\n+ \"droppedCapabilities\" : [ \"SYS_BOOT\", \"KILL\" ],\n\nforgive the nitpick, but how about \"addCapabilities\" / \"dropCapabilities\"\nin the job json so it's not using past tense?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/892/files/e019aa14389f6e4669b79ba0a921969a4aaa993f#r57958285\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. I'll get rid of the builder :)\nOn Thu, Mar 31, 2016 at 4:06 PM, Matt Brown notifications@github.com\nwrote:\n\nIn helios-client/src/main/java/com/spotify/helios/common/JobValidator.java\nhttps://github.com/spotify/helios/pull/896#discussion_r58118888:\n\n@@ -529,4 +523,34 @@ private boolean validateRepositoryName(final String repositoryName,\n   private boolean legalPort(final int port) {\n     return port >= 0 && port <= 65535;\n   }\n+\n-  public static Builder newBuilder() {\n-    return new Builder();\n-  }\n  +\n-  public static class Builder {\n  +\n-    private boolean shouldValidateJobHash = true;\n-    private boolean shouldValidateAddCapabilities = false;\n\nif you really want to make this optional (and I don't think it needs to\nbe), it should at least default to true, so that it isn't accidentally left\nin the off position\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/896/files/6193a4dc713e6138693cb27dd2151fb0a105acaf#r58118888\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. Does this prevent us from uploading snapshots?\n. Yea ill remove.. This JSON doesn't match the class attributes.\n. Do we need this to be annotated with @Nullable? Isn't this required? Same for job and reason below.\n. unused\n. Or is this for backwards compatibility somehow?\n. Is this class necessary or can we just use a Builder?\n. These two can be private.\n. set the reason here too?\n. set the reason?\n. We can include a JSON example to be consistent.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. fixed\n. In that case, I'd ditch the Parameters and just make it a simple Builder.\n. Any reason to not have @JsonInclude(JsonInclude.Include.NON_NULL) as we do elsewhere?\n. Can be private.\n. Any reason to not have @JsonInclude(JsonInclude.Include.NON_NULL) as we do elsewhere?\n. We should make a defensive copy of this list, ie ImmutableList.copyOf().\n. We should make a defensive copy of this list, ie ImmutableList.copyOf().\n. The JSON will only include non null values. See if it makes sense to use.\n. I'm not sure why DeploymentGroup needs Parameters either. It makes sense for Job to have it since it allows us to customize Job.Builder attributes easily.\n. Unused var\n. can probably remove this line\n. test overlap and token as well?\n. Can be package local. While you're at it, a lot of the attrs above can also be private.\n. This makes sense, but was there a reason we didn't do this before?\n. Perhaps keep this as before for easier debugging, ie have it match the method name?\n. Shouldn't we also remove Paths.statusRollingOpsTasks? And shouldn't we use the same code @gimaker had here to ensure consistency?\n. Is it intentional that you don't remove /config/rolling-operations/<uuid>, /status/rolling-operations/<uuid>, or /status/rolling-operations-tasks/<uuid>?\n. ArrayList<RollingOperation> can be ArrayList<> since java7+ has type inference for generic instance creation.\n. can be package local\n. Is this method just @VisibleForTesting?\n. Let's include the group name and say it's getting the ids that failed.\n. nitpick: redundant var\n. Check if the list is not empty and .get(0) instead?\n. Is there a reason to have this setter instead of using a randomly assigned uuid and having the RollingOperation constructor check not null on it? We use this in some tests, but I don't see us testing the Id itself (and we shouldn't, I think).\n. Previously, if the DG didn't exist, no ZK data would be changed. Do we want to update this znode if the deployment group doesn't exist? Seems like we shouldn't?\n. Why not use ZooKeeperMasterModel.getDeploymentGroup()?\n. placeholder for me. I've only reviewed up to here.\n. Just test they exist?\n. Yea, I think it's clearer/more readable to .get(0) when you actually mean to get the first item. In this case with staffan, it actually made sense to use a for loop because I was actually iterating through the whole way.\n. Hm, we actually don't use NON_NULL anywhere expect DeploymentGroup. Just leave this as is then.\n. My IntelliJ is catching this. But sure, add something to checkstyle.xml.\n. Sorry, I must've missed that one. Can you link me to it?\n. derp, how did we miss this? The log messages and pull(PROBE_IMAGE) are wrong. PR here to fix: https://github.com/spotify/helios/pull/918\n. Oops, I meant printf\n. sounds good\n. Yes, the idea is for users to not to use HeliosSoloDeployment at all. I'm also planning on making breaking changes to remove the innumerable configuration options.\n. missed that one, thanks\n. Thanks! This is much better.\n. I'll log something.\n. This is one of the words I always misspell. Along with diarrhea\n. good idea. It'll prevent mistakes if we ever change the port.\n. Why nice?\n. This calls into Files.deleteIfExists() which according to its javadocs:\n* @return  {@code true} if the file was deleted by this method; {@code\n     *          false} if the file could not be deleted because it did not\n     *          exist\nDoes it matter if it's false in this case?\n. I'll change to Closeable\n. Sorry this code is slightly outdated. Do we want to make the deployment attribute a HeliosDeployment instance? In that case, there might be more than one URI if it's an ExistingHeliosDeployment.\nWrapping an ExistingHeliosDeployment in HeliosDeploymentResource makes the API a bit more consistent and takes care of closing the HeliosClient associated with the ExistingHeliosDeployment.\n. Should we rename or move these methods out? If the latter, to which class? These methods are called by several different classes.\n. I kept the AssertionError from previous code. Not sure why it was originally that. What kind of Exception should we use? Would it make sense to use a type of Exception even though they won't be thrown by helios code?\n. Oh nice. My brain isn't working right now. Can you give an example? I couldn't figure out how to extend the class and return a JobId required by the calls above.\n. Ah, nvm I figured it out.\n. Add a note about Docker for Mac too?\n. \ud83d\udc4d \n. Will this always be the case?\n. Good note.\n. Let's do 1. We've split the tests in a way to minimize test times.\n. Is there a better way? A lot of the anonymous functions written for Polling only return true or null.\n. Not sure why I did this. You're right.\n. The single method here is only used in one class. I'll just move the method there and delete the class.\n. There's a similar method that uses CustomTypeSafeMatcher in SoloMasterProberTest. I'll just move that here.\n. Forgot about this one.\n. extra backtick in \"`job\"\n. I think you're right. I'll replace it.\nOn Fri, Jul 8, 2016 at 9:24 AM, Matt Brown notifications@github.com wrote:\n\nIn helios-tools/pom.xml\nhttps://github.com/spotify/helios/pull/947#discussion_r70072613:\n\n@@ -58,6 +58,10 @@\n       com.typesafe\nconfig\n\n-    \n-      javax.ws.rs\n-      jsr311-api\n\nI could be wrong, but if this dependency is for things like the @GET\nannotation then I think the better dependency is javax.xml.ws:jaxws-api\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/spotify/helios/pull/947/files/b2aee7e1503abf81db1f7041984354ed2169eeea#r70072613,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AAdVbco5-cBGJvobMwLW097igY1pRh7wks5qTk-rgaJpZM4JHdpM\n.\n\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\n. Ay, true.\n. Yea, I was trying to save chars.\n. Thanks\n. Why? hostStatus.getStatus() is an enum so I can just use != right? I still have to do hostStatus == null to prevent an NPE in hostStatus.getStatus().\n. Haha, see https://github.com/spotify/helios/pull/969#discussion_r76539981\n. I need TimeUnit for RateLimitedService because ScheduledExecutorService.scheduleWithFixedDelay() takes TimeUnit.\n. Use awaitility here?\n. Shouldn't this be set to true as daemon threads do not prevent the JVM from exiting when the program finishes.\n. Sorry I only got a chance to see this now.\nWhy not just use message instead of formatting it? Or message + String.format(\"Timed out after %d %s\", timeout, timeUnit.toString().toLowerCase()). It's not obvious to callers that the message they pass in will be string formatted. \n. I know the other options aren't in order, but can we put this above securityOpt?\n. Since State is an enum, we should use == here.\n. Wouldn't !expected.equals(actual) always be true because of line 1017? Do we need this second conditional?\n. Isn't this line redundant because of the line below?\n. I think we'll need to do the latter.\n. Seems to be in a transaction. See https://github.com/spotify/helios/blob/master/helios-services/src/main/java/com/spotify/helios/master/ZooKeeperMasterModel.java#L730.\n. Nice comment.\n. We have a CreateWithDataAndVersion class. It might be nicer to make a SetWithDataAndVersion class instead of doing both check() with version followed by a set() without a version and executing both in a transaction.\n. @feroozkhanchintu Thanks for your PR.\nAlways calling HeliosClient.listHosts() might slow down this command if there are a lot of hosts but only a few hosts that match a specified pattern or selector. HeliosClient.listHosts(pattern, selectors) makes the server filter and results in less data being transmitted over the wire. (The ZK operation is still to get all the child nodes of /config/hosts though.)\nI'm also a fan of keeping as much logic on the server side as possible.\n. Agreed, not for this PR.\n. Sorry, I should've explained.\n- == never throws NullPointerException\n- == is subject to type compatibility check at compile time\n- It's faster\nSee more here http://stackoverflow.com/a/2937561/553994\n. We have PomVersion.compareTo() which compares semantic version strings. Perhaps use this class and rename it instead of using three specific hardcoded versions?\n. Are these hardcoded strings bad for an open-source project?\n. Any reason we don't use java.util.Optional?\n. We should mention here that syslog-redirector is deprecated as of Docker version XX. Also mention the right way to log to syslog.\n. Since server side is now java 8, we should use java Optional where we can.\n. Add a javadoc for this to differentiate it from the method above?\n. Perhaps just call this var response since it's not curl?\n. We can rename this constant to something like TASK_STATUS_EVENT_TOPIC since it's not kafka specific anymore. Same thing with DEPLOYMENT_GROUP_EVENTS_KAFKA_TOPIC.\n. Why not do this at the parent level?\n. Write a test for this function?\n. Unrelated to PR, but should we always return true here? Is there value in checking we can talk to kafka by using KafkaProducer.waitOnMetadata() for example?. You meanOptional? Maybe.... Leftover?. nitpick: This and the constructor be package-local.. Good catch.. Is it worth testing this class or too much of a pain?. Personally, I don't think it's that useful.. Thanks. Delete this commented out stuff?. oh thanks. but why should the file be left as is?. The motivation is thatisHostUp()andgetAgentInfo()are fewer ZK ops thangetHostStatus(). Does that make sense?. We can just change&&to||, but I guess more specific errors can't hurt.. This should've been!=.. It's a view of the list around the specified index. I'll update the docs.. I made it immutable to be simpler. What word should I use instead?. You mean httpclient? Yea I was going to do this originally but hesitated because I didn't want to change too much. I'll move it.. I'm not 100% sure, but it seems likeExecutors.defaultThreadFactory()will do this type of naming. Perhaps just create oneThreadFactoryinstance for allHeliosClients instead of creating a new instance of aThreadFactoryfor everyHeliosClientinstance? Or will that not be enough in some way?. Ah sorry. I see now. . Yea I started doing this.. agreed. Hm, not having more specific regex here might be hard to do and be backwards compatible. We need to deal with strings likehttp=8080:80andhttp=0.0.0.0:8080:80. What kind of regex would match both with the appropriate named capture groups? I.e. something that'll makePortMappingParserTestpass.. A space after the version is OK?. Good first point. On the second, a machine could have multiple IPs though.. I've wondered what's the correct terminology here. Does one say a port is exposed on an IP and that an IP is bound to an interface?. I'm also wondering why we should mention the word \"interface\" specifically. Isn't saying IP enough?. Aren't there overloaded constructors that supply0.0.0.0as the default below?. Ah, thanks for the references.. option: You could useassumeFalse()if you think it's more concise.. I guess we shouldn't ignore? Redefining close() seems like a better idea though.. Should we keepscheduler.shutdownNow();`?. ah sorry, just saw this now.. https://github.com/spotify/helios/pull/1091. s/tha/that. Double copyright header.. Test a host pattern that returns an empty list of jobs too?. It took me a couple of reads to understand this comment because I thought this line\n\nignoreFailures=true and parallelism > 1 has the same output as parallelism=1\n\nmeant\n\nignoreFailures=true and parallelism > 1 has the same output as [only, aka ignoreFailures=false] parallelism=1\n\nPerhaps reword as\nSetting ignoreFailures to true causes the parallelism option to have no effect on the generated list of RolloutTasks. Parallelism=n creates undeploy and deploy tasks for groups of n hosts as a time and then does AWAIT_RUNNING on each of those n hosts. ignoreFailures removes all AWAIT_RUNNING RolloutTasks.. Would it help the user to enumerate all the checks that are skipped that are usually checked for AWAIT_RUNNING? e.g. retrieving job status, job unexpectedly undeployed, etc.. You can use Paths.statusDeploymentGroup(\"ignore_failure_group\") here and Paths.statusDeploymentGroupTasks( for line 428.. Would it worth testing this too? Maybe it's too similar?\n```java\n    // the current task is the AWAIT_RUNNING one\n    final DeploymentGroupTasks tasks = DeploymentGroupTasks.newBuilder()\n        .setTaskIndex(2)\n        .setRolloutTasks(ImmutableList.of(\n            RolloutTask.of(RolloutTask.Action.UNDEPLOY_OLD_JOBS, \"host1\"),\n            RolloutTask.of(RolloutTask.Action.DEPLOY_NEW_JOB, \"host1\"),\n            RolloutTask.of(RolloutTask.Action.AWAIT_RUNNING, \"host1\"),\n            RolloutTask.of(RolloutTask.Action.UNDEPLOY_OLD_JOBS, \"host2\"),\n            RolloutTask.of(RolloutTask.Action.DEPLOY_NEW_JOB, \"host2\"),\n            RolloutTask.of(RolloutTask.Action.AWAIT_RUNNING, \"host2\")\n        ))\n        .setDeploymentGroup(deploymentGroup)\n        .build();\nfinal RollingUpdateOpFactory opFactory = new RollingUpdateOpFactory(tasks, eventFactory);\n\nfinal RollingUpdateOp nextOp = opFactory.error(\"something went wrong\", \"host1\",\n    RollingUpdateError.TIMED_OUT_WAITING_FOR_JOB_TO_REACH_RUNNING);\n\nassertThat(nextOp.operations(), contains(\n    new SetData(Paths.statusDeploymentGroupTasks(\"ignore_failure_group\"),\n        tasks.toBuilder()\n            .setTaskIndex(3)\n            .build()\n            .toJsonBytes()\n    )\n\n```. Needed to be able to build the latest skydns. Didn't want to figure out a way to pin the version of skydns. https://github.com/spotify/helios/issues/901. Outdated comment?. I wonder why we ever had this param.. nit: Let's since you mean let us.. don't need the comma at the end.. I prefer the word partition instead of chunk :) https://en.wikipedia.org/wiki/Partition_of_a_set. I think we kept these hardcoded versions because @dflemstr said variable versions don't play well with maven tools that upgrade dependency versions. I don't have strong feelings though. And looking at the versions-maven-plugin, I think it can handle variable versions.\n@dflemstr can you explain again why you didn't like this? I'm just curious.. Can you change to just \"you can use helios hosts\"? We'll assume the user by this point knows how the hosts subcommand works.. As I mentioned before, the downside I see so far with this approach is that jobs can be created with null values for rollout options. They'll exist on disk with null values for these options. When you inspect them, they'll output null values. Then when you start a rolling-update, the master will fill in the defaults.\nThis is different behavior from (almost all) the other options in the job config. The state other options are in when created by the master and on disk is the final configuration at deployment time.. for consistency with the other args. Do you mean instead of Optional.fromNullable(rolloutOptions).orNull(); we do Optional.fromNullable(rolloutOptions).or(EMPTY_ROLLOUT_OPTIONS);?. storeTrue() implicitly sets the default to false. > I think the use of Optional here is totally unnecessary - could just be this.rolloutOptions = rolloutOptions.\nGood point. Why do we have all these Optional.fromNullable(X).orNull() anyways?. firstNonNull() will throw NPE if both are null. So I think this is OK.. @caipre Your link didn't go to a line. Are you referring to this line?. It's ugly. Let me know if anyone has a better way to not default to false.. > I'd say that filling with defaults gives the most information, and can be clear enough if we document the defaults and their application during job creation.\nagreed. token is already in the job. migrate doesn't make sense, but it seems more work and potentially more confusing to exclude some RolloutOption fields than to include everything.. If I add a unit test for the fallback logic in ZKMM, do we still need this?. Do you think enforcing this in the DeploymentGroup constructor is cleaner than adding a bunch of null checks everywhere we do DeploymentGroup.getRolloutOptions().getXXX()?. Thanks I'll use that.. good catch. done. done. done. @danielhan As talked about, let's make sure we test this somewhere.. Same here. Let's keep testing this somewhere.. \ud83d\udc4d  You can also set some of the CLI args to check it does a merge of both CLI args and job rollout options with correct precedence.. I guess if NPE is here, we'll see it in logs?. No reason. I can revert.. ah forgot that part.. Thanks. I moved them to the right place.. see line above. NPE starts here. This variable can be null.. thanks, fixed. Looks like we don't really use properties for repeated versions. I'm happy to merge if we just do a simple find replace without the property.. Shouldn't we use cloudplatformprojects.readonly and devstorage.read_only?. Hm, I'm actually not sure what makes sense from an open source POV.. sure done. Thanks. Added.. Thanks. Updated with \"This configuration will prevent new containers from deploying during the gracePeriod because of port conflicts\". @vbhavsar, is \"runtime\" word at the end of this sentence correct? Should it be \"nvidia\" or \"nvidia driver\" instead?. I had to increase the timeout. A docker executor seems to have less resources and therefore is slower. Without this increased timeout, downloading deps often timed out and failed the build.. This check doesn't seem to be valuable anymore. It'll be simpler to remove it since my java11 build is parallelism 1.. https://github.com/spotify/helios/pull/1248. This means this profile is activated if the jdk is 1.8? What happens if the jdk is another version?. ",
    "mattnworb": "I've seen some anecdotes where people create jobs like foobar:1 and foobar:1-darkload and then are confused when helios deploy foobar:1 gets the ambiguous reference error.\n. @erikgrinaker one possible solution would be to add flags to the helios-agent for either a config file that resembles ~/.docker/config.json or the server/auth token/email parameters themselves, and pass those down to the DockerClient used within the agent. I don't think it is very high on our radar at the moment to add this to helios as we don't have a use case for it internally at Spotify, but we would be very happy to review any PRs that added this support.\n. @davidxia still interested in doing this? Let me know if you want any help in resolving issues. \n. I rebased this against master and it seems like only two tests are failing, it's probably something simple - HeliosIT and HeliosSoloIT are unable to connect to the local helios cluster that was set up with ./solo/helios-up:\n```\n+ export DOCKER_HOST=tcp://172.17.42.1:2375\n+ DOCKER_HOST=tcp://172.17.42.1:2375\n+ cd solo\n+ sed -i 's/docker run --rm/docker run/' ./helios-up\n+ ./helios-up\nWARNING: No memory limit support\nWARNING: No swap limit support\nhelios-solo started\nhelios-solo is reachable on: http://172.17.42.1:5801 (Internal IP: 172.17.0.2)\n...\nRunning com.spotify.helios.HeliosSoloIT\n13:26:15.015 [main] INFO  c.s.helios.testing.TemporaryJobs - Using profile: local\n13:26:15.602 [helios-test-runner-0] INFO  c.s.helios.testing.DefaultDeployer - Getting list of hosts\n13:26:15.839 [pool-1-thread-1] WARN  c.spotify.helios.client.HeliosClient - Failed to connect, retrying in 5 seconds.\n...\n13:27:13.052 [pool-1-thread-1] WARN  c.spotify.helios.client.HeliosClient - Failed to connect, retrying in 5 seconds.\nTests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 65.586 sec <<< FAILURE! - in com.spotify.helios.HeliosSoloIT\nsoloTest(com.spotify.helios.HeliosSoloIT)  Time elapsed: 65.043 sec  <<< FAILURE!\njava.lang.AssertionError: Failed to get list of Helios hosts\n```\n. Figured out the reason for the test failures; we need to upgrade the helios-solo docker image to have Java 8 installed as well. Currently helios-solo-container's start.sh tries to start the agent and master but only has Java 7 available\n\nException in thread \"main\" Exception in thread \"main\" java.lang.UnsupportedClassVersionError: com/spotify/helios/agent/AgentMain : Unsupported major.minor version 52.0\n\nI'm not positive how to get Java 8 installed it in ubuntu:trusty base image though.\n. finally CircleCI is all green :white_check_mark: :tada: :fireworks: \n. @rohansingh @gimaker @davidxia \nplease check the Dockerfile stuff I added for installing oracle-jdk-8\n. also I pushed helios-solo-base:0.3 (with java8) to docker hub\n. @davidxia I don't think so. :+1: :tada: \nDoes 13b1010 need to be a part of this though?\nI do think we should probably do a release before merging this, just to clear out what has been merged but not released. Then we can do a release that is just Java 8 migration to make that easy to roll back if necessary.\n. I would vote for making the current behavior the documented way. While it's a bit ugly since it conflicts with the order from docker, it wouldn't break anything. \nI think the only other options is to do a breaking change as you mention or deprecate this parameter and use a new one with the intended behavior.\n. This approach would seem to assume that the clocks on agents are synchronized, wouldn't it? Maybe this is totally implausible / not worth worrying about, but it seems like if there was a host1 healthy and alive and registered and I started up a duplicate host1 with the clock set 15 minutes in the future, the latter would deregister the former.\nYou may not even need to add the touch() method to NodeUpdater as you could check the mtime of the  host agent info path, right?\n. > the same effect as a user deregistering a perfectly OK host.\nGood point \ud83d\ude00\n. @davidxia stat-ing the hostInfoPath looks a lot cleaner :+1: \n. re: b970a89, makes sense to me. I think ideally it wouldn't be necessary to signal between those components because it seems like the other two should not be started until the former has registered at least once, but this looks like a good way to solve the immediate problem.\n. If you are adding the findbugs-annotations <dependency> just to satisfy maven-enforcer, you can put that in the <dependencyManagement> section instead (possibly just in the root pom). That way if we ever remove the dns dependency then the findbugs-annotation one doesn't linger on the classpath.\n. +1 but do you think it would be useful to somehow configure it to not fallback to http, for anyone who wants to enforce only https communication?\n. On second thought I suppose that any users who wanted their masters to be available only over https or only over http could just not register those DNS records, so maybe the client doesn't need to care about flags or options.\n. +1 if you added something to the docs about the new A record resolution logic.\n. thanks, now here is your +1\n. @davidxia @gimaker @rohansingh \n. first official PR as team member: :six: :six: :six: \n. @rohansingh @davidxia @gimaker \n. I am wondering if it would make more sense to use Docker's built-in support for labels for images/containers rather than adding a metadata field for Jobs in Helios. It would seem to duplicate the same idea.\nI think for the use case we are interested in at Spotify (git commit id of the image) an image label would suffice just fine\n. After more discussion @gimaker and I decided to keep the addition of job metadata rather than using image labels, as it might be convenient to set metadata of a job independent of how the image was built (or if deploying a job for an image you did not build yourself, etc.).\n6921b14 adds a GIT_COMMIT_ID entry to the job metadata by default if it is set as an environment variable when the job is created.\n@gimaker @davidxia one more review would be very helpful if you have a chance :smile: \n. change is best viewed in preview mode \n@davidxia @rohansingh @gimaker\n. +1\nor just log the whole taskStatus?\n. @davidxia @gimaker \n. @gimaker @davidxia \n. I suggest using joda-time's DateTime instead, which would allow you to specify the timezone you want the value to be in, as Date can have unexpected timezone logic.\nDatetime is also easier to test against\n. A few other thoughts on this: \n1. I'm not sure if we really care about the JSON format of a job but I think that \"Tue Oct 06 16:02:46 EDT 2015\" is a really ugly format for dates. I think it would be simpler to store a date as milliseconds since epoch, UTC, and relying on Date.toString() (if that is what is happening here) means that you might get timezone ids in the string based on where the user created the job from, which feels irrelevant. If we don't store as milliseconds then ISO-8601 in utc time still feels better.\n2. Can the master assigned the created time to the job instead of generating it on the CLI side? The latter gives us the time on the CLI host, whereas the time on the master feels more authoritative. \n. +1 on the changes in 91262d6, although it feels like it should use the Clock abstraction instead of System.currentTimeMillis() (but this feels very minor)\n. I was assuming that the thing that created the JobsResource had a Clock instance it would pass in, but :+1: either way\n. thanks!\n. @rohansingh @gimaker @davidxia \n. For this use case of wanting to add new fields to the Job definition, the reverse situation of running the master at a newer version than the client/CLI (where the master knows of a new Job field) works fine.\n. :+1: :100: \n. :+1: \n. :+1: \n. @davidxia @rohansingh @gimaker \nnote the test failure seems to happen on master branch as well\n. I was considering rewriting the test for the same static var reasons (it also builds a bunch of Json with string literals), decided to do that in a separate PR. \n. +1\n. @gimaker\nthe tests that fail also fail on master :confused: \n. Some other thoughts that I think we can address outside of this PR:\n1. The stuff where the Nop (I always thought it was spelt noop :wink: ) instances return null for every method feels like a perfect use case for Optionals instead\n2. Would be very, very helpful to start documenting the ideas here even at this early stage, for all of\n   - how to configure the master to use authentication\n   - what the protocol between master and client looks like (i.e. the response header, the challenge, etc)\n   - an overview of how the authentication code / modules within the master codebase are laid out (for example how the Provider gives you a Factory which gives you an Authenticator etc etc)\nshould be a lot easier to implement and understand all of this if we were working from an already-written markdown doc rather than documenting it after it's built\n. We'll also need a place to store expiration times for access tokens, or (if crtauth provides that by baking it into the token) then a method for checking if an arbitrary token (as present in the client request) is still valid.\n. @davidxia right, I just mean on the server-side we will need a way to validate that a given token from the request is still valid or not; so a method we can call in one of these interfaces like boolean isValid(string token, string user). \n. closing as #694 replaces this\n. @rohansingh @gimaker @davidxia \n. :+1: \n. @rohansingh @gimaker @davidxia \n. @rohansingh yep!\n. A bunch of TODOs I forgot to mention:\n- checking the client version against the --auth-minimum-version flag is not implemented yet\n- none of the client-side stuff, obviously\n- I want to refactor the ServerAuthentication interface so that an implementer just has to return a io.dropwizard.auth.Authenticator instance. Right now they have to return the Jersey-specific InjectableProvider which is a) confusing and b) involves a lot of boilerplate that we can probably handle in our code instead.\n- to actually authenticate all calls with this approach we have to add @Auth(required = false) HeliosUser user as a method parameter to all Jersey resource classes\n- AuthenticationLoader does not yet load plugins defined in external jar files\n. this work is now in #698\n. +1 for moving the low-level http stuff out of HeliosClient.\nWith that stuff moved into a RequestDirector, seems like adding a HeliosClient unit test would be a lot easier now :wink: \n. :+1: :100: \n. I'm not sure if I totally understand the purpose but LGTM\n. @rohansingh @gimaker @davidxia \nI cleaned up the integration between Jersey and the authentication plugin a bunch, I think it makes it simpler to understand how this all works. See notes in 34e0c0f. \nOnly thing left AFAIK is being able to load the plugin from any arbitrary path. \nIf you haven't taken a look already please review, this is just about my final draft.\n. 47303bd adds support for loading the authentication plugin from an arbitrary path; which would be needed for any Helios user that wants to set up their own custom authentication scheme. \nWith that I feel like this PR is mostly done; server-side authentication is available and can be enabled, and we have http-basic and crtauth implementations.\nI'd like to still add more integration tests but I'm don't think those are necessary for this PR as authentication in the masters is not enabled by default (so if something is broken if we merge this now there is no impact).\nThoughts on merging @rohansingh @gimaker ?\n. ugh I rebased on master on accident and now Github has re-ordered all the commits so the comments look out of order\n. and now all the tests pass again in CircleCI after removing the thing where I tried to archive the logs from maven-invoker-plugin\n. @gimaker found a nice way to modify SystemTestBase to be able to write system tests where auth is enabled, check out 32630ae \n. +1\n. I agree about -z https not working shouldn't be a big deal.\nWe could add a --insecure flag like curl for this case, to disable verification of the certificate to the actual host / IP.\n. :+1: \n. Apache HttpClient supports PATCH :smirk: \n. oh man: the source code for HttpsURLConnectionImpl\n:+1: \n. more lines removed than added is :ok_hand: \n. @nresare \n. @rohansingh @davidxia @gimaker \n. @nresare yep: https://github.com/spotify/helios/blob/fix-license-header/java.header\n. I like moving the method params for \"an http request\" into the HeliosRequest class\n. Overall, looks good to me.\nI couldn't follow why the AuthProviderSelector was necessary at first, so some comments about why it is necessary might be useful (since multiple plugins may be present?). I'm still not sure if there is a real need to use AtomicReference in there though.\nAt some point we should probably unite the 3 places in Helios that try to load things through java.util.ServiceLoader\n. Possibly a style issue, but AuthProvider.Factory feels unnecessary to me. I think you could just move the AuthProvider create(RequestDispatcher requestDispatcher) method into the plugin interface and then you wouldn't need a factory.\n. +1 that was fast\n. I think we should add block.on.buffer.full also: https://github.com/mattnworb/helios/commit/3e74c500745fc9ac740873a6c71a8f1c45371b26\n. ah cool, didn't realize block.on.buffer.full replaced async\n. this should resolve an error on our internal jenkins when mvn -e -P sign-artifacts -P build-images -P jacoco -P build-solo clean deploy is run. For some reason the build order was computed as\n[INFO] Reactor Build Order:\n[INFO] \n[INFO] Helios Parent\n[INFO] Helios Authentication\n[INFO] Helios CRTauth plugin\n[INFO] Helios Service Registration\n[INFO] Helios Client\n[INFO] Helios Tools\n[INFO] Helios Testing Common Library\n[INFO] Helios Services\n[INFO] Helios API Documentation\n[INFO] Helios System Tests\n[INFO] Helios Testing Library\n[INFO] Helios Integration Tests\nand then API doc module breaks with\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.8:run (prepare-reports) on project helios-api-documentation: An Ant BuildException has occured: /var/lib/jenkins/jobs/helios-build/workspace/helios-testing/target/classes does not exist.\naround Ant part ...<report>... @ 8:11 in /var/lib/jenkins/jobs/helios-build/workspace/helios-api-documentation/target/antrun/build-main.xml\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:217)\n@davidxia @rohansingh @gimaker \n. Is the AuthProvider.Context really necessary? It feels to me like a way of hiding the parameters being passed.\nAlso I think that AuthProviderSelector could just implement the AuthProvider.Factory interface, and decide which implementation to return internally.\n. besides the comment above: :+1: \n. :+1: \nonly comment would be (as I think I mentioned above) that if you passed in the CrtAuthClient as a parameter to the CrtAuthProvider constructor (i.e. move the instantiation of CrtAuthClient one layer up) then CrtAuthProviderTest could be a lot simpler, as you could mock out the calls to the client instead of providing actually valid ssh keys, but since you already have that test...\n. @gimaker @davidxia @rohansingh \nUltimately it would be nicer if this \"plugin\" framework had a way for implementers to easily be able to wire up what they need without too much extra work. For example: crtauth needs an instance of KeyProvider and doesn't really care about the actual concrete class, but since we use java.util.ServiceLoader there is a requirement for the loaded classes to have no-arg constructors so passing \"configuration\" down to the plugin is a bit tricky. I want to stay away from putting too much logic around this, so we don't end up practically rewriting Guice or Spring or some other sort of DI framework, but at the same time all this stuff with environment variables feels super hacky.\n. added a test of HTTP Basic too, now that SystemTestBase allows for mocking out environment variables.\n. @gimaker @davidxia @rohansingh \n. @davidxia I could be missing something but I'm not sure what the \"return username\" endpoint would leak if someone needs to already have a token to access it\n. @gimaker @davidxia @rohansingh \nThe main part of the change is the additional Future handling in AuthenticatingRequestDispatcher.\n. I also removed Utils.getClient and moved that method into the Command classes by introducing an AbstractCliCommand that they can all extend.\n. It feels kludgy to me to have to add things to CliMain so that they can be passed from the CliParser/argparse-handling-layer over to how the HeliosClient is created, but I don't see another way\n. also PRs to merge into other branches == :angry: \n. @gimaker \"testing\" in that this way we could set up the CLI on some robots (Jenkins) to eagerly authenticate as a test before we roll it out to a larger user-base or use the \"authentication required if client version >= some version\" flag. \nI doubt anyone would want to type these in regular use, yeah, just for automation.\n. @rohansingh @gimaker \n. I think it would be good to have at least a few tests, for example RecordingTlsClientProtocol has some meaty logic in it outside of the stuff it inherits from it's superclass.\n. Would this force anyone who wants to access their Helios installation over https into needing to have an ssh-agent process running on the client side, even if they didn't care to send client certificates in the TLS handshake?\nSeems like AgentProxies.newInstance() would probably throw an exception if SSH_AUTH_SOCK is not set, etc\n. :+1: if you document this somehow (maybe update/replace authentication.md)\n. Nice job on the doc. Do you think you could also add an example of the nginx config to have it pass the certificate to an arbitrary endpoint (or point to the nginx doc on that topic)?\nFor using existing x509 certificates, could the client just check for a file in a well-known location before it contacts the ssh-agent? Or is there more hooks we have to add in besides just that?\n. > Yup, that's exactly what I was thinking. Figured we can implement that later though.\nRight, I was asking more to see if it was a simple one or three line thing that could be added now or if it needs to be plugged into a lot more places with the bouncycastle stuff.\nAssuming the answer is the latter, merging sounds good to me though.\n. well this didn't help diagnose the test problem but the additional logging would be good to have anyway\n. had to make the dependency on dnsjava in helios-tools explicit as it is no longer silently/transitively inherited via helios-client\n. @gimaker @rohansingh @davidxia \nshould fix a problem @alanw had this morning\n. @gimaker it's in com.spotify.helios.cli.Output and cli.command.HostResolver. Code fails to compile without 68c122f\n. For reference the problematic project is getting com.spotify:dns:2.x from our Cool Awesome Internal RPC Framework v0.6 and upgrading to com.spotify:dns:3.x would mean moving to 1.x of Cool Awesome Internal RPC Framework, which feels like a much more sizable change than shading the dependency here.\n. rebased on master to give the tests a chance to go green \n. Going to merge this as the tests that fail are totally unrelated and pretty flaky, and I don't think anyone else on the team can really review systemd stuff. Thanks @dflemstr !\n. finally :clap: \n. :+1: \nThere are a few refactorings in SystemTestBase I'd like to keep, but I'll just cherry-pick those.\n. branch with those commits picked: https://github.com/spotify/helios/tree/mattbrown/awesome-tests\n. +1\n. +1\n. does #728 replace this?\n. Clock and SystemClock exist in helios-services already, which depends on helios-tools which depends on helios-client; so if you add a Clock and SystemClock to helios-client you could also remove the duplicates from helios-services.\nI think the intent here makes sense, just small things to clean up above.\n. :+1: \n. :+1: \n. :+1: have been meaning to do this myself\n. :+1: \n. these look like useful link updates actually\n. Thanks!\n. :+1: \n. If it's not obvious which classes need to be included from the jnr stuff to prevent any errors, and we also can't really do away with the jnr library for unix socket support, this makes sense to me - +1\n. :+1: \n. Good idea. Why do you need to move the <plugin> in pom.xml though?\n. who you calling a peanut?\n. easier to follow than do .. while (false) :+1: \n. LGTM\n. lgtm\n. \n. LGTM. \nSince you asked above, if you were to use ExpectedException you can use matchers against the inner exception or message or etc\n. This will output once per connection, but some commands connect to more than one master (or connect more than once) so it might be very redundant.\nI think it would be best if we could hold off on outputting anything, or at least a multi-line message, unless the server is returning 401 Unauthorized  / Forbidden / whatever\n. Is the Identity instance usable after the AgentProxy has been closed?\n. Looks like it is usable, so nevermind.\n. :+1: to the change itself.\nI think this class would be a lot more testable though (it has no test today) if instead you passed either the List<Identity> or AgentProxy instance in the constructor, so that the class that is making http requests didn't also have to be concerned with how to get the ssh-agent identities or how to get an AgentProxy etc. \n. looks sensible to me :+1: \n. @rohansingh @davidxia @gimaker @negz \nThis is just a start but wanted to share it early to make sure this path isn't too complicated\n. FWIW I broke something along the way with these changes, so it still needs a ton of work\n$> bin/helios -vvvv -d shared.cloud.spotify.net masters\nrunning in helios project, using /Users/mattbrown/code/helios/helios-tools/target/helios-tools-0.8.0-SNAPSHOT-shaded.jar\n16:44:56.393 DEBUG AgentProxies$DefaultAgentProxy: connected to [family=PF_UNIX path=/private/tmp/com.apple.launchd.Aifmik2Y6u/Listeners]\n16:44:56.424 DEBUG AgentOutput: Sent SSH2_AGENTC_REQUEST_IDENTITIES message to ssh-agent.\n16:44:56.424 DEBUG AgentInput: Received SSH2_AGENT_IDENTITIES_ANSWER message from ssh-agent.\n16:44:56.838 DEBUG AuthenticatingHttpConnector: connecting to https://172.16.11.109:443/masters/?user=mattbrown\n16:44:56.860 TRACE DefaultHttpConnector: req: GET https://172.16.11.109:443/masters/?user=mattbrown 1 Helios-Version=[0.8.0-SNAPSHOT] 0 \"\"\n16:44:57.262 DEBUG RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\njava.lang.RuntimeException: java.lang.UnsupportedOperationException\n    at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1454) ~[na:1.8.0_40]\n    at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1440) ~[na:1.8.0_40]\n    at sun.net.www.protocol.http.HttpURLConnection.getHeaderField(HttpURLConnection.java:2978) ~[na:1.8.0_40]\n    at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:489) ~[na:1.8.0_40]\n    at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338) ~[na:1.8.0_40]\n    at com.spotify.helios.client.AuthenticatingHttpConnector.connect(AuthenticatingHttpConnector.java:112) ~[helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at com.spotify.helios.client.DefaultRequestDispatcher$1.call(DefaultRequestDispatcher.java:72) ~[helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at com.spotify.helios.client.DefaultRequestDispatcher$1.call(DefaultRequestDispatcher.java:68) ~[helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_40]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_40]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_40]\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_40]\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [na:1.8.0_40]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_40]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_40]\n    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_40]\nCaused by: java.lang.UnsupportedOperationException: null\n    at com.spotify.helios.client.tls.SshAgentSSLSocketFactory.createSocket(SshAgentSSLSocketFactory.java:82) ~[helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:453) ~[na:1.8.0_40]\n    at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_40]\n    at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1512) ~[na:1.8.0_40]\n    at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1440) ~[na:1.8.0_40]\n    at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) ~[na:1.8.0_40]\n    ... 12 common frames omitted\nLooking at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:453) it calls a different overload of SshAgentSSLSocketFactory.createSocket(..) because it caught an IOException from the original overload of createSocket (something with bouncycastle and the error message java.io.IOException: Internal TLS error, this could be an attack)\nbut perhaps that helps make my case that this could all use better testing :smirk: \n. Yea #780 replaces this\n. For what it is worth this would probably also happen with any % character in the response or anything else that might end up URL encoded\n. :+1: \n. > If data in ZooKeeper for a deployment group gets denormalized\nany idea how the data gets in that state in the first place?\n. :+1: \n. Had a similar issue pop-up today, where a user creating a deployment group failed with 500 errors.\n- /config/deployment-groups/foobar did not exist\n- /status/deployment-groups/foobar exists, is empty\n- /status/deployment-group-tasks/foobar exists, is empty\nAfter removing the last two, creating the DG succeeds.\nMakes me wonder if perhaps the issue isn't a race between creating the DG and removing it but if something else caused the data in ZK to get in this state. \n. @gimaker @rohansingh \n. checkstyle errors have been vanquished\n. What is the request (including all headers) that you are sending?\n. @alejandrojnm sorry for the slow response. Does your helios master log anything related to your command?\n. I think you're missing some words here\n\nActually plumb the HeliosDeployment up up to .\n. I like the concept of aHeliosDeploymentwhich (if my interpretation is correct) represents a deployed cluster somewhere andHeliosSoloDeployment` points to a deployment of solo.\n\nIt seems like HeliosSoloDeployment does all of it's work in the constructor; for testing and re-use (such as the addition of other methods to the interface in the future) it would be nicer if this was separated out.\nDo you have in mind what the use of HeliosSoloDeployment by TemporaryJobs (i.e. the API to set one up) would look like? Because I think thinking about that can help answer the previous point.\nFor instance, maybe something like \njava\nHeliosDeployment solo = new HeliosSoloDeployment();\nsolo.start();\nwould be a simple way to separate the instantiation of the object from the \"now do some work\" part of it.\n. Ah I missed that the way to build and start a HeliosSoloDeployment is HeliosSoloDeployment.fromEnv().build() which negates my previous comment a bit.\nI think it would be good to have a unit test of HeliosSoloDeployment separate from the IT.\n. re the gist - is there teardown work that has to happen to clean up the helios solo container? is the @Rule on TemporaryJobs taking care of that?\nI think the idea of declaratively using a self-container solo container is intriguing\n. thanks for addressing my comments. :+1: \n. :ok: \n. This was bugging me.\n. thanks in advance @gimaker \n. @rohansingh @davidxia \n. Can test be added to verify the behavior change?\n. Nothing off of the top of my head, unless you can simply test that the operations you expect to be made are the ones that are made?\nI think in general it would be really nice to have better testing and testability of all the ZK code but that shouldn't hold this fix up. :+1: \n. @gimaker nice! :+1: \nThe theory framework seems pretty lame if you can only set up ints values for the parameters. I've used org.junit.runners.Parameterized before which is a bit more extensible.\n. :+1: \n. @rohansingh @gimaker @davidxia \nReport for this build: https://codecov.io/github/spotify/helios?ref=6043e0077be3ad6c431f1b7a7aaf0fd2c606a7d8\ncodecov also has some cool features for setting the commit status if the coverage number decreases and commenting on PRs. I'd hesitate to turn on the former but the latter would be very useful IMO.\n. I set up PR comments on the docker-client project as a test.\n. I will enable codecov to comment on PRs about changes in coverage.\n. @rohansingh @gimaker @davidxia \n. @gimaker @rohansingh @davidxia \n. I've been meaning to investigate if using the Apache HttpClient in a RequestDispatcher implementation would simplify things and also make it feasible to write some meaningful unit tests.\n. I like the separation of HttpsHandler into it's own type, that's smart\n:+1: \n. :tada: \n. :+1: \n. :+1: thank you debug symbols\n. @rohansingh @davidxia @gimaker \n. The output is a bit too verbose, I thought about trying to generate the fingerprint or at least deriving the same format inside the public key file itself but it didn't appear obvious.\n16:19:48.075 DEBUG DefaultRequestDispatcher: configured SshAgentSSLSocketFactory with identity=DefaultIdentity{keyFormat=ssh-rsa, publicKey=Sun RSA public key, 8192 bits\n  modulus: 808462599718312385346868551833732190485071359460387746499709719808758082776576083221020409467677111657772964728340153038583361710350171863665654950156782913921872788878580673880572003398789248888986310100086907881944707539684213026243780071542923106361959206499910162849609059569053657861786405198201286115174398986128380002220241555634767908642170923276411977136065522870529267315008675566853410321505557170100772739397174822757786018428265646425184979567468881666890656723577278412412295202377117396568721814642616763220145500147375420955039547750590153116674899469472021069856148330141657695984461978195898690521310202612348990581874200398364720244790982023337170186260057572293532670105625937201144415749824539490372244146013841135389702031737258032555568021689246367733142621554947534858706731345084232314734357020203407217293367092998075601831074561111303495493894587293901702246140538349008058080480904271212139386995406331693273975433844826200424790004368419937949816594757150002512168228930883961091184465010628466819985866197634796069538977758627296536711871304067662347113141304944150436993306389960799112772705316966891354360508590911648838705818729700669769590246025006779439695826789530439909160200176355509904597620716489439144897373831334848456589464753930085779941927855305519295264586831481624116792288911582841848586686049828081093263648598381369115870498134531961536139756276586734633330736477512437244855993240526636351621954217199152584783806015435889941873687333329978931126114246409792184217565490541639036651055200903170027116190924218067447018152352400599853204181198284911920639269543713562139081150226946244809860176209309225584961640108009976770013400389619072622937952672828449893541499938540355016121074032097689622219899225803393123737773654860697744699335070625347048428645100966443175144650038627295528000606056965389635788928701972579506282575401611610143237045976542044834574098673894498541598101358295616575726046562110543444263505639515572489299576433162382219988641287240277734501982256859856983892782738649511155488503456418490560457213252006445226764363138797217537165591917332035668570552050930877290229318142508538992727582561568796029986735888351558408231953009199496912555199932304183526384412272096778259370008113182159432293771229555218661221254511847147839165976656955603684159354410250067327108850175425289619366487331670469728691779656492064704961107443809041487721264362964069984106607510732908601966166212301175401\n  public exponent: 65537, comment=/tmp/hugekey_rsa}\n. @davidxia thanks, that code only partially works (it's saves the sha1 encoding, I think most tools these days report sha256 encodings, at least my ssh-add -l and ssh-keygen -lf does), but it points me in the right direction.\n. #786 seems like it logs the fingerprint or some additional key info so i'll merge this half of it now.\n. nice find :+1: \n. Does the stuff in CertificateFileHttpsHandler with loading a Keystore instance load the user's keystore from disk? Why is the password hardcoded? How come we have to add something to the keystore to load the certificate?\nCan you add some comments about how that whole dance works?\n. @rohansingh ah cool. I think you could stick that whole second paragraph in the source as comments and it'd be :ok_hand: \n. See https://github.com/spotify/helios/issues/486. This behavior was accidentally implemented backwards in Helios from how docker itself behaves, but was kept this was to not break compatibility with existing jobs. \n. really nice that all of the ZookeeperOperation implementations already have pretty toStrings \n. @gimaker @rohansingh @davidxia \n. @gimaker @davidxia thanks for the review. I updated the commit to make the log messages more correct and to also handle the updateDeploymentGroupHosts path.\n. @davidxia I think it just updates the original comment in-place\n. The potential infinite loop is because when continueing the loop the next iterations don't fail if the identities.poll() returns null because no more identities are left. \n. @rohansingh @davidxia @gimaker \n. hmm RetryingRequestDispatcher doesn't retry in this case, this needs some work.\n. This is a better version of #792 \n@rohansingh @gimaker @davidxia \n. ...and the integration tests finally pass after retrying the build 3 times in Circle.\n. @rohansingh cc @negz \n. extraHosts support was recently added to docker-client in https://github.com/spotify/docker-client/pull/290 (which is what Helios uses to communicate with docker) so it should be feasible to add this to Helios without having to touch too many other things.\nIn general though, we think using some sort of service discovery mechanism is usually better for these things (going by your example of telling the container how to find a database), so I'm not sure support for this will be added by us - but we are happy to review any contributions for it.\n. Good idea. What does the new line separator end up actually separating?\n. :+1: \n. @rohansingh @davidxia @gimaker \n. Had to rebase on top of master to resolve merge conflicts.\n@rohansingh as a part of this I refactored the signature of CertificateHttpsHandler a bit to make it possible to catch the exceptions in just one place, let me know if this looks weird to you at all. The basic gist was changing \ndiff\n-    abstract Certificate getCertificate() throws Exception;\n-    abstract PrivateKey getPrivateKey() throws Exception;\n+    protected abstract CertificateAndPrivateKey createCertificateAndPrivateKey()\n+        throws IOException, GeneralSecurityException;\nAlso I changed the \"don't fail on setting up client certificate\" part to only catch exceptions when creating the Certificate and PrivateKey instances as it seems to me the operations on the Keystore instance are probably safe once you get past that.\n. damn I can't believe that markdown diff snippet worked on the first try.\n. It took 4 retries to get the CircleCI build to be green :unamused: :triumph: \n\n. Nice :+1: :100: \n. I think this is the largest bump I've seen from codecov messages so far\n\nMerging #799 into master will increase coverage by +0.76% as of 9c0e48d\n. @davidxia @rohansingh @gimaker \n. +1 for not using printf when the String argument is known not to contain any string formatting patterns.\n\nWhy would foo/bar be output as foo%2Fbar though?\n. :+1: \n. the stuff with the list of Rules in ZookeeperACLProvider is nice :ok_hand: \nbut it feels weird to me to hardcode the list of rules in the constructor as oppose to passing them in - this makes the other functionality of that class (scanning the list of rules and OR-ing things together) harder to unit test since you can't test it with different variations of rules. I would suggest moving the hard-coding up a level so you can test the class.\n. @gimaker or just add an overloaded constructor like \npublic ZooKeeperAclProvider(final String masterDigest, final String agentDigest) {\n  this(masterDigest, agentDigest, DEFAULT_RULES);\n}\npublic ZooKeeperAclProvider(final String masterDigest, final String agentDigest, List<Rule> rules) {\n   ...\n}\n. :+1: from me, nice work. The only thing I really have to add here are grammar/documentation nitpicks\n. Is there a short answer as to why all these tests were disabled since 084adaaa8de1c428f312347ba81335b0784a1b0e ?\n. :+1: to cleanup though - it's too bad codecov doesn't count reduced complexity in the tests themselves\n. :+1: \n. :+1: \n. @davidxia \n. d'oh. The code was shadowing clientCertificatePath but then assigning to this.clientCertificatePath but then passing clientCertificatePath to the HeliosClient ctor.\n. :+1: \n. @rohansingh @davidxia \n. @negz \n. :+1: \n. @rohansingh @davidxia @negz \n. Added a test of this, not sure why I forgot that before.\nI think I caught a bug in doing this - the bind argument used when creating the helios-solo container had a value of\n///var/run/docker.sock:///var/run/docker.sock\nThe extra // in front of the path seems wrong.\nMocking the docker calls is pretty ugly :confused: \n. @negz @davidxia @gimaker @rohansingh \n. Fixed some awkwardness in the wording and added a note about the localhost/127.0.0.1 problem. Please review!\n. @davidxia I have a feeling that won't be possible without a lot of junit hacking.\nAssuming you are looking to avoid the time-wait of starting up helios-solo for each test class, I wonder if revamping how HeliosSoloDeployment works could also work:\n- HeliosSoloDeployment uses a fixed name for the container it starts (e.g. helios-solo-auto)\n- at startup, if the helios-solo-auto container is running, just use it\n- otherwise create and start that container\n- after 10 minutes (or some other value) of non-use, the container kills itself\n. @davidxia good point, they should. And/or we could hack TemporaryJobs to ignore filters when using HeliosSoloDeployment as the two concepts don't make sense together. \n. did you base this off of an old branch?\n. I think in that case the command could just report what happened, for instance\nremoved job 1\nno jobs found for job2\ncannot remove job3: because ...\nbut yea the need to refactor the command pretty significantly (and also the resource handler on the server side) is what stopped me from implementing this when i proposed it \n. @rohansingh @davidxia \n. lgtm\n. :+1: \n. Are you having trouble when building with Maven or with your IDE? \nThe file is generated by Maven here.\nSome IDEs either don't know how to parse the pom to do things like this, or they don't know to include target/generated-sources/templated in the classpath. \nFor example sometimes in IntelliJ you have to run mvn compile from the console and add the target/generated-sources/ directories to the classpath manually.\n. :+1: \nwow I missed before that PEMParser.parseObject returns you a plain old Object\n. makes a lot of sense :+1: \n. change makes sense to me but it would be nicer if these values weren't hardcoded to begin with IMHO\n. :+1: \n. I have a few questions.\n1. This works ok with the agent's restricted ACLs?\n2. What happens if agent A goes offline for a long period of time (for instance one month) and is replaced by B with the same name? Is it safe that a potentially \"old\" version of a job would be deployed?\n3. What if A is in a deployment group - when A is offline, it's job isn't updated any longer, is it? Would job:oldversion or job:current-dg-version be deployed to B?\n. Ignoring the ACL or implementation I think that having the \"new\" agent come online with no jobs deployed to it is less of a surprise, especially if it's deployment group (if the agent is in one) would roll out a possibly-newer job anyway.\n. Another option would be to have an API endpoint on the master that the agent could call to handle the deregistration server-side.  \nIt's too bad we didn't add this PR before ACLs\n. I like that idea \ud83d\udc4d\ud83c\udffb\n. the reRegister test seems like it would be better handled in a unit test where you could start up the ZK testing server, call the reregister function, and assert that certain nodes have had their values/ctime/mtime changed (from client.stat(path)) and that other nodes have not been changed.\n. By \"shoddy\" do you mean DeadAgentReaperTest is flaky or not as clean as you would like? I think it would read a lot easier if it was a ParameterizedTest (you could do if (expectReaped) instead of the filter stuff), and if the test invoked runOneIteration and did verify(mastermodel).deregister(the-host, ..) instead of invoking the getDeadAgents method.\n. Oh right good point. Disregard my comments about ParameterizedTest then. But I still think you could get mileage out of calling runOneIteration (or whatever does the actual get list and deregister) and calling verify on the mock\n. Btw one other thought, and this is probably wider than just this one change, but for these periodic tasks like \"reaping\", does it really make sense for each master in a cluster to be running these same tasks? I assume using transactions with the ZK operations prevents data inconsistencies but it seems unnecessary for each node to be doing the same work. We could use Curator's LeaderLatch to designate one node as the batch job runner.\n. I am down with this too :+1: \n. @rohansingh @davidxia @gimaker \nThis change works in helios-solo but to be honest I am not sure if it makes any sense for the more-usual case of a non-container helios-agent that communicates with docker using unix sockets. In that case, connecting to the bridge interface and the external port wouldn't work, would it? This logic is probably too specialized for helios-solo.\nI also considered having helios-solo add a JVM system property at startup when launching the agent that could be checked here in the Healthchecker to tweak the logic, but that feels too hacky - this issue seems like it should be easily generalized.\n. lgtm\n. so it would retry immediately (and infinitely)?\n. I was going to suggest adding a test to ensure the desired behavior, by mocking out the zooKeeperRegistrar and having the first call return an exception and the second one succeed, but I am not really sure how you would test the Thread.sleep part.\n:+1: \n. but that wouldn't help test that the thread actually slept, would it?\n. nice, I didn't know about Sleeper :+1: \n. :fireworks: :tada: all the tests pass on the first try :tada: :fireworks: \n@negz @gimaker @rohansingh @davidxia \n. to be clear having the HttpHealthChecker branch on a system property to check if it is running inside helios-solo or not feels pretty hacky; but I wasn't sure how to programmatically have the agent know if it is running inside a container (which is on the same host as the container for the actual job).\nThis is more or less the equivalent of having a global variable that get set/read in unclear places, but it tackles this case unobtrusively IMHO.\n. @gimaker fair point. See ce4a65c\n. @davidxia \n. @gimaker @davidxia \n. @spotify/helios-team \n. :+1: \n. :+1: nice job figuring out the steps to reproduce\n. I think it would be nicer to have one gauge per healthcheck, rather than one overall gauge, so the values can be reported separately. \nAlso I think the same gauges should be added to AgentService as well.\nPerhaps something like this in MasterService/AgentService\n``` java\n    // after all environment.healthchecks().register(..) calls...\n    for (final String name : environment.healthChecks().getNames()) {\n      environment.metrics().register(\"helios.\" + name + \".ok\", new Gauge() {\n        @Override\n        public Boolean getValue() {\n          final HealthCheck.Result result = environment.healthChecks().runHealthCheck(name);\n          return result.isHealthy();\n        }\n      });\n    }\n``\n. We should be able to set up an alert if _any_ of the healthchecks go to 0; basically a group byhost, healthcheckname`\n. :+1: for the actual healthcheck change but I think the two things pointed out above are not necessary\n. I upgraded java in helios-solo-base back in that original PR, in 0.3\n$> docker run -it spotify/helios-solo-base:0.3 bin/bash\nroot@3024bb6592c7:/# java -version\njava version \"1.8.0_60\"\nJava(TM) SE Runtime Environment (build 1.8.0_60-b27)\nJava HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)\nroot@3024bb6592c7:/#\nbut it doesn't hurt to bump the version again\n. I think a much simpler test would be to extract the anonymous class for the \"Gauge of a Healthcheck\" to it's own class and write a unit test for that, to see what value it returns when the Healthcheck returns different values. Then you wouldn't need these descriptors or pojos or tests that make http calls to a servlet. \n. If you think the system test adds value I won't quibble with adding it; although I don't think I would have chosen to test that piece of logic myself around whether or not the metric is actually added.\nBut:\n- adding Gauge and Metrics as Descriptor subclasses feels weird to me, since they really aren't descriptors in the main sense of the class\n- Instead of making them Descriptors to gain access to easy JSON to POJO serialization/deserialization, I personally find it easier to just use ObjectMapper and it's tree API (JsonNode, ObjectNode) in tests\n- I think these classes, and AgentArgsBuilder, should be in src/test/java if they are only for testing purposes and never to be used in the actual main source code\n. :+1: :+1: \n. Take a look at the tests I added in HeliosSoloDeploymentTest, where the DockerClient is mocked out and the test asserts the values passed to dockerClient.createContainer etc. You could have a test of the default logic that it matches the hard-coded format and another test that builder.registrationFormat(\"foo\") gets passed in as expected, etc\n. I think a way to override this format with the builder is missing though?\n. In a test of a real container / helios-solo instance you'd have to find a way to test what registrar format helios-solo uses when talking to skydns, either by querying skydns or making sure a second container that does dns lookups for the first one works. \nIf you think of the environment variable as the API to helios-solo then I would just test that this class communicates with the API contract correctly myself :smile: \n. @negz good catch. I never noticed that test before. I agree on modernizing it\n. Since TemporaryJobs already looks in helios.conf and helios-base.conf for a bunch of configuration it shouldn't be too hard to add registrarHostFormat there also.\nThe only thing I'd urge if we go with that approach is that it's possible for the internal configuration from spotify-helios-testing (or any user, really) to default to the value we prefer. \n. lgtm\n. the Conf stuff in 6b80697 looks nice, just a few small comments above\n. :+1: \n. lgtm\n. I'll give you my lambdas when you pry them from my cold, dead hands\n. Other than above, looks good to me, nice job :+1: :ok_hand: \n. @spotify/helios-team \n. @gimaker \n[mattbrown@mattbrown-air ~]$ fwc --raw\n11:01:19 INFO: 0: {\"id\"=>\"core.output\", \"type\"=>\"metric\", \"data\"=>{\"time\"=>1454515279, \"key\"=>\"helios-master\", \"value\"=>0.0, \"host\"=>\"mattbrown-air.local\", \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.container_started_counter\"}}}\n11:01:19 INFO: 1: {\"id\"=>\"core.input\", \"type\"=>\"metric\", \"data\"=>{\"key\"=>\"helios-master\", \"value\"=>0.0, \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.container_started_counter\"}, \"time\"=>\"2016-02-03 11:01:19 -0500\", \"host\"=>\"mattbrown-air.local\", \"fixed_tags\"=>[], \"fixed_attr\"=>{}}}\n11:01:19 INFO: 2: {\"id\"=>\"core.output\", \"type\"=>\"metric\", \"data\"=>{\"time\"=>1454515279, \"key\"=>\"helios-master\", \"value\"=>0.0, \"host\"=>\"mattbrown-air.local\", \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.containers_exited_counter\"}}}\n11:01:19 INFO: 3: {\"id\"=>\"core.input\", \"type\"=>\"metric\", \"data\"=>{\"key\"=>\"helios-master\", \"value\"=>0.0, \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.containers_exited_counter\"}, \"time\"=>\"2016-02-03 11:01:19 -0500\", \"host\"=>\"mattbrown-air.local\", \"fixed_tags\"=>[], \"fixed_attr\"=>{}}}\n11:01:19 INFO: 4: {\"id\"=>\"core.output\", \"type\"=>\"metric\", \"data\"=>{\"time\"=>1454515279, \"key\"=>\"helios-master\", \"value\"=>0.0, \"host\"=>\"mattbrown-air.local\", \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.containers_running_counter\"}}}\n11:01:19 INFO: 5: {\"id\"=>\"core.input\", \"type\"=>\"metric\", \"data\"=>{\"key\"=>\"helios-master\", \"value\"=>0.0, \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.containers_running_counter\"}, \"time\"=>\"2016-02-03 11:01:19 -0500\", \"host\"=>\"mattbrown-air.local\", \"fixed_tags\"=>[], \"fixed_attr\"=>{}}}\n11:01:19 INFO: 6: {\"id\"=>\"core.output\", \"type\"=>\"metric\", \"data\"=>{\"time\"=>1454515279, \"key\"=>\"helios-master\", \"value\"=>0.0, \"host\"=>\"mattbrown-air.local\", \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.containers_threw_exception_counter\"}}}\n11:01:19 INFO: 7: {\"id\"=>\"core.input\", \"type\"=>\"metric\", \"data\"=>{\"key\"=>\"helios-master\", \"value\"=>0.0, \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.containers_threw_exception_counter\"}, \"time\"=>\"2016-02-03 11:01:19 -0500\", \"host\"=>\"mattbrown-air.local\", \"fixed_tags\"=>[], \"fixed_attr\"=>{}}}\n11:01:19 INFO: 8: {\"id\"=>\"core.output\", \"type\"=>\"metric\", \"data\"=>{\"time\"=>1454515279, \"key\"=>\"helios-master\", \"value\"=>0.0, \"host\"=>\"mattbrown-air.local\", \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.docker_timeout_counter\"}}}\n11:01:19 INFO: 9: {\"id\"=>\"core.input\", \"type\"=>\"metric\", \"data\"=>{\"key\"=>\"helios-master\", \"value\"=>0.0, \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.docker_timeout_counter\"}, \"time\"=>\"2016-02-03 11:01:19 -0500\", \"host\"=>\"mattbrown-air.local\", \"fixed_tags\"=>[], \"fixed_attr\"=>{}}}\n11:01:19 INFO: 10: {\"id\"=>\"core.output\", \"type\"=>\"metric\", \"data\"=>{\"time\"=>1454515279, \"key\"=>\"helios-master\", \"value\"=>0.0, \"host\"=>\"mattbrown-air.local\", \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.image_cache_hit_counter\"}}}\n11:01:19 INFO: 11: {\"id\"=>\"core.input\", \"type\"=>\"metric\", \"data\"=>{\"key\"=>\"helios-master\", \"value\"=>0.0, \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.image_cache_hit_counter\"}, \"time\"=>\"2016-02-03 11:01:19 -0500\", \"host\"=>\"mattbrown-air.local\", \"fixed_tags\"=>[], \"fixed_attr\"=>{}}}\n11:01:19 INFO: 12: {\"id\"=>\"core.output\", \"type\"=>\"metric\", \"data\"=>{\"time\"=>1454515279, \"key\"=>\"helios-master\", \"value\"=>0.0, \"host\"=>\"mattbrown-air.local\", \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.image_pull_failed\"}}}\n11:01:19 INFO: 13: {\"id\"=>\"core.input\", \"type\"=>\"metric\", \"data\"=>{\"key\"=>\"helios-master\", \"value\"=>0.0, \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.image_pull_failed\"}, \"time\"=>\"2016-02-03 11:01:19 -0500\", \"host\"=>\"mattbrown-air.local\", \"fixed_tags\"=>[], \"fixed_attr\"=>{}}}\n11:01:19 INFO: 14: {\"id\"=>\"core.output\", \"type\"=>\"metric\", \"data\"=>{\"time\"=>1454515279, \"key\"=>\"helios-master\", \"value\"=>0.0, \"host\"=>\"mattbrown-air.local\", \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.image_pull_successful\"}}}\n11:01:19 INFO: 15: {\"id\"=>\"core.input\", \"type\"=>\"metric\", \"data\"=>{\"key\"=>\"helios-master\", \"value\"=>0.0, \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.image_pull_successful\"}, \"time\"=>\"2016-02-03 11:01:19 -0500\", \"host\"=>\"mattbrown-air.local\", \"fixed_tags\"=>[], \"fixed_attr\"=>{}}}\n11:01:19 INFO: 16: {\"id\"=>\"core.output\", \"type\"=>\"metric\", \"data\"=>{\"time\"=>1454515279, \"key\"=>\"helios-master\", \"value\"=>0.0, \"host\"=>\"mattbrown-air.local\", \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.supervisor_closed_counter\"}}}\n11:01:19 INFO: 17: {\"id\"=>\"core.input\", \"type\"=>\"metric\", \"data\"=>{\"key\"=>\"helios-master\", \"value\"=>0.0, \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.supervisor_closed_counter\"}, \"time\"=>\"2016-02-03 11:01:19 -0500\", \"host\"=>\"mattbrown-air.local\", \"fixed_tags\"=>[], \"fixed_attr\"=>{}}}\n11:01:19 INFO: 18: {\"id\"=>\"core.output\", \"type\"=>\"metric\", \"data\"=>{\"time\"=>1454515279, \"key\"=>\"helios-master\", \"value\"=>0.0, \"host\"=>\"mattbrown-air.local\", \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.supervisor_run_counter\"}}}\n11:01:19 INFO: 19: {\"id\"=>\"core.input\", \"type\"=>\"metric\", \"data\"=>{\"key\"=>\"helios-master\", \"value\"=>0.0, \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.supervisor_run_counter\"}, \"time\"=>\"2016-02-03 11:01:19 -0500\", \"host\"=>\"mattbrown-air.local\", \"fixed_tags\"=>[], \"fixed_attr\"=>{}}}\n11:01:19 INFO: 20: {\"id\"=>\"core.output\", \"type\"=>\"metric\", \"data\"=>{\"time\"=>1454515279, \"key\"=>\"helios-master\", \"value\"=>0.0, \"host\"=>\"mattbrown-air.local\", \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.supervisor_stopped_counter\"}}}\n11:01:19 INFO: 21: {\"id\"=>\"core.input\", \"type\"=>\"metric\", \"data\"=>{\"key\"=>\"helios-master\", \"value\"=>0.0, \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.supervisor_stopped_counter\"}, \"time\"=>\"2016-02-03 11:01:19 -0500\", \"host\"=>\"mattbrown-air.local\", \"fixed_tags\"=>[], \"fixed_attr\"=>{}}}\n11:01:19 INFO: 22: {\"id\"=>\"core.output\", \"type\"=>\"metric\", \"data\"=>{\"time\"=>1454515279, \"key\"=>\"helios-master\", \"value\"=>0.0, \"host\"=>\"mattbrown-air.local\", \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.supervisors_created_counter\"}}}\n11:01:19 INFO: 23: {\"id\"=>\"core.input\", \"type\"=>\"metric\", \"data\"=>{\"key\"=>\"helios-master\", \"value\"=>0.0, \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.agent_supervisor.supervisors_created_counter\"}, \"time\"=>\"2016-02-03 11:01:19 -0500\", \"host\"=>\"mattbrown-air.local\", \"fixed_tags\"=>[], \"fixed_attr\"=>{}}}\n11:01:19 INFO: 24: {\"id\"=>\"core.output\", \"type\"=>\"metric\", \"data\"=>{\"time\"=>1454515279, \"key\"=>\"helios-master\", \"value\"=>0.0, \"host\"=>\"mattbrown-air.local\", \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.zookeeper.transient_error_count\"}}}\n11:01:19 INFO: 25: {\"id\"=>\"core.input\", \"type\"=>\"metric\", \"data\"=>{\"key\"=>\"helios-master\", \"value\"=>0.0, \"attributes\"=>{\"metric_type\"=>\"counter\", \"what\"=>\"helios.zookeeper.transient_error_count\"}, \"time\"=>\"2016-02-03 11:01:19 -0500\", \"host\"=>\"mattbrown-air.local\", \"fixed_tags\"=>[], \"fixed_attr\"=>{}}}\n11:01:19 INFO: 26: {\"id\"=>\"core.output\", \"type\"=>\"metric\", \"data\"=>{\"time\"=>1454515279, \"key\"=>\"helios-master\", \"value\"=>0.0, \"host\"=>\"mattbrown-air.local\", \"attributes\"=>{\"metric_type\"=>\"meter\", \"unit\"=>\"n/s\", \"stat\"=>\"1m\", \"what\"=>\"helios.agent_supervisor.container_started_meter\"}}}\n11:01:19 INFO: 27: {\"id\"=>\"core.input\", \"type\"=>\"metric\", \"data\"=>{\"key\"=>\"helios-master\", \"value\"=>0.0, \"attributes\"=>{\"metric_type\"=>\"meter\", \"unit\"=>\"n/s\", \"stat\"=>\"1m\", \"what\"=>\"helios.agent_supervisor.container_started_meter\"}, \"time\"=>\"2016-02-03 11:01:19 -0500\", \"host\"=>\"mattbrown-air.local\", \"fixed_tags\"=>[], \"fixed_attr\"=>{}}}\nwith fwc the stuff in the data hash is the real event being sent. So a single event (picked arbitrarily) looks like:\n{\n    \"key\"=>\"helios-master\", \n    \"value\"=>0.0, \n    \"attributes\"=>{\n        \"metric_type\"=>\"meter\", \n        \"unit\"=>\"n/s\", \n        \"stat\"=>\"1m\", \n        \"what\"=>\"helios.agent_supervisor.container_started_meter\"\n    }, \n    \"time\"=>\"2016-02-03 11:01:19 -0500\", \n    \"host\"=>\"mattbrown-air.local\", \n    \"fixed_tags\"=>[], \n    \"fixed_attr\"=>{}\n}\nffwd is enriching the event with the time host etc. Only key, value, attributes are coming from helios.\nSince this metric was a meter, there is another metric emitted that looks similar except it has \"stat\" => \"5m\".\nAlso note that this was from running the master and the metric is for helios.agent_supervisor.container_started_meter, which is a value that is never incremented (or relevant) for the master. This is what I was referring to in wanting to refactor MetricsImpl to eliminate emission of unused metrics in the master or agent.\n. c4925d9 splits out com.spotify.helios.servicescommon.statistics.MetricsImpl so that masters do not create/track SupervisorMetrics and agents do not have real MasterMetrics. This is meant to reduce the amount of data sent downstream by the reporter - there is no point in the master reporting agent-specific metrics that will always have values of 0.\nPlease review 2d8cc6b - I noticed that the metrics being emitted to ffwd did not include the internal Dropwizard metrics, which seems like they would be useful to have. Is there any reason why back in the day a separate MetricRegistry was constructed?\n. d675660 seems to have fixed the unit test flakiness \n. thanks for the diligent review @gimaker :grinning: \n. I think the logic looks ok but it would be nice if there was a test of the cached-file-writing and -reading too, perhaps by extracting that logic out of the ssh handler and into it's own class.\n. Overall :+1: and this definitely looks cleaner\n. Oh lame, because I commented on the commit itself my comments are lost, dumb github\n:+1: \n. I kicked off a rebuild of the CircleCI build as the timeout looks unrelated to this change.\n. LGTM, but I think it would be nice if a test could be added that verifies that the udp prober works in an actual TemporaryJob\n. Thanks for adding the test. I am confused though on how it asserts that the UDP port is \"probed\"? Simply by asserting that the job is running ok?\nAlso, why the indirection in the test where the outer @Test calls testResult() in a nested class that looks a lot like a test itself?\n. Nah, I don't know a better way\n:+1: \n. @pehrs this should be available in helios-testing:0.8.753 now.\n. @spotify/helios-team \n. old:\n$> helios -z https://foobar.spotify.net masters\n10:12:55.367 WARN  Endpoints: Unable to resolve hostname foobar.spotify.net into IP address: {}\njava.net.UnknownHostException: foobar.spotify.net: unknown error\n    at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method) ~[na:1.8.0_71]\n    at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928) ~[na:1.8.0_71]\n    at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323) ~[na:1.8.0_71]\n    at java.net.InetAddress.getAllByName0(InetAddress.java:1276) ~[na:1.8.0_71]\n    at java.net.InetAddress.getAllByName(InetAddress.java:1192) ~[na:1.8.0_71]\n    at java.net.InetAddress.getAllByName(InetAddress.java:1126) ~[na:1.8.0_71]\n    at org.apache.http.impl.conn.SystemDefaultDnsResolver.resolve(SystemDefaultDnsResolver.java:45) ~[helios-tools-0.8.700-shaded.jar:0.8.700]\n    at com.spotify.helios.client.Endpoints.of(Endpoints.java:93) ~[helios-tools-0.8.700-shaded.jar:0.8.700]\n    at com.spotify.helios.client.Endpoints$1.get(Endpoints.java:69) [helios-tools-0.8.700-shaded.jar:0.8.700]\n    at com.spotify.helios.client.Endpoints$1.get(Endpoints.java:66) [helios-tools-0.8.700-shaded.jar:0.8.700]\n    at com.spotify.helios.client.HeliosClient$Builder.createHttpConnector(HeliosClient.java:606) [helios-tools-0.8.700-shaded.jar:0.8.700]\n    at com.spotify.helios.client.HeliosClient$Builder.createDispatcher(HeliosClient.java:595) [helios-tools-0.8.700-shaded.jar:0.8.700]\n    at com.spotify.helios.client.HeliosClient$Builder.build(HeliosClient.java:580) [helios-tools-0.8.700-shaded.jar:0.8.700]\n    at com.spotify.helios.cli.Utils.getClient(Utils.java:76) [helios-tools-0.8.700-shaded.jar:0.8.700]\n    at com.spotify.helios.cli.command.ControlCommand.run(ControlCommand.java:121) [helios-tools-0.8.700-shaded.jar:0.8.700]\n    at com.spotify.helios.cli.command.ControlCommand.run(ControlCommand.java:90) [helios-tools-0.8.700-shaded.jar:0.8.700]\n    at com.spotify.helios.cli.CliMain.run(CliMain.java:75) [helios-tools-0.8.700-shaded.jar:0.8.700]\n    at com.spotify.helios.cli.CliMain.main(CliMain.java:53) [helios-tools-0.8.700-shaded.jar:0.8.700]\n10:12:55.885 WARN  RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\n...\nit tries again...\nand again...\n...\nnew:\n$> ./bin/helios -z https://foobar.spotify.net masters\nrunning in helios project, using /Users/mattbrown/code/helios/helios-tools/target/helios-tools-0.8.0-SNAPSHOT-shaded.jar\n10:13:00.382 WARN  Endpoints: Unable to resolve hostname foobar.spotify.net into IP address: {}\njava.net.UnknownHostException: foobar.spotify.net: unknown error\n    at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method) ~[na:1.8.0_71]\n    at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928) ~[na:1.8.0_71]\n    at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323) ~[na:1.8.0_71]\n    at java.net.InetAddress.getAllByName0(InetAddress.java:1276) ~[na:1.8.0_71]\n    at java.net.InetAddress.getAllByName(InetAddress.java:1192) ~[na:1.8.0_71]\n    at java.net.InetAddress.getAllByName(InetAddress.java:1126) ~[na:1.8.0_71]\n    at org.apache.http.impl.conn.SystemDefaultDnsResolver.resolve(SystemDefaultDnsResolver.java:45) ~[helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at com.spotify.helios.client.Endpoints.of(Endpoints.java:93) ~[helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at com.spotify.helios.client.Endpoints$1.get(Endpoints.java:69) [helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at com.spotify.helios.client.Endpoints$1.get(Endpoints.java:66) [helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at com.spotify.helios.client.HeliosClient$Builder.createHttpConnector(HeliosClient.java:606) [helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at com.spotify.helios.client.HeliosClient$Builder.createDispatcher(HeliosClient.java:595) [helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at com.spotify.helios.client.HeliosClient$Builder.build(HeliosClient.java:580) [helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at com.spotify.helios.cli.Utils.getClient(Utils.java:76) [helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at com.spotify.helios.cli.command.ControlCommand.run(ControlCommand.java:121) [helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at com.spotify.helios.cli.command.ControlCommand.run(ControlCommand.java:90) [helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at com.spotify.helios.cli.CliMain.run(CliMain.java:75) [helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\n    at com.spotify.helios.cli.CliMain.main(CliMain.java:53) [helios-tools-0.8.0-SNAPSHOT-shaded.jar:0.8.0-SNAPSHOT]\nno endpoints found to connect to, check your configuration\n. @spotify/helios-team \n. hmm, this breaks ConfigTest, which seems to create a HeliosClient using the helios-ci profile in such a way that would never work in a non-Spotify environment\n. refactored the ConfigTest a bit in 1522546 so that it's not actually constructing a real HeliosClient that does real DNS queries any more.\n. My own two cents is that it is not a very useful test of ReportingZooKeeperClient to verify that it called a certain method of the underlying client. It's more useful to test the values that those methods return.\nWhat I would test though instead would be the new Timer time(name, callable) method in ZooKeeperMetricsImpl - you could have a test that asserts that a call like .time(\"foo\", ..) adds a timer to the registry with the expected full name using the prefix.\n. also yay for java 8 language features :tada: \n. Ah I didn't realize the errors you were having were these kinds - I think you can configure the javadoc plugin to not fail on these for missing tags in functions, etc.\nThe more annoying ones I've seen are where the new java 8 javadoc tool validates the html in any javadoc comment\n. for instance if the $DEFAULTFILE that the upstart scripts source contains a declaration of JAVA=, then this should respect that path for launching java when running helios-agent or helios-master.\n. @spotify/helios-team \n. :+1: \n. :+1: \nI am really curious why some of the checks in this test have a timeout of 5 minutes, which seems huge.\n. Doesn't the call to awaitHostRegistered earlier in the test take care of this? It seems like you would just be reducing the window of a race here by checking an earlier-written path, instead of removing it altogether. Maybe awaitHostRegistered should not just check that the host is in hosts -q but also that the status is UP.\n. ah I understand now, makes sense. :+1: \n. nice :+1: \n. this change would make it harder to debug if there was > 1 job running and you were trying to figure out which they were, although maybe there are more logs that would tell you that instead\n. Tests are green now !\n. @spotify/helios-team \n. :+1: for consolidation\n. lgtm\n. you could move the duplicated <labels> up out of the <executions> section and into the parent <configuration> of the plugin, so you don't have to duplicate them, but otherwise :+1: \n. @davidxia I think so. A problem arises though if the network connection to Docker Hub has an issue; for some test users it ended up failing their build and tests, even though they had the alpine image already.\n@negz nice change. Agree with Staffan on the test but otherwise :+1: \n. :+1: for the unit tests\n. :+1: \n. still :+1: from me\n. @negz on this branch I started refactoring the PortAllocator by extracting an interface for it. My idea was to keep the original implementation as \"SimplePortAllocator\" and then create a variant that could do randomization like this. I was thinking then that the choice of which allocation could be done via a CLI argument to the agent. I didn't actually get to the point of creating another implementation yet though.\nAfterwards I realized that a better way would be to just extract the \"dynamic port allocation\" part to an interface, and leave the static port assignment in PortAllocator (as it seems pretty uncontroversial and unlikely to change strategies - that is why they are static).\nBut perhaps having multiple implementations and making the dynamic port allocation \"strategy\" a CLI argument is way overkill.\n. Instead of shuffling the list on each call to allocate, I think it would be simpler in some ways to shuffle it once at construction-time and maintain an index into the list (like the old implementation did) so that each new call to allocate just moves the index up (and checks if it can use the value in potentialPorts[index]\n. :+1: looks good to me!\n. I made sure the new behavior is tested in unit tests but I did not add any system tests as I am not really sure how to reliably reproduce the race condition or assert that they have been fixed in a test.\n. This is what the log output from the agent looks like with this change when the ID has changed from what is already registered in ZooKeeper:\n```\n            .  . _ \n /   |   \\   _ |  | |_|  __   /  _  \\     _   /  |_\n/    ~    _/  \\|  | |  |/  _ \\/  /  /  /\\  \\  / _/  \\ /    \\   \\\n\\    Y    /\\  /|  ||  (  <_> ) \\  /    |    \\/ //  >  /|   |  \\  |\n _|  /  _  >_/_|_/_  > _|  /_  / ___  >|  /_|\n       \\/       \\/                   \\/          \\//___/      \\/     \\/\n13:04:02.519 helios[75898]: INFO  [AgentService STARTING] SetUIDListener: Opened application@1b957672{HTTP/1.1}{0.0.0.0:5803}\n13:04:02.520 helios[75898]: INFO  [AgentService STARTING] SetUIDListener: Opened admin@20f0ff7a{HTTP/1.1}{0.0.0.0:5804}\n13:04:02.526 helios[75898]: WARN  [Reactor(zk-client-async-init)] AgentZooKeeperRegistrar: Another agent already registered as 'mattbrown-air.local' (local=22F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229 remote=92F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229). That agent's registration expires in 106 seconds\n13:04:02.526 helios[75898]: INFO  [Reactor(zk-client-async-init)] ZooKeeperRegistrarService: registration not successful, sleeping for 1 seconds\n13:04:02.530 helios[75898]: INFO  [AgentService STARTING] Server: jetty-9.0.z-SNAPSHOT\n13:04:02.720 helios[75898]: INFO  [AgentService STARTING] WebApplicationImpl: Initiating Jersey application, version 'Jersey: 1.18.1 02/19/2014 03:28 AM'\n13:04:02.864 helios[75898]: INFO  [AgentService STARTING] DropwizardResourceConfig: The following paths were found for the configured resources:\nGET     /helios/tasks (com.spotify.helios.agent.AgentModelTaskResource)\nGET     /helios/taskstatus (com.spotify.helios.agent.AgentModelTaskStatusResource)\n\n13:04:03.354 helios[75898]: INFO  [AgentService STARTING] ContextHandler: Started i.d.j.MutableServletContextHandler@1c1841f9{/,null,AVAILABLE}\n13:04:03.358 helios[75898]: INFO  [AgentService STARTING] AdminEnvironment: tasks =\nPOST    /tasks/gc (io.dropwizard.servlets.tasks.GarbageCollectionTask)\n\n13:04:03.371 helios[75898]: INFO  [AgentService STARTING] ContextHandler: Started i.d.j.MutableServletContextHandler@480a9a74{/,null,AVAILABLE}\n13:04:03.377 helios[75898]: INFO  [AgentService STARTING] ServerConnector: Started application@1b957672{HTTP/1.1}{0.0.0.0:5803}\n13:04:03.378 helios[75898]: INFO  [AgentService STARTING] ServerConnector: Started admin@20f0ff7a{HTTP/1.1}{0.0.0.0:5804}\n13:04:03.620 helios[75898]: WARN  [Reactor(zk-client-async-init)] AgentZooKeeperRegistrar: Another agent already registered as 'mattbrown-air.local' (local=22F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229 remote=92F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229). That agent's registration expires in 104 seconds\n13:04:03.620 helios[75898]: WARN  [Reactor(zk-client-async-init)] ZooKeeperRegistrarService: registration not successful, sleeping for 1 seconds\n13:04:05.519 helios[75898]: WARN  [Reactor(zk-client-async-init)] AgentZooKeeperRegistrar: Another agent already registered as 'mattbrown-air.local' (local=22F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229 remote=92F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229). That agent's registration expires in 103 seconds\n13:04:05.519 helios[75898]: WARN  [Reactor(zk-client-async-init)] ZooKeeperRegistrarService: registration not successful, sleeping for 3 seconds\n13:04:09.228 helios[75898]: WARN  [Reactor(zk-client-async-init)] AgentZooKeeperRegistrar: Another agent already registered as 'mattbrown-air.local' (local=22F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229 remote=92F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229). That agent's registration expires in 99 seconds\n13:04:09.228 helios[75898]: WARN  [Reactor(zk-client-async-init)] ZooKeeperRegistrarService: registration not successful, sleeping for 7 seconds\n13:04:16.371 helios[75898]: WARN  [Reactor(zk-client-async-init)] AgentZooKeeperRegistrar: Another agent already registered as 'mattbrown-air.local' (local=22F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229 remote=92F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229). That agent's registration expires in 92 seconds\n13:04:16.371 helios[75898]: WARN  [Reactor(zk-client-async-init)] ZooKeeperRegistrarService: registration not successful, sleeping for 17 seconds\n13:04:33.743 helios[75898]: WARN  [Reactor(zk-client-async-init)] AgentZooKeeperRegistrar: Another agent already registered as 'mattbrown-air.local' (local=22F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229 remote=92F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229). That agent's registration expires in 74 seconds\n13:04:33.743 helios[75898]: WARN  [Reactor(zk-client-async-init)] ZooKeeperRegistrarService: registration not successful, sleeping for 30 seconds\n13:05:03.751 helios[75898]: WARN  [Reactor(zk-client-async-init)] AgentZooKeeperRegistrar: Another agent already registered as 'mattbrown-air.local' (local=22F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229 remote=92F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229). That agent's registration expires in 44 seconds\n13:05:03.751 helios[75898]: WARN  [Reactor(zk-client-async-init)] ZooKeeperRegistrarService: registration not successful, sleeping for 30 seconds\n13:05:33.767 helios[75898]: WARN  [Reactor(zk-client-async-init)] AgentZooKeeperRegistrar: Another agent already registered as 'mattbrown-air.local' (local=22F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229 remote=92F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229). That agent's registration expires in 14 seconds\n13:05:33.767 helios[75898]: WARN  [Reactor(zk-client-async-init)] ZooKeeperRegistrarService: registration not successful, sleeping for 30 seconds\n13:06:03.777 helios[75898]: WARN  [Reactor(zk-client-async-init)] AgentZooKeeperRegistrar: Another agent has already registered as 'mattbrown-air.local', but its ID node was last updated more than 120000 milliseconds ago. I'm deregistering the agent with the old ID of 92F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229 and replacing it with this new agent with ID '22F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229'.\n13:06:03.795 helios[75898]: INFO  [Reactor(zk-client-async-init)] ZooKeeperRegistrarUtil: re-registering host: mattbrown-air.local, new host id: 22F5FBA85B0BAB5FF6C6F2B435D5D0D3F05DF229\n13:06:03.853 helios[75898]: INFO  [Reactor(zk-client-async-init)] AgentZooKeeperRegistrar: ZooKeeper registration complete\n13:06:03.853 helios[75898]: INFO  [Reactor(zk-client-async-init)] ZooKeeperRegistrarService: Successfully registered host in zookeeper\n```\n. @spotify/helios-team \nit might be easier to review each commit at a time\n. I hit rebuilt enough times to finally get Circle back to green\n. seems like it all could be logged in one statement but :+1: for more info anyway\n. for instance:\n$ bin/helios --http-timeout 1 --retry-timeout 300 -d some.slow.site status\nrunning in helios project, using /Users/mattbrown/code/helios/helios-tools/target/helios-tools-0.8.0-SNAPSHOT-shaded.jar\n16:44:31.030 WARN  RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\n16:44:37.125 WARN  RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\n16:44:43.209 WARN  RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\n16:44:49.290 WARN  RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\n16:44:55.377 WARN  RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\n16:45:01.464 WARN  RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\n16:45:07.558 WARN  RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\n16:45:13.637 WARN  RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\n16:45:19.730 WARN  RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\n16:45:25.880 WARN  RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\n16:45:31.969 WARN  RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\n16:45:38.050 WARN  RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\n16:45:44.135 WARN  RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\n16:45:50.216 WARN  RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\n16:45:56.294 WARN  RetryingRequestDispatcher: Failed to connect, retrying in 5 seconds.\n...\n. @spotify/helios-team \n. was hoping for more eyeballs but I am fine making executive decisions\n. nice, didn't realize the joda-time formatter was thread-safe itself.\nin retrospect this was pretty silly of me to nag about\n:+1: \n. @spotify/helios-team \n. @spotify/helios-team \n. @negz @spotify/helios-team \n. looks ok to me but tagged some of the main authors for more feedback\n(and thanks @deverant )\n. @deverant this should be available in helios-solo 0.8.753 now.\n. :+1: \n. @rohansingh merge?\n. @spotify/helios-team \n. @spotify/helios-team \n. I find it comical that this causes the code coverage to go down\n. @gimaker d'oh, thanks. I'll refactor that test to add my additional cases.\n. @gimaker consolidated my new test with the existing one\n. @spotify/helios-team \n. what does the slf4j setup look like in your project where this happens? Is slf4j-simple on the classpath by any chance? http://www.slf4j.org/apidocs/org/slf4j/impl/SimpleLogger.html\n. I think the test would be more reliable as a unit test, to assert that each job returned by listJobs has stop called on it\n. test looks good to me\n. In the case that the job has been deleted from the master, but the agent is in the process of undeploying it, does it really make sense to undeploy it again? Maybe it is simpler to just have HSD wait a little bit longer?\nI think maybe we might need to customize the cleanup logic that HSD goes through, where somehow shutting down the container causes everything that helios-solo-container has ever deployed to be removed/undeployed - which means that helios-solo would have to keep track of what it has deployed somehow. \nI think trying to do cleanup on the java side, in HeliosSoloDeployment.close(), is bound to always miss stuff.\n. Not directly related to this change but it worries me that the API for using this new flavor of Helios-solo keeps changing. It makes it harder for users to keep up with updates or makes a larger barrier to change. \nIf we have time it might make sense to revisit this so that all of this setup can be done inside TemporaryJobs or some other Helios-supplied component instead of user code. \n. In order to talk to HeliosSoloDeployment, we pass in heliosSoloDeployment.client() to temporaryJobs.client(..). \nPerhaps we could add a similar concept where TemporaryJob has a thing that it uses to ask how to get logs, and then we customize it for HSD\n. Do you still want to make this change? Sounds okay to me \ud83d\udc4d \n. found by @javoire\n. Looks good, but a few thoughts:\n- perhaps the \"cap\" whitelist should be stored on the helios master, so that it can report an error if someone tries to create a job with capabilities that are not allowed? Looks like in this patch, any unallowed capabilities are simply dropped without any feedback to the user\n- representing the \"caps\" as a Set instead of List makes checks like \"is every requested cap in the whitelist\" a simpler operation\n- is \"cap\" a well enough known name for this sort of thing? I'd go with calling it \"capability\" inside the json unless it's really standard\n. also I feel like it is safer for the default to be an empty whitelist, i.e. nothing is allowed\n. a few minor things above but :+1: if they are all addressed\n. thanks to @mindjiver for suggesting\n. This fixes NoClassDefFoundErrors that @marcusb gets when building a project that uses Hadoop and helios-testing. \n. :+1: \n. I don't follow why you have to disable the JobValidator behavior when testing the CLI, if the validation is on the server, isn't that covered by using a mock HeliosClient?\n. and a Builder for a class with three params seems very verbose too\n. :+1: \n. :+1: \n. Nice explanation in the commit message \ud83d\udc4d\ud83c\udffb\nIs it meant to commit the -verbose flag to skydns in here? I wonder if the output would become large if solo was running for a while and sending lots of DNS requests (I'm not sure how docker logs stores things). \nFor the unpredictable nature of building SkyDNS, is it available as a package to install with apt-get?\n. :+1: overall\n. The tests for this are pretty hacky. A much cleaner approach would be to add some sort of \"FileReader\" interface to TemporaryJobBuilder that hides the logic of reading from the classpath or from the target/ folder so I could swap that out in the test with a temporary file, but that felt like a more intrusive change.\n. @davidxia @negz @gimaker @rohansingh @dflemstr \n. Found a test which was relying on imageFromBuild() produced by some other module in the project; the test doesn't actually care what image is used - seems like the test only passed through an accident before. Fixed in 7518d82\n. At a high level this makes a lot of sense, need to dig further into the diff though.\nWould there be a need to migrate data already in zookeeper if/when deploying this change to a new format? \nWill there be a problem in deploying this change if some masters with the old implementation are running while new ones are? Similarly, does it require all agents to be upgraded at the same moment in time?\nI think the part of this where you ensured that the REST API is unchanged is nice, I wouldn't call that a hack at all.\nThe idea of having a history of rolling operations sounds really nice, but storing the \"history\" of anything in zookeeper generally becomes problematic for a number of reasons: the data set size is limited to what will fit in RAM, the znode data model (something resembling a filesystem) is not a great fit for most queries, etc. I think we would be better off storing long-term history in another, non-zookeeper, system (like a traditional RDBMS) \n. > A migration of ZK data will be tricky. Will older helios CLIs be able to handle the new types of data? We've had issues with data/schema changes causing old CLIs to break.\n@davidxia The CLI doesn't talk to Zookeeper, does it?\n(it drives me nuts that Github doesn't have inline responses for top-level comments)\n. @negz do you think it would be feasible to outline somewhere (here in a comment, or in a doc added to the repo) what the new ZK schema looks like for what you are adding? I think that could help in reviewing.\n. as long as the JSON returned by the endpoint in the new version matches the JSON that was returned by the old version (i.e. what an old CLI client expects), then it should be fine.\n. 88 comments \ud83d\ude32 \n. tangential question but why is the type of BiPredicate in Operator declared as <String, Object>, for example\njava\nOperator(final String operatorName, final BiPredicate<String, Object> predicate)\nwhen none of the implementations match a non-String value on the right-hand side? i.e. why not BiPredicate<String, String>?\n. \ud83d\udc4d \n. @davidxia @gimaker @negz \n. cc @mzeo\n. unfortunately there is no :embarrassed: emoji on github\n. @gimaker @davidxia @negz \n. \ud83d\udc4d  but where was it broken before? I can't tell what the fix is here\n. I swear I've tested this logic this time. \n@spotify/helios-team \n. \ud83d\udc4d \n. codecov complains because the diff doesn't touch testable files\n```\nNothing to diff\nThis commit did not change files that are tracked by Codecov.\n```\n. Thanks! I think this makes a lot of sense to have in general. I also think that #910 will fix the multiple-helios-solo-collision issue.\nIf you are interested in adding tests for this new logic, I think com.spotify.helios.system.ReapingTest would be the right place \ud83d\ude09 \nTo set this for helios-solo, I think you would just want to modify solo/docker/start.sh to pass the argument to AgentMain.\n. Thanks, great idea. But I'm afraid that this logic in RetryingRequestDispatcher will make this irrelevant - IIRC the underlying exception is only logged when the retry loop has finished.\nPerhaps that class should log the exception message on each try, combined with the addition of ipUri in this commit\n. personally I think it would make more sense if the error message from the RetryingRequestDispatcher was more informative than\nFailed to connect, retrying in 5 seconds.\nFailed to connect, retrying in 5 seconds.\ni.e. if it included the error message from your change as well. So I'd be in favor of keeping this and modifying the other class too, but I am curious what the rest of the team thinks as well.\n. nice catch\n. Thanks for tracing this down to the problematic code path. It seems like a bunch of tools make assumptions that docker-for-mac breaks.\nI'm curious - does docker-for-mac set DOCKER_HOST for you by default, or tell you to run some sort of command similar to docker-machine env .. to set the proper variables? Or is it leaving it up to the user to figure out they have to set DOCKER_HOST to something?\n. @daenney ah that is what I figured, thanks. Hopefully my beta invite comes through soon so I can test this out locally as well \ud83d\ude04 \n. @daenney just for completeness can you paste how your TemporaryJobs setup code looks like?\n. @danielnorberg i also got my key just 2 days after signing up, there doesn't appear to be an actual wait\n. #920 fixes a part of this, where TemporaryJobs.Builder tries to use the DOCKER_HOST value as a http:// URI.\nThe next issue I am seeing is that the container that is launched for my TemporaryJobs locally (with Docker for Mac) has an IP address like 172.17.0.1, which TemporaryJob uses when probing the container to see if the service in it is up or not. For some reason this is not routable from the test running on the host. This might have more to do with Docker for Mac than anything in Helios though, I need to look further.\n. Another incompatibility: exec healthchecks are rejected if you run helios-solo locally on Docker for Mac because these checks fail - docker.info().executionDriver() is \"\" for some reason.\n. I created a topic on the Docker for Mac forums about the IP address issues I am having now: https://forums.docker.com/t/ip-address-given-to-container-is-not-routable-in-bridge-mode/12725. This might be the last issue with helios-testing and Docker for Mac.\n. So, helios assumes that the IP address of the container (i.e. docker inspect -f {{.NetworkSettings.IPAddress}} container) is the address to use to reach the exposed ports of the service in that container. When TemporaryJob deploys a job with a port mapping, it probes the port at container_ip:port to check if the service is running before continuing with the test.\nDocker for Mac gives a container an IP on a bridge, like 172.17.0.x, and then seems to map the external port of the container on the host (OS X). So if you expose port 80 on a container and it has IP address 172.17.0.2, you would connect to localhost:80 from the OS X host, not 172.17.0.2:80 (which you would use if using regular docker or docker-machine).\nRouting to the container's IP seems to be not yet be supported in Docker for Mac.\n. #927 addresses some of my last comment. TemporaryJobs tries to figure out what address to use to communicate with any ports mapped from the container to the host, and ends up using a bad value for Docker for Mac. \nI've found another new issue though - it seems like the port mapped by Docker for Mac accepts connections as soon as the container is started (i.e. if port 8080 on the container is mapped to localhost:31234, attempts to connect to localhost:31234 are accepted immediately), even before the process inside the container is listening to the port. This breaks assumptions by DefaultProber, which is responsible for determining when the service inside the container is \"up\" and when the actual test can resume. \nIf the TemporaryJob has a healthcheck, then there isn't an issue, but in cases where the \"probing\" of the port is assumed to be enough, then the test will end up running as soon as the container is started, which might be well before the container is actually accepting connections on the internal port.\n. Regarding the watchdog - it feels weird to launch a thread that connects a socket and then blocks forever (or attempts to do so via sleeping in a loop that never finishes). Also, the class that launches this thread doesn't wait for the connection to be established - should it? \nYou could avoid the need to have a thread block forever (which I assume is to keep the socket that it holds from being GCed) by moving the socket/connection up to the HeliosSoloDeployment class level, and have the class establish the connection before moving on with it's other work - i.e. remove the executorService and daemon thread. Plus then you can cleanly close the connection / watchdog when HSD is closed.\n. Would this change completely remove support for the -Dhelios.testing.profile switch? So any user who was using profiles to be able to specify a helios cluster to deploy their TemporaryJobs would transparently lose that functionality with this change?\n. @davidxia can you update docs/testing_framework.md too then? (ignoring the question of if it's a good idea to break it like this since we have a lot of internal code using the profile args)\n. @spotify/helios-team \n. yeah, it is kind of silly to have all this default logic and construction of things in the constructor of Builder rather than in it's build() method.\n. @spotify/helios-team @mbruggmann \n. This is related to #916 but also I was surprised to see that an exec healthcheck didn't work when deploying things to a local helios-solo with docker-machine / vmware.\n. is this ready for review? I am confused by the \"WIP\" titles and multiple PRs\n. @davidxia great, will take a look today. Can we merge and release #927 first though, so that users can upgrade to it and not have to deal with any incompatibilities before having to deal with the incompatibilities in this change?\n. Is it no longer possible to control REGISTRAR_HOST_FORMAT via the typesafe config file? If not, that will break a large number of internal user's tests. \nIf that functionality is unchanged then the TODOs in HeliosSoloIT seems incorrect\n. docs/integration_tests_with_helios_solo.md and probably docs/testing_framework.md should be updated as they are outdated now\n. I like the idea of including a documentation update in the PR that makes the change itself, so that it is much harder to forget to update the docs.\n. I love diff stats like +53 \u2212489 but what was the reason for this change?\n. @spotify/helios-team \nthis could be improved - as mentioned above, the conditions in determineHeliosHost() don't feel exhaustive to me. It feels like this check should be generalized, but I am not sure how to express the general condition.\n. Tests were failing because some paths in one of the tests ends up calling DockerHost.fromEnv() and then the class being tested branches based upon those values, meaning the behavior in CircleCI is different than local. Fixed in ec2adf5.\n. please review as this is the last hurdle I know of preventing helios-testing from working with Docker for Mac @gimaker @rohansingh @davidxia \n. ugh had to rebase, @davidxia @negz please take another look\n. lgtm if you fix the import order\n. \ud83d\udc4d for the intent though\n. \ud83d\udc4d \ud83d\ude4c \n. \ud83d\udc4d \n. @negz @gimaker @davidxia \n. LGTM, I think this approach will definitely work and solve the problem.\nBut would it be simpler conceptually if step 3 in the outline above simply didn't cause the group/state to be marked as FAILED?\ni.e., when doing a rollout because hosts changed, don't change the DeploymentGroupStatus.state to FAILED if the reason is HOSTS_CHANGED\n. > I also think we should not trigger rolling-updates when hosts change if there's already a manually triggered rolling-update in progress (since it, depending on the exact implementation, leads to really subtle behaviour).\n+1, in a separate PR\nAlthough it would raise the question of how and when those new hosts would get the job version being rolled out, assuming that the initial batch of operations would include the list of hosts prior to the new hosts addition and therefore ignore the new host (not sure if this is accurate?). \n. > That said, I suppose it depends whether you frame it as \"the deployment group is failed\" or \"the rolling update you performed on the deployment group failed\". \nYep I think this accurately captures the different perspectives. In my mind I see it as the former since we have a \"inspect deployment group\" command which reports the state (which can be failed). But I agree the latter phrasing is the ideal way to look at things and for it to eventually be modeled as. \n. This is as expected, because a container running a while/sleep loop like 'while true; do date; sleep 60; done' doesn't respond to the SIGTERM that docker stop sends (which is what helios sends via the Docker Remote API when you undeploy a job). Helios tells Docker to wait 120 seconds after sending the SIGTERM to kill an unresponsive container with SIGKILL.\nIf you have a more well-behaved container that exits immediately, then the job exits immediately and the history reflects that:\n$ ./helios-solo create nginx:2 nginx:1.9.0\n...\n$ ./helios-solo deploy nginx:2 solo.local\n...\n$ ./helios-solo undeploy nginx:2 solo.local; sleep 2; ./helios-solo history nginx:2\nUndeploying nginx:2:a3f56da08826edc44068e3a6f88cd7af23003ba4 from [solo.local]\nsolo.local.: done\nHOST           TIMESTAMP                  STATE            THROTTLED    CONTAINERID\nsolo.local.    2016-05-31 16:51:09.821    PULLING_IMAGE    NO           <none>\nsolo.local.    2016-05-31 16:51:11.039    CREATING         NO           <none>\nsolo.local.    2016-05-31 16:51:11.191    STARTING         NO           c176ce1e2cbdd52c275a04890da5fb6c9f6dd7d13caecf02a30b0b09dced3029\nsolo.local.    2016-05-31 16:51:11.468    RUNNING          NO           c176ce1e2cbdd52c275a04890da5fb6c9f6dd7d13caecf02a30b0b09dced3029\nsolo.local.    2016-05-31 16:51:52.879    STOPPED          NO           c176ce1e2cbdd52c275a04890da5fb6c9f6dd7d13caecf02a30b0b09dced3029\nOne unexpected thing I found while looking at this is that the STOPPING state is only stored in the history when the job has a gracePeriod configured. I think it would make more sense to write the STOPPING state unconditionally before telling docker to stop the container, since the stop itself might not be instantaneous. I'll create a new issue for this.\n. @spotify/helios-team \nprobably easier to review by ignoring whitespace with https://github.com/spotify/helios/pull/934/files?w=1\n. @negz I think there is still a lot of value in having the standalone helios-solo homebrew and Debian packages. They can be used outside of the Maven-build-and-test context for things like running jobs locally during development or to do one-off tests of how helios or your jobs would behave or otherwise test things without involving a real Helios cluster. IMHO the packages or the scripts around starting/creating/stopping/using the helios-solo-container should not go anywhere. I don't think it was ever the intent of HSD to obsolete these packages and functionality completely.\n. whew tests are \u2705  now, after I accidentally introduced an off-by-one bug in my use of an iterator.\n. @spotify/helios-team \n. @gimaker @davidxia FYI I also edited the doc file to add word wrapping and otherwise make future diffs easier to read\n. \ud83d\udc4d\ud83c\udffb but might as well make it possible to pass in an environment variable to define the whitelist so this doesn't have to be tweaked more than once in the docker image \n. \ud83d\udc4d  for the new commits - impressive bash array usage \n. \ud83d\udc4d \n. +1\n. @spotify/helios-team @lndbrg \n. good catch! fixed in 024eb58\n. The difference with argToStringMap is that it is used for Arguments that have nargs(\"+\") like --labels in the host command.\nI don't totally get the nuances of the nargs part in argparse4j but I modeled this one after the --env command, meaning the input is expected to be like\nhelios create -m a=1 -m b=2 -m c=3 job:version image args...\nwith nargs(\"+\") here it would have just one -m like\nhelios create -m a=1 b=2 c=3 job:version image args...\nI'm not sure if that would be a problem with all of the other arguments in JobCreateCommand though - none of the other args in JobCreateCommand have nargs(\"+\") (even though they often have multiple values like --env and -p or --volume) except for args at the end which has nargs(\"*\")\n. I think the set prefix in the field name is confusing (or accidental?)\n. I think it would help your test to have the Clock as a constructor arg\n. good catch, updated\n. very good catch, updated\n. allow me to preemptively apologize for this\n. if Job is meant to be immutable, then this method breaks that, as Date can be altered\n. just curious - is it not possible to have the recursive delete added to the transaction above? or just not desired in this case?\n. yes very true\n. should comment/document why this is here too\n. this seems copied verbatim from com.spotify.helios.serviceregistration.ServiceRegistrarLoader, would be better to not have to duplicate this service loading code\n. would be helpful to have a one-line summary javadoc here\n. feels like createFactory(Path) and createFactory() could be one method\n. NopAuthClient and NopAuthHeader could probably be package-private instead of public if they are not intended to be used outside of the \"factory\" that returns them\n. How about something like --requires-auth=STRING where STRING can be a client version number to require auth after or just the string yes for all versions\n```\n--requires-auth=0.8.526\nor\n--requires-auth=yes\n``\n. yep that is exactly what I was referring to here\n. since the MetricRegistry maintains a ConcurrentHashMap for all of the registered metrics, and examines it when you callregister.meter(name)`, feels like you could do away with the same map-keeping here, as it is pretty unlikely to try to request a metric with this name that isn't a meter\n. I think it should be possible to refactor this so there doesn't need to be code duplicated and still make it possible for the \"plugin implementation\" to not have to refer to too much beyond importing the interface that it is implementing.\nAnother thing to keep in mind is that for authentication we might want to allow loading of more than one \"provider\" from ServiceLoader (rather than the first one it finds) to support different auth schemes etc.\nI'll take a stab at this refactoring.\n. see AccessTokenFilter for why this is needed\n. see AccessTokenFilter for why this is needed\n. regarding authenticator.getHttpAuthHeaderKey(), feels like it would be semantic for things like access tokens to always be sent in an Authorization header rather than customizing the header name for each implementation. \nIs there a need for crtauth to use a non-standard header?\nusually the format of the Authorization header is like Authorization: <scheme> <token> like Authorization: crtauth abcdef1234\n. either is fine; maybe squeezing the two things (enable or a version string) into one arg is too much\n. I'm not sure if 2617 is relevant here, but this is how the protocol for crtauth is documented as far as I know: https://github.com/spotify/crtauth/blob/master/PROTOCOL.md#overview\n. guava's Multimap could be a good fit here\n. this method is never called in this class\n. If we are always dealing with character-based payloads (and maybe we aren't) then logging the decoded version of the payload in a toString() would probably be more useful than the raw byte array\n. e.toString() would return the same thing\n. I think I caught all these in #698 but let me know if you find more\n. yea, let's refactor that when we have a better idea of what loading the client-side part will look like\n. thanks, fixed all of these in 0fa40a2\n. it comes from B64Code.decode(string, encoding) if the string is not actually base64 encoded (for example if it contains chars outside of the base64 range)\n. Yeah I struggled with how much of an \"example\" to make this versus being practical. \nTo be useful, BasicServiceAuthentication needs some way to verify the supplied username/password, but if we wanted to do that by requiring the user to inject an Authenticator that does the verification, then it we don't have an easy way to do that given that the plugin loading infrastructure requires the class to have a no-arg constructor. \nWe have the same problem in crtauth where the plugin is explicitly wiring the CrtAuthServer with the LDAP version of KeyProvider, even though any KeyProvider would work.\n. nope, thanks for catching\n. I don't think it is useful enough to mention here in the argparse help message, but I will add a note to authentication.md that it can be used (but really shouldn't, since it is so insecure) and which environment variables need to be set.\n. That sort of conditional logic - which classes to instantiate based on some config - is what I really wasn't sure how to do here. It's almost as if we would want to include some sort of Guice or Spring module for wiring up an user-supplied object graph of these things; I'm not sure if there demand is really there to match the complexity that would add.\n. I think that would make sense; I initially thought it would be simplest for plugin-implementors to implement both at the same time / in the same \"package\" (and that the class loading code could be reused).\n. I suggest renaming this and the method below currentAuthorizationToken to make it clear it returns the token \n. we don't have support on the server-side either and I can't see why it would be useful so I don't think this needs to be worried about\n. should break request.toBuilder().header(HttpHeaders.AUTHORIZATION, authHeader).build(); into it's own method as you call it twice and the long chain is a little hard to read\n. I think that if the \"new\" token is the same as the previous one, we should return an error / not make a new request, assuming that the server will continue to think that value is not valid. \n. does f actually fail if the server returns 401 Unauthorized? It looks to me like DefaultRequestDispatcher doesn't throw an exception on 4xx or 5xx\n. should probably copy the headers if you are aiming for immutability \n. I think these 3 lines could just be return ImmutableList.copyOf(loader.iterator())\n. Good point. currentAuthorizationHeader sounds good to me\n. Right, but my question was just if the future ever actually has an exception associated it versus returning a Future with a \"failed\" statuscode.\n. Yeah I think that is a very good idea.\n. AuthenticatingRequestDispatcher can check the protocol of the URI it is requesting prior to using the AuthProvider to get a token (either stored or new).\n. my understanding of the initial interface was that this method was for returning the token-stored-on-disk, if any\nbut in general, I don't think you need locking here.\n. should check status code, I believe the reference python implementation does (and the server will indeed send 200 OK as long as everything is OK)\n. should also check that the part of the header value before : has the expected value\n. minor but since every path through this AsyncFunction returns an immediate future, this can just be a Function (and you don't have to wrap the return values in Futures)\n. same comment as above; for completeness we should check the prefix of the header value\n. Command => colon?\n. can we call it AUTH_CRT_SERVER_NAME for total clarity?\n. I would suggest having the constructor take the CrtAuthClient as a parameter (and wire it up in the authProviderFactory() method below); this way you can unit test this class with a mock and not have to do real ssh key stuff in the unit test.\n. predicates :metal: \n. I tried to stage this into separate commits but in reality i wrote all the code at once; probably introduced it then\n. I think you could just have a normal field of the class like private String token and assign it inside the anonymous Function. \nThere would be some issues with thread-visibility (should it be volatile etc) but since this isn't code that is called very frequently I don't think it would matter ... but I also think the AtomicReference is fine too.\n. :+1: \n. minor: is it possible to have this try-catch wrap just whatever line is potentially throwing it?\n. are you sure? it compiles for me\ntechnically it should be possible to have an inner class (not a nested one) do this, as inner classes have an implicit reference to the parent instance\n. but I think the AtomicReference is fine too\n. since the interface has throws IOException should probably package any exception as that type\n. :+1: \n. super minor: if you just catch (DockerRequestException e) you can skip half this block\n. You can only iterate over the items in an Iterator once. Does this mean a new instance of DefaultRequestDispatcher has to be made for each request?\n. Ah I see that the EndpointIterator will cycle over the elements. In that case I'd declare this field as EndpointIterator to make that behavior clear, as this class depends on that behavior as opposed to the Iterator behavior. \n(It'd be even nicer if there was some interface to use in the declaration like CyclingIterator)\n. if we only ever call Supplier.get() once, I think you could just simplify and have the constructor parameter be the actual list / iterable\n. I think this could just be a regular Queue / LinkedList, we only insert at the end and remove from the head\n. :flushed: \n. constructor should copy the list since it stores the size here, so that any modifications to the passed-in list don't have weird side effects\n. method is unused\n. if we don't actually care what format the hostname has (which this error message kind of suggests we do), I'd make this message be Endpoints must be of the form %s://<host>:<port>\n. is retryCondition always Predicates.alwaysTrue()?\n. the name here is confusing, because it seems to do the opposite:\nif retry:\n   resolve future with the result\nelse:\n   call handleFailure and potentially retry\n. I think that declaring this field as EndpointIterator would be best, since it communicates that the code only works with an instance of EndpointIterator rather than any Iterator<Endpoint>.\n. fairly minor, but just in case you weren't aware for enums you can also use == for comparisons\n. instead of concatenating two strings this could also just be String.format(\"timed out waiting for ... (previous job states: %s)\", previousJobStatesString)\n. this line and the comment above this line seem to contradict\n. fair point, seems like it should\n. +1\n. any particular reason to use this style instead of when(mockCreation.id()).thenReturn(\"potato\") ?\n. Yeah the mockito runner won't know about @Mock variables inside a method (if the annotation can even be used there)\nYet Another Opinion on Style but I like just declaring the fields (when they need to be fields) using the expression @gimaker mentioned, not super crazy about @Mock as I feel like it saves you just a few keystrokes\n. Not really, just wondering if you knew something I didn't know :smile: There are some cases where doThrow() can express things that the reverse style can't; but it's not a huge deal.\n. bug 1: this used to be Queues.newArrayDeque(null) which throws a NullPointerException as Guava tries to add all of a null Iterable to the queue\n. bug found as a result of now having a test here: this was !identities.isEmpty() in b02b0a5, but since a copy of the list held as a field is made in Deque<Identity> the original collection is always non empty. A NPE would be thrown on the next log line when accessing identity.getComment() as identity was null when number of identities < number of endpoints.\n. since the client is static (and why is that?) this adds a new mock expectations for each @Test. \nIf the mock is static then this should probably be @BeforeClass or more simply they should both be non-static\n. 443 but the json file has 456?\n. if the issue was that this path didn't exist (not clear to me if that was the case), why not wrap it in an if (client.exists(path)) block like the other deletes?\n. I think docker-client would be a nice destination\n. probably sounds like a nit, but I think even a one-sentence javadoc summary of the class would be useful\n. same here - e.g. I assume this is meant to provide a HeliosClient that points at the \"deployment\"\n. another nit but prefer to declare vars as close to their point of use as possible, so in this case next to the assignment\n. This is a nice fix and makes sense to me. But I'm missing why you had to pass the ZookeeperClient to this class to accomplish that?\n. :+1: for commenting\n. Yes, thanks\n. allows the CLI options to be passed down to this method for use in constructing the HeliosClient\nkind of ugly but I didn't want to add hostnameVerificationEnabled as a field to the existing Target parameter as it didn't really seem to fit there.\n. Just reluctance to add an 8th parameter to that method, especially if it was only passed to the Command to be passed to Utils.getClient(). \n. I do agree though the Commands could be refactored to have easier access to all these params w/o passing them in every single run() though\n. can this and the digestCalculator be created just once as a field of the class?\n. BaseEncoding.base16().upperCase().withSeparator(\":\", 2) could be a field too\n. are these standard names for these files? You could also just set one env var for each path.\n. u fancy\n. should be private final\n. super nit: this line is indented different than one below it\n. throw RuntimeException(e) or throw Throwables.propogate(e) would be more idiomatic\n. good catch! I liked \"initiated\"\n. does anyone call the process tls-agent instead of ssh-agent?\n. Only that I wanted to have instances of ClientCertificatePath always represent known-to-exist file paths and I hate having constructors that throw exceptions.\n. But if the transaction failed, an error message will be logged right next to this one, right? Or is it the word \"starting\" that is an issue? In the end all I really wanted to do was log the operations about to be undertaken.\n. should add a help(..) here too\n. why perform the calculation for the \"agent digest\" here, but have the master digest passed in as an argument? Why not simply pass both?\n. this block is repeated in AgentService and MasterService, just with a reversal of the labels, maybe it can be moved into the CuratorClientFactory?\n. typo: \"needs to ~~be~~ create\" \n. maybe call this something like PATH_COMPONENT_WILDCARD since it's not a true wildcard like .*\n. Yeah. A comment mentioning that it is meant to match a single path segment would probably suffice.\n. hm I guess it wasn't clear to me at first read that the agent password is used in both the digest and authentication, and same for the master. I bet that would be cleared up with a nice new docs/security.md file though :wink: \n. Just style. I've seen for (..) { return ..} far less than the other. It can be a little confusing to see a for loop that iterates exactly once.\nYou can also do return entry.getValue().iterator().next() ... which is what the for (..) { return } variant basically does anyway.\n. should bump this to whatever the next release is after the few in-progress improvements/PRs are merged\n. good catch, thanks\n. grammar nit: should be a comma in ... with the data that Helios stores in Zookeeper, Helios...\n. should be \"one set for the masters\"\n. \"meaing\" => \"meaning\"\n. Is this something that a Helios cluster admin could configure separately? For example, configure the zookeeper connection string to use SSL or whatever they call it. Are there code changes necessary in Helios to support encrypted connections to Zookeeper?\n. add a link here to bin/helios-initialize-acl ?\n. can we call this (and the shell script) Migrator instead of Initializer?\n. would be good to include what those different values are in the log message\n. id should be final too\n. because then you wouldn't need to document what the class / script does, as migrator is pretty self explanatory, and I was initially confused what being initialized meant :smile: \n. be a good citizen and call System.exit too :wink: \n. shouldn't you close the client when it is done?\ntry (ZooKeeperClient client = new DefaultZooKeeperClient(curator, zooKeeperClusterId)) {\n client.start();\n client.initializeAclRecursive(\"/\", aclProvider);\n}\n. the \"single\" part of this comment is no longer true\n. \"If no authentication is provided\"\n. how about Optional<ACLProvider> ?\n. curious: why do you need to use .split(Pattern.quote(\"/\")) instead of .split('/')?\n. is start() really called more than once? \nIf it is - shouldn't the lines below for adding connectionStateListener etc also be in this block?\n. elsewhere in this diff two List<ACL>s are compared by converting both to sets and calling .equals(), should do the same here for consistency\n. pretty minor but you can also use assertThat(aclProvider.getAclForPath(\"/foo/baz\"), contains(new ACL(DELETE, id1))) might be less verbose\n. in case it's not clear (and this always confuses me) but hamcrest's contains(..) method tests not just that the item is in the collection but that the item contains the arguments in that order when iterated over\n. same comment - can also be\nassertThat(aclProvider.getAclForPath(\"/foo/bar\"), containsInAnyOrder(...))\n. Cool, so I think it would be good to link to that page here and change the doc to say that when not using SSL the credentials are sent in plaintext, and if you want to use SSL read this link\n. why remove this?\n. Ha, I guess it is subjective because I found migrate more clear than initialize. In either case I think it is now clear to all involved :+1: \n. the single-quote there was a mistake - my question was why (or is it?) is it necessary to quote a /?\n. Gotcha. thanks\n. I just meant since Optional is used a lot elsewhere in the codebase, but since this is totally an internal interface/class then sounds fine\n. I think this should be set in the build() method instead so that an executor is not created/started here and then replaced for people calling setExecutorService\n. this signature could probably just be ScheduledExecutorService and we wrap it in listeningDecorator(), I'm not sure that callers would care or have a LSES beforehand\n. I don't think this would need to be static if you just made the Clock a constructor parameter, which makes a lot more sense to me personally\n. or VisibleForTesting either\n. I think it would be helpful to log the amount of downtime here too\n. I think these should all be constructor parameters too (and maybe CLI args)\n. do you need to start this service somewhere?\n. looks like a good fit for a parameterized test :smile: \n. the start time (5000) is after the current time (2000)?\n. so that you can change the schedule without rebuilding the code\n. and/or disable the \"reaping\" altogether if the interval is 0 or some value\n. I see. Is that normal for InterruptingScheduledService and friends?\n. I think that might really surprise someone if we were to ever actually reach uptime of 3 years :smirk: \nI don't expect to have to change it often but if there ever is it is nicer IMHO to have the option to change configuration rather than re-releasing Helios (and have the default value be the 30 min one here). \nI think the clutter of adding new CLI args has more to do with how we've implemented it with the Config, Parser, Server etc. I am curious if we would get more mileage out of just reading some sort of yaml/json file at startup and not having to have any new switch/flag be added in 3-4 places.\n. it takes the fields annotated with @Mock and sets their value to Mockito.mock(TheClass.class) at setup-time. I replaced it here because it felt like less code to just create the one mock myself in line 70 (and moreso because then I couldn't initialize the DefaultDeployer in a single line and I didn't want to add a @Before method :grinning: )\n. nitpick: for for\n. We should note where this username comes from. Whether it is System.getProperty(\"user.name\") or some environment variable or CLI switch.\n. I think we should mention that this description of what the server does with the certificate is completely specific to the helios-master installation, i.e. however the admins have set it up themselves.\n. yea, just copy and paste the second sentence of your comment above :smile: \n. :+1: \n. How come this is needed? The same config is in <pluginManagement> of the parent pom\n. looks like this duplicates what is in line 15\n. I don't think this would work as com.spotify.helios.master.MasterService and com.spotify.helios.agent.AgentService are in separate packages.\n. Oops I thought you were referring to the class itself. Yeah, great point.\n. logging e.getMessage() might be useful too\n. strictly speaking the \"boolean\" part of these comments are outdated now\n. this looks identical to waitForMasterToBeFullUp(), can you extract the callable so that both functions can use the same code?\n. I don't think these casts to (ZooKeeperCallable<Void>) are necessary; or at least they seem to be in some spots in the diff and not others.\n. should be \"getState\" here\n. \"getAcl\"\n. Thanks. I remembered about this after making the commit; although I like the more verbose style here though to be extra clear.\n. please tell me if this and the validateArgument function below is way too much functional-style\n. \n. Optional<OldJobReaper> might be nicer than a nullable reference.\nCould then be \njava\noldJobReaper.ifPresent(reaper -> reaper.startAsync().awaitRunning());\nbelow (although maybe this is more verbose?)\n. I think this is the same as Collection.sort(copy) since Long implements Comparable\n. This seems to be uncalled\n. is this needed? you've already stubbed the behavior \n. should also verify that the method is not called when !expectReap\n. how about just if (!events.isEmpty()) ? more readable, less verbose\n. MasterService does not pass a latch to ZooKeeperRegistrarService\n. just inconsistencies from adding it in some places versus modifying existing code. I'll update\n. SimpleDateFormat is not thread-safe. In this case I don't think it matters since only one thread calls runOneIteration, but I think it's best to avoid using/declaring it in a way that could later become problematic if more than one thread was to be used here.\n. because URLConnection.setConnectTimeout and setReadTimeout only take ints\n. i just felt like it was more \"fluent\", :smile: \n. 400 seconds??\n. I think it would be nice if this was customizable in any case, for instance when constructing the HeliosSoloDeployment instance (there are other builder args already)\n. could also be Futures.immediateFuture(...)\n. not really important but a small tip: I've taken to writing checks like this using assertThat instead, like\njava\nassertThat(portRange, hasEntry(firstTaskStatus2.getPorts().get(\"foo\").getExternalPort()));\nbecause when it fails, junit will give you an error message like \"Expected {4711, 4722} to have entry 1234\" (or whatever), whereas assertTrue(something that is false) just reports an empty and unhelpful AssertionError\n. or i guess if the thing you are really testing is that firstTaskStatus2.getPorts().get(\"foo\").getExternalPort() is in some expected range it would be nicer to reverse it like\njava\nassertThat(blah.getExternalPort(), isIn(portRange))\nwhich reads nicer when it fails (something like \"expected N to be one of {1, 2, 3, 4}\")\n. +1 for treating src/main and src/test the same\n. pretty minor, and this pattern exists already in Helios, but even the guava Optional docs recommend using something like Objects.firstNonNull(a, b) for this use case: https://github.com/google/guava/wiki/UsingAndAvoidingNullExplained#convenience-methods\n. I interpret it more as don't bother creating an Optional just to use or(..) when firstNonNull(a, b) is clearer\n. forgive the nitpick, but how about \"addCapabilities\" / \"dropCapabilities\" in the job json so it's not using past tense?\n. I think this can also be final Set<String> disallowedCaps = Sets.difference(caps, whitelistedCapabilities)\n. how about The following Linux capabilities are not allowed:\n. use of \"this agent\" this might be copy and pasted from previous commit?\n. to be extra careful with the set math, might want to test which capability was deemed to be not allowed\n. checking that these aren't empty before calling the setter seems like overkill\n. if the setAddCapabilities method took a Collection instead of a Set, you could just pass the list, and rely on ImmutableSet.copyOf(collection) in the method to do the type conversion\n. if you really want to make this optional (and I don't think it needs to be), it should at least default to true, so that it isn't accidentally left in the off position\n. no, this section of the pom is only for telling maven where to find artifacts. <dependencyManagement> covers where to upload them.\n. I'm not sure if -verbose should be put in the image. Seems like you could run the docker image with -e SKYDNS_OPTS=-verbose at runtime if you really wanted verbose logging.\n. removed this or() as the first part of the condition is always false in this branch\n. This looks to be the same logic except the boolean values are reversed. How about making re-using the IN definition to avoid duplication?\njava\nNOT_IN(\"notin\", new BiPredicate<String, Object>() {\n  @Override\n  public boolean test(final String a, final Object b) {\n    return !IN.predicate.test(a, b);\n  }\n}\n. intentional; why was this line ever here? No code in helios integration tests has this package name.\nMy guess is it was copy & pasted from somewhere else.\n. I don't have a strong preference, just trying to fix a broken part of the current implementation\n. this field is only used within thegetOrCreateHeliosSoloDeployment() method; I think it would be cleaner if the reference to the static HeliosSoloDeployment was moved to TemporaryJobs rather than the HeliosSoloDeployment class itself.\nIn other words - make the class that wants to use HSD and doesn't care if it gets an old instance or creates a new instance be the thing that holds the reference.\n. changing the access modifier will force any users of the previous style of HeliosSoloDeployment/TemporaryJobs fail to compile - is that intentional? \n. usual calling style here is throw Throwables.propogate ...\n. Assuming that the purpose of extending Callable is to use it with Polling.awaitUnchecked(..), I think it would be clearer instead to have something like\n``` java\ninterface SoloHostProber {\n    boolean check(HeliosClient client, HostAndPort hap);\n}\n// in the place where it is used\nPolling.awaitUnchecked(time, new Callable() { return soloHostProber.check(client, hap); });\n```\nthen you don't have to remember to properly call some methods in-order before others. It would also reduce boilerplate in the implementations of these interfaces.\n. might make sense to break this anonymous class into it's own class to make it easier to test on it's own\n. why would the line not be HELO? would this be a case where you want to fail totally? Maybe at least log something.\n. I could be wrong but I don't think echo will expand \\n\nbash\n$ echo \"hello\\n\\n\\nworld\"\nhello\\n\\n\\nworld\n. This would make it very simple to add TestNG support too, and simplify the case where people want to use TemporaryJobs in something that isn't a test. \n. if you pass a throwable as the last argument to a log method, logback (or slf4j, not sure which) interprets it as you wanting to log the stacktrace, so this will look like\nWARN HeliosClient did not close cleanly: {}\njava.util.IOException: blah\n  at foo.bar.blah():123\n  at ...\nif you just want to log the message and not the stacktrace do log.warn(\"HeliosClient did not close cleanly: {}\", e.toString())\n. file.delete() will return false if file cannot be deleted, maybe handle that?\n. why extend AutoCloseable instead of Closeable?\n. if the Set in uris() had more than one elemnent, Iterables.getOnlyElement() will throw an exception. Is it really necessary to fail if there is > 1 URI?\n. and if all implementations of uri() today only return a collection with one element, then why model it as a collection at all?\n. comment above this line is outdated now\n. I vote: no, it's not\n. typo: guarantee \n. why java.lang.AssertionError? It is strange for any user-level code to throw/return/construct instances of Error rather than Exception.\n. minor but I find that assertThat(errors, hasSize(2)) outputs more helpful info when it fails than this form, as it prints the whole collection and it's elements\n. you can extend org.hamcrest.CustomTypeSafeMatcher instead and it takes care of these instanceof checks for you\n. the first point is true with either interface as Closeable extends AutoCloseable\n. should make sure these are sorted, simple search and replace won't work\n. a better error message for both the log line and exception would be something like \"container <containerId> was not found during health checking, or has no State object\" - as-is the ID of the container is not included and it's not totally clear what it means for the state to be null\n. Ah good point. I interpreted the fact that the return value from the method being ignored in one of these branches as meaning that the entire method call wasn't needed. I'll update this.\n. tackled in 393ee89\n. I thought this was a java.io.File instance, nevermind\n. potentially not but it is the best identifier I found for now. As I mentioned in the PR summary, I feel like there should be a broader condition to test for that I am not able to see the answer to (\"what IP to use to reach a mapped port?\") but I wanted to at least improve things for these users.\n. this looks like duplication from what is under the 5) branch\n1. if it is really necessary to duplicate, can these commands be moved to a function to minimize duplication?\n2. would it be simpler to move the tests that need this image to be executed by the 5) node?\n. random question: why the mix of private and package-private access levels? It seems unnecessary to me to default everything to private and then carve out exceptions for the ones that happen to be used by a test - because if you can relax the access level as soon as it's needed, then what is the point of ever having it be private \ud83d\ude04  I would just make them all the same minimum level (package private in this case)\n. if the connection is already made at this point, the log line should really be \"Connected to helios-solo watchdog at ...\"\n. declaring that the build() method returns a superclass isn't really useful at all since the class is HeliosSoloDeployment.Builder (there is no illusion about what types this Builder builds), and it's not like the Builder itself has any parent classes where someone might refer to it as HeliosDeployment.Builder etc. \nWorst case this type of change forces code using this to have to cast the build() result to (HeliosSoloDeployment) which is just unnecessary. \n(also I am a little sour since this undoes something I just did in https://github.com/spotify/helios/commit/144600b120bd095dd2882f0dd63a5a065949056e )\n. I see that this signature is somewhat forced on this class by Polling.awaitUnchecked, but a method that can only ever return true or null feels pretty weird\n. why catch and log the exception here when it is just rethrown, rather than handling it once at the call-site?\n. why does this class have both a static and member reference to a HeliosDeployment instance? that's confusing\n. ah nevermind, figured it out - but maybe some comments would help\n. I think this may cause an error if $HELIOS_SOLO_SUICIDE is not defined\nFrom a quick test:\n+ '[' '' -eq 1 ']'\n/tmp/su.sh: line 3: [: : integer expression expected\n1 should probably be wrapped in quotes\n. actually it should be [[ and ]] not [ ]\n. this could be CustomTypeSafeMatcher\n. maybe name this TestMatchers or something similar to make it clear it is about matchers and not a catch-all static method class?\n. is this meant to be commented out? just remove the method if it has no actual code in it\n. +1\n. this.heliosDeployment is never used outside of this constructor, it doesn't need to be a field.\n. Is this class still necessary? It's not used anywhere anymore.\n. If TemporaryJobs now manages HeliosSoloDeployment, is it intended or accidental that it doesn't close the HSD instance here in TemporaryJobs.close()?\n. It definitely serializes as \"jobId\". \nI made a test for this and it seems like Jackson knows to modify the jobId field of the class after calling the constructor (through reflection somehow). So in the end, the data in the object is as expected if you translate it to json and back. \nChanging the annotations might cause incompatibilities with already stored versions \ud83d\ude1e \n. also I think this comment is at the wrong place in the file, not in the middle of the builder class?\n. the outer parenthesis here are unnecessary\n. minor: you can compare enums with ==\n. \ud83d\udc4d \n. it might just be a point of personal preference, but I find it's less characters to type and read \ud83d\ude0e \n. thanks, fixed\n. I could be wrong, but if this dependency is for things like the @GET annotation then I think the better dependency is javax.xml.ws:jaxws-api\n. or javax.ws.rs:javax.ws.rs-api \nthere seems to be 1000 variations of that API jar\n. This should probably clarify that the expression in (bar, quz) is not limited to two values\n. It might be cleared to just list the operators rather than the left-hand and right-hand side\n- =\n- !=\n- in (..)\n- notin (..)\n. i think this could also be something like allOf(containsString(\"..\"), ..) if you knew the set of strings at build-time\n. I would prefer to leave it as a List of String so that the HeliosClient user can pass in any form of selector without having to necessarily have the operator defined in it's code - since HostSelector and the parsing is done in a class included in HeliosClient. This way if there are new operators it won't require a new HeliosClient version.\nWe could though add overloads of this method for both parsed and unparsed parameters if anyone finds that useful\njava\nlistHost(List<String> unparsedSelectors);\nlistHost(List<HostSelector> selectors);\n. I thought so, but -s foo=bar -s x=y resulted in the second argument clobbering the first and only the x=y selector was sent in the HTTP request.\n. #957 changed the CLI argument to be -s or --selector rather than -l\n. nitpick but you have to quote the parenthesis in most shells\nhelios hosts -s key1=value1 -s key2!=value2 -s \"key3 in (value3a, value3b)\"\n. why does attaching to the container need to be scheduled at a fix rate? won't this cause the attaching to happen many times for each container (and log the INFO message above too)?\n. not very important, but - does the delay matter at all? Is it enough to call executor.submit(runnable) and just know that the task will be executed asynchronously from this Service / run loop? (i.e. whenever the executor gets to it)\n. nitpicky but isn't schedule(Runnable, long, Timeunit) the same as submit(Runnable)?\n. small typo: direcory -> directory\n. agreed\n. isJobHistoryDisabled() would be a more readable way to phrase this.. or isJobHistoryEnabled()\n. --disable-job-history might be more intuitive ?\n. i think this is missing scope=test\n. minor: there is no real benefit (or cost, AFAIK) to using ThreadLocalRandom here over a simple new Random().nextInt(DELAY) - there is no contention here.\n. typo in tiem\n. here is the actual fix\n. and here is the actual test - the rest of the changes in SupervisorTest are refactoring to make this test possible (to use a different Job instance)\n. oops, that was my intent. thanks for catching\n. I figured I could avoid a tiny bit of concatenation at the call site - although really what I was saving was the line on the caller side being even longer. I figured since the only callers are within this same test code that the lack-of-obviousness would be ok\n. nitpick but it does not default to 120s in regards to the value of the field in this Job class. The agent will default to using 120s if Job.secondsToWaitBeforeKill is empty/null. I rather leave the value undocumented in this class so we can change the agent if we have to later without changing this part (so it's really just an issue of moving the documentation).\n. To be honest I am not sure about the intent of this loop here - maybe @rohansingh or @rculbertson might remember.\n. sadly I attempted to put this in alphabetical order\n. fixed\n. fairly minor but I think you can leave off the .immutableCopy(). ImmutableList.copyOf(Sets.difference(a, b)) should be the same thing. The SetView returned by Sets.difference(a, b) already implements Collection.\n. YMMV on whether this is more or less readable, but this could also be \njava\nfinal List<String> hosts = hostsAndStatuses.entrySet().stream()\n  // we only care about hosts that are UP\n  .filter(entry -> entry.getValue.getStatus() == HostStatus.Status.UP)\n  .map(entry -> entry.getKey())\n  .collect(Collectors.toList());\n. does the list of hosts stored in this path for \"removed hosts\" grow unbounded over time? It appears that way from previouslyRemovedHosts being passed into the removedHosts function.\nI would be concerned that someone who uses auto-scaling with their deployment group will end up with hundreds (or more) of removed hosts in this list over time.\n. this check-then-set is done in a transaction, right? Because I don't think it catches all possible races if not in a transaction.\n. The best approach is for every piece of code that tries to check this path to be able to handle the case where it does not exist. \n. I think it would definitely be a problem if in fact a host whose labels no longer match the DG ends up staying in this list until the host is unregistered (which could be never).\n\nHowever any agents that are removed without unregistering will cause the list of stored hosts to grow unbounded\n\nby \"removed\" do you mean the host's labels no longer match the DG selector?\n. +1 to Rohan, I think it makes more sense to use java8 optional once it can be used in all places in the codebase - meaning client and server.\n. for future reference it would be good to log something when this unparsable-data-found-but-falling-back-to-empty event happened\n. I think if the path is created on the line above, then this call will be a no-op and you won't write the []\n. a debug log will never really be seen - since we log at info above that we are starting some transaction with a list of operations, it would be good to have a matching log entry also at info about how the transaction was beaten / rolled back / whatever the zookeeper term is\n. just curious, why Callable<Void> instead of Runnable?\n. since this is an abstract class, I don't think there is any value in providing this default implementation.\nFor instance if someone adds a subclass and forgets to override toString(), it would make more sense for SomeNewClass.toString() to return com.spotify.helios.foo.SomeNewClass@123456 or whatever the default is rather than Descriptior{}.\n. i was trying to use a verb, but I'll find a better one :)\n. if all possible names for the sources are defined within this class, then this means that the only secret providers that could be used have to be defined in the helios source code. in other words it is not very open for extension or pluggable. I think the plugin approach used by the service registration code would work great here to avoid tight coupling.\n. it's not clear from this help text that this is meant to be a host:port string\n. this seems to be the same exact code as class FsSmallFileReader - why does it need to be a static method too? \nI don't think this method is used in the class\n. if this static method of DefaultSecretVolumeManager only exists to help make it possible to construct a parameter to the DefaultSecretVolumeManager constructor, you should just add an overload to the constructor that takes this type as a parameter.\n. it would make more sense to keep baseUrl as the original HttpUrl, so that you don't keep re-parsing it on each request.\ni.e. you take an InetSocketAddress and turn it into a HttpUrl then turn that into a String, just to turn that back into a HttpUrl here \n. I think this comment might be better stated as : giving precedence to the updateHosts list, so that we do not end up in a state where we updated a host and then removed the job from it (because of buggy logic in the calling method)\n. this method is always called with the same arguments for timeout/timeunit so I inlined it with the previous method\n. moved the assertion into this method rather than assert something like this in each callsite as it was previously\n. is the idea that the user will specify 0 or more prefixes like \"foobar.helios.\" and then the helios agent or master will provide the rest of the topic name, like \"foobar.helios.\" + \"HeliosTaskStatusEvents\"?\nIt would probably be helpful to document somewhere the full list of topics that the helios codebase will send messages to (when enabled)\n. i see from above that this is following the existing pattern but it feels weird to me that this returns null rather than an empty list, especially when the provider.senders() method has logic to return an empty list when nothing is enabled.\n. again, this follows the existing coding style, but I think it would make more sense if this was simply an empty list if neither the kakfa sender or pubsub sender were enabled - or a list of just one element if just one of them was enabled - rather than always having two instances which internally have to check if they are enabled or not.\n(it is much simpler to simply never call a KafkaSender if kafka-sending is not enabled than to call a KafkaSender that has to wrap it's logic in != null checks)\n. > Is there a good place to put it? A new .md file in docs/?\nthat would be perfect, thanks\n. awesome, thank you!\n. trying to run this locally and it looks to me like you actually need to add an explicit dependency on v3 of protobuf-java (or manage it via dependencyManagement), as whatever loading of pubsub classes occurs also loads the v3 protobuf classes \nCaused by: java.lang.ClassNotFoundException: com.google.protobuf.GeneratedMessageV3\n. because the problematic dependency is only in this pom\n. might make sense to catch when this returns false and throw some sort of IllegalStateException\n. I guess this comes from the original source, but fwiw this pattern with the finally looks strange to me. I think this could just be:\njava\ntry {\n  this.jg = new JsonFactory().createGenerator(logFile, JsonEncoding.UTF8)\n} catch (IOException e) {\n  //log the exception, and set the final field\n  this.jg = null;\n}\nor arguably this field doesn't need to be final if it can be null sometimes and set to an instance other times.\n. since we don't have any issues with the kafka sender and bad behavior when messages can't be published, I opted to leave that out here to avoid changing anything that is working ok today.. would it be easier if model.getHostStatus(host) returned an Optional<String> instead? seems like every place that calls that method has to perform the null check, so might be nice to address that with the type system itself. this might be easier to read as a private method rather than an inline function, since this doesn't close over anything in the current method body. would be helpful to have more context in log messages like this - like what method this is from, or what we were doing with this host in the first place, etc. In this case though is the log warning even useful?. getHostStatus(host) should not be called anymore, so do you still need the when()? removing this catches the case where it is called unexpectedly. i think it would be too cumbersome as this is constructing new instances of KafkaSender GooglePubSubSender etc directly. good catch, thanks. fixed. please smash that approve button now. it is removed in a later commit. no arguments are passed here . This seems like a really broad catch - would be better to only have the pubsub field checks in this try. Is there not a simple boolean method that could be checked rather than using an exception?. Also, iirc Message.toString() logs a message JD and just a portion of the message as hex, not the full message contents, in case you are expecting that to be logged. . s/JD/ID ... commenting from a mobile device is less easy than it seems. it seems like before, a null or empty source was allowed. Now this will cause an error. Am I correct on that and is that change intentional?. @piccobit thanks - it would be helpful to see both changes in context.. should add a note in here about why we are using this version, linking to the two other issues, so that someone doesn't accidentally change it in the future without being aware.\nI'm not sure that we need to care that the same version of netty-all is used in helios-services (where it really matters) and in the system tests (where it probably doesn't matter what version is used), but if the latter works with this change then that is fine too. what is a \"local view\"?. what does the context parameter do?. \"view\" in the method name and comments makes it sound like that if the passed-in list changes, those changes will be reflected in what is returned here - but the use of ImmutableList.copyOf() would seem to negate that. I'd opt to make this be the toString implementation instead - do we ever really want a task list of 100s of items returned as a String?. maybe just avoid specifying if it is a view or a copy of the list or etc if the implementation isn't guaranteed or if that detail isn't all that important. this should probably be in a <dependencyManagement> section. ah I see httpcore is in dependencyManagement in the parent pom, so why not manage both there?. we should manage this in the parent pom too IMHO. The naming behavior of defaultThreadFactory is exactly what I am seeking to avoid in this PR:\n```\n\nNew threads have names\naccessible via {@link Thread#getName} of\npool-N-thread-M, where N is the sequence\nnumber of this factory, and M is the sequence number\nof the thread created by this factory.\n```\n\nIf you have an application with several executors using the defaultThreadFactory, let alone more than one HeliosClient instance, then it is nearly impossible to tell who or what spawned a thread named pool-4-thread-2.\nI want to put helios-client in the thread name to make it very clear what these threads are responsible for.. I think this could be useful - if so you can use boolean com.google.common.net.InetAddresses.isInetAddress(String), which avoids any DNS lookups. do we definitely still need a regex here? A regex that checks if something is a valid IP address feels prone to failure to me. Would it be easier (and more correct) to split the string up by various tokens and then pass the ip address portion to the guava boolean isInetAddress(string) function?. I think you're right, that a regex is better. I think it would help to clarify that this is referring to an IP address of the host that the Helios job will run on, and there is no way for Helios to verify that this IP address actually exists or will work at deploy-time, and putting bad values here will cause your deploy to fail.. And in other words this is likely only useful if you want to deploy a job that exposes a port mapping that only listens on localhost. No, I don't think so, but that was an accident https://github.com/docker/distribution/blob/master/reference/regexp.go#L36. added a commit to catch this, good catch. oh duh, thanks, will update. why do we ignore this exception? \nfwiw it seems like docker-client's DefaultLogStream.close() can not throw an IOException, so perhaps the LogStream interface should redefine close() to not throw a checked exception.. I meant to add that back, thanks for catching. there is a dockerClient.ping() you can use too. can you call the flag (and the field names) --docker-connection-pool-size instead to be very clear about what the connection pool is for?. I think it would be better to not call .connectionPoolSize(int) at all if the flag is not specified for helios-agent, that way the default value when not specified still resides within the DefaultDockerClient.Builder class as it does today.. minor but you could save a little repetition and lines of code in this test if you moved all of the initialization to the field declarations\njava\nprivate final DockerClient dockerClient = mock(DockerClient.class);\nprivate final DockerDaemonHealthChecker checker = new DockerDaemonHealthChecker(dockerClient);\nno need for a Before method then . tack. an empty map should be treated the same, not sure that adds much value. fixed, thanks. good point; originally I did not have the three overloads of jobs() calling each other, so it was repeated three times in there before I condensed the logic down.. need to add tests of this logic / flag. this is done now. good point. I think that renaming this --do-not-await-running as mentioned above would make it more clear. seems like this map doesn't need to be declared outside of the if statement below. will need to change to 8.6.0 once the new stuff in docker-client is released. Good point, although this is kind of difficult with the structure of ContainerRegistryAuthSupplier that is currently in docker-client. It would need to be refactored too, to allow passing the GoogleCredentials in. I think this would be a better design, but I'll probably hold off on doing so until there is a need for Helios to use the ADC.. no: https://github.com/spotify/docker-client/commit/b44708fc5bf78801c07849ea75410c1979264844\nI just meant that I didn't think to add a factory like ContainerRegistryAuthSupplier.forCredentials(), all it has now is forApplicationDefaultCredentials() and forStream(InputStream credentialsStream). just curious, why getRootCause(exception.getCause()) instead of getRootCause(exception)?. it used to be used in debug logging as a marker. Why not send the header along with the certificate if both are present, rather than making this an either-or? Seems like they could be separate concerns as one happens at the SSL layer and the other at the HTTP request layer.. I think one benefit of sending both would be that we could merge and release this ahead of any ability in the backend/server-side to handle these tokens, to decouple the two.. for maximum flexibility, this could take a Matcher<Iterable<String>> (or some other collection type) which would allow you to do argThat(hasKeys(contains(\"Authorization\")) at the call site, to re-use the existing matcher. If hamcrest had a matcher that let you match on a map.keys() then you wouldn't need this at all even. would be helpful to mention what happens if --google-credentials FILE isn't passed. minor: no need to call .toString(). Not critical, but you should leave out the {} when passing an exception to slf4j. It will end up logging something like:\nWARN Could not log container stats. Exception was {}\njava.lang.WhateverTheExceptionWas:foobar\n  at ...\n  at ...\nbecause there is an overload for each Logger method that takes a Throwable as the last parameter: https://www.slf4j.org/api/org/slf4j/Logger.html#warn(java.lang.String,%20java.lang.Throwable)\nalso probably more idiomatic to just have the message look like\njava\nlog.warn(\"Could not log container stats\", ex);. did you mean to change this value too, used for the Google Cloud Pubsub healthchecks IIRC?. might be nice if this could mention the job too - something to identify which rolling-update on the group it was - but seems like that might not be available in this method. +1 to this. nit: this is misspelt . maybe this diagram would show more data if you labeled it as \"R2\" or \"R1\" for which version is running?. I think the use of Optional here is totally unnecessary - could just be this.rolloutOptions = rolloutOptions.. what happens if this assumption is violated?. isn't this the default already? . I think this causes problems if something depending on helios as a dependency also specifies a jackson.version property. But then again, we should be consistent in this pom and either use properties for repeated versions like this or no use it at all. . ah that is pretty weird. I guess the only other option really is to write your own ArgumentAction that doesn't do that, but that feels silly.. No idea - perhaps the idea was to use the same pattern as Optional.fromNullable(x).or(foo), but even that is better expressed nowadays as firstNonNull(x, foo).. Just to toss the idea out there, you could introduce a new type (\"JobRolloutOptions\", for instance) with just the relevant fields, and then have the merging code/function be capable of producing a RolloutOptions from this type and the existing values/defaults.. could this be more specific, and say \"Values specified in the job object\"? \"here\" was confusing to me at first. nitpick: use @throws, and also I think you mean to say \"any attribute in either instance\" not \"the attribute\". consider extracting a function for this repeated ternary statement - I assume you don't want firstNonNull as it is ok if the second param is null?. should probably rename this method now since it refers to JobId. info to match the log message that is logged when a rollout fails due to timeout here https://github.com/spotify/helios/blob/f02446657f06c4fa8f7aad9a23bf87d429fb80db/helios-services/src/main/java/com/spotify/helios/rollingupdate/RollingUpdateOpFactory.java#L198-L199. yes. this unboxing already existed, but as far as I can tell this value should never actually be null (I think it would have been preferable if the internal representation could be some type without nullable fields).. just curious, why change this?. no need, I was just curious.. how about adding something like \"see above for an example of what this looks like in JSON\"?. For clarity, I'd move the Since timeout ... part below the json snippet, otherwise \"Will ultimately have these options at rolling-update time.\" looks like an incomplete sentence. looks to me like this is already documented in this file and more extensively https://github.com/spotify/helios/blob/8424a029eebdd3044d73f580d5e0ad74fba819fb/docs/user_manual.md#ramdisks-1. looks like an accidental import. typo in 127u?. I would suggest using interfaceAddress.getAddress().isLoopbackAddress() instead of testing against /127. could test addr.getAddress() instanceof Inet4Address instead of doing a regex. also I think you can just do InetAddress.getHostAddress() to get the ip address as a string, rather than toString, so you won't have to bother with the / as the host-ip separator.. This might be helpful for creating stub inetaddress instances in your test: https://github.com/google/guava/blob/08983fb25b9f915d2ca51393697f34c9a3854bbd/guava/src/com/google/common/net/InetAddresses.java#L135. How about adding a brief description of the reason only one of the two can be chosen? Maybe just as simple as \"Please choose one or the other, as the two conflict with each other\".. is there a test for grace period and no static ports?. could be worthwhile to extract jdk1.8.0 or 1.8.0 to a parameter, see an example of how that is done in https://github.com/spotify/scio/blob/master/.circleci/config.yml for scala_212_version. could you a comment here for future readers about why we need to install / set python 2.7.10, and why virtualenv/nose/pep8 is needed?. The format here doesn't match what is mentioned in the bullet points below, since the latter is representing it as a JSON object. Could you make this line match the below (i.e. with double quotes)?. Should we still do some validation of String source = entry.getValue() ? For instance I'd expect that if it contained a /, it would need to be an absolute path. I'd imagine that for the other types of volumes a / would never be allowed (although trying to reimplement docker's validation logic would be overkill). could you add a test of .addVolume(\"/foo\", \"\") too? (if it is not covered elsewhere). why is this changed, wouldn't it lose the source part of the volume specification?. with this refactoring of the test, the Datapoint class seems a bit less useful, I wonder if it would just be simpler to pass the data in Datapoint's fields into this method as parameters?. unfortunate that the structure of the superclass means you have to store this data in a class field rather than a local somewhere, but I suppose not using the structure imposed on you by the superclass would be more annoying. Since this is on github.com and open source, shouldn't link to our internal GHE :). I'd skip the constant since it is used in only one method. It is a weird pattern that I don't think should be continued. This code used to do a lot of Optional.fromNullable(foo).orElse(blah).get(), but at some point that was changed to use MoreObjects.firstNonNull(a, b) as there is no real reason to create an Optional instance that is then discarded immediately.. could you add this as a comment into the config for future readers?. builder.secondsToWaitBeforeKill is a primitive type and therefore checkNotNull was pointless. same reasoning as above, getTimestamp() is a primitive value and not an object. the configuration in this profile with -J-Xbootclasspath/p: is only needed on Java 8. errorprone doesn't support < 8. on >= 9 this method of overriding the system javac is not allowed.\nmore details in https://github.com/google/error-prone/blob/gh-pages/docs/installation.md#maven, sorry I forgot to add this link initially.. to be more specific, I think all this is needed because \"Using the error-prone-javac is required when running on JDK 8\". I had to look this up in the code myself, but HeliosClient.Builder is using the timeout you pass it as both the connect and read timeout on the underlying HttpClient. It might be worth clarifying that here - it is not the timeout related to the deployment itself, but rather the HTTP messages sent to Helios' API.. I think it would be good to add some comments here about why we had to add this wrapper script (since the EnvironmentFile etc directives in the unit file don't support running arbitrary shell scripts). I think we also want to add a After=network.target to this section to signal that this service should be started after the network is available. ",
    "bismoy2013": "It's giving this : \nContainers: 0\nImages: 0\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Dirs: 0\nExecution Driver: native-0.2\nKernel Version: 3.13.0-24-generic\nDebug mode (server): true\nDebug mode (client): false\nFds: 10\nGoroutines: 10\nEventsListeners: 0\nInit Path: /usr/bin/docker\nSockets: [tcp://0.0.0.0:2375 unix:///var/run/docker.sock]\n. well that sounds quite interesting ...ditching the whole *vagrant up * part ..will try tomorrow let's see what happens or may be someone can help me thru ! Thanks for ur time Rohan !!\n. well I tried that and I don't know why I'm getting Permission denied !!\n2014/08/14 02:05:58 Post http:///var/run/docker.sock/images/create?fromImage=busybox&tag=: dial unix /var/run/docker.sock: permission denied\nMay be there is something missing in my system ?\n. I ran this DOCKER_HOST=tcp://192.168.33.10:2375 mvn clean test , and got the following errors , perhaps now things should be clear because I am not sure about the first one where you mentioned \"Looks like the DOCKER_HOST is being reset to /var/run/docker.sock somehow?\"\nResults :\nTests in error: \n  FlappingTest.test:70 \u00bb NullPointer\nTests run: 57, Failures: 0, Errors: 1, Skipped: 2\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Helios Parent ..................................... SUCCESS [1.890s]\n[INFO] Helios Client ..................................... SUCCESS [4.405s]\n[INFO] Helios Service Registration ....................... SUCCESS [0.248s]\n[INFO] Helios Tools ...................................... SUCCESS [1.555s]\n[INFO] Helios Testing Common Library ..................... SUCCESS [0.561s]\n[INFO] Helios Services ................................... SUCCESS [1:17.418s]\n[INFO] Helios System Tests ............................... FAILURE [13:55.154s]\n[INFO] Helios Testing Library ............................ SKIPPED\n[INFO] Helios Integration Tests .......................... SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 15:21.487s\n[INFO] Finished at: Thu Aug 14 02:29:21 IST 2014\n[INFO] Final Memory: 24M/312M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.16:test (default-test) on project helios-system-tests: There are test failures.\n[ERROR] \n[ERROR] Please refer to /Github/helios/helios-system-tests/target/surefire-reports for the individual test results.\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn  -rf :helios-system-tests\n. Well still 1 error although seeing what mentioned , I also feel things are okay then , just thought there should have been 0 errors , anyway if you want have a look what I'm getting after running the command you gave here : \nat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)\nat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\nat java.net.Socket.connect(Socket.java:579)\nat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\nat sun.net.www.http.HttpClient.openServer(HttpClient.java:432)\nat sun.net.www.http.HttpClient.openServer(HttpClient.java:527)\nat sun.net.www.http.HttpClient.<init>(HttpClient.java:211)\nat sun.net.www.http.HttpClient.New(HttpClient.java:308)\nat sun.net.www.http.HttpClient.New(HttpClient.java:326)\nat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)\nat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)\nat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)\nat sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1300)\nat java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:468)\nat com.sun.jersey.client.urlconnection.URLConnectionClientHandler._invoke(URLConnectionClientHandler.java:253)\nat com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:153)\nat com.spotify.docker.client.InterruptibleURLConnectionClientHandler$RequestTask.call(InterruptibleURLConnectionClientHandler.java:87)\nat com.spotify.docker.client.InterruptibleURLConnectionClientHandler$RequestTask.call(InterruptibleURLConnectionClientHandler.java:73)\nat java.util.concurrent.FutureTask.run(FutureTask.java:262)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)\nResults :\nTests in error: \n  FlappingTest>SystemTestBase.dockerSetup:229->SystemTestBase.assertDockerReachable:239 \u00bb Docker\nTests run: 1, Failures: 0, Errors: 1, Skipped: 0\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Helios Parent ..................................... SUCCESS [6.383s]\n[INFO] Helios Client ..................................... SUCCESS [3.838s]\n[INFO] Helios Service Registration ....................... SUCCESS [0.367s]\n[INFO] Helios Tools ...................................... SUCCESS [0.480s]\n[INFO] Helios Testing Common Library ..................... SUCCESS [0.860s]\n[INFO] Helios Services ................................... SUCCESS [3.642s]\n[INFO] Helios System Tests ............................... FAILURE [5.460s]\n[INFO] Helios Testing Library ............................ SKIPPED\n[INFO] Helios Integration Tests .......................... SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 21.504s\n[INFO] Finished at: Sat Aug 16 01:37:06 IST 2014\n[INFO] Final Memory: 23M/293M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.16:test (default-test) on project helios-system-tests: There are test failures.\n[ERROR] \n[ERROR] Please refer to /home/bismoy/Music/Github/helios/helios-system-tests/target/surefire-reports for the individual test results.\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn  -rf :helios-system-tests\n. ",
    "william2771": ":confetti_ball: :+1: \n. ",
    "hadesbox": "Thanks it does \"just work\" (TM)!!! \n. ",
    "beeva-josemanuelrincon": "thanks soo much!\nCongrats is the amazing project.\n. ",
    "sncohen": "We all want to make this better, but we've not been able to reach consensus on how it should be changed after discussing it at two different standups. Maybe it's time to backlog this?\n. :+1: \n. ",
    "kobolog": "@davidxia yeah, I guess I'll get there eventually.\n. Since we are going to implement some mechanics to work with secrets at some point, I'd say we should avoid doing it in a fast-and-easy way right now and instead implement some extensible way of providing secrets to Helios. Maybe a pluggable SecretProvider, so that it could be a FileProvider for @jypma case?\n. So the list is in fact starting to look less sad.\n. Okay to merge?\n. What happens if a new agent is registered replacing the old one, but the old one is actually still running its jobs?\n. @evgeny-goldin \n. re @evgeny-goldin, do you think we should still keep this open considering that I've implemented Kafka event delivery thingie?\n. This is how Helios posts stuff to Slack, if you're curious why the hell we need Kafka here.\n. @rculbertson @rohansingh @davidxia @danielnorberg\n. LGTM.\n. 1) Why do you recommend to add more code bloat than there is already? Function is a minimal unit of reusable code, so obviously functions that got called only from one place are useless, force people to jump around the codebase and should be inlined.\n2) Why do you think while (true) is better than having an expression-based while? How is it more clear? It was clear before, when the loop condition was visible on top of the loop construct, but your version lacks that clarity.\n. Docs say that:\n\nThe producer manages a single background thread that does I/O as well as a TCP connection to each of the brokers it needs to communicate with. Failure to close the producer after use will leak these resources.\n\nSo I guess maybe it's because the background thread is never stopped or something like that.\n. Why? It doesn't throw.\n. I think there should be no spaces inside function call operators.\n. ",
    "mudasirmirza": "Thanks for the consideration.\n. Thanks allot guys, I will update the version in our environment. Really appreciate the effort.\n. ",
    "mbruggmann": "As discussed with @drewcsillag this would be handy for Kafka, where the broker service needs to be configured with the exposed hostname/port at startup time, which is then forwarded to consumers through an external zookeeper install.\n. Close-reopen to (hopefully) trigger pull request build.\n. :+1: This includes the fix for not overriding the helios client if it was already specified in the config file.\n. The system tests failing on CircleCI might be caused by https://github.com/docker/docker/issues/2436.\n. I did some investigation into the failing test today. \nMethod\n- Run a container that traps SIGTERM\n- Stop it using docker stop <containerid>\n- Check if the signal was trapped, or if the container was killed.\nResults: Trap in busybox (ash shell)\ndocker run busybox /bin/sh -c \"trap 'echo term; exit' SIGTERM; trap 'echo kill; exit' SIGKILL; while true; do sleep 1; done;\"\n- OSX+boot2docker (native docker driver): works\n- Ubuntu+docker, Spotify config (native docker driver): works\n- CircleCI (lxc docker driver): Container is killed without signal being trapped\nResults: Trap in ubuntu (bash shell)\ndocker run ubuntu /bin/bash -c \"trap 'echo term; exit' 15; trap 'echo kill; exit' 2; while true; do sleep 1; done;\"\n- OSX+boot2docker (native docker driver): works\n- Ubuntu+docker, Spotify config (native docker driver): works\n- CircleCI (lxc docker driver): Container is killed without signal being trapped\nConclusion\nLooks like the containers don't get the TERM signal if docker runs the lxc driver. In any case, I'd still try to stop them correctly at least, meaning this would work on the docker native driver and (potentially) start working later on when this is fixed in lxc.\nProposed solution\nI'll push a patch that will assumeThat(\"runs on docker with native driver\"), which will only run the termination test on docker installs with the native driver.\n. We'll rebase this onto #207 \n. :+1: \n. @martnu @rohansingh @davidxia \n. I'm back may 26 but feel free to edit/improve/merge/discard this as you please.\n. Weekly inventory check. What do you guys think about merging this?\n. We tried that. The reason I kept it this way is because otherwise, we'll not tell listeners about the exit code (since the run method will be interrupted). \nIf we do it this way, docker will stop (or eventually kill) the container, letting the run method finish normally without ever throwing the InterruptedException.\nninja edit: Also what @danielnorberg said below.\n. ",
    "carlanton": "That makes sense... I'll try to do something clever with Conul's service check thingy instead.\nI'll ping you when it's ready to be open sourced!\nThanks :-)\n. :+1: It's a killer feature!\n. I'm also having this problem! Using version 0.8.40 from Github:\n$ tar xvzf helios-debs.tar.gz \ntar: Unrecognized archive format\ntar: Error exit delayed from previous errors.\nand\n$ gunzip helios-debs.tar.gz \ngunzip: helios-debs.tar.gz: unexpected end of file\ngunzip: helios-debs.tar.gz: uncompress failed\n. Hmm.. Something breaks the tests. I guess I missed something... Any hints?\n. Cool! Brb, writing tests...\n. @rohansingh @drewcsillag Something like this? :-)\n. Oh god, CircleCI is killing me :-/\n. Okay. It seems like the CircleCI actually do fail on the ResourcesTest:\nFailed tests: \n  ResourcesTest.testClient:91 expected:<10485760> but was:<0>\nIt works on my laptop in the Helios Vagrant VM... \nvagrant@ubuntu-14:/vagrant$ docker version\nClient version: 1.3.1\nClient API version: 1.15\nGo version (client): go1.3.3\nGit commit (client): 4e9bbfa\nOS/Arch (client): linux/amd64\nServer version: 1.3.1\nServer API version: 1.15\nGo version (server): go1.3.3\nGit commit (server): 4e9bbfa\nNow what? :-)\n. Can this be related to lxc? I noticed that you are settings -e lxc in circle.sh. I think I'm running Docker with the native driver. \nI tried to disable it but CircleCI wasn't too happy about it :-)\n. Okay I think I give up for today. The test passes with the LXC driver locally (-:\n. @rohansingh Thanks! :+1: \n. Cool! I have seen the same problem for a while :-)\nI reposted your logs here: https://gist.github.com/carlanton/7c8311c8d6f31d8a8242 and I think the problem is visible around here: https://gist.github.com/carlanton/7c8311c8d6f31d8a8242#file-vanomashey-logs-txt-L1805\nI'm just guessing here, but my theory is that your Docker application tries to shutdown gracefully when receiving SIGINT/SIGTERM and for some reason fails to do that.\nIt's strange that running docker stop works though....\n. @vanomashey Check your docker logs!\nWhen you are running \"docker stop your-container\", the default timeout before killing the application with SIGKILL is 10 seconds.\nWhen Helios is stopping a container the timeout is set to 2 minutes. I had had the same \"issue\" when trying out the image google/python-hello:2.7 and found this line in my docker logs:\n[info] Container e1d40ba2eddbb95ed9a13466b695bd5649460735985172a74aeb6c997a6a232c failed to exit within 120 seconds of SIGTERM - using the force\nYou can also try \"docker stop -t 120 your-container\" :-)\n. @vanomashey Did you check your docker logs? :-)\nHelios undeploy gives the container 2 minute to shutdown gracefully before it killing it with SIGKILL. This is the expected behavior when the container application ignores SIGTERM.\n. @vanomashey Currently there is no way to change the default timeout. I think you should fix your application instead ;-)\n. No, I guess it's just us at SVT for now :-) Maybe we should write blog post or something about it because Helios and Consul fits together surprisingly well.\nSVT or \"Sveriges Television\", is the Swedish national public TV broadcaster. I'm working with infrastructure in \"SVT interactive\", where we develop SVT's online services (svt.se, svtplay.se etc.). We started experimenting with Helios and Consul last year for our new microservice architecture. It's working really well and we will deploy it to production during the spring :-)\n. @davidxia Thanks! I added your tests to this pr.\n@rohansingh Old jobs without tags will get tags = null. When tags is null, hashCode() will return 0 as it did before => the job hash is unchanged.\n. Status on this? :-)\n. @rohansingh No worries :-) I've just rebased it with master!\n. @rohansingh Yay! Thanks! No problem, we've planned to upgrade all of our Helios clusters in the end of this month so you're right on time ;-)\nBy the way, I've heard a rumor that you guys are moving stuff over to Google Cloud. Should we be worried of you abandoning Helios in favor for Google's container engine? :-O\n. I will write some tests for this...\nEDIT: Okay, this code will not work at all. Brb, rewrite :-)\n. I added some tests but CircleCI died unexpectedly :-)\n. @davidxia Actually, you do support CPU and memory limits since https://github.com/spotify/helios/pull/259 :-P It's not exposed in the helios cli, but you can use them if you are defining your job in JSON!\n@udomsak I THINK the container will be restarted by Helios if it's killed by the OOM: https://docs.docker.com/reference/run/#runtime-constraints-on-resources\n. ",
    "brainsprain": ":+1: \n. ",
    "dflemstr": "@drewcsillag I think @mbruggmann accidentally pushed his branch. Also, this is a WIP.\n. @drewcsillag Ready for review, but it depends on the new version of docker-client which might not be built yet.\n. Btw, sorry for creating so many commits. My laptop disk is broken so I needed the CI to build and test this for me. Can rebase if needed. \n. @drewcsillag squashed\n. :+1:\n. @davidxia rebased and added documentation; maybe it's too detailed?\n. I don't think that this process is easily modeled by Guava Service. The\napproach taken in this PR seemed the most reasonable under these\ncircumstances, but something better might be possible if one wouldn't use\nGuava Service.\nOn Sep 29, 2014 8:54 PM, \"Marc Bruggmann\" notifications@github.com wrote:\n\nIn helios-services/src/main/java/com/spotify/helios/agent/Supervisor.java:\n\n// Stop the runner\n   if (runner != null) {\n-        runner.stopAsync().awaitTerminated();\n-        runner.stop();\n\nWe tried that. The reason I kept it this way is because otherwise, we'll\nnot tell listeners about the exit code (since the run method will be\ninterrupted).\nIf we do it this way, docker will stop (or eventually kill) the container,\nletting the run method finish normally without ever throwing the\nInterruptedException.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/207/files#r18174957.\n. Minor: ThreadLocalRandom?  Also this is logic in ctor, consider Effective Java item #1: create a static factory method.\n. Consider using java.time.Duration instead of long\n. Ha, somehow I ended up in some parallel universe without those comments :D Sorry, trying to get used to this style of reviews\n. So that the future can be failed with a checked exception instead of just a RuntimeException\n. \n",
    "tedyoung": "I'm curious what the problems are that you've seen for >17 character hostnames?\n. Thanks. I ask because I use pretty long hostnames (often >30 characters) and haven't seen any issues in Docker 1.3 in getting the hostname, i.e., hostname from the shell works fine and calling InetAddress.getLocalHost().getHostName() from my code works as well.\n. I think you're hitting up against the limit on host names: they can't be more than 64 characters. It looks like (though I haven't traced through the code) that the way the hostname is being generated, 17 characters is just the input into the string generation, but the output becomes 64 characters, which is fine. When you set the input to 18 characters, the resulting generated hostname is 65 characters long, which causes the error.\nFor example, this doesn't work (hostname is 65 chars):\nsudo docker run -it --rm --hostname \"helios-8901234567890123456789012345678901234567890123456789012345\" busybox\nsethostname invalid argument\nOne character less works:\nsudo docker run -it --rm --hostname \"helios-890123456789012345678901234567890123456789012345678901234\" busybox\n\n. Not sure if this is your problem, but I've seen removal of the containers (i.e., a docker rm imagename) take 1-3 minutes depending on which filesystem you're working with. To see if this is the problem you're seeing, try doing a manual deployment and then do a docker rm from the commandline and see how long it takes.\nIf that turns out to be your issue, you can look at this thread for some ideas: https://groups.google.com/d/msg/docker-user/XnIPl_titUE/NFCnXh04pvIJ -- specifically, it seems that adding the option --storage-opt dm.blkdiscard=false can help speed up the removal of the containers.\n. I'll have to defer to the Helios experts here, then.\n. FYI, the gliderlabs/alpine looks to be even more widely used than the onescience image.\n. ",
    "rschildmeijer": ":+1: \n. ",
    "spiffytech": "I encountered this error because the init scripts don't have shebangs at the top. I've submitted pull request #807 for this.\n. ",
    "alejandrojnm": "hi @drewcsillag i change my conf like you tell me and not work this is all my conf\nif i type helios host this is the result\nHOST         STATUS           DEPLOYED    RUNNING    CPUS    MEM     LOAD AVG    MEM USAGE    OS                    HELIOS    DOCKER          \ndocker01.    Up 26 minutes    1           1          1       0 gb    0.00        0.76         Linux 3.16-3-amd64    0.8.76    1.3.0 (1.15)\nHost1 (docker, zookeeper, helios, helios-master, helios-agent, helios-services)\n/etc/default/helios-master\nENABLED=true\nHELIOS_MASTER_OPTS=\"--syslog --zk localhost:2128 --admin --http\"\nHELIOS_MASTER_JVM_OPTS=\"-Xms256m\"\n/etc/default/helios-agent\nENABLED=true\nHELIOS_AGENT_OPTS=\"--syslog --zk localhost:2181 --state-dir /var/lib/helios-agent --admin --http\"\nHELIOS_AGENT_JVM_OPTS=\"-Xmx256m\"\nHost 2 (docker, helios-agent, helios-services)\n/etc/default/helios-agent\nENABLED=true\nHELIOS_AGENT_OPTS=\"--syslog --zk 10.10.10.34:2181 --state-dir /var/lib/helios-agent --admin --http\"\nHELIOS_AGENT_JVM_OPTS=\"-Xmx256m\"\nthx for all\n. Hi @drewcsillag i found the problem, if you see my old post about the helios don't start, that is the problem i run manualy helios-agent --zk host1:2181 in the host2 and work fine, i think is because i use debian testing (8.0). any way i will try reinstall debian to see if work. thx for all\n. Hello @davidxia think that they are doing is more less the idea I had. I do not have much experience with java and Dropwizard. Like most python. It would be possible to read the api that have developed with Dropwizard with python ??\nI was thinking about an interface, using bootstrap, angular and django to manage permissions for users, hosts, etc.\nThat's the idea I have, you'll say to me in that I can help.\n. Hello @davidxia I have been working on the GUI and this is the idea so far, I hope I your opinion on this, I would have to remove or put something. Not finished yet \n. Hi @davidxia It's just a mockup for now, tonight I'll do a preview and upload it to github tomorrow so wish to download it and test it, and tell me what you think. I'm glad you liked the design.\n. Hi @davidxia i release the first beta of Helios manager :) you can find it here. for now only reads data. Just need install Django 1.7, request 2.0 and conf the settings. I wait for your opinion, and tell me that I could add or remove. I'm still working.\n. Hello @davidxia and @aidanhs, I have been complicated, but I'm working again with the helios-manager, I'm making some changes to the UI, I hope to upload them as soon as they finished, and will also be made to create jobs, in addition to the deploy and undeploy the jobs\n. I hope this week, push all commit to github :D\nOn Fri, Aug 7, 2015 at 7:44 AM, Florian Maier notifications@github.com\nwrote:\n\n@alejandrojnm https://github.com/alejandrojnm very cool, sounds like\nsomething i want to test-drive asap ;-)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/issues/283#issuecomment-128681311.\n. @marsmensch, @davidxia, @aidanhs  this is how it looks. so far this is how this being.\n\n\n\n\n\n\n\n\n\n\n. Hello @marsmensch i up the latest changes, still missing some things like create and deploy job to job, and I'm working to list the images of the private registry, I'm working on it. Hope you try and tell if a need change something or add something\n. Yes is that\n. You can close this issue\n. i send this, using Method PUT and header [Content-Type:application/json]:\nhttp://10.10.10.200:5801/hosts/helios-01/jobs/Tytan:v1:55f1ae3ae433c528939968412610596c19ac5176\n. Now you can do deploy, jobs. I would like you to try the aplicaccion. https://github.com/alejandrojnm/Helios-Manager\n. @davidxia  you're absolutely right, the problem is, the authentication in the registry, allowed him to access my private network and ready all worked. Thank you so much for everything.\n. ",
    "marsmensch": "@alejandrojnm very cool, sounds like something i want to test-drive asap ;-)\n. I like the look, clean and smooth. Maybe a little too red for the details\nmenu for my taste, but thats really not that important atm. Well done!\nOn Fri, Aug 14, 2015, 15:42 Alejandro JNM notifications@github.com wrote:\n\n@marsmensch https://github.com/marsmensch, @davidxia\nhttps://github.com/davidxia, @aidanhs https://github.com/aidanhs this\nis how it looks. so far this is how this being.\n[image: 1]\nhttps://cloud.githubusercontent.com/assets/7557763/9275034/478c1bde-4268-11e5-893a-4abddf25eea2.png\n[image: 2]\nhttps://cloud.githubusercontent.com/assets/7557763/9275029/471d8606-4268-11e5-839e-f62e82abc054.png\n[image: 3]\nhttps://cloud.githubusercontent.com/assets/7557763/9275027/47147e08-4268-11e5-9d0e-bfdac610447a.png\n[image: 4]\nhttps://cloud.githubusercontent.com/assets/7557763/9275026/46fe13fc-4268-11e5-8272-b775e6b68ae1.png\n[image: 5]\nhttps://cloud.githubusercontent.com/assets/7557763/9275033/4757a958-4268-11e5-8297-4cf40c2dd742.png\n[image: 6]\nhttps://cloud.githubusercontent.com/assets/7557763/9275028/471b5638-4268-11e5-9141-386756a49caa.png\n[image: 7]\nhttps://cloud.githubusercontent.com/assets/7557763/9275030/471fb200-4268-11e5-8813-be8d4739f501.png\n[image: 8]\nhttps://cloud.githubusercontent.com/assets/7557763/9275031/473ab910-4268-11e5-8402-2c52d2760b6b.png\n[image: 9]\nhttps://cloud.githubusercontent.com/assets/7557763/9275032/474574ae-4268-11e5-8558-9c81b507beb6.png\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/issues/283#issuecomment-131109038.\n. Hi @alejandrojnm, will test-drive on wednesday and get back to you here. Thanks!\n. \n",
    "artjomzab": ":+1: \n. ",
    "konradjurk": "Awesome. Does this UI need to run on the same instance as the Helios master?\n. ",
    "vanomashey": "Hi Ted,\nThere is no problem with docker. Docker stop and docker rm works very fast. The whole restart of the container takes around 5-10 seconds. Right now in logs it seems it cannot stop it few times.\nI saved logs here: https://drive.google.com/file/d/0B1m28OI2tFFGczE0RG42OUdUUnM/view?usp=sharing\nBut there is another strange thing.  When I try to remove the container the via helios, It takes 2 minutes. If I restart the helios agent, it kills container immediately. Any idea about what might be wrong?\nRegards,\nIvan\n. Also it works very fast if I just restart the helios-agent. It kills all unnecessary docker containers immediately within 1-5 seconds. which is very strange. That is why I suspect problem with helios and not with docker. \n. Hi Anton,\nEven helios kills the container within 1-2 seconds if I restart it. I guess there is a clear issue in helios somewhere. I will try with new version.\nRegards,\nIvan\n. Tryed with the updated version. takes 2 minutes. not more nor less. around 2 minutes. Is there reason for this?\n. Hey Carla, \nHow did you set up helios sigkill time to lawer value? \n. Nope. Not anymore.\n2015-01-06 18:42 GMT+01:00 David Xia notifications@github.com:\n\n@vanomashey https://github.com/vanomashey Is this still an issue?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/issues/314#issuecomment-68901369.\n\n\n\nBest Regards,\nIvan Mashey,\nMobile: +31655887134\nSkype: vano_mashey\nwww.attractgroup.com\n. ",
    "jypma": "@davidxia Thanks for your comments. It's nice to see that indeed authorization support has been added to the docker client since I wrote the issue, so that's indeed available now.\nAs far as where to store the credentials, I see several possibilities:\n1. Put them together with the job config. The password is visible to the machine creating the job, on helios-master, in zookeeper, and on the machine running helios agent to pull.\n2. Manage them directly in zookeeper through helios-master API, write-only. The password is visible on helios-master, in zookeeper, and on the machine running helios agent to pull.\n3. Have helios-agent read a docker-like ~/.helioscfg file that contains passwords, readable only be the user helios-agent is running under. No API changes. The password is only stored on each helios-agent machine that actually needs to pull.\nOption (3) seems most secure to me, and possibly fastest to implement. However, it's also less generic, as it would only allow one fixed username/password to authenticate against any one registry. That would cover most typical machine-initiated build / test / deploy jobs though...\nWhat do you think?\n. Unfortunately, it seems that DefaultDockerClient only has room for 1 authConfig object that is then sent along with a pull command to any repository. What you'd want to do instead is have an authConfig per repository, or even pass in the authConfig as an argument to the pull method. So, still changes needed to the base docker client, or some nasty overriding and copying of methods into the helios project.\n. Yes, we're using quay.io, but right now we're manually copying stuff from there into a private repo, so helios can make docker pull from it. We prefer not managing our own docker registry though, since our images can grow quite big and we'd rather not manage the storage and access ourselves.\n. Using helios 0.8.245.\n. ",
    "erikgrinaker": "Docker stores authentication details for docker login in the file ~/.docker/config.json, which looks something like this:\njson\n{\n        \"auths\": {\n                \"registry.domain.com\": {\n                        \"auth\": \"dXNlcm5hbWU6cGFzc3dvcmQ=\",\n                        \"email\": \"user@domain.com\"\n                }   \n        }   \n}\nThe auth string is in standard HTTP Authorization format, i.e. the base64-encoded value of <username>:<password>. This is obviously completely insecure, but anyone using private registries will likely have these files lying around anyway.\nIs there any chance of having Helios support this format? The alternative right now seems to be allowing full read-access to our private registry from our server networks, which is even less secure.\n. ",
    "elifarley": "This is related to #462.\n. ",
    "nresare": "I can reproduce this with current master (c12daf62) by setting up a vagrant box and appending --zk-namespace helios to HELIOS_MASTER_OPTS in /etc/default/helios-master and HELIOS_AGENT_OPTS in /etc/default/helios-agent and and restarting them in that order.\nOnce that is done when you create a job with helios create bar:v1 nginx and start it with helios deploy bar:v1 ubuntu-14 you can't undeploy it with helios undeploy bar:v1 ubuntu-14, the status returned being JOB_NOT_FOUND.\n. This can be reproduced with latest master with any master URL that doesn't return the expected data. For example: helios status -z http://xkcd.com:80/foo -vvvv\n. It turns out there is a bit of a chicken and egg problem with adding the helios user to the appropriate group as the user is created by the helios-agent deb package postinst script and the daemon is started only a few lines later. \nMy suggestion would be to duplicate the helios user creation command into the Vagrantfile and rely on the fact that the postinst checks for existance of an account named helios before attempting to add it, then add the helios user to the docker group after having crated the package (but before installing and starting helios-agent). Thoughts?\n. Sure, let's close it.\n. I'm trusting that the header.java file is updated, which validates the diff even though I haven't looked through :) :+1: \n. Well, deb packaging is not a proper standard, so the fact that you sometimes can install deb packages on a distribution it was not built for is more of a happy coincidence than something you can expect. Because of this I think it's a good idea to be explicit about what target environment we're building for.\n. Please see updated wording.\n. ",
    "philipvonbargen": "@rohansingh I should have gotten all cases now\n. Heh, apparently we did not catch everything while renaming, will update in a bit.\n. ",
    "rasmusbergpalm": "Yes! We've stumbled on this a few times. :sparkling_heart: \n. ",
    "evgeny-goldin": "No, we shouldn't keep this PR.\n. With previous version:\n$ helios -z 'http://helios-master.vm:5801' inspect \"nginx:v1\"\nId: nginx:v1:50aa1e78592550f60780432cf667406294480e1e\nImage: nginx:1.7.9\nToken:\nCommand: []\nEnv:\nPorts: http=80:8080/tcp\nReg: nginx/http=http\nGrace period (seconds): null\nVolumes:\n$ helios --version\nSpotify Helios CLI 0.8.200\n$\n. :+1: \n. Downgrading Helios to 0.8.145 didn't help, neither did apt-get purging Docker.\n. Java is Oracle 8 JRE:\nroot@helios-agent1:~# java -version\njava version \"1.8.0_40\"\nJava(TM) SE Runtime Environment (build 1.8.0_40-b26)\nJava HotSpot(TM) 64-Bit Server VM (build 25.40-b25, mixed mode)\nroot@helios-agent1:~#\n. The error was caused by broken JSON at \"~/.helios/config\" of the \"root\" user. What could be done, though, is a more explicit error of what went wrong and a path to the JSON file that couldn't be read. Also, it would help if the error stack trace was displayed.\n. Great, thanks!\n. ",
    "gimaker": ":+1: \n. :+1: \n. @rculbertson @davidxia @rohansingh \n. :+1: \n. :+1: \n. :-1: I'd rather have longer tests than mutable public static fields, ew\nQuestion: should it exit 10 times in 200 seconds? If yes, then why does the test fail? By virtue of being too timing dependence?\n. So... that means that it takes helios over 20 seconds to (re)start the container on average? That sounds fishy. Is that expected?\n. :+1: \n. :+1: \n. looks like some unrelated stuff was included in the commit, but :+1: \n. :+1: \n. where is the unit test david?\n. - host + image = :+1: \n. :+1: assuming it correctly creates the (nested) directory, does it?\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. omg yes :+1: \n. @rohansingh @davidxia @rculbertson \n. :+1: \n. :+1: (but this is an area I'm certainly a novice in, so you should probably wait for another thumbs-up)\n. @rohansingh @rculbertson @davidxia \n. @davidxia: there are now more immutable lists\n. :+1: \n. :+1: \n. :+1: \n. @davidxia @rohansingh \n. :+1: \n. @rohansingh @davidxia \n. @davidxia @rohansingh \n. @rohansingh @davidxia \n. tests would be nice... but... :+1: \n. :+1: \n. Maybe pass an interface to ZKMasterModel to make it less coupled?\nE.g. interface DeploymentEventSink { void emitEvent(DeploymentEvent ev); }\n. :+1: \n. :+1: with a few nitpicks\n. :+1: \n. :+1: \nNo strong opinions on server/client side filtering for this case. That said, I'm tempted to move the label filtering to the server-side so that it works when --json is specified (or, alternatively, just do the label filtering on the client-side as well).\n. Why is the additional information needed?\nEmitting the full DeploymentGroupStatus blob makes it harder to change how deployment groups works internally. I.e. if we change DeploymentGroupStatus that will also break anything that depend on the format of the emitted events. But maybe that's okay?\n. :+1: \n. :+1: \n. :+1: \n. what problem does this address?\n. seems like this could only happen in certain racy edge cases, and then the behaviour would be that the job fails to reach the running state?\n. I have no clue to be honest.\n. :+1: \n. how do you tell this DeploymentGroupEvent (DGE) apart from other DGEs?\nI.e. how do I tell this DGE apart from a DGE emitted when a rollout is started due to more hosts being added to the deployment group?\n. a more descriptive commit message would make this easier to review. e.g. what logic is moved from the CLI to the server?\n. There's an open PR that addresses this: #607\n@momomagic: Note that deployment groups are still somewhat experimental and that we are currently working to fix some performance issues.\n. @momomagic Rolling-updates on deployment groups are slow once you start having \"many\" (10 or so) deployment groups. Nothing breaks, but rolling-updates become slow. We're working to fix that. Other functionality should not be affected.\nI don't know of any regressions related to healthchecks -- please submit a separate issue if you're having problems.\n. :+1: \n. @davidxia @rohansingh \n. @rohansingh @davidxia It's a monster.\n. That should work, albeit a little ugly (would be nicer to have a single underlying ExecutorService with multiple threads).\nAlso, with 10 concurrent threads there's a chance they might contend with each other (e.g. two threads doing the same operations, and then one failing to commit the results). Not sure it will be a problem but you could do something like randomize the order in which deployments are processed to reduce any potential problem (or add a delay).\n. I'd prefer to filter things in the status endpoint, e.g. add this check (status == UP) to DeploymentGroupResource.java:195. Or even return the DOWN hosts but include the status in the response, so that you can filter on the client side.\nWith this change, hosts going DOWN temporarily will trigger rolling updates (since the hosts are changed) which seems a bit counter-intuitive.\n. :+1: \n. @davidxia \n. yeah we should catch and log those exceptions in rollingUpdateStep()\nlet me add that to the PR\n. :+1: \n. Not completely sure how to test this in a reasonable manner -- wokring on that part...\n. @davidxia \n. :+1: \n. @davidxia @matslina \nNot sure I like this. With this in place there are more edge cases. I.e.\nA deployment will only undeploy jobs previously deployed by it, unless:\n- the job is exactly the same and was manually deployed, OR\n- the job is exactly the same and --migrate was specified\n. :+1: \n. Can we hold off on merging this? I suspect there will be ramifications elsewhere, and would like to look into those before merging anything. Specifically:\n- How will rolling-update behave if a host fails, but the rollout does not? I suspect the failed host will not be shown at all in the output.\n- \"deployment-group-status\" should probably show the failure threshold, and be explicit about which host failed.\netc.\n. @rohansingh @davidxia \n. @matslina \n. :+1: \n. @davidxia @rohansingh \n. @davidxia yes please, patches welcome ;)\n. :-1: The rolling-update op factory today contains only logic for acting on and moving between tasks in a rollout. Adding logic to it where around how to start rolling-updates seems out of place -- for example, the start() method cannot be called once a rolling-update is started, and nextTask() and error() cannot be called unless it has already been started.\nIMO, a better, but still not that nice, solution would be to just return a RollingUpdateOp from getInitRollingUpdateOps (it doesn't need to be created by the factory).\nAlthough, at that point maybe it should be renamed to ZkOpsAndEventsPair or something like that :)\n. @davidxia  @rohansingh \n. :-1: I think saying to the user that everything went fine when the service is in partially rolled out state is a recipe for confusion.\n. :+1: \n. :+1:\n. :+1: \n. :+1: \n. :+1: \n. @mattnworb: We could tweak the docker-maven-plugin to emit such a label and use that, but if you don't use docker-maven-plugin or if you supply your own Dockerfile you'd have to add and maintain that line by yourself -- which seems error-prone and tedious.\n. :+1:\n. :+1: \n. @rohansingh @davidxia @mattnworb \n. @mattnworb Aye, we could. I don't have a strong opinion either way. TaskStatus is quite hefty though, as it contains a reference to the Job.\n. :+1: \n. :+1: \n. @rohansingh @davidxia @mattnworb \n. :+1:\nAlthough I have to say that the general practice in that file seems very smelly. static variables don't get reset between tests, or even different test classes -- changing a static variable could potentially affect other tests that depend on it, and make it hard to figure out why things blow up.\n. @davidxia Sorry I was referring to mutating static non-final variables, like the CliConfigTest does.\n. :+1: \n. :+1: \n. :+1: good catch\n. :+1: \n. @davidxia @rohansingh @mattnworb \n. @rohansingh @davidxia @mattnworb \n. Dependency hell.... :angry: \n. @mattnworb: Re AuthProvider.Factory, although I guess that would work technically, as the ClientAuthenticationPlugin is essentially a factory in it's own right. That said, I don't want to couple the plugins with the auth providers.\nThe AuthProvider.Factory is created before the HeliosClient is -- so we need some kind of factory.\n. :+1: \n. @rohansingh @davidxia @mattnworb \n. I'm feeling pretty \"done\" with this, with regards to this PR. There's more stuff to do, but it feels better to do that separately.\n. @mattnworb how does AuthProviderSelector know which implementation to use ahead of time?\nThe Context object enables adding more parameters later on without breaking the API, so yes - I'd say it's necessary if you don't want to break API in the future. \n. Hmm I guess if you defer creation of the AuthProvider until you get a 401, and pass in the auth-scheme when calling the factory...\n. rebased on master\n. Is this for testing purposes only? I think most people will be too lazy to actually type out all those CLI flags. :)\nThis makes it impossible for AuthProviders act on authentication parameters (e.g. realm). Doesn't seem like a huge concern in practice though so maybe we're okay with that?\n. looks alright to me\n. Looks good too me. Too much boilerplate and too few tests, but I can understand why :)\n. :+1: maybe log when it's stopped too?\n. does helios-tools use dnsjava? I couldn't any references to it. Although there does seem to be some dependencies on the xbill stuff.\n. :+1: \n. @rohansingh @davidxia @mattnworb \n. sure :+1: \n. lgtm\n. @davidxia  @rohansingh @mattnworb \n. :100: \n. Would it make more sense to write an integration test that tries to run a broken image and inspect the output of that?\nIIUC this test mostly tests that failed() is called correctly on the listener when an exception is thrown by run0() ?\n. I like it\n. :+1: \n. :-1: I'd rather understand how we get into this state in the first place and do a proper fix. Preferably with tests to verify.\n. Seems like the config/status/hosts should always exist (provided that the DG does), right? Checking that status & hosts exists seems superfluous. Seems like we should be able to put deletes for all 3 paths in a transaction.\n. :+1: \n. :+1: \n. checkstyle errors though\n. Can you elaborate a bit on \"Propagate DNS for machines using NetworkManager (Java DBUS interface?)\"?\n. :+1: \n. @mattnworb @rohansingh @davidxia \n. :+1: lgtm\n. @mattnworb: It's tricky since it's triggered by a race condition, but I'll give it some more thought and see if I can figure out a way to do it. If you have any ideas please share.\n. +:100: for better testing of the ZK code.\n. @mattnworb Tests added. By using TestingServer I think they turned out pretty decent.\n. @mattnworb You can use things other than ints with theories, but it requires extra scaffolding and it didn't seem worth the effort. The nice thing about theories is that it generates all the parameter permutations for you, which AFAIK Parameterized doesn't support. \n. :+1: \n. :+1: \n. :+1: \n. Fix the build, though (checkstyle errors)\n. :+1: \n. @mattnworb that's because it get's a response. it doesn't inspect the http code and retry based on those\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. @rohansingh @mattnworb @davidxia \nThis is still WIP. There stuff left to do:\n- ~~Documentation~~\n- ~~Tests~~\n- Ideally, we also set it up so that at least some integration tests run with ZK ACLs disabled, to verify that that still works as it should\n- ~~Fix a bunch of currently failing tests~~\n. @mattnworb yeah that [moving the list of rules out of the ctor] probably makes sense. it bugged me too -- but I wasn't  sure the extra boilerplate needed would be worth it.\n(If you just want to be able to test with different rules, a quick and dirty approach is to create an auxiliary getAclForPath(String path, List<Rule> rules) method and make it package-local)\n. @mattnworb Yeah, supplying the rules in the ctor is what I'm thinking of doing. But it requires extra scaffolding/refactoring since the rules contain the digests as it is now (in the Id object).\n. @rohansingh nope, not really. Ideally it'd be good to have some tests that run without ACLs to make sure things still work when they're disabled, but maybe that's not needed? Other than that, the only thing that comes to mind is if we want to give agents more permissions on everything under status - e.g. give the CRWD on everything under /status/hosts/<host>, to avoid problems if we ever need more permissions.\n. Another thing that might be prudent to do is to run all tests and examine the logs for NoAuth exceptions (that may be problematic even though they don't cause tests to fail?).\n. :+1: \n. @rohansingh WFM. If we want to change permission in the future, helios-initialize-acl will need to be re-run with \"super\" authentication, as neither the client nor the master user has ADMIN permissions (and thus cannot change ACLs on existing nodes).\n. @rculbertson @rohansingh @mattnworb @davidxia \n. I'll leave the removal of the unrelated but unused masterAdminEndpoint(). startJob(), deregisterHost(), and readLogFully() to another PR.\n. :+1:\n. :+1: \n. Oops\n. All of this would make a bit more sense if we only run one helios-solo deployment per test run. Now there's two ways of cleaning up the helios-solo deployment (either via the watchdog, or being explicitly shutdown via HeliosSoloDeployment.close() -- and they work differently)\n. @davidxia There was code doing what you describe (running one shared helios-solo) in my PoC branch - why doesn't that approach work?\n. :+1: \n. :+1: \n. @davidxia @mattnworb @rohansingh \n. 1. Yes.\n2. \u00af(\u30c4)/\u00af\n3. job:oldversion, then when the master detects that the hosts has changed it will in most cases initiate a rolling-update and deployed job:current-dg-version shortly after. \n. Alternatively, we could just delete everything under /config/hosts/<host>, including jobs.\n. Actually, the latter (deleting existing jobs when re-registering) will not work with the current ACLs, as agents do not have any permissions except READ on the /config/hosts/<host>/jobs tree.\n. (Technically, agents are never in deployment groups - but jobs deployed to them might be.)\nI'm inclined to agree, but I'm not sure how to best overcome the hurdles posed by ACLs... Giving agents more permissions on /config/hosts/<host>/jobs allows any compromised agent to have free reign on other agents (e.g. deploy rouge jobs, undeploy jobs, etc.) \n. What other options do we have?\n- Periodically garbage collected DOWN'ed hosts from the masters? This ought to be work, but with a delay. On the plus side, it'd also get rid of those \"DOWN since 96 days\" hosts :)\n. It's not too late to pull the breaks on rolling out the ACLs. Soon it is, but not yet.\n. @mattnworb Any thoughts on how that would work in practice? I assume the agent would need to provide some kind of authentication to the master, to stop unauthenticated users from exploiting that endpoint.\n. Okay, here's an idea: do this plus garbage collect DOWN hosts (when they're down for, say, more than 3 days or so). That way, a stale job might be deployed but it will at most be 3 days or so stale.\n. Tests for the reaping of dead agents are pretty shoddy -- not completely sure how to test that functionality properly, ideas welcome!\n. \"Shoddy\" as in it doesn't test the actual functionality, but only a small part of the implementation. There's currently no test that verifies that agents are actually reaped by the service.\nSince the reaping logic operates on a list of hosts (with various continue statements in there) I wanted to do the tests on an actual list to test that logic, as opposed to running a parameterized test multiple times on single-item lists.\n. HeliosSoloDeploymentTest... grumble grumble\n. I feel pretty much down with this unless people have more comments/concerns.\n. @rohansingh @mattnworb \n. :+1: \n. Would prefer to change the logic so that it uses the bridge address when Helios \u00eds running in a container, regardless if it's helios-solo or not.\n. :+1: \n. :+1: \n. @davidxia @rohansingh @mattnworb @negz \n. @mattnworb @davidxia @rohansingh @negz \n. @davidxia I haven't seen what you describe myself, but please share the details if you have.\nFor reference, what it might looks like in a situation where it fails today is this:\n/config/hosts/host1/jobs/job-1\n/config/hosts/host1/jobs/job-2\n/status/hosts/host1/jobs/job-1\n...\nCurrently when deregistering, it will try to delete all jobs that are present in /status/hosts/host1/jobs tree. In the above example that is just job-1, and since /config/hosts/host1/jobs isn't empty (still contains job-2) it will fail to deregister the host.\n. Kudos to @mattnworb for figuring this out, btw.\n. For our particular use case, we need the overall metric as well, right? Since we do alerting on aggregate value.\n. Tests would be nice, too.\n. :+1: aside from my one comment\n. Would it be possible have a saner default, and override this in e.g. our internal spotify-helios-testing library somehow? I guess you already considered that option and ruled it out for good reason though? [lots of question marks used to signify that I really don't know what I'm talking about]\n. Otherwise LGTM.\n. :tada: :tada: \n. Hrm? PR description doesn't match the single commit in it\n. @davidxia it don't think it's github - I checked out your java7 branch and it has only one commit as shown here in it (Allow user to bind admin port on different host). I think you fat-fingered something :)\n. @mattnworb @rohansingh @davidxia @negz \n. I struggled with this, so please suggest changes or modify and add commits as you see fit.\n. Can you give an example of how a what will look like?\n. Thank you @mattnworb, that's helpful :bow: Those whats look good to me.\n. LGTM. All the commits seem sensible. I will confess to not reading all the code in detail though - but I skimmed all of it and scrutinized some of it and it looks good afaict.\n. I would like to see a test for how this behaves when the cached certificate is expired. From what I gather from the code it looks like it would just fail without re-generating the cert??\n. :+1: :100: \n. Relevant tests are failure, otherwise LGTM\n. LGTM but any chance we can add tests for this?\n. > Instead of shuffling the list on each call to allocate, I think it would be simpler in some ways to shuffle it once at construction-time and maintain an index into the list (like the old implementation did) so that each new call to allocate just moves the index up (and checks if it can use the value in potentialPorts[index]\n+1 on this. Doing that would also avoid what the old comment in the code says:\n\nIndex for port allocation. Reused between allocations so we do not immediately reuse ports.\n\nWith a big port range it's unlikely that ports are immediately unused but possible.\n. > To fix this, introduce a new type of InterruptingScheduledService that awaits on a CountDownLatch before executing it's task.\nInterruptingScheduledService == SignalAwaitingService?\n. Oh never mind. Just me getting confused by the wording in that sentence.\n. :+1: nice detective work and fixes!\n. Example before and after output?\n. :+1: thanks!\n. :+1: lgtm\n. @mattnworb should we merge this?\n. :+1: \n. :+1: \n. checkstyle error, otherwise :+1: \n. There's already a HostMatcherTest class has tests for RollingUpdateService.HostMatcher\n. https://github.com/spotify/helios/blob/master/helios-services/src/test/java/com/spotify/helios/rollingupdate/HostMatcherTest.java\n. :+1: \n. :+1: good catch. checkstyle errors though\n. :+1: \n. :+1: \n. :+1: \n. > We admit this is super funky, but there's no way to force skydns to\n\nspeak only TCP right now.\n\nAnd it shouldn't. What it should do is properly forward the response with the truncation flag set to the requester (who would then retry using TCP). Instead it just blows up and never responds to the requester at all :(\n. @davidxia what happened to the nice commit message? :(\n. Oh nevermind. I see that it's still there.\n. :+1: \n. @mattnworb @davidxia @negz @rohansingh \n. > Would be nice to have docs :)\nPRs welcome :)\n. > tangential question but why is the type of BiPredicate in Operator declared as , for example\nI believe it was to allow cases where the \"operand\" is not a string -- like in this case, where technically it's a set.\nStill on the fence if I should have the parse() method parse the set from the string and let the in and notin operators deal with sets, or if I should just do it the way it is now. Also not sure it matters much, and neither seems ideal. If you go with the latter changing it to BiPredicate<String, String> would make sense but it's tehnically an API break so would rather not (even though it should be safe).\n. :+1:, comments aside\n. @negz @davidxia @mattnworb \n. >  \ufffcbut where was it broken before? I can't tell what the fix is here\n@mattnworb equality was broken before. a in (a,b) and a in (a, b) were considered different even they aren't really (the extra space does not matter). This in turn can result in conflicts when creating deployment groups. E.g. instead of getting NOT_MODIFIED you'd get CONFLICT by just changing the formatting of the host selectors.\n. :+1: \n. Not to be a stickler... but the commit message could be improved. Just prefixing it with helios-testing: or HeliosDeploymentResource would help to give some context.\n. :+1:  overall\n. :+1: \n. :+1: \n. :+1: \n. Reviewable, yes. Releasable... maybe.\n. @davidxia \n. > But would it be simpler conceptually if step 3 in the outline above simply didn't cause the group/state to be marked as FAILED?\nI agree with this (but I don't know how hairy it would be to implement so take my opinion lightly).\nI also think we should not trigger rolling-updates when hosts change if there's already a manually triggered rolling-update in progress (since it, depending on the exact implementation, leads to really subtle behaviour).\n. > That seems reasonable, but it's tangential to the intent of this PR right? i.e. We allow new HOSTS_CHANGED rolling updates to start while MANUAL rolling update is ROLLING_OUT at the moment.\nTangential, yes -- but I think it'd avoid certain odd cases that are otherwise present with this PR.\n. :+1: \n. I'll see if I can add a test for this as well, or at least try it out manually.\n. @davidxia @rohansingh @negz @mattnworb \n. Test failure in JobHistoryTest.testHobHistory() seems unrelated - it's failing on master too.\n. :100: \n. Test failure is unrelated\n. if you don't use maven, and consequently don't have the target directory, then the only thing that will happen is that we create the target directory and put stuff in there. is that really so bad?\n. should the json name also be jobId, then?\n. yes. if we don't, them those deployment groups will be defunct -- we might not even be able to remove them at that point.\n. Composition > inheritance?\n. There are fringe cases where the \"manual\" part of this statement is not quite true. E.g. if someone created an overlapping deployment group and used rolling-update with the --migrate flag.\nMaybe something along the lines of:\n\"Job unexpectedly undeployed on host %s. Perhaps it was undeployed manually?\"\n. same way as the other one -- undeploy -a --yes \n. why optional?\n. i see\n. You might want to check out the built-in choices() method. E.g.\n.choices(Collections2.transform(\n    Arrays.asList(HostStatus.Status.values()), new Function<HostStatus.Status, String>() {\n      @Override\n      public String apply(final HostStatus.Status input) {\n        return input.toString();\n      }\n    }).toArray(new String[0]));\n(it's icky in java 7, much nicer in java 8)\n. erroring out here would probably be nice, and avoid confusion when you have typos.\nE.g. \"list --status up\" or \"list --status DWN\" will list all hosts but not actually filter anything out\n. statusFilterStr.toUpperCase() ?\n. What happens in the status query-param is missing? It should still work, to remain backwards-compatible with older versions of the CLI - does it?\nIf it doesn't work without specifying the query-param you might want to use @DefaultValue(\"\") or [guava] Optional.\n. Same thing here.\nA test checking that it works to call this endpoint wihtout the status param would be nice.\n. hostStatus(final String host, final Map<String, String> queryParams)\n-->\nhostStatus(final String host, @Nullable final String statusFilter)\nor similar ?\n. or Optional instead of Nullable\n. This needs to be cleaned up\n. The name probably makes no sense -- but at least this logic is easier to unit test now\n. This hacky \"test\" obviously needs to be fixed or deleted\n. probably very much a non-issue but this looks like an unbounded queue\n. The exiting executor service seems unnecessary\n. how many? should we limit these more?\n. for existing deployment groups this field will be missing. what will happen for them?\nwill failureThreshold be set to 0? there's some @DefaultValue annotation I seem to recall\n. Baking this logic into RollingUpdateOpFactory this way seems a bit out of place and conflated, but I see that you need to emit an event AND move to the next task. Looks like it's time for some refactoring of RollingUpdateOpFactory...\n. No. The eventType is rollingUpdatedFinished both for rollingUpdateDone and rollingUpdateFailed\n. For what objects?\nA DeploymentGroupEvent class? Probably. But I'd rather do that in a separate commit.\nOr specific classes for specific events?\n. add comment or rename to make it clear that this is a percentage value?\n. Why @Nullable Integer and not just an int? Using @DefaultValue to set a default seems nicer, alternatively you can let it be 0 for existing deployment groups. This should be safe since we do a greater-than comparison (as opposed to a >=). x > 0 is only true if x is non-zero, which means one host failed -- which is the current behaviour.\n. Looks like spurious changes with no changed behaviour in this file. Necessary?\n. in the above ^ if block a task failed event should be emitted if failedTarget is non-null, right?\n. Not sure exactly how it'd look (not sure a class hierachy makes sense, especially since things like failedTask make no sense what-so-ever for rollout-task events), but passing in a metadata Map<String, Object> is surely kludgy.\n. But it will be null (for existing deployment groups that are currently being rolled out). What do you do if it's null?\n. Note: It being 0 is fine -- it just means that any host failing will cause the DG to fail.\n. The result is the same. I have no strong opinions on how to get there.\n. Calling addMetadata() will throw an exception unless you called setMetadata() (with a mutable map) first, because p.metadata defaults to emptyMap() which is immutable.\nmetadata in the builder should probably default to Maps.newHashMap().\nsetMetadata() should make a copy of the input map.\n. ...so that cases where you call setMetadata with an immutable map and then call addMetadata will still work.\n. There's a argToStringMap method in com.spotify.helios.cli.Utils that looks similar, AFAICT. Would be nice to kill off one of them maybe. (I'm not very fond of argToStringMap).\n. Can you not test AgentZooKeeperRegistrar in isolation, without CLI and crap?\nAt a glance, agentService seems easily mockable (only stopAsync() is called). name and id should be as well.\n. maybe use a millisecond timestamp to be consistent with the timestamps reported in the task status history?\n. Date is itself not immutable (you can call e.g. setHour() on Date), so even if the reference remains unchanged this effectively makes Job mutable.\n. It's possible, but there's no recursive delete in ZK so first have to do a recursive list outside the transaction and then add a delete for each node to the transaction. This seemed like more work - and the recursive delete after the fact seemed good enough so I opted for being lazy.\n. If you want to make sure environment is always empty unless explicitly set, should you not do this in @After also?\n. Catch UnknownHostException here and emit a warning?\n. intentional?\n. Could use some minor formatting improvements. ServerAuthentication -> ServerAuthentication\n. My interpretation of http://www.ietf.org/rfc/rfc2617.txt is that realm must always be included, but the text isn't trivial to parse so I could be wrong.\n. almost :) e.toString will print the full class name, with the package. This has not been changed in this patch, just moved around but I can change it.\n. Indeed, but I'm not sure it adds much value?\n. AFAIK, we're always dealing with text payloads. That said I'm not sure we ever actually call Reponse.toString() anywhere. I changed it to use the decoded payload.\n. Presumably this will be used for loading plugins for the client as well - perhaps the ServerAuthenticationConfig name is unfortunate?\n. The AuthenticationPlugin interface doesn't really make sense anymore, with ClientAuthentication removed. Remove one layer here for the server-side as well? E.g. AuthenticationPlugin -> ServerAuthPlugin\n. Pardon the seemingly spurious changes -- this made sense one commit ago.\n. Class names could be shorter here. Perhaps just call it AuthenticationPlugin on both the server and client and let the name be overloaded?\n. It doesn't return a token per se -- it returns the value to be put in the Authorization header. Which typically includes things like the scheme-name.\ncurrentAuthorizationHeader() ?\n. Renewing the authorization can fail, which was the reason behind the fallback. The intent was to return the original response if we can't authenticate for some reason.\n. There was.\nSo, in an initial draft plugins had access to the CLI so that they could inject new parameters. In an effort to keep things smalled and focused, we dropped it for now. I still think it make sense though.\n. The returned future typically returns a 401 back to the user if things fail. Which may not be ideal.\nE.g. if you...\n- receive a 401\n- proceed to authenticate, but authentication fails\n- then the user will: get the first 401 back. and error message will be lost.\n. Any ideas for how to do that?\n. AuthProviderSelector.class --> CrtAuthProvider.class\n. If should return the current token (token) if there is one, otherwise it should return null. It should not block on getting a token.\n. spurious change?\n. :+1: \n. Maybe default the server name to something like \"heliosmaster\"?\n. We probably need to tweak the AuthProvider interface to work well with the stored token, or rethink something else.\nCurrently, with AuthProviderSelector, the first request is always sent without authorization. Upon getting a 401 back it will select an AuthProvider and proceed to call renewAuthorizationHeader(). Now, we could tweak that to only call renewAuthorizationHeader() if the currentAuthorizationHeader() returns null on the selected provider - BUT, if we do that and the current token is expired the request will failed. Is there a way to tell if a token is expired on the client-side?\n. The help seems wrong/outdated (--force-authentication doesn't exist).\n. Random note: I had a hard time figuring out that the AgentProxy.sign() method does the SHA1-hashing (as opposed to the content signer doing the hashing somewhere in the super class and then sending the hash for signing). It's not very apparent from the AgentProxy interface that it will hash and sign.\n. Maybe disable the use of client certs if user is null? It's conceivable that someone wants to use SSL without using client certificates for authentication.\n. Exactly how the signature is produced depends on the key type, so might be hard to be explicit unless you want to restrict the interface to specific key type.\n\"Upon receiving this request, the agent will look up the private key that\ncorresponds to the public key contained in key_blob. It will use this\nprivate key to sign the \"data\" and produce a signature blob using the\nkey type-specific method described in RFC 4253 section 6.6 \"Public Key\nAlgorithms\".\"\n. Explicitly being vague by removing the part about the SHA1 hash from the comment might actually make it clearer :)\n. synchronized is harmless but superfluous here\n. hashCode() should be regenerated to look at containerError as well\n. Using the ExpectedException rule makes this a bit nicer in my opinion. Look here for more info: http://junit.org/apidocs/org/junit/rules/ExpectedException.html\n. I guess this is a matter of opinion on style, but.... If these mocks are only used in a single test consider declaring them locally in that test.\nE.g. instead of having members just do final ImageInfo mockImageInfo = mock(ImageInfo.class) etc in the test method.\n. Hmm, I guess that might not necessarily work if you use the future, via throwing of the exception, as a synchronization point which it kind of looks like you do? :)\n. @negz: Yeah you can't use the @Mock annotation for that. It's simple to do nonetheless.\nJust do final MyClass myMock = Mockito.mock(MyClass.class); (bonus points for doing a static import of that function to make it a bit more terse`.\nThis is basically what \"@Mock\" does under the hoods.\n. HTTPS and the client-side certificates are two different things though, right? Having HTTPS-configured without using client-certs seems like a valid setup.\n. By design authentication cannot happen over an unencrypted channel :)\n. agentProxyOpt might be set even if there are no identities (e.g. agentProxy.list() threw an exception) but maybe that's intentional?\nIf so, I don't understand why we don't always use an AuthenticatingHttpConnector.\n. +1 We even had one in one of the ol' CRTAuth PRs: https://github.com/spotify/helios/pull/711/files#diff-891c4312a410a016e6bea1cc41bfbcceR31\n. I am very confused. It just looks like you're reordering a line. What problem does this fix and how?\n. Maybe rename to emitDeploymentGroupEvents? It's using a rather specific topic after all.\n. Could just pass in a mocked KafkaSender here\n. To me having to pass in the client is real ugly, but I couldn't really figure out a better way to do it. We need to pass it in so that we can do a exists() to check if the node exists or not. AFAIK there's no way to do a \"create-if-it-doesn't-exist\" operation in ZK.\n. An alternative way would be to try without the create first and then retry with the create operation if the first transaction fails, but that also seems ugly.\n. I care little about this, but... seems like the current pattern is to pass global options as parameters to the run() method (e.g. username, json). why not just follow that convention? or change it around so everything is accessed via options?\n. Seems a bit misleading to say that the rolling-update finished here. I'd interpret \"rolling-update finished\" as \"my job rolled out to all hosts\".\n. Rolling updates that are started when hosts change take a slightly different code path so those will not be logged with this change.\n. what is tls-agent?\n. I still find it somewhat confusing that these are logged before the transaction is successfully committed. This is intended to be racey so the transaction will fail on occasion.\n. It's no biggie, but I could see it being confusing if you the knowledge that these are expected to fail every once in a while.\n. Pass master-digest, agent-digest and the agent password? You could, but it's redundant since the agent-digest can be computed from the agent password.\n. That is so frigging long though :disappointed: \n. If you have a good idea of how to do it, please share. I can move it to another place to reduce clutter, but I can't really figure out a great way to reduce duplication. \n. An empty password is valid, no? Stupid, but valid.\n. It's an annoying function -- it throws a [checked] java.security.NoSuchAlgorithmException exception that has to be dealt with. I can use it if you feel strongly about it, but if so I'll probably wrap it in a function that re-throws that exception as a runtime exception. \n. Done.\n. Not 100% sure, but it looks like it's possible without code changes: https://cwiki.apache.org/confluence/display/ZOOKEEPER/ZooKeeper+SSL+User+Guide\n(if so, then the current doc seems a bit misleading...)\n. initializeAclRecursive() seems rather specific, and only has one caller -- why not keep this code in the ZooKeeperAclInitializer?\n(Thought I already added a comment about this yesterday but can't find it anywhere today...)\n. contains() != equals. It uses equals for a reason. If the expected value is [acl1] we don't want the test to pass if the actual value is [acl1, acl2].\n. I commented this line out in my branch for some reason that I forgot, but I doubt it was meant to stay commented out...\n. IIRC, I had to comment it out because doing a pull() would override the latest tag but I could be remembering that incorrectly.\n. The comment above is now on the wrong method\n. check for empty/null agent password\n. nitpick: There's an extra space after the =\n. Why?\n. Nope -- this, that the service doesn't need to be started, is now also covered by the test.\n. My idea for \"disabling\" reaping was to set the reaping timeout to something like 999 days. That ought to be good enough?\n. Do you really think we need to be able to change how often that code is run? I don't seem the practical value in tweaking some internal, rather inconsequential value via arguments. Adding CLI args for it seems like it would just add clutter.\n. The only good reason I can think of for adding those CLI args is to set the interval low for running integration tests.\n. Done.\n. Done.\n. Well, hmm, maybe you do need to start it actually. Let me investigate...\n. Okay, yeah. It needs to be started :)\n. :/\n. Well that's unfortunate.\n. Maybe just remove that whole paragraph since it is indeed covered below?\n. The reason that I put it there is that I wanted to state that Helios doesn't offer a complete authentication solution early on in the documentation.\n. Reworded it a bit but left it in there.\n. Yeah, but it's tricky given that it depends on whether you're using the CLI or the API. For the API you have to set the user explicitly, and for the CLI it comes from the --username arg which defaults to System.getProperty(\"user.name\").\n. Tricky is the wrong word -- but explaining it is quite lengthy and wordy. Still worth it?\n. I added a similar sentence below the actual list - if you approve?\n. @davidxia @rohansingh Do you have the commands we typically use to generate a cert available somewhere for me to copy-paste?\nMight be nice to put a complete example in there. \"Generate a cert/key pair by doing X. Then create your HeliosClient like this ...\"\n. That only creates an unsigned cert, no? What about the (self-)signing part?\n. Thinking about it a little more, it probably doesn't make sense to give an exampled detail here -- how you should do it will depend on the specific setup. E.g. we use self-signed certs and maintain a list of trusted certs, but other's might have proper PKI setup.\n. why was this needed? otherwise lgtm\n. HealthCheckGauge should probably be used here too.\n. This is irrelevant for the function of the code but I would probably just have made this an inner class^Winterface of ZooKeeperModelReporter to reduce clutter - given how \"big\" this interface is :)\n. We shouldn't use a real timer, that actually gathers and keeps data, for a \"no op\" implementation. Unfortunately Timer isn't an interface so we can't really return a no-op timer...\n. If you changed this interface slightly you could do an actual no-op implementation. E.g. something like:\nvoid updateTimer(String name, final long durationMillis);\n. Eh... isn't this a bit too verbose?\n. The general pattern seems to be that DefaultZooKeeperClient does not log anything at all, sans failed cluster-id checks. Instead the caller is responsible for logging, which I think makes sense since whether an exception is a failure or merely expected behaviour depends.\n. nitpick: it's no longer 2014 :)\n. nitpick: 2014 -> 2016\n. this doesn't really matter but why not use longs for the durations?\n. I'm slightly curious why you call this forDispatcher as opposed to just builder\n. Hmm. This sounds like admitting defeat. \"It doesn't work, so let's do it again!\" Not sure I like it -- undeploying a second time because the first undeploy failed for reasons unknown.\n. (But I don't have a big problem with this, it's fine -- waiting for a job be undeployed when without calling undeploy (defensively) seems like a worse option :))\n. The await should be outside the if statement, right?\nI.e. the job might previously have been told to undeploy but still be running. (Since undeploy doesn't wait for job to stop)\n. why suppress rather than fix these?\n. \"... inside tear it down\" missing an and?\n. Seems reasonable but can't really comment much on this ^\n. spurious change, or intentional?\n. For port mappings and registrations, it seems like we merge the values if they come from more than one source. Perhaps do the same here for consistency?\n. If you're cleaning up, maybe remove <logger name=\"com.spotify.helios.client\" level=\"info\"/> as well then?\n. [Under the assumption that the last good rolling operation == the last op that succeeded]\nIMO, hosts changing should cause the current target version to be deployed, not the last succeesfully deployed version.\nFor example, if I deploy version B but the rollout fails on some random host. Then when I add hosts I want version B to be deployed on them, not some older version A.\n. This log message seems a little bit confusing, maybe just get rid of it. It's hard to know exactly what \"agent has registered with ZooKeeper\" means\n. nitpick: Is there any benefit to using hostStatus(String) over hostStatus(List<String>)? this doesn't matter i just personally don't like .get(0) if it can be avoided :)\n. nitpick: this still refers to a singular host\n. echo -e expands \\n and so does printf (plain echo does not)\n. nitpick: mixed tabs and spaces here (and I'm probably to blame for it!)\n. nice!\n. If we're making breaking changes already, I think it'd make sense to split this class up into two parts:\n- One class with all the helios logic, but without any JUnit stuff. Make it AutoCloseable and usable when you don't use JUnit.\n- One simple class that adds the JUnit fluff.\n. a) you can use try-with-resources, b) you're no longer bound to throwing check exceptions deriving from IOException\n. The coupling between DefaultDeployer and TemporaryJobsResource is pretty weird.\nNote that TemporaryJobs is currently not usable without TemporaryJobsResource (== I think this commit needs some more work). If you try to use TemporaryJobs with TemporaryJobsResource then Deployer.readyToDeploy() will not be called, which means that TemporaryJobs.deploy() will throw an exception.\n. IMO it'd make more sense to move the singleton to either a) the HeliosSoloDeployment class, or b) it's own class or such.\n. > If TemporaryJobs now managed HeliosSoloDeployment, is it intended or accidental that it doesn't close the HSD instance in TemporaryJobs.close()?\nIntended, if you ask me. I think it makes sense to decouple the lifetime of the HeliosDeployment from the lifetime of the TemporaryJobs instance.\nFor example, you might want to reuse HeliosDeployment across multiple tests, but clean up jobs after each of them. I think it's fair to make the user responsible for closing the HeliosDeployment.\n. I'd like to deconstruct TemporaryJobs even more, right now it serves two purposes:\n- It glues everything together and is the place where you provide configuration, and\n- It keeps tracks of created jobs and undeploys them when close() is called\nWhy not separate those responsibilities out into separate classes (that are hopefully easier to reason about on their own)? E.g. a separate collection-of-temporary-jobs-that-takes-care-undeploying and a temporary-job-factory.\n. Maybe only trigger roling-updates if the current state is FAILED or SUCCEEDED (not ROLLING_OUT)?\nIf I understand correctly, you can now time things to get perhaps unexpected results:\n- trigger a manual rolling-update of a bad, broken job\n- immediately after, before the rolling-update fails, a new hosts is added\n- a roll out (with reason=HOSTS_CHANGED) is triggered and fails\n- more hosts are added...\n- this triggers a rolling-update because the last reason had reason=HOSTS_CHANGED\n. (This probably doesn't matter in practice)\n. This PR also changes behaviour slightly in that a DG can go from FAILED to ACTIVE without user-intervention.\n. nitpick: maybe add a comment explaining that this will be re-run and the hosts updated on the next iteration.\n. I think there's still a race condition here, e.g.:\n- Get DG and DG status [in the context of this code]\n- Manual rolling-update triggered [concurrently with this code running]\n- updateOnHostChange() is run (with stale data), and returns true (would have returned false if it had the most up to date data)\n- --> Current [manual] rolling-update is overwritten\nYou could add a ZK check() in the transaction to ensure that the current version of the DG config and/or status node has not changed if you want to avoid the race.\n. I don't have a strong opinion.\n. I believe this will throw a NoNodeException if the nodes does not exist. You probably want to catch and act on that. The node does not exist --> status will be null below.\n. I think that checking only the config node is enough to avoid the race condition.\n. Yes... but is that what you want? allowHostChange() returns true if status is null, allowing a roll out to be started if there is not known status. With the exception thrown, status can never be null --> roll outs will never be triggered when the status is unknown.\n. What's up with all the \"patterns\"?\nThe left hand side is always a label, no \"patterns\" allowed. The right hand side is always an operand. \n. s/labels/hostSelectors\n. The code would be less confusing to read if you used List<HostSelector> instead of List<String> here -- but I see how that could be tricky if you want to send them as query parameters (since host selectors need to encoded as json).\n. Although a query parameter like %7B%22label%22%3A%22foo%22%2C%22operator%22%2C%22EQUALS%22%2C%22operand%22%2C%22bar%22%7D isn't terrible unless a human is supposed to craft it :)\n. I am a-okay with only having List<String>. Functionality-wise it makes sense. It just makes the code a tad harder to follow.\nUsing unparsed host selectors is also a lot more friendly to the user if you do a request with curl.\n. Does -s site=foo bar!=yes actually work? Don't you have to specify -s once for each selector? E.g. -s site=foo -s bar!=yes\n. note that HostSelector::parse returns null on bad input\nI'm guessing this will yield a 500 with no error message on bad input?\n. That's probably because you removed action(append()). With the current implementation specifying -l seems to work. OTOH only specifying -l once also seems to work.\nI just wonder how this interacts with positional arguments...\n. Rename to AWAIT_UNDEPLOYED? Stopping and undeploying jobs in helios has different meanings, see e.g. helios stop -h\n. These spurious import changes makes this a pain to review :(\n. What's the difference between FORCE_UNDEPLOY_JOBS and UNDEPLOY_OLD_JOBS? Would be nice it was documented somewhere (doesn't have to be here, but somewhere in the code).\n. What is a \"redundant deployment\"? (add javadoc plz)\n. The name of this method is pretty misleading -- as it can return true even if the deployment is not owned by the DG.\n. Perhaps personal preference but I think this code would be easier to read if you stayed away from early-returns. E.g.\n/**\n   * Returns {@code true} if the deployment is already in the desired state for the deployment\n   * group, meaning that:\n   *\n   * <ul>\n   *   <li>The deployment in owned by the given deployment group</li>\n   *   <li>The deployment is of the job that is the target for the deployment group</li>\n   *   <li>The goal of the deployment is {@code START}</li>\n   * </ul>\n   */\n  private boolean deploymentIsAtDesiredState(final Deployment deployment,\n                                             final DeploymentGroup deploymentGroup) {\n    // TODO: Break the first checkout out into it's own \"isOwnedByDepoloymentGroup()\" helper method\n    return Objects.equals(deployment.getDeploymentGroupName(), deploymentGroup.getName()) &&\n           deployment.getJobId().equals(deploymentGroup.getJobId()) &&\n           Goal.START.equals(deployment.getGoal());\n  }\n(note that the above example also reverses the logic, i.e. deploymentIsAtDesiredState(deployment, dg) == !redundantDeployment(deployment, dg))\n. This class could and should be unit-tested.\nWhat's the benefit of having two separate planners? Why just modify the existing planner to deal with removed hosts as well?\n. Undeploying the job == removing it from /config/hosts/<host>/jobs (see e.g.the undeployJob() mehtod), right?\nSo isnt' this branch always going to be true? I put a breakpoint on the line after this block, L958, and ran this in the debugger and it was never hit.\n. Agian, I think the name should be ...Undeployed rather than ...Stopped, since we want the job undeployed and not stopped.\nAlso I think this entire method could be simplified a lot:\nprivate RollingUpdateOp rollingUpdateAwaitUndeployed(final ZooKeeperClient client,\n                                                       final RollingUpdateOpFactory opFactory,\n                                                       final DeploymentGroup deploymentGroup,\n                                                       final String host) {\n    // Yield until the job is undeployed on the given host, or the rollout times out.\n    // Once the job is fully undeployed \"task status\" path for it, i.e.\n    // /status/hosts/<host>/job/<job-id>, will be removed so we use that to\n    // determine when the job has been undeployed.\n    final TaskStatus taskStatus = getTaskStatus(client, host, deploymentGroup.getJobId());\n    if (taskStatus == null) {\n      // The job is undeployed!\n      return opFactory.nextTask();\n    } else if (isRolloutTimedOut(client, deploymentGroup)) {\n        return opFactory.error(\"timed out while retrieving job status\", host,\n                               RollingUpdateError.TIMED_OUT_WAITING_FOR_JOB_TO_UNDEPLOY);\n    } else {\n      return opFactory.yield();\n    }\n  }\n(The DeploymentGroupTest system tests pass with this change)\n. Do you expect this to happen? If not, maybe make it an opFactory.error() instead?\nIt's not obvious to me that carrying on like normal in case of errors results in correct behaviour.\n. What does force do? (add javadoc plz)\n. I find it quite hard to understand what these node store, at a glance. Is it all removed hosts, as the name suggests?\nOr is it actually hosts-pending-undeploy-of-dg-job?\nMaybe document what is stored in them, or name it to reflect the contents better?\n. :ok_hand: \n. You have inverted the logic here -- which I assume was unintended?. Looks like perhaps this entire file should be left as-is.. Why change this logic at all?\nAFAICT this is just doing the same thing as before, but adds a new method to do it.. it does. Consider rewording this to talk about which interface to listen on. E.g. \"... and specifies the IP of the interface to expose the port on.\". Is this public API that's used anywhere? If yes, add an overload that uses a default ip. Can only see PortMapping(final int internalPort, final Integer externalPort) and PortMapping(final int internalPort) - but on second thought it seems unlikely that is used apart from json parsing. I think it's perfectly fine as it is now. Personally, I find it clearer if you speak about interfaces but that's just me.\nFor inspiration, this is what the unbound docs say:\n```\n       interface: \n              Interface  to  use  to connect to the network. This interface is\n              listened to for queries from clients, and answers to clients are\n              given  from  it.  Can be given multiple times to work on several\n              interfaces. If none are given the default is to listen to local-\n              host.   The  interfaces  are not changed on a reload (kill -HUP)\n              but only on restart.  A port number can be specified with  @port\n              (without spaces between interface and port number), if not spec-\n              ified the default port (from port) is used.\n   ip-address: <ip address[@port]>\n          Same as interface: (for easy of compatibility with nsd.conf).\n\n```\nand nginx (no mention of interfaces here!):\nSyntax: listen address[:port] ...\n...\nSets the address and port for IP, or the path for a UNIX-domain socket on which the server will accept requests. Both address and port, or only address or only port can be specified. An address may also be a hostname, for example:. nitpick: you could just make this local (it's used in single place afaict). Perhaps mention that this also effectively makes the rollout \"fully parallel\"? Specifying --ignore--failures will, aside from ignoring errors, also cause similar behaviours as when using e.g. --par 10000. nitpick: I think it would make more sense to have this be of type GoogleCredentials and load the credential file elsewhere (e.g. in AgentParser).\nThat would make it easier to add a flag like --docker-gcp-use-application-default-credentials in the future if we want to do that.. Derp.\nThat seems less than ideal. For example, if I have a build agent in GCP that both pulls and pushes images to GCR using something like dockerfile-maven-plguin. Then I can't use the SA that is associated with the VM? Instead I must create a SA key and add glue to make sure it's loaded and used?. unused import? :). This is all copy-pasta from VolumeTest\nIdeas for better ways to test this appreciated. helios-client still uses Java 7 - no streams allwoed :|. I don't think migrate and token make sense to allow in a job.. ",
    "codecov-io": "Current coverage is 47.06%\n\nMerging #384 into master will increase coverage by +1.22% as of 97ef6bb\n\ndiff\n@@            master    #384   diff @@\n======================================\n  Files          263     263        \n  Stmts        11153   12456   +1303\n  Branches      1553    1585     +32\n  Methods          0       0        \n======================================\n+ Hit           5113    5862    +749\n- Partial        446     449      +3\n- Missed        5594    6145    +551\n\nReview entire Coverage Diff as of 97ef6bb\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 43.55%\nMerging #769 into master will increase coverage by +0.17% as of d8bdaae\n\ndiff\n@@            master    #769   diff @@\n======================================\n  Files          257     261     +4\n  Stmts        10760   10956   +196\n  Branches      1508    1523    +15\n  Methods          0       0       \n======================================\n+ Hit           4668    4772   +104\n- Partial        417     431    +14\n- Missed        5675    5753    +78\n\nReview entire Coverage Diff as of d8bdaae\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 43.80%\nMerging #780 into master will increase coverage by +0.27% as of 6dd6567\n\ndiff\n@@            master    #780   diff @@\n======================================\n  Files          257     260     +3\n  Stmts        10735   10849   +114\n  Branches      1500    1510    +10\n  Methods          0       0       \n======================================\n+ Hit           4674    4752    +78\n- Partial        416     426    +10\n- Missed        5645    5671    +26\n\nReview entire Coverage Diff as of 6dd6567\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 42.38%\nMerging #781 into master will decrease coverage by -0.54% as of aa8f943\n\ndiff\n@@            master    #781   diff @@\n======================================\n  Files          256     256       \n  Stmts        10734   10734       \n  Branches      1504    1504       \n  Methods          0       0       \n======================================\n- Hit           4608    4550    -58\n+ Partial        412     409     -3\n- Missed        5714    5775    +61\n\nReview entire Coverage Diff as of aa8f943\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 42.93%\nMerging #782 into master will increase coverage by +0.01% as of 68f9864\n\ndiff\n@@            master    #782   diff @@\n======================================\n  Files          256     256       \n  Stmts        10734   10738     +4\n  Branches      1504    1505     +1\n  Methods          0       0       \n======================================\n+ Hit           4608    4610     +2\n- Partial        412     413     +1\n- Missed        5714    5715     +1\n\nReview entire Coverage Diff as of 68f9864\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 42.96%\nMerging #783 into master will increase coverage by +0.04% as of 3278cdd\n\ndiff\n@@            master    #783   diff @@\n======================================\n  Files          256     256       \n  Stmts        10734   10734       \n  Branches      1504    1504       \n  Methods          0       0       \n======================================\n+ Hit           4608    4612     +4\n+ Partial        412     411     -1\n+ Missed        5714    5711     -3\n\nReview entire Coverage Diff as of 3278cdd\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 43.11%\nMerging #784 into master will increase coverage by +0.14% as of 42e053f\n\ndiff\n@@            master    #784   diff @@\n======================================\n  Files          257     257       \n  Stmts        10756   10725    -31\n  Branches      1507    1499     -8\n  Methods          0       0       \n======================================\n+ Hit           4622    4624     +2\n+ Partial        413     411     -2\n+ Missed        5721    5690    -31\n\nReview entire Coverage Diff as of 42e053f\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 42.99%\nMerging #785 into master will increase coverage by +0.02% as of fe0c351\n\ndiff\n@@            master    #785   diff @@\n======================================\n  Files          257     257       \n  Stmts        10756   10757     +1\n  Branches      1507    1507       \n  Methods          0       0       \n======================================\n+ Hit           4622    4625     +3\n- Partial        413     414     +1\n+ Missed        5721    5718     -3\n\nReview entire Coverage Diff as of fe0c351\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 43.42%\nMerging #786 into master will decrease coverage by -0.10% as of 5e86197\n\ndiff\n@@            master    #786   diff @@\n======================================\n  Files          257     257       \n  Stmts        10730   10765    +35\n  Branches      1500    1508     +8\n  Methods          0       0       \n======================================\n+ Hit           4670    4675     +5\n- Partial        415     417     +2\n- Missed        5645    5673    +28\n\nReview entire Coverage Diff as of 5e86197\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 43.66%\nMerging #787 into dxia/drd-tests will increase coverage by +0.13% as of 6a8c4f6\n\ndiff\n@@            dxia/drd-tests    #787   diff @@\n==============================================\n  Files                  260     260       \n  Stmts                10824   10868    +44\n  Branches              1512    1516     +4\n  Methods                  0       0       \n==============================================\n+ Hit                   4712    4745    +33\n- Partial                422     424     +2\n- Missed                5690    5699     +9\n\nReview entire Coverage Diff as of 6a8c4f6\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 44.01%\nMerging #790 into master will increase coverage by +0.05% as of 849431d\n\ndiff\n@@            master    #790   diff @@\n======================================\n  Files          264     264       \n  Stmts        11045   11045       \n  Branches      1525    1525       \n  Methods          0       0       \n======================================\n+ Hit           4856    4861     +5\n+ Partial        442     440     -2\n+ Missed        5747    5744     -3\n\nReview entire Coverage Diff as of 849431d\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 43.93%\nMerging #791 into master will decrease coverage by -0.33% as of 9626a09\n\ndiff\n@@            master    #791   diff @@\n======================================\n  Files          264     264       \n  Stmts        11088   11048    -40\n  Branches      1528    1525     -3\n  Methods          0       0       \n======================================\n- Hit           4908    4854    -54\n- Partial        440     442     +2\n- Missed        5740    5752    +12\n\nReview entire Coverage Diff as of 9626a09\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 44.09%\nMerging #793 into master will increase coverage by +0.12% as of 8f9decf\n\ndiff\n@@            master    #793   diff @@\n======================================\n  Files          264     264       \n  Stmts        11045   11052     +7\n  Branches      1525    1527     +2\n  Methods          0       0       \n======================================\n+ Hit           4857    4873    +16\n  Partial        442     442       \n+ Missed        5746    5737     -9\n\nReview entire Coverage Diff as of 8f9decf\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 44.26%\nMerging #797 into master will not affect coverage as of 807c959\n\ndiff\n@@            master    #797   diff @@\n======================================\n  Files          264     264       \n  Stmts        11088   11090     +2\n  Branches      1528    1528       \n  Methods          0       0       \n======================================\n+ Hit           4908    4909     +1\n- Partial        440     442     +2\n+ Missed        5740    5739     -1\n\nReview entire Coverage Diff as of 807c959\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 44.94%\nMerging #798 into master will increase coverage by +3.91% as of e64dad0\n\ndiff\n@@            master    #798   diff @@\n======================================\n  Files          113     261    +148\n  Stmts         4562   10937   +6375\n  Branches       840    1522    +682\n  Methods          0       0        \n======================================\n+ Hit           1872    4916   +3044\n- Partial        200     441    +241\n- Missed        2490    5580   +3090\n\nReview entire Coverage Diff as of e64dad0\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 44.98%\nMerging #799 into master will increase coverage by +0.72% as of 845b590\n\ndiff\n@@            master    #799   diff @@\n======================================\n  Files          264     260     -4\n  Stmts        11090   10909   -181\n  Branches      1528    1517    -11\n  Methods          0       0       \n======================================\n- Hit           4909    4907     -2\n  Partial        441     441       \n+ Missed        5740    5561   -179\n\nReview entire Coverage Diff as of 845b590\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 44.89%\nMerging #800 into master will decrease coverage by -0.03% as of 2b0357e\n\ndiff\n@@            master    #800   diff @@\n======================================\n  Files          261     261       \n  Stmts        10940   10940       \n  Branches      1522    1522       \n  Methods          0       0       \n======================================\n- Hit           4915    4911     -4\n- Partial        442     443     +1\n- Missed        5583    5586     +3\n\nReview entire Coverage Diff as of 2b0357e\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 44.93%\nMerging #801 into master will increase coverage by +0.03% as of d2b4e83\n\ndiff\n@@            master    #801   diff @@\n======================================\n  Files          261     261       \n  Stmts        10940   10940       \n  Branches      1522    1522       \n  Methods          0       0       \n======================================\n+ Hit           4913    4916     +3\n+ Partial        444     442     -2\n+ Missed        5583    5582     -1\n\nReview entire Coverage Diff as of d2b4e83\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 44.95%\nMerging #802 into master will increase coverage by +0.03% as of d93dc0b\n\ndiff\n@@            master    #802   diff @@\n======================================\n  Files          261     261       \n  Stmts        10940   10941     +1\n  Branches      1522    1522       \n  Methods          0       0       \n======================================\n+ Hit           4915    4919     +4\n- Partial        441     443     +2\n+ Missed        5584    5579     -5\n\nReview entire Coverage Diff as of d93dc0b\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 45.68%\nMerging #803 into master will increase coverage by +4.80% as of a308b56\n\ndiff\n@@            master    #803   diff @@\n======================================\n  Files          114     262    +148\n  Stmts         4589   11024   +6435\n  Branches       845    1531    +686\n  Methods          0       0        \n======================================\n+ Hit           1876    5036   +3160\n- Partial        202     437    +235\n- Missed        2511    5551   +3040\n\nReview entire Coverage Diff as of a308b56\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 44.89%\nMerging #805 into master will increase coverage by +0.01% as of 697db7e\n\ndiff\n@@            master    #805   diff @@\n======================================\n  Files          261     261       \n  Stmts        10941   10941       \n  Branches      1522    1522       \n  Methods          0       0       \n======================================\n+ Hit           4911    4912     +1\n+ Partial        445     443     -2\n- Missed        5585    5586     +1\n\nReview entire Coverage Diff as of 697db7e\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 45.01%\nMerging #806 into master will increase coverage by +0.13% as of 44a7f92\n\ndiff\n@@            master    #806   diff @@\n======================================\n  Files          261     261       \n  Stmts        10941   10926    -15\n  Branches      1522    1521     -1\n  Methods          0       0       \n======================================\n+ Hit           4911    4918     +7\n+ Partial        445     441     -4\n+ Missed        5585    5567    -18\n\nReview entire Coverage Diff as of 44a7f92\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 44.93%\nMerging #808 into master will decrease coverage by -0.04% as of 898f2e6\n\ndiff\n@@            master    #808   diff @@\n======================================\n  Files          261     259     -2\n  Stmts        10925   10852    -73\n  Branches      1521    1511    -10\n  Methods          0       0       \n======================================\n- Hit           4914    4876    -38\n+ Partial        443     430    -13\n+ Missed        5568    5546    -22\n\nReview entire Coverage Diff as of 898f2e6\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 44.89%\nMerging #809 into master will decrease coverage by -0.07% as of 1f6ceef\n\ndiff\n@@            master    #809   diff @@\n======================================\n  Files          261     261       \n  Stmts        10926   10941    +15\n  Branches      1521    1522     +1\n  Methods          0       0       \n======================================\n- Hit           4913    4912     -1\n- Partial        442     443     +1\n- Missed        5571    5586    +15\n\nReview entire Coverage Diff as of 1f6ceef\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 44.97%\nMerging #810 into master will increase coverage by +0.01% as of cb80a69\n\ndiff\n@@            master    #810   diff @@\n======================================\n  Files          261     261       \n  Stmts        10926   10925     -1\n  Branches      1521    1521       \n  Methods          0       0       \n======================================\n  Hit           4913    4913       \n  Partial        442     442       \n+ Missed        5571    5570     -1\n\nReview entire Coverage Diff as of cb80a69\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 44.97%\nMerging #811 into master will not affect coverage as of 676e15e\n\ndiff\n@@            master    #811   diff @@\n======================================\n  Files          261     261       \n  Stmts        10925   10925       \n  Branches      1521    1521       \n  Methods          0       0       \n======================================\n- Hit           4914    4913     -1\n+ Partial        443     442     -1\n- Missed        5568    5570     +2\n\nReview entire Coverage Diff as of 676e15e\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 44.90%\nMerging #812 into master will increase coverage by +0.02% as of d43c410\n\ndiff\n@@            master    #812   diff @@\n======================================\n  Files          259     259       \n  Stmts        10852   10852       \n  Branches      1511    1511       \n  Methods          0       0       \n======================================\n+ Hit           4871    4873     +2\n  Partial        432     432       \n+ Missed        5549    5547     -2\n\nReview entire Coverage Diff as of d43c410\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 45.16%\nMerging #813 into master will increase coverage by +4.28% as of 60ac36d\n\ndiff\n@@            master    #813   diff @@\n======================================\n  Files          114     261    +147\n  Stmts         4589   10927   +6338\n  Branches       845    1522    +677\n  Methods          0       0        \n======================================\n+ Hit           1876    4935   +3059\n- Partial        202     440    +238\n- Missed        2511    5552   +3041\n\nReview entire Coverage Diff as of 60ac36d\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 44.93%\nMerging #814 into master will increase coverage by +4.05% as of 8731edb\n\ndiff\n@@            master    #814   diff @@\n======================================\n  Files          114     259    +145\n  Stmts         4589   10852   +6263\n  Branches       845    1511    +666\n  Methods          0       0        \n======================================\n+ Hit           1876    4876   +3000\n- Partial        202     432    +230\n- Missed        2511    5544   +3033\n\nReview entire Coverage Diff as of 8731edb\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 45.08%\nMerging #815 into master will increase coverage by +4.20% as of 4437849\n\ndiff\n@@            master    #815   diff @@\n======================================\n  Files          114     259    +145\n  Stmts         4589   10885   +6296\n  Branches       845    1512    +667\n  Methods          0       0        \n======================================\n+ Hit           1876    4907   +3031\n- Partial        202     431    +229\n- Missed        2511    5547   +3036\n\nReview entire Coverage Diff as of 4437849\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 45.84%\nMerging #819 into master will increase coverage by +0.10% as of ed59cb3\n\ndiff\n@@            master    #819   diff @@\n======================================\n  Files          262     262       \n  Stmts        11026   11026       \n  Branches      1532    1532       \n  Methods          0       0       \n======================================\n+ Hit           5044    5055    +11\n+ Partial        438     437     -1\n+ Missed        5544    5534    -10\n\nReview entire Coverage Diff as of ed59cb3\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 45.76%\nMerging #822 into master will not affect coverage as of e8eca7c\n\ndiff\n@@            master    #822   diff @@\n======================================\n  Files          262     262       \n  Stmts        11026   11030     +4\n  Branches      1532    1534     +2\n  Methods          0       0       \n======================================\n+ Hit           5046    5048     +2\n- Partial        437     440     +3\n+ Missed        5543    5542     -1\n\nReview entire Coverage Diff as of e8eca7c\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 45.77%\nMerging #825 into master will increase coverage by +0.02% as of f865b69\n\ndiff\n@@            master    #825   diff @@\n======================================\n  Files          262     262       \n  Stmts        11036   11030     -6\n  Branches      1538    1534     -4\n  Methods          0       0       \n======================================\n  Hit           5049    5049       \n  Partial        439     439       \n+ Missed        5548    5542     -6\n\nReview entire Coverage Diff as of f865b69\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 45.83%\nMerging #826 into master will increase coverage by +0.08% as of 21abe5f\n\ndiff\n@@            master    #826   diff @@\n======================================\n  Files          262     262       \n  Stmts        11036   11032     -4\n  Branches      1538    1535     -3\n  Methods          0       0       \n======================================\n+ Hit           5049    5056     +7\n  Partial        439     439       \n+ Missed        5548    5537    -11\n\nReview entire Coverage Diff as of 21abe5f\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 45.92%\nMerging #827 into master will increase coverage by +0.15% as of 5a11ba4\n\ndiff\n@@            master    #827   diff @@\n======================================\n  Files          262     263     +1\n  Stmts        11038   11101    +63\n  Branches      1539    1548     +9\n  Methods          0       0       \n======================================\n+ Hit           5053    5098    +45\n- Partial        441     444     +3\n- Missed        5544    5559    +15\n\nReview entire Coverage Diff as of 5a11ba4\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 45.76%\nMerging #829 into master will increase coverage by +0.05% as of f349b78\n\ndiff\n@@            master    #829   diff @@\n======================================\n  Files          262     262       \n  Stmts        11038   11038       \n  Branches      1539    1539       \n  Methods          0       0       \n======================================\n+ Hit           5046    5052     +6\n+ Partial        441     439     -2\n+ Missed        5551    5547     -4\n\nReview entire Coverage Diff as of f349b78\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 45.76%\nMerging #831 into master will decrease coverage by -0.01% as of c2f8927\n\ndiff\n@@            master    #831   diff @@\n======================================\n  Files          262     262       \n  Stmts        11038   11047     +9\n  Branches      1539    1541     +2\n  Methods          0       0       \n======================================\n+ Hit           5053    5056     +3\n+ Partial        441     440     -1\n- Missed        5544    5551     +7\n\nReview entire Coverage Diff as of c2f8927\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 45.82%\nMerging #832 into master will increase coverage by +0.05% as of ee1d5c2\n\ndiff\n@@            master    #832   diff @@\n======================================\n  Files          262     262       \n  Stmts        11038   11052    +14\n  Branches      1539    1542     +3\n  Methods          0       0       \n======================================\n+ Hit           5053    5065    +12\n+ Partial        441     440     -1\n- Missed        5544    5547     +3\n\nReview entire Coverage Diff as of ee1d5c2\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 45.67%\nMerging #833 into master will decrease coverage by -0.29% as of ce4a65c\n\ndiff\n@@            master    #833   diff @@\n======================================\n  Files          263     262     -1\n  Stmts        11124   11059    -65\n  Branches      1553    1535    -18\n  Methods          0       0       \n======================================\n- Hit           5113    5051    -62\n+ Partial        442     439     -3\n  Missed        5569    5569\n\nReview entire Coverage Diff as of ce4a65c\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 45.91%\nMerging #837 into master will increase coverage by +5.12% as of 38378b1\n\ndiff\n@@            master    #837   diff @@\n======================================\n  Files          114     263    +149\n  Stmts         4604   11157   +6553\n  Branches       850    1556    +706\n  Methods          0       0        \n======================================\n+ Hit           1878    5123   +3245\n- Partial        205     446    +241\n- Missed        2521    5588   +3067\n\nReview entire Coverage Diff as of 38378b1\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 45.83%\nMerging #838 into master will decrease coverage by -0.06% as of 4e862ac\n\ndiff\n@@            master    #838   diff @@\n======================================\n  Files          263     263       \n  Stmts        11157   11152     -5\n  Branches      1556    1553     -3\n  Methods          0       0       \n======================================\n- Hit           5120    5111     -9\n+ Partial        446     442     -4\n- Missed        5591    5599     +8\n\nReview entire Coverage Diff as of 4e862ac\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.00%\nMerging #840 into master will decrease coverage by -0.13% as of a097504\n\ndiff\n@@            master    #840   diff @@\n======================================\n  Files          265     264     -1\n  Stmts        12583   12468   -115\n  Branches      1594    1586     -8\n  Methods          0       0       \n======================================\n- Hit           5931    5860    -71\n+ Partial        451     447     -4\n+ Missed        6201    6161    -40\n\nReview entire Coverage Diff as of a097504\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.17%\nMerging #841 into master will decrease coverage by -0.27% as of 970e8ba\n\ndiff\n@@            master    #841   diff @@\n======================================\n  Files          266     267     +1\n  Stmts        12535   12558    +23\n  Branches      1596    1601     +5\n  Methods          0       0       \n======================================\n- Hit           5947    5924    -23\n- Partial        450     451     +1\n- Missed        6138    6183    +45\n\nReview entire Coverage Diff as of 970e8ba\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.04%\nMerging #842 into master will increase coverage by +0.09% as of 358f6d9\n\ndiff\n@@            master    #842   diff @@\n======================================\n  Files          263     263       \n  Stmts        12456   12460     +4\n  Branches      1585    1585       \n  Methods          0       0       \n======================================\n+ Hit           5849    5862    +13\n+ Partial        446     443     -3\n+ Missed        6161    6155     -6\n\nReview entire Coverage Diff as of 358f6d9\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.02%\nMerging #843 into master will increase coverage by +0.07% as of b45c6dd\n\ndiff\n@@            master    #843   diff @@\n======================================\n  Files          263     263       \n  Stmts        12460   12460       \n  Branches      1585    1585       \n  Methods          0       0       \n======================================\n+ Hit           5851    5859     +8\n+ Partial        448     446     -2\n+ Missed        6161    6155     -6\n\nReview entire Coverage Diff as of b45c6dd\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 46.99%\nMerging #844 into master will decrease coverage by -0.05% as of 60eae32\n\ndiff\n@@            master    #844   diff @@\n======================================\n  Files          263     263       \n  Stmts        12460   12460       \n  Branches      1585    1585       \n  Methods          0       0       \n======================================\n- Hit           5862    5856     -6\n- Partial        444     446     +2\n- Missed        6154    6158     +4\n\nReview entire Coverage Diff as of 60eae32\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.08%\nMerging #846 into master will increase coverage by +0.09% as of d675660\n\ndiff\n@@            master    #846   diff @@\n======================================\n  Files          263     265     +2\n  Stmts        12460   12579   +119\n  Branches      1585    1594     +9\n  Methods          0       0       \n======================================\n+ Hit           5855    5923    +68\n- Partial        448     450     +2\n- Missed        6157    6206    +49\n\nReview entire Coverage Diff as of d675660\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.39%\nMerging #847 into master will increase coverage by +0.12% as of c7c490c\n\ndiff\n@@            master    #847   diff @@\n======================================\n  Files          268     269     +1\n  Stmts        12583   12625    +42\n  Branches      1598    1605     +7\n  Methods          0       0       \n======================================\n+ Hit           5949    5983    +34\n- Partial        450     458     +8\n  Missed        6184    6184\n\nReview entire Coverage Diff as of c7c490c\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.54%\nMerging #848 into master will decrease coverage by -0.05% as of 60ed1f6\n\ndiff\n@@            master    #848   diff @@\n======================================\n  Files          270     270       \n  Stmts        12696   12730    +34\n  Branches      1612    1614     +2\n  Methods          0       0       \n======================================\n+ Hit           6043    6052     +9\n+ Partial        456     455     -1\n- Missed        6197    6223    +26\n\nReview entire Coverage Diff as of 60ed1f6\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.25%\nMerging #850 into master will increase coverage by +0.15% as of 1522546\n\ndiff\n@@            master    #850   diff @@\n======================================\n  Files          266     265     -1\n  Stmts        12591   12584     -7\n  Branches      1595    1595       \n  Methods          0       0       \n======================================\n+ Hit           5931    5947    +16\n- Partial        448     451     +3\n+ Missed        6212    6186    -26\n\nReview entire Coverage Diff as of 1522546\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.47%\nMerging #851 into master will increase coverage by +0.16% as of 3624391\n\ndiff\n@@            master    #851   diff @@\n======================================\n  Files          266     266       \n  Stmts        12592   12535    -57\n  Branches      1596    1596       \n  Methods          0       0       \n======================================\n- Hit           5958    5951     -7\n+ Partial        451     448     -3\n+ Missed        6183    6136    -47\n\nReview entire Coverage Diff as of 3624391\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.11%\nMerging #853 into master will decrease coverage by -0.03% as of 6f39e46\n\ndiff\n@@            master    #853   diff @@\n======================================\n  Files          267     267       \n  Stmts        12558   12558       \n  Branches      1601    1601       \n  Methods          0       0       \n======================================\n- Hit           5921    5917     -4\n- Partial        451     453     +2\n- Missed        6186    6188     +2\n\nReview entire Coverage Diff as of 6f39e46\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.15%\nMerging #855 into master will decrease coverage by -0.08% as of aef10df\n\ndiff\n@@            master    #855   diff @@\n======================================\n  Files          267     267       \n  Stmts        12558   12558       \n  Branches      1601    1601       \n  Methods          0       0       \n======================================\n- Hit           5932    5922    -10\n- Partial        452     454     +2\n- Missed        6174    6182     +8\n\nReview entire Coverage Diff as of aef10df\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.14%\nMerging #856 into master will increase coverage by +0.04% as of 6bf4bae\n\ndiff\n@@            master    #856   diff @@\n======================================\n  Files          267     267       \n  Stmts        12558   12558       \n  Branches      1601    1601       \n  Methods          0       0       \n======================================\n+ Hit           5915    5920     +5\n- Partial        450     451     +1\n+ Missed        6193    6187     -6\n\nReview entire Coverage Diff as of 6bf4bae\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.18%\nMerging #857 into master will decrease coverage by -0.06% as of 9ae8a82\n\ndiff\n@@            master    #857   diff @@\n======================================\n  Files          267     267       \n  Stmts        12558   12558       \n  Branches      1601    1601       \n  Methods          0       0       \n======================================\n- Hit           5933    5926     -7\n- Partial        452     456     +4\n- Missed        6173    6176     +3\n\nReview entire Coverage Diff as of 9ae8a82\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.08%\nMerging #859 into master will decrease coverage by -0.01% as of 577ba7f\n\ndiff\n@@            master    #859   diff @@\n======================================\n  Files          267     267       \n  Stmts        12558   12565     +7\n  Branches      1601    1601       \n  Methods          0       0       \n======================================\n+ Hit           5914    5916     +2\n- Partial        451     453     +2\n- Missed        6193    6196     +3\n\nReview entire Coverage Diff as of 577ba7f\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.13%\nMerging #860 into master will increase coverage by +0.04% as of 91c32ea\n\ndiff\n@@            master    #860   diff @@\n======================================\n  Files          267     267       \n  Stmts        12558   12565     +7\n  Branches      1601    1601       \n  Methods          0       0       \n======================================\n+ Hit           5914    5922     +8\n- Partial        451     452     +1\n+ Missed        6193    6191     -2\n\nReview entire Coverage Diff as of 91c32ea\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.21%\nMerging #862 into master will decrease coverage by -0.06% as of 8485c96\n\ndiff\n@@            master    #862   diff @@\n======================================\n  Files          268     268       \n  Stmts        12576   12576       \n  Branches      1598    1598       \n  Methods          0       0       \n======================================\n- Hit           5945    5938     -7\n+ Partial        451     449     -2\n- Missed        6180    6189     +9\n\nReview entire Coverage Diff as of 8485c96\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.18%\nMerging #863 into master will decrease coverage by -0.09% as of 6c9749a\n\ndiff\n@@            master    #863   diff @@\n======================================\n  Files          268     268       \n  Stmts        12576   12576       \n  Branches      1598    1598       \n  Methods          0       0       \n======================================\n- Hit           5945    5934    -11\n+ Partial        451     450     -1\n- Missed        6180    6192    +12\n\nReview entire Coverage Diff as of 6c9749a\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.25%\nMerging #864 into master will decrease coverage by -0.02% as of 72407ea\n\ndiff\n@@            master    #864   diff @@\n======================================\n  Files          268     268       \n  Stmts        12576   12576       \n  Branches      1598    1598       \n  Methods          0       0       \n======================================\n- Hit           5945    5943     -2\n+ Partial        453     450     -3\n- Missed        6178    6183     +5\n\nReview entire Coverage Diff as of 72407ea\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.27%\nMerging #865 into master will increase coverage by +0.03% as of 08f957c\n\ndiff\n@@            master    #865   diff @@\n======================================\n  Files          268     268       \n  Stmts        12576   12583     +7\n  Branches      1598    1598       \n  Methods          0       0       \n======================================\n+ Hit           5941    5948     +7\n+ Partial        449     448     -1\n- Missed        6186    6187     +1\n\nReview entire Coverage Diff as of 08f957c\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.28%\nMerging #866 into master will increase coverage by +0.04% as of a9e93ed\n\ndiff\n@@            master    #866   diff @@\n======================================\n  Files          268     268       \n  Stmts        12583   12583       \n  Branches      1598    1598       \n  Methods          0       0       \n======================================\n+ Hit           5945    5950     +5\n+ Partial        453     448     -5\n  Missed        6185    6185\n\nReview entire Coverage Diff as of a9e93ed\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.41%\nMerging #867 into master will increase coverage by +0.14% as of 1dda20c\n\ndiff\n@@            master    #867   diff @@\n======================================\n  Files          268     269     +1\n  Stmts        12583   12647    +64\n  Branches      1598    1605     +7\n  Methods          0       0       \n======================================\n+ Hit           5949    5997    +48\n  Partial        450     450       \n- Missed        6184    6200    +16\n\nReview entire Coverage Diff as of 1dda20c\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.01%\nMerging #868 into master will decrease coverage by -0.04% as of 835334a\n\ndiff\n@@            master    #868   diff @@\n======================================\n  Files          271     271       \n  Stmts        12805   12802     -3\n  Branches      1643    1642     -1\n  Methods          0       0       \n======================================\n- Hit           6153    6147     -6\n+ Partial        466     463     -3\n- Missed        6186    6192     +6\n\nReview entire Coverage Diff as of 835334a\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.03%\nMerging #870 into master will increase coverage by +0.44% as of 5a41af5\n\ndiff\n@@            master    #870   diff @@\n======================================\n  Files          270     271     +1\n  Stmts        12696   12718    +22\n  Branches      1612    1639    +27\n  Methods          0       0       \n======================================\n+ Hit           6043    6109    +66\n- Partial        456     464     +8\n+ Missed        6197    6145    -52\n\nReview entire Coverage Diff as of 5a41af5\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.55%\nMerging #871 into master will decrease coverage by -0.04% as of 3b43059\n\ndiff\n@@            master    #871   diff @@\n======================================\n  Files          270     270       \n  Stmts        12696   12696       \n  Branches      1612    1612       \n  Methods          0       0       \n======================================\n- Hit           6043    6038     -5\n- Partial        456     457     +1\n- Missed        6197    6201     +4\n\nReview entire Coverage Diff as of 3b43059\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.61%\nMerging #872 into master will increase coverage by +0.02% as of 83998f3\n\ndiff\n@@            master    #872   diff @@\n======================================\n  Files          270     270       \n  Stmts        12696   12730    +34\n  Branches      1612    1612       \n  Methods          0       0       \n======================================\n+ Hit           6043    6061    +18\n- Partial        456     457     +1\n- Missed        6197    6212    +15\n\nReview entire Coverage Diff as of 83998f3\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.61%\nMerging #873 into master will increase coverage by +0.02% as of 45ac54c\n\ndiff\n@@            master    #873   diff @@\n======================================\n  Files          270     270       \n  Stmts        12696   12703     +7\n  Branches      1612    1612       \n  Methods          0       0       \n======================================\n+ Hit           6043    6048     +5\n+ Partial        456     455     -1\n- Missed        6197    6200     +3\n\nReview entire Coverage Diff as of 45ac54c\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.59%\nMerging #874 into master will not affect coverage as of 11c3a69\n\ndiff\n@@            master    #874   diff @@\n======================================\n  Files          270     270       \n  Stmts        12703   12703       \n  Branches      1612    1612       \n  Methods          0       0       \n======================================\n  Hit           6046    6046       \n+ Partial        454     453     -1\n- Missed        6203    6204     +1\n\nReview entire Coverage Diff as of 11c3a69\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 47.99%\nMerging #875 into master will decrease coverage by -0.01% as of c9037df\n\ndiff\n@@            master    #875   diff @@\n======================================\n  Files          271     271       \n  Stmts        12725   12735    +10\n  Branches      1639    1642     +3\n  Methods          0       0       \n======================================\n+ Hit           6108    6112     +4\n+ Partial        466     465     -1\n- Missed        6151    6158     +7\n\nReview entire Coverage Diff as of c9037df\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.07%\nMerging #877 into master will increase coverage by +0.03% as of 5efe7b1\n\ndiff\n@@            master    #877   diff @@\n======================================\n  Files          271     271       \n  Stmts        12810   12810       \n  Branches      1645    1645       \n  Methods          0       0       \n======================================\n+ Hit           6155    6159     +4\n+ Partial        464     462     -2\n+ Missed        6191    6189     -2\n\nReview entire Coverage Diff as of 5efe7b1\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.12%\nMerging #878 into master will increase coverage by +0.03% as of e263cc3\n\ndiff\n@@            master    #878   diff @@\n======================================\n  Files          271     271       \n  Stmts        12803   12805     +2\n  Branches      1644    1644       \n  Methods          0       0       \n======================================\n+ Hit           6157    6162     +5\n+ Partial        467     465     -2\n+ Missed        6179    6178     -1\n\nReview entire Coverage Diff as of e263cc3\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.03%\nMerging #879 into master will decrease coverage by -0.01% as of 862126c\n\ndiff\n@@            master    #879   diff @@\n======================================\n  Files          271     271       \n  Stmts        12803   12803       \n  Branches      1644    1644       \n  Methods          0       0       \n======================================\n- Hit           6151    6150     -1\n+ Partial        466     463     -3\n- Missed        6186    6190     +4\n\nReview entire Coverage Diff as of 862126c\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.03%\nMerging #881 into master will decrease coverage by -0.01% as of e3ed38c\n\ndiff\n@@            master    #881   diff @@\n======================================\n  Files          271     271       \n  Stmts        12810   12805     -5\n  Branches      1645    1643     -2\n  Methods          0       0       \n======================================\n- Hit           6155    6151     -4\n+ Partial        464     460     -4\n- Missed        6191    6194     +3\n\nReview entire Coverage Diff as of e3ed38c\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.05%\nMerging #882 into master will increase coverage by +0.05% as of 94c73af\n\ndiff\n@@            master    #882   diff @@\n======================================\n  Files          271     271       \n  Stmts        12803   12808     +5\n  Branches      1644    1645     +1\n  Methods          0       0       \n======================================\n+ Hit           6146    6155     +9\n  Partial        463     463       \n+ Missed        6194    6190     -4\n\nReview entire Coverage Diff as of 94c73af\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.57%\nMerging #884 into master will increase coverage by +0.52% as of 55411a0\n\ndiff\n@@            master    #884   diff @@\n======================================\n  Files          271     271       \n  Stmts        12805   12853    +48\n  Branches      1643    1650     +7\n  Methods          0       0       \n======================================\n+ Hit           6153    6243    +90\n- Partial        466     470     +4\n+ Missed        6186    6140    -46\n\nReview entire Coverage Diff as of 55411a0\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.13%\nMerging #886 into master will increase coverage by +0.05% as of 96c1a64\n\ndiff\n@@            master    #886   diff @@\n======================================\n  Files          271     271       \n  Stmts        12853   12853       \n  Branches      1650    1650       \n  Methods          0       0       \n======================================\n+ Hit           6181    6187     +6\n- Partial        467     468     +1\n+ Missed        6205    6198     -7\n\nReview entire Coverage Diff as of 96c1a64\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.08%\nMerging #887 into master will increase coverage by +0.01% as of f6fd045\n\ndiff\n@@            master    #887   diff @@\n======================================\n  Files          271     271       \n  Stmts        12850   12850       \n  Branches      1649    1649       \n  Methods          0       0       \n======================================\n+ Hit           6177    6179     +2\n+ Partial        468     466     -2\n  Missed        6205    6205\n\nReview entire Coverage Diff as of f6fd045\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.08%\nMerging #888 into master will increase coverage by +0.06% as of f71a083\n\ndiff\n@@            master    #888   diff @@\n======================================\n  Files          271     271       \n  Stmts        12847   12846     -1\n  Branches      1648    1648       \n  Methods          0       0       \n======================================\n+ Hit           6170    6177     +7\n+ Partial        469     466     -3\n+ Missed        6208    6203     -5\n\nReview entire Coverage Diff as of f71a083\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.05%\nMerging #889 into master will decrease coverage by -0.02% as of a2bf610\n\ndiff\n@@            master    #889   diff @@\n======================================\n  Files          271     271       \n  Stmts        12846   12846       \n  Branches      1648    1648       \n  Methods          0       0       \n======================================\n- Hit           6176    6173     -3\n+ Partial        467     465     -2\n- Missed        6203    6208     +5\n\nReview entire Coverage Diff as of a2bf610\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.07%\nMerging #890 into master will increase coverage by +0.03% as of 9b45e0d\n\ndiff\n@@            master    #890   diff @@\n======================================\n  Files          271     271       \n  Stmts        12846   12846       \n  Branches      1648    1648       \n  Methods          0       0       \n======================================\n+ Hit           6172    6176     +4\n+ Partial        469     467     -2\n+ Missed        6205    6203     -2\n\nReview entire Coverage Diff as of 9b45e0d\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.07%\nMerging #891 into master will increase coverage by +0.03% as of 9b45e0d\n\ndiff\n@@            master    #891   diff @@\n======================================\n  Files          271     271       \n  Stmts        12846   12846       \n  Branches      1648    1648       \n  Methods          0       0       \n======================================\n+ Hit           6172    6176     +4\n+ Partial        469     467     -2\n+ Missed        6205    6203     -2\n\nReview entire Coverage Diff as of 9b45e0d\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.89%\nMerging #892 into master will increase coverage by +0.29% as of 58baf61\n\ndiff\n@@            master    #892   diff @@\n======================================\n  Files          271     271       \n  Stmts        12684   12690     +6\n  Branches      1648    1633    -15\n  Methods          0       0       \n======================================\n+ Hit           6165    6205    +40\n- Partial        467     469     +2\n+ Missed        6052    6016    -36\n\nReview entire Coverage Diff as of 58baf61\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.87%\nMerging #895 into master will decrease coverage by -0.02% as of 4677665\n\ndiff\n@@            master    #895   diff @@\n======================================\n  Files          271     271       \n  Stmts        12690   12690       \n  Branches      1633    1633       \n  Methods          0       0       \n======================================\n- Hit           6205    6202     -3\n- Partial        469     470     +1\n- Missed        6016    6018     +2\n\nReview entire Coverage Diff as of 4677665\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.92%\nMerging #896 into master will increase coverage by +0.02% as of d9f8a01\n\ndiff\n@@            master    #896   diff @@\n======================================\n  Files          271     271       \n  Stmts        12690   12706    +16\n  Branches      1633    1634     +1\n  Methods          0       0       \n======================================\n+ Hit           6206    6217    +11\n  Partial        467     467       \n- Missed        6017    6022     +5\n\nReview entire Coverage Diff as of d9f8a01\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.92%\nMerging #897 into master will increase coverage by +0.03% as of de350d6\n\ndiff\n@@            master    #897   diff @@\n======================================\n  Files          271     271       \n  Stmts        12706   12690    -16\n  Branches      1634    1633     -1\n  Methods          0       0       \n======================================\n- Hit           6213    6208     -5\n+ Partial        471     466     -5\n+ Missed        6022    6016     -6\n\nReview entire Coverage Diff as of de350d6\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.96%\nMerging #899 into master will increase coverage by +0.05% as of 0d0e5c9\n\ndiff\n@@            master    #899   diff @@\n======================================\n  Files          271     271       \n  Stmts        12706   12706       \n  Branches      1634    1634       \n  Methods          0       0       \n======================================\n+ Hit           6215    6221     +6\n- Partial        470     473     +3\n+ Missed        6021    6012     -9\n\nReview entire Coverage Diff as of 0d0e5c9\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.92%\nMerging #900 into master will increase coverage by +0.01% as of 886f1dc\n\ndiff\n@@            master    #900   diff @@\n======================================\n  Files          271     271       \n  Stmts        12706   12706       \n  Branches      1634    1634       \n  Methods          0       0       \n======================================\n+ Hit           6215    6216     +1\n+ Partial        470     468     -2\n- Missed        6021    6022     +1\n\nReview entire Coverage Diff as of 886f1dc\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 49.04%\nMerging #902 into master will increase coverage by +0.12% as of 7518d82\n\ndiff\n@@            master    #902   diff @@\n======================================\n  Files          271     271       \n  Stmts        12706   12719    +13\n  Branches      1634    1636     +2\n  Methods          0       0       \n======================================\n+ Hit           6217    6238    +21\n- Partial        468     472     +4\n+ Missed        6021    6009    -12\n\nReview entire Coverage Diff as of 7518d82\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 48.22%\nMerging #903 into master will decrease coverage by 0.74%\n1. 3 files (not in diff) in ...otify/helios/testing were modified. more \n   - Misses -10 \n   - Partials -1\n2. 4 files (not in diff) in ...scommon/coordination were modified. more \n   - Misses +8 \n   - Partials -1 \n   - Hits -7\n3. 1 files (not in diff) in ...a/com/spotify/helios were modified. more \n   - Misses -1 \n   - Partials -2 \n   - Hits +3\n4. File ...eperMasterModel.java was modified. more \n   - Misses +8 \n   - Partials -2 \n   - Hits -6\n5. File ...questDispatcher.java (not in diff) was modified. more \n   - Misses -3 \n   - Partials 0 \n   - Hits 0\n\ndiff\n@@             master       #903   diff @@\n==========================================\n  Files           271        275     +4   \n  Lines         12747      13035   +288   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1642       1669    +27   \n==========================================\n+ Hits           6243       6286    +43   \n- Misses         6030       6281   +251   \n+ Partials        474        468     -6\n\n\nPowered by Codecov. Last updated by aafa74c...5c8f05d\n. ## Current coverage is 49.07%\nMerging #904 into master will not affect coverage as of 1802736\n\ndiff\n@@            master    #904   diff @@\n======================================\n  Files          271     271       \n  Stmts        12719   12731    +12\n  Branches      1636    1640     +4\n  Methods          0       0       \n======================================\n+ Hit           6242    6248     +6\n  Partial        469     469       \n- Missed        6008    6014     +6\n\nReview entire Coverage Diff as of 1802736\nPowered by Codecov. Updated on successful CI builds.\n. ## Current coverage is 49.09%\nMerging #908 into master will increase coverage by +<.01%\n\ndiff\n@@             master       #908   diff @@\n==========================================\n  Files           271        271          \n  Lines         12733      12732     -1   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1640       1641     +1   \n==========================================\n+ Hits           6246       6250     +4   \n  Misses         6010       6010          \n+ Partials        477        472     -5\n1. 2 files (not in diff) in ...elios/servicescommon were modified. more \n   - Misses -2 \n   - Partials -3 \n   - Hits +5\n2. 3 files (not in diff) in ...spotify/helios/agent were modified. more \n   - Misses -3 \n   - Partials -1 \n   - Hits +4\n3. File ...g/TemporaryJobs.java (not in diff) was modified. more \n   - Misses +2 \n   - Partials -1 \n   - Hits -1\n4. File ...rs/HostSelector.java (not in diff) was modified. more \n   - Misses +3 \n   - Partials 0 \n   - Hits -4\n\n\nPowered by Codecov. Last updated by 76c49a0\n. ## Current coverage is 49.04%\nMerging #911 into master will decrease coverage by -0.09%\n1. 2 files (not in diff) in ...spotify/helios/agent were modified. more \n   - Misses +5 \n   - Partials +1 \n   - Hits -6\n2. 1 files (not in diff) in ...a/com/spotify/helios were modified. more \n   - Misses +7 \n   - Hits -7\n\ndiff\n@@             master       #911   diff @@\n==========================================\n  Files           271        271          \n  Lines         12734      12752    +18   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1640       1643     +3   \n==========================================\n- Hits           6259       6253     -6   \n- Misses         6004       6027    +23   \n- Partials        471        472     +1\n\nPowered by Codecov. Last updated by c03d478...4561da7\n. ## Current coverage is 49.03%\nMerging #913 into master will decrease coverage by -0.09%\n1. 3 files (not in diff) in ...a/com/spotify/helios were modified. more \n   - Misses +11 \n   - Partials -1 \n   - Hits -10\n\ndiff\n@@             master       #913   diff @@\n==========================================\n  Files           271        271          \n  Lines         12734      12744    +10   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1640       1641     +1   \n==========================================\n- Hits           6259       6249    -10   \n- Misses         6004       6025    +21   \n+ Partials        471        470     -1\n\nPowered by Codecov. Last updated by c03d478...f34adb6\n. ## Current coverage is 44.83%\nMerging #915 into master will decrease coverage by -13.77%\n1. 9 files (not in diff) in ...otify/helios/testing were deleted. more\n2. 13 files (not in diff) in ...cescommon/statistics were deleted. more\n3. 24 files (not in diff) in ...scommon/coordination were deleted. more\n4. 25 files (not in diff) in ...elios/servicescommon were deleted. more\n5. 7 files (not in diff) in ...helios/rollingupdate were deleted. more\n6. 6 files (not in diff) in ...ios/master/resources were deleted. more\n7. 4 files (not in diff) in ...elios/master/metrics were deleted. more\n8. 2 files (not in diff) in ...y/helios/master/http were deleted. more\n9. 21 files (not in diff) in ...potify/helios/master were deleted. more\n10. 39 files (not in diff) in ...spotify/helios/agent were deleted. more\n\ndiff\n@@             master       #915   diff @@\n==========================================\n  Files           271        115    -156   \n  Lines         12747       5195   -7552   \n  Methods           0          0           \n  Messages          0          0           \n  Branches       1642        865    -777   \n==========================================\n- Hits           7470       2329   -5141   \n+ Misses         5277       2649   -2628   \n- Partials          0        217    +217\n\n\nPowered by Codecov. Last updated by 9c8958c...f29d55a\n. ## Current coverage is 58.09%\nMerging #917 into master will increase coverage by 9.06%\n1. 14 files (not in diff) in ...otify/helios/testing were deleted. more\n2. 13 files (not in diff) in ...cescommon/statistics were deleted. more\n3. 24 files (not in diff) in ...scommon/coordination were deleted. more\n4. 25 files (not in diff) in ...elios/servicescommon were deleted. more\n5. 7 files (not in diff) in ...helios/rollingupdate were deleted. more\n6. 6 files (not in diff) in ...ios/master/resources were deleted. more\n7. 4 files (not in diff) in ...elios/master/metrics were deleted. more\n8. 2 files (not in diff) in ...y/helios/master/http were deleted. more\n9. 21 files (not in diff) in ...potify/helios/master were deleted. more\n10. 39 files (not in diff) in ...spotify/helios/agent were deleted. more\n\ndiff\n@@             master       #917   diff @@\n==========================================\n  Files           271        115    -156   \n  Lines         12748       5195   -7553   \n  Methods           0          0           \n  Messages          0          0           \n  Branches       1643        865    -778   \n==========================================\n- Hits           6251       3018   -3233   \n+ Misses         6027       2177   -3850   \n+ Partials        470          0    -470\n\n\nPowered by Codecov. Last updated by ce7fc5a...41358a4\n. ## Current coverage is 58.69%\nMerging #918 into master will increase coverage by +<.01%\n1. 3 files (not in diff) in ...a/com/spotify/helios were modified. more \n   - Misses -10 \n   - Hits +10\n2. File ...g/TemporaryJobs.java (not in diff) was modified. more \n   - Misses -1 \n   - Partials 0 \n   - Hits +1\n\ndiff\n@@             master       #918   diff @@\n==========================================\n  Files           271        271          \n  Lines         12747      12747          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1642       1642          \n==========================================\n+ Hits           7470       7481    +11   \n+ Misses         5277       5266    -11   \n  Partials          0          0\n\n\nPowered by Codecov. Last updated by 9c8958c...f465f26\n. ## Current coverage is 49.47%\nMerging #932 into master will increase coverage by 0.46%\n\ndiff\n@@             master       #932   diff @@\n==========================================\n  Files           271        271          \n  Lines         12760      12780    +20   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1644       1651     +7   \n==========================================\n+ Hits           6251       6322    +71   \n+ Misses         6036       5979    -57   \n- Partials        473        479     +6\n\nPowered by Codecov. Last updated by f6d401d...119d1ed\n. ## Current coverage is 48.97%\nMerging #934 into master will decrease coverage by <.01%\n1. 2 files (not in diff) in ...elios/servicescommon were modified. more \n   - Misses +12 \n   - Partials -3 \n   - Hits -9\n2. 3 files (not in diff) in ...spotify/helios/agent were modified. more \n   - Misses +4 \n   - Partials -2 \n   - Hits -2\n3. 1 files (not in diff) in ...a/com/spotify/helios were modified. more \n   - Misses +1 \n   - Partials +1 \n   - Hits -2\n4. File ...g/TemporaryJobs.java (not in diff) was modified. more \n   - Misses -2 \n   - Partials +1 \n   - Hits +1\n\ndiff\n@@             master       #934   diff @@\n==========================================\n  Files           271        271          \n  Lines         12759      12759          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1644       1644          \n==========================================\n- Hits           6260       6248    -12   \n- Misses         6022       6037    +15   \n+ Partials        477        474     -3\n\nPowered by Codecov. Last updated by 01306d0...d3b3373\n. ## Current coverage is 48.95%\nMerging #935 into master will decrease coverage by 9.62%\n1. 28 files (not in diff) in ...y/helios/cli/command were modified. more \n   - Misses +154 \n   - Partials +76 \n   - Hits -230\n2. 7 files (not in diff) in ...m/spotify/helios/cli were modified. more \n   - Misses +27 \n   - Partials +23 \n   - Hits -50\n3. 12 files (not in diff) in ...otify/helios/testing were modified. more \n   - Misses +13 \n   - Partials +83 \n   - Hits -96\n4. 3 files (not in diff) in ...cescommon/statistics were modified. more \n   - Misses +6 \n   - Partials +1 \n   - Hits -7\n5. 18 files (not in diff) in ...scommon/coordination were modified. more \n   - Misses +31 \n   - Partials +51 \n   - Hits -82\n6. 12 files (not in diff) in ...elios/servicescommon were modified. more \n   - Misses +37 \n   - Partials +19 \n   - Hits -56\n7. 4 files (not in diff) in ...helios/rollingupdate were modified. more \n   - Misses +6 \n   - Partials +13 \n   - Hits -19\n8. 5 files (not in diff) in ...ios/master/resources were modified. more \n   - Misses +32 \n   - Hits -32\n9. 9 files (not in diff) in ...potify/helios/master were modified. more \n   - Misses +79 \n   - Partials +22 \n   - Hits -101\n10. 27 files (not in diff) in ...spotify/helios/agent were modified. more \n    - Misses +76 \n    - Partials +69 \n    - Hits -145\n\ndiff\n@@             master       #935   diff @@\n==========================================\n  Files           271        271           \n  Lines         12759      12759           \n  Methods           0          0           \n  Messages          0          0           \n  Branches       1644       1644           \n==========================================\n- Hits           7473       6246   -1227   \n- Misses         5286       6038    +752   \n- Partials          0        475    +475\n\nPowered by Codecov. Last updated by 0f7e980...d619f7a\n. ## Current coverage is 48.98%\nMerging #937 into master will decrease coverage by <.01%\n1. 2 files (not in diff) in ...elios/servicescommon were modified. more \n   - Misses +12 \n   - Partials -3 \n   - Hits -9\n2. 2 files (not in diff) in ...spotify/helios/agent were modified. more \n   - Misses +2 \n   - Partials -2\n3. 1 files (not in diff) in ...a/com/spotify/helios were modified. more \n   - Misses +1 \n   - Partials +1 \n   - Hits -2\n4. File ...g/TemporaryJobs.java (not in diff) was modified. more \n   - Misses -2 \n   - Partials +1 \n   - Hits +1\n\ndiff\n@@             master       #937   diff @@\n==========================================\n  Files           271        271          \n  Lines         12759      12759          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1644       1644          \n==========================================\n- Hits           6260       6250    -10   \n- Misses         6022       6035    +13   \n+ Partials        477        474     -3\n\nPowered by Codecov. Last updated by 01306d0...4b3e199\n. ## Current coverage is 49.04%\nMerging #939 into master will increase coverage by <.01%\n\ndiff\n@@             master       #939   diff @@\n==========================================\n  Files           271        271          \n  Lines         12759      12760     +1   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1644       1644          \n==========================================\n+ Hits           6248       6257     +9   \n+ Misses         6037       6028     -9   \n- Partials        474        475     +1\n\nPowered by Codecov. Last updated by 7afbe2d...e7a459a\n. ## Current coverage is 49.53%\nMerging #940 into master will increase coverage by 0.02%\n\ndiff\n@@             master       #940   diff @@\n==========================================\n  Files           271        271          \n  Lines         12787      12787          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1654       1654          \n==========================================\n+ Hits           6331       6334     +3   \n+ Misses         5973       5968     -5   \n- Partials        483        485     +2\n\nPowered by Codecov. Last updated by 6ddc244...8e413ab\n. ## Current coverage is 49.60%\nMerging #942 into master will increase coverage by 0.04%\n\ndiff\n@@             master       #942   diff @@\n==========================================\n  Files           271        272     +1   \n  Lines         12789      12829    +40   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1654       1657     +3   \n==========================================\n+ Hits           6338       6364    +26   \n- Misses         5969       5983    +14   \n  Partials        482        482\n\nPowered by Codecov. Last updated by d6194e2...c00624b\n. ## Current coverage is 49.56%\nMerging #944 into master will increase coverage by <.01%\n\ndiff\n@@             master       #944   diff @@\n==========================================\n  Files           272        272          \n  Lines         12830      12834     +4   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1657       1658     +1   \n==========================================\n+ Hits           6359       6361     +2   \n  Misses         5989       5989          \n- Partials        482        484     +2\n\nPowered by Codecov. Last updated by c319197...4951b7a\n. ## Current coverage is 49.59%\nMerging #945 into master will increase coverage by 0.03%\n\ndiff\n@@             master       #945   diff @@\n==========================================\n  Files           272        272          \n  Lines         12830      12830          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1657       1657          \n==========================================\n+ Hits           6359       6363     +4   \n+ Misses         5989       5985     -4   \n  Partials        482        482\n\nPowered by Codecov. Last updated by c319197...7ef4cdc\n. \n",
    "dligthart": "Tests in error:\n  HealthCheckTest.testTcp:138->SystemTestBase.awaitTaskState:932 \u00bb Timeout\nTests run: 87, Failures: 0, Errors: 1, Skipped: 1\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO]\n[INFO] Helios Parent ..................................... SUCCESS [3.464s]\n[INFO] Helios Client ..................................... SUCCESS [14.232s]\n[INFO] Helios Service Registration ....................... SUCCESS [1.052s]\n[INFO] Helios Tools ...................................... SUCCESS [6.494s]\n[INFO] Helios Testing Common Library ..................... SUCCESS [2.214s]\n[INFO] Helios Services ................................... SUCCESS [1:53.860s]\n[INFO] Helios System Tests ............................... FAILURE [20:55.925s]\n[INFO] Helios Testing Library ............................ SKIPPED\n[INFO] Helios Integration Tests .......................... SKIPPED\n[INFO] Helios API Documentation .......................... SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 23:19.647s\n[INFO] Finished at: Fri Mar 06 12:22:49 UTC 2015\n[INFO] Final Memory: 27M/118M\n[INFO] -----------------------------------------------------------------------\n.  wget http://mirror.cogentco.com/pub/apache/zookeeper/stable/zookeeper-3.4.6.tar.gz\n    5  ls\n    6  tar xzvf zookeeper-3.4.6.tar.gz -C /usr/local/\n    7  sudo tar xzvf zookeeper-3.4.6.tar.gz -C /usr/local/\n    8  cd /usr/local/zookeeper-3.4.6/conf\n    9  ls\n   10  cp zoo_sample.cfg zoo.cfg\n   11  ls\n   12  vim zoo\n   13  sudo yum install  vim -y\n   14  ls\n   15  sudo vim zoo.cf\n   16  sudo vim zoo.cfg\n   17  ls\n   18  cd ..\n   19  ls\n   20  cd bin\n   21  ls\n   22  sudo ./zkServer.sh start\n   23  sudo yum install maven -y\n   24   ls\n   25  cd ..\n   26  l\n   27  scd ~\n   28  cd ~\n   29  ls\n   30  sudo yum install docker -y\n   31    sudo yum install git -y\n   32  ls\n   33  sudo git clone https://github.com/spotify/helios.git\n   34   ls\n   35  cd helios/\n   36  ls\n   37  sudo mvn clean test\n   38  sudo docekr ps\n   39  sudo docker ps\n   40  sudo service docker start\n   41  sudo mvn clean test\n   42      lsof\n   43  sudo yum install lsof -y\n   44  sudo mvn clean test\n. [INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO]\n[INFO] Helios Parent ..................................... SUCCESS [35.883s]\n[INFO] Helios Client ..................................... SUCCESS [51.505s]\n[INFO] Helios Service Registration ....................... SUCCESS [22.005s]\n[INFO] Helios Tools ...................................... SUCCESS [43.944s]\n[INFO] Helios Testing Common Library ..................... SUCCESS [42.727s]\n[INFO] Helios Services ................................... SUCCESS [3:42.758s]\n[INFO] Helios System Tests ............................... SUCCESS [3:34.844s]\n[INFO] Helios Testing Library ............................ SUCCESS [3:18.733s]\n[INFO] Helios Integration Tests .......................... SUCCESS [4:57.484s]\n[INFO] Helios API Documentation .......................... SUCCESS [2:20.108s]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 21:12.317s\n[INFO] Finished at: Fri Mar 06 13:58:03 UTC 2015\n[INFO] Final Memory: 53M/264M\n[INFO] ------------------------------------------------------------------------\nwhen running: \nsudo mvn clean compile site\n. sudo ./helios\nls: cannot access /home/centos/helios/bin/../helios-tools/target/helios-tools*-shaded.jar: No such file or directory\ndirname: missing operand\nTry 'dirname --help' for more information.\nbasename: missing operand\nTry 'basename --help' for more information.\n. Running mvn install now\n. mvn install worked but didn't install - it just created the executables.\n. ",
    "perfa": "Hmm... The build failure looks unrelated to my changes, is the CI setup flakey?\n. Tests pass. It'd be great if this could make it in soon.\n. ",
    "gwilton": "Think i figured out the issue after enabling DEBUG logging on both the agent and the master.\nHELIOS_AGENT_OPTS=\"--syslog \\\n    --logconfig /usr/share/helios/logback-agent.xml \\\nMy problem was that I was configuring the agent and the master as service-registry.\n--service-registry http://127.0.0.1:8500 \\\n--service-registrar-plugin /usr/share/helios/lib/plugins/helios-consul-0.24.jar \\\nIf both agent and master run with these options they register & deregister each other services. These options should only be used on the master. This was not originally clear to me.\n. ",
    "andredasilvapinto": "In my case it didn't give that error but the one I've written in my previous post (maybe this only happens if it is a new docker installation and you haven't started docker before - so the file wasn't created yet). I was not the only one as when I was searching for information regarding the error I've found this http://stackoverflow.com/a/29360837/43046 and eventually fixed it by starting the service.\nRegarding the other reference to docker info. I think it is not very obvious (at least it wasn't for me), that docker info would require the service to be on (I thought it could be a command that retrieved only static information about the installation/configuration).\nIn any case, if you think it is clear this way, I have no objections. I just wanted to refer this as it might have been just something that was forgotten and I thought it could help other people. Given that it was not the case, I think you should do as you want :) Maybe it was only me that interpreted it wrong. \nYou can close the issue if you want.\nRegards.\n. Maybe, but if you want to change it I would suggest simply adding:\n\"Ensure you have installed Docker, configured it correctly _and started the service_ so that commands like docker info work.\"\nBut again, it's not a big problem. Actually I think it is more like a bug in docker which will probably be fixed in the next versions and then the error message from docker info will be much more useful and obvious.\n. ",
    "martnu": "Looks good to me! :+1: \n. ",
    "rouzwawi": ":+1:  thanks for the quick fix\n. ",
    "damoon": "if the -z param is used wrong the help states:\ncom.spotify.helios.common.HeliosException: Master endpoints must be of the form \"http[s]://heliosmaster.domain.net:<port>\"\ni added helios behind a nginx with ssl.\n\"curl https://helios.domain.de/hosts\" works and lists the mashines added to helios.\nif i run \"helios -z https://helios.domain.de:443 hosts\" i get the answer:\ncom.spotify.helios.common.HeliosException: request failed: Response{method='GET', uri=http://helios/hosts/?user=root, status=400, payload=<html>\n<head><title>400 The plain HTTP request was sent to HTTPS port</title></head>\n<body bgcolor=\"white\">\n<center><h1>400 Bad Request</h1></center>\n<center>The plain HTTP request was sent to HTTPS port</center>\n<hr><center>nginx</center>\n</body>\n</html>\n}\nSo i assume it is the client who does not support https (ignores it and always uses http).\n. i just recreated this with 0.8.444\n. ",
    "ukarlsson": ":+1: \n. ",
    "aes": ":+1:\n. ",
    "gtonic": "My intention was having hostname in the API (helios-client, cli) without any automatic name generation at first place. We should of course talk about length limit/validation here (either 64 or 255). \nBasically I'd use it to give a container a more meaningful name, which will result in better readable logfile in my case. \n. @davidxia updated PR with your suggestions (validation and tests, documentation and reset of circle ci nodes to '5')\n. @davidxia thanks for reviewing and providing additional tests. \nsuggestion: this already is a long-living branch (also with merges from upstream/master), so I'm a bit afraid of rebasing it. I can just submit you a patch or have it in a separate branch for merging: https://circleci.com/gh/molindo/helios/56\n. @davidxia looks good to me.\n. ",
    "rohan": "Hey! As much as I would love a job at Spotify, I\u2019m actually not the @rohan you\u2019re looking for. GitHub gave me this username about a year ago, and I\u2019m still cleaning up the merge conflicts :)\nRohan\nOn Wed, Jul 29, 2015 at 10:26 AM, David Xia notifications@github.com\nwrote:\n\n@rohan?\nOn Wed, Jul 29, 2015 at 1:23 PM, Staffan Gim\u00e5ker notifications@github.com\nwrote:\n\nI have no clue to be honest.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/spotify/helios/pull/594#issuecomment-126025826.\n\nDavid Xia\nSoftware Engineer\ndxia@spotify.com\nReply to this email directly or view it on GitHub:\nhttps://github.com/spotify/helios/pull/594#issuecomment-126026097\n. \n\n",
    "momomagic": "Totally agree , its a rollout option and ports dynamically matched with a service registry plugin .\n. @gimaker What kind of performance issues do you have there , I have problem with healthchecking is not working with new update \n. ",
    "cldmnky": "We decided to use the cli interface in the module which worked out well, so this is kind of solved on our side :-)\n. ",
    "udomsak": "Thank you @davidxia :) , \nSo looklike for TCP load-balancer i need to install  HA-Proxy new on also Sky-DNS or some  Service-discovery on each Helios-Agent. ? \n. @davidxia Can i pass parameter for limit CPU or Memory usage through Helios or not ?  So if not does helios support for this control or not.?\nSo what hapen if Container is  use resource over limit  that define as above question it's automatic restart or not. ? \n. @davidxia Actually i plan to use multiple container in under helios-client. One of this application can cause memory leak ( Nodejs ).  To pervent that i need to restart container when they over limit. \n@davidxia Thank you for clarify.  you can close this question but it best to open it and catagolize with Scaling question. \n. @carlanton  yep it will restart if over limit but i does not option set in helios client command ( as you refer and @davidxia suggest me for patching.\nFYI  my scenario example\nPer Docker host\n\nrun multiple  nodejs application ( Actionhero ) that have many worker ( assume 15 worker). \neach application container must does not  consume resource over 70% of system resource.\neach application container must does not have effect with other container when resource over consume.\nrun other container for other application ( handle some task ). \n\nfrom above requirement i want to limit resource consume. \n. ## Just Ideas  ( sorry if i miss understand about Helios architecture ).\nfor rolling-update ( Zero downtime deployment ).  I trying how to setup policy for helios client when deploy. \n-  deploy new job in background waiting application do some initial when ready switch to new deploy. \n-  current helios do switch from old to new one immediately. \nIdea from this scenario must have thrid-party service to handle this.\n\nrun ha-proxy  with consul-template  for dynamic add  back-end container. \nremove previous version jobs. ( automatic ).  \n\nidea from this https://github.com/DevTable/gantryd  ( gantryd ) \nrolling update host group\n\ndifficult to make it fast without broker or bus messaging help ( parallel deploy ). I not try before for large deployment just THINK.   \nnot sure this good or not if we have plugin to connect Helios to connect broker or bus messaging. \n. thx @davidxia \n. @davidxia sorry for not-smart question. API is locate  @  https://github.com/spotify/helios/tree/master/helios-services/src/main/java/com/spotify/helios/master/resources  \n\nmy fault not clear for reading README.md.thx.  :) \n. Ok, This is rolling update and  current Helios not flexible enough for adjust deployment policy. :)\n. ",
    "rikhal": "@davidxia This fixes the issue. Thanks.\n. ",
    "drorweiss": "Hi @davidxia. That's right, I'm Codota's CEO :) . \nYes, I think that users who read the Helios code, and try to learn from it will find Codota useful in figuring out the code. I believe it's a compelling code browsing experience.\nWould be great if you try it and tell us what you think (possible also via email - dror at codota dot com)\n. ",
    "piccobit": "Hi David,\nThanks a lot for taking a look at this issue! \nYes, it is still a problem for us, but I can, of course, provide you with any logs you need. \nTomorrow, I'll provide you with fresh logs containing, hopefully, the issue. \n. Hi David,\nHere is a fresh log from tomorrow morning (sorry for the long log, but I'm not allowed to attach the log):\n```\nListening for transport dt_socket at address: 5006\n10:19:16.128 helios[14151]: DEBUG [AgentMain STARTING] logging: Logging Provider: org.jboss.logging.Slf4jLoggerProvider\n10:19:16.131 helios[14151]: INFO  [AgentMain STARTING] Version: HV000001: Hibernate Validator 5.1.1.Final\n10:19:16.136 helios[14151]: DEBUG [AgentMain STARTING] DefaultTraversableResolver: Cannot find javax.persistence.Persistence on classpath. Assuming non JPA 2 environment. All properties will per default be traversable.\n10:19:16.151 helios[14151]: DEBUG [AgentMain STARTING] ValidationXmlParser: Trying to load META-INF/validation.xml for XML based Validator configuration.\n10:19:16.152 helios[14151]: DEBUG [AgentMain STARTING] ValidationXmlParser: No META-INF/validation.xml found. Using annotation based configuration only.\n10:19:16.376 helios[14151]: DEBUG [AgentMain STARTING] log: Logging to Logger[org.eclipse.jetty.util.log] via org.eclipse.jetty.util.log.Slf4jLog\n10:19:16.382 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: i.d.j.MutableServletContextHandler@3b60b48a{/,null,null} added {org.eclipse.jetty.servlet.ServletHandler@49ecee7a,AUTO}\n10:19:16.382 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: i.d.j.MutableServletContextHandler@71bc895{/,null,null} added {org.eclipse.jetty.servlet.ServletHandler@310db1d8,AUTO}\n10:19:16.402 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@310db1d8 added {tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true,AUTO}\n10:19:16.403 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@310db1d8 added {[/tasks/]=>tasks,POJO}\n10:19:16.405 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: filterNameMap={}\n10:19:16.405 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: pathFilters=null\n10:19:16.405 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletFilterMap=null\n10:19:16.406 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletPathMap={/tasks/=tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true}\n10:19:16.406 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletNameMap={tasks=tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true}\n10:19:16.436 helios[14151]: INFO  [AgentMain STARTING] AgentService: Starting metrics\n10:19:16.485 helios[14151]: INFO  [AgentMain STARTING] CuratorFrameworkImpl: Starting\n10:19:16.487 helios[14151]: DEBUG [AgentMain STARTING] CuratorZookeeperClient: Starting\n10:19:16.487 helios[14151]: DEBUG [AgentMain STARTING] ConnectionState: Starting\n10:19:16.487 helios[14151]: DEBUG [AgentMain STARTING] ConnectionState: reset\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:zookeeper.version=3.5.0-alpha-1615249, built on 08/01/2014 22:13 GMT\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:host.name=helios-agent-host.example.com\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:java.version=1.7.0_85\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:java.vendor=Oracle Corporation\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-7-openjdk-amd64/jre\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:java.class.path=/usr/share/helios/lib/services/helios-services-0.8.562-shaded.jar\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:java.io.tmpdir=/tmp\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:java.compiler=\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:os.name=Linux\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:os.arch=amd64\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:os.version=3.13.0-66-generic\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:user.name=helios\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:user.home=/var/lib/helios-agent\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:user.dir=/var/lib/helios-agent\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:os.memory.free=195MB\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:os.memory.max=3555MB\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Client environment:os.memory.total=240MB\n10:19:16.494 helios[14151]: INFO  [AgentMain STARTING] ZooKeeper: Initiating client connection, connectString=helios-master-host.example.com:2181 sessionTimeout=60000 watcher=org.apache.curator.ConnectionState@3932b27d\n10:19:16.499 helios[14151]: DEBUG [AgentMain STARTING] ClientCnxn: zookeeper.disableAutoWatchReset is false\n10:19:16.512 helios[14151]: INFO  [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Opening socket connection to server helios-master-host.example.com/10.222.132.82:2181. Will not attempt to authenticate using SASL (unknown error)\n10:19:16.517 helios[14151]: INFO  [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Socket connection established to helios-master-host.example.com/10.222.132.82:2181, initiating session\n10:19:16.519 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Session establishment request sent on helios-master-host.example.com/10.222.132.82:2181\n10:19:16.667 helios[14151]: DEBUG [AgentMain STARTING] ServiceRegistrars: No service registrar plugin configured\n10:19:16.668 helios[14151]: INFO  [AgentMain STARTING] ServiceRegistrars: No address nor domain configured, not creating service registrar.\n10:19:16.692 helios[14151]: INFO  [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Session establishment complete on server helios-master-host.example.com/10.222.132.82:2181, sessionid = 0x150af72f074002a, negotiated timeout = 40000\n10:19:16.698 helios[14151]: INFO  [AgentMain STARTING-EventThread] ConnectionStateManager: State change: CONNECTED\n10:19:16.698 helios[14151]: DEBUG [Curator-ConnectionStateManager-0] PersistentPathChildrenCache: connection state change: CONNECTED\n10:19:17.001 helios[14151]: INFO  [AgentMain STARTING] ServerFactory: Starting helios-agent\n10:19:17.005 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {dw{STOPPED,8<=0<=1024,i=0,q=0},AUTO}\n10:19:17.006 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {com.spotify.helios.servicescommon.ManagedStatsdReporter@4b8a34d2,AUTO}\n10:19:17.006 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {com.spotify.helios.servicescommon.RiemannSupport@5e91edbc,AUTO}\n10:19:17.006 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {com.spotify.helios.agent.DockerHealthChecker@3e8f0225,AUTO}\n10:19:17.006 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {com.spotify.helios.servicescommon.RiemannHeartBeat@1ed5eb39,AUTO}\n10:19:17.007 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {com.spotify.helios.servicescommon.coordination.ZooKeeperHealthChecker@406a400c,AUTO}\n10:19:17.007 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {AgentService [NEW],AUTO}\n10:19:17.008 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {org.eclipse.jetty.server.handler.ErrorHandler@60c25cf7,AUTO}\n10:19:17.009 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@49ecee7a added {io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc,AUTO}\n10:19:17.010 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@49ecee7a added {[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc,POJO}\n10:19:17.011 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: filterNameMap={io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc=io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc}\n10:19:17.011 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: pathFilters=[[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc]\n10:19:17.011 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletFilterMap={}\n10:19:17.011 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletPathMap=null\n10:19:17.011 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletNameMap={}\n10:19:17.012 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@49ecee7a added {io.dropwizard.servlets.ThreadNameFilter-7d4b57ae,AUTO}\n10:19:17.012 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@49ecee7a added {[/]/[]==1=>io.dropwizard.servlets.ThreadNameFilter-7d4b57ae,POJO}\n10:19:17.012 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: filterNameMap={io.dropwizard.servlets.ThreadNameFilter-7d4b57ae=io.dropwizard.servlets.ThreadNameFilter-7d4b57ae, io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc=io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc}\n10:19:17.012 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: pathFilters=[[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc, [/]/[]==1=>io.dropwizard.servlets.ThreadNameFilter-7d4b57ae]\n10:19:17.012 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletFilterMap={}\n10:19:17.012 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletPathMap=null\n10:19:17.012 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletNameMap={}\n10:19:17.017 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@49ecee7a added {io.dropwizard.jetty.BiDiGzipFilter-857e3fa,AUTO}\n10:19:17.017 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@49ecee7a added {[/]/[]==31=>io.dropwizard.jetty.BiDiGzipFilter-857e3fa,POJO}\n10:19:17.017 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: filterNameMap={io.dropwizard.jetty.BiDiGzipFilter-857e3fa=io.dropwizard.jetty.BiDiGzipFilter-857e3fa, io.dropwizard.servlets.ThreadNameFilter-7d4b57ae=io.dropwizard.servlets.ThreadNameFilter-7d4b57ae, io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc=io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc}\n10:19:17.017 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: pathFilters=[[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc, [/]/[]==1=>io.dropwizard.servlets.ThreadNameFilter-7d4b57ae, [/]/[]==31=>io.dropwizard.jetty.BiDiGzipFilter-857e3fa]\n10:19:17.017 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletFilterMap={}\n10:19:17.017 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletPathMap=null\n10:19:17.017 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletNameMap={}\n10:19:17.022 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@49ecee7a added {com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f@84b6e75==com.sun.jersey.spi.container.servlet.ServletContainer,1,true,AUTO}\n10:19:17.022 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@49ecee7a added {[/]=>com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f,POJO}\n10:19:17.022 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: filterNameMap={io.dropwizard.jetty.BiDiGzipFilter-857e3fa=io.dropwizard.jetty.BiDiGzipFilter-857e3fa, io.dropwizard.servlets.ThreadNameFilter-7d4b57ae=io.dropwizard.servlets.ThreadNameFilter-7d4b57ae, io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc=io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc}\n10:19:17.022 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: pathFilters=[[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc, [/]/[]==1=>io.dropwizard.servlets.ThreadNameFilter-7d4b57ae, [/]/[]==31=>io.dropwizard.jetty.BiDiGzipFilter-857e3fa]\n10:19:17.022 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletFilterMap={}\n10:19:17.022 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletPathMap={/=com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f@84b6e75==com.sun.jersey.spi.container.servlet.ServletContainer,1,true}\n10:19:17.022 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletNameMap={com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f=com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f@84b6e75==com.sun.jersey.spi.container.servlet.ServletContainer,1,true}\n10:19:17.023 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: com.codahale.metrics.jetty9.InstrumentedHandler@33dba6d1 added {i.d.j.MutableServletContextHandler@3b60b48a{/,null,null},AUTO}\n10:19:17.025 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@310db1d8 added {com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true,AUTO}\n10:19:17.025 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@310db1d8 added {[/]=>com.codahale.metrics.servlets.AdminServlet-3c873f94,POJO}\n10:19:17.025 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: filterNameMap={}\n10:19:17.025 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: pathFilters=null\n10:19:17.025 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletFilterMap=null\n10:19:17.025 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletPathMap={/tasks/=tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true, /=com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true}\n10:19:17.026 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletNameMap={com.codahale.metrics.servlets.AdminServlet-3c873f94=com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true, tasks=tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true}\n10:19:17.026 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@310db1d8 added {io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97,AUTO}\n10:19:17.026 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.servlet.ServletHandler@310db1d8 added {[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97,POJO}\n10:19:17.026 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: filterNameMap={io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97=io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97}\n10:19:17.026 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: pathFilters=[[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97]\n10:19:17.026 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletFilterMap={}\n10:19:17.026 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletPathMap={/tasks/=tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true, /*=com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true}\n10:19:17.026 helios[14151]: DEBUG [AgentMain STARTING] ServletHandler: servletNameMap={com.codahale.metrics.servlets.AdminServlet-3c873f94=com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true, tasks=tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true}\n10:19:17.034 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: HttpConnectionFactory@665dd0e2{HTTP/1.1} added {HttpConfiguration@193cd9ef{32768,8192/8192,https://:0,[ForwardedRequestCustomizer@6a07e6da]},POJO}\n10:19:17.036 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: com.codahale.metrics.jetty9.InstrumentedConnectionFactory@4f22f4e6 added {HttpConnectionFactory@665dd0e2{HTTP/1.1},AUTO}\n10:19:17.038 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@1f7d19f2{null}{0.0.0.0:0} added {org.eclipse.jetty.server.Server@c2028db,UNMANAGED}\n10:19:17.038 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@1f7d19f2{null}{0.0.0.0:0} added {dw{STOPPED,8<=0<=1024,i=0,q=0},AUTO}\n10:19:17.039 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@1f7d19f2{null}{0.0.0.0:0} added {org.eclipse.jetty.util.thread.ScheduledExecutorScheduler@31741ab7,AUTO}\n10:19:17.039 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@1f7d19f2{null}{0.0.0.0:0} added {org.eclipse.jetty.io.ArrayByteBufferPool@4e326f04,POJO}\n10:19:17.039 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@1f7d19f2{null}{0.0.0.0:0} added {com.codahale.metrics.jetty9.InstrumentedConnectionFactory@4f22f4e6,AUTO}\n10:19:17.040 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@1f7d19f2{HTTP/1.1}{0.0.0.0:0} added {org.eclipse.jetty.server.ServerConnector$ServerConnectorManager@7ebdcdf7,MANAGED}\n10:19:17.041 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {dw-admin{STOPPED,1<=0<=64,i=0,q=0},AUTO}\n10:19:17.041 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: HttpConnectionFactory@7e7eb4f6{HTTP/1.1} added {HttpConfiguration@309ac35b{32768,8192/8192,https://:0,[ForwardedRequestCustomizer@8fa44b]},POJO}\n10:19:17.041 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: com.codahale.metrics.jetty9.InstrumentedConnectionFactory@19378d8d added {HttpConnectionFactory@7e7eb4f6{HTTP/1.1},AUTO}\n10:19:17.042 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@e2e30ea{null}{0.0.0.0:0} added {org.eclipse.jetty.server.Server@c2028db,UNMANAGED}\n10:19:17.042 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@e2e30ea{null}{0.0.0.0:0} added {dw-admin{STOPPED,1<=0<=64,i=0,q=0},AUTO}\n10:19:17.042 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@e2e30ea{null}{0.0.0.0:0} added {org.eclipse.jetty.util.thread.ScheduledExecutorScheduler@566b3836,AUTO}\n10:19:17.042 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@e2e30ea{null}{0.0.0.0:0} added {org.eclipse.jetty.io.ArrayByteBufferPool@7a8d59eb,POJO}\n10:19:17.043 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@e2e30ea{null}{0.0.0.0:0} added {com.codahale.metrics.jetty9.InstrumentedConnectionFactory@19378d8d,AUTO}\n10:19:17.043 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: ServerConnector@e2e30ea{HTTP/1.1}{0.0.0.0:0} added {org.eclipse.jetty.server.ServerConnector$ServerConnectorManager@560a9228,MANAGED}\n10:19:17.043 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {application@1f7d19f2{HTTP/1.1}{0.0.0.0:5803},AUTO}\n10:19:17.043 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {admin@e2e30ea{HTTP/1.1}{0.0.0.0:5804},AUTO}\n10:19:17.044 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: io.dropwizard.jetty.RoutingHandler@b7d2830 added {com.codahale.metrics.jetty9.InstrumentedHandler@33dba6d1,AUTO}\n10:19:17.044 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: io.dropwizard.jetty.RoutingHandler@b7d2830 added {i.d.j.MutableServletContextHandler@71bc895{/,null,null},AUTO}\n10:19:17.049 helios[14151]: DEBUG [AgentMain STARTING] AbstractLifeCycle: starting io.dropwizard.jetty.Slf4jRequestLog@69b08471\n10:19:17.050 helios[14151]: DEBUG [AgentMain STARTING] AbstractLifeCycle: STARTED io.dropwizard.jetty.Slf4jRequestLog@69b08471\n10:19:17.050 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.handler.RequestLogHandler@4656b3b1 added {io.dropwizard.jetty.Slf4jRequestLog@69b08471,UNMANAGED}\n10:19:17.050 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {io.dropwizard.jetty.Slf4jRequestLog@69b08471,MANAGED}\n10:19:17.051 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.handler.RequestLogHandler@4656b3b1 added {io.dropwizard.jetty.RoutingHandler@b7d2830,AUTO}\n10:19:17.052 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.handler.StatisticsHandler@154162c3 added {org.eclipse.jetty.server.handler.RequestLogHandler@4656b3b1,AUTO}\n10:19:17.052 helios[14151]: DEBUG [AgentMain STARTING] ContainerLifeCycle: org.eclipse.jetty.server.Server@c2028db added {org.eclipse.jetty.server.handler.StatisticsHandler@154162c3,AUTO}\n10:19:17.056 helios[14151]: INFO  [AgentService STARTING] AgentService: \n            .  . _  \n /   |   \\   _ |  | |_|  __   /  _  \\     _   /  |_ \n/    ~    _/  \\|  | |  |/  _ \\/  /  /  /\\  \\  / _/  \\ /    \\   \\\n\\    Y    /\\  /|  ||  (  <_> ) \\  /    |    \\/ //  >  /|   |  \\  |\n _|  /  _  >_/_|_/_  > _|  /_  / ___  >|  /_|\n       \\/       \\/                   \\/          \\//___/      \\/     \\/      \n10:19:17.057 helios[14151]: DEBUG [PersistentPathChildrenCache STARTING] PersistentPathChildrenCache: starting cache\n10:19:17.058 helios[14151]: DEBUG [Reactor(zk-ppcc:/config/hosts/helios-agent-host/jobs)] PersistentPathChildrenCache: updating: /config/hosts/helios-agent-host/jobs\n10:19:17.058 helios[14151]: DEBUG [Reactor(zk-ppcc:/config/hosts/helios-agent-host/jobs)] PersistentPathChildrenCache: syncing: /config/hosts/helios-agent-host/jobs\n10:19:17.065 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 1,3  replyHeader:: 1,400144,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974} \n10:19:17.066 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 2,3  replyHeader:: 2,400144,0  request:: '/config/hosts/helios-agent-host/id,F  response:: s{24,24,1446052557953,1446052557953,0,0,0,0,40,0,24} \n10:19:17.067 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 3,12  replyHeader:: 3,400144,0  request:: '/config/hosts/helios-agent-host/jobs,T  response:: v{},s{21,21,1446052557932,1446052557932,0,32,0,0,0,0,358972} \n10:19:17.072 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 4,4  replyHeader:: 4,400144,0  request:: '/config/hosts/helios-agent-host/id,F  response:: #46354130343641383244353633363436344239323732304444373945384439463330333341353941,s{24,24,1446052557953,1446052557953,0,0,0,0,40,0,24} \n10:19:17.073 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 5,12  replyHeader:: 5,400144,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: v{},s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974} \n10:19:17.073 helios[14151]: INFO  [Reactor(zk-client-async-init)] AgentZooKeeperRegistrar: Matching agent id node already present, not registering agent F5A046A82D5636464B92720DD79E8D9F3033A59A: helios-agent-host\n10:19:17.073 helios[14151]: DEBUG [Reactor(zk-client-async-init)] AgentZooKeeperRegistrar: Creating up node: /status/hosts/helios-agent-host/up\n10:19:17.074 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.server.Server@c2028db\n10:19:17.074 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting dw{STOPPED,8<=0<=1024,i=0,q=0}\n10:19:17.076 helios[14151]: DEBUG [Reactor(zk-ppcc:/config/hosts/helios-agent-host/jobs)] PersistentPathChildrenCache: children: []\n10:19:17.087 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED dw{STARTED,8<=8<=1024,i=8,q=0}\n10:19:17.090 helios[14151]: DEBUG [AgentService STARTING] ContainerLifeCycle: application@1f7d19f2{HTTP/1.1}{0.0.0.0:5803} added {sun.nio.ch.ServerSocketChannelImpl[/0.0.0.0:5803],POJO}\n10:19:17.090 helios[14151]: INFO  [AgentService STARTING] SetUIDListener: Opened application@1f7d19f2{HTTP/1.1}{0.0.0.0:5803}\n10:19:17.090 helios[14151]: DEBUG [AgentService STARTING] ContainerLifeCycle: admin@e2e30ea{HTTP/1.1}{0.0.0.0:5804} added {sun.nio.ch.ServerSocketChannelImpl[/0.0.0.0:5804],POJO}\n10:19:17.091 helios[14151]: INFO  [AgentService STARTING] SetUIDListener: Opened admin@e2e30ea{HTTP/1.1}{0.0.0.0:5804}\n10:19:17.091 helios[14151]: DEBUG [AgentService STARTING] LifecycleEnvironment: managed objects = [com.spotify.helios.servicescommon.ManagedStatsdReporter@4b8a34d2, com.spotify.helios.servicescommon.RiemannSupport@5e91edbc, com.spotify.helios.agent.DockerHealthChecker@3e8f0225, com.spotify.helios.servicescommon.RiemannHeartBeat@1ed5eb39, com.spotify.helios.servicescommon.coordination.ZooKeeperHealthChecker@406a400c, AgentService [STARTING], AgentService [STARTING]]\n10:19:17.092 helios[14151]: INFO  [Reactor(zk-client-async-init)] AgentZooKeeperRegistrar: ZooKeeper registration complete\n10:19:17.096 helios[14151]: INFO  [AgentService STARTING] Server: jetty-9.0.z-SNAPSHOT\n10:19:17.106 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting org.eclipse.jetty.server.Server@c2028db\n10:19:17.106 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.spotify.helios.servicescommon.ManagedStatsdReporter@4b8a34d2\n10:19:17.106 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.spotify.helios.servicescommon.ManagedStatsdReporter@4b8a34d2\n10:19:17.106 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.spotify.helios.servicescommon.RiemannSupport@5e91edbc\n10:19:17.106 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.spotify.helios.servicescommon.RiemannSupport@5e91edbc\n10:19:17.106 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.spotify.helios.agent.DockerHealthChecker@3e8f0225\n10:19:17.106 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.spotify.helios.agent.DockerHealthChecker@3e8f0225\n10:19:17.106 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.spotify.helios.servicescommon.RiemannHeartBeat@1ed5eb39\n10:19:17.107 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.spotify.helios.servicescommon.RiemannHeartBeat@1ed5eb39\n10:19:17.107 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.spotify.helios.servicescommon.coordination.ZooKeeperHealthChecker@406a400c\n10:19:17.108 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.spotify.helios.servicescommon.coordination.ZooKeeperHealthChecker@406a400c\n10:19:17.108 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting AgentService [STARTING]\n10:19:17.108 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED AgentService [STARTING]\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.server.handler.ErrorHandler@60c25cf7\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting org.eclipse.jetty.server.handler.ErrorHandler@60c25cf7\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.ErrorHandler@60c25cf7\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting dw-admin{STOPPED,1<=0<=64,i=0,q=0}\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED dw-admin{STARTED,1<=1<=64,i=0,q=0}\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.server.handler.StatisticsHandler@154162c3\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting org.eclipse.jetty.server.handler.StatisticsHandler@154162c3\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.server.handler.RequestLogHandler@4656b3b1\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting org.eclipse.jetty.server.handler.RequestLogHandler@4656b3b1\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting io.dropwizard.jetty.RoutingHandler@b7d2830\n10:19:17.109 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting io.dropwizard.jetty.RoutingHandler@b7d2830\n10:19:17.110 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.codahale.metrics.jetty9.InstrumentedHandler@33dba6d1\n10:19:17.110 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting com.codahale.metrics.jetty9.InstrumentedHandler@33dba6d1\n10:19:17.110 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting i.d.j.MutableServletContextHandler@3b60b48a{/,null,null}\n10:19:17.115 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting i.d.j.MutableServletContextHandler@3b60b48a{/,null,STARTING}\n10:19:17.116 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@49ecee7a\n10:19:17.116 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: filterNameMap={io.dropwizard.jetty.BiDiGzipFilter-857e3fa=io.dropwizard.jetty.BiDiGzipFilter-857e3fa, io.dropwizard.servlets.ThreadNameFilter-7d4b57ae=io.dropwizard.servlets.ThreadNameFilter-7d4b57ae, io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc=io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc}\n10:19:17.116 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: pathFilters=[[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc, [/]/[]==1=>io.dropwizard.servlets.ThreadNameFilter-7d4b57ae, [/]/[]==31=>io.dropwizard.jetty.BiDiGzipFilter-857e3fa]\n10:19:17.116 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: servletFilterMap={}\n10:19:17.116 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: servletPathMap={/=com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f@84b6e75==com.sun.jersey.spi.container.servlet.ServletContainer,1,true}\n10:19:17.116 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: servletNameMap={com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f=com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f@84b6e75==com.sun.jersey.spi.container.servlet.ServletContainer,1,true}\n10:19:17.117 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@49ecee7a\n10:19:17.117 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@49ecee7a\n10:19:17.117 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc\n10:19:17.117 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED io.dropwizard.jersey.filter.AllowedMethodsFilter-61fca9cc\n10:19:17.118 helios[14151]: DEBUG [AgentService STARTING] FilterHolder: Filter.init io.dropwizard.jersey.filter.AllowedMethodsFilter@c0c4929\n10:19:17.118 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting io.dropwizard.servlets.ThreadNameFilter-7d4b57ae\n10:19:17.118 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED io.dropwizard.servlets.ThreadNameFilter-7d4b57ae\n10:19:17.118 helios[14151]: DEBUG [AgentService STARTING] FilterHolder: Filter.init io.dropwizard.servlets.ThreadNameFilter@43a2b7e8\n10:19:17.118 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting io.dropwizard.jetty.BiDiGzipFilter-857e3fa\n10:19:17.118 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED io.dropwizard.jetty.BiDiGzipFilter-857e3fa\n10:19:17.118 helios[14151]: DEBUG [AgentService STARTING] FilterHolder: Filter.init io.dropwizard.jetty.BiDiGzipFilter@25fa74b6\n10:19:17.120 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f@84b6e75==com.sun.jersey.spi.container.servlet.ServletContainer,1,true\n10:19:17.120 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.sun.jersey.spi.container.servlet.ServletContainer-6b8dfe2f@84b6e75==com.sun.jersey.spi.container.servlet.ServletContainer,1,true\n10:19:17.120 helios[14151]: DEBUG [AgentService STARTING] ServletHolder: Filter.init com.sun.jersey.spi.container.servlet.ServletContainer@49162edf\n10:19:17.130 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:/status/hosts/helios-agent-host/up serverPath:/status/hosts/helios-agent-host/up finished:false header:: 6,1  replyHeader:: 6,400145,0  request:: '/status/hosts/helios-agent-host/up,,v{s{31,s{'world,'anyone}}},1  response:: '/status/hosts/helios-agent-host/up \n10:19:17.131 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 7,3  replyHeader:: 7,400145,0  request:: '/status/hosts/helios-agent-host,F  response:: s{18,18,1446052557907,1446052557907,0,46,0,0,0,6,400145} \n10:19:17.132 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 8,3  replyHeader:: 8,400145,0  request:: '/status/hosts/helios-agent-host,F  response:: s{18,18,1446052557907,1446052557907,0,46,0,0,0,6,400145} \n10:19:17.133 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 9,3  replyHeader:: 9,400145,0  request:: '/status/hosts/helios-agent-host,F  response:: s{18,18,1446052557907,1446052557907,0,46,0,0,0,6,400145} \n10:19:17.134 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 10,3  replyHeader:: 10,400145,0  request:: '/status,F  response:: s{4,4,1446052557726,1446052557726,0,3,0,0,0,3,15} \n10:19:17.135 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 11,3  replyHeader:: 11,400145,0  request:: '/status/hosts/helios-agent-host/environment,F  response:: s{297744,384673,1446107828884,1446559355698,17,0,0,0,2,0,297744} \n10:19:17.138 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:/status/hosts/helios-agent-host/up serverPath:/status/hosts/helios-agent-host/up finished:false header:: 12,3  replyHeader:: 12,400145,0  request:: '/status/hosts/helios-agent-host/up,T  response:: s{400145,400145,1446715157091,1446715157091,0,0,0,94768500370047018,0,0,400145} \n10:19:17.140 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 13,3  replyHeader:: 13,400145,0  request:: '/status/hosts/helios-agent-host/labels,F  response:: s{297745,384672,1446107828889,1446559355695,17,0,0,0,2,0,297745} \n10:19:17.141 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 14,3  replyHeader:: 14,400145,0  request:: '/status/hosts/helios-agent-host/agentinfo,F  response:: s{29,399922,1446052609183,1446711858715,5554,0,0,0,608,0,29} \n10:19:17.142 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 15,3  replyHeader:: 15,400145,0  request:: '/status/hosts,F  response:: s{11,11,1446052557787,1446052557787,0,19,0,0,0,3,351126} \n10:19:17.154 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 16,5  replyHeader:: 16,400146,0  request:: '/status/hosts/helios-agent-host/environment,#7b7d,-1  response:: s{297744,400146,1446107828884,1446715157138,18,0,0,0,2,0,297744} \n10:19:17.163 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 17,5  replyHeader:: 17,400147,0  request:: '/status/hosts/helios-agent-host/labels,#7b7d,-1  response:: s{297745,400147,1446107828889,1446715157140,18,0,0,0,2,0,297745} \n10:19:17.165 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 18,5  replyHeader:: 18,400148,0  request:: '/status/hosts/helios-agent-host/agentinfo,#7b22696e707574417267756d656e7473223a5b222d44636f6d2e73756e2e6d616e6167656d656e742e6a6d7872656d6f74652e706f72743d39323033222c222d44636f6d2e73756e2e6d616e6167656d656e742e6a6d7872656d6f74652e726d692e706f72743d39323033222c222d44636f6d2e73756e2e6d616e6167656d656e742e6a6d7872656d6f74652e73736c3d66616c7365222c222d44636f6d2e73756e2e6d616e6167656d656e742e6a6d7872656d6f74652e61757468656e7469636174653d66616c7365222c222d446a6176612e6e65742e70726566657249507634537461636b3d74727565222c222d6167656e746c69623a6a6477703d7472616e73706f72743d64745f736f636b65742c7365727665723d792c73757370656e643d6e2c616464726573733d35303036225d2c226e616d65223a2231343135314075756434706477752e637730312e636f6e746977616e2e636f6d222c22737065634e61\n6d65223a224a617661205669727475616c204d616368696e652053706563696669636174696f6e222c227370656356656e646f72223a224f7261636c6520436f72706f726174696f6e222c227370656356657273696f6e223a22312e37222c22737461727454696d65223a313434363731353135353636302c22757074696d65223a313430362c2276657273696f6e223a22302e382e353632222c22766d4e616d65223a224f70656e4a444b2036342d4269742053657276657220564d222c22766d56656e646f72223a224f7261636c6520436f72706f726174696f6e222c22766d56657273696f6e223a2232342e38352d623033227d,-1  response:: s{29,400148,1446052609183,1446715157141,5555,0,0,0,604,0,29} \n10:19:17.166 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:/status/hosts serverPath:/status/hosts finished:false header:: 19,12  replyHeader:: 19,400148,0  request:: '/status/hosts,T  response:: v{'xxxx018,'helios-agent-host,'helios-master-host},s{11,11,1446052557787,1446052557787,0,19,0,0,0,3,351126} \n10:19:17.169 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:/status/hosts/xxxx018 serverPath:/status/hosts/xxxx018 finished:false header:: 20,4  replyHeader:: 20,400148,0  request:: '/status/hosts/xxxx018,T  response:: ,s{350448,350448,1446128476820,1446128476820,0,26,0,0,0,6,386007} \n10:19:17.170 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:/status/hosts/helios-agent-host serverPath:/status/hosts/helios-agent-host finished:false header:: 21,4  replyHeader:: 21,400148,0  request:: '/status/hosts/helios-agent-host,T  response:: ,s{18,18,1446052557907,1446052557907,0,46,0,0,0,6,400145} \n10:19:17.171 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:/status/hosts/helios-master-host serverPath:/status/hosts/helios-master-host finished:false header:: 22,4  replyHeader:: 22,400148,0  request:: '/status/hosts/helios-master-host,T  response:: ,s{301105,301105,1446109821539,1446109821539,0,8,0,0,0,6,386023} \n10:19:17.198 helios[14151]: INFO  [AgentService STARTING] WebApplicationImpl: Initiating Jersey application, version 'Jersey: 1.18.1 02/19/2014 03:28 AM'\n10:19:17.236 helios[14151]: DEBUG [AgentService STARTING] DropwizardResourceConfig: resources = [com.spotify.helios.agent.AgentModelTaskResource, com.spotify.helios.agent.AgentModelTaskStatusResource]\n10:19:17.236 helios[14151]: DEBUG [AgentService STARTING] DropwizardResourceConfig: providers = [io.dropwizard.jersey.caching.CacheControlledResourceMethodDispatchAdapter, io.dropwizard.jersey.guava.OptionalResourceMethodDispatchAdapter, io.dropwizard.jersey.guava.OptionalQueryParamInjectableProvider, io.dropwizard.jersey.validation.ConstraintViolationExceptionMapper, io.dropwizard.jersey.jackson.JsonProcessingExceptionMapper, com.codahale.metrics.jersey.InstrumentedResourceMethodDispatchAdapter, io.dropwizard.errors.EarlyEofExceptionMapper]\n10:19:17.256 helios[14151]: INFO  [AgentService STARTING] DropwizardResourceConfig: The following paths were found for the configured resources:\nGET     /helios/tasks (com.spotify.helios.agent.AgentModelTaskResource)\nGET     /helios/taskstatus (com.spotify.helios.agent.AgentModelTaskStatusResource)\n\n10:19:17.440 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAddCookies: CookieSpec selected: default\n10:19:17.440 helios[14151]: DEBUG [jersey-client-async-executor-0] RequestAddCookies: CookieSpec selected: default\n10:19:17.445 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAuthCache: Auth cache not set in the context\n10:19:17.445 helios[14151]: DEBUG [jersey-client-async-executor-0] RequestAuthCache: Auth cache not set in the context\n10:19:17.446 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection request: [route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:19:17.446 helios[14151]: DEBUG [jersey-client-async-executor-0] PoolingHttpClientConnectionManager: Connection request: [route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:19:17.454 helios[14151]: DEBUG [jersey-client-async-executor-0] PoolingHttpClientConnectionManager: Connection leased: [id: 1][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 2 of 100; total allocated: 2 of 100]\n10:19:17.454 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection leased: [id: 0][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 2 of 100; total allocated: 2 of 100]\n10:19:17.455 helios[14151]: DEBUG [jersey-client-async-executor-1] MainClientExec: Opening connection {}->unix://localhost:80\n10:19:17.455 helios[14151]: DEBUG [jersey-client-async-executor-0] MainClientExec: Opening connection {}->unix://localhost:80\n10:19:17.543 helios[14151]: INFO  [AgentService STARTING] ContextHandler: Started i.d.j.MutableServletContextHandler@3b60b48a{/,null,AVAILABLE}\n10:19:17.543 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED i.d.j.MutableServletContextHandler@3b60b48a{/,null,AVAILABLE}\n10:19:17.544 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.codahale.metrics.jetty9.InstrumentedHandler@33dba6d1\n10:19:17.544 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting i.d.j.MutableServletContextHandler@71bc895{/,null,null}\n10:19:17.544 helios[14151]: INFO  [AgentService STARTING] AdminEnvironment: tasks = \nPOST    /tasks/gc (io.dropwizard.servlets.tasks.GarbageCollectionTask)\n\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] AdminEnvironment: health checks = [deadlocks, docker, zookeeper]\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting i.d.j.MutableServletContextHandler@71bc895{/,null,STARTING}\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@310db1d8\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: filterNameMap={io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97=io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97}\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: pathFilters=[[/]/[]==1=>io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97]\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: servletFilterMap={}\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: servletPathMap={/tasks/=tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true, /*=com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true}\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] ServletHandler: servletNameMap={com.codahale.metrics.servlets.AdminServlet-3c873f94=com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true, tasks=tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true}\n10:19:17.545 helios[14151]: DEBUG [AgentService STARTING] AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@310db1d8\n10:19:17.546 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@310db1d8\n10:19:17.546 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true\n10:19:17.546 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED tasks@6907b8e==io.dropwizard.servlets.tasks.TaskServlet,1,true\n10:19:17.546 helios[14151]: DEBUG [AgentService STARTING] ServletHolder: Filter.init io.dropwizard.servlets.tasks.TaskServlet@63c29805\n10:19:17.546 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true\n10:19:17.546 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.codahale.metrics.servlets.AdminServlet-3c873f94@179a3f85==com.codahale.metrics.servlets.AdminServlet,1,true\n10:19:17.546 helios[14151]: DEBUG [AgentService STARTING] ServletHolder: Filter.init com.codahale.metrics.servlets.AdminServlet@7c8aa36d\n10:19:17.549 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97\n10:19:17.549 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED io.dropwizard.jersey.filter.AllowedMethodsFilter-535f2c97\n10:19:17.549 helios[14151]: DEBUG [AgentService STARTING] FilterHolder: Filter.init io.dropwizard.jersey.filter.AllowedMethodsFilter@31b54801\n10:19:17.550 helios[14151]: INFO  [AgentService STARTING] ContextHandler: Started i.d.j.MutableServletContextHandler@71bc895{/,null,AVAILABLE}\n10:19:17.550 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED i.d.j.MutableServletContextHandler@71bc895{/,null,AVAILABLE}\n10:19:17.550 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED io.dropwizard.jetty.RoutingHandler@b7d2830\n10:19:17.550 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.RequestLogHandler@4656b3b1\n10:19:17.550 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.StatisticsHandler@154162c3\n10:19:17.550 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting application@1f7d19f2{HTTP/1.1}{0.0.0.0:5803}\n10:19:17.550 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.util.thread.ScheduledExecutorScheduler@31741ab7\n10:19:17.550 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.util.thread.ScheduledExecutorScheduler@31741ab7\n10:19:17.550 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.codahale.metrics.jetty9.InstrumentedConnectionFactory@4f22f4e6\n10:19:17.551 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting HttpConnectionFactory@665dd0e2{HTTP/1.1}\n10:19:17.551 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED HttpConnectionFactory@665dd0e2{HTTP/1.1}\n10:19:17.551 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.codahale.metrics.jetty9.InstrumentedConnectionFactory@4f22f4e6\n10:19:17.551 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.server.ServerConnector$ServerConnectorManager@7ebdcdf7\n10:19:17.551 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@1d6807d4 keys=-1 selected=-1\n10:19:17.551 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@1d6807d4 keys=0 selected=0\n10:19:17.552 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=8<=1024,i=8,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@1d6807d4 keys=0 selected=0\n10:19:17.552 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@5b456a1f keys=-1 selected=-1\n10:19:17.552 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@5b456a1f keys=0 selected=0\n10:19:17.552 helios[14151]: DEBUG [dw-46-selector-0] SelectorManager: Starting Thread[dw-46-selector-0,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@1d6807d4 keys=0 selected=0\n10:19:17.552 helios[14151]: DEBUG [dw-46-selector-0] SelectorManager: Selector loop waiting on select\n10:19:17.552 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=8<=1024,i=7,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@5b456a1f keys=0 selected=0\n10:19:17.553 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@5f4f60b0 keys=-1 selected=-1\n10:19:17.553 helios[14151]: DEBUG [dw-47-selector-1] SelectorManager: Starting Thread[dw-47-selector-1,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@5b456a1f keys=0 selected=0\n10:19:17.553 helios[14151]: DEBUG [dw-47-selector-1] SelectorManager: Selector loop waiting on select\n10:19:17.553 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@5f4f60b0 keys=0 selected=0\n10:19:17.553 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=8<=1024,i=6,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@5f4f60b0 keys=0 selected=0\n10:19:17.553 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@2c5506e9 keys=-1 selected=-1\n10:19:17.553 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@2c5506e9 keys=0 selected=0\n10:19:17.553 helios[14151]: DEBUG [dw-49-selector-2] SelectorManager: Starting Thread[dw-49-selector-2,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@5f4f60b0 keys=0 selected=0\n10:19:17.553 helios[14151]: DEBUG [dw-49-selector-2] SelectorManager: Selector loop waiting on select\n10:19:17.553 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=8<=1024,i=5,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@2c5506e9 keys=0 selected=0\n10:19:17.554 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@2c35fbb6 keys=-1 selected=-1\n10:19:17.554 helios[14151]: DEBUG [dw-51-selector-3] SelectorManager: Starting Thread[dw-51-selector-3,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@2c5506e9 keys=0 selected=0\n10:19:17.554 helios[14151]: DEBUG [dw-51-selector-3] SelectorManager: Selector loop waiting on select\n10:19:17.554 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@2c35fbb6 keys=0 selected=0\n10:19:17.554 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=8<=1024,i=4,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@2c35fbb6 keys=0 selected=0\n10:19:17.554 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@2a65ea1a keys=-1 selected=-1\n10:19:17.554 helios[14151]: DEBUG [dw-52-selector-4] SelectorManager: Starting Thread[dw-52-selector-4,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@2c35fbb6 keys=0 selected=0\n10:19:17.554 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@2a65ea1a keys=0 selected=0\n10:19:17.554 helios[14151]: DEBUG [dw-52-selector-4] SelectorManager: Selector loop waiting on select\n10:19:17.555 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=8<=1024,i=3,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@2a65ea1a keys=0 selected=0\n10:19:17.555 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@4cdf1446 keys=-1 selected=-1\n10:19:17.555 helios[14151]: DEBUG [dw-50-selector-5] SelectorManager: Starting Thread[dw-50-selector-5,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@2a65ea1a keys=0 selected=0\n10:19:17.555 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@4cdf1446 keys=0 selected=0\n10:19:17.555 helios[14151]: DEBUG [dw-50-selector-5] SelectorManager: Selector loop waiting on select\n10:19:17.555 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=8<=1024,i=2,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@4cdf1446 keys=0 selected=0\n10:19:17.555 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@2d64880e keys=-1 selected=-1\n10:19:17.555 helios[14151]: DEBUG [dw-53-selector-6] SelectorManager: Starting Thread[dw-53-selector-6,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@4cdf1446 keys=0 selected=0\n10:19:17.555 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@2d64880e keys=0 selected=0\n10:19:17.555 helios[14151]: DEBUG [dw-53-selector-6] SelectorManager: Selector loop waiting on select\n10:19:17.556 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=8<=1024,i=1,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@2d64880e keys=0 selected=0\n10:19:17.556 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.server.ServerConnector$ServerConnectorManager@7ebdcdf7\n10:19:17.556 helios[14151]: DEBUG [dw-54-selector-7] SelectorManager: Starting Thread[dw-54-selector-7,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@2d64880e keys=0 selected=0\n10:19:17.556 helios[14151]: DEBUG [dw-54-selector-7] SelectorManager: Selector loop waiting on select\n10:19:17.556 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=9<=1024,i=0,q=0} dispatched org.eclipse.jetty.server.AbstractConnector$Acceptor@183ee521\n10:19:17.556 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=9<=1024,i=1,q=0} dispatched org.eclipse.jetty.server.AbstractConnector$Acceptor@7282ee7a\n10:19:17.557 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=10<=1024,i=1,q=0} dispatched org.eclipse.jetty.server.AbstractConnector$Acceptor@7d4cf957\n10:19:17.557 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw{STARTED,8<=11<=1024,i=1,q=1} dispatched org.eclipse.jetty.server.AbstractConnector$Acceptor@84a83ca\n10:19:17.557 helios[14151]: INFO  [AgentService STARTING] ServerConnector: Started application@1f7d19f2{HTTP/1.1}{0.0.0.0:5803}\n10:19:17.557 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED application@1f7d19f2{HTTP/1.1}{0.0.0.0:5803}\n10:19:17.557 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting admin@e2e30ea{HTTP/1.1}{0.0.0.0:5804}\n10:19:17.557 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.util.thread.ScheduledExecutorScheduler@566b3836\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.util.thread.ScheduledExecutorScheduler@566b3836\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting com.codahale.metrics.jetty9.InstrumentedConnectionFactory@19378d8d\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting HttpConnectionFactory@7e7eb4f6{HTTP/1.1}\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED HttpConnectionFactory@7e7eb4f6{HTTP/1.1}\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED com.codahale.metrics.jetty9.InstrumentedConnectionFactory@19378d8d\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.server.ServerConnector$ServerConnectorManager@560a9228\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@7ecaf643 keys=-1 selected=-1\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@7ecaf643 keys=0 selected=0\n10:19:17.558 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=1<=64,i=1,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@7ecaf643 keys=0 selected=0\n10:19:17.559 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@565dd9a1 keys=-1 selected=-1\n10:19:17.559 helios[14151]: DEBUG [dw-admin-61-selector-0] SelectorManager: Starting Thread[dw-admin-61-selector-0,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@7ecaf643 keys=0 selected=0\n10:19:17.559 helios[14151]: DEBUG [dw-admin-61-selector-0] SelectorManager: Selector loop waiting on select\n10:19:17.559 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@565dd9a1 keys=0 selected=0\n10:19:17.559 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=2<=64,i=1,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@565dd9a1 keys=0 selected=0\n10:19:17.559 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@781c1417 keys=-1 selected=-1\n10:19:17.559 helios[14151]: DEBUG [dw-admin-73-selector-1] SelectorManager: Starting Thread[dw-admin-73-selector-1,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@565dd9a1 keys=0 selected=0\n10:19:17.559 helios[14151]: DEBUG [dw-admin-73-selector-1] SelectorManager: Selector loop waiting on select\n10:19:17.559 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@781c1417 keys=0 selected=0\n10:19:17.560 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=3<=64,i=1,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@781c1417 keys=0 selected=0\n10:19:17.560 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@5eef3d7b keys=-1 selected=-1\n10:19:17.560 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@5eef3d7b keys=0 selected=0\n10:19:17.560 helios[14151]: DEBUG [dw-admin-74-selector-2] SelectorManager: Starting Thread[dw-admin-74-selector-2,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@781c1417 keys=0 selected=0\n10:19:17.560 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=4<=64,i=1,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@5eef3d7b keys=0 selected=0\n10:19:17.560 helios[14151]: DEBUG [dw-admin-74-selector-2] SelectorManager: Selector loop waiting on select\n10:19:17.560 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@5d8f1dcf keys=-1 selected=-1\n10:19:17.560 helios[14151]: DEBUG [dw-admin-75-selector-3] SelectorManager: Starting Thread[dw-admin-75-selector-3,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@5eef3d7b keys=0 selected=0\n10:19:17.560 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@5d8f1dcf keys=0 selected=0\n10:19:17.560 helios[14151]: DEBUG [dw-admin-75-selector-3] SelectorManager: Selector loop waiting on select\n10:19:17.561 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=5<=64,i=1,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@5d8f1dcf keys=0 selected=0\n10:19:17.561 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@59687025 keys=-1 selected=-1\n10:19:17.561 helios[14151]: DEBUG [dw-admin-76-selector-4] SelectorManager: Starting Thread[dw-admin-76-selector-4,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@5d8f1dcf keys=0 selected=0\n10:19:17.561 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@59687025 keys=0 selected=0\n10:19:17.561 helios[14151]: DEBUG [dw-admin-76-selector-4] SelectorManager: Selector loop waiting on select\n10:19:17.561 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=6<=64,i=1,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@59687025 keys=0 selected=0\n10:19:17.561 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@988c127 keys=-1 selected=-1\n10:19:17.561 helios[14151]: DEBUG [dw-admin-77-selector-5] SelectorManager: Starting Thread[dw-admin-77-selector-5,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@59687025 keys=0 selected=0\n10:19:17.562 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@988c127 keys=0 selected=0\n10:19:17.562 helios[14151]: DEBUG [dw-admin-77-selector-5] SelectorManager: Selector loop waiting on select\n10:19:17.562 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=7<=64,i=1,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@988c127 keys=0 selected=0\n10:19:17.562 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: starting org.eclipse.jetty.io.SelectorManager$ManagedSelector@7310516f keys=-1 selected=-1\n10:19:17.562 helios[14151]: DEBUG [dw-admin-78-selector-6] SelectorManager: Starting Thread[dw-admin-78-selector-6,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@988c127 keys=0 selected=0\n10:19:17.562 helios[14151]: DEBUG [dw-admin-78-selector-6] SelectorManager: Selector loop waiting on select\n10:19:17.562 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.io.SelectorManager$ManagedSelector@7310516f keys=0 selected=0\n10:19:17.562 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=8<=64,i=1,q=0} dispatched org.eclipse.jetty.io.SelectorManager$ManagedSelector@7310516f keys=0 selected=0\n10:19:17.562 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.server.ServerConnector$ServerConnectorManager@560a9228\n10:19:17.563 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=9<=64,i=0,q=0} dispatched org.eclipse.jetty.server.AbstractConnector$Acceptor@764a6b8e\n10:19:17.563 helios[14151]: DEBUG [dw-admin-79-selector-7] SelectorManager: Starting Thread[dw-admin-79-selector-7,5,main] on org.eclipse.jetty.io.SelectorManager$ManagedSelector@7310516f keys=0 selected=0\n10:19:17.563 helios[14151]: DEBUG [dw-admin-79-selector-7] SelectorManager: Selector loop waiting on select\n10:19:17.563 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=9<=64,i=1,q=0} dispatched org.eclipse.jetty.server.AbstractConnector$Acceptor@10b0b504\n10:19:17.563 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=10<=64,i=0,q=1} dispatched org.eclipse.jetty.server.AbstractConnector$Acceptor@413c222b\n10:19:17.563 helios[14151]: DEBUG [AgentService STARTING] QueuedThreadPool: dw-admin{STARTED,1<=10<=64,i=0,q=2} dispatched org.eclipse.jetty.server.AbstractConnector$Acceptor@5dafbf53\n10:19:17.564 helios[14151]: INFO  [AgentService STARTING] ServerConnector: Started admin@e2e30ea{HTTP/1.1}{0.0.0.0:5804}\n10:19:17.564 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED admin@e2e30ea{HTTP/1.1}{0.0.0.0:5804}\n10:19:17.564 helios[14151]: DEBUG [AgentService STARTING] AbstractLifeCycle: STARTED org.eclipse.jetty.server.Server@c2028db\n10:19:17.619 helios[14151]: DEBUG [jersey-client-async-executor-0] DefaultHttpClientConnectionOperator: Connecting to localhost/127.0.0.1:80\n10:19:17.619 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultHttpClientConnectionOperator: Connecting to localhost/127.0.0.1:80\n10:19:17.630 helios[14151]: DEBUG [jersey-client-async-executor-0] DefaultManagedHttpClientConnection: http-outgoing-1: Shutdown connection\n10:19:17.630 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-0: Shutdown connection\n10:19:17.652 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-0: Close connection\n10:19:17.652 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection released: [id: 0][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 1 of 100; total allocated: 1 of 100]\n10:19:17.652 helios[14151]: DEBUG [jersey-client-async-executor-0] DefaultManagedHttpClientConnection: http-outgoing-1: Close connection\n10:19:17.652 helios[14151]: DEBUG [jersey-client-async-executor-0] PoolingHttpClientConnectionManager: Connection released: [id: 1][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:19:17.655 helios[14151]: ERROR [Reactor(agent)] Reaper: reaping failed\ncom.spotify.docker.client.DockerException: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.DefaultDockerClient.propagate(DefaultDockerClient.java:1141) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1072) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.listContainers(DefaultDockerClient.java:314) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Reaper.reap0(Reaper.java:60) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Reaper.reap(Reaper.java:51) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Agent$Update.run(Agent.java:190) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.servicescommon.DefaultReactor.run(DefaultReactor.java:100) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.AbstractExecutionThreadService$1$2.run(AbstractExecutionThreadService.java:60) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.Callables$3.run(Callables.java:95) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_85]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_85]\n    at java.lang.Thread.run(Thread.java:745) [na:1.7.0_85]\nCaused by: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1070) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 10 common frames omitted\nCaused by: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:517) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector$1.run(ApacheConnector.java:527) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    at jersey.repackaged.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:49) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:45) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:523) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$1.run(ClientRuntime.java:169) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:315) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:297) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:320) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$2.run(ClientRuntime.java:201) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    ... 3 common frames omitted\nCaused by: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.ApacheUnixSocket.setSoLinger(ApacheUnixSocket.java:165) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.BHttpConnectionBase.shutdown(BHttpConnectionBase.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.DefaultManagedHttpClientConnection.shutdown(DefaultManagedHttpClientConnection.java:97) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.LoggingManagedHttpClientConnection.shutdown(LoggingManagedHttpClientConnection.java:89) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolEntry.shutdownConnection(CPoolEntry.java:74) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolProxy.shutdown(CPoolProxy.java:96) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ConnectionHolder.abortConnection(ConnectionHolder.java:127) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:352) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:71) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:469) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 20 common frames omitted\n10:19:17.656 helios[14151]: DEBUG [Reactor(agent)] Agent: tasks: {}\n10:19:17.656 helios[14151]: DEBUG [Reactor(agent)] Agent: executions: {}\n10:19:17.656 helios[14151]: DEBUG [Reactor(agent)] Agent: supervisors: {}\n10:19:17.660 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 23,3  replyHeader:: 23,400148,0  request:: '/status/hosts/helios-agent-host,F  response:: s{18,18,1446052557907,1446052557907,0,46,0,0,0,6,400145} \n10:19:17.661 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 24,3  replyHeader:: 24,400148,0  request:: '/status/hosts/helios-agent-host/hostinfo,F  response:: s{30,399923,1446052613595,1446711867032,5554,0,0,0,446,0,30} \n10:19:17.663 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAddCookies: CookieSpec selected: default\n10:19:17.663 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAuthCache: Auth cache not set in the context\n10:19:17.663 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection request: [route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:19:17.663 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection leased: [id: 2][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 1 of 100; total allocated: 1 of 100]\n10:19:17.663 helios[14151]: DEBUG [jersey-client-async-executor-1] MainClientExec: Opening connection {}->unix://localhost:80\n10:19:17.663 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultHttpClientConnectionOperator: Connecting to localhost/127.0.0.1:80\n10:19:17.663 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-2: Shutdown connection\n10:19:17.663 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-2: Close connection\n10:19:17.663 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection released: [id: 2][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:19:17.664 helios[14151]: ERROR [Reactor(agent)] Reaper: reaping failed\ncom.spotify.docker.client.DockerException: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.DefaultDockerClient.propagate(DefaultDockerClient.java:1141) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1072) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.listContainers(DefaultDockerClient.java:314) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Reaper.reap0(Reaper.java:60) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Reaper.reap(Reaper.java:51) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Agent$Update.run(Agent.java:190) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.servicescommon.DefaultReactor.run(DefaultReactor.java:100) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.AbstractExecutionThreadService$1$2.run(AbstractExecutionThreadService.java:60) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.Callables$3.run(Callables.java:95) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_85]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_85]\n    at java.lang.Thread.run(Thread.java:745) [na:1.7.0_85]\nCaused by: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1070) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 10 common frames omitted\nCaused by: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:517) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector$1.run(ApacheConnector.java:527) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    at jersey.repackaged.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:49) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:45) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:523) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$1.run(ClientRuntime.java:169) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:315) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:297) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:320) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$2.run(ClientRuntime.java:201) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    ... 3 common frames omitted\nCaused by: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.ApacheUnixSocket.setSoLinger(ApacheUnixSocket.java:165) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.BHttpConnectionBase.shutdown(BHttpConnectionBase.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.DefaultManagedHttpClientConnection.shutdown(DefaultManagedHttpClientConnection.java:97) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.LoggingManagedHttpClientConnection.shutdown(LoggingManagedHttpClientConnection.java:89) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolEntry.shutdownConnection(CPoolEntry.java:74) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolProxy.shutdown(CPoolProxy.java:96) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ConnectionHolder.abortConnection(ConnectionHolder.java:127) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:352) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:71) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:469) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 20 common frames omitted\n10:19:17.664 helios[14151]: DEBUG [Reactor(agent)] Agent: tasks: {}\n10:19:17.664 helios[14151]: DEBUG [Reactor(agent)] Agent: executions: {}\n10:19:17.664 helios[14151]: DEBUG [Reactor(agent)] Agent: supervisors: {}\n10:19:17.671 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 25,5  replyHeader:: 25,400149,0  request:: '/status/hosts/helios-agent-host/hostinfo,#7b22617263686974656374757265223a22616d643634222c2263707573223a382c22646f636b65724365727450617468223a6e756c6c2c22646f636b6572486f7374223a22756e69783a2f2f2f7661722f72756e2f646f636b65722e736f636b222c22646f636b657256657273696f6e223a6e756c6c2c22686f73746e616d65223a227575643470647775222c226c6f6164417667223a312e32312c226d656d6f7279467265654279746573223a3638323139323839362c226d656d6f7279546f74616c4279746573223a31363737303132393932302c226f734e616d65223a224c696e7578222c226f7356657273696f6e223a22332e31332e302d36362d67656e65726963222c2273776170467265654279746573223a333832393630383434382c2273776170546f74616c4279746573223a343239343936333230302c22756e616d65223a224c696e757820757564347064777520332e31332e302d36362d67656e657\n2696320233130387e70726563697365312d5562756e747520534d5020546875204f637420382031303a30373a3336205554432032303135207838365f3634207838365f3634207838365f363420474e552f4c696e7578227d,-1  response:: s{30,400149,1446052613595,1446715157661,5555,0,0,0,446,0,30} \n10:19:18.063 helios[14151]: DEBUG [pool-10-thread-1] TaskHistoryWriter: writing queued item to zookeeper testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548 1446215842271\n10:19:18.065 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 26,3  replyHeader:: 26,400149,0  request:: '/history,F  response:: s{12,12,1446052557798,1446052557798,0,1,0,0,0,1,13} \n10:19:18.066 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 27,3  replyHeader:: 27,400149,0  request:: '/history/jobs,F  response:: s{13,13,1446052557805,1446052557805,0,24,0,0,0,2,351524} \n10:19:18.067 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 28,3  replyHeader:: 28,400149,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548,F  response:: s{351524,351524,1446136946738,1446136946738,0,1,0,0,0,1,351525} \n10:19:18.068 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 29,3  replyHeader:: 29,400149,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts,F  response:: s{351525,351525,1446136946750,1446136946750,0,1,0,0,0,1,351526} \n10:19:18.069 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 30,3  replyHeader:: 30,400149,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts/helios-agent-host,F  response:: s{351526,351526,1446136946759,1446136946759,0,1,0,0,0,1,351527} \n10:19:18.070 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 31,3  replyHeader:: 31,400149,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts/helios-agent-host/events,F  response:: s{351527,351527,1446136946767,1446136946767,0,1216,0,0,0,30,358976} \n10:19:18.096 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 32,1  replyHeader:: 32,400150,-110  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts/helios-agent-host/events/1446215842271,#7b22636f6e7461696e65724964223a6e756c6c2c22656e76223a7b7d2c22676f616c223a22554e4445504c4f59222c226a6f62223a7b22636f6d6d616e64223a5b222f62696e2f7368202d6320277768696c6520747275653b20646f20656e763b20736c6565702036303b20646f6e6527225d2c2263726561746564223a313434363139393837303238322c226372656174696e6755736572223a227569646733323030222c22656e76223a7b7d2c2265787069726573223a6e756c6c2c226772616365506572696f64223a6e756c6c2c226865616c7468436865636b223a6e756c6c2c22686f73746e616d65223a6e756c6c2c226964223a22746573746a6f623a313a39643636353834613361646337663363396164306336393439396565326466346361613434353438222c22696d616765223a227562756e74753a31342e3\n034222c226d65746164617461223a7b7d2c226e6574776f726b4d6f6465223a6e756c6c2c22706f727473223a7b7d2c22726567697374726174696f6e223a7b7d2c22726567697374726174696f6e446f6d61696e223a22222c227265736f7572636573223a6e756c6c2c2273656375726974794f7074223a5b5d2c22746f6b656e223a22222c22766f6c756d6573223a7b7d7d2c22706f727473223a7b7d2c227374617465223a2253544f50504544222c227468726f74746c6564223a224e4f227d,v{s{31,s{'world,'anyone}}},0  response::\n10:19:18.100 helios[14151]: DEBUG [pool-10-thread-1] TaskHistoryWriter: item we wanted in is already there\n10:19:22.076 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 33,3  replyHeader:: 33,400151,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974} \n10:19:27.078 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 34,3  replyHeader:: 34,400151,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974} \n10:19:32.080 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 35,3  replyHeader:: 35,400151,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974} \n10:19:37.081 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 36,3  replyHeader:: 36,400151,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974} \n10:19:42.083 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 37,3  replyHeader:: 37,400151,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974} \n10:19:47.076 helios[14151]: DEBUG [Reactor(zk-ppcc:/config/hosts/helios-agent-host/jobs)] PersistentPathChildrenCache: updating: /config/hosts/helios-agent-host/jobs\n10:19:47.084 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 38,3  replyHeader:: 38,400151,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974} \n10:19:47.666 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAddCookies: CookieSpec selected: default\n10:19:47.666 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAuthCache: Auth cache not set in the context\n10:19:47.666 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection request: [route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:19:47.666 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection leased: [id: 3][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 1 of 100; total allocated: 1 of 100]\n10:19:47.666 helios[14151]: DEBUG [jersey-client-async-executor-1] MainClientExec: Opening connection {}->unix://localhost:80\n10:19:47.667 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultHttpClientConnectionOperator: Connecting to localhost/127.0.0.1:80\n10:19:47.667 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-3: Shutdown connection\n10:19:47.667 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-3: Close connection\n10:19:47.667 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection released: [id: 3][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:19:47.668 helios[14151]: ERROR [Reactor(agent)] Reaper: reaping failed\ncom.spotify.docker.client.DockerException: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.DefaultDockerClient.propagate(DefaultDockerClient.java:1141) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1072) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.listContainers(DefaultDockerClient.java:314) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Reaper.reap0(Reaper.java:60) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Reaper.reap(Reaper.java:51) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Agent$Update.run(Agent.java:190) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.servicescommon.DefaultReactor.run(DefaultReactor.java:100) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.AbstractExecutionThreadService$1$2.run(AbstractExecutionThreadService.java:60) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.Callables$3.run(Callables.java:95) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_85]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_85]\n    at java.lang.Thread.run(Thread.java:745) [na:1.7.0_85]\nCaused by: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1070) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 10 common frames omitted\nCaused by: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:517) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector$1.run(ApacheConnector.java:527) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    at jersey.repackaged.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:49) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:45) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:523) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$1.run(ClientRuntime.java:169) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:315) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:297) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:320) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$2.run(ClientRuntime.java:201) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    ... 3 common frames omitted\nCaused by: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.ApacheUnixSocket.setSoLinger(ApacheUnixSocket.java:165) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.BHttpConnectionBase.shutdown(BHttpConnectionBase.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.DefaultManagedHttpClientConnection.shutdown(DefaultManagedHttpClientConnection.java:97) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.LoggingManagedHttpClientConnection.shutdown(LoggingManagedHttpClientConnection.java:89) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolEntry.shutdownConnection(CPoolEntry.java:74) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolProxy.shutdown(CPoolProxy.java:96) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ConnectionHolder.abortConnection(ConnectionHolder.java:127) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:352) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:71) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:469) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 20 common frames omitted\n10:19:47.669 helios[14151]: DEBUG [Reactor(agent)] Agent: tasks: {}\n10:19:47.669 helios[14151]: DEBUG [Reactor(agent)] Agent: executions: {}\n10:19:47.669 helios[14151]: DEBUG [Reactor(agent)] Agent: supervisors: {}\n10:19:52.086 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 39,3  replyHeader:: 39,400151,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974} \n10:19:57.087 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 40,3  replyHeader:: 40,400152,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974} \n10:20:02.089 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 41,3  replyHeader:: 41,400152,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974} \n10:20:07.091 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 42,3  replyHeader:: 42,400152,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974} \n10:20:12.092 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 43,3  replyHeader:: 43,400154,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974} \n10:20:17.076 helios[14151]: DEBUG [Reactor(zk-ppcc:/config/hosts/helios-agent-host/jobs)] PersistentPathChildrenCache: updating: /config/hosts/helios-agent-host/jobs\n10:20:17.094 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 44,3  replyHeader:: 44,400154,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974} \n10:20:17.167 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 45,3  replyHeader:: 45,400154,0  request:: '/status/hosts/helios-agent-host,F  response:: s{18,18,1446052557907,1446052557907,0,46,0,0,0,6,400145} \n10:20:17.168 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 46,3  replyHeader:: 46,400154,0  request:: '/status/hosts/helios-agent-host/agentinfo,F  response:: s{29,400148,1446052609183,1446715157141,5555,0,0,0,604,0,29} \n10:20:17.193 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 47,5  replyHeader:: 47,400155,0  request:: '/status/hosts/helios-agent-host/agentinfo,#7b22696e707574417267756d656e7473223a5b222d44636f6d2e73756e2e6d616e6167656d656e742e6a6d7872656d6f74652e706f72743d39323033222c222d44636f6d2e73756e2e6d616e6167656d656e742e6a6d7872656d6f74652e726d692e706f72743d39323033222c222d44636f6d2e73756e2e6d616e6167656d656e742e6a6d7872656d6f74652e73736c3d66616c7365222c222d44636f6d2e73756e2e6d616e6167656d656e742e6a6d7872656d6f74652e61757468656e7469636174653d66616c7365222c222d446a6176612e6e65742e70726566657249507634537461636b3d74727565222c222d6167656e746c69623a6a6477703d7472616e73706f72743d64745f736f636b65742c7365727665723d792c73757370656e643d6e2c616464726573733d35303036225d2c226e616d65223a2231343135314075756434706477752e637730312e636f6e746977616e2e636f6d222c22737065634e61\n6d65223a224a617661205669727475616c204d616368696e652053706563696669636174696f6e222c227370656356656e646f72223a224f7261636c6520436f72706f726174696f6e222c227370656356657273696f6e223a22312e37222c22737461727454696d65223a313434363731353135353636302c22757074696d65223a36313530362c2276657273696f6e223a22302e382e353632222c22766d4e616d65223a224f70656e4a444b2036342d4269742053657276657220564d222c22766d56656e646f72223a224f7261636c6520436f72706f726174696f6e222c22766d56657273696f6e223a2232342e38352d623033227d,-1  response:: s{29,400155,1446052609183,1446715217168,5556,0,0,0,605,0,29} \n10:20:17.670 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAddCookies: CookieSpec selected: default\n10:20:17.670 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAuthCache: Auth cache not set in the context\n10:20:17.670 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection request: [route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:20:17.670 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection leased: [id: 4][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 1 of 100; total allocated: 1 of 100]\n10:20:17.670 helios[14151]: DEBUG [jersey-client-async-executor-1] MainClientExec: Opening connection {}->unix://localhost:80\n10:20:17.670 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultHttpClientConnectionOperator: Connecting to localhost/127.0.0.1:80\n10:20:17.670 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-4: Shutdown connection\n10:20:17.670 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-4: Close connection\n10:20:17.670 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection released: [id: 4][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:20:17.672 helios[14151]: ERROR [Reactor(agent)] Reaper: reaping failed\ncom.spotify.docker.client.DockerException: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.DefaultDockerClient.propagate(DefaultDockerClient.java:1141) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1072) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.listContainers(DefaultDockerClient.java:314) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Reaper.reap0(Reaper.java:60) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Reaper.reap(Reaper.java:51) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Agent$Update.run(Agent.java:190) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.servicescommon.DefaultReactor.run(DefaultReactor.java:100) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.AbstractExecutionThreadService$1$2.run(AbstractExecutionThreadService.java:60) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.Callables$3.run(Callables.java:95) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_85]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_85]\n    at java.lang.Thread.run(Thread.java:745) [na:1.7.0_85]\nCaused by: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1070) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 10 common frames omitted\nCaused by: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:517) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector$1.run(ApacheConnector.java:527) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    at jersey.repackaged.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:49) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:45) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:523) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$1.run(ClientRuntime.java:169) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:315) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:297) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:320) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$2.run(ClientRuntime.java:201) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    ... 3 common frames omitted\nCaused by: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.ApacheUnixSocket.setSoLinger(ApacheUnixSocket.java:165) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.BHttpConnectionBase.shutdown(BHttpConnectionBase.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.DefaultManagedHttpClientConnection.shutdown(DefaultManagedHttpClientConnection.java:97) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.LoggingManagedHttpClientConnection.shutdown(LoggingManagedHttpClientConnection.java:89) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolEntry.shutdownConnection(CPoolEntry.java:74) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolProxy.shutdown(CPoolProxy.java:96) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ConnectionHolder.abortConnection(ConnectionHolder.java:127) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:352) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:71) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:469) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 20 common frames omitted\n10:20:17.672 helios[14151]: DEBUG [Reactor(agent)] Agent: tasks: {}\n10:20:17.672 helios[14151]: DEBUG [Reactor(agent)] Agent: executions: {}\n10:20:17.672 helios[14151]: DEBUG [Reactor(agent)] Agent: supervisors: {}\n10:20:17.674 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAddCookies: CookieSpec selected: default\n10:20:17.674 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAuthCache: Auth cache not set in the context\n10:20:17.674 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection request: [route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:20:17.674 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection leased: [id: 5][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 1 of 100; total allocated: 1 of 100]\n10:20:17.674 helios[14151]: DEBUG [jersey-client-async-executor-1] MainClientExec: Opening connection {}->unix://localhost:80\n10:20:17.674 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultHttpClientConnectionOperator: Connecting to localhost/127.0.0.1:80\n10:20:17.674 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-5: Shutdown connection\n10:20:17.674 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-5: Close connection\n10:20:17.674 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection released: [id: 5][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:20:17.676 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 48,3  replyHeader:: 48,400155,0  request:: '/status/hosts/helios-agent-host,F  response:: s{18,18,1446052557907,1446052557907,0,46,0,0,0,6,400145} \n10:20:17.676 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 49,3  replyHeader:: 49,400155,0  request:: '/status/hosts/helios-agent-host/hostinfo,F  response:: s{30,400149,1446052613595,1446715157661,5555,0,0,0,446,0,30} \n10:20:17.701 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 50,5  replyHeader:: 50,400156,0  request:: '/status/hosts/helios-agent-host/hostinfo,#7b22617263686974656374757265223a22616d643634222c2263707573223a382c22646f636b65724365727450617468223a6e756c6c2c22646f636b6572486f7374223a22756e69783a2f2f2f7661722f72756e2f646f636b65722e736f636b222c22646f636b657256657273696f6e223a6e756c6c2c22686f73746e616d65223a227575643470647775222c226c6f6164417667223a302e38352c226d656d6f7279467265654279746573223a313434303636393639362c226d656d6f7279546f74616c4279746573223a31363737303132393932302c226f734e616d65223a224c696e7578222c226f7356657273696f6e223a22332e31332e302d36362d67656e65726963222c2273776170467265654279746573223a333832393631363634302c2273776170546f74616c4279746573223a343239343936333230302c22756e616d65223a224c696e757820757564347064777520332e31332e302d36362d67656e6\n572696320233130387e70726563697365312d5562756e747520534d5020546875204f637420382031303a30373a3336205554432032303135207838365f3634207838365f3634207838365f363420474e552f4c696e7578227d,-1  response:: s{30,400156,1446052613595,1446715217676,5556,0,0,0,447,0,30} \n10:20:22.096 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 51,3  replyHeader:: 51,400157,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974} \n10:20:27.097 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 52,3  replyHeader:: 52,400157,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974} \n10:20:31.125 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Got notification sessionid:0x150af72f074002a\n10:20:31.126 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Got WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/config/hosts/helios-agent-host/jobs for sessionid 0x150af72f074002a\n10:20:31.126 helios[14151]: DEBUG [AgentMain STARTING-EventThread] PersistentPathChildrenCache: children event: WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/config/hosts/helios-agent-host/jobs\n10:20:31.126 helios[14151]: DEBUG [Reactor(zk-ppcc:/config/hosts/helios-agent-host/jobs)] PersistentPathChildrenCache: updating: /config/hosts/helios-agent-host/jobs\n10:20:31.126 helios[14151]: DEBUG [Reactor(zk-ppcc:/config/hosts/helios-agent-host/jobs)] PersistentPathChildrenCache: syncing: /config/hosts/helios-agent-host/jobs\n10:20:31.131 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 53,12  replyHeader:: 53,400158,0  request:: '/config/hosts/helios-agent-host/jobs,T  response:: v{'testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548},s{21,21,1446052557932,1446052557932,0,33,0,0,0,1,400158} \n10:20:31.131 helios[14151]: DEBUG [Reactor(zk-ppcc:/config/hosts/helios-agent-host/jobs)] PersistentPathChildrenCache: children: [testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548]\n10:20:31.132 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 54,4  replyHeader:: 54,400158,0  request:: '/config/hosts/helios-agent-host/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548,T  response:: #7b226465706c6f7965724d6173746572223a6e756c6c2c226465706c6f79657255736572223a227569646733323030222c226465706c6f796d656e7447726f75704e616d65223a6e756c6c2c22676f616c223a225354415254222c226a6f62223a7b22636f6d6d616e64223a5b222f62696e2f7368202d6320277768696c6520747275653b20646f20656e763b20736c6565702036303b20646f6e6527225d2c2263726561746564223a313434363139393837303238322c226372656174696e6755736572223a227569646733323030222c22656e76223a7b7d2c2265787069726573223a6e756c6c2c226772616365506572696f64223a6e756c6c2c226865616c7468436865636b223a6e756c6c2c22686f73746e616d65223a6e756c6c2c226964223a22746573746a6f623a313a3964363635383461336164633766336339616430633639\n3439396565326466346361613434353438222c22696d616765223a227562756e74753a31342e3034222c226d65746164617461223a7b7d2c226e6574776f726b4d6f6465223a6e756c6c2c22706f727473223a7b7d2c22726567697374726174696f6e223a7b7d2c22726567697374726174696f6e446f6d61696e223a22222c227265736f7572636573223a6e756c6c2c2273656375726974794f7074223a5b5d2c22746f6b656e223a22222c22766f6c756d6573223a7b7d7d7d,s{400158,400158,1446715231115,1446715231115,0,1,0,0,514,1,400158} \n10:20:31.132 helios[14151]: DEBUG [Reactor(zk-ppcc:/config/hosts/helios-agent-host/jobs)] PersistentPathChildrenCache: child: /config/hosts/helios-agent-host/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548={\"deployerMaster\":null,\"deployerUser\":\"myuser\",\"deploymentGroupName\":null,\"goal\":\"START\",\"job\":{\"command\":[\"/bin/sh -c 'while true; do env; sleep 60; done'\"],\"created\":1446199870282,\"creatingUser\":\"myuser\",\"env\":{},\"expires\":null,\"gracePeriod\":null,\"healthCheck\":null,\"hostname\":null,\"id\":\"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\",\"image\":\"ubuntu:14.04\",\"metadata\":{},\"networkMode\":null,\"ports\":{},\"registration\":{},\"registrationDomain\":\"\",\"resources\":null,\"securityOpt\":[],\"token\":\"\",\"volumes\":{}}}\n10:20:31.133 helios[14151]: DEBUG [Reactor(zk-ppcc:/config/hosts/helios-agent-host/jobs)] PersistentAtomicReference: set: (/var/lib/helios-agent/task-config.json) {/config/hosts/helios-agent-host/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548=Task{job=Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, resources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}, goal=START, deployerUser=myuser, deployerMaster=null, deploymentGroupName=null}}\n10:20:31.142 helios[14151]: DEBUG [Reactor(zk-ppcc:/config/hosts/helios-agent-host/jobs)] PersistentAtomicReference: write: (/var/lib/helios-agent/task-config.json.tmp) {\n  \"/config/hosts/helios-agent-host/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\" : {\n    \"deployerMaster\" : null,\n    \"deployerUser\" : \"myuser\",\n    \"deploymentGroupName\" : null,\n    \"goal\" : \"START\",\n    \"job\" : {\n      \"command\" : [ \"/bin/sh -c 'while true; do env; sleep 60; done'\" ],\n      \"created\" : 1446199870282,\n      \"creatingUser\" : \"myuser\",\n      \"env\" : { },\n      \"expires\" : null,\n      \"gracePeriod\" : null,\n      \"healthCheck\" : null,\n      \"hostname\" : null,\n      \"id\" : \"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\",\n      \"image\" : \"ubuntu:14.04\",\n      \"metadata\" : { },\n      \"networkMode\" : null,\n      \"ports\" : { },\n      \"registration\" : { },\n      \"registrationDomain\" : \"\",\n      \"resources\" : null,\n      \"securityOpt\" : [ ],\n      \"token\" : \"\",\n      \"volumes\" : { }\n    }\n  }\n}\n10:20:31.143 helios[14151]: DEBUG [Reactor(zk-ppcc:/config/hosts/helios-agent-host/jobs)] PersistentAtomicReference: move: /var/lib/helios-agent/task-config.json.tmp -> /var/lib/helios-agent/task-config.json\n10:20:31.145 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAddCookies: CookieSpec selected: default\n10:20:31.145 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAuthCache: Auth cache not set in the context\n10:20:31.145 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection request: [route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:20:31.145 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection leased: [id: 6][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 1 of 100; total allocated: 1 of 100]\n10:20:31.145 helios[14151]: DEBUG [jersey-client-async-executor-1] MainClientExec: Opening connection {}->unix://localhost:80\n10:20:31.146 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultHttpClientConnectionOperator: Connecting to localhost/127.0.0.1:80\n10:20:31.146 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-6: Shutdown connection\n10:20:31.146 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-6: Close connection\n10:20:31.146 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection released: [id: 6][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:20:31.147 helios[14151]: ERROR [Reactor(agent)] Reaper: reaping failed\ncom.spotify.docker.client.DockerException: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.DefaultDockerClient.propagate(DefaultDockerClient.java:1141) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1072) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.listContainers(DefaultDockerClient.java:314) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Reaper.reap0(Reaper.java:60) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Reaper.reap(Reaper.java:51) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Agent$Update.run(Agent.java:190) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.servicescommon.DefaultReactor.run(DefaultReactor.java:100) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.AbstractExecutionThreadService$1$2.run(AbstractExecutionThreadService.java:60) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.Callables$3.run(Callables.java:95) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_85]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_85]\n    at java.lang.Thread.run(Thread.java:745) [na:1.7.0_85]\nCaused by: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1070) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 10 common frames omitted\nCaused by: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:517) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector$1.run(ApacheConnector.java:527) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    at jersey.repackaged.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:49) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:45) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:523) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$1.run(ClientRuntime.java:169) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:315) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:297) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:320) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$2.run(ClientRuntime.java:201) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    ... 3 common frames omitted\nCaused by: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.ApacheUnixSocket.setSoLinger(ApacheUnixSocket.java:165) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.BHttpConnectionBase.shutdown(BHttpConnectionBase.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.DefaultManagedHttpClientConnection.shutdown(DefaultManagedHttpClientConnection.java:97) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.LoggingManagedHttpClientConnection.shutdown(LoggingManagedHttpClientConnection.java:89) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolEntry.shutdownConnection(CPoolEntry.java:74) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolProxy.shutdown(CPoolProxy.java:96) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ConnectionHolder.abortConnection(ConnectionHolder.java:127) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:352) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:71) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:469) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 20 common frames omitted\n10:20:31.147 helios[14151]: DEBUG [Reactor(agent)] Agent: tasks: {testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548=Task{job=Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, resources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}, goal=START, deployerUser=myuser, deployerMaster=null, deploymentGroupName=null}}\n10:20:31.147 helios[14151]: DEBUG [Reactor(agent)] Agent: executions: {}\n10:20:31.147 helios[14151]: DEBUG [Reactor(agent)] Agent: supervisors: {}\n10:20:31.148 helios[14151]: DEBUG [Reactor(agent)] Agent: Allocated ports for job testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548: {}\n10:20:31.149 helios[14151]: DEBUG [Reactor(agent)] PersistentAtomicReference: set: (/var/lib/helios-agent/executions.json) {testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548=Execution{job=Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, resources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}, ports={}, goal=START}}\n10:20:31.154 helios[14151]: DEBUG [Reactor(agent)] PersistentAtomicReference: write: (/var/lib/helios-agent/executions.json.tmp) {\n  \"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\" : {\n    \"goal\" : \"START\",\n    \"job\" : {\n      \"command\" : [ \"/bin/sh -c 'while true; do env; sleep 60; done'\" ],\n      \"created\" : 1446199870282,\n      \"creatingUser\" : \"myuser\",\n      \"env\" : { },\n      \"expires\" : null,\n      \"gracePeriod\" : null,\n      \"healthCheck\" : null,\n      \"hostname\" : null,\n      \"id\" : \"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\",\n      \"image\" : \"ubuntu:14.04\",\n      \"metadata\" : { },\n      \"networkMode\" : null,\n      \"ports\" : { },\n      \"registration\" : { },\n      \"registrationDomain\" : \"\",\n      \"resources\" : null,\n      \"securityOpt\" : [ ],\n      \"token\" : \"\",\n      \"volumes\" : { }\n    },\n    \"ports\" : { }\n  }\n}\n10:20:31.155 helios[14151]: DEBUG [Reactor(agent)] PersistentAtomicReference: move: /var/lib/helios-agent/executions.json.tmp -> /var/lib/helios-agent/executions.json\n10:20:31.155 helios[14151]: DEBUG [Reactor(agent)] Agent: creating job supervisor: Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, resources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}\n10:20:31.161 helios[14151]: DEBUG [Reactor(agent)] Supervisor: Supervisor testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548: setting goal: START\n10:20:31.161 helios[14151]: DEBUG [Reactor(supervisor-testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548)] Supervisor: Supervisor testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548: update: performedCommand=null, command=com.spotify.helios.agent.Supervisor$Start@7364fa21, done=false\n10:20:31.162 helios[14151]: DEBUG [Reactor(supervisor-testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548)] Supervisor: starting job (delay=0): Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, resources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}\n10:20:31.167 helios[14151]: DEBUG [Reactor(supervisor-testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548)] Supervisor: Supervisor testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548: state changed\n10:20:31.167 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] ZooKeeperAgentModel: setting task status: TaskStatus{job=Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, resources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}, goal=START, state=PULLING_IMAGE, containerId=null, throttled=NO, ports={}, env={}}\n10:20:31.168 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAddCookies: CookieSpec selected: default\n10:20:31.168 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAuthCache: Auth cache not set in the context\n10:20:31.168 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection request: [route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:20:31.168 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection leased: [id: 7][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 1 of 100; total allocated: 1 of 100]\n10:20:31.168 helios[14151]: DEBUG [jersey-client-async-executor-1] MainClientExec: Opening connection {}->unix://localhost:80\n10:20:31.168 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultHttpClientConnectionOperator: Connecting to localhost/127.0.0.1:80\n10:20:31.168 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-7: Shutdown connection\n10:20:31.168 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-7: Close connection\n10:20:31.169 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection released: [id: 7][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:20:31.169 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: set: (/var/lib/helios-agent/task-status.json) {testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548=[B@42d66ff5}\n10:20:31.169 helios[14151]: ERROR [Reactor(agent)] Reaper: reaping failed\ncom.spotify.docker.client.DockerException: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.DefaultDockerClient.propagate(DefaultDockerClient.java:1141) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1072) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.listContainers(DefaultDockerClient.java:314) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Reaper.reap0(Reaper.java:60) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Reaper.reap(Reaper.java:51) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.Agent$Update.run(Agent.java:190) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.servicescommon.DefaultReactor.run(DefaultReactor.java:100) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.AbstractExecutionThreadService$1$2.run(AbstractExecutionThreadService.java:60) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.Callables$3.run(Callables.java:95) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_85]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_85]\n    at java.lang.Thread.run(Thread.java:745) [na:1.7.0_85]\nCaused by: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1070) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 10 common frames omitted\nCaused by: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:517) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector$1.run(ApacheConnector.java:527) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    at jersey.repackaged.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:49) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:45) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:523) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$1.run(ClientRuntime.java:169) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:315) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:297) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:320) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$2.run(ClientRuntime.java:201) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    ... 3 common frames omitted\nCaused by: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.ApacheUnixSocket.setSoLinger(ApacheUnixSocket.java:165) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.BHttpConnectionBase.shutdown(BHttpConnectionBase.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.DefaultManagedHttpClientConnection.shutdown(DefaultManagedHttpClientConnection.java:97) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.LoggingManagedHttpClientConnection.shutdown(LoggingManagedHttpClientConnection.java:89) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolEntry.shutdownConnection(CPoolEntry.java:74) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolProxy.shutdown(CPoolProxy.java:96) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ConnectionHolder.abortConnection(ConnectionHolder.java:127) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:352) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:71) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:469) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 20 common frames omitted\n10:20:31.170 helios[14151]: DEBUG [Reactor(agent)] Agent: tasks: {testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548=Task{job=Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, resources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}, goal=START, deployerUser=myuser, deployerMaster=null, deploymentGroupName=null}}\n10:20:31.170 helios[14151]: DEBUG [Reactor(agent)] Agent: executions: {testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548=Execution{job=Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, resources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}, ports={}, goal=START}}\n10:20:31.170 helios[14151]: DEBUG [Reactor(agent)] Agent: supervisors: {testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548=Supervisor{job=Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, resources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}, currentCommand=com.spotify.helios.agent.Supervisor$Start@7364fa21, performedCommand=com.spotify.helios.agent.Supervisor$Start@7364fa21}}\n10:20:31.172 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: write: (/var/lib/helios-agent/task-status.json.tmp) {\n  \"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\" : \"eyJjb250YWluZXJJZCI6bnVsbCwiZW52Ijp7fSwiZ29hbCI6IlNUQVJUIiwiam9iIjp7ImNvbW1hbmQiOlsiL2Jpbi9zaCAtYyAnd2hpbGUgdHJ1ZTsgZG8gZW52OyBzbGVlcCA2MDsgZG9uZSciXSwiY3JlYXRlZCI6MTQ0NjE5OTg3MDI4MiwiY3JlYXRpbmdVc2VyIjoidWlkZzMyMDAiLCJlbnYiOnt9LCJleHBpcmVzIjpudWxsLCJncmFjZVBlcmlvZCI6bnVsbCwiaGVhbHRoQ2hlY2siOm51bGwsImhvc3RuYW1lIjpudWxsLCJpZCI6InRlc3Rqb2I6MTo5ZDY2NTg0YTNhZGM3ZjNjOWFkMGM2OTQ5OWVlMmRmNGNhYTQ0NTQ4IiwiaW1hZ2UiOiJ1YnVudHU6MTQuMDQiLCJtZXRhZGF0YSI6e30sIm5ldHdvcmtNb2RlIjpudWxsLCJwb3J0cyI6e30sInJlZ2lzdHJhdGlvbiI6e30sInJlZ2lzdHJhdGlvbkRvbWFpbiI6IiIsInJlc291cmNlcyI6bnVsbCwic2VjdXJpdHlPcHQiOltdLCJ0b2tlbiI6IiIsInZvbHVtZXMiOnt9fSwicG9ydHMiOnt9LCJzdGF0ZSI6IlBVTExJTkdfSU1BR0UiLCJ0aHJvdHRsZWQiOiJOTyJ9\"\n}\n10:20:31.172 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: move: /var/lib/helios-agent/task-status.json.tmp -> /var/lib/helios-agent/task-status.json\n10:20:31.173 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: set: (/var/lib/helios-agent/task-history.json) {testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548=[TaskStatusEvent{timestamp=1446715231172, host=helios-agent-host, status=TaskStatus{job=Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, resources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}, goal=START, state=PULLING_IMAGE, containerId=null, throttled=NO, ports={}, env={}}}]}\n10:20:31.174 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 55,3  replyHeader:: 55,400158,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,32,0,0,0,0,358974} \n10:20:31.174 helios[14151]: DEBUG [Reactor(agent-model-task-statuses)] ZooKeeperUpdatingPersistentDirectory: create: [testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548]\n10:20:31.174 helios[14151]: DEBUG [Reactor(agent-model-task-statuses)] ZooKeeperUpdatingPersistentDirectory: update: []\n10:20:31.174 helios[14151]: DEBUG [Reactor(agent-model-task-statuses)] ZooKeeperUpdatingPersistentDirectory: delete: []\n10:20:31.175 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 56,3  replyHeader:: 56,400158,-101  request:: '/status/hosts/helios-agent-host/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548,F  response::\n10:20:31.175 helios[14151]: DEBUG [Reactor(agent-model-task-statuses)] ZooKeeperUpdatingPersistentDirectory: creating node: /status/hosts/helios-agent-host/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\n10:20:31.178 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: write: (/var/lib/helios-agent/task-history.json.tmp) {\n  \"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\" : [ {\n    \"host\" : \"helios-agent-host\",\n    \"status\" : {\n      \"containerId\" : null,\n      \"env\" : { },\n      \"goal\" : \"START\",\n      \"job\" : {\n        \"command\" : [ \"/bin/sh -c 'while true; do env; sleep 60; done'\" ],\n        \"created\" : 1446199870282,\n        \"creatingUser\" : \"myuser\",\n        \"env\" : { },\n        \"expires\" : null,\n        \"gracePeriod\" : null,\n        \"healthCheck\" : null,\n        \"hostname\" : null,\n        \"id\" : \"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\",\n        \"image\" : \"ubuntu:14.04\",\n        \"metadata\" : { },\n        \"networkMode\" : null,\n        \"ports\" : { },\n        \"registration\" : { },\n        \"registrationDomain\" : \"\",\n        \"resources\" : null,\n        \"securityOpt\" : [ ],\n        \"token\" : \"\",\n        \"volumes\" : { }\n      },\n      \"ports\" : { },\n      \"state\" : \"PULLING_IMAGE\",\n      \"throttled\" : \"NO\"\n    },\n    \"timestamp\" : 1446715231172\n  } ]\n}\n10:20:31.178 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: move: /var/lib/helios-agent/task-history.json.tmp -> /var/lib/helios-agent/task-history.json\n10:20:31.179 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] KafkaSender: KafkaProducer isn't set. Not sending anything.\n10:20:31.183 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAddCookies: CookieSpec selected: default\n10:20:31.183 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAuthCache: Auth cache not set in the context\n10:20:31.183 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection request: [route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:20:31.184 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection leased: [id: 8][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 1 of 100; total allocated: 1 of 100]\n10:20:31.184 helios[14151]: DEBUG [jersey-client-async-executor-1] MainClientExec: Opening connection {}->unix://localhost:80\n10:20:31.184 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultHttpClientConnectionOperator: Connecting to localhost/127.0.0.1:80\n10:20:31.184 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-8: Shutdown connection\n10:20:31.184 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-8: Close connection\n10:20:31.184 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection released: [id: 8][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:20:31.185 helios[14151]: WARN  [TaskRunner(testjob:1:9d66584)] TaskRunner: Pulling image ubuntu:14.04 failed after 0s\ncom.spotify.docker.client.DockerException: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.DefaultDockerClient.propagate(DefaultDockerClient.java:1141) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1082) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.pull(DefaultDockerClient.java:665) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.pull(DefaultDockerClient.java:650) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.7.0_85]\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[na:1.7.0_85]\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_85]\n    at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_85]\n    at com.spotify.helios.agent.MonitoredDockerClient$MonitoringInvocationHandler.invoke(MonitoredDockerClient.java:62) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.sun.proxy.$Proxy19.pull(Unknown Source) ~[na:na]\n    at com.spotify.helios.agent.TaskRunner.pullImage(TaskRunner.java:257) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.TaskRunner.createAndStartContainer(TaskRunner.java:202) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.TaskRunner.run0(TaskRunner.java:152) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.TaskRunner.run(TaskRunner.java:131) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.AbstractExecutionThreadService$1$2.run(AbstractExecutionThreadService.java:60) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.Callables$3.run(Callables.java:95) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_85]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_85]\n    at java.lang.Thread.run(Thread.java:745) [na:1.7.0_85]\nCaused by: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1080) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 17 common frames omitted\nCaused by: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:517) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector$1.run(ApacheConnector.java:527) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    at jersey.repackaged.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:49) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:45) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:523) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$1.run(ClientRuntime.java:169) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:315) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:297) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:320) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$2.run(ClientRuntime.java:201) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    ... 3 common frames omitted\nCaused by: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.ApacheUnixSocket.setSoLinger(ApacheUnixSocket.java:165) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.BHttpConnectionBase.shutdown(BHttpConnectionBase.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.DefaultManagedHttpClientConnection.shutdown(DefaultManagedHttpClientConnection.java:97) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.LoggingManagedHttpClientConnection.shutdown(LoggingManagedHttpClientConnection.java:89) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolEntry.shutdownConnection(CPoolEntry.java:74) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolProxy.shutdown(CPoolProxy.java:96) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ConnectionHolder.abortConnection(ConnectionHolder.java:127) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:352) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:71) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:469) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 20 common frames omitted\n10:20:31.187 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAddCookies: CookieSpec selected: default\n10:20:31.187 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAuthCache: Auth cache not set in the context\n10:20:31.187 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection request: [route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:20:31.187 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection leased: [id: 9][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 1 of 100; total allocated: 1 of 100]\n10:20:31.187 helios[14151]: DEBUG [jersey-client-async-executor-1] MainClientExec: Opening connection {}->unix://localhost:80\n10:20:31.187 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultHttpClientConnectionOperator: Connecting to localhost/127.0.0.1:80\n10:20:31.187 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-9: Shutdown connection\n10:20:31.187 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-9: Close connection\n10:20:31.187 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection released: [id: 9][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:20:31.188 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] ZooKeeperAgentModel: setting task status: TaskStatus{job=Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, resources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}, goal=START, state=FAILED, containerId=null, throttled=NO, ports={}, env={}}\n10:20:31.188 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: set: (/var/lib/helios-agent/task-status.json) {testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548=[B@350164a0}\n10:20:31.188 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: write: (/var/lib/helios-agent/task-status.json.tmp) {\n  \"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\" : \"eyJjb250YWluZXJJZCI6bnVsbCwiZW52Ijp7fSwiZ29hbCI6IlNUQVJUIiwiam9iIjp7ImNvbW1hbmQiOlsiL2Jpbi9zaCAtYyAnd2hpbGUgdHJ1ZTsgZG8gZW52OyBzbGVlcCA2MDsgZG9uZSciXSwiY3JlYXRlZCI6MTQ0NjE5OTg3MDI4MiwiY3JlYXRpbmdVc2VyIjoidWlkZzMyMDAiLCJlbnYiOnt9LCJleHBpcmVzIjpudWxsLCJncmFjZVBlcmlvZCI6bnVsbCwiaGVhbHRoQ2hlY2siOm51bGwsImhvc3RuYW1lIjpudWxsLCJpZCI6InRlc3Rqb2I6MTo5ZDY2NTg0YTNhZGM3ZjNjOWFkMGM2OTQ5OWVlMmRmNGNhYTQ0NTQ4IiwiaW1hZ2UiOiJ1YnVudHU6MTQuMDQiLCJtZXRhZGF0YSI6e30sIm5ldHdvcmtNb2RlIjpudWxsLCJwb3J0cyI6e30sInJlZ2lzdHJhdGlvbiI6e30sInJlZ2lzdHJhdGlvbkRvbWFpbiI6IiIsInJlc291cmNlcyI6bnVsbCwic2VjdXJpdHlPcHQiOltdLCJ0b2tlbiI6IiIsInZvbHVtZXMiOnt9fSwicG9ydHMiOnt9LCJzdGF0ZSI6IkZBSUxFRCIsInRocm90dGxlZCI6Ik5PIn0=\"\n}\n10:20:31.188 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: move: /var/lib/helios-agent/task-status.json.tmp -> /var/lib/helios-agent/task-status.json\n10:20:31.188 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: set: (/var/lib/helios-agent/task-history.json) {testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548=[TaskStatusEvent{timestamp=1446715231172, host=helios-agent-host, status=TaskStatus{job=Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, resources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}, goal=START, state=PULLING_IMAGE, containerId=null, throttled=NO, ports={}, env={}}}, TaskStatusEvent{timestamp=1446715231188, host=helios-agent-host, status=TaskStatus{job=Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, \nresources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}, goal=START, state=FAILED, containerId=null, throttled=NO, ports={}, env={}}}]}\n10:20:31.189 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: write: (/var/lib/helios-agent/task-history.json.tmp) {\n  \"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\" : [ {\n    \"host\" : \"helios-agent-host\",\n    \"status\" : {\n      \"containerId\" : null,\n      \"env\" : { },\n      \"goal\" : \"START\",\n      \"job\" : {\n        \"command\" : [ \"/bin/sh -c 'while true; do env; sleep 60; done'\" ],\n        \"created\" : 1446199870282,\n        \"creatingUser\" : \"myuser\",\n        \"env\" : { },\n        \"expires\" : null,\n        \"gracePeriod\" : null,\n        \"healthCheck\" : null,\n        \"hostname\" : null,\n        \"id\" : \"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\",\n        \"image\" : \"ubuntu:14.04\",\n        \"metadata\" : { },\n        \"networkMode\" : null,\n        \"ports\" : { },\n        \"registration\" : { },\n        \"registrationDomain\" : \"\",\n        \"resources\" : null,\n        \"securityOpt\" : [ ],\n        \"token\" : \"\",\n        \"volumes\" : { }\n      },\n      \"ports\" : { },\n      \"state\" : \"PULLING_IMAGE\",\n      \"throttled\" : \"NO\"\n    },\n    \"timestamp\" : 1446715231172\n  }, {\n    \"host\" : \"helios-agent-host\",\n    \"status\" : {\n      \"containerId\" : null,\n      \"env\" : { },\n      \"goal\" : \"START\",\n      \"job\" : {\n        \"command\" : [ \"/bin/sh -c 'while true; do env; sleep 60; done'\" ],\n        \"created\" : 1446199870282,\n        \"creatingUser\" : \"myuser\",\n        \"env\" : { },\n        \"expires\" : null,\n        \"gracePeriod\" : null,\n        \"healthCheck\" : null,\n        \"hostname\" : null,\n        \"id\" : \"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\",\n        \"image\" : \"ubuntu:14.04\",\n        \"metadata\" : { },\n        \"networkMode\" : null,\n        \"ports\" : { },\n        \"registration\" : { },\n        \"registrationDomain\" : \"\",\n        \"resources\" : null,\n        \"securityOpt\" : [ ],\n        \"token\" : \"\",\n        \"volumes\" : { }\n      },\n      \"ports\" : { },\n      \"state\" : \"FAILED\",\n      \"throttled\" : \"NO\"\n    },\n    \"timestamp\" : 1446715231188\n  } ]\n}\n10:20:31.189 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: move: /var/lib/helios-agent/task-history.json.tmp -> /var/lib/helios-agent/task-history.json\n10:20:31.190 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] KafkaSender: KafkaProducer isn't set. Not sending anything.\n10:20:31.190 helios[14151]: DEBUG [Reactor(supervisor-testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548)] Supervisor: Supervisor testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548: update: performedCommand=com.spotify.helios.agent.Supervisor$Start@7364fa21, command=com.spotify.helios.agent.Supervisor$Start@7364fa21, done=true\n10:20:31.193 helios[14151]: ERROR [Reactor(supervisor-testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548)] Supervisor: docker error\ncom.spotify.docker.client.DockerException: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.DefaultDockerClient.propagate(DefaultDockerClient.java:1141) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1082) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.inspectImage(DefaultDockerClient.java:850) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.7.0_85]\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[na:1.7.0_85]\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_85]\n    at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_85]\n    at com.spotify.helios.agent.MonitoredDockerClient$MonitoringInvocationHandler.invoke(MonitoredDockerClient.java:62) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.sun.proxy.$Proxy19.inspectImage(Unknown Source) ~[na:na]\n    at com.spotify.helios.agent.TaskRunner.pullImage(TaskRunner.java:272) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.TaskRunner.createAndStartContainer(TaskRunner.java:202) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.TaskRunner.run0(TaskRunner.java:152) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.TaskRunner.run(TaskRunner.java:131) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.AbstractExecutionThreadService$1$2.run(AbstractExecutionThreadService.java:60) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.Callables$3.run(Callables.java:95) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_85]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_85]\n    at java.lang.Thread.run(Thread.java:745) [na:1.7.0_85]\nCaused by: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1080) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 16 common frames omitted\nCaused by: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:517) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector$1.run(ApacheConnector.java:527) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    at jersey.repackaged.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:49) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:45) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:523) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$1.run(ClientRuntime.java:169) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:315) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:297) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:320) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$2.run(ClientRuntime.java:201) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    ... 3 common frames omitted\nCaused by: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.ApacheUnixSocket.setSoLinger(ApacheUnixSocket.java:165) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.BHttpConnectionBase.shutdown(BHttpConnectionBase.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.DefaultManagedHttpClientConnection.shutdown(DefaultManagedHttpClientConnection.java:97) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.LoggingManagedHttpClientConnection.shutdown(LoggingManagedHttpClientConnection.java:89) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolEntry.shutdownConnection(CPoolEntry.java:74) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolProxy.shutdown(CPoolProxy.java:96) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ConnectionHolder.abortConnection(ConnectionHolder.java:127) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:352) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:71) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:469) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 20 common frames omitted\n10:20:31.194 helios[14151]: DEBUG [Reactor(supervisor-testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548)] Supervisor: starting job (delay=1000): Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, resources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}\n10:20:31.221 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 57,1  replyHeader:: 57,400159,0  request:: '/status/hosts/helios-agent-host/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548,#7b22636f6e7461696e65724964223a6e756c6c2c22656e76223a7b7d2c22676f616c223a225354415254222c226a6f62223a7b22636f6d6d616e64223a5b222f62696e2f7368202d6320277768696c6520747275653b20646f20656e763b20736c6565702036303b20646f6e6527225d2c2263726561746564223a313434363139393837303238322c226372656174696e6755736572223a227569646733323030222c22656e76223a7b7d2c2265787069726573223a6e756c6c2c226772616365506572696f64223a6e756c6c2c226865616c7468436865636b223a6e756c6c2c22686f73746e616d65223a6e756c6c2c226964223a22746573746a6f623a313a39643636353834613361646337663363396164306336393439396565326466346361613434353438222c22696d616765223a227562756e74753a31342e3034222c226d65746164617461223a7b\n7d2c226e6574776f726b4d6f6465223a6e756c6c2c22706f727473223a7b7d2c22726567697374726174696f6e223a7b7d2c22726567697374726174696f6e446f6d61696e223a22222c227265736f7572636573223a6e756c6c2c2273656375726974794f7074223a5b5d2c22746f6b656e223a22222c22766f6c756d6573223a7b7d7d2c22706f727473223a7b7d2c227374617465223a2250554c4c494e475f494d414745222c227468726f74746c6564223a224e4f227d,v{s{31,s{'world,'anyone}}},0  response:: '/status/hosts/helios-agent-host/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548 \n10:20:31.222 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 58,3  replyHeader:: 58,400159,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,33,0,0,0,1,400159} \n10:20:31.223 helios[14151]: DEBUG [Reactor(agent-model-task-statuses)] ZooKeeperUpdatingPersistentDirectory: create: []\n10:20:31.223 helios[14151]: DEBUG [Reactor(agent-model-task-statuses)] ZooKeeperUpdatingPersistentDirectory: update: [testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548]\n10:20:31.223 helios[14151]: DEBUG [Reactor(agent-model-task-statuses)] ZooKeeperUpdatingPersistentDirectory: delete: []\n10:20:31.224 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 59,3  replyHeader:: 59,400159,0  request:: '/status/hosts/helios-agent-host/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548,F  response:: s{400159,400159,1446715231175,1446715231175,0,0,0,0,519,0,400159} \n10:20:31.224 helios[14151]: DEBUG [Reactor(agent-model-task-statuses)] ZooKeeperUpdatingPersistentDirectory: setting node: /status/hosts/helios-agent-host/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\n10:20:31.234 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 60,5  replyHeader:: 60,400160,0  request:: '/status/hosts/helios-agent-host/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548,#7b22636f6e7461696e65724964223a6e756c6c2c22656e76223a7b7d2c22676f616c223a225354415254222c226a6f62223a7b22636f6d6d616e64223a5b222f62696e2f7368202d6320277768696c6520747275653b20646f20656e763b20736c6565702036303b20646f6e6527225d2c2263726561746564223a313434363139393837303238322c226372656174696e6755736572223a227569646733323030222c22656e76223a7b7d2c2265787069726573223a6e756c6c2c226772616365506572696f64223a6e756c6c2c226865616c7468436865636b223a6e756c6c2c22686f73746e616d65223a6e756c6c2c226964223a22746573746a6f623a313a39643636353834613361646337663363396164306336393439396565326466346361613434353438222c22696d616765223a227562756e74753a31342e3034222c226d65746164617461223a7b\n7d2c226e6574776f726b4d6f6465223a6e756c6c2c22706f727473223a7b7d2c22726567697374726174696f6e223a7b7d2c22726567697374726174696f6e446f6d61696e223a22222c227265736f7572636573223a6e756c6c2c2273656375726974794f7074223a5b5d2c22746f6b656e223a22222c22766f6c756d6573223a7b7d7d2c22706f727473223a7b7d2c227374617465223a224641494c4544222c227468726f74746c6564223a224e4f227d,-1  response:: s{400159,400160,1446715231175,1446715231223,1,0,0,0,512,0,400159} \n10:20:32.063 helios[14151]: DEBUG [pool-10-thread-1] TaskHistoryWriter: writing queued item to zookeeper testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548 1446715231172\n10:20:32.065 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 61,3  replyHeader:: 61,400160,0  request:: '/history,F  response:: s{12,12,1446052557798,1446052557798,0,1,0,0,0,1,13} \n10:20:32.066 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 62,3  replyHeader:: 62,400160,0  request:: '/history/jobs,F  response:: s{13,13,1446052557805,1446052557805,0,24,0,0,0,2,351524} \n10:20:32.067 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 63,3  replyHeader:: 63,400160,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548,F  response:: s{351524,351524,1446136946738,1446136946738,0,1,0,0,0,1,351525} \n10:20:32.068 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 64,3  replyHeader:: 64,400160,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts,F  response:: s{351525,351525,1446136946750,1446136946750,0,1,0,0,0,1,351526} \n10:20:32.070 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 65,3  replyHeader:: 65,400160,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts/helios-agent-host,F  response:: s{351526,351526,1446136946759,1446136946759,0,1,0,0,0,1,351527} \n10:20:32.071 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 66,3  replyHeader:: 66,400160,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts/helios-agent-host/events,F  response:: s{351527,351527,1446136946767,1446136946767,0,1216,0,0,0,30,358976} \n10:20:32.083 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 67,1  replyHeader:: 67,400161,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts/helios-agent-host/events/1446715231172,#7b22636f6e7461696e65724964223a6e756c6c2c22656e76223a7b7d2c22676f616c223a225354415254222c226a6f62223a7b22636f6d6d616e64223a5b222f62696e2f7368202d6320277768696c6520747275653b20646f20656e763b20736c6565702036303b20646f6e6527225d2c2263726561746564223a313434363139393837303238322c226372656174696e6755736572223a227569646733323030222c22656e76223a7b7d2c2265787069726573223a6e756c6c2c226772616365506572696f64223a6e756c6c2c226865616c7468436865636b223a6e756c6c2c22686f73746e616d65223a6e756c6c2c226964223a22746573746a6f623a313a39643636353834613361646337663363396164306336393439396565326466346361613434353438222c22696d616765223a227562756e74753a31342e3034222c22\n6d65746164617461223a7b7d2c226e6574776f726b4d6f6465223a6e756c6c2c22706f727473223a7b7d2c22726567697374726174696f6e223a7b7d2c22726567697374726174696f6e446f6d61696e223a22222c227265736f7572636573223a6e756c6c2c2273656375726974794f7074223a5b5d2c22746f6b656e223a22222c22766f6c756d6573223a7b7d7d2c22706f727473223a7b7d2c227374617465223a2250554c4c494e475f494d414745222c227468726f74746c6564223a224e4f227d,v{s{31,s{'world,'anyone}}},0  response:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts/helios-agent-host/events/1446715231172 \n10:20:32.085 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 68,12  replyHeader:: 68,400161,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts/helios-agent-host/events,F  response:: v{'1446215837354,'1446215830161,'1446215840410,'1446215832212,'1446215821176,'1446215836309,'1446215833217,'1446215829141,'1446215840418,'1446215838374,'1446215833232,'1446215831170,'1446215841425,'1446215841447,'1446215842271,'1446215839395,'1446215837362,'1446215835302,'1446215836319,'1446215838381,'1446215834257,'1446215821183,'1446215834278,'1446215829099,'1446215831185,'1446215835294,'1446215832204,'1446215830150,'1446715231172,'1446215839387,'1446215821963},s{351527,351527,1446136946767,1446136946767,0,1217,0,0,0,31,400161} \n10:20:32.092 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 69,2  replyHeader:: 69,400162,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts/helios-agent-host/events/1446215821176,-1  response:: null\n10:20:32.092 helios[14151]: DEBUG [pool-10-thread-1] TaskHistoryWriter: writing queued item to zookeeper testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548 1446715231188\n10:20:32.093 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 70,3  replyHeader:: 70,400162,0  request:: '/history,F  response:: s{12,12,1446052557798,1446052557798,0,1,0,0,0,1,13} \n10:20:32.094 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 71,3  replyHeader:: 71,400162,0  request:: '/history/jobs,F  response:: s{13,13,1446052557805,1446052557805,0,24,0,0,0,2,351524} \n10:20:32.095 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 72,3  replyHeader:: 72,400162,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548,F  response:: s{351524,351524,1446136946738,1446136946738,0,1,0,0,0,1,351525} \n10:20:32.096 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 73,3  replyHeader:: 73,400162,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts,F  response:: s{351525,351525,1446136946750,1446136946750,0,1,0,0,0,1,351526} \n10:20:32.097 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 74,3  replyHeader:: 74,400162,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts/helios-agent-host,F  response:: s{351526,351526,1446136946759,1446136946759,0,1,0,0,0,1,351527} \n10:20:32.098 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 75,3  replyHeader:: 75,400162,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts/helios-agent-host/events,F  response:: s{351527,351527,1446136946767,1446136946767,0,1218,0,0,0,30,400162} \n10:20:32.108 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 76,1  replyHeader:: 76,400163,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts/helios-agent-host/events/1446715231188,#7b22636f6e7461696e65724964223a6e756c6c2c22656e76223a7b7d2c22676f616c223a225354415254222c226a6f62223a7b22636f6d6d616e64223a5b222f62696e2f7368202d6320277768696c6520747275653b20646f20656e763b20736c6565702036303b20646f6e6527225d2c2263726561746564223a313434363139393837303238322c226372656174696e6755736572223a227569646733323030222c22656e76223a7b7d2c2265787069726573223a6e756c6c2c226772616365506572696f64223a6e756c6c2c226865616c7468436865636b223a6e756c6c2c22686f73746e616d65223a6e756c6c2c226964223a22746573746a6f623a313a39643636353834613361646337663363396164306336393439396565326466346361613434353438222c22696d616765223a227562756e74753a31342e3034222c22\n6d65746164617461223a7b7d2c226e6574776f726b4d6f6465223a6e756c6c2c22706f727473223a7b7d2c22726567697374726174696f6e223a7b7d2c22726567697374726174696f6e446f6d61696e223a22222c227265736f7572636573223a6e756c6c2c2273656375726974794f7074223a5b5d2c22746f6b656e223a22222c22766f6c756d6573223a7b7d7d2c22706f727473223a7b7d2c227374617465223a224641494c4544222c227468726f74746c6564223a224e4f227d,v{s{31,s{'world,'anyone}}},0  response:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts/helios-agent-host/events/1446715231188 \n10:20:32.110 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 77,12  replyHeader:: 77,400163,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts/helios-agent-host/events,F  response:: v{'1446215837354,'1446215830161,'1446215840410,'1446215832212,'1446215836309,'1446215833217,'1446215829141,'1446215840418,'1446215838374,'1446215833232,'1446215831170,'1446215841425,'1446715231188,'1446215841447,'1446215842271,'1446215839395,'1446215837362,'1446215835302,'1446215836319,'1446215838381,'1446215834257,'1446215821183,'1446215834278,'1446215829099,'1446215831185,'1446215835294,'1446215832204,'1446215830150,'1446715231172,'1446215839387,'1446215821963},s{351527,351527,1446136946767,1446136946767,0,1219,0,0,0,31,400163} \n10:20:32.116 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 78,2  replyHeader:: 78,400164,0  request:: '/history/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548/hosts/helios-agent-host/events/1446215821183,-1  response:: null\n10:20:32.195 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] ZooKeeperAgentModel: setting task status: TaskStatus{job=Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, resources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}, goal=START, state=PULLING_IMAGE, containerId=null, throttled=NO, ports={}, env={}}\n10:20:32.195 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: set: (/var/lib/helios-agent/task-status.json) {testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548=[B@43787fb0}\n10:20:32.195 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: write: (/var/lib/helios-agent/task-status.json.tmp) {\n  \"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\" : \"eyJjb250YWluZXJJZCI6bnVsbCwiZW52Ijp7fSwiZ29hbCI6IlNUQVJUIiwiam9iIjp7ImNvbW1hbmQiOlsiL2Jpbi9zaCAtYyAnd2hpbGUgdHJ1ZTsgZG8gZW52OyBzbGVlcCA2MDsgZG9uZSciXSwiY3JlYXRlZCI6MTQ0NjE5OTg3MDI4MiwiY3JlYXRpbmdVc2VyIjoidWlkZzMyMDAiLCJlbnYiOnt9LCJleHBpcmVzIjpudWxsLCJncmFjZVBlcmlvZCI6bnVsbCwiaGVhbHRoQ2hlY2siOm51bGwsImhvc3RuYW1lIjpudWxsLCJpZCI6InRlc3Rqb2I6MTo5ZDY2NTg0YTNhZGM3ZjNjOWFkMGM2OTQ5OWVlMmRmNGNhYTQ0NTQ4IiwiaW1hZ2UiOiJ1YnVudHU6MTQuMDQiLCJtZXRhZGF0YSI6e30sIm5ldHdvcmtNb2RlIjpudWxsLCJwb3J0cyI6e30sInJlZ2lzdHJhdGlvbiI6e30sInJlZ2lzdHJhdGlvbkRvbWFpbiI6IiIsInJlc291cmNlcyI6bnVsbCwic2VjdXJpdHlPcHQiOltdLCJ0b2tlbiI6IiIsInZvbHVtZXMiOnt9fSwicG9ydHMiOnt9LCJzdGF0ZSI6IlBVTExJTkdfSU1BR0UiLCJ0aHJvdHRsZWQiOiJOTyJ9\"\n}\n10:20:32.196 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: move: /var/lib/helios-agent/task-status.json.tmp -> /var/lib/helios-agent/task-status.json\n10:20:32.196 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: set: (/var/lib/helios-agent/task-history.json) {testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548=[TaskStatusEvent{timestamp=1446715232196, host=helios-agent-host, status=TaskStatus{job=Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, resources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}, goal=START, state=PULLING_IMAGE, containerId=null, throttled=NO, ports={}, env={}}}]}\n10:20:32.196 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: write: (/var/lib/helios-agent/task-history.json.tmp) {\n  \"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\" : [ {\n    \"host\" : \"helios-agent-host\",\n    \"status\" : {\n      \"containerId\" : null,\n      \"env\" : { },\n      \"goal\" : \"START\",\n      \"job\" : {\n        \"command\" : [ \"/bin/sh -c 'while true; do env; sleep 60; done'\" ],\n        \"created\" : 1446199870282,\n        \"creatingUser\" : \"myuser\",\n        \"env\" : { },\n        \"expires\" : null,\n        \"gracePeriod\" : null,\n        \"healthCheck\" : null,\n        \"hostname\" : null,\n        \"id\" : \"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\",\n        \"image\" : \"ubuntu:14.04\",\n        \"metadata\" : { },\n        \"networkMode\" : null,\n        \"ports\" : { },\n        \"registration\" : { },\n        \"registrationDomain\" : \"\",\n        \"resources\" : null,\n        \"securityOpt\" : [ ],\n        \"token\" : \"\",\n        \"volumes\" : { }\n      },\n      \"ports\" : { },\n      \"state\" : \"PULLING_IMAGE\",\n      \"throttled\" : \"NO\"\n    },\n    \"timestamp\" : 1446715232196\n  } ]\n}\n10:20:32.196 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: move: /var/lib/helios-agent/task-history.json.tmp -> /var/lib/helios-agent/task-history.json\n10:20:32.196 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] KafkaSender: KafkaProducer isn't set. Not sending anything.\n10:20:32.197 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 79,3  replyHeader:: 79,400164,0  request:: '/status/hosts/helios-agent-host/jobs,F  response:: s{23,23,1446052557950,1446052557950,0,33,0,0,0,1,400159} \n10:20:32.197 helios[14151]: DEBUG [Reactor(agent-model-task-statuses)] ZooKeeperUpdatingPersistentDirectory: create: []\n10:20:32.197 helios[14151]: DEBUG [Reactor(agent-model-task-statuses)] ZooKeeperUpdatingPersistentDirectory: update: [testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548]\n10:20:32.197 helios[14151]: DEBUG [Reactor(agent-model-task-statuses)] ZooKeeperUpdatingPersistentDirectory: delete: []\n10:20:32.197 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAddCookies: CookieSpec selected: default\n10:20:32.198 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAuthCache: Auth cache not set in the context\n10:20:32.198 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection request: [route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:20:32.198 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection leased: [id: 10][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 1 of 100; total allocated: 1 of 100]\n10:20:32.198 helios[14151]: DEBUG [jersey-client-async-executor-1] MainClientExec: Opening connection {}->unix://localhost:80\n10:20:32.198 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultHttpClientConnectionOperator: Connecting to localhost/127.0.0.1:80\n10:20:32.198 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-10: Shutdown connection\n10:20:32.198 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-10: Close connection\n10:20:32.198 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection released: [id: 10][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:20:32.198 helios[14151]: DEBUG [AgentMain STARTING-SendThread(helios-master-host.example.com:2181)] ClientCnxn: Reading reply sessionid:0x150af72f074002a, packet:: clientPath:null serverPath:null finished:false header:: 80,3  replyHeader:: 80,400164,0  request:: '/status/hosts/helios-agent-host/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548,F  response:: s{400159,400160,1446715231175,1446715231223,1,0,0,0,512,0,400159} \n10:20:32.198 helios[14151]: DEBUG [Reactor(agent-model-task-statuses)] ZooKeeperUpdatingPersistentDirectory: setting node: /status/hosts/helios-agent-host/jobs/testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\n10:20:32.199 helios[14151]: WARN  [TaskRunner(testjob:1:9d66584)] TaskRunner: Pulling image ubuntu:14.04 failed after 0s\ncom.spotify.docker.client.DockerException: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.DefaultDockerClient.propagate(DefaultDockerClient.java:1141) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1082) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.pull(DefaultDockerClient.java:665) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.pull(DefaultDockerClient.java:650) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.7.0_85]\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[na:1.7.0_85]\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_85]\n    at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_85]\n    at com.spotify.helios.agent.MonitoredDockerClient$MonitoringInvocationHandler.invoke(MonitoredDockerClient.java:62) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.sun.proxy.$Proxy19.pull(Unknown Source) ~[na:na]\n    at com.spotify.helios.agent.TaskRunner.pullImage(TaskRunner.java:257) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.TaskRunner.createAndStartContainer(TaskRunner.java:202) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.TaskRunner.run0(TaskRunner.java:152) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.helios.agent.TaskRunner.run(TaskRunner.java:131) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.AbstractExecutionThreadService$1$2.run(AbstractExecutionThreadService.java:60) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.google.common.util.concurrent.Callables$3.run(Callables.java:95) [helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_85]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_85]\n    at java.lang.Thread.run(Thread.java:745) [na:1.7.0_85]\nCaused by: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1080) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 17 common frames omitted\nCaused by: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.lang.UnsupportedOperationException: Unimplemented\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:517) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector$1.run(ApacheConnector.java:527) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    at jersey.repackaged.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:49) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:45) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:523) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$1.run(ClientRuntime.java:169) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:315) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:297) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:267) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:320) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.client.ClientRuntime$2.run(ClientRuntime.java:201) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_85]\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_85]\n    ... 3 common frames omitted\nCaused by: java.lang.UnsupportedOperationException: Unimplemented\n    at com.spotify.docker.client.ApacheUnixSocket.setSoLinger(ApacheUnixSocket.java:165) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.BHttpConnectionBase.shutdown(BHttpConnectionBase.java:306) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.DefaultManagedHttpClientConnection.shutdown(DefaultManagedHttpClientConnection.java:97) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.LoggingManagedHttpClientConnection.shutdown(LoggingManagedHttpClientConnection.java:89) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolEntry.shutdownConnection(CPoolEntry.java:74) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.conn.CPoolProxy.shutdown(CPoolProxy.java:96) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ConnectionHolder.abortConnection(ConnectionHolder.java:127) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:352) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:71) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    at org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:469) ~[helios-services-0.8.562-shaded.jar:0.8.562]\n    ... 20 common frames omitted\n10:20:32.200 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAddCookies: CookieSpec selected: default\n10:20:32.200 helios[14151]: DEBUG [jersey-client-async-executor-1] RequestAuthCache: Auth cache not set in the context\n10:20:32.200 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection request: [route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:20:32.201 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection leased: [id: 11][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 1 of 100; total allocated: 1 of 100]\n10:20:32.201 helios[14151]: DEBUG [jersey-client-async-executor-1] MainClientExec: Opening connection {}->unix://localhost:80\n10:20:32.201 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultHttpClientConnectionOperator: Connecting to localhost/127.0.0.1:80\n10:20:32.201 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-11: Shutdown connection\n10:20:32.201 helios[14151]: DEBUG [jersey-client-async-executor-1] DefaultManagedHttpClientConnection: http-outgoing-11: Close connection\n10:20:32.201 helios[14151]: DEBUG [jersey-client-async-executor-1] PoolingHttpClientConnectionManager: Connection released: [id: 11][route: {}->unix://localhost:80][total kept alive: 0; route allocated: 0 of 100; total allocated: 0 of 100]\n10:20:32.201 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] ZooKeeperAgentModel: setting task status: TaskStatus{job=Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, resources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}, goal=START, state=FAILED, containerId=null, throttled=NO, ports={}, env={}}\n10:20:32.204 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: set: (/var/lib/helios-agent/task-status.json) {testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548=[B@1abc78a1}\n10:20:32.204 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: write: (/var/lib/helios-agent/task-status.json.tmp) {\n  \"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\" : \"eyJjb250YWluZXJJZCI6bnVsbCwiZW52Ijp7fSwiZ29hbCI6IlNUQVJUIiwiam9iIjp7ImNvbW1hbmQiOlsiL2Jpbi9zaCAtYyAnd2hpbGUgdHJ1ZTsgZG8gZW52OyBzbGVlcCA2MDsgZG9uZSciXSwiY3JlYXRlZCI6MTQ0NjE5OTg3MDI4MiwiY3JlYXRpbmdVc2VyIjoidWlkZzMyMDAiLCJlbnYiOnt9LCJleHBpcmVzIjpudWxsLCJncmFjZVBlcmlvZCI6bnVsbCwiaGVhbHRoQ2hlY2siOm51bGwsImhvc3RuYW1lIjpudWxsLCJpZCI6InRlc3Rqb2I6MTo5ZDY2NTg0YTNhZGM3ZjNjOWFkMGM2OTQ5OWVlMmRmNGNhYTQ0NTQ4IiwiaW1hZ2UiOiJ1YnVudHU6MTQuMDQiLCJtZXRhZGF0YSI6e30sIm5ldHdvcmtNb2RlIjpudWxsLCJwb3J0cyI6e30sInJlZ2lzdHJhdGlvbiI6e30sInJlZ2lzdHJhdGlvbkRvbWFpbiI6IiIsInJlc291cmNlcyI6bnVsbCwic2VjdXJpdHlPcHQiOltdLCJ0b2tlbiI6IiIsInZvbHVtZXMiOnt9fSwicG9ydHMiOnt9LCJzdGF0ZSI6IkZBSUxFRCIsInRocm90dGxlZCI6Ik5PIn0=\"\n}\n10:20:32.204 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: move: /var/lib/helios-agent/task-status.json.tmp -> /var/lib/helios-agent/task-status.json\n10:20:32.205 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: set: (/var/lib/helios-agent/task-history.json) {testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548=[TaskStatusEvent{timestamp=1446715232196, host=helios-agent-host, status=TaskStatus{job=Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, resources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}, goal=START, state=PULLING_IMAGE, containerId=null, throttled=NO, ports={}, env={}}}, TaskStatusEvent{timestamp=1446715232205, host=helios-agent-host, status=TaskStatus{job=Job{id=testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548, image=ubuntu:14.04, hostname=null, created=1446199870282, command=[/bin/sh -c 'while true; do env; sleep 60; done'], env={}, \nresources=null, ports={}, registration={}, gracePeriod=null, expires=null, registrationDomain=, creatingUser=myuser, token=, healthCheck=null, securityOpt=[], networkMode=null, metadata={}}, goal=START, state=FAILED, containerId=null, throttled=NO, ports={}, env={}}}]}\n10:20:32.206 helios[14151]: DEBUG [TaskRunner(testjob:1:9d66584)] PersistentAtomicReference: write: (/var/lib/helios-agent/task-history.json.tmp) {\n  \"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\" : [ {\n    \"host\" : \"helios-agent-host\",\n    \"status\" : {\n      \"containerId\" : null,\n      \"env\" : { },\n      \"goal\" : \"START\",\n      \"job\" : {\n        \"command\" : [ \"/bin/sh -c 'while true; do env; sleep 60; done'\" ],\n        \"created\" : 1446199870282,\n        \"creatingUser\" : \"myuser\",\n        \"env\" : { },\n        \"expires\" : null,\n        \"gracePeriod\" : null,\n        \"healthCheck\" : null,\n        \"hostname\" : null,\n        \"id\" : \"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\",\n        \"image\" : \"ubuntu:14.04\",\n        \"metadata\" : { },\n        \"networkMode\" : null,\n        \"ports\" : { },\n        \"registration\" : { },\n        \"registrationDomain\" : \"\",\n        \"resources\" : null,\n        \"securityOpt\" : [ ],\n        \"token\" : \"\",\n        \"volumes\" : { }\n      },\n      \"ports\" : { },\n      \"state\" : \"PULLING_IMAGE\",\n      \"throttled\" : \"NO\"\n    },\n    \"timestamp\" : 1446715232196\n  }, {\n    \"host\" : \"helios-agent-host\",\n    \"status\" : {\n      \"containerId\" : null,\n      \"env\" : { },\n      \"goal\" : \"START\",\n      \"job\" : {\n        \"command\" : [ \"/bin/sh -c 'while true; do env; sleep 60; done'\" ],\n        \"created\" : 1446199870282,\n        \"creatingUser\" : \"myuser\",\n        \"env\" : { },\n        \"expires\" : null,\n        \"gracePeriod\" : null,\n        \"healthCheck\" : null,\n        \"hostname\" : null,\n        \"id\" : \"testjob:1:9d66584a3adc7f3c9ad0c69499ee2df4caa44548\",\n        \"image\" : \"ubuntu:14.04\",\n        \"metadata\" : { },\n        \"networkMode\" : null,\n        \"ports\" : { },\n        \"registration\" : { },\n        \"registrationDomain\" : \"\",\n        \"resources\" : null,\n        \"securityOpt\" : [ ],\n        \"token\" : \"\",\n        \"volumes\" : { }\n      },\n      \"ports\" : { },\n      \"state\" : \"FAILED\",\n      \"throttled\" : \"NO\"\n    },\n    \"timestamp\" : 1446715232205\n  } ]\n}\n```\nThanks again for your help!\n~ HD ~\n. Hi David, \nThis PR is also on the top of my list of suspect issues! \nWhen will this PR be included in the next helios release? \nBR,\n~ HD ~\n. Great,\nThanks a lot!\nBR,\n~ HD ~\n\nHans-Dieter Stich\nhd.stich.photography :: hans-dieter@stich.email :: GPG Fingerprint EA29 9AA5 5E63 EE11 5A82  E1CF DB77 2F67 7E44 9378\nVon: David Xia notifications@github.com\nGesendet: 05.11.2015 20:39\nAn: spotify/helios\nCc: Hans-Dieter Stich\nBetreff: Re: [helios] Deploying an images fails under Ubuntu 12.04 (#730)\nI've made a PR here https://github.com/spotify/helios/pull/734\n\nReply to this email directly or view it on GitHubhttps://github.com/spotify/helios/issues/730#issuecomment-154168332.\n. Seems to be fixed at least with release 0.8.605.\n. @rohansingh Ah, yes, you're right! In our use case with the named volume I've removed the second condition completely and also the printing of the source in case of an error! . Ok, I've separated the checks for the volume source to prevent the NPE and requested another PR.\nApologize the mess, these are my first PRs, I'm more a Gerrit guy. ;-). @mattnworb I'm preparing another PR adding an option to allow also 'named volumes' and this PR needs the above change.\nIt would be fine for me if you cancel this PR and my new PR includes the changes above.. ",
    "negz": "LGTM\n. LGTM\n. Or wait, were you right all along?\nhttps://docs.oracle.com/javase/8/docs/api/java/util/List.html#subList-int-int-\nIt says the high index is exclusive? So if you want to include the entire list you need to specify one index higher than what actually exists.\n. @mattnworb Habit from ALF PRs - it's short for 'peanut gallery'. ;)\n. @rohansingh Should we cap the scope at building helios-up functionality into helios-testing, or do we need to cover the helios-solo CLI use case(s).\nIt might be nice to (somehow) build this such that we can spin up solo from either helios-testing or via the Helios CLI proper? Otherwise we'll be left with the shell scripts hanging around.\n. I'd like to add a test or two to ensure this actually, you know, works. I figured I'd get the PR out early to ensure I'm on a reasonable path.\n. @gimaker I think an integration test makes a lot of sense, but I don't think that lessens the value of this test.\n. Anyone against me merging this?\n. Thanks!\n. @mattnworb Thanks, done!\n@gimaker We should replicate the functionality of https://github.com/spotify/helios/blob/master/solo/helios-up#L56. @rohansingh can likely explain better, but I believe on some machines (Ubuntu desktop?) /etc/resolv.conf just points to dnsmasq on localhost, and thus we currently shell out to nmcli and pass its DNS servers through to Docker. We should be able to do the same from Java via NetworkManager's DBUS API. What's not clear to me is why we need this workaround for dnsmasq, when we don't work around a similar paradigm for unbound in production.\n. https://gist.github.com/negz/34f308bb0bf4a2701837\nIt occurred to me that we could just put the onus on the caller to deploy Helios solo from their tests and pass the resulting client into their TemporaryJobs instance, rather than trying to deploy it magically. Principle of least surprise and all that. Thoughts?\n. Soooo, shall I merge?\n. LGTM\nIn retrospect this was way more straightforward than I had expected it to be when I wrote the TODO.\n. LGTM\nIt does feel a little hacky, but I can't think of a better way.\n. LGTM\n. @mattnworb Whoops - I cut it to move it and never pasted. Fixed.\nI'll take a look at the new stuff in HeliosSoloDeploymentTest. I actually just spotted HeliosSoloIT in the integration tests which AFAIK tests exactly what I want to, but currently doesn't use a HeliosSoloDeployment to turn up solo. I might update that too unless there's a reason for it not to.\n. https://github.com/spotify/helios/blob/master/helios-integration-tests/src/test/java/com/spotify/helios/HeliosSoloIT.java#L182\n@mattnworb - If I understand the above we have an existing IT that does something like what you described. It's currently using its own bespoke Helios Solo deployment code, so we should probably either update it to use the HeliosSoloDeployment or just scrap it.\n. @gimaker I definitely considered it, but only ruled it out for a good reason if you consider being lazy (and not really understanding how spotify-helios-testing works) a good reason. :)\n. Note I've implemented the REGISTRAR_HOST_FORMAT env var by setting it in helios-base.conf in spotify-helios-testing per suggestion, but this PR is still required to ensure resolution of the address returned by the SRV record actually works.\n. Argh. Turns out it's not so simple - setting the REGISTRAR_HOST_FORMAT in helios-base.conf affects TemporaryJobs containers - not the Helios Solo container where it needs to be picked up by the registration plugin. Back to the drawing board. :(\n. @spotify/helios-team PTAL. Pretty sure this will actually work now.\n. @spotify/helios-team PTAL. Pretty confident this is all good - just waiting to be blessed by the CircleCI gods.\n. LGTM\n. LGTM\n. LGTM\n. @mattnworb You mentioned you'd also taken a pass at refactoring PortAllocator. Any suggestions as to how I could clean it up further?\n. @mattnworb @gimaker SGTM - I've already updated the code to take @mattnworb's suggestion into account. Just need to figure out that failing integration test before I update the PR.\n. @spotify/helios-team PTAL. \n. @deverant I can't find where HELIOS_NAME is defaulting to helios.solo. Could you point it out to me?\nI updated this script recently to support the HeliosSoloDeployment Java implementation of Helios Solo. The update was necessary because the HeliosSoloDeployment uses a namespaced HELIOS_NAME like namespace.solo.local.\nI had intended my change to cover the above HELIOS_NAME as well as solo.local. as set by helios-up, but I missed the place where we set HELIOS_NAME to helios.solo.\n. https://ghe.spotify.net/helios/spotify-helios-testing/blob/master/src/main/resources/helios-base.conf#L9\nhttps://github.com/spotify/helios/blob/af0c71efcfef49b37eb99e0086fd4e1dab471cce/helios-integration-tests/src/test/java/com/spotify/helios/HeliosSoloIT.java#L95\nI think service discovery inside Helios Solo could break if the SkyDNS domain is set to anything but .local due to the interaction with TemporaryJobs as configured by helios-base.conf above. SPOTIFY_POD and SPOTIFY_DOMAIN are set to to randomjobprefix.local in an attempt to namespace them to the relevant job. At least it would break for the HeliosSoloIT above. Not sure if others follow that convention.\nAll in all these interactions are super confusing and hurt my brain. :(\n. @deverant Wouldn't the curl to work as long as the HELIOS_NAME ends in .local?\nmessremb(negz@spotify-puppet)$ echo moo.solo.local.|python -c \"import sys;h=sys.stdin.read().strip().split('.');h.reverse();print '/'.join(h)\"\n/local/solo/moo\nmessremb(negz@spotify-puppet)$ echo solo.local.|python -c \"import sys;h=sys.stdin.read().strip().split('.');h.reverse();print '/'.join(h)\"\n/local/solo\n. Ahhh, the double slash breaks it? Gotcha.\n. I switched to VMWare Fusion based docker-machine this morning and have subsequently spent the last four hours trying to get a working Helios Solo to replicate this. I'm going to take a break before I throw my laptop out the window and look again tomorrow.\n. LGTM!\n. @mattnworb All good points.\nI hadn't considered the deployment issue. I think it's likely that a migration would be needed and both versions couldn't coexist at the moment. I'd have to double check to be sure, but I think chances of everything just working are pretty slim.\nSpace wise, could we perhaps trim the history to some reasonable number? Currently the additional overhead is:\n- Two new JSON blobs created per rolling update (one for jobId and rollout options, another for status)\n- A list of rolling updates associated with each deployment group\nWe could possibly trim this down to the 10-20 latest rolling updates per DG?\n. @davidxia Thanks for the review! Yeah the integration tests are not passing yet so there's probably still bugs in here. The idea was to get it out for review to get feedback while I was working on finding and fixing them.\n. We decided not to proceed with this PR in our retro today. We like the change conceptually, but we're concerned that the time and effort required to validate and deploy it outweighs its benefits. I'll leave the code in my rollup branch.\n. LGTM\n. LGTM\n. Derp. LGTM.\n. LGTM\nThough ideally TemporaryJobs would forego all the magic Helios-client-discovery logic if someone explicitly passes a client to the Builder.\n. LGTM\n. @mattnworb PTAL\n. @davidxia I forget, but it could have been me erroneously assuming that we had to rm the containers in order to ensure we pulled the latest image on the next run.\n. The new tests are super dirty copy pasta. I'm open to suggestions as to how they might be tightened up a bit.\n. @mattnworb This way feels more intuitive to me personally.\nThat said, I suppose it depends whether you frame it as \"the deployment group is failed\" or \"the rolling update you performed on the deployment group failed\". The lack of separation between rolling updates and deployment groups really muddies the waters for me.\nThe alternatives:\n- Break up the DeploymentGroupStatus states per your proposal in Trello. We'd break FAILED into two new states, one being \"this deployment group failed during a host update, so it's kind of unstable but it's cool to automatically deploy again\" with the other being \"this deployment group failed during a manual update, so it's unstable and we shouldn't allow any automatic updates until a human intervenes\". I veered away from that proposal because I found it difficult to succinctly capture the above concepts in an enum. (HOSTS_CHANGED_FAILED and MANUAL_FAILED?)\n- Set the DeploymentGroupStatus to DONE rather than FAILED at https://github.com/spotify/helios/blob/master/helios-services/src/main/java/com/spotify/helios/rollingupdate/RollingUpdateOpFactory.java#L164 if the RollingUpdateReason was hosts changed.\nI think either of the above implementations would be best served by modeling the RollingUpdateReason in the DeploymentGroup per this change, so they'd share a bit of code anyhow.\n@spotify/helios-devs  What do you guys think?\n. > I agree with this (but I don't know how hairy it would be to implement so take my opinion lightly).\n@gimaker @mattnworb I believe it would be pretty trivial to implement, as it would share a bunch of implementation with this PR.\nI prefer the implementation in this PR because, while both options are flawed, it seems better for a rolling update that leaves some hosts in a broken state to (or a deployment group with some hosts in a broken state) to indicate that by having state FAILED. Even if the next rolling update would be allowed. My understanding is that you're proposing we leave the deployment group in state DONE if a HOSTS_CHANGED rolling update fails.\n. > I also think we should not trigger rolling-updates when hosts change if there's already a manually triggered rolling-update in progress (since it, depending on the exact implementation, leads to really subtle behaviour).\n@gimaker That seems reasonable, but it's tangential to the intent of this PR right? i.e. We allow new HOSTS_CHANGED rolling updates to start while MANUAL rolling update is ROLLING_OUT at the moment.\n. > +1, in a separate PR\n@mattnworb I just noticed @gimaker's comment at https://github.com/spotify/helios/pull/932#discussion-diff-64818492R487. I think the behaviour described was actually introduced in this PR so maybe it is actually worth doing here?\nPreventing HOSTS_CHANGED rolling updates when we're ROLLING_OUT is easy, but I have no idea how we'd get those hosts added later. Just hope the races were rare and ask people to do a MANUAL rolling update if they hit that edge case?\n. > I have no idea how we'd get those hosts added later. Just hope the races were rare and ask people to do a MANUAL rolling update if they hit that edge case?\n@gimaker just pointed out that updateDeploymentGroupHosts() is called by a reactor once per second, so we could just return early without saving the new hosts if a rolling update was in progress and it would try again.\n. LGTM\n(Though I'm still not a fan of maintaining these scripts and HSD.)\n. I won't argue that they don't add value - my main concern is around the cognitive and (potential) support burden of maintaining two similar yet different implementations of the same concept. I wonder whether the value they add is worth that.\n. LGTM\n. We'll indicate that the terminal state was HEALTHCHECKING in stateInfo. Possibly makes the errors a little less readable, but it will shed light on whatever the final state was without needing a switch or a bunch of conditionals.\n. I've configured IntelliJ to use the Google style guide and it refuses to let me have these at the indent level I found them.\n. /me learns how to do that.\n. Yeah - this comment was ganked from the same updateState method I ganked most of it from. I could get rid of both?\nAs an aside, I'm also open to implementations that avoid me having to call update() the statusUpdater twice.\n. I'll take care of that in a separate PR. :)\n. In TaskStatus.java containerError is created like:\njava\nthis.containerError = Optional.fromNullable(builder.containerError).or(\"\")\nIs it safe for me to assume that means taskStatus.getContainerError() should never return null? If so, is there still value in using isNullOrEmpty()? I notice there's also an isEmpty() method available on taskStatus.getContainer() already.\n. Done!\n. Done!\n. Fixed, hopefully. I scrapped the autogenerated idea and hand-updated the hashCode. :(\n. +1 - I tried to do something similar using the @Mock annotation, but was told off by IntelliJ.\n. https://github.com/spotify/helios/commit/ab133a06749986d1e5707ca6913aaea9ccda8e13\nI've really just copied the pattern from Drew's implementation in testPullTimeoutVariation. I'm open to switching ExpectedException, but I'm not clear as to how I'd inspect the ExecutionException to discover the real RuntimeException within if I did so.\n. I saw doThrow in the test above and used tab completion to test whether there was a similar doReturn. I then noticed the when().thenReturn() pattern in another test file. So not really. Is there a reason to prefer one or the other?\n. @gimaker mentioned that when().thenReturn() is the more common pattern in the codebase, so I'm going to switch it over.\n. So I think I'll leave it this way if it's not a huge deal. run() is explicitly calling result.setException() so it seems nice to ensure that's being propagated, and it matches the pattern of the test above.\n. It was like that when I copy pasta'd it from services-common. I'll add a TODO for now and fix it when we copy the implementation into the docker-client in a future PR.\n. Out of curiosity, is this purely stylistic or are there other advantages in Java? For whatever reason I generally use the loop, but I don't feel strongly about it.\n. I know I'm a bit late to the game here but for future reference there's BASH fanciness for this:\nbash\nmessremb(negz@~)$ echo $JAVA\nmessremb(negz@~)$ echo ${JAVA:-/usr/bin/java}\n/usr/bin/java\nmessremb(negz@~)$ export JAVA=/usr/bin/megajava\nmessremb(negz@~)$ echo ${JAVA:-/usr/bin/java}\n/usr/bin/megajava\n. I added it recently.\n. These patterns are handy to know, and I'd prefer to use them, but I don't think I can due to the fact that portRange is a Guava Range and not a Collection. I could probably do a song and dance to convert it into some kind of Collection, but I think that would be even messier.\n. :s/deployment group/rolling operation/\n. Done, thanks! I swore I was doing that before - must have been a regression.\n. Done.\n. I don't actually know. I was talking about this with @mattnworb the other day - we seem to have Builders with both patterns. I used this pattern because I copy pasta'd from DeploymentGroup.\n. You're right - group name and reason should not be nullable.\nI wonder about jobId though. For some reason that was nullable on DeploymentGroup. Maybe it shouldn't be on RollingOperation? Though I was thinking maybe a RollingOperation with a null jobId could perform a rolling restart....\n. Ack\n. Ack\n. Done.\n. No, I must have missed that elsewhere. What does it do?\n. Ack.\n. Good point. Will do.\n. Sound good.\n(It would be really nice to switch the whole codebase over to one consistent builder pattern.)\n. Most likely it just didn't occur to me at the time. Though possibly it was to reuse the existing ZookeeperClient. I suppose I could overload getDeploymentGroup to allow passing in a client.\n. No, this was mostly just for testing/internal purposes.\nHow might I ensure predictable IDs for rolling operations in tests if I removed this?\n. Is this a style preference, or are there other advantages? I've always felt this way was cleaner. (I think I remember you talking about this with @gimaker recently?)\n. For now yes (I didn't know about @VisibleForTesting), but I expect we'll want to expose it in future so we can add functionality to list recent rolling ops for a DG via the API/CLI.\n. No, I think that's an oversight. Good catch.\n. Not sure. I'm afraid you'd have to ask whoever wrote it originally.\n. Yeah, oversight on my part I think.\n. We're not very consistent with this at the moment. My implementation follows what pattern I can see - i.e. this isn't used in DeploymentGroupStatus either (but is in DeploymentGroup).\nFWIW I'm not a fan of the concept in general. I prefer a JSON API with nil values than a JSON API that returns different keys under different circumstances.\n. We're not very consistent with this at the moment. My implementation follows what pattern I can see - i.e. this wasn't used in DeploymentGroupTasks either (but is in DeploymentGroup).\nFWIW I'm not a fan of the concept in general. I prefer a JSON API with nil values than a JSON API that returns different keys under different circumstances.\n. Thanks. Could we make our linter catch this?\n. I don't think testing RolloutOptions fits here. I've added a separate test for RolloutOptions.\n. I mean I added one in response to your comment. I'll push it soon.\n. I actually have no idea why this changed. Might have been a find and replace snafu.\n. Left it public but added it to the interface for future use.\n. Done.\n. Okaaaay done. But I prefer my way. :(\n. https://github.com/spotify/helios/pull/903/files#diff-64b87e65f5290ca066c979f34868c419R135\nThe above test currently requires me to create RollingOperations with a predictable ID, because I can't mock out calls to ZK paths composed of UUID that I don't know in advance of its creation.\nI could possibly avoid this situation by breaking said test up into a few smaller ones, but it doesn't seem that bad to me to expose a knob to override the randomly generated UUID even if we don't use it much.\n. Done.\n. Isn't that still the case?\nMy understanding is that previously it would hit client.getNode(Paths.statusDeploymentGroupHosts(groupName)), raise a NoNodeException, and thus return if the DG didn't exist. That should still be the case.\nAt the line you've commented on it's handling the case where the deployment group exists, but has never been deployed to before. So client.transaction(operations) should only be committing the operation that adds the updated hots.\nWhat am I missing?\n. Done (except the tasks bit per your other comment).\n. Done.\n. Done.\n. Why change this?\n. Would this be susceptible to the same issue described in connectToWatchdog()? i.e. Do we need to read/write some data to ensure it's really connected?\n. Typo in guarantee. :)\n. Is it worth setting this as an env var, rather than having it static in the Python script?\n. Oh, I see now this goes back to latest here. \n. We don't need to determine the gateway, but could there still be value in running the probe container when using Docker for Mac?\nFor context, checkDockerAndGetGateway() used to be two separate methods with distinct responsibilities - one to test whether we can communicate with Docker from inside the container and another to determine the gateway IP from inside the container. We merged them because they spun up the same Docker image twice for different reasons.\n. Thanks! I double checked for spaces but apparently needed to triple check. :( \n. Oh, right. Wrong getJobId. Yeah I was concerned about fixing it due to backwards compatibility. I could remove the TODO and just update the javadoc and leave a comment near the constructor annotation?\n. Is that preferred? I knew you could, but thought for some reason I should default to equals.\n. Okay, changed. :)\n. Done.\n. Done.\n. > This PR also changes behaviour slightly in that a DG can go from FAILED to ACTIVE without user-intervention.\nI thought that was kind of the point. :) \n. Do you think it's really worth explaining here when it's also explained in the allowHostChange javadoc? I considered it but it seemed like it might be excessive.\n. Good point. I've taken a pass in https://github.com/spotify/helios/pull/932/commits/425a04f9e3749545af2d1a4281d2db0613d94a68. Does that look about\u00a0right?\nI feel like updateDeploymentGroupHosts is getting a bit bloated, so I'm open to refactoring if anyone has any ideas.\n. Won't the NoNodeException get caught waaaay down at line 558?\n. Oh, gotcha. Fixed.\n. I'll leave it as is.\n. @saunakchakrabarti Yes, albeit automatically. My IntelliJ should be configured with the Spotify style guide, and does automatic import sorting so that I don't go crazy fighting Checkstyle. \n. I'm tempted to address this TODO as a separate commit in this PR, but I'm not quite sure about the current implementation.\nDo we ask the runner to stop the container, but then fall back to killing it in the supervisor because we want to handle the case where the container is somehow running but its runner is not? If that's not a concern it might be nicer to move the retry logic into the runner and just have it send a bunch of docker.stopContainer()s with the requested SIGKILL deadline.\n. Ack, SGTM.\n. Ah good to know. I felt like I might have been doing a little too much copying here.\n. Thanks. This was copy-pasta but your proposed style fits with the rest of the class better.\n. This should not happen, because two masters cannot work on the same rolling update at the same time, right?\n. I guess this would need to be created for all existing DG's? Or I could just change the code to create it if it doesn't exist?\n. Per discussion with @gimaker this can happen. I'll add a check().\n. @gimaker Please confirm that this giant comment is accurate. \ud83d\ude04 \n. > does the list of hosts stored in this path for \"removed hosts\" grow unbounded over time? \n@mattnworb Hosts are removed from this list if and when they are 'marked undeployed' per https://github.com/spotify/helios/pull/986/commits/58fedba3cae84ed8125fee61827057d2d9cf26b7#diff-8d500c531ffb1f7836e9d5b60585a002R979.\nI think hosts will be 'marked undeployed' if they're unregistered from the Helios master, because the getDeployment(host, jobid) check at  https://github.com/spotify/helios/pull/986/commits/58fedba3cae84ed8125fee61827057d2d9cf26b7#diff-8d500c531ffb1f7836e9d5b60585a002R948 will come up blank and assume the job is gone, causing the 'await stopped' rollout task to proceed to the 'mark undeployed' task.\nHowever any agents that are removed without unregistering will cause the list of stored hosts to grow unbounded, and potentially worse will cause any rolling update of the deployment groups they match to fail until the agent either comes back online or is unregistered.\n. @mattnworb \n\nThe best approach is for every piece of code that tries to check this path to be able to handle the case where it does not exist.\n\nAgreed. Will do.\n. @davidxia \n\nWe have a CreateWithDataAndVersion class. It might be nicer to make a SetWithDataAndVersion class instead of doing both check() with version followed by a set() without a version and executing both in a transaction.\n\nI agree, but I don't really want to do it in this PR. The current implementation follows a pattern we're already using - https://github.com/spotify/helios/blob/master/helios-services/src/main/java/com/spotify/helios/master/ZooKeeperMasterModel.java#L532\n. Why? Is there some technical advantage, or is this a style convention?\n. You're right. I'll remove it.\n. It is, thanks. It's an artifact from when I was trying a different approach.\n. > by \"removed\" do you mean the host's labels no longer match the DG selector?\nIn this context I mean more like the agent's host is recycled or whatever without unregistering from Helios.\n. Discussed this with @mattnworb in person. There's only one condition I can envision where this list will grow unbounded, which is if an unbounded amount of agents are removed from a DG (i.e. the agents are still running, but with labels that no longer match the DG) and we're unable to undeploy the job from those machines for some reason. In this case the failure to undeploy will cause the rolling update to fail, so hopefully the DG owner will notice and take action.\nI was also concerned about the case where agents disappear permanently without being degregistered from Helios (i.e. the agents' hosts are decommissioned without deregistering). This would cause the list to grow unbounded, except that we have code to automatically deregister these AWOL agents after ~7 days, at which point the undeploy will 'succeed' because the deployment of the DG's job to the agent will have been deleted.\nI'm going to add a test for the condition explained in the second paragraph, just to be sure.\n. Or rather re-order it, since we do actually want to catch that condition.\n. If you're actually curious I attempted to explain it in https://github.com/spotify/helios/pull/986/commits, and by answering @davidxia's question on this PR. Point taken regarding documenting it in code though.\n. Sorry. I would appreciate any tips on making IntelliJ less crap at managing imports. I'll disable my import-optimisation for now.\n. Having renamed this method to redundantUndeployment and added a docstring I strongly feel my current implementation is clearer than this suggestion. I think it makes it more obvious why we're doing these checks.\nAs someone new-ish to the codebase I had a really hard time parsing the existing code and what its purpose was. It would be obvious to me from your proposed docstring what the desired state was, but not why.\n. I've broken the 'migration' bit out into its own 'isMigration' check.\n. It's something like REMOVED_HOSTS_PENDING_UNDEPLOYMENT but statusDeploymentGroupRemovedHosts is already long enough. I don't really want to introduce statusDeploymentGroupRemovedHostsPendingUndeployment.\nI'll add a comment, but I'm open to more succinct/clear naming suggestions.\n. (Which will make it the only commented path amidst an entire file of paths that could really use explanatory comments.)\n. Done.\n. Added javadoc and also renamed the var to something more self explanatory.\n. Done.\n. Done. Nice catch.\n. Good point. I don't expect this to happen. The KeeperError check is for the getNode, and it shouldn't be possible to hit this code without Paths.statusDeploymentGroupRemovedHosts() having been written either at hosts changed time or deployment group creation time. I'll make it an error.\n. Done.\n. Oh yeah. That line was supposed to go away. Fixed.\n. Done.\n. Sure. I just copied the existing pattern from rollingUpdateStep(). I'll log at info in both places.\n. My understanding is that if this call fails startContainer() will throw SecretVolumeException and the Supervisor will retry.\nThere's an open question as to what to do if this call fails due to secrets already existing for the requested container name. Currently secret-volume will return a 500 and thus this would fail forever until the secrets were manually destroyed (either via the secret-volume API or just unmounting them manually).\nsecret-volume could, instead of returning 500, either:\n- Just return 200 OK, thus causing whatever secrets exist at the path to be exposed. This could be insecure and lead to stale secret data being exposed.\n- Internally destroy the existing secrets at the requested path and attempt to replace them with newly procured secrets.\n. ",
    "atombender": "I'd recommend seriously considering it. Becoming consensus-backend-agnostic seems to be a trend (e.g., see Kafka, Mesos, Docker Swarm).\nThe proliferation of consensus databases (there are now three leading ones: Etcd, ZooKeeper, Consul) poses a complexity and system administration problem.\nAs far as I know, Etcd has feature parity with ZK \u2014 hiearchical node tree, autoincrement keys, watches, etc. Might not be too hard to abstract.\n. ",
    "lvidarte": "Very clear, thanks\n. ",
    "igorbernstein": "Sweet! thanks for fixing this...it prevented me from using helios testing last time around. I'll try to reintegrate helios-testing next time I have a free moment\n. ",
    "atsticks": "As a Workaround add something like the following and it will smoothly compile and run AFAIS:\n``` java\npackage com.spotify.helios.common;\n/*\n * Created by atsticks on 15.01.16.\n /\npublic class Version {\npublic static final String POM_VERSION = \"0.8.0-SNAPSHOT\";\npublic static final String RECOMMENDED_VERSION = \"0.8.0-SNAPSHOT\";\nprivate Version(){}\n\n}\n```\n. ",
    "spdage": "+1\n. ",
    "pehrs": "@davidxia I have a service that exposes an UDP port and the default Prober implementation assumes that ports are TCP no UDP and will hang because it cannot see the tcp port\n. ",
    "deverant": "@negz Oops, maybe the HELIOS_NAME setup is a mismatch with some earlier version of helios. Can't see it in the current master at least myself either.\nI guess that means the DOMAIN setup is not needed, but I think the curl to setup skydns records in etcd is still broken and needs fixing?\n. Check my initial commit message, the URL has a double forward-slash and that, at least when I tested, returned 301 instead of doing anything meaningful.\n. ",
    "mzeo": "I believe the problem comes from here:\nhttps://github.com/spotify/helios/blob/master/helios-testing/src/main/java/com/spotify/helios/testing/TemporaryJobs.java#L145\nAs I'm not using junit, the test writer isn't created (https://github.com/spotify/helios/blob/master/helios-testing/src/main/java/com/spotify/helios/testing/TemporaryJobs.java#L337)\nOtherwise my logback.xml works as I expect, and looks like this:\n```\n\n\n\n\n\n        %d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n\n      \n\n\n\n\n\n\n```\n. \ud83d\udc4d \n. done\n. It looked odd to me as well. I removed the finally, because it was totally unnecessary. I think it looks better now.\n. ",
    "gilles": "nice.\nFor the test since you're reading via a Resource, you should be able to just put test files in test/resources/target/... and test/resources/docker/\n. This assumes you're running with maven or any build system that relies on the maven directory structure\n. Maybe this could be null which means disabled by default?\n. ",
    "spkrka": "Ping @mattnworb @davidxia @gimaker @rohansingh \nI might need help adding this setting for helios-solo. My idea is to set the grace period to something like a day for helios-solo, which would keep containers alive for the period of the integration test even if there are multiple helios-solo instances running.\n. Yes, if #920 solves it, I guess my primary motivation disappears. Not sure if this is still useful in general, though I do think the current reaping might be overly aggressive.\nIf you want to adopt this PR that's fine, otherwise I can close it.\n. Yes, that seems like a good plan!\n. Ping @mattnworb @davidxia @rohansingh @gimaker \n. Thanks, perhaps the errors I saw when running helios-solo was from an older version, so this change is pointless. Should I close the PR?\n. I agree!\n. ",
    "daenney": "I can use an IP but in this case that would be localhost/127.0.0.1 which also seems to confuse Helios: https://github.com/spotify/helios/blob/9b45e0ded17147d9f6081874c80b5bf94a1dd4da/docs/integration_tests_with_helios_solo.md#issues-if-docker_host-refers-to-localhost-or-127001 \ud83d\ude1e \nUnfortunately I don't get additional beta keys to hand out but after signing up for it I got my key within 3 days. So big chance that you can get one for yourself fairly quickly.\n. It doesn't set DOCKER_HOST for you as the CLI binary that comes with it (and the other components) know where to find the socket in this case and just use that. Similarly it doesn't require the eval $(docker-machine env default) stuff.\nThe part I haven't figured out is why it goes coocoo when you specify a tcp://127.0.0.1:2375 as DOCKER_HOST, then even the Docker for Mac CLI breaks but setting it to a unix socket always works as expected even though it clearly is bound on that port. I'm guessing that has to do with how they're leveraging xhyve somehow.\n. Sure thing, here you go.\n``` java\n    private static final HeliosDeploymentResource soloResource =\n            new HeliosDeploymentResource(HeliosSoloDeployment.fromEnv().build());\nprivate static final TemporaryJobs temporaryJobs = TemporaryJobs.builder()\n        .client(soloResource.client())\n        .build();\n\n@ClassRule\npublic static final RuleChain chain = RuleChain\n        .outerRule(soloResource)\n        .around(temporaryJobs);\n\n@BeforeClass\npublic static void setupContainers() throws IOException {\n    TemporaryJob podserviceJob = temporaryJobs.job()\n            .image(\"<the-registry>/spotify/podservice-integrationtest:latest\")\n            .port(\"podservice\", 12000)\n            .deploy();\n\n``\n. Interesting rabbit hole you've gone down so far \ud83d\ude04 \n. This needs an additional space:\" exited during ...otherwise you'll get thecontainerId` glued to it.\n. ",
    "sfat": "yo, @drewcsillag, check this out: http://blog.xebia.com/wp-content/uploads/2008/10/organize_static_imports.gif\nSeems to be as same as in IntelliJ (as I use IntelliJ and hate wildcards). You need to specify a silly huge number to avoid getting your imports wrapped in wildcards. For instance, I specify a number like 2000 for each type (non-static and static).\n. ",
    "saunakchakrabarti": "Thanks - I'll remove (4) - wondering if there is a better place for that?  Perhaps, the Onboarding checklist?\n. Thanks - I'll move into the developer notes and use the backticks.\n. Just curious - how come the import order changed?  Was this also fixing a style issue?\n. ",
    "lndbrg": "maybe rename this arg to to something more logical?\n. getPubsubOptions should probably be renamed to getPubsubPrefixes\n. You're missing the topic for interpolation here.. ",
    "zalenski": "updated\n. I'll change to empty list. It probably even breaks the code using it right now.\n. Yes, that would be the idea. I followed the current style of not documenting the topics\u2026 ;)\nIs there a good place to put it? A new .md file in docs/?\n. updated\n. Added a minimal event_senders.md\n. Updated to only add the kafka sender to the event sender list if the kafka producer is present.\n. done\n. added some tests.\n(PubSubOptions.getService() starts a real pubsub service straight away, so did a senders(Supplier<PubSub> pubsubSupplier) to test it)\n. ",
    "carlpett": "Indeed. Fixed!. ",
    "caipre": "You could use $1 here rather than stop/start to give the exact command.. Yeah, they're not mutually exclusive at all. I'm not sure there's much value in doing both though. So long as the master is https, the client cert is only used for authentication, and examining the token seems like a better method.. Fair point. I'll make the change.. Done.. I'm going to skip this one; happy to learn more about it, but feels unnecessary for the moment.. At least for the cases I've looked at, errMsg includes that information.\nhttps://github.com/spotify/helios/blob/master/helios-services/src/main/java/com/spotify/helios/master/ZooKeeperMasterModel.java#L846. If I'm [reading this correctly][0], it shouldn't matter. As you say, the topic is used to test the connection and authorization. Both the HeliosTaskStatusEvents and HeliosDeploymentGroupEvents topics are meant to exist, it's just that we haven't been publishing messages to the latter.\n[0]: https://github.com/spotify/helios/blob/master/helios-services/src/main/java/com/spotify/helios/servicescommon/EventSenderFactory.java#L71-L74. Minor: credentials isn't final above, so you could just replace it inline.\nif (!scopes.isEmpty()) {\n    credentials = credentials.createScoped(scopes);\n}. I think we want the null here. The Builder uses DEFAULT_ROLLOUT_OPTIONS, and that's the preferred way to construct a job. If you use the raw constructor, we should allow null. Whether Optional.fromNullable(null).orNull() makes any sense is a different question. \ud83d\ude09 . On the other hand, if we fill in the null values on job create, the config file used to create the job will differ from the config stored in zk. Though I suppose that's strictly true even when we keep the null values, since our JSON reader allows missing fields while the writer writes all fields.\nField in config / cli|Behavior|Field in Zookeeper / helios inspect\n-|-|-\nPresent|Fill with null|As given in config / cli\nPresent|Fill with default|As given in config / cli\nAbsent|Fill with null|Strictly differs but logically equivalent\nAbsent|Fill with default|Differs from config and may be unexpected\nNeither alternative to the handing of absent fields is clearly preferable in my view. I'd say that filling with defaults gives the most information, and can be clear enough if we document the defaults and their application during job creation.. We should add a test where the RolloutOptions are partially specified.. I changed the behavior of this Builder to not set defaults for fields since I wanted the rollingUpdateCommand to be able to send a partially filled out set of options. An alternative approach might be to preserve the \"builder gives defaults\" pattern here and change this line to use the raw constructor.. Above this line a bit: https://github.com/spotify/helios/pull/1165/files#diff-09ebe80c4b7d83ee0a1323aed194d6baR172. Are you willing to assert that every codepath that constructs a DeploymentGroup does so with a RolloutOptions that has all fields filled in?. This is how I worded it in the internal document:\n\nWhen a deployment is issued to a helios master, the RolloutOptions will be constructed from the request parameters and job configuration with the following precedence:\n\nParameters included in the API request (eg, flags passed on the command line)\nParameters defined in the job configuration (of the job being rolled out)\nThe default value for the parameter as defined in the RolloutOptions class. Probably this should be configurable, but read-only is a good first step.. What about moving these to GoogleCredentialsAccessTokenProvider.DEFAULT_SCOPES?. An explanation of why this is a problem may be useful (or a link to docs explaining it). Something like \"This configuration will prevent new containers from deploying during the gracePeriod because of port conflicts.\". The method accepting a Map<String, Map> was deprecated; apparently the value isn't needed.. It's covered by the next line. \ud83d\ude09 . Good idea, I'll add a check.. Done.. \n\n",
    "mavenraven": "Ah, thanks for the tip.. Great idea, thanks!. thanks, fixed. fixed. \ud83d\udc4d . nit: We could use EMPTY_ROLLOUT_OPTIONS here. @davidxia @caipre is adding this necessary?. yep, exactly!. @davidxia this will still throw if getRolloutOptions() returns null, right?. ",
    "danielhan": "@dxia added test for timeout during rollout. @davidxia added test for group id change during rollout.. @dxia making sure cli option takes precedence. @mattnworb I made the changes you suggested.  One thing to note is that Inet4Address is not mockable, being a final class, nor does it have a public constructor.  These characteristics prevent the possibility of cleanly unit testing, so I combined the logic into a single method.. @davidxia @mattnworb updated logic so that we only guess the IP when we find the loopback address.  as i mentioned earlier in response to a question that Matt had, if a user maps an externally routable IP address the Mac's name in /etc/hosts, InetAddress.getLocalHost() returns that IP.. ",
    "lavanyachennupati": "done!. done!. ",
    "aero2806": "Yeah \n. ",
    "myott": "\ud83d\udc4d absolutely. Appears that you can do this.runtime = runtime here.. ",
    "vbhavsar": "Hmm, you're right. I wonder why other fields follow this pattern. . Will address this in a separate PR.. It is a bit confusing to read. I will change the wording. . "
}